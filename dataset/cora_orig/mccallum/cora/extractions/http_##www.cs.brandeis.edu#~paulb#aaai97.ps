URL: http://www.cs.brandeis.edu/~paulb/aaai97.ps
Refering-URL: http://www.cs.brandeis.edu/~paulb/pub.html
Root-URL: http://www.cs.brandeis.edu
Email: fjamesp,marc,paulbg@cs.brandeis.edu  bkb@apple.com  johnston@cse.ogi.edu  
Title: Semantic Indexing and Typed Hyperlinking  
Author: J. Pustejovsky B. Boguraev M. Verhagen P. Buitelaar M. Johnston 
Address: Waltham, MA 02254-9110,  California,  Portland, Oregon 97291-1000, USA,  
Affiliation: Department of Computer Science, Brandeis University Computer Science Department,  Intelligent Systems, Apple Computer, Cupertino,  Center for Human Computer Communication, Oregon Graduate Institute,  
Abstract: In this paper, we describe linguistically sophisticated tools for the automatic annotation and navigation of on-line documents. Creation of these tools relies on research into finite-state technologies for the design and development of lexically-intensive semantic indexing, shallow semantic understanding, and content abstraction techniques for texts. These tools utilize robust language processing techniques to generate multi-purpose data structures called lexical webs, used in the system TexTract, an automated semantic indexing program designed to parse, index, and hyperlink electronic documents. 
Abstract-found: 1
Intro-found: 1
Reference: <author> S. Bergler, </author> <title> Evidential Analysis of Reported Speech, </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, Bran-deis University(1992). </institution>
Reference: <author> B. Boguraev, </author> <title> Building a Lexicon: The Contribution of Computational Lexicography, </title> <editor> in: L. Bates and R. Weischedel, eds. </editor> <booktitle> Challenges in Natural Language Processing (Cambridge University Press, </booktitle> <address> Cambridge and New York, </address> <year> 1992). </year>
Reference-contexts: Impor tant for our concerns in the present research is to ac-knowledge that lexical semantics is both an expressive and highly structured knowledge base, incorporating knowledge which linguists rarely associate with words <ref> (cf. Boguraev, 1992) </ref>. This includes information relating, for example, not only what an object is, sortally, but also what it is used for, how it comes about, and what it is made of; these are termed an object's qualia structure (cf. Pustejovsky, 1991).
Reference: <author> B. Boguraev, </author> <year> (1996). </year> <title> "WordWeb and Apple Guide: Comparative Indexing over Technical Manuals," </title> <institution> Apple Research Laboratories, </institution> <type> Technical Report TR-107, </type> <institution> Cupertino, </institution> <address> CA. </address>
Reference-contexts: While recent work in lexical acquisition has demonstrated that statistically-based methods for mining large text corpora for lexical and world knowledge are both feasible and useful, the most successful techniques are proving to be those which integrate statistical methods with language-specific knowledge encoding techniques <ref> (cf. Boguraev and Pustejovsky, 1996 and Weischedel et al., 1994) </ref>. This is the approach taken in the Core Lexical Engine project and the one we assume here. <p> One task to which such domain catalogs have been applied is that of the automatic population of Apple Guide databases, which drive the delivery of context-sensitive on-line help in the Macintosh OS environment <ref> (cf. Boguraev, 1996) </ref>. Given the high precision of the linguistic analysis, the contextually-sensitive patterns targeting the syntactic environments of domain objects, and the closed nature of technical domains, this kind of system is capable of achieving very high degrees of recall and precision.
Reference: <author> C. Kennedy and B. Boguraev, </author> <year> 1996a. </year> <title> Anaphora for everyone: Pronominal anaphora resolution without a parser. </title> <booktitle> In The Proceedings of the 16th International Conference on Computational Linguistics. </booktitle> <address> Copenhagen, Denmark. </address>
Reference-contexts: Johnston et al., 1995). Other issues currently being dealt with in the context of improving acquisition and shallow semantic analysis include: discourse anaphoric processing and sor-tal anaphora <ref> (cf. Kennedy and Boguraev, 1996a) </ref>; and identification of salient topics and topic structure within text (cf. Kennedy and Boguraev, 1996b).
Reference: <author> C. Kennedy and B. Boguraev, </author> <year> 1996b. </year> <title> Anaphora in a wider context: Tracking discourse referents. </title> <publisher> In W. </publisher>
Reference-contexts: Johnston et al., 1995). Other issues currently being dealt with in the context of improving acquisition and shallow semantic analysis include: discourse anaphoric processing and sor-tal anaphora (cf. Kennedy and Boguraev, 1996a); and identification of salient topics and topic structure within text <ref> (cf. Kennedy and Boguraev, 1996b) </ref>. From Indexing to Lexical Webs In essence, a Lexical Web is the next generation of the domain cataloging strategy, one which is made possible by the general functionality of the Core Lexical Engine technology.
Reference: <editor> Wahlster (ed.), </editor> <booktitle> The Proceedings of the 12th Euro-pean Conference on Artificial Intelligence. </booktitle> <address> London: </address> <publisher> John Wiley and Sons, Ltd. </publisher>
Reference: <author> B. Boguraev, J. Pustejovsky and M. Johnston, </author> <title> forthcoming, Content Abstraction and Indexing for Homogeneous Corpora. </title>
Reference: <author> B. Boguraev and J. Pustejovsky, </author> <title> Corpus Processing for Lexical Acquisition (Bradford Books/MIT Press, </title> <address> Cambridge, MA, </address> <year> 1996). </year>
Reference-contexts: While recent work in lexical acquisition has demonstrated that statistically-based methods for mining large text corpora for lexical and world knowledge are both feasible and useful, the most successful techniques are proving to be those which integrate statistical methods with language-specific knowledge encoding techniques <ref> (cf. Boguraev and Pustejovsky, 1996 and Weischedel et al., 1994) </ref>. This is the approach taken in the Core Lexical Engine project and the one we assume here. <p> One task to which such domain catalogs have been applied is that of the automatic population of Apple Guide databases, which drive the delivery of context-sensitive on-line help in the Macintosh OS environment <ref> (cf. Boguraev, 1996) </ref>. Given the high precision of the linguistic analysis, the contextually-sensitive patterns targeting the syntactic environments of domain objects, and the closed nature of technical domains, this kind of system is capable of achieving very high degrees of recall and precision.
Reference: <author> P. Buitelaar, </author> <year> 1997a, </year> <title> A Lexicon for Underspecified Semantic Tagging, to appear in: </title> <booktitle> Proceedings of ANLP 97, SIGLEX Workshop, </booktitle> <address> Washington DC. </address>
Reference-contexts: Roget and WordNet, however, are both used in the construction of CoreLex, since they are vast resources of lexical semantic knowledge which can be mined, restructured and extended <ref> (for discussion of this process, cf. Buitelaar 1997a and 1997b) </ref>. The top lattice structure is defined as those typed feature structures corresponding to the ten top under-specified categories, together with the categories generated by the application of projective transformations on this category, including :, &lt;, ffi, &gt;, act, and =.
Reference: <author> P. Buitelaar, </author> <year> 1997b, </year> <title> CORELEX: A Semantic Lexicon with Systematic Polysemous Classes, </title> <note> submitted to: ACL/EACL 1997. </note>
Reference: <author> J. Cowie,T. Wakao,W. Jin L. Guthrie, J. Puste-jovsky, S. </author> <title> Waterman, </title> <booktitle> The Diderot Information Extraction System in Proceedings of the First Pacific Conference on Computational Linguistics, </booktitle> <address> Vancou-ver, </address> <month> April 20, </month> <year> 1993. </year>
Reference: <author> J.L. Fagan, </author> <year> 1987. </year> <title> Experiments in Automatic Phrase Indexing for Document Retrieval: A Comparison of Syntactic and Non-syntactic Methods. </title> <type> PhD Thesis, </type> <institution> Cornell University, </institution> <month> September </month> <year> 1987. </year>
Reference: <author> J. </author> <title> Grimshaw, </title> <publisher> Argument Structure (MIT Press, </publisher> <address> Cambridge, </address> <year> 1990). </year>
Reference-contexts: Pustejovsky, 1991). Furthermore, words are also encoded with fairly rich information regarding the expression of arguments of various sorts, its argument structure <ref> (cf. Grimshaw, 1990) </ref>, and a fine-grained encoding of how a word denotes events, i.e., its event structure (cf. Moens and Steedman, 1988, Pas-sonneau, 1988). For more details on the theoretical model, see Pustejovsky (1995).
Reference: <author> R. Grishman and J. Sterling, </author> <title> Acquisition of Se-lectional Patterns, </title> <booktitle> in Proceedings of the 14th Int'l Conf. on Computational Linguistics (COLING 92), </booktitle> <address> Nantes, France, </address> <month> July, </month> <year> 1992. </year>
Reference-contexts: The approach to lexical indexing outlined here instantiates the model of corpus-driven lexical semantics acquisition presented in Pustejovsky et al. (1993) <ref> (cf. also Grishman and Sterling, 1992) </ref>. This model is based on the notion of targeted acquisition, driven by an appropriately selected corpus, and is applicable to a range of lexical types, all of which require richly instantiated lexical entries.
Reference: <author> Guthrie, L, J. Pustejovsky, Y. Wilks, and B. Slator. </author> <title> "The Role of Lexicons in Natural Language Processing," </title> <journal> Communications of the ACM, </journal> <volume> 39:1, </volume> <month> January, </month> <year> 1996. </year>
Reference: <author> M. Johnston, B. Boguraev, and J. Pustejovsky, </author> <title> The Acquisition and Interpretation of Complex Nomi-nals", </title> <booktitle> in Working Notes of AAAI Spring Symposium on the representation and acquisition of lexical knowledge, AAAI, </booktitle> <year> 1995. </year>
Reference-contexts: Linguistically distinct phenomena are associated with dedicated pattern matchers, each designed to identify salient semantic objects and relations in a text. In our previous research, we have developed a variety of knowledge extraction pattern matchers <ref> (cf. Johnston et al., 1995, Boguraev et al., forthcoming) </ref>. In the current context, we wish to further extend and refine these pattern matchers to apply to a much broader class of linguistic phenomena. We identify the following pat tern matchers needed for richer text indexing: a. <p> Complex Nominal Parsing: Complex nominal parsing involves examination of the internal structure of complex nominals including: noun-noun compounds (`disk drive utility'), possessive's (`hard disk drive's SCSI ID number'), and nominal post-modificational constructions (`release hatch of disk drive') in order to extract knowledge about the domain <ref> (cf. Johnston et al., 1995) </ref>. Other issues currently being dealt with in the context of improving acquisition and shallow semantic analysis include: discourse anaphoric processing and sor-tal anaphora (cf. Kennedy and Boguraev, 1996a); and identification of salient topics and topic structure within text (cf. Kennedy and Boguraev, 1996b).
Reference: <author> J. Justeson and S. Katz, </author> <year> 1995, </year> <title> Technical Terminology: Some Linguistic Properties and an Algorithm for Identification in Text, </title> <booktitle> Natural Language Engineering, </booktitle> <address> 1.1. </address>
Reference: <author> W. Lehnert and B. Sundheim, </author> <year> 1991, </year> <title> A Performance Evaluation of Text-Analysis Technologies, </title> <journal> AI Magazine, </journal> <pages> 81-94. </pages>
Reference: <author> G. Miller, </author> <year> 1990, </year> <title> WordNet: An on-line Lexical Database, </title> <journal> International Journal of Lexicography, </journal> <volume> 3, </volume> <pages> 235-312. </pages>
Reference-contexts: CoreLex embodies most of the principles of Generative Lexicon Theory, by representing how senses are related to one another as well as operating with un-derspecified semantic types. These assumptions are fundamentally different from existing sources such as Roget and WordNet <ref> (cf. Miller 1990) </ref>, both useful and extensive semantic lexicons, but which do not account for any regularities between senses nor do they relate semantic information to syntactic form.
Reference: <author> M. Moens and M. Steedman, </author> <title> Temporal Ontology and Temporal Reference, </title> <journal> Computational Linguistics, </journal> <volume> 14(2) </volume> <pages> 15-28. </pages>
Reference: <author> R. Passonneau, </author> <title> A Computational Model of the Semantics of Tense and Aspect, </title> <note> Computational Linguistics 14 (1988). </note>
Reference: <author> J. Pustejovsky, </author> <title> The Generative Lexicon, </title> <note> Computational Linguistics 17 (1991a) 409-441. </note>
Reference: <author> J. Pustejovsky, </author> <title> The acquisition of lexical semantic knowledge from large corpora. </title> <booktitle> In Proceedings of the DARPA Spoken and Written Language Workshop. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference: <author> J. Pustejovsky, </author> <title> The Generative Lexicon: A Theory of Computational Lexical Semantics (MIT Press, </title> <address> Cambridge, MA, </address> <year> 1995). </year>
Reference: <author> J. Pustejovsky, S. Bergler and P. Anick, </author> <title> Lexical Semantic Techniques for Corpus Analysis, </title> <note> Computational Linguistics, Special Issue on Corpus Linguistics, 19.2, </note> <year> 1993. </year>
Reference-contexts: For example, when we combine the qualia structure of an NP with that of a governing verb, a richer notion of com-positionality emerges (i.e. co-composition), one which captures the creative use of words <ref> (Pustejovsky and Boguraev 1993) </ref>. The Current Technology: Lexical Indexing The language of types described above is necessary for encoding lexical knowledge but is not in itself sufficient for the characterization of how words are used in specific domains with specialized senses.
Reference: <author> J. Pustejovsky and P. Boguraev, </author> <title> Lexical Knowledge Representation and Natural Language Processing, </title> <journal> Artificial Intelligence, </journal> <month> 63 </month> <year> (1993) </year> <month> 193-223. </month>
Reference-contexts: For example, when we combine the qualia structure of an NP with that of a governing verb, a richer notion of com-positionality emerges (i.e. co-composition), one which captures the creative use of words <ref> (Pustejovsky and Boguraev 1993) </ref>. The Current Technology: Lexical Indexing The language of types described above is necessary for encoding lexical knowledge but is not in itself sufficient for the characterization of how words are used in specific domains with specialized senses.
Reference: <author> A. Voutilainen, J. Heikkila and A. Anttila, </author> <year> 1992, </year> <title> Constraint Grammar of English. A Performance-Oriented Introduction, Publications No. </title> <type> 21, </type> <institution> Dept. of General Linguistics, University of Helsinki. </institution>
Reference: <author> Weischedel R. et al., </author> <title> "Description of the plum system as used for muc-5", </title> <booktitle> Proceedings of the 5th Message Understanding Conference, </booktitle> <publisher> Kaufmann, </publisher> <year> 1994. </year>
References-found: 28

