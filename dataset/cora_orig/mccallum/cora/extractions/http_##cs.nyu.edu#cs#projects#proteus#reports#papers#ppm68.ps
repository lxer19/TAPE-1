URL: http://cs.nyu.edu/cs/projects/proteus/reports/papers/ppm68.ps
Refering-URL: http://cs.nyu.edu/cs/projects/proteus/reports/index.html
Root-URL: http://www.cs.nyu.edu
Email: fgrishman,sterlingg@cs.nyu.edu  
Title: Generalizing Automatically Generated Selectional Patterns  
Author: Ralph Grishman and John Sterling 
Address: 715 Broadway, 7th Floor, New York, NY 10003, U.S.A.  
Affiliation: Computer Science Department, New York University  
Abstract: Frequency information on co-occurrence patterns can be automatically collected from a syntactically analyzed corpus; this information can then serve as the basis for selectional constraints when analyzing new text from the same domain. This information, however, is necessarily incomplete. We report on measurements of the degree of selectional coverage obtained with different sizes of corpora. We then describe a technique for using the corpus to identify selectionally similar terms, and for using this similarity to broaden the selectional coverage for a fixed corpus size. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jing-Shin Chang, Yih-Fen Luo, and Keh-Yih Su. GPSM: </author> <title> A generalized probabilistic semantic model for ambiguity resolution. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Assn. for Computational Linguistics, </booktitle> <pages> pages 177-184, </pages> <address> Newark, DE, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Greater coverage can be obtained by generalizing from the patterns collected so that patterns with semantically related words will also be considered acceptable. In most cases this has been done using manually-created word classes, generalizing from specific words to their classes <ref> [12, 1, 10] </ref>. If a pre-existing set of classes is used (as in [10]), there is a risk that the classes available may not match the needs of the task. <p> The "traditional" approach to generalizing this information has been to assign the words to a set of semantic classes, and then to collect the frequency information on combinations of semantic classes <ref> [12, 1] </ref>. Since at least some of these classes will be domain specific, there has been interest in automating the acquisition of these classes as well. This can be done by clustering together words which appear in the same context.
Reference: [2] <author> Ido Dagan, Shaul Marcus, and Shaul Markovitch. </author> <title> Contextual word similarity and estimation from sparse data. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Assn. for Computational Linguistics, </booktitle> <pages> pages 31-37, </pages> <address> Columbus, OH, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: This is the approach we have adopted for our current experiments [6], and which has also been employed by Dagan et al. <ref> [2] </ref>. We compute from the co-occurrence data a "confusion matrix", which measures the interchangeability of 1 words in particular contexts. <p> these filters may yeild an un-normalized confusion matrix (i.e., P w j P C (w j jw i ) &lt; 1), we renormalize the matrix so that P A similar approach to pattern generalization, using a similarity measure derived from co-occurrence data, has been recently described by Dagan et al. <ref> [2] </ref>. Their approach differs from the one described here in two significant regards: their co-occurrence data is based on linear distance within the sentence, rather than on syntactic relations, and they use a different similarity measure, based on mutual information. <p> For example, we can replace "speak" (which takes a human subject) by "sleep" (which takes an animate subject), and still have a selectionally valid pattern, but not the other way around. 5 This is similar to tests conducted by Pereira et al. [9] and Dagan et al. <ref> [2] </ref>. The cited tests, however, were based on selected words or word pairs of high frequency, whereas our test sets involve a representative set of high and low frequency triples. 6 This is a different criterion from the one used in our earlier papers. <p> We have also demonstrated that | for a given corpus size | coverage can be significantly improved by using the corpus to identify selectionally related terms, and using these similarities to generalize the patterns observed in the training corpus. This is consistent with other recent results using related techniques <ref> [2, 9] </ref>. We believe that these techniques can be further improved in several ways. The experiments reported above have only generalized over the first (head) position of the triples; we need to measure the effect of generalizing over the argument position as well.
Reference: [3] <author> U. Essen and V. Steinbiss. </author> <title> Cooccurrence smoothing for stochastic language modeling. </title> <booktitle> In ICASSP92, </booktitle> <address> pages I-161 - I-164, San Francisco, CA, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: We have therefore elected to try an approach which directly uses a form of similarity measure to smooth (generalize) the probabilities. Co-occurrence smoothing is a method which has been recently proposed for smoothing n-gram models <ref> [3] </ref>. 3 The core of this method involves the computation of a co-occurrence matrix (a matrix of confusion probabilities) P C (w j jw i ), which indicates the probability of word w j occurring in contexts in which word w i occurs, averaged over these contexts.
Reference: [4] <author> R. Grishman, L. Hirschman, </author> <title> and N.T. Nhan. Discovery procedures for sublanguage selectional patterns: Initial experiments. </title> <journal> Computational Linguistics, </journal> <volume> 12(3) </volume> <pages> 205-16, </pages> <year> 1986. </year>
Reference-contexts: In our earlier work <ref> [4] </ref> we fit the growth data to curves of the form 1 exp (fix), on the assumption that all selectional patterns are equally likely.
Reference: [5] <author> Ralph Grishman and John Sterling. </author> <title> Acquisition of selectional patterns. </title> <booktitle> In Proc. 14th Int'l Conf. Computational Linguistics (COLING 92), </booktitle> <address> Nantes, France, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: We compute from the co-occurrence data a "confusion matrix", which measures the interchangeability of 1 words in particular contexts. We then use the confusion matrix directly to generalize the semantic patterns. 2 Acquiring Semantic Patterns Based on a series of experiments over the past two years <ref> [5, 6] </ref> we have developed the following procedure for acquiring semantic patterns from a text corpus: 1. Parse the training corpus using a broad-coverage grammar, and regularize the parses to produce something akin to an LFG f-structure, with explicitly labeled syntactic relations such as SUBJECT and OBJECT. 1 2. <p> be resolved empirically; however, we believe that there is a virtue to our non-symmetric measure, because sub-stitutibility in selectional contexts is not a symmetric relation. 4 4 If w 1 allows a broader range of arguments than w 2 , then 3 4 Evaluation 4.1 Evaluation Metric We have previously <ref> [5] </ref> described two methods for the evaluation of semantic constraints. For the current experiments, we have used one of these methods, where the constraints are evaluated against a set of manually classified semantic triples. 5 For this evaluation, we select a small test corpus separate from the training corpus.
Reference: [6] <author> Ralph Grishman and John Sterling. </author> <title> Smoothing of automatically generated selectional constraints. </title> <booktitle> In Proceedings of the Human Language Technology Workshop, </booktitle> <address> Princeton, NJ, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: An alternative approach is to use the word similarity information directly, to infer information about the likelihood of a co-occurrence pattern from information about patterns involving similar words. This is the approach we have adopted for our current experiments <ref> [6] </ref>, and which has also been employed by Dagan et al. [2]. We compute from the co-occurrence data a "confusion matrix", which measures the interchangeability of 1 words in particular contexts. <p> We compute from the co-occurrence data a "confusion matrix", which measures the interchangeability of 1 words in particular contexts. We then use the confusion matrix directly to generalize the semantic patterns. 2 Acquiring Semantic Patterns Based on a series of experiments over the past two years <ref> [5, 6] </ref> we have developed the following procedure for acquiring semantic patterns from a text corpus: 1. Parse the training corpus using a broad-coverage grammar, and regularize the parses to produce something akin to an LFG f-structure, with explicitly labeled syntactic relations such as SUBJECT and OBJECT. 1 2.
Reference: [7] <author> Donald Hindle. </author> <title> Noun classification from predicate-argument structures. </title> <booktitle> In Proceedings of the 28th Annual Meeting of the Assn. for Computational Linguistics, </booktitle> <pages> pages 268-275, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This can be done using the co-occurrence data, i.e., by identifying words which occur in the same contexts (for example, verbs which occur with the same subjects and objects). From the co-occurrence data one can compute a similarity relation between words <ref> [8, 7] </ref>. This similarity information can then be used in several ways. One approach is to form word clusters based on this similarity relation [8]. This approach was taken by Sekine et al. at UMIST, who then used these clusters to generalize the semantic patterns [11].
Reference: [8] <author> Lynette Hirschman, Ralph Grishman, and Naomi Sager. </author> <title> Grammatically-based automatic word class formation. </title> <booktitle> Information Processing and Management, </booktitle> 11(1/2):39-57, 1975. 
Reference-contexts: This can be done using the co-occurrence data, i.e., by identifying words which occur in the same contexts (for example, verbs which occur with the same subjects and objects). From the co-occurrence data one can compute a similarity relation between words <ref> [8, 7] </ref>. This similarity information can then be used in several ways. One approach is to form word clusters based on this similarity relation [8]. This approach was taken by Sekine et al. at UMIST, who then used these clusters to generalize the semantic patterns [11]. <p> From the co-occurrence data one can compute a similarity relation between words [8, 7]. This similarity information can then be used in several ways. One approach is to form word clusters based on this similarity relation <ref> [8] </ref>. This approach was taken by Sekine et al. at UMIST, who then used these clusters to generalize the semantic patterns [11]. Pereira et al. [9] used a variant of this approach, "soft clusters", in which words can be members of different clusters to different degrees. <p> A similar approach to word cluster formation was described by Hirschman et al. in 1975 <ref> [8] </ref>. More recently, Pereira et al. [9] have described a word clustering method using "soft clusters", in which a word can belong to several clusters, with different cluster membership probabilities. Cluster creation has the advantage that the clusters are amenable to manual review and correction.
Reference: [9] <author> Fernando Pereira, Naftali Tishby, and Lillian Lee. </author> <title> Distributional clustering of English words. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Assn. for Computational Linguistics, </booktitle> <pages> pages 183-190, </pages> <address> Columbus, OH, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: This similarity information can then be used in several ways. One approach is to form word clusters based on this similarity relation [8]. This approach was taken by Sekine et al. at UMIST, who then used these clusters to generalize the semantic patterns [11]. Pereira et al. <ref> [9] </ref> used a variant of this approach, "soft clusters", in which words can be members of different clusters to different degrees. An alternative approach is to use the word similarity information directly, to infer information about the likelihood of a co-occurrence pattern from information about patterns involving similar words. <p> A similar approach to word cluster formation was described by Hirschman et al. in 1975 [8]. More recently, Pereira et al. <ref> [9] </ref> have described a word clustering method using "soft clusters", in which a word can belong to several clusters, with different cluster membership probabilities. Cluster creation has the advantage that the clusters are amenable to manual review and correction. <p> For example, we can replace "speak" (which takes a human subject) by "sleep" (which takes an animate subject), and still have a selectionally valid pattern, but not the other way around. 5 This is similar to tests conducted by Pereira et al. <ref> [9] </ref> and Dagan et al. [2]. The cited tests, however, were based on selected words or word pairs of high frequency, whereas our test sets involve a representative set of high and low frequency triples. 6 This is a different criterion from the one used in our earlier papers. <p> We have also demonstrated that | for a given corpus size | coverage can be significantly improved by using the corpus to identify selectionally related terms, and using these similarities to generalize the patterns observed in the training corpus. This is consistent with other recent results using related techniques <ref> [2, 9] </ref>. We believe that these techniques can be further improved in several ways. The experiments reported above have only generalized over the first (head) position of the triples; we need to measure the effect of generalizing over the argument position as well.
Reference: [10] <author> Philip Resnik. </author> <title> A class-based approach to lexical discovery. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Assn. for Computational Linguistics, </booktitle> <address> Newark, DE, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Greater coverage can be obtained by generalizing from the patterns collected so that patterns with semantically related words will also be considered acceptable. In most cases this has been done using manually-created word classes, generalizing from specific words to their classes <ref> [12, 1, 10] </ref>. If a pre-existing set of classes is used (as in [10]), there is a risk that the classes available may not match the needs of the task. <p> In most cases this has been done using manually-created word classes, generalizing from specific words to their classes [12, 1, 10]. If a pre-existing set of classes is used (as in <ref> [10] </ref>), there is a risk that the classes available may not match the needs of the task.
Reference: [11] <author> Satoshi Sekine, Sofia Ananiadou, Jeremy Carroll, and Jun'ichi Tsujii. </author> <title> Linguistic knowledge generator. </title> <booktitle> In Proc. 14th Int'l Conf. Computational Linguistics (COLING 92), </booktitle> <pages> pages 560-566, </pages> <address> Nantes, France, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: This similarity information can then be used in several ways. One approach is to form word clusters based on this similarity relation [8]. This approach was taken by Sekine et al. at UMIST, who then used these clusters to generalize the semantic patterns <ref> [11] </ref>. Pereira et al. [9] used a variant of this approach, "soft clusters", in which words can be members of different clusters to different degrees. <p> which it occurs as the subject and the object of each verb 2 2. defining a similarity measure between words, which reflects the number of common contexts in which they appear 3. forming clusters based on this similarity measure Such a procedure was performed by Sekine et al. at UMIST <ref> [11] </ref>; these clusters were then manually reviewed and the resulting clusters were used to generalize selectional patterns. A similar approach to word cluster formation was described by Hirschman et al. in 1975 [8].
Reference: [12] <author> Paola Velardi, Maria Teresa Pazienza, and Michela Fasolo. </author> <title> How to encode semantic knowledge: A method for meaning representation and computer-aided acquisition. </title> <journal> Computational Linguistics, </journal> <volume> 17(2) </volume> <pages> 153-170, </pages> <year> 1991. </year> <month> 7 </month>
Reference-contexts: Greater coverage can be obtained by generalizing from the patterns collected so that patterns with semantically related words will also be considered acceptable. In most cases this has been done using manually-created word classes, generalizing from specific words to their classes <ref> [12, 1, 10] </ref>. If a pre-existing set of classes is used (as in [10]), there is a risk that the classes available may not match the needs of the task. <p> The "traditional" approach to generalizing this information has been to assign the words to a set of semantic classes, and then to collect the frequency information on combinations of semantic classes <ref> [12, 1] </ref>. Since at least some of these classes will be domain specific, there has been interest in automating the acquisition of these classes as well. This can be done by clustering together words which appear in the same context.
References-found: 12

