URL: http://www-ai.ijs.si/DunjaMladenic/papers/PWW/pwwCONALD98.ps.gz
Refering-URL: http://www-ai.ijs.si/DunjaMladenic/pww.html
Root-URL: 
Email: E-mail: Dunja.Mladenic@ijs.si, Marko.Grobelnik@ijs.si  
Phone: Phone: (+38)(61) 1773 272, Fax: (+38)(61) 1258-158  
Title: Feature selection for classification based on text hierarchy  
Author: Dunja Mladenic and Marko Grobelnik 
Address: Jamova 39, 1111 Ljubljana, Slovenia  
Affiliation: Department of Intelligent Systems, J.Stefan Institute,  
Abstract: This paper describes automatic document categorization based on large text hierarchy. We handle the large number of features and training examples by taking into account hierarchical structure of examples and using feature selection for large text data. We experimentally evaluate feature subset selection on real-world text data collected from the existing Web hierarchy named Yahoo. In our learning experiments naive Bayesian classifier was used on text data using feature-vector document representation that includes word sequences (n-grams) instead of just single words (unigrams). Experimental evaluation on real-world data collected form the Web shows that our approach gives promising results and can potentially be used for document categorization on the Web. Additionally the best result on our data is achieved for relatively small feature subset, while for larger subset the performance substantially drops. The best performance among six tested feature scoring measure was achieved by the feature scoring measure called Odds ratio that is known from information retrieval.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., Verkamo, A.I., </author> <year> 1996. </year> <title> Fast Discovery of Association Rules, </title> <editor> In Fayyad, U.M., Piatetsky-Shapiro, G., Smyth, P., Uthurusamy, R. (eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining AAAI Press/The MIT Press, </booktitle> <pages> pp. 307|328. </pages>
Reference-contexts: Each new pass generates features of length i only from the candidate features (of length i-1) generated in the previous pass. This process is similar to the large k-itemset generation used in association rules algorithm <ref> [1] </ref>. Learning algorithm used in our experiments is based on using naive Bayesian classifier on text data as described in [5] and [9].
Reference: [2] <author> Domingos, P., Pazzani, M., </author> <year> 1997. </year> <title> On the Optimality of the Simple Bayesian Classifier under Zero-One Loss, Machine Learning 29, </title> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 103|130. </pages>
Reference-contexts: In machine learning setting this document representation can be seen as a fixed size vector having a feature for each word from the domain containing word frequency in a document. The assumption about feature independence used in naive Bayesian classifier is clearly incorrect. According to <ref> [2] </ref> this does not necessary mean that classifier will have poor performance because of that. We use the Yahoo hierarchy to learn document categorization. For a new document, the classifier returns probability distribution over categories included in the hierarchy.
Reference: [3] <author> Filo, D., Yang, J., </author> <year> 1997. </year> <institution> Yahoo! Inc., </institution> <year> 1997. </year> <note> http://www.yahoo.com/docs/pr/ </note>
Reference-contexts: The problem of automatic document categorization is well known in information retrieval and usually tested on publicly available databases (eg. Reuters, MEDLINE). Here we use machine learning for document categorization using one of the existing Web hierarchies named Yahoo <ref> [3] </ref>. In this way, the current situation on the Web captured in the Web hierarchy is used for automatic document categorization. The proposed approach is not limited to Web hierarchy and can be applied on other hierarchies like for instance, thesaurus.
Reference: [4] <author> Grobelnik, M., Mladenic, D., </author> <title> Learning Machine: design and implementation, </title> <type> Technical Report IJS-DP-7824, </type> <institution> Department for Intelligent Systems, J.Stefan Institute, </institution> <year> 1998. </year>
Reference-contexts: The final result of learning is a set of specialized classifiers. 4 Experimental results Our experiments are performed using our recently developed machine learning system Learning Machine <ref> [4] </ref> that supports usage of different machine learning techniques on large data sets with especially designed modules for learning on text and collecting data from the Web.
Reference: [5] <author> Joachims, T., </author> <year> 1997. </year> <title> A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. 143|151. </pages>
Reference-contexts: This process is similar to the large k-itemset generation used in association rules algorithm [1]. Learning algorithm used in our experiments is based on using naive Bayesian classifier on text data as described in <ref> [5] </ref> and [9]. In their approach documents are represented as word-vectors where a feature is defined for each word position in the document having a word at that position as a feature value.
Reference: [6] <author> John, G.H., Kohavi, R., Pfleger, K., </author> <title> Irrelevant Features and the Subset Selection Problem, </title> <booktitle> Proc. of the 11th International Conference on Machine Learning ICML94, </booktitle> <pages> pp. 121|129, </pages> <year> 1994. </year>
Reference-contexts: The best performance among six tested measures was achieved for relatively small feature subset by Odds ratio. 2 Feature subset selection in text-learning Selection of a subset of features to be used in inductive learning has already been addressed in machine learning. According to John et al. <ref> [6] </ref> there are two main approaches used in machine learning to feature subset selection: filtering approach and wrapper approach.
Reference: [7] <author> Koller, D., Sahami, M., </author> <title> Hierarchically classifying documents using very few words, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. 170|178, </pages> <year> 1997. </year>
Reference-contexts: Scoring of individual features can be performed using some of the methods used in machine learning for feature selection during the learning process, for example, Inf ormation gain used in decision tree induction [12]. Expected cross entropy used in text-classification experiments <ref> [7] </ref> is similar to Information gain. The difference is that instead of calculating average over all possible feature values, only the value denoting that word occurred in a document is considered. Our experiments show that this means an important difference in the resulting performance. <p> occurring given the class value `positive', P (W jneg) is the conditional probability of word W occurring given the class value `negative', T F (W ) is term frequency (the number of occurrence of word W ). 3 Learning from large text hierarchy Hierarchically classifying documents was recently addressed in <ref> [7] </ref>, where the Reuters dataset was used and three hierarchical structures were manually constructed each having a 3-level hierarchy that is based on up to 1,000 documents and the biggest having 12 nodes. The general idea of using hierarchical instead of `flattened' approach is the same as used here. <p> One possibility is to use `flattened' approach and generate one huge classifier with many class values, each value corresponding to one category requiring a large number of features to cover all the variety of the large number of documents included in different categories. As pointed out in <ref> [7] </ref>, a better idea is to somehow split the whole problem into subproblems. We use the hierarchical structure to decompose the problem into a set of subproblems corresponding to individual categories.
Reference: [8] <author> Kononenko, I. </author> <title> On biases estimating multi-valued attributes. </title> <booktitle> Proc. of the 14th International Joint Conference on Artificial Intelligence IJCAI-95, </booktitle> <pages> pp. 1034-1040, </pages> <year> 1995. </year>
Reference-contexts: We propose a new feature scoring measure called W eight of evidence f or text that is based on the average absolute weight of evidence used in machine learning <ref> [8] </ref>. Odds ratio is commonly used in information retrieval, where the problem is to rank out documents according to their relevance for the positive class value using occurrence of different words as features [13].
Reference: [9] <author> Mitchell, </author> <title> T.M., Machine Learning, </title> <publisher> The McGraw-Hill Companies, Inc., </publisher> <year> 1997. </year>
Reference-contexts: This process is similar to the large k-itemset generation used in association rules algorithm [1]. Learning algorithm used in our experiments is based on using naive Bayesian classifier on text data as described in [5] and <ref> [9] </ref>. In their approach documents are represented as word-vectors where a feature is defined for each word position in the document having a word at that position as a feature value.
Reference: [10] <author> Mladenic, D., </author> <title> Feature subset selection in text-learning, </title> <booktitle> Proc. of the 10th European Conference on Machine Learning ECML98, </booktitle> <year> 1998. </year>
Reference-contexts: This is consistent with the results reported in text-learning on the problem of predicting clicked hyperlinks from the set of Web documents visited by the user <ref> [10] </ref>. The best performance in rank and probability is achieved 4 here by Odds ratio on feature subset having relatively small number of features `References': 1.5-3, `Education': 2-5, `Computers and Internet': 6-8 times the number of features in positive class.
Reference: [11] <author> Mladenic, D., Grobelnik, M., </author> <year> 1998. </year> <title> Efficient text categorization, </title> <booktitle> Text Mining workshop on the 10th European Conference on Machine Learning ECML98, (submitted). </booktitle>
Reference-contexts: We tested different pruning levels (1,2,. . . 15) on several domains <ref> [11] </ref> and here use 3 as one of the best performing.
Reference: [12] <author> Quinlan, J.R., </author> <title> Constructing Decision Tree in C4.5: Programs for Machine Learning, </title> <publisher> pp.17|26, Morgan Kaufman Publishers, </publisher> <year> 1993. </year> <editor> [13] van Rijsbergen, C.J,. Harper, D.J., Porter, M.F., </editor> <title> The selection of good search terms, </title> <booktitle> Information Processing & Management, 17, </booktitle> <address> pp.77|91, </address> <year> 1981. </year>
Reference-contexts: The number of features to select is an experimental issue that we address in our experiments. Scoring of individual features can be performed using some of the methods used in machine learning for feature selection during the learning process, for example, Inf ormation gain used in decision tree induction <ref> [12] </ref>. Expected cross entropy used in text-classification experiments [7] is similar to Information gain. The difference is that instead of calculating average over all possible feature values, only the value denoting that word occurred in a document is considered.
Reference: [14] <author> Yang, Y., Pedersen, J.O., </author> <title> A Comparative Study on Feature Selection in Text Categorization, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. 412|420, </pages> <year> 1997. </year> <month> 6 </month>
Reference-contexts: The difference is that instead of calculating average over all possible feature values, only the value denoting that word occurred in a document is considered. Our experiments show that this means an important difference in the resulting performance. M utual inf ormation used in text-classification experiments <ref> [14] </ref> is similar to Cross entropy used on text data with the latter additionally taking into account word probability. We propose a new feature scoring measure called W eight of evidence f or text that is based on the average absolute weight of evidence used in machine learning [8]. <p> Additionally we used a very simple measure based on word frequency that is shown to work well in the text-classification domains <ref> [14] </ref>.
References-found: 13

