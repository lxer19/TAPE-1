URL: http://www.ai.mit.edu/people/mhcoen/aaai98.ps
Refering-URL: http://www.ai.mit.edu/projects/hal/
Root-URL: 
Email: mhcoen@ai.mit.edu  
Title: Design Principles for Intelligent Environments  
Author: Michael H. Coen 
Address: 545 Technology Square Cambridge, MA 02139  
Affiliation: MIT Artificial Intelligence Lab  
Abstract: This paper describes design criteria for creating highly embedded, interactive spaces that we call Intelligent Environments. The motivation for building these systems is to bring computation into the real, physical world to support what is traditionally considered noncomputational activity. We describe an existing prototype space, known as the Intelligent Room, which was created to experiment with different forms of natural, multimodal human-computer interaction. We discuss design decisions encountered while creating the Intelligent Room and how the experiences gained during its use have shaped the creation of its successor. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Abowd, G., Atkeson, C., Feinstein, A., Hmelo, C., Kooper, R., Long, S., Sawhney, N., and Tani, M. </author> <title> Teach and Learning a Multimedia Authoring: The Classroom 2000 project. </title> <booktitle> Proceedings of the ACM Multimedia96 Conference . 1996. </booktitle>
Reference-contexts: Each of these projects makes use of embedded computation to enable unusual human-computer interactions, e.g., vision-based person trac king. However their modal processing is extraordinarily specific to their applications, and the applicability of such carefully tuned systems to other domains is unclear. The Classroom 2000 project (Abowd et al. <ref> [ 1] </ref>) is an educational environment that aut omatically creates records linking simultaneous streams of information, e.g. what the teacher is saying while a student is writing down her notes on a digital pad.
Reference: 2. <author> Black, A. and Taylor, P. </author> <title> Festival Speech Synthesis System: </title> <institution> system documentation (1.1.1) Human Communication Research Centre Technical Report HCRC/TR-83 . University of Edinburgh. </institution> <year> 1997. </year>
Reference-contexts: Thus, a user can interact with the room easily, regardless of her proximity to a keyboard or monitor. The Intelligent Room is capable of addressing users via the Festival Speech Synthesis System (Black et al. <ref> [ 2] </ref>). Utterances spoken by the room are also displayed on a scrollable LCD sign in case a user was unable to understand what was said.
Reference: 3. <author> Bobick, A.; Intille, S.; Davis, J.; Baird, F.; Pinhanez, C.; Campbell, L.; Ivanov, Y.; Schtte, A.; and Wilson, A. </author> <title> Design Decisions for Interactive Environments: Evaluating the KidsRoom. </title> <booktitle> Proceedings of the 1998 AAAI Spring Symposium on Intelligent Environments . AAAI TR SS-98-02. </booktitle> <year> 1998. </year>
Reference-contexts: We discuss below the design of our laboratorys Intelligent Room and how experiences gained during its use have shaped the creation of its successor. Given the increasingly widespread interest in highly interactive, computational environments (Bobick et al. <ref> [ 3] </ref>), (Coen [6,7,8]), (Cooperstock et al. [ 10]), (Lucente et al. [ 17]), we hope these experiences will prove useful to other IE designers and implementers in the AI community. Some of the earliest work in t his area has been done wholly outside the AI community. <p> The Intelligent Room has a desktop environment directly motivated by the DigitalDesk, which recognizes a wider range of complex hand gestures (Dang [11]). Other substantial efforts towards highly interactive environments include an automated teleconferencing office (Cooperstock et al. [10]) and an immersive fictional theater (Bobick et al. <ref> [ 3] </ref>). Each of these projects makes use of embedded computation to enable unusual human-computer interactions, e.g., vision-based person trac king. However their modal processing is extraordinarily specific to their applications, and the applicability of such carefully tuned systems to other domains is unclear. <p> This approach differs from the overhead tracking system described in Bobick et al. <ref> [ 3] </ref>. Their domain had 27 high ceilings, for which it is quite reasonable to look for people from a single camera birds eye perspective. Rooms with ordinary height ceilings do not make this possible, so a stereo vision system seems necessary for performing background segmentation.
Reference: 4. <author> Cohen, P., </author> <title> "The role of natural language in a multimodal interface," </title> <booktitle> Proceedings of User Interface Software Technology (UIST'92) Conference , Academic Press, </booktitle> <address> Monterey, California, </address> <year> 1992. </year>
Reference-contexts: Selecting the modal components of an IE requires a careful strategic approach because of the a priori assumption that the IE is actually going to be embedded in the real-world. In particular, there is a need for the use of synergy (Cohen <ref> [4] </ref>) to allow imperfect modalities to reinforce and support each other. We discuss below the design of our laboratorys Intelligent Room and how experiences gained during its use have shaped the creation of its successor.
Reference: 5. <author> Cohen, P., Chen, L., Clow, J., Johnston, M., McGee, D., Pittman, K., and Smith, I. Quickset: </author> <title> A multimodal interface for distributed interactive simulation, </title> <booktitle> Proceedings of the UIST96 Demonstration Session , Seattle. </booktitle> <year> 1996. </year>
Reference-contexts: Mozer ([ 18]) describes a house that automatically controls basic residential comfort systems, such as heating and ventilation, by lea rning patterns in its occupants behavior. Related user-interface work such as Cohen et al. <ref> [ 5] </ref> uses multimodal interface technology to facilitate human interaction with a preexisting distributed simulator. In doing so, it provides a novel user-interface to a complex software system, but it is one that requires tying down the user to a particular computer and a specific application.
Reference: 6. <author> Coen, M. </author> <title> Building Brains for Rooms: Designing Distributed Software Agents. </title> <booktitle> Proceedings of the Ninth Conference on Innovative Applications of Artificial Intelligence. </booktitle> <address> (IAAI97). Providence, R.I. </address> <year> 1997. </year> <note> http://www.ai.mit.edu/people/mhcoen/brain.ps </note>
Reference-contexts: What emerged from an iterative development process is a modular system of software agents known collectively as the Scatterbrain (described in detail in Coen <ref> [ 6] </ref>). The Scatterbrain currently consists of approximately 50 conference area in the Intelligent Room distinct, intercommunicating software agents that run on ten different networked workstations.
Reference: 7. <author> Coen, M. </author> <title> Towards Interactive Environments: The Intelligent Room (a short paper). </title> <booktitle> Proceedings of the 1997 Conference on Human Computer Interaction (HCI97). </booktitle> <address> Bristol, U.K. </address> <year> 1997. </year>
Reference: 8. <editor> Coen, M. (ed.) </editor> <booktitle> Proceedings of the 1998 AAAI Spring Symposium on Intelligent Environments. AAAI TR SS-98-02. </booktitle> <year> 1998. </year>
Reference-contexts: Room Interactions Our approach with the Intelligent Room has been to create a platform for HCI research that connects with real-world phenomena through several computer vision and speech To appear in Proceedings of the 1998 National Conference on Artificial Intelligence <ref> (AAAI-98) </ref> recognition systems. These allow the room to watch where people are moving, under certain circumstances where they are pointing, and to listen to a fairly wide variety of spoken language utterances. The Intelligent Room supports a variety of application domains.
Reference: 9. <author> Coen, M; Thomas, K; Weisman, L; Groh, M; and Yee, A. </author> <title> A Natural Language Modality for an Embedded Multimodal Environment. </title> <publisher> Forthcoming. </publisher>
Reference-contexts: We tuned DragonDictates performance by creating sets of specialized grammars for different room contexts and having the rooms software agent controller dynamically activate different subsets of grammars depending on the context of the activity in the Intelligent Room (Coen et al. <ref> [ 9] </ref>). This allows us to overcome the combinatorial increase in parsing time due to incorporating natural syntactic variability in the recognition grammars. Instead of keeping a single enormous recognition grammar active, the room keeps subsets of small grammars active in parallel, given what it currently expects to hear.
Reference: 10. <author> Cooperstock, J; Fels, S.; Buxton, W. and Smith, K. </author> <title> Environments: </title> <booktitle> Throwing Away Your Ke yboard and Mouse . Commmunications of the ACM . 1997. </booktitle>
Reference-contexts: We discuss below the design of our laboratorys Intelligent Room and how experiences gained during its use have shaped the creation of its successor. Given the increasingly widespread interest in highly interactive, computational environments (Bobick et al. [ 3]), (Coen [6,7,8]), (Cooperstock et al. <ref> [ 10] </ref>), (Lucente et al. [ 17]), we hope these experiences will prove useful to other IE designers and implementers in the AI community. Some of the earliest work in t his area has been done wholly outside the AI community. <p> The Intelligent Room has a desktop environment directly motivated by the DigitalDesk, which recognizes a wider range of complex hand gestures (Dang [11]). Other substantial efforts towards highly interactive environments include an automated teleconferencing office (Cooperstock et al. <ref> [10] </ref>) and an immersive fictional theater (Bobick et al. [ 3]). Each of these projects makes use of embedded computation to enable unusual human-computer interactions, e.g., vision-based person trac king.
Reference: 11. <author> Dang, D. </author> <title> Template Based Gesture Recognition. </title> <type> SM Thesis. </type> <institution> Massachusetts Institute of Technology. </institution> <year> 1996. </year>
Reference-contexts: It recognized and responded to predetermined hand gestures made by users while interacting with real paper documents on the surface of a desk. The Intelligent Room has a desktop environment directly motivated by the DigitalDesk, which recognizes a wider range of complex hand gestures (Dang <ref> [11] </ref>). Other substantial efforts towards highly interactive environments include an automated teleconferencing office (Cooperstock et al. [10]) and an immersive fictional theater (Bobick et al. [ 3]). Each of these projects makes use of embedded computation to enable unusual human-computer interactions, e.g., vision-based person trac king. <p> Interactive Table Through a ceiling mounted camera, the room can detect hand-pointing gestures and newly placed documents on the surface of the conference table. The gesture recognition system has been used to support a wide variety of functions (described in Dang <ref> [11] </ref>). We found, however, that making gestures over the surface of a table was not a particularly natural form of interaction and required extensive practice to master.
Reference: 12. <author> Davis, J. and Bobick, A. </author> <title> The representation and recognition of action using temporal templates. </title> <booktitle> Proceedings Computer Vision and Pattern Recognition (CVPR97). </booktitle> <address> pp.928-934. </address> <year> 1997. </year>
Reference: 13. <author> DeBonet, J. </author> <title> Multiple Room Occupant Location and Identification.1996. </title> <address> http://www.ai.mit.edu/people/jsd/jsd.doit/Research/HCI/Tra cking_public </address>
Reference-contexts: Room Vision Systems Person Tracking The Intelligent Room can track up to four people moving in the conference area of the room at up to 15Hz. The rooms person tracking system (DeBonet <ref> [ 13] </ref>) uses two wall-mounted cameras, each approximately 8 from the ground. (A debugging window from the system showing the view from one of the cameras is shown in Figure 2.) We initially decided that incorporating a tracking system in the Intelligent Room was essential for a number of reasons.
Reference: 14. <author> Druin, A.; and Perlin, K. </author> <title> Immersive Environments: a physical approach to the computer interface. </title> <booktitle> Proceedings of the Conference on Human Factors in Computer Systems (CHI94), </booktitle> <pages> pages 325-326, </pages> <year> 1994. </year>
Reference: 15. <author> Katz, B. </author> <title> Using English for Indexing and Retrieving. </title> <booktitle> In Artificial Intelligence at MIT: Expanding Frontiers . Winston, </booktitle> <editor> P.; and Shellard, S. (editors). </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA. Volume 1. </address> <year> 1990. </year>
Reference-contexts: Summits output. This is performed primarily by the START nat ural-language information retrieval system (Katz <ref> [15] </ref>). DragonDictate is a commercially available system primarily used for discrete speech dictation, meaning that users must pause after each word. This, when coupled with its relatively low word accuracy, would be an intolerable speech interface to the room.
Reference: 16. <author> Lien, J., Zlochower, A., Cohn, J., Li, C., and Kanade, T. </author> <title> Automatically Recognizing Facial Expressions in the Spatio-Temporal Domain. </title> <booktitle> Proceedings of the Workshop on Perceptual User Interfaces (PUI97). </booktitle> <address> Alberta, Canada. pp.94-97. </address> <year> 1997. </year>
Reference: 17. <author> Lucente, M.; Zwart, G.; George, A. </author> <title> Visualization Space: A Testbed for Deviceless Multimodal User Interface. </title> <booktitle> Proceedings of the AAAI 1998 Spring Symposium on Intelligent Environments . AAAI TR SS-98-02. </booktitle> <year> 1998. </year>
Reference-contexts: We discuss below the design of our laboratorys Intelligent Room and how experiences gained during its use have shaped the creation of its successor. Given the increasingly widespread interest in highly interactive, computational environments (Bobick et al. [ 3]), (Coen [6,7,8]), (Cooperstock et al. [ 10]), (Lucente et al. <ref> [ 17] </ref>), we hope these experiences will prove useful to other IE designers and implementers in the AI community. Some of the earliest work in t his area has been done wholly outside the AI community.
Reference: 18. <author> Mozer, M. </author> <title> The Neural Network House: An Environment that Adapts to its Inhabitants. </title> <booktitle> Proceedings of the AAAI 1998 Spring Symposium on Intelligent Environments . AAAI TR SS-98-02. </booktitle> <year> 1998. </year>
Reference: 19. <author> Newman, W. and Wellner, P. </author> <title> A Desk Supporting Computer-based interaction with paper. </title> <booktitle> Proceedings of the Conference on Human Factors in Computing Systems (CHI92). </booktitle> <address> p587-592. </address> <year> 1992. </year>
Reference-contexts: Also, because the room senses at a distance, objects, in particular people and furniture, do not need to be physically augmented and/or wired for the room to become aware of them. Other related work The DigitalDesk project (Wellner [ 26], Newman et al. <ref> [19] </ref>) was an early and influe ntial system that had a birds eye view of a desktop through an overhead video camera. It recognized and responded to predetermined hand gestures made by users while interacting with real paper documents on the surface of a desk.
Reference: 20. <author> Saund, E. </author> <title> Example Line Drawing Analysis for the ZombieBoard Diagrammatic User Interface . http://www.parc.xerox.com/spl/members/saund/lda-example/lda-example.html. 1996. </title>
Reference: 21. <author> Stauffer, C. </author> <title> Adaptive Manifolds for Object Classification . 1996. </title> <address> http://www.ai.mit.edu/people/stauffer/Projects/Manifold/ </address>
Reference-contexts: Our assumption is that occlusion of a chair provides evidence that someone is sitting in it, and this person can be located using prior knowledge of the chairs position. This system will use low dimension eigenspaces for approximating object manifolds under varying pose and lighting conditions (Stauffer <ref> [21] </ref>). The advantage to this approach is that the system need not be given in advance an explicit model of the chairs it will be locating. The system can construct object manifolds itself by having a user rotate any new types of chairs she brings inside. 6.
Reference: 22. <author> Stiefelhagen, R., Yang, J., and Waibel, A. </author> <title> Tracking Eyes and Monitoring Eye Gaze. </title> <booktitle> Proceedings of the Workshop on Perceptual User Interfaces (PUI97). </booktitle> <address> Alberta, Canada. pp.98-100. </address> <year> 1997. </year>
Reference: 23. <author> Torrance, M. </author> <title> Advances in Human-Computer Interaction: The Intelligent Room. </title> <booktitle> Working Notes of the CHI 95 Research Symposium , May 6-7, </booktitle> <address> Denver, Colorado. </address> <year> 1995. </year>
Reference: 24. <author> Want, R.; Schilit, B.; Adams, N.; Gold, R.; Petersen, K.; Goldberg, D.; Ellis, J.; and Weiser, M. </author> <title> The ParcTab Ubiquitous Computing Experiment. </title> <note> Xerox Parc technical report. </note>
Reference: 25. <author> Weiser, M. </author> <booktitle> The Computer for the 21 st Century. </booktitle> <publisher> Scientific American . pp.94-100, </publisher> <month> September, </month> <year> 1991. </year>
Reference: 26. <author> Wellner, P. </author> <title> The DigitalDesk Calculator: Tangible Manipulation on a Desk Top Display, </title> <booktitle> Proceedings of UIST91. </booktitle> <address> pp.27-33. </address> <year> 1991. </year>
Reference-contexts: Also, because the room senses at a distance, objects, in particular people and furniture, do not need to be physically augmented and/or wired for the room to become aware of them. Other related work The DigitalDesk project (Wellner <ref> [ 26] </ref>, Newman et al. [19]) was an early and influe ntial system that had a birds eye view of a desktop through an overhead video camera. It recognized and responded to predetermined hand gestures made by users while interacting with real paper documents on the surface of a desk.
Reference: 27. <author> Zue, V. </author> <title> Human Computer Interactions Using Language Based Technology . IEEE Intern ational Symposium on Speech, Image Processing & Neural Networks. </title> <institution> Hong Kong. </institution> <year> 1994 </year>
Reference-contexts: For processing spoken utterances, we use both the Summit ( Zue et al. <ref> [ 27] </ref>) and DragonDictate speech recognition systems in parallel. Each of these has different strengths and used together they have fairly robust performance. The Summit system recognizes continuous speech and is particularly adept at handling syntactic variability during recognition.
References-found: 27

