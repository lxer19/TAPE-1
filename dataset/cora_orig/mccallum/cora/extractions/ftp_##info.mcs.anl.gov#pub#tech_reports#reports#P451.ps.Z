URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P451.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Email: bouarich@@mcs.anl.gov.  
Title: STENMIN: A Software Package for Large, Sparse Unconstrained Optimization Using Tensor Methods  
Author: Ali Bouaricha 
Keyword: Categories and Subject Descriptors: G.1.3 [Numerical Analysis]: Numerical Linear Algebra-sparse and very large systems; G.1.6 [Numerical Analysis]: Optimization-unconstrained optimization; G.4 [Mathematics of Computing]: Mathematical Software General Terms: Algorithms Additional Key Words and Phrases: tensor methods, sparse problems, large-scale optimization, rank-deficient matrices  
Address: Author's address:  Illinois, 60439.  
Note: Part of this work was performed while the author was research associate  This work was supported in part by the Office of Scientific Computing, U.S. Department of Energy, under Contract W-31-109-Eng-38.  
Affiliation: Argonne National Laboratory  at CERFACS (Centre Europeen de Recherche et de Formation Avancee en Calcul Scientifique, Toulouse, France).  Mathematics and Computer Science Division, Argonne National Laboratory, Argonne,  
Abstract: We describe a new package for minimizing an unconstrained nonlinear function where the Hessian is large and sparse. The software allows the user to select between a tensor method and a standard method based upon a quadratic model. The tensor method models the objective function by a fourth-order model, where the third- and fourth-order terms are chosen such that the extra cost of forming and solving the model is small. The new contribution of this package consists of the incorporation of an entirely new way of minimizing the tensor model that makes it suitable for solving large, sparse optimization problems efficiently. The test results indicate that, in general, the tensor method is often more efficient and more reliable than the standard Newton method for solving large, sparse unconstrained optimization problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. M. Averick, R. G. Carter, J. J. More, and G. L. Xue. </author> <title> The MINPACK-2 test problem collection. </title> <type> Technical Report ANL/MCS-P153-0692, </type> <institution> Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: AT X (K) STRSLT 0.1884575867777E-13 STRSLT SCALED GRADIENT AT X (K) STRSLT 0.1113397081739E-05 ------------------------------------------ STRSLT NUMBER OF FUNCTION EVALUATIONS 5 STRSLT NUMBER OF GRADIENT EVALUATIONS 5 STRSLT NUMBER OF HESSIAN EVALUATIONS 4 In the Appendix, we give another example of use-the optimal design with composite materials problem-from the MINPACK-2 collection <ref> [1] </ref>. 9. Test Results We tested our tensor and Newton methods on the set of unconstrained optimization problems from the CUTE [2] and the MINPACK-2 [1] collections. Most of these problems have nonsingular Hessians at the solution. <p> 5 STRSLT NUMBER OF HESSIAN EVALUATIONS 4 In the Appendix, we give another example of use-the optimal design with composite materials problem-from the MINPACK-2 collection <ref> [1] </ref>. 9. Test Results We tested our tensor and Newton methods on the set of unconstrained optimization problems from the CUTE [2] and the MINPACK-2 [1] collections. Most of these problems have nonsingular Hessians at the solution. We also created singular test problems as proposed in [3, 17] by modifying the nonsingular test problems from the CUTE collection. The dimensions of these problems range from 100 to 10000.
Reference: [2] <author> I. Bongartz, A. R. Conn, N. I. M. Gould, and Ph. L. Toint. CUTE: </author> <title> Constrained and Unconstrained Testing Environment. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 21(1) </volume> <pages> 123-160, </pages> <year> 1995. </year>
Reference-contexts: Implementation Details This software package has been coded in Fortran 77. The user has the choice between single-and double-precision versions. The user must then preprocess the package at compile time using either the tosngl or todble tools from CUTE <ref> [2] </ref>, for the single- and double-precision versions, respectively. The tosngl program picks up the appropriate version by selecting any statement that begins with CS in the first column, where the S character means that this is a single-precision version. <p> Test Results We tested our tensor and Newton methods on the set of unconstrained optimization problems from the CUTE <ref> [2] </ref> and the MINPACK-2 [1] collections. Most of these problems have nonsingular Hessians at the solution. We also created singular test problems as proposed in [3, 17] by modifying the nonsingular test problems from the CUTE collection. The dimensions of these problems range from 100 to 10000.
Reference: [3] <author> A. Bouaricha. </author> <title> Solving large sparse systems of nonlinear equations and nonlinear least squares problems using tensor methods on sequential and parallel computers. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, University of Colorado at Boulder, </institution> <year> 1992. </year>
Reference-contexts: For these reasons, this software uses a line search method. The global framework for the line search method we used in conjunction with our tensor method for large, sparse unconstrained optimization is similar to the one used for systems of nonlinear equations <ref> [3, 5] </ref>. This strategy has proved successful for large, sparse systems of nonlinear equations. This approach always tries the full tensor step first. <p> Test Results We tested our tensor and Newton methods on the set of unconstrained optimization problems from the CUTE [2] and the MINPACK-2 [1] collections. Most of these problems have nonsingular Hessians at the solution. We also created singular test problems as proposed in <ref> [3, 17] </ref> by modifying the nonsingular test problems from the CUTE collection. The dimensions of these problems range from 100 to 10000. All our computations were performed on a Sun SPARC 10 Model 40 machine using double-precision arithmetic.
Reference: [4] <author> A. Bouaricha. </author> <title> Tensor methods for large, sparse unconstrained optimization. </title> <type> Technical report, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, preprint MCS-P452-0794, </institution> <year> 1994. </year>
Reference-contexts: They are not efficient for sparse problems, however, because they destroy the sparsity of the Hessian due to the orthogonal transformation of the variable space. To preserve the sparsity of the Hessian, we developed in <ref> [4] </ref> an entirely new way of minimizing the tensor model that employs a sparse variant of the Cholesky decomposition. This makes the new algorithms well suited for sparse problems. <p> An Iteration of Tensor Methods In this section, we present the overall algorithm for tensor methods for large, sparse unconstrained optimization. Algorithm 2.1 is a slightly modified version of the algorithm described in <ref> [4] </ref> in the way the tensor step is selected when the fi equation (see algorithm below) has more than one root. In general, this new way of computing the tensor step appears to perform better than the strategy described in [4], in both function evaluations and execution times. <p> is a slightly modified version of the algorithm described in <ref> [4] </ref> in the way the tensor step is selected when the fi equation (see algorithm below) has more than one root. In general, this new way of computing the tensor step appears to perform better than the strategy described in [4], in both function evaluations and execution times. A summary of the experimental results for this implementation is presented in x9. Algorithm 2.1. <p> In step 2, the Hessian matrix is either calculated analytically or approximated by a graph coloring algorithm described in [9]. In step 4.3, the matrix ^ r 2 f (x c ) is factorized using the augmented system approach described in <ref> [4] </ref>. <p> In step 6, we compute a next iterate x + by performing the standard backtracking line search global strategy described in Algorithm 2.2. The line search tensor method is much simpler to 5 implement and to understand than the two-dimensional trust region tensor method introduced in <ref> [4] </ref>, and is appreciably faster. For these reasons, this software uses a line search method. The global framework for the line search method we used in conjunction with our tensor method for large, sparse unconstrained optimization is similar to the one used for systems of nonlinear equations [3, 5]. <p> A summary for the test problems whose Hessians at the solution have ranks n, n 1, and 19 n 2 is presented in Table 9.1. The descriptions of the test problems and the detailed results are given in <ref> [4] </ref>. In Table 9.1 the columns "better" and "worse" represent the number of times the tensor method was better and worse, respectively, than Newton's method by more than one gradient evaluation.
Reference: [5] <author> A. Bouaricha and R. B. Schnabel. TENSOLVE: </author> <title> A software package for solving systems of nonlinear equations and nonlinear least squares problems using tensor methods. </title> <type> Preprint MCS-P463-0894, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: For these reasons, this software uses a line search method. The global framework for the line search method we used in conjunction with our tensor method for large, sparse unconstrained optimization is similar to the one used for systems of nonlinear equations <ref> [3, 5] </ref>. This strategy has proved successful for large, sparse systems of nonlinear equations. This approach always tries the full tensor step first.
Reference: [6] <author> W. J. Cody. </author> <title> MACHAR: A subroutine to dynamically determine machine parameters. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 14 </volume> <pages> 303-311, </pages> <year> 1988. </year>
Reference-contexts: The subroutine DSYPRC [10, 11], which is used for modifying the negative eigencomponents obtained when factorizing an indefinite Hessian matrix using the Harwell code MA27. 4. The function DPMEPS <ref> [6] </ref>, which is used for dynamically determining the machine precision. The program was developed and tested on a Sun SPARC 10 Model 40 computer. The machine precision is calculated by the package and used in several places including finite differences stepsizes and stopping criteria.
Reference: [7] <author> T. F. Coleman, B. S. Garbow, and J. J. </author> <title> More. Fortran subroutines for estimating sparse Hessian matrices. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 11:378, </volume> <year> 1985. </year>
Reference-contexts: The following software are included in the package: 1. Harwell code MA27 [13], which is used for computing the L T DL factorization of the sparse Hessian matrix. 2. The Coleman and More graph coloring software <ref> [9, 8, 7] </ref>, which is used for estimating a finite-difference approximation of a sparse Hessian matrix. 3. The subroutine DSYPRC [10, 11], which is used for modifying the negative eigencomponents obtained when factorizing an indefinite Hessian matrix using the Harwell code MA27. 4.
Reference: [8] <author> T. F. Coleman, B. S. Garbow, and J. J. </author> <title> More. Software for estimating sparse Hessian matrices. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 11 </volume> <pages> 363-377, </pages> <year> 1985. </year>
Reference-contexts: The following software are included in the package: 1. Harwell code MA27 [13], which is used for computing the L T DL factorization of the sparse Hessian matrix. 2. The Coleman and More graph coloring software <ref> [9, 8, 7] </ref>, which is used for estimating a finite-difference approximation of a sparse Hessian matrix. 3. The subroutine DSYPRC [10, 11], which is used for modifying the negative eigencomponents obtained when factorizing an indefinite Hessian matrix using the Harwell code MA27. 4.
Reference: [9] <author> T. F. Coleman and J. J. </author> <title> More. Estimation of sparse Hessian matrices and graph coloring problems. </title> <journal> Math. Programming, </journal> <volume> 28 </volume> <pages> 243-270, </pages> <year> 1984. </year>
Reference-contexts: descent then x t if f (x t + ) &lt; f (x c ) + 10 4 rf (x c )d t then x + = x t else Find an acceptable x n + in the Newton direction d n using the line search given by Algorithm A6.3.1 <ref> [9, p.325] </ref> 4 Find an acceptable x t + in the tensor direction d t using the line search given by Algorithm A6.3.1 [9, p.325] if f (x n + ) then x + = x n else x + = x t endif endif else Find an acceptable x n <p> then x + = x t else Find an acceptable x n + in the Newton direction d n using the line search given by Algorithm A6.3.1 <ref> [9, p.325] </ref> 4 Find an acceptable x t + in the tensor direction d t using the line search given by Algorithm A6.3.1 [9, p.325] if f (x n + ) then x + = x n else x + = x t endif endif else Find an acceptable x n + in the Newton direction d n using the line search given by Algorithm A6.3.1 [9, p.325] x + = x n endif <p> the line search given by Algorithm A6.3.1 <ref> [9, p.325] </ref> if f (x n + ) then x + = x n else x + = x t endif endif else Find an acceptable x n + in the Newton direction d n using the line search given by Algorithm A6.3.1 [9, p.325] x + = x n endif In step 1, the gradient is either computed analytically or approximated by the algorithm A5.6.3 given in Dennis and Schnabel [12]. In step 2, the Hessian matrix is either calculated analytically or approximated by a graph coloring algorithm described in [9]. <p> In step 2, the Hessian matrix is either calculated analytically or approximated by a graph coloring algorithm described in <ref> [9] </ref>. In step 4.3, the matrix ^ r 2 f (x c ) is factorized using the augmented system approach described in [4]. <p> The following software are included in the package: 1. Harwell code MA27 [13], which is used for computing the L T DL factorization of the sparse Hessian matrix. 2. The Coleman and More graph coloring software <ref> [9, 8, 7] </ref>, which is used for estimating a finite-difference approximation of a sparse Hessian matrix. 3. The subroutine DSYPRC [10, 11], which is used for modifying the negative eigencomponents obtained when factorizing an indefinite Hessian matrix using the Harwell code MA27. 4.
Reference: [10] <author> A. R. Conn, N. I. M. Gould, and Ph. L. Toint. </author> <title> An introduction to the structure of large scale nonlinear optimization problems and the LANCELOT project. </title> <type> Report 89-19, </type> <institution> Namur University, </institution> <address> Namur, Belgium, </address> <year> 1989. </year>
Reference-contexts: Harwell code MA27 [13], which is used for computing the L T DL factorization of the sparse Hessian matrix. 2. The Coleman and More graph coloring software [9, 8, 7], which is used for estimating a finite-difference approximation of a sparse Hessian matrix. 3. The subroutine DSYPRC <ref> [10, 11] </ref>, which is used for modifying the negative eigencomponents obtained when factorizing an indefinite Hessian matrix using the Harwell code MA27. 4. The function DPMEPS [6], which is used for dynamically determining the machine precision. The program was developed and tested on a Sun SPARC 10 Model 40 computer.
Reference: [11] <author> A. R. Conn, N. I. M. Gould, and Ph. L. Toint. LANCELOT. </author> <title> Springer Series in Computational Mathematics. </title> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Harwell code MA27 [13], which is used for computing the L T DL factorization of the sparse Hessian matrix. 2. The Coleman and More graph coloring software [9, 8, 7], which is used for estimating a finite-difference approximation of a sparse Hessian matrix. 3. The subroutine DSYPRC <ref> [10, 11] </ref>, which is used for modifying the negative eigencomponents obtained when factorizing an indefinite Hessian matrix using the Harwell code MA27. 4. The function DPMEPS [6], which is used for dynamically determining the machine precision. The program was developed and tested on a Sun SPARC 10 Model 40 computer.
Reference: [12] <author> J. E. Dennis and R. B. Schnabel. </author> <title> Numerical methods for unconstrained optimization and nonlinear equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: endif else Find an acceptable x n + in the Newton direction d n using the line search given by Algorithm A6.3.1 [9, p.325] x + = x n endif In step 1, the gradient is either computed analytically or approximated by the algorithm A5.6.3 given in Dennis and Schnabel <ref> [12] </ref>. In step 2, the Hessian matrix is either calculated analytically or approximated by a graph coloring algorithm described in [9]. In step 4.3, the matrix ^ r 2 f (x c ) is factorized using the augmented system approach described in [4].
Reference: [13] <author> I. S. Duff and J. K. Reid. MA27: </author> <title> A set of Fortran subroutines for solving sparse symmetric sets of linear equations. </title> <type> Technical Report R-10533, </type> <institution> AERE Harwell Laboratory, Harwell, UK, </institution> <year> 1983. </year>
Reference-contexts: Calculate b and fl in the tensor model (1.3), so that the tensor model interpolates f (x) and rf (x) at x 1 4. Find a potential minimizer d t of the tensor model Factorize r 2 f (x c ) using the MA27 package <ref> [13] </ref> if r 2 f (x c ) has full rank then 4.1. <p> In step 4.6, we obtain a perturbation such as r 2 f (x c ) + I is safely positive definite by using the Gill, Murray, Ponceleon, and Saunders method [14]. After we compute the LDL T of the Hessian matrix using the MA27 package <ref> [13] </ref>, we change the block diagonal matrix D to D + E. The modified matrix is block diagonal positive definite. This guarantees that the decomposition L (D + E)L T is sufficiently positive definite. Note that the Hessian matrix is not modified if it is already positive definite. <p> Note that a statement that begins by neither CS nor CD will be picked by both tools. The following software are included in the package: 1. Harwell code MA27 <ref> [13] </ref>, which is used for computing the L T DL factorization of the sparse Hessian matrix. 2. The Coleman and More graph coloring software [9, 8, 7], which is used for estimating a finite-difference approximation of a sparse Hessian matrix. 3.
Reference: [14] <author> P. E. Gill, W. Murray, D. B. Ponceleon, and M. A. Saunders. </author> <title> Preconditioners for indefinite systems arising in optimization and nonlinear least squares problems. </title> <type> Technical Report SOL 90-8, </type> <institution> Department of Operations Research, Stanford University, California, </institution> <year> 1990. </year> <month> 22 </month>
Reference-contexts: In step 4.6, we obtain a perturbation such as r 2 f (x c ) + I is safely positive definite by using the Gill, Murray, Ponceleon, and Saunders method <ref> [14] </ref>. After we compute the LDL T of the Hessian matrix using the MA27 package [13], we change the block diagonal matrix D to D + E. The modified matrix is block diagonal positive definite. This guarantees that the decomposition L (D + E)L T is sufficiently positive definite.
Reference: [15] <author> J. J. More, B. S. Garbow, and K. E. Hillstrom. </author> <title> Testing unconstrained optimization software. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 7(1) </volume> <pages> 17-41, </pages> <year> 1981. </year>
Reference-contexts: C C STENMIN MINIMIZES AN UNCONSTRAINED NONLINEAR FUNCTION IN N C UNKNOWNS WHERE THE HESSIAN IS LARGE AND SPARSE, USING TENSOR C METHODS. C C EXAMPLE OF USE FOR STENMIN. THE TEST PROBLEM IS THE C THE BROYDEN TRIDIAGONAL <ref> [15] </ref>. C C ALI BOUARICHA, OCTOBER 1994. C MCS DIVISION, ARGONNE NATIONAL LAB.
Reference: [16] <author> R. B. Schnabel and T. Chow. </author> <title> Tensor methods for unconstrained optimization using second derivatives. </title> <journal> SIAM J. Optimization, </journal> <volume> 1 </volume> <pages> 293-315, </pages> <year> 1991. </year>
Reference-contexts: We abbreviate terms of the form dd; ddd, and dddd by d 2 ; d 3 , and d 4 , respectively.) Schnabel and Chow <ref> [16] </ref> select T c and V c such that the model interpolates function and gradient values from p past iterates, where p is a small number. This strategy results in T c and V c being low-rank tensors, which is crucial for the efficiency of the tensor method. <p> The whole process of forming the tensor model requires only O (n 2 ) arithmetic operations. The storage needed for forming and storing the tensor model is only a total of 6n. The tensor algorithms described in <ref> [16] </ref> are QR-based algorithms involving orthogonal transformations of the variable space. These algorithms are effective for minimizing the tensor model when the Hessian is dense because they are stable numerically, especially when the Hessian is singular.

References-found: 16

