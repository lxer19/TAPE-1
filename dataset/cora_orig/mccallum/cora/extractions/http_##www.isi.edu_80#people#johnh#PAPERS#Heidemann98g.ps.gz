URL: http://www.isi.edu:80/people/johnh/PAPERS/Heidemann98g.ps.gz
Refering-URL: http://www.isi.edu:80/people/johnh/PAPERS/Heidemann98g.html
Root-URL: http://www.isi.edu
Title: Automatic Selection of Nearby Web Servers  
Author: John Heidemann Vikram Visweswaraiah 
Date: December 8, 1998  
Abstract: Performance of global services such as the worldwide web can be improved by physically distributed replicas. Most replicated systems today request users to select manually a nearby replica (typically from a list of sites), yet users often are not willing or able to make an informed choice. This paper describes an approach to automatic replica selection which works without change to existing web clients and web and FTP servers. We describe the assumptions behind our approach and propose several metrics for estimation of network distance. We examine the feasibility behind this approach and compare the time required for replica-selection by each algorithm. Finally, we describe a small change to HTTP that would improve replica selection transparency. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Berners-Lee, R. Fielding, and H. Frystyk. </author> <title> Hypertext transfer protocol|HTTP/1.0. RFC 1945, Inter net Request For Comments, </title> <month> May </month> <year> 1995. </year>
Reference-contexts: That URL queries the replica selector which then redirects the client to a nearby server to provide the data. We exploit the HTTP/1.0 temporary redirection message (return code 302) to perform this redirection <ref> [1] </ref>. All existing browsers we are aware of respond to this message by transparently resubmitting the request to the newly suggested URL. Our basic architecture makes no assumptions about client location, but some replica selection algorithms are more stringent. We describe these in the next section.
Reference: [2] <author> R. Braden. </author> <title> Requirements for Internet hosts| communication layers. RFC 1122, Internet Request For Comments, </title> <month> October </month> <year> 1989. </year>
Reference-contexts: Unfortunately, source routing is not universally supported across the Internet. Source routing at gateways is optional <ref> [2] </ref> and is often disabled because of security concerns. Since the effectiveness of direct latency measurement degree of source routing support hosts full 25 unidirectional 14 none 48 host unavailable 13 total 100 Table 1: Evaluation of support for source routing in the Internet.
Reference: [3] <author> Robert L. Carter and Mark E. Crovella. </author> <title> Dynamic server selection using bandwidth probing in wide-area networks. </title> <type> Technical Report 96-007, </type> <institution> Boston University, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: Crovella and Carter have compared the effectiveness of hops count and latency distance metrics, concluding that latency is a better predictor of performance [5]. They also explored approaches to measure bandwidth, annotating links based on expected transfer times <ref> [3] </ref>. They find that replica selection based on bandwidth measurements performs substantially better than random selection, while bandwidth and best-of-5-RTT measurements perform comparably. The CPAN Multiplex Dispatcher automatically matches file requests to a nearby replica of a Perl FTP archive [4].
Reference: [4] <author> Tom Christiansen. </author> <note> CPAN multiplex dispatcher. At http://www.perl.com/CPAN, 1996. </note>
Reference-contexts: They find that replica selection based on bandwidth measurements performs substantially better than random selection, while bandwidth and best-of-5-RTT measurements perform comparably. The CPAN Multiplex Dispatcher automatically matches file requests to a nearby replica of a Perl FTP archive <ref> [4] </ref>. It uses client and server TLDs as its nearness metric. Our work explores additional algorithms and algorithm performance. Commercial applications of similar approaches exist (for example, IBM's Wom-Plex and systems under development at other companies) but are difficult to compare because of lack of publicly available implementation details.
Reference: [5] <author> Mark E. Crovella and Robert L. Carter. </author> <title> Dynamic server selection in the internet. </title> <booktitle> In Proceedings of the Third IEEE Workshop on the Architecture and Implementation of High Performance Communication Subsystems. IEEE, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: We measure latency rather than bandwidth because latency can often be more easily measured and that low latencies are correlated with high bandwidths. Crovella and Carter found bandwidth a slightly better predictor of performance, but by a only by a small margin <ref> [5] </ref>. 3.1 Random selection A first approach makes no attempt to select a server intelligently. It simply chooses one at random. This approach is very fast, but better results should be possible. This algorithm is similar in effect to round-robin server selection with DNS records [12]. <p> Their work focuses mainly on network bandwidth costs of replica selection rather than end-user elapsed times as ours does. Crovella and Carter have compared the effectiveness of hops count and latency distance metrics, concluding that latency is a better predictor of performance <ref> [5] </ref>. They also explored approaches to measure bandwidth, annotating links based on expected transfer times [3]. They find that replica selection based on bandwidth measurements performs substantially better than random selection, while bandwidth and best-of-5-RTT measurements perform comparably.
Reference: [6] <author> Ramesh Govindan and Anoop Reddy. </author> <title> Analysis of internet inter-domain topology and route stability. </title> <booktitle> In Proceedings of the IEEE Infocom, </booktitle> <pages> pages 851-858, </pages> <address> Kobe, Japan, </address> <month> April </month> <year> 1997. </year> <note> IEEE. </note>
Reference-contexts: Since whois queries take a substantial amount of time and location information changes very slowly, caching it at the replica selector can save substantial time. Caching latency measurements can also be helpful, but must be done with consideration to route stability <ref> [6] </ref>. 5 Algorithm Evaluation Section 3 described several approaches that can be used to select replicas. These algorithms have different levels of network and run-time overhead and can generate different quality results. This section evaluates these algorithms considering replica selection run-time. <p> Because source routing can be prevented at any hop in the routing path and routes change over time <ref> [6] </ref> we made observations for 10 consecutive days. Table 2 summarizes these results. Full source routing was supported by 27 hosts (about 30%) at some time and never available at the other 64 hosts.
Reference: [7] <author> Richard G. Guy, John S. Heidemann, Wai Mak, Thomas W. Page, Jr., Gerald J. Popek, and Dieter Rothmeier. </author> <title> Implementation of the Ficus replicated file system. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 63-71, </pages> <address> Anaheim, CA, June 1990. </address> <publisher> USENIX. </publisher>
Reference-contexts: A number of existing replication mechanisms can keep web replicas consistent in this manner; examples include web and FTP mirroring which provide master/slave replication and peer-to-peer replicated filing systems such as Ficus <ref> [7] </ref> which provide wide-area, optimistic replication. The replica selector does the actual work in our system. It handles HTTP-requests from the client and a returns redirect message after selecting a nearby replica. Several approaches to measure and approximate nearness are described in the next section. <p> A number of replication schemes have been proposed or deployed for web use. Caching file-systems such as AFS are attractive [11] and have been used to for replicated, co-located servers [12]. Wide-area, peer replicated file systems such as Ficus support independent update at different sites <ref> [7, 16] </ref>. In addition, a number of application-level approaches to replication or mirroring exist (for example, Lee McLoughlin's program "mirror" [14]). 7 Future Work Several directions exist for extensions of our work.
Reference: [8] <author> James D. Guyton and Michael F. Schwartz. </author> <title> Locating nearby copies of replicated internet servers. </title> <booktitle> In Proceedings of the ACM SIGCOMM, </booktitle> <pages> pages 288-298, </pages> <address> Cambridge, Massachusetts, </address> <month> August </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: Hop-count, bandwidth, and propagation latency all might be compared as a measure of nearness between clients and servers. Our algorithms uniformly compare approximations of latency. Hop-count can be thought of as a simpler approximation of latency, thus latency seems preferable. Although Guyton and Schultz preferred weighted hop-counts to latency <ref> [8] </ref>, the technical reasons for their choice seem to have been eclipsed by hardware improvements. We measure latency rather than bandwidth because latency can often be more easily measured and that low latencies are correlated with high bandwidths. <p> This section briefly reviews this work. Guyton and Schwartz examine the cost of locating servers through several schemes <ref> [8] </ref> including forms of broadcast and triangularization [10]. They compare hop count and round-trip packet latency, choosing weighted hop-count because of poor clock resolution and RTT variability. They assume an active client and, in some cases, additional servers (beacons) deployed throughout the network.
Reference: [9] <author> K. Harrenstien, M. Stahl, and E. Feinler. NIC-NAME/WHOIS. </author> <title> RFC 954, Internet Request For Comments, </title> <month> October </month> <year> 1985. </year>
Reference-contexts: In such flat name-spaces a name-based approximation may not be substantially better than random selection. 3.3 Geographic approximation A third approach is to assume that geographic location approximates network latency. We can determine the geographic location of a site by mapping its host-name through a whois database <ref> [9] </ref>, then mapping the telephone number or address in the database entry into latitude and longitude. Proposals have also been made to add geographic location to the DNS. This approach addresses both the second and third problems of domain-name-based approximation.
Reference: [10] <author> Steven Michael Hotz. </author> <title> Routing Information Organization to Support Scalable Interdomain Routing with Heterogeneous Path Requirements. </title> <type> PhD thesis, </type> <institution> USC, </institution> <year> 1996. </year>
Reference-contexts: This section briefly reviews this work. Guyton and Schwartz examine the cost of locating servers through several schemes [8] including forms of broadcast and triangularization <ref> [10] </ref>. They compare hop count and round-trip packet latency, choosing weighted hop-count because of poor clock resolution and RTT variability. They assume an active client and, in some cases, additional servers (beacons) deployed throughout the network.
Reference: [11] <author> John Howard, Michael Kazar, Sherri Menees, Da-vid Nichols, Mahadev Satyanarayanan, Robert Side-botham, and Michael West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Caching can allow easier deployment and can adapt better to changing use, however. A number of replication schemes have been proposed or deployed for web use. Caching file-systems such as AFS are attractive <ref> [11] </ref> and have been used to for replicated, co-located servers [12]. Wide-area, peer replicated file systems such as Ficus support independent update at different sites [7, 16].
Reference: [12] <author> E. D. Katz, M. Butler, and R. McGrath. </author> <title> A scalable HTTP server: The NCSA prototype. </title> <booktitle> In Proceedings of the First International World Wide Web Conference, </booktitle> <pages> pages 155-164, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Two problems often occur with this kind of replication today. First, objects in the web have the server's hostname embedded in the URL. Although a host-name can point to multiple servers, selection between these servers is often random <ref> [12] </ref>. Random selection works well to balance server load when all replicas are co-located, but it does little to optimize performance when they are geographically distributed. An alternative is to list all the replicas to users and ask them to pick one. This approach has a number of problems. <p> The servers replicate a common set of data. We assume a moderate number (2-50) of geographically distributed replicas, some of which will be closer to the client than others. With co-located replicas simpler solutions (for example, round-robin DNS records <ref> [12] </ref>) will probably provide a simpler solution than our system. Large numbers of replicas (20-100's) may require caching or hybrid approaches to replica selection to limit selection time; we discuss these in Sections 3.6 and 4. We assume that all replicas are kept in loose consistency through an external mechanism. <p> We use the later approach, although the straightforward adaptation to a stand-alone server would improve performance. Logically there is a single replica selector per replicated service. To improve availability and distribute load we expect most services implement multiple (replicated) replica selectors with mechanisms such as round-robin DNS records <ref> [12] </ref>. Because the messages exchanged between the selector and the client and server are brief, the geographic location of selectors will not substantially affect performance and so the client's choice of selector is relatively unimportant. <p> It simply chooses one at random. This approach is very fast, but better results should be possible. This algorithm is similar in effect to round-robin server selection with DNS records <ref> [12] </ref>. We consider it only for comparison to other approaches. 3.2 Domain-name-based approximation Our next approach to very roughly approximate latency is to compare top-level domain names of the client and servers. <p> Caching can allow easier deployment and can adapt better to changing use, however. A number of replication schemes have been proposed or deployed for web use. Caching file-systems such as AFS are attractive [11] and have been used to for replicated, co-located servers <ref> [12] </ref>. Wide-area, peer replicated file systems such as Ficus support independent update at different sites [7, 16]. In addition, a number of application-level approaches to replication or mirroring exist (for example, Lee McLoughlin's program "mirror" [14]). 7 Future Work Several directions exist for extensions of our work.
Reference: [13] <author> Ari Luotonen and Kevin Altis. </author> <title> World-Wide Web proxies. </title> <booktitle> In Proceedings of the Fourth International World Wide Web Conference, </booktitle> <month> December </month> <year> 1994. </year>
Reference-contexts: Sonar servers are assumed to be co-located with the clients and determine and cache round-trip time to the servers as an estimate of nearness. Proxy caches are similar to web replication in that they make copies of data at distributed servers to improve performance <ref> [13] </ref>. A primary difference between caching and replication is that caching is typically on-demand (with lazy evaluation), client-based, and best effort. Replication is often pre-emptive, server-based, and pre-meditated.
Reference: [14] <author> Lee McLoughlin. </author> <title> Mirror. A perl program to duplicate FTP-able directory trees, </title> <note> available from http://www.perl.com/CPAN/scripts/ftpstuff/mirror-2.8.tar.gz, 1996. </note>
Reference-contexts: Wide-area, peer replicated file systems such as Ficus support independent update at different sites [7, 16]. In addition, a number of application-level approaches to replication or mirroring exist (for example, Lee McLoughlin's program "mirror" <ref> [14] </ref>). 7 Future Work Several directions exist for extensions of our work. Our current framework for replica selection makes it easy to experiment with new distance measures; we would like to explore additional metrics. Our current architecture takes as an assumption that neither the client nor the server can change.
Reference: [15] <author> Keith Moore, Shirley Browne, Stan Green, and Reed Wade. </author> <title> Resource cataloging and distribution service (RCDS). </title> <type> Technical Report 97-346, </type> <institution> University of Ten-nessee, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: Our architecture assumes that backwards compatibility is important enough that a solution must be provided without changes to either the clients or servers. This opportunity for incremental deployment distinguishes our work from other approaches such as Sonar <ref> [15] </ref> and most current proposals for URNs [18]. Although our system is backwards compatible with existing clients and servers, minor changes to the client can improve the user-model, performance, and accuracy of our system. <p> Like URNs, PURLs (Persistent URLs) provide a persistent identifiers [17]. PURLs are implemented with the same redirection mechanism we employ. Currently PURLs do not employ replication. Another URN-like system is the Resource Cataloging and Distribution System (RCDS) <ref> [15] </ref>. RCDS provides a multi-step resolution system where URNs are converted to Location Independent File Names (LIFNs) which are mapped to nearby standard web or FTP servers with the help of a Sonar service.
Reference: [16] <author> T. W. Page, R. G. Guy, J. S. Heidemann, D. Ratner, P. Reiher, A. Goel, G. H. Kuenning, and G. J. Popek. </author> <title> Perspectives on optimistically replicated peer-to-peer filing. </title> <journal> Software|Practice and Experience, </journal> <volume> 28(2) </volume> <pages> 155-180, </pages> <month> February </month> <year> 1998. </year>
Reference-contexts: A number of replication schemes have been proposed or deployed for web use. Caching file-systems such as AFS are attractive [11] and have been used to for replicated, co-located servers [12]. Wide-area, peer replicated file systems such as Ficus support independent update at different sites <ref> [7, 16] </ref>. In addition, a number of application-level approaches to replication or mirroring exist (for example, Lee McLoughlin's program "mirror" [14]). 7 Future Work Several directions exist for extensions of our work.
Reference: [17] <author> Keith Shafer, Stuart Weibel, Erik Jul, and Jon Fausey. </author> <title> Introduction to persistent uniform resource locators. </title> <note> http://purl.oclc.org/OCLC/PURL/INET96, 1995. </note>
Reference-contexts: URNs are a complementary approach to our mechanisms and may eliminate the binding-time issues we observe (see Section 4.1); URN mechanisms could employ replica selection algorithms such as we describe to improve performance. Like URNs, PURLs (Persistent URLs) provide a persistent identifiers <ref> [17] </ref>. PURLs are implemented with the same redirection mechanism we employ. Currently PURLs do not employ replication. Another URN-like system is the Resource Cataloging and Distribution System (RCDS) [15].
Reference: [18] <author> Karen Sollins and Larry Masinter. </author> <title> Functional requirements for uniform resource names. RFC 1737, Internet Request For Comments, </title> <month> December </month> <year> 1994. </year>
Reference-contexts: Our architecture assumes that backwards compatibility is important enough that a solution must be provided without changes to either the clients or servers. This opportunity for incremental deployment distinguishes our work from other approaches such as Sonar [15] and most current proposals for URNs <ref> [18] </ref>. Although our system is backwards compatible with existing clients and servers, minor changes to the client can improve the user-model, performance, and accuracy of our system. <p> Commercial applications of similar approaches exist (for example, IBM's Wom-Plex and systems under development at other companies) but are difficult to compare because of lack of publicly available implementation details. The primary goal of Uniform Resource Names (URNs) is to provide a "globally unique, persistent identifier" <ref> [18] </ref>. URN protocols are still being discussed, but most of the implementation proposals for URNs employ some form of replication to provide high availability.
References-found: 18

