URL: http://www.cs.cmu.edu/~yannick/flairs98.ps.gz
Refering-URL: http://www.cs.cmu.edu/~yannick/publis.html
Root-URL: 
Email: yannick@cmu.edu  
Title: A hierarchical ensemble of decision trees applied to classifying data from a psychological experiment  
Author: Yannick Lallement 
Address: 5000 Forbes Avenue Pittsburgh, PA 15213  
Affiliation: Human-Computer Interaction Institute Carnegie Mellon University  
Abstract: Classifying by hand complex data coming from psychology experiments can be a long and difficult task, because of the quantity of data to classify and the amount of training it may require. One way to alleviate this problem is to use machine learning techniques. We built a classifier based on decision trees that reproduces the classifying process used by two humans on a sample of data and that learns how to classify unseen data. The automatic classifier proved to be more accurate, more constant and much faster than classification by hand. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackerman, P. L., and Kanfer, R. </author> <year> 1994. </year> <title> Kanfer-ackerman air traffic controller task cd rom database, data-collection program, and playback program. </title> <type> Technical report, </type> <institution> University of Minnesota, Department of Psychology. </institution>
Reference-contexts: We built a learning classifier to make the classification faster and more reliable. In the following, we describe our data, the classifier we built, and the results we obtained. The KA-ATC c fl Task In the Kanfer-Ackerman Air Traffic Control c fl task <ref> (Ackerman & Kanfer 1994) </ref> is used to study problem-solving and learning in a dynamic environment. When performing this task, participants are presented with 0 Copyright c fl 1998, American Association for Artificial Intelligence (www.aaai.org). <p> Participants perform the task during 10-minute trials, and are instructed to maximize their score. Timestamped keystroke data from over 3500 participants are available on a CD-ROM <ref> (Ackerman & Kan-fer 1994) </ref>. These data are the basis for learning models that use different AI and cognitive architectures. We are interested in a specific study (study number 2) in which each of the 58 participants performed 24 10-minute trials.
Reference: <author> Breiman, L. </author> <year> 1994. </year> <title> Bagging predictors. </title> <type> Technical Report 421, </type> <institution> Department of statistics, University of California, Berkeley. </institution>
Reference-contexts: Training Each tree was trained on a random subset of the training set (using the bagging method <ref> (Breiman 1994) </ref>): the trees of the full trial level were trained using 40% of our 424 hand-labelled trials for training, and 20% for pruning, leaving 40% unused.
Reference: <author> Dietterich, T. G. </author> <year> 1997. </year> <title> Machine learning research: Four current directions. </title> <journal> AI Magazine 18(4). </journal>
Reference: <author> John, B. E., and Lallement, Y. </author> <year> 1997. </year> <title> Strategy use while learning to perform the kanfer-ackerman air traffic controller task. </title> <booktitle> In Proceedings of the nineteen annual Cognitive Science Society Conference. </booktitle>
Reference-contexts: Prior to writing a cognitive model of one of those participants, we became interested in a particular aspect of the task: how participants accept planes from the queue into the hold-pattern <ref> (John & Lallement 1997) </ref>. Several strategies can be observed, that we present in the next section. Queue acceptance strategies brought. Each timeline represents a trials, with seconds since the start of the trial (x-axis) and the 12 hold-rows (y-axis). The three hold-levels are separated by dotted horizontal lines.
Reference: <author> Murthy, S. K.; Kasif, S.; and Salzberg, S. </author> <year> 1994. </year> <title> A system for induction of oblique decision trees. </title> <journal> Journal of artificial intelligence research 2 </journal> <pages> 1-32. </pages>
Reference-contexts: The structure of a training example as it will be used by the decision trees is given in table 2. The classifier Architecture An initial experiment with a single ID3 (Quinlan 1986) and a single OC-1 <ref> (Murthy, Kasif, & Salzberg 1994) </ref> Tree Training Pruning Test Full trial 175 75 174 Half trial 420 180 248 Quarter trial 840 360 496 Table 3: Number of training, pruning and test examples for each type of tree Tree set Average Average number of leaves depth Full trial 9.78 4.82 Half
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine learning 1(1) </booktitle> <pages> 81-106. </pages>
Reference-contexts: The structure of a training example as it will be used by the decision trees is given in table 2. The classifier Architecture An initial experiment with a single ID3 <ref> (Quinlan 1986) </ref> and a single OC-1 (Murthy, Kasif, & Salzberg 1994) Tree Training Pruning Test Full trial 175 75 174 Half trial 420 180 248 Quarter trial 840 360 496 Table 3: Number of training, pruning and test examples for each type of tree Tree set Average Average number of leaves
Reference: <author> Quinlan, J. R. </author> <year> 1996. </year> <editor> Bagging, boosting and c4.5. </editor> <booktitle> In Thirteenth National Conference on Artificial Intelligence. </booktitle> <address> Portland, Oregon: </address> <publisher> AAI Press / MIT Press. </publisher>
References-found: 7

