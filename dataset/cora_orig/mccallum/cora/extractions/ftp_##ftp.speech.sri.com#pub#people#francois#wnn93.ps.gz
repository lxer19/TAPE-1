URL: ftp://ftp.speech.sri.com/pub/people/francois/wnn93.ps.gz
Refering-URL: http://www.speech.sri.com/people/francois/publications.html
Root-URL: 
Title: SIMPLE ALGORITHMS FOR FAST ADAPTIVE FILTERING  
Author: Fran~coise Beaufays and Bernard Widrow 
Affiliation: Department of Electrical Engineering Stanford University  
Abstract: This problem can be overcome by preprocessing the inputs to the LMS filter with a fixed data-independent transformation that, at least partially, decorrelates the inputs. Typically, the preprocessing consists of a DFT or a DCT transformation followed by a power normalization stage. The resulting algorithms are called DFT-LMS and DCT-LMS. This technique is to be contrasted with more traditional approaches such as recursive least squares algorithms, where an estimate of the inverse input autocorrelation matrix is used to improve the filter convergence speed. After placing DFT-LMS and DCT-LMS into context, we propose three different approaches to explain the algorithms both intuitively and analytically. We discuss the convergence speed improvement brought by these algorithms over conventional LMS, and we make a short analysis of their computational cost. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Beaufays, F. and Widrow, B. </author> <year> 1993. </year> <title> Transform domain adaptive filters: an analytical approach. </title> <note> Submitted to IEEE Trans. on Signal Proc. </note>
Reference-contexts: The analysis is further complicated by the fact that only asymptotically do the eigen-values stabilize to fixed magnitudes independent of n, and that power normalization is a nonlinear operation. Successive matrix manipulations and passages to the limit allowed us to prove the following asymptotic results <ref> (see Beaufays, 1993, 1994) </ref> for more details): Eigenvalue spread after DFT = 1 + ; Eigenvalue spread after DCT = 1 + : Note that with the DCT, the asymptotic eigenvalue spread is never higher than 2! As a numerical example, let the correlation be equal to 0.95.
Reference: <author> Beaufays, F. </author> <year> 1994. </year> <type> Ph.D. Thesis, </type> <note> in preparation. </note> <institution> Information Systems Lab., Stanford University, Stanford, </institution> <address> CA. </address>
Reference: <author> Bershad, N. and Macchi, O. </author> <year> 1989. </year> <title> Comparison of RLS and LMS algorithms for tracking a chirped signal. </title> <booktitle> Proc. </booktitle>
Reference-contexts: On the other hand, RLS suffers from poor tracking capabilities in nonstationary environments <ref> (Bershad, 1989) </ref>, from high computational cost, and from lack of robustness under certain input conditions. The computational cost and robustness issues have been addressed by researchers in developing other exact least squares algorithms, the most famous of them being the recursive lattice filter algorithms.
Reference: <editor> ICASSP. </editor> <address> Glasgow, Scotland: </address> <pages> 896-899. </pages>
Reference: <author> Franklin, G. F. et al. </author> <year> 1990. </year> <title> Digital Control of Dynamic Systems. Second edition. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference: <author> Gray, R. M. </author> <year> 1977. </year> <title> Toeplitz and Circulant Matrices: II. </title> <type> Tech. report. 6504-1. </type> <institution> Information Systems Lab., Stan-ford University. </institution>
Reference: <author> Grenander, U. and Szego, G. </author> <year> 1984. </year> <title> Toeplitz forms and their applications. Second edition. </title> <publisher> Chelsea Publishing Company, </publisher> <address> New York. </address>
Reference: <author> Haykin, S. </author> <year> 1991. </year> <title> Adaptive Filter Theory. Second edition. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> North, R. C. et al. </author> <year> 1993. </year> <title> A floating-point arithmetic error analysis of direct and indirect coefficient updating techniques for adaptive lattice filters. </title> <journal> IEEE Trans. on Signal Proc. </journal> <volume> 41, </volume> <month> no.5(May):1809-1823. </month>
Reference-contexts: The computational cost and robustness issues have been addressed by researchers in developing other exact least squares algorithms, the most famous of them being the recursive lattice filter algorithms. Lattice filters typically require less computations per iteration than RLS, but even their most robust forms can present stability problems <ref> (North, 1993) </ref>. In addition, they are long and complicated to implement. LMS is intrinsically slow because it does not decor-relate its inputs prior to adaptive filtering, but preprocessing the inputs with an estimate of the inverse input autocorrelation matrix in the fashion of RLS leads to the problems cited above.
Reference: <author> Rao, K. R. and Yip, P. </author> <year> 1990. </year> <title> Discrete cosine transform. </title> <publisher> Academic Press, Inc, </publisher> <address> San Diego, CA. </address>
Reference: <author> Widrow, B. and Stearns, S. D. </author> <year> 1985. </year> <title> Adaptive Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address> <booktitle> Fran~coise Beaufays received the bachelor degree in mechanical and electrical engineering in 1988 from Uni-versite Libre de Bruxelles, </booktitle> <address> Brussels, Belgium, </address> <note> and the MSEE degree in 1989 from Stanford University. She is currently a PhD student and research assistant at Stan-ford University, working under the supervision of Prof. </note>
Reference-contexts: INTRODUCTION It is well know from the theory of LMS <ref> (Widrow, 1985) </ref> that the mean square error of an adaptive filter trained with the LMS algorithm decreases over time as a sum of exponentials whose time constants are inversely proportional to the eigenvalues of the autocorrelation matrix of the inputs to the filter. <p> Large eigenvalues, on the other hand, put a limit on the maximum learning rate that can be chosen without encountering stability problems <ref> (Widrow,1985) </ref>. It results from these two counteracting factors that the best convergence properties are obtained when all the eigenvalues are equal, that is when the input autocorrelation matrix is proportional to the identity matrix. <p> Unitary transformations perform only rotations and symmetries, they do not modify the shape of the object they transform. The mean square error of LMS is a quadratic function of the weights <ref> (Widrow, 1985) </ref>. Writing the MSE as a function of the weights and fixing it to some constant value, we get an implicit quadratic function of the weights that represents a hyperellipsoid in the n-dimensional weight space.
Reference: <author> B. Widrow. </author> <title> Her interests include linear adaptive filtering, signal processing, and nonlinear neural networks. </title>
References-found: 12

