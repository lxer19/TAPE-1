URL: http://dis.cs.umass.edu/~nagendra/coord_learn.ps
Refering-URL: http://dis.cs.umass.edu/~nagendra/home.html
Root-URL: 
Email: fnagendra,lesserg@cs.umass.edu  
Title: Learning Situation-Specific Coordination in Cooperative Multi-agent Systems  
Author: M V Nagendra Prasad, and Victor R Lesser 
Keyword: Multi-agent Systems, Coordination, Learning  
Address: Amherst, MA 01002.  
Affiliation: Department of Computer Science University of Massachusetts,  
Abstract: Achieving effective cooperation in a multi-agent system is a difficult problem for a number of reasons such as limited and possiblyout-dated views of activitiesof other agents and uncertainty about the outcomes of interacting non-local tasks. In this paper, we present a learning system called COLLAGE, that endows the agents with the capability to learn how to choose the most appropriate coordination strategy from a set of available coordination strategies. COLLAGE relies on meta-level information about agents' problem solving situations to guide them towards a suitable choice for a coordination strategy. We present empirical results that strongly indicate the effectiveness of the learning algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Sutton A. Barto and C. Watkins. </author> <title> Learning and sequential decision making. </title> <editor> In M. Gariel and J. W. Moore, editors, </editor> <booktitle> Learning and Computational Neuroscience, </booktitle> <address> Cambridge, MA, 1990. </address> <publisher> MIT Press. </publisher>
Reference-contexts: We then present some of our experimental results and conclude. 2 Related Work Previous work related to learning in multi-agent systems is limited and much of this work relies on techniques derived from reinforcement learning <ref> [1, 35] </ref>, genetic algorithms [16] and classifier systems [17]. In Sen, Sekaran and Hale [29] the agents use reinforcement learning to evolve complimentary policies in a box pushing task, and in Crites and Barto [3] a team of reinforcement learning agents optimize elevator dispatching performance. <p> Decisions about coordination strategy choice are made based on similar past cases. Outcomes decide the desirability of the strategies. We define a similarity function and a utility functions as follows: s : P 2 ! <ref> [0; 1] </ref> In the experiments presented later, we use the Euclidean metric for similarity. The desirability of a coordination strategy is determined by a similarity-weighted sum of the utility it yielded in the similar past cases in a small neighborhood around the present situation vector.
Reference: [2] <author> D. W. Aha, D. Kibler, and M. K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: Learning in COLLAGE (COordination Learner for muLtiple AGEnt systems) falls into the category of Instance-Based Learning algorithms <ref> [2] </ref> originally proposed for supervised classification learning. We, however, use the IBL-paradigm for unsupervised learning of decision-theoretic choice. We would like to note that we could as well have used some other method like neural nets or decision trees.
Reference: [3] <author> Robert H. Crites and Andrew G. Barto. </author> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: In Sen, Sekaran and Hale [29] the agents use reinforcement learning to evolve complimentary policies in a box pushing task, and in Crites and Barto <ref> [3] </ref> a team of reinforcement learning agents optimize elevator dispatching performance. In both these works, the agents do not communicate with one another and any agent treats the other agents as a part of the environment.
Reference: [4] <author> Keith S. Decker. </author> <title> Environment Centered Analysis and Design of Coordination Mechanisms. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1995. </year> <month> 19 </month>
Reference-contexts: GPGP coordination mechanisms rely on the coordination problem instance being represented in a framework called TMS <ref> [4, 5] </ref>. The TMS framework (Task Analysis, Environment Modeling, and Simulation) [4, 5] represents coordination problems in a formal, domain-independent way. <p> GPGP coordination mechanisms rely on the coordination problem instance being represented in a framework called TMS <ref> [4, 5] </ref>. The TMS framework (Task Analysis, Environment Modeling, and Simulation) [4, 5] represents coordination problems in a formal, domain-independent way.
Reference: [5] <author> Keith S. Decker and Victor R. Lesser. </author> <title> Quantitative modeling of complex computational task environments. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 217-224, </pages> <address> Washington, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: GPGP coordination mechanisms rely on the coordination problem instance being represented in a framework called TMS <ref> [4, 5] </ref>. The TMS framework (Task Analysis, Environment Modeling, and Simulation) [4, 5] represents coordination problems in a formal, domain-independent way. <p> GPGP coordination mechanisms rely on the coordination problem instance being represented in a framework called TMS <ref> [4, 5] </ref>. The TMS framework (Task Analysis, Environment Modeling, and Simulation) [4, 5] represents coordination problems in a formal, domain-independent way. <p> Figure 1 is an example of a simple tasks structure. Besides task/subtask relationships, there can be other interdependencies, called coordination interrelationships, between tasks in a task group <ref> [5] </ref>. In this paper, we will be dealing with two such interrelationships: * facilitates relationship or soft interrelationship: Task A facilitates Task B if the the execution Task A produces a result that affects an optional parameter of Task B.
Reference: [6] <author> Keith S. Decker and Victor R. Lesser. </author> <title> Designing a family of coordination algorithms. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems, </booktitle> <pages> pages 73-80, </pages> <address> San Francisco, CA, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: 1 Introduction Coordination is the process of effectively managing interdependencies between activities distributed across agents so as to derive maximum benefit from them <ref> [21, 6] </ref>. Based on structure and uncertainty in their environment, agents have to choose and temporally order their activities to mitigate the effects of harmful interdependencies and exploit the beneficial interdependencies among them. <p> In human organizations, environmental factors such as dynamism and task uncertainty have a strong effect on what coordinated actions are and how organizationally acceptable outcomes arise [18, 9, 33]. These effects have been observed in purely computational organizations as well <ref> [8, 7, 6, 23, 26] </ref>. Achieving effective coordination in a multi-agent system (MAS) is a difficult problem for a number of reasons. <p> In certain situations, coordination protocols that permit some level of non-coherent activity and avoid the additional overhead for coordination may lead to better performance <ref> [7, 6, 23, 34] </ref>. For example, when the agents are under severe time pressure and the load of the activities at the agents is high, sophisticated agent coordination strategies do not generally payoff. <p> Experimental results have already verified that for some environments a subset of the mechanisms is more effective than using the entire set of mechanisms <ref> [6, 23] </ref>. In this work, we present a learning extension, called COLLAGE, that endows the agents with the capability to choose a suitable subset of the coordination mechanisms based on the present problem solving situation, instead of having a fixed subset across all problem instances 4 in an environment. <p> It might be possible to view rough commitments as precompiled social laws [30] 1 . The latter two coordination strategies are the alternatives normally used in the distributed data processing domain [23]. Decker and Lesser <ref> [6] </ref> proposed balanced as a sophisticated strategy that exploits a number of mechanisms to achieve coordination. In our experiments, agents learn to choose among these three coordination strategies in the domain of distributed data processing. <p> The agent performance can be characterized along a number of dimensions like total quality, number of methods executed, number of communications, and termination time. Decker and Lesser <ref> [6] </ref> explore the performance space of the coordination strategies based on these four performance measures and identify five prototypical coordination strategies: balanced with all mechanisms for detecting soft and hard coordination interrelation ships and forming commitments on them turned on. <p> An example global situation vector looks as follows: (0.82 0.77 0.66 0.89 1.0 0.87). Here the low value of the third component represents large quality gains by detecting and coordinating on hard interrelationships. Thus two of the more sophisticated coordination strategies called balanced and tough <ref> [6] </ref> are found to be better performers in this situation. On the other hand, in a global situation vector such as (0.80 0.90 0.88 0.80 0.61 0.69) the low values of fifth and sixth components indicate high time pressure and load in the present problem solving episode. <p> Even if the agents use sophisticated strategies to coordinate, they may not have the time to benefit from it. Hence, relatively simple coordination strategies like simple or mute <ref> [6] </ref> do better in this scenario. Note, however, that in most situation vectors, these trade-offs are subtle and not as obvious as the above examples. It is difficult for a human to look at the situations and easily predict which strategy is the best performer.
Reference: [7] <author> E. Durfee and V. Lesser. </author> <title> Predictability vs. responsiveness: Coordinating problem solvers in dynamic domains. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 66-71, </pages> <address> St. Paul, Minnesota, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: In human organizations, environmental factors such as dynamism and task uncertainty have a strong effect on what coordinated actions are and how organizationally acceptable outcomes arise [18, 9, 33]. These effects have been observed in purely computational organizations as well <ref> [8, 7, 6, 23, 26] </ref>. Achieving effective coordination in a multi-agent system (MAS) is a difficult problem for a number of reasons. <p> In certain situations, coordination protocols that permit some level of non-coherent activity and avoid the additional overhead for coordination may lead to better performance <ref> [7, 6, 23, 34] </ref>. For example, when the agents are under severe time pressure and the load of the activities at the agents is high, sophisticated agent coordination strategies do not generally payoff.
Reference: [8] <author> Mark S. Fox. </author> <title> An organizational view of distributed systems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 11(1) </volume> <pages> 70-80, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: In human organizations, environmental factors such as dynamism and task uncertainty have a strong effect on what coordinated actions are and how organizationally acceptable outcomes arise [18, 9, 33]. These effects have been observed in purely computational organizations as well <ref> [8, 7, 6, 23, 26] </ref>. Achieving effective coordination in a multi-agent system (MAS) is a difficult problem for a number of reasons.
Reference: [9] <author> J. </author> <title> Galbraith. Organizational Design. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1977. </year>
Reference-contexts: In human organizations, environmental factors such as dynamism and task uncertainty have a strong effect on what coordinated actions are and how organizationally acceptable outcomes arise <ref> [18, 9, 33] </ref>. These effects have been observed in purely computational organizations as well [8, 7, 6, 23, 26]. Achieving effective coordination in a multi-agent system (MAS) is a difficult problem for a number of reasons.
Reference: [10] <author> A. Garland and R. Alterman. </author> <title> Multi-agent learning through collective memory. </title> <booktitle> In Proceedings of the 1996 AAAI Spring Symposium on Adaptation, Co-evolution and Learning in Multiagent Systems, </booktitle> <address> Stanford, CA, </address> <year> 1996. </year>
Reference-contexts: Despite these limitations, combining their work on learning situation representations with the learning presented here on situation-based choice of coordination could have interesting implications for situation-specific learning. Garland and Alterman <ref> [10] </ref> discuss issues in knowledge reuse in multi-agent systems. Agents working in a simulated MOVERS-WORLD domain have to collaborate to move objects too heavy for a single agent to move. Agents use their past experience to anticipate coordination rather than dynamically communicate to establish it.
Reference: [11] <author> Alan Garvey, Keith Decker, and Victor Lesser. </author> <title> A negotiation-based interface between a real-time scheduler and a decision-maker. </title> <type> CS Technical Report 94-08, </type> <institution> University of Mas-sachusetts, </institution> <year> 1994. </year>
Reference-contexts: Thus, by choosing different sets of coordination mechanisms, the amount of communication and other overheads associated with coordination can be varied. The interface between the coordination mechanisms and the local scheduler is bidirectional and negotiation-based, permitting the coordination module to ask what-if questions <ref> [11] </ref>. This is a crucial ability whose utility becomes obvious later, when we describe our learning algorithm in more detail. In GPGP, a coordination strategy can be derived by activating a subset of the coordination mechanisms.
Reference: [12] <author> Alan Garvey and Victor Lesser. </author> <title> Design-to-time real-time scheduling. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 23(6) </volume> <pages> 1491-1502, </pages> <year> 1993. </year>
Reference-contexts: The local scheduler of an agent uses the information in the belief database to build schedules of method execution actions in order to maximize utility. In this work we use a design-to-time scheduler <ref> [12] </ref> that can take into account, the constraints provided by the coordination module and provide schedules that maximize the global utility measure.
Reference: [13] <author> Itzhak Gilboa and David Schmeidler. </author> <title> Case-based Decision Theory. </title> <journal> The Quaterly Journal of Economics, </journal> <pages> pages 605-639, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: This is done in order to avoid biasing the similarity metric in favor of any particular feature. 10 4.2 Choosing a Coordination Strategy COLLAGE chooses a coordination strategy based on how the set of available strategies performed in similar past cases. We adopt the notation from Gilboa and Schmeidler <ref> [13] </ref> 3 . <p> The experimenter was limited to setting certain numerical parameters like mean of the task structure depth or mean and variance of the number 3 Gilboa and Schmeidler <ref> [13] </ref> describe case-based decision theory as a normative theory of human behavior during decision making. Even though, we adopt their notation, there are crucial differences in the motivations and structure of the two works.
Reference: [14] <author> John J. Grefenstette. </author> <title> The Evolution of Strategies for multi-agent environments. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 1(1) </volume> <pages> 65-89, </pages> <year> 1992. </year>
Reference-contexts: Tan [36] deals with a predator-prey domain in a grid world. The agents share perception information to overcome perceptual limitations or communicate policy functions learned through reinforcement learning. Grefenstette <ref> [14] </ref> uses genetic algorithms to learn reactive decision rules for agents in a predator-prey domain similar to that in [36]. <p> Agents sharing perceptual information as in Tan [36] and Greffenstette <ref> [14] </ref> or bidding information as in Weiss [37] do not make explicit the notion of situating the local control knowledge in a more global, abstract situation. <p> The information shared is weak, and the studies were conducted in domains such as predator-prey <ref> [36, 14] </ref> or blocks world [37] where the need for sharing meta-level information and situating learning in this information is not apparent. Our previous work explored learning situation-specific organizational roles in a heterogeneous multi-agent system [26].
Reference: [15] <author> Thomas Haynes and Sandip Sen. </author> <title> Learning cases to resolve conflicts and improve group behavior. </title> <note> Submitted, </note> <year> 1996. </year>
Reference-contexts: Sandholm and Crites [28] study the emergence of cooperation in the Iterated Prisoner's Dilemma problem, where the agents are self-interested, and an agent is not free to ask for any kind of information from the other agents. Haynes and Sen <ref> [15] </ref> present studies in learning cases to resolve conflicts among agents in a predator-prey domain similar to that used by Tan [36]. There is no communication between the agents, and the cases result from the agent's perception of the problem solving state.
Reference: [16] <author> J. H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <institution> University of Michigan Press, Ann Arbor, Michigan, </institution> <year> 1975. </year>
Reference-contexts: We then present some of our experimental results and conclude. 2 Related Work Previous work related to learning in multi-agent systems is limited and much of this work relies on techniques derived from reinforcement learning [1, 35], genetic algorithms <ref> [16] </ref> and classifier systems [17]. In Sen, Sekaran and Hale [29] the agents use reinforcement learning to evolve complimentary policies in a box pushing task, and in Crites and Barto [3] a team of reinforcement learning agents optimize elevator dispatching performance.
Reference: [17] <author> J. H. Holland. </author> <title> Properties of bucket brigade algorithm. </title> <booktitle> In First International Conference on Genetic Algorithms and their Applications, </booktitle> <pages> pages 1-7, </pages> <address> Pittsburgh, PA, </address> <year> 1985. </year>
Reference-contexts: We then present some of our experimental results and conclude. 2 Related Work Previous work related to learning in multi-agent systems is limited and much of this work relies on techniques derived from reinforcement learning [1, 35], genetic algorithms [16] and classifier systems <ref> [17] </ref>. In Sen, Sekaran and Hale [29] the agents use reinforcement learning to evolve complimentary policies in a box pushing task, and in Crites and Barto [3] a team of reinforcement learning agents optimize elevator dispatching performance. <p> In both these works, the agents do not communicate with one another and any agent treats the other agents as a part of the environment. Weiss [37] uses a variant of Holland's <ref> [17] </ref> bucket brigade algorithm for learning hierarchical organization structuring relationships in a block world domain via strengthening promising chains of actions through bid-reward cycles. Tan [36] deals with a predator-prey domain in a grid world.
Reference: [18] <author> Paul Lawrence and Jay Lorsch. </author> <title> Organization and Environment. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1967. </year>
Reference-contexts: In human organizations, environmental factors such as dynamism and task uncertainty have a strong effect on what coordinated actions are and how organizationally acceptable outcomes arise <ref> [18, 9, 33] </ref>. These effects have been observed in purely computational organizations as well [8, 7, 6, 23, 26]. Achieving effective coordination in a multi-agent system (MAS) is a difficult problem for a number of reasons.
Reference: [19] <author> V. R. Lesser and L. D. Erman. </author> <title> Distributed interpretation: A model and an experiment. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(12):1144-1163, </volume> <month> December </month> <year> 1980. </year> <month> 20 </month>
Reference-contexts: However, as mentioned previously, an agent cannot utilize the entire global problem solving state even if other agents are willing to communicate all the necessary information because of the limitations on its computational resources, representational heterogeneity 7 and phenomenon like distraction <ref> [19] </ref> 8 . This leads us to the importance of communicating only relevant information at suitable abstractions to other agents. We call such meta-level information a situation. An agent thus needs to associate appropriate views of the global situation with the knowledge learned about effective control decisions. <p> if that agent provided all the details about its internal state (similarity metric used, most similar case retrieved and adapted etc.). 8 The phenomenon of distraction arises when incorrect or irrelevant information provided by another agent with weak constraint knowledge could lead the receiving agent to explore along unproductive directions <ref> [19] </ref>. 18 gies are complex. Some of the work in multi-agent robotic systems [22] also deals with realistic and complex systems but that work is primarily concerned with homogeneous agents. The work in this paper deals with heterogeneous agents.
Reference: [20] <author> Victor R. Lesser. </author> <title> A retrospective view of FA/C distributed problem solving. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(6) </volume> <pages> 1347-1362, </pages> <year> 1991. </year>
Reference-contexts: The ideas presented here are relevant to the more general problem of learning cooperative control in a multi-agent system. Effective cooperation in a multi-agent system requires that the global problem-solving state influence the local control decisions made by an agent. We call such an influence cooperative control <ref> [20] </ref>. Coordination strategies are a form of cooperative control. An agent with a purely local view of the problem solving state cannot learn to make effective problem solving control decisions about coordination because these decision may have global implications.
Reference: [21] <author> Thomas Malone and Kevin Crowston. </author> <title> Toward an interdisciplinary theory of coordination. </title> <type> Center for Coordination Science Technical Report 120, </type> <institution> MIT Sloan School of Management, </institution> <year> 1991. </year>
Reference-contexts: 1 Introduction Coordination is the process of effectively managing interdependencies between activities distributed across agents so as to derive maximum benefit from them <ref> [21, 6] </ref>. Based on structure and uncertainty in their environment, agents have to choose and temporally order their activities to mitigate the effects of harmful interdependencies and exploit the beneficial interdependencies among them.
Reference: [22] <author> M. J. Mataric. </author> <title> Learning to behave socially. </title> <booktitle> In Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB-94), </booktitle> <year> 1994. </year>
Reference-contexts: Some of the work in multi-agent robotic systems <ref> [22] </ref> also deals with realistic and complex systems but that work is primarily concerned with homogeneous agents. The work in this paper deals with heterogeneous agents. They need to speak a common language to the extent of being able to understand TMS representations.
Reference: [23] <author> M. V. Nagendra Prasad, Keith S. Decker, Alan Garvey, and Victor R. Lesser. </author> <title> Exploring Organizational Designs with TAEMS: A Case Study of Distributed Data Processing. </title> <booktitle> In Proceedings of the Second International Conference on Multi-Agent Systems, </booktitle> <address> Kyoto, Japan, </address> <month> December </month> <year> 1996. </year> <note> AAAI Press. </note>
Reference-contexts: In human organizations, environmental factors such as dynamism and task uncertainty have a strong effect on what coordinated actions are and how organizationally acceptable outcomes arise [18, 9, 33]. These effects have been observed in purely computational organizations as well <ref> [8, 7, 6, 23, 26] </ref>. Achieving effective coordination in a multi-agent system (MAS) is a difficult problem for a number of reasons. <p> In certain situations, coordination protocols that permit some level of non-coherent activity and avoid the additional overhead for coordination may lead to better performance <ref> [7, 6, 23, 34] </ref>. For example, when the agents are under severe time pressure and the load of the activities at the agents is high, sophisticated agent coordination strategies do not generally payoff. <p> Experimental results have already verified that for some environments a subset of the mechanisms is more effective than using the entire set of mechanisms <ref> [6, 23] </ref>. In this work, we present a learning extension, called COLLAGE, that endows the agents with the capability to choose a suitable subset of the coordination mechanisms based on the present problem solving situation, instead of having a fixed subset across all problem instances 4 in an environment. <p> Rough commitments are a form of tacit social contract between agents about the completion times of their tasks. It might be possible to view rough commitments as precompiled social laws [30] 1 . The latter two coordination strategies are the alternatives normally used in the distributed data processing domain <ref> [23] </ref>. Decker and Lesser [6] proposed balanced as a sophisticated strategy that exploits a number of mechanisms to achieve coordination. In our experiments, agents learn to choose among these three coordination strategies in the domain of distributed data processing. <p> Such a graph-grammar-based task structure specification language is powerful enough to model the topological relationships occurring in task structures representing many real life applications. For the experiments below, we use this tool to model a distributed data processing domain as in <ref> [23] </ref>. We now briefly introduce this domain and refer the interested reader to [23] for further details. The distributed data processing domain consists of a number of geographically dispersed data processing centers (agents). <p> For the experiments below, we use this tool to model a distributed data processing domain as in <ref> [23] </ref>. We now briefly introduce this domain and refer the interested reader to [23] for further details. The distributed data processing domain consists of a number of geographically dispersed data processing centers (agents). <p> The processing centers have limited resources to conduct their analysis on the incoming data and they have to do this within certain deadlines. Results of processing data at a center may need to be communicated to other centers due to the interrelationships between the tasks at these centers. In <ref> [23] </ref>, we give details of how a stochastic graph-grammar can model task structures arising in a domain such as this. In the same work, we also present the results of empirical explorations of the effects of varying deadlines and crisis task group arrival probability. <p> type chosen in the 100 test runs for Mode 1 learning when crisis task group probability was 1.0 5 . 5.2 Experiments with Synthetic domains In order to test COLLAGE on interesting scenarios with a range of characteristics, we created a number of synthetic domain theories using graph grammar formalisms <ref> [23] </ref>. A synthetic grammar represents a domain theory that is not grounded in any real life application. We tested COLLAGE on three different synthetic environments generated by their corresponding grammars.
Reference: [24] <author> M. V. Nagendra Prasad and V. R. Lesser. </author> <title> Learning Situation-Specific Coordination in Gener alized Partial Global Planning. </title> <booktitle> In 1996 AAAI Spring Symposium on Adaptation, Co-evolution and Learning in Multi-agent Systems, </booktitle> <address> Stanford, CA, 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The utility of a is defined as U ( p new ; a) = jM a j hq;a;ri2M a 5 Experiments 5.1 Experiments in the DDP domain Our earlier work on learning coordination <ref> [24] </ref> relied on a weak task environment generator where the task structures were generated randomly. <p> This often gives rise to a wide range of task structures and a huge variance in the types of capabilities needed by the system to effectively handle them. Accordingly, the learning algorithms showed at best modest gains in performance <ref> [24] </ref>. More importantly, it is unlikely that most real applications involve an infinite variety of task structures. The domain semantics dictate and limit morphology of the task structures.
Reference: [25] <author> M. V. Nagendra Prasad, V. R. Lesser, and S. E. Lander. </author> <title> Cooperative learning over com posite search spaces: Experiences with a multi-agent design system. </title> <booktitle> In Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1996. </year> <note> AAAI Press. </note>
Reference-contexts: Motivation for this work is different from ours. In their work, agents exploit their past experience to reduce the control effort needed to achieve coordination in future problem instances. Similar use of past experience has been studied in Nagendra Prasad, Lander and Lesser <ref> [25] </ref>. Certain multi-agent learning systems in the literature deal with a different task from that presented in this paper. Systems like ILS [32] and MALE [31] use multi-agent techniques to build hybrid learners from multiple learning agents. <p> We call this form of knowledge situation-specific control. We have looked at the relevance of the theory of situation-specific learning in the context of learning other forms of problem solving control in cooperative multi-agent systems <ref> [25, 26] </ref>. Moreover, COLLAGE demonstrates the utility and viability of learning coordination in complex, realistic multi-agent systems.
Reference: [26] <author> M. V. Nagendra Prasad, V. R. Lesser, and S. E. Lander. </author> <title> Learning organizational roles in a heterogeneous multi-agent system. </title> <booktitle> In Proceedings of the Second International Conference on Multi-Agent Systems, </booktitle> <address> Kyoto, Japan, </address> <month> December </month> <year> 1996. </year> <note> AAAI Press. </note>
Reference-contexts: In human organizations, environmental factors such as dynamism and task uncertainty have a strong effect on what coordinated actions are and how organizationally acceptable outcomes arise [18, 9, 33]. These effects have been observed in purely computational organizations as well <ref> [8, 7, 6, 23, 26] </ref>. Achieving effective coordination in a multi-agent system (MAS) is a difficult problem for a number of reasons. <p> Our previous work explored learning situation-specific organizational roles in a heterogeneous multi-agent system <ref> [26] </ref>. An organizational role represents a set of tasks an agent can perform on a composite solution. Agent use abstractions of problem solving state to learn to play the roles they are best suited for in a multi-agent search process for cooperatively constructing an overall 3 solution. <p> We call this form of knowledge situation-specific control. We have looked at the relevance of the theory of situation-specific learning in the context of learning other forms of problem solving control in cooperative multi-agent systems <ref> [25, 26] </ref>. Moreover, COLLAGE demonstrates the utility and viability of learning coordination in complex, realistic multi-agent systems.
Reference: [27] <author> J. S. Rosenschein and G. Zlotkin. </author> <title> Designing conventions for automated negotition. </title> <journal> AI Magazine, </journal> <pages> pages 29-46, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: TMS can model aspects of coordination in complex worth-oriented domains <ref> [27] </ref> where states have functions that rate their acceptability (not necessarily a binary rating). There are deadlines associated with the tasks and some of the subtasks may be interdependent, that is, cannot be solved independently in isolation.
Reference: [28] <author> T. Sandholm and R. Crites. </author> <title> Multi-agent reinforcement learning in the repeated prisoner's dilemma. </title> <note> to appear in Biosystems, </note> <year> 1995. </year>
Reference-contexts: The agents share perception information to overcome perceptual limitations or communicate policy functions learned through reinforcement learning. Grefenstette [14] uses genetic algorithms to learn reactive decision rules for agents in a predator-prey domain similar to that in [36]. Sandholm and Crites <ref> [28] </ref> study the emergence of cooperation in the Iterated Prisoner's Dilemma problem, where the agents are self-interested, and an agent is not free to ask for any kind of information from the other agents.
Reference: [29] <author> Sandip Sen, M. Sekaran, and J. Hale. </author> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 426-431, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1994. </year> <note> AAAI. </note>
Reference-contexts: We then present some of our experimental results and conclude. 2 Related Work Previous work related to learning in multi-agent systems is limited and much of this work relies on techniques derived from reinforcement learning [1, 35], genetic algorithms [16] and classifier systems [17]. In Sen, Sekaran and Hale <ref> [29] </ref> the agents use reinforcement learning to evolve complimentary policies in a box pushing task, and in Crites and Barto [3] a team of reinforcement learning agents optimize elevator dispatching performance.
Reference: [30] <author> Y. Shoham and M. Tennenholtz. </author> <title> On the synthesis of useful social laws for artificial agent societies (preliminary report). </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 276-281, </pages> <address> San Jose, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Rough commitments are a form of tacit social contract between agents about the completion times of their tasks. It might be possible to view rough commitments as precompiled social laws <ref> [30] </ref> 1 . The latter two coordination strategies are the alternatives normally used in the distributed data processing domain [23]. Decker and Lesser [6] proposed balanced as a sophisticated strategy that exploits a number of mechanisms to achieve coordination.
Reference: [31] <author> S. S. Sian. </author> <title> Extending learning to multiple agents: issues and a model for multi-agent machine learning. </title> <booktitle> In Proceedings of Machine Learning - EWSL 91, </booktitle> <pages> pages 440-456, </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Similar use of past experience has been studied in Nagendra Prasad, Lander and Lesser [25]. Certain multi-agent learning systems in the literature deal with a different task from that presented in this paper. Systems like ILS [32] and MALE <ref> [31] </ref> use multi-agent techniques to build hybrid learners from multiple learning agents.
Reference: [32] <author> B. Silver, W. Frawely, G. Iba, J. Vittal, and K. Bradford. </author> <title> A framework for multi-paradigmatic learning. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 348-358, </pages> <year> 1990. </year>
Reference-contexts: Similar use of past experience has been studied in Nagendra Prasad, Lander and Lesser [25]. Certain multi-agent learning systems in the literature deal with a different task from that presented in this paper. Systems like ILS <ref> [32] </ref> and MALE [31] use multi-agent techniques to build hybrid learners from multiple learning agents.
Reference: [33] <author> Arthur L. Stinchcombe. </author> <title> Information and Organizations. </title> <publisher> University of California Press, </publisher> <address> Berkeley, CA, </address> <year> 1990. </year>
Reference-contexts: In human organizations, environmental factors such as dynamism and task uncertainty have a strong effect on what coordinated actions are and how organizationally acceptable outcomes arise <ref> [18, 9, 33] </ref>. These effects have been observed in purely computational organizations as well [8, 7, 6, 23, 26]. Achieving effective coordination in a multi-agent system (MAS) is a difficult problem for a number of reasons.
Reference: [34] <author> T. Sugawara and V. R. Lesser. </author> <title> On-line learning of coordination plans. </title> <booktitle> In Proceedings of the Twelfth International Workshop on Distributed AI, </booktitle> <address> Hidden Valley, Pa, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In certain situations, coordination protocols that permit some level of non-coherent activity and avoid the additional overhead for coordination may lead to better performance <ref> [7, 6, 23, 34] </ref>. For example, when the agents are under severe time pressure and the load of the activities at the agents is high, sophisticated agent coordination strategies do not generally payoff. <p> The system was tested in a parametric design domain and the learning agents produced designs that, on an average, were better than those produced by a system with agents playing roles hand-coded by a human expert. Sugawara and Lesser <ref> [34] </ref> also recognize the need for situation specificity in learning coordination, though they do have the notion of two-phase coordination. They are concerned with learning to make the situations more discriminating to avoid using an inappropriate coordination strategy in the domain of distributed network diagnosis. <p> As the number of coordination alternatives become large in number, the learning phase could become computationally very intensive and the instance-base size could increase enormously with respect to Mode 2. We are looking at how to integrate methods for progressively refining situation vectors such as those in <ref> [34] </ref>, ways to organize the instance-base to access and detect regions where there is insufficient learning and also ways to do more directed experimentation during learning rather than randomly sampling the problem space. In COLLAGE, all the agents form identical instance-bases.
Reference: [35] <author> R. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: We then present some of our experimental results and conclude. 2 Related Work Previous work related to learning in multi-agent systems is limited and much of this work relies on techniques derived from reinforcement learning <ref> [1, 35] </ref>, genetic algorithms [16] and classifier systems [17]. In Sen, Sekaran and Hale [29] the agents use reinforcement learning to evolve complimentary policies in a box pushing task, and in Crites and Barto [3] a team of reinforcement learning agents optimize elevator dispatching performance.
Reference: [36] <author> M. Tan. </author> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Pro ceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 330-337, </pages> <year> 1993. </year>
Reference-contexts: Weiss [37] uses a variant of Holland's [17] bucket brigade algorithm for learning hierarchical organization structuring relationships in a block world domain via strengthening promising chains of actions through bid-reward cycles. Tan <ref> [36] </ref> deals with a predator-prey domain in a grid world. The agents share perception information to overcome perceptual limitations or communicate policy functions learned through reinforcement learning. Grefenstette [14] uses genetic algorithms to learn reactive decision rules for agents in a predator-prey domain similar to that in [36]. <p> Tan <ref> [36] </ref> deals with a predator-prey domain in a grid world. The agents share perception information to overcome perceptual limitations or communicate policy functions learned through reinforcement learning. Grefenstette [14] uses genetic algorithms to learn reactive decision rules for agents in a predator-prey domain similar to that in [36]. Sandholm and Crites [28] study the emergence of cooperation in the Iterated Prisoner's Dilemma problem, where the agents are self-interested, and an agent is not free to ask for any kind of information from the other agents. <p> Haynes and Sen [15] present studies in learning cases to resolve conflicts among agents in a predator-prey domain similar to that used by Tan <ref> [36] </ref>. There is no communication between the agents, and the cases result from the agent's perception of the problem solving state. <p> Agents sharing perceptual information as in Tan <ref> [36] </ref> and Greffenstette [14] or bidding information as in Weiss [37] do not make explicit the notion of situating the local control knowledge in a more global, abstract situation. <p> The information shared is weak, and the studies were conducted in domains such as predator-prey <ref> [36, 14] </ref> or blocks world [37] where the need for sharing meta-level information and situating learning in this information is not apparent. Our previous work explored learning situation-specific organizational roles in a heterogeneous multi-agent system [26].
Reference: [37] <author> G. Weiss. </author> <title> Some studies in distributed machine learning and organizational design. </title> <type> Technical Report FKI-189-94, </type> <institution> Institut fur Informatik, TU Munchen, </institution> <year> 1994. </year> <month> 22 </month>
Reference-contexts: In both these works, the agents do not communicate with one another and any agent treats the other agents as a part of the environment. Weiss <ref> [37] </ref> uses a variant of Holland's [17] bucket brigade algorithm for learning hierarchical organization structuring relationships in a block world domain via strengthening promising chains of actions through bid-reward cycles. Tan [36] deals with a predator-prey domain in a grid world. <p> Agents sharing perceptual information as in Tan [36] and Greffenstette [14] or bidding information as in Weiss <ref> [37] </ref> do not make explicit the notion of situating the local control knowledge in a more global, abstract situation. The information shared is weak, and the studies were conducted in domains such as predator-prey [36, 14] or blocks world [37] where the need for sharing meta-level information and situating learning in <p> Tan [36] and Greffenstette [14] or bidding information as in Weiss <ref> [37] </ref> do not make explicit the notion of situating the local control knowledge in a more global, abstract situation. The information shared is weak, and the studies were conducted in domains such as predator-prey [36, 14] or blocks world [37] where the need for sharing meta-level information and situating learning in this information is not apparent. Our previous work explored learning situation-specific organizational roles in a heterogeneous multi-agent system [26]. An organizational role represents a set of tasks an agent can perform on a composite solution.
References-found: 37

