URL: http://www.speech.sri.com/people/konig/papers/icassp96_preprint.ps
Refering-URL: http://www.speech.sri.com/people/konig/resume.html
Root-URL: 
Email: fkonig,bourlard,morgang@icsi.berkeley.edu  
Title: REMAP EXPERIMENTS WITH SPEECH RECOGNITION  
Author: Yochai Konig yz Herve Bourlard yfl Nelson Morgan zy 
Address: Mons, Mons, Belgium  
Affiliation: International Computer Science Institute, Berkeley, CA EECS Department, University of California at Berkeley, Berkeley, CA Faculte Polytechnique de  
Abstract: In this report we present experimental and theoretical results using a framework for training and modeling continuous speech recognition systems based on the theoretically optimal Maximum a Posteriori (MAP) criterion. This is in constrast to most state-of-the-art systems which are trained according to a Maximum Likelihood (ML) criterion. Although the algorithm is quite general, we applied it to a particular form of hybrid system combining Hidden Markov Models (HMMs) and Artificial Neural Networks (ANNs) in which the ANN targets and weights are iteratively re-estimated to guarantee the increase of the posterior probability of the correct model, hence actually minimizing the error rate. More specifically, this training approach is applied to a transition-based model that uses local conditional transition probabilities (i.e., the posterior probability of the current state given the current acoustic vector and the previous state) to estimate the posterior probabilities of sentences. Experimental results on isolated and continuous speech recognition tasks show an increase in the estimates of posterior probabilities of the correct sentences after training, and significant decreases in error rates in comparison to a baseline system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley and Sons, Inc, </publisher> <year> 1973. </year>
Reference-contexts: 1. INTRODUCTION 1.1. Maximum a Posteriori (MAP) Framework In statistical pattern classification, it is known that the system leading to the minimum probability of error is the one that is trained to maximize the a posteriori probability of the correct class conditioned on the evidence <ref> [1] </ref> and uses that same criterion during recognition. <p> fi t and, consequently, estimates of P (q ` n jx n ; q k n1 ; fi t ), how can we determine new ANN targets that: 1. will be smooth estimates of conditional transition prob abilities, 8 possible (k; `) state transition pairs in M and 8n 2 <ref> [1; n] </ref>. 2. when used in training the ANN for iteration t + 1, will lead to new estimates of fi t+1 and P (q ` n jx n ; q k that are guaranteed to incrementally increase (2)? 2 It could be argued that these models are no longer HMMs <p> Compute ANN targets P (q ` n jX; q k n1 ; fi t ; M ) according to (11), 8 possible (k; `) state transition pairs in M and 8n 2 <ref> [1; n] </ref>. 2. For all x n 's in X, train the ANN to minimize the rela tive entropy between the outputs and targets. This provides us with a new set of parameters fi t , for t = t + 1. 3. Iterate from 1 until convergence.
Reference: [2] <author> H. Bourlard and N. Morgan. </author> <title> Connectionist Speech Recognition A Hybrid Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference-contexts: In particular, we have shown that fairly simple layered structures can be used to estimate local emission probabilities for HMMs <ref> [2] </ref>. This approach is now usually referred to as a hybrid HMM/ANN system. <p> Thus, it is a true recognition model, i.e., it directly maps from acoustic sequences to sentences, unlike Hidden Markov Models (HMMs) that model the inverse modeling (the likelihood of producing an acoustic sequence). In <ref> [2] </ref> it was shown that it is possible to compute the global posterior probability P (M jX; L; fi) of (1) and (2) as: X P (M; j jX; L; fi) (7) X P (M j j ; X; L; fi)P ( j jX; L; fi) in which "8 j " <p> The combined results for all the four cuts are summarized in Table 1. Note that the row entitled "Baseline Hybrid" refers to an ANN trained on targets of 1's and 0's that have been obtained from a forced Viterbi procedure by our standard HMM/ANN system as described in <ref> [2] </ref>; the row entitled "DHMM, pre-REMAP" means a Discriminant HMM using the same training approach, with hard targets determined by the first system, and additional inputs to represent the previous state. The rightmost column gives the average probability of the correct model over all test words as determined during recognition.
Reference: [3] <author> J. M. Steeneken and D. H. Van Leeuwen. </author> <title> Multi lingual assessment of speaker independent large vocabulary speech-recognition systems: the sqale project (speech recognition quality assessment for language engineering). </title> <booktitle> In Proceedings European Conf. on Speech Communication and Technology. (EUROSPEECH), </booktitle> <address> Madrid, Spain, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: A number of speech recognition systems based on this latter approach have been proved, in controlled tests, to be to be both effective in terms of accuracy (comparable or better than equivalent state-of-the-art systems) and efficient in terms of CPU and memory run-time requirements <ref> [3] </ref>. 1.4. Posterior-based Hybrid HMM/ANN In [4], we presented a new hybrid HMM/ANN approach that directly optimizes the acoustic parameter set fi according to the MAP criterion, i.e., maximizing P (M jX; fi) where M is the correct HMM associated with X.
Reference: [4] <author> H. Bourlard, Y. Konig, and N. Morgan. </author> <title> REMAP: Re cursive estimation and maximization of a posteriori probabilities, application to transition-based connectionist speech recognition. </title> <type> Technical Report TR-94-064, </type> <institution> International Computer Science Institute, </institution> <address> Berke-ley, CA, </address> <year> 1994. </year>
Reference-contexts: Posterior-based Hybrid HMM/ANN In <ref> [4] </ref>, we presented a new hybrid HMM/ANN approach that directly optimizes the acoustic parameter set fi according to the MAP criterion, i.e., maximizing P (M jX; fi) where M is the correct HMM associated with X. <p> This algorithm, which we call REMAP (Recursive Estimation and maximization of A Posteriori Probabilities), iteratively re-estimates ANN targets and weights to guarantee an increase of the posterior probability of the correct sequence. We show in <ref> [4] </ref> that estimation of the new ANN targets can be done using "forward" and "backward" recurrences that are reminiscent of the EM-based Forward-Backward training algorithm of standard HMMs. In [5] we reported initial experimental results on an isolated word recognition task. <p> Considering the second factor of (8) as the acoustic model and assuming that it is independent of the language model parameters (coupled with other standard assumptions <ref> [4] </ref>), we can then rewrite it as: P ( j jX; fi) = n=1 The first factor in (8) can be considered independent of the acoustic sequence X (since the state sequence is assumed) and will be further discussed later in this paper. <p> In <ref> [4] </ref>, we prove that a re-estimation of ANN targets that guarantee convergence to a local maximum of (2) is given by 4 : n jx n ; q k n jX; q k which means that the new ANN target associated with x n and a specific transition q k ! <p> Roughly speaking, our global optimization goal (2) is realized through the estimation of the targets (that correspond to a "local" acoustic window, e.g., 100 ms) by using the whole utterance. In <ref> [4] </ref>, we further prove that alternating ANN target estimation (the "estimation" step) and ANN training (the "maximization" step) is guaranteed to incrementally increase (2) over t. <p> Iterate from 1 until convergence. This procedure is thus composed of two steps: an Estimation (E) step, corresponding to step 1 above, and a Maximization (M) step, corresponding to step 2. Convergence of this training scheme can be proved <ref> [4] </ref>. In this regard, it is reminiscent of the EM algorithm [8]. However, in the standard EM algorithm, the M step involves the actual maximization of the likelihood function. <p> P (M jL; fi) can be assumed independent of the acoustic model parameters and can be estimated using standard language modeling techniques. In principle P (jM; L; fi) and P (jL; fi) can be estimated during training by dynamic programming techniques similar to our ff and fi recurrences <ref> [4] </ref>, and the ratio of these two terms represents the additional state transition information that is gained by knowing the specific word sequence. 6. CONCLUSIONS AND FUTURE WORK We presented a discriminant training algorithm for hybrid HMM/ANN systems based on a global MAP criterion.
Reference: [5] <author> H. Bourlard, Y. Konig, and N. Morgan. </author> <title> REMAP: Recursive estimation and maximization of a posteriori probabilities in connectionist speech recognition. </title> <booktitle> In Proceedings European Conf. on Speech Communication and Technology. (EUROSPEECH), </booktitle> <address> Madrid, Spain, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: We show in [4] that estimation of the new ANN targets can be done using "forward" and "backward" recurrences that are reminiscent of the EM-based Forward-Backward training algorithm of standard HMMs. In <ref> [5] </ref> we reported initial experimental results on an isolated word recognition task. Here, we report extended experimental results on the earlier task and on a small continuous speech recognition task (all the experiments were done using only acoustic information).
Reference: [6] <author> J. R. Glass. </author> <title> Finding Acoustic Regularities in Speech Applications to Phonetic Recognition. </title> <type> PhD thesis, </type> <institution> M.I.T, </institution> <month> May </month> <year> 1988. </year>
Reference-contexts: REMAP FOR DISCRIMINANT HMMS 3.1. Motivations Discriminant HMMs as described above use conditional transition probabilities as the key building block for acoustic recognition. It is, however, well known that estimating transitions accurately is a difficult problem <ref> [6] </ref>. In our previous hybrid systems, the targets used for ANN training are typically given by the best segmentation resulting from a Viterbi alignment. This procedure thus yields rigid transition targets, which may not be optimal in the case of training (and testing!) of conditional transition probabilities.
Reference: [7] <author> Y. Bengio, R. De Mori, G. Flammia, and R. Kompe. </author> <title> Global optimization of a neural network-hidden Markov model hybrid. </title> <journal> IEEE trans. on Neural Networks, </journal> <volume> 3(2) </volume> <pages> 252-258, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In the following derivation we omit the dependency on the language model L; this will be further discussed in Section 5. Although, in principle, we could use a generalized back-propagation-like gradient procedure in fi to maximize (2) (see, e.g., <ref> [7] </ref>), an EM-like algorithm should have better convergence properties, and would preserve the statistical interpretation of the ANN outputs.
Reference: [8] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maxi mum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 34 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: This procedure is thus composed of two steps: an Estimation (E) step, corresponding to step 1 above, and a Maximization (M) step, corresponding to step 2. Convergence of this training scheme can be proved [4]. In this regard, it is reminiscent of the EM algorithm <ref> [8] </ref>. However, in the standard EM algorithm, the M step involves the actual maximization of the likelihood function. In a related approach, 4 In the following, we consider only one particular training sequence X associated with one particular model M .
Reference: [9] <author> Y. Bengio and P. Frasconi. </author> <title> An input output HMM ar chitecture. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7. </volume> <publisher> MIT press, </publisher> <address> Cambridge, </address> <year> 1995. </year>
Reference-contexts: Similarly, REMAP increases the global posterior function during the M step (in the direction of targets that actually maximize that global function), rather than actually maximizing it. Recently, a similar approach was suggested for mapping input sequences to output sequences <ref> [9] </ref>. 4. EXPERIMENTS AND RESULTS We report on experiments with isolated and continuous speech, where recognition was based on acoustic information.
Reference: [10] <author> R.A. Cole, M. Fanty, and T. Lander. </author> <title> Telephone speech corpus development at CSLU. </title> <booktitle> In Proceedings In-t'l Conference on Spoken Language Processing, </booktitle> <address> Yoko-hama, Japan, </address> <month> September </month> <year> 1994. </year> <month> 4 </month>
Reference-contexts: For this purpose we chose the Numbers'93 corpus. It is a continuous-speech database collected by CSLU at the Ore-gon Graduate Institute. It consists of numbers spoken naturally over telephone lines on the public-switched network <ref> [10] </ref>. The Numbers'93 database consists of 2167 speech files of spoken numbers produced by 1132 callers. We used 877 of these utterances for training and 657 for cross-validation and testing (200 for cross-validation) saving the remaining utterances for final testing purposes.
References-found: 10

