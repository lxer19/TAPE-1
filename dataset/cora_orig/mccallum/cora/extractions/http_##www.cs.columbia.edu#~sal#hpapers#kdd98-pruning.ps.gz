URL: http://www.cs.columbia.edu/~sal/hpapers/kdd98-pruning.ps.gz
Refering-URL: http://www.cs.columbia.edu:80/~sal/recent-papers.html
Root-URL: 
Email: fandreas,salg@cs.columbia.edu  pkc@cs.fit.edu  
Title: Pruning Classifiers in a Distributed Meta-Learning System  
Author: Andreas L. Prodromidis Salvatore Stolfo Philip K. Chan 
Note: This research is supported by the Intrusion Detection Program (BAA9603) from DARPA (F30602-96-1-0311), NSF (IRI-96-32225 and CDA-96-25374) and NYSSTF (423115-445). Supported in part by IBM  
Date: March 16, 1998  
Address: 1214 Amsterdam Ave. Mail Code 0401 New York, NY 10027  Melbourne, FL 32901  
Affiliation: Department of Computer Science Columbia University  Computer Science Florida Institute of Technology  
Abstract: JAM is a powerful and portable agent-based distributed data mining system that employs meta-learning techniques to integrate a number of independent classifiers (concepts) derived in parallel from independent and (possibly) inherently distributed databases. Although meta-learning promotes scalability and accuracy in a simple and straightforward manner, brute force meta-learning techniques can result in large, inefficient and some times inaccurate meta-classifier hierarchies. In this paper we explore several techniques for evaluating classifiers and we demonstrate that meta-learning combined with certain pruning methods can achieve similar or even better performance results in a much more cost effective manner. Keywords: classifier evaluation, pruning, metrics, distributed mining, meta-learning. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Ali and M. Pazzani. </author> <title> Error reduction through learning multiple descriptions. </title> <booktitle> Machine Learning, </booktitle> <address> 24:173202, </address> <year> 1996. </year>
Reference-contexts: In this work, we focus on the diversity and specialty metrics. Apart from accuracy and these metrics, correlation error and coverage have also been used to analyze and explain the properties and performance of classifiers. Ali and Pazzani <ref> [1] </ref> define as correlation error the fraction of instances for which a pair of base classifiers make the same incorrect predictions and Brodley and Lane [3] measured coverage by computing the fraction of instances for which at least one of the base classifiers produces the correct prediction. 2.1 Diversity Brodley [4] <p> of the base classifiers. (When the predictions of the classifiers are distributed evenly across the possible classes, the entropy is higher and the set of classifiers more diverse.) Kwok and Carter [14] correlate the error rates of a set of decision trees to their syntactical diversity, while Ali and Paz-zani <ref> [1] </ref> studied the impact of the number of gain ties 2 on the accuracy of an ensemble of classifiers.
Reference: [2] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: ID3, its successor C4.5, and Cart <ref> [2] </ref> are decision tree based algorithms, Bayes, described in [10], is a naive Bayesian classifier and Ripper [8] is a rule induction algorithm.
Reference: [3] <author> C. Brodley and T. Lane. </author> <title> Creating and exploiting coverage and diversity. In Work. </title> <booktitle> Notes AAAI-96 Workshop Integrating Multiple Learned Models, </booktitle> <pages> pages 814, </pages> <year> 1996. </year>
Reference-contexts: Ali and Pazzani [1] define as correlation error the fraction of instances for which a pair of base classifiers make the same incorrect predictions and Brodley and Lane <ref> [3] </ref> measured coverage by computing the fraction of instances for which at least one of the base classifiers produces the correct prediction. 2.1 Diversity Brodley [4] defines diversity by measuring the classification overlap of a pair of classifiers, i.e. the percentage of the instances classified the same way by two classifiers
Reference: [4] <author> C.Brodley. </author> <title> Addressing the selective superiority problem: Automatic algorithm/model class selection. </title> <booktitle> In Proc. 10th Intl. Conf. Machine Learning, </booktitle> <pages> pages 17 24. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: [1] define as correlation error the fraction of instances for which a pair of base classifiers make the same incorrect predictions and Brodley and Lane [3] measured coverage by computing the fraction of instances for which at least one of the base classifiers produces the correct prediction. 2.1 Diversity Brodley <ref> [4] </ref> defines diversity by measuring the classification overlap of a pair of classifiers, i.e. the percentage of the instances classified the same way by two classifiers while Chan [5] associates it with the entropy in the predictions of the base classifiers. (When the predictions of the classifiers are distributed evenly across
Reference: [5] <author> P. Chan. </author> <title> An Extensible Meta-Learning Approach for Scalable and Accurate Inductive Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY, </address> <year> 1996. </year>
Reference-contexts: by computing the fraction of instances for which at least one of the base classifiers produces the correct prediction. 2.1 Diversity Brodley [4] defines diversity by measuring the classification overlap of a pair of classifiers, i.e. the percentage of the instances classified the same way by two classifiers while Chan <ref> [5] </ref> associates it with the entropy in the predictions of the base classifiers. (When the predictions of the classifiers are distributed evenly across the possible classes, the entropy is higher and the set of classifiers more diverse.) Kwok and Carter [14] correlate the error rates of a set of decision trees
Reference: [6] <author> P. Chan and S. Stolfo. </author> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Work. Multistrategy Learning, </booktitle> <pages> pages 150165, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Meta-learning <ref> [6] </ref> is a technique that addresses the scaling problem of machine learning, i.e. the problem of learning useful information from large and inherently distributed databases.
Reference: [7] <author> P. Chan and S. Stolfo. </author> <title> Sharing learned models among remote database partitions by local meta-learning. </title> <booktitle> In Proc. Second Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 27, </pages> <year> 1996. </year>
Reference-contexts: Retaining a large number of base classifiers and meta-classifiers may not be feasible. Meta classifiers are defined recursively as collections of classifiers structured in multi-level trees <ref> [7] </ref> and determining the optimal set of classifiers is a combinatorial problem. Pre-training pruning 1 refers to the filtering of the classifiers before they are used in the training of a meta-classifier.
Reference: [8] <author> W. Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Proc. 12th Intl. Conf. Machine Learning, </booktitle> <pages> pages 115123, </pages> <year> 1995. </year>
Reference-contexts: ID3, its successor C4.5, and Cart [2] are decision tree based algorithms, Bayes, described in [10], is a naive Bayesian classifier and Ripper <ref> [8] </ref> is a rule induction algorithm. Learning tasks Two data sets of real credit card transactions were used in our experiments provided by the Chase and First Union Banks, members of the FSTC (Financial Services Technology Consortium). These two data sets contain credit card transactions labeled as fraudulent or legitimate.
Reference: [9] <author> T.G. Dietterich. </author> <title> Machine learning research: Four current directions. </title> <journal> AI Magazine, </journal> <volume> 18(4):97136, </volume> <year> 1997. </year>
Reference-contexts: The focus of this paper, however, is on evaluation methods that are suitable for multiple class problems and on metrics that provide information about the interdependencies among the base classifiers and their potential when forming ensembles of classifiers <ref> [9, 13] </ref>. Margineantu and Dietterich [15] acknowledged the importance of pruning ensembles of classifiers and studied the problem of evaluating and pruning the set of hypothesis (classifiers) obtained by the boosting algorithm ADABOOST [11].
Reference: [10] <author> C. </author> <type> Elkan. </type> <institution> Boosting and naive bayesian learning [http://www-cse.ucsd.edu/~elkan/papers/bnb.ps]. Department of Computer Science and Engineering, Univ. of California, </institution> <address> San Diego, CA, </address> <year> 1997. </year>
Reference-contexts: ID3, its successor C4.5, and Cart [2] are decision tree based algorithms, Bayes, described in <ref> [10] </ref>, is a naive Bayesian classifier and Ripper [8] is a rule induction algorithm. Learning tasks Two data sets of real credit card transactions were used in our experiments provided by the Chase and First Union Banks, members of the FSTC (Financial Services Technology Consortium).
Reference: [11] <author> Y. Freund and R. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Proc. Thirteenth Conf. Machine Learning, </booktitle> <pages> pages 148156, </pages> <year> 1996. </year>
Reference-contexts: Margineantu and Dietterich [15] acknowledged the importance of pruning ensembles of classifiers and studied the problem of evaluating and pruning the set of hypothesis (classifiers) obtained by the boosting algorithm ADABOOST <ref> [11] </ref>. According to their findings, by examining the diversity and accuracy of the available classifiers, it is possible for a subset of classifiers to achieve similar levels of performance as the entire set.
Reference: [12] <author> J. Furnkranz and G. </author> <title> Widmer. Incremental reduced error pruning. </title> <booktitle> In Proc. 11th Intl. Conf. Mach. Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference: [13] <author> L. Hansen and P. Salamon. </author> <title> Neural network ensembles. </title> <journal> IEEE Trans. Pattern Analysis and Mach. Itell., </journal> <volume> 12:9931001, </volume> <year> 1990. </year>
Reference-contexts: The focus of this paper, however, is on evaluation methods that are suitable for multiple class problems and on metrics that provide information about the interdependencies among the base classifiers and their potential when forming ensembles of classifiers <ref> [9, 13] </ref>. Margineantu and Dietterich [15] acknowledged the importance of pruning ensembles of classifiers and studied the problem of evaluating and pruning the set of hypothesis (classifiers) obtained by the boosting algorithm ADABOOST [11].
Reference: [14] <author> S. Kwok and C. Carter. </author> <title> Multiple decision trees. </title> <booktitle> In Uncertainty in Aritificial Intelligence 4, </booktitle> <pages> pages 327335, </pages> <year> 1990. </year>
Reference-contexts: instances classified the same way by two classifiers while Chan [5] associates it with the entropy in the predictions of the base classifiers. (When the predictions of the classifiers are distributed evenly across the possible classes, the entropy is higher and the set of classifiers more diverse.) Kwok and Carter <ref> [14] </ref> correlate the error rates of a set of decision trees to their syntactical diversity, while Ali and Paz-zani [1] studied the impact of the number of gain ties 2 on the accuracy of an ensemble of classifiers.
Reference: [15] <author> D. Margineantu and T. Dietterich. </author> <title> Pruning adaptive boosting. </title> <booktitle> In Proc. Fourteenth Intl. Conf. Machine Learning, </booktitle> <pages> pages 211218, </pages> <year> 1997. </year>
Reference-contexts: The focus of this paper, however, is on evaluation methods that are suitable for multiple class problems and on metrics that provide information about the interdependencies among the base classifiers and their potential when forming ensembles of classifiers [9, 13]. Margineantu and Dietterich <ref> [15] </ref> acknowledged the importance of pruning ensembles of classifiers and studied the problem of evaluating and pruning the set of hypothesis (classifiers) obtained by the boosting algorithm ADABOOST [11].
Reference: [16] <author> T. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18:203226, </volume> <year> 1982. </year>
Reference-contexts: Furthermore, it improves accuracy by combining different learning systems each having different inductive bias (e.g representation, search heuristics, search space) <ref> [16] </ref>. By combining separately learned concepts, meta-learning is expected to derive a higher level learned model that explains a large database more accurately than any of the individual learners.
Reference: [17] <author> A. L. Prodromidis. </author> <title> On the management of distributed learning agents. </title> <type> Technical Report CUCS-032-97 (PhD Thesis proposal), </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY, </address> <year> 1997. </year>
Reference-contexts: Pre-training pruning 1 refers to the filtering of the classifiers before they are used in the training of a meta-classifier. Instead of treating classifiers as 1 As opposed to post-training pruning <ref> [17] </ref> which denotes the evaluation and revision/pruning of the meta classifier after it is computed. 1 transactions. sealed entities combined in a brute force manner, with pre-training pruning we introduce a pre-meta-learning stage for analyzing the available classifiers and qualifying them for inclusion in a meta-classifier.
Reference: [18] <author> F. Provost and T. Fawcett. </author> <title> Anayly-sis and visualization of classifier performance: Comparison under imprecise class and cost distributions. </title> <booktitle> In Proc. Third Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 4348, </pages> <year> 1997. </year> <month> 10 </month>
Reference-contexts: Before we present the metrics employed in this study, we summarize the previous and current research within the Machine Learning and KDD communities. Provost and Fawcett <ref> [18] </ref> introduced the ROC convex hull method for its intuitiveness and flexibility. The method evaluates models for binary classification problems, by mapping them onto a True Positive/False Negative plane and by allowing comparisons under different met-rics (true positive/false negative rates, accuracy, 2 cost, etc.).
Reference: [19] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <address> 1:81106, </address> <year> 1986. </year>
Reference: [20] <author> J. R. Quinlan. C4.5: </author> <title> programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference: [21] <author> S. Stolfo, W. Fan, W. Lee, A. Prodromidis, and P. Chan. </author> <title> Credit card fraud detection using meta-learning: Issues and initial results. </title> <booktitle> Working notes of AAAI Workshop on AI Approaches to Fraud Detection and Risk Management, </booktitle> <year> 1997. </year>
Reference-contexts: A different strategy that combines CCS rates and coverage would be to iteratively select classifiers based on their CCS score on examples which the preceding classifiers failed to cover. In another study <ref> [21] </ref> concerning credit card fraud detection we employ evaluation formulas for selecting classifiers that are based on characteristics such as diversity, coverage and correlated error or their combinations, i.e.
Reference: [22] <author> S. Stolfo, A. Prodromidis, S. Tselepis, W. Lee, W. Fan, and P. Chan. </author> <title> JAM: Java agents for meta-learning over distributed databases. </title> <booktitle> In Proc. 3rd Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 7481, </pages> <year> 1997. </year>
Reference-contexts: By combining separately learned concepts, meta-learning is expected to derive a higher level learned model that explains a large database more accurately than any of the individual learners. The J AM system <ref> [22] </ref> is designed to implement meta-learning, hence it can take full advantage of its inherent parallelism and distributed nature.
References-found: 22

