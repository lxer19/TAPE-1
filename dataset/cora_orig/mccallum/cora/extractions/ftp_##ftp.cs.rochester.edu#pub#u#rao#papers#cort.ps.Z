URL: ftp://ftp.cs.rochester.edu/pub/u/rao/papers/cort.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/rao/papers.html
Root-URL: 
Email: rao@cs.rochester.edu  
Title: Cortical Mechanisms of Visual Recognition and Learning: A Hierarchical Kalman Filter Model  
Author: Rajesh P. N. Rao 
Keyword: Recognition, Prediction, Learning, Internal Models, Visual Cortex, Kalman Filtering.  
Note: Submitted for publication, March 1997.  
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: We describe a biologically plausible model of dynamic recognition and learning in the visual cortex based on the statistical theory of Kalman filtering from optimal control theory. The model utilizes a hierarchical network whose successive levels implement Kalman filters operating over successively larger spatial and temporal scales. Each hierarchical level in the network predicts the current visual recognition state at a lower level and adapts its own recognition state using the residual error between the prediction and the actual lower-level state. Simultaneously, the network also learns an internal model of the spatiotemporal dynamics of the input stream by adapting the synaptic weights at each hierarchical level in order to minimize prediction errors. The Kalman filter model respects key neuroanatomical data such as the reciprocity of connections between visual cortical areas, and assigns specific computational roles to the inter-laminar connections known to exist between neurons in the visual cortex. Previous work elucidated the usefulness of this model in explaining neurophysiological phenomena such as endstopping and other related extra-classical receptive field effects. In this paper, in addition to providing a more detailed exposition of the model, we present a variety of experimental results demonstrating the ability of this model to perform robust spatiotemporal segmentation and recognition of objects and image sequences in the presence of varying amounts of occlusion, background clutter, and noise. 
Abstract-found: 1
Intro-found: 1
Reference: [ Abeles, 1991 ] <author> M. Abeles. Corticonics: </author> <title> Neural Circuits of the Cerebral Cortex. </title> <address> New York: </address> <publisher> Cam-bridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: The output o i of this neuron, given the pre-synaptic activity r, is the weighted sum: o i = j=1 This output can, for instance, be encoded as the spike rate of the neuron, although more complex schemes employing timing information may also be used <ref> [ Abeles, 1991 ] </ref> . The axonal output vector U r in this case forms the Kalman filter's prediction of the next expected input I at the lower level.
Reference: [ Albus, 1991 ] <author> J.S. Albus. </author> <title> Outline for a theory of intelligence. </title> <journal> IEEE Trans. Systems, Man, and Cybernetics, </journal> <volume> 21(3):473509, </volume> <year> 1991. </year>
Reference: [ Athans, 1974 ] <author> M. Athans. </author> <title> The importance of Kalman filtering methods for economic systems. </title> <journal> Annals of Economic and Social Measurement, </journal> <volume> 3:4964, </volume> <year> 1974. </year>
Reference-contexts: The new corrected estimate is then used to predict the next state, thereby completing one full cycle of filter operation. Since its discovery about three decades ago, the Kalman filter has been applied successfully to a wide range of problems in fields as diverse as economics <ref> [ Athans, 1974 ] </ref> and engineering [ Cipra, 1993 ] . Early applications of the filter were predominantly in aerospace engineering.
Reference: [ Ayache and Faugeras, 1986 ] <author> N. Ayache and O.D. Faugeras. </author> <title> HYPER: A new approach for the recognition and positioning of two-dimensional objects. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 8(1):4454, </volume> <year> 1986. </year>
Reference: [ Barlow, 1985 ] <author> H.B. Barlow. </author> <title> Cerebral cortex as model builder. </title> <booktitle> In Models of the Visual Cortex, </booktitle> <pages> pages 3746. </pages> <address> New York: </address> <publisher> John Wiley and Sons, </publisher> <year> 1985. </year>
Reference-contexts: is usually an appropriate function of the on-going needs of the task at hand, as we shall see in Section 7. 3 Internal World Models and the Estimation Problem There is a growing consensus among cognitive neuroscientists that the brain learns and maintains an internal model of the external world <ref> [ Barlow, 1985; 1994 ] </ref> , and that conscious experience involves 4 an active interaction between external sensory events and this internal modeling process [ Picton and Stuss, 1994 ] . <p> once the outliers were detected and discounted for within the first four or five frames of the image sequence. 9 Neural Implementation in the Visual Cortex The mammalian neocortex possesses a remarkable regularity in its neuroanatomical structure, as seen in the pattern of connections within and between different cortical areas <ref> [ Creutzfeldt, 1977; Barlow, 1985 ] </ref> . <p> Given that the cortex possesses roughly the same neuroanatomical input-output structure and pattern of connections across many different cortical areas <ref> [ Creutzfeldt, 1977; Barlow, 1985; Pandya et al., 1988 ] </ref> , a crucial test for any putative model of the cortex is whether it is general enough to be uniformly applicable to different cortical areas without regard to the specific input modality.
Reference: [ Barlow, 1994 ] <author> H. Barlow. </author> <title> What is the computational goal of the neocortex? In C. </title> <editor> Koch and J.L. Davis, editors, </editor> <booktitle> Large-Scale Neuronal Theories of the Brain, </booktitle> <pages> pages 122. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference: [ Bell and Sejnowski, 1996 ] <author> A.J. Bell and T.J. Sejnowski. </author> <title> The `independent components' of natural scenes are edge filters. </title> <note> Submitted to Vision Research, </note> <year> 1996. </year>
Reference-contexts: It also shares the 2 favorable properties of some recently proposed learning algorithms <ref> [ Olshausen and Field, 1996; Bell and Sejnowski, 1996 ] </ref> that have been shown to develop localized receptive fields similar to those of simple cells in the primary visual cortex from natural image inputs.
Reference: [ Blake and Yuille, 1992 ] <author> A. Blake and A. Yuille, </author> <title> editors. Active Vision. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Although Kalman filters have previously been used in computer vision (see, for example, <ref> [ Blake and Yuille, 1992 ] </ref> ), most of these applications have relied on hand-built dynamic models of restricted visual phenomena such as translating contours.
Reference: [ Bolz et al., 1989 ] <editor> J. Bolz, C.D. Gilbert, and T.N. Wiesel. </editor> <booktitle> Pharmacological analysis of cortical circuitry. Trends in Neurosciences, </booktitle> <address> 12(8):292296, </address> <year> 1989. </year>
Reference-contexts: The signal subsequently undergoes a normalization (corresponding to the matrix N in the Kalman filter). A plausible site for this normalization is layer 2+3 (composed of layers 2 and 3), given that this layer receives a substantial projection from layer 4 <ref> [ Gilbert and Wiesel, 1981; Bolz et al., 1989 ] </ref> . Normalization of responses has also been proposed by other authors to explain saturation of neural responses in V1 at high stimulus contrasts [ Heeger et al., 1996 ] . <p> The corrected estimate then generates the next state 41 prediction r (t + 1) via the synapses V . Neurons in layer 5 appear to be ideally located for such a computation, given that layer 2+3 cells project extensively into layer 5 <ref> [ Gilbert and Wiesel, 1981; Bolz et al., 1989 ] </ref> . Furthermore, layer 5 projects to layer 6, which is known to be a major source of cortico-thalamic feedback [ Gilbert and Wiesel, 1981 ] .
Reference: [ Broida and Chellappa, 1986 ] <author> T.J. Broida and R. Chellappa. </author> <title> Estimation of object motion parameters from noisy images. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 8(1):9099, </volume> <year> 1986. </year>
Reference: [ Bryson and Ho, 1975 ] <author> A.E. Bryson and Y.-C. Ho. </author> <title> Applied Optimal Control. </title> <address> New York: </address> <publisher> John Wiley and Sons, </publisher> <year> 1975. </year>
Reference-contexts: (t) is a Gaussian stochastic noise process with mean zero and a covariance matrix given by = E [nn T ] (E denotes the expectation 1 We shall be concerned here with the discrete Kalman filter, although similar results can be obtained for the continuous time version of the filter <ref> [ Bryson and Ho, 1975 ] </ref> . 5 general problem faced by an organism relying on an internal model of its environment (from [ O'Reilly, 1996 ] ). <p> The quadratic optimization function above is a special case of the more general weighted least squares criterion <ref> [ Bryson and Ho, 1975 ] </ref> : J = (I U r) T 1 (I U r) + (r r) T M 1 (r r) (11) This weighted least-squares criterion becomes meaningful when interpreted in terms of the stochastic model described in the previous section. <p> The Kalman filter estimate b r is in fact the mean of the Gaussian distribution of the state r after measurement of I <ref> [ Bryson and Ho, 1975 ] </ref> . The matrix N , which performs a form of divisive normalization, can likewise be shown to be the corresponding covariance matrix. Recall that r and M were the mean and covariance before measurement of I. <p> In the case of dynamic (time-varying) stimuli, the EM algorithm prescribes the use of r (t) = b r (tjN ), which is the optimal temporally smoothed state estimate <ref> [ Bryson and Ho, 1975 ] </ref> for time t ( N ), given input data for each of the time instants 1; : : : ; N . Unfortunately, the smoothed state estimate requires knowledge of future inputs and is computationally quite expensive. <p> This is not as serious a limitation as it seems because (a) any finite-order Markov process, where the next state depends on a finite number of past states, can be represented as a first-order Markov process <ref> [ Bryson and Ho, 1975 ] </ref> , and (b) since the matrices U and V are not fixed but can be adapted according to the input stimuli, many processes that may initially appear to be non-Markov can nevertheless be handled by the adaptive filter by finding appropriate U and V such
Reference: [ Cantoni and Levialdi, 1986 ] <editor> V. Cantoni and S. Levialdi, editors. </editor> <booktitle> Pyramidal Systems for Computer Vision (Proceedings of a NATO Advanced Research Workshop, </booktitle> <address> Maratea, Italy, May 59, 1986), Berlin, 1986. </address> <publisher> Springer. </publisher> <pages> 47 </pages>
Reference-contexts: Modeling such phenomena at a single spatial and/or temporal resolution generally leads to an incomplete and often incorrect understanding of the observed phenomenon. There has consequently been much recent interest in multiscale signal processing methods. Techniques such as image pyramids <ref> [ Cantoni and Levialdi, 1986 ] </ref> , wavelets [ Daubechies, 1992 ] , and scale-space theory [ Lindeberg, 1994 ] have found wide applications in computer vision and image processing.
Reference: [ Carandini and Heeger, 1994 ] <author> M. Carandini and D. Heeger. </author> <title> Summation and division by neurons in visual cortex. </title> <booktitle> Science, </booktitle> <address> 264:13331336, </address> <year> 1994. </year>
Reference-contexts: It has been suggested that such normalization of responses may occur as a result of biophysical membrane properties of neurons in the visual cortex <ref> [ Carandini and Heeger, 1994 ] </ref> . The preceding sequence of neural processing suffices to compute the weighted Kalman residual N W G (I U r), which is used to correct the previous state estimate r (t) at time instant t (see Equation 52).
Reference: [ Chou et al., 1994 ] <author> K.C. Chou, A.S. Willsky, and A. Benveniste. </author> <title> Multiscale recursive estimation, data fusion, and regularization. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 39(3):464478, </volume> <month> March </month> <year> 1994. </year>
Reference-contexts: Most natural phenomena manifest themselves over a multitude of spatial and temporal scales. For example, the rich class of stochastic processes possessing 1=f fi power spectra exhibit statistical and fractal self-similarities that can be satisfactorily captured only in a multiscale framework <ref> [ Chou et al., 1994 ] </ref> . Modeling such phenomena at a single spatial and/or temporal resolution generally leads to an incomplete and often incorrect understanding of the observed phenomenon. There has consequently been much recent interest in multiscale signal processing methods.
Reference: [ Chrisman, 1992 ] <author> L. Chrisman. </author> <title> Reinforcement learning with perceptual aliasing. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <year> 1992. </year>
Reference-contexts: and track the new interposed sequence, as evident from the accurate predictions and low residual errors in the subsequent time steps. 6.7 Hidden State and Perceptual Aliasing In the final experiment, we investigated how the filter handles the pervasive problem of hidden state [ McCallum, 1996 ] or perceptual aliasing <ref> [ Whitehead and Ballard, 1991; Chrisman, 1992 ] </ref> in partially observable environments. This problem has received much attention in the reinforcement learning community (for example, see [ Kaelbling et al., 1996 ] ).
Reference: [ Churchland and Sejnowski, 1992 ] <author> P. Churchland and T. Sejnowski. </author> <title> The Computational Brain. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: In the standard neural implementation of this type of an operation <ref> [ Churchland and Sejnowski, 1992 ] </ref> , the matrix A represents the synaptic strength of neurons (each row represents the synapses of one neuron) and the components of the vector v denote the pre-synaptic inputs to these neurons.
Reference: [ Cipra, 1993 ] <author> B. Cipra. </author> <title> Engineers look to Kalman filtering for guidance. </title> <journal> SIAM News, </journal> <volume> 26(5), </volume> <year> 1993. </year>
Reference-contexts: Since its discovery about three decades ago, the Kalman filter has been applied successfully to a wide range of problems in fields as diverse as economics [ Athans, 1974 ] and engineering <ref> [ Cipra, 1993 ] </ref> . Early applications of the filter were predominantly in aerospace engineering. <p> In 1969, it left an indelible mark on human history when it helped man set foot on the moon: the descent of the Apollo 11 lunar module to the surface of the moon was guided by a 21-state Kalman filter <ref> [ Cipra, 1993 ] </ref> . Even today, Kalman filters form the heart of inertial navigation systems that safely guide commercial airplanes to their respective destinations. In this paper, we describe a hierarchical Kalman filter based model of dynamic recognition and visual learning.
Reference: [ Creutzfeldt, 1977 ] <editor> O.D. Creutzfeldt. </editor> <booktitle> Generality of the functional structure of the neocortex. </booktitle> <address> Natur-wissenschaften, 64:507517, </address> <year> 1977. </year>
Reference-contexts: once the outliers were detected and discounted for within the first four or five frames of the image sequence. 9 Neural Implementation in the Visual Cortex The mammalian neocortex possesses a remarkable regularity in its neuroanatomical structure, as seen in the pattern of connections within and between different cortical areas <ref> [ Creutzfeldt, 1977; Barlow, 1985 ] </ref> . <p> Given that the cortex possesses roughly the same neuroanatomical input-output structure and pattern of connections across many different cortical areas <ref> [ Creutzfeldt, 1977; Barlow, 1985; Pandya et al., 1988 ] </ref> , a crucial test for any putative model of the cortex is whether it is general enough to be uniformly applicable to different cortical areas without regard to the specific input modality.
Reference: [ Crick, 1984 ] <author> F. Crick. </author> <title> Function of the thalamic reticular complex: The searchlight hypothesis. </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci., 81:45864590, </institution> <year> 1984. </year>
Reference-contexts: This implementation conforms to many of the known neuroanatomical connections between laminae in primate V1 (compare with Figure 21). the thalamic reticular complex (including the perigeniculate nucleus) <ref> [ Crick, 1984 ] </ref> . The modulated residual signal is then linearly filtered through the synapses of layer 4 cells in V1, which would correspond to the feedforward weights W = U T in the Kalman filter (see Figure 22).
Reference: [ Daubechies, 1992 ] <author> I. Daubechies. </author> <title> Ten lectures on wavelets. </title> <booktitle> CBMS-NSF Regional Conferences Series in Applied Mathematics. </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1992. </year>
Reference-contexts: Modeling such phenomena at a single spatial and/or temporal resolution generally leads to an incomplete and often incorrect understanding of the observed phenomenon. There has consequently been much recent interest in multiscale signal processing methods. Techniques such as image pyramids [ Cantoni and Levialdi, 1986 ] , wavelets <ref> [ Daubechies, 1992 ] </ref> , and scale-space theory [ Lindeberg, 1994 ] have found wide applications in computer vision and image processing. In this section, we propose a method for learning and using hierarchical internal models of natural dynamic phenomena in the external world.
Reference: [ Dayan and Hinton, 1996 ] <author> P. Dayan and G.E. Hinton. </author> <title> Varieties of Helmholtz machine. Neural Networks, </title> <address> 9(8):13851403, </address> <year> 1996. </year>
Reference-contexts: For the current purposes, we shall pursue a simple generalization of the familiar single level model used in the Kalman filter derivations above, bearing in mind that other more complex interactions between higher and lower level signals may also envisioned <ref> [ Dayan and Hinton, 1996 ] </ref> . Consider the first hierarchical-level.
Reference: [ Dayan et al., 1995 ] <author> P. Dayan, G.E. Hinton, R.M. Neal, </author> <title> and R.S. Zemel. The Helmholtz machine. </title> <booktitle> Neural Computation, </booktitle> <address> 7:889904, </address> <year> 1995. </year>
Reference: [ Dempster et al., 1977 ] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Series B, </journal> <volume> 39:138, </volume> <year> 1977. </year>
Reference-contexts: Fortunately, one can appeal to the well-known Expectation-Maximization (EM) algorithm from statistics <ref> [ Dempster et al., 1977 ] </ref> and allow the overall scheme to converge by choosing appropriate values for the state r in the above learning rules for u and v (note that in the above rules, we did not specify values for r (t) (comprising R (t) in Equation 31 and
Reference: [ Desimone and Ungerleider, 1989 ] <author> R. Desimone and L.G. Ungerleider. </author> <title> Neural mechanisms of visual processing in monkeys. </title> <editor> In F. Boller and J. Grafman, editors, </editor> <booktitle> Handbook of Neuropsychology, </booktitle> <volume> volume 2, chapter 14, </volume> <pages> pages 267299. </pages> <address> New York: </address> <publisher> Elsevier, </publisher> <year> 1989. </year>
Reference: [ Dickmanns and Mysliwetz, 1992 ] <editor> E.D. Dickmanns and B.D. Mysliwetz. </editor> <title> Recursive 3D road and relative ego-state recognition. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 14(2):199213, </volume> <year> 1992. </year>
Reference: [ Felleman and Van Essen, 1991 ] <author> D.J. Felleman and D.C. Van Essen. </author> <title> Distributed hierarchical processing in the primate cerebral cortex. Cerebral Cortex, </title> <address> 1:147, </address> <year> 1991. </year> <month> 48 </month>
Reference-contexts: For example, a ubiquitous feature of cortico-cortical connectivity is the reciprocity of connections between various cortical areas: if area A projects to area B, then area B almost invariably projects to area A <ref> [ Rockland and Pandya, 1979; Felleman and Van Essen, 1991 ] </ref> . These connections typically terminate and originate in distinct cortical laminae depending on their source and destination, which allows one to designate one cortical area as being at a higher or lower hierarchical level with respect to another. <p> Similarly, one might attempt to view the motor cortical hierarchy, comprising the primary motor cortex (M1), supplementary motor area (SMA), premotor cortex, and related regions in prefrontal cortex <ref> [ Pandya et al., 1988; Felleman and Van Essen, 1991 ] </ref> , as implementing a hierarchical prediction and estimation scheme that generates appropriate motor signals for intended body movements and receives as inputs the various afferent feedback signals from muscle spindles, Golgi tendon organs, and other motor receptors from corresponding parts
Reference: [ Feller, 1968 ] <author> W. Feller. </author> <title> An Introduction to Probability Theory and Its Applications, volume 1. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: Another possible limitation of the model is the assumption of Gaussian probability distributions when modeling the state and noise processes. Although such an assumption is partially supported by the Central Limit Theorem from statistics <ref> [ Feller, 1968 ] </ref> , the unimodality of the Gaussian distribution does not allow a simultaneous representation of multiple object hypotheses such as in a cluttered scene [ Isard and Blake, 1996 ] .
Reference: [ Ferster, 1994 ] <author> D. Ferster. </author> <title> Linearity of synaptic interactions in the assembly of receptive fields in cat visual cortex. Current Opinion in Neurobiology, </title> <address> 4:563568, </address> <year> 1994. </year>
Reference-contexts: In the context of biological modeling, it has been argued that some cortical neurons may very well operate linearly in much of their dynamic range <ref> [ Kohonen, 1988; Ferster, 1994 ] </ref> . Kohonen, in particular, argues that strong nonlinearities at the single neuron level are generally seen more often in evolutionarily older (subcortical) structures than the more recently 43 evolved neocortex.
Reference: [ Gilbert and Wiesel, 1981 ] <author> C.D. Gilbert and T.N. Wiesel. </author> <title> Laminar specialization and intracortical connections in cat primary visual cortex. </title> <booktitle> In The Organization of the Cerebral Cortex, </booktitle> <pages> pages 163191. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1981. </year>
Reference-contexts: The signal subsequently undergoes a normalization (corresponding to the matrix N in the Kalman filter). A plausible site for this normalization is layer 2+3 (composed of layers 2 and 3), given that this layer receives a substantial projection from layer 4 <ref> [ Gilbert and Wiesel, 1981; Bolz et al., 1989 ] </ref> . Normalization of responses has also been proposed by other authors to explain saturation of neural responses in V1 at high stimulus contrasts [ Heeger et al., 1996 ] . <p> The corrected estimate then generates the next state 41 prediction r (t + 1) via the synapses V . Neurons in layer 5 appear to be ideally located for such a computation, given that layer 2+3 cells project extensively into layer 5 <ref> [ Gilbert and Wiesel, 1981; Bolz et al., 1989 ] </ref> . Furthermore, layer 5 projects to layer 6, which is known to be a major source of cortico-thalamic feedback [ Gilbert and Wiesel, 1981 ] . <p> Furthermore, layer 5 projects to layer 6, which is known to be a major source of cortico-thalamic feedback <ref> [ Gilbert and Wiesel, 1981 ] </ref> . Layer 6 neurons could therefore synaptically encode the feedback weights U of the Kalman filter and convey the feedback signal U r (t + 1) to the dLGN for predictive inhibition of the next input I (t + 1).
Reference: [ Girosi et al., 1995 ] <author> F. Girosi, M. Jones, and T. Poggio. </author> <title> Regularization theory and neural networks architectures. </title> <booktitle> Neural Computation, </booktitle> <address> 7(2):219269, </address> <year> 1995. </year>
Reference-contexts: The underlying idea is to include a penalty term in the optimization function J for each of the parameters r, U and V . These penalty terms can be justified in a wide variety of settings: * Regularization Theory <ref> [ Poggio et al., 1985; Girosi et al., 1995 ] </ref> : Here, the penalty term is used to prevent the estimator from overfitting its estimate to the input data, thereby allowing the estimator to retain an ability to generalize to novel data that differs in some ways from the input data <p> See <ref> [ Girosi et al., 1995 ] </ref> and references therein for further details regarding regularization theory. * Minimum Description Length (MDL) principle [ Rissanen, 1989; Zemel, 1994 ] : This is a formal information-theoretic formulation of the well-known Occam's Razor principle: Given the choice between a set of possible explanations, pick the
Reference: [ Grossberg, 1976 ] <author> S. Grossberg. </author> <title> Adaptive pattern classification and universal recoding: II. Feedback, expectation, olfaction, illusions. </title> <journal> Biological Cybernetics, </journal> <volume> 23:187202, </volume> <year> 1976. </year>
Reference: [ Hackbusch, 1985 ] <author> W. Hackbusch. </author> <title> Multi-grid methods and applications. </title> <publisher> Berlin: Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: An additional computational advantage of such a hierarchical scheme is the possibility of faster learning and faster convergence to the desired estimates as is often witnessed in multigrid methods for optimization <ref> [ Hackbusch, 1985 ] </ref> . 8.1 Modeling Top-Down Influences The central issue when defining a hierarchical model is the specification of how top-down signals from a higher level influence the state at a lower level.
Reference: [ Hallam, 1983 ] <author> J. Hallam. </author> <title> Resolving observer motion by object tracking. </title> <booktitle> In Proc. of 8th International Joint Conf. on Artificial Intelligence, </booktitle> <volume> volume 2, </volume> <pages> pages 792798, </pages> <year> 1983. </year>
Reference: [ Harpur and Prager, 1996 ] <author> G.F. Harpur and R.W. Prager. </author> <title> Development of low-entropy coding in a recurrent network. Network, </title> <address> 7:277284, </address> <year> 1996. </year>
Reference-contexts: as a consequence of the calculation of the probability mass of the model parameters for use in their respective log P based encoding terms in J (see [ Rao and Ballard, 1996a ] for details). * Seeking Higher-Order Statistical Correlations: As proposed by [ Olshausen and Field, 1996 ] and <ref> [ Harpur and Prager, 1996 ] </ref> , the addition of certain nonlinear penalty terms allows an esti mator to seek more than pairwise correlations in the input data. <p> For example, some previously used penalty functions include f (x) = g (x) = h (x) = ffx 2 [ Rao and Ballard, 1996a ] , f (x) = ff log (1+ x 2 ) [ Olshausen and Field, 1996 ] , and f (x) = ffjxj 1=r <ref> [ Harpur and Prager, 1996 ] </ref> where the functions are applied to all components x of a given vector x and the results are summed in the optimization function.
Reference: [ Harth et al., 1987 ] <author> E. Harth, </author> <title> K.P. Unnikrishnan, and A.S. Pandya. The inversion of sensory processing by feedback pathways: A model of visual cognitive functions. </title> <booktitle> Science, </booktitle> <address> 237:184187, </address> <year> 1987. </year>
Reference: [ Heeger et al., 1996 ] <author> D.J. Heeger, E.P. Simoncelli, and J.A. Movshon. </author> <title> Computational models of cortical visual processing. </title> <booktitle> Proc. </booktitle> <institution> National Acad. Sciences, 93:623627, </institution> <year> 1996. </year>
Reference-contexts: Normalization of responses has also been proposed by other authors to explain saturation of neural responses in V1 at high stimulus contrasts <ref> [ Heeger et al., 1996 ] </ref> . It has been suggested that such normalization of responses may occur as a result of biophysical membrane properties of neurons in the visual cortex [ Carandini and Heeger, 1994 ] .
Reference: [ Hinton et al., 1995 ] <author> G.E. Hinton, P. Dayan, B.J. Frey, and R.M. Neal. </author> <title> The wake-sleep algorithm for unsupervised neural networks. </title> <booktitle> Science, </booktitle> <address> 268:11581161, </address> <year> 1995. </year>
Reference: [ Huber, 1981 ] <author> P.J. Huber. </author> <title> Robust Statistics. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: A quadratic optimization function is however susceptible to outliers (or gross errors) i.e. data points that lie far away from the majority of the data points in I <ref> [ Huber, 1981 ] </ref> . <p> These deviating pixels need to be treated as outliers and discounted for in the minimization process in order to get an accurate estimate of the state r. The field of robust statistics <ref> [ Huber, 1981 ] </ref> suggests some useful techniques for preventing gross outliers from influencing the solution to an estimation problem.
Reference: [ Humphrey, 1992 ] <author> N. Humphrey. </author> <title> A History of the Mind. </title> <address> New York: </address> <publisher> Simon and Schuster, </publisher> <year> 1992. </year>
Reference-contexts: This idea is reminiscent of Mackay's epistemological automata [ MacKay, 1956 ] , which perceives by comparing its expectations of sensory inputs with the actual inputs. The evolutionary origins of this ongoing sensory-model comparison process can perhaps be traced back to the simple feedback loops of micro-organisms <ref> [ Humphrey, 1992 ] </ref> , where the modeling occurs at the organism's peripheral surface, as opposed to higher mammals, where this modeling presumably occurs at the level of the cerebral cortex. of an internal model.
Reference: [ Isard and Blake, 1996 ] <author> M. Isard and A. Blake. </author> <title> Contour tracking by stochastic propogation of conditional density. </title> <booktitle> In Proc. of ECCV, </booktitle> <pages> pages 343356, </pages> <year> 1996. </year> <month> 49 </month>
Reference-contexts: Although such an assumption is partially supported by the Central Limit Theorem from statistics [ Feller, 1968 ] , the unimodality of the Gaussian distribution does not allow a simultaneous representation of multiple object hypotheses such as in a cluttered scene <ref> [ Isard and Blake, 1996 ] </ref> .
Reference: [ James et al., 1995 ] <author> A.C. James, J.M. Hupe, S.L. Lomber, B. Payne, P. Girard, and J. Bullier. </author> <title> Feed--back connections contribute to center-surround interactions in neurons of monkey areas V1 and V2. </title> <publisher> Soc. </publisher> <address> Neuro. Abstr., 21:904, </address> <year> 1995. </year>
Reference-contexts: In addition, preliminary neurophysiological results from experiments involving the inactivation of V2 also seem to indicate that feedback from a higher area plays a crucial role in mediating extra-classical effects in a lower area such as V1 <ref> [ James et al., 1995 ] </ref> .
Reference: [ Jordan and Jacobs, 1994 ] <author> M.I. Jordan and R.A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <booktitle> Neural Computation, </booktitle> <address> 6(2):181214, </address> <year> 1994. </year>
Reference-contexts: The output of the neurons representing N is then used to correct the current state estimate and generate the next state prediction r (t) as in Equation 52 above. The neurons representing the gain matrices G and G td may be regarded as gating neurons <ref> [ Jordan and Jacobs, 1994 ] </ref> , which compute the values in G or G td based on their inputs (the residuals). These outputs then serve to inhibit the incoming residual signals according to whether these signals represent outliers or not.
Reference: [ Kaelbling et al., 1996 ] <author> L.P. Kaelbling, M.L. Littman, and A.W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4:237285, </volume> <year> 1996. </year>
Reference-contexts: This problem has received much attention in the reinforcement learning community (for example, see <ref> [ Kaelbling et al., 1996 ] </ref> ). The essence of the problem lies in the fact that a given observation of the environment by itself might be insufficient to determine the corresponding state of the environment.
Reference: [ Kalman and Bucy, 1961 ] <author> R.E. </author> <title> Kalman and R.S. Bucy. New results in linear filtering and prediction theory. </title> <journal> Trans. ASME J. Basic Eng., </journal> <volume> 83:95108, </volume> <year> 1961. </year>
Reference-contexts: Perhaps the best known algorithm for accurate estimation and prediction of the internal state of an observed dynamic system is the Kalman filter <ref> [ Kalman, 1960; Kalman and Bucy, 1961 ] </ref> . The Kalman filter is a linear dynamical system that attempts to mimic the behavior of an observed natural process. It does so by calculating, at each time instant, an optimal estimate of the current internal state of the observed process.
Reference: [ Kalman, 1960 ] <author> R.E. </author> <title> Kalman. A new approach to linear filtering and prediction theory. </title> <journal> Trans. ASME J. Basic Eng., </journal> <volume> 82:3545, </volume> <year> 1960. </year>
Reference-contexts: Perhaps the best known algorithm for accurate estimation and prediction of the internal state of an observed dynamic system is the Kalman filter <ref> [ Kalman, 1960; Kalman and Bucy, 1961 ] </ref> . The Kalman filter is a linear dynamical system that attempts to mimic the behavior of an observed natural process. It does so by calculating, at each time instant, an optimal estimate of the current internal state of the observed process.
Reference: [ Kawato et al., 1993 ] <author> M. Kawato, H. Hayakawa, and T. Inui. </author> <title> A forward-inverse optics model of reciprocal connections between visual cortical areas. Network, </title> <address> 4:415422, </address> <year> 1993. </year>
Reference: [ Kohonen, 1988 ] <author> T. Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Berlin: Springer-Verlag, </publisher> <address> second edition, </address> <year> 1988. </year>
Reference-contexts: In the context of biological modeling, it has been argued that some cortical neurons may very well operate linearly in much of their dynamic range <ref> [ Kohonen, 1988; Ferster, 1994 ] </ref> . Kohonen, in particular, argues that strong nonlinearities at the single neuron level are generally seen more often in evolutionarily older (subcortical) structures than the more recently 43 evolved neocortex.
Reference: [ Leonardis and Bischof, 1996 ] <author> A. Leonardis and H. Bischof. </author> <title> Dealing with occlusions in the eigenspace approach. </title> <booktitle> In Proc. of CVPR, </booktitle> <pages> pages 453458, </pages> <year> 1996. </year>
Reference-contexts: This was apparently sufficient for the 100% recognition rate that was obtained in this simple case for 36 different testing views of each object, each test view being 5 ffi away from the nearest training view. The second to last row depicts how the effect of occlusions spreads globally <ref> [ Leonardis and Bischof, 1996 ] </ref> , as seen in the mediocre prediction and relatively large residuals at many locations. This is handled via robust estimation (Section 7).
Reference: [ Lindeberg, 1994 ] <author> T. Lindeberg. </author> <title> Scale-Space Theory in Computer Vision. </title> <address> Netherlands: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference-contexts: There has consequently been much recent interest in multiscale signal processing methods. Techniques such as image pyramids [ Cantoni and Levialdi, 1986 ] , wavelets [ Daubechies, 1992 ] , and scale-space theory <ref> [ Lindeberg, 1994 ] </ref> have found wide applications in computer vision and image processing. In this section, we propose a method for learning and using hierarchical internal models of natural dynamic phenomena in the external world.
Reference: [ Lund, 1981 ] <author> J.S. Lund. </author> <title> Intrinsic organization of the primate visual cortex, area 17, as seen in Golgi preparations. </title> <booktitle> In The Organization of the Cerebral Cortex, </booktitle> <pages> pages 105124. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1981. </year>
Reference: [ MacKay, 1956 ] <author> D.M. MacKay. </author> <title> The epistemological problem for automata. </title> <booktitle> In Automata Studies, </booktitle> <pages> pages 235251. </pages> <address> Princeton, NJ: </address> <publisher> Princeton University Press, </publisher> <year> 1956. </year>
Reference-contexts: Neisser proposed an analysis-by-synthesis approach to perception, wherein an internal world model is adapted according to the sensory stimuli received by the perceiver. This idea is reminiscent of Mackay's epistemological automata <ref> [ MacKay, 1956 ] </ref> , which perceives by comparing its expectations of sensory inputs with the actual inputs.
Reference: [ Matthies et al., 1989 ] <author> L. Matthies, T. Kanade, and R. Szeliski. </author> <title> Kalman filter-based algorithms for estimating depth from image sequences. </title> <journal> International Journal of Computer Vision, </journal> <volume> 3:209236, </volume> <year> 1989. </year>
Reference: [ Maunsell and Newsome, 1987 ] <author> J.H.R. Maunsell and W.T. Newsome. </author> <title> Visual processing in monkey extrastriate cortex. </title> <journal> Annual Review of Neuroscience, </journal> <volume> 10:363401, </volume> <year> 1987. </year> <month> 50 </month>
Reference: [ Maybeck, 1979 ] <author> P.S. Maybeck. </author> <title> Stochastic Models, Estimation, and Control (Vols. I and II). </title> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1979. </year>
Reference-contexts: One obvious shortcoming of the model is the assumption of linearity when modeling the measurement and state transition processes (Section 3). Indeed, this is the primary weakness of the standard Kalman filter. It is therefore not surprising that non-linear alternatives such as the extended Kalman filter have been proposed <ref> [ Maybeck, 1979 ] </ref> . The model presented here readily generalizes to the extended Kalman filter case, where the prediction step (Equation 19) can be made non-linear [ Rao and Ballard, 1996a ] .
Reference: [ McCallum, 1996 ] <author> R.A. McCallum. </author> <title> Hidden state and reinforcement learning with instance-based state identification. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <volume> 26(3):464473, </volume> <year> 1996. </year>
Reference-contexts: soon corrects itself and begins to recognize and track the new interposed sequence, as evident from the accurate predictions and low residual errors in the subsequent time steps. 6.7 Hidden State and Perceptual Aliasing In the final experiment, we investigated how the filter handles the pervasive problem of hidden state <ref> [ McCallum, 1996 ] </ref> or perceptual aliasing [ Whitehead and Ballard, 1991; Chrisman, 1992 ] in partially observable environments. This problem has received much attention in the reinforcement learning community (for example, see [ Kaelbling et al., 1996 ] ).
Reference: [ Mishkin et al., 1983 ] <author> M. Mishkin, L.G. Ungerleider, and K.A. Macko. </author> <title> Object vision and spatial vision: Two cortical pathways. </title> <booktitle> Trends in Neuroscience, </booktitle> <address> 6:414417, </address> <year> 1983. </year>
Reference: [ Mumford, 1992 ] <author> D. Mumford. </author> <title> On the computational architecture of the neocortex. II. The role of cortico-cortical loops. </title> <journal> Biological Cybernetics, </journal> <volume> 66:241251, </volume> <year> 1992. </year>
Reference: [ Murase and Nayar, 1995 ] <author> H. Murase and S.K. Nayar. </author> <title> Visual learning and recognition of 3D objects from appearance. </title> <address> IJCV, 14:524, </address> <year> 1995. </year>
Reference-contexts: elsewhere [ Rao and Ballard, 1996a ] , the Kalman filter model of recognition can be regarded as a natural generalization of some previous schemes for appearance-based recognition such as principal component analysis (PCA) (cf. the Eigenface method of [ Turk and Pentland, 1991 ] and the Eigenspace method of <ref> [ Murase and Nayar, 1995 ] </ref> ). <p> error), measured as sum of squared pixel-wise errors, across the five training objects as a function of number of exposures to the set of objects. 6.5 View-Based Recognition of 3D Objects In a second experiment, we evaluated the ability of the filter to recognize 3D objects using a view-based approach <ref> [ Poggio and Edelman, 1990; Murase and Nayar, 1995 ] </ref> . Thirty-six views of two different 3D objects, each view 10 ffi azimuth apart from the next, were used for training the filter (Figure 7 (a)).
Reference: [ Murphy and Sillito, 1987 ] <author> P.C. Murphy and A.M. Sillito. </author> <title> Corticofugal feedback influences the generation of length tuning in the visual pathway. </title> <booktitle> Nature, </booktitle> <address> 329:727729, </address> <year> 1987. </year>
Reference: [ Neisser, 1967 ] <author> U. Neisser. </author> <title> Cognitive Psychology. </title> <address> New York: Appleton-Century-Crofts, </address> <year> 1967. </year>
Reference-contexts: The concept of an internal world model and its relationship to sensory feedback has been a dominant theme in modern cognitive psychology <ref> [ Neisser, 1967 ] </ref> . Neisser proposed an analysis-by-synthesis approach to perception, wherein an internal world model is adapted according to the sensory stimuli received by the perceiver.
Reference: [ Newman, 1988 ] <author> J.D. Newman. </author> <title> Primate hearing mechanisms. </title> <editor> In H.D. Steklis and J. Erwin, editors, </editor> <booktitle> Comparative Primate Biology, </booktitle> <volume> Volume 4: </volume> <pages> Neurosciences, pages 469499. </pages> <address> New York: </address> <publisher> Alan R. Liss, Inc., </publisher> <year> 1988. </year>
Reference-contexts: Such an architecture may allow modeling of the auditory system comprising of the medial geniculate nucleus (MGN) and the hierarchically organized areas of the primate auditory cortex (A1, A2, and related areas in the superior temporal gyrus) <ref> [ Pandya et al., 1988; Newman, 1988 ] </ref> .
Reference: [ Olshausen and Field, 1996 ] <author> B.A. </author> <title> Olshausen and D.J. Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. </title> <booktitle> Nature, </booktitle> <address> 381:607609, </address> <year> 1996. </year>
Reference-contexts: It also shares the 2 favorable properties of some recently proposed learning algorithms <ref> [ Olshausen and Field, 1996; Bell and Sejnowski, 1996 ] </ref> that have been shown to develop localized receptive fields similar to those of simple cells in the primary visual cortex from natural image inputs. <p> The penalty terms for model parameters arise as a consequence of the calculation of the probability mass of the model parameters for use in their respective log P based encoding terms in J (see [ Rao and Ballard, 1996a ] for details). * Seeking Higher-Order Statistical Correlations: As proposed by <ref> [ Olshausen and Field, 1996 ] </ref> and [ Harpur and Prager, 1996 ] , the addition of certain nonlinear penalty terms allows an esti mator to seek more than pairwise correlations in the input data. <p> For example, some previously used penalty functions include f (x) = g (x) = h (x) = ffx 2 [ Rao and Ballard, 1996a ] , f (x) = ff log (1+ x 2 ) <ref> [ Olshausen and Field, 1996 ] </ref> , and f (x) = ffjxj 1=r [ Harpur and Prager, 1996 ] where the functions are applied to all components x of a given vector x and the results are summed in the optimization function.
Reference: [ Orban, 1992 ] <author> G.A. Orban. </author> <title> Neuronal Operations in the Visual Cortex. </title> <address> New York: </address> <publisher> Springer Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Such an assumption is also supported by neuroanatomical evidence that feedback connections from the cortex contact inhibitory interneurons in the dLGN <ref> [ Orban, 1992 ] </ref> . The resulting residual activities of the dLGN neurons are assumed to be conveyed to the cortex via the feedforward thalamo-cortical pathway.
Reference: [ O'Reilly, 1996 ] <author> R.C. O'Reilly. </author> <title> The LEABRA model of neural interactions and learning in the neocortex. </title> <type> PhD thesis, </type> <institution> Department of Psychology, Carnegie Mellon University, </institution> <year> 1996. </year>
Reference-contexts: the expectation 1 We shall be concerned here with the discrete Kalman filter, although similar results can be obtained for the continuous time version of the filter [ Bryson and Ho, 1975 ] . 5 general problem faced by an organism relying on an internal model of its environment (from <ref> [ O'Reilly, 1996 ] </ref> ). The underlying goal is to optimally estimate, at each time instant, the hidden internal state of the environment given only the sensory measurements I. (b) depicts a single-level Kalman filter solution to the estimation problem.
Reference: [ Pandya et al., 1988 ] <author> D.N. Pandya, B. Seltzer, and H. Barbas. </author> <title> Input-output organization of the primate cerebral cortex. </title> <editor> In H.D. Steklis and J. Erwin, editors, </editor> <booktitle> Comparative Primate Biology, </booktitle> <volume> Volume 4: </volume> <pages> Neurosciences, pages 3980. </pages> <address> New York: </address> <publisher> Alan R. Liss, Inc., </publisher> <year> 1988. </year>
Reference-contexts: Given that the cortex possesses roughly the same neuroanatomical input-output structure and pattern of connections across many different cortical areas <ref> [ Creutzfeldt, 1977; Barlow, 1985; Pandya et al., 1988 ] </ref> , a crucial test for any putative model of the cortex is whether it is general enough to be uniformly applicable to different cortical areas without regard to the specific input modality. <p> Such an architecture may allow modeling of the auditory system comprising of the medial geniculate nucleus (MGN) and the hierarchically organized areas of the primate auditory cortex (A1, A2, and related areas in the superior temporal gyrus) <ref> [ Pandya et al., 1988; Newman, 1988 ] </ref> . <p> Similarly, one might attempt to view the motor cortical hierarchy, comprising the primary motor cortex (M1), supplementary motor area (SMA), premotor cortex, and related regions in prefrontal cortex <ref> [ Pandya et al., 1988; Felleman and Van Essen, 1991 ] </ref> , as implementing a hierarchical prediction and estimation scheme that generates appropriate motor signals for intended body movements and receives as inputs the various afferent feedback signals from muscle spindles, Golgi tendon organs, and other motor receptors from corresponding parts
Reference: [ Pentland, 1992 ] <author> A.P. Pentland. </author> <title> Dynamic vision. In G.A. </title> <editor> Carpenter and S. Grossberg, editors, </editor> <booktitle> Neural Networks for Vision and Image Processing, </booktitle> <pages> pages 133159. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference: [ Picton and Stuss, 1994 ] <author> T.W. Picton and D.T. Stuss. </author> <title> Neurobiology of conscious experience. Current Opinion in Neurobiology, </title> <address> 4:256265, </address> <year> 1994. </year> <month> 51 </month>
Reference-contexts: Models and the Estimation Problem There is a growing consensus among cognitive neuroscientists that the brain learns and maintains an internal model of the external world [ Barlow, 1985; 1994 ] , and that conscious experience involves 4 an active interaction between external sensory events and this internal modeling process <ref> [ Picton and Stuss, 1994 ] </ref> . The concept of an internal world model and its relationship to sensory feedback has been a dominant theme in modern cognitive psychology [ Neisser, 1967 ] .
Reference: [ Poggio and Edelman, 1990 ] <author> T. Poggio and S. Edelman. </author> <title> A network that learns to recognize 3D objects. </title> <booktitle> Nature, </booktitle> <address> 343:263266, </address> <year> 1990. </year>
Reference-contexts: error), measured as sum of squared pixel-wise errors, across the five training objects as a function of number of exposures to the set of objects. 6.5 View-Based Recognition of 3D Objects In a second experiment, we evaluated the ability of the filter to recognize 3D objects using a view-based approach <ref> [ Poggio and Edelman, 1990; Murase and Nayar, 1995 ] </ref> . Thirty-six views of two different 3D objects, each view 10 ffi azimuth apart from the next, were used for training the filter (Figure 7 (a)).
Reference: [ Poggio et al., 1985 ] <author> T. Poggio, V. Torre, and C. Koch. </author> <title> Computational vision and regularization theory. </title> <booktitle> Nature, </booktitle> <address> 317:314319, </address> <year> 1985. </year>
Reference-contexts: The underlying idea is to include a penalty term in the optimization function J for each of the parameters r, U and V . These penalty terms can be justified in a wide variety of settings: * Regularization Theory <ref> [ Poggio et al., 1985; Girosi et al., 1995 ] </ref> : Here, the penalty term is used to prevent the estimator from overfitting its estimate to the input data, thereby allowing the estimator to retain an ability to generalize to novel data that differs in some ways from the input data
Reference: [ Rao and Ballard, 1996a ] <author> R.P.N. Rao and D.H. Ballard. </author> <title> Dynamic model of visual recognition predicts neural response properties in the visual cortex. </title> <journal> Neural Computation, </journal> <volume> 9(4) </volume> <pages> 805-847, </pages> <year> 1997. </year> <note> Also, Technical Report 96.2, </note> <institution> National Resource Laboratory for the Study of Brain and Behavior, Department of Computer Science, University of Rochester, </institution> <year> 1996. </year>
Reference-contexts: Even today, Kalman filters form the heart of inertial navigation systems that safely guide commercial airplanes to their respective destinations. In this paper, we describe a hierarchical Kalman filter based model of dynamic recognition and visual learning. As described elsewhere <ref> [ Rao and Ballard, 1996a ] </ref> , the Kalman filter model of recognition can be regarded as a natural generalization of some previous schemes for appearance-based recognition such as principal component analysis (PCA) (cf. the Eigenface method of [ Turk and Pentland, 1991 ] and the Eigenspace method of [ Murase <p> However, in complex dynamic environments, the formulation of such hand-coded models becomes increasingly difficult. An interesting alternative <ref> [ Rao and Ballard, 1996a ] </ref> is to initialize the matrices U and V to small random values, and then adapt these values in response to input data, thereby learning an internal model of the input environment. <p> The penalty terms for model parameters arise as a consequence of the calculation of the probability mass of the model parameters for use in their respective log P based encoding terms in J (see <ref> [ Rao and Ballard, 1996a ] </ref> for details). * Seeking Higher-Order Statistical Correlations: As proposed by [ Olshausen and Field, 1996 ] and [ Harpur and Prager, 1996 ] , the addition of certain nonlinear penalty terms allows an esti mator to seek more than pairwise correlations in the input data. <p> For example, some previously used penalty functions include f (x) = g (x) = h (x) = ffx 2 <ref> [ Rao and Ballard, 1996a ] </ref> , f (x) = ff log (1+ x 2 ) [ Olshausen and Field, 1996 ] , and f (x) = ffjxj 1=r [ Harpur and Prager, 1996 ] where the functions are applied to all components x of a given vector x and the <p> It is therefore not surprising that non-linear alternatives such as the extended Kalman filter have been proposed [ Maybeck, 1979 ] . The model presented here readily generalizes to the extended Kalman filter case, where the prediction step (Equation 19) can be made non-linear <ref> [ Rao and Ballard, 1996a ] </ref> . Unfortunately, the introduction of nonlinearities often complicates the corresponding estimation process, forcing the use of approximations (such as Taylor series based approximations) to make the mathematical derivations tractable. As a result, many important properties such as optimality and stability may be lost.
Reference: [ Rao and Ballard, 1996b ] <author> R.P.N. Rao and D.H. Ballard. </author> <title> The visual cortex as a hierarchical predictor. </title> <type> Technical Report 96.4, </type> <institution> National Resource Laboratory for the Study of Brain and Behavior, Department of Computer Science, University of Rochester, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: Simulation results using a hierarchical Kalman filter network trained on natural images support this 42 interpretation of extra-classical effects in the visual cortex <ref> [ Rao and Ballard, 1996b; 1996a ] </ref> .
Reference: [ Rao and Ballard, 1997 ] <author> R.P.N. Rao and D.H. Ballard. </author> <title> Kalman filter networks for invariant recognition, motion, and stereo. </title> <type> Technical Report 97.2, </type> <institution> National Resource Laboratory for the Study of Brain and Behavior, Department of Computer Science, University of Rochester, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: transformation state x: I (t) = DI 0 x (t) + n (t) (58) It is a straightforward exercise to formulate an optimization function based on the above equation and derive a Kalman filter update rule for the current transformation state x and a learning rule for the matrix D <ref> [ Rao and Ballard, 1997 ] </ref> . Note that these rules require the original image I (0), which is the top-down predicted image representing the current object hypothesis (What), to be supplied by the object estimation network maintaining the current object state r.
Reference: [ Rissanen, 1989 ] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> Singapore: World Scientific, </publisher> <year> 1989. </year>
Reference-contexts: See [ Girosi et al., 1995 ] and references therein for further details regarding regularization theory. * Minimum Description Length (MDL) principle <ref> [ Rissanen, 1989; Zemel, 1994 ] </ref> : This is a formal information-theoretic formulation of the well-known Occam's Razor principle: Given the choice between a set of possible explanations, pick the simplest one.
Reference: [ Rockland and Pandya, 1979 ] <author> K.S. Rockland and D.N. Pandya. </author> <title> Laminar origins and terminations of cortical connections of the occipital lobe in the rhesus monkey. </title> <journal> Brain Research, </journal> <volume> 179:320, </volume> <year> 1979. </year>
Reference-contexts: For example, a ubiquitous feature of cortico-cortical connectivity is the reciprocity of connections between various cortical areas: if area A projects to area B, then area B almost invariably projects to area A <ref> [ Rockland and Pandya, 1979; Felleman and Van Essen, 1991 ] </ref> . These connections typically terminate and originate in distinct cortical laminae depending on their source and destination, which allows one to designate one cortical area as being at a higher or lower hierarchical level with respect to another.
Reference: [ Softky, 1996 ] <editor> W.R. Softky. Unsupervised pixel-prediction. In D. Touretzky, M. Mozer, and M. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 809815. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference: [ Turk and Pentland, 1991 ] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1):7186, </volume> <year> 1991. </year>
Reference-contexts: As described elsewhere [ Rao and Ballard, 1996a ] , the Kalman filter model of recognition can be regarded as a natural generalization of some previous schemes for appearance-based recognition such as principal component analysis (PCA) (cf. the Eigenface method of <ref> [ Turk and Pentland, 1991 ] </ref> and the Eigenspace method of [ Murase and Nayar, 1995 ] ).
Reference: [ Ungerleider and Mishkin, 1982 ] <author> L. Ungerleider and M. Mishkin. </author> <title> Two cortical visual systems. </title> <editor> In D. Ingle, M. Goodale, and R. Mansfield, editors, </editor> <booktitle> Analysis of Visual Behavior, </booktitle> <pages> pages 549585. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1982. </year>
Reference: [ Van Essen and Maunsell, 1983 ] <author> D.C. Van Essen and J.H.R. Maunsell. </author> <title> Hierarchical organization and functional streams in the visual cortex. </title> <booktitle> Trends in Neuroscience, </booktitle> <address> 6:370375, </address> <year> 1983. </year>
Reference: [ Van Essen, 1985 ] <author> D.C. Van Essen. </author> <title> Functional organization of primate visual cortex. </title> <editor> In A. Peters and E.G. Jones, editors, Cerebral Cortex, </editor> <volume> volume 3, </volume> <pages> pages 259329. </pages> <publisher> Plenum, </publisher> <year> 1985. </year> <month> 52 </month>
Reference: [ Whitehead and Ballard, 1991 ] <author> S.D. Whitehead and D.H. Ballard. </author> <title> Learning to perceive and act by trial and error. </title> <booktitle> Machine Learning, </booktitle> <address> 7(1):4583, </address> <year> 1991. </year>
Reference-contexts: and track the new interposed sequence, as evident from the accurate predictions and low residual errors in the subsequent time steps. 6.7 Hidden State and Perceptual Aliasing In the final experiment, we investigated how the filter handles the pervasive problem of hidden state [ McCallum, 1996 ] or perceptual aliasing <ref> [ Whitehead and Ballard, 1991; Chrisman, 1992 ] </ref> in partially observable environments. This problem has received much attention in the reinforcement learning community (for example, see [ Kaelbling et al., 1996 ] ).
Reference: [ Zemel, 1994 ] <author> R.S. Zemel. </author> <title> A Minimum Description Length Framework for Unsupervised Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1994. </year>
Reference-contexts: See [ Girosi et al., 1995 ] and references therein for further details regarding regularization theory. * Minimum Description Length (MDL) principle <ref> [ Rissanen, 1989; Zemel, 1994 ] </ref> : This is a formal information-theoretic formulation of the well-known Occam's Razor principle: Given the choice between a set of possible explanations, pick the simplest one.
Reference: [ Zipser et al., 1996 ] <author> K. Zipser, V.A.F. Lamme, and P.H. Schiller. </author> <title> Contextual modulation in primary visual cortex. </title> <journal> Journal of Neuroscience, </journal> <volume> 16(22):73767389, </volume> <year> 1996. </year> <month> 53 </month>
Reference-contexts: One explanation of this highly nonlinear response suppression attributes the attenuation in response to lateral inhibition from neighboring cells. However, recent experiments by Zipser et al. <ref> [ Zipser et al., 1996 ] </ref> indicate that these extra-classical effects in layer 2+3 are seen only 80-100 milliseconds after stimulus onset.
References-found: 82

