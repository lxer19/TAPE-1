URL: http://graphics.lcs.mit.edu/~satyan/pubs/MIT-LCS-TR-729.ps.Z
Refering-URL: http://graphics.lcs.mit.edu/~satyan/pubs.html
Root-URL: 
Title: Automatic Extraction of Textured Vertical Facades from Pose Imagery  
Author: Satyan Coorg Seth Teller 
Date: January, 1998  
Affiliation: Computer Graphics Group  
Pubnum: MIT LCS TR-729  
Abstract: This technical report (TR) has been made available free of charge from the MIT Laboratory for Computer Science, at www.lcs.mit.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Becker and J. V. Michael Bove. </author> <title> Semiautomatic 3-d model extraction from uncalibrated 2-d camera views. In Proceedings of Visual Data Exploration and Analysis II, </title> <booktitle> SPIE Vol. </booktitle> <volume> 2410, </volume> <pages> pages 447-461, </pages> <year> 1995. </year>
Reference-contexts: The constraints generated by this process are then used to solve for 3-D structure. The more successful systems <ref> [1, 8] </ref> exploit geometric structure inherent in the environment, such as blocks in [8] or parallel and perpendicular lines [1]. While such systems can generate high quality results, the user must process each input image, e.g., by identifying edges (for correspondence) and occluded pixels (for texture computation). <p> The constraints generated by this process are then used to solve for 3-D structure. The more successful systems [1, 8] exploit geometric structure inherent in the environment, such as blocks in [8] or parallel and perpendicular lines <ref> [1] </ref>. While such systems can generate high quality results, the user must process each input image, e.g., by identifying edges (for correspondence) and occluded pixels (for texture computation). This makes them impractical for use on large datasets, such as ours. <p> Note that it is straightforward to identify such edges in each node; they are edges whose computed azimuth is . For example, in Figure 9, red edges are used for tiles with azimuth= 2 (i.e., normal = <ref> [1; 0; 0] </ref> T ). Using such edges, Figure 3 shows the application of incidence counting to tile location. Part (a) shows three planes with common normal N and different offsets, with projected (and blurred) edges E1 and E2 from two nodes.
Reference: [2] <author> B. Brillault-O'Mahony. </author> <title> New method for vanishing point detection. </title> <booktitle> Image Understanding, </booktitle> <volume> 54 </volume> <pages> 289-300, </pages> <year> 1991. </year>
Reference-contexts: (e.g., tree edges); thus they tend to be uncorrelated both within a node and across different nodes. 5 Unlike computer vision techniques that recover 3-D orientation information from aggregate properties of a set of edges (e.g., moments in shape from texture [23] and common intersection points in vanishing point estimation <ref> [2] </ref>), this technique recovers 3-D orientation from a single (presumed horizontal) edge.
Reference: [3] <author> F. J. Canny. </author> <title> A computational approach to edge detection. </title> <journal> IEEE Trans PAMI, </journal> <volume> 8(6) </volume> <pages> 679-698, </pages> <year> 1986. </year>
Reference-contexts: The facade extraction algorithm considered edges detected on six faces of a cubical environment map representing a node. Each face of the environment map was generated at 1024 fi 1024 resolution by resampling the input images. Edge pixels were detected using the Canny edge detector <ref> [3] </ref> and converted to line segments by linking pixels with similar gradient orientation. Approximately 1000 edges were computed for each cube face (ignoring edges less than 10 pixels in length). Some of the important parameters supplied to the algorithm are listed in Table 2.
Reference: [4] <author> S. E. Chen. </author> <title> Quicktime VR an image-based approach to virtual environment navigation. </title> <booktitle> In SIGGRAPH 95 Conference Proceedings, </booktitle> <pages> pages 29-38, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: This makes them impractical for use on large datasets, such as ours. Such systems are also difficult to assess algorithmically, as a "human-in-the-loop" is performing non-algorithmic tasks. Mosaicing [21, 22] seamlessly stitches together multiple images taken from the same viewpoint. As in QuickTime VR <ref> [4] </ref>, such mosaics can be used for walkthroughs of virtual environments. However, as mosaics themselves do not contain 3-D information (i.e., depth), the range of viewpoints is severely limited.
Reference: [5] <author> R. Collins. </author> <title> A space-sweep approach to true multi-image matching. </title> <booktitle> In CVPR96, </booktitle> <pages> pages 358-363, </pages> <year> 1996. </year>
Reference-contexts: However, in order to perform automatic matching, such algorithms require images to be taken from closely-spaced cameras under stable illumination conditions. Such conditions would be difficult, if not impossible, to achieve in extended outdoor environments. Space-sweep techniques have been used recently <ref> [5, 20] </ref> to perform matching and reconstruction from an arbitrary number of images. Unlike traditional image-space algorithms that rely on correspondence between image features, these world-space algorithms traverse the entire 3-D region of interest and identify likely locations of 3-D features. <p> Space-sweep techniques have been used recently [5, 20] to perform matching and reconstruction from an arbitrary number of images. Unlike traditional image-space algorithms that rely on correspondence between image features, these world-space algorithms traverse the entire 3-D region of interest and identify likely locations of 3-D features. In <ref> [5] </ref>, a counting metric based on edge pixels is used to recover 3-D information from a few (seven) aerial images. In [20], a correlation-based metric using pixel colors is used to identify likely locations (and colors) of 3-D voxels, which can then be used to generate photo-realistic renderings. <p> In this paper, we employ a related space-sweep technique, but our algorithm handles an arbitrary number of general camera positions, works in outdoor scenes with widely varying illumination (unlike [20]), and generates a 3-D model suited for graphics rendering (unlike <ref> [5] </ref>). Image-based rendering systems [11, 17, 16] use multiple images to produce the image from a novel viewpoint using either interpolation or disparity (depth), or a combination of both. By using images directly, such systems generate visually realistic results, while partially avoiding the difficult task of 3-D reconstruction. <p> These azimuths are then verified by the space-sweep algorithm described in the next section. 3 The Space-Sweep Algorithm The space-sweep algorithm to locate and verify tiles is based on an incidence counting idea, related to that proposed by Collins <ref> [5] </ref>. If any sparse set of features in several nodes is projected into 3-D, regions with high incidence of such projections correspond to likely locations of 3-D features, for the following reasons.
Reference: [6] <author> S. Coorg, N. Master, and S. Teller. </author> <title> Acquisition of a large pose-mosaic dataset. </title> <type> Technical Report TM-568, </type> <institution> Laboratory for Computer Science, MIT, </institution> <year> 1998. </year>
Reference-contexts: Second, a global optimization algorithm based on a few semi-automatically generated correspondences between these spherical mosaics registered them in a global coordinate system, producing pose mosaics. Both these techniques are described in a separate paper <ref> [6] </ref>. 1.2 Comparison with Related Work Interactive modeling systems allow a user to identify geometric features in photographs and establish correspondences between them. The constraints generated by this process are then used to solve for 3-D structure.
Reference: [7] <author> J. D. M. McKeown, C. McGlone, S. D. Cochran, Y. C. Hsieh, M. Roux, and J. Shufelt. </author> <title> Automatic cartographic feature extraction using photogrammetric principles. </title> <booktitle> In Digital Photogrammetry, </booktitle> <pages> pages 195-212. </pages> <address> ASPRS, </address> <year> 1997. </year>
Reference-contexts: A technique similar to ours has been used in photogrammetry to generate buildings from monocular (i.e., single) aerial images <ref> [7] </ref>; the novelty of our approach is the use multiple nodes to discount non-correlated azimuths, and robust azimuth verification, which is described in the next section. 2.2 Histogramming Azimuths Our technique to identify likely azimuths is based on the following idea: 2-D edges arising from truly (i.e., world-space) horizontal line segments
Reference: [8] <author> P. E. Debevec, C. J. Taylor, and J. Malik. </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. </title> <booktitle> In SIGGRAPH 96 Conference Proceedings, </booktitle> <pages> pages 11-20, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Three-dimensional modeling of existing urban architecture has numerous applications, including virtual environments <ref> [8, 13] </ref>, urban planning, military simulation, etc. <p> An alternate approach has been to design semi-automatic modeling systems and allow a human to aid the reconstruction process (e.g., the user marks and corresponds image features). Though such systems can yield high quality results <ref> [8] </ref>, they require the user to process each image in the input dataset, and thus do not scale for large sets of images or structures. In this paper, we present a novel algorithm that automatically recovers 3-D information by analyzing a large set of images. <p> The constraints generated by this process are then used to solve for 3-D structure. The more successful systems <ref> [1, 8] </ref> exploit geometric structure inherent in the environment, such as blocks in [8] or parallel and perpendicular lines [1]. While such systems can generate high quality results, the user must process each input image, e.g., by identifying edges (for correspondence) and occluded pixels (for texture computation). <p> The constraints generated by this process are then used to solve for 3-D structure. The more successful systems [1, 8] exploit geometric structure inherent in the environment, such as blocks in <ref> [8] </ref> or parallel and perpendicular lines [1]. While such systems can generate high quality results, the user must process each input image, e.g., by identifying edges (for correspondence) and occluded pixels (for texture computation). This makes them impractical for use on large datasets, such as ours. <p> In addition, parts of the facade are occluded by various objects (e.g., trees) in most nodes, making it difficult to determine (and use) a single "best" node, or even interpolate between various nodes based on viewing direction <ref> [8] </ref>. Instead, it is necessary to combine the information present in all relevant nodes to compute a single facade texture. This is a formidable task due to the sheer size of pixel data that has to be processed, even for a human-assisted modeling system. <p> Despite this optimization, it is possible for some blurring to persist in the texture. This is caused by parts of the facade extruding out of its plane. One possible solution would be to construct disparity maps to capture such extrusions, as in <ref> [8] </ref>. 6 Results We have implemented the algorithm (and associated visualization) described in this paper in about 5000 lines of C++ code. In addition to extracting all significant vertical facades in the office complex (the primary focus of the dataset), the algorithm surprised us by automatically extracting several neighboring facades. <p> This could be corrected by clipping the model to the edges of computed textures. Also, textures could be augmented with disparity maps to model small extrusions, as in <ref> [8] </ref>. Other areas for future work include designing pose imagery-based extraction algorithms to recover more general geometry (e.g., arbitrarily oriented facades and non-facade structures), and recovering more general reflectance properties (e.g., specular coefficients, BRDFS) of the model.
Reference: [9] <author> U. R. Dhond and J. K. Aggarwal. </author> <title> Structure from stereo A review. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 19(6) </volume> <pages> 1489-1510, </pages> <address> Nov.-Dec. </address> <year> 1989. </year>
Reference-contexts: However, as mosaics themselves do not contain 3-D information (i.e., depth), the range of viewpoints is severely limited. In our work, we use mosaics to organize a large dataset to significant engineering advantage, but recover facade geometry and texture, providing the user greater navigational flexibility. Stereo vision <ref> [9] </ref> is the classic computer vision approach to recovering 3-D information from a few (usually two or three) pose-annotated images. This technique matches corresponding features (e.g., points or edges) across images, and locates 3-D features by triangulation.
Reference: [10] <author> J. D. Foley, A. van Dam, S. K. Feiner, and J. F. Hughes. </author> <title> Computer Graphics, Principles and Practice, Second Edition. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: As raw RGB color of the same texel can vary significantly in each node, we use a color space where at least some of the components are stable under different lighting conditions. In our implementation, we use the CIE xyY color representation <ref> [10] </ref>, which decouples chromaticity x; y from luminance Y . Under illumination by (predominantly) white sunlight, the luminance Y of a texel can vary significantly, but its chromaticity remains reasonably stable.
Reference: [11] <author> S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen. </author> <booktitle> The lumigraph. In SIGGRAPH 96 Conference Proceedings, </booktitle> <pages> pages 43-54, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: In this paper, we employ a related space-sweep technique, but our algorithm handles an arbitrary number of general camera positions, works in outdoor scenes with widely varying illumination (unlike [20]), and generates a 3-D model suited for graphics rendering (unlike [5]). Image-based rendering systems <ref> [11, 17, 16] </ref> use multiple images to produce the image from a novel viewpoint using either interpolation or disparity (depth), or a combination of both. By using images directly, such systems generate visually realistic results, while partially avoiding the difficult task of 3-D reconstruction.
Reference: [12] <author> P. J. Huber. </author> <title> Robust Statistics. </title> <publisher> Wiley, </publisher> <year> 1981. </year>
Reference-contexts: We use the median to compute a single x and y value for each texel from observed x and y values in various nodes. The median possesses useful robustness properties, i.e., it is less sensitive to outliers than is simple averaging <ref> [12] </ref>. This is important in our application as outliers are fairly common, typically arising from occlusion by foliage or other structures.
Reference: [13] <author> W. Jepson, R. Liggett, and S. Friedman. </author> <title> An environment for real-time urban simulation. </title> <booktitle> In Proc. 1995 Symposium on Interactive 3D Graphics, </booktitle> <pages> pages 165-166, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Three-dimensional modeling of existing urban architecture has numerous applications, including virtual environments <ref> [8, 13] </ref>, urban planning, military simulation, etc.
Reference: [14] <author> S. B. Kang and R. Szeliski. </author> <title> 3-D scene recovery using omnidirectional multibaseline stereo. </title> <booktitle> In International Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 364-370, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: A drawback of this technique is the inherent trade-off between the inter-camera distance (baseline) and ease of matching: larger baselines allow more stable triangulation, and thus higher quality 3-D models, but matching across images from widely separated cameras is an extremely hard problem. Multi-baseline stereo (e.g., <ref> [14] </ref>) attempts to address this drawback by using several images simultaneously. However, in order to perform automatic matching, such algorithms require images to be taken from closely-spaced cameras under stable illumination conditions. Such conditions would be difficult, if not impossible, to achieve in extended outdoor environments.
Reference: [15] <author> R. Kumar, P. Anandan, and K. Hanna. </author> <title> Shape recovery from multiple views: a parallax based approach. </title> <booktitle> In ARPA Image Understanding Workshop, </booktitle> <address> Monterey, CA, </address> <month> Nov. </month> <year> 1994. </year> <month> 15 </month>
Reference-contexts: This is important in our application as outliers are fairly common, typically arising from occlusion by foliage or other structures. Fortunately, such outliers do not appear at the same texel on different rectified nodes (i.e., they exhibit parallax <ref> [15] </ref>), allowing the median to generate a good estimate of the texel's chromaticity. We also use the median to select a single luminance value for the texel.
Reference: [16] <author> M. Levoy and P. Hanrahan. </author> <title> Light field rendering. </title> <booktitle> In SIGGRAPH 96 Conference Proceedings, </booktitle> <pages> pages 31-42, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: In this paper, we employ a related space-sweep technique, but our algorithm handles an arbitrary number of general camera positions, works in outdoor scenes with widely varying illumination (unlike [20]), and generates a 3-D model suited for graphics rendering (unlike [5]). Image-based rendering systems <ref> [11, 17, 16] </ref> use multiple images to produce the image from a novel viewpoint using either interpolation or disparity (depth), or a combination of both. By using images directly, such systems generate visually realistic results, while partially avoiding the difficult task of 3-D reconstruction.
Reference: [17] <author> L. McMillan and G. Bishop. </author> <title> Plenoptic modeling: An image-based rendering system. </title> <booktitle> In SIGGRAPH 95 Conference Proceedings, </booktitle> <pages> pages 39-46, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: In this paper, we employ a related space-sweep technique, but our algorithm handles an arbitrary number of general camera positions, works in outdoor scenes with widely varying illumination (unlike [20]), and generates a 3-D model suited for graphics rendering (unlike [5]). Image-based rendering systems <ref> [11, 17, 16] </ref> use multiple images to produce the image from a novel viewpoint using either interpolation or disparity (depth), or a combination of both. By using images directly, such systems generate visually realistic results, while partially avoiding the difficult task of 3-D reconstruction.
Reference: [18] <author> F. P. Preparata and M. I. Shamos. </author> <title> Computational Geometry: an Introduction. </title> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: Third, the correlation function can be evaluated efficiently by exploiting the rectangular geometric structure inherent in Equation 2. The evaluation technique is a straightforward modification of a segment-tree based plane-sweep algorithm that computes the total area of m rectangles in optimal O (m log m) time <ref> [18] </ref>. 3.2 Maxima Location and Tile Generation Given a grid cell C, the azimuth of a tile, and a set of nodes 1 : : : k, the space-sweep algorithm locates tiles as follows: /* Project edges from each node onto canonical planes */ for node i 2 1 : :
Reference: [19] <author> Y. Sato, M. D. Wheeler, and K. </author> <title> Ikeuchi. Object shape and reflectance modeling from observation. </title> <booktitle> In SIGGRAPH 97 Conference Proceedings, </booktitle> <pages> pages 379-388, </pages> <month> Aug. </month> <year> 1997. </year>
Reference-contexts: One possible automatic approach would be to fit the the various observations onto a standard reflectance model and recover coefficients of the model. An example of this technique is <ref> [19] </ref>, where the 11 authors estimate specular and diffuse coefficients of a small object from many images. However, such al-gorithms tend to be less effective when there is significant occlusion and illumination variation.
Reference: [20] <author> S. Seitz and C. Dyer. </author> <title> Photorealistic scene reconstruction by voxel coloring. </title> <booktitle> In CVPR97, </booktitle> <pages> pages 1067-1073, </pages> <year> 1997. </year>
Reference-contexts: However, in order to perform automatic matching, such algorithms require images to be taken from closely-spaced cameras under stable illumination conditions. Such conditions would be difficult, if not impossible, to achieve in extended outdoor environments. Space-sweep techniques have been used recently <ref> [5, 20] </ref> to perform matching and reconstruction from an arbitrary number of images. Unlike traditional image-space algorithms that rely on correspondence between image features, these world-space algorithms traverse the entire 3-D region of interest and identify likely locations of 3-D features. <p> In [5], a counting metric based on edge pixels is used to recover 3-D information from a few (seven) aerial images. In <ref> [20] </ref>, a correlation-based metric using pixel colors is used to identify likely locations (and colors) of 3-D voxels, which can then be used to generate photo-realistic renderings. <p> In this paper, we employ a related space-sweep technique, but our algorithm handles an arbitrary number of general camera positions, works in outdoor scenes with widely varying illumination (unlike <ref> [20] </ref>), and generates a 3-D model suited for graphics rendering (unlike [5]). Image-based rendering systems [11, 17, 16] use multiple images to produce the image from a novel viewpoint using either interpolation or disparity (depth), or a combination of both. <p> For example, in Figure 7, if the ordering favors either B, C, or D over A, it would preclude at least one of A's observations from supporting it, resulting in A's removal. 10 We considered two possible facade orderings: 1. Order facades according to occlusion, similar to <ref> [20] </ref>. That is, if A and B are two facades such that A &lt; B in the ordering, only A can occlude B from any node, not vice-versa. Such an ordering exists for facades with the same normal in Figure 7, it is simply the ordering opposite N.
Reference: [21] <author> R. Szeliski. </author> <title> Video mosaics for virtual environments. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 16(2) </volume> <pages> 22-30, </pages> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: This makes them impractical for use on large datasets, such as ours. Such systems are also difficult to assess algorithmically, as a "human-in-the-loop" is performing non-algorithmic tasks. Mosaicing <ref> [21, 22] </ref> seamlessly stitches together multiple images taken from the same viewpoint. As in QuickTime VR [4], such mosaics can be used for walkthroughs of virtual environments. However, as mosaics themselves do not contain 3-D information (i.e., depth), the range of viewpoints is severely limited. <p> The first step rewarps each rectified node to achieve a higher correlation with the median texture. The second step recomputes the median using the new values. These steps are repeated until the median values converge. Our rewarping technique is a correlation-optimization based on luminance values, identical to mo-saicing optimization <ref> [21, 22] </ref>. Briefly, the optimization uses a 8-parameter, 2-D projective transformation described by a 3 fi 3 matrix M: u = M 00 x+M 01 y+M 02 M 20 x+M 21 y+1 In our algorithm, u; v are texture coordinates, and x; y coordinatize the rectified node.
Reference: [22] <author> R. Szeliski and H. Shum. </author> <title> Creating full-view panoramic mosaics and texture-mapped 3D models. </title> <booktitle> In SIGGRAPH 97 Conference Proceedings, </booktitle> <pages> pages 251-258, </pages> <month> Aug. </month> <year> 1997. </year>
Reference-contexts: This makes them impractical for use on large datasets, such as ours. Such systems are also difficult to assess algorithmically, as a "human-in-the-loop" is performing non-algorithmic tasks. Mosaicing <ref> [21, 22] </ref> seamlessly stitches together multiple images taken from the same viewpoint. As in QuickTime VR [4], such mosaics can be used for walkthroughs of virtual environments. However, as mosaics themselves do not contain 3-D information (i.e., depth), the range of viewpoints is severely limited. <p> The first step rewarps each rectified node to achieve a higher correlation with the median texture. The second step recomputes the median using the new values. These steps are repeated until the median values converge. Our rewarping technique is a correlation-optimization based on luminance values, identical to mo-saicing optimization <ref> [21, 22] </ref>. Briefly, the optimization uses a 8-parameter, 2-D projective transformation described by a 3 fi 3 matrix M: u = M 00 x+M 01 y+M 02 M 20 x+M 21 y+1 In our algorithm, u; v are texture coordinates, and x; y coordinatize the rectified node.
Reference: [23] <author> A. P. Witkin. </author> <title> Recovering Surface Shape and Orientation from Texture. </title> <journal> Artificial Intelligence, </journal> <volume> 17(1-3):17-45, </volume> <month> Aug. </month> <year> 1981. </year> <title> 16 edges. Vertical edges are colored blue, and other edges are colored with (absolute values of) the tile normal derived from their azimuth (e.g., red is [1; 0; 0] T , green is [0; 1; 0] T , etc.). differences in illumination, and the effects of shadows, reflections, and occlusion. 17 (a) (b) Note the removal of occlusion, shadows, luminance variations, etc. from the texture. (a) (b) (c) small facades, and Part (c) shows the horizontal lines that generate this model. </title> <journal> spheres). </journal> <volume> 18 19 </volume>
Reference-contexts: is not true of azimuths computed from non-horizontal edges (e.g., tree edges); thus they tend to be uncorrelated both within a node and across different nodes. 5 Unlike computer vision techniques that recover 3-D orientation information from aggregate properties of a set of edges (e.g., moments in shape from texture <ref> [23] </ref> and common intersection points in vanishing point estimation [2]), this technique recovers 3-D orientation from a single (presumed horizontal) edge.
References-found: 23

