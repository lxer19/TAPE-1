URL: http://www.cs.rice.edu/~cormac/icfp96.ps
Refering-URL: http://www.cs.umd.edu/~keleher/818.html
Root-URL: 
Email: cormac@cs.rice.edu  nikhil@crl.dec.com  
Title: pHluid The Design of a Parallel Functional Language Implementation on Workstations  
Author: Cormac Flanagan Rishiyur S. Nikhil 
Keyword: parallel and distributed implementations; garbage collection and run-time systems; data flow.  
Address: Houston, Texas 77251-1892, USA  One Kendall Square, Bldg. 700 Cambridge, Massachusetts 02139, USA  
Affiliation: Rice University Department of Computer Science  Digital Equipment Corp. Cambridge Research Laboratory  
Abstract: This paper describes the distributed memory implementation of a shared memory parallel functional language. The language is Id, an implicitly parallel, mostly functional language that is currently evolving into a dialect of Haskell. The target is a distributed memory machine, because we expect these to be the most widely available parallel platforms in the future. The difficult problem is to bridge the gap between the shared memory language model and the distributed memory machine model. The language model assumes that all data is uniformly accessible, whereas the machine has a severe memory hierarchy: a processor's access to remote memory (using explicit communication) is orders of magnitude slower than its access to local memory. Thus, avoiding communication is crucial for good performance. The Id language, and its general dataflow-inspired compilation to multithreaded code are described elsewhere. In this paper, we focus on our new parallel runtime system and its features for avoiding communication and for tolerating its latency when necessary: multithreading, scheduling and load balancing; the distributed heap model and distributed coherent cacheing, and parallel garbage collection. We have completed the first implementation, and we present some preliminary performance measurements. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Aditya, S., Arvind, Augustsson, L., Maessen, J.-W., and Nikhil, R. S. </author> <title> Semantics of pH: A Parallel Dialect of Haskell. </title> <booktitle> In in Proc. Haskell Workshop (at FPCA 95), </booktitle> <address> La Jolla, CA (June 1995). </address>
Reference-contexts: 1 Introduction This paper describes the distributed memory implementation of a shared memory parallel functional language. The language is Id [19], an implicitly parallel, mostly functional language designed in the dataflow group at MIT. Id is semantically similar to Haskell [14] and is in fact currently evolving into pH <ref> [1] </ref>, a dialect of Haskell. <p> Recognizing this semantic similarity, and because of various other syntactic similarities, Id is currently evolving into pH <ref> [1] </ref>, a dialect of Haskell. code for latency tolerance- we do not start with a compiler for sequential machines and add parallelism as an afterthought. a binary tree: def leaves Empty = 1 | leaves (Node x l r) = leaves l + leaves r; It is displayed here in graphical <p> The if tests if the tree is empty; if so, the result is 1, and the return instruction sends a message to the continuation (cfp, cip) carrying the result. * If the test fails, we initiate two "heap I-structure loads": the hiloads conceptually send messages to the heap locations T <ref> [1] </ref> and T [2] requesting their contents, passing the current frame pointer (implicitly) and the labels a: and b: respectively (explicitly) as their respective continuations. The current function invocation then goes dormant, with no threads active. * The heap location T [1] eventually responds with a message, kicking off the thread <p> hiloads conceptually send messages to the heap locations T <ref> [1] </ref> and T [2] requesting their contents, passing the current frame pointer (implicitly) and the labels a: and b: respectively (explicitly) as their respective continuations. The current function invocation then goes dormant, with no threads active. * The heap location T [1] eventually responds with a message, kicking off the thread at a:, placing the value (the left sub-tree) into local variable L. <p> For example, consider the following code fragment describing two threads, starting at labels a: and b:, respectively: a: ... b: ... hiload b T <ref> [1] </ref> ... foo ... the thread at a: executes an hiload instruction to ini-tiate access to a heap location, and continues (at foo). Conceptually, it sends a message to the heap location, which eventually produces a response message that enables the other thread at b:.
Reference: [2] <author> Agarwal, A., Simoni, R., Hennessy, J., and Horowitz, M. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proc. 15th. Ann. Intl. Symp. on Computer Architecture, </booktitle> <address> Hawaii (May 1988). </address>
Reference-contexts: if the tree is empty; if so, the result is 1, and the return instruction sends a message to the continuation (cfp, cip) carrying the result. * If the test fails, we initiate two "heap I-structure loads": the hiloads conceptually send messages to the heap locations T [1] and T <ref> [2] </ref> requesting their contents, passing the current frame pointer (implicitly) and the labels a: and b: respectively (explicitly) as their respective continuations. <p> A general observation here is that our distributed cache-coherence protocol relies heavily on the mostly-functional nature of the source language; the protocol is trivial compared to those required for imperative languages <ref> [2] </ref>. We mentioned that Id is not a pure functional language it has side-effecting constructs that operate on "M-structures".
Reference: [3] <author> Appel, A. </author> <title> Garbage Collection can be Faster than Stack Allocation. </title> <journal> Information Processing Letters 25, </journal> <volume> 4 (1987), </volume> <pages> 275-279. </pages>
Reference-contexts: In a previous sequential implementation, and in an initial parallel implementation, we allocated everything in a single heap (as in the SML/NJ implementation <ref> [4, 3] </ref>). However, we chose to separate out those objects that can be explicitly deallocated and which are not accessed remotely, in order to reduce garbage collection pressure.
Reference: [4] <author> Appel, A., and MacQueen, D. B. </author> <title> A Standard ML Compiler. </title> <booktitle> In Proc. Conf. on Functional Programming and Computer Architecture, </booktitle> <address> Portland, Oregon (September 1987). </address> <publisher> Springer-Verlag LNCS 274. </publisher>
Reference-contexts: In a previous sequential implementation, and in an initial parallel implementation, we allocated everything in a single heap (as in the SML/NJ implementation <ref> [4, 3] </ref>). However, we chose to separate out those objects that can be explicitly deallocated and which are not accessed remotely, in order to reduce garbage collection pressure.
Reference: [5] <author> Armstrong, J., Virding, R., and Williams, M. </author> <title> Concurrent Programming in Erlang. </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year> <note> ISBN: 0-13-285792-8. </note>
Reference-contexts: There have of course been several implementations on shared memory machines (such as [16] and [7]), and implementations of message-passing functional languages on distributed memory machines (such as Erlang <ref> [5] </ref>). However, implementing a shared memory language on a distributed memory machine appears to require a substantially different approach, with latency-tolerance a high priority throughout the design.
Reference: [6] <author> Arvind, Heller, S., and Nikhil, R. S. </author> <title> Programming Generality and Parallel Computers. </title> <publisher> ESCOM Science Publishers, </publisher> <address> P.O.Box 214, 2300 AE Leiden, The Netherlands, </address> <year> 1988, </year> <pages> pp. 255-286. </pages> <booktitle> Proc. 4th Intl. Symp. on Biological and Artificial Intelligence Systems, </booktitle> <address> Trento, Italy, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: The program we used does a lot of heap allocation: paraffins (18) enumerates all paraffin molecules containing up to 18 carbon atoms, unique up to certain symmetry conditions- the Id program is described in detail in <ref> [6] </ref>. The C version did not have any garbage collection (it simply had a large enough heap).
Reference: [7] <author> Augustsson, L., and Johnsson, T. </author> <title> Parallel Graph Reduction with the &lt;nu,G&gt;-machine. </title> <booktitle> In Proc. Fourth Intl. Conf. on Functional Programming Languages and Computer Architecture, </booktitle> <month> Lon-don (September </month> <year> 1989), </year> <pages> pp. 202-213. </pages>
Reference-contexts: There have of course been several implementations on shared memory machines (such as [16] and <ref> [7] </ref>), and implementations of message-passing functional languages on distributed memory machines (such as Erlang [5]). However, implementing a shared memory language on a distributed memory machine appears to require a substantially different approach, with latency-tolerance a high priority throughout the design.
Reference: [8] <author> Blumofe, R. D., Joerg, C. F., Kuszmaul, B. C., Leiserson, C. E., Randall, K. H., and Zhou, Y. Cilk: </author> <title> An Efficient Multithreaded Runtime System. </title> <booktitle> In Proc. 5th. ACM Symp. on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> Santa Barbara, CA (July 19-21 1995), </address> <pages> pp. 207-216. </pages>
Reference-contexts: by a halt instruction, we avoid the overhead of manipulating the stealable queue by simply placing the arguments into appropriate registers and invoking the fast entry point (described above) of that function's codeblock. 3 Our work-stealing algorithm is a variant of that used in the Cilk system developed at MIT <ref> [8] </ref>. Our algorithm was developed jointly with Martin Carlisle of Princeton University. 3.2 Distributed memory model and distributed coherent cacheing The memory of each PE is divided into five regions: * Compiled code and static data: Static data includes data structures that describe codeblocks and the other memory areas, etc.
Reference: [9] <author> Cheney, C. J. </author> <title> A Nonrecursive List Compacting Algorithm. </title> <journal> Communications of the ACM 13, </journal> <month> 11 (November </month> <year> 1970), </year> <pages> 677-678. </pages>
Reference-contexts: Whenever any PE runs out of heap space, all the processors suspend their current activities to participate in a global garbage collection cycle. The garbage collector is a parallel extension of the conventional Cheney two-space copying collector <ref> [9] </ref>. The first action taken by each PE during garbage collection is to deallocate the cache pages (using munmap), since these pages will become inconsistent during garbage collection. Special care must be taken with hiload requests queued on a cache page.
Reference: [10] <author> Culler, D. E., Goldstein, S. C., Schauser, K. E., and von Eicken, T. v. </author> <title> TAM A Compiler Controlled Threaded Abstract Machine. </title> <journal> J. Parallel and Distributed Computing, </journal> <note> Special Issue on Dataflow 18 (June 1993). </note>
Reference-contexts: However, implementing a shared memory language on a distributed memory machine appears to require a substantially different approach, with latency-tolerance a high priority throughout the design. The Berkeley Id/TAM compiler <ref> [10] </ref> shares the same dataflow heritage as our pHluid compiler and, not surprisingly, has many similarities (although they share no code).
Reference: [11] <editor> Gaudiot, J.-L., and Bic (editors), L. </editor> <booktitle> Advanced Topics in Data-flow Computing. </booktitle> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: The pHluid system is a compiler and runtime system for the Id language that we have been building at Digital's Cambridge Research Laboratory for some years. Based on ideas originating in dataflow architectures 1 <ref> [11] </ref> the compiler produces multithreaded code for conventional machines.
Reference: [12] <author> Halstead, R. </author> <title> Multilisp: A language for concurrent symbolic computataion. </title> <journal> ACM Trans. Program. Lang. Syst. </journal> <volume> 7, 4 (1985), </volume> <pages> 501-538. </pages>
Reference-contexts: The reason for this approach is that items deeper in the stack are likely to represent fatter chunks of work, being higher in the call tree <ref> [12, 16] </ref>. The net effect of our work-stealing algorithm is that we do not gratuitously fork functions to other PEs.
Reference: [13] <author> Hartel, Pieter, H., Feeley, M., Alt, M., Au-gustsson, L., Baumann, P., Beemster, M., Chailloux, E., Flood, C. H., Grieskamp, W., van Groningen, J. H. G., Hammond, K., Haus-man, B., Ivory, M. Y., Jones, R. E., Lee, P., Leroy, X., Lins, R. D., Loosemore, S., Ro-jemo, N., Serrano, M., Talpin, J.-P., Thack-ray, J., Thomas, S., Weis, P., and Went-worth, E. P. </author> <title> Benchmarking Implementations of Functional Languages with "Pseudoknot", a Float-Intensive Benchmark. </title> <booktitle> In Workshop on Implementation of Functional Languages, </booktitle> <editor> J. R. W. Glauert (editor), </editor> <booktitle> School of Information Systems, </booktitle> <institution> Univ. </institution> <note> of East Anglia (September 1994). </note>
Reference-contexts: once using sbrk () at the start of the program. paraffins (18) Time (secs) Relative speed pHluid 0.53 1x C (malloc) 0.40 1.3x C (sbrk) 0.20 2.6x We believe that these indicate that pHluid's uniprocessor performance for Id programs is approaching competitive performance for functional language implementations (the study in <ref> [13] </ref> measures the performance of another program, the pseudoknot benchmark. It compares a functional version compiled with over 25 functional language compilers, and a C version. The best functional versions were about 1.4x, 1.5x, 1.9x and 2.0x slower than C).
Reference: [14] <author> Hudak, P., Peyton Jones, S., Wadler, P., Boutel, B., Fairbairn, J., Fasel, J., Guzman, M. M., Hammond, K., Hughes, J., Johnsson, T., Kieburtz, R., Nikhil, R., Partain, W., and Peterson, J. </author> <title> Report on the Programming Language Haskell, A Non-strict, Purely Functional Language, Version 1.2. </title> <journal> ACM SIGPLAN Notices 27, </journal> <month> 5 (May </month> <year> 1992). </year>
Reference-contexts: 1 Introduction This paper describes the distributed memory implementation of a shared memory parallel functional language. The language is Id [19], an implicitly parallel, mostly functional language designed in the dataflow group at MIT. Id is semantically similar to Haskell <ref> [14] </ref> and is in fact currently evolving into pH [1], a dialect of Haskell.
Reference: [15] <institution> Kendall Square Research. Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: Larger shared memory machines are of course possible, as demonstrated by the Stanford DASH multiprocessor [17], and the KSR machines <ref> [15] </ref>, but they are likely to be high-end machines and not widely available. Further, scalable shared memory machines (like the DASH) are also built with physically distributed memories for scalability, and face some similar problems with their memory hierarchies.
Reference: [16] <author> Kranz, D. A., Halstead Jr., R. H., and Mohr, E. Mul-T: </author> <title> A High Performance Parallel Lisp. </title> <booktitle> In Proc. ACM Symp. on Programming Language Design and Implementation, </booktitle> <address> Portland, Oregon (June 1989). </address>
Reference-contexts: The reason for this approach is that items deeper in the stack are likely to represent fatter chunks of work, being higher in the call tree <ref> [12, 16] </ref>. The net effect of our work-stealing algorithm is that we do not gratuitously fork functions to other PEs. <p> There have of course been several implementations on shared memory machines (such as <ref> [16] </ref> and [7]), and implementations of message-passing functional languages on distributed memory machines (such as Erlang [5]). However, implementing a shared memory language on a distributed memory machine appears to require a substantially different approach, with latency-tolerance a high priority throughout the design.
Reference: [17] <author> Lenoski, D., Laudon, J., Gharachorloo, K., Weber, W.-D., Gupta, A., Hennessy, J., Horowitz, M., and Lam, M. S. a. </author> <title> The Stanford DASH Multiprocessor. </title> <booktitle> IEEE Computer (March 1992), </booktitle> <pages> 63-79. </pages>
Reference-contexts: Larger shared memory machines are of course possible, as demonstrated by the Stanford DASH multiprocessor <ref> [17] </ref>, and the KSR machines [15], but they are likely to be high-end machines and not widely available. Further, scalable shared memory machines (like the DASH) are also built with physically distributed memories for scalability, and face some similar problems with their memory hierarchies.
Reference: [18] <author> Nikhil, R. S. </author> <title> A Multithreaded Implementation of Id using P-RISC Graphs. </title> <booktitle> In Proc. 6th. Ann. Wk-shp. on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, Oregon, </address> <note> Springer-Verlag LNCS 768 (August 12-14 1993), pp. 390-405. </note>
Reference-contexts: However, note that all these sources of delay are handled uniformly by couching the code as multiple, fine grain, message-driven threads, each of which never suspends. All this was by way of background, and is discussed in more detail in <ref> [18] </ref>. The important points to remember for purposes of this paper are: * Parallelism in pHluid is at two levels: the function call is the unit of work distribution across processors. This is real parallelism, in that these processors actually execute simultaneously.
Reference: [19] <author> Nikhil, R. S. </author> <title> An Overview of the Parallel Language Id (a foundation for pH, a parallel dialect of Haskell). </title> <type> Tech. Rep. Draft, </type> <institution> Digital Equipment Corp., Cambridge Research Laboratory, </institution> <month> September 23 </month> <year> 1993. </year>
Reference-contexts: 1 Introduction This paper describes the distributed memory implementation of a shared memory parallel functional language. The language is Id <ref> [19] </ref>, an implicitly parallel, mostly functional language designed in the dataflow group at MIT. Id is semantically similar to Haskell [14] and is in fact currently evolving into pH [1], a dialect of Haskell. <p> Readers familiar with topics such as Id, Id compilation, dataflow, message driven execution, fine grain multithreading, etc. may wish to skip this section and just read the last paragraph.] Id <ref> [19] </ref> is an implicitly parallel, mostly functional, language designed in the dataflow group at MIT. It has many features common to other modern functional languages like Haskell and SML- higher-order functions, a Hindley-Milner polymorphic type system with user-defined algebraic types, pattern-matching notation, array and list comprehensions, etc. <p> The main novelty of Id is its implicitly parallel evaluation model: everything is evaluated eagerly, except for expressions inside conditionals and inside lambdas. This is described in more detail in <ref> [19] </ref>, but a key behavior relevant to this paper is that most data structures have I-structure semantics: given an expression of the form: cons e1 e2 we allocate the data structure and evaluate e1 and e2 in parallel.
Reference: [20] <author> Spertus, E., and Dally, W. J. </author> <title> Evaluating the Locality Benefits of Active Messages. </title> <booktitle> In Proc. 5th. ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> Santa Barbara, CA (July 19-21 1995), </address> <pages> pp. 189-198. </pages> <note> [21] von Eicken, </note> <author> T., Culler, D. E., Goldstein, S. C., and Schauser, K. E. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. 19th. Ann. Intl. Symp. on Computer Architecture, </booktitle> <address> Gold Coast, Australia (May 1992), </address> <pages> pp. 256-266. </pages>
Reference-contexts: TAM itself (Threaded Abstract Machine) is somewhat similar to our P-RISC Assembler, but our scheduling discipline for threads is quite different (these two scheduling disciplines have been compared in detail by Ellen Spertus at MIT, and is reported in <ref> [20] </ref>). The Berkeley Id-on-TAM system has been implemented on distributed memory machines with relatively fast communication facilities, such as the Connection Machine CM-5 and the MIT J-Machine, but not to our knowledge on workstation farms.
References-found: 20

