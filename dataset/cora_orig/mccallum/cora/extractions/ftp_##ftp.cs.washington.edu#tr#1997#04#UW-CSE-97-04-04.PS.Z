URL: ftp://ftp.cs.washington.edu/tr/1997/04/UW-CSE-97-04-04.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: A Compiler Abstraction for Machine Independent Parallel Communication Generation  
Author: Bradford L. Chamberlain Sung-Eun Choi Lawrence Snyder 
Note: This research was supported by DARPA Grant N00014-92-J-1824, AFOSR Grant E30602-97-1-0152, and a grant of HPC time from the Arctic Region Supercomputing Center.  
Address: Box 352350 Seattle, WA 98195-2350  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: In this paper, we consider the problem of generating efficient, portable communication in compilers for parallel languages. We introduce the Ironman abstraction, which separates data transfer from its implementing communication paradigm. This is done by annotating the compiler-generated code with legal ranges for data transfer in the form of calls to the Ironman library. On each target platform, these library calls are instantiated to perform the transfer using the machine's optimal communication paradigm. We confirm arguments against generating message passing calls in the compiler based on our experiences using PVM and MPI | specifically, the observation that these interfaces do not perform well on machines that are not built with a message passing communication paradigm. The overhead for using Ironman, as opposed to a machine-specific back end, is demonstrated to be negligible. We give performance results for a number of benchmarks running with PVM, MPI, and machine-specific implementations of the Ironman abstraction, yielding performance improvements of up to 42% of com munication time and 1-14% of total computation time. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <institution> The Power Challenge. </institution> <type> Technical report, </type> <institution> Silicon Graphics, Inc., </institution> <year> 1995. </year>
Reference-contexts: Ironman calls are implemented using the optimal communication mechanism of the target machine, be it message passing (SP2 [3]), put- and get-based shared memory operations (T3D [15]), or cache-coherent assignment (SGI PowerChallenge <ref> [1] </ref>), and, like message passing libraries, the calls are made available as a custom library on each platform. Compilation with this library achieves data transfer specialized to the machine rather than forcing it into the one-size-fits-all paradigm of message passing.
Reference: 2. <author> Sarita V. Adve and Kouroush Gharachorloo. </author> <title> Shared memory consistency models: A tutorial. </title> <type> Technical Report 95/7, </type> <institution> Digital Western Research Laboratory, </institution> <year> 1995. </year>
Reference-contexts: To this end, Ironman's paradigm-neutral approach allows the compiler writer to use Active Messages when it is available without relying on its presence on every platform. Consistency models have been used to express how a shared memory changes state <ref> [2, 5, 18] </ref>. The consistency model is the mechanism by which the programmer and the compiler agree on when memory updates take place, and as such is a source language rather than a compiler concept like Ironman.
Reference: 3. <author> T. Agerwala, J. L. Martin, J.H. Mirza, D.C. Sadler, D.M Dias, and M. Snir. </author> <title> SP2 system architecture. </title> <journal> IBM System Journal, </journal> <volume> 34(2) </volume> <pages> 152-184, </pages> <year> 1995. </year>
Reference-contexts: Ironman calls are implemented using the optimal communication mechanism of the target machine, be it message passing (SP2 <ref> [3] </ref>), put- and get-based shared memory operations (T3D [15]), or cache-coherent assignment (SGI PowerChallenge [1]), and, like message passing libraries, the calls are made available as a custom library on each platform.
Reference: 4. <author> Ray Barriuso and Allan Knies. </author> <title> SHMEM user's guide for C. </title> <type> Technical report, </type> <institution> Cray Research Inc., </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: On the Paragon, we use the MPICH [14] implementation of MPI and the native NX communication library routines. On the T3D we use a vendor-optimized version of PVM [16], CRI/EPCC MPI [6], and the native SHMEM <ref> [4] </ref> library routines. All benchmark programs were run on dedicated partitions. Measured deviations were always below 1% and therefore will not be reported. All timings were taken using the machines' native timers.
Reference: 5. <author> Brian N. Bershad, Matt J. Zekausaka, and Wayne A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In CompCon Conference, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: To this end, Ironman's paradigm-neutral approach allows the compiler writer to use Active Messages when it is available without relying on its presence on every platform. Consistency models have been used to express how a shared memory changes state <ref> [2, 5, 18] </ref>. The consistency model is the mechanism by which the programmer and the compiler agree on when memory updates take place, and as such is a source language rather than a compiler concept like Ironman.
Reference: 6. <author> Kenneth Cameron, Lyndon J. Clarke, and A. Gordon Smith. </author> <title> CRI/EPCC MPI for CRAY T3D. </title> <booktitle> In 1st European Cray T3D Workshop, </booktitle> <month> September </month> <year> 1995. </year>
Reference-contexts: On the Paragon, we use the MPICH [14] implementation of MPI and the native NX communication library routines. On the T3D we use a vendor-optimized version of PVM [16], CRI/EPCC MPI <ref> [6] </ref>, and the native SHMEM [4] library routines. All benchmark programs were run on dedicated partitions. Measured deviations were always below 1% and therefore will not be reported. All timings were taken using the machines' native timers. <p> Some effort has been made to implement highly-tuned versions of the MPI and PVM libraries for platforms that do not inherently support send/receive-based message-passing, such as the Cray T3D <ref> [16, 6] </ref>. However, such tuning does not remove the existence of the paradigm skew and results in performance that falls short of optimal for that platform, as we demonstrate in our experiments.
Reference: 7. <author> L. F. Cannon. </author> <title> A Cellular Computer to Implement the Kalman Filter Algorithm. </title> <type> PhD thesis, </type> <institution> Montana State University, </institution> <year> 1969. </year>
Reference-contexts: To do this, we performed experiments using a set of benchmark programs that use kernel computations and communication patterns that are commonly found in large-scale scientific programs (see Figure 7). Cannon's Algorithm for Matrix Multiplication (Cannon). Cannon's algorithm <ref> [7] </ref> is a systolic approach to matrix multiplication. As an initialization step, it uses cyclic shifts to skew the operand matrices. The result matrix is then computed by repeatedly multiplying elements in a pointwise manner and performing cyclic shifts, causing corresponding elements to flow past one another. Jacobi Iterations (Jacobi).
Reference: 8. <author> Bradford L. Chamberlain, Sung-Eun Choi, E Christopher Lewis, Calvin Lin, Lawrence Snyder, and W. Derrick Weathersby. Factor-Join: </author> <title> A unique approach to compiling array languages for parallel machines. </title> <booktitle> In Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: Compilation with this library achieves data transfer specialized to the machine rather than forcing it into the one-size-fits-all paradigm of message passing. We have used the Ironman abstraction in the implementation of our ZPL compiler and runtime system <ref> [8] </ref>, although the principle is general and applicable to any compiler for a parallel machine. In addition to describing the Ironman concepts, this paper reports on our experience using Ironman and presents performance measurements based on ZPL programs with various instantiations of the Ironman primitives.
Reference: 9. <author> Sung-Eun Choi and Lawrence Snyder. </author> <title> Quantifying the effect of communication optimizations. </title> <booktitle> to appear in International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: Our ZPL compiler generates ANSI C code annotated with Ironman calls to indicate the required data transfers. Point-to-point communications are optimized by the compiler using message vectorization, the removal of redundant communications, and the overlapping of computation and communication <ref> [9] </ref>. Note that Ironman calls are no harder to insert and optimize than asynchronous message passing calls, since both rely on similar analysis of Modify/Use information.
Reference: 10. <author> Intel Corporation. </author> <title> Paragon User's Guide. </title> <year> 1993. </year>
Reference-contexts: For clarity, we will keep the two roles distinct in our discussion. It is also important to note that some parallel machines provide more than one communication mechanism. For example, the NX libraries on the Intel Paragon <ref> [10] </ref> provide a blocking send and receive, csend and crecv, as well as the asynchronous analog, isend and irecv. <p> Finally, we evaluate five benchmark programs that use the Ironman interface with the goal of evaluating the benefits of Ironman in applications. 3.1 Methodology Experiments were run on two platforms: the Intel Paragon <ref> [10] </ref> and the Cray T3D [15] (see Figure 4). On the Paragon, we use the MPICH [14] implementation of MPI and the native NX communication library routines. On the T3D we use a vendor-optimized version of PVM [16], CRI/EPCC MPI [6], and the native SHMEM [4] library routines.
Reference: 11. <author> W. Crowley, C. P. Hendrickson, and T. I. Luby. </author> <title> The Simple code. </title> <type> Technical Report UCID-17715, </type> <institution> Lawrence Livermore Laboratory, </institution> <year> 1978. </year>
Reference-contexts: Every iteration, each array element is replaced by the average of its four nearest neighbors. Convergence is detected when the maximum difference between the new val ues and the old values is less than some constant. Simple Hydrodynamics (Simple). The Simple code <ref> [11] </ref> is a basic hydrodynamics simulation. Our implementation uses 8-way nearest neighbor com munication, global reductions, and a solver. Shallow Water Model (SWM). The shallow water mathematical model is a finite-differencing method used in many atmospheric and fluid computations.
Reference: 12. <author> Marios D. Dikaiakos, Calvin Lin, Daphne Manoussaki, and Diana E. Woodward. </author> <title> The portable parallel implementation of two novel mathematical biology algorithms in ZPL. </title> <booktitle> In 9 th International Conference on Supercomputing, </booktitle> <year> 1995. </year>
Reference-contexts: Our benchmarks are written in ZPL, a portable data parallel array language developed at the University of Washington. ZPL is useful for solving regular problems similar to those suitable for Fortran 90 and has been used for scientific and engineering applications <ref> [12, 19, 22, 21] </ref> as well as to implement many standard parallel benchmarks. Our ZPL compiler generates ANSI C code annotated with Ironman calls to indicate the required data transfers.
Reference: 13. <author> A. Belguelin et al. </author> <title> A user's guide to PVM. </title> <type> Technical report, </type> <institution> Oak Ridge National Laboratories, </institution> <year> 1991. </year>
Reference-contexts: For parallel language compilers this problem is perhaps most troubling in the context of communication. To be machine independent, most parallel language compilers have adopted a message passing communication abstraction implemented by general purpose libraries such as PVM <ref> [13] </ref> or MPI [20]. Though widely supported and often vendor-optimized, the message passing abstraction is a blunt instrument for producing high performance object code. Message passing has demonstrated suboptimal performance on shared address space computers like the Cray T3D [23]. <p> Even message passing machines can benefit from alternative Ironman instantiations. 4 Related Work The desire for portable high-performance data transfer has motivated a variety of communication interfaces and paradigms. Message-passing interfaces such as PVM <ref> [13] </ref> and MPI [20] were designed to provide an intuitive and portable means Fig. 9. Performance of benchmark programs on the Cray T3D. The Ironman libraries are instantiated using MPI, PVM or SHMEM.
Reference: 14. <author> William Gropp and Ewing Lusk. </author> <title> User's guide for mpich, a portable implementation of MPI. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <year> 1996. </year>
Reference-contexts: Finally, we evaluate five benchmark programs that use the Ironman interface with the goal of evaluating the benefits of Ironman in applications. 3.1 Methodology Experiments were run on two platforms: the Intel Paragon [10] and the Cray T3D [15] (see Figure 4). On the Paragon, we use the MPICH <ref> [14] </ref> implementation of MPI and the native NX communication library routines. On the T3D we use a vendor-optimized version of PVM [16], CRI/EPCC MPI [6], and the native SHMEM [4] library routines. All benchmark programs were run on dedicated partitions.
Reference: 15. <author> Cray Research Inc. </author> <title> Cray T3D System Architecture Overview Manual. </title> <institution> Mendota Heights, MN, </institution> <year> 1993. </year>
Reference-contexts: Ironman calls are implemented using the optimal communication mechanism of the target machine, be it message passing (SP2 [3]), put- and get-based shared memory operations (T3D <ref> [15] </ref>), or cache-coherent assignment (SGI PowerChallenge [1]), and, like message passing libraries, the calls are made available as a custom library on each platform. Compilation with this library achieves data transfer specialized to the machine rather than forcing it into the one-size-fits-all paradigm of message passing. <p> Finally, we evaluate five benchmark programs that use the Ironman interface with the goal of evaluating the benefits of Ironman in applications. 3.1 Methodology Experiments were run on two platforms: the Intel Paragon [10] and the Cray T3D <ref> [15] </ref> (see Figure 4). On the Paragon, we use the MPICH [14] implementation of MPI and the native NX communication library routines. On the T3D we use a vendor-optimized version of PVM [16], CRI/EPCC MPI [6], and the native SHMEM [4] library routines.
Reference: 16. <author> Cray Research Inc. </author> <title> PVM and HeNCE Programmer's Manual. </title> <institution> Mendota Heights, MN, </institution> <year> 1994. </year> <note> SR-2501 5.0. </note>
Reference-contexts: On the Paragon, we use the MPICH [14] implementation of MPI and the native NX communication library routines. On the T3D we use a vendor-optimized version of PVM <ref> [16] </ref>, CRI/EPCC MPI [6], and the native SHMEM [4] library routines. All benchmark programs were run on dedicated partitions. Measured deviations were always below 1% and therefore will not be reported. All timings were taken using the machines' native timers. <p> Some effort has been made to implement highly-tuned versions of the MPI and PVM libraries for platforms that do not inherently support send/receive-based message-passing, such as the Cray T3D <ref> [16, 6] </ref>. However, such tuning does not remove the existence of the paradigm skew and results in performance that falls short of optimal for that platform, as we demonstrate in our experiments.
Reference: 17. <author> Vijay Karamcheti and Andrew A. Chien. </author> <title> Optimizing memory system performance for communication in parallel computers. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Another attempt to reduce the cost of message passing on the T3D has been undertaken by the Illinois Fast Messages project <ref> [17] </ref>. Their approach has been to implement a message passing library by bypassing the T3D's SHMEM library and using specific hardware characteristics.
Reference: 18. <author> Peter Keleher, Alan L. Cox, and Willie Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: To this end, Ironman's paradigm-neutral approach allows the compiler writer to use Active Messages when it is available without relying on its presence on every platform. Consistency models have been used to express how a shared memory changes state <ref> [2, 5, 18] </ref>. The consistency model is the mechanism by which the programmer and the compiler agree on when memory updates take place, and as such is a source language rather than a compiler concept like Ironman.
Reference: 19. <author> E Christopher Lewis, Calvin Lin, Lawrence Snyder, and George Turkiyyah. </author> <title> A portable parallel n-body solver. </title> <editor> In D. Bailey, P. Bjorstad, J. Gilbert, M. Mascagni, R. Schreiber, H. Simon, V. Torczon, and L. Watson, editors, </editor> <booktitle> Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 331-336. </pages> <publisher> SIAM, </publisher> <year> 1995. </year>
Reference-contexts: Our benchmarks are written in ZPL, a portable data parallel array language developed at the University of Washington. ZPL is useful for solving regular problems similar to those suitable for Fortran 90 and has been used for scientific and engineering applications <ref> [12, 19, 22, 21] </ref> as well as to implement many standard parallel benchmarks. Our ZPL compiler generates ANSI C code annotated with Ironman calls to indicate the required data transfers.
Reference: 20. <author> Message Passing Interface Forum. </author> <title> MPI: A Message Passing Interface Standard. </title> <month> June </month> <year> 1995. </year>
Reference-contexts: For parallel language compilers this problem is perhaps most troubling in the context of communication. To be machine independent, most parallel language compilers have adopted a message passing communication abstraction implemented by general purpose libraries such as PVM [13] or MPI <ref> [20] </ref>. Though widely supported and often vendor-optimized, the message passing abstraction is a blunt instrument for producing high performance object code. Message passing has demonstrated suboptimal performance on shared address space computers like the Cray T3D [23]. <p> Even message passing machines can benefit from alternative Ironman instantiations. 4 Related Work The desire for portable high-performance data transfer has motivated a variety of communication interfaces and paradigms. Message-passing interfaces such as PVM [13] and MPI <ref> [20] </ref> were designed to provide an intuitive and portable means Fig. 9. Performance of benchmark programs on the Cray T3D. The Ironman libraries are instantiated using MPI, PVM or SHMEM.
Reference: 21. <author> Wilkey Richardson, Mary Bailey, and William H. Sanders. </author> <title> Using ZPL to develop a parallel Chaos router simulator. </title> <booktitle> In 1996 Winter Simulation Conference, </booktitle> <month> December </month> <year> 1996. </year>
Reference-contexts: Our benchmarks are written in ZPL, a portable data parallel array language developed at the University of Washington. ZPL is useful for solving regular problems similar to those suitable for Fortran 90 and has been used for scientific and engineering applications <ref> [12, 19, 22, 21] </ref> as well as to implement many standard parallel benchmarks. Our ZPL compiler generates ANSI C code annotated with Ironman calls to indicate the required data transfers.
Reference: 22. <author> Prasenjit Saha, Joachim Stadel, and Scott Tremaine. </author> <title> A parallel integration method for solar system dynamics. </title> <note> to appear in Astronomical Journal, </note> <month> June </month> <year> 1997. </year>
Reference-contexts: Our benchmarks are written in ZPL, a portable data parallel array language developed at the University of Washington. ZPL is useful for solving regular problems similar to those suitable for Fortran 90 and has been used for scientific and engineering applications <ref> [12, 19, 22, 21] </ref> as well as to implement many standard parallel benchmarks. Our ZPL compiler generates ANSI C code annotated with Ironman calls to indicate the required data transfers.
Reference: 23. <author> T. Stricker, J. Subhlok, D. O'Hallaron, S. Hinrichsand, and T. Gross. </author> <title> Decoupling synchronization and data transfer in message passsing systems of parallel computers. </title> <booktitle> In 9 th International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Though widely supported and often vendor-optimized, the message passing abstraction is a blunt instrument for producing high performance object code. Message passing has demonstrated suboptimal performance on shared address space computers like the Cray T3D <ref> [23] </ref>. Furthermore, as explained below, the marshalling, synchronization, and buffering required by the message passing abstraction are frequently unnecessary in the context of a specific machine or instance of data transfer. <p> In addition to describing the Ironman concepts, this paper reports on our experience using Ironman and presents performance measurements based on ZPL programs with various instantiations of the Ironman primitives. This paper makes the following contributions: Confirms claims from Stricker et al. <ref> [23] </ref> regarding the weaknesses of message passing, including the problems of marshalling, synchronization and buffering. Identifies practical problems with compilers using the MPI message passing library. <p> Synchronization is needed to preserve message passing semantics, and to ensure that the transfer has occurred, either in principle or in fact. Stricker et al. demonstrate that message passing's synchronization causes a perfor mance degradation on the T3D <ref> [23] </ref>. Buffering is often used by message passing libraries on the source and/or destination processors. This prevents programs from blocking during calls to the message passing library and allows data to arrive before its corresponding receive is posted. <p> Furthermore, the interfaces themselves can cause data to be marshalled unnecessarily on machines whose built-in communication primitives require no marshalling <ref> [23] </ref>. Some effort has been made to implement highly-tuned versions of the MPI and PVM libraries for platforms that do not inherently support send/receive-based message-passing, such as the Cray T3D [16, 6]. <p> Ironman's paradigm-neutral approach would enable this. Stricker et al. measured the costs involved in performing data transfer on the T3D using a variety of communication paradigms <ref> [23] </ref>. Their results quantify the effects of using a standard message-passing library like PVM, and indicate that the deposit-based paradigm outperforms others due to its reduced synchronization and buffering requirements.
Reference: 24. <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This is further evidence that a single data transfer paradigm will be unlikely to provide portable performance across all platforms. Another important data transfer paradigm is the Active Messages interface designed by von Eicken et al. <ref> [24] </ref>. Consider this to be a highly optimized communication interface that is becoming increasingly widespread. As such, it represents technology that compiler writers would like to use when available, but might hesitate to rely upon since portability is constrained by availability.
References-found: 24

