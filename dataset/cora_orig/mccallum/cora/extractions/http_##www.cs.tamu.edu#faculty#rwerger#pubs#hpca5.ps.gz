URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/hpca5.ps.gz
Refering-URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/
Root-URL: http://www.cs.tamu.edu
Title: Hardware for Speculative Parallelization of Partially-Parallel Loops in DSM Multiprocessors 1  
Author: Ye Zhang Lawrence Rauchwerger and Josep Torrellas 
Note: http://www.cs.tamu.edu/faculty/rwerger/  
Affiliation: University of Illinois at Urbana-Champaign, http://iacoma.cs.uiuc.edu Texas A&M University,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Gopal, T. N. Vijaykumar, J. E. Smith, and G. S. Sohi. </author> <title> Speculative Versioning Cache. </title> <booktitle> In HPCA-4, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: The safe state is typically established at the beginning of the loop. Such a scheme is somewhat related to speculative paral-lelization inside a multiprocessor chip <ref> [1, 2, 3, 5] </ref>, which also relies on extending the cache coherence protocol to detect dependence violations. <p> Such limitations include the need to bound the size of the speculative state to fit in a buffer or L1 cache, and a strict in-order task commit policy that may result in load-imbalance among processors <ref> [1, 2, 3, 5] </ref>. Unfortunately, our scheme has higher recovery costs if a dependence violation is detected, because execution has to backtrack to a safe state that is usually the beginning of the loop.
Reference: [2] <author> L. Hammond, M. Willey, and K. Olukotun. </author> <title> Data Speculation Support for a Chip Multiprocessor. </title> <booktitle> In ASPLOS-VIII, </booktitle> <pages> pages 58-69, </pages> <month> October </month> <year> 1998. </year>
Reference-contexts: The safe state is typically established at the beginning of the loop. Such a scheme is somewhat related to speculative paral-lelization inside a multiprocessor chip <ref> [1, 2, 3, 5] </ref>, which also relies on extending the cache coherence protocol to detect dependence violations. <p> Such limitations include the need to bound the size of the speculative state to fit in a buffer or L1 cache, and a strict in-order task commit policy that may result in load-imbalance among processors <ref> [1, 2, 3, 5] </ref>. Unfortunately, our scheme has higher recovery costs if a dependence violation is detected, because execution has to backtrack to a safe state that is usually the beginning of the loop.
Reference: [3] <author> V. Krishnan and J. Torrellas. </author> <title> Hardware and Software Support for Speculative Execution of Sequential Binaries on a Chip-Multiprocessor. </title> <booktitle> In ICS-1998, </booktitle> <month> July </month> <year> 1998. </year>
Reference-contexts: The safe state is typically established at the beginning of the loop. Such a scheme is somewhat related to speculative paral-lelization inside a multiprocessor chip <ref> [1, 2, 3, 5] </ref>, which also relies on extending the cache coherence protocol to detect dependence violations. <p> Such limitations include the need to bound the size of the speculative state to fit in a buffer or L1 cache, and a strict in-order task commit policy that may result in load-imbalance among processors <ref> [1, 2, 3, 5] </ref>. Unfortunately, our scheme has higher recovery costs if a dependence violation is detected, because execution has to backtrack to a safe state that is usually the beginning of the loop.
Reference: [4] <author> L. Rauchwerger and D. Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatization and Reduction Parallelization. </title> <booktitle> In PLDI-1995, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: In general, however, the inspector may be computationally expensive and have side-effects. Recently, we have introduced a new framework for speculative parallelization in hardware [8]. The scheme is based on a software-based run-time parallelization scheme that we proposed earlier <ref> [4] </ref>. The idea is to execute the code (loops) speculatively in parallel. As parallel execution proceeds, extra hardware added to the directory-based cache coherence of the DSM machine detects if there is a dependence violation.
Reference: [5] <author> J. Steffan and T. Mowry. </author> <title> The Potential for Using Thread-Level Data Speculation to Facilitate Automatic Parallelization. </title> <booktitle> In HPCA-4, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: The safe state is typically established at the beginning of the loop. Such a scheme is somewhat related to speculative paral-lelization inside a multiprocessor chip <ref> [1, 2, 3, 5] </ref>, which also relies on extending the cache coherence protocol to detect dependence violations. <p> Such limitations include the need to bound the size of the speculative state to fit in a buffer or L1 cache, and a strict in-order task commit policy that may result in load-imbalance among processors <ref> [1, 2, 3, 5] </ref>. Unfortunately, our scheme has higher recovery costs if a dependence violation is detected, because execution has to backtrack to a safe state that is usually the beginning of the loop.
Reference: [6] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Run-time Compilation Methods for Multicomputers. </title> <booktitle> In ICPP-1991, </booktitle> <pages> pages 26-30, </pages> <month> August </month> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference: [7] <author> Y. Zhang. </author> <title> DSM Hardware for Speculative Parallelization. </title> <type> Ph.D. Thesis, </type> <institution> University of Illinois, </institution> <month> January </month> <year> 1999. </year>
Reference-contexts: The ANPA is more general than the non-privatization algorithm of [8]. It was not presented there for lack of space. In the rest of this section, we summarize the ANPA and APA. See <ref> [7, 8] </ref> for details. We assume the general case of a dynamically-scheduled loop. The non-analyzable variables that may cause a dependence violation we call arrays under test. 2.1 Advanced Non-Privatization Algorithm (ANPA) In this algorithm, the arrays under test are backed up in software before the loop starts. <p> The Read and Write bits of all the lines in the cache are cleared in hardware at the beginning of every iteration. The bits for a cache line are lost if the line is displaced from the cache. These algorithms, plus further possible optimizations, are described in detail in <ref> [7] </ref>. 2.2 Advanced Privatization Algorithm (APA) In this algorithm, each processor makes a private copy of the array under test and operates on it. In case of dependence violation during loop execution, the recovery to a safe state simply consists of discarding the private arrays. <p> The Free Sector Stack is used to identify free sectors. Overall, sector allocation, log storage, and sector overflow handling are all done in hardware for performance reasons. Undo Buffer overflow, however, is handled in software by the node's processor to reduce hardware costs. It is described in <ref> [7] </ref>. 3.1.2 Committing Iterations on the Fly When an iteration finishes, it cannot be committed until all the lower-numbered iterations are also finished and committed. To keep track of the state of each iteration, processors share three data structures. <p> The space can now be reused. In our experiments, we optimize this operation to reduce the number of messages sent to directory controllers and to handle the possible deallocation of overflowed Undo Buffers in software. The details are presented in <ref> [7] </ref>. 3.1.3 Dependence Violation and Restart If a directory controller detects a dependence violation, it sends a cross-processor interrupt to all processors, passing, as argument, the ID of the smaller iteration involved in the violation. <p> Then, all processors synchronize and parallel execution resumes starting from the read-first iteration. 4 Experimental Results We have evaluated the proposed hardware through simulations. Due to space constraints, we only present a summary of the results here. Full details are in <ref> [7] </ref>. We use MINT to perform execution-driven simulations of a 16-node CC-NUMA shared-memory multiprocessor. Each node has a 200-MHz 4-issue dynamic superscalar with 4 integer, 2 load-store and 2 floating-point functional units and a 32-entry instruction queue. Each node has a 32-Kbyte L1 cache and a 512-Kbyte L2 cache. <p> From the figure, we see that the extended ANPA/APA perform better than the base ANPA/APA and much better than the serial execution. Overall, the extended algorithms deliver average speedups of 4.2 over Serial1 and 31 over Serial16. More in-depth analysis can be found in <ref> [7] </ref>. 5 Summary Hardware-based speculative parallelization of non-analyzable codes on DSM multiprocessors is challenging. In this paper, we have extended past work by proposing a scheme to par-allelize codes (loops) that have a modest number of cross-iteration dependences.
Reference: [8] <author> Y. Zhang, L. Rauchwerger, and J. Torrellas. </author> <title> Hardware for Speculative Run-Time Parallelization in Distributed Shared-Memory Multiprocessors. </title> <booktitle> In HPCA-4, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: Each wavefront is then executed in parallel by the executor, with synchronization separating the wavefronts. In general, however, the inspector may be computationally expensive and have side-effects. Recently, we have introduced a new framework for speculative parallelization in hardware <ref> [8] </ref>. The scheme is based on a software-based run-time parallelization scheme that we proposed earlier [4]. The idea is to execute the code (loops) speculatively in parallel. As parallel execution proceeds, extra hardware added to the directory-based cache coherence of the DSM machine detects if there is a dependence violation. <p> Simulation results suggest that this form of DSM speculative par-allelization is promising: a 16-processor parallel execution of 4 important loops runs 4.2 and 31 times faster than two different serial executions of the loops. In the following, we briefly describe the speculative par-allelization scheme that was introduced in <ref> [8] </ref>, then present the new extensions for loops with dependences, and finally evaluate these new extensions. 2 DSM Hardware for Speculative Parallelization In previous work, we proposed a scheme to speculatively execute non-analyzable loops in parallel in a DSM machine [8]. <p> briefly describe the speculative par-allelization scheme that was introduced in <ref> [8] </ref>, then present the new extensions for loops with dependences, and finally evaluate these new extensions. 2 DSM Hardware for Speculative Parallelization In previous work, we proposed a scheme to speculatively execute non-analyzable loops in parallel in a DSM machine [8]. The idea is to extend the directory-based cache-coherence protocol of the machine to detect, in hardware, any violation of a cross-iteration dependence. The loop is executed in parallel. <p> If the hardware detects such a violation, then the state is rolled back to a safe state and the execution is retried on a single processor. Otherwise, parallel execution completes successfully. For fully-parallel loops, the scheme is shown to deliver speedups between 4 and 11 for 16 processors <ref> [8] </ref>. The scheme can be fleshed out into different hardware algorithms with different cost and performance. We envision the DSM machine to support a few such algorithms mapped to the same hardware, and the compiler to select the algorithm on an array-by-array basis [8]. <p> between 4 and 11 for 16 processors <ref> [8] </ref>. The scheme can be fleshed out into different hardware algorithms with different cost and performance. We envision the DSM machine to support a few such algorithms mapped to the same hardware, and the compiler to select the algorithm on an array-by-array basis [8]. First, there are the privatization and the non-privatization algorithms. The latter should be used when the compiler can prove that the array under test is not privatizable. This is because privatization, while requiring extra storage, uncovers more parallelism because it eliminates output- and anti-dependences [8]. <p> algorithm on an array-by-array basis <ref> [8] </ref>. First, there are the privatization and the non-privatization algorithms. The latter should be used when the compiler can prove that the array under test is not privatizable. This is because privatization, while requiring extra storage, uncovers more parallelism because it eliminates output- and anti-dependences [8]. Among the privatization algorithms, we focus here on the most general one, namely the Advanced Pri-vatization Algorithm (APA) presented in [8]. Among the non-privatization ones, we focus here on the equivalent to APA, namely the Advanced Non-Privatization Algorithm (ANPA). The ANPA is more general than the non-privatization algorithm of [8]. <p> This is because privatization, while requiring extra storage, uncovers more parallelism because it eliminates output- and anti-dependences <ref> [8] </ref>. Among the privatization algorithms, we focus here on the most general one, namely the Advanced Pri-vatization Algorithm (APA) presented in [8]. Among the non-privatization ones, we focus here on the equivalent to APA, namely the Advanced Non-Privatization Algorithm (ANPA). The ANPA is more general than the non-privatization algorithm of [8]. It was not presented there for lack of space. <p> <ref> [8] </ref>. Among the privatization algorithms, we focus here on the most general one, namely the Advanced Pri-vatization Algorithm (APA) presented in [8]. Among the non-privatization ones, we focus here on the equivalent to APA, namely the Advanced Non-Privatization Algorithm (ANPA). The ANPA is more general than the non-privatization algorithm of [8]. It was not presented there for lack of space. In the rest of this section, we summarize the ANPA and APA. See [7, 8] for details. We assume the general case of a dynamically-scheduled loop. <p> The ANPA is more general than the non-privatization algorithm of [8]. It was not presented there for lack of space. In the rest of this section, we summarize the ANPA and APA. See <ref> [7, 8] </ref> for details. We assume the general case of a dynamically-scheduled loop. The non-analyzable variables that may cause a dependence violation we call arrays under test. 2.1 Advanced Non-Privatization Algorithm (ANPA) In this algorithm, the arrays under test are backed up in software before the loop starts. <p> In the directory, the current iteration number is compared to MaxR1st. If the former is lower, the parallelization fails; otherwise, MinW is updated. More details can be found in <ref> [8] </ref>. 3 Parallelizing Loops with Dependences The algorithms presented thus far target loops that are fully parallel in most of the invocations. Now, we extend the algorithms to handle loops that often have cross-iteration dependences.
References-found: 8

