URL: http://www.cs.rpi.edu/~zaki/PS/ICDE99.ps.gz
Refering-URL: http://www.cs.rpi.edu/~zaki/papers.html
Root-URL: http://www.cs.rpi.edu
Email: zaki@cs.rpi.edu  fho,ragrawalg@almaden.ibm.com  
Title: Parallel Classification for Data Mining on Shared-Memory Multiprocessors  
Author: Mohammed J. Zaki Ching-Tien Ho and Rakesh Agrawal 
Address: Troy, NY 12180  San Jose, CA 95120  
Affiliation: Computer Science Department Rensselaer Polytechnic Institute,  IBM Almaden Research Center  
Abstract: We present parallel algorithms for building decision-tree classifiers on shared-memory multiprocessor (SMP) systems. The proposed algorithms span the gamut of data and task parallelism. The data parallelism is based on attribute scheduling among processors. This basic scheme is extended with task pipelining and dynamic load balancing to yield faster implementations. The task parallel approach uses dynamic subtree partitioning among processors. Our performance evaluation shows that the construction of a decision-tree classifier can be effectively parallelized on an SMP machine with good speedup. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, et al. </author> <title> An interval classifier for database mining applications. </title> <booktitle> In VLDB Conference, </booktitle> <month> Aug </month> <year> 1992. </year>
Reference-contexts: Trees can also be converted into SQL statements that can be used to access databases efficiently <ref> [1] </ref>. Finally, decision-tree classifiers obtain similar, and often better, accuracy compared to other methods [10]. Prior to interest in classification for database-centric data mining, it was tacitly assumed that the training sets could fit in memory. Recent work has targeted the massive training sets usual in data mining. <p> However, due to the decreasing cost of RAM, the second configuration is also increasingly realizable in practice. We present this case to study the impact of large memories. Datasets: We use synthetic datasets proposed in <ref> [1] </ref>, using two classification functions of different complexity. These functions divide the database into two classes. Function 2 is a simple function to learn and results in fairly small decision trees, while Function 7 is the most complex function and produces large trees (see Table 1).
Reference: [2] <author> R. Agrawal, T. Imielinski, A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Trans on Knowledge and Data Engg. </journal> , <volume> 5(6) </volume> <pages> 914-925, </pages> <month> Dec </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Classification is one of the primary data mining task <ref> [2] </ref>. The input to a classification system consists of example tuples, called a training set, with each tuple having several attributes. Attributes can be continuous, coming from an ordered domain, or categorical, coming from an unordered domain.
Reference: [3] <author> K. Alsabti, S. Ranka, and V. Singh. </author> <title> CLOUDS: A decision tree classifier for large datasets. </title> <booktitle> In 4th Intl. Conf. on Knowledge Discovery and Data Mining, </booktitle> <month> Aug </month> <year> 1998. </year>
Reference-contexts: Recent work has targeted the massive training sets usual in data mining. Developing classification models using larger training sets can enable the development of higher accuracy models. Various studies have confirmed this [5, 6]. Recent classifiers that can handle disk-resident data include SLIQ [9], SPRINT [12], and CLOUDS <ref> [3] </ref>. As data continue to grow in size and complexity, high-performance scalable data mining tools must necessarily rely on parallel computing techniques. Past research on parallel classification has been focussed on distributed-memory (also called shared-nothing) machines.
Reference: [4] <author> L. Breiman, et al. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: Past research on parallel classification has been focussed on distributed-memory (also called shared-nothing) machines. Examples include parallel ID3 [7], which assumed that the entire dataset could fit in memory; Darwin toolkit with parallel CART <ref> [4] </ref> from Thinking Machine, whose details are not available in published literature; parallel SPRINT on IBM SP2 [12]; and ScalParC [8] on a Cray T3D. <p> Decision TreeTraining Set High CarType in -sports High Low Car Type Class family sports sports family family truck High High High High Low 5 0 2 4 32 23 43 Age &lt; 27.5 A decision-tree classifier is usually built in two phases <ref> [4, 11] </ref>: a growth phase and a prune phase. The tree is grown using a divide-and-conquer approach. If all examples in the training set S belong to a single class, then S becomes a leaf. <p> SPRINT uses the gini index for this purpose. For a data set S containing examples from n classes, gini (S) is defined to be gini (S) = 1 P j where p j is the relative frequency of class j in S <ref> [4] </ref>. For continuous attributes, the candidate split points are mid-points between every two consecutive attribute values in the training data. <p> requires more memory, because we need a separate hash probe per group. 4 Performance Evaluation We use the execution time as the main metric of classifier performance, since [9] has shown that SLIQ/SPRINT achieve similar or better classification accuracy and produce smaller trees when compared to other classifiers like CART <ref> [4] </ref> and C4 (a predecessor of C4.5 [11]). 4.1 Experimental Setup Machine Configuration: Experiments were performed on two SMP machines ( with different configurations) with a 112 MHz PowerPC-604 processor, and a 1 MB L2-Cache. Machine A has 4 processors, 128 MB memory and 300 MB disk.
Reference: [5] <author> J. Catlett. </author> <title> Megainduction: Machine Learning on Very Large Databases. </title> <type> PhD thesis, </type> <institution> U. of Sydney, </institution> <year> 1991. </year>
Reference-contexts: Recent work has targeted the massive training sets usual in data mining. Developing classification models using larger training sets can enable the development of higher accuracy models. Various studies have confirmed this <ref> [5, 6] </ref>. Recent classifiers that can handle disk-resident data include SLIQ [9], SPRINT [12], and CLOUDS [3]. As data continue to grow in size and complexity, high-performance scalable data mining tools must necessarily rely on parallel computing techniques.
Reference: [6] <author> P. Chan, S. Stolfo. </author> <title> Experiments on multistrategy learning by meta-learning. </title> <booktitle> In 2nd Intl. Conf. on Info. and Knowledge Mgmt., </booktitle> <month> Nov </month> <year> 1993. </year>
Reference-contexts: Recent work has targeted the massive training sets usual in data mining. Developing classification models using larger training sets can enable the development of higher accuracy models. Various studies have confirmed this <ref> [5, 6] </ref>. Recent classifiers that can handle disk-resident data include SLIQ [9], SPRINT [12], and CLOUDS [3]. As data continue to grow in size and complexity, high-performance scalable data mining tools must necessarily rely on parallel computing techniques.
Reference: [7] <author> D. Fifield. </author> <title> Distributed tree construction from large data-sets. </title> <type> Bachelor Thesis, </type> <institution> Australian Natl. U., </institution> <year> 1992. </year>
Reference-contexts: As data continue to grow in size and complexity, high-performance scalable data mining tools must necessarily rely on parallel computing techniques. Past research on parallel classification has been focussed on distributed-memory (also called shared-nothing) machines. Examples include parallel ID3 <ref> [7] </ref>, which assumed that the entire dataset could fit in memory; Darwin toolkit with parallel CART [4] from Thinking Machine, whose details are not available in published literature; parallel SPRINT on IBM SP2 [12]; and ScalParC [8] on a Cray T3D.
Reference: [8] <author> M. Joshi, G. Karypis, V. Kumar. ScalParC: </author> <title> A scalable and parallel classification algorithm for mining large datasets. </title> <booktitle> In Intl. Parallel Processing Symp. </booktitle> , <year> 1998. </year>
Reference-contexts: Examples include parallel ID3 [7], which assumed that the entire dataset could fit in memory; Darwin toolkit with parallel CART [4] from Thinking Machine, whose details are not available in published literature; parallel SPRINT on IBM SP2 [12]; and ScalParC <ref> [8] </ref> on a Cray T3D. While distributed-memory machines provide massive parallelism, shared-memory machines (also called shared-everything systems), are also capable of delivering high performance for low to medium degree of parallelism at an economically attractive price. Increasingly SMP machines are being networked together via high-speed links to form hierarchical clusters.
Reference: [9] <author> M. Mehta, R. Agrawal, J. Rissanen. SLIQ: </author> <title> A fast scalable classifier for data mining. </title> <booktitle> In 5th Intl. Conf. on Extending Database Technology, </booktitle> <month> March </month> <year> 1996. </year>
Reference-contexts: Recent work has targeted the massive training sets usual in data mining. Developing classification models using larger training sets can enable the development of higher accuracy models. Various studies have confirmed this [5, 6]. Recent classifiers that can handle disk-resident data include SLIQ <ref> [9] </ref>, SPRINT [12], and CLOUDS [3]. As data continue to grow in size and complexity, high-performance scalable data mining tools must necessarily rely on parallel computing techniques. Past research on parallel classification has been focussed on distributed-memory (also called shared-nothing) machines. <p> This phase requires access only to the fully grown tree, while the tree growth phase usually requires multiple passes over the training data. Previous studies from SLIQ suggest that usually less than 1% of the total time needed to build a classifier was spent in the prune phase <ref> [9] </ref>. We therefore concentrate on the tree building phase, which depends on two factors: 1) how to find split points that define node tests, and 2) having chosen a split point, how to partition the data. We now describe how the above two steps are handled in serial SPRINT [12]. <p> We will only discuss the tree growth phase due to its compute- and data-intensive nature. Tree pruning is relatively inexpensive <ref> [9] </ref>, as it requires access to only the decision-tree grown in the training phase. 3.1 SMP Schemes While building a decision-tree, there are three main steps that must be performed for each node at each level of the tree: (i) evaluate split points for each attribute (denoted as step E); (ii) <p> Another disadvantage is that it requires more memory, because we need a separate hash probe per group. 4 Performance Evaluation We use the execution time as the main metric of classifier performance, since <ref> [9] </ref> has shown that SLIQ/SPRINT achieve similar or better classification accuracy and produce smaller trees when compared to other classifiers like CART [4] and C4 (a predecessor of C4.5 [11]). 4.1 Experimental Setup Machine Configuration: Experiments were performed on two SMP machines ( with different configurations) with a 112 MHz PowerPC-604
Reference: [10] <editor> D. Michie, et al. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: The goal of classification is to induce a model from the training set, that can be used to predict the class of a new tuple. Classification has applications in diverse fields such as retail target marketing, fraud detection, and medical diagnosis <ref> [10] </ref>. Amongst many classification methods proposed over the years [13, 10], decision trees are particularly suited for data mining, since they can be built relatively fast compared to other methods and they are easy to interpret [11]. <p> Classification has applications in diverse fields such as retail target marketing, fraud detection, and medical diagnosis [10]. Amongst many classification methods proposed over the years <ref> [13, 10] </ref>, decision trees are particularly suited for data mining, since they can be built relatively fast compared to other methods and they are easy to interpret [11]. Trees can also be converted into SQL statements that can be used to access databases efficiently [1]. <p> Trees can also be converted into SQL statements that can be used to access databases efficiently [1]. Finally, decision-tree classifiers obtain similar, and often better, accuracy compared to other methods <ref> [10] </ref>. Prior to interest in classification for database-centric data mining, it was tacitly assumed that the training sets could fit in memory. Recent work has targeted the massive training sets usual in data mining. Developing classification models using larger training sets can enable the development of higher accuracy models.
Reference: [11] <author> J. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year>
Reference-contexts: Amongst many classification methods proposed over the years [13, 10], decision trees are particularly suited for data mining, since they can be built relatively fast compared to other methods and they are easy to interpret <ref> [11] </ref>. Trees can also be converted into SQL statements that can be used to access databases efficiently [1]. Finally, decision-tree classifiers obtain similar, and often better, accuracy compared to other methods [10]. <p> Decision TreeTraining Set High CarType in -sports High Low Car Type Class family sports sports family family truck High High High High Low 5 0 2 4 32 23 43 Age &lt; 27.5 A decision-tree classifier is usually built in two phases <ref> [4, 11] </ref>: a growth phase and a prune phase. The tree is grown using a divide-and-conquer approach. If all examples in the training set S belong to a single class, then S becomes a leaf. <p> separate hash probe per group. 4 Performance Evaluation We use the execution time as the main metric of classifier performance, since [9] has shown that SLIQ/SPRINT achieve similar or better classification accuracy and produce smaller trees when compared to other classifiers like CART [4] and C4 (a predecessor of C4.5 <ref> [11] </ref>). 4.1 Experimental Setup Machine Configuration: Experiments were performed on two SMP machines ( with different configurations) with a 112 MHz PowerPC-604 processor, and a 1 MB L2-Cache. Machine A has 4 processors, 128 MB memory and 300 MB disk.
Reference: [12] <author> J. Shafer, R. Agrawal, M. Mehta. SPRINT: </author> <title> A scalable parallel classifier for data mining. </title> <booktitle> In 22nd VLDB Conference, </booktitle> <month> Sept </month> <year> 1996. </year>
Reference-contexts: Recent work has targeted the massive training sets usual in data mining. Developing classification models using larger training sets can enable the development of higher accuracy models. Various studies have confirmed this [5, 6]. Recent classifiers that can handle disk-resident data include SLIQ [9], SPRINT <ref> [12] </ref>, and CLOUDS [3]. As data continue to grow in size and complexity, high-performance scalable data mining tools must necessarily rely on parallel computing techniques. Past research on parallel classification has been focussed on distributed-memory (also called shared-nothing) machines. <p> Examples include parallel ID3 [7], which assumed that the entire dataset could fit in memory; Darwin toolkit with parallel CART [4] from Thinking Machine, whose details are not available in published literature; parallel SPRINT on IBM SP2 <ref> [12] </ref>; and ScalParC [8] on a Cray T3D. While distributed-memory machines provide massive parallelism, shared-memory machines (also called shared-everything systems), are also capable of delivering high performance for low to medium degree of parallelism at an economically attractive price. <p> We therefore concentrate on the tree building phase, which depends on two factors: 1) how to find split points that define node tests, and 2) having chosen a split point, how to partition the data. We now describe how the above two steps are handled in serial SPRINT <ref> [12] </ref>. It builds the tree in a breadth-first order and uses a one-time pre-sorting technique to reduce the cost of finding split point for a continuous attribute. 2.1 Attribute Lists SPRINT initially creates a disk-based attribute list for each attribute in the data. <p> This approach exploits the intra-node parallelism, i.e., that available within a decision tree node. We use attribute data parallelism, where the attributes are divided equally among the different processors so that each processor is responsible for 1=P attributes. The parallel implementation of SPRINT on an IBM SP2 <ref> [12] </ref> is based on record data parallelism, where each processor is responsible for processing roughly 1=P fraction of each attribute list. Record parallelism is not well suited to SMP systems since it is likely to cause excessive synchronization, and replication of data structures.
Reference: [13] <author> S. Weiss, C. </author> <title> Kulikowski. Computer Systems that Learn. </title> <publisher> Morgan Kaufman, </publisher> <year> 1991. </year>
Reference-contexts: Classification has applications in diverse fields such as retail target marketing, fraud detection, and medical diagnosis [10]. Amongst many classification methods proposed over the years <ref> [13, 10] </ref>, decision trees are particularly suited for data mining, since they can be built relatively fast compared to other methods and they are easy to interpret [11]. Trees can also be converted into SQL statements that can be used to access databases efficiently [1].
Reference: [14] <author> M. Zaki, C-T. Ho, R. Agrawal. </author> <title> Parallel Classification for Data Mining on Shared-Memory Multiprocessors. </title> <type> IBM Technical Report, </type> <year> 1998. </year> <note> Available from www.almaden.ibm.com/cs/quest/publications.html. 8 </note>
Reference-contexts: Section 3 describes our new SMP algorithms based on various data and task parallelization schemes. We give experimental results in Section 4 and conclude with a summary in Section 5. A more detailed version of this paper appears in <ref> [14] </ref>. 2 Serial Classification Each node in a decision tree classifier is either a leaf, indicating a class, or a decision node, specifying some test on one or more attributes, with one branch or subtree for each of the possible outcomes of the split test.
References-found: 14

