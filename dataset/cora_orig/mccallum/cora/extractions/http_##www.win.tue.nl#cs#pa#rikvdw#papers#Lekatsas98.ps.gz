URL: http://www.win.tue.nl/cs/pa/rikvdw/papers/Lekatsas98.ps.gz
Refering-URL: http://www.win.tue.nl/cs/pa/rikvdw/bibl.html
Root-URL: http://www.win.tue.nl
Email: flekatsas,wolfg@ee.princeton.edu  
Title: Code Compression for Embedded Systems  
Author: Haris Lekatsas and Wayne Wolf 
Address: Princeton University  
Affiliation: Department of Electrical Engineering  
Abstract: Memory is one of the most restricted resources in many modern embedded systems. Code compression can provide substantial savings in terms of size. In a compressed code CPU, a cache miss triggers the decompression of a main memory block, before it gets transferred to the cache. Because the code must be decompressible starting from any point (or at least at cache block boundaries), most file-oriented compression techniques cannot be used. We propose two algorithms to compress code in a space-efficient and simple to decompress way, one which is independent of the instruction set and another which depends on the instruction set. We perform experiments on two instruction sets, a typical RISC (MIPS) and a typical CISC (x86) and compare our results to existing file-oriented compression algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Bell, J. Cleary, and I. Witten. </author> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: Due to the constraints stated above most of the existing state of art algorithms cannot be used directly. In terms of compression ratios the the algorithms that use finite context modelling such as PPM (Prediction by Partial Match) <ref> [1] </ref>, DMC [3], and WORD [8] seem to achieve the best performance. However they require large amounts of memory both for compression and decompression, making them unsuitable for program compression. <p> This greatly simplifies the design of the midpoint calculation unit. 4 SADC: An ISA-dependent method using a dictionary Another valid choice is to build a dictionary with common sequences of instructions. Dictionaries can be static, semi-adaptive, or dynamic <ref> [1] </ref>. Dynamic dictionaries are adap tive and change while the compressor or decompressor goes through the program. This technique cannot be used here because it does not allow for random access. Static dictionaries are built once and used for all programs, while semiadaptive are built for each subject program.
Reference: [2] <author> J. Bentley, D. Sleator, R. Tarjan, and V. Wei. </author> <title> A locally adaptive data compression scheme. </title> <journal> Communications of the ACM, </journal> <volume> Vol. 29(No 4):pp 320-330, </volume> <year> 1986. </year>
Reference: [3] <author> G. Cormack and R. Horspool. </author> <title> Data compression using dynamic markov modelling. </title> <journal> The Computer Journal, </journal> <volume> Vol. 30(No 6), </volume> <year> 1987. </year>
Reference-contexts: Due to the constraints stated above most of the existing state of art algorithms cannot be used directly. In terms of compression ratios the the algorithms that use finite context modelling such as PPM (Prediction by Partial Match) [1], DMC <ref> [3] </ref>, and WORD [8] seem to achieve the best performance. However they require large amounts of memory both for compression and decompression, making them unsuitable for program compression. The Ziv-Lempel family of algorithms [14] use pointers to previous occurences of strings which makes an individual block decompression scheme impossible.
Reference: [4] <author> D. Huffman. </author> <title> A method for the construction of minimum-redundancy codes. </title> <booktitle> In Proceedings of the IRE, </booktitle> <volume> volume Vol. 4D, </volume> <pages> pages pp 1098-1101, </pages> <month> September </month> <year> 1952. </year>
Reference-contexts: The Ziv-Lempel family of algorithms [14] use pointers to previous occurences of strings which makes an individual block decompression scheme impossible. An instruction-based compression approach which overcomes the above problems is described by Kozuch and Wolfe [5], where byte-based Huffman codes <ref> [4] </ref> are used, yielding a compression ratio around 0.73 (compressed size/original size). Such a scheme is simple to implement, and certainly allows decompression to take place from any place in the code.
Reference: [5] <author> M. Kozuch and A. Wolfe. </author> <title> Compression of embedded system programs. </title> <booktitle> 1994 IEEE International Conference on Computer Design: VLSI in Computers & Processors, </booktitle> <year> 1994. </year>
Reference-contexts: The Ziv-Lempel family of algorithms [14] use pointers to previous occurences of strings which makes an individual block decompression scheme impossible. An instruction-based compression approach which overcomes the above problems is described by Kozuch and Wolfe <ref> [5] </ref>, where byte-based Huffman codes [4] are used, yielding a compression ratio around 0.73 (compressed size/original size). Such a scheme is simple to implement, and certainly allows decompression to take place from any place in the code.
Reference: [6] <author> S. Liao, S. Devadas, and K. Keutzer. </author> <title> Code density optimization for embedded dsp processors using data compression techniques. </title> <booktitle> In Proceedings of the 1995 Chapel Hill Conference on Advanced Research in VLSI, </booktitle> <pages> pages pp 393-399, </pages> <year> 1995. </year>
Reference-contexts: However, if we use 32-bit symbols we need a Huffman table with 2 32 entries making the size of the decompressor prohibitively large. Futhermore, this method does not take into account dependencies between instructions, limiting the overall compression performance. Another dictionary-based approach is described by Liao et al. <ref> [6] </ref>, where a dictionary is generated for TMS320C25 code. Their approach requires little or no extra hardware, but gives only modest compression ratios. In this paper, we present two different approaches which produce significantly better compression.
Reference: [7] <author> A. Mayne and E. James. </author> <title> Information compression by factoring common strings. </title> <journal> Computer Journal, </journal> <volume> Vol. 18(No 2):pp 157-160, </volume> <year> 1975. </year>
Reference: [8] <author> A. Moffat. </author> <title> Word based text compression. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Mel-bourne,Parkville,Victoria,Australia, </institution> <year> 1987. </year>
Reference-contexts: Due to the constraints stated above most of the existing state of art algorithms cannot be used directly. In terms of compression ratios the the algorithms that use finite context modelling such as PPM (Prediction by Partial Match) [1], DMC [3], and WORD <ref> [8] </ref> seem to achieve the best performance. However they require large amounts of memory both for compression and decompression, making them unsuitable for program compression. The Ziv-Lempel family of algorithms [14] use pointers to previous occurences of strings which makes an individual block decompression scheme impossible.
Reference: [9] <author> J. Rissanen and G. Landon. </author> <title> Universal modeling and coding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. </volume> <pages> 27:pp 12-23, </pages> <year> 1981. </year>
Reference: [10] <author> J. Storer. </author> <title> NP-completeness results concerning data compression. </title> <type> Technical Report No 234, </type> <institution> Department of Electrical Engineering and Computer Science, Princeton University, Princeton, N.J., </institution> <year> 1977. </year>
Reference-contexts: This means that we can augment the instruction set by about 200 new opcodes. We are effectively building for each subject program a semiadaptive dictionary which maps indices to opcodes or opcode combinations. Finding the best dictionary for a given program is shown by Storer <ref> [10] </ref> to be NP-hard. We therefore do not attempt to generate an optimal dictionary. Once the dictionary is generated we are also faced with the problem of how to parse the subject program and replace occurences of opcode combinations with dictionary indices.
Reference: [11] <author> T. Welch. </author> <title> A technique for high-performance data compression. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages pp 8-19, </pages> <month> June </month> <year> 1984. </year>
Reference: [12] <author> I. Witten, R. Neal, and J. Cleary. </author> <title> Arithmetic coding for data compression. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> Vol. 30(No. </volume> <pages> 6):pp 520-540, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Compressing data which may also be writable would complicate the design as we would have to provide a fast compressor as well when writing to main memory. 3 SAMC: An ISA-independent method using statistical coding Our first method uses a binary arithmetic coder <ref> [12] </ref> driven by a Markov model. In the following we will refer to this method as SAMC for Semiadaptive Markov Compression. Since we are compressing cache blocks, an adaptive method cannot be used effectively as the coder will not be able to gather enough statistical information from just one block. <p> To avoid the multiplication in the midpoint calculation unit we can constrain the probability of the less probable symbol to the nearest integral power of 1 2 , thus requiring only shifts. Wit-ten et al <ref> [12] </ref> showed that the worst-case efficiency is about 95% when we pose this constraint. In our case, in order to save space, we only store the probability of a 0 input.
Reference: [13] <author> A. Wolfe and A. Chanin. </author> <title> Executing compressed programs on an embedded RISC architecture. </title> <booktitle> Proc. 25th Ann. International Symposium on Microarchitecture, </booktitle> <pages> pages pp 81-91, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: However, in deep-submicron pipelined processors, we believe that most instruction decode time can be hidden (as in the Intel architectures) particularly in the case of compressed code. We decompress our code before inserting it in the cache, following the design first proposed by Wolfe and Chanin <ref> [13] </ref>. We therefore assume that the processor executes normal uncompressed code and that decompression of a cache line occurs only when there is a cache miss. The loss in performance should therefore depend on the instruction cache hit ratio. <p> Such an approach has several practical advantages: We can use well-characterized existing processor pipelines, take advantage of existing programming development tools, use existing workstations as development platforms, etc. Wolfe and Chanin <ref> [13] </ref> designed a system with run time decompression. Figure 1 shows the memory organization for such a system. The instruction cache is used as a decompression buffer storing recently used decompressed cache blocks. The decompression is being done by the cache refill engine whenever a cache miss occurs.
Reference: [14] <author> J. Ziv and A. Lempel. </author> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 23(No 3):pp 337-343, </volume> <month> May </month> <year> 1977. </year>
Reference-contexts: However they require large amounts of memory both for compression and decompression, making them unsuitable for program compression. The Ziv-Lempel family of algorithms <ref> [14] </ref> use pointers to previous occurences of strings which makes an individual block decompression scheme impossible. An instruction-based compression approach which overcomes the above problems is described by Kozuch and Wolfe [5], where byte-based Huffman codes [4] are used, yielding a compression ratio around 0.73 (compressed size/original size).
References-found: 14

