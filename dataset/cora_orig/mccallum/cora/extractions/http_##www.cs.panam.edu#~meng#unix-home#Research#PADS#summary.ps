URL: http://www.cs.panam.edu/~meng/unix-home/Research/PADS/summary.ps
Refering-URL: http://www.cs.panam.edu/~meng/unix-home/Research/PADS/
Root-URL: http://www.cs.panam.edu
Title: Distributed Simulations Issues and Implementations in Cluster of Workstations Environment  
Author: Xiannong Meng 
Keyword: distributed simulations, distributed computing, distributed debugging, com munications.  
Address: Edinburg, TX 78539  
Affiliation: Department of Computer Science The University of Texas Pan American  
Email: meng@panam.edu  
Web: http ==www:cs:panam:edu=~meng/  
Date: May 31, 1996 Revised: December 31, 1996  
Abstract: We investigate in this paper various issues in parallel and distributed simulations in general, and its implementation in cluster of workstation environment in particular. The first part of the paper surveys the filed of parallel and distributed simulations, describes the issues that many researchers have been tackling. The second part of the paper presents the research projects carried out by the author in three areas, state saving in distributed simulations, implementing and performance issues in cluster of workstations, and debugging in distributed simulation environment. Our research found that the memory requirement of state saving can be reduced in certain types of distributed simulations; while decreasing communications among participating processes may reduce the cost of communications, it may result in more rollbacks because the loss of synchronizations; debugging distributed programs is difficult, but it can be aided using library functions we developed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H.H. Ammar and S. Deng. </author> <title> Time Warp simulation using time scale decomposition. </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> 2(2), </volume> <month> April </month> <year> 1992. </year>
Reference-contexts: Time division distributed simulation means the simulation time is divided into a number of intervals, each of which is simulated on a different processor <ref> [1] </ref>. Distributed vs. parallel Distributed simulation usually refers to simulations which use explicit message passing as primary communication mode; while parallel simulation implies that shared memory is used as the communication mechanism. The key here is the speed of the communication.
Reference: [2] <author> D. Baik and B.P. Zeigler. </author> <title> Performance evaluation of hierarchical distributed simulators. </title> <booktitle> In Proceedings of 1985 Winter Simulation Conference, </booktitle> <pages> pages 421-427, </pages> <year> 1985. </year>
Reference-contexts: In a centralized approach, a dedicated processor maintains the global 3 clock. The value of the global clock is decided by the next interactive events among partici-pating LPs [8]. Different mechanisms for decentralized control of global clock for synchronous simulation were proposed <ref> [34, 2, 9, 10] </ref>. No causality error will occur in synchronous simulation. In asynchronous distributed simulation, each LP has its own local simulation clock and each LP can advance the local clock at its own pace.
Reference: [3] <author> R.E. Bryant. </author> <title> Simulation of packet communication architecture computer systems. </title> <type> Technical Report MIT,LCS,TR-188, </type> , <year> 1977. </year> <month> 21 </month>
Reference-contexts: The discussion focuses on two threads of distributed simulations, the conservative ones and the optimistic ones. 2.1 Conservative Distributed Simulations Works by Chandy et al. [5] and Bryant <ref> [3] </ref> are considered as the pioneering work in the area. They laid the foundation for conservative distributed simulation (see Section 1.2). The frame work of conservative distributed simulation was developed. The problems of deadlock, the schemes for deadlock resolution were discussed.
Reference: [4] <author> Christopher Carothers, Richard Fujimoto, and Paul England. </author> <title> Effect of communication over-heads on Time Warp performance: An experimental study. </title> <booktitle> In Proceedings of 1994 PADS Workshop, </booktitle> <year> 1994. </year>
Reference-contexts: Communication is a central component of any parallel and distributed simulation. Effective communication can speed up simulation, while slow, less efficient communication can degrade the simulation significantly. Both Carothers et.al. <ref> [4] </ref> and Meng [26] studied the impact of communications overhead on the performance of distributed simulations. If the simulation uses shared memory as communications channel, it is less of a problem. However, if the communication is through a local area network such as Ethernet, the overhead can be heavy.
Reference: [5] <author> K.M. Chandy and J. Misra. </author> <title> Distribtued simulation: A case study in design and verification of distributed programs. </title> <journal> IEEE Transaction on Software Engineering, </journal> <volume> SE-5(5), </volume> <year> 1979. </year>
Reference-contexts: The discussion focuses on two threads of distributed simulations, the conservative ones and the optimistic ones. 2.1 Conservative Distributed Simulations Works by Chandy et al. <ref> [5] </ref> and Bryant [3] are considered as the pioneering work in the area. They laid the foundation for conservative distributed simulation (see Section 1.2). The frame work of conservative distributed simulation was developed. The problems of deadlock, the schemes for deadlock resolution were discussed.
Reference: [6] <author> K.M. Chandy and J. Misra. </author> <title> Asynchronous distributed simulation via a sequence of parallel computations. </title> <journal> Communications of the ACM, </journal> <volume> 24(4), </volume> <month> April </month> <year> 1981. </year>
Reference-contexts: The problems of deadlock, the schemes for deadlock resolution were discussed. Peacock et al. [34, 33] and Holmes [16] proposed mechanism of using probe message periodically to avoid deadlock. Chandy and Misra also suggested a scheme for deadlock detection and recovery <ref> [6] </ref>. Reynolds suggested using shared memory among neighbors to avoid deadlock [36]. The basic scheme of original Chandy, Misra and Bryant is the following [29]. Without losing generality it is assumed that one LP corresponding to one PP.
Reference: [7] <author> Liang Chen. </author> <title> Performance Analysis and Improvement of Parallel Simulation. </title> <type> PhD thesis, </type> <institution> School of Industrial and Systems Engineering, Georgia Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: Any state information before this boundary thus can be reclaimed. The second model uses the Central Limit Theorem to estimate the boundary of LVT, thus yielding the boundary of fossil collection. * Chen's work <ref> [7] </ref> unifies the performance model for memory effect on message distribution, rollback, and GVT advances in shared memory environment, using a Markov process model.
Reference: [8] <author> T. Christopher, M. Evens, R.R. Gargeya, and T. Leonhardt. </author> <title> Structure of a distributed simulation system. </title> <booktitle> In IEEE COMPSAC, </booktitle> <pages> pages 548-589. </pages> <publisher> IEEE, </publisher> <year> 1982. </year>
Reference-contexts: This global clock value can be maintained using centralized or decentralized approach. In a centralized approach, a dedicated processor maintains the global 3 clock. The value of the global clock is decided by the next interactive events among partici-pating LPs <ref> [8] </ref>. Different mechanisms for decentralized control of global clock for synchronous simulation were proposed [34, 2, 9, 10]. No causality error will occur in synchronous simulation. In asynchronous distributed simulation, each LP has its own local simulation clock and each LP can advance the local clock at its own pace.
Reference: [9] <author> A.I. </author> <title> Concepcion. A hierarchical computer architecture for distribued simulation. </title> <type> Technical Report MSU-ENGR-85-030, </type> <institution> Michigan State University, </institution> <year> 1985. </year>
Reference-contexts: In a centralized approach, a dedicated processor maintains the global 3 clock. The value of the global clock is decided by the next interactive events among partici-pating LPs [8]. Different mechanisms for decentralized control of global clock for synchronous simulation were proposed <ref> [34, 2, 9, 10] </ref>. No causality error will occur in synchronous simulation. In asynchronous distributed simulation, each LP has its own local simulation clock and each LP can advance the local clock at its own pace.
Reference: [10] <author> A.I. </author> <title> Concepcion. Mapping distributed simulators onto the hierarchical multibus multiprocessor architecture. </title> <booktitle> In Proceedings of SCS Distributed Simulation Conference, </booktitle> <year> 1985. </year>
Reference-contexts: In a centralized approach, a dedicated processor maintains the global 3 clock. The value of the global clock is decided by the next interactive events among partici-pating LPs [8]. Different mechanisms for decentralized control of global clock for synchronous simulation were proposed <ref> [34, 2, 9, 10] </ref>. No causality error will occur in synchronous simulation. In asynchronous distributed simulation, each LP has its own local simulation clock and each LP can advance the local clock at its own pace.
Reference: [11] <author> Alois Ferscha and Satish K. Tripathi. </author> <title> Parallel and distributed simulation of discrete systems. </title> <type> TR CS-TR-3336, </type> <institution> University of Maryland, Computer Science Department, University of Mary-land, College Park, MD 20742, </institution> <year> 1994. </year>
Reference-contexts: In PADS, events can be divided into different categories, depending on how they interact with each other (see <ref> [11] </ref> for reference). If the execution of one event does not affect another, they are concurrent events. For example, in the traffic simulation example, if there is an over-pass at the intersection, then the traffic from two orthogonal directions do not interfere with each other. <p> LP j then can reply with its local clock. This process on LP i is repeated for all empty incoming channels. After this collecting process, LP i can then advance. Fujimoto [12] as well as Ferscha and Tripathi <ref> [11] </ref> summarizes a few other variations and improvements over the basic schemes described above. 2.2 Optimistic Distributed Simulations In 1985, Jefferson published his well-known paper on virtual time [18], which started a different branch of distributed simulation | the optimistic approach (see Section 1.2).
Reference: [12] <author> Richard Fujimoto. </author> <title> Parallel discrete event simulation. </title> <journal> Communications of ACM, </journal> <volume> 33(10), </volume> <month> Oc-tobor </month> <year> 1990. </year>
Reference-contexts: Most distributed simulations are conducted in a cluster of workstations (COWS) while most parallel simulations are conducted on a multiprocessor computer with shared memory. Conservative vs. optimistic In PADS, the key to a correct simulation execution is local causality constraint: events are processed in a non-decreasing timestamp order <ref> [12] </ref>. In a conservative approach, no violation of local causality is allowed. No LP will proceed until it is certain that no local causality will be broken. In an optimistic approach, on the other hand, each LP advances at its own pace. <p> LP j then can reply with its local clock. This process on LP i is repeated for all empty incoming channels. After this collecting process, LP i can then advance. Fujimoto <ref> [12] </ref> as well as Ferscha and Tripathi [11] summarizes a few other variations and improvements over the basic schemes described above. 2.2 Optimistic Distributed Simulations In 1985, Jefferson published his well-known paper on virtual time [18], which started a different branch of distributed simulation | the optimistic approach (see Section 1.2). <p> After the restoration is done, the cancelation phase is invoked. In the cancelation phase, the event list is traversed, all the events that are incorrectly scheduled are eliminated (here we are doing an aggressive cancelation see <ref> [12] </ref> or [14] for reference).
Reference: [13] <author> Anat Gafni. </author> <title> Space management and cancellation mechanisms for Time Warp. </title> <type> Technical Report TR-85-341, </type> <institution> Univ. of Southern California, </institution> <year> 1985. </year>
Reference-contexts: The event that caused causality violation is called a straggler. The simulation is guaranteed to proceed eventually because the global clock never decreases. As long as each LP is making progress locally, the global clock will always be advanced. The rollback process is proved to be bounded <ref> [13] </ref>. Many related research projects have been spun in many research areas since the introduction of optimistic simulation in 1985. Much progress has been made in areas such as global virtual time calculation, cancelation strategy, memory management, communications overhead, performance model, among others.
Reference: [14] <author> Anat Gafni. </author> <title> Rollback mechanisms for optimistic distributed simulation systems. </title> <editor> In David Jef-ferson Brian Unger, editor, </editor> <booktitle> Distributed Simulation (Simulation Series), </booktitle> <pages> pages 61-67. </pages> <publisher> SCS, SCS, </publisher> <year> 1988. </year>
Reference-contexts: Whether or not to use the lazy cancelation strategy depends on the nature of the application. It does not seem that one is superior than the other in all applications. 2.2.3 Rollback Distance vs. Forwarding Distance Gafni <ref> [14] </ref> proved that rollback in optimistic PADS is bounded. If we define the rollback distance as the number of events affected and the forwarding distance as the number of events processed since the last rollback, one would find they follow a model of random walk. <p> After the restoration is done, the cancelation phase is invoked. In the cancelation phase, the event list is traversed, all the events that are incorrectly scheduled are eliminated (here we are doing an aggressive cancelation see [12] or <ref> [14] </ref> for reference).
Reference: [15] <author> A. Gupta, </author> <title> I.F. Akyildiz, and R.M Fujimoto. Performance analysis of Time Warp with multiple homogeneous processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17, </volume> <year> 1991. </year>
Reference-contexts: Especially he established and studied a model for Time-Warp based simulation of network of queues. * Gupta et.al. <ref> [15] </ref> studied performance model of parallel and distributed simulation for homo geneous and heterogeneous multiprocessor, shared memory environment. * Kleinrock [19] and Nicol [31] developed separately some performance bounds for Time Warp system which is the basis of optimistic parallel and distributed simulation.
Reference: [16] <author> V. Holmes. </author> <title> Parallel algorithms on multiple processor architectures. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, </institution> <year> 1978. </year>
Reference-contexts: They laid the foundation for conservative distributed simulation (see Section 1.2). The frame work of conservative distributed simulation was developed. The problems of deadlock, the schemes for deadlock resolution were discussed. Peacock et al. [34, 33] and Holmes <ref> [16] </ref> proposed mechanism of using probe message periodically to avoid deadlock. Chandy and Misra also suggested a scheme for deadlock detection and recovery [6]. Reynolds suggested using shared memory among neighbors to avoid deadlock [36]. The basic scheme of original Chandy, Misra and Bryant is the following [29].
Reference: [17] <author> Stewart Hoover and Ronald Perry. </author> <title> Simulation A Problem-Solving Approach. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Computer simulation is the process of designing a mathematical or logical model of a real system and then conducting computer-based experiments with the model to describe, explain, and predict the behavior of the real system <ref> [17] </ref>. To simulate a real system, a model of the system has to be designed and built which describes relations among the entities in the system and makes necessary assumptions about the system. Once established, the model has 1 to be translated into a particular simulation language. <p> The change of any state is caused by events occurring over the simulation time. 1.1.1 Static and Dynamic Certain simulations do not involve time, for example, Buffon's needle experiment in which a needle is dropped at random onto a plane ruled with a series of parallel lines some distance apart <ref> [17] </ref>. In these simulations, state variables change when certain events happen. But these events are not related to time. This type of simulation is static. Other simulations are dynamic. Dynamic simulation can be divided into two groups according to how state of the system is changed.
Reference: [18] <author> David R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3), </volume> <month> July </month> <year> 1985. </year>
Reference-contexts: After this collecting process, LP i can then advance. Fujimoto [12] as well as Ferscha and Tripathi [11] summarizes a few other variations and improvements over the basic schemes described above. 2.2 Optimistic Distributed Simulations In 1985, Jefferson published his well-known paper on virtual time <ref> [18] </ref>, which started a different branch of distributed simulation | the optimistic approach (see Section 1.2). Because of its great potential optimistic distributed simulation based on the concept of virtual time has caught the attention of research community and industry. <p> A virtual time system is a distributed system executing in coordination with an imaginary virtual clock that ticks virtual time. Virtual time itself is a global, one-dimensional, temporal coordinate system imposed on a distributed computation. (see ref. <ref> [18] </ref>). The concept of virtual time can be used in many different distributed computing applications, in particular distributed simulations. Virtual time is the core of optimistic distributed simulations. We can use the same system model described in Section 2.1 to show how an optimistic distributed simulation works. <p> Some of the important achievements are briefly discussed following. 2.2.1 Global Virtual Time Calculation Virtual time is the most important concept and building block of optimistic parallel and distributed simulation. Virtual time was first proposed and thoroughly studied in <ref> [18] </ref>. Jefferson in his paper [18] has the following description. Virtual time systems are subject to two fundamental semantic rules: Rule 1. The virtual send time of each message must be less than its virtual receive time. Rule 2. <p> Some of the important achievements are briefly discussed following. 2.2.1 Global Virtual Time Calculation Virtual time is the most important concept and building block of optimistic parallel and distributed simulation. Virtual time was first proposed and thoroughly studied in <ref> [18] </ref>. Jefferson in his paper [18] has the following description. Virtual time systems are subject to two fundamental semantic rules: Rule 1. The virtual send time of each message must be less than its virtual receive time. Rule 2.
Reference: [19] <author> L. Kleinrock. </author> <title> On distributed systems performance. </title> <journal> Computer Networks and ISDN Journal, </journal> <volume> 20, </volume> <year> 1990. </year>
Reference-contexts: Especially he established and studied a model for Time-Warp based simulation of network of queues. * Gupta et.al. [15] studied performance model of parallel and distributed simulation for homo geneous and heterogeneous multiprocessor, shared memory environment. * Kleinrock <ref> [19] </ref> and Nicol [31] developed separately some performance bounds for Time Warp system which is the basis of optimistic parallel and distributed simulation.
Reference: [20] <institution> Lawrence Livermore National Laboratory. The parallel tools consortium. &lt;http://www--ptools.llnl.gov/ptools.html&gt;, </institution> <year> 1993. </year>
Reference-contexts: Currently most research in PADS center around technique and methodology. We are lack of good tools to write and debug parallel and distributed simulation programs. The parallel and distributed computing community has formed The Parallel Tools Consortium (PTool) (see <ref> [20, 32] </ref> for example) to explore the tool needed for parallel and distributed programming. While much of the effort there can be used directly in PADS, simulation has its own special requirement for tools. * Design and develop user friendly interface of using PADS.
Reference: [21] <author> A.M. Law and W.D. </author> <title> Kelton. Simulation Modeling and Analysis. </title> <publisher> McGraw-Hill, </publisher> <year> 1982. </year>
Reference-contexts: Dynamic simulation can be divided into two groups according to how state of the system is changed. Discrete simulation concerns the modeling of a system as it evolves over time by a representation in which the state variables change only at a countable number of points in time <ref> [21] </ref>. Continuous simulation concerns the modeling over time of a system by a representation in which the state variables change continuously with respect to time. 1.1.2 How Simulation Is Driven A simulation can be driven by different mechanisms.
Reference: [22] <author> Boris Lubachevsky, Alan Weiss, and Adam Shwartz. </author> <title> An analysis of rollback-based simulation. </title> <booktitle> ACM Transcation on Modeling and Comptuer Simulation, </booktitle> <volume> 1(2), </volume> <month> April </month> <year> 1991. </year>
Reference-contexts: If we define the rollback distance as the number of events affected and the forwarding distance as the number of events processed since the last rollback, one would find they follow a model of random walk. Lubachevsky et. al. <ref> [22] </ref> used a model of Branching Random Walks with a Barrier to study the efficiency of rollbacks. Their work pointed out that some rollbacks may appear to terminate locally or temporarily. <p> In particular, Nicol [31] derived upper and lower bounds for optimal performance, upper bounds on Time Warp's performance, and lower bounds of a conservative protocol. * Lubachevsky et.al. <ref> [22] </ref> concluded that the average time o to complete simulation of a system with N nodes and R events on a p-processor PRAM satisfies the following. o = O R + N The first part R p is the "computation time" on each processor, the second component R N log p
Reference: [23] <author> Xiannong Meng. </author> <title> An empirical study of distributed simulation in a loosely coupled environment for a class of simulation problems. </title> <booktitle> Proceedings of IASTED International Conference on Modelling and Simulation, </booktitle> <address> Pittsburg, Pennsylvania, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Yet their computing power is steadily increasing. The use of COWS, or Cluster Of WorkStation, as primer computing engine has become a focus of many research projects. We have conducted experiments of implementing parallel and distributed simulation in COWS <ref> [23, 25, 26] </ref>. An empirical study of distributed simulation is described in [23], in which a network simulation program is distributed across a network of SPARC workstations. The program simulates a number of computers connected through a network. Each computer has an input stream of packets. <p> The use of COWS, or Cluster Of WorkStation, as primer computing engine has become a focus of many research projects. We have conducted experiments of implementing parallel and distributed simulation in COWS [23, 25, 26]. An empirical study of distributed simulation is described in <ref> [23] </ref>, in which a network simulation program is distributed across a network of SPARC workstations. The program simulates a number of computers connected through a network. Each computer has an input stream of packets.
Reference: [24] <author> Xiannong Meng. </author> <title> A space efficient algorithm for optimistic distributed discrete event simulation. </title> <booktitle> Proceedings of the Third International Conference for Young Computer Scientists, </booktitle> <address> Beijing, China, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: See <ref> [24] </ref> for more implementation and other performance details. 3.2 Parallel and Distributed Simulations in Cluster of Workstations With the fast development of modern technology, UNIX based workstations become increasingly available and less expensive. Yet their computing power is steadily increasing.
Reference: [25] <author> Xiannong Meng. </author> <title> Distributed simulation in a loosely coupled environment using the TCP/IP protocol. </title> <booktitle> In Proceedings of the Fourteenth Annual International Pheonix Conference on Computer and Communications, </booktitle> <pages> pages 122-127, </pages> <year> 1995. </year>
Reference-contexts: Yet their computing power is steadily increasing. The use of COWS, or Cluster Of WorkStation, as primer computing engine has become a focus of many research projects. We have conducted experiments of implementing parallel and distributed simulation in COWS <ref> [23, 25, 26] </ref>. An empirical study of distributed simulation is described in [23], in which a network simulation program is distributed across a network of SPARC workstations. The program simulates a number of computers connected through a network. Each computer has an input stream of packets.
Reference: [26] <author> Xiannong Meng. </author> <title> Synchronizations and rollbacks in optimistic distributed simulation schemes. </title> <type> Technical Report CS-95-8, </type> <institution> University of Texas Pan American, Department of Computer Science, 1201 W. University Dr. </institution> <address> Edinburg, TX 78539, </address> <year> 1995. </year>
Reference-contexts: Communication is a central component of any parallel and distributed simulation. Effective communication can speed up simulation, while slow, less efficient communication can degrade the simulation significantly. Both Carothers et.al. [4] and Meng <ref> [26] </ref> studied the impact of communications overhead on the performance of distributed simulations. If the simulation uses shared memory as communications channel, it is less of a problem. However, if the communication is through a local area network such as Ethernet, the overhead can be heavy. Our study [26] found the <p> and Meng <ref> [26] </ref> studied the impact of communications overhead on the performance of distributed simulations. If the simulation uses shared memory as communications channel, it is less of a problem. However, if the communication is through a local area network such as Ethernet, the overhead can be heavy. Our study [26] found the following. * Less communication among participating processes reduces the communications overhead, thus reduces the total delay. However, less communication often means less synchronization among processes, which may cause more rollbacks. <p> Yet their computing power is steadily increasing. The use of COWS, or Cluster Of WorkStation, as primer computing engine has become a focus of many research projects. We have conducted experiments of implementing parallel and distributed simulation in COWS <ref> [23, 25, 26] </ref>. An empirical study of distributed simulation is described in [23], in which a network simulation program is distributed across a network of SPARC workstations. The program simulates a number of computers connected through a network. Each computer has an input stream of packets.
Reference: [27] <author> Xiannong Meng. </author> <title> Debugging aid for distributed programs in TCP/IP based network. </title> <booktitle> In 1996 IASTED International Conference on Networks. International Association of Science and Technology Development, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: It is very difficult, often impossible, to repeat the same sequence of execution. This makes the debugging of distributed program challenging. The mechanism proposed to eliminate non-determinism is to have user control these two factors. See <ref> [27, 28] </ref> for detailed discussion. To take care of the process scheduling issue, we combine multiple user processes into a single user process executing on one machine. The communication functions in user program are completely re-directed to the library functions provided by the EMUNET where user have the control.
Reference: [28] <author> Xiannong Meng. EMUNET: </author> <title> Design and implementation a debugging aid for distributed programs in tcp/ip based network. </title> <type> Technical Report CS-96-12, </type> <institution> University of Texas Pan American, </institution> <year> 1996. </year>
Reference-contexts: It is very difficult, often impossible, to repeat the same sequence of execution. This makes the debugging of distributed program challenging. The mechanism proposed to eliminate non-determinism is to have user control these two factors. See <ref> [27, 28] </ref> for detailed discussion. To take care of the process scheduling issue, we combine multiple user processes into a single user process executing on one machine. The communication functions in user program are completely re-directed to the library functions provided by the EMUNET where user have the control.
Reference: [29] <author> Jayadev Misra. </author> <title> Distributed discrete-event simulation. Computing Survey, </title> <type> 18(1), </type> <month> March </month> <year> 1986. </year> <title> [30] nachos. The nachos system. </title> <institution> Department of Computer Science, University of California, Berkeley, </institution> <year> 1995. </year>
Reference-contexts: Chandy and Misra also suggested a scheme for deadlock detection and recovery [6]. Reynolds suggested using shared memory among neighbors to avoid deadlock [36]. The basic scheme of original Chandy, Misra and Bryant is the following <ref> [29] </ref>. Without losing generality it is assumed that one LP corresponding to one PP. Each LP j has two one-way communication channels for every other LP i in the system, one for sending message to LP i (out-going channel), the other for receiving message from LP i (incoming channel).
Reference: [31] <author> D.M. Nicol. </author> <title> Performance bounds on parallel self-initiating discrete event simulation. </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> 1(1), </volume> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Especially he established and studied a model for Time-Warp based simulation of network of queues. * Gupta et.al. [15] studied performance model of parallel and distributed simulation for homo geneous and heterogeneous multiprocessor, shared memory environment. * Kleinrock [19] and Nicol <ref> [31] </ref> developed separately some performance bounds for Time Warp system which is the basis of optimistic parallel and distributed simulation. In particular, Nicol [31] derived upper and lower bounds for optimal performance, upper bounds on Time Warp's performance, and lower bounds of a conservative protocol. * Lubachevsky et.al. [22] concluded that <p> queues. * Gupta et.al. [15] studied performance model of parallel and distributed simulation for homo geneous and heterogeneous multiprocessor, shared memory environment. * Kleinrock [19] and Nicol <ref> [31] </ref> developed separately some performance bounds for Time Warp system which is the basis of optimistic parallel and distributed simulation. In particular, Nicol [31] derived upper and lower bounds for optimal performance, upper bounds on Time Warp's performance, and lower bounds of a conservative protocol. * Lubachevsky et.al. [22] concluded that the average time o to complete simulation of a system with N nodes and R events on a p-processor PRAM satisfies the following.
Reference: [32] <author> Cherri Pancake. </author> <title> A collaborative effort in parallel tool design. </title> <type> Tech. Report Technical Report 94-80-14, </type> <institution> Oregon State University, </institution> <year> 1994. </year>
Reference-contexts: Currently most research in PADS center around technique and methodology. We are lack of good tools to write and debug parallel and distributed simulation programs. The parallel and distributed computing community has formed The Parallel Tools Consortium (PTool) (see <ref> [20, 32] </ref> for example) to explore the tool needed for parallel and distributed programming. While much of the effort there can be used directly in PADS, simulation has its own special requirement for tools. * Design and develop user friendly interface of using PADS.
Reference: [33] <author> J.K. Peacock, J.W. Wong, and E.G. Manning. </author> <title> A distributed approach to queueing network simulation. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 399-406. </pages> <publisher> IEEE, </publisher> <year> 1979. </year>
Reference-contexts: They laid the foundation for conservative distributed simulation (see Section 1.2). The frame work of conservative distributed simulation was developed. The problems of deadlock, the schemes for deadlock resolution were discussed. Peacock et al. <ref> [34, 33] </ref> and Holmes [16] proposed mechanism of using probe message periodically to avoid deadlock. Chandy and Misra also suggested a scheme for deadlock detection and recovery [6]. Reynolds suggested using shared memory among neighbors to avoid deadlock [36].
Reference: [34] <author> J.K. Peacock, J.W. Wong, and E.G. Manning. </author> <title> Distributed simulation using a network of processors. </title> <journal> Computer Network, </journal> <volume> 3(1), </volume> <year> 1979. </year> <month> 23 </month>
Reference-contexts: In a centralized approach, a dedicated processor maintains the global 3 clock. The value of the global clock is decided by the next interactive events among partici-pating LPs [8]. Different mechanisms for decentralized control of global clock for synchronous simulation were proposed <ref> [34, 2, 9, 10] </ref>. No causality error will occur in synchronous simulation. In asynchronous distributed simulation, each LP has its own local simulation clock and each LP can advance the local clock at its own pace. <p> They laid the foundation for conservative distributed simulation (see Section 1.2). The frame work of conservative distributed simulation was developed. The problems of deadlock, the schemes for deadlock resolution were discussed. Peacock et al. <ref> [34, 33] </ref> and Holmes [16] proposed mechanism of using probe message periodically to avoid deadlock. Chandy and Misra also suggested a scheme for deadlock detection and recovery [6]. Reynolds suggested using shared memory among neighbors to avoid deadlock [36].
Reference: [35] <author> P. Reiher, R. Fujimoto, S. Bellenot, and D. Jefferson. </author> <title> Cancellation strategies in optimistic execution systems. </title> <editor> In David Nichol, editor, </editor> <booktitle> Distributed Simulation, </booktitle> <pages> pages 112 - 121. </pages> <publisher> SCS, SCS, </publisher> <year> 1990. </year>
Reference-contexts: When the event is proved not needed (for example, when the local event time is about to surpass that of to-be-canceled event), then the anti-message is sent out to cancel the original event. Study by Reiher and Fujimoto <ref> [35] </ref> shows that lazy cancelation can arbitrarily outperform aggressive cancelation and vice versa. Whether or not to use the lazy cancelation strategy depends on the nature of the application. It does not seem that one is superior than the other in all applications. 2.2.3 Rollback Distance vs.
Reference: [36] <author> P. Reynolds. </author> <title> A shared resource algorithm for distributed simulation. </title> <booktitle> In Proceedings of the 9th International Symposium on Computer Architecture, </booktitle> <pages> pages 259-266. </pages> <publisher> IEEE, </publisher> <year> 1982. </year>
Reference-contexts: Peacock et al. [34, 33] and Holmes [16] proposed mechanism of using probe message periodically to avoid deadlock. Chandy and Misra also suggested a scheme for deadlock detection and recovery [6]. Reynolds suggested using shared memory among neighbors to avoid deadlock <ref> [36] </ref>. The basic scheme of original Chandy, Misra and Bryant is the following [29]. Without losing generality it is assumed that one LP corresponding to one PP.
Reference: [37] <author> Robert Ronngren, Luis Barriga, and Rassul Ayani. </author> <title> An incremental benchmark suite for performance tuning of parallel discrete event simulation. </title> <booktitle> In Proceedings of the 29th Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 373-382, </pages> <year> 1996. </year>
Reference-contexts: simulation of a system with N nodes and R events on a p-processor PRAM satisfies the following. o = O R + N The first part R p is the "computation time" on each processor, the second component R N log p is roughly the communications overhead. * Ronngren et.al. <ref> [37] </ref> developed a suite of benchmark program to measure the performance of parallel discrete event driven simulation.
Reference: [38] <author> Andrew S. Tanenbaum. </author> <title> Computer Networks. </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference: [39] <author> Ronald W. Tarr and John W. Jacobs. </author> <title> Distributed Interactive Simulation (DIS). </title> <booktitle> In 1994 Summer Computer Simulation Conference. the Society for Computer Simulation, </booktitle> <address> La Jolla, CA, </address> <month> July 18-20, </month> <year> 1994 1994. </year>
Reference-contexts: DIS is a synthetic environment within which humans may interact through simulations at multiple sites networked using compliant architectures, modeling, protocols, standards, and data bases <ref> [39] </ref>. While PADS concentrates on statistical behaviors of the system being simulated, DIS focuses on interactive and graphical representation aspects of the system. DIS has proven to be an important technology and is finding many applications in different disciplines. Developed originally by U.S.
Reference: [40] <institution> Stanford University. The PARADISE project. &lt;http://www-dsg.stanford.edu/paradise.html&gt;. </institution>
Reference-contexts: DIS has proven to be an important technology and is finding many applications in different disciplines. Developed originally by U.S. Army as a mechanism to simulate warfare and battlefield situation, DIS technology is being applied to many other areas such as education and training <ref> [40] </ref>. DIS combines technologies of distributed simulation and virtual reality to offer a number of useful capabilities such as creating an infinite set of realistic synthetic environment at relatively inexpensive cost. DIS has grown into a rich field of its own.
Reference: [41] <author> Christopher Young and Philip Wilsey. </author> <title> Optimistic fossil collection for Time Warp simulation. </title> <booktitle> In Proceedings of the 29th Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 364-372, </pages> <year> 1996. </year>
Reference-contexts: However, returning these fossil to memory requires processing power. Two different strategies exist to implement fossil collection: lazy strategy which does not collect fossil until running out memory (or below some threshold); aggressive strategy which collects fossil at a fixed interval, 8 or upon certain events. Young et. al. <ref> [41] </ref> proposed an optimistic fossil collection strategy where the fossil can be collected aggressively at a certain risk of restarting the simulation at some check point. <p> This is often a good indicator whether or not the simulation is running effectively. 9 2.2.6 Performance Model While a large amount of literature on parallel and distributed simulations exists, relatively little is available on the subject of performance model of parallel and distributed simulations. * Young et. al. <ref> [41] </ref> used statistical method to model the behavior of fossil collection algorithm they proposed. They developed two models. The first one is based on rollback length, where for a given failure rate ff and the probability of rollback in one time unit p, the rollback distance can be calculated.
References-found: 40

