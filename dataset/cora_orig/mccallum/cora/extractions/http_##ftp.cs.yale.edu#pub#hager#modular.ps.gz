URL: http://ftp.cs.yale.edu/pub/hager/modular.ps.gz
Refering-URL: http://ftp.cs.yale.edu/pub/hager/
Root-URL: http://www.cs.yale.edu
Email: Email: hager@cs.yale.edu  
Phone: Phone: 203 432-6432  
Title: A Modular System for Robust Positioning Using Feedback from Stereo Vision  
Author: Gregory D. Hager 
Date: March 11, 1997  
Address: New Haven, CT 06520-8285  
Affiliation: Department of Computer Science Yale University  
Note: A revised version of this article will appear in The IEEE Transactions on Robotics and Automation  
Abstract: This article introduces a modular framework for robot motion control using stereo vision. The approach is based on a small number of generic motion control operations referred to as primitive skills. Each primitive skill uses visual feedback to enforce a specific task-space kinematic constraint between a robot end-effector and a set of target features. By observing both the end-effector and target features, primitive skills are able to position with an accuracy that is independent of errors in hand-eye calibration. Furthermore, primitive skills are easily combined to form more complex kinematic constraints as required by different applications. These control laws have been integrated into a system that performs tracking and control on a single processor at real-time rates. Experiments with this system have shown that it is extremely accurate, and that it is insensitive to camera calibration error. The system has been applied to a number of example problems, showing that modular, high precision, vision-based motion control is easily achieved with off-the-shelf hardware. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.K. Allen, B. Yoshimi, and A. Timcenko. </author> <title> Hand-eye coordination for robotics tracking and grasping. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 33-70. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction The problem of "visual servoing" | guiding a robot using visual feedback | has been an area of active study for several decades [7]. Over the last several years, a great deal of progress has been made on both theoretical and applied aspects of this problem <ref> [1, 8, 10, 14, 29, 30, 36] </ref>. However, in spite of these advances, vision-based robotic systems are still the exception rather than the rule. <p> Prior depth estimates [10], adaptive estimation [29] or metric information on the target object (from which depth can be inferred) are common solutions to this problem. Two cameras in a stereo arrangement can be used to provide complete three-dimensional information about the environment <ref> [1, 2, 6, 18, 21, 19, 24, 30] </ref>. Stereo-based motion control systems have been implemented using both free-standing and arm-mounted cameras, although the former arrangement is more common.
Reference: [2] <author> Russell L. Anderson. </author> <title> Dynamic sensing in a ping-pong playing robot. </title> <journal> IEEE Trans. Robot. Automat. </journal>
Reference-contexts: Prior depth estimates [10], adaptive estimation [29] or metric information on the target object (from which depth can be inferred) are common solutions to this problem. Two cameras in a stereo arrangement can be used to provide complete three-dimensional information about the environment <ref> [1, 2, 6, 18, 21, 19, 24, 30] </ref>. Stereo-based motion control systems have been implemented using both free-standing and arm-mounted cameras, although the former arrangement is more common.
Reference: [3] <author> R. R. Burridge, A. A. Rizzi, and D. E. Koditschek. </author> <title> Controller composition for dynamically dexterous tasks. </title> <booktitle> In Int. Symp. </booktitle> <institution> Robot. Res., </institution> <year> 1995. </year>
Reference-contexts: For example, as noted experimentally, the motion to place a 23 screwdriver onto a screw should only take place when the tip of the screwdriver lies along the axis of the screw. Interesting work along these lines has been recently presented by [25] and <ref> [3] </ref>. An important open problem at the control level is the development of globally asymptotically stable control methods for visual control. The methods described in this article are locally stable, and perform well away from singularities. However, trajectories that move through a singularity cannot be performed.
Reference: [4] <author> A. Castano and S. A. Hutchinson. </author> <title> Visual compliance: </title> <journal> Task-directed visual servo control. IEEE Trans. Robot. Automat., </journal> <volume> 10(3) </volume> <pages> 334-342, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The second criteria is the number of cameras and their kinematic relationship to the end-effector. A majority of the recently constructed visual servoing systems employ a single camera, typically mounted on the arm itself e:g: <ref> [4, 8, 10, 17, 28, 29, 36] </ref>. A single camera minimizes the visual processing needed to perform visual servoing, however the loss of depth information complicates the control design as well as limiting the types of positioning operations than can be implemented.
Reference: [5] <author> F. Chaumette, P. Rives, and B. Espiau. </author> <title> Classification and realization of the different vision-based tasks. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 199-228. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: = x e ffi e P; (1) and the constraint on end-effector pose is then E 0 (x e ; x t ) = x e ffi e P x t ffi t S: (3) This is a constraint of degree 3 which is kinematically equivalent to a spherical joint <ref> [5] </ref>. 1 This is closely related to the notion of "class" defined in [8]. 4 The visual servoing problem is to define a control system that moves the end-effector into a configuration in which the task-space error is zero.
Reference: [6] <author> W.Z. Chen, U.A. Korde, </author> <title> and S.B. Skaar. Position control experiments using vision. </title> <journal> Int. J. Robot. Res., </journal> <volume> 13(3) </volume> <pages> 199-208, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Prior depth estimates [10], adaptive estimation [29] or metric information on the target object (from which depth can be inferred) are common solutions to this problem. Two cameras in a stereo arrangement can be used to provide complete three-dimensional information about the environment <ref> [1, 2, 6, 18, 21, 19, 24, 30] </ref>. Stereo-based motion control systems have been implemented using both free-standing and arm-mounted cameras, although the former arrangement is more common. <p> Reconstructed pose forms the basis of a position-based servo algorithm for aligning and positioning the gripper relative to the object. The affine approximation leads to a linear estimation and control problem, however it also means that the system calibration is only locally valid. A similar image-based system appears in <ref> [6, 32] </ref> with the difference that an attempt is made to modify the approximate linear model online. This article describes an image-based ECL system that uses a globally valid perspective model.
Reference: [7] <author> P. I. Corke. </author> <title> Visual control of robot manipulators|a review. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 1-32. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year> <month> 24 </month>
Reference-contexts: 1 Introduction The problem of "visual servoing" | guiding a robot using visual feedback | has been an area of active study for several decades <ref> [7] </ref>. Over the last several years, a great deal of progress has been made on both theoretical and applied aspects of this problem [1, 8, 10, 14, 29, 30, 36]. However, in spite of these advances, vision-based robotic systems are still the exception rather than the rule. <p> The final section describes work currently in progress. 2 Related Work Visual servoing has been an active area of research over the last 30 years with the result that a large variety of experimental systems have been built (see <ref> [7] </ref> for an extensive review). Systems can be categorized according to several properties as discussed below.
Reference: [8] <author> B. Espiau, F. Chaumette, and P. Rives. </author> <title> A New Approach to Visual Servoing in Robotics. </title> <journal> IEEE Trans. Robot. Automat., </journal> <volume> 8 </volume> <pages> 313-326, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction The problem of "visual servoing" | guiding a robot using visual feedback | has been an area of active study for several decades [7]. Over the last several years, a great deal of progress has been made on both theoretical and applied aspects of this problem <ref> [1, 8, 10, 14, 29, 30, 36] </ref>. However, in spite of these advances, vision-based robotic systems are still the exception rather than the rule. <p> However, in spite of these advances, vision-based robotic systems are still the exception rather than the rule. In particular, if we restrict our attention to vision-based positioning relative to a static (unmoving) target, it can be argued that the basic principles for implementing visual feedback are by now well-understood <ref> [8, 10, 22] </ref>. Why is visual servoing not in wider use? One reason is the fact that vision is itself a complex problem. In order to provide information from vision at servo rates, most systems rely on task-specific image processing algorithms, often combined with specialized hardware. <p> The second criteria is the number of cameras and their kinematic relationship to the end-effector. A majority of the recently constructed visual servoing systems employ a single camera, typically mounted on the arm itself e:g: <ref> [4, 8, 10, 17, 28, 29, 36] </ref>. A single camera minimizes the visual processing needed to perform visual servoing, however the loss of depth information complicates the control design as well as limiting the types of positioning operations than can be implemented. <p> Modeling the effect of visual feedback is a central part of this approach. As suggested by Espiau et al: <ref> [8] </ref>, one way to model the effect of visual feedback is in terms of the constraints it imposes on the position of the manipulator. <p> The first part describes a general framework for vision-based control of position and points out several important properties of the approach. The development follows that of <ref> [8] </ref>, the formal underpinning of which are discussed in greater detail in [31]. The second part reviews results related to the projection and reconstruction of points and lines from stereo images. <p> is then E 0 (x e ; x t ) = x e ffi e P x t ffi t S: (3) This is a constraint of degree 3 which is kinematically equivalent to a spherical joint [5]. 1 This is closely related to the notion of "class" defined in <ref> [8] </ref>. 4 The visual servoing problem is to define a control system that moves the end-effector into a configuration in which the task-space error is zero. The end-effector is modeled as a Cartesian positioning device with negligible dynamics. <p> Note that when J is square, J + = J 1 : 2 It is interesting to note that in that case of a direct visual servo system, it has been shown that bounded-input bounded-output stability can be guaranteed if M is positive definite <ref> [8, 31] </ref>.
Reference: [9] <author> Olivier Faugeras. </author> <title> Three Dimensional Computer Vision. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: To simplify the exposition, all observed values are expressed in normalized coordinates in which values are scaled to metric units for a camera fitted with a unit focal length lens <ref> [9] </ref>. The units for linear and angular quantities are millimeters and degrees, respectively, unless otherwise specified. <p> The vector l i parameterizing the projection of L in normalized coordinates in camera i is given by L 0 = R i (L v fi (L d c i )) L 0 L 0 2 y 8 These are also known as the Plucker coodinates for the line <ref> [9] </ref>.
Reference: [10] <author> J.T. Feddema, C.S.G. Lee, and O.R. Mitchell. </author> <title> Weighted selection of image features for resolved rate visual feedback control. </title> <journal> IEEE Trans. Robot. Automat, </journal> <volume> 7(1) </volume> <pages> 31-47, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The problem of "visual servoing" | guiding a robot using visual feedback | has been an area of active study for several decades [7]. Over the last several years, a great deal of progress has been made on both theoretical and applied aspects of this problem <ref> [1, 8, 10, 14, 29, 30, 36] </ref>. However, in spite of these advances, vision-based robotic systems are still the exception rather than the rule. <p> However, in spite of these advances, vision-based robotic systems are still the exception rather than the rule. In particular, if we restrict our attention to vision-based positioning relative to a static (unmoving) target, it can be argued that the basic principles for implementing visual feedback are by now well-understood <ref> [8, 10, 22] </ref>. Why is visual servoing not in wider use? One reason is the fact that vision is itself a complex problem. In order to provide information from vision at servo rates, most systems rely on task-specific image processing algorithms, often combined with specialized hardware. <p> The second criteria is the number of cameras and their kinematic relationship to the end-effector. A majority of the recently constructed visual servoing systems employ a single camera, typically mounted on the arm itself e:g: <ref> [4, 8, 10, 17, 28, 29, 36] </ref>. A single camera minimizes the visual processing needed to perform visual servoing, however the loss of depth information complicates the control design as well as limiting the types of positioning operations than can be implemented. <p> A single camera minimizes the visual processing needed to perform visual servoing, however the loss of depth information complicates the control design as well as limiting the types of positioning operations than can be implemented. Prior depth estimates <ref> [10] </ref>, adaptive estimation [29] or metric information on the target object (from which depth can be inferred) are common solutions to this problem. Two cameras in a stereo arrangement can be used to provide complete three-dimensional information about the environment [1, 2, 6, 18, 21, 19, 24, 30].
Reference: [11] <author> A. Fox and S. Hutchinson. </author> <title> Exploiting visual constraints in the synthesis of uncertainty-tolerant motion plans. </title> <journal> IEEE Trans. Robot. Automat., </journal> <volume> 11 </volume> <pages> 56-71, </pages> <year> 1995. </year>
Reference-contexts: Work is proceeding on occlusion detection and compensation. In particular, the design of motion strategies that plan an occlusion-free path o*ine or online are of interest. O*ine vision planning using visibility models and a prior world model information has already been investigated <ref> [33, 11] </ref>. Online motion compensation based on occlusion detection does not appear to have been considered to date. Work is also proceeding on extending the framework to more complex task representations.
Reference: [12] <author> G. Franklin, J. Powell, and A. Emami-Naeini. </author> <title> Feedback Control of Dynamic Systems. </title> <publisher> Addison-Wesley, </publisher> <address> 2nd edition, </address> <year> 1991. </year>
Reference-contexts: For example, any configuration at which feature projection is singular is also a singular configuration for the error function. All feedback algorithms in this article employ image errors in proportional control arrangements <ref> [12] </ref> as follows. Define J e (x e ) = @x e fi fi : Recall r denotes the end-effector velocity screw. <p> Then (6) becomes _ e = Ju = k J b J 1 e = k Me; (7) where M = J b J 1 : A differential equation of the form _ e = k Me is asymptotically stable if the eigenvalues of M have strictly positive real parts <ref> [12] </ref>. In general, the entries of the Jacobian matrix are continuous functions of the system calibration parameters, and therefore the eigenvalues of M vary continuously with the calibration parameters. <p> true system Jacobian and ^ J pp (P) represents the Jacobian matrix computed using ^ l and ^ as well as observed values of P: A noted earlier, such a system is asymptotically stable at P if and only if the eigenvalues of M (P) have strictly positive real parts <ref> [12] </ref>. First fix l = ^ l = 1 and consider the effect of errors in the estimate of camera vergence. Expression (46) for this case was constructed using a symbolic mathematics package, and the equations describing the points at which the system is asymptotically stable were computed. <p> This system has characteristic polynomial x 2 + x +tk = 0 <ref> [12] </ref>.
Reference: [13] <author> G. D. Hager. </author> <title> Calibration-free visual control using projective invariance. </title> <booktitle> In Proc. 5th Int. Conf. Comp. Vision, </booktitle> <pages> pages 1009-1015, </pages> <year> 1995. </year>
Reference-contexts: = (S P) fi (Q P): (33) Although E pl is a mapping into &lt;(3); placing a point onto a line is a constraint of degree 2: It is interesting to note that this is a positioning operation which cannot be performed in a calibration insensitive fashion using position-based control <ref> [13] </ref>. The points P and Q define a line in space. Let L parameterize this line. <p> Because the central axes of the screwdriver and the screw are not directly observable, other image information must be used to compute their locations. The occluding contours of the screwdriver shaft and the screw provide enough information to determine the "virtual" projection of the central axis <ref> [13] </ref>. The intersection of the axis with tip of the screwdriver 13 and the top of the screw respectively form fixed observable points on each as required for line parameterization. One possibility for solving this problem is to extend the set of primitive skills to include a "line-to-line" positioning primitive. <p> Online motion compensation based on occlusion detection does not appear to have been considered to date. Work is also proceeding on extending the framework to more complex task representations. In recent work <ref> [13] </ref>, it was noted that projective invariants [26] provide a basis for specifying robot positions and motion independent of geometric reconstructions, and consequently independent of camera calibration.
Reference: [14] <author> G. D. Hager, W-C. Chang, and A. S. Morse. </author> <title> Robot hand-eye coordination based on stereo vision. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 15(1) </volume> <pages> 30-39, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: 1 Introduction The problem of "visual servoing" | guiding a robot using visual feedback | has been an area of active study for several decades [7]. Over the last several years, a great deal of progress has been made on both theoretical and applied aspects of this problem <ref> [1, 8, 10, 14, 29, 30, 36] </ref>. However, in spite of these advances, vision-based robotic systems are still the exception rather than the rule. <p> Few ECL systems have been reported in the literature. Wijesoma et al: [35] describe an ECL monocular hand-eye system for planar positioning using image feedback. An ECL solution to the problem of three DOF positioning using stereo is described in <ref> [14] </ref>. A six DOF ECL servoing system employing stereo vision is described Hollinghurst and Cipolla [19]. They employ an affine approximation to the perspective transformation to reconstruct the position and orientation of planes on an object and on a robot manipulator. <p> Thus, the system cannot execute a positioning operation that requires stationing at any point along the camera baseline. To solve this problem, first consider computing only pure translations, v: The solution to this problem was presented in <ref> [14] </ref>. <p> Eventually, the calibration describes a camera system which could not physically produce the actual observations of P and P then becomes an unstable equilibrium. Numerical computation of the set of stable points for problems of higher degree has shown qualitatively similar effects <ref> [14] </ref>. 16 Suppose now that = ^ and consider errors in the estimated baseline ^ l: In the closed loop equa-tion, the ratio ^ l=l appears as a gain term, and therefore does not affect stability of the continuous time system.
Reference: [15] <author> G. D. Hager and K. Toyama. XVision: </author> <title> Combining image warping and geometric constraints for fast visual tracking. </title> <booktitle> In Computer Vision-ECCV '96 (Fourth European Conference on Computer Vision), </booktitle> <pages> pages 507-517. </pages> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference-contexts: The visual information needed to instantiate hand-eye skills is extremely local: the location of features such as corners and edges in one or more images. The image processing needed to extract these features is straightforward, and can easily be performed on standard workstations or PC's <ref> [15] </ref>. The interface to the robot hardware is a one-way stream of velocity or position commands expressed in robot base coordinates. This enhances 1 portability and modularity, making it simple to retro-fit an existing system with visual control capabilities. <p> As the system runs, it logs 5 minutes of joint motion information at 20 Hz which can be used to examine the dynamic behavior of the system. All test cases were designed not to pass near singularities. The XVision tracking system <ref> [15] </ref> provides visual input for the controller. XVision is designed to provides extremely fast edge detection on a memory-mapped framebuffer. In addition, it supports simultaneous tracking of multiple edge segments, and can also enforce constraints among segments.
Reference: [16] <author> R. M. Haralick and L. G. Shapiro. </author> <title> Computer and Robot Vision: Volume II. </title> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference-contexts: The development follows that of [8], the formal underpinning of which are discussed in greater detail in [31]. The second part reviews results related to the projection and reconstruction of points and lines from stereo images. More detail can be found in standard vision references such as <ref> [16] </ref>. 3 3.1 A Framework for Vision-Based Control of Position Unless otherwise noted, all positions, orientations and feature coordinates are expressed relative to the robot base coordinate system denoted by W: The pose of an object in this coordinate system is represented by a pair x = (t; R); t 2 <p> If the physical wrist center is not directly observable it is well-known that any three non-collinear points with known end-effector coordinates can be used to reconstruct its location <ref> [16, Chapter 14] </ref>. 4.4 Sensitivity to Calibration Errors In the absence of noise and calibration error, the systems defined above are guaranteed to be asymptotically stable at points where the Jacobian matrix is nonsingular.
Reference: [17] <author> K. Hashimoto. </author> <title> LQ optimal and nonlinear approaches to visual servoing. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 165-198. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: The second criteria is the number of cameras and their kinematic relationship to the end-effector. A majority of the recently constructed visual servoing systems employ a single camera, typically mounted on the arm itself e:g: <ref> [4, 8, 10, 17, 28, 29, 36] </ref>. A single camera minimizes the visual processing needed to perform visual servoing, however the loss of depth information complicates the control design as well as limiting the types of positioning operations than can be implemented.
Reference: [18] <author> G. Hirzinger, G. Grunwald, B. Brunner, and H. Heindl. </author> <title> A sensor-based telerobotic system for the space robot experiment ROTEX. </title> <booktitle> 2. Int. Symposium on Experimental Robotics, 1991. </booktitle> <address> Toulouse, France. </address>
Reference-contexts: Prior depth estimates [10], adaptive estimation [29] or metric information on the target object (from which depth can be inferred) are common solutions to this problem. Two cameras in a stereo arrangement can be used to provide complete three-dimensional information about the environment <ref> [1, 2, 6, 18, 21, 19, 24, 30] </ref>. Stereo-based motion control systems have been implemented using both free-standing and arm-mounted cameras, although the former arrangement is more common.
Reference: [19] <author> N. Hollinghurst and R. Cipolla. </author> <title> Uncalibrated stereo hand eye coordination. </title> <journal> Image and Vision Computing, </journal> <volume> 12(3) </volume> <pages> 187-192, </pages> <year> 1994. </year>
Reference-contexts: Prior depth estimates [10], adaptive estimation [29] or metric information on the target object (from which depth can be inferred) are common solutions to this problem. Two cameras in a stereo arrangement can be used to provide complete three-dimensional information about the environment <ref> [1, 2, 6, 18, 21, 19, 24, 30] </ref>. Stereo-based motion control systems have been implemented using both free-standing and arm-mounted cameras, although the former arrangement is more common. <p> Wijesoma et al: [35] describe an ECL monocular hand-eye system for planar positioning using image feedback. An ECL solution to the problem of three DOF positioning using stereo is described in [14]. A six DOF ECL servoing system employing stereo vision is described Hollinghurst and Cipolla <ref> [19] </ref>. They employ an affine approximation to the perspective transformation to reconstruct the position and orientation of planes on an object and on a robot manipulator. Reconstructed pose forms the basis of a position-based servo algorithm for aligning and positioning the gripper relative to the object.
Reference: [20] <author> B.K.P. Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: Finally, when dealing with vector or matrix quantities, the notation (a; b) is shorthand for the column concatenation (stacking) of the vectors a and b and (a j b) is shorthand for the row concatenation of a and b: 3 This differs somewhat from use in e:g: <ref> [20] </ref> where the baseline is the line segment defined by the two center points. 7 Points A point in three-dimensional space is written in uppercase boldface Roman letters, e:g: P or S: A subscripted lowercase boldface letter, e:g: p i denotes the projection of P in camera i: The stereo projection
Reference: [21] <author> K. Hosoda and M. Asada. </author> <title> Versatile visual servoing without knowledge of true jacobian. </title> <booktitle> In IEEE Int. Workshop on Intelligent Robots and Systems, </booktitle> <pages> pages 186-191. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: Prior depth estimates [10], adaptive estimation [29] or metric information on the target object (from which depth can be inferred) are common solutions to this problem. Two cameras in a stereo arrangement can be used to provide complete three-dimensional information about the environment <ref> [1, 2, 6, 18, 21, 19, 24, 30] </ref>. Stereo-based motion control systems have been implemented using both free-standing and arm-mounted cameras, although the former arrangement is more common.
Reference: [22] <author> S. Hutchinson, G.D. Hager, and P. Corke. </author> <title> A tutorial introduction to visual servo control. </title> <journal> IEEE Trans. Robot. Automat, </journal> <volume> 12(5), </volume> <year> 1996. </year>
Reference-contexts: However, in spite of these advances, vision-based robotic systems are still the exception rather than the rule. In particular, if we restrict our attention to vision-based positioning relative to a static (unmoving) target, it can be argued that the basic principles for implementing visual feedback are by now well-understood <ref> [8, 10, 22] </ref>. Why is visual servoing not in wider use? One reason is the fact that vision is itself a complex problem. In order to provide information from vision at servo rates, most systems rely on task-specific image processing algorithms, often combined with specialized hardware. <p> In particular, for position-based systems such as the stereo systems mentioned above, the fact that they are EOL means that the positioning accuracy of the system is limited by the accuracy of stereo reconstruction and the accuracy of the hand-eye calibration <ref> [22] </ref>. A system that observes both the manipulator and the target will be referred to as an "endpoint-closed-loop" (ECL) system. Few ECL systems have been reported in the literature. Wijesoma et al: [35] describe an ECL monocular hand-eye system for planar positioning using image feedback.
Reference: [23] <author> C.P. Lu, E. J. Mjolsness, and G. D. Hager. </author> <title> Online computation of exterior orientation with application to hand-eye calibration. </title> <booktitle> Mathematical and Computer Modeling, </booktitle> <volume> 24(5) </volume> <pages> 121-143, </pages> <year> 1996. </year>
Reference-contexts: Specifics of the tracking setup for each application are described below. The hand-eye system was calibrated by tracking a point on the manipulator as it moved to a series of positions, and applying a least-squares minimization to generate the calibration parameters <ref> [23] </ref>. 5.1 Accuracy Several experiments were performed to determine the positioning accuracy and stability of the control methods. Stereo images from the experimental setup are shown in Figure 4.
Reference: [24] <author> N. Maru, H. Kase, A. Nishikawa, and F. Miyazaki. </author> <title> Manipulator control by visual servoing with the stereo vision. </title> <booktitle> In IEEE Int. Workshop on Intelligent Robots and Systems, </booktitle> <pages> pages 1866-1870. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: Prior depth estimates [10], adaptive estimation [29] or metric information on the target object (from which depth can be inferred) are common solutions to this problem. Two cameras in a stereo arrangement can be used to provide complete three-dimensional information about the environment <ref> [1, 2, 6, 18, 21, 19, 24, 30] </ref>. Stereo-based motion control systems have been implemented using both free-standing and arm-mounted cameras, although the former arrangement is more common.
Reference: [25] <author> J.D. Morrow, B.J. Nelson, and P.K. Khosla. </author> <title> Vision and force driven sensorimotor primitives for robotics assembly skills. </title> <booktitle> In IEEE Int. Workshop on Intelligent Robots and Systems, </booktitle> <pages> pages 234-240. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year> <month> 25 </month>
Reference-contexts: For example, as noted experimentally, the motion to place a 23 screwdriver onto a screw should only take place when the tip of the screwdriver lies along the axis of the screw. Interesting work along these lines has been recently presented by <ref> [25] </ref> and [3]. An important open problem at the control level is the development of globally asymptotically stable control methods for visual control. The methods described in this article are locally stable, and perform well away from singularities. However, trajectories that move through a singularity cannot be performed.
Reference: [26] <author> J. Mundy and A. Zisserman. </author> <title> Geometric Invariance in Computer Vision. </title> <publisher> MIT Press, </publisher> <address> Cam--bridge, Mass., </address> <year> 1992. </year>
Reference-contexts: Online motion compensation based on occlusion detection does not appear to have been considered to date. Work is also proceeding on extending the framework to more complex task representations. In recent work [13], it was noted that projective invariants <ref> [26] </ref> provide a basis for specifying robot positions and motion independent of geometric reconstructions, and consequently independent of camera calibration. Development of these concepts is currently underway, including both the visual tracking methods needed to compute projective invariants, and the design and implementation of vision-based motion strategies that employ invariants.
Reference: [27] <author> J.R. Munkres. </author> <title> Analysis on Manifolds. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: x t ) = E (C e (x e ); C t (x t )): Suppose that E 0 (x e ; x t ) = 0; x t is held fixed, and E 0 considered as a function of x e satisfies the conditions of the implicit function theorem <ref> [27] </ref>. Then in the neighborhood of x e ; the task error function defines a manifold of dimension n: This manifold represents the directions in which the manipulator can move while maintaining the desired kinematic relationship with the target.
Reference: [28] <author> B. Nelson and P. K. Khosla. </author> <title> Increasing the tracking region of an eye-in-hand system by singularity and joint limit avoidance. </title> <booktitle> In Proc. IEEE Int. Conf. Robot. and Automat., </booktitle> <pages> pages 418-423. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: The second criteria is the number of cameras and their kinematic relationship to the end-effector. A majority of the recently constructed visual servoing systems employ a single camera, typically mounted on the arm itself e:g: <ref> [4, 8, 10, 17, 28, 29, 36] </ref>. A single camera minimizes the visual processing needed to perform visual servoing, however the loss of depth information complicates the control design as well as limiting the types of positioning operations than can be implemented.
Reference: [29] <author> N. P. Papanikolopoulos, P. K. Khosla, and T. Kanade. </author> <title> Visual Tracking of a Moving Target by a Camera Mounted on a Robot: A Combination of Vision and Control. </title> <journal> IEEE Trans. Robot. Automat., </journal> <volume> 9(1) </volume> <pages> 14-35, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction The problem of "visual servoing" | guiding a robot using visual feedback | has been an area of active study for several decades [7]. Over the last several years, a great deal of progress has been made on both theoretical and applied aspects of this problem <ref> [1, 8, 10, 14, 29, 30, 36] </ref>. However, in spite of these advances, vision-based robotic systems are still the exception rather than the rule. <p> The second criteria is the number of cameras and their kinematic relationship to the end-effector. A majority of the recently constructed visual servoing systems employ a single camera, typically mounted on the arm itself e:g: <ref> [4, 8, 10, 17, 28, 29, 36] </ref>. A single camera minimizes the visual processing needed to perform visual servoing, however the loss of depth information complicates the control design as well as limiting the types of positioning operations than can be implemented. <p> A single camera minimizes the visual processing needed to perform visual servoing, however the loss of depth information complicates the control design as well as limiting the types of positioning operations than can be implemented. Prior depth estimates [10], adaptive estimation <ref> [29] </ref> or metric information on the target object (from which depth can be inferred) are common solutions to this problem. Two cameras in a stereo arrangement can be used to provide complete three-dimensional information about the environment [1, 2, 6, 18, 21, 19, 24, 30].
Reference: [30] <author> A.A. Rizzi and D. E. Koditschek. </author> <title> Further progress in robot juggling: The spatial two-juggle. </title> <booktitle> In Proc. IEEE Int. Conf. Robot. and Automat., </booktitle> <pages> pages 919-924. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction The problem of "visual servoing" | guiding a robot using visual feedback | has been an area of active study for several decades [7]. Over the last several years, a great deal of progress has been made on both theoretical and applied aspects of this problem <ref> [1, 8, 10, 14, 29, 30, 36] </ref>. However, in spite of these advances, vision-based robotic systems are still the exception rather than the rule. <p> Prior depth estimates [10], adaptive estimation [29] or metric information on the target object (from which depth can be inferred) are common solutions to this problem. Two cameras in a stereo arrangement can be used to provide complete three-dimensional information about the environment <ref> [1, 2, 6, 18, 21, 19, 24, 30] </ref>. Stereo-based motion control systems have been implemented using both free-standing and arm-mounted cameras, although the former arrangement is more common.
Reference: [31] <author> C. Samson, M. Le Borgne, and B. Espiau. </author> <title> Robot Control: The Task Function Approach. </title> <publisher> Clarendon Press, Oxford, </publisher> <address> England, </address> <year> 1992. </year>
Reference-contexts: The first part describes a general framework for vision-based control of position and points out several important properties of the approach. The development follows that of [8], the formal underpinning of which are discussed in greater detail in <ref> [31] </ref>. The second part reviews results related to the projection and reconstruction of points and lines from stereo images. <p> Note that when J is square, J + = J 1 : 2 It is interesting to note that in that case of a direct visual servo system, it has been shown that bounded-input bounded-output stability can be guaranteed if M is positive definite <ref> [8, 31] </ref>.
Reference: [32] <author> S. B. Skaar, W. H. Brockman, and W. S. Jang. </author> <title> Three-Dimensional Camera Space Manipulation. </title> <journal> Int. J. Robot. Res., </journal> <volume> 9(4) </volume> <pages> 22-39, </pages> <year> 1990. </year>
Reference-contexts: Reconstructed pose forms the basis of a position-based servo algorithm for aligning and positioning the gripper relative to the object. The affine approximation leads to a linear estimation and control problem, however it also means that the system calibration is only locally valid. A similar image-based system appears in <ref> [6, 32] </ref> with the difference that an attempt is made to modify the approximate linear model online. This article describes an image-based ECL system that uses a globally valid perspective model.
Reference: [33] <author> K. Tarabanis and P. Allen. </author> <title> Sensor planning in computer vision. </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> 11(1) </volume> <pages> 96-105, </pages> <year> 1995. </year>
Reference-contexts: Work is proceeding on occlusion detection and compensation. In particular, the design of motion strategies that plan an occlusion-free path o*ine or online are of interest. O*ine vision planning using visibility models and a prior world model information has already been investigated <ref> [33, 11] </ref>. Online motion compensation based on occlusion detection does not appear to have been considered to date. Work is also proceeding on extending the framework to more complex task representations.
Reference: [34] <author> L.E. Weiss, A.C. Sanderson, </author> <title> and C.P. Neuman. Dynamic sensor-based control of robots with visual feedback. </title> <journal> IEEE J. Robot. Automat., </journal> <volume> RA-3(5):404-417, </volume> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: The first criteria is whether visual feedback is directly converted to joint torques (referred to as direct visual servo), or whether internal encoder feedback is used to implement a velocity servo in a hierarchical control arrangement (referred to as look-and-move) <ref> [34] </ref>. A look-and-move arrangement allows the visual control system to treat the robot as a set of decoupled integrators of joint or Cartesian velocity inputs, thereby simplifying the control design problem. In practice, nearly all implemented systems are of the look-and-move variety as is the system described in this article.
Reference: [35] <author> S.W. Wijesoma, D.F.H Wolfe, and R.J. Richards. </author> <title> Eye-to-hand coordination for vision-guided robot control applications. </title> <journal> Int. J. Robot. Res., </journal> <volume> 12(1) </volume> <pages> 65-78, </pages> <year> 1993. </year>
Reference-contexts: A system that observes both the manipulator and the target will be referred to as an "endpoint-closed-loop" (ECL) system. Few ECL systems have been reported in the literature. Wijesoma et al: <ref> [35] </ref> describe an ECL monocular hand-eye system for planar positioning using image feedback. An ECL solution to the problem of three DOF positioning using stereo is described in [14]. A six DOF ECL servoing system employing stereo vision is described Hollinghurst and Cipolla [19].
Reference: [36] <author> W.J. Wilson. </author> <title> Visual servo control of robots using kalman filter estimates of robot pose relative to work-pieces. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 71-104. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year> <month> 26 </month>
Reference-contexts: 1 Introduction The problem of "visual servoing" | guiding a robot using visual feedback | has been an area of active study for several decades [7]. Over the last several years, a great deal of progress has been made on both theoretical and applied aspects of this problem <ref> [1, 8, 10, 14, 29, 30, 36] </ref>. However, in spite of these advances, vision-based robotic systems are still the exception rather than the rule. <p> The second criteria is the number of cameras and their kinematic relationship to the end-effector. A majority of the recently constructed visual servoing systems employ a single camera, typically mounted on the arm itself e:g: <ref> [4, 8, 10, 17, 28, 29, 36] </ref>. A single camera minimizes the visual processing needed to perform visual servoing, however the loss of depth information complicates the control design as well as limiting the types of positioning operations than can be implemented. <p> The former define servoing error in a Cartesian reference frame (using vision-based pose estimation) while the latter compute feedback directly from errors measured in the camera image. Most stereo systems are position-based, while monocular systems tend to be image-based (for an exception, see <ref> [36] </ref>). Arguments have been made for both types of systems. In particular, position-based systems are often characterized as more "natural" to program since they inherently operate in the robot task space, whereas image-based systems operate in a less intuitive projection of the task space.
References-found: 36

