URL: ftp://dis.cs.umass.edu/pub/coop-learn-aaai97.ps
Refering-URL: http://dis.cs.umass.edu/~nagendra/home.html
Root-URL: 
Email: fnagendra,lesserg@cs.umass.edu  
Title: Learning Problem Solving Control in Cooperative Multi-Agent Systems agent set. Much of the explicitly cooperative
Author: M V Nagendra Prasad and Victor R Lesser communication(Seeley ). 
Note: the  
Address: Amherst, MA 01003.  
Affiliation: Department of Computer Science University of Massachusetts,  
Abstract: The work presented here deals with ways to improve problem solving control skills of cooperative agents. We propose situation-specific learning as a way to learn cooperative control knowledge in complex multi-agent systems. We demonstrate the power of situation-specific learning in the context of dynamic choice of a coordination strategy based on the problem solving situation. We present a learning system, called COLLAGE, that uses meta-level information in the form of abstract characterization of the coordination problem instance to learn to choose the appropriate coordination strategy from among a class of strategies. The work presented here deals with ways to improve problem solving control skills of cooperative agents. Problem solving control is the process by which an agent organizes its computation. An agent in a MAS faces control uncertainties due to its partial local view of the problem solving states of the other agents in the system. Uncertainties about progress of problem solving in other agents, characteristics of non-local interacting sub-problems, expectations of non-local problem-solving activity, and generated partial results can lead to global incoherence and degradation in system performance if they are not managed effectively through cooperation. Effective cooperation in such systems requires that the global problem-solving state influence the local control decisions made by an agent. We call such an influence cooperative control(Lesser 1991). Cooperation among agents can be explicit or implicit. In explicitly cooperative systems, the agents interact and exchange information or perform actions so as to benefit other agents. On the other hand, implicitly cooperative agents perform actions that are a part of their own goal-seeking process but these actions affect the other agents in beneficial ways(Mataric 1993). Agents exchange information through communication. An agent in a MAS needs to communicate with other agents to acquire a view of the non-local problem solving so as to make local decisions that are influenced by more global considerations. The agents perform direct communication or indirect communication as a part of their process of interaction with other agents. Direct communication involves intentionality entailing a sender and one or more receivers. The communication is purposeful and is aimed at particular members of fl This material is based upon work supported by the National Science Foundation under Grant Nos. IRI-9523419. The content of this paper does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. In this paper, we deal with an explicitly cooperative directly communicating system. In such systems, the local problem solving control of an agent can interact in complex and intricate ways with the problem solving control of other agents. In these systems, an agent cannot make effective control decisions based on purely local problem solving state. A given local problem solving state can map to a multitude of global problem solving states. An agent with a purely local view of the problem solving situation cannot learn effective control decisions that may have global implications, due the uncertainty about the overall state of the system. This gives rise to the need for learning more globally situated control knowledge. On the other hand, an agent cannot utilize the entire global problem solving state even if other agents are willing to communicate all the necessary information because of the limitations on its computational resources, representational heterogeneity and phenomenon like distraction. If an agent A provides another agent B with enormous amount of problem solving state information, then agent B has to divert its computational resources away from local problem solving towards the task of assimilating this information. This can have drastic consequences in time constrained or resource bounded domains. Moreover, the phenomenon of distraction can become a serious problem. The incorrect or irrelevant information provided by another agent with weakly constrained knowledge could lead the receiving agent's computations along unproductive directions(Lesser 1991). This brings us to the importance of communicating only relevant information at suitable abstractions to other agents. We call such information a situation. An agent needs to associate appropriate views of the global situation with the knowledge learned about effective control decisions. We call this form of knowledge situation-specific control. In complex multi-agent systems, the knowledge engineering involved in providing the agents with effective control strategies is a formidable and perhaps even impossible task due to the dynamic nature of the interactions among agents and between the environment and agents. Thus, we appeal to machine learning techniques to learn situation specific problem solving control in such systems. Learning can involve acquiring strategies for nego 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W.; Kibler, D.; and Albert, M. K. </author> <year> 1991. </year> <title> Instance-based Learning Algorithms. </title> <booktitle> Machine Learning 6 </booktitle> <pages> 37-66. </pages>
Reference: <author> Crites, R. H., and Barto, A. G. </author> <year> 1996. </year> <title> Improving elevator performance using reinforcement learning. </title> <editor> In Touretzky, D.; Mozer, M.; and Hasselmo, M., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Decker, K. S., and Lesser, V. R. </author> <year> 1993. </year> <title> Quantitative modeling of complex computational task environments. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 217-224. </pages>
Reference: <author> Decker, K. S., and Lesser, V. R. </author> <year> 1995. </year> <title> Designing a family of coordination algorithms. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems, </booktitle> <pages> 73-80. </pages> <address> San Francisco, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Gilboa, I., and Schmeidler, D. </author> <year> 1995. </year> <title> Case-based Decision Theory. </title> <journal> The Quaterly Journal of Economics 605-639. </journal>
Reference: <author> Lesser, V. R. </author> <year> 1991. </year> <title> A retrospective view of FA/C distributed problem solving. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 21(6) </journal> <pages> 1347-1362. </pages>
Reference: <author> Mataric, M. J. </author> <year> 1993. </year> <title> Kin recognition, similarity, and group behavior. </title> <booktitle> In Proceedings of the Fifteenth Annual Cognitive Science Society Conference, </booktitle> <pages> 705-710. </pages>
Reference: <author> Nagendra Prasad, M. V., and Lesser, V. R. </author> <year> 1996. </year> <title> Off-line Learning of Coordination in Functionally Structured Agents for Distributed Data Processing. </title> <booktitle> In ICMAS96 Workshop on Learning, Interaction and Organizations in Multiagent Environments. </booktitle>
Reference: <author> Nagendra Prasad, M. V., and Lesser, V. R. </author> <year> 1997. </year> <title> Learning situation-specific coordination in cooperative multi-agent systems. </title> <institution> Computer Science Technical Report 97-12, University of Massachusetts. </institution>
Reference-contexts: This paper presents some of the empirical evidence that we developed to validate these ideas. Further experimental evidence for the effectiveness of learning situation-specific coordination can be found in <ref> (Nagendra Prasad & Lesser 1997) </ref>. (Nagendra Prasad, Lesser, & Lander 1996b) and (Nagendra Prasad, Lesser, & Lander 1996a) deal with situation-specific learning of other aspects of cooperative control.
Reference: <author> Nagendra Prasad, M. V.; Decker, K. S.; Garvey, A.; and Lesser, V. R. </author> <year> 1996. </year> <title> Exploring Organizational Designs with TAEMS: A Case Study of Distributed Data Processing. </title> <booktitle> In Proceedings of the Second International Conference on Multi-Agent Systems. </booktitle> <address> Kyoto, Japan: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Nagendra Prasad, M. V.; Lesser, V. R.; and Lander, S. E. </author> <year> 1996a. </year> <title> Cooperative learning over composite search spaces: Experiences with a multi-agent design system. </title> <booktitle> In Thirteenth National Conference on Artificial Intelligence (AAAI-96). </booktitle> <address> Portland, Oregon: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This paper presents some of the empirical evidence that we developed to validate these ideas. Further experimental evidence for the effectiveness of learning situation-specific coordination can be found in (Nagendra Prasad & Lesser 1997). (Nagendra Prasad, Lesser, & Lander 1996b) and <ref> (Nagendra Prasad, Lesser, & Lander 1996a) </ref> deal with situation-specific learning of other aspects of cooperative control.
Reference: <author> Nagendra Prasad, M. V.; Lesser, V. R.; and Lander, S. E. </author> <year> 1996b. </year> <title> Learning organizational roles in a heterogeneous multi-agent system. </title> <booktitle> In Proceedings of the Second International Conference on Multi-Agent Systems. </booktitle> <address> Kyoto, Japan: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This paper presents some of the empirical evidence that we developed to validate these ideas. Further experimental evidence for the effectiveness of learning situation-specific coordination can be found in (Nagendra Prasad & Lesser 1997). <ref> (Nagendra Prasad, Lesser, & Lander 1996b) </ref> and (Nagendra Prasad, Lesser, & Lander 1996a) deal with situation-specific learning of other aspects of cooperative control.
Reference: <author> Sandholm, T., and Crites, R. </author> <year> 1995. </year> <title> Multi-agent reinforcement learning in the repeated prisoner's dilemma. </title> <note> to appear in Biosystems. </note>
Reference: <author> Seeley, T. D. </author> <year> 1989. </year> <title> The Honey Bee Colony as a Superor-ganism. </title> <journal> American Scientist 77 </journal> <pages> 546-553. </pages>
Reference: <author> Sen, S.; Sekaran, M.; and Hale, J. </author> <year> 1994. </year> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 426-431. </pages> <address> Seattle, WA: </address> <publisher> AAAI. </publisher>
Reference: <author> Tan, M. </author> <year> 1993. </year> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 330-337. </pages>
Reference: <author> Weiss, G. </author> <year> 1994. </year> <title> Some studies in distributed machine learning and organizational design. </title> <type> Technical Report FKI-189-94, </type> <institution> Institut fur Informatik, TU Munchen. </institution>
References-found: 17

