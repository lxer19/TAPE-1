URL: http://www.math.tau.ac.il/~megiddo/psfiles/lpcitrev.ps.gz
Refering-URL: http://www.math.tau.ac.il/~megiddo/pub.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Parallel Linear Programming in Fixed Dimension Almost Surely in Constant Time  
Author: Noga Alon and Nimrod Megiddo 
Date: Revised; January 1992  
Address: San Jose, California 95120 and  Israel  
Affiliation: IBM Almaden Research Center  School of Mathematical Sciences Tel Aviv University,  
Abstract: For any fixed dimension d, the linear programming problem with n inequality constraints can be solved on a probabilistic CRCW PRAM with O(n) processors almost surely in constant time. The algorithm always finds the correct solution. With nd= log 2 d processors, the probability that the algorithm will not finish within O(d 2 log 2 d) time tends to zero exponentially with n.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon and J. H. Spencer, </author> <title> The Probabilistic Method, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Note that 2 k = (n 1=(d+1) ). We first describe the organization of the memory shared by our processors. 3.1 The shared memory The shared memory consists of four types of cells as follows. (i) The Base B, consisting of k cells, B <ref> [ 1 ] </ref>; . . . ; B [k]. 2 The notation f (n) = (g (n)) means that there exists a constant c &gt; 0 such that f (n) cg (n). 9 (ii) The Sequence S, consisting of 2n cells, S [ 1 ]; . . . ; S [2n]. <p> The Base B, consisting of k cells, B <ref> [ 1 ] </ref>; . . . ; B [k]. 2 The notation f (n) = (g (n)) means that there exists a constant c &gt; 0 such that f (n) cg (n). 9 (ii) The Sequence S, consisting of 2n cells, S [ 1 ]; . . . ; S [2n]. <p> length n 1=4 , so these cells are also addressed as S [I; J ], I = 1; . . . ; n 1=4 , J = 1; . . . ; 2n 3=4 . (iii) The Table T , consisting of m = C d n 11=(2d) cells T <ref> [ 1 ] </ref>; . . . ; T [m], where C d = log (16d) + 2. <p> blocks of length m 0 = n 11=(2d) , so these cells are also addressed as T [I; J ], I = 1; . . . ; m 0 , J = 1; . . . ; C d . (iv) The Area R, consisting of n 3=4 cells, R <ref> [ 1 ] </ref>; . . . ; R [n 3=4 ]. <p> Injecting into the Table: Operation 1 First, for any violated cell S [ i ], processor P i generates a random integer I 1 i between 1 and m 0 , and attempts to copy the contents of S [ i ] into T <ref> [I 1 i ; 1] </ref>. Next, if P i has attempted to write and failed, then it generates a random integer I 2 i between 1 and m 0 and attempts again to copy the contents of S [ i ] into T [I 2 i ; 2]. <p> Proof: Part (i) is due to Chernoff [ 3 ] . (see also <ref> [ 1 ] </ref> , p. 237). Part (ii) follows immediately from the fact that probfX &gt; ag &lt; a p a a a a Acknowledgement.
Reference: [2] <author> A. Borodin, J. von zur Gathen and J. E. Hopcroft, </author> <title> "Fast parallel matrix and GCD computations," </title> <note> Information and Control 52 (1982) 241-256. </note>
Reference-contexts: It follows that all the subproblems LP B can be solved in O (log 2 d) time, as each of them amounts to solving a system of linear equations of order (2d) fi (2d), and we have at least d 3 processors (see <ref> [ 2 ] </ref> ). If any of the LP B 's is discovered to be infeasible then LP is infeasible, and the algorithm stops. <p> Next, if P i has attempted to write and failed, then it generates a random integer I 2 i between 1 and m 0 and attempts again to copy the contents of S [ i ] into T <ref> [I 2 i ; 2] </ref>. In general, each such processor attempts to write at most C d 1 times, each time into a different block of the Table. Proposition 3.2.
Reference: [3] <author> H. Chernoff, </author> <title> "A measure of asymptotic efficiency for tests of hypothesis based on the sum of observations," </title> <journal> Ann. Math. Stat. </journal> <month> 23 </month> <year> (1952) </year> <month> 493-507. </month>
Reference-contexts: Thus, we can apply here estimates for independent Bernoulli variables. By an estimate due to Chernoff <ref> [ 3 ] </ref> (apply Proposition 4.1 part (i) with n = ~ i1 and p = ~ i1 =m 0 ), prob n i1 =m 0 j X i1 = ~ i1 e (~ 2 Let j denote the largest integer such that 2 2 j 1 n 1 1 16d <p> Proof: Part (i) is due to Chernoff <ref> [ 3 ] </ref> . (see also [ 1 ] , p. 237). Part (ii) follows immediately from the fact that probfX &gt; ag &lt; a p a a a a Acknowledgement.
Reference: [4] <author> K. L. Clarkson, </author> <title> "Linear programming in O(n3 d 2 ) time," </title> <note> Information Processing Letters 22 (1986) 21-24. </note>
Reference-contexts: Megiddo [ 11 ] showed that for any d, this problem can be solved in O (n) time. Clarkson <ref> [ 4 ] </ref> and Dyer [ 7 ] improved the constant of proportionality. Clarkson [ 5 ] later developed linear-time probabilistic algorithms with even better complexity.
Reference: [5] <author> K. L. Clarkson, </author> <title> "Las Vegas algorithms for linear and integer programming when the dimension is small," </title> <type> unpublished manuscript, </type> <note> 1988; a preliminary version appeared in Proceedings of the 29th Annual IEEE Symposium on Foundations of Computer Science (1988), pp. 452-456. </note>
Reference-contexts: Megiddo [ 11 ] showed that for any d, this problem can be solved in O (n) time. Clarkson [ 4 ] and Dyer [ 7 ] improved the constant of proportionality. Clarkson <ref> [ 5 ] </ref> later developed linear-time probabilistic algorithms with even better complexity. The problem in fixed dimension is interesting from the point of view of parallel computation, since 1 the general linear programming problem is known to be P-complete. <p> The algorithm of [ 11 ] can be parallelized efficiently, but the exact parallel complexity of the problem in fixed dimension is still not known. 1 Here we develop a very efficient probabilistic parallel algorithm based on Clarkson's <ref> [ 5 ] </ref> scheme. In this paper, when we say that a sequence of events fE n g 1 n=1 occurs almost surely, we mean that there exists an * &gt; 0 such that prob (E n ) 1 e n * . <p> The main result of this paper generalizes a known fact [ 12; 10 ] that the maximum of n items can be computed almost surely in constant time. As mentioned above, the basic idea of the underlying sequential algorithm is due to Clarkson <ref> [ 5 ] </ref> . His beautiful iterative sequential algorithm uses an idea of Welzl [ 14 ] . As in Clarkson's algorithm, we also sample constraints repeatedly with variable probabilities. Several additional ideas and some modifications were, however, required in order to achieve the result of this paper. <p> If v 6= 0, then i 2 V (u; v) if and only if either a i v &lt; 0 or a i v = 0 and a i u &lt; b i . The following proposition is essentially due to Clarkson <ref> [ 5 ] </ref> : Proposition 2.2. <p> The algorithm As mentioned above, the underlying scheme of our algorithm is the same as that of the iterative algorithm in the paper by Clarkson <ref> [ 5 ] </ref> , but the adaptation to a parallel machine requires many details to be modified. <p> We omit the details. Remarks The total work done by all the processors in our algorithm is O (d 3 n), whereas Clarkson's sequential algorithm <ref> [ 5 ] </ref> runs in expected O (d 2 n) time. We can easily modify our algorithm to run on a probabilistic CRCW PRAM with n=(d log 2 d) processors in O (d 3 log 2 d) time, so that the total work is O (d 2 n).
Reference: [6] <author> X. Deng, </author> <title> "An optimal parallel algorithm for linear programming in the plane," </title> <type> unpublished manuscript, </type> <institution> Dept. of Operations Research, Stanford University and Dept. of Computer Science and Engineering, University of California at San Diego. </institution>
Reference: [7] <author> M. E. Dyer, </author> <title> "On a multidimensional search technique and its application to the Euclidean one-center problem," </title> <journal> SIAM J. Comput. </journal> <month> 15 </month> <year> (1986) </year> <month> 725-738. </month>
Reference-contexts: Megiddo [ 11 ] showed that for any d, this problem can be solved in O (n) time. Clarkson [ 4 ] and Dyer <ref> [ 7 ] </ref> improved the constant of proportionality. Clarkson [ 5 ] later developed linear-time probabilistic algorithms with even better complexity. The problem in fixed dimension is interesting from the point of view of parallel computation, since 1 the general linear programming problem is known to be P-complete.
Reference: [8] <author> M. E. Dyer and A. M. </author> <title> Freeze, "A randomized algorithm for fixed dimensional linear programming," </title> <note> Mathematical Programming 44 (1989) 203-212. </note>
Reference: [9] <author> G. S. Lueker, N. Megiddo and V. Ramachandran, </author> <title> "Linear programming with two variables per inequality in poly log time," </title> <note> SIAM J. Comput. , to appear. </note>
Reference: [10] <author> N. Megiddo, </author> <title> "Parallel algorithms for finding the maximum and the median almost surely in constant-time," </title> <type> Technical Report, </type> <institution> Graduate School of Industrial Administration, Carnegie-Mellon University, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: A consequence of this estimate is that with probability 1, only a finite number of the events do not occur. The main result of this paper generalizes a known fact <ref> [ 12; 10 ] </ref> that the maximum of n items can be computed almost surely in constant time. As mentioned above, the basic idea of the underlying sequential algorithm is due to Clarkson [ 5 ] .
Reference: [11] <author> N. Megiddo, </author> <title> "Linear programming in linear time when the dimension is fixed," </title> <journal> J. </journal> <note> ACM 31 (1984) 114-127. </note>
Reference-contexts: 1. Introduction The linear programming problem in fixed dimension is to maximize a linear function of a fixed number, d, of variables, subject to n linear inequality constraints, where n is not fixed. Megiddo <ref> [ 11 ] </ref> showed that for any d, this problem can be solved in O (n) time. Clarkson [ 4 ] and Dyer [ 7 ] improved the constant of proportionality. Clarkson [ 5 ] later developed linear-time probabilistic algorithms with even better complexity. <p> Clarkson [ 5 ] later developed linear-time probabilistic algorithms with even better complexity. The problem in fixed dimension is interesting from the point of view of parallel computation, since 1 the general linear programming problem is known to be P-complete. The algorithm of <ref> [ 11 ] </ref> can be parallelized efficiently, but the exact parallel complexity of the problem in fixed dimension is still not known. 1 Here we develop a very efficient probabilistic parallel algorithm based on Clarkson's [ 5 ] scheme.
Reference: [12] <author> R. Reischuk, </author> <title> "A fast probabilistic parallel sorting algorithm," </title> <booktitle> in: Proceedings of the 22th Annual IEEE Symposium on Foundations of Computer Science (1981), </booktitle> <pages> pp. 212-219. </pages>
Reference-contexts: A consequence of this estimate is that with probability 1, only a finite number of the events do not occur. The main result of this paper generalizes a known fact <ref> [ 12; 10 ] </ref> that the maximum of n items can be computed almost surely in constant time. As mentioned above, the basic idea of the underlying sequential algorithm is due to Clarkson [ 5 ] .
Reference: [13] <author> L. G. Valiant, </author> <title> "Parallelism in comparison algorithms," </title> <journal> SIAM J. Comput. </journal> <month> 4 </month> <year> (1975) </year> <month> 348-355. </month>
Reference-contexts: The final step is essentially a computation of the minimum of d = O (n d=(d+1) ) numbers. An algorithm of Valiant 4 <ref> [ 13 ] </ref> (which can be easily implemented on a CRCW PRAM) finds the minimum of m elements, using p processors, in O (log (log m= log (p=m))) time.
Reference: [14] <author> E. Welzl, </author> <title> "Partition trees for triangle counting and other range searching problems," </title> <booktitle> in: Proceedings of the 4th Annual ACM Symposium on Computational Geometry (1988), </booktitle> <pages> pp. 23-33. </pages>
Reference-contexts: As mentioned above, the basic idea of the underlying sequential algorithm is due to Clarkson [ 5 ] . His beautiful iterative sequential algorithm uses an idea of Welzl <ref> [ 14 ] </ref> . As in Clarkson's algorithm, we also sample constraints repeatedly with variable probabilities. Several additional ideas and some modifications were, however, required in order to achieve the result of this paper.
References-found: 14

