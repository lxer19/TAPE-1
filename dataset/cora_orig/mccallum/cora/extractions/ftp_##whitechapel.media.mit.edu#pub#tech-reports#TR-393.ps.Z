URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-393.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Title: Bayesian Similarity Measure for Direct Image Matching  
Author: Baback Moghaddam, Chahab Nastar and Alex Pentland 
Address: 20 Ames Street, Cambridge MA 02139, U.S.A.  
Affiliation: The Media Laboratory, Massachusetts Institute of Technology  
Note: A  
Pubnum: Perceptual Computing Section,  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 393 Appears in: International Conference on Pattern Recognition, Vienna, Austria, August 1996. Abstract We propose a probabilistic similarity measure for direct image matching based on a Bayesian analysis of image deformations. We model two classes of variation in object appearance: intra-object and extra-object. The probability density functions for each class are then estimated from training data and used to compute a similarity measure based on the a posteriori probabilities. Furthermore, we use a novel representation for characterizing image differences using a deformable technique for obtaining pixel-wise correspondences. This representation, which is based on a deformable 3D mesh in XYI-space, is then experimentally compared with two simpler representations: intensity differences and optical flow. The performance advantage of our deformable matching technique is demonstrated using a typically hard test set drawn from the US Army's FERET face database. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. J. Bathe. </author> <title> Finite Element Procedures in Engineering Analysis. </title> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference-contexts: The intensity surface is modeled as a deformable mesh and is governed by Lagrangian dynamics <ref> [1] </ref> : M U + C _ U + KU = F (t) (2) where U = [: : : ; x i ; y i ; z i ; : : :] T is a vector storing nodal displacements, M, C and K are respectively the mass, damping and stiffness <p> Solving the governing equations in the modal basis leads to scalar equations where the unknown ~u (i) is the amplitude of mode i <ref> [1] </ref> The closed-form expression of the displacement field is then given by U i=1 towards S 0 by image forces with P t 3N , which means that only P scalar equations of the type of (5) need to be solved.
Reference: [2] <author> David Beymer. </author> <title> Vectorizing face images by interleaving shape and texture computations. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1537, </pages> <institution> 7 Artificial Intelligence Laboratory, Massachusetts In--stitute of Technology, </institution> <year> 1995. </year>
Reference-contexts: Furthermore, we use a novel representation for d (I 1 ; I 2 ) which combines both the spatial (XY) and grayscale (I) components of the image in a unified XYI framework (unlike previous approaches which essentially treat the shape and texture components independently, e.g., <ref> [3, 4, 7, 2] </ref>). Specifically, I 1 is modeled as a physically-based deformable 3D surface (or manifold) in XYI-space which deforms in accordance with attractive "physical forces" exerted by I 2 .
Reference: [3] <author> I. Craw and P. Cameron. </author> <title> Face recognition by computer. </title> <editor> In D. Hogg and R. Boyle, editors, </editor> <booktitle> Proc. British Machine Vision Conference, </booktitle> <pages> pages 498-507. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Furthermore, we use a novel representation for d (I 1 ; I 2 ) which combines both the spatial (XY) and grayscale (I) components of the image in a unified XYI framework (unlike previous approaches which essentially treat the shape and texture components independently, e.g., <ref> [3, 4, 7, 2] </ref>). Specifically, I 1 is modeled as a physically-based deformable 3D surface (or manifold) in XYI-space which deforms in accordance with attractive "physical forces" exerted by I 2 .
Reference: [4] <author> I. Craw and et al. </author> <title> Automatic face recognition: Combining configuration and texture. </title> <editor> In Martin Bichsel, editor, </editor> <booktitle> Proc. Int'l Workshop on Automatic Face and Gesture Recognition, </booktitle> <address> Zurich, </address> <year> 1995. </year>
Reference-contexts: Furthermore, we use a novel representation for d (I 1 ; I 2 ) which combines both the spatial (XY) and grayscale (I) components of the image in a unified XYI framework (unlike previous approaches which essentially treat the shape and texture components independently, e.g., <ref> [3, 4, 7, 2] </ref>). Specifically, I 1 is modeled as a physically-based deformable 3D surface (or manifold) in XYI-space which deforms in accordance with attractive "physical forces" exerted by I 2 .
Reference: [5] <author> B.K.P. Horn and G. Schunck. </author> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 185-203, </pages> <year> 1981. </year>
Reference-contexts: This manifold matching technique can be viewed as a more general formulation for image correspondence which, unlike optical flow, does not require a constant brightness assumption <ref> [5] </ref>. 1 Finally, we experimentally compare our deformable matching technique with two alternative (non-deformable) methods: one using intensity differences with d (I 1 ; I 2 ) = I 2 I 1 , and a standard correspondence method using optical flow with d (I 1 ; I 2 ) = f <p> The elasticity of the surface provides an intrinsic smoothness constraint for computing the final displacement field. We note that this formulation provides an interesting alternative to optical flow methods for obtaining correspondence, without the classical brightness constraint <ref> [5] </ref>. Indeed, the brightness constraint corresponds to a particular case of our formulation where the closest point P i has to have the same intensity as M i | i.e., ! parallel to the XY plane. We do not make that assumption here.
Reference: [6] <author> I.T. Jolliffe. </author> <title> Principal Component Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: Recently, an efficient density estimation method was proposed by Moghaddam & Pentland [9] which divides the vector space R N into two complementary subspaces using an eigenspace decomposition. This method relies on a Principal Components Analysis (PCA) <ref> [6] </ref> to form a low-dimensional estimate of the complete likelihood which can be evaluated using only the first M principal components, where M &lt;< N .
Reference: [7] <author> A. Lanitis, C. J. Taylor, and T. F. Cootes. </author> <title> A unified approach to coding and interpreting face images. </title> <booktitle> In IEEE Proceedings of the Fifth International Conference on Computer Vision (ICCV'95), </booktitle> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Furthermore, we use a novel representation for d (I 1 ; I 2 ) which combines both the spatial (XY) and grayscale (I) components of the image in a unified XYI framework (unlike previous approaches which essentially treat the shape and texture components independently, e.g., <ref> [3, 4, 7, 2] </ref>). Specifically, I 1 is modeled as a physically-based deformable 3D surface (or manifold) in XYI-space which deforms in accordance with attractive "physical forces" exerted by I 2 .
Reference: [8] <author> B. Moghaddam and A. Pentland. </author> <title> Face recognition using view-based and modular eigenspaces. Automatic Systems for the Identification and Inspection of Humans, </title> <type> 2277, </type> <year> 1994. </year>
Reference-contexts: Note that this performance is better than or similar to recognition rates obtained by any algorithm tested on this database, and that it is lower (by about 10%) than the typical rates that we have obtained with the FERET database <ref> [8] </ref>. We attribute this lower performance to the fact that these images were selected to be particularly challenging.
Reference: [9] <author> B. Moghaddam and A. Pentland. </author> <title> Probabilistic visual learning for object detection. </title> <booktitle> In IEEE Proceedings of the Fifth International Conference on Computer Vision (ICCV'95), </booktitle> <address> Cambridge, USA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: a posteriori probability given by Bayes rule, using estimates of the likelihoods P (d (I 1 ; I 2 )j I ) and P (d (I 1 ; I 2 )j E ) which are derived from training data using an efficient subspace method for density estimation of high-dimensional data <ref> [9] </ref>. Furthermore, we use a novel representation for d (I 1 ; I 2 ) which combines both the spatial (XY) and grayscale (I) components of the image in a unified XYI framework (unlike previous approaches which essentially treat the shape and texture components independently, e.g., [3, 4, 7, 2]). <p> Furthermore, this computation would be highly inefficient since the intrinsic dimensionality or major degrees-of-freedom of ~ U for each class is likely to be significantly smaller than N . Recently, an efficient density estimation method was proposed by Moghaddam & Pentland <ref> [9] </ref> which divides the vector space R N into two complementary subspaces using an eigenspace decomposition. <p> The component of ~ U which lies in the feature space F is referred to as the "distance-in-feature-space" (DIFS) and is a Mahalanobis distance for Gaussian densities. As shown in <ref> [9] </ref>, the complete likelihood estimate can be written as the product of two independent marginal 3 (a) F and its orthogonal complement F for a Gaussian density, (b) a typical eigenvalue spectrum and its division into the two orthogonal subspaces. <p> For this purpose we have used an automatic face-processing system which extracts faces from the input image and normalizes for translation, scale as well as slight rotations (both in-plane and out-of-plane). This system is described in detail in Moghaddam & Pentland <ref> [9] </ref> and uses maximum-likelihood estimation of object location (in this case the position and scale of a face and the location of individual facial features) to geometrically align faces into standard normalized form as shown in Figure 6.
Reference: [10] <author> C. Nastar. </author> <title> Vibration modes for nonrigid motion analysis in 3D images. </title> <booktitle> In Proceedings of the Third Eu-ropean Conference on Computer Vision (ECCV '94), </booktitle> <address> Stockholm, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Specifically, I 1 is modeled as a physically-based deformable 3D surface (or manifold) in XYI-space which deforms in accordance with attractive "physical forces" exerted by I 2 . The dynamics of this system are efficiently solved for using the analytic modes of vibration <ref> [10] </ref>, yielding a 3D correspondence field for warping I 1 into I 2 . In addition, we use the parametric representation, d (I 1 ; I 2 ) = ~ U, where ~ U is the modal amplitude spectrum of the resultant deformation [12]. <p> We do not make that assumption here. Solutions of the governing equation are typically obtained using an eigenvector-based modal decomposition <ref> [13, 11, 10] </ref>. In particular, the vibration modes (i) of the previous deformable surface are the vector solutions of the eigenproblem : K = ! 2 M (4) where !(i) is the i-th eigenfrequency of the system. <p> The modal superposition equation (6) can be seen as a Fourier expansion with high-frequencies neglected <ref> [10] </ref>. <p> The modal superposition equation (6) can be seen as a Fourier expansion with high-frequencies neglected [10]. In our formulation, however, we make use of the analytic modes <ref> [10, 12] </ref>, which are known sine and cosine functions for specific surface topologies (p; p 0 ) = [: : : ; cos 2n p 0 (2j 1) These analytic expressions avoid costly eigenvector decompositions and furthermore allow the total number of modes to be easily adjusted for the application.
Reference: [11] <author> C. Nastar and N. Ayache. </author> <title> Fast segmentation, tracking, and analysis of deformable objects. </title> <booktitle> In IEEE Proceedings of the Third International Conference on Computer Vision (ICCV'93), </booktitle> <address> Berlin, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: section we briefly review the mathematics of this approach (for further details the reader 1 In fact, by simply disabling the I component of our deformations we can obtain a standard 2D deformable mesh which yields correspondences similar to an optical flow technique with thin-plate regularizers. 1 is referred to <ref> [11, 12] </ref>). <p> We do not make that assumption here. Solutions of the governing equation are typically obtained using an eigenvector-based modal decomposition <ref> [13, 11, 10] </ref>. In particular, the vibration modes (i) of the previous deformable surface are the vector solutions of the eigenproblem : K = ! 2 M (4) where !(i) is the i-th eigenfrequency of the system.
Reference: [12] <author> C. Nastar and A. Pentland. </author> <title> Matching and recognition using deformable intensity surfaces. </title> <booktitle> In IEEE International Symposium on Computer Vision, Coral Gables, </booktitle> <address> USA, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: In addition, we use the parametric representation, d (I 1 ; I 2 ) = ~ U, where ~ U is the modal amplitude spectrum of the resultant deformation <ref> [12] </ref>. <p> of our method over optical flow is key, since this simpler method relies all too heavily on the constant brightness assumption and is prone to failure when there are large grayscale variations between the images of different individuals (e.g., presence/absence of facial hair). 2 XYI Image Warping In previous work <ref> [12] </ref>, we formulated an image matching technique based on a 3D surface representation of an image I (x; y) | i.e., as the surface (x; y; I (x; y)) as shown, for example, in Figure 1 | and developed an efficient method to warp one image onto another using a physically-based <p> section we briefly review the mathematics of this approach (for further details the reader 1 In fact, by simply disabling the I component of our deformations we can obtain a standard 2D deformable mesh which yields correspondences similar to an optical flow technique with thin-plate regularizers. 1 is referred to <ref> [11, 12] </ref>). <p> The modal superposition equation (6) can be seen as a Fourier expansion with high-frequencies neglected [10]. In our formulation, however, we make use of the analytic modes <ref> [10, 12] </ref>, which are known sine and cosine functions for specific surface topologies (p; p 0 ) = [: : : ; cos 2n p 0 (2j 1) These analytic expressions avoid costly eigenvector decompositions and furthermore allow the total number of modes to be easily adjusted for the application.
Reference: [13] <author> A. Pentland and S. Sclaroff. </author> <title> Closed-form solutions for physically based shape modelling and recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-13(7):715-729, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: We do not make that assumption here. Solutions of the governing equation are typically obtained using an eigenvector-based modal decomposition <ref> [13, 11, 10] </ref>. In particular, the vibration modes (i) of the previous deformable surface are the vector solutions of the eigenproblem : K = ! 2 M (4) where !(i) is the i-th eigenfrequency of the system.
Reference: [14] <author> J.Y.A. Wang and E.H. Adelson. </author> <title> Representing moving images with layers. </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 3(5) </volume> <pages> 625-638, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: The particular optical flow algorithm used in our experiment was that of Wang & Adelson <ref> [14] </ref>. For each method, the eigenspace analysis was used to derive corresponding density estimates for the intra/extra classes and recognition proceeded exactly as described in the previous section.
Reference: [15] <author> J.J. Weng. </author> <title> On comprehensive visual learning. </title> <booktitle> In Proc. NSF/ARPA Workshop on Performance vs. Methodology in Computer Vision, </booktitle> <address> Seattle, WA, </address> <month> June </month> <year> 1994. </year> <month> 8 </month>
Reference-contexts: Furthermore, by equating similarity with the a posteriori probability P ( I jd (I 1 ; I 2 ), we obtain an optimal non-linear decision rule for matching and recognition. This aspect of our approach differs from methods which use linear discriminant analysis techniques for visual object recognition (e.g., <ref> [15] </ref>). Furthermore, we have experimentally shown that our deformable XYI warping method for obtaining pixel correspondences does indeed lead to an effective representation for d (I 1 ; I 2 ), especially when compared with simpler methods such as intensity differences and optical flow.
References-found: 15

