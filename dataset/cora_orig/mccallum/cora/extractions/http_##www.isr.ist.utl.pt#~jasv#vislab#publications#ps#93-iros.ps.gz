URL: http://www.isr.ist.utl.pt/~jasv/vislab/publications/ps/93-iros.ps.gz
Refering-URL: http://www.isr.ist.utl.pt/~jasv/vislab/publications/publications.html
Root-URL: 
Email: e-mail:giulio@vision.dist.unige.it  e-mail: jasv@kappa.ist.utl.pt  
Phone: (2)  
Title: Robotic Bees  
Author: G. Sandini () J. Santos-Victor () F. Curotto () S. Garibaldi () 
Note: A discussion about the potentialities of the approach and the implications in terms of sensor's structure is also presented.  
Address: Via Opera Pia 11A I16145 Genoa, Italy  Complexo 1, Av. Rovisco Pais, 1096 Lisboa codex, Portugal  
Affiliation: (1) DIST University of Genova Laboratory for Integrated Advanced Robotics (LIRA Lab)  CAPS/DEEC/ Instituto Superior Tecnico Institute of Systems and Robotics  
Abstract: This paper presents some experiments of a real-time navigation system driven by two cameras pointed laterally to the navigation direction (divergent stereo). The approach is based on the observation that the stereo set-up traditionally used in vision (i.e. with the optical axis pointing forward) may not be the best one for navigation, and particularly for continuous control of a mobile actor moving in unconstrained environment. Similarly to what has been proposed [5, 3], our approach [8, 10] is that for navigation purposes the driving information is not distance (as it is obtainable by a stereo set-up) but motion and, more precisely, by optical flow information computed over different areas of the visual field. Following this idea, a mobile vehicle has been equipped with a pair of cameras looking laterally (much like honeybees) and a controller based on fast, real-time computation of optical flow, has been implemented. The control of the mobile robot (ROBEE) is based upon the comparison between the apparent image speed of the left and the right eye. The solution adopted is derived from recent studies [13] describing the behavior of freely flying honeybees and the mechanisms they use to perceive range. This qualitative information (no explicit measure of depth is performed) is used in many experiments to show the robustness of the approach and a detailed description of the control structure is presented to demonstrate the feasibility of the approach in driving the mobile robot within a very cluttered environment. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Aloimonos. </author> <title> Purposive and qualitative active vision. </title> <booktitle> In Proc. of Int. Workshop on Active Control in Visual Perception, </booktitle> <address> Antibes (France), </address> <year> 1990. </year>
Reference-contexts: Following this ideas, one could say that the goal of a vision system in a "living" agent is not generality but specificity: the physical structure and the purpose drives perception <ref> [1] </ref>.
Reference: [2] <author> K. Astrom and B. Wittenmark. </author> <title> Computer Controlled Systems: Theory and design. </title> <publisher> Prentice-Hall, </publisher> <year> 1986. </year>
Reference-contexts: Since we are using digital control, we must determine how the computer (where the control algorithm is implemented) "sees" the system. Using a zero-order hold sampling procedure, we can use the step invariant method to determine the discretized system <ref> [2] </ref>.
Reference: [3] <author> D. Coombs and K. Roberts. </author> <title> Centering behaviour using peripheral vision. In D.P. </title> <editor> Casasent, editor, </editor> <booktitle> Intelligent Robots and Computer Vision XI: Algorithms, Techniques a nd Active Vision, </booktitle> <pages> pages 714-21. </pages> <booktitle> SPIE, </booktitle> <volume> Vol. 1825, </volume> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: The main conclusion there is that it is possible to design the set-up in such a way that this rotational component does not degrade the performance of the system. An alternative solution has been proposed recently by Coombs and Roberts <ref> [3] </ref> based on the use of a gaze control system counter-rotating the cameras with respect to the vehicle. 4. Conclusions In this paper, we have presented a qualitative approach to visually-guided navigation based on optical flow. <p> Before concluding it is important to refer to a recent article <ref> [3] </ref> presenting a system (beebot) which has very strong similarities with our ROBEE approach.
Reference: [4] <author> E. DeMicheli, G. Sandini, M. Tistarelli, and V. Torre. </author> <title> Estimation of visual motion and 3d motion parameters from singular points. </title> <booktitle> In Proc. of IEEE Int. Workshop on Intelligent RObots and Systems, </booktitle> <address> Tokyo, Japan, </address> <year> 1988. </year>
Reference-contexts: The optical flow V = (u; v), is usually obtained by using the fundamental optical flow constraint [7], shown in equation (1), or by imposing the stationarity of the flow field <ref> [6, 14, 4] </ref>, according to equation (2): @I u + @y @I = 0 (1) dt By using expression (2), one ends up with the following system of equations, which can be solved to obtain both components of the optical flow.
Reference: [5] <author> N. Franceschini, J. Pichon, and C. Blanes. </author> <title> Real time visuo-motor control: from flies to robots. </title> <booktitle> In Fifth Int. Conference on Advanced Robotics, </booktitle> <address> Pisa, Italy, </address> <month> June </month> <year> 1991. </year>
Reference: [6] <author> Nagel H. </author> <title> On the estimation of optical flow: Relations be-tween different approaches and some new results. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 299-323, </pages> <year> 1987. </year>
Reference-contexts: The optical flow V = (u; v), is usually obtained by using the fundamental optical flow constraint [7], shown in equation (1), or by imposing the stationarity of the flow field <ref> [6, 14, 4] </ref>, according to equation (2): @I u + @y @I = 0 (1) dt By using expression (2), one ends up with the following system of equations, which can be solved to obtain both components of the optical flow.
Reference: [7] <author> B. K. P. Horn and B. G. Schunck. </author> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 No.1-3:185-204, </volume> <year> 1981. </year>
Reference-contexts: Optical Flow Computation To compare the image velocity as seen by the left and right cameras, we compute an average of the optical flow on each side. The optical flow V = (u; v), is usually obtained by using the fundamental optical flow constraint <ref> [7] </ref>, shown in equation (1), or by imposing the stationarity of the flow field [6, 14, 4], according to equation (2): @I u + @y @I = 0 (1) dt By using expression (2), one ends up with the following system of equations, which can be solved to obtain both components
Reference: [8] <author> G. Sandini, J. Santos-Victor, F. Curotto, and S. Garibaldi. </author> <title> Robotic bees. </title> <type> Technical report, </type> <institution> LIRA-Lab University of Genova, </institution> <month> October </month> <year> 1992. </year>
Reference: [9] <author> G. Sandini and M. Tistarelli. </author> <title> Robust Obstacle Detection Using Optical Flow. </title> <booktitle> Proc. of IEEE Intl. Workshop on Robust Computer Vision, </booktitle> <address> Seattle, (WA), </address> <month> Oct. </month> <pages> 1-3, </pages> <year> 1990. </year>
Reference-contexts: On the other hand we are currently using only a very limited portion of the visual field and, the use of a more frontal part of the visual field, is currently investigated to extract other motion-derived measures (e.g. time-to-crash) which could help ROBEE not only to avoid obstacles <ref> [9] </ref>, but also to control docking speed. Before concluding it is important to refer to a recent article [3] presenting a system (beebot) which has very strong similarities with our ROBEE approach.
Reference: [10] <author> J. Santos-Victor, G. Sandini, F. Curotto, and S. Garibaldi. </author> <title> Divergent stereo for robot navigation: </title> <note> Learning from bees. In To appear on Proc. </note> <institution> CVPR-93, </institution> <address> New Yors, U.S.A., </address> <year> 1993. </year>
Reference-contexts: In order to obtain reliable flow estimates, the usual procedure is to perform some smoothing both in time and space domains. We have developed a simplified method for the computation of the optical flow <ref> [10] </ref>. On one hand, the time filter is causal, in the sense that we only need the images available up to the instant under analysis.
Reference: [11] <author> J. Santos-Victor, G. Sandini, F. Curotto, and S. Garibaldi. </author> <title> Divergent stereo for robot navigation: A step forward to a robotic bee. </title> <note> Submitted for publication, </note> <year> 1993. </year>
Reference-contexts: The robot control system allows the possibility of performing rotations, at a given speed, while still moving forwards. The rotation velocity is used to control the robot's motion direction. In an extended version of this paper <ref> [11] </ref> experiments involving also the control of the forward velocity are presented. <p> This interferes while decreasing the derivative gain. In the center, it is seen the unstable behavior due to the insertion of the integral action. the simple control introduced here. The analysis of the influence of the rotational speed in the the proposed scheme is studied by the authors in <ref> [11] </ref>, where some solutions are pointed out. The main conclusion there is that it is possible to design the set-up in such a way that this rotational component does not degrade the performance of the system. <p> A deeper descussion about the differences of the two systems will be presented in an article submitted for publication <ref> [11] </ref> Acknowledgements The research described in this paper has been supported by the Special Projects on Robotics of the Italian National Council of Research and by the ESPRIT Project VAPII
Reference: [12] <author> M.V. Srinivasan. </author> <title> Distance perception in insects. </title> <booktitle> Current Directions in Psychological Science, </booktitle> <volume> 1 </volume> <pages> 22-26, </pages> <year> 1992. </year>
Reference: [13] <author> M.V. Srinivasan, M. Lehrer, W.H. Kirchner, and S.W. Zhang. </author> <title> Range perception through apparent image speed in freely flying honeybees. </title> <journal> Visual Neuroscience, </journal> <volume> 6 </volume> <pages> 519-535, </pages> <year> 1991. </year>
Reference: [14] <author> S. Uras, F. Girosi, A. Verri, and V. Torre. </author> <title> Computational approach to motion perception. </title> <journal> Biological Cybernetics, </journal> <volume> 60 </volume> <pages> 69-87, </pages> <year> 1988. </year>
Reference-contexts: The optical flow V = (u; v), is usually obtained by using the fundamental optical flow constraint [7], shown in equation (1), or by imposing the stationarity of the flow field <ref> [6, 14, 4] </ref>, according to equation (2): @I u + @y @I = 0 (1) dt By using expression (2), one ends up with the following system of equations, which can be solved to obtain both components of the optical flow.
References-found: 14

