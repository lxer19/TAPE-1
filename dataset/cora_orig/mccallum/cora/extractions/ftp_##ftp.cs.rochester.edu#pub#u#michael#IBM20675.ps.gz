URL: ftp://ftp.cs.rochester.edu/pub/u/michael/IBM20675.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/michael/
Root-URL: 
Email: fmichael,scottg@cs.rochester.edu fashwini,bhlimg@watson.ibm.com  
Title: Coherence Controller Architectures for SMP-Based CC-NUMA Multiprocessors  
Author: Maged M. Michael Ashwini K. Nanda Beng-Hong Lim and Michael L. Scott 
Note: hardware-based and protocol-processor-based controllers by separating or duplicating critical components.  
Address: Rochester, NY 14627 Yorktown Heights, NY 10598  
Affiliation: University of Rochester IBM Research Department of Computer Science Thomas J. Watson Research Center  
Abstract: However, with the advent of SMP processing nodes and the availability of faster processors and networks, the tradeoff between custom hardware and protocol processors needs to be reexamined. This paper studies the performance of custom-hardware and protocol-processor-based coherence controllers in SMP-node-based CC-NUMA systems on applications from the SPLASH-2 suite. Using realistic parameters and detailed models of existing state-of-the-art system components, it shows that the occupancy of coherence controllers can limit the performance of applications with high communication bandwidth requirements where the execution time using protocol processors can be twice as long as using custom hardware. To gain a deeper understanding of the tradeoff, we investigate the effect on performance penalty of varying several architectural parameters that influence the communication characteristics of the applications and the underlying system. We characterize each application's communication requirements and its impact on the performance penalty of protocol processors. This characterization can help system designers predict performance penalties for other applications. We also study the potential of improving the performance of fl This work was performed at IBM Thomas J. Watson Research Center.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Bianchini, D. Chaiken, K. Johnson, D. Kranz, J. Kubiatowicz, B.-H. Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture (ISCA'95), </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Previous research has shown convincingly that scalable shared-memory performance can be achieved on directory-based cache-coherent multiprocessors such as the Stanford DASH [5] and MIT Alewife <ref> [1] </ref> machines. A key component of these machines is the coherence controller on each node that provides cache coherent access to memory that is distributed among the nodes of the multiprocessor. <p> compute processor cycles (5 ns.). response time, and that memory and cache-to-cache data transfers drive the critical quad-word first on the bus to minimize latency. 2.2 Coherence Controller Architectures We consider two main coherence controller designs: a custom hardware coherence controller similar to that in the DASH [5] and Alewife <ref> [1] </ref> systems, and a coherence controller based on commodity protocol processors similar to those in the Typhoon [11] system and its prototypes [10]. The two designs share some common components and features (see figures 2 and 3).
Reference: [2] <author> M. Heinrich, J. Kuskin, D. Ofelt, J. Heinlein, J. P. Singh, R. Simoni, K. Gharachorloo, J. Baxter, D. Nakahira, M. Horowitz, A. Gupta, M. Rosemblum, and J. Hennessy. </author> <title> The Performance Impact of Flexibility in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 274-285, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Simulations of Stanford FLASH, which uses a customized protocol processor optimized for handling coherence actions, show that the performance penalty of its protocol processor in comparison to custom hardware controllers is within 12% for most of their benchmarks <ref> [2] </ref>. Simulations of the Wisconsin Typhoon Simple-COMA system, which uses a protocol processor integrated with the other components of the coherence controller, also show competitive perfor mance that is within 30% of custom-hardware CC-NUMA controllers [11] and within 20% of custom-hardware Simple-COMA controllers [10]. <p> The Stanford FLASH designers find that the performance penalty of using a protocol processor is less than 12% for the applications that they simulated, including Ocean and Radix <ref> [2] </ref>.
Reference: [3] <author> C. Holt, M. Heinrich, J. P. Singh, E. Rothberg, and J. Hennessy. </author> <title> The Effects of Latency, Occupance, and Bandwidth in Distributed Shared Memory Multiprocessors. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Other differences between the two studies are: i) they compare Simple COMA systems, while we compare CC-NUMA systems, ii) they assume a slower network with a latency of 500 ns, which mitigates the penalty of protocol processors, and iii) they considered only uniprocessor nodes. Holt et al. <ref> [3] </ref> perform a study similar to ours on comparing various coherence controller architectures and the effect of latency, occupancy and bandwidth on application per formance. They also find that the occupancy of coherence controllers is critical to the performance of high-bandwidth applications.
Reference: [4] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Si-moni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Bax-ter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hen-nessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA) 1994, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: Instead of hardwiring protocol handlers, the Sun Microsystems S3.mp [8] multiprocessor uses hardware sequencers for modularity in implementing protocol handlers. Subsequent designs for scalable shared-memory multiprocessors, such as the Stanford FLASH <ref> [4] </ref> and the Wis-consin Typhoon machines [11], have touted the use of programmable protocol processors instead of custom hardware FSMs to implement the coherence protocols.
Reference: [5] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Previous research has shown convincingly that scalable shared-memory performance can be achieved on directory-based cache-coherent multiprocessors such as the Stanford DASH <ref> [5] </ref> and MIT Alewife [1] machines. A key component of these machines is the coherence controller on each node that provides cache coherent access to memory that is distributed among the nodes of the multiprocessor. <p> system latencies in compute processor cycles (5 ns.). response time, and that memory and cache-to-cache data transfers drive the critical quad-word first on the bus to minimize latency. 2.2 Coherence Controller Architectures We consider two main coherence controller designs: a custom hardware coherence controller similar to that in the DASH <ref> [5] </ref> and Alewife [1] systems, and a coherence controller based on commodity protocol processors similar to those in the Typhoon [11] system and its prototypes [10]. The two designs share some common components and features (see figures 2 and 3).
Reference: [6] <author> T. Lovett and R. Clapp. STiNG: </author> <title> A CC-NUMA Computer System for the Commercial Marketplace. </title> <booktitle> In Proceedings of the 23rd International Symposium on Computer Architecture, </booktitle> <pages> pages 308-317, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: We consider symmetric multiprocessor (SMP) nodes as well as uniprocessor nodes as the building block for a multiprocessor. The availability of cost-effective SMPs based on the Intel Pentium-Pro [9] makes SMP nodes an attractive choice for CC-NUMA designers <ref> [6] </ref>. However, the added load presented to the coherence controller by multiple SMP processors may affect the choice between custom hardware FSMs and protocol processors. We base our experimental evaluation of the alternative coherence controller architectures on realistic hardware parameters for state-of-the-art system components.
Reference: [7] <author> A.-T. Nguyen, M. Michael, A. Sharma, and J. Torrellas. </author> <title> The Augmint Multiprocessor Simulation Toolkit for Intel x86 Architectures. </title> <booktitle> In Proceedings of the 1996 IEEE International Conference on Computer Design (ICCD), </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: In our protocol, we allow remote owners to respond directly to remote requesters with data, but invalidation acknowledgments are collected only at the home node. 3 Performance Results 3.1 Experimental Methodology We use execution-driven simulation (based on a version of the Augmint simulation toolkit <ref> [7] </ref> that runs on the PowerPC architecture) to evaluate the performance of the four coherence controller designs, HWC, PPC, 2HWC, and 2PPC. Our simulator includes detailed contention models for SMP buses, memory controllers, interleaved memory banks, protocol engines, directory DRAM, and external point contention for the interconnection network.
Reference: [8] <author> A. Nowatzyk, G. Aybay, M. Browne, E. Kelly, M. Parkin, B. Radke, and S.Vishin. </author> <title> The S3.mp Scalable Shared Memory Multiprocessor. </title> <booktitle> In Proceedings of 1995 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: In DASH and Alewife, the cache coherence protocol is hardwired in custom hardware finite state machines (FSMs) within the coherence controllers. Instead of hardwiring protocol handlers, the Sun Microsystems S3.mp <ref> [8] </ref> multiprocessor uses hardware sequencers for modularity in implementing protocol handlers. Subsequent designs for scalable shared-memory multiprocessors, such as the Stanford FLASH [4] and the Wis-consin Typhoon machines [11], have touted the use of programmable protocol processors instead of custom hardware FSMs to implement the coherence protocols. <p> We use the term protocol engine to refer to both the protocol processor in the PPC design and the protocol FSM in the HWC design. For distributing the protocol requests between the two engines, we use a policy similar to that used in the S3.mp system <ref> [8] </ref>, where protocol requests for memory addresses on the local node are handled by one protocol engine (LPE) and protocol requests for memory addresses on remote nodes are handled by the other protocol engine (RPE). Only the LPE needs to access the directory.
Reference: [9] <institution> Pentium Pro Family Developer's Manual. Intel Corporation, </institution> <year> 1996. </year>
Reference-contexts: We consider symmetric multiprocessor (SMP) nodes as well as uniprocessor nodes as the building block for a multiprocessor. The availability of cost-effective SMPs based on the Intel Pentium-Pro <ref> [9] </ref> makes SMP nodes an attractive choice for CC-NUMA designers [6]. However, the added load presented to the coherence controller by multiple SMP processors may affect the choice between custom hardware FSMs and protocol processors. <p> The controller runs at 100 MHz, the same frequency as the SMP bus. All the coherence controller components are on the same chip ex (e,g. Pentium Pro <ref> [9] </ref>) allow users to designate regions of memory to be cached write-through. 3 with local and remote protocol FSMs (2HWC). cept the directories. Figure 3 shows a block diagram of a protocol-processor-based coherence controller (PPC). The protocol processor (PP) is a PowerPC 604 running at 200 MHz.
Reference: [10] <author> S. Reinhardt, R. Pfile, and D. Wood. </author> <title> Decoupled Hardware Support for Duistributed Shared Memory. </title> <booktitle> In Proceedings of the 23rd International Symposium on Computer Architecture, </booktitle> <pages> pages 34-43, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Simulations of the Wisconsin Typhoon Simple-COMA system, which uses a protocol processor integrated with the other components of the coherence controller, also show competitive perfor mance that is within 30% of custom-hardware CC-NUMA controllers [11] and within 20% of custom-hardware Simple-COMA controllers <ref> [10] </ref>. Even so, the choice between custom hardware and protocol processors for implementing coherence protocols remains a key design issue for scalable shared-memory multiprocessors. The goal of this research is to examine in detail the performance tradeoffs between these two alternatives in designing a CC-NUMA multiprocessor coherence controller. <p> to minimize latency. 2.2 Coherence Controller Architectures We consider two main coherence controller designs: a custom hardware coherence controller similar to that in the DASH [5] and Alewife [1] systems, and a coherence controller based on commodity protocol processors similar to those in the Typhoon [11] system and its prototypes <ref> [10] </ref>. The two designs share some common components and features (see figures 2 and 3). Both designs use duplicate directories to allow fast response to common requests on the pipelined SMP bus (one directory lookup per 2 bus cycles). <p> The relative increase in latency from HWC to PPC is only 49%, which is consistent with the 33% increase reported for Typhoon <ref> [10] </ref>, taking into account that we consider a more decoupled coherence controller design and we use a faster network than that used in the Typhoon study. It is worth noting that in Table 3, there is no entry for updating the directory state at the home node. <p> It is hard to compare our results to theirs because of the difficulty in determining what fraction of the performance difference is due to Simple COMA vs. CC-NUMA, and custom hardware vs. protocol processors. In <ref> [10] </ref>, Reinhardt et al. compare the Wisconsin Typhoon and its first-generation prototypes with an idealized Simple COMA system. Here, their results show that the performance penalty of using integrated protocol processors is less than 20%. In contrast, we find larger performance penalties of close to 100%.
Reference: [11] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA) 1994, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: Instead of hardwiring protocol handlers, the Sun Microsystems S3.mp [8] multiprocessor uses hardware sequencers for modularity in implementing protocol handlers. Subsequent designs for scalable shared-memory multiprocessors, such as the Stanford FLASH [4] and the Wis-consin Typhoon machines <ref> [11] </ref>, have touted the use of programmable protocol processors instead of custom hardware FSMs to implement the coherence protocols. <p> Simulations of the Wisconsin Typhoon Simple-COMA system, which uses a protocol processor integrated with the other components of the coherence controller, also show competitive perfor mance that is within 30% of custom-hardware CC-NUMA controllers <ref> [11] </ref> and within 20% of custom-hardware Simple-COMA controllers [10]. Even so, the choice between custom hardware and protocol processors for implementing coherence protocols remains a key design issue for scalable shared-memory multiprocessors. <p> quad-word first on the bus to minimize latency. 2.2 Coherence Controller Architectures We consider two main coherence controller designs: a custom hardware coherence controller similar to that in the DASH [5] and Alewife [1] systems, and a coherence controller based on commodity protocol processors similar to those in the Typhoon <ref> [11] </ref> system and its prototypes [10]. The two designs share some common components and features (see figures 2 and 3). Both designs use duplicate directories to allow fast response to common requests on the pipelined SMP bus (one directory lookup per 2 bus cycles). <p> In <ref> [11] </ref>, Reinhardt et al. introduce the Wisconsin Typhoon architecture that relies on a SPARC processor core integrated with the other components of the coherence controller to 9 execute coherence handlers that implement a Simple COMA protocol.
Reference: [12] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year> <month> 11 </month>
Reference-contexts: What distinguishes our work from previous research is that we consider commodity protocol processors on SMP-based CC-NUMA and a wider range of architectural parameters. We simulate eight applications from the SPLASH-2 benchmark suite <ref> [12] </ref> to compare the application performance of the architectures. The results show that for a 64-processor system based on four-processor SMP nodes, protocol processors result in a performance penalty (increase in execution time relative to that of custom hardware controllers) of 4% - 93%. <p> All coherence controller implementations use the same cache coherence protocol. We use eight benchmarks from the SPLASH-2 suite <ref> [12] </ref>, (table 5) to evaluate the performance of the four coherence controller implementations. All the benchmarks are written in C and compiled using IBM XLC C compiler with optimization level -O2. All experimental results reported in this paper are for the parallel phase only of these applications. <p> LU and Cholesky are run on 32-processor systems (8 nodes fi 4 processors each) as they suffer from load imbalance on 64 processors with the data sets used <ref> [12] </ref>. <p> We notice a significant increase in execution time (regardless of the coherence controller architecture) relative to the corresponding execution times on the base system, for FFT, Water-Nsquared, Cholesky, and LU, which have high spatial locality <ref> [12] </ref>, while there is minor increase in execution time for the other benchmarks. <p> Table 6 shows communication statistics collected from simulations of HWC and PPC on the base system configuration 3 Applications like Radix maintain a constant communication rate with different data sizes <ref> [12] </ref>. processors per SMP node. (except Cholesky and LU are run on 32 processors). <p> Note that the high queuing delay for FFT is attributed to its bursty communication pattern <ref> [12] </ref>.
References-found: 12

