URL: http://www.aic.nrl.navy.mil/~spears/papers/foga96.ps.gz
Refering-URL: http://www.aic.nrl.navy.mil/~spears/pubs.html
Root-URL: 
Email: spears@aic.nrl.navy.mil  kdejong@gmu.edu  
Title: Analyzing GAs Using Markov Models with Semantically Ordered and Lumped States  
Author: William M. Spears Kenneth A. De Jong 
Address: Washington, DC 20375-5337  Fairfax, VA 22030  
Affiliation: Code 5510 AI Center Naval Research Laboratory  Computer Science Department George Mason University  
Abstract: At the previous FOGA workshop, we presented some initial results on using Markov models to analyze the transient behavior of genetic algorithms (GAs) being used as function optimizers (GAFOs). In that paper, the states of the Markov model were ordered via a simple and mathematically convenient lexicographic ordering used initially by Nix and Vose. In this paper, we explore alternative orderings of states based on interesting semantic properties such as average fitness, degree of homogeneity, average attractive force, etc. We also explore lumping techniques for reducing the size of the state space. Analysis of these reordered and lumped Markov models provides new insights into the transient behavior of GAs in general and GAFOs in particular.
Abstract-found: 1
Intro-found: 1
Reference: <author> T. Dayar & W. J. Stewart. </author> <year> (1996) </year> <month> Quasi-lumpability, </month> <title> lower bounding coupling matrices, and nearly completely decomposable Markov chains. </title> <note> To appear in the SIAM Journal on Matrix Analysis and Applications. </note>
Reference: <author> K. A. De Jong, W. M. Spears, & D. F. Gordon. </author> <title> (1994) Using Markov chains to analyze GAFOs. </title> <booktitle> Proceedings of the Foundations of Genetic Algorithms Workshop. </booktitle> <address> Estes Park, </address> <publisher> CO: Morgan Kaufmann, </publisher> <pages> 115 - 137. </pages>
Reference-contexts: In this paper, we extend and expand on the results we presented in the previous FOGA workshop concerning the use of Markov models to analyze the transient behavior of GAs being used for function optimization <ref> (De Jong, Spears and Gordon, 1994) </ref>. <p> exact state transition probabilities Q i;j , which specify how likely it is that a simple GA in state i (the current population) will be in state j in the next generation: Q i;j = n! y=0 M F i z j;y (2) As we showed in our previous paper <ref> (De Jong, Spears and Gordon, 1994) </ref>, the resulting state transition matrix Q can be used in a variety of ways to gain important insights about the 1 For programming convenience we transpose the Z matrix of Nix and Vose (1992). 2 In their paper they assume a standard bit flipping mutation <p> To get a better sense of the generality of these results, we also computed for those models investigated in our earlier paper <ref> (De Jong, Spears and Gordon, 1994) </ref>. Table 3 summarizes some representative results at k = 100. For these particular functions l = 2, n = 10, N = 286, and = 1:0. We analyzed two settings for as indicated in the table. <p> At low mutation rates Hamming distance provides a good ordering, but degrades significantly at higher mutation rates. It is also interesting to note that ordering by average fitness is a much better predictor at higher mutation rates. This agrees with the results presented in our earlier paper <ref> (De Jong, Spears and Gordon, 1994) </ref> which suggested that optimal GAFO performance is frequently obtained using higher mutation rates than those traditionally used. The quantitative and qualitative analyses of these Markov models provide a simple intuitive picture of the transient behavior of a GA. <p> To address this issue, we have explored several approaches for reducing the size of the state space in order to allow us to scale to larger GA models. In keeping with the theme of our earlier paper <ref> (De Jong, Spears and Gordon, 1994) </ref>, we are interested in having models make predictions about GAFO behavior, that is, predictions appropriate to the use of GAs as optimizers. To answer such questions, we need only combine Q k with a set of initial conditions concerning a GA at generation 0.
Reference: <author> J. Kemeny & J. Snell. </author> <title> (1960) Finite Markov Chains. </title> <address> D. </address> <publisher> Van Nostrand, </publisher> <address> New York. </address>
Reference: <author> A. E. Nix & M. D. Vose. </author> <title> (1992) Modelling genetic algorithms with Markov chains. </title> <journal> Annals of Mathematics and Artificial Intelligence #5, </journal> <volume> 79 - 88. </volume>
Reference-contexts: In addition, we explore various lumping techniques for reducing the size of the Markov models and evaluate the effects of lumping on model accuracy. Our work continues to be based on the Nix and Vose Markov model <ref> (Nix and Vose, 1992) </ref> of a simple GA using fixed-length binary strings, 1-point crossover, bit-flipping mutation, and fitness proportional selection. If l is the length of the binary strings, then there are r = 2 l possible strings.
Reference: <author> R. Sidje & W. J. Stewart. </author> <title> (1996) A survey of methods for computing large sparse matrix exponentials arising in Markov chains. </title> <note> Submitted for publication. </note>
Reference-contexts: Preliminary results appear to confirm this hypothesis. Transient behavior has also been investigated by the Markov community, but traditional techniques often involve the computation of "matrix exponentials", in which the Markov chain is described by a system of ordinary differential equations <ref> (e.g., see Sidje and Stewart, 1996) </ref>. Although these techniques are not related to our lumping algorithm, it is possible that they could answer the same types of questions. Future work will focus on this possibility.
Reference: <author> W. M. Spears. </author> <title> (1996) A compression algorithm for probability transition matrices. </title> <type> NRL--AIC Technical Report. </type> <note> In preparation. </note>
Reference-contexts: Unfortunately, this data is not known in general, and is frequently a function of time. However, recall that the column masses provide reasonable estimates of the relative amount of time spent in particular states, and hence are good candidates for the weights to be used for lumping <ref> (for a mathematical treatment of this lumping algorithm see Spears, 1996) </ref>. Mathematically, the lumping algorithm can be described as follows. Assume that two states have been chosen for lumping. <p> The rest of the values in Q 0 (which refer to states 2 and 3) are weighted averages (sometimes trivial) of sums of the values in the 2nd and 3rd rows and columns of Q. In general, the exact lumping of arbitrary states is not always possible <ref> (see Spears, 1996) </ref>. So we are left with a situation in which states with identical rows can be combined without difficulty, but is not likely to result in a significant reduction in the number of states since identical rows are encountered relatively infrequently.
Reference: <author> W. J. Stewart & W. Wu. </author> <title> (1992) Numerical experiments with iteration and aggregation for Markov chains. </title> <journal> ORSA Journal on Computing, </journal> <volume> Volume 4, #3, 336 - 350. </volume>
Reference-contexts: This theorem states that if lumping is performed in this manner, the steady state behavior of the lumped system is the same as the original system. Interestingly, this form of aggregation has also been applied in the Markov community <ref> (e.g., see Stewart and Wu, 1992) </ref>. In both cases the emphasis has been on examining steady state behavior. Note that one difference between our method of lumping and the more traditional method is the choice of weights we focus on column mass instead of steady state values.
Reference: <author> M. Vose. </author> <title> (1995) Modeling Simple Genetic Algorithms. </title> <journal> Evolutionary Computation, </journal> <volume> Volume 3, #4, </volume> <pages> 453-472. </pages>
Reference-contexts: In this section, results are presented which provide additional insight into the transient behavior of GAFOs, and which tie in nicely with other recent mathematical results such as <ref> (Vose, 1995) </ref>. 2.1 ORDERING STATES BY AVERAGE FITNESS Perhaps the most intuitive hypothesis as to the character of this emerging set of high probability states is that they correspond to states (populations) with high average fitness.
References-found: 8

