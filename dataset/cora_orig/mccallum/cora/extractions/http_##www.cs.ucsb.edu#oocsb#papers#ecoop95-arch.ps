URL: http://www.cs.ucsb.edu/oocsb/papers/ecoop95-arch.ps
Refering-URL: http://www.cs.ucsb.edu/oocsb/papers/oo-hardware.html
Root-URL: http://www.cs.ucsb.edu
Title: Do Object-Oriented Languages Need Special Hardware Support?  
Author: Urs Hlzle David Ungar 
Date: 253-282, August 1995.  
Note: ECOOP 95 Conference Proceedings, Springer Verlag Lecture Notes in Computer Science 952, p.  
Abstract: Previous studies have shown that object-oriented programs have different execution characteristics than procedural programs, and that special object-oriented hardware can improve performance. The results of these studies may no longer hold because compiler optimizations can remove a large fraction of the differences. Our measurements show that SELF programs are more similar to C programs than are C++ programs, even though SELF is much more radically object-oriented than C++ and thus should differ much more from C. Furthermore, the benefit of tagged arithmetic instructions in the SPARC architecture (originally motivated by Smalltalk and Lisp implementations) appears to be small. Also, special hardware could hardly reduce message dispatch overhead since dispatch sequences are already very short. Two generic hardware features, instruction cache size and data cache write policy, have a much greater impact on performance.
Abstract-found: 1
Intro-found: 1
Reference: [B+87] <author> C. Baker et al. </author> <title> The Symbolics Ivory processor: a 40 bit tagged architecture Lisp microprocessor. </title> <booktitle> Proceedings of the 1987 IEEE International Conference on Computer Design, p. </booktitle> <pages> 512-15, </pages> <address> Rye Brook, NY, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: Using a non-optimizing Smalltalk compiler, Ungar reported that SOAR Smalltalk would have been 26% slower without the tagged instructions [Ung87]. Similarly, Lisp machines have included hardware support for tagging <ref> [B+87, SH87] </ref>. But how useful is hardware support in a system with an optimizing compiler? To answer this question, it is necessary to examine the code generated for integer addition (or subtraction) in SELF-93.
Reference: [CGZ94] <author> Brad Calder, Dirk Grunwald, and Benjamin Zorn. </author> <title> Quantifying Behavioral Differences Between C and C++ Programs. </title> <type> Technical Report CU-CS-698-94, </type> <institution> University of Colorado, Boulder, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Even for a hybrid language like C++ (which has C at its core and thus shouldnt behave too differently at runtime), significant differences were found. Table 2 shows data from a study by Calder et al. <ref> [CGZ94] </ref> which measured the behavior of several large C++ applications and compared it to that of the SPECint92 suite and other C programs. The study used the GNU C and C++ compilers on the MIPS architecture. <p> C++ SPECint92 ratio C++ / SPEC basic block size 8.0 4.9 1.6 call/return frequency 4.6% 0.7% 6.7 instructions per conditional branch 15.9 6.4 2.5 Table 2. Differences between C++ and C (from <ref> [CGZ94] </ref>) 4 one-third of their time in such routines, and separating out SELF-only ensures that our data is not biased by the execution behavior of non-SELF code. <p> If SELF used indirect function calls to implement message dispatch, one could explain the lower frequency of conditional branches with the object-oriented programming style which typically replaces if or switch statements with dynamic dispatch; Calder et al. observed this effect when comparing C++ programs to the SPEC C programs <ref> [CGZ94] </ref>. However, since the SELF implementation uses comparisons (i.e., inline caching [HCU91]) rather than indirect function calls, we cannot explain the difference in this way. <p> What is even more surprising is that SELF-93 is closer to GNU C than is GNU C++. Figure 3 illustrates this point by summarizing the C++ data from <ref> [CGZ94] </ref> and our own measurements. In each category, SELF is closer to SPEC than is C++. For example, the basic block size in C++ is 1.6 times higher than SPEC, but in SELF it is only 1.1 times higher than SPEC. in comparison to the SPECint median.
Reference: [CUL89] <author> Craig Chambers, David Ungar, and Elgin Lee. </author> <title> An Efficient Implementation of SELF, a Dynamically-Typed Object-Oriented Language Based on Prototypes. </title> <booktitle> In OOPSLA 89 Conference Proceedings, p. </booktitle> <pages> 49-70, </pages> <address> New Orleans, LA, </address> <month> October </month> <year> 1989. </year> <note> Published as SIGPLAN Notices 24(10), </note> <month> October </month> <year> 1989. </year> <title> Also published in Lisp and Symbolic Computation 4(3), </title> <publisher> Kluwer Academic Publishers, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: The SELF implementation employs several optimizations aimed specifically at reducing the overhead of dynamic dispatch <ref> [CUL89, HCU91, CU91, HU94] </ref>. With these optimizations, programs run several times faster than comparable Smalltalk programs (since the fastest Smalltalk system uses only a non-optimizing compiler) and about half as fast as optimized C programs (for the benchmarks mentioned in [CU91] and [HU94]).
Reference: [CU91] <author> Craig Chambers and David Ungar. </author> <title> Making Pure Object-Oriented Languages Practical. </title> <booktitle> OOPSLA 91 Conference Proceedings, </booktitle> <address> Phoenix, AZ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: The SELF implementation employs several optimizations aimed specifically at reducing the overhead of dynamic dispatch <ref> [CUL89, HCU91, CU91, HU94] </ref>. With these optimizations, programs run several times faster than comparable Smalltalk programs (since the fastest Smalltalk system uses only a non-optimizing compiler) and about half as fast as optimized C programs (for the benchmarks mentioned in [CU91] and [HU94]). <p> With these optimizations, programs run several times faster than comparable Smalltalk programs (since the fastest Smalltalk system uses only a non-optimizing compiler) and about half as fast as optimized C programs (for the benchmarks mentioned in <ref> [CU91] </ref> and [HU94]). Since the goal of this study was to determine if object-oriented programs still need special hardware support when compiled with state of-the-art optimization techniques, the SELF-93 implementation was a good match. We measured the behavior of several large SELF applications with an instruction-level tracing tool. <p> Of course, programs with a higher frequency of integer arithmetic (such as the Stanford integer benchmarks) could benefit more from tagged arithmetic instructions. However, this class of programs is also more amenable to optimizations that eliminate type checks <ref> [CU91] </ref>. This result stands in marked contrast to Ungars measurements showing that SOAR would have been 26% slower without instructions for tagged arithmetic [Ung87].
Reference: [Cme91] <author> Robert F. Cmelik, Shing I. Kong, David R. Ditzel, and Edmund J. Kelly. </author> <title> An Analysis of MIPS and SPARC Instruction Set Utilization on the SPEC Benchmarks. </title> <booktitle> ASPLOS IV, </booktitle> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Since there are only four benchmarks in the SPECint89 suite, the individual data points for C are shown directly, and the box plots (dotted) are given for reference only. The SPECint89 data are taken from Cmelik et al <ref> [Cme91] </ref>. 1. Overall, there are few differences between the SPEC benchmarks and SELF. Often, the differences between the individual SPEC benchmarks are bigger than the difference between SELF and C (in the graph, the SPEC boxes are usually larger than the corresponding SELF boxes). 2.
Reference: [CK93] <author> Robert F. Cmelik and David Keppel. Shade: </author> <title> A Fast Instruction-Set Simulator for Execution Profiling. </title> <institution> Sun Microsystems Laboratories, </institution> <note> Technical Report SMLI TR-93-12, 1993. Also published as Technical Report CSE-TR 93-06-06, </note> <institution> University of Washington, </institution> <year> 1993. </year>
Reference-contexts: The measurements were made with a combination of publicly available tools which were adapted for our purposes: the Shade tracing tool <ref> [CK93] </ref>, the Dinero cache simulator [Hill87], and the SPA SPARC simulator [Irl91]. Our simulator models the Cypress CY7C601 chip, a scalar implementation of the SPARC architecture used in the SPARCstation-2 workstation. To reduce the simulation time, execution times were kept relatively short (25M instructions or less, see appendix).
Reference: [Deu83] <author> L. Peter Deutsch. </author> <title> The Dorado Smalltalk-80 Implementation: Hardware Architectures Impact on Software Architecture. </title> <booktitle> In [Kra83]. </booktitle>
Reference-contexts: For example, the Xerox Dorado, for a long time the fastest Smalltalk implementation available, contained microcode support for large portions of the Smalltalk virtual machine <ref> [Deu83] </ref>. Ungar reported that the SOAR system would have been 26% slower without instructions for tagged arithmetic [Ung87]. Williams and Wolczko argued that software-controlled caching improves the performance and locality of object-oriented systems [WW90].
Reference: [DTM94] <author> Amer Diwan, David Tarditi, and Eliot Moss. </author> <title> Memory Subsystem Performance of Programs with Intensive Heap Allocation. </title> <booktitle> In 21st Annual ACM Symposium on Principles of Programming Languages, p. </booktitle> <pages> 1-14, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: The write-allocate policy has been shown to be beneficial for programs with intensive heap allocation; we will discuss it in more detail below. We assume that there is no cost associated with write misses, i.e., that write buffers can absorb almost all writes (see <ref> [DTM94, Rei93, Rei94] </ref> for justifications of this assumption). Even though many SELF programs allocate objects at a rate of about 1 Mbyte/s, the data cache performance is good. <p> Even with an 8K data cache, the median overhead would still be less than 17%. This data is consistent with that of Diwan et al <ref> [DTM94] </ref> who have measured allocation-intensive ML programs and found very low data cache overheads for the same cache organization (write-allocate, subblock placement). <p> Such low data cache overheads leave little room for improvement through special cache features (e.g., [PS89, WW90]). (direct-mapped cache, 32-byte lines) (direct-mapped, 32-byte lines, write-allocate with subblock placement) 11 Diwan et al <ref> [DTM94] </ref> also observed that the data cache overhead of ML programs increased substantially with a write-noallocate policy, i.e., with a cache that does not allocate a cache line on a write miss. <p> Write-allocate caches with subblock placement reduce read miss ratios by up to a factor of two, and write miss ratios by a factor of ten. These findings are consistent with other work for non-object oriented languages <ref> [KLS92, Jou93, Rei93, DTM94] </ref>. Instruction cache size significantly impacts performance. For example, doubling the instruction cache from 32K to 64K improves performance by 15% on a SPARCstation-2. This improvement is higher than that of any OO-specific architectural feature we considered.
Reference: [GR83] <author> Adele Goldberg and David Robson. </author> <title> Smalltalk-80: The Language and its Implementation. Second Edition, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1985 </year>
Reference-contexts: Thus, the results of these studies may overstate the benefits of special hardware features. For this study, we used the SELF-93 implementation [Hl94]. Two factors make it a good vehicle for measuring the execution characteristics of object-oriented programs: SELF [US87], like Smalltalk-80 <ref> [GR83] </ref>, is a very pure object-oriented language. All data are objects, and virtually every operation (e.g., instance variable access, integer addition, or control structures like if and while) involves message sends. The pure object-oriented language model also inspires a highly object-oriented programming style that makes frequent use of fine-grained abstractions.
Reference: [HCU91] <author> Urs Hlzle, Craig Chambers, and David Ungar. </author> <title> Optimizing Dynamically-Typed Object-Oriented Languages with Polymorphic Inline Caches. </title> <booktitle> In ECOOP 91 Conference Proceedings, Geneva, 1991. Published as Springer Verlag Lecture Notes in Computer Science 512, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: The SELF implementation employs several optimizations aimed specifically at reducing the overhead of dynamic dispatch <ref> [CUL89, HCU91, CU91, HU94] </ref>. With these optimizations, programs run several times faster than comparable Smalltalk programs (since the fastest Smalltalk system uses only a non-optimizing compiler) and about half as fast as optimized C programs (for the benchmarks mentioned in [CU91] and [HU94]). <p> Second, object types are represented by the address of the type descriptor, and the implementation of message dispatch compares the receivers type against the expected type (s) <ref> [HCU91] </ref>. Since message dispatch is frequent, so are 32-bit constants. Compared to the SPEC benchmarks, SELF shows few differences. Besides the differences in conditional branches and sethi instructions already mentioned above, only logical instructions and comparisons are markedly different. Logical instructions are more frequent in SELF for two reasons. <p> However, since the SELF implementation uses comparisons (i.e., inline caching <ref> [HCU91] </ref>) rather than indirect function calls, we cannot explain the difference in this way. <p> Thus, we include all such type tests in our overhead, as well as the time spent dispatching non-inlined message sends. 1 Message dispatch in SELF is implemented with polymorphic inline caching <ref> [HCU91] </ref>, so that most sends incur only a small overhead (a load and a compare-and-branch sequence) in addition to the cost of a direct call. the two smallest programs (Richards and DeltaBlue) whose overhead lies around 25%.
Reference: [HU94] <author> Urs Hlzle and David Ungar. </author> <title> Optimizing dynamically-dispatched calls with run-time type feedback. </title> <booktitle> In PLDI 94 Conference Proceedings, </booktitle> <pages> pp. 326-335, </pages> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year> <note> Published as SIGPLAN Notices 29(6), </note> <month> June </month> <year> 1994. </year>
Reference-contexts: The SELF implementation employs several optimizations aimed specifically at reducing the overhead of dynamic dispatch <ref> [CUL89, HCU91, CU91, HU94] </ref>. With these optimizations, programs run several times faster than comparable Smalltalk programs (since the fastest Smalltalk system uses only a non-optimizing compiler) and about half as fast as optimized C programs (for the benchmarks mentioned in [CU91] and [HU94]). <p> With these optimizations, programs run several times faster than comparable Smalltalk programs (since the fastest Smalltalk system uses only a non-optimizing compiler) and about half as fast as optimized C programs (for the benchmarks mentioned in [CU91] and <ref> [HU94] </ref>). Since the goal of this study was to determine if object-oriented programs still need special hardware support when compiled with state of-the-art optimization techniques, the SELF-93 implementation was a good match. We measured the behavior of several large SELF applications with an instruction-level tracing tool.
Reference: [Hl94] <author> Urs Hlzle. </author> <title> Adaptive Optimization for Self: Reconciling High Performance with Exploratory Programming. </title> <type> Ph.D. Thesis, Technical Report STAN-CS-TR-94-1520, </type> <institution> Department of Computer Science, Stanford University, </institution> <year> 1994. </year>
Reference-contexts: However, none of the systems previously studied employed compiler optimizations specifically aimed at reducing the overhead of message passing. Thus, the results of these studies may overstate the benefits of special hardware features. For this study, we used the SELF-93 implementation <ref> [Hl94] </ref>. Two factors make it a good vehicle for measuring the execution characteristics of object-oriented programs: SELF [US87], like Smalltalk-80 [GR83], is a very pure object-oriented language. <p> For example, the load class instruction is handled equally fast with a normal load instruction <ref> [Hl94] </ref>. Including only the tagged arithmetic instructions, Ungars estimate would be 12%. The SOAR code sequences for integer arithmetic without tagged instructions are slowed down because SOAR does not have a branch on overow instruction. <p> At first sight, the relatively high overhead seems to indicate a possible opportunity for hardware support. However, the main reason for the overhead is the extreme frequency of dynamic dispatch. The median time per test is 8 cycles <ref> [Hl94] </ref>; for Richards and DeltaBlue, the time per test is even lower (4.5 and 3.4 cycles per test, respectively) since these programs send many messages to integers, so that integer tag tests dominate the dispatch overhead.
Reference: [Hill87] <author> Mark D. Hill. </author> <title> Aspects of Cache Memory and Instruction Buffer Performance. </title> <type> Technical Report UCB/CSD 87/381, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: The measurements were made with a combination of publicly available tools which were adapted for our purposes: the Shade tracing tool [CK93], the Dinero cache simulator <ref> [Hill87] </ref>, and the SPA SPARC simulator [Irl91]. Our simulator models the Cypress CY7C601 chip, a scalar implementation of the SPARC architecture used in the SPARCstation-2 workstation. To reduce the simulation time, execution times were kept relatively short (25M instructions or less, see appendix).
Reference: [Irl91] <author> Gordon Irlam. </author> <title> SPASPARC analyzer tool set. </title> <note> Available via ftp from cs.adelaide.edu.au, </note> <year> 1991. </year>
Reference-contexts: The measurements were made with a combination of publicly available tools which were adapted for our purposes: the Shade tracing tool [CK93], the Dinero cache simulator [Hill87], and the SPA SPARC simulator <ref> [Irl91] </ref>. Our simulator models the Cypress CY7C601 chip, a scalar implementation of the SPARC architecture used in the SPARCstation-2 workstation. To reduce the simulation time, execution times were kept relatively short (25M instructions or less, see appendix).
Reference: [Jou93] <author> Norm Jouppi. </author> <title> Cache Write Policies and Performance. </title> <booktitle> In ISCA'20 Conference Proceedings, </booktitle> <pages> pp. 191-201, </pages> <address> San Diego, CA, </address> <year> 1993. </year> <note> Published as Computer Architecture News 21(2), </note> <month> May </month> <year> 1993. </year>
Reference-contexts: Similar results have also been reported by Reinhold for Scheme programs [Rei93], by Jouppi for the SPEC benchmark suite <ref> [Jou93] </ref>, and by Koopman et al for combinator graph reduction [KLS92]. <p> Write-allocate caches with subblock placement reduce read miss ratios by up to a factor of two, and write miss ratios by a factor of ten. These findings are consistent with other work for non-object oriented languages <ref> [KLS92, Jou93, Rei93, DTM94] </ref>. Instruction cache size significantly impacts performance. For example, doubling the instruction cache from 32K to 64K improves performance by 15% on a SPARCstation-2. This improvement is higher than that of any OO-specific architectural feature we considered.
Reference: [Kra83] <author> Glenn Krasner, ed., </author> <title> Smalltalk-80: Bits of History and Words of Advice. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: Many previous studies have found the execution characteristics of object-oriented languages to be very different from C. For example, Smalltalk studies <ref> [Kra83, Ung87] </ref> have shown calls to be much more frequent than in other languages. Even for a hybrid language like C++ (which has C at its core and thus shouldnt behave too differently at runtime), significant differences were found. <p> Finding (looking up) the correct target address was a bottleneck in early Smalltalk implementations <ref> [Kra83] </ref>. Since message dispatch is extremely frequent in pure object-oriented languages (recall that virtually every operation involves message sends), hardware support could possibly speed up execution. To test this hypothesis, we measured the time spent in message dispatch (including all integer tag tests).
Reference: [KLS92] <author> Philip Koopman, Peter Lee, and Daniel Siewiorek. </author> <title> Cache behavior of combinator graph reduction. </title> <journal> TOPLAS 14(2): </journal> <pages> 265-297, </pages> <year> 1992. </year>
Reference-contexts: Similar results have also been reported by Reinhold for Scheme programs [Rei93], by Jouppi for the SPEC benchmark suite [Jou93], and by Koopman et al for combinator graph reduction <ref> [KLS92] </ref>. <p> Write-allocate caches with subblock placement reduce read miss ratios by up to a factor of two, and write miss ratios by a factor of ten. These findings are consistent with other work for non-object oriented languages <ref> [KLS92, Jou93, Rei93, DTM94] </ref>. Instruction cache size significantly impacts performance. For example, doubling the instruction cache from 32K to 64K improves performance by 15% on a SPARCstation-2. This improvement is higher than that of any OO-specific architectural feature we considered.
Reference: [Pat85] <author> David A. Patterson. </author> <title> Reduced Instruction Set Computers. </title> <booktitle> Communications of the ACM 28 (1): </booktitle> <pages> 8-21, </pages> <month> January </month> <year> 1985. </year>
Reference: [PS89] <author> Chih-Jui Peng and Gurindar Sohi. </author> <title> Cache memory design considerations to support languages with dynamic heap allocation. </title> <type> Technical Report 860, </type> <institution> University of Wisconsin, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: Similar results have also been reported by Reinhold for Scheme programs [Rei93], by Jouppi for the SPEC benchmark suite [Jou93], and by Koopman et al for combinator graph reduction [KLS92]. Such low data cache overheads leave little room for improvement through special cache features (e.g., <ref> [PS89, WW90] </ref>). (direct-mapped cache, 32-byte lines) (direct-mapped, 32-byte lines, write-allocate with subblock placement) 11 Diwan et al [DTM94] also observed that the data cache overhead of ML programs increased substantially with a write-noallocate policy, i.e., with a cache that does not allocate a cache line on a write miss.
Reference: [PH93] <author> D. N. Pnevmatikatos and M. D. Hill. </author> <title> Cache Performance of the Integer SPEC Benchmarks on a RISC. </title> <booktitle> Computer Architecture News 18 (2): </booktitle> <pages> 53-68. </pages>
Reference: [Rei93] <author> Mark Reinhold. </author> <title> Cache Performance of Garbage-Collected Programming Languages. </title> <type> Ph.D. Thesis, Technical Report MIT/LCS/TR-581, </type> <institution> Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: The write-allocate policy has been shown to be beneficial for programs with intensive heap allocation; we will discuss it in more detail below. We assume that there is no cost associated with write misses, i.e., that write buffers can absorb almost all writes (see <ref> [DTM94, Rei93, Rei94] </ref> for justifications of this assumption). Even though many SELF programs allocate objects at a rate of about 1 Mbyte/s, the data cache performance is good. <p> This data is consistent with that of Diwan et al [DTM94] who have measured allocation-intensive ML programs and found very low data cache overheads for the same cache organization (write-allocate, subblock placement). Similar results have also been reported by Reinhold for Scheme programs <ref> [Rei93] </ref>, by Jouppi for the SPEC benchmark suite [Jou93], and by Koopman et al for combinator graph reduction [KLS92]. <p> Write-allocate caches with subblock placement reduce read miss ratios by up to a factor of two, and write miss ratios by a factor of ten. These findings are consistent with other work for non-object oriented languages <ref> [KLS92, Jou93, Rei93, DTM94] </ref>. Instruction cache size significantly impacts performance. For example, doubling the instruction cache from 32K to 64K improves performance by 15% on a SPARCstation-2. This improvement is higher than that of any OO-specific architectural feature we considered.
Reference: [Rei94] <author> Mark Reinhold. </author> <title> Cache Performance of Garbage-Collected Programs. </title> <booktitle> In PLDI 94 Conference Proceedings, </booktitle> <pages> pp. 206-217, </pages> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year> <note> Published as SIGPLAN Notices 29(6), </note> <month> June </month> <year> 1994. </year>
Reference-contexts: The write-allocate policy has been shown to be beneficial for programs with intensive heap allocation; we will discuss it in more detail below. We assume that there is no cost associated with write misses, i.e., that write buffers can absorb almost all writes (see <ref> [DTM94, Rei93, Rei94] </ref> for justifications of this assumption). Even though many SELF programs allocate objects at a rate of about 1 Mbyte/s, the data cache performance is good.
Reference: [SUH86] <author> A. Dain Samples, David Ungar, and Paul Hilfinger. </author> <title> SOAR: Smalltalk Without Bytecodes. </title> <booktitle> OOPSLA 86 Conference Proceedings, </booktitle> <pages> pp. 107-118, </pages> <address> Portland, OR, </address> <month> September </month> <year> 1986. </year> <note> Published as SIGPLAN Notices 21(11), </note> <month> November </month> <year> 1986. </year>
Reference-contexts: SELF-only (left) and SELF (right) C++ (compiled with GNU g++) SPECint89 median + 7 4 Hardware Support for Tagged Arithmetic Tagged integer addition and subtraction instructions are a unique SPARC feature that was motivated by the results of the SOAR project <ref> [SUH86] </ref>. The special instructions can perform an integer operation, a tag check (assuming a tag in the least significant two bits), and an overow check in parallel [U+84]. Using a non-optimizing Smalltalk compiler, Ungar reported that SOAR Smalltalk would have been 26% slower without the tagged instructions [Ung87].
Reference: [SP92] <author> SPARC International. </author> <title> The SPARC Architecture Manual (Version 8). </title> <publisher> Prentice Hall, </publisher> <address> NJ, </address> <year> 1992. </year> <month> 14 </month>
Reference-contexts: We also investigate if hardware-assisted message lookup could improve performance. Finally, we examine the cache behavior of the programs. 2 Methods The SELF-93 system compiles into native SPARC code, and thus all our data is for the SPARC V8 architecture <ref> [SP92] </ref>. The measurements were made with a combination of publicly available tools which were adapted for our purposes: the Shade tracing tool [CK93], the Dinero cache simulator [Hill87], and the SPA SPARC simulator [Irl91].
Reference: [SH87] <author> Peter Steenkiste and John Hennessy. </author> <title> Tags and type checking in LISP: Hardware and Software Approaches. </title> <booktitle> In ASPLOS II Conference Proceedings, </booktitle> <month> October </month> <year> 1987. </year>
Reference-contexts: Using a non-optimizing Smalltalk compiler, Ungar reported that SOAR Smalltalk would have been 26% slower without the tagged instructions [Ung87]. Similarly, Lisp machines have included hardware support for tagging <ref> [B+87, SH87] </ref>. But how useful is hardware support in a system with an optimizing compiler? To answer this question, it is necessary to examine the code generated for integer addition (or subtraction) in SELF-93.
Reference: [Sun90] <author> Sun Microsystems. </author> <title> The Viking Microprocessor (T.I. TMS S390Z50) User Documentation. Part No. </title> <address> 800-4510-02, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Furthermore, existing superscalar SPARC implementations cannot execute the tagged instructions in parallel with other instructions <ref> [Sun90] </ref> (probably because of the potential trap), and so one tagged instruction consumes as much time as several other instructions (up to three for SuperSPARC [Sun90]). <p> Furthermore, existing superscalar SPARC implementations cannot execute the tagged instructions in parallel with other instructions <ref> [Sun90] </ref> (probably because of the potential trap), and so one tagged instruction consumes as much time as several other instructions (up to three for SuperSPARC [Sun90]). We assumed that the compiler has no type information on the arguments of integer additions or subtractions, and thus has to explicitly test both tags for all integer operations.
Reference: [U+84] <author> David Ungar, Ricki Blau, Peter Foley, A. Dain Samples, and David Patterson. </author> <title> Architecture of SOAR: Smalltalk on a RISC. </title> <booktitle> 11th Annual Symposium on Computer Architecture, </booktitle> <address> Ann Arbor, Michigan, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: The special instructions can perform an integer operation, a tag check (assuming a tag in the least significant two bits), and an overow check in parallel <ref> [U+84] </ref>. Using a non-optimizing Smalltalk compiler, Ungar reported that SOAR Smalltalk would have been 26% slower without the tagged instructions [Ung87]. Similarly, Lisp machines have included hardware support for tagging [B+87, SH87].
Reference: [Ung87] <author> David Ungar. </author> <title> The Design and Evaluation of a High-Performance Smalltalk System. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: For example, the Xerox Dorado, for a long time the fastest Smalltalk implementation available, contained microcode support for large portions of the Smalltalk virtual machine [Deu83]. Ungar reported that the SOAR system would have been 26% slower without instructions for tagged arithmetic <ref> [Ung87] </ref>. Williams and Wolczko argued that software-controlled caching improves the performance and locality of object-oriented systems [WW90]. However, none of the systems previously studied employed compiler optimizations specifically aimed at reducing the overhead of message passing. Thus, the results of these studies may overstate the benefits of special hardware features. <p> Many previous studies have found the execution characteristics of object-oriented languages to be very different from C. For example, Smalltalk studies <ref> [Kra83, Ung87] </ref> have shown calls to be much more frequent than in other languages. Even for a hybrid language like C++ (which has C at its core and thus shouldnt behave too differently at runtime), significant differences were found. <p> The special instructions can perform an integer operation, a tag check (assuming a tag in the least significant two bits), and an overow check in parallel [U+84]. Using a non-optimizing Smalltalk compiler, Ungar reported that SOAR Smalltalk would have been 26% slower without the tagged instructions <ref> [Ung87] </ref>. Similarly, Lisp machines have included hardware support for tagging [B+87, SH87]. But how useful is hardware support in a system with an optimizing compiler? To answer this question, it is necessary to examine the code generated for integer addition (or subtraction) in SELF-93. <p> However, this class of programs is also more amenable to optimizations that eliminate type checks [CU91]. This result stands in marked contrast to Ungars measurements showing that SOAR would have been 26% slower without instructions for tagged arithmetic <ref> [Ung87] </ref>. Why the big difference? By analyzing Ungars data, we could find several reasons for the higher estimate: Ungars data includes speedups from several tagged instructions (such as load and load class) that are not needed in SELF.
Reference: [US87] <author> David Ungar and Randall B. Smith. </author> <title> SELF: The Power of Simplicity. </title> <booktitle> In OOPSLA 87 Conference Proceedings, p. </booktitle> <pages> 227-241, </pages> <address> Orlando, FL, </address> <month> October </month> <year> 1987. </year> <note> Published as SIGPLAN Notices 22(12), </note> <month> December </month> <year> 1987. </year> <title> Also published in Lisp and Symbolic Computation 4(3), </title> <publisher> Kluwer Academic Publishers, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: Thus, the results of these studies may overstate the benefits of special hardware features. For this study, we used the SELF-93 implementation [Hl94]. Two factors make it a good vehicle for measuring the execution characteristics of object-oriented programs: SELF <ref> [US87] </ref>, like Smalltalk-80 [GR83], is a very pure object-oriented language. All data are objects, and virtually every operation (e.g., instance variable access, integer addition, or control structures like if and while) involves message sends.
Reference: [WWT87] <author> Ifor Williams, Mario Wolczko, and Trevor Hopkins. </author> <title> Dynamic Grouping in an Object-Oriented Virtual Memory Hierarchy. </title> <booktitle> In ECOOP 87 Conference Proceedings, Special Issue of BIGRE, </booktitle> <pages> pp. 87-96, </pages> <address> Paris, France, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Since the performance impact of these misses is relatively high, software optimizations to reduce them may be worth investigating. Finally, even though cache-level data locality is good, page-level locality may be poor <ref> [WWT87] </ref> and thus needs to be studied. 9 Conclusions We have measured the execution behavior of a pure object-oriented language (SELF) whose implementation uses compiler optimizations specifically targeted at object-oriented languages.
Reference: [WW90] <author> Ifor Williams and Mario Wolczko. </author> <title> An Object-Based Memory Architecture. </title> <booktitle> In Proc. 4th Intl. Workshop on Persistent Object Systems, </booktitle> <address> Martha's Vineyard, MA, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: Ungar reported that the SOAR system would have been 26% slower without instructions for tagged arithmetic [Ung87]. Williams and Wolczko argued that software-controlled caching improves the performance and locality of object-oriented systems <ref> [WW90] </ref>. However, none of the systems previously studied employed compiler optimizations specifically aimed at reducing the overhead of message passing. Thus, the results of these studies may overstate the benefits of special hardware features. For this study, we used the SELF-93 implementation [Hl94]. <p> Similar results have also been reported by Reinhold for Scheme programs [Rei93], by Jouppi for the SPEC benchmark suite [Jou93], and by Koopman et al for combinator graph reduction [KLS92]. Such low data cache overheads leave little room for improvement through special cache features (e.g., <ref> [PS89, WW90] </ref>). (direct-mapped cache, 32-byte lines) (direct-mapped, 32-byte lines, write-allocate with subblock placement) 11 Diwan et al [DTM94] also observed that the data cache overhead of ML programs increased substantially with a write-noallocate policy, i.e., with a cache that does not allocate a cache line on a write miss.
Reference: [WLM92] <author> Paul R. Wilson, Michael S. Lam, and Thomas G. Moher. </author> <title> Caching Considerations for Generational Garbage Collection. </title> <booktitle> In Lisp and Functional Programming '92 Proceedings, p. </booktitle> <pages> 32-42, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year> <month> 15 </month>
Reference-contexts: Between two garbage collections, the allocation pointer sweeps through the entire allocation space (400 Kbytes in SELF). Since the fields of newly allocated objects are initialized before they are read, each initializing store will cause a cache miss in a system without write-allocate. Wilson et al. <ref> [WLM92] </ref> argue that this allocation behavior will result in especially high cache overheads for direct-mapped caches since the allocation pointer will sweep through the entire cache and evict every single cache line.
References-found: 32

