URL: http://www.iro.umontreal.ca/~feeley/papers/pslsw95.ps.gz
Refering-URL: http://www.iro.umontreal.ca/~feeley/
Root-URL: http://www.iro.umontreal.ca
Email: feeley@iro.umontreal.ca  
Title: Lazy Remote Procedure Call and its Implementation in a Parallel Variant of C  
Author: Marc Feeley 
Address: Montreal, Quebec, CANADA  
Affiliation: Dept. d'informatique et de recherche operationnelle Universite de Montreal  
Abstract: Lazy task creation (LTC) is an efficient approach for executing divide and conquer parallel programs that has been used in the implementation of Multilisp's future construct. Unfortunately it requires a specialized memory management scheme, in particular for stack frames, which makes it hard to use in the context of conventional languages. We have designed a variant of LTC which has a stack management discipline that is compatible with the semantics of conventional languages. This mechanism, which we call lazy remote procedure call, has been used to implement a parallel variant of C. A first prototype of our system has been ported to shared-memory multiprocessors and network of workstations. Experimental results on a Cray T3D multiprocessor show that good performance can be achieved on several symbolic programs. 
Abstract-found: 1
Intro-found: 1
Reference: [ Bilardi and Nicolau, 1989 ] <author> G. Bilardi and A. Nicolau. </author> <title> Adaptive bitonic sorting: An optimal parallel algorithm for shared-memory machines. </title> <journal> SIAM Journal of Computing, </journal> <volume> 12(2) </volume> <pages> 216-228, </pages> <month> April </month> <year> 1989. </year>
Reference: [ Bobrow and Wegbreit, 1973 ] <author> D. Bobrow and B. Wegbreit. </author> <title> A model and stack implementation of multiple environments. </title> <journal> Communications of the ACM, </journal> <volume> 16(10) </volume> <pages> 591-603, </pages> <year> 1973. </year>
Reference-contexts: Because processors don't steal tasks from other processors, load balancing is not automatic. Task distribution is directly dependent on the distribution of the data and the uniformity of the work to be performed on the data. The task scheduling policy adopted by Olden requires that a spaghetti stack <ref> [ Bobrow and Wegbreit, 1973 ] </ref> be used to manage the stack. This adds overhead to all function calls and is not compatible with the stack management of traditional compilers.
Reference: [ Callahan and Smith, 1989 ] <author> D. Callahan and B. Smith. </author> <title> A future-based parallel language for a general-purpose highly-parallel computer. </title> <booktitle> In Papers from the Second Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 95-113. </pages> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1989. </year>
Reference-contexts: is like fib but with the recursive calls exchanged), the spawning tree is lopsided on the side of the compound statement so join points rarely end up stealing calls. 7 Related work One of the earliest attempts to adapt the future construct to C was done at Tera Computer Company <ref> [ Callahan and Smith, 1989 ] </ref> (to our knowledge this system has never been implemented). The authors propose to extend C with a placeholder type qualifier, a task spawning construct, data sharing directives, and synchronization variables.
Reference: [ Feeley et al., 1994 ] <author> M. Feeley, M. Turcotte, and G. Lapalme. </author> <title> Using Multilisp for solving constraint satisfaction problems: an application to nucleic acid 3D structure determination. </title> <journal> Lisp and Symbolic Computation, </journal> 7(2/3):231-246, 1994. 
Reference-contexts: We have also used a 3000 line program (nucleic) that computes the 3D structure of a nucleic acid <ref> [ Feeley et al., 1994 ] </ref>[ Hartel et al., 1996 ] . All these programs are based on divide and conquer parallel algorithms. These programs are briefly described in Appendix A.
Reference: [ Feeley, 1993a ] <author> M. Feeley. </author> <title> An Efficient and General Implementation of Futures on Large Scale Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Brandeis University Department of Computer Science, </institution> <year> 1993. </year> <note> Available as publication #869 from departe-ment d'informatique et recherche operationnelle de l'Universite de Montreal. </note>
Reference-contexts: When implemented with lazy task creation (LTC), futures can execute parallel divide and conquer programs very efficiently on shared-memory computers as shown by the Mul-T [ Mohr, 1991 ] and Gambit <ref> [ Feeley, 1993a ] </ref> systems. LTC dynamically groups tasks together to adjust the effective task granularity of the program thus relieving the programmer from having to think about task granularity, load balancing, number of processors and processor speed. <p> This approach can be inefficient because the topmost continuation typically contains less work than older continuations and because subsequent blocking is likely. An alternative approach, used in Gambit <ref> [ Feeley, 1993a ] </ref> , is to immediately copy the stealable continuations to the RTQ in reverse order (so that older continuations are stolen first) and to invoke the oldest con-tinuation. <p> This mutual exclusion problem can be solved in a variety of ways including the use of polling to handle steal requests [ Feeley, 1993b ] , disabling interrupts during the critical sections, and serializing access to the LTS with hardware locks or software locks <ref> [ Feeley, 1993a ] </ref> . 4.3 Why defer the function call? When a parallel call is executed, lazy RPC defers the function call and immediately begins the execution of the compound statement. <p> As expected the overhead is inversely proportional to the parallel call granularity: it is between 10% and 15% for very fine grain programs (fib, fibr, sum and scan), and below 6% for larger grain programs. Similar overheads are reported for LTC in <ref> [ Feeley, 1993a ] </ref> for the Multilisp version of these benchmarks. 6.4 Speedup The speedup obtained for queens, fib and fibr is very close to linear. Because these benchmarks have imbalanced spawning trees it shows that lazy RPC is balancing the load very evenly.
Reference: [ Feeley, 1993b ] <author> M. Feeley. </author> <title> Polling efficiently on stock hardware. </title> <booktitle> In Proceedings of the 1993 ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <year> 1993. </year>
Reference-contexts: This mutual exclusion problem can be solved in a variety of ways including the use of polling to handle steal requests <ref> [ Feeley, 1993b ] </ref> , disabling interrupts during the critical sections, and serializing access to the LTS with hardware locks or software locks [ Feeley, 1993a ] . 4.3 Why defer the function call? When a parallel call is executed, lazy RPC defers the function call and immediately begins the execution
Reference: [ Halstead, 1985 ] <author> R. Halstead. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> In ACM Trans. on Prog. Languages and Systems, </journal> <pages> pages 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: 1 Introduction The future construct of Multilisp <ref> [ Halstead, 1985 ] </ref> has proven to be a convenient and effective means of expressing parallelism in Lisp and in symbolic processing applications.
Reference: [ Hartel et al., 1996 ] <author> P. H. Hartel, M. Feeley, M. Alt, L. Augustsson, P. Baumann, M. Beemster, E. Chailloux, C. H. Flood, W. Grieskamp, J. H. G. Van Groningen, K. Hammond, B. Hausman, M. Y. Ivory, R. E. Jones, P. Lee, X. Leroy, R. D. Lins, S. Loosemore, N. Rojemo, M. Serrano, J.-P. Talpin, J. Thackray, S. Thomas, P. Weis, and P. Wentworth. </author> <title> Benchmarking implementations of functional languages with "pseudoknot" a float-intensive benchmark. </title> <note> To appear in Journal of Functional Programming, </note> <year> 1996. </year>
Reference: [ Katz and Weise, 1990 ] <author> M. Katz and D. Weise. </author> <title> Continuing into the future: on the interaction of futures and first-class continuations. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <address> Nice, France, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: This has the advantage of reducing the likelihood of further blocking by giving more time to compute the placeholder's value before it is needed, and it allows the implementation of the Katz-Weise semantics for first-class continuations <ref> [ Katz and Weise, 1990 ] </ref> . 2.3 Incompatibility with C There are several reasons why this implementation of LTC can not be applied directly to C. First, placeholders are objects with indefinite extent.
Reference: [ Mohr, 1991 ] <author> E. Mohr. </author> <title> Dynamic Partitioning of Parallel Lisp Programs. </title> <type> PhD thesis, </type> <institution> Yale University Department of Computer Science, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: When implemented with lazy task creation (LTC), futures can execute parallel divide and conquer programs very efficiently on shared-memory computers as shown by the Mul-T <ref> [ Mohr, 1991 ] </ref> and Gambit [ Feeley, 1993a ] systems. LTC dynamically groups tasks together to adjust the effective task granularity of the program thus relieving the programmer from having to think about task granularity, load balancing, number of processors and processor speed. <p> After a processor has suspended its current task, it can proceed in two different ways. The "tail-biting" approach used in the Encore version of Mul-T <ref> [ Mohr, 1991 ] </ref> consists of invoking the topmost stealable continuation with the placeholder just created. This approach can be inefficient because the topmost continuation typically contains less work than older continuations and because subsequent blocking is likely. <p> The number of entries on the LTS (N LTS ) is at most d times s + 1 where d is the nesting depth of parallel calls and s is the maximum number of calls stolen from a processor. According to <ref> [ Mohr, 1991 ] </ref> , if p is the number of processors, s &lt; pd for LTC with "polite stealing" (which requires that a processor attempt to steal from a given processor only after having tried to steal from all other processors). <p> The overhead of a non-stolen call includes a thread suspension on a join variable and a subsequent thread resumption. A form of load-based inlining is available in Cid to reduce the number of non-stolen calls but this is not as efficient as creating tasks lazily <ref> [ Mohr, 1991 ] </ref> . Olden [ Rogers et al., 1995 ] is another system that has adapted LTC to C for programming distributed-memory machines. Instead of migrating data between processors when non-local data is accessed, Olden migrates the task to the processor containing the data.
Reference: [ Nikhil, 1994 ] <author> R. S. Nikhil. Cid: </author> <title> A parallel, shared-memory C for distributed-memory machines. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 376-390, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: The extensions to C are not as natural as ParSubC's because the user must indicate how a spawned task shares data with its parent task. Cid <ref> [ Nikhil, 1994 ] </ref> shares with ParSubC the desire for minimal extensions to C and compatibility with existing code and compilers. Cid is mainly intended for programming distributed-memory machines.
Reference: [ Numrich, 1994 ] <author> R. W. </author> <title> Numrich. The Cray T3D Address Space and How to Use It. </title> <institution> Cray Research Inc., </institution> <year> 1994. </year>
Reference-contexts: Each processor is a 150MHz DEC Alpha microprocessor with 64MB of local memory. The processors are interconnected by a 3D torus network. By configuring the virtual memory circuit attached to each processor (the "annex"), it is possible to implement a coherent shared address space for up to 16 processors <ref> [ Numrich, 1994 ] </ref> . When this is done, a cache miss to local memory costs about 30 cycles and a remote memory reference costs 3 to 4 times that depending on the distance of the remote processor.
Reference: [ Rogers et al., 1995 ] <author> A. Rogers, M. C. Carlisle, J. H. Reppy, and L. J. Hendren. </author> <title> Supporting dynamic data structures on distributed-memory machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(2) </volume> <pages> 233-263, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: A form of load-based inlining is available in Cid to reduce the number of non-stolen calls but this is not as efficient as creating tasks lazily [ Mohr, 1991 ] . Olden <ref> [ Rogers et al., 1995 ] </ref> is another system that has adapted LTC to C for programming distributed-memory machines. Instead of migrating data between processors when non-local data is accessed, Olden migrates the task to the processor containing the data.
Reference: [ Wagner and Calder, 1993 ] <author> D. B. Wagner and B. G. Calder. </author> <title> Leapfrogging: A portable technique for implementing efficient futures. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Finally, the & operator can not be used on local variables because stack frames move as a result of task migration (the compaction of the spaghetti stack needed to avoid stack overflows also moves stack frames). The leapfrogging technique <ref> [ Wagner and Calder, 1993 ] </ref> is closely related to lazy RPC. It was specifically designed to be compatible with conventional compiler technology and to require no syntactic extension to C++. A subclass of the class Future must be declared for each function to be called in parallel.
References-found: 14

