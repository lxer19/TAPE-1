URL: http://www.cs.indiana.edu/pub/chiuk/Graphics/EGRW94.ps.Z
Refering-URL: http://www.cs.indiana.edu/pub/chiuk/Graphics/
Root-URL: http://www.cs.indiana.edu
Title: Rendering, Complexity, and Perception  
Author: Kenneth Chiu and Peter Shirley 
Address: Bloomington IN 47405, USA  
Affiliation: Indiana University,  
Abstract-found: 0
Intro-found: 1
Reference: [AGW86] <author> Phil Amburn, Eric Grant, and Turner Whitted. </author> <title> Managing geometric complexity with enhanced procedural models. </title> <journal> Computer Graphics, </journal> <volume> 20(4) </volume> <pages> 189-195, </pages> <month> August </month> <year> 1986. </year> <booktitle> ACM Siggraph '86 Conference Proceedings. </booktitle>
Reference-contexts: Procedural models can also communicate with each other. Thus, the vegetation model underneath a tree model can understand that it is in shade rather than sunlight, and modify its growth accordingly <ref> [AGW86] </ref>. The question arises: when do we stop? If we are modelling a plant, why stop at the leaves? Why is it not necessary to model the microfacets of surfaces? The answer is that the depth to which it is necessary to model the actual geometry depends upon the application.
Reference: [BM93] <author> Barry G. Becker and Nelson L. Max. </author> <title> Smooth transitions between bump rendering algorithms. </title> <journal> Computer Graphics, </journal> <pages> pages 183-190, </pages> <month> August </month> <year> 1993. </year> <booktitle> ACM Siggraph '93 Conference Proceedings. </booktitle>
Reference-contexts: The former is a component of perceptual realism, and the latter is visceral realism. 3 Modeling Issues 3.1 Mapping Methods Texture-mapping, bump-mapping, and solid textures are several well-known methods for adding complexity to an image <ref> [BM93] </ref>. While computationally practical and very effective for some kinds of complexity, they suffer from some fundamental limitations. First, they do not actually extend the geometry.
Reference: [CHS + 93] <author> K. Chiu, M. Herf, P. Shirley, S. Swamy, C. Wang, and K. Zimmerman. </author> <title> Spatially nonuniform scaling functions for high contrast images. </title> <booktitle> In Graphics Interface '93, </booktitle> <pages> pages 245-244, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: These and other effects would need to be simulated accurately for applications such as safety engineering, architectural simulation, or set design. Correctly modeling them requires a full, quantitative model for human perception. Though this is not yet available, some useful incomplete models have been developed <ref> [MG88, NKON90, TR93, SW91, GFMS94, CHS + 93] </ref>. Visceral realism. A viscerally realistic image suspends our disbelief in the same way that a good movie convinces us that it is really happening.
Reference: [Coh92] <author> Michael F. Cohen. </author> <booktitle> Is image synthesis a solved problem? In Proceedings of the Third Eurographics Workshop on Rendering, </booktitle> <pages> pages 161-167, </pages> <year> 1992. </year>
Reference: [FS90] <author> Mark Friedell and Jean-Louis Schulmann. </author> <title> Constrained grammar-directed generation of landscapes. </title> <booktitle> In Graphics Interface '90, </booktitle> <pages> pages 244-251, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: By procedural model, we mean that not only are the primitives generated by an algorithm, but that in addition the primitives are generated only on demand, and not explicitly stored. Some examples of high complexity procedural models are <ref> [Pru93, PH89, FS90] </ref>. Procedural models have other important advantages besides storage cost. Through procedural models, users can produce highly detailed and complex scenes with relatively sparse input.
Reference: [FS93] <author> Thomas A. Funkhouser and Carlo H. Sequin. </author> <title> Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments. </title> <journal> Computer Graphics, </journal> <pages> pages 247-254, </pages> <month> August </month> <year> 1993. </year> <booktitle> ACM Siggraph '93 Conference Proceedings. </booktitle>
Reference-contexts: For convincing complexity then, actual geometry is often necessary. Unfortunately, this produces an enormous number of primitives. Just correctly modeling the carpet in a large room can involve millions of primitives. 3.2 Levels of Detail To reduce the number of primitives, multiple levels of detail are commonly used <ref> [FS93] </ref>. In this technique, an object has more than one representation. The representations have different degrees of detail. Depending on various factors such as the projected screen size of the object, the appropriate level of detail is selected.
Reference: [GFMS94] <author> Andrew Glassner, Kenneth P. Fishkin, David H. Marimont, and Mau-reen C. Stone. </author> <title> Device-directed rendering. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 0(0) </volume> <pages> 0-0, </pages> <month> November </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: These and other effects would need to be simulated accurately for applications such as safety engineering, architectural simulation, or set design. Correctly modeling them requires a full, quantitative model for human perception. Though this is not yet available, some useful incomplete models have been developed <ref> [MG88, NKON90, TR93, SW91, GFMS94, CHS + 93] </ref>. Visceral realism. A viscerally realistic image suspends our disbelief in the same way that a good movie convinces us that it is really happening.
Reference: [Gla93] <author> Andrew S. Glassner. </author> <title> Dynamic stratification. </title> <booktitle> In Proceedings of the Fourth Eurographics Workshop on Rendering, </booktitle> <pages> pages 5-14, </pages> <year> 1993. </year>
Reference-contexts: We are aware of only two methods that do not use shadow rays: Glassner's <ref> [Gla93] </ref>, and Kirk and Arvo's [KA91b]. But both of these force sampling of luminaires on the hemisphere. Virtual luminaires, 4 however, especially non-planar virtual luminaires, makes determining where the luminaires are in the first place computationally expensive.
Reference: [Gla94] <author> Andrew S. Glassner. </author> <title> Principles of Image Synthesis. </title> <address> Morgan-Kaufman, </address> <year> 1994. </year>
Reference-contexts: An initial set of samples through a pixel gathers information about what is seen through that pixel and determines whether more rays need to be sent. The difficulties of adaptive sampling are subtle [KA91a], but it can still work well in practice <ref> [Gla94] </ref>. To adaptively sample we need to know the accuracy desired at a given pixel. However, we assume that the acceptable error at a pixel depends not on the strict radiance of that pixel, but rather on the perceived brightness as produced by the visual system. <p> For input, we must support luminaire distribution data that is supplied by manufacturers. The most common format in use now is the IES file format for far-field photometry [oNA91]. This format is also reviewed in Glassner's book <ref> [Gla94] </ref>. Rendering, Complexity, and Perception 13 To approximate near-field photometry, we assume the power leaves the surface of the luminaire with equal radiance at each point. Traditionally, integer-valued output files such as ppm have been used for computer graphics. For output format, we store floating point values.
Reference: [HK93] <author> Pat Hanrahan and Wolfgang Krueger. </author> <title> Reflection from layered surfaces due to subsurface scattering. </title> <journal> Computer Graphics, </journal> <pages> pages 165-174, </pages> <month> August </month> <year> 1993. </year> <booktitle> ACM Siggraph '93 Conference Proceedings. </booktitle>
Reference-contexts: Second, does a good analytical model exist for the BRDF of the surface? For a complex surface such as skin, modeling the cellular structure might be necessary to capture the proper appearance <ref> [HK93] </ref>. But if a suitable analytical function exists, such as for brushed steel, then the microfacets need not be modeled. <p> Even Rendering, Complexity, and Perception 5 in those cases where such a function cannot be found, tabulated results from Monte-Carlo simulations can be substituted for the microstructure <ref> [HK93] </ref>. 4 Image Metamers The human visual process synthesizes many different signals into an internal mental image. Normally, the input to this process is the light coming from the various surfaces in the scene.
Reference: [Ins86] <institution> American National Standard Institute. Nomenclature and definitions for illumination engineering. </institution> <note> ANSI Report, 1986. ANSI/IES RP-16-1986. </note>
Reference-contexts: Instead of display dependent RGB files, we output tristimulous XYZ response in SI units (lm=(m 2 sr)). This unit is sometimes called the nit <ref> [Ins86] </ref>. You will also see luminances reported in Lamberts, which are =1000 nits each. There is also a concept of sco-topic (night/rod) luminance, which has the same units as photopic luminances.
Reference: [JC93] <author> Frederik W. Jansen and Alan Chalmers. </author> <booktitle> Realism in real time? In Proceedings of the Fourth Eurographics Workshop on Rendering, </booktitle> <pages> pages 27-46, </pages> <year> 1993. </year>
Reference-contexts: Zonal methods (radiosity) are common, but sometimes the integration domain is too complex for them to be practical. To handle arbitrary BRDFs, two-pass methods are also very numerous, and these are surveyed in <ref> [JC93] </ref>. Many of the methods have generated quite successful scenes (e.g., [KJ92]), but they still use shadow rays and will fail in highly complex scenes where virtual luminaires are important.
Reference: [KA91a] <author> David Kirk and James Arvo. </author> <title> Unbiased sampling techniques for image sysnthesis. </title> <journal> Computer Graphics, </journal> <volume> 25(4) </volume> <pages> 153-156, </pages> <month> July </month> <year> 1991. </year> <booktitle> ACM Siggraph '91 Conference Proceedings. </booktitle>
Reference-contexts: An initial set of samples through a pixel gathers information about what is seen through that pixel and determines whether more rays need to be sent. The difficulties of adaptive sampling are subtle <ref> [KA91a] </ref>, but it can still work well in practice [Gla94]. To adaptively sample we need to know the accuracy desired at a given pixel.
Reference: [KA91b] <author> David Kirk and James Arvo. </author> <title> Unbiased variance reduction for global illumination. </title> <booktitle> In Proceedings of the Second Eurographics Workshop on Rendering, </booktitle> <year> 1991. </year>
Reference-contexts: We are aware of only two methods that do not use shadow rays: Glassner's [Gla93], and Kirk and Arvo's <ref> [KA91b] </ref>. But both of these force sampling of luminaires on the hemisphere. Virtual luminaires, 4 however, especially non-planar virtual luminaires, makes determining where the luminaires are in the first place computationally expensive. We believe that in a highly complex environment, treating anything as a special case is only marginally beneficial.
Reference: [Kaj86] <author> James T. Kajiya. </author> <title> The rendering equation. </title> <journal> Computer Graphics, </journal> <volume> 20(4) </volume> <pages> 143-150, </pages> <month> August </month> <year> 1986. </year> <booktitle> ACM Siggraph '86 Conference Proceedings. </booktitle>
Reference-contexts: Many of the methods have generated quite successful scenes (e.g., [KJ92]), but they still use shadow rays and will fail in highly complex scenes where virtual luminaires are important. Interestingly, the only rendering algorithm that can compute the global illumination in an arbitrarily complex environment is Kajiya's path-tracing algorithm <ref> [Kaj86] </ref> and even his algorithm breaks down for scenes with many luminaires. Ward's Radiance program performs fairly well in the presence of dozens or hundreds of luminaires, but degenerates to Kajiya's algorithm for global illumination if the variation in surface normals is high, as is common in outdoor scenes.
Reference: [Kaj88] <author> James T. Kajiya. </author> <title> An overview and comparison of rendering methods. A Consumer's and Developer's Guide to Image Synthesis, </title> <address> pages 259-263, </address> <year> 1988. </year> <note> ACM Siggraph '88 Course 12 Notes. </note>
Reference-contexts: We propose that a raytracing-based scheme will be more flexible, easier to program, and ultimately faster. This is an extension of arguments made by Kajiya at SIGGRAPH '88 <ref> [Kaj88] </ref>. To expend resources where they will provide the most benefit, some kind of multi-stage adaptive sampling [vWvNJ91] is probably required. An initial set of samples through a pixel gathers information about what is seen through that pixel and determines whether more rays need to be sent.
Reference: [Kaj89] <author> James T. Kajiya. </author> <title> Rendering fur with three dimensional textures. </title> <journal> Computer Graphics, </journal> <volume> 23(3) </volume> <pages> 271-280, </pages> <month> July </month> <year> 1989. </year> <booktitle> ACM Siggraph '89 Conference Proceedings. Rendering, Complexity, and Perception 15 </booktitle>
Reference-contexts: Carpets and fur are two examples that are difficult to render accurately without self-shadowing. Lastly, they have difficulty handling dynamic objects. Replacing a field of wheat waving in the wind with a texture map would require that the map be accurately recomputed for every frame. Texels <ref> [Kaj89] </ref> are somewhat less limiting. They include self-shadowing, and thus can produce convincing images of textures like fur. Extension to geometry that is significantly non-isotropic would greatly increase memory usage, however, since the scalar density in each cell must be replaced with a higher tensor quantity.
Reference: [KJ92] <author> Arjan J. F. Kok and Frederik W. Jasen. </author> <title> Adaptive sampling of area light sources in ray tracing including diffuse interreflection. </title> <journal> Computer Graphics forum, </journal> <volume> 11(3) </volume> <pages> 289-298, </pages> <year> 1992. </year> <note> Eurographics '92. </note>
Reference-contexts: Zonal methods (radiosity) are common, but sometimes the integration domain is too complex for them to be practical. To handle arbitrary BRDFs, two-pass methods are also very numerous, and these are surveyed in [JC93]. Many of the methods have generated quite successful scenes (e.g., <ref> [KJ92] </ref>), but they still use shadow rays and will fail in highly complex scenes where virtual luminaires are important.
Reference: [Kok93] <author> Arjan F. Kok. </author> <title> Grouping of patches in progressive radiosity. </title> <booktitle> In Proceedings of the Fourth Eurographics Workshop on Rendering, </booktitle> <pages> pages 221-231, </pages> <year> 1993. </year>
Reference-contexts: Figures 4-5 are images created with this method. Our current research focuses on improving brute-force methods that maintain the principle of no shadow rays. The "smartness" of the method will undoubtably focus on the use of low-resolution environments <ref> [RPV93, Kok93] </ref>. We believe most future rendering methods will use such low-resolution environments for at least the indirect lighting calculation. 4 A virtual luminaire is a luminaire that does not generate its own light, but rather specularly reflects or refracts the light from other luminaires.
Reference: [MG88] <author> Gary W. Meyer and Donald P. Greenberg. </author> <title> Color-defective vision and computer graphics displays. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 8(9) </volume> <pages> 28-40, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: These and other effects would need to be simulated accurately for applications such as safety engineering, architectural simulation, or set design. Correctly modeling them requires a full, quantitative model for human perception. Though this is not yet available, some useful incomplete models have been developed <ref> [MG88, NKON90, TR93, SW91, GFMS94, CHS + 93] </ref>. Visceral realism. A viscerally realistic image suspends our disbelief in the same way that a good movie convinces us that it is really happening. <p> Meyer has similar results for color-blind virtual viewers <ref> [MG88] </ref>. A viewer of the resulting image is unable to distinguish different colors where a color-blind virtual viewer is also unable to distinguish different colors. Tumblin and Rushmeier have investigated the more difficult problem of imitating the effects of dark adaptation under low-light conditions [TR93].
Reference: [NKON90] <author> Eihachiro Nakamae, Kazufumi Kaneda, Takashi Okamoto, and Tomoyuki Nishita. </author> <title> A lighting model aiming at drive simulators. </title> <journal> Computer Graphics, </journal> <volume> 24(3) </volume> <pages> 395-404, </pages> <month> August </month> <year> 1990. </year> <booktitle> ACM Siggraph '90 Conference Proceedings. </booktitle>
Reference-contexts: These and other effects would need to be simulated accurately for applications such as safety engineering, architectural simulation, or set design. Correctly modeling them requires a full, quantitative model for human perception. Though this is not yet available, some useful incomplete models have been developed <ref> [MG88, NKON90, TR93, SW91, GFMS94, CHS + 93] </ref>. Visceral realism. A viscerally realistic image suspends our disbelief in the same way that a good movie convinces us that it is really happening. <p> Tumblin and Rushmeier have investigated the more difficult problem of imitating the effects of dark adaptation under low-light conditions [TR93]. Nakamae et al. used glare to simulate high-intensity luminaires on a low-intensity display <ref> [NKON90] </ref>. Ultimately, all of these effects must be used in an integrated framework. 5 Requirements of Future Renderers From the above discussion, we can summarize what capabilities a renderer must have to meet the realism requirements of current and future applications. Procedural models.
Reference: [oNA91] <author> Illumination Engineering Society of North America. </author> <title> Ies standard file format for electronic transfer of photometric data and related information. </title> <booktitle> IES Lighting Measurement Series, 1991. </booktitle> <address> IES LM-63-1991. </address>
Reference-contexts: For input, we must support luminaire distribution data that is supplied by manufacturers. The most common format in use now is the IES file format for far-field photometry <ref> [oNA91] </ref>. This format is also reviewed in Glassner's book [Gla94]. Rendering, Complexity, and Perception 13 To approximate near-field photometry, we assume the power leaves the surface of the luminaire with equal radiance at each point. Traditionally, integer-valued output files such as ppm have been used for computer graphics.
Reference: [PH89] <author> Ken Perlin and Eric M. Hoffert. </author> <title> Hypertexture. </title> <journal> Computer Graphics, </journal> <volume> 23(3) </volume> <pages> 253-262, </pages> <month> July </month> <year> 1989. </year> <booktitle> ACM Siggraph '89 Conference Proceedings. </booktitle>
Reference-contexts: By procedural model, we mean that not only are the primitives generated by an algorithm, but that in addition the primitives are generated only on demand, and not explicitly stored. Some examples of high complexity procedural models are <ref> [Pru93, PH89, FS90] </ref>. Procedural models have other important advantages besides storage cost. Through procedural models, users can produce highly detailed and complex scenes with relatively sparse input.
Reference: [Pru93] <author> Przemyslaw Prusinkiewicz. </author> <title> Modeling and visualization of biological structures. </title> <booktitle> In Graphics Interface '93, </booktitle> <pages> pages 128-137, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: By procedural model, we mean that not only are the primitives generated by an algorithm, but that in addition the primitives are generated only on demand, and not explicitly stored. Some examples of high complexity procedural models are <ref> [Pru93, PH89, FS90] </ref>. Procedural models have other important advantages besides storage cost. Through procedural models, users can produce highly detailed and complex scenes with relatively sparse input.
Reference: [Rea93] <author> Mark S. </author> <title> Rea, editor. </title> <booktitle> The Illumination Engineering Society Lighting Handbook. Illumination Engineering Society, </booktitle> <address> New York, NY, 8th edition, </address> <year> 1993. </year>
Reference-contexts: From 0.01 to 3 cd=m 2 we are in the mesopic region, where some color and detail is visible. Below 0.01 cd=m 2 we are in the scotopic region where cones are inactive so there is no color vision <ref> [Rea93] </ref>. Another question we should ask is whether the one byte mantissa and exponent give us enough dynamic range. The brightest commonly seen object is the Sun, which has an average luminance of is about 1600 fi 10 6 cd=m 2 viewed from sea level [Rea93]. <p> there is no color vision <ref> [Rea93] </ref>. Another question we should ask is whether the one byte mantissa and exponent give us enough dynamic range. The brightest commonly seen object is the Sun, which has an average luminance of is about 1600 fi 10 6 cd=m 2 viewed from sea level [Rea93]. This is well within the capabilities of the format. The biggest hole in standardization now is scene file format.
Reference: [RPV93] <author> Holly Rushmeier, Charles Patterson, and Aravindan Veerasamy. </author> <title> Geometric simplification for indirect illumination calculations. </title> <booktitle> In Graphics Interface '93, </booktitle> <pages> pages 227-236, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Figures 4-5 are images created with this method. Our current research focuses on improving brute-force methods that maintain the principle of no shadow rays. The "smartness" of the method will undoubtably focus on the use of low-resolution environments <ref> [RPV93, Kok93] </ref>. We believe most future rendering methods will use such low-resolution environments for at least the indirect lighting calculation. 4 A virtual luminaire is a luminaire that does not generate its own light, but rather specularly reflects or refracts the light from other luminaires.
Reference: [SW91] <author> Maureen C. Stone and William E. Wallace. </author> <title> Gamut mapping computer generated imagery. </title> <booktitle> In Graphics Interface '91, </booktitle> <pages> pages 32-39, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: These and other effects would need to be simulated accurately for applications such as safety engineering, architectural simulation, or set design. Correctly modeling them requires a full, quantitative model for human perception. Though this is not yet available, some useful incomplete models have been developed <ref> [MG88, NKON90, TR93, SW91, GFMS94, CHS + 93] </ref>. Visceral realism. A viscerally realistic image suspends our disbelief in the same way that a good movie convinces us that it is really happening.
Reference: [TR93] <author> Jack Tumblin and Holly Rushmeier. </author> <title> Tone reproduction for realistic computer generated images. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 13(7), </volume> <year> 1993. </year>
Reference-contexts: These and other effects would need to be simulated accurately for applications such as safety engineering, architectural simulation, or set design. Correctly modeling them requires a full, quantitative model for human perception. Though this is not yet available, some useful incomplete models have been developed <ref> [MG88, NKON90, TR93, SW91, GFMS94, CHS + 93] </ref>. Visceral realism. A viscerally realistic image suspends our disbelief in the same way that a good movie convinces us that it is really happening. <p> A viewer of the resulting image is unable to distinguish different colors where a color-blind virtual viewer is also unable to distinguish different colors. Tumblin and Rushmeier have investigated the more difficult problem of imitating the effects of dark adaptation under low-light conditions <ref> [TR93] </ref>. Nakamae et al. used glare to simulate high-intensity luminaires on a low-intensity display [NKON90].
Reference: [vWvNJ91] <author> Theo van Walsum, Peter R. van Nieuwenhuizen, and Frederik Jansen. </author> <title> Refinement criteria for adaptive stochastic ray tracing of textures. </title> <booktitle> In Euro-graphics '91, </booktitle> <pages> pages 155-166, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: We propose that a raytracing-based scheme will be more flexible, easier to program, and ultimately faster. This is an extension of arguments made by Kajiya at SIGGRAPH '88 [Kaj88]. To expend resources where they will provide the most benefit, some kind of multi-stage adaptive sampling <ref> [vWvNJ91] </ref> is probably required. An initial set of samples through a pixel gathers information about what is seen through that pixel and determines whether more rays need to be sent. The difficulties of adaptive sampling are subtle [KA91a], but it can still work well in practice [Gla94].
Reference: [War92] <author> Gregory J. Ward. </author> <title> Measuring and modeling anisotropic reflection. </title> <journal> Computer Graphics, </journal> <volume> 26(4) </volume> <pages> 265-272, </pages> <month> July </month> <year> 1992. </year> <booktitle> ACM Siggraph '92 Conference Proceedings. </booktitle>
Reference-contexts: For other applications, where perhaps visceral realism is more important, a larger degree of error is tolerable, but the accuracy must still be high enough to provide a convincing approximation to actual materials <ref> [War92, WAT92] </ref>. Rendering, Complexity, and Perception 7 A balanced perspective must be used when evaluating the last requirement. While calculations should have a solid grounding in physics, we must not produce a renderer that reduces the complexity of the scene to make possible the generation of accurate radiometric values.
Reference: [WAT92] <author> Stephen H. Westin, James R. Arvo, and Kenneth E. Torrance. </author> <title> Measuring and modeling anisotropic reflection. </title> <journal> Computer Graphics, </journal> <volume> 26(2) </volume> <pages> 255-264, </pages> <month> July </month> <year> 1992. </year> <title> ACM Siggraph '92 Conference Proceedings. This article was processed using the L a T E X macro package with LMAMULT style </title>
Reference-contexts: For other applications, where perhaps visceral realism is more important, a larger degree of error is tolerable, but the accuracy must still be high enough to provide a convincing approximation to actual materials <ref> [War92, WAT92] </ref>. Rendering, Complexity, and Perception 7 A balanced perspective must be used when evaluating the last requirement. While calculations should have a solid grounding in physics, we must not produce a renderer that reduces the complexity of the scene to make possible the generation of accurate radiometric values.
References-found: 31

