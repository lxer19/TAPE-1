URL: http://www.cc.gatech.edu/faculty/ashwin/papers/er-90-03.ps.Z
Refering-URL: http://www.cs.gatech.edu/faculty/ashwin/ABSTRACTS-summary.html
Root-URL: 
Title: Decision Models: A Theory of Volitional Explanation  
Author: Ashwin Ram 
Address: Atlanta, Georgia 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Note: Proceedings of the Twelvth Annual Conference of the Cognitive Science Society, Cambridge, MA,  
Email: E-mail: ashwin@cc.gatech.edu  
Phone: (404) 853-9372  
Date: August 1990.  
Abstract: This paper presents a theory of motivational analysis, the construction of volitional explanations to describe the planning behavior of agents. We discuss both the content of such explanations, as well as the process by which an understander builds the explanations. Explanations are constructed from decision models, which describe the planning process that an agent goes through when considering whether to perform an action. Decision models are represented as explanation patterns, which are standard patterns of causality based on previous experiences of the understander. We discuss the nature of explanation patterns, their use in representing decision models, and the process by which they are retrieved, used and evaluated.
Abstract-found: 1
Intro-found: 1
Reference: [Hammond, 1986] <author> K. J. Hammond. </author> <title> Case-Based Planning: An Integrated Theory of Planning, Learning and Memory. </title> <type> Ph.D. thesis, </type> <institution> Yale University, Department of Computer Science, </institution> <address> New Haven, CT, </address> <month> October </month> <year> 1986. </year> <note> Research Report #488. </note>
Reference-contexts: Such decisions represent the planning process that the agent underwent prior to the action. A complete model of this process requires a sophisticated vocabulary of goals, goal interactions, and plans, such as that of [Wilensky, 1983] or <ref> [Hammond, 1986] </ref>. There are three basic kinds of decisions: 1. Choice: The agent chooses to participate or not to participate in a given volitional role in some action. The explanation must describe why he made this choice. 2.
Reference: [Kass et al., 1986] <author> A. Kass, D. Leake, and C. Owens. SWALE: </author> <title> A Program That Explains, </title> <address> pages 232-254. </address> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1986. </year>
Reference-contexts: How specific is the XP? Is it abstract and very general (e.g., a proverb), or is it detailed and specific? Intuitively, a "good" explanation is not necessarily one that can be proven to be "true" (criterion 4 There is also the possibility of modifying the hypothesis to fit the situation <ref> [Schank, 1986; Kass et al., 1986] </ref>. 4), but also one that seems plausible (1 and 2), fits the situation well (2 and 5), and is relevant to the goals of the reasoner (criterion 3).
Reference: [Ram, 1984] <author> A. Ram. </author> <title> Modelling Characters and their Decisions: A Theory of Compliance Decisions. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, IL, </institution> <month> August </month> <year> 1984. </year> <note> Technical Report T-145. </note>
Reference-contexts: Prior to this, the volitional-agent undergoes a decision process in which he considers his goals, goal-orderings and expected-outcome, which then mentally-results in the volitional-role-relation being considered becoming true (in) or false (out) depending on the outcome of the decision. explainer must be able to model inter-agent interactions <ref> [Schank and Abelson, 1977; Wilensky, 1983; Ram, 1984] </ref>. 3. Coercion: The agent is forced to participate or not to participate in a given volitional role in an action. This case arises when an agent is physically coerced into participation or non participation.
Reference: [Ram, 1987] <author> A. Ram. </author> <title> AQUA: Asking Questions and Understanding Answers. </title> <booktitle> In Proceedings of the Sixth Annual National Conference on Artificial Intelligence, </booktitle> <pages> pages 312-316, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1987. </year> <booktitle> American Association for Artificial Intelligence, </booktitle> <publisher> Morgan Kaufman Publishers, Inc. </publisher>
Reference-contexts: We will illustrate our ideas with examples taken from this program. Further details may be found in <ref> [Ram, 1987; Schank and Ram, 1988; Ram, 1989] </ref>. 2 What is an explanation? The need for an explanation arises when some observed fact doesn't quite fit into the reasoner's world model, i.e., the reasoner detects an anomaly. An explanation is a knowledge structure that makes the anomaly go away.
Reference: [Ram, 1989] <author> A. Ram. </author> <title> Question-driven understanding: An integrated theory of story understanding, memory and learning. </title> <type> Ph.D. thesis, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <month> May </month> <year> 1989. </year> <note> Research Report #710. </note>
Reference-contexts: We will illustrate our ideas with examples taken from this program. Further details may be found in <ref> [Ram, 1987; Schank and Ram, 1988; Ram, 1989] </ref>. 2 What is an explanation? The need for an explanation arises when some observed fact doesn't quite fit into the reasoner's world model, i.e., the reasoner detects an anomaly. An explanation is a knowledge structure that makes the anomaly go away. <p> This allows XPs to be useful in novel situations, while retaining the advantages of pre-stored structures in stereotypical situations. The incremental elaboration of XPs in novel situations is discussed in <ref> [Ram, 1989; Ram, 1990b] </ref>. of new explanations. Explanatory cases in AQUA are based on the theory of explanation patterns described by [Schank, 1986], to which we add a theory of the representational structure and content of the XPs used in story understanding. <p> Volitional explanations thus correspond to the filling out of the "belief-goal-plan-action" chain [Schank and Abelson, 1977; Wilks, 1977; Wilensky, 1978; Schank, 1986], although we need to expand the vocabulary of this chain in order to model such explanations adequately <ref> [Ram, 1989] </ref>. <p> Internals: (1) A is religious and believes in the religion R (an emotional-state, perhaps caused by a social state, such as upbringing). 3 Typewriter font represents actual vocabulary items used by the AQUA program. Further details of the representation may be found in <ref> [Ram, 1989] </ref>. (2) A is strongly zealous about R (an emotional state). (3) A wants to spread his religion R (a goal, initi ated by (1) and (2)). (4) A places a high priority on his goal in (3), and is willing to sacrifice other goals which we would normally place <p> AQUA indexes motivational XPs in memory using typical contexts in which the XPs might be encountered (situation indices), as well as character stereotypes representing typical categories of people to whom the XPs might be applicable (stereotype indices) <ref> [Ram, 1989] </ref>. The third type of index is known as the anomaly index or category index. Recall that in addition to explaining the occurrence of the event, it is important for the XP to address the anomaly which arose from the failure of the reasoner to model the situation correctly.
Reference: [Ram, 1990a] <author> A. Ram. </author> <title> Goal-Based Explanation. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Automated Abduction, </booktitle> <address> Palo Alto, CA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: Similarly, if an error arose due to a mis-indexed knowledge structure, the explanation, when available, should be used to re-index the knowledge structure appropriately. The explanation is therefore constrained by the needs of the learning process <ref> [Ram, 1990a] </ref>. 6 Conclusion Abduction, or inference to the best explanation, is a central component of the reasoning process.
Reference: [Ram, 1990b] <author> A. Ram. </author> <title> Incremental Learning of Explanation Patterns and their Indices. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <address> Austin, TX, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: This allows XPs to be useful in novel situations, while retaining the advantages of pre-stored structures in stereotypical situations. The incremental elaboration of XPs in novel situations is discussed in <ref> [Ram, 1989; Ram, 1990b] </ref>. of new explanations. Explanatory cases in AQUA are based on the theory of explanation patterns described by [Schank, 1986], to which we add a theory of the representational structure and content of the XPs used in story understanding.
Reference: [Rieger, 1975] <author> C. Rieger. </author> <title> Conceptual Memory and Inference. </title> <editor> In R. C. Schank, editor, </editor> <booktitle> Conceptual Information Processing. </booktitle> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1975. </year>
Reference-contexts: Explanations can be divided into two broad categories, physical and volitional. 3.1 Physical explanations Physical explanations link events with the states that result from them, and further events that they enable, using causal chains similar to those of <ref> [Rieger, 1975] </ref> and [Schank and Abelson, 1977]. Physical explanations answer questions about the physical causality of the domain.
Reference: [Schank and Abelson, 1977] <author> R. C. Schank and R. Abel-son. </author> <title> Scripts, Plans, Goals and Understanding: An Inquiry into Human Knowledge Structures. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1977. </year>
Reference-contexts: Explanations can be divided into two broad categories, physical and volitional. 3.1 Physical explanations Physical explanations link events with the states that result from them, and further events that they enable, using causal chains similar to those of [Rieger, 1975] and <ref> [Schank and Abelson, 1977] </ref>. Physical explanations answer questions about the physical causality of the domain. <p> Volitional explanations thus correspond to the filling out of the "belief-goal-plan-action" chain <ref> [Schank and Abelson, 1977; Wilks, 1977; Wilensky, 1978; Schank, 1986] </ref>, although we need to expand the vocabulary of this chain in order to model such explanations adequately [Ram, 1989]. <p> Prior to this, the volitional-agent undergoes a decision process in which he considers his goals, goal-orderings and expected-outcome, which then mentally-results in the volitional-role-relation being considered becoming true (in) or false (out) depending on the outcome of the decision. explainer must be able to model inter-agent interactions <ref> [Schank and Abelson, 1977; Wilensky, 1983; Ram, 1984] </ref>. 3. Coercion: The agent is forced to participate or not to participate in a given volitional role in an action. This case arises when an agent is physically coerced into participation or non participation.
Reference: [Schank and Ram, 1988] <author> R. C. Schank and A. Ram. </author> <title> Question-driven Parsing: A New Approach to Natural Language Understanding. </title> <journal> Journal of Japanese Society for Artificial Intelligence, </journal> <volume> 3(3) </volume> <pages> 260-270, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: We will illustrate our ideas with examples taken from this program. Further details may be found in <ref> [Ram, 1987; Schank and Ram, 1988; Ram, 1989] </ref>. 2 What is an explanation? The need for an explanation arises when some observed fact doesn't quite fit into the reasoner's world model, i.e., the reasoner detects an anomaly. An explanation is a knowledge structure that makes the anomaly go away.
Reference: [Schank, 1986] <author> R. C. Schank. </author> <title> Explanation Patterns: Understanding Mechanically and Creatively. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1986. </year>
Reference-contexts: This paper presents a theory of explanation based on the claim that new explanations are built, not by chaining inference rules together, but rather by reusing explanations that have been encountered in previous situations and are already known to the system <ref> [Schank, 1986] </ref>. <p> However, this is too inefficient in complicated situations, where the causal chains could be several steps long. To get around this problem, AQUA uses pre-stored explanations for stereotypical situations. These explanations represent standard patterns that are observed in these situations, and hence are called explanation patterns <ref> [Schank, 1986] </ref>. An explanation pattern (XP) is a stock explanation for a stereotypical situation. For example, religious fanatic does terrorist act is a standard XP many people have about the Middle East terrorism problem. <p> The incremental elaboration of XPs in novel situations is discussed in [Ram, 1989; Ram, 1990b]. of new explanations. Explanatory cases in AQUA are based on the theory of explanation patterns described by <ref> [Schank, 1986] </ref>, to which we add a theory of the representational structure and content of the XPs used in story understanding. <p> Volitional explanations thus correspond to the filling out of the "belief-goal-plan-action" chain <ref> [Schank and Abelson, 1977; Wilks, 1977; Wilensky, 1978; Schank, 1986] </ref>, although we need to expand the vocabulary of this chain in order to model such explanations adequately [Ram, 1989]. <p> How specific is the XP? Is it abstract and very general (e.g., a proverb), or is it detailed and specific? Intuitively, a "good" explanation is not necessarily one that can be proven to be "true" (criterion 4 There is also the possibility of modifying the hypothesis to fit the situation <ref> [Schank, 1986; Kass et al., 1986] </ref>. 4), but also one that seems plausible (1 and 2), fits the situation well (2 and 5), and is relevant to the goals of the reasoner (criterion 3).
Reference: [Wilensky, 1978] <author> R. Wilensky. </author> <title> Understanding Goal-Based Stories. </title> <type> Ph.D. thesis, </type> <institution> Yale University, Department of Computer Science, </institution> <address> New Haven, CT, </address> <year> 1978. </year>
Reference-contexts: However, there are two problems with this view. The first problem is the familiar one of combinatorial explosion of inferences. Most explanation programs create explanations by chaining together inference rules that describe the causality of the domain. For example, PAM <ref> [Wilensky, 1978] </ref> used a set of planning rules connecting together typical goals and plans of people, and chained them together to form motivational explanations for actions observed in a story. However, this process is very inefficient in complex domains, where the causal chains may be several steps long. <p> How might a reasoner construct such an explanation? PAM <ref> [Wilensky, 1978] </ref> used a set of planning rules connecting typical goals and plans of people, and chained them together to form explanations such as the above. However, this is too inefficient in complicated situations, where the causal chains could be several steps long. <p> Volitional explanations thus correspond to the filling out of the "belief-goal-plan-action" chain <ref> [Schank and Abelson, 1977; Wilks, 1977; Wilensky, 1978; Schank, 1986] </ref>, although we need to expand the vocabulary of this chain in order to model such explanations adequately [Ram, 1989].
Reference: [Wilensky, 1983] <author> R. Wilensky. </author> <title> Planning and Understanding. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: Such decisions represent the planning process that the agent underwent prior to the action. A complete model of this process requires a sophisticated vocabulary of goals, goal interactions, and plans, such as that of <ref> [Wilensky, 1983] </ref> or [Hammond, 1986]. There are three basic kinds of decisions: 1. Choice: The agent chooses to participate or not to participate in a given volitional role in some action. The explanation must describe why he made this choice. 2. <p> Prior to this, the volitional-agent undergoes a decision process in which he considers his goals, goal-orderings and expected-outcome, which then mentally-results in the volitional-role-relation being considered becoming true (in) or false (out) depending on the outcome of the decision. explainer must be able to model inter-agent interactions <ref> [Schank and Abelson, 1977; Wilensky, 1983; Ram, 1984] </ref>. 3. Coercion: The agent is forced to participate or not to participate in a given volitional role in an action. This case arises when an agent is physically coerced into participation or non participation.
Reference: [Wilks, 1977] <author> Y. Wilks. </author> <title> What Sort of Taxonomy of Causation Do We Need for Language Understanding. </title> <booktitle> Cognitive Science, </booktitle> <address> 1:235, </address> <year> 1977. </year> <month> 205 </month>
Reference-contexts: Volitional explanations thus correspond to the filling out of the "belief-goal-plan-action" chain <ref> [Schank and Abelson, 1977; Wilks, 1977; Wilensky, 1978; Schank, 1986] </ref>, although we need to expand the vocabulary of this chain in order to model such explanations adequately [Ram, 1989].
References-found: 14

