URL: http://www.cs.utexas.edu/users/pclark/papers/noise.ps
Refering-URL: http://www.cs.utexas.edu/users/pclark/papers/noise.abs.html
Root-URL: 
Title: In: Machine Learning, Meta-reasoning and Logics, pp207-232,  Learning from Imperfect Data  
Author: Eds: P. B. Brazdil and K. Konolige, Pavel Brazdil Peter Clark 
Date: 1990  
Web: http://www.cs.utexas.edu/users/pclark/papers/noise.ps  
Note: (1990), Boston: Kluwer.  
Abstract: Systems interacting with real-world data must address the issues raised by the possible presence of errors in the observations it makes. In this paper we first present a framework for discussing imperfect data and the resulting problems it may cause. We distinguish between two categories of errors in data random errors or `noise', and systematic errors and examine their relationship to the task of describing observations in a way which is also useful for helping in future problem-solving and learning tasks. Secondly we proceed to examine some of the techniques currently used in AI research for recognising such errors.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Michel M. Manago and Yves Kodratoff. </author> <title> Noise and knowledge acquisition. </title> <editor> In J. McDermott, editor, </editor> <booktitle> IJCAI-87, </booktitle> <pages> pages 348-354, </pages> <publisher> Kaufmann, </publisher> <address> Ca, </address> <year> 1987. </year>
Reference-contexts: Such random fluctuations may be inherent in the world being observed (eg. due to gusts of wind in a robot laboratory, imprecision in a robot arm) or in transmission of observations to the learning system (eg. imperfect measuring equipment, transcription errors). 2 Manago and Kodratoff <ref> [1] </ref> present a more detailed analysis of the sources of noise in data, in particular for the case where the `measuring device' used to relay information about the concept to the system is a human.
Reference: [2] <author> T. M. Mitchell, R. M. Keller, and S. T. Kedar-Cabelli. </author> <title> Explanation-based generalization: a unifying view. </title> <journal> Machine Learning Journal, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: In the latter case, if there is insufficient power to apply the theory the theory is said to be intractable <ref> [2] </ref> [3]. 3 Problems Created by Imperfect Data Although we have indicated different sources of error in a system's theory, both internal and external, it should be noted that there is a degree of overlap between these categories.
Reference: [3] <author> P. V. Tadepalli. </author> <title> Learning in intractable domains. </title> <editor> In T. M. Mitchell, J. G. Carbonell, and R. S. Michalski, editors, </editor> <title> Machine Learning: A Guide to Current Research, </title> <publisher> Kluwer, </publisher> <address> Lancaster, UK, </address> <year> 1986. </year>
Reference-contexts: In the latter case, if there is insufficient power to apply the theory the theory is said to be intractable [2] <ref> [3] </ref>. 3 Problems Created by Imperfect Data Although we have indicated different sources of error in a system's theory, both internal and external, it should be noted that there is a degree of overlap between these categories.
Reference: [4] <author> T. M. Mitchell, P. Utgoff, and R. Banerji. </author> <title> Learning by experimentation: acquiring and refining problem-solving heuristics. </title> <editor> In J. G. Carbonell, R. S. Michalski, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> vol. 1, </volume> <publisher> Tioga, </publisher> <address> Palo Alto, Ca, </address> <year> 1983. </year>
Reference-contexts: We shall now proceed to look at what these problems are and how they problems can be overcome. 3.1 The Perfect Data Assumption The objective of many learning systems is to construct an internal model of the world which is completely consistent with observations (eg. Lex <ref> [4] </ref>, ID3 [5]). Should a new observation be made which is inconsistent with the world, this acts as a cue for the system to alter its model. We shall call such systems perfect modelling systems obeying a consistency requirement. On first sight this requirement seems reasonable.
Reference: [5] <author> J. Ross Quinlan. </author> <title> Learning efficient classification procedures and their application to chess endgames. </title> <editor> In J. G. Carbonell, R. S. Michalski, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> vol. 1, </volume> <publisher> Tioga, </publisher> <address> Palo Alto, Ca, </address> <year> 1983. </year>
Reference-contexts: We shall now proceed to look at what these problems are and how they problems can be overcome. 3.1 The Perfect Data Assumption The objective of many learning systems is to construct an internal model of the world which is completely consistent with observations (eg. Lex [4], ID3 <ref> [5] </ref>). Should a new observation be made which is inconsistent with the world, this acts as a cue for the system to alter its model. We shall call such systems perfect modelling systems obeying a consistency requirement. On first sight this requirement seems reasonable.
Reference: [6] <author> R. S. Michalski and J. Larson. </author> <title> Selection of most representative training examples and incremental generation of VL 1 hypotheses: the underlying methodology and the description of programs ESEL and AQ11. </title> <type> Technical Report UIUCDCS-R-78-867, </type> <institution> The University of of Illinois at Urbana-Champaign, Department of Computer Science, Urbana, </institution> <year> 1978. </year>
Reference-contexts: Examples of this are : * Data filtering Data is pre-filtered to extract only the most representative examples for feeding to the learning algorithm. For example the ESEL subsystem for the AQ11 induction program filters training data in this way <ref> [6] </ref>. * Using "coarse-grained" attributes and classes In many cases, it is not necessary to predict the world perfectly, but only to a certain accuracy. By using a larger grain size for attributes and classes, effects 5 of noise can be hidden from the learning system.
Reference: [7] <author> Ivan Bratko, Igor Mozetic, and Nada Lavrac. </author> <title> Automatic synthesis and compression of car-diological knowledge. </title> <booktitle> In Machine Intelligence 11, </booktitle> <publisher> Ellis Horwood, </publisher> <address> Chichester, UK, </address> <year> 1986. </year> <note> (to be published). </note>
Reference-contexts: Then a learning algorithm making the `noiseless domain' assumption can be effectively applied. Many qualitative modelling systems can hide noisy components in this way (another example of this is Kardio <ref> [7] </ref> where coarse grained attributes such as `heart rate' hide part of the noise in data).
Reference: [8] <author> L. Brieman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: This process of threshold selection can be automated by the use of cross-validation techniques such as described by Brieman et al. <ref> [8] </ref> and Watkins [9]. Typically, systems assess reliability using statistically-based measures of generality and accuracy of model components. This can be viewed as the application of a weak kind of meta-knowledge to assess the quality of the knowledge itself.
Reference: [9] <author> C. J. C. H. Watkins. </author> <title> Combining cross-validation and search. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning (proceedings of the 2nd European Working Session on Learning), </booktitle> <pages> pages 79-87, </pages> <address> Sigma, Wilmslow, UK, </address> <year> 1987. </year>
Reference-contexts: This process of threshold selection can be automated by the use of cross-validation techniques such as described by Brieman et al. [8] and Watkins <ref> [9] </ref>. Typically, systems assess reliability using statistically-based measures of generality and accuracy of model components. This can be viewed as the application of a weak kind of meta-knowledge to assess the quality of the knowledge itself.
Reference: [10] <author> M. Pazzani, M. Dyer, and M. Flowers. </author> <title> The role of prior causal theories in generalization. </title> <booktitle> In AAAI-86, </booktitle> <year> 1986. </year>
Reference-contexts: This can be viewed as the application of a weak kind of meta-knowledge to assess the quality of the knowledge itself. The extension of this is to employ more sophisticated meta-knowledge to assess reliability; for example Occam <ref> [10] </ref> uses additional meta-knowledge about which other hypotheses a particular hypothesis supports, whether there exist alternative competing hypotheses to the currently believed on etc. to assess confidence in a model component. * Pruned general-to-specific 1 search One of the most common techniques for filtering out noisy components of data is to
Reference: [11] <author> Ross King. </author> <title> An inductive learning approach to the problem of predicting a protein's secondary structure from its amino acid sequence. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning (proceedings of the 2nd European Working Session on Learning), </booktitle> <address> Sigma, Wilmslow, UK, </address> <year> 1987. </year>
Reference-contexts: Several rule induction systems do this, the measure of unreliability being computed from the number and distribution of examples covered by the hypothesis in various ways eg. using percentage tests such as by King <ref> [11] </ref>, Sequoia [12] and Plage [13] 1 The word `general' is applied of course to components of the model (eg. a production rule, a decision tree branch) rather than to the model as a whole the latter's generality (usually covering the entire space of examples) typically remains unchanged during search if
Reference: [12] <author> J. Haiech, J. Quinqueton, and J. Sallantin. </author> <title> Concept formation from sequential data. </title> <booktitle> In Proceedings of the 1st European Working Session on Learning (EWSL-86), </booktitle> <institution> Universite de Paris-Sud, Orsay, France, </institution> <year> 1986. </year> <month> 9 </month>
Reference-contexts: Several rule induction systems do this, the measure of unreliability being computed from the number and distribution of examples covered by the hypothesis in various ways eg. using percentage tests such as by King [11], Sequoia <ref> [12] </ref> and Plage [13] 1 The word `general' is applied of course to components of the model (eg. a production rule, a decision tree branch) rather than to the model as a whole the latter's generality (usually covering the entire space of examples) typically remains unchanged during search if generality is
Reference: [13] <author> O. Gascuel. Plage: </author> <title> a way to give and use knowledge in learning. </title> <booktitle> In Proceedings of the 1st European Working Session on Learning (EWSL-86), </booktitle> <institution> Universite de Paris-Sud, Orsay, France, </institution> <year> 1986. </year>
Reference-contexts: Several rule induction systems do this, the measure of unreliability being computed from the number and distribution of examples covered by the hypothesis in various ways eg. using percentage tests such as by King [11], Sequoia [12] and Plage <ref> [13] </ref> 1 The word `general' is applied of course to components of the model (eg. a production rule, a decision tree branch) rather than to the model as a whole the latter's generality (usually covering the entire space of examples) typically remains unchanged during search if generality is defined as the
Reference: [14] <author> Peter Clark and Tim Niblett. </author> <title> Induction in noisy domains. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning (proceedings of the 2nd European Working Session on Learning), </booktitle> <address> Sigma, Wilmslow, UK, </address> <year> 1987. </year>
Reference-contexts: rule, a decision tree branch) rather than to the model as a whole the latter's generality (usually covering the entire space of examples) typically remains unchanged during search if generality is defined as the proportion of example space covered. 6 or using chi squared tests such as in CN2 2 <ref> [14] </ref>.
Reference: [15] <author> Bojan Cestnik, Igor Kononenko, and Ivan Bratko. </author> <title> Assistant 86: a knowledge-elicitation tool for sophisticated users. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning (proceedings of the 2nd European Working Session on Learning), </booktitle> <pages> pages 31-45, </pages> <address> Sigma, Wilmslow, UK, </address> <year> 1987. </year>
Reference-contexts: The ID3 algorithm, performing a general-to-specific search already, can be similarly modified to cope with noise by halting branch growth earlier, as illustrated for example by Assistant-86 <ref> [15] </ref>. * Rule truncation, or post-pruning An alternative to prematurely halting a general-to-specific search, as above, is to allow it to run to completion (ie. form a model completely consistent 3 with observation) then to `post-prune' the final model to remove components deemed unreliable. <p> Again in rule induction systems this technique is common, for example in the most recent ID3 descendants C4 [16], Assistant-86 <ref> [15] </ref> and by Niblett and Bratko [17]. Niblett [18] gives a review of pre- and post-pruning techniques used for decision trees. AQ15 [19] employs a post-pruning technique for production rules (termed `rule truncation').
Reference: [16] <author> J. R. Quinlan, P. J. Compton, K. A. Horn, and L. Lazarus. </author> <title> Inductive knowledge acquisition: a case study. </title> <booktitle> In Proceedings of the second Australian Conference on the Applications of Expert Systems, </booktitle> <pages> pages 183-204, </pages> <institution> New South Wales Institute of Technology, </institution> <address> Sydney, </address> <year> 1986. </year>
Reference-contexts: The advantage of this is that the quality of pruned and unpruned versions of a hypothesis can be directly compared rather than having to estimate the latter's quality during search. Again in rule induction systems this technique is common, for example in the most recent ID3 descendants C4 <ref> [16] </ref>, Assistant-86 [15] and by Niblett and Bratko [17]. Niblett [18] gives a review of pre- and post-pruning techniques used for decision trees. AQ15 [19] employs a post-pruning technique for production rules (termed `rule truncation').
Reference: [17] <author> Tim Niblett and Ivan Bratko. </author> <title> Learning decision rules in noisy domains. </title> <booktitle> In Expert Systems 86, </booktitle> <address> Brighton, UK, </address> <year> 1986. </year>
Reference-contexts: Again in rule induction systems this technique is common, for example in the most recent ID3 descendants C4 [16], Assistant-86 [15] and by Niblett and Bratko <ref> [17] </ref>. Niblett [18] gives a review of pre- and post-pruning techniques used for decision trees. AQ15 [19] employs a post-pruning technique for production rules (termed `rule truncation').
Reference: [18] <author> Tim Niblett. </author> <title> Constructing decision trees in noisy domains. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning (proceedings of the 2nd European Working Session on Learning), </booktitle> <pages> pages 67-78, </pages> <address> Sigma, Wilmslow, UK, </address> <year> 1987. </year>
Reference-contexts: Again in rule induction systems this technique is common, for example in the most recent ID3 descendants C4 [16], Assistant-86 [15] and by Niblett and Bratko [17]. Niblett <ref> [18] </ref> gives a review of pre- and post-pruning techniques used for decision trees. AQ15 [19] employs a post-pruning technique for production rules (termed `rule truncation').
Reference: [19] <author> R. S. Michalski, I. Mozetic, J. Hong, and N. Lavrac. </author> <title> The AQ15 inductive learning system : an overview and experiments. </title> <booktitle> In Proceedings of IMAL 1986, </booktitle> <institution> Universite de Paris-Sud, Orsay, France, </institution> <year> 1986. </year>
Reference-contexts: Again in rule induction systems this technique is common, for example in the most recent ID3 descendants C4 [16], Assistant-86 [15] and by Niblett and Bratko [17]. Niblett [18] gives a review of pre- and post-pruning techniques used for decision trees. AQ15 <ref> [19] </ref> employs a post-pruning technique for production rules (termed `rule truncation'). <p> This avoids heavy reliance on a specific, possibly unreliable, part of the system, allowing noisy effects to be over-ridden and smoothed out by other model components. Statistical methods such as Bayesian techniques (eg. [22]) can be employed. AQ15 <ref> [19] </ref> performs weighted rule application in this way, and Quinlan [23] suggests how decision tree application can be made less brittle by introducing a degree of corroboration between decision tree branches. * The "exception-based" paradigm For any system learning incrementally, noise presents the problem of when to ignore observations conflicting theory
Reference: [20] <author> R. A. Corlett. </author> <title> Explaining induced decision trees. </title> <booktitle> In Expert Systems 83, </booktitle> <pages> pages 136-142, </pages> <year> 1983. </year>
Reference-contexts: Niblett [18] gives a review of pre- and post-pruning techniques used for decision trees. AQ15 [19] employs a post-pruning technique for production rules (termed `rule truncation'). The use of post-pruning of decision tree branches to generate production rules has been used by Corlett <ref> [20] </ref> and Quinlan [21]. * Corroborative application of model components Another technique to prevent unreliable model components degrading performance is to allow all components to contribute in some way during problem-solving, with different weights attached to their decisions.
Reference: [21] <author> J. Ross Quinlan. </author> <title> Generating production rules from decision trees. </title> <editor> In J. McDermott, editor, </editor> <booktitle> IJCAI-87, </booktitle> <pages> pages 304-307, </pages> <publisher> Kaufmann, </publisher> <address> Ca, </address> <year> 1987. </year>
Reference-contexts: Niblett [18] gives a review of pre- and post-pruning techniques used for decision trees. AQ15 [19] employs a post-pruning technique for production rules (termed `rule truncation'). The use of post-pruning of decision tree branches to generate production rules has been used by Corlett [20] and Quinlan <ref> [21] </ref>. * Corroborative application of model components Another technique to prevent unreliable model components degrading performance is to allow all components to contribute in some way during problem-solving, with different weights attached to their decisions.
Reference: [22] <author> D. Michie. </author> <title> Bayes, turing and the logic of corroboration. In Machine intelligence and related topics: an information scientist's weekend book, </title> <publisher> Gordon & Breach, </publisher> <address> NY, </address> <year> 1982. </year>
Reference-contexts: This avoids heavy reliance on a specific, possibly unreliable, part of the system, allowing noisy effects to be over-ridden and smoothed out by other model components. Statistical methods such as Bayesian techniques (eg. <ref> [22] </ref>) can be employed.
Reference: [23] <author> J. R. Quinlan. </author> <title> Decision trees as probabilistic classifiers. </title> <editor> In P. Langley, editor, </editor> <booktitle> Proc. 4th International Workshop on Machine Learning, </booktitle> <publisher> Kaufmann, </publisher> <address> Ca, </address> <year> 1987. </year>
Reference-contexts: This avoids heavy reliance on a specific, possibly unreliable, part of the system, allowing noisy effects to be over-ridden and smoothed out by other model components. Statistical methods such as Bayesian techniques (eg. [22]) can be employed. AQ15 [19] performs weighted rule application in this way, and Quinlan <ref> [23] </ref> suggests how decision tree application can be made less brittle by introducing a degree of corroboration between decision tree branches. * The "exception-based" paradigm For any system learning incrementally, noise presents the problem of when to ignore observations conflicting theory and when to modify the theory.
Reference: [24] <author> M. Lebowitz. </author> <title> Concept learning in a rich input domain : generalization-based memory. </title> <editor> In J. G. Carbonell, R. S. Michalski, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> vol. 2, </volume> <publisher> Tioga, </publisher> <address> Palo Alto, Ca, </address> <year> 1986. </year>
Reference-contexts: This threshold is typically a certain number of failures (eg. 2), and has the effect requiring a certain weight of evidence before change can be justified. Examples of systems using this approach are Unimem <ref> [24] </ref>, Alfred [25] and by Emde [26]. 4.2 Techniques for Dealing with Systematic Errors The task of dealing with systematic errors is related to that of dealing with random errors, in that it concerns the problem of finding an adequate model of observations.
Reference: [25] <author> C. Reisbeck. </author> <title> Failure-driven reminding for incremental learning. </title> <booktitle> In IJCAI-81, </booktitle> <year> 1981. </year>
Reference-contexts: This threshold is typically a certain number of failures (eg. 2), and has the effect requiring a certain weight of evidence before change can be justified. Examples of systems using this approach are Unimem [24], Alfred <ref> [25] </ref> and by Emde [26]. 4.2 Techniques for Dealing with Systematic Errors The task of dealing with systematic errors is related to that of dealing with random errors, in that it concerns the problem of finding an adequate model of observations.
Reference: [26] <author> W. Emde. </author> <title> Big flood in the blocks world; or non-cumulative learning. </title> <booktitle> In ECAI-86, </booktitle> <pages> pages 569-575, </pages> <year> 1986. </year>
Reference-contexts: This threshold is typically a certain number of failures (eg. 2), and has the effect requiring a certain weight of evidence before change can be justified. Examples of systems using this approach are Unimem [24], Alfred [25] and by Emde <ref> [26] </ref>. 4.2 Techniques for Dealing with Systematic Errors The task of dealing with systematic errors is related to that of dealing with random errors, in that it concerns the problem of finding an adequate model of observations.
Reference: [27] <author> Pavel Brazdil. </author> <title> Knowledge states and meta-knowledge maintenance. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning (proceedings of the 2nd European Working Session on Learning), </booktitle> <address> Sigma, Wilmslow, UK, </address> <year> 1987. </year>
Reference-contexts: Such knowledge sources may vary in complexity, from a simple measuring tool to a complex system such as another active agent in the world (see <ref> [27] </ref> for a discussion of the latter). The detection of systematic errors is partly dependent on a system's ability to model the source from which data is obtained.
Reference: [28] <author> Richard Burton. </author> <title> Diagnosing bugs in a simple procedural skill. </title> <editor> In D. Sleeman and J. S. Brown, editors, </editor> <booktitle> Intelligent Tutoring Systems, </booktitle> <pages> pages 157-183, </pages> <publisher> Academic Press, </publisher> <year> 1982. </year>
Reference-contexts: However, where the knowledge sources are complex, such as is the case if the source is another active agent in the world, the theory of the knowledge source may be complex. For example Buggy, the tutoring system for correcting simple arithmetic problems <ref> [28] </ref>, included the capacity to model the pupil's reasoning and hence explain the data it observes (namely the pupil's possibly incorrect solutions to problems) not only as random errors but also as complex systematic errors.
Reference: [29] <author> Robert Hall. </author> <title> Learning by Failing to Explain. </title> <type> Technical Report 906 (AI-TR-906), </type> <institution> MIT AI Laboratory, </institution> <year> 1986. </year>
Reference-contexts: This general problem of debugging incomplete and inadequate domain theories has been recently addressed in research in explanation-based learning (eg. by Hall <ref> [29] </ref>, VanLehn [30] and in Disciple [31]), although there is still much research needed in addressing this difficult task. 5 Conclusion Systems operating in the real world must address the problems presented by the possibility of errors in observations.
Reference: [30] <author> K. VanLehn. </author> <title> Learning a domain theory by completing explanations. </title> <editor> In T. M. Mitchell, J. G. Carbonell, and R. S. Michalski, editors, </editor> <title> Machine Learning: A Guide to Current Research, </title> <publisher> Kluwer, </publisher> <address> Lancaster, UK, </address> <year> 1986. </year>
Reference-contexts: This general problem of debugging incomplete and inadequate domain theories has been recently addressed in research in explanation-based learning (eg. by Hall [29], VanLehn <ref> [30] </ref> and in Disciple [31]), although there is still much research needed in addressing this difficult task. 5 Conclusion Systems operating in the real world must address the problems presented by the possibility of errors in observations.
Reference: [31] <editor> Yves Kodratoff and Gheorghe Tecuci. </editor> <title> What is an explanation in disciple? In P. Langley, editor, </title> <booktitle> Proc. 4th International Workshop on Machine Learning, </booktitle> <publisher> Kaufmann, </publisher> <address> Ca, </address> <year> 1987. </year> <month> 10 </month>
Reference-contexts: This general problem of debugging incomplete and inadequate domain theories has been recently addressed in research in explanation-based learning (eg. by Hall [29], VanLehn [30] and in Disciple <ref> [31] </ref>), although there is still much research needed in addressing this difficult task. 5 Conclusion Systems operating in the real world must address the problems presented by the possibility of errors in observations.
References-found: 31

