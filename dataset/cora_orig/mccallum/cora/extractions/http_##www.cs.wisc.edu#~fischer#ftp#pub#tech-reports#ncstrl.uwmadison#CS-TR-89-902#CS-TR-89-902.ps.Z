URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-89-902/CS-TR-89-902.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-89-902/
Root-URL: http://www.cs.wisc.edu
Email: sarita@cs.wisc.edu markhill@cs.wisc.edu  
Title: Weak Ordering A New Definition And Some Implications  
Author: Sarita V. Adve Mark D. Hill 
Note: 1  
Address: Madison, Wisconsin 53706  
Affiliation: Computer Sciences Department University of Wisconsin  
Abstract: University of Wisconsin - Madison Computer Sciences Technical Report #902 December 1989 A model for correct program behavior commonly and often implicitly assumed by programmers is that of sequential consistency, which guarantees that all memory accesses execute atomically and in program order. An alternative programmer's model, weak ordering, offers greater performance potential, especially for highly parallel shared memory systems. Weak ordering was first defined by Dubois, Scheurich and Briggs in terms of a set of constraints for hardware, which are not necessary for its intuitive goal. We define a system to be weakly ordered with respect to a set of software constraints if it appears sequentially consistent to software obeying those constraints. We argue that this definition more directly reflects the intuition behind weak ordering, facilitates a formal statement of the programmer's view of the system, and does not specify unnecessary directives for hardware. For software that does not allow data races, we show that the new definition allows a violation of the old definition and makes possible implementations that have a higher performance potential. We give an example of one such implementation for cache-based systems with general interconnects to illustrate the power of the new definition. The insight gained from the implementation of weak ordering can also be applied to sequential consistency. We give an algorithm and an implementation for cache-based systems with general interconnects that has a higher performance potential than others we are aware of. We also investigate a markedly new approach that is allowed by our definition to implement weak ordering, and possibly hhhhhhhhhhhhhhhhhh The material presented here is based on research supported in part by the National Science Foundation's Presidential Young Investigator and Computer and Computation Research Programs under grants MIPS-8957278 and CCR-8902536, A. T. & T. Bell Laboratories, Digital Equipment Corporation, Texas Instruments, Cray Research and the graduate school at the University of Wisconsin--Madison. sequential consistency.
Abstract-found: 1
Intro-found: 1
Reference: [ASH88] <author> A. Agarwal, R. Simoni, M. Horowitz and J. Hennessy, </author> <title> An Evaluation of Directory Schemes for Cache Coherence, </title> <booktitle> 15th Annual International Symposium on Computer Architecture, </booktitle> <address> Honolulu, Hawaii, </address> <month> June </month> <year> 1988, </year> <pages> 280-289. </pages>
Reference-contexts: No restrictions are placed on the kind of data a cache may contain, nor are any assumptions made regarding the atomicity of any transactions on the interconnection network. A straightforward directory-based, write-back cache coherency protocol, similar to those discussed in <ref> [ASH88] </ref>, is assumed. The protocol ensures that every processor observes writes on a given location in the same order. The protocol we assume involves two states for lines in memory, viz., valid and modified; and three for lines in a cache, viz., read-only (RO), read/write (RW) and invalid. <p> On a write miss that is present in RO state in more than one cache, the memory (or directory) is required to send a message to these caches to invalidate the line. This invalidation message may be a broadcast or a multicast depending on the size of the directory <ref> [ASH88] </ref>. Our protocol allows the line requested by the write to be forwarded along with these invalidation messages. On receipt of an invalidation, a cache is required to return an acknowledgement (ack) message to the memory.
Reference: [AST67] <author> D. W. Anderson, F. J. Sparacio and R. M. Tomasulo, </author> <title> The IBM System/360 Model 91: </title> <journal> Machine Philosophy and Instruction-Handling, IBM Journal, </journal> <month> January </month> <year> 1967, </year> <pages> 8-24. </pages>
Reference-contexts: An exception to this are architectures that allow imprecise interrupts <ref> [AST67] </ref>, thus violating sequential consistency on those interrupts, in favor of high performance [SmP88].
Reference: [ArB86] <author> J. Archibald and J. Baer, </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model, </title> <journal> ACM Transactions on Computer Systems 4, </journal> <month> 4 (November </month> <year> 1986), </year> <pages> 273-298. </pages>
Reference-contexts: Systems with general interconnection networks without caches The execution is possible even if accesses of a processor are issued in program order, but reach memory modules in a different order [Lam79]. Shared bus based systems with caches A cache coherency protocol is required <ref> [ArB86] </ref>. Even so, the execution is possible if the accesses of a processor are issued out of order, or if reads are allowed to pass writes in write buffers. <p> by the corresponding memory module, and (3) a memory module services all accesses to a given location in FIFO order. - 4 - - -- For cache-based systems, where processors are connected to memory modules through a common bus, a number of cache-coherence protocols have been proposed in the literature <ref> [ArB86] </ref>. Most ensure sequential consistency. In particular, Rudolph and Segall have developed two protocols, which they formally prove guarantee sequential consistency [RuS84]. The proofs rely on the presence of a single shared bus.
Reference: [Arc87] <author> J. Archibald, </author> <title> The Cache Coherence Problem in Shared-Memory Multiprocessors, </title> <type> Ph.D. Thesis, </type> <institution> University of Washington, </institution> <month> March </month> <year> 1987. </year>
Reference-contexts: Intuitively, a weakly ordered system is one that hhhhhhhhhhhhhhhhhh 2. In the most general case, a cache-coherency protocol merely ensures that all write operations to any given memory location are observed by all processors in the system in the same order <ref> [Arc87] </ref>. Note that this is different from the definition of sequential consistency which requires all writes to all memory locations to be observed in the same order [Sch89]. - -- appears to be sequentially consistent if programs always obey a certain set of constraints.
Reference: [BeG81] <author> P. A. Bernstein and N. Goodman, </author> <title> Concurrency Control in Distributed Systems, </title> <journal> Computing Surveys 13, </journal> <month> 2 (June, </month> <year> 1981), </year> <pages> 185-221. </pages>
Reference-contexts: The conditions for sequential consistency of memory accesses are analogous to the serialization condition for transactions in concurrent database systems. Hence, it may appear that sequential consistency of memory accesses can be ensured by the algorithms proposed for database systems <ref> [BeG81, Pap86] </ref>. However, there are some major differences which make this approach inefficient. Database systems seek to serialize the effects of entire transactions, which may be a series of reads and writes, while we are only concerned with the atomicity of - 7 - - -- individual reads and writes.
Reference: [BNR89] <author> R. Bisiani, A. Nowatzyk and M. Ravishankar, </author> <title> Coherent Shared Memory on a Distributed Memory Machine, </title> <booktitle> Proc. International Conference on Parallel Processing , August 1989, </booktitle> <address> I-133-141. </address>
Reference-contexts: It was recognized later in [ScD88, Sch89] that the above three conditions are not necessary for meeting this intuitive definition. In Section 3.1, we give a new definition that is closer to the intuitive goal. Bisiani, Nowatzyk and Ravishankar have proposed an algorithm <ref> [BNR89] </ref> for the implementation of weak ordering on distributed memory systems. The algorithm depends on assigning timestamps to every processor request on its creation. Processors communicate with each other to determine approximately the oldest message in the system. <p> As before, either Lemma 1 or the technique of Appendix E can be used to prove the correctness of the above sets of conditions. It is interesting to note that the scheme given by Bisiani et al. in <ref> [BNR89] </ref> and discussed in Section 2.2 obeys the above conditions and those imposed by the algorithm of Section 4.1.
Reference: [BMW85] <author> W. C. Brantley, K. P. McAuliffe and J. Weiss, </author> <title> RP3 Process-Memory Element, </title> <booktitle> International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985, </year> <pages> 772-781. </pages>
Reference-contexts: Memory is divided into distinct modules and each module is connected to all processors through exactly one bus. The RP3 system built at IBM, in cooperation with the Ultracomputer project from NYU, is a cache-based system, where processor memory communication is via an Omega network <ref> [BMW85, GGK83, PBG85] </ref>. However, in this system, the management of cache coherency for shared, writable variables is entrusted to the software. Thus from the viewpoint of the hardware, the RP3 is similar to the system studied by Lamport in [Lam79].
Reference: [Col84a] <author> W. W. Collier, </author> <title> Architectures for Systems of Parallel Processes, </title> <type> Technical Report Tech. Rep. </type> <institution> 00.3253, IBM Corp., </institution> <address> Poughkeepsie, N.Y., </address> <month> 27 January </month> <year> 1984. </year>
Reference-contexts: In subsequent sections, an access will be said to be complete only if it is globally performed. Collier has developed a general framework to characterize architectures in terms of the ordering of events that they permit <ref> [Col84a, Col84b, Col85, Col90] </ref>. An architecture is defined as a set of rules, where each rule is a restriction on the order of execution of certain events.
Reference: [Col84b] <author> W. W. Collier, </author> <title> Write Atomicity in Distributed Systems, </title> <type> Technical Report Tech. Rep. </type> <institution> 00.3304, IBM Corp., </institution> <address> Poughkeepsie, N.Y., </address> <month> 19 October </month> <year> 1984. </year>
Reference-contexts: In subsequent sections, an access will be said to be complete only if it is globally performed. Collier has developed a general framework to characterize architectures in terms of the ordering of events that they permit <ref> [Col84a, Col84b, Col85, Col90] </ref>. An architecture is defined as a set of rules, where each rule is a restriction on the order of execution of certain events.
Reference: [Col85] <author> W. W. Collier, </author> <title> Reasoning about Parallel Architectures, </title> <type> Technical Report, </type> <institution> IBM Corp., </institution> <address> Poughkeepsie, N.Y., </address> <year> 1985. </year> - -- 
Reference-contexts: In subsequent sections, an access will be said to be complete only if it is globally performed. Collier has developed a general framework to characterize architectures in terms of the ordering of events that they permit <ref> [Col84a, Col84b, Col85, Col90] </ref>. An architecture is defined as a set of rules, where each rule is a restriction on the order of execution of certain events.
Reference: [Col90] <author> W. W. Collier, </author> <title> Reasoning about Parallel Architectures, </title> <publisher> Prentice-Hall, </publisher> <address> Inc., </address> <note> To appear 1990. </note>
Reference-contexts: In subsequent sections, an access will be said to be complete only if it is globally performed. Collier has developed a general framework to characterize architectures in terms of the ordering of events that they permit <ref> [Col84a, Col84b, Col85, Col90] </ref>. An architecture is defined as a set of rules, where each rule is a restriction on the order of execution of certain events.
Reference: [DeM88] <author> R. De Leone and O. L. Mangasarian, </author> <title> Asynchronous Parallel Successive Overrelaxation for the Symmetric Linear Complementarity Problem, </title> <booktitle> Mathematical Programming 42(1988), </booktitle> <pages> 347-361. </pages>
Reference-contexts: However, there are some disadvantages of defining a weakly ordered system in this manner. First, the operation of algorithms that are inherently asynchronous and do not rely on sequential consistency for correctness <ref> [DeM88] </ref> is left unspecified. This disadvantage is easily handled by implementing weakly ordered systems so that for such algorithms, reasonable results are obtained. Second, programmers may wish to debug programs on a weakly ordered system that do not (yet) fully obey the synchronization model.
Reference: [DSB86] <author> M. Dubois, C. Scheurich and F. A. Briggs, </author> <title> Memory Access Buffering in Multiprocessors, </title> <booktitle> Proc. Thirteenth Annual International Symposium on Computer Architecture 14, </booktitle> <month> 2 (June </month> <year> 1986), </year> <pages> 434-442. </pages>
Reference-contexts: Additionally, an optimizing compiler might have more flexibility in rearranging code between synchronization operations. The above factors motivate an alternative programmer's model which relies on synchronization that is visible to the hardware to order memory accesses. Dubois, Scheurich and Briggs have discussed such systems in <ref> [DSB86, DSB88, Sch89] </ref> and have called them weakly ordered. Intuitively, a weakly ordered system is one that hhhhhhhhhhhhhhhhhh 2. <p> As will be apparent later, this option allows the RP3 to function as a weakly ordered system. Dubois, Scheurich and Briggs have analyzed the problem for systems that allow caching of shared variables, without imposing any constraints on the interconnection network <ref> [DSB86, DSB88, ScD87, Sch89] </ref>. They have introduced and defined strong and weak ordering as two models of shared memory system behavior [DSB86]. <p> Dubois, Scheurich and Briggs have analyzed the problem for systems that allow caching of shared variables, without imposing any constraints on the interconnection network [DSB86, DSB88, ScD87, Sch89]. They have introduced and defined strong and weak ordering as two models of shared memory system behavior <ref> [DSB86] </ref>. Strong ordering essentially requires that when a processor i observes a write operation by some other processor k, it also observes all accesses that were observed by k before it issued the write. <p> In addition, accesses issued by a single processor are also required to be initiated, issued and performed (refer <ref> [DSB86] </ref> for the distinction between these terms) in program order. It was claimed earlier that a system that is strongly ordered is also sequentially consistent [DSB86, DSB88, ScD87, ScD88]. However, Figure 2 shows an execution that is not precluded by the above conditions for strong ordering, but violates sequential consistency. <p> In addition, accesses issued by a single processor are also required to be initiated, issued and performed (refer [DSB86] for the distinction between these terms) in program order. It was claimed earlier that a system that is strongly ordered is also sequentially consistent <ref> [DSB86, DSB88, ScD87, ScD88] </ref>. However, Figure 2 shows an execution that is not precluded by the above conditions for strong ordering, but violates sequential consistency. It has been confirmed by Dubois and Scheurich that strong ordering is not equivalent to sequential consistency [DuS89]. <p> Weak ordering As discussed in the introduction, weakly ordered systems depend on explicit, hardware recognizable, synchronization operations, to order the effects of events initiated by different processors in a system. Dubois, Scheurich and Briggs first defined this key idea in <ref> [DSB86] </ref> as follows: - 8 - - -- Definition 1: In a multiprocessor system, storage accesses are weakly ordered if (1) accesses to global synchronizing variables are strongly ordered, (2) no access to a synchronizing variable is issued by a processor before all previous global data accesses have been globally performed, <p> respect to some processor (P 1 ). (t 2 may be the same as t 1 and in the algorithms that follow, P 2 is the same as P 1 .) The term "performed with respect to a processor" used above was first defined by Dubois, Scheurich and Briggs in <ref> [DSB86] </ref>. A read is performed with respect to a processor when no subsequently issued store by the processor can affect the value returned by the read. <p> Note that Condition 5 bears a strong resemblance to the necessary condition for strong ordering given by Dubois et al. in <ref> [DSB86] </ref>. Intuitively this algorithm is a direct attempt to ensure that all writes to all locations appear to have occurred to every processor in the same order. As discussed in Section 2.1, Collier has proved that this is equivalent to write atomicity and hence ensures sequential consistency.
Reference: [DSB88] <author> M. Dubois, C. Scheurich and F. A. Briggs, </author> <title> Synchronization, Coherence, and Event Ordering in Multiprocessors, </title> <booktitle> IEEE Computer 21, </booktitle> <month> 2 (February </month> <year> 1988), </year> <pages> 9-21. </pages>
Reference-contexts: Additionally, an optimizing compiler might have more flexibility in rearranging code between synchronization operations. The above factors motivate an alternative programmer's model which relies on synchronization that is visible to the hardware to order memory accesses. Dubois, Scheurich and Briggs have discussed such systems in <ref> [DSB86, DSB88, Sch89] </ref> and have called them weakly ordered. Intuitively, a weakly ordered system is one that hhhhhhhhhhhhhhhhhh 2. <p> As will be apparent later, this option allows the RP3 to function as a weakly ordered system. Dubois, Scheurich and Briggs have analyzed the problem for systems that allow caching of shared variables, without imposing any constraints on the interconnection network <ref> [DSB86, DSB88, ScD87, Sch89] </ref>. They have introduced and defined strong and weak ordering as two models of shared memory system behavior [DSB86]. <p> In addition, accesses issued by a single processor are also required to be initiated, issued and performed (refer [DSB86] for the distinction between these terms) in program order. It was claimed earlier that a system that is strongly ordered is also sequentially consistent <ref> [DSB86, DSB88, ScD87, ScD88] </ref>. However, Figure 2 shows an execution that is not precluded by the above conditions for strong ordering, but violates sequential consistency. It has been confirmed by Dubois and Scheurich that strong ordering is not equivalent to sequential consistency [DuS89].
Reference: [DuS89] <author> M. Dubois and C. Scheurich, </author> <title> Private Communication, </title> <month> November </month> <year> 1989. </year>
Reference-contexts: However, Figure 2 shows an execution that is not precluded by the above conditions for strong ordering, but violates sequential consistency. It has been confirmed by Dubois and Scheurich that strong ordering is not equivalent to sequential consistency <ref> [DuS89] </ref>. In [Sch89], strong ordering was discarded in favor of a similar model called concurrent consistency. The sufficient conditions to ensure concurrent consistency given in [Sch89] can be seen to be similar to those discussed above for strong ordering.
Reference: [GVW89] <author> J. R. Goodman, M. K. Vernon and P. J. Woest, </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors, </title> <booktitle> Proc. Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, </address> <month> April </month> <year> 1989, </year> <pages> 64-75. </pages>
Reference-contexts: Another interesting problem for future research is the construction of other synchronization models that may allow implementations with higher performance. These could be optimized for particular synchronization primitives that may be offered by specific systems, e.g., QOLB in the Wisconsin Multicube <ref> [GVW89] </ref>. 7. Acknowledgements We would like to thank William Collier, Garth Gibson, Richard Kessler, Viranjit Madan, Bart Miller and Robert Netzer for their useful comments on earlier drafts of the paper.
Reference: [GGK83] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph and M. Snir, </author> <title> The NYU Ultracomputer--Designing an MIMD Shared Memory Parallel Computer, </title> <journal> IEEE Trans. on Computers C-32, </journal> <month> 2 (February </month> <year> 1983), </year> <pages> 175-189. </pages>
Reference-contexts: Memory is divided into distinct modules and each module is connected to all processors through exactly one bus. The RP3 system built at IBM, in cooperation with the Ultracomputer project from NYU, is a cache-based system, where processor memory communication is via an Omega network <ref> [BMW85, GGK83, PBG85] </ref>. However, in this system, the management of cache coherency for shared, writable variables is entrusted to the software. Thus from the viewpoint of the hardware, the RP3 is similar to the system studied by Lamport in [Lam79].
Reference: [Jef85] <author> D. R. Jefferson, </author> <title> Virtual Time, </title> <journal> ACM Trans. on Programming Languages and Systems 7, </journal> <month> 3 (July </month> <year> 1985), </year> <pages> 404-425. </pages>
Reference-contexts: Extensions to accommodate these are given, but may result in an even more pessimistic analysis. There have been other approaches that partially or completely rely on software to make parallel executions appear sequential. Jefferson first introduced the concept of virtual time and the time warp mechanism in <ref> [Jef85] </ref> for correct synchronization of distributed message passing systems. Knight [Kni86], and Tinker and Katz [TiK88] have described schemes for parallelizing mostly functional languages with a few side-effects. Each of these approaches schedules parallel tasks optimistically, without any attention to serializability.
Reference: [Kni86] <author> T. Knight, </author> <title> An Architecture for Mostly Functional Languages, </title> <booktitle> Proc. ACM Conference on LISP and Functional Programming, </booktitle> <year> 1986, </year> <pages> 105-112. </pages>
Reference-contexts: There have been other approaches that partially or completely rely on software to make parallel executions appear sequential. Jefferson first introduced the concept of virtual time and the time warp mechanism in [Jef85] for correct synchronization of distributed message passing systems. Knight <ref> [Kni86] </ref>, and Tinker and Katz [TiK88] have described schemes for parallelizing mostly functional languages with a few side-effects. Each of these approaches schedules parallel tasks optimistically, without any attention to serializability.
Reference: [Kro81] <author> D. Kroft, </author> <title> Lockup-Free Instruction Fetch/Prefetch Cache Organization, </title> <booktitle> Proc. Eighth Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1981, </year> <pages> 81-87. </pages>
Reference-contexts: More sophisticated architectures allow overlap of instruction execution, out-of-order memory accesses, write buffers, caches (which may be lock-up free <ref> [Kro81] </ref>), etc. In these machines, an ordering of memory accesses based on wall-clock time of issue or execution may violate program order, but interlock logic assures that accesses appear to execute in program order, thereby maintaining sequential consistency 1 .
Reference: [Lam78] <author> L. Lamport, </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System, </title> <journal> Communications of the ACM 21, </journal> <month> 7 (July </month> <year> 1978), </year> <pages> 558-565. </pages>
Reference-contexts: To formally specify the second feature of DRF, viz., an indication of when there is "enough" synchronization in a program, we first define a set of happens-before relations for a program. Our definition is closely related to the "happened-before" relation defined by Lamport <ref> [Lam78] </ref> for message passing systems, and the "approximate temporal order" used by Netzer and Miller [NeM89] for detecting races in shared memory parallel programs that use semaphores.
Reference: [Lam79] <author> L. Lamport, </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs, </title> <journal> IEEE Trans. on Computers C-28, </journal> <month> 9 (September </month> <year> 1979), </year> <pages> 690-691. </pages>
Reference-contexts: This paper is primarily concerned with the programmer's model of a shared memory system and its implications on hardware design and performance. A model for correct behavior of programs commonly (and often implicitly) assumed by programmers is that of sequential consistency, formally defined by Lamport <ref> [Lam79] </ref> as follows: [A system is sequentially consistent if] the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual pro cessor appear in this sequence in the order specified by its program. <p> Systems with general interconnection networks without caches The execution is possible even if accesses of a processor are issued in program order, but reach memory modules in a different order <ref> [Lam79] </ref>. Shared bus based systems with caches A cache coherency protocol is required [ArB86]. Even so, the execution is possible if the accesses of a processor are issued out of order, or if reads are allowed to pass writes in write buffers. <p> Section 2.1 deals primarily with sequential consistency, while Section 2.2 concerns weak ordering. Though a large amount of literature exists on this subject, to our knowledge, no such survey has been published so fat. 2.1. Sequential consistency As discussed in the introduction, sequential consistency was first defined by Lamport <ref> [Lam79] </ref>, and discussed for shared memory systems with a general interconnection network, but without any caches. <p> However, in this system, the management of cache coherency for shared, writable variables is entrusted to the software. Thus from the viewpoint of the hardware, the RP3 is similar to the system studied by Lamport in <ref> [Lam79] </ref>. The second condition given by Lamport is met in the RP3, by requiring a process to wait for an acknowledgement from memory for its previous miss on a shared writable variable, before it can issue another access to such a variable.
Reference: [Lam86a] <author> L. Lamport, </author> <title> The Mutual Exclusion Problem, </title> <journal> Parts I and II , Journal of the Association of Computing Machinery 33, </journal> <month> 2 (April </month> <year> 1986), </year> <pages> 313-348. </pages>
Reference-contexts: Depending on the implementation, these operations can themselves result in some performance penalty. However, we believe that slow synchronization operations coupled with fast reads and writes will yield better performance than the alternative, where hardware must assume all accesses could be used for synchronization (e.g., in <ref> [Lam86a, Lam87] </ref>). - 10 - - -- Finally, programmers may wish that a synchronization model be specified so that it is possible and practical to verify whether a program, or at least an execution of a program, meets the conditions of the model.
Reference: [Lam86b] <author> L. Lamport, </author> <title> On Interprocess Communication, Parts I and II , Distributed Computing , January 1986, </title> <type> 78-101. </type>
Reference-contexts: For this reason, programs that require atomicity of multiple accesses for correctness, may be best executed on weakly ordered systems. Henceforth, we will always interpret sequential consistency as implying atomicity for individual memory accesses. In <ref> [Lam86b] </ref>, Lamport has developed a formalism for proving the correctness of programs, without making any assumptions regarding the atomicity of memory accesses. However, it is assumed that an access is not issued until the previous access of the same process is complete.
Reference: [Lam87] <author> L. Lamport, </author> <title> A Fast Mutual Exclusion Algorithm, </title> <journal> ACM Transactions on Computer Systems 5, </journal> <month> 1 (February </month> <year> 1987), </year> <pages> 1-11. </pages>
Reference-contexts: Depending on the implementation, these operations can themselves result in some performance penalty. However, we believe that slow synchronization operations coupled with fast reads and writes will yield better performance than the alternative, where hardware must assume all accesses could be used for synchronization (e.g., in <ref> [Lam86a, Lam87] </ref>). - 10 - - -- Finally, programmers may wish that a synchronization model be specified so that it is possible and practical to verify whether a program, or at least an execution of a program, meets the conditions of the model.
Reference: [NeM89] <author> R. Netzer and B. Miller, </author> <title> Detecting Data Races in Parallel Program Executions, </title> <type> Computer Sciences Technical Report #894, </type> <institution> University of Wisconsin, Madison, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: Our definition is closely related to the "happened-before" relation defined by Lamport [Lam78] for message passing systems, and the "approximate temporal order" used by Netzer and Miller <ref> [NeM89] </ref> for detecting races in shared memory parallel programs that use semaphores. A happens-before relation for a program is a partial order defined for an execution of the program on an abstract, idealized architecture, where all memory accesses are executed atomically and in program order. <p> Furthermore, current work is being done on determining when programs are data-race-free, and on debugging executions that are not <ref> [NeM89] </ref>. Note that since synchronization operations may now be different from normal reads and writes, the term operations in the definition of sequential consistency should be extended to include these synchronization operations as well. 4.
Reference: [Nic89] <author> K. E. Nicholas, </author> <title> Levels of Ordering and Consistency in Shared-Memory Multiprocessors, </title> <type> Master's Thesis, </type> <institution> Department of Electrical and Computer Engineering, Brigham Young University, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: Collier has similarly proved equivalence (and inequivalence) of various other sets of rules, and hence of the architectures defined by them. However, no implementations of any of these rules have been given for cache-based systems with general interconnects. Nicholas <ref> [Nic89] </ref> has identified several possible orderings of memory accesses in shared memory systems, and characterized them in terms of the architectural rules defined by Collier. A hierarchy of systems based on the orderings of events that they permit has been constructed. <p> They propose the use of an explicit lock instruction and give the minimum such locks that need to be used. Nicholas also explores this concept <ref> [Nic89] </ref> and concludes that explicit synchronization is required in general, for multiple memory accesses to be atomic. For this reason, programs that require atomicity of multiple accesses for correctness, may be best executed on weakly ordered systems.
Reference: [Pap86] <author> C. Papadimitriou, </author> <title> The Theory of Database Concurrency Control, </title> <publisher> Computer Science Press, </publisher> <address> Rockville, Maryland 20850, </address> <year> 1986. </year>
Reference-contexts: The conditions for sequential consistency of memory accesses are analogous to the serialization condition for transactions in concurrent database systems. Hence, it may appear that sequential consistency of memory accesses can be ensured by the algorithms proposed for database systems <ref> [BeG81, Pap86] </ref>. However, there are some major differences which make this approach inefficient. Database systems seek to serialize the effects of entire transactions, which may be a series of reads and writes, while we are only concerned with the atomicity of - 7 - - -- individual reads and writes. <p> This may involve a trade-off between the allowable software and computational complexity of the verification algorithm. Database scheduling algorithms make a similar compromise by allowing only conflict serializable as opposed to the more general view serializable schedules for computational ease <ref> [Pap86] </ref>. In the next section, we identify a synchronization model that we believe offers a good trade-off between hardware and software flexibility, at least within the domain of currently existing architectures and programming styles. 3.2. A synchronization model: Data-Race-Free We identify a synchronization model that we call "Data-Race-Free" (DRF).
Reference: [PBG85] <author> G. F. Pfister, W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfelder, K. P. McAuliffe, E. A. Melton, V. A. Norton and J. Weiss, </author> <title> The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture, </title> <booktitle> International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985, </year> <pages> 764-771. </pages>
Reference-contexts: Memory is divided into distinct modules and each module is connected to all processors through exactly one bus. The RP3 system built at IBM, in cooperation with the Ultracomputer project from NYU, is a cache-based system, where processor memory communication is via an Omega network <ref> [BMW85, GGK83, PBG85] </ref>. However, in this system, the management of cache coherency for shared, writable variables is entrusted to the software. Thus from the viewpoint of the hardware, the RP3 is similar to the system studied by Lamport in [Lam79].
Reference: [RuS84] <author> L. Rudolph and Z. Segall, </author> <title> Dynamic Decentralized Cache Schemes for MIMD Parallel Processors, </title> <booktitle> Proc. Eleventh International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1984, </year> <pages> 340-347. </pages>
Reference-contexts: Most ensure sequential consistency. In particular, Rudolph and Segall have developed two protocols, which they formally prove guarantee sequential consistency <ref> [RuS84] </ref>. The proofs rely on the presence of a single shared bus. However, to increase the number of processors that can be efficiently utilized with these protocols, they propose the use of multiple buses which can together be logically treated as one single bus.
Reference: [ScD87] <author> C. Scheurich and M. Dubois, </author> <title> Correct Memory Operation of Cache-Based Multiprocessors, </title> <booktitle> Proc. Fourteenth Annual International Symposium on Computer Architecture, </booktitle> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1987, </year> <pages> 234-243. </pages>
Reference-contexts: Note that a cache coherency protocol does not preclude this possibility . Correct behavior can be achieved if a processor does not issue an access until values read or written by its previous accesses (in program order) have been observed by the rest of the processors in the system <ref> [ScD87] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh The problem of maintaining sequential consistency manifests itself when two or more processors interact through memory operations on common variables. <p> As will be apparent later, this option allows the RP3 to function as a weakly ordered system. Dubois, Scheurich and Briggs have analyzed the problem for systems that allow caching of shared variables, without imposing any constraints on the interconnection network <ref> [DSB86, DSB88, ScD87, Sch89] </ref>. They have introduced and defined strong and weak ordering as two models of shared memory system behavior [DSB86]. <p> In addition, accesses issued by a single processor are also required to be initiated, issued and performed (refer [DSB86] for the distinction between these terms) in program order. It was claimed earlier that a system that is strongly ordered is also sequentially consistent <ref> [DSB86, DSB88, ScD87, ScD88] </ref>. However, Figure 2 shows an execution that is not precluded by the above conditions for strong ordering, but violates sequential consistency. It has been confirmed by Dubois and Scheurich that strong ordering is not equivalent to sequential consistency [DuS89]. <p> Recall that particularly for cache-based systems, "completion" is to be interpreted as "globally performed". The conditions given by Dubois and Scheurich for sequential consistency in cache-based systems <ref> [ScD87, Sch89] </ref>, require a process to always wait for the previous access to complete before the next access can be issued. The algorithm given by Sha-sha and Snir finds the minimum number of pairs of accesses between which such a delay is required. <p> Where and for how long must P 0 stall? P 1 ? g A straightforward implementation of sequential consistency. P 0 and P 1 must stall at each access until its previously issued access completes <ref> [ScD87] </ref>. g A straightforward implementation of Definition 1. P 0 must stall at Unset (s) to complete previously issued accesses, including to x, before executing Unset (s). P 1 must stall at TestAndSet (s) until P 0 's Unset (s) is complete. g The implementation of Section 4.3. <p> The new implementation of Section 4.3 also inspires a more aggressive implementation of sequential consistency for cache-based systems with general interconnects. The conditions for sequential consistency in <ref> [ScD87, Sch89] </ref> require a processor to wait for its write operation to be visible to all other processors before it proceeds. <p> This is exactly what the algorithm requires. Such a scheme can be implemented to allow a limited number of outstanding writes. 5.3. A qualitative analysis The implementation described in the previous subsection does not perform any worse than any based on the conditions given in <ref> [ScD87, Sch89] </ref>. However, it potentially performs better because of the following reasons. (1) After issuing a write to a valid line, a processor is required to wait only until it receives its copy of the line. <p> While invalidations corresponding to this write are being received and acknowledged by the other processors, all subsequent accesses of this processor that hit in the cache are immediately serviced. As opposed to this, the conditions given in <ref> [ScD87, Sch89] </ref> allow only accesses to local variables to be serviced while an ack is outstanding.
Reference: [ScD88] <author> C. Scheurich and M. Dubois, </author> <title> Concurrent Miss Resolution in Multiprocessor Caches, </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <address> University Park PA, </address> <month> August, </month> <year> 1988, </year> <month> I-118-125. </month> - -- 
Reference-contexts: In addition, accesses issued by a single processor are also required to be initiated, issued and performed (refer [DSB86] for the distinction between these terms) in program order. It was claimed earlier that a system that is strongly ordered is also sequentially consistent <ref> [DSB86, DSB88, ScD87, ScD88] </ref>. However, Figure 2 shows an execution that is not precluded by the above conditions for strong ordering, but violates sequential consistency. It has been confirmed by Dubois and Scheurich that strong ordering is not equivalent to sequential consistency [DuS89]. <p> Intuitively, the definition states that if shared variables are always accessed in critical sections, a weakly ordered system appears to be sequentially consistent. It was recognized later in <ref> [ScD88, Sch89] </ref> that the above three conditions are not necessary for meeting this intuitive definition. In Section 3.1, we give a new definition that is closer to the intuitive goal. Bisiani, Nowatzyk and Ravishankar have proposed an algorithm [BNR89] for the implementation of weak ordering on distributed memory systems.
Reference: [Sch89] <author> C. E. Scheurich, </author> <title> Access Ordering and Coherence in Shared Memory Multiprocessors, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Engineering, </institution> <type> Technical Report CENG 89-19, </type> <institution> University of Southern California, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Additionally, an optimizing compiler might have more flexibility in rearranging code between synchronization operations. The above factors motivate an alternative programmer's model which relies on synchronization that is visible to the hardware to order memory accesses. Dubois, Scheurich and Briggs have discussed such systems in <ref> [DSB86, DSB88, Sch89] </ref> and have called them weakly ordered. Intuitively, a weakly ordered system is one that hhhhhhhhhhhhhhhhhh 2. <p> Note that this is different from the definition of sequential consistency which requires all writes to all memory locations to be observed in the same order <ref> [Sch89] </ref>. - -- appears to be sequentially consistent if programs always obey a certain set of constraints. Dubois et al. have formalized weak ordering in terms of certain conditions on hardware, which we shall discuss later. <p> As will be apparent later, this option allows the RP3 to function as a weakly ordered system. Dubois, Scheurich and Briggs have analyzed the problem for systems that allow caching of shared variables, without imposing any constraints on the interconnection network <ref> [DSB86, DSB88, ScD87, Sch89] </ref>. They have introduced and defined strong and weak ordering as two models of shared memory system behavior [DSB86]. <p> However, Figure 2 shows an execution that is not precluded by the above conditions for strong ordering, but violates sequential consistency. It has been confirmed by Dubois and Scheurich that strong ordering is not equivalent to sequential consistency [DuS89]. In <ref> [Sch89] </ref>, strong ordering was discarded in favor of a similar model called concurrent consistency. The sufficient conditions to ensure concurrent consistency given in [Sch89] can be seen to be similar to those discussed above for strong ordering. <p> It has been confirmed by Dubois and Scheurich that strong ordering is not equivalent to sequential consistency [DuS89]. In <ref> [Sch89] </ref>, strong ordering was discarded in favor of a similar model called concurrent consistency. The sufficient conditions to ensure concurrent consistency given in [Sch89] can be seen to be similar to those discussed above for strong ordering. <p> One drawback, however, of this model of "almost sequential consistency" is that it does not give a formal specification of the software for which a system obeying this model will appear sequentially consistent. Along with concurrent consistency, the models of sequential consistency and weak ordering were identified in <ref> [Sch89] </ref> for shared memory systems. The work on weak ordering is the topic of the next subsection. For sequential consistency in cache-based systems, a sufficient condition has been proposed. <p> Recall that particularly for cache-based systems, "completion" is to be interpreted as "globally performed". The conditions given by Dubois and Scheurich for sequential consistency in cache-based systems <ref> [ScD87, Sch89] </ref>, require a process to always wait for the previous access to complete before the next access can be issued. The algorithm given by Sha-sha and Snir finds the minimum number of pairs of accesses between which such a delay is required. <p> Intuitively, the definition states that if shared variables are always accessed in critical sections, a weakly ordered system appears to be sequentially consistent. It was recognized later in <ref> [ScD88, Sch89] </ref> that the above three conditions are not necessary for meeting this intuitive definition. In Section 3.1, we give a new definition that is closer to the intuitive goal. Bisiani, Nowatzyk and Ravishankar have proposed an algorithm [BNR89] for the implementation of weak ordering on distributed memory systems. <p> The new implementation of Section 4.3 also inspires a more aggressive implementation of sequential consistency for cache-based systems with general interconnects. The conditions for sequential consistency in <ref> [ScD87, Sch89] </ref> require a processor to wait for its write operation to be visible to all other processors before it proceeds. <p> This is exactly what the algorithm requires. Such a scheme can be implemented to allow a limited number of outstanding writes. 5.3. A qualitative analysis The implementation described in the previous subsection does not perform any worse than any based on the conditions given in <ref> [ScD87, Sch89] </ref>. However, it potentially performs better because of the following reasons. (1) After issuing a write to a valid line, a processor is required to wait only until it receives its copy of the line. <p> While invalidations corresponding to this write are being received and acknowledged by the other processors, all subsequent accesses of this processor that hit in the cache are immediately serviced. As opposed to this, the conditions given in <ref> [ScD87, Sch89] </ref> allow only accesses to local variables to be serviced while an ack is outstanding.
Reference: [ShS88] <author> D. Shasha and M. Snir, </author> <title> Efficient and Correct Execution of Parallel Programs that Share Memory, </title> <journal> ACM Trans. on Programming Languages and Systems 10, </journal> <month> 2 (April </month> <year> 1988), </year> <pages> 282-312. </pages>
Reference-contexts: A hierarchy of systems based on the orderings of events that they permit has been constructed. Shasha and Snir have proposed a software algorithm to ensure sequential consistency <ref> [ShS88] </ref>. Their scheme statically identifies a minimal set of pairs of accesses within a process, such that delaying the issue of one of the elements of each pair until the completion of the other, guarantees sequential consistency. <p> So far, we have been interpreting the term "operations" in Lamport's definition of sequential consistency to mean "memory accesses". However, it is possible for a program to require atomicity at a higher level of granularity (e.g., processor instructions, or an aggregate of instructions). Shasha and Snir <ref> [ShS88] </ref> show that in general, atomicity of multiple memory accesses cannot be ensured just by introducing delays between accesses of a process. They propose the use of an explicit lock instruction and give the minimum such locks that need to be used.
Reference: [SmP88] <author> J. E. Smith and A. R. Pleszkun, </author> <title> Implementing Precise Interrupts in Pipelined Processors, </title> <journal> IEEE Trans. on Computers C-37, </journal> <month> 5 (May </month> <year> 1988), </year> <pages> 562-573. </pages>
Reference-contexts: An exception to this are architectures that allow imprecise interrupts [AST67], thus violating sequential consistency on those interrupts, in favor of high performance <ref> [SmP88] </ref>.
Reference: [TiK88] <author> P. Tinker and M. Katz, </author> <title> Parallel Execution of Sequential Scheme with ParaTran, </title> <booktitle> Proc. ACM Conference on LISP and Functional Programming, </booktitle> <month> July </month> <year> 1988, </year> <pages> 28-39. </pages>
Reference-contexts: There have been other approaches that partially or completely rely on software to make parallel executions appear sequential. Jefferson first introduced the concept of virtual time and the time warp mechanism in [Jef85] for correct synchronization of distributed message passing systems. Knight [Kni86], and Tinker and Katz <ref> [TiK88] </ref> have described schemes for parallelizing mostly functional languages with a few side-effects. Each of these approaches schedules parallel tasks optimistically, without any attention to serializability. The runtime environment detects if any execution could preclude serialization, in which case the relevant process is rolled back to a consistent state.
References-found: 36

