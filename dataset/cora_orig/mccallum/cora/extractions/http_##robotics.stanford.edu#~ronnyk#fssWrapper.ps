URL: http://robotics.stanford.edu/~ronnyk/fssWrapper.ps
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: fronnyk,sommdag@CS.Stanford.EDU  
Title: Feature Subset Selection Using the Wrapper Method: Overfitting and Dynamic Search Space Topology  
Author: Ron Kohavi and Dan Sommerfield 
Address: Stanford, CA. 94305  
Affiliation: Computer Science Department Stanford University  
Note: Appears in the First International Conference on Knowledge Discovery and Data Mining (KDD-95)  
Abstract: In the wrapper approach to feature subset selection, a search for an optimal set of features is made using the induction algorithm as a black box. The estimated future performance of the algorithm is the heuristic guiding the search. Statistical methods for feature subset selection including forward selection, backward elimination, and their stepwise variants can be viewed as simple hill-climbing techniques in the space of feature subsets. We utilize best-first search to find a good feature subset and discuss overfitting problems that may be associated with searching too many feature subsets. We introduce compound operators that dynamically change the topology of the search space to better utilize the information available from the evaluation of feature subsets. We show that compound operators unify previous approaches that deal with relevant and irrelevant features. The improved feature subset selection yields significant improvements for real-world datasets when using the ID3 and the Naive-Bayes induction algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: The problem is especially severe when large databases, with many features, are searched for patterns without filtering of important features by human experts or when no such experts exist. Common machine learning algorithms, including top-down induction of decision trees, such as CART, ID3, and C4.5 <ref> (Breiman, Friedman, Olshen & Stone 1984, Quinlan 1993) </ref>, and nearest-neighbor algorithms, such as IB1, are known to suffer from irrelevant features. Naive-Bayes classifiers, which assume independence of features given the instance label, suffer from correlated and redundant features.
Reference: <author> Caruana, R. & Freitag, D. </author> <year> (1994), </year> <title> Greedy attribute selection, </title> <editor> in W. W. Cohen & H. Hirsh, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1993), </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning, </title> <booktitle> in "Proceedings of the 13th International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 1022-1027. </pages>
Reference: <author> Geman, S. & Bienenstock, E. </author> <year> (1992), </year> <title> "Neural networks and the bias/variance dilemma", </title> <booktitle> Neural Computation 4, </booktitle> <pages> 1-48. </pages>
Reference-contexts: An example of an over-specialized hypothesis, or classifier, is a lookup table on all the features. Overfitting is closely related to the bias-variance tradeoff <ref> (Geman & Bienenstock 1992, Breiman et al. 1984) </ref>: if the algorithm fits the data too well, the variance term is large, and hence the overall error is increased.
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <booktitle> in "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 121-129. </pages> <note> Available by anonymous ftp from: starry.Stanford.EDU:pub/ronnyk/ml94.ps. </note>
Reference-contexts: Compound operators improve the search by finding nodes with higher accuracy faster; however, whenever it is easy to overfit, they cause overfitting earlier. 6 Experimental Results In order to compare the feature subset selection, we used ID3 and Naive-Bayes, both implemented in MLC ++ <ref> (Kohavi, John, Long, Manley & Pfleger 1994) </ref>. The ID3 version does no pruning by itself; pruning is thus achieved by the feature subset selection mechanism. The Naive-Bayes algorithm assumes the features are independent given the instance label.
Reference: <author> Kohavi, R. </author> <year> (1994), </year> <title> Feature subset selection as search with probabilistic estimates, </title> <booktitle> in "AAAI Fall Symposium on Relevance", </booktitle> <pages> pp. 122-126. </pages>
Reference-contexts: Compound operators improve the search by finding nodes with higher accuracy faster; however, whenever it is easy to overfit, they cause overfitting earlier. 6 Experimental Results In order to compare the feature subset selection, we used ID3 and Naive-Bayes, both implemented in MLC ++ <ref> (Kohavi, John, Long, Manley & Pfleger 1994) </ref>. The ID3 version does no pruning by itself; pruning is thus achieved by the feature subset selection mechanism. The Naive-Bayes algorithm assumes the features are independent given the instance label.
Reference: <author> Kohavi, R., John, G., Long, R., Manley, D. & Pfleger, K. </author> <year> (1994), </year> <title> MLC++: A machine learning library in C++, </title> <booktitle> in "Tools with Artificial Intelligence", </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 740-743. </pages> <note> Available by anonymous ftp from: starry.Stanford.EDU:pub/ronnyk/mlc/ toolsmlc.ps. </note>
Reference-contexts: Compound operators improve the search by finding nodes with higher accuracy faster; however, whenever it is easy to overfit, they cause overfitting earlier. 6 Experimental Results In order to compare the feature subset selection, we used ID3 and Naive-Bayes, both implemented in MLC ++ <ref> (Kohavi, John, Long, Manley & Pfleger 1994) </ref>. The ID3 version does no pruning by itself; pruning is thus achieved by the feature subset selection mechanism. The Naive-Bayes algorithm assumes the features are independent given the instance label.
Reference: <author> Langley, P. </author> <year> (1994), </year> <title> Selection of relevant features in machine learning, </title> <booktitle> in "AAAI Fall Symposium on Relevance", </booktitle> <pages> pp. 140-144. </pages>
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994), </year> <title> Induction of selective bayesian classifiers, </title> <booktitle> in "Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Seattle, WA, </address> <pages> pp. 399-406. </pages>
Reference: <author> Miller, A. J. </author> <year> (1990), </year> <title> Subset Selection in Regression, </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994), </year> <title> UCI repository of machine learning databases, For information contact ml-repository@ics.uci.edu. </title>
Reference-contexts: The first p-val column indicates the probability that FSS improves ID3 and the second column indicates the probability that FSS improves Naive-Bayes. The p-values were computed using a one-tailed t-test. Because small datasets are easier to overfit using our approach, we chose real-world datasets from the U.C. Irvine repository <ref> (Murphy & Aha 1994) </ref> that had at least 250 instances. For datasets with over 1000 instances, a separate test set with one-third of the instances was used; for datasets with fewer than 1000 instances, 5-fold cross-validation was used. Table 1 describes general information about the datasets used.
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, Califor-nia. </address>
Reference: <author> Schaffer, C. </author> <year> (1993), </year> <title> "Selecting a classification method by cross-validation", </title> <booktitle> Machine Learning 13(1), </booktitle> <pages> 135-143. </pages>
Reference: <author> Weiss, S. M. & Kulikowski, C. A. </author> <year> (1991), </year> <title> Computer Systems that Learn, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA. </address>
Reference-contexts: Operators determine the partial ordering between the states. In this paper, we use the wrapper method wherein the criteria to optimize is the estimated prediction accuracy. Methods that wrap around the induction algorithm, such as holdout, bootstrap, and cross-validation <ref> (Weiss & Kulikowski 1991) </ref> are used to estimate the prediction accuracy. To conduct a search, one needs to define the following: Search Space Operators The operators in the search space are usually either "add feature" or "delete feature" or both.
Reference: <author> Wolpert, D. H. </author> <year> (1992), </year> <title> "On the connection between in-sample testing and generalization error", </title> <booktitle> Complex Systems 6, </booktitle> <pages> 47-94. </pages>
References-found: 15

