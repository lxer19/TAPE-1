URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-94-45.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: @virginia.edu  
Title: Braid: Integrating Task and Data Parallelism hand, applications exist which contain a mix of the
Author: Emily A. West Andrew S. Grimshaw 
Address: -west grimshaw  
Affiliation: Department of Computer Science, University of Virginia  
Note: This work was partially funded by the General Electric Faculty for the Future Program and NSF grants ASC-9201822 and CDA 8922545-01.  1: Introduction On the other  
Abstract: Archetype data parallel or task parallel applications are well served by contemporary languages. However, for applications containing a balance of task and data parallelism the choice of language is less clear. While there are languages that enable both forms of parallelism, e.g., one can write data parallel programs using a task parallel language, there are few languages which support both. We present a set of data parallel extensions to the Mentat Programming Language (MPL) which allow us to integrate task parallelism, data parallelism, and nested task and data parallelism within a single language on top of a single run time system. The result is an object-oriented language, Braid, that supports both task and data parallelism on MIMD machines. In addition, the data parallel extensions define a language in and of itself which makes a number of contributions to the data parallel programming style. These include subset-level operations (a more general notion of element-level operations), compiler provided iteration within a data parallel data set and the ability to define complex data parallel operations. Many parallel languages have been introduced over the past decade. Broadly speaking most can be categorized as supporting either data parallelism or task parallelism (alternatively called control parallelism). Both task and data parallel languages have applications for which they are well suited. For example, task parallel languages are well suited for applications where there are multiple actions to be performed, e.g., a pipeline, or different interacting entities to be modeled. Similarly, data parallel languages are well suited to applications where the same action is to be performed on many different data items, for example image convolution where each pixel can be processed in parallel. force the designer to make a choice. If one type of parallelism dominates the computation, then a language to exploit that parallelism is called for and the other type can be ignored. However, for applications containing a balance of task and data parallelism the choice of language is less clear. As an example, global climate modeling [18] involves the interaction of distinct oceanic and atmospheric weather models. The two resulting PDEs can be individually solved in a data parallel fashion by superimposing a grid upon the space to be modeled. Each grid location is then an item in a data set. Because the interactions between neighboring grid points along the borders of the oceanic and atmospheric models are quite different from the interactions between grid points internal to either model, a distinction is made between the oceanic and atmospheric portions of the computation. This calls for the use of task parallel constructs to govern inter-model interactions. Global climate modeling is readily identifiable as an application which would benefit from a language supporting both task and data parallelism. However, one must ask if this example is unique, or whether there exists a group of applications for which a mixed language is the solution. Multi-disciplinary optimization problems contain data parallelism within a larger problem. For example, DNA protein sequence comparison [8] contains this type of parallelism. These examples are by no means exhaustive - merely representative. One difficulty with identifying applications is that the tools available inuence how, and which, applications are coded. Recent activity in the field of combined task and data parallel language design [3, 5, 7, 22] is further evidence of the importance of the problem. We have designed a set of data parallel extensions to the Mentat Programming Language (MPL) which allow us to integrate task parallelism, data parallelism, and nested task and data parallelism within a single language on top of a single run time system. The result is an object-oriented language, Braid, that supports both task and data parallelism on MIMD machines. The data parallel extensions include both element-centered operations as well as a more general notion of subset-centered operations which has not been To appear: Frontiers 95 - The Fifth Symposium on the Frontiers of Massively Parallel Computation, Feb. 6 - 9, 1995, McLean, Virginia. Available as: University of Virginia Department of Computer Science Technical Report CS-94-45. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> American National Standards Institute 1990. ANSI X3J3/S8.115. Fortran 90. </institution>
Reference-contexts: Figure 6 illustrates both task and data parallel invocations within the same code fragment. 5: Related Work In terms of purely data parallel languages, Dataparallel C [13, 20, 21], pC++ [2, 16], C** [15], Fortran D [6], Fortran 90 <ref> [1] </ref>, and High Performance Fortran (HPF) [17] are the languages from which are related to our work. C** and pC++ are based on C++. Dataparallel C is based on C, but uses some ideas from object-oriented language design. HPFs origin is Fortran.
Reference: [2] <author> F. Bodin et al., </author> <title> Distributed pC++: Basic Ideas for an Object Parallel Language, </title> <booktitle> Proceedings Object-Oriented Numerics Conference, </booktitle> <month> April 25-27, </month> <year> 1993, </year> <title> Sunriver, </title> <booktitle> Oregon, </booktitle> <pages> pp. 1-24. </pages>
Reference-contexts: Specifying a data parallel computation in terms of a single element is the approach we have used in creating our data parallel extensions. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 4, 6, 13, 15, 16, 17, 20, 21] </ref>. However, we have extended the notion to encompass subset level data parallelism. By subset level data parallelism we mean allowing the definition of operations in which subsets (as opposed to elements) are the data granules, e.g., a row or a column. <p> Figure 6 illustrates both task and data parallel invocations within the same code fragment. 5: Related Work In terms of purely data parallel languages, Dataparallel C [13, 20, 21], pC++ <ref> [2, 16] </ref>, C** [15], Fortran D [6], Fortran 90 [1], and High Performance Fortran (HPF) [17] are the languages from which are related to our work. C** and pC++ are based on C++. Dataparallel C is based on C, but uses some ideas from object-oriented language design.
Reference: [3] <author> K.M. Chandy et al., </author> <title> Integrated Support for Task and Data Parallelism, </title> <journal> Intl. J. of Supercomputer Applications, </journal> <volume> 8(2), </volume> <year> 1994, </year> <pages> pp. 80-98. </pages>
Reference-contexts: These examples are by no means exhaustive - merely representative. One difficulty with identifying applications is that the tools available inuence how, and which, applications are coded. Recent activity in the field of combined task and data parallel language design <ref> [3, 5, 7, 22] </ref> is further evidence of the importance of the problem. <p> We begin with a task parallel paradigm and incorporate data parallelism. Braid also allows execution of task and data parallel components at the same level, while Subhlok presents data parallelism contained within task parallelism. Foster et al. [5] and Chandy et al. <ref> [3] </ref> describe an interface for combining Fortran M and HPF to achieve the integration of task and data parallelism. Their approach differs from our own and Subhloks in that the mechanism used is actually two separate compilers, one task parallel and one data parallel.
Reference: [4] <author> B. Chapman, P. Mehrotra, and H. Zima, </author> <title> Programming in Vienna Fortran, </title> <journal> Scientific Programming, </journal> <volume> Vol. 1, No. 1, </volume> <month> Aug. </month> <year> 1992, </year> <pages> pp. 31-50. </pages>
Reference-contexts: Specifying a data parallel computation in terms of a single element is the approach we have used in creating our data parallel extensions. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 4, 6, 13, 15, 16, 17, 20, 21] </ref>. However, we have extended the notion to encompass subset level data parallelism. By subset level data parallelism we mean allowing the definition of operations in which subsets (as opposed to elements) are the data granules, e.g., a row or a column.
Reference: [5] <author> I. Foster, M. Xu, B. Avalani, A. Choudhary, </author> <title> A Compilation System that Integrates High Performance Fortran and Fortran M, </title> <booktitle> Proccedings of the 1994 Scalable High Performance Computing Conference. </booktitle>
Reference-contexts: These examples are by no means exhaustive - merely representative. One difficulty with identifying applications is that the tools available inuence how, and which, applications are coded. Recent activity in the field of combined task and data parallel language design <ref> [3, 5, 7, 22] </ref> is further evidence of the importance of the problem. <p> We begin with a task parallel paradigm and incorporate data parallelism. Braid also allows execution of task and data parallel components at the same level, while Subhlok presents data parallelism contained within task parallelism. Foster et al. <ref> [5] </ref> and Chandy et al. [3] describe an interface for combining Fortran M and HPF to achieve the integration of task and data parallelism. Their approach differs from our own and Subhloks in that the mechanism used is actually two separate compilers, one task parallel and one data parallel.
Reference: [6] <editor> G.C. Fox et al., </editor> <title> Fortran D Language Specifications, </title> <type> Technical Report SCCS 42c, </type> <institution> NPAC, Syracuse University, Syracuse, NY. </institution>
Reference-contexts: Specifying a data parallel computation in terms of a single element is the approach we have used in creating our data parallel extensions. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 4, 6, 13, 15, 16, 17, 20, 21] </ref>. However, we have extended the notion to encompass subset level data parallelism. By subset level data parallelism we mean allowing the definition of operations in which subsets (as opposed to elements) are the data granules, e.g., a row or a column. <p> Figure 6 illustrates both task and data parallel invocations within the same code fragment. 5: Related Work In terms of purely data parallel languages, Dataparallel C [13, 20, 21], pC++ [2, 16], C** [15], Fortran D <ref> [6] </ref>, Fortran 90 [1], and High Performance Fortran (HPF) [17] are the languages from which are related to our work. C** and pC++ are based on C++. Dataparallel C is based on C, but uses some ideas from object-oriented language design. HPFs origin is Fortran.
Reference: [7] <author> D. Gannon, </author> <type> personal communications, </type> <year> 1993, 1994 </year>
Reference-contexts: These examples are by no means exhaustive - merely representative. One difficulty with identifying applications is that the tools available inuence how, and which, applications are coded. Recent activity in the field of combined task and data parallel language design <ref> [3, 5, 7, 22] </ref> is further evidence of the importance of the problem. <p> Both Subhlok and Foster and Chandy have produced preliminary performance data. The overriding difference between our work and that mentioned above is our basis in C++ and the object-oriented approach. We are aware of efforts by Gannon <ref> [7] </ref> and Quinn [22]. The base languages of these projects are C++ and C respectively, however, no results combining task and data parallelism have been reported at the time of this writing. 6: Status and Future Work Braid integrates both task and data parallelism into a single language design.
Reference: [8] <author> A.S. Grimshaw, E.A. West, W. Pearson, </author> <title> No Pain and Gain! - Experiences with Mentat on a Biological Application, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(4), </volume> <month> June </month> <year> 1993, </year> <pages> pp. 309-328. </pages>
Reference-contexts: However, one must ask if this example is unique, or whether there exists a group of applications for which a mixed language is the solution. Multi-disciplinary optimization problems contain data parallelism within a larger problem. For example, DNA protein sequence comparison <ref> [8] </ref> contains this type of parallelism. These examples are by no means exhaustive - merely representative. One difficulty with identifying applications is that the tools available inuence how, and which, applications are coded. <p> In [27] the language is described in detail, as are the translations of the data parallel extensions to the underlying task parallel run-time system. The current version of the MPL compiler is operational and has been used to develop a number of real-world applications <ref> [8, 10] </ref>. While hand translations have been performed for Braids language features, the data parallel portion of the compiler is not yet complete.
Reference: [9] <author> A.S. Grimshaw, </author> <title> Easy to Use Object-Oriented Parallel Programming with Mentat, </title> <booktitle> IEEE Computer, </booktitle> <month> May, </month> <year> 1993, </year> <pages> pp. 39-51. </pages>
Reference: [10] <author> A.S. Grimshaw, W.T. Strayer, and P. Narayan, </author> <title> Dynamic, Object-Oriented Parallel Processing, </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 1(2), </volume> <month> May </month> <year> 1993, </year> <pages> pp. 33-47. </pages>
Reference-contexts: In [27] the language is described in detail, as are the translations of the data parallel extensions to the underlying task parallel run-time system. The current version of the MPL compiler is operational and has been used to develop a number of real-world applications <ref> [8, 10] </ref>. While hand translations have been performed for Braids language features, the data parallel portion of the compiler is not yet complete.
Reference: [11] <author> A.S. Grimshaw, </author> <title> The Mentat Computation Model - Data-Driven Support for Dynamic Object-Oriented Parallel Processing, </title> <type> Technical Report CS-93-30, </type> <institution> University of Virginia, Computer Science Department, </institution> <address> Charlottesville, VA, </address> <year> 1993. </year>
Reference-contexts: task and data parallel objects, and second, management of concurrent data parallel objects by a task parallel manager. 4 From an implementation perspective, the Mentat Run-Time System monitors the use of results of mentat object member function invocations of task and data parallel objects and constructs medium grain program graphs <ref> [11, 12] </ref>. We demonstrate the effect in Figure 5 and Figure 6. The first example illustrates the use of encapsulated data parallelism in which a task parallel object member function invokes a data parallel object in the course of execution. This encapsulated data parallelism is completely hidden from the invoker. <p> The motivation for the work is exploring the trade-off between pure data parallelism and groups of pipelined data parallel operations organized into tasks. Our language differs in that task parallel loops are not restricted to constant bounds and communication is implicit. Further, our underlying model, MDF <ref> [11] </ref>, supports both dynamic program graphs and persistence rather than a static pure data mode. In terms of data parallelism, our work differs as mentioned above with respect to the various Fortran dialects. Finally, Subhlok et al. are adding task parallel constructs to existing data parallel language mechanisms.
Reference: [12] <author> A.S. Grimshaw, J.B. Weissman, and W.T. Strayer, </author> <title> Portable Run-Time Support for Dynamic Object-Oriented Parallel Processing, </title> <note> to appear ACM Transactions on Computer Systems. </note>
Reference-contexts: task and data parallel objects, and second, management of concurrent data parallel objects by a task parallel manager. 4 From an implementation perspective, the Mentat Run-Time System monitors the use of results of mentat object member function invocations of task and data parallel objects and constructs medium grain program graphs <ref> [11, 12] </ref>. We demonstrate the effect in Figure 5 and Figure 6. The first example illustrates the use of encapsulated data parallelism in which a task parallel object member function invokes a data parallel object in the course of execution. This encapsulated data parallelism is completely hidden from the invoker.
Reference: [13] <author> P.J. Hatcher et al., </author> <title> Compiling Data-Parallel Programs for MIMD Architectures, </title> <booktitle> European Workshop on Parallel Computing, </booktitle> <address> March 1992, Barcelona, Spain. </address>
Reference-contexts: Specifying a data parallel computation in terms of a single element is the approach we have used in creating our data parallel extensions. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 4, 6, 13, 15, 16, 17, 20, 21] </ref>. However, we have extended the notion to encompass subset level data parallelism. By subset level data parallelism we mean allowing the definition of operations in which subsets (as opposed to elements) are the data granules, e.g., a row or a column. <p> Furthermore, the entire code block itself could be a mentat object member function implementation executing concurrently with other mentat object member functions. Figure 6 illustrates both task and data parallel invocations within the same code fragment. 5: Related Work In terms of purely data parallel languages, Dataparallel C <ref> [13, 20, 21] </ref>, pC++ [2, 16], C** [15], Fortran D [6], Fortran 90 [1], and High Performance Fortran (HPF) [17] are the languages from which are related to our work. C** and pC++ are based on C++.
Reference: [14] <author> J.F. Karpovich et al., </author> <title> A Parallel Object-Oriented Framework for Stencil Algorithms, </title> <booktitle> Proceedings of the Second Symposium on High-Performance Distributed Computing, </booktitle> <address> July, 1993, Spokane, WA, </address> <pages> pp. 34-41. </pages>
Reference: [15] <author> J.R. Larus, B. Richards, and G. Viswanathan, </author> <title> C**: A Large-Grain, Object-Oriented, Data-Parallel Programming Language, </title> <type> Technical Report 1126, </type> <institution> University of Wisconsin, Computer Science Department, Madison, Wisconsin, </institution> <year> 1992. </year>
Reference-contexts: Specifying a data parallel computation in terms of a single element is the approach we have used in creating our data parallel extensions. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 4, 6, 13, 15, 16, 17, 20, 21] </ref>. However, we have extended the notion to encompass subset level data parallelism. By subset level data parallelism we mean allowing the definition of operations in which subsets (as opposed to elements) are the data granules, e.g., a row or a column. <p> Figure 6 illustrates both task and data parallel invocations within the same code fragment. 5: Related Work In terms of purely data parallel languages, Dataparallel C [13, 20, 21], pC++ [2, 16], C** <ref> [15] </ref>, Fortran D [6], Fortran 90 [1], and High Performance Fortran (HPF) [17] are the languages from which are related to our work. C** and pC++ are based on C++. Dataparallel C is based on C, but uses some ideas from object-oriented language design. HPFs origin is Fortran.
Reference: [16] <author> J.K. Lee and D. Gannon, </author> <title> Object Oriented Parallel Programming Experiments and Results, </title> <booktitle> Proceedings of Supercomputing 91, 1991, </booktitle> <address> Albuquerque, NM, </address> <pages> pp. 273-282. </pages>
Reference-contexts: Specifying a data parallel computation in terms of a single element is the approach we have used in creating our data parallel extensions. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 4, 6, 13, 15, 16, 17, 20, 21] </ref>. However, we have extended the notion to encompass subset level data parallelism. By subset level data parallelism we mean allowing the definition of operations in which subsets (as opposed to elements) are the data granules, e.g., a row or a column. <p> Figure 6 illustrates both task and data parallel invocations within the same code fragment. 5: Related Work In terms of purely data parallel languages, Dataparallel C [13, 20, 21], pC++ <ref> [2, 16] </ref>, C** [15], Fortran D [6], Fortran 90 [1], and High Performance Fortran (HPF) [17] are the languages from which are related to our work. C** and pC++ are based on C++. Dataparallel C is based on C, but uses some ideas from object-oriented language design.
Reference: [17] <author> D.B. Loveman, </author> <title> High Performance Fortran, </title> <journal> IEEE Parallel & Distributed Technology: Systems & Applications, </journal> <volume> Vol. 1, No. 1, </volume> <month> Feb., </month> <year> 1993, </year> <pages> pp. 25-42. </pages>
Reference-contexts: Specifying a data parallel computation in terms of a single element is the approach we have used in creating our data parallel extensions. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 4, 6, 13, 15, 16, 17, 20, 21] </ref>. However, we have extended the notion to encompass subset level data parallelism. By subset level data parallelism we mean allowing the definition of operations in which subsets (as opposed to elements) are the data granules, e.g., a row or a column. <p> Figure 6 illustrates both task and data parallel invocations within the same code fragment. 5: Related Work In terms of purely data parallel languages, Dataparallel C [13, 20, 21], pC++ [2, 16], C** [15], Fortran D [6], Fortran 90 [1], and High Performance Fortran (HPF) <ref> [17] </ref> are the languages from which are related to our work. C** and pC++ are based on C++. Dataparallel C is based on C, but uses some ideas from object-oriented language design. HPFs origin is Fortran.
Reference: [18] <author> C.R. Mechoso, J.D. Farrara, J.A. Spahr, </author> <title> Running a Climate Model in a Heterogeneous, Distributed Computer Environment, </title> <booktitle> Proceedings of the Third IEEE International Symposium on High Performance Distributed Computing, </booktitle> <address> April 2-5, 1994, San Francisco, California, </address> <pages> pp. 79-84. </pages>
Reference-contexts: If one type of parallelism dominates the computation, then a language to exploit that parallelism is called for and the other type can be ignored. However, for applications containing a balance of task and data parallelism the choice of language is less clear. As an example, global climate modeling <ref> [18] </ref> involves the interaction of distinct oceanic and atmospheric weather models. The two resulting PDEs can be individually solved in a data parallel fashion by superimposing a grid upon the space to be modeled. Each grid location is then an item in a data set.
Reference: [19] <author> Mentat Research Group, </author> <title> Mentat 2.5 Programming Language Reference Manual, </title> <type> Technical Report CS-94-05, </type> <institution> University of Virginia, Department of Computer Science, </institution> <address> Charlottesville, VA, </address> <year> 1994. </year>
Reference-contexts: Mentat has been ported to a variety of MIMD platforms and has been used to implement real-world applications in industry, government, and academia. The Mentat Programming Language (MPL) <ref> [19] </ref> is a task parallel language based on C++ [23]. The Mentat approach exploits the object-oriented paradigm to provide high-level abstractions that mask the complex aspects of parallel programming, communication, synchronization, and scheduling from the programmer.
Reference: [20] <author> N. Nedeljkovic and M.J. Quinn, </author> <title> Data-Parallel Programming on a Network of Heterogeneous Workstations, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(4), </volume> <month> June </month> <year> 1993, </year> <month> pp.257-268. </month>
Reference-contexts: Specifying a data parallel computation in terms of a single element is the approach we have used in creating our data parallel extensions. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 4, 6, 13, 15, 16, 17, 20, 21] </ref>. However, we have extended the notion to encompass subset level data parallelism. By subset level data parallelism we mean allowing the definition of operations in which subsets (as opposed to elements) are the data granules, e.g., a row or a column. <p> Furthermore, the entire code block itself could be a mentat object member function implementation executing concurrently with other mentat object member functions. Figure 6 illustrates both task and data parallel invocations within the same code fragment. 5: Related Work In terms of purely data parallel languages, Dataparallel C <ref> [13, 20, 21] </ref>, pC++ [2, 16], C** [15], Fortran D [6], Fortran 90 [1], and High Performance Fortran (HPF) [17] are the languages from which are related to our work. C** and pC++ are based on C++.
Reference: [21] <author> M.J. Quinn and P.J. Hatcher, </author> <title> Data-Parallel Programming on Multicomputers, </title> <journal> IEEE Software, </journal> <month> Sept. </month> <year> 1990, </year> <pages> pp. 69-76. </pages>
Reference-contexts: Specifying a data parallel computation in terms of a single element is the approach we have used in creating our data parallel extensions. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 4, 6, 13, 15, 16, 17, 20, 21] </ref>. However, we have extended the notion to encompass subset level data parallelism. By subset level data parallelism we mean allowing the definition of operations in which subsets (as opposed to elements) are the data granules, e.g., a row or a column. <p> The starting point is the element to which the operation is being applied, and the relative addresses are resolved at run-time. For example, W ()-&gt;pixel, or W ()-&gt;S ()- &gt;pixel. The programmer may also specify boundary conditions as well. These mechanisms are similar to <ref> [21] </ref>. Invocation of an aggregate operation is shown on lines 18-21 of Figure 1. <p> Furthermore, the entire code block itself could be a mentat object member function implementation executing concurrently with other mentat object member functions. Figure 6 illustrates both task and data parallel invocations within the same code fragment. 5: Related Work In terms of purely data parallel languages, Dataparallel C <ref> [13, 20, 21] </ref>, pC++ [2, 16], C** [15], Fortran D [6], Fortran 90 [1], and High Performance Fortran (HPF) [17] are the languages from which are related to our work. C** and pC++ are based on C++.
Reference: [22] <author> M. J. Quinn, </author> <type> personal communication, </type> <year> 1992. </year>
Reference-contexts: These examples are by no means exhaustive - merely representative. One difficulty with identifying applications is that the tools available inuence how, and which, applications are coded. Recent activity in the field of combined task and data parallel language design <ref> [3, 5, 7, 22] </ref> is further evidence of the importance of the problem. <p> Both Subhlok and Foster and Chandy have produced preliminary performance data. The overriding difference between our work and that mentioned above is our basis in C++ and the object-oriented approach. We are aware of efforts by Gannon [7] and Quinn <ref> [22] </ref>. The base languages of these projects are C++ and C respectively, however, no results combining task and data parallelism have been reported at the time of this writing. 6: Status and Future Work Braid integrates both task and data parallelism into a single language design.
Reference: [23] <author> B. Stroustrup, </author> <title> The C++ Programming Language, 2nd ed. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1991. </year>
Reference-contexts: Mentat has been ported to a variety of MIMD platforms and has been used to implement real-world applications in industry, government, and academia. The Mentat Programming Language (MPL) [19] is a task parallel language based on C++ <ref> [23] </ref>. The Mentat approach exploits the object-oriented paradigm to provide high-level abstractions that mask the complex aspects of parallel programming, communication, synchronization, and scheduling from the programmer. The programmer uses application domain knowledge to specify those object classes that are of sufficient computational complexity to warrant parallel execution.
Reference: [24] <author> J. Subhlok, J.M. Stichnoth, D.R. OHallaron, and T. Gross, </author> <title> Exploiting Task and Data Parallelism on a Multicomputer, </title> <booktitle> Proceedings of the 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> May, 1993, San Diego, CA, </address> <pages> pp. 13-22. </pages>
Reference-contexts: The graph is constructed at run-time using compiler generated information. A C E defined data parallel operations. Currently, there are two groups which have reported work on integrated task and data parallelism. Both are Fortran based. Subhlok et al. <ref> [24] </ref> describe a parallelizing Fortran 77 based compiler augmented with Fortran 90, Fortran D and HPF constructs whose target is the iWarp system. The motivation for the work is exploring the trade-off between pure data parallelism and groups of pipelined data parallel operations organized into tasks.
Reference: [25] <author> J.B. </author> <title> Weissman and A.S. Grimshaw, Multigranular Scheduling of Data Parallel Programs, </title> <type> Technical Report CS-93-38, </type> <institution> University of Virginia, Department of Computer Science, </institution> <address> Charlottesville, VA, </address> <month> July, </month> <year> 1993. </year>
Reference-contexts: Joint research is now being conducted within the Mentat group to allow the run-time system to combine the compiler generated information with information about the current machine architecture. The run-time system will employ heuristic algorithms to automatically handle the decomposition, distribution and alignment of the data parallel object <ref> [25, 26] </ref>. Cooperating with the run-time system in this manner allows us to divorce ourselves from the underlying machine architecture. This exibility is critical in order to exploit a heterogeneous system architecture.
Reference: [26] <author> J.B. </author> <title> Weissman and A.S. Grimshaw, Network Partitioning of Data Parallel Computations, </title> <booktitle> Proceedings of the Symposium on High-Performance Distributed Computing (HPDC-3), August, 1994, </booktitle> <address> San Francisco, CA, </address> <pages> pp. 149-156. </pages>
Reference-contexts: Joint research is now being conducted within the Mentat group to allow the run-time system to combine the compiler generated information with information about the current machine architecture. The run-time system will employ heuristic algorithms to automatically handle the decomposition, distribution and alignment of the data parallel object <ref> [25, 26] </ref>. Cooperating with the run-time system in this manner allows us to divorce ourselves from the underlying machine architecture. This exibility is critical in order to exploit a heterogeneous system architecture.
Reference: [27] <author> E. A. West, </author> <title> Combining Control and Data Parallelism: Data Parallel Extensions to the Mentat Programming Language,, </title> <type> Technical Report CS-94-16, </type> <institution> University of Virginia, Department of Computer Science, </institution> <address> Charlottesville, VA, </address> <month> May, </month> <year> 1994. </year> <title> Information regarding current work on Braid can be found at http://uvacs.cs.virginia.edu/~eaw2t/Braid.html. For information on Mentat and the Mentat Programming Language see http://uvacs.cs.virginia.edu/~mentat/. </title>
Reference-contexts: Rather the programmer is allowed to define complex data parallel operations for a data set. In this paper we focus on the language design. Complete language details and a description of the translations from the source language to the target task parallel run-time system can be found in <ref> [27] </ref>. We begin with background material on data parallel computation and Mentat to set the context for this work. The extensions are then presented, followed by examples that illustrate the integration of task and data parallelism. <p> Additionally, we have introduced the idea of subset parallelism which allows the user to define complex data parallel operations across subsets of a data set. In <ref> [27] </ref> the language is described in detail, as are the translations of the data parallel extensions to the underlying task parallel run-time system. The current version of the MPL compiler is operational and has been used to develop a number of real-world applications [8, 10].
References-found: 27

