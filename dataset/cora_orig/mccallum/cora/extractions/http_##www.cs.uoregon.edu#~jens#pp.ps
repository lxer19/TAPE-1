URL: http://www.cs.uoregon.edu/~jens/pp.ps
Refering-URL: http://www.cs.uoregon.edu/~jens/research.html
Root-URL: http://www.cs.uoregon.edu
Title: Latency avoiding by adequate mapping  
Author: Jens Mache 
Date: December 9, 1994  
Abstract: But let's be careful: Mapping affects the load balance and the number of remote accesses (with the latter causing latency). This paper discusses adequate mapping for two well-known algorithms (summing an array, matrix-vector multiplication) and shows the resulting effects (using pC++ and the TAU environment). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Babb II, B. Bose, W. Harrison, T. Lewis, V. Lo, S. Rajopadhye, W. Rudd and S. Thakkar. </author> <title> Mapping divide-and-conquer algorithms to parallel architectures. </title> <institution> Oregon Advanced Computing Institute, </institution> <year> 1989 </year>
Reference-contexts: Yes, adequate mapping can minimize remote accesses and therefore avoid latency. I will show this using two well known algoritms, summing an array and matrix-vector multiplication. 1.5 Current research in mapping Most of the current research discusses how to map special algorithms onto special architectures (e.g. <ref> [1, 3] </ref>) or even tries to automate this (see [2]). Mapping in these cases is mapping to hypercubes, meshes, fat trees or what not. And it's often a one-to-one mapping, assuming we have as many processors as we would like to have.
Reference: [2] <author> V. Lo, S. Rajopadhye, S. Gupta, D. Keldsen, M. A. Mohamed, B. Nitzberg, J. A. Telle and X. Zhong. OREGAMI: </author> <title> Tools for mapping parallel computations to parallel architectures. </title> <booktitle> In International Journal of Parallel Programming, </booktitle> <pages> pages 237-270, </pages> <note> No. 3, </note> <year> 1991 </year>
Reference-contexts: I will show this using two well known algoritms, summing an array and matrix-vector multiplication. 1.5 Current research in mapping Most of the current research discusses how to map special algorithms onto special architectures (e.g. [1, 3]) or even tries to automate this (see <ref> [2] </ref>). Mapping in these cases is mapping to hypercubes, meshes, fat trees or what not. And it's often a one-to-one mapping, assuming we have as many processors as we would like to have. This paper is different: I will discuss mapping that is many-to-one and high-level.
Reference: [3] <author> V. Kumar, A. Grama, A. Gupta and G. Karypis. </author> <title> Introduction to parallel computing, design and analysis of algorithms. </title> <address> Benjamin/Cummings, </address> <year> 1994 </year>
Reference-contexts: Yes, adequate mapping can minimize remote accesses and therefore avoid latency. I will show this using two well known algoritms, summing an array and matrix-vector multiplication. 1.5 Current research in mapping Most of the current research discusses how to map special algorithms onto special architectures (e.g. <ref> [1, 3] </ref>) or even tries to automate this (see [2]). Mapping in these cases is mapping to hypercubes, meshes, fat trees or what not. And it's often a one-to-one mapping, assuming we have as many processors as we would like to have.
Reference: [4] <author> D. Gannon, S. Yang, P. Beckman. </author> <title> User guide for a portable parallel C++ programming system, pC++. </title> <institution> Indiana University, </institution> <year> 1994 </year>
Reference: [5] <author> B. Mohr, D. Brown, A. Malony. </author> <title> TAU: a portable parallel program analysis environment for pC++. </title> <institution> University of Oregon, </institution> <year> 1994 </year>
References-found: 5

