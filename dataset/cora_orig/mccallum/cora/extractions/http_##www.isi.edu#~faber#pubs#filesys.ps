URL: http://www.isi.edu/~faber/pubs/filesys.ps
Refering-URL: http://www.isi.edu/div7/atomic2/server_solutions.html
Root-URL: http://www.isi.edu
Title: Optimizing Throughput in a Workstation-based Network File System over a High Bandwidth Local Area Network 1  
Author: Theodore Faber 
Keyword: Distributed Systems, Network File Systems, High Speed LANs  
Address: 4676 Admiralty Way Marina del Rey, CA 90292  
Affiliation: University of Southern California/Information Sciences Institute  
Note: Draft: Please do not distribute!  
Email: faber@isi.edu  
Phone: Phone: 310-821-5080 x190 FAX: 310-823-6714  
Abstract: This paper describes methods of optimizing a client/server file system to take advantage of high bandwidth local area networks in a conventional distributed computing environment. The environment contains hardware that removes network and disk bandwidth bottlenecks. The specific bottlenecks at clients include excessive context switching, inefficient data translation, and cumbersome data encapsulation methods. When these bottlenecks are removed, the protocol write performance of a current implementation of Suns Network File System improves by 30%. Not all optimizations proposed have been implemented, and other experimental evidence suggests that NFS protocol performance can be increased further. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sun Microsystems, Inc., </author> <title> Network Filesystem Specification, </title> <address> RFC-1094, March 1, </address> <year> 1989. </year>
Reference-contexts: In most configurations, the there are signif icant I/O bottlenecks in the form of disks at the server and the network connecting them to the cli ent. Figure 1 shows several bottlenecks in an NFS system. When the server disk is written synchronously, as required by NFS version 2 <ref> [1] </ref>, the bandwidth is restricted to about 1.5 Mbps. Us ing the server disk asynchronously is more efficient 1 , but even when the server does not write to the disk, this implementation of NFS transfers data at only 46 Mbps. The network is not the bot 1.
Reference: [2] <author> Sun Microsystems, Inc., NFS: </author> <title> Network File System Version 3 Protocol Specification, Sun Microsystems, </title> <publisher> Inc., </publisher> <address> Mountain View, CA, </address> <month> Feb 16, </month> <year> 1994. </year>
Reference-contexts: Us ing the server disk asynchronously is more efficient 1 , but even when the server does not write to the disk, this implementation of NFS transfers data at only 46 Mbps. The network is not the bot 1. This is a loose estimate of NFS version 3 <ref> [2] </ref> bandwidth, which allows asynchronous writes. All values are averages of 25 writes of a 64 megabyte (MB) file between two SparcStation 20/71 workstations running SunOS 4.1.3 connected via a Myrinet.
Reference: [3] <author> Robert Felderman, Annette DeSchon, Danny Cohen, and Gregory Finn, </author> <title> Atomic: A High Speed Local Communication Architecture, </title> <journal> Journal of High Speed Networks, </journal> <volume> vol. 3, </volume> <pages> pp. 1-29, </pages> <year> 1994. </year>
Reference-contexts: Reducing context switches, removing unnec essary data translations, and avoiding heavyweight data encapsulation methods resulted in a 30% increase in Sun Network File System [1,2] (NFS) protocol write bandwidth using the ATOMIC LAN <ref> [3] </ref>. The use of the ATOMIC LAN changes the file system throughput bottleneck from net work bandwidth to client processor cycles, which makes these optimizations effective. 1. This work is supported by the Advanced Research Projects Agency through Ft. Huachuca contract #DABT63-93-C-0062 entitled Netstation Architecture and Advanced Atomic Network.
Reference: [4] <author> J. H. Howard, M. L. Lazar, S. G. Menees, D. A. Nichols, M. Satyanarayanan, R. N. Sidebotham, and M. J. West, </author> <title> Scale and Performance in a Distributed File System, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 6(1), </volume> <pages> pp. 51-81, </pages> <publisher> ACM, </publisher> <month> February </month> <year> 1988. </year>
Reference-contexts: Optimizing the request/response paradigm offers similar if not superior opportunity for improvement. The goal of the ATOMIC file server is to scale this conventional software architecture along the bandwidth axis. Other systems drawn from this architecture, for example NFS [1,2], the Andrew File System (AFS) <ref> [4] </ref>, the Sprite file system [5], the Spring file system [8,13], and Ficus [14] concentrated their efforts on adding functionality to the file system, and optimizing other parameters. AFS added global availability and security enhancements, and used caching to reduce network usage. Sprite concentrates on transparency and caching behavior.
Reference: [5] <author> Michael Nelson, Brent Welch, John Osterhout, </author> <title> Caching in the Sprite Network File System, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 6, no. 1, </volume> <pages> pp. 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The goal of the ATOMIC file server is to scale this conventional software architecture along the bandwidth axis. Other systems drawn from this architecture, for example NFS [1,2], the Andrew File System (AFS) [4], the Sprite file system <ref> [5] </ref>, the Spring file system [8,13], and Ficus [14] concentrated their efforts on adding functionality to the file system, and optimizing other parameters. AFS added global availability and security enhancements, and used caching to reduce network usage. Sprite concentrates on transparency and caching behavior.
Reference: [6] <author> T. Anderson, M. Dahlin, J. Neefe, D. Patterson, D. Roselli, R. Wang, </author> <title> Serverlesss Network File Systems, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 14, No. 1, </volume> <month> February </month> <year> 1996, </year> <pages> pp 41-79. </pages>
Reference: [7] <author> John F. Karpovich, Andrew S. Grimshaw, and James C. </author> <title> French, Extensible File Systems (ELFS): An Object-Oriented Approach to High Performance File I/O, </title> <booktitle> Proceedings of the Ninth Annual Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <month> October </month> <year> 1994. </year>
Reference: [8] <author> Michael N. Nelson, Yousef A. Khalidi, Peter W. Madany, </author> <title> The Spring File System, </title> <institution> Sun Microsystems Technical Report SMLI TR-93-10, </institution> <month> February </month> <year> 1993. </year>
Reference: [9] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Selzovic, Wen-King Su, and Myricom, Inc., Myrinet: </author> <title> A Gigabit-per-second Local Area Network, </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 29-36, </pages> <publisher> IEEE, </publisher> <month> February </month> <year> 1995. </year>
Reference-contexts: The testbed architecture uses a centralized fast cache server architecture, consisting of a Sun SparcStation 20/71 supporting a Texas Memory Systems SAM-300 RAM disk, used as primary file storage. The client workstations, other SparcStations, are connected to the server 4 Draft: Please do not distribute! via a Myrinet <ref> [9] </ref>, Myricoms commercial implementation of the ATOMIC LAN. The ATOMIC testbed has removed the hardware bottlenecks to remote file access at network bandwidth, and revealed software bottlenecks in NFS.
Reference: [10] <author> P. Chen, E. Lee, G. Gibson, R. Katz, D. Patterson, </author> <title> RAID: High Performance, Reliable Secondary Storage, </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 26, no. 2, </volume> <pages> pp. 145-186, </pages> <month> June </month> <year> 1994. </year> <title> 19 Draft: Please do not distribute! </title>
Reference-contexts: The workload is much burstier than a disk hooked to a supercomputing cluster, and files are smaller. Short files limit the applicability of striping solutions, such as RAID-based systems <ref> [10] </ref>. Furthermore, complex caching and file location algorithms cut into the bottleneck resource, client CPU cycles. <p> The ATOMIC file server is the first to scale this paradigm for throughput. Other file systems have been scaled for throughput, but not in our environment. The Extensible File System (ELFS)[7], NFS/bds [12], and numerous RAID-based systems <ref> [10] </ref> are examples of high bandwidth systems. These systems are in use in supercomputer environments, which have less bursty workloads, and hosts with more powerful CPUs.
Reference: [11] <author> J. Hartman and J. Osterhaut, </author> <title> The Zebra Striped Network File System, </title> <booktitle> Proc. of the 14th ACM Symposium on Operating System Principles, Asheville, NC, </booktitle> <pages> pp. 29-43. </pages>
Reference-contexts: Like most network file systems in use today, the ISI server adopts a client/server model of the file system for simplicity and efficiency of CPU utilization. There are other models being proposed for network file systems, including xFS [6]which is a serverless system, and Zebra <ref> [11] </ref> which uses log-structured software striping. Since CPU cycles at the client are the bottleneck in the ATOMIC environment, the techniques in this paper should be applicable to xFS or Zebra clients. The request/response paradigm is used for data transfer.
Reference: [12] <author> L. McVoy, </author> <note> NFS/bds - NFS goes to the gym, December 1995, http://reality.sgi.com/lm/talks/bds.ps </note>
Reference-contexts: Another data transfer interface that has been advocated is the use of a streaming data protocol such as TCP to stream files to the client, rather 6 Draft: Please do not distribute! than requesting individual disk blocks. This technique, used by NFS/bds <ref> [12] </ref>, has shown promise, especially on machines using very powerful CPUs. The added overhead of converting the data from an undelimited byte stream to operating system structures and potentially re-aligning data uses more of the client CPU than the workstations in the ATOMIC environment can spare. <p> The Spring system and Ficus provide general extensibility mechanisms, and explored reliability and replication. The ATOMIC file server is the first to scale this paradigm for throughput. Other file systems have been scaled for throughput, but not in our environment. The Extensible File System (ELFS)[7], NFS/bds <ref> [12] </ref>, and numerous RAID-based systems [10] are examples of high bandwidth systems. These systems are in use in supercomputer environments, which have less bursty workloads, and hosts with more powerful CPUs.
Reference: [13] <author> Y. Khalidi and M. Nelson, </author> <title> Extensible File Systems in Spring, </title> <booktitle> Proc. of the 14th ACM Symposium on Operating System Principles, </booktitle> <address> Asheville, NC, pp.1-14. </address>
Reference: [14] <author> J. Heidemann and G. Popek, </author> <title> File system development with Stackable Layers, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 12, no. 1, </volume> <pages> pp. 58-89, </pages> <year> 1994. </year>
Reference-contexts: The goal of the ATOMIC file server is to scale this conventional software architecture along the bandwidth axis. Other systems drawn from this architecture, for example NFS [1,2], the Andrew File System (AFS) [4], the Sprite file system [5], the Spring file system [8,13], and Ficus <ref> [14] </ref> concentrated their efforts on adding functionality to the file system, and optimizing other parameters. AFS added global availability and security enhancements, and used caching to reduce network usage. Sprite concentrates on transparency and caching behavior. The Spring system and Ficus provide general extensibility mechanisms, and explored reliability and replication.
Reference: [15] <author> Sun Microsystems, Inc., </author> <title> Remote Procedure Call Specification, </title> <address> RFC-1057, </address> <month> June 1, </month> <year> 1988. </year>
Reference-contexts: Using an alternate implementation of the data transfer protocol reduced context switches by 40%. In NFS, data transfers are effected via a remote procedure call protocol specifically, Suns RPC protocol <ref> [15] </ref>. The implementation in SunOS 4.1.3 allows only one fixed-size request to be pending, yet optimizing for throughput requires making multiple data requests to fill the bandwidth/delay pipeline between the server and client. <p> Creating such a data transfer mechanism and placing it in SunOS was straightforward, especially since Sun RPC is designed to allow such an implementation <ref> [15] </ref>. The replacement protocol sends and receives standard Sun RPC messages, using the transaction ID to associate requests with responses. NFS was modified to use our single process RPC implementation instead of biod processes.
Reference: [16] <institution> Transarc Corporation, AFS Systems Administrators Guide, Transarc Corporation, </institution> <address> Pittsburgh, Pa., </address> <year> 1993. </year>
Reference-contexts: This is done by using multiple processes, called biod or nfsiod processes, each running an instantiation of the RPC protocol. This is a common technique for file systems and other transaction-based services. AFS uses such processes, called afsd processes <ref> [16] </ref>, and the Apache Web Server uses a distinct process to service each request [17]. To reduce the context switching overhead of using multiple processes, an implementation of Sun RPC that runs in a single process and allows multiple requests to be outstanding was implemented.
Reference: [17] <author> Apache Server Documentation, </author> <month> &lt;http://www.apache.org/docs/process-model.html&gt; </month>
Reference-contexts: This is a common technique for file systems and other transaction-based services. AFS uses such processes, called afsd processes [16], and the Apache Web Server uses a distinct process to service each request <ref> [17] </ref>. To reduce the context switching overhead of using multiple processes, an implementation of Sun RPC that runs in a single process and allows multiple requests to be outstanding was implemented.
Reference: [18] <author> P. Druschel and L. Peterson, Fbufs: </author> <title> A High-Bandwidth Cross-Domain Transfer Facility, </title> <booktitle> Proc. of the 14th ACM Symposium on Operating System Principles, Asheville, NC, </booktitle> <pages> pp. 189-202, </pages> <year> December1993. </year>
Reference-contexts: All kernel services can benefit from a common data representation <ref> [18] </ref>. Unnecessary copies to translate between incompatible data structures chokes the performance of network file systems, and of any service that transports data. This problem is not unique to SunOS, or even Unix operat ing systems; inter-service communication costs are a frequent bottleneck in other systems as well [19].
Reference: [19] <author> J. B. Chen, Y. Endo, K. Chan, D. Mazires, A. Dias, M. Seltzer, and M. Smith, </author> <title> The Measured Performance of Personal Computer Operating Systems, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 14, No. 1, </volume> <pages> pp 3-40, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Unnecessary copies to translate between incompatible data structures chokes the performance of network file systems, and of any service that transports data. This problem is not unique to SunOS, or even Unix operat ing systems; inter-service communication costs are a frequent bottleneck in other systems as well <ref> [19] </ref>. Continuing to patch sections of code will not solve this fundamental problem. Network services must be integrated more closely with other operating system services. Averages of 25 writes of a 64 MB file using various buffer sizes in the write system call.
Reference: [20] <author> R. Srinivasan, XDR: </author> <title> External Data Representation Standard, </title> <address> RFC-1832, </address> <month> August, </month> <year> 1995. </year>
Reference-contexts: In NFS, this function is provided by the external data representation (XDR) standard <ref> [20] </ref>. Although streamlining XDR throughout our entire NFS implementation was not attempted, evidence exists that simpler methods can yield significant gains. Under SunOS 4.1.3, XDR is implemented as a very general set of routines which provide a broad range of encapsulation options [21]. For NFS, however, this is excessive.
Reference: [21] <author> Sun Microsystems, </author> <title> Network Programming Guide, Sun Microsystems, </title> <address> Mountain View, CA, </address> <pages> pp. 103-129, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Although streamlining XDR throughout our entire NFS implementation was not attempted, evidence exists that simpler methods can yield significant gains. Under SunOS 4.1.3, XDR is implemented as a very general set of routines which provide a broad range of encapsulation options <ref> [21] </ref>. For NFS, however, this is excessive. A file system needs to communicate simple data structures, mainly integers and opaque data. Encapsulating such data can be done by putting the integers in network standard byte order and agreeing on their order. Opaque data is unencapsulated by definition.
Reference: [22] <author> R. Macklem, </author> <title> Lessons Learned Tuning the 4.3 BSD Reno Implementation of the NFS Protocol, </title> <booktitle> Proc. of the Winter 1991 USENIX Conference, </booktitle> <address> Dallas, TX, </address> <pages> pp. 53-64, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: It seems likely that optimizing the XDR code in NFS should increase NFS throughput further. BSD 4.3 Reno and later releases have converted XDR and RPC routines to macros and report reductions in server CPU utilization <ref> [22] </ref>. In our case that experiment was not undertaken, because it would necessitate a major rewrite of NFS. Removing this bottleneck should make the effects of the con text switch reduction optimizations in Section 3. 1 more visible.
Reference: [23] <author> S. Hotz, </author> <type> personal communication, </type> <institution> hotz@isi.edu. </institution>
Reference-contexts: RPC protocols implemented on this network interface processor have been shown to be extremely efficient. The DTP protocol, developed at ISI, is capable of 30,000 null RPC calls per second running on the Myricom host interface <ref> [23] </ref>. By balancing the load of file system access and network protocol implementation, a very efficient server architecture can be created. The implementation of such a server is future work. 5.
Reference: [24] <author> D. Hitz, G. Harris, J. Lau, and A. Schwartz, </author> <title> Using Unix as One Component of a Lightweight Distributed Kernel for Multiprocessor File Servers, </title> <booktitle> Proc. of the Winter 1990 USENIX Conference, </booktitle> <pages> pp. 285-296, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: RAM disks like the SAM-300 are programmable, and have support for direct network connections. On-board processors can act as the file server, running tuned file system code rather than a full operating system. Auspex has demonstrated essentially this architecture in a conventional networking environment <ref> [24] </ref>, and we suggest that it will be effective in the ATOMIC environment as well. Reducing the operating system overheads that are so CPU intensive promises a lean, scalable server. Furthermore, the performance of the SAM-300 is currently limited by the Sbus interface card connecting it to the host.
References-found: 24

