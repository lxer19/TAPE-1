URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/ANL9549.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts95.htm
Root-URL: http://www.mcs.anl.gov
Title: Distribution Category: Parallel Solution of the Time-dependent Ginzburg-Landau Equations and Other Experiences Using BlockComm-Chameleon and
Author: Mathematics and by Erhan Coskun and Man Kam Kwong 
Note: IL 60115. Present address: Karadeniz Technical  2 This author was supported by the Mathematical, Information, and Computational Sciences Dvision subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38.  
Address: 9700 South Cass Avenue Argonne, IL  Trabzon, 61080 Turkey.  
Affiliation: Computer Science (UC-405) ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  1 Department of Mathematical Sciences, Northern Illinois University, DeKalb,  University, Department of Mathematics,  
Email: E-mail: erhan@osf01.bim.ktu.edu.tr  
Phone: 60439  
Date: September 1995  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. G. Babb II, </author> <title> Programming Parallel Processors, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: For each problem, the same program (recompiled with the appropriate makefiles) were used in the three systems. The Intel iPSC/860 at Argonne has eight nodes. All processor nodes are identical and are connected by bidirectional links in a hypercube topology. See <ref> [1] </ref> for its hardware and software specifications. We used this machine mainly for program development because it is freely accessible and there is no limitation on the amount of time one can work on the machine. The Argonne IBM SP 3 has 128 nodes. <p> Line 2 serves a dual purpose: the number of command line arguments is checked, and if that is equal to two, the values of argv <ref> [1] </ref> and argv [2] are assigned to n intervals and interval size. In lines 4-5, PCN's sys module is used to define ni and li to be the integer values represented by the strings n intervals and interval size, respectively.
Reference: [2] <author> K. M. Chandy and Stephen Taylor, </author> <title> An Introduction to Parallel Programming, </title> <publisher> Jones and Barlett Publishers, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: Line 2 serves a dual purpose: the number of command line arguments is checked, and if that is equal to two, the values of argv [1] and argv <ref> [2] </ref> are assigned to n intervals and interval size. In lines 4-5, PCN's sys module is used to define ni and li to be the integer values represented by the strings n intervals and interval size, respectively.
Reference: [3] <author> Erhan Coskun, </author> <title> Numerical Analysis of Ginzburg-Landau Models for Superconductivity, </title> <type> Ph.D. dissertation, </type> <institution> 1994, Northern Illinois University, DeKalb, Ill. </institution>
Reference-contexts: 1 Introduction In our study of the mathematical modeling of superconductivity, we have developed an efficient algorithm to solve numerically the time-dependent Ginzburg-Landau (TDGL) equations in two dimensions (see <ref> [3] </ref>). The corresponding problem in three dimensions is, however, very computationally extensive. The study is impractical on a conventional uniprocessor computer, even if the most efficient algorithm is used. <p> We also tested the codes using, as a parallel computing environment, a cluster of Sun Sparc workstations in the Mathematics and Computer Science Division of Argonne National Laboratory. Although our main objective was to develop a parallel code for the forward Euler method (see <ref> [3] </ref>) to solve the TDGL equations, we started with three simpler warm-up problems. Our experience with these three problems is also described here; they are used as examples to illustrate some of the concepts of parallel programming tools. <p> Our experience with these three problems is also described here; they are used as examples to illustrate some of the concepts of parallel programming tools. More in-depth discussion of all the problems considered in this report, together with all complete parallel codes and running procedures, is given in <ref> [3] </ref>. Dave Levine of Argonne National Laboratory has also developed parallel codes for solving the TDGL using BlockComm, but with a different method of discretizing the equations; consult [5], [10], and the forthcoming paper [16]. <p> The parallel codes for this problem with BlockComm and PCN, which we named ProgPdeBC and ProgPdePCN, respectively, are given in the appendix. Problem 4: Mathematical details of the TDGL are given elsewhere (see <ref> [3] </ref>, [14], [15], and the references cited therein). It suffices to say that we are solving a system of (partial differential) evolution equations governing two unknown functions of time and space position: a 5 complex-valued scalar OE (called the order parameter); and a three-dimensional vector A (called the vector potential). <p> The resulting system is then solved using a forward Euler method. A parallel BlockComm code ProgTdglBC, for implementing this algorithm is given in the Appendix. Since the code itself is rather complicated and specialized, we will present in this report only the performance results, and refer the readers to <ref> [3] </ref> for a detail discussion of the code. We note that we have also developed a parallel PCN code for this problem, but performance results were less complete.
Reference: [4] <author> I. Foster and S. Tuecke, </author> <title> Parallel Programming with PCN, </title> <type> Technical Report ANL-91/32, Revision 1, </type> <institution> Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: The programmer needs to specify only which modules are to be run concurrently and what data communications are needed between modules. The actual assignment of tasks to specific processors and message passing are transparent to the programmer. See <ref> [4] </ref> for more information and its use for various parallel environments. The two programming tools we used are highly portable over a wide variety of computer architectures. We have used three different parallel environments in our study: the Intel iPSC/860, the IBM SP, and clusters of Sun Sparc workstations. <p> Line 8 is a call to the procedure main body; the infix operator in is used to specify the map function vts:array (ni), which creates a virtual array topology of size ni. This topology guarantees the portability of the program across different computer platforms. See <ref> [4] </ref> for more on virtual topologies and map functions. Line 9 sets the return code variable to zero. <p> The approach of multilingual programming permits us to take advantage of the unique features of PCN, such as mapping, communication, and scheduling, to complement the proven efficiency of Fortran and C programming for sequential computation <ref> [4] </ref>. This approach calls for dividing up a sequential program into some convenient parts and converting these pieces to procedures to be called by PCN.
Reference: [5] <author> N. Galbreath, W. Gropp, D. Gunter, G. Leaf, and D. Levine, </author> <title> Parallel Solution of the Three-Dimensional, Time-Dependent Ginzburg-Landau Equation, </title> <booktitle> Proceedings of the SIAM Parallel Processing for Scientific Computing Conference, </booktitle> <year> 1993, </year> <pages> 160-164. </pages>
Reference-contexts: Dave Levine of Argonne National Laboratory has also developed parallel codes for solving the TDGL using BlockComm, but with a different method of discretizing the equations; consult <ref> [5] </ref>, [10], and the forthcoming paper [16]. Earlier, two other colleagues, Paul Plassmann and Steve Wright, developed a parallel code for solving the static Ginzburg-Landau equations using optimization techniques; their work is reported in [6]. 2 Preliminaries We begin by introducing some terminology that will be used throughout this report.
Reference: [6] <author> J. Garner, M. Spanbauer, R. Benedek, K. Strandburg, S. Wright, and P. Plassmann, </author> <title> Critical Fields of Josephson-coupled Superconducting Multilayers, </title> <journal> Physical Review B, </journal> <volume> 45 (1992), </volume> <pages> 7973-7983. </pages>
Reference-contexts: Earlier, two other colleagues, Paul Plassmann and Steve Wright, developed a parallel code for solving the static Ginzburg-Landau equations using optimization techniques; their work is reported in <ref> [6] </ref>. 2 Preliminaries We begin by introducing some terminology that will be used throughout this report. We also briefly describe the parallel programming tools and environments we used.
Reference: [7] <author> W. D. Gropp, </author> <title> Unpublished information, </title> <institution> Argonne National Laboratory, Argonne, Ill. </institution> <year> (1993). </year>
Reference-contexts: It provides shortcuts for many common message-passing tasks often found in the computational technique of 3 domain decomposition. Both packages are still under active development. One can consult <ref> [7] </ref> for the most current documentation about BlockComm. Although the use BlockComm greatly simplifies the coding of domain decomposition algorithms, it does not provide the data reduction and broadcast routines that are needed in our case.
Reference: [8] <author> W. Gropp and E. Lusk, </author> <title> Users Guide for the ANL IBM SP, </title> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <note> Technical Report ANL/MCS-TM-199. See also http://www.mcs.anl.gov/Projects/sp/index.html. </note>
Reference-contexts: The peak performance of each node is 125 MFlops. There are several transport layers on the SP including EUI, EUIH, and p4. EUIH is the low-overhead implementation of the EUI interface. EUI is IBM's message-passing interface to the high-performance switch. See <ref> [8] </ref> for more current information about the SP and how to use these transport layers. 3 Test Problems In this section, we describe the four test problems in our experiments. Our ultimate goal is to develop a parallel code implementing the forward Euler algorithm for the TDGL equations.
Reference: [9] <author> William Gropp, Ewing Lusk, and Anthony Skjellum, </author> <title> Using MPI, </title> <publisher> MIT Press, </publisher> <year> 1995. </year> <note> See also http://www.mcs.anl.gov/Projects/mpi/index.html. </note>
Reference-contexts: We tackled the 3D problem using two different state-of-the-art parallel computing tools: BlockComm/Chameleon and PCN, the development of both involves Argonne scientists. Since the completion of this work, a new tool, the Message Passing Interface (MPI) <ref> [9] </ref>, has emerged. It has an excellent prospect to become the standard message-passing tool. Future extension of our work will definitely include MPI. We had access to two high-performance distributed-memory supercomputers: the Intel iPSC/860 and IBM SP.
Reference: [10] <author> W. D. Gropp, H. Kaper, G. Leaf, D. Levine, M. Palumbo, and V. Vinokur, </author> <title> Numerical Simulation of Vortex Dynamics in Type-II Superconductors, </title> <type> Preprint MCS-P476-1094, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1994. </year>
Reference-contexts: Dave Levine of Argonne National Laboratory has also developed parallel codes for solving the TDGL using BlockComm, but with a different method of discretizing the equations; consult [5], <ref> [10] </ref>, and the forthcoming paper [16]. Earlier, two other colleagues, Paul Plassmann and Steve Wright, developed a parallel code for solving the static Ginzburg-Landau equations using optimization techniques; their work is reported in [6]. 2 Preliminaries We begin by introducing some terminology that will be used throughout this report. <p> exgp = sz (szeg,0) sy = sz (szstart,1) + 1 ey = sz (szend,1) + 1 sygp = sz (szsg,1) eygp = sz (szeg,1) return end #include ``tools.h'' #include ``comm/comm.h'' #include &lt;stdio.h&gt; #include ``blkcm/bc.h'' #include ``blkcm/mesh.h'' #include ``comm/io/pio.h'' #ifdef rs6000 #define checkindex_ checkindex #endif void checkindex_ (size,sx,ex,sxgp,exgp,sy,ey,sygp,eygp, nx,ny,myid,nproc) BCArrayPart size <ref> [10] </ref>; int *sx, *ex, *sxgp, *exgp; int *sy, *ey, *sygp, *eygp; int *nx,*ny; int *myid, *nproc; - FILE *pw; static char filename [] = ``blk_rep''; int i, lx, ly; int glx, gly; /*dimension of blocks with ghosts*/ if ( *myid == 0 ) - printf (``Writing report"n''); if ((pw = fopen
Reference: [11] <author> W. D. Gropp and B. Smith, </author> <title> Chameleon Parallel Programming Tools User Manual, </title> <type> Technical Report ANL-93/23, </type> <institution> Argonne National Laboratory, Argonne, Ill., </institution> <year> 1993. </year> <note> See also ftp://info.mcs.anl.gov/pub/tech_reports/reports/ANL9323.ps.Z. </note>
Reference-contexts: We close this section by introducing the parallel programming tools Chameleon, Block-Comm, and PCN, used in our study. Chameleon is a library of low-level, comprehensive, and very efficient message-passing routines developed by W. Gropp and B. Smith <ref> [11] </ref>. BlockComm is a library of high-level message-passing routines designed by Gropp to manage the efficient communication of blocks of data between processors. It provides shortcuts for many common message-passing tasks often found in the computational technique of 3 domain decomposition. Both packages are still under active development. <p> For more precise syntax definitions of Chameleon routine calls, consult the Chameleon manual <ref> [11] </ref>. 12 call getindex (N,sx,ex) 13 call PIgsync (0) 14 t1=SYGetElapsedTime () 15 call compute (sx,ex,myid) 16 t2=SYGetElapsedTime () - t1 Now that each processor knows the value of N, the next step is to find out the range of those terms in the harmonic series that it is responsible to
Reference: [12] <author> R. W. Hockey, </author> <title> Parallel Computers, </title> <publisher> Adam Hilger Ltd., </publisher> <address> Bristol, </address> <year> 1981. </year>
Reference: [13] <author> V. Kumar, A. Grama, A. Gupta, and G. Karypis, </author> <title> Introduction to Parallel Computing, </title> <editor> B. C. </editor> <publisher> Publishing Company, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: All causes of imperfect speedup of a parallel system are collectively referred to as the overhead resulting from parallel processing. Some factors that cause overhead are as follows (see <ref> [13] </ref>, [17], and [18]): * lack of a perfect degree of parallelism in the algorithm, * lack of perfect load balancing, * communication or contention time, and * extra computation.
Reference: [14] <author> Man K. Kwong, </author> <title> Sweeping Algorithm for Inverting the Discrete Ginzburg-Landau Operator, </title> <journal> Applied Math. and Computations, </journal> <volume> 53 (1993), </volume> <pages> 129-150. </pages>
Reference-contexts: The parallel codes for this problem with BlockComm and PCN, which we named ProgPdeBC and ProgPdePCN, respectively, are given in the appendix. Problem 4: Mathematical details of the TDGL are given elsewhere (see [3], <ref> [14] </ref>, [15], and the references cited therein). It suffices to say that we are solving a system of (partial differential) evolution equations governing two unknown functions of time and space position: a 5 complex-valued scalar OE (called the order parameter); and a three-dimensional vector A (called the vector potential). <p> It suffices to say that we are solving a system of (partial differential) evolution equations governing two unknown functions of time and space position: a 5 complex-valued scalar OE (called the order parameter); and a three-dimensional vector A (called the vector potential). We used an unconventional method (see <ref> [14] </ref>) to discretize the equations with respect to the space variables. The resulting system is then solved using a forward Euler method. A parallel BlockComm code ProgTdglBC, for implementing this algorithm is given in the Appendix.
Reference: [15] <author> Man K. Kwong, </author> <title> Numerical Experiments on the G-L Equations, </title> <booktitle> Proceedings of the First World Congress of Nonlinear Analysts, </booktitle> <address> Tampa, Florida, </address> <month> Aug. </month> <note> 1992 (to appear). Also Mathematics and Computer Science Division Preprint MCS-P371-0793, </note> <institution> Argonne National Laboratory, Argonne, Ill., </institution> <month> July </month> <year> 1993. </year> <month> 37 </month>
Reference-contexts: The parallel codes for this problem with BlockComm and PCN, which we named ProgPdeBC and ProgPdePCN, respectively, are given in the appendix. Problem 4: Mathematical details of the TDGL are given elsewhere (see [3], [14], <ref> [15] </ref>, and the references cited therein). It suffices to say that we are solving a system of (partial differential) evolution equations governing two unknown functions of time and space position: a 5 complex-valued scalar OE (called the order parameter); and a three-dimensional vector A (called the vector potential).
Reference: [16] <author> D. Levine, </author> <title> Unpublished information, </title> <institution> Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: Dave Levine of Argonne National Laboratory has also developed parallel codes for solving the TDGL using BlockComm, but with a different method of discretizing the equations; consult [5], [10], and the forthcoming paper <ref> [16] </ref>. Earlier, two other colleagues, Paul Plassmann and Steve Wright, developed a parallel code for solving the static Ginzburg-Landau equations using optimization techniques; their work is reported in [6]. 2 Preliminaries We begin by introducing some terminology that will be used throughout this report.
Reference: [17] <author> T. G. Lewis and H. El-Rewini, </author> <title> Introduction to Parallel Computing, </title> <publisher> Prentice Hall, </publisher> <address> Engle-wood Cliffs, New Jersey, </address> <year> 1992. </year>
Reference-contexts: On the other hand, distributed-memory programming is a bit more involved, but it has the advantages of massive parallelism. Our experiments were done exclusively on distributed-memory environments. A parallel system <ref> [17] </ref> is the combination of an algorithm and the parallel architecture on which it is implemented. As mentioned in [17], the performance of a parallel algorithm cannot be evaluated in isolation from a parallel architecture. <p> On the other hand, distributed-memory programming is a bit more involved, but it has the advantages of massive parallelism. Our experiments were done exclusively on distributed-memory environments. A parallel system <ref> [17] </ref> is the combination of an algorithm and the parallel architecture on which it is implemented. As mentioned in [17], the performance of a parallel algorithm cannot be evaluated in isolation from a parallel architecture. Therefore, it is more appropriate to talk about performance of a parallel system than performance of a parallel algorithm. Various metrics are used to measure the performance of a parallel system. <p> All causes of imperfect speedup of a parallel system are collectively referred to as the overhead resulting from parallel processing. Some factors that cause overhead are as follows (see [13], <ref> [17] </ref>, and [18]): * lack of a perfect degree of parallelism in the algorithm, * lack of perfect load balancing, * communication or contention time, and * extra computation.
Reference: [18] <author> P. Messina and A. Murli, </author> <title> Parallel Computing: Problems, Methods and Applications, </title> <address> El-sevier, New York, </address> <year> 1988. </year>
Reference-contexts: More modern architectures use multiple CPUs, each capable of executing instructions entirely independently of others. How a processor accesses the computer memory (shared memory or distributed memory) affects how a parallel program will be designed and coded. It is generally accepted <ref> [18] </ref> that shared-memory parallel programming can usually be done through minor extensions to existing programming languages, operating systems, and code libraries. On the other hand, distributed-memory programming is a bit more involved, but it has the advantages of massive parallelism. Our experiments were done exclusively on distributed-memory environments. <p> All causes of imperfect speedup of a parallel system are collectively referred to as the overhead resulting from parallel processing. Some factors that cause overhead are as follows (see [13], [17], and <ref> [18] </ref>): * lack of a perfect degree of parallelism in the algorithm, * lack of perfect load balancing, * communication or contention time, and * extra computation.
Reference: [19] <author> J. M. Ortega, </author> <title> Introduction to Parallel and Vector Solution of Linear Systems, </title> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference: [20] <author> M. J. Quinn, </author> <title> Designing Efficient Algorithms for Parallel Computers, </title> <publisher> McGraw-Hill Book Company, </publisher> <address> New York, </address> <year> 1987. </year>
Reference: [21] <author> G. Sewell, </author> <title> The Numerical Solution of Ordinary and Partial Differential Equations, </title> <publisher> Academic Press, </publisher> <address> CA, </address> <year> 1988. </year> <month> 38 </month>
Reference-contexts: Therefore, the truncation error is identically zero as well. When the parameter c is greater than approximately 2 2 , the coefficient matrix in the linear system is positive definite (see <ref> [21] </ref>). The SOR (successive overrelaxation) method is, therefore, guaranteed to converge if the relaxation parameter is chosen from the interval (0,2). The parallel codes for this problem with BlockComm and PCN, which we named ProgPdeBC and ProgPdePCN, respectively, are given in the appendix.
References-found: 21

