URL: http://www.cs.brandeis.edu/~aeg/papers/intro.reinf.learn.ps
Refering-URL: http://www.cs.brandeis.edu/~aeg/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Phone: 2  
Title: An Introduction to Reinforcement Learning  
Author: Leslie Pack Kaelbling ? Michael L. Littman ?? Andrew W. Moore ??? 
Address: Box 1910, Providence, RI, 02912-1910 USA  5000 Forbes Ave, Pittsburgh, PA 15213 USA  
Affiliation: 1 Brown University, Computer Science Department,  Carnegie Mellon University, School of Computer Science and Robotics Institute,  
Abstract: This paper surveys the historical basis of reinforcement learning and some of the current work from a computer scientist's point of view. It is an outgrowth of a number of talks given by the authors, including a NATO Advanced Study Institute and tutorials at AAAI '94 and Machine Learning '94. Reinforcement learning is a popular model of the learning problems that are encountered by an agent that learns behavior through trial-and-error interactions with a dynamic environment. It has a strong family resemblance to work in psychology, but differs considerably in the details and in the use of the word "reinforcement." It is appropriately thought of as a class of problems, rather than as a set of techniques. The paper addresses a variety of subproblems in reinforcement learning, including exploration vs. exploitation, learning from delayed reinforcement, learning and using models, generalization and hierarchy, and hidden state. It concludes with a survey of some practical systems and an assessment of the practical utility of current reinforcement-learning systems.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> David H. Ackley and Michael L. Littman. </author> <title> Generalization and scaling in reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing 2, </booktitle> <address> San Mateo, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Popular methods include various neural network methods and local memory-based methods, such as generalizations of nearest neighbor methods. Other mappings, especially the policy mapping from S ! A typically need specialized algorithms. CRBP The complementary reinforcement backpropagation algorithm <ref> [1] </ref> (crbp) addresses the problem of learning from immediate reinforcement when reinforcement is boolean and when the action is represented as a vector of boolean values. As shown in Figure 4, it consists of a feed-forward network from an encoding of the state to an encoding of the action.
Reference: 2. <author> Charles W. Anderson. </author> <title> Learning and Problem Solving with Multilayer Connectionist Systems. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1986. </year>
Reference-contexts: the critic (TD) network on input s t with error e v = (r + flv t+1 v t ) ; and the policy network with error e p = (r + flv t+1 v t )(a 1=2) : This network structure has been used in a variety of applications <ref> [2, 4, 25, 50] </ref>. Generalizing the Value Function Another popular generalizing algorithm is to use a function approximator to represent the value function. Moore and Boyan have used local memory-based methods in conjunction with value iteration; Lin has used back-propagation networks for Q-learning.
Reference: 3. <author> Andrew G. Barto, Steven J. Bradtke, and Satinder Pal Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <type> Technical Report 93-02, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1993. </year>
Reference-contexts: Running prioritized sweeping on the problem discussed in the previous section, we see a large improvement over Dyna. The optimal policy is reached in about half the number of steps and one-third the computation as Dyna required. 4.3 Other Model-Based Methods RTDP (real-time dynamic programming) <ref> [3] </ref> is another model-based method that uses Q-learning to concentrate computational effort on the areas of the state-space in which the agent is most likely to be.
Reference: 4. <author> Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-13(5):834-846, </volume> <year> 1983. </year>
Reference-contexts: This class of algorithms is called temporal difference methods [51]. We will consider two different temporal-difference learning strategies for the discounted infinite-horizon model. AHC and TD The adaptive heuristic critic algorithm is a learning analogue of policy iteration <ref> [4] </ref>. A block diagram is shown in Figure 3. It consists of two components, a critic and a reinforcement-learning component. The reinforcement-learning component can be an instance of any of the k-armed bandit algorithms, modified to deal with multiple states. <p> the critic (TD) network on input s t with error e v = (r + flv t+1 v t ) ; and the policy network with error e p = (r + flv t+1 v t )(a 1=2) : This network structure has been used in a variety of applications <ref> [2, 4, 25, 50] </ref>. Generalizing the Value Function Another popular generalizing algorithm is to use a function approximator to represent the value function. Moore and Boyan have used local memory-based methods in conjunction with value iteration; Lin has used back-propagation networks for Q-learning.
Reference: 5. <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Prince-ton, NJ, </address> <year> 1957. </year>
Reference-contexts: It is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to mdp models; see, for instance, <ref> [5, 7, 19] </ref>. 3.2 Finding a Policy Given a Model Before we consider algorithms for learning to behave in mdp environments, we will explore techniques for determining the optimal policy given a correct model. These techniques will serve as the foundation and inspiration for the learning algorithms to follow.
Reference: 6. <author> Donald A. Berry and Bert Fristedt. </author> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <address> London, UK, </address> <year> 1985. </year>
Reference-contexts: There is a wide variety of solutions to this problem. We will consider a representative selection of them. A number of important theoretical results are included in a survey by Berry and Fristedt <ref> [6] </ref>. 2.1 Bayesian Approach If the agent is going to be in the casino for a total of h steps, we can use basic Bayesian reasoning to solve for an optimal strategy as follows. We will use a dynamic programming approach that computes a mapping from belief states to actions.
Reference: 7. <author> Dmitri P. Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <year> 1987. </year>
Reference-contexts: It is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to mdp models; see, for instance, <ref> [5, 7, 19] </ref>. 3.2 Finding a Policy Given a Model Before we consider algorithms for learning to behave in mdp environments, we will explore techniques for determining the optimal policy given a correct model. These techniques will serve as the foundation and inspiration for the learning algorithms to follow.
Reference: 8. <author> Dmitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Value iteration is very flexible. The assignments to V need not be done in strict order as shown above, but instead can occur asynchronously in parallel provided that they all get updated infinitely often on an infinite run. These issues are treated extensively in <ref> [8] </ref>, where convergence results are also proved. The computational complexity of the algorithm is polynomial in the number of states. The convergence in terms of number of iterations is first order.
Reference: 9. <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: The standard approach in operations research is to solve for the optimal policy (or a close approximation thereof) based on its representation as a piecewise-linear and convex function on the belief space. This method is computationally intractable, but may serve as inspiration for methods that make further approximation <ref> [9] </ref>. 7 Practial Reinforcement Learning One reason that reinforcement learning is popular is that is serves as a theoretical tool for studying the principles of agents learning to act.
Reference: 10. <author> David Chapman and Leslie Pack Kaelbling. </author> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <address> Sydney, Australia, </address> <year> 1991. </year>
Reference-contexts: Decision Trees In domains that are characterized by a set of boolean or discrete-valued variables, it is possible to learn compact decision trees for representing Q values. The G-learning algorithm <ref> [10] </ref>, works as follows. It starts by assuming that no partitioning is necessary and tries to learn Q values for the entire domain as if it were one state.
Reference: 11. <author> Lonnie Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188, </pages> <address> San Jose, CA, 1992. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Fig. 8. Structure of a POMDP agent. Now we are left with the problem of finding a policy to map belief states into action. This problem can be formulated as an mdp, but it is difficult to solve because the input space is infinite. Chrisman's approach <ref> [11] </ref> is myopic (does not take into account future uncertainty), but yields a policy without a large amount of computation.
Reference: 12. <author> W. S. Cleveland and S. J. Delvin. </author> <title> Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 83(403) </volume> <pages> 596-610, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: A typical human learning the task requires an order of magnitude more practise to achieve proficiency at mere tens of hits. The juggling robot learned a model of the world from experience, which was generalized to unvisited states by a function approximation scheme known as locally weighted regression <ref> [12, 35] </ref>. Between each trial, a form of dynamic programming specific to linear control policies and locally linear world transitions was used to improve the policy. The form of dynamic programming is known as linear quadratic regulator design [42]. Fig. 9. Schaal and Atkeson's Devil Sticking robot.
Reference: 13. <author> Peter Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8(3) </booktitle> <pages> 341-362, </pages> <year> 1992. </year>
Reference-contexts: When = 1, it is roughly equivalent to rewarding all the states according to their value at the end of a run. It is computationally more expensive to execute the general T D (), though it often converges considerably faster for large . See <ref> [13, 15] </ref> for theoretical convergence results. Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins' Q-learning [58, 59]. Q-learning is typically easier to implement in a general-purpose computer. In order to understand Q-learning, we have to develop some additional notation.
Reference: 14. <author> Peter Dayan and Geoffrey E. Hinton. </author> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Lin [24] used this approach, first training the behaviors and then training the gating function. Many of the other hierarchical learning methods can be cast in this framework. Fig. 7. The second trial. Feudal Q Feudal Q learning <ref> [14, 58] </ref> involves a hierarchy of learning modules. In the simplest case, there is a high-level master and a low-level slave. The master receives reinforcement form the outside world. Its actions consist of command that it can give to the low-level learner.
Reference: 15. <author> Peter Dayan and Terrence J. Sejnowski. </author> <title> TD() converges with probability 1. </title> <journal> Machine Learning, </journal> <volume> 14(3), </volume> <year> 1994. </year>
Reference-contexts: When = 1, it is roughly equivalent to rewarding all the states according to their value at the end of a run. It is computationally more expensive to execute the general T D (), though it often converges considerably faster for large . See <ref> [13, 15] </ref> for theoretical convergence results. Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins' Q-learning [58, 59]. Q-learning is typically easier to implement in a general-purpose computer. In order to understand Q-learning, we have to develop some additional notation.
Reference: 16. <author> Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <address> Washington, DC, </address> <year> 1993. </year>
Reference-contexts: By taking into account the start state, it can find the optimal policy along a path from the start to the goal, without necessarily visiting the rest of the state space. The Plexus planning system <ref> [16] </ref> exploits a similar intuition. It starts by making an approximate version of the MDP which is much smaller than the original one. The approximate MDP contains a set of states, called the envelope, that includes the agent's current state and the goal state, if there is one.
Reference: 17. <author> Vijay Gullapalli. </author> <title> A stochastic reinforcement learning algorithm for learning real-valued functions. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 671-692, </pages> <year> 1990. </year>
Reference-contexts: When a new state is received as input, an action must be chosen. One method might be to do a local gradient-ascent search on the action in order to find one with high value. In addition, the exploration strategy is important. Gullapalli <ref> [17, 18] </ref> has developed a "neural" reinforcement-learning unit for use in continuous action spaces. The unit generates actions with a normal distribution; it adjusts the mean and variance based on previous experience.
Reference: 18. <author> Vijay Gullapalli. </author> <title> Reinforcement Learning and its application to control. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1992. </year>
Reference-contexts: When a new state is received as input, an action must be chosen. One method might be to do a local gradient-ascent search on the action in order to find one with high value. In addition, the exploration strategy is important. Gullapalli <ref> [17, 18] </ref> has developed a "neural" reinforcement-learning unit for use in continuous action spaces. The unit generates actions with a normal distribution; it adjusts the mean and variance based on previous experience.
Reference: 19. <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1960. </year>
Reference-contexts: It is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to mdp models; see, for instance, <ref> [5, 7, 19] </ref>. 3.2 Finding a Policy Given a Model Before we consider algorithms for learning to behave in mdp environments, we will explore techniques for determining the optimal policy given a correct model. These techniques will serve as the foundation and inspiration for the learning algorithms to follow.
Reference: 20. <author> Leslie Pack Kaelbling. </author> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, MA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Hierarchical Distance to Goal Especially if we consider reinforcement learning modules to be part of larger agent architectures, it is important to consider problems in which goals are dynamically input to the learner. Kael-bling's HDG algorithm <ref> [20] </ref> uses a hierarchical approach to solving problems when goals of achievement (the agent should get to a particular state as quickly as possible) are given to an agent dynamically. The HDG algorithm works by analogy with navigation in a harbor.
Reference: 21. <author> Leslie Pack Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1993. </year>
Reference-contexts: This still has a finite danger of starving an optimal but unlucky action, but the danger can be made extremely small. Techniques like this have been used in several reinforcement learning algorithms includ-ing the Interval Exploration method <ref> [21] </ref> (described shortly), the exploration bonus idea in Dyna [52] and the exploration mechanism in Prioritized Sweeping [36]. 2.4 Ad Hoc Randomized Strategies In reinforcement-learning practice, some simple, ad hoc strategies have been popular. <p> There is no understanding of the regret of this algorithm. 2.6 Interval-based Techniques Exploration is often more efficient when it is based on second-order information about the certainty or variance of the estimated values of actions. Kaelbling's interval estimation algorithm <ref> [21] </ref> stores statistics for each action a i : s i is the number of successes and n i the number of trials. <p> In general, however, that approach suffers from the problem of very unreliable reinforcement: if a single learner generates an inappropriate output bit, all of the learners receive a low reinforcement value. The cascade method <ref> [21] </ref> allows a collection of learners to be trained collectively to generate appropriate joint outputs; it is considerably more reliable, but can require additional computational effort. 5.4 Adaptive Resolution Models In many cases, what we would like to do is partition the domain into regions of states that can be considered
Reference: 22. <author> Leslie Pack Kaelbling. </author> <title> Associative reinforcement learning: A generate and test algorithm. </title> <journal> Machine Learning, </journal> <volume> 15(3), </volume> <year> 1994. </year>
Reference-contexts: Taking inspiration from mainstream machine learning work, Kael-bling developed two algorithms for learning boolean functions from reinforcement: one uses the bias of k-DNF to drive the generalization process [23]; the other searches the space of syntactic descriptions of functions using a simple generate-and-test method <ref> [22] </ref>. The restriction to a single boolean output makes the basic techniques impractical. In very benign learning situations, it is possible to use a collection of learners to learn independently the individual bits that make up a complex output.
Reference: 23. <author> Leslie Pack Kaelbling. </author> <title> Associative reinforcement learning: Functions in k-DNF. </title> <journal> Machine Learning, </journal> <volume> 15(3), </volume> <year> 1994. </year>
Reference-contexts: A boolean function has a vector of boolean inputs and a single boolean output. Taking inspiration from mainstream machine learning work, Kael-bling developed two algorithms for learning boolean functions from reinforcement: one uses the bias of k-DNF to drive the generalization process <ref> [23] </ref>; the other searches the space of syntactic descriptions of functions using a simple generate-and-test method [22]. The restriction to a single boolean output makes the basic techniques impractical.
Reference: 24. <author> Long-Ji Lin. </author> <title> Hierachical learning of robot skills by reinforcement. </title> <booktitle> In Proceedings of the International Conference on Neural Networks, </booktitle> <year> 1993. </year>
Reference-contexts: Mahadevan and Connell [31] used the dual approach: they fixed the gating function, and supplied reinforcement functions for the individual behaviors, which were learned. Lin <ref> [24] </ref> used this approach, first training the behaviors and then training the gating function. Many of the other hierarchical learning methods can be cast in this framework. Fig. 7. The second trial. Feudal Q Feudal Q learning [14, 58] involves a hierarchy of learning modules.
Reference: 25. <author> Long-Ji Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1993. </year>
Reference-contexts: the critic (TD) network on input s t with error e v = (r + flv t+1 v t ) ; and the policy network with error e p = (r + flv t+1 v t )(a 1=2) : This network structure has been used in a variety of applications <ref> [2, 4, 25, 50] </ref>. Generalizing the Value Function Another popular generalizing algorithm is to use a function approximator to represent the value function. Moore and Boyan have used local memory-based methods in conjunction with value iteration; Lin has used back-propagation networks for Q-learning.
Reference: 26. <author> Long-Ji Lin and Tom M. Mitchell. </author> <title> Memory approaches to reinforcement learning in non-markovian domains. </title> <type> Technical Report CMU-CS-92-138, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The network can be trained using backpropagation through time (or some other suitable technique) and learns to retain "history features" to predict value. This approach has been used by a number of researchers <ref> [33, 26, 45] </ref>. It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems.
Reference: 27. <author> Michael L. Littman. </author> <title> Memoryless policies: Theoretical limitations and practical results. </title> <booktitle> In From Animals to Animats 3, </booktitle> <address> Brighton, UK, </address> <year> 1994. </year>
Reference-contexts: This approach may yield plausible results in some cases, but again, there are no guarantees. It is reasonable, though, to ask what the optimal deterministic policy (mapping from observations to actions, in this case) is. It is NP-hard <ref> [27] </ref> to find this mapping, and it often has very poor performance. In the case of our agent trying to get home, for instance, the optimal deterministic policy will be to go right when in the woods.
Reference: 28. <author> William S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observed markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28(1) </volume> <pages> 47-65, </pages> <year> 1991. </year>
Reference-contexts: POMDP approach Another strategy consists of using hidden Markov mo-del (HMM) techniques to learn a model of the environment, including the hidden state, then to use that model to construct a perfect memory controller <ref> [28, 34] </ref>. Figure 8 illustrates the basic structure. The component on the left is the state estimator, which computes the agent's belief state as a function of the old belief state, the last action, and the current observation.
Reference: 29. <author> Pattie Maes and Rodney A. Brooks. </author> <title> Learning to coordinate behaviors. </title> <booktitle> In Proceedings Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 796-802. </pages> <publisher> AAAI, Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Having eventually reached a sufficiently high resolution, it discovers the gap and proceeds greedily towards the goal, only to be temporarily blocked by the goal's barrier region. of the environment, what behavior's actions should be switched through and actually executed. Maes and Brooks <ref> [29] </ref> used a version of this architecture in which the individual behaviors were fixed a priori and the gating function was learned from reinforcement. Mahadevan and Connell [31] used the dual approach: they fixed the gating function, and supplied reinforcement functions for the individual behaviors, which were learned.
Reference: 30. <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <address> Anaheim, CA, </address> <year> 1991. </year>
Reference-contexts: The tapered stick is hit alternately by each of the two hand sticks. The task is to keep the devil stick from falling for as many hits as possible. The robot has three motors indicated by torque vectors t 1 ; t 2 ; t 3 . 2. In <ref> [30] </ref> the task is for a mobile robot to push large boxes for extended periods of time. Box-pushing is a well-known difficult robotics problem, characterized by immense uncertainty in the results of actions.
Reference: 31. <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Scaling reinforcement learning to robotics by exploiting the subsumption architecture. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 328-332, </pages> <year> 1991. </year>
Reference-contexts: Maes and Brooks [29] used a version of this architecture in which the individual behaviors were fixed a priori and the gating function was learned from reinforcement. Mahadevan and Connell <ref> [31] </ref> used the dual approach: they fixed the gating function, and supplied reinforcement functions for the individual behaviors, which were learned. Lin [24] used this approach, first training the behaviors and then training the gating function. Many of the other hierarchical learning methods can be cast in this framework.
Reference: 32. <author> M. J. Mataric. </author> <title> Reward Functions for Accelerated Learning. </title> <editor> In W. W. Cohen and H. Hirsh, editors, </editor> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: The robot learned to perform very competitively with the performance of a human-programmed solution. Another aspect of this work, described in Section 5.5, was a pre-programmed breakdown of the monolithic task description into a set of lower-level tasks to be learned. 3. In <ref> [32] </ref> a robotics experiment was performed in what, from the viewpoint of theoretical reinforcement learning, is an unthinkably high dimensional state space, containing many dozens of degrees of freedom. Four mobile robots traveled within a small enclosure collecting small disks and trans porting them to a destination region.
Reference: 33. <author> L. Meeden, G. McGraw, and D. Blank. </author> <title> Emergent control and planning in an autonomous vehicle. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 735-740, </pages> <address> Hillsdale, NJ, 1993. </address> <publisher> Erlbaum. </publisher>
Reference-contexts: The network can be trained using backpropagation through time (or some other suitable technique) and learns to retain "history features" to predict value. This approach has been used by a number of researchers <ref> [33, 26, 45] </ref>. It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems.
Reference: 34. <author> George E. Monahan. </author> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28(1) </volume> <pages> 1-16, </pages> <year> 1982. </year>
Reference-contexts: POMDP approach Another strategy consists of using hidden Markov mo-del (HMM) techniques to learn a model of the environment, including the hidden state, then to use that model to construct a perfect memory controller <ref> [28, 34] </ref>. Figure 8 illustrates the basic structure. The component on the left is the state estimator, which computes the agent's belief state as a function of the old belief state, the last action, and the current observation.
Reference: 35. <author> A. W. Moore and C. G. Atkeson. </author> <title> An Investigation of Memory-based Function Approximators for Learning Control. </title> <type> Technical Report, </type> <institution> MIT Artifical Intelligence Laboratory, </institution> <year> 1992. </year>
Reference-contexts: A typical human learning the task requires an order of magnitude more practise to achieve proficiency at mere tens of hits. The juggling robot learned a model of the world from experience, which was generalized to unvisited states by a function approximation scheme known as locally weighted regression <ref> [12, 35] </ref>. Between each trial, a form of dynamic programming specific to linear control policies and locally linear world transitions was used to improve the policy. The form of dynamic programming is known as linear quadratic regulator design [42]. Fig. 9. Schaal and Atkeson's Devil Sticking robot.
Reference: 36. <author> A. W. Moore and C. G. Atkeson. </author> <title> Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <year> 1993. </year>
Reference-contexts: Techniques like this have been used in several reinforcement learning algorithms includ-ing the Interval Exploration method [21] (described shortly), the exploration bonus idea in Dyna [52] and the exploration mechanism in Prioritized Sweeping <ref> [36] </ref>. 2.4 Ad Hoc Randomized Strategies In reinforcement-learning practice, some simple, ad hoc strategies have been popular. They are rarely, if ever, the best choice for the models of optimality we have used, but they may be viewed as reasonable, computationally tractable, heuristics. <p> It is particularly unhelpful when the goal has just been reached or when the agent is stuck in a dead end; it continues to update random state-action pairs, rather than concentrating on the "interesting" parts of the state space. These problems are addressed by Prioritized Sweeping <ref> [36] </ref> and Queue-Dyna [40], which are two independently-developed but very similar techniques. We will describe prioritized sweeping in some detail. The algorithm is very similar to Dyna, except in the choice of which state-action pairs to update. To make appropriate choices, we must store additional information in the model.
Reference: 37. <author> Andrew W. Moore. </author> <title> Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued spaces. </title> <booktitle> In Proc. Eighth International Machine Learning Workshop, </booktitle> <year> 1991. </year>
Reference-contexts: It outperformed Q-learning with backpropagation in a simple video-game domain. It cannot, however, acquire partitions in which attributes are only significant in combination (such as those needed to solve parity problems). Variable Resolution Dynamic Programming The VRDP algorithm <ref> [37] </ref> enables conventional dynamic programming to be performed in real-valued multivariate state-spaces where straightforward discretization would fall prey to the curse of dimensionality. A kd-tree (similar to a decision tree) is used to partition state space into coarse regions.
Reference: 38. <author> Andrew W. Moore. </author> <title> The parti-game algorithm for variable resolution reinforce-ment learning in multidimensional state spaces. </title> <editor> In S. J. Hanson, J. D Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: This algorithm proved effective on a number of problems for which full high-resolution arrays would have been impractical. It has the disadvantage of requiring a guess at an initially valid trajectory through state-space. PartiGame Algorithm Moore's PartiGame algorithm <ref> [38] </ref> is another solution to the problem of learning to achieve goal configurations in deterministic high-dimensional continuous spaces by learning an adaptive-resolution model. It also divides the domain into cells; but in each cell, the actions available consist of aiming at the neighboring cells.
Reference: 39. <author> Kumpati Narendra and M. A. L. Thathachar. </author> <title> Learning Automata: An Introduction. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: It may also converge unnecessarily slowly unless the temperature schedule is manually tuned with great care. 2.5 Learning Automata There is a whole branch of the theory of adaptive control devoted to learning automata (see Narendra and Thathachar for a survey <ref> [39] </ref>). They were originally described explicitly as finite state automata as in the Tsetlin automaton shown in Figure 2. It is inconvenient to describe algorithms in this way, so a move was made to describe the internal state as a probability distribution according to which actions would be chosen.
Reference: 40. <author> Jing Peng and Ronald J. Williams. </author> <title> Efficient learning and planning within the dyna framework. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 1(4) </volume> <pages> 437-454, </pages> <year> 1993. </year>
Reference-contexts: It is particularly unhelpful when the goal has just been reached or when the agent is stuck in a dead end; it continues to update random state-action pairs, rather than concentrating on the "interesting" parts of the state space. These problems are addressed by Prioritized Sweeping [36] and Queue-Dyna <ref> [40] </ref>, which are two independently-developed but very similar techniques. We will describe prioritized sweeping in some detail. The algorithm is very similar to Dyna, except in the choice of which state-action pairs to update. To make appropriate choices, we must store additional information in the model.
Reference: 41. <author> Jing Peng and Ronald J. Williams. </author> <title> Incremental multi-step Q-learning. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> New Brunswick, New Jersey, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Q-learning can also be extended to update states that occurred more than one step previously, as in T D () <ref> [41] </ref>. Q-learning is the most popular algorithm for learning from delayed reinforcement. It does not, however, address any of the issues involved in generalizing over large state and/or action spaces. In addition, it may converge quite slowly to a good policy.
Reference: 42. <author> A. P. Sage and C. C. White. </author> <title> Optimum Systems Control. </title> <publisher> Prentice Hall, </publisher> <year> 1977. </year>
Reference-contexts: Between each trial, a form of dynamic programming specific to linear control policies and locally linear world transitions was used to improve the policy. The form of dynamic programming is known as linear quadratic regulator design <ref> [42] </ref>. Fig. 9. Schaal and Atkeson's Devil Sticking robot. The tapered stick is hit alternately by each of the two hand sticks. The task is to keep the devil stick from falling for as many hits as possible.
Reference: 43. <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3 </volume> <pages> 211-229, </pages> <year> 1959. </year>
Reference-contexts: Reinforcement learning applications are common here too. One application, spectacularly far ahead of its time, was Samuel's checkers playing system <ref> [43] </ref>. This learned a value function represented by a linear function approximator, and employed a training scheme similar to the Bellman-equation backups of value iteration, temporal differencing and Q-learning. More recently Tesauro [54, 55] applied the temporal differencing algorithm to Backgammon.
Reference: 44. <author> S. Schaal and C. Atkeson. </author> <title> Robot Juggling: An Implementation of Memory-based Learning. </title> <journal> Control Systems Magazine, </journal> <volume> 14, </volume> <year> 1994. </year>
Reference-contexts: Here we will concentrate on the following four examples, although many other interesting ongoing robotics investigations are underway. 1. In <ref> [44] </ref> a two armed robot, shown in Figure 9, learns to juggle a device known as a devil-stick. This is a complex non-linear control task involving a six-dimensional state space and less than 200 msecs per control decision.
Reference: 45. <author> Jurgen H. Schmidhuber. </author> <title> Reinforcement learning in markovian and non-markovian environments. </title> <editor> In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The network can be trained using backpropagation through time (or some other suitable technique) and learns to retain "history features" to predict value. This approach has been used by a number of researchers <ref> [33, 26, 45] </ref>. It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems.
Reference: 46. <author> Nicol N. Schraudolph, Peter Dayan, and Terrence J. Sejnowski. </author> <title> Using the td(lambda) algorithm to learn an evaluation function for the game of go. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Other games that have been studied include Go <ref> [46] </ref> and Chess [56]. It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains. 7.2 Robotics In recent years there have been many robotics applications that have used reinforcement learning.
Reference: 47. <author> Satinder Pal Singh. </author> <title> Reinforcement learning with a hierarchy of abstract models. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 202-207, </pages> <address> San Jose, CA, 1992. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The reinforcement functions for the individual behaviors (commands) are given, but learning takes place simultaneously at both the high and low levels. Compositional Q Singh's compositional learning <ref> [48, 47] </ref> consists of a hierarchy based on the temporal sequencing of subgoals. The elemental tasks are behaviors that achieve some recognizable condition. The high-level goal of the system is to achieve some set of conditions in sequential order.
Reference: 48. <author> Satinder Pal Singh. </author> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 323-340, </pages> <year> 1992. </year>
Reference-contexts: The reinforcement functions for the individual behaviors (commands) are given, but learning takes place simultaneously at both the high and low levels. Compositional Q Singh's compositional learning <ref> [48, 47] </ref> consists of a hierarchy based on the temporal sequencing of subgoals. The elemental tasks are behaviors that achieve some recognizable condition. The high-level goal of the system is to achieve some set of conditions in sequential order.
Reference: 49. <author> Satinder Pal Singh, Tommi Jaakkola, and Michael I. Jordan. </author> <title> Model-free reinforcement learning for non-Markovian decision problems. </title> <booktitle> In Proceedings of the Machine Learning Conference, </booktitle> <year> 1994. </year>
Reference-contexts: If there is randomness in the agent's actions, it will not get stuck in the woods forever. Jordan, Jaakola, and Singh <ref> [49] </ref> have developed an algorithm for finding locally optimal stochastic policies, but finding a globally optimal policy is still NP hard.
Reference: 50. <author> Richard S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1984. </year>
Reference-contexts: Since the probabilities will tend to move toward 0.5, this makes the action more random and increases search. The hope is that the random distribution will generate an action that works better, and then that action will be reinforced. ARC The associative reinforcement comparison (arc) algorithm <ref> [50] </ref> is an instance of the ahc architecture for the case of boolean actions, consisting of two feed-forward networks. One learns the value of situations, the other learns a policy. These can be simple linear networks or can have hidden units. <p> the critic (TD) network on input s t with error e v = (r + flv t+1 v t ) ; and the policy network with error e p = (r + flv t+1 v t )(a 1=2) : This network structure has been used in a variety of applications <ref> [2, 4, 25, 50] </ref>. Generalizing the Value Function Another popular generalizing algorithm is to use a function approximator to represent the value function. Moore and Boyan have used local memory-based methods in conjunction with value iteration; Lin has used back-propagation networks for Q-learning.
Reference: 51. <author> Richard S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Instead, we will use insights from value iteration to adjust the value of a state based on the immediate reward and the value of the next state. This class of algorithms is called temporal difference methods <ref> [51] </ref>. We will consider two different temporal-difference learning strategies for the discounted infinite-horizon model. AHC and TD The adaptive heuristic critic algorithm is a learning analogue of policy iteration [4]. A block diagram is shown in Figure 3. It consists of two components, a critic and a reinforcement-learning component. <p> In most implementations, however, both components operate simultaneously. It remains to explain how the critic can learn the value of a policy. It uses Sutton's TD algorithm <ref> [51] </ref> with the weight update rule V (s) := (1 ff)V (s) + ff (r + flV (s 0 )) : Whenever a state s is visited, its value is updated to be closer to r + flV (s 0 ), where r is the instantaneous reward received and V (s
Reference: 52. <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <address> Austin, TX, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This still has a finite danger of starving an optimal but unlucky action, but the danger can be made extremely small. Techniques like this have been used in several reinforcement learning algorithms includ-ing the Interval Exploration method [21] (described shortly), the exploration bonus idea in Dyna <ref> [52] </ref> and the exploration mechanism in Prioritized Sweeping [36]. 2.4 Ad Hoc Randomized Strategies In reinforcement-learning practice, some simple, ad hoc strategies have been popular. <p> TD and Q-learning do very little computation per interaction with the world, so are fast, but they do not fully exploit the information it gets from the world, requiring it to visit some parts of the environment repeatedly before it can determine the optimal policy. 4.1 Dyna Sutton's Dyna architecture <ref> [52, 53] </ref> allows the middle ground to be exploited, yielding strategies that are both more effective than model-free learning and more computationally efficient than the naive model-based approach. It simultaneously uses experience to build a model, uses experience to adjust the policy, and uses the model to adjust the policy.
Reference: 53. <author> Richard S. Sutton. </author> <title> Reinforcement learning architectures for animats. </title> <booktitle> In Proceedings of the International Workshop on the Simulation of Adaptive Behavior: From Animals to Animats, </booktitle> <pages> pages 288-296, </pages> <address> Cambridge, MA, 1991. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: TD and Q-learning do very little computation per interaction with the world, so are fast, but they do not fully exploit the information it gets from the world, requiring it to visit some parts of the environment repeatedly before it can determine the optimal policy. 4.1 Dyna Sutton's Dyna architecture <ref> [52, 53] </ref> allows the middle ground to be exploited, yielding strategies that are both more effective than model-free learning and more computationally efficient than the naive model-based approach. It simultaneously uses experience to build a model, uses experience to adjust the policy, and uses the model to adjust the policy.
Reference: 54. <author> Gerald Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-277, </pages> <year> 1992. </year>
Reference-contexts: One application, spectacularly far ahead of its time, was Samuel's checkers playing system [43]. This learned a value function represented by a linear function approximator, and employed a training scheme similar to the Bellman-equation backups of value iteration, temporal differencing and Q-learning. More recently Tesauro <ref> [54, 55] </ref> applied the temporal differencing algorithm to Backgammon. Backgammon has approximately 10 20 states, making table-based reinforcement learning virtually impossible.
Reference: 55. <author> Gerald Tesauro. </author> <title> TD-Gammon, a self-teaching backgammon program, achieves master-level play. Neural Computation, </title> <note> To appear. </note>
Reference-contexts: One application, spectacularly far ahead of its time, was Samuel's checkers playing system [43]. This learned a value function represented by a linear function approximator, and employed a training scheme similar to the Bellman-equation backups of value iteration, temporal differencing and Q-learning. More recently Tesauro <ref> [54, 55] </ref> applied the temporal differencing algorithm to Backgammon. Backgammon has approximately 10 20 states, making table-based reinforcement learning virtually impossible.
Reference: 56. <author> S. </author> <title> Thrun. </title> <type> Personal Communication. </type> , <year> 1994. </year>
Reference-contexts: Other games that have been studied include Go [46] and Chess <ref> [56] </ref>. It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains. 7.2 Robotics In recent years there have been many robotics applications that have used reinforcement learning.
Reference: 57. <author> Sebastian B. Thrun. </author> <title> The role of exploration in learning control. </title> <editor> In David A. White and Donald A. Sofge, editors, </editor> <title> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: They are rarely, if ever, the best choice for the models of optimality we have used, but they may be viewed as reasonable, computationally tractable, heuristics. Thrun <ref> [57] </ref> has surveyed a variety of these techniques. The simplest exploration strategy is to take the action with the best estimated expected reward by default. But with probability p, choose an action at random.
Reference: 58. <author> Christopher J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, UK, </address> <year> 1989. </year>
Reference-contexts: It is computationally more expensive to execute the general T D (), though it often converges considerably faster for large . See [13, 15] for theoretical convergence results. Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins' Q-learning <ref> [58, 59] </ref>. Q-learning is typically easier to implement in a general-purpose computer. In order to understand Q-learning, we have to develop some additional notation. <p> Lin [24] used this approach, first training the behaviors and then training the gating function. Many of the other hierarchical learning methods can be cast in this framework. Fig. 7. The second trial. Feudal Q Feudal Q learning <ref> [14, 58] </ref> involves a hierarchy of learning modules. In the simplest case, there is a high-level master and a low-level slave. The master receives reinforcement form the outside world. Its actions consist of command that it can give to the low-level learner.
Reference: 59. <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: It is computationally more expensive to execute the general T D (), though it often converges considerably faster for large . See [13, 15] for theoretical convergence results. Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins' Q-learning <ref> [58, 59] </ref>. Q-learning is typically easier to implement in a general-purpose computer. In order to understand Q-learning, we have to develop some additional notation.
Reference: 60. <author> Ronald J. Williams. </author> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 229-256, </pages> <year> 1992. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: The second network is trained in a standard supervised mode to estimate r as a function of s. This is the reinforcement comparison learning rule (an instance of the reinforce framework <ref> [60] </ref>). One further generalization of this scheme is to use the TD rule to allow the prediction network to predict the long-run value of a state, rather than just the expected immediate reinforcement.
References-found: 60

