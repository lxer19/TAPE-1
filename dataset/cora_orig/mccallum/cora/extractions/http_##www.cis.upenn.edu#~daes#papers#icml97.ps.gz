URL: http://www.cis.upenn.edu/~daes/papers/icml97.ps.gz
Refering-URL: http://www.cis.upenn.edu/~daes/papers.html
Root-URL: 
Email: daes@linc.cis.upenn.edu  ungar@central.cis.upenn.edu  foster@hellspark.wharton.upenn.edu  
Title: Characterizing the generalization performance of model selection strategies  
Author: Dale Schuurmans Lyle H. Ungar Dean P. Foster 
Date: July 1997.  
Note: Appears in Proceedings of the Fourteenth International Conference on Machine Learning (ICML-97), Nashville,  
Address: Philadelphia, PA 19104-6228  Philadelphia, PA 19104-6389  Philadelphia, PA 19104-6302  
Affiliation: Inst. for Research in Cognitive Science University of Pennsylvania  Computer Infor. Science University of Pennsylvania  Department of Statistics University of Pennsylvania  
Abstract: We investigate the structure of model selection problems via the bias/variance decomposition. In particular, we characterize the essential aspects of a model selection task by the bias and variance profiles it generates over the sequence of hypothesis classes. With this view, we develop a new understanding of complexity-penalization methods: First, the penalty terms can be interpreted as postulating a particular profile for the variances as a function of model complexityif the postulated and true profiles do not match, then systematic under-fitting or over-fitting results, depending on whether the penalty terms are too large or too small. Second, we observe that it is generally best to penalize according to the true variances of the task, and therefore no fixed penalization strategy is optimal across all problems. We then use this characterization to introduce the notion of easy versus hard model selection problems. Here we show that if the variance profile grows too rapidly in relation to the biases, then standard model selection techniques become prone to significant errors. This can happen, for example, in regression problems where the independent variables are drawn from wide-tailed distributions. To counter this, we discuss a new model selection strategy that dramatically outperforms standard complexity-penalization and hold-out meth ods on these hard tasks.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ash, R. B. </author> <year> 1972. </year> <title> Real Analysis and Probability. </title> <address> San Diego: </address> <publisher> Academic Press. </publisher>
Reference-contexts: This is sufficient to ensure that H is a closed linear subspace of a Hilbert space defined by the inner product hf; gi 4 R g (x)) 2 dP X ; see, e.g., <ref> (Ash 1972, Chapter 3) </ref>. Given these conditions, we can apply the relevant projection theorem to obtain h opt , and the subsequent analysis becomes a simple consequence of generalized Pythagorean relations.
Reference: <author> Cherkassky, V.; Mulier, F.; and Vapnik, V. </author> <year> 1996. </year> <title> Comparison of VC-method with classical methods for model selection. </title> <type> Preprint. </type>
Reference: <author> Craven, P., and Wahba, G. </author> <year> 1979. </year> <title> Smoothing noisy data with spline functions. </title> <journal> Numer. Math. 31:377403. </journal>
Reference-contexts: There are many variants of this basic approach, including the minimum description length principle (Rissanen 1986), Bayesian maximum a posteriori selection, structural risk minimization (Vapnik 1982; 1996), generalized cross validation <ref> (Craven & Wahba 1979) </ref>, and even regularization (Moody 1992). These strategies differ in the specific complexity values they assign and the particular tradeoff function they optimize, but the basic idea is still the same. The other most common strategy is hold-out testing. <p> To describe these strategies, let r = i=t be the number of complexity levels being considered per training example. 3 The first penalization strategy we consider is Generalized Cross Validation GCV <ref> (Craven & Wahba 1979) </ref>.
Reference: <author> Efron, B. </author> <year> 1979. </year> <journal> Computers and the theory of statistics. SIAM Review 21:46080. </journal>
Reference: <author> Foster, D., and George, E. </author> <year> 1994. </year> <title> The risk inflation criterion for multiple regression. </title> <journal> Ann. Statist. 22:194775. </journal>
Reference: <author> Geman, S.; Bienenstock, E.; and Doursat, R. </author> <year> 1992. </year> <title> Neural networks and the bias/variance dilemma. </title> <booktitle> Neural Comp. </booktitle> <address> 4:158. </address>
Reference-contexts: This tradeoff can be formalized in terms of the bias/variance decomposition of expected hypothesis error. It is well known that (1) can be decomposed into bias and variance terms by expanding around the mean hypothesis fl h fl of the distribution P H <ref> (Geman, Bienenstock, & Doursat 1992) </ref>.
Reference: <author> Hoeffding, W. </author> <year> 1963. </year> <title> Probability inequalities for sums of bounded random variables. </title> <journal> JASA 58(301):1330. </journal>
Reference-contexts: This observation leads to a series of specific predictions about the behavior of penalization strategies: (1) if the penalization profile is much 4 This argument could be formalized into precise quantitative statements, for example, by an elementary application of Hoeffding-Chernoff bounds <ref> (Hoeffding 1963) </ref>, but we do not pur sue this here.
Reference: <author> Kearns, M.; Mansour, Y.; Ng, A.; and Ron, D. </author> <year> 1995. </year> <title> An experimental and theoretical comparison of model selection methods. </title> <booktitle> In Proceedings COLT-95. </booktitle>
Reference-contexts: E.g., Tables 1 and 2 show that VAR performs well 5 Note that this is similar to an observation made by Kearns et al. <ref> (Kearns et al. 1995) </ref> in the context of learning classifications. <p> Among the many avenues for future work, we are currently extending the same style of bias/variance analysis to classification (as opposed to regression) problems <ref> (Kearns et al. 1995) </ref>. Note that the decomposition of prediction error into additive bias and variance components is not so obvious for classification however (Kohavi & Wolpert 1996). Acknowledgements Thanks to Adam Grove for many important suggestions that improved this paper and the ideas therein.
Reference: <author> Kohavi, R., and Wolpert, D. </author> <year> 1996. </year> <title> Bias plus variance decomposition for zero-one loss functions. </title> <booktitle> In Proceedings ML-96. </booktitle>
Reference-contexts: Among the many avenues for future work, we are currently extending the same style of bias/variance analysis to classification (as opposed to regression) problems (Kearns et al. 1995). Note that the decomposition of prediction error into additive bias and variance components is not so obvious for classification however <ref> (Kohavi & Wolpert 1996) </ref>. Acknowledgements Thanks to Adam Grove for many important suggestions that improved this paper and the ideas therein. Helpful comments were also received by two anonymous referees. The first author gratefully acknowledges the support of IRCS.
Reference: <author> Kohavi, R. </author> <year> 1995. </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> In Proceedings IJCAI-95. </booktitle>
Reference: <author> Moody, J., and Utans, J. </author> <year> 1992. </year> <title> Principled architecture selection for neural networks: Application to corporate bond rating prediction. </title> <booktitle> In Proceedings NIPS-4. </booktitle>
Reference-contexts: There are many variants of this basic approach, including the minimum description length principle (Rissanen 1986), Bayesian maximum a posteriori selection, structural risk minimization (Vapnik 1982; 1996), generalized cross validation (Craven & Wahba 1979), and even regularization <ref> (Moody 1992) </ref>. These strategies differ in the specific complexity values they assign and the particular tradeoff function they optimize, but the basic idea is still the same. The other most common strategy is hold-out testing. <p> To describe these strategies, let r = i=t be the number of complexity levels being considered per training example. 3 The first penalization strategy we consider is Generalized Cross Validation GCV (Craven & Wahba 1979). Following <ref> (Moody & Utans 1992) </ref> we can write the adjusted error estimate of this strategy as cerr GCV (h fl i ) = cerr (h fl i ) + (1 r) 2 cerr (h fl i ): The other penalization strategy we consider is Vapnik's Structural Risk Minimization procedure SRM (Vapnik 1996),
Reference: <author> Moody, J. </author> <year> 1992. </year> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. </title> <booktitle> In Proceedings NIPS-4. </booktitle>
Reference-contexts: There are many variants of this basic approach, including the minimum description length principle (Rissanen 1986), Bayesian maximum a posteriori selection, structural risk minimization (Vapnik 1982; 1996), generalized cross validation (Craven & Wahba 1979), and even regularization <ref> (Moody 1992) </ref>. These strategies differ in the specific complexity values they assign and the particular tradeoff function they optimize, but the basic idea is still the same. The other most common strategy is hold-out testing. <p> To describe these strategies, let r = i=t be the number of complexity levels being considered per training example. 3 The first penalization strategy we consider is Generalized Cross Validation GCV (Craven & Wahba 1979). Following <ref> (Moody & Utans 1992) </ref> we can write the adjusted error estimate of this strategy as cerr GCV (h fl i ) = cerr (h fl i ) + (1 r) 2 cerr (h fl i ): The other penalization strategy we consider is Vapnik's Structural Risk Minimization procedure SRM (Vapnik 1996),
Reference: <author> Rissanen, J. </author> <year> 1986. </year> <title> Stochastic complexity and modeling. </title>
Reference-contexts: There are many variants of this basic approach, including the minimum description length principle <ref> (Rissanen 1986) </ref>, Bayesian maximum a posteriori selection, structural risk minimization (Vapnik 1982; 1996), generalized cross validation (Craven & Wahba 1979), and even regularization (Moody 1992). These strategies differ in the specific complexity values they assign and the particular tradeoff function they optimize, but the basic idea is still the same.
Reference: <author> Ann. Statist. </author> <month> 14:1080100. </month>
Reference: <author> Schaffer, C. </author> <year> 1993. </year> <title> Overfitting avoidance as bias. Mach. Learn. </title> <publisher> 10(2):15378. </publisher>
Reference-contexts: However, the reason for VAR's failure is that it does not pay explicit attention to the inter-hypothesis dis tances, and can therefore sometimes be fooled. Of course, we do not expect a free lunch in general <ref> (Schaffer 1993) </ref>, and there are certainly model selection problems where ADJ does not dominate, e.g., Table 3. However, one should be able to exploit additional information about the task (here knowledge of P X ) to obtain significant improvements across a wide range of problem types and conditions.
Reference: <author> Schuurmans, D. </author> <year> 1997. </year> <title> A new metric-based approach to model selection. </title> <note> In Proceedings AAAI-97. To appear. </note>
Reference-contexts: bad performance obtained by all standard model selection methods on these difficult tasks raises the question of whether it is possible to do better on hard problems, or whether we have to live with the potential of making disastrous mistakes. 5 A new model selection technique In a recent paper, <ref> (Schuurmans 1997) </ref>, one of the authors introduces a new strategy for model selection that takes a fundamentally different approach to the problem than previous techniques. This new strategy seems to avoid many of the catastrophic overfitting errors that plague standard complexity-penalization and hold-out methods on difficult model selection tasks. <p> Our empirical results support this view for the case of hard model selection tasks. (Further support to this claim is provided in <ref> (Schuurmans 1997) </ref> which considers a different class of polynomial curve-fitting problems.) To summarize, the new metric-based technique ADJ appears to effectively avoid dangerous under and over-fitting, and provides a safe and responsive model selection strategy, at least for the regression problems considered here.
Reference: <author> Vapnik, V. </author> <year> 1982. </year> <title> Estimation of Dependences Based on Empirical Data. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Vapnik, V. </author> <year> 1996. </year> <title> The Nature of Statistical Learning Theory. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Following (Moody & Utans 1992) we can write the adjusted error estimate of this strategy as cerr GCV (h fl i ) = cerr (h fl i ) + (1 r) 2 cerr (h fl i ): The other penalization strategy we consider is Vapnik's Structural Risk Minimization procedure SRM <ref> (Vapnik 1996) </ref>, which following (Cherkassky, Mulier, & Vapnik 3 For most natural orderings H 1 H 2 , the complexity level i corresponds to the number of free parameters used in the definition of function class H i .
Reference: <author> Weiss, S. M., and Kulikowski, C. A. </author> <year> 1991. </year> <title> Computer Systems that Learn. </title> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 19

