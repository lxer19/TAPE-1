URL: http://www.cs.nyu.edu/cs/faculty/siegel/Limited.ps
Refering-URL: http://www.cs.nyu.edu/cs/faculty/siegel/index.html
Root-URL: http://www.cs.nyu.edu
Title: Chernoff-Hoeffding Bounds for Applications with Limited Independence  
Author: Jeanette P. Schmidt Alan Siegel Aravind Srinivasan 
Address: Brooklyn, NY 11201  New York, NY 10012  Ithaca, NY 14853  
Affiliation: Computer Science Department Polytechnic University  Computer Science Department Courant Institute, New York University  Department of Computer Science Cornell University  
Abstract: Chernoff-Hoeffding bounds are fundamental tools used in bounding the tail probabilities of the sums of bounded and independent random variables. We present a simple technique which gives slightly better bounds than these, and which more importantly requires only limited independence among the random variables, thereby importing a variety of standard results to the case of limited independence for free. Additional methods are also presented, and the aggregate results are sharp and provide a better understanding of the proof techniques behind these bounds. They also yield improved bounds for various tail probability distributions and enable improved approximation algorithms for jobshop scheduling. The "limited independence" result implies that a reduced amount of randomness and weaker sources of randomness are sufficient for randomized algorithms whose analyses use the Chernoff-Hoeffding bounds, e.g., the analysis of randomized algorithms for random sampling and oblivious packet routing.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon, L. Babai, and A. Itai. </author> <title> A fast and simple randomized parallel algorithm for the maximal independent set problem. </title> <journal> Journal of Algorithms, </journal> <volume> 7 </volume> <pages> 567-583, </pages> <year> 1986. </year>
Reference-contexts: They are frequently used in the design and analysis of randomized algorithms, derandomization, and in the probabilistic method. We present a sim ple method which generalizes, somewhat, the classical method for proving the Chernoff-Hoeffding bounds, in the case of bounded random variables confined to the interval <ref> [0; 1] </ref>. More importantly, this approach requires only limited independence among the random variables, and thereby imports a variety of standard results to the case of limited independence for free. <p> However, in the binary case, the first n moments are sufficient to generate all higher moments, which motivates our method. Interestingly, this formulation also can be applied to general X i that take arbitrary values in the interval <ref> [0; 1] </ref>, even though it is not true that the first n moments of X = P n i=1 X i determine all higher ones. The results have many applications to tail probability distributions. <p> The results have many applications to tail probability distributions. They imply similar "limited independence" results when X 1 ; X 2 ; : : : ; X n take values in the interval <ref> [0; 1] </ref>; this can be extended to bounded random variables, by scaling their ranges to [0; 1]. In the case of the hypergeometric distribution (sampling without replacement), it provides an elementary mechanism to attain slightly better bounds than those implied in [17] and by Chvatal [13]. <p> The results have many applications to tail probability distributions. They imply similar "limited independence" results when X 1 ; X 2 ; : : : ; X n take values in the interval <ref> [0; 1] </ref>; this can be extended to bounded random variables, by scaling their ranges to [0; 1]. In the case of the hypergeometric distribution (sampling without replacement), it provides an elementary mechanism to attain slightly better bounds than those implied in [17] and by Chvatal [13]. <p> In particular, we derive good upper bounds on E [(( P n P n i=1 X i ]) k ], where X 1 ; X 2 ; : : :; X n are k-wise independent random variables, each of which lies in the interval <ref> [0; 1] </ref>; this leads to better independence bounds than our h (n; ; ffi) when ffi &lt; 1. <p> Next, via known constructions of random variables with limited independence using fewer random bits (Joffe [19], Carter & Wegman [9], Mehlhorn & Vishkin [26], Alon, Babai & Itai <ref> [1] </ref>, Siegel [43]), we can reduce the randomness required for certain algorithms. <p> as good as G (; ffi) and F (n; ; ffi) hold with the much smaller independence k = bffi 2 c, when ffi &lt; 1. 7 2.2 Tail probabilities of bounded random variables We now show that almost the same results hold for arbitrary r.v.'s which take values in <ref> [0; 1] </ref>. Analogous bounds for bounded r.v.'s that are constrained to lie in other intervals can be obtained by a linear transformation of their ranges to [0; 1]. <p> 7 2.2 Tail probabilities of bounded random variables We now show that almost the same results hold for arbitrary r.v.'s which take values in <ref> [0; 1] </ref>. Analogous bounds for bounded r.v.'s that are constrained to lie in other intervals can be obtained by a linear transformation of their ranges to [0; 1]. Given arbitrary r.v.'s X i such that 0 X i 1, i = 1; 2; : : :; n, we wish to upper bound P r (X (1 + ffi)), where X = P n ffi &gt; 0. <p> So, if S j (z) is minimized at z fl in the domain <ref> [0; 1] </ref> n under the constraint that P n i=1 z i = a, then 0 &lt; z fl i &lt; 1 for at most one i, 1 i n. <p> However, note that if random variables X 1 ; X 2 ; : : : ; X n take arbitrary values in the interval <ref> [0; 1] </ref> and if X = i=1 , then such a result is not true: in fact, no bound can be put on the number of higher moments needed to generate all the moments of X. <p> Theorem 5 If X is the sum of k-wise independent r.v.'s, each of which is confined to the interval <ref> [0; 1] </ref> with = E [X], then: (I) For ffi 1, j k , then P r (jX j ffi) e bk=2c . (b) if k = ffi 2 e 1=3 , then P r (jX j ffi) e b ffi 2 =3 c : (II) For ffi 1, j ffie <p> To prove that (Ia) holds we apply Theorem 4.III with C = , T = ffi and k = ffi 2 =e 1=3 , which is permissible since k and 2 [X] for variables in the range <ref> [0; 1] </ref>. <p> Furthermore, 14 P r k j2fi 1 ;:::;i k g (X j = 1) = j=1 p i j . Hence, p k (r) = p T (r) (1) kr ffi k k ! i 1 &lt;:::&lt;i k j=1 for some ffi k 2 <ref> [0; 1] </ref>, and an identical inequality holds without the k subscripts. <p> However, n k-wise independent r.v.'s require a sample space of size at least bk=2c X i (O (n=k)) bk=2c ; as shown for binary unbiased r.v.'s by Chor, Goldreich, H-astad, Friedman, Rudich & Smolensky [12] and for general r.v.'s by Alon, Babai & Itai <ref> [1] </ref>. Noting next that any nonzero probability in a sample space of size t is at least 1=t, we see that to get a tail probability of the form e ck , we need at least ( k log (n=k) )-wise independence. <p> with limited independence using a small number of random bits; for example, the construction of [19] and the use of universal hash functions [9] to generate jF j many k-wise independent random elements from a finite field F using O (k log jF j) random bits, and the result of <ref> [1] </ref> using coding techniques [25], which gives a polynomial (in n) time algorithm to construct n k-wise independent and unbiased random bits, given O (k log n) independent unbiased bits for any k, k n.
Reference: [2] <author> N. Alon, O. Goldreich, J. H-astad, and R. Peralta. </author> <title> Simple constructions of almost k-wise independent random variables. Random Structures and Algorithms, </title> <booktitle> 3(3) </booktitle> <pages> 289-303, </pages> <year> 1992. </year>
Reference-contexts: Thus, the independence we get cannot in general be reduced by more than a factor of O (log n). However, by using results from the newly developing theory of approximating probability distri 16 butions (Naor & Naor [29], Azar, Motwani & Naor [5], Alon, Goldreich, H-astad & Peralta <ref> [2] </ref>, Even, Goldreich, Luby, Nisan & Velickovic [14] and Chari, Rohatgi & Srinivasan [10]), we get optimal results in the case where the X i 's are binary with P r (X i = 1) = 1=2. <p> Constructions of k-wise *-biased sources of size poly (k; log n; 1 * ) were presented in <ref> [29, 2] </ref>. Such sample spaces have been shown to have several applications to explicit constructions and to derandomization, mainly since probabilistic analyses may be expected to be robust under small perturbations of the probabilities.
Reference: [3] <author> N. Alon, J. Spencer, and P. Erd-os. </author> <title> The Probabilistic Method. </title> <publisher> Wiley-Interscience Series, John Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: The classical idea behind the Chernoff-Hoeffding bounds (see, for instance, Chernoff [11], Hoeffding [17], Raghavan [35] and Alon, Spencer, & Erd-os <ref> [3] </ref>) is as follows. For any fixed t &gt; 0, P r (X a) = P r (e tX e at ) e at ; by Markov's inequality. <p> (1 ffi)) = P r (n X n (1 ffi)) F (n; ; ffi) = (n (1ffi)) ) n (1ffi) The following simple upper bounds for F (n; ; ffi) and F (n; ; ffi) are sufficient to derive most of the useful approximations that have appeared in the literature <ref> [17, 4, 35, 3] </ref>. <p> The upper bounds for G (; ffi) are either straightforward or have been established in <ref> [4, 35, 3] </ref>. The second claim follows immediately from Theorem 1, while the third claim follows by obtaining lower tail bounds from the upper tail of P n i=1 (1 X i ), and importing the upper tail bounds established in [17]. <p> Lemma 5 gives good improvements over inequality (15) in many interesting 18 cases, e.g., consider the case p = constant, ffi = constant, and (M 0:5+* ) n = o (N ), for any fixed * &gt; 0. Also, the CH bounds <ref> [11, 17, 35, 3] </ref> depend only on and not on the actual values of p i , and give the upper bound F (n; ; ffi) U 2 (n; ; ffi). <p> We feel that the above is a natural derandomization of the randomized algorithm since it sets the delays one-by-one, as opposed to the more complex ways used before. 3.2.2 Exact Partitions in Set Discrepancy Set discrepancy problems <ref> [3] </ref> are combinatorially important, special cases of which can model divide-and-conquer situations; see, e.g., the RNC edge coloring algorithm of Karloff & Shmoys [21]. <p> It is known that a 2-coloring with disc () = O ( p log n) exists and can be computed in polynomial (in jXj and n) time <ref> [3] </ref>, and that a 2-coloring with disc () = O ( 0:5+* p log n) for any fixed * &gt; 0 can be computed in NC [8, 27, 29], where : = max i jS i j. <p> It is well-known <ref> [3] </ref> and easily checked via the CH bounds that there is a constant c &gt; 0 such that if (y) is picked uniformly and independently from f0; 1g, then for any S i , P r (disc i () &gt; c p n ; hence, P r (disc () &gt; c
Reference: [4] <author> D. Angluin and L.G. Valiant. </author> <title> Fast probabilistic algorithms for Hamiltonian circuits and match-ings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18 </volume> <pages> 155-193, </pages> <year> 1979. </year>
Reference-contexts: (1 ffi)) = P r (n X n (1 ffi)) F (n; ; ffi) = (n (1ffi)) ) n (1ffi) The following simple upper bounds for F (n; ; ffi) and F (n; ; ffi) are sufficient to derive most of the useful approximations that have appeared in the literature <ref> [17, 4, 35, 3] </ref>. <p> F (n; ; ffi) G (; ffi) = ( (1 + ffi) 1+ffi ) (see, for example, [35]); for ffi &lt; 1, G (; ffi) e ffi 2 =3 <ref> [4] </ref>; for ffi &gt; 2e 1, G (; ffi) 2 (1+ffi) [35]; and At the heart of these estimates are the simple calculations associated with the multiplicative nature of E [e X i ]. Recall that e tX = P 1 t i i! X i . <p> The upper bounds for G (; ffi) are either straightforward or have been established in <ref> [4, 35, 3] </ref>. The second claim follows immediately from Theorem 1, while the third claim follows by obtaining lower tail bounds from the upper tail of P n i=1 (1 X i ), and importing the upper tail bounds established in [17].
Reference: [5] <author> Y. Azar, R. Motwani, and J. Naor. </author> <title> Approximating arbitrary probability distributions using small sample spaces. </title> <type> Manuscript, </type> <year> 1990. </year>
Reference-contexts: Thus, the independence we get cannot in general be reduced by more than a factor of O (log n). However, by using results from the newly developing theory of approximating probability distri 16 butions (Naor & Naor [29], Azar, Motwani & Naor <ref> [5] </ref>, Alon, Goldreich, H-astad & Peralta [2], Even, Goldreich, Luby, Nisan & Velickovic [14] and Chari, Rohatgi & Srinivasan [10]), we get optimal results in the case where the X i 's are binary with P r (X i = 1) = 1=2.
Reference: [6] <author> M. Bellare, O. Goldreich, and S. Goldwasser. </author> <title> Randomness in interactive proofs. </title> <booktitle> In Proc. IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 563-573, </pages> <year> 1990. </year>
Reference-contexts: These samples can be generated by O (log ( 1 * )) random samples from U , using standard methods. Note that the above construction is not optimal with regards to the number of random bits used (see Bellare, Goldreich & Goldwasser <ref> [6] </ref> for an optimal construction), but is extremely simple. It is also easily parallelizable, while it is not known how to parallelize other schemes for reducing randomness, e.g., random walks on expanders. <p> The above constructions are not optimal with regards to the minimum number of random bits used. Using random walks on expander graphs to generate the random bits, it is shown in <ref> [6] </ref> that O (log (jU j) + log ( 1 ffi )) random bits are necessary and sufficient for this problem. <p> Aravind also thanks Suresh Chari, Alessandro Panconesi and Pankaj Rohatgi for valuable discussions. We also thank Mihir Bellare for pointing out references <ref> [6] </ref> and [7] to us. 27
Reference: [7] <author> M. Bellare and J. Rompel. </author> <title> Randomness efficient sampling of arbitrary functions. </title> <type> Technical Report MIT/LCS/TM-433.b, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: It has come to our attention that via weaker bounds on the kth moment, essentially the same bounds for the random sampling problem have been obtained by Bellare & Rompel <ref> [7] </ref>. We believe that there should be additional applications yielding reduced randomness. <p> Also, via weaker bounds on the kth moment, it has been independently shown in <ref> [7] </ref> that essentially the same bounds as those given in Theorem 8 can be obtained for random sampling; they also show how iterated sampling can be used to decrease the number of random bits, at the expense of a controlled increase in the sample size. <p> Aravind also thanks Suresh Chari, Alessandro Panconesi and Pankaj Rohatgi for valuable discussions. We also thank Mihir Bellare for pointing out references [6] and <ref> [7] </ref> to us. 27
Reference: [8] <author> B. Berger and J. Rompel. </author> <title> Simulating (log c n)-wise independence in NC. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 38(4) </volume> <pages> 1026-1046, </pages> <year> 1991. </year>
Reference-contexts: We unfortunately have been unable to analytically compute the optimum of this linear program. However, we now consider an important case where some of the multipliers are negative, and which is a feasible solution to the above LP; our results generalize a result of <ref> [22, 8, 27] </ref>. <p> A slightly weaker form of a special case of one of the inequalities proven in Theorem 4 was also obtained in <ref> [8] </ref> and some related formulae were given in [27]. The proof of Theorem 4, as well as related proofs presented elsewhere, is based upon estimates for the k-th moment of X. Estimates related to ours, but for a more general class of random variables, were established in [28]. <p> that a 2-coloring with disc () = O ( p log n) exists and can be computed in polynomial (in jXj and n) time [3], and that a 2-coloring with disc () = O ( 0:5+* p log n) for any fixed * &gt; 0 can be computed in NC <ref> [8, 27, 29] </ref>, where : = max i jS i j.
Reference: [9] <author> J. L. Carter and M. N. Wegman. </author> <title> Universal classes of hash functions. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18 </volume> <pages> 143-154, </pages> <year> 1979. </year>
Reference-contexts: First, it means that any random process whose analysis uses the Chernoff-Hoeffding bounds can be simulated with a weaker random source than one which outputs unbiased and independent bits. Next, via known constructions of random variables with limited independence using fewer random bits (Joffe [19], Carter & Wegman <ref> [9] </ref>, Mehlhorn & Vishkin [26], Alon, Babai & Itai [1], Siegel [43]), we can reduce the randomness required for certain algorithms. <p> We now present some further computational applications of the new results. 19 3.1 Reduced independence for randomized algorithms There are known constructions of r.v.'s with limited independence using a small number of random bits; for example, the construction of [19] and the use of universal hash functions <ref> [9] </ref> to generate jF j many k-wise independent random elements from a finite field F using O (k log jF j) random bits, and the result of [1] using coding techniques [25], which gives a polynomial (in n) time algorithm to construct n k-wise independent and unbiased random bits, given O <p> For instance, if U is a finite field and if the field operations can be done in polynomial (in 1 ffi and log ( 1 * )) time, then any number of k-wise independent samples from U can be generated from k independent random samples <ref> [19, 9] </ref>. <p> 1) P r (T max (T 1)) + (log N + N ) P r (T max &gt; (T 1)) 1 ) + (log N + N ) 2N 22 Note that for any k, k-wise independent (i)s can be generated from k log N random bits using hash functions <ref> [9] </ref>, since the (i)s can be thought of as belonging to the field GF (2 n ).
Reference: [10] <author> S. Chari, P. Rohatgi, and A. Srinivasan. </author> <title> Improved algorithms via approximations of probability distributions. </title> <booktitle> In Proc. ACM Symposium on Theory of Computing, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: However, by using results from the newly developing theory of approximating probability distri 16 butions (Naor & Naor [29], Azar, Motwani & Naor [5], Alon, Goldreich, H-astad & Peralta [2], Even, Goldreich, Luby, Nisan & Velickovic [14] and Chari, Rohatgi & Srinivasan <ref> [10] </ref>), we get optimal results in the case where the X i 's are binary with P r (X i = 1) = 1=2.
Reference: [11] <author> H. Chernoff. </author> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23 </volume> <pages> 493-509, </pages> <year> 1952. </year>
Reference-contexts: 1 Introduction The most fundamental tools used in bounding the tail probabilities of the sums of bounded and independent random variables, are based on techniques initiated by Chernoff <ref> [11] </ref> and generalized by Hoeffding [17] more than thirty years ago. They are frequently used in the design and analysis of randomized algorithms, derandomization, and in the probabilistic method. <p> The classical idea behind the Chernoff-Hoeffding bounds (see, for instance, Chernoff <ref> [11] </ref>, Hoeffding [17], Raghavan [35] and Alon, Spencer, & Erd-os [3]) is as follows. For any fixed t &gt; 0, P r (X a) = P r (e tX e at ) e at ; by Markov's inequality. <p> Let X : P n : E [X] = i=1 p i . We want good upper bounds on P r (X (1 + ffi)), for ffi &gt; 0. Chernoff <ref> [11] </ref> implicitly showed that for identically distributed 0-1 variables X 1 ; X 2 ; : : : X n and for a &gt; , min t e at L (n; ; a) = ( a n ) na : Hoeffding [17] extended this by showing that L (n; ; a) <p> x = p 2 for x 1=2, we get cosh (k=6) e k=6 = p 2, and hence P r (jX j T ) kC k=2 12 This concludes the proof of estimate (III). 2 We now combine the results of Theorem 4 and Theorem 2 to establish Chernoff-like bounds <ref> [11, 17] </ref>, where the independence k might even be much smaller than the deviation we wish to bound. <p> Lemma 5 gives good improvements over inequality (15) in many interesting 18 cases, e.g., consider the case p = constant, ffi = constant, and (M 0:5+* ) n = o (N ), for any fixed * &gt; 0. Also, the CH bounds <ref> [11, 17, 35, 3] </ref> depend only on and not on the actual values of p i , and give the upper bound F (n; ; ffi) U 2 (n; ; ffi).
Reference: [12] <author> B. Chor, O. Goldreich, J. H-astad, J. Friedman, S. Rudich, and R. Smolensky. </author> <title> The bit extraction problem or t-resilient functions. </title> <booktitle> In Proc. IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 396-407, </pages> <year> 1985. </year>
Reference-contexts: However, n k-wise independent r.v.'s require a sample space of size at least bk=2c X i (O (n=k)) bk=2c ; as shown for binary unbiased r.v.'s by Chor, Goldreich, H-astad, Friedman, Rudich & Smolensky <ref> [12] </ref> and for general r.v.'s by Alon, Babai & Itai [1].
Reference: [13] <author> V. Chvatal. </author> <title> The tail of the hypergeometric distribution. </title> <journal> Discrete Mathematics, </journal> <volume> 25 </volume> <pages> 285-287, </pages> <year> 1979. </year>
Reference-contexts: In the case of the hypergeometric distribution (sampling without replacement), it provides an elementary mechanism to attain slightly better bounds than those implied in [17] and by Chvatal <ref> [13] </ref>. The method also yields good upper bounds for the tail probabilities of the sums of random variables with limited independence. These constructions also provide pointers to further improvement of the independence bounds. <p> Let X be the number of red balls picked in the random sample, and let p : = M=N . Then for ffi &gt; 0, a special case of a result of Hoeffding [17] (see Chvatal <ref> [13] </ref> for another proof) implies that P r (X np (1 + ffi)) F (n; np; ffi): (15) We prove the following strengthened version of inequality (15).
Reference: [14] <author> G. Even, O. Goldreich, M. Luby, N. Nisan, and B. Velickovic. </author> <title> Approximations of general independent distributions. </title> <booktitle> In Proc. ACM Symposium on Theory of Computing, </booktitle> <pages> pages 10-16, </pages> <year> 1992. </year>
Reference-contexts: Also, independent of our work, a result similar to part (I) of Theorem 6 has been proven by Even, Goldreich, Luby, Nisan & Velickovic <ref> [14] </ref>: they show that jp k (0) p (0)j 2 (k) . Theorem 6, in fact, achieves its greatest strength when is small, say = o (n), or even = O (1). <p> However, by using results from the newly developing theory of approximating probability distri 16 butions (Naor & Naor [29], Azar, Motwani & Naor [5], Alon, Goldreich, H-astad & Peralta [2], Even, Goldreich, Luby, Nisan & Velickovic <ref> [14] </ref> and Chari, Rohatgi & Srinivasan [10]), we get optimal results in the case where the X i 's are binary with P r (X i = 1) = 1=2.
Reference: [15] <author> B.V. Gladkov. </author> <title> Sums of random variables, any r of which are independent. </title> <journal> Mat. Zametkii, </journal> <volume> 32 </volume> <pages> 385-399, </pages> <year> 1982. </year>
Reference: [16] <author> B.V. Gladkov. </author> <title> A central limit theorem for sums of random variables, any r of which are independent. </title> <journal> Diskretnaia Mat., </journal> <volume> 1 </volume> <pages> 22-28, </pages> <year> 1989. </year> <title> English translation by V.A. </title> <journal> Vatutin, in Discrete Mathematics and Applications, </journal> <volume> No. 1 (1991), </volume> <pages> pages 73-79. </pages>
Reference-contexts: That formulation however, is considerably more complicated than ours, and is not as tight for the cases specifically considered here. In particular, Theorem 5 cannot be derived from the bounds in [28] for the k-th moment. Other related work was done by Gladkov ([15], with later improvements in <ref> [16] </ref>).
Reference: [17] <author> W. Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> American Statistical Association Journal, </journal> <pages> pages 13-30, </pages> <year> 1963. </year>
Reference-contexts: 1 Introduction The most fundamental tools used in bounding the tail probabilities of the sums of bounded and independent random variables, are based on techniques initiated by Chernoff [11] and generalized by Hoeffding <ref> [17] </ref> more than thirty years ago. They are frequently used in the design and analysis of randomized algorithms, derandomization, and in the probabilistic method. <p> The classical idea behind the Chernoff-Hoeffding bounds (see, for instance, Chernoff [11], Hoeffding <ref> [17] </ref>, Raghavan [35] and Alon, Spencer, & Erd-os [3]) is as follows. For any fixed t &gt; 0, P r (X a) = P r (e tX e at ) e at ; by Markov's inequality. <p> In the case of the hypergeometric distribution (sampling without replacement), it provides an elementary mechanism to attain slightly better bounds than those implied in <ref> [17] </ref> and by Chvatal [13]. The method also yields good upper bounds for the tail probabilities of the sums of random variables with limited independence. These constructions also provide pointers to further improvement of the independence bounds. <p> Chernoff [11] implicitly showed that for identically distributed 0-1 variables X 1 ; X 2 ; : : : X n and for a &gt; , min t e at L (n; ; a) = ( a n ) na : Hoeffding <ref> [17] </ref> extended this by showing that L (n; ; a) is an upper bound for the above minimum even if the X i 's are not identically distributed and range between 0 and 1. <p> (1 ffi)) = P r (n X n (1 ffi)) F (n; ; ffi) = (n (1ffi)) ) n (1ffi) The following simple upper bounds for F (n; ; ffi) and F (n; ; ffi) are sufficient to derive most of the useful approximations that have appeared in the literature <ref> [17, 4, 35, 3] </ref>. <p> The second claim follows immediately from Theorem 1, while the third claim follows by obtaining lower tail bounds from the upper tail of P n i=1 (1 X i ), and importing the upper tail bounds established in <ref> [17] </ref>. <p> Given arbitrary r.v.'s X i such that 0 X i 1, i = 1; 2; : : :; n, we wish to upper bound P r (X (1 + ffi)), where X = P n ffi &gt; 0. Hoeffding <ref> [17] </ref> has proved upper tail bounds for bounded random variables, assuming full independence among the X i 's; the main point of interest here again is that partial independence suffices, giving almost as good bounds as Hoeffding's. <p> However, the intuition gained from Section 2.1 has helped us obtain a large deviation bound for X, which is as good as the known bound <ref> [17] </ref>. This is despite the fact that we have not considered all the higher moments of X; one of the original motivations for Chernoff to consider E [e tX ] was that it generates all the higher moments of X. <p> x = p 2 for x 1=2, we get cosh (k=6) e k=6 = p 2, and hence P r (jX j T ) kC k=2 12 This concludes the proof of estimate (III). 2 We now combine the results of Theorem 4 and Theorem 2 to establish Chernoff-like bounds <ref> [11, 17] </ref>, where the independence k might even be much smaller than the deviation we wish to bound. <p> Let X be the number of red balls picked in the random sample, and let p : = M=N . Then for ffi &gt; 0, a special case of a result of Hoeffding <ref> [17] </ref> (see Chvatal [13] for another proof) implies that P r (X np (1 + ffi)) F (n; np; ffi): (15) We prove the following strengthened version of inequality (15). <p> Lemma 5 gives good improvements over inequality (15) in many interesting 18 cases, e.g., consider the case p = constant, ffi = constant, and (M 0:5+* ) n = o (N ), for any fixed * &gt; 0. Also, the CH bounds <ref> [11, 17, 35, 3] </ref> depend only on and not on the actual values of p i , and give the upper bound F (n; ; ffi) U 2 (n; ; ffi).
Reference: [18] <author> M. Hofri. </author> <title> Probabilistic Analysis of Algorithms. </title> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: We use the kth moment inequality P r (jX E [X]j ffiE [X]) E [jX E [X]j k ] (ffiE [X]) k ; which is attributable, in various formulations and generalizations, to Chebyshev, Markov, and Loeve <ref> [18] </ref>, and has been used to attain probability deviation estimates for over a century.
Reference: [19] <author> A. Joffe. </author> <title> On a set of almost deterministic k-independent random variables. </title> <journal> The Annals of Probability, </journal> <volume> 2(1) </volume> <pages> 161-162, </pages> <year> 1974. </year>
Reference-contexts: First, it means that any random process whose analysis uses the Chernoff-Hoeffding bounds can be simulated with a weaker random source than one which outputs unbiased and independent bits. Next, via known constructions of random variables with limited independence using fewer random bits (Joffe <ref> [19] </ref>, Carter & Wegman [9], Mehlhorn & Vishkin [26], Alon, Babai & Itai [1], Siegel [43]), we can reduce the randomness required for certain algorithms. <p> We now present some further computational applications of the new results. 19 3.1 Reduced independence for randomized algorithms There are known constructions of r.v.'s with limited independence using a small number of random bits; for example, the construction of <ref> [19] </ref> and the use of universal hash functions [9] to generate jF j many k-wise independent random elements from a finite field F using O (k log jF j) random bits, and the result of [1] using coding techniques [25], which gives a polynomial (in n) time algorithm to construct n <p> For instance, if U is a finite field and if the field operations can be done in polynomial (in 1 ffi and log ( 1 * )) time, then any number of k-wise independent samples from U can be generated from k independent random samples <ref> [19, 9] </ref>.
Reference: [20] <author> H. J. Karloff and P. Raghavan. </author> <title> Randomized algorithms and pseudorandom numbers. </title> <booktitle> In Proc. ACM Symposium on Theory of Computing, </booktitle> <pages> pages 310-321, </pages> <year> 1988. </year>
Reference-contexts: has the advantage of being elementary and parallelizable. 3.1.2 Reduced randomness for oblivious permutation routing We now show how our results directly imply bounds that match the explicit constructions of algorithms with reduced randomness due to Peleg & Upfal [32], for oblivious permutation routing on fixed interconnection networks (see also <ref> [20, 35, 36, 46, 48] </ref>). <p> Explicit constructions of algorithms with a spectrum of time-randomness parameters are among the results proved in [32] for the degree-4 butterfly network; these are also extendible to other networks (see Karloff & Raghavan <ref> [20] </ref> for a protocol for the hypercube with slightly weaker bounds). Here, we show how our results of Section 2.1 directly imply the bounds of [32] for the hypercube; we believe that similar results should hold for other interconnection networks.
Reference: [21] <author> H. J. Karloff and D. B. Shmoys. </author> <title> Efficient parallel algorithms for edge coloring problems. </title> <journal> Journal of Algorithms, </journal> <volume> 8 </volume> <pages> 39-52, </pages> <year> 1987. </year>
Reference-contexts: the randomized algorithm since it sets the delays one-by-one, as opposed to the more complex ways used before. 3.2.2 Exact Partitions in Set Discrepancy Set discrepancy problems [3] are combinatorially important, special cases of which can model divide-and-conquer situations; see, e.g., the RNC edge coloring algorithm of Karloff & Shmoys <ref> [21] </ref>.
Reference: [22] <author> C. Kruskal, L. Rudolph, and M. Snir. </author> <title> A complexity theory of efficient parallel algorithms. </title> <journal> Theoretical Computer Science, </journal> <volume> 71 </volume> <pages> 95-132, </pages> <year> 1990. </year>
Reference-contexts: We unfortunately have been unable to analytically compute the optimum of this linear program. However, we now consider an important case where some of the multipliers are negative, and which is a feasible solution to the above LP; our results generalize a result of <ref> [22, 8, 27] </ref>. <p> Moreover, the large deviation bounds derived in Theorem 5 for k-wise independent random variables agree with the simple exponential forms of the large deviation bounds most often cited for sequences of fully independent Bernoulli trials. Theorem 4 is similar in spirit and proof to Lemma 4.19 of <ref> [22] </ref> for identically distributed X i and constant k, but the present result is somewhat tighter even in the case of identically distributed X i , especially if X = P n i=1 X i has small variance.
Reference: [23] <author> F. T. Leighton, B. Maggs, and S. Rao. </author> <title> Universal packet routing algorithms. </title> <booktitle> In Proc. IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 256-269, </pages> <year> 1988. </year>
Reference-contexts: ; X 2 ; : : : ; X n seen above; this is not the case for these two problems and in general for many other problems. 3.2.1 Improved algorithms for packet routing and jobshop scheduling We now present simpler approximation algorithms for packet routing (Leighton, Maggs & Rao <ref> [23] </ref>) and jobshop scheduling (Shmoys, Stein & Wein [41]) which provide improved approximation guarantees, by using ideas from above. <p> One of the results of <ref> [23] </ref> tackles a special case of this problem; the general case is handled in [41]. Both these papers give polynomial-time algorithms to produce good approximations to an optimal schedule. <p> We denote the offset P r1 t (O i;r ). As shown in <ref> [23, 41] </ref>, if the d i 's are generated uniformly and independently, then with high probability, every machine at every unit of time will have (a congestion of) at most D (n; m max ) : c log log (nm max ) jobs scheduled on it for some constant c, where
Reference: [24] <author> N. Linial and N. Nisan. </author> <title> Approximate inclusion-exclusion. </title> <journal> Combinatorica, </journal> <volume> 10(4) </volume> <pages> 349-365, </pages> <year> 1990. </year>
Reference-contexts: Part (V) is an immediate consequence of Theorem 2. This concludes the proof of Theorem 6. 2 It is worth pointing out that parts (I) through (III) of Theorem 6 are not strong when &gt; p n, since it follows from the work of Linial & Nisan <ref> [24] </ref> that P r (X = `) = P r (Y = `)(1 + 15 O (e 2 (k`)= n )) independently of , which gives a much sharper bound in this case.
Reference: [25] <author> F. J. MacWilliams and N. J. A. Sloane. </author> <title> The Theory of Error Correcting Codes. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1977. </year>
Reference-contexts: a small number of random bits; for example, the construction of [19] and the use of universal hash functions [9] to generate jF j many k-wise independent random elements from a finite field F using O (k log jF j) random bits, and the result of [1] using coding techniques <ref> [25] </ref>, which gives a polynomial (in n) time algorithm to construct n k-wise independent and unbiased random bits, given O (k log n) independent unbiased bits for any k, k n.
Reference: [26] <author> K. Mehlhorn and U. Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memories. </title> <journal> Acta Informatica, </journal> <volume> 21 </volume> <pages> 339-374, </pages> <year> 1984. </year>
Reference-contexts: Next, via known constructions of random variables with limited independence using fewer random bits (Joffe [19], Carter & Wegman [9], Mehlhorn & Vishkin <ref> [26] </ref>, Alon, Babai & Itai [1], Siegel [43]), we can reduce the randomness required for certain algorithms.
Reference: [27] <author> R. Motwani, J. Naor, and M. Naor. </author> <title> The probabilistic method yields deterministic parallel algorithms. </title> <booktitle> In Proc. IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 8-13, </pages> <year> 1989. </year>
Reference-contexts: We unfortunately have been unable to analytically compute the optimum of this linear program. However, we now consider an important case where some of the multipliers are negative, and which is a feasible solution to the above LP; our results generalize a result of <ref> [22, 8, 27] </ref>. <p> A slightly weaker form of a special case of one of the inequalities proven in Theorem 4 was also obtained in [8] and some related formulae were given in <ref> [27] </ref>. The proof of Theorem 4, as well as related proofs presented elsewhere, is based upon estimates for the k-th moment of X. Estimates related to ours, but for a more general class of random variables, were established in [28]. <p> that a 2-coloring with disc () = O ( p log n) exists and can be computed in polynomial (in jXj and n) time [3], and that a 2-coloring with disc () = O ( 0:5+* p log n) for any fixed * &gt; 0 can be computed in NC <ref> [8, 27, 29] </ref>, where : = max i jS i j.
Reference: [28] <author> S.V. Nagaev and I.F. Pinelis. </author> <title> Some inequalities for the distribution of the sums of independent random variables. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 22 </volume> <pages> 248-256, </pages> <year> 1977. </year>
Reference-contexts: The proof of Theorem 4, as well as related proofs presented elsewhere, is based upon estimates for the k-th moment of X. Estimates related to ours, but for a more general class of random variables, were established in <ref> [28] </ref>. That formulation however, is considerably more complicated than ours, and is not as tight for the cases specifically considered here. In particular, Theorem 5 cannot be derived from the bounds in [28] for the k-th moment. Other related work was done by Gladkov ([15], with later improvements in [16]). <p> Estimates related to ours, but for a more general class of random variables, were established in <ref> [28] </ref>. That formulation however, is considerably more complicated than ours, and is not as tight for the cases specifically considered here. In particular, Theorem 5 cannot be derived from the bounds in [28] for the k-th moment. Other related work was done by Gladkov ([15], with later improvements in [16]).
Reference: [29] <author> J. Naor and M. Naor. </author> <title> Small-bias probability spaces: efficient constructions and applications. </title> <booktitle> In Proc. ACM Symposium on Theory of Computing, </booktitle> <pages> pages 213-223, </pages> <year> 1990. </year>
Reference-contexts: Thus, the independence we get cannot in general be reduced by more than a factor of O (log n). However, by using results from the newly developing theory of approximating probability distri 16 butions (Naor & Naor <ref> [29] </ref>, Azar, Motwani & Naor [5], Alon, Goldreich, H-astad & Peralta [2], Even, Goldreich, Luby, Nisan & Velickovic [14] and Chari, Rohatgi & Srinivasan [10]), we get optimal results in the case where the X i 's are binary with P r (X i = 1) = 1=2. <p> A sample space X for n-bit vectors was defined to be k-wise *-biased by Naor & Naor <ref> [29] </ref> (see also Vazirani [49]) if 8S f1; 2; : : :; ng; 1 jSj k; jP r ( i2S M x i = 0)j *; where L denotes the XOR function and x i denotes the ith bit of an n-bit string x picked uniformly at random from X. <p> Constructions of k-wise *-biased sources of size poly (k; log n; 1 * ) were presented in <ref> [29, 2] </ref>. Such sample spaces have been shown to have several applications to explicit constructions and to derandomization, mainly since probabilistic analyses may be expected to be robust under small perturbations of the probabilities. <p> that a 2-coloring with disc () = O ( p log n) exists and can be computed in polynomial (in jXj and n) time [3], and that a 2-coloring with disc () = O ( 0:5+* p log n) for any fixed * &gt; 0 can be computed in NC <ref> [8, 27, 29] </ref>, where : = max i jS i j.
Reference: [30] <author> N. Nisan and D. Zuckerman. </author> <title> More deterministic simulation in Logspace. </title> <booktitle> In Proc. ACM Symposium on Theory of Computing, </booktitle> <pages> pages 235-244, </pages> <year> 1993. </year>
Reference-contexts: Despite its seemingly weak nature, such a model has been shown to be able to simulate complexity classes such as RP (Vazirani & Vazirani [50]), and the study of a generalization of this model due to Zuckerman [53] has led to rich results recently (Nisan & Zuckerman <ref> [30] </ref>, Wigderson & Zuckerman [52]). Noting that for such a source, E [ j=1 for all k 1 and for all distinct indices i 1 ; i 2 ; : : : ; i k , we see that P r ( i=1 for an *-semirandom source.
Reference: [31] <author> A. Panconesi and A. Srinivasan. </author> <title> Fast randomized algorithms for distributed edge coloring. </title> <booktitle> In Proc. ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 251-262, </pages> <year> 1992. </year>
Reference-contexts: + : * If X 1 ; X 2 ; : : : ; X n are k-wise independent, then P r (X a) min i=1;2;:::;k z i a : As an example of a distribution which benefits from the above, consider the self-weakening random variables defined and used in <ref> [31] </ref>: random bits X 1 ; X 2 ; : : : ; X n are defined to be self-weakening with parameter in [31] if for all j and for all distinct indices X i 1 ; X i 2 ; : : : ; X i j , E [ <p> a) min i=1;2;:::;k z i a : As an example of a distribution which benefits from the above, consider the self-weakening random variables defined and used in <ref> [31] </ref>: random bits X 1 ; X 2 ; : : : ; X n are defined to be self-weakening with parameter in [31] if for all j and for all distinct indices X i 1 ; X i 2 ; : : : ; X i j , E [ `=1 X i ` ] `=1 E [X i ` ]; note that z j n n ) j in this case. <p> Hence, Theorem 7 directly implies one of the main theorems of <ref> [31] </ref>, which states that if X 1 ; X 2 ; : : :; X n are self-weakening random bits with parameter with X = P n i=1 X i and = E [X], then for any ffi &gt; 0, P r (X (1 + ffi)) is at most times any <p> Indeed, it was the work of <ref> [31] </ref> which mainly motivated the methods of Section 2.1. Further, the applications sketched in Section 3.2 use Theorem 7. Theorem 7 helps improve the known upper tail probability bounds for the hypergeometric distribution, an important source of self-weakening random variables.
Reference: [32] <author> D. Peleg and E. Upfal. </author> <title> A time-randomness tradeoff for oblivious routing. </title> <journal> SIAM J. Comput., </journal> <volume> 19 </volume> <pages> 256-266, </pages> <year> 1990. </year>
Reference-contexts: We believe that there should be additional applications yielding reduced randomness. A spectrum of explicit constructions of oblivious routing algorithms on the butterfly with varying time-randomness parameters is among the results of Peleg & Upfal <ref> [32] </ref>; our "limited independence" result directly matches these bounds on the hypercube and, we believe, should extend to other interconnection networks. Finally, we combine the method of conditional probabilities [34, 44] with the new construction to obtain two results. <p> Our construction has the advantage of being elementary and parallelizable. 3.1.2 Reduced randomness for oblivious permutation routing We now show how our results directly imply bounds that match the explicit constructions of algorithms with reduced randomness due to Peleg & Upfal <ref> [32] </ref>, for oblivious permutation routing on fixed interconnection networks (see also [20, 35, 36, 46, 48]). <p> Further, the routing must be oblivious in that the path P (x) chosen for a packet x must be "independent" of the path P (y) chosen for any other packet y (see <ref> [32] </ref> for a precise definition when randomized routing protocols are allowed). Explicit constructions of algorithms with a spectrum of time-randomness parameters are among the results proved in [32] for the degree-4 butterfly network; these are also extendible to other networks (see Karloff & Raghavan [20] for a protocol for the hypercube <p> path P (x) chosen for a packet x must be "independent" of the path P (y) chosen for any other packet y (see <ref> [32] </ref> for a precise definition when randomized routing protocols are allowed). Explicit constructions of algorithms with a spectrum of time-randomness parameters are among the results proved in [32] for the degree-4 butterfly network; these are also extendible to other networks (see Karloff & Raghavan [20] for a protocol for the hypercube with slightly weaker bounds). Here, we show how our results of Section 2.1 directly imply the bounds of [32] for the hypercube; we believe that similar results <p> time-randomness parameters are among the results proved in <ref> [32] </ref> for the degree-4 butterfly network; these are also extendible to other networks (see Karloff & Raghavan [20] for a protocol for the hypercube with slightly weaker bounds). Here, we show how our results of Section 2.1 directly imply the bounds of [32] for the hypercube; we believe that similar results should hold for other interconnection networks. <p> It is also shown in [35] that if each (i) is uniformly distributed in f1; 2; : : :; N g, then 8i, E [ P N j=1 H ij ] n. Here is the theorem that matches the explicit construction of <ref> [32] </ref>. <p> Hence, we get bounds that match those of <ref> [32] </ref>. 2 The above example typifies the type of application we expect our methods to find, i.e., as direct "plug-in"s in analyses where the CH bounds are normally used. 3.2 The New Formulation and the Method of Conditional Probabilities The method of conditional probabilities [34, 44] is an important technique for
Reference: [33] <author> S. A. Plotkin, D. B. Shmoys, and E. Tardos. </author> <title> Fast approximation algorithms for fractional packing and covering problems. </title> <booktitle> In Proc. IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 495-504, </pages> <year> 1991. </year>
Reference-contexts: We get a much faster implementation of the sequential jobshop scheduling algorithm of Shmoys, Stein & Wein [41]. It is comparable in time complexity to the speedups due to Plotkin, Shmoys & Tardos <ref> [33] </ref> and Stein [45] but importantly, the approximation bound it presents is better than the ones of [33] and [45]. Here, we show that a problem can be derandomized directly, thereby avoiding the bottleneck step of solving a huge linear program. <p> It is comparable in time complexity to the speedups due to Plotkin, Shmoys & Tardos <ref> [33] </ref> and Stein [45] but importantly, the approximation bound it presents is better than the ones of [33] and [45]. Here, we show that a problem can be derandomized directly, thereby avoiding the bottleneck step of solving a huge linear program. We also prove an "exact partition" result for set discrepancy, and derive a polynomial-time algorithm for it. The organization of the paper is as follows. <p> The first application, to jobshop scheduling, is a "natural" derandomization of the randomized algorithm of [41], faster than the derandomization techniques of [41] and <ref> [33] </ref>; this is shown in Section 3.2.1. The second application is to discrepancy theory, and is discussed in Section 3.2.2. <p> This step is then derandomized to deterministically compute initial delays leading to a congestion bound of O (log (n m max )). Linear programming is used for the derandomization, making this step the bottleneck. This step is sped up in <ref> [33, 45] </ref>. Here, we get a better congestion bound of D (n; m max ) as opposed to the previously known O (log (nm max )) bound, with an algorithm which is more direct than the ones of [33, 45], while having time complexities comparable to theirs. <p> This step is sped up in <ref> [33, 45] </ref>. Here, we get a better congestion bound of D (n; m max ) as opposed to the previously known O (log (nm max )) bound, with an algorithm which is more direct than the ones of [33, 45], while having time complexities comparable to theirs. We assign random initial delays fd i 2 f1; 2; : : : ; max gg uniformly and independently to the jobs.
Reference: [34] <author> P. Raghavan. </author> <title> Probabilistic construction of deterministic algorithms: approximating packing integer programs. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 37 </volume> <pages> 130-143, </pages> <year> 1988. </year>
Reference-contexts: Finally, we combine the method of conditional probabilities <ref> [34, 44] </ref> with the new construction to obtain two results. We get a much faster implementation of the sequential jobshop scheduling algorithm of Shmoys, Stein & Wein [41]. <p> get bounds that match those of [32]. 2 The above example typifies the type of application we expect our methods to find, i.e., as direct "plug-in"s in analyses where the CH bounds are normally used. 3.2 The New Formulation and the Method of Conditional Probabilities The method of conditional probabilities <ref> [34, 44] </ref> is an important technique for the deran-domization of algorithms; the reader is referred to [35] for details. We now show how this method can be combined with the formulation of Section 2.6. <p> We may now use the above form as a pessimistic estimator <ref> [34] </ref> to deterministically set the delays d i for the jobs one-by-one by the method of conditional probabilities [34, 44], to achieve the congestion bound of D (n; m max ). <p> We may now use the above form as a pessimistic estimator [34] to deterministically set the delays d i for the jobs one-by-one by the method of conditional probabilities <ref> [34, 44] </ref>, to achieve the congestion bound of D (n; m max ).
Reference: [35] <author> P. Raghavan. </author> <title> Lecture notes on randomized algorithms. </title> <type> Technical Report RC 15340 (#68237), </type> <institution> IBM T.J.Watson Research Center, </institution> <month> January </month> <year> 1990. </year> <note> Also available as CS661 Lecture Notes, Technical report YALE/DCS/RR-757, </note> <institution> Department of Computer Science, Yale University, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: The classical idea behind the Chernoff-Hoeffding bounds (see, for instance, Chernoff [11], Hoeffding [17], Raghavan <ref> [35] </ref> and Alon, Spencer, & Erd-os [3]) is as follows. For any fixed t &gt; 0, P r (X a) = P r (e tX e at ) e at ; by Markov's inequality. <p> (1 ffi)) = P r (n X n (1 ffi)) F (n; ; ffi) = (n (1ffi)) ) n (1ffi) The following simple upper bounds for F (n; ; ffi) and F (n; ; ffi) are sufficient to derive most of the useful approximations that have appeared in the literature <ref> [17, 4, 35, 3] </ref>. <p> F (n; ; ffi) G (; ffi) = ( (1 + ffi) 1+ffi ) (see, for example, <ref> [35] </ref>); for ffi &lt; 1, G (; ffi) e ffi 2 =3 [4]; for ffi &gt; 2e 1, G (; ffi) 2 (1+ffi) [35]; and At the heart of these estimates are the simple calculations associated with the multiplicative nature of E [e X i ]. <p> F (n; ; ffi) G (; ffi) = ( (1 + ffi) 1+ffi ) (see, for example, <ref> [35] </ref>); for ffi &lt; 1, G (; ffi) e ffi 2 =3 [4]; for ffi &gt; 2e 1, G (; ffi) 2 (1+ffi) [35]; and At the heart of these estimates are the simple calculations associated with the multiplicative nature of E [e X i ]. Recall that e tX = P 1 t i i! X i . Consider X 2 , for instance. <p> The upper bounds for G (; ffi) are either straightforward or have been established in <ref> [4, 35, 3] </ref>. The second claim follows immediately from Theorem 1, while the third claim follows by obtaining lower tail bounds from the upper tail of P n i=1 (1 X i ), and importing the upper tail bounds established in [17]. <p> Lemma 5 gives good improvements over inequality (15) in many interesting 18 cases, e.g., consider the case p = constant, ffi = constant, and (M 0:5+* ) n = o (N ), for any fixed * &gt; 0. Also, the CH bounds <ref> [11, 17, 35, 3] </ref> depend only on and not on the actual values of p i , and give the upper bound F (n; ; ffi) U 2 (n; ; ffi). <p> has the advantage of being elementary and parallelizable. 3.1.2 Reduced randomness for oblivious permutation routing We now show how our results directly imply bounds that match the explicit constructions of algorithms with reduced randomness due to Peleg & Upfal [32], for oblivious permutation routing on fixed interconnection networks (see also <ref> [20, 35, 36, 46, 48] </ref>). <p> We now follow the discussion of the standard aspects of this from <ref> [35] </ref>. Assume FIFO queues at each edge, and that phase (I) routes i from i to (i) by "correcting" its bits from left to right assuming that the nodes of the hypercube are indexed by n bits, and that phase (II) "corrects" bits right to left. <p> So, phase (II) is like "running phase (I) backwards", and so we consider phase (I) alone here. It is shown in <ref> [35] </ref> that the time taken for packet i in phase (I) is at most n + j=1 where H ij = 1 if the paths &lt; i; (i) &gt; and &lt; j; (j) &gt; share an edge in phase (I), and 0 otherwise (recall that n = log 2 N ). <p> It is also shown in <ref> [35] </ref> that if each (i) is uniformly distributed in f1; 2; : : :; N g, then 8i, E [ P N j=1 H ij ] n. Here is the theorem that matches the explicit construction of [32]. <p> we expect our methods to find, i.e., as direct "plug-in"s in analyses where the CH bounds are normally used. 3.2 The New Formulation and the Method of Conditional Probabilities The method of conditional probabilities [34, 44] is an important technique for the deran-domization of algorithms; the reader is referred to <ref> [35] </ref> for details. We now show how this method can be combined with the formulation of Section 2.6. This will enable us to derive simple and efficient deterministic polynomial-time algorithms from randomized algorithms which can be analyzed using our formulation, in a unified way.
Reference: [36] <author> A. Ranade. </author> <title> How to emulate shared memory. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 41 </volume> <pages> 307-326, </pages> <year> 1991. </year>
Reference-contexts: has the advantage of being elementary and parallelizable. 3.1.2 Reduced randomness for oblivious permutation routing We now show how our results directly imply bounds that match the explicit constructions of algorithms with reduced randomness due to Peleg & Upfal [32], for oblivious permutation routing on fixed interconnection networks (see also <ref> [20, 35, 36, 46, 48] </ref>).
Reference: [37] <author> H. Robbins. </author> <title> A remark on Stirling's formula. </title> <journal> Amer. Math. Monthly, </journal> <volume> 62 </volume> <pages> 26-29, </pages> <year> 1955. </year>
Reference-contexts: Con sequently E (X ) k cosh s 36 2 [X] T 0 : (9) T 0 is readily bounded by expanding (8) to get T 0 = k! ( 2 [X]) k=2 . We may apply a strong version of Stirling's Formula <ref> [37] </ref>: (r=e) r p 2re 1=(12r) ; which is valid for all r 1, to bound both k! and (k=2)!. This yields T 0 p e .
Reference: [38] <author> M. Santha and U. V. Vazirani. </author> <title> Generating quasi-random sequences from semi-random sources. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 33 </volume> <pages> 75-87, </pages> <year> 1986. </year>
Reference-contexts: An alternative approach might be to derive Chernoff-Hoeffding bounds for a sum of Bernoulli trials as a function of the variance as well as , a, and n, as in [42]. A final application is to the semi-random source introduced by Santha & Vazirani <ref> [38] </ref>. A random source which outputs bits X 1 ; X 2 ; : : :; X n is defined to be *-semirandom in [38] if 8i 1=2 * P r (X i = 1jX 1 ; X 2 ; : : : ; X i1 ) 1=2 + *; i.e., <p> A final application is to the semi-random source introduced by Santha & Vazirani <ref> [38] </ref>. A random source which outputs bits X 1 ; X 2 ; : : :; X n is defined to be *-semirandom in [38] if 8i 1=2 * P r (X i = 1jX 1 ; X 2 ; : : : ; X i1 ) 1=2 + *; i.e., the random bits can be correlated, but only to a limited extent, independent of the past history.
Reference: [39] <author> J.P. Schmidt and A. Siegel. </author> <title> On aspects of universality and performance for closed hashing. </title> <booktitle> In Proc. ACM Symposium on Theory of Computing, </booktitle> <pages> pages 355-366, </pages> <year> 1989. </year>
Reference-contexts: For example, we will take the liberty of redirecting, somewhat, the estimation method as appropriate, when attaining improved tools for analyzing the behavior of the sum of k-wise independent random variables. The results simplify and sharpen some of the analyses done in <ref> [39] </ref> and [40].
Reference: [40] <author> J.P. Schmidt and A. Siegel. </author> <title> The analysis of closed hashing under limited randomness. </title> <booktitle> In Proc. ACM Symposium on Theory of Computing, </booktitle> <pages> pages 224-234, </pages> <year> 1990. </year>
Reference-contexts: For example, we will take the liberty of redirecting, somewhat, the estimation method as appropriate, when attaining improved tools for analyzing the behavior of the sum of k-wise independent random variables. The results simplify and sharpen some of the analyses done in [39] and <ref> [40] </ref>. <p> Such instances are not unusual when pseudorandom integers are being generated uniformly in the range [0; n] and a successful trial corresponds to just a few different values. This is precisely the usual circumstance in, for instance, hashing <ref> [40, 51] </ref>. As an example, consider the (uniformly distributed) random placement of n balls among n slots. The expected number of items in slot 1 is just 1. The probability p (0) that no item lands in a given slot is about 1 e .
Reference: [41] <author> D. B. Shmoys, C. Stein, and J. Wein. </author> <title> Improved approximation algorithms for shop scheduling problems. </title> <booktitle> In Proc. ACM/SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 131-140, </pages> <year> 1991. </year>
Reference-contexts: Finally, we combine the method of conditional probabilities [34, 44] with the new construction to obtain two results. We get a much faster implementation of the sequential jobshop scheduling algorithm of Shmoys, Stein & Wein <ref> [41] </ref>. It is comparable in time complexity to the speedups due to Plotkin, Shmoys & Tardos [33] and Stein [45] but importantly, the approximation bound it presents is better than the ones of [33] and [45]. <p> The first application, to jobshop scheduling, is a "natural" derandomization of the randomized algorithm of <ref> [41] </ref>, faster than the derandomization techniques of [41] and [33]; this is shown in Section 3.2.1. The second application is to discrepancy theory, and is discussed in Section 3.2.2. <p> The first application, to jobshop scheduling, is a "natural" derandomization of the randomized algorithm of <ref> [41] </ref>, faster than the derandomization techniques of [41] and [33]; this is shown in Section 3.2.1. The second application is to discrepancy theory, and is discussed in Section 3.2.2. <p> X n seen above; this is not the case for these two problems and in general for many other problems. 3.2.1 Improved algorithms for packet routing and jobshop scheduling We now present simpler approximation algorithms for packet routing (Leighton, Maggs & Rao [23]) and jobshop scheduling (Shmoys, Stein & Wein <ref> [41] </ref>) which provide improved approximation guarantees, by using ideas from above. <p> One of the results of [23] tackles a special case of this problem; the general case is handled in <ref> [41] </ref>. Both these papers give polynomial-time algorithms to produce good approximations to an optimal schedule. Let P i be the total time needed for job J i , and let P max = max i2 [1;n] P i . <p> Let j be the total time for which machine M j is needed, and let max = max i2 [1;m] i . Before an actual schedule is constructed in <ref> [41] </ref>, a pseudo-schedule S is constructed which temporarily assumes that each machine can work on upto D operations simultaneously, where D &gt; 1 depends on the input instance. The pseudo-schedule is later used to construct an actual schedule. The only step where randomization is used in [41] is during the construction <p> schedule is constructed in <ref> [41] </ref>, a pseudo-schedule S is constructed which temporarily assumes that each machine can work on upto D operations simultaneously, where D &gt; 1 depends on the input instance. The pseudo-schedule is later used to construct an actual schedule. The only step where randomization is used in [41] is during the construction of the pseudo-schedule and is the following. An initial random delay d i 2 f1; 2; : : :; max g is assigned for each job J i . <p> We denote the offset P r1 t (O i;r ). As shown in <ref> [23, 41] </ref>, if the d i 's are generated uniformly and independently, then with high probability, every machine at every unit of time will have (a congestion of) at most D (n; m max ) : c log log (nm max ) jobs scheduled on it for some constant c, where
Reference: [42] <author> A. Siegel. </author> <title> Toward a usable theory of Chernoff Bounds for heterogeneous and partially dependent random variables. </title> <type> Manuscript, </type> <month> September </month> <year> 1992. </year>
Reference-contexts: Moreover, the use of moment generating functions embed the problem of attaining probability estimates in a space rich with algebraic structure and convex inequalities. (More about the computational aspects of such an alternative approach can be found in <ref> [42] </ref>.) The need for higher moments is due to the fact that a direct application of Markov's or Chebyshev's inequality to upper bound P r (X E [X] (1 + ffi)) leads to weak bounds. Higher moments and exponentials give dramatically better estimates. <p> Similar improvements can also be made in the case of non-binary r.v.'s. An alternative approach might be to derive Chernoff-Hoeffding bounds for a sum of Bernoulli trials as a function of the variance as well as , a, and n, as in <ref> [42] </ref>. A final application is to the semi-random source introduced by Santha & Vazirani [38].
Reference: [43] <author> A. Siegel. </author> <title> On universal classes of fast hash functions, their time-space tradeoff, and their applications. </title> <booktitle> In Proc. IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 20-25, </pages> <year> 1989. </year>
Reference-contexts: Next, via known constructions of random variables with limited independence using fewer random bits (Joffe [19], Carter & Wegman [9], Mehlhorn & Vishkin [26], Alon, Babai & Itai [1], Siegel <ref> [43] </ref>), we can reduce the randomness required for certain algorithms.
Reference: [44] <author> J. Spencer. </author> <title> Ten Lectures on the Probabilistic Method. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1987. </year>
Reference-contexts: Finally, we combine the method of conditional probabilities <ref> [34, 44] </ref> with the new construction to obtain two results. We get a much faster implementation of the sequential jobshop scheduling algorithm of Shmoys, Stein & Wein [41]. <p> get bounds that match those of [32]. 2 The above example typifies the type of application we expect our methods to find, i.e., as direct "plug-in"s in analyses where the CH bounds are normally used. 3.2 The New Formulation and the Method of Conditional Probabilities The method of conditional probabilities <ref> [34, 44] </ref> is an important technique for the deran-domization of algorithms; the reader is referred to [35] for details. We now show how this method can be combined with the formulation of Section 2.6. <p> We may now use the above form as a pessimistic estimator [34] to deterministically set the delays d i for the jobs one-by-one by the method of conditional probabilities <ref> [34, 44] </ref>, to achieve the congestion bound of D (n; m max ).
Reference: [45] <author> C. Stein. </author> <title> Approximation algorithms for multicommodity flow and shop scheduling problems. </title> <type> PhD thesis, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <year> 1992. </year> <title> Available as M IT =LCS=T R 550. </title>
Reference-contexts: We get a much faster implementation of the sequential jobshop scheduling algorithm of Shmoys, Stein & Wein [41]. It is comparable in time complexity to the speedups due to Plotkin, Shmoys & Tardos [33] and Stein <ref> [45] </ref> but importantly, the approximation bound it presents is better than the ones of [33] and [45]. Here, we show that a problem can be derandomized directly, thereby avoiding the bottleneck step of solving a huge linear program. <p> It is comparable in time complexity to the speedups due to Plotkin, Shmoys & Tardos [33] and Stein <ref> [45] </ref> but importantly, the approximation bound it presents is better than the ones of [33] and [45]. Here, we show that a problem can be derandomized directly, thereby avoiding the bottleneck step of solving a huge linear program. We also prove an "exact partition" result for set discrepancy, and derive a polynomial-time algorithm for it. The organization of the paper is as follows. <p> This step is then derandomized to deterministically compute initial delays leading to a congestion bound of O (log (n m max )). Linear programming is used for the derandomization, making this step the bottleneck. This step is sped up in <ref> [33, 45] </ref>. Here, we get a better congestion bound of D (n; m max ) as opposed to the previously known O (log (nm max )) bound, with an algorithm which is more direct than the ones of [33, 45], while having time complexities comparable to theirs. <p> This step is sped up in <ref> [33, 45] </ref>. Here, we get a better congestion bound of D (n; m max ) as opposed to the previously known O (log (nm max )) bound, with an algorithm which is more direct than the ones of [33, 45], while having time complexities comparable to theirs. We assign random initial delays fd i 2 f1; 2; : : : ; max gg uniformly and independently to the jobs.
Reference: [46] <author> L. G. Valiant. </author> <title> A scheme for fast parallel communication. </title> <journal> SIAM J. Comput., </journal> <volume> 11 </volume> <pages> 350-361, </pages> <year> 1982. </year>
Reference-contexts: has the advantage of being elementary and parallelizable. 3.1.2 Reduced randomness for oblivious permutation routing We now show how our results directly imply bounds that match the explicit constructions of algorithms with reduced randomness due to Peleg & Upfal [32], for oblivious permutation routing on fixed interconnection networks (see also <ref> [20, 35, 36, 46, 48] </ref>). <p> Here, we show how our results of Section 2.1 directly imply the bounds of [32] for the hypercube; we believe that similar results should hold for other interconnection networks. Consider the implementation of Valiant's two-phase scheme <ref> [46] </ref> (see also Valiant and Brebner [48]) on a hypercube with N = 2 n nodes: (I) Each vertex i picks a random (i) 2 f1; 2; : : :; N g as an intermediate destination for i, and routes i there; (II) Each packet i is routed to its final
Reference: [47] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: This is required, for instance, in PAC learning <ref> [47] </ref> and in running BPP algorithms. What was known so far is that N (ffi; *) = O ( 1 ffi 2 log ( 1 * )) with all the samples being independent.
Reference: [48] <author> L. G. Valiant and G. J. Brebner. </author> <title> Universal schemes for parallel communication. </title> <booktitle> In Proc. ACM Symposium on Theory of Computing, </booktitle> <pages> pages 263-277, </pages> <year> 1981. </year> <month> 30 </month>
Reference-contexts: has the advantage of being elementary and parallelizable. 3.1.2 Reduced randomness for oblivious permutation routing We now show how our results directly imply bounds that match the explicit constructions of algorithms with reduced randomness due to Peleg & Upfal [32], for oblivious permutation routing on fixed interconnection networks (see also <ref> [20, 35, 36, 46, 48] </ref>). <p> Here, we show how our results of Section 2.1 directly imply the bounds of [32] for the hypercube; we believe that similar results should hold for other interconnection networks. Consider the implementation of Valiant's two-phase scheme [46] (see also Valiant and Brebner <ref> [48] </ref>) on a hypercube with N = 2 n nodes: (I) Each vertex i picks a random (i) 2 f1; 2; : : :; N g as an intermediate destination for i, and routes i there; (II) Each packet i is routed to its final 21 destination (i).
Reference: [49] <author> U. V. Vazirani. </author> <title> Randomness, Adversaries and Computation. </title> <type> PhD thesis, </type> <institution> EECS, University of California at Berkeley, </institution> <year> 1986. </year>
Reference-contexts: A sample space X for n-bit vectors was defined to be k-wise *-biased by Naor & Naor [29] (see also Vazirani <ref> [49] </ref>) if 8S f1; 2; : : :; ng; 1 jSj k; jP r ( i2S M x i = 0)j *; where L denotes the XOR function and x i denotes the ith bit of an n-bit string x picked uniformly at random from X.
Reference: [50] <author> U. V. Vazirani and V. V. Vazirani. </author> <title> Random polynomial time is equal to slightly-random polynomial time. </title> <booktitle> In Proc. IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 417-428, </pages> <year> 1985. </year> <note> See also U. </note> <author> V. Vazirani and V. V. Vazirani, </author> <title> Random polynomial time is equal to semi-random polynomial time, </title> <type> Technical Report 88-959, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1988. </year>
Reference-contexts: Despite its seemingly weak nature, such a model has been shown to be able to simulate complexity classes such as RP (Vazirani & Vazirani <ref> [50] </ref>), and the study of a generalization of this model due to Zuckerman [53] has led to rich results recently (Nisan & Zuckerman [30], Wigderson & Zuckerman [52]).
Reference: [51] <author> M.N. Wegman and J.L. Carter. </author> <title> New hash functions and their use in authentication and set equality. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 22 </volume> <pages> 265-279, </pages> <year> 1981. </year>
Reference-contexts: Such instances are not unusual when pseudorandom integers are being generated uniformly in the range [0; n] and a successful trial corresponds to just a few different values. This is precisely the usual circumstance in, for instance, hashing <ref> [40, 51] </ref>. As an example, consider the (uniformly distributed) random placement of n balls among n slots. The expected number of items in slot 1 is just 1. The probability p (0) that no item lands in a given slot is about 1 e .
Reference: [52] <author> A. Wigderson and D. Zuckerman. </author> <title> Expanders that beat the eigenvalue bound: Explicit construction and applications. </title> <booktitle> In Proc. ACM Symposium on Theory of Computing, </booktitle> <pages> pages 245-251, </pages> <year> 1993. </year>
Reference-contexts: its seemingly weak nature, such a model has been shown to be able to simulate complexity classes such as RP (Vazirani & Vazirani [50]), and the study of a generalization of this model due to Zuckerman [53] has led to rich results recently (Nisan & Zuckerman [30], Wigderson & Zuckerman <ref> [52] </ref>). Noting that for such a source, E [ j=1 for all k 1 and for all distinct indices i 1 ; i 2 ; : : : ; i k , we see that P r ( i=1 for an *-semirandom source.
Reference: [53] <author> D. Zuckerman. </author> <title> Simulating BPP using a general weak random source. </title> <booktitle> In Proc. IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 79-89, </pages> <year> 1991. </year> <month> 31 </month>
Reference-contexts: Despite its seemingly weak nature, such a model has been shown to be able to simulate complexity classes such as RP (Vazirani & Vazirani [50]), and the study of a generalization of this model due to Zuckerman <ref> [53] </ref> has led to rich results recently (Nisan & Zuckerman [30], Wigderson & Zuckerman [52]).
References-found: 53

