URL: http://www.cis.ohio-state.edu/~szhu/Gestalt.ps.gz
Refering-URL: http://www.cis.ohio-state.edu/~szhu/publication.html
Root-URL: http://www.cis.ohio-state.edu
Email: szhu@cis.ohio-state.edu  
Title: Embedding Gestalt Laws in the Markov Random Fields a theory for shape modeling and perceptual organization  
Author: Song-Chun Zhu 
Keyword: Key words: Gestalt laws, perceptual grouping, shape modeling, Markov random field, maximum entropy, shape synthesis, active contour.  
Web: http://www.cis.ohio-state.edu/szhu/  
Affiliation: Dept. of Computer and Information Science Ohio State University  
Note: To appear in IEEE Trans on PAMI, first appeared in POCV98.  
Abstract: An important goal of the research in the middle level vision and perceptual organization is to bridge the gap between low level representation, such as raw images and edge maps and the high level descriptions such as deformable templates for object recognition. This task requires a generic probability model of shapes, which characterizes the most common features of real objects, and which is compatible with the existing statistical theories in both the low level and the high level vision. In this paper, a novel theoretic framework is proposed for shape modeling based on the maximum entropy principle, and it learns generic probabilistic shape models from observed realistic object shapes. The learned models are of the forms of Gibbs distributions defined on the Markov random fields. The neighborhood structures of the random fields corresponds to the Gestalt laws -co-linearity, co-circularity, proximity, parallelism, and symmetry, and thus both contour-based and region-based features are accounted for. The potential functions of the Gibbs distributions are learned so that the shape models duplicate the observed statistics of natural shapes, and multiple shape features are weighted by the potential functions to form a single probability measure. Stochastic algorithms are proposed for learning the shape distributions and for sampling random shapes from the models. The paper also provides a quantitative measure for the non-accidental arrangement by comparing the observed statistics and the statistics of the random shapes. Our experiments demonstrate that global shape properties can arise from the local interactions of local features. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Blum, </author> <title> "Biological shape and visual science". </title> <journal> J. of Theoretical Biology, </journal> <volume> 33, pp205-287, </volume> <year> 1973. </year>
Reference-contexts: We will discuss this issue in section (10). 3 Basic issues in shape modeling and learning Let (s) be a simple, closed and non-self-intersecting contour on 2D plane 2 , s 2 <ref> [0; 1] </ref> is the arc-length and (0) = (1). Fix the resolution, we sample N points in [0; 1] at equal distance ds = 1 N and represent the curve as, = ((x 0 ; y 0 ); (x 1 ; y 1 ); :::; (x N1 ; y N1 )); <p> We will discuss this issue in section (10). 3 Basic issues in shape modeling and learning Let (s) be a simple, closed and non-self-intersecting contour on 2D plane 2 , s 2 <ref> [0; 1] </ref> is the arc-length and (0) = (1). Fix the resolution, we sample N points in [0; 1] at equal distance ds = 1 N and represent the curve as, = ((x 0 ; y 0 ); (x 1 ; y 1 ); :::; (x N1 ; y N1 )); 2 ae R 2N ; where is the space of shapes satisfying the hard constraints being closed <p> Given a shape , a symmetry mapping function (s) is defined on (s) as a transform, (s) : <ref> [0; 1] </ref> ! [0; 1]; (s) = t () (t) = s: (s) divides the circular domain [0; 1] into a set of disjoint intervals I = [I i ; I i = (s i0 ; s i1 ) ae [0; 1]; I i " I j = ;; i 6= <p> Given a shape , a symmetry mapping function (s) is defined on (s) as a transform, (s) : <ref> [0; 1] </ref> ! [0; 1]; (s) = t () (t) = s: (s) divides the circular domain [0; 1] into a set of disjoint intervals I = [I i ; I i = (s i0 ; s i1 ) ae [0; 1]; I i " I j = ;; i 6= j; and it <p> Given a shape , a symmetry mapping function (s) is defined on (s) as a transform, (s) : <ref> [0; 1] </ref> ! [0; 1]; (s) = t () (t) = s: (s) divides the circular domain [0; 1] into a set of disjoint intervals I = [I i ; I i = (s i0 ; s i1 ) ae [0; 1]; I i " I j = ;; i 6= j; and it establishes a piecewisely continuous mapping (s; (s)) between these intervals. <p> (s) is defined on (s) as a transform, (s) : <ref> [0; 1] </ref> ! [0; 1]; (s) = t () (t) = s: (s) divides the circular domain [0; 1] into a set of disjoint intervals I = [I i ; I i = (s i0 ; s i1 ) ae [0; 1]; I i " I j = ;; i 6= j; and it establishes a piecewisely continuous mapping (s; (s)) between these intervals. Intuitively each pair of intervals represent an elongated part of the shape. <p> The energy functional is defined to enforce the following two aspects. 1. The two matched linelets `(s) and `( (s)) should be as close, parallel, and symmetric to each other as possible. 2. The number of intervals (or discontinuities of (s)) in <ref> [0; 1] </ref> should be as small as possible. The optimal (s) is computed by a stochastic algorithm based on local neighborhood in the Markov random field, and the medial axis is computed based on (s). Details of the definition of the energy functional and the algorithm is referred to [40].
Reference: [2] <author> F. L. Bookstein, </author> <title> "Size and shape spaces for landmark data in two dimensions", </title> <journal> Statistical Science, vol.1, No.2, </journal> <pages> 181-242, </pages> <year> 1986. </year>
Reference-contexts: Watson in [17]), often turn out to be too hard for the most gifted mathematicians to answer. The theory of shape is also studied by Bookstein in morphometrics a discipline studying the deformations and variabilities of biologic organisms <ref> [2] </ref>. Here the point set is the unique landmarks which correspond biologically from object to object. Recently Mardia and Dryden studied the theory of shape in the context of image analysis.
Reference: [3] <author> C.A. Burbeck and S.M. Pizer, </author> <title> "Object representation by cores: identifying and representing primitive spatial regions", </title> <journal> Vision Research, </journal> <volume> 35(13), </volume> <pages> 1917-1930, </pages> <year> 1995. </year>
Reference-contexts: It is evident in psychophysics experiments that early human vision are sensitive to the medial axis [20] <ref> [3] </ref>, and it is also discovered in physiology experiment in monkeys that some neurons in the primary visual cortex compute the symmetric axes as soon as they detect the edge elements [22] To capture the region based properties, the author has defined a novel stochastic algorithm for computing the medial axis
Reference: [4] <author> A. Chakraborty, L.H. Straib, and J.S. Duncan, </author> <title> "Deformable boundary finding influenced by region homogeneity", </title> <booktitle> Proc. of CVPR, </booktitle> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: In Yuille's templates, shapes consist of piecewise conics, and deformations are modeled by the coefficients. Other deformable models of shapes are defined in terms of basis function, such as B-splines, sine waves <ref> [4] </ref>, and implicit polynomials [18] and Super quadratics [30].
Reference: [5] <author> S.J. Dickinson, A.P. Pentland, and A. Rosenfeld, </author> <title> "From volumes to views: an approach to 3D object recognition", CVGIP: Image Understanding, </title> <journal> Vol.55, No.2, </journal> <pages> pp. 130-154, </pages> <month> March </month> <year> 1992. </year>
Reference: [6] <author> J. Elder and S. Zucker, </author> <title> "A measure of closure", </title> <type> Technical Report, </type> <institution> Center for Intelligent Machines, MaGill Univ. </institution> <year> 1994. </year>
Reference-contexts: Similar to Lowe's theory, non-accidental properties are also studied for convexity of a sequence of line segments by Jacobs [32]. Recently a quantitative measure of closure of shape is given by Elder and Zucker <ref> [6] </ref>.
Reference: [7] <author> J. Feldman, </author> <title> "Regularity-based perceptual grouping", </title> <institution> TR-21, Center for Cognitive Science, Rutgers University, </institution> <address> New Brunswick, New Jersey. </address>
Reference: [8] <author> U. Grenander, </author> <title> General Pattern Theory, </title> <publisher> Oxford University Press, </publisher> <year> 1993. </year>
Reference-contexts: the set of landmarks must be extracted from images reliably, and 2). there is a well defined correspondence between the points in the two sets. 2.2 Deformable templates the high level vision Another elegant theory for shape modeling was pioneered by Grenander in a discipline which he named pattern theory <ref> [8] </ref>. In pattern theory, shape primitives are selected, such as edge segments, and then these primitives are arranged in a pre-specified configuration, e.g. a circular graph. Then transformation groups (rotation, scaling etc.) act on the subgraphs to account for global and local deformations.
Reference: [9] <author> U. Grenander, Y. Chow, and K.M. Keenan, </author> <title> Hands: A Pattern Theoretical Study of Biological Shapes, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Then transformation groups (rotation, scaling etc.) act on the subgraphs to account for global and local deformations. For example, the contour of a human hand is represented as a ring of linelets <ref> [9] </ref>, and a global transform specifies the location, size, and orientations of the hand, and local transforms define the relative orientation of fingers, and so on. Then a probability distributions is defined on these groups. This shape modeling scheme has been used in representing leaves, and brain mapping [10].
Reference: [10] <author> U. Grenander and M. Miller, </author> <title> Pattern Theory: From Representation to Inference, </title> <note> 1998 (in press). </note>
Reference-contexts: Then a probability distributions is defined on these groups. This shape modeling scheme has been used in representing leaves, and brain mapping <ref> [10] </ref>. In computer vision deformable templates was proposed by Yuille in studying shapes of human eyes, mouth, and eyebrow etc. [35]. In Yuille's templates, shapes consist of piecewise conics, and deformations are modeled by the coefficients.
Reference: [11] <author> H. </author> <title> Helmholtz, Treatise on Physiological Optics, Dover, </title> <booktitle> New York 1962 (first published in 1867). </booktitle> <pages> 39 </pages>
Reference-contexts: Even worse, the Gestalt psychology only provides a descriptive theory, it does not specify the computational process for achieving the percept from parts to whole. Besides the Gestalt laws, there are two other theories for perceptual organization. One is the likelihood principle <ref> [11] </ref>, which assigns a high probability for grouping two elements, such as line segments, if the placement of the two elements have a low likelihood of resulting from accidental arrangement [24][25].
Reference: [12] <author> J.E. Hochberg, </author> <title> "Effects of the Gestalt revolution: the Cornell symposium on perception", </title> <journal> Psychological Review, </journal> <volume> 64(2), </volume> <pages> 73-84, </pages> <year> 1957. </year>
Reference-contexts: One is the likelihood principle [11], which assigns a high probability for grouping two elements, such as line segments, if the placement of the two elements have a low likelihood of resulting from accidental arrangement [24][25]. The other theory is the simplicity or minimum description length principle <ref> [12] </ref>, which states that perceptual organization should achieve the shortest coding length. We argue that the three theories could be unified into a single framework, provided that the following two conditions are observed. 1.
Reference: [13] <author> D.W. Jacobs, </author> <title> Recognizing 3D objects using 2D images, </title> <type> Unpublished Ph.D. dissertation, </type> <institution> Dept. of EECS, MIT, </institution> <year> 1992. </year>
Reference: [14] <author> E.T. </author> <title> Jaynes, </title> <journal> "Information theory and statistical mechanics", Physical Review 106, </journal> <pages> 620-630, </pages> <year> 1957. </year>
Reference-contexts: From p we choose the one which has the maximum 18 entropy (ME), i.e., p fl () = arg max p () log p ()d subject to the constraint equations, Z Z (ff) The ME principle was originally proposed in statistical mechanics by Jaynes in 1957 <ref> [14] </ref>, in statistical mechanics when a massive physical system reaches equilibrium, its micro-states are subject to a maximum entropy distribution. Since among all distributions in p , p fl () has the highest entropy or randomness in the uncon strained dimensions, it provides an unbiased estimation.
Reference: [15] <author> G. </author> <title> Kanizsa, Organization in Vision, Praeger, </title> <address> New York, </address> <year> 1979. </year>
Reference: [16] <author> M. Kass, A. Witkin, and D. Terzopoulos, "Snakes: </author> <title> active contour models", </title> <booktitle> Proc. of Int'l Conf. on Computer Vision, </booktitle> <address> London, </address> <year> 1987. </year>
Reference-contexts: Thus in early vision tasks, generic shape models are 7 found to be successful. The first important generic shape model in the literature is the active contour model (SNAKE) <ref> [16] </ref>, and the internal energy in the snake model implicitly defines a prior probability distribution of curve (s): p ((s)) = Z expf Z where Z is a normalization constant, _ (s) and (s) are the first and second derivatives of the curve and s is usually the arc-length.
Reference: [17] <author> D. G. Kendall, </author> <title> "A survey of the statistical theory of shape", </title> <journal> Statistical Science, vol.4, No.2, </journal> <pages> 87-120, </pages> <year> 1989. </year>
Reference-contexts: In this section, we shall review the theories in these four areas. 2.1 Statistical theory of shape The term "theory of shape" was coined by David Kendall in 1977 (see <ref> [17] </ref>). In Kendall's theory, a shape is defined as a set of k points in m dimensions. Thus a shape is naturally represented as an m fi k matrix. <p> The real shape space P k m is the quotient of the preshape sphere by some transformation group , e.g. SO (m). Then each SO (m) equivalence class is viewed as a single point-a real shape <ref> [17] </ref>. Then he studies the probability structures in this shape space. For example, assuming independent identical uniform distribution for the k points, Kendall makes inference about the statistical significance of co-linearity in a given point set, and of the size of a hole in the Delaunay tessellation of Galaxy [17], that <p> shape <ref> [17] </ref>. Then he studies the probability structures in this shape space. For example, assuming independent identical uniform distribution for the k points, Kendall makes inference about the statistical significance of co-linearity in a given point set, and of the size of a hole in the Delaunay tessellation of Galaxy [17], that is, how likely the co-linearity or hole is not an accidental arrangement by the iid uniform distribution. <p> Watson in <ref> [17] </ref>), often turn out to be too hard for the most gifted mathematicians to answer. The theory of shape is also studied by Bookstein in morphometrics a discipline studying the deformations and variabilities of biologic organisms [2].
Reference: [18] <author> D. Keren, D. Cooper, and J. Subrahmonia, </author> <title> "Describing complicated objects by implicit polynomials", </title> <journal> IEEE Trans. on PAMI, Vol.16, </journal> <volume> No.1, </volume> <month> January </month> <year> 1994. </year>
Reference-contexts: In Yuille's templates, shapes consist of piecewise conics, and deformations are modeled by the coefficients. Other deformable models of shapes are defined in terms of basis function, such as B-splines, sine waves [4], and implicit polynomials <ref> [18] </ref> and Super quadratics [30]. A more general template model for flexible object is studied by Zhu and Yuille in the FORMS system, in which object shapes are defined in graphs computed from medial axis, and deformation are specified by the PCA modes learned from animal shapes [37].
Reference: [19] <author> K. Koffka, </author> <title> Principles of Gestalt Psychology, </title> <publisher> Harcourt, Brace and Company, </publisher> <address> New York, </address> <year> 1935. </year>
Reference-contexts: Many theories have been proposed to account for this phenomenon, among which the Gestalt psychology is the most influential one. The Gestalt psychologists proposed an enormous number of laws, which govern the grouping from parts to whole <ref> [19] </ref>, for examples, proximity, continuity, co-linearity, co-circularity, parallelism, symmetry, closure, familiarity, ... a b c In Gestalt psychology, the coordination of these rules is guided by the law of Pragnanz, stated in [19]: "of several geometrically possible organizations that one will actually occur which possesses the best, simplest and most stable <p> The Gestalt psychologists proposed an enormous number of laws, which govern the grouping from parts to whole <ref> [19] </ref>, for examples, proximity, continuity, co-linearity, co-circularity, parallelism, symmetry, closure, familiarity, ... a b c In Gestalt psychology, the coordination of these rules is guided by the law of Pragnanz, stated in [19]: "of several geometrically possible organizations that one will actually occur which possesses the best, simplest and most stable shape (p.138)" But what is meant by a good, simple and stable shape? The Gestalt psychologists explained it in terms of the "field forces" by analogy to the field theory of 2
Reference: [20] <author> I. Kovacs and B. Julesz, </author> <title> "Perceptual sensitivity maps within globally defined visual shapes", </title> <booktitle> Nature 371, </booktitle> <pages> 644-646, </pages> <year> 1996. </year>
Reference-contexts: It is evident in psychophysics experiments that early human vision are sensitive to the medial axis <ref> [20] </ref> [3], and it is also discovered in physiology experiment in monkeys that some neurons in the primary visual cortex compute the symmetric axes as soon as they detect the edge elements [22] To capture the region based properties, the author has defined a novel stochastic algorithm for computing the medial
Reference: [21] <author> D.J. </author> <title> Kriegman and T.O. Binford, "Generic models for robot navigation", </title> <booktitle> DARPA Proc. of Image Understanding Workshop, </booktitle> <address> Morgan-Kaufman, </address> <year> 1988. </year>
Reference: [22] <author> T. S. Lee, D. B. Mumford and S.C. Zhu, and V. Lamme. </author> <title> "The role of V1 in shape representation", Computational Neuroscience, Edited by Bower, </title> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: It is evident in psychophysics experiments that early human vision are sensitive to the medial axis [20] [3], and it is also discovered in physiology experiment in monkeys that some neurons in the primary visual cortex compute the symmetric axes as soon as they detect the edge elements <ref> [22] </ref> To capture the region based properties, the author has defined a novel stochastic algorithm for computing the medial axis in a Markov random field in a companion paper [40].
Reference: [23] <author> M. Leyton, </author> <title> Symmetry, Causality, Mind. </title> <publisher> MIT Press. </publisher> <address> Cambridge, Mass. </address> <year> 1992. </year>
Reference: [24] <author> L. D. Lowe, </author> <title> Perceptual organization and visual recognition, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference: [25] <author> D.G. Lowe, </author> <title> "Visual recognition as probabilistic inference from spatial relations", </title> <editor> in A. Blake and T. Troscianko (eds.) </editor> <title> AI and Eye, </title> <publisher> John Wiley & Sons Ltd., </publisher> <year> 1990. </year> <month> 40 </month>
Reference-contexts: These laws play a dominant role in designing algorithms for solving the middle level vision problems, such as, image segmentation, contour tracing, edgel grouping, and occlusion recovery. Although many successful algorithms have been reported for perceptual organization in the literature <ref> [25, 27, 31] </ref>, a rigorous mathematical framework for shape modeling and for integrating the Gestalt laws has yet to be found. 1 One probability distribution is better than the others if it leads to smaller expected coding length, or equivalently it has a smaller Kullback-Leibler divergence from the true distribution. <p> The problem of perceptual organization was first studied by Lowe <ref> [25] </ref>. Lowe pointed out that the goal of grouping is to identify features that are likely to have arisen from some scene property rather than reflecting some accidental arrangement in the image.
Reference: [26] <author> K.V. </author> <title> Mardia and I.L. Dryden, "Statistical analysis of shape data", </title> <journal> Biometrika, </journal> <volume> 76, </volume> <pages> 271-281, </pages> <year> 1989. </year>
Reference: [27] <author> R. Mohan and R. Nevatia, </author> <title> "Perceptual organization for scene segmentation and description", </title> <journal> IEEE Trans on PAMI, vol.14, </journal> <volume> No.6, </volume> <month> June </month> <year> 1992. </year>
Reference-contexts: These laws play a dominant role in designing algorithms for solving the middle level vision problems, such as, image segmentation, contour tracing, edgel grouping, and occlusion recovery. Although many successful algorithms have been reported for perceptual organization in the literature <ref> [25, 27, 31] </ref>, a rigorous mathematical framework for shape modeling and for integrating the Gestalt laws has yet to be found. 1 One probability distribution is better than the others if it leads to smaller expected coding length, or equivalently it has a smaller Kullback-Leibler divergence from the true distribution.
Reference: [28] <author> D. B. Mumford, </author> <title> "Elastica and Computer Vision", in C.L. Bajaj (ed.) Algebraic Geometry and Its Applications, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: An explicit probability model for open curves, called the Elastica model, was first derived from Brownian motion by Mumford <ref> [28] </ref> and it was also studied by Williams and Jacobs [32]: p () = Z expf Z where (s) is the curvature of the curve. In the discrete cases, both equation (1) and equation (2) are defined on interactions between nodes in a local neighborhood, illustrated in figure (2).
Reference: [29] <author> R. L. Ogniewicz, </author> <title> Discrete Voronoi skeleton. </title> <publisher> Hartung-Gorre,1993. </publisher>
Reference-contexts: Details of the definition of the energy functional and the algorithm is referred to [40]. The medial axis computed in this statistical framework has many advantages over the purely geometrically defined ones <ref> [29] </ref>.
Reference: [30] <author> A. P. Pentland, </author> <title> "Perceptual organization and the representation of natural form", </title> <journal> Artificial Intelligence, </journal> <volume> 28 pp293-331, </volume> <year> 1986. </year>
Reference-contexts: In Yuille's templates, shapes consist of piecewise conics, and deformations are modeled by the coefficients. Other deformable models of shapes are defined in terms of basis function, such as B-splines, sine waves [4], and implicit polynomials [18] and Super quadratics <ref> [30] </ref>. A more general template model for flexible object is studied by Zhu and Yuille in the FORMS system, in which object shapes are defined in graphs computed from medial axis, and deformation are specified by the PCA modes learned from animal shapes [37].
Reference: [31] <author> S. Sarkar and K.L. Boyer, </author> <title> "Integration, inference, and management of spatial information using Bayesian networks: perceptual organization", </title> <journal> IEEE Trans on PAMI, </journal> <volume> vol. 15, No. 3, </volume> <month> March </month> <year> 1993. </year>
Reference-contexts: These laws play a dominant role in designing algorithms for solving the middle level vision problems, such as, image segmentation, contour tracing, edgel grouping, and occlusion recovery. Although many successful algorithms have been reported for perceptual organization in the literature <ref> [25, 27, 31] </ref>, a rigorous mathematical framework for shape modeling and for integrating the Gestalt laws has yet to be found. 1 One probability distribution is better than the others if it leads to smaller expected coding length, or equivalently it has a smaller Kullback-Leibler divergence from the true distribution.
Reference: [32] <author> L.R. Williams and D.W. Jacobs, </author> <title> "Stochastic completion fields: a neural model of illusory contour shape and salience", </title> <journal> Neural Computation, </journal> <volume> 9, </volume> <pages> 837-858, </pages> <year> 1997. </year>
Reference-contexts: However it doesn't provide a probability measure for shape, and it is unclear how to de-correlate multiple properties in computing the significance of non-accidental arrangement. Similar to Lowe's theory, non-accidental properties are also studied for convexity of a sequence of line segments by Jacobs <ref> [32] </ref>. Recently a quantitative measure of closure of shape is given by Elder and Zucker [6]. <p> An explicit probability model for open curves, called the Elastica model, was first derived from Brownian motion by Mumford [28] and it was also studied by Williams and Jacobs <ref> [32] </ref>: p () = Z expf Z where (s) is the curvature of the curve. In the discrete cases, both equation (1) and equation (2) are defined on interactions between nodes in a local neighborhood, illustrated in figure (2).
Reference: [33] <author> G. Winkler, </author> <title> Image Analysis, Random Fields, and Dynamic Monte Carlo Methods, </title> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: So fl is updated by d (ff) = (ff) (ff) 20 The whole learning process simulates an inhomogeneous Markov chain <ref> [33] </ref>, and this computational scheme has been successfully applied to texture modeling by Zhu, Wu and Mumford [39]. 6.3 The learnt shape model Suppose we adopt the six features as discussed in section (4), we derive the following shape model, p () = Z expf Z (1) ((s))+ (2) (r (s))+ <p> Detailed discussion of the MCMC method is referred to <ref> [33] </ref>.
Reference: [34] <author> Z.Y. Yang, </author> <title> Visual Binding: Theories and Models, </title> <type> unpublished Ph.D dissertation, </type> <institution> National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Science, </institution> <year> 1997. </year>
Reference-contexts: Similar to Lowe's theory, non-accidental properties are also studied for convexity of a sequence of line segments by Jacobs [32]. Recently a quantitative measure of closure of shape is given by Elder and Zucker [6]. Some good perspectives of the perceptual grouping problem is given from physics <ref> [34] </ref> In the literature of perceptual grouping, attempt has not yet been made for learning and verifying probabilities models from real shapes. 2.4 Active contour models the low level vision In the early stage of vision computation, it is impractical to build shape templates for all objects and match these templates
Reference: [35] <author> A. L. Yuille, </author> <title> "Deformable templates for face recognition", </title> <journal> Journal of Cognitive Neurosciences, Vol.3, </journal> <volume> No.1, </volume> <year> 1991. </year>
Reference-contexts: Then a probability distributions is defined on these groups. This shape modeling scheme has been used in representing leaves, and brain mapping [10]. In computer vision deformable templates was proposed by Yuille in studying shapes of human eyes, mouth, and eyebrow etc. <ref> [35] </ref>. In Yuille's templates, shapes consist of piecewise conics, and deformations are modeled by the coefficients. Other deformable models of shapes are defined in terms of basis function, such as B-splines, sine waves [4], and implicit polynomials [18] and Super quadratics [30].
Reference: [36] <author> S. C. Zhu and A. L. Yuille, </author> <title> "Region competition: Unifying Snake/balloon, Region Growing and Bayes/MDL/Energy for multi-band Image Segmentation", </title> <journal> IEEE Trans. PAMI, vol.18, </journal> <volume> no.9, </volume> <month> Sept. </month> <year> 1996. </year>
Reference-contexts: Therefore the models in the middle level should be compatible with both the low and high level representations. We argue that the Markov random field shape models in this paper can be easily incorporated into the statistical framework for image segmentation <ref> [36] </ref>, and they also provide a mechanism for computing object shapes as well as their medial axes from raw images, and for passing such a description to the high level.
Reference: [37] <author> S. C. Zhu and A. L. Yuille, </author> <title> "FORMS: a Flexible Object Recognition and Modeling System", </title> <journal> Int'l Journal of Computer Vision, Vol.20, </journal> <volume> No.3, </volume> <month> Dec, </month> <year> 1996. </year>
Reference-contexts: A more general template model for flexible object is studied by Zhu and Yuille in the FORMS system, in which object shapes are defined in graphs computed from medial axis, and deformation are specified by the PCA modes learned from animal shapes <ref> [37] </ref>. Other shape models are represented in Bayes networks [21][31][5]. <p> The skeleton forms a graph representation of shape. For example, figure 24 shows a shape model for high level recognition in the FORMS system proposed by Zhu etc. <ref> [37] </ref>. The skeleton of a dog is generated by a grammar, and the seven deformed parts are joined by two hinge joints.
Reference: [38] <author> S.C. Zhu and D.B. Mumford, </author> <title> "Prior learning and Gibbs reaction-diffusion", </title> <journal> IEEE Trans. PAMI, vol.19, </journal> <volume> no.11, </volume> <month> Nov. </month> <year> 1997. </year> <month> 41 </month>
Reference-contexts: We argue that an effective model p () should capture the most common characteristics of natural shapes, as by the Gestalt laws. A quantitative comparison between a generic model and a non-parametric model is studied in the context of image modeling by Zhu and Mumford <ref> [38] </ref>, where local generic properties such as intensity gradients are found to be more important measured in light of an in 3 In the N-space, a modest number for M should be at least in the order of 10 6 , otherwise the non-parametric model are too sparse to be useful <p> Two properties are interesting in figure (6) Firstly, The middle part of (1) obs is close to an exponential curve (see the dashed curve in figure 7.a), but (1) obs have heavier tails. Secondly, unlike the scale invariant properties found in the image intensity of natural images <ref> [38] </ref>, the histogram of curvature is only approximately invariant to scales. This is mainly because the sub-sampling procedure smoothes the curve faster at the high curvature portions than at the low curvature portions.
Reference: [39] <author> S.C. Zhu, Y.N. Wu and D.B. Mumford, </author> <title> "Filters, Random fields, And Maxi--mum Entropy (FRAME): towards a unified theory for texture modeling", </title> <journal> Int'l Journal of Computer Vision 27(2) 1-20, </journal> <volume> March/April, </volume> <year> 1998. </year>
Reference-contexts: When the above dynamical process converges, i.e. d (ff) dt = 0, then p (; ; fl) duplicates the observed statistics. It is well known that fl has a unique solution as log p (; ; fl) is straight concave with respect to fl <ref> [39] </ref>, provided that the constraint equations are consistent. In the above equation, for a given fl, E p (;;fl) [H (ff) ] is hard to compute analytically. <p> So fl is updated by d (ff) = (ff) (ff) 20 The whole learning process simulates an inhomogeneous Markov chain [33], and this computational scheme has been successfully applied to texture modeling by Zhu, Wu and Mumford <ref> [39] </ref>. 6.3 The learnt shape model Suppose we adopt the six features as discussed in section (4), we derive the following shape model, p () = Z expf Z (1) ((s))+ (2) (r (s))+ (3) (r (s))+ (4) (rr (s))+ (5) (r 2 r (s))ds+flkBkg: (14) In equation (14), B is <p> At step k + 1, we should choose the feature which has the largest non-accidental statistics. The distance between (fi) syn could be measured by L 1 norm, or the quadratic form-Mahanalobis distance. In modeling texture <ref> [39] </ref>, Zhu, Wu and Mumford proposed a minimum entropy principle for selecting the optimal set of features from a dictionary of Gabor filters. <p> So we choose not to discuss the feature selection issue explicitly, and the proof for the following proposition is referred to our texture paper <ref> [39] </ref>. Proposition. At each step, choosing feature which has the maximum non-accidental statistics is a steepest descent step of minimizing the Kullback-Leibler distance D (f jj p). In summary, the overall algorithm for shape learning is list below.
Reference: [40] <author> S. C. Zhu, </author> <title> "Stochastic computation of medial axis in Markov random field", </title> <booktitle> Proc. Int'l Conf. Computer Vision and Pattern Recognition, </booktitle> <address> Santa Barbara, CA, </address> <year> 1998. </year> <title> Relevant references are available at the author's web page: </title> <note> http://robotics.stanford.edu/ szhu/publication.html 42 </note>
Reference-contexts: monkeys that some neurons in the primary visual cortex compute the symmetric axes as soon as they detect the edge elements [22] To capture the region based properties, the author has defined a novel stochastic algorithm for computing the medial axis in a Markov random field in a companion paper <ref> [40] </ref>. <p> The optimal (s) is computed by a stochastic algorithm based on local neighborhood in the Markov random field, and the medial axis is computed based on (s). Details of the definition of the energy functional and the algorithm is referred to <ref> [40] </ref>. The medial axis computed in this statistical framework has many advantages over the purely geometrically defined ones [29]. <p> Detailed discussion of the MCMC method is referred to [33]. The algorithm for sampling p (; ; fl) is given as follows. 7 As the mapping function (s) is computed in a Markov random field, thus updating (s) only involves local computations <ref> [40] </ref>. 24 Algorithm I: stochastic algorithm for shape sampling Step 1: initialize = ((x 0 ; y 0 ); :::; (x N1 ; y N1 )). Step 2: initialize the mapping function (s) for . if region based features are chosen in . Step 3: sweep 0. <p> We now discuss how the models can be extended to overcome the third limitation. One can easily compute the medial axes (or skeletons) for the shapes in figure 23 by connecting the centers of the mapping line segments, and a detailed algorithm 37 is referred to a companion paper <ref> [40] </ref>. The skeleton forms a graph representation of shape. For example, figure 24 shows a shape model for high level recognition in the FORMS system proposed by Zhu etc. [37].
References-found: 40

