URL: http://www.cs.pitt.edu/~fiechter/papers/lin_colt97.ps
Refering-URL: http://www.cs.pitt.edu/~fiechter/papers/
Root-URL: 
Email: fiechter@cs.pitt.edu  
Title: PAC Adaptive Control of Linear Systems  
Author: Claude-Nicolas Fiechter 
Address: Pittsburgh Pittsburgh, PA 15260  
Affiliation: Department of Computer Science University of  
Abstract: We consider a special case of reinforcement learning where the environment can be described by a linear system. The states of the environment and the actions the agent can perform are represented by real vectors and the system dynamic is given by a linear equation with a stochastic component. The problem is equivalent to the so-called linear quadratic regulator problem studied in the optimal and adaptive control literature. We propose a learning algorithm for that problem and analyze it in a PAC learning framework. Unlike the algorithms in the adaptive control literature, our algorithm actively explores the environment to learn an accurate model of the system faster. We show that the control law produced by our algorithm has, with high probability, a value that is close to that of an optimal policy relative to the magnitude of the initial state of the system. The time taken by the algorithm is polynomial in the dimension n of the state-space and in the dimension r of the action-space when the ratio n=r is a constant.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arunabha Bagchi. </author> <title> Optimal Control of Stochastic Systems. </title> <publisher> Prentice Hall, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: Such linear systems are widely used to model real-world control applications and have been extensively studied in the optimal and adaptive control literature <ref> [2, 1, 8] </ref>. The main difference between our approach and those found in the adaptive control literature is our emphasis on computational and learning efficiency. <p> Similarly, we say that a system is k-reachable if for every x there is an input sequence that will drive the system from 0 to x in no more than k steps. It is well-known <ref> [1] </ref> and easy to show that the system is k-reachable if and only if the n fi rk reachability matrix defined as G k = [B AB A 2 B : : : A k1 B] has full rank.
Reference: [2] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming and Stochastic Control. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: Such linear systems are widely used to model real-world control applications and have been extensively studied in the optimal and adaptive control literature <ref> [2, 1, 8] </ref>. The main difference between our approach and those found in the adaptive control literature is our emphasis on computational and learning efficiency. <p> control theory is that the linear system (1) with quadratic cost functional (2) can be optimally controlled by a simple linear feedback if the pair (A; B) is controllable and the pair (A; C) is observable, where C = p Q, that is, Q = C T C (see, e.g., <ref> [2] </ref>). In the following we will assume that these two conditions of controllability and observability are satisfied. We will further assume that each of the components in the control vector u plays a role in controlling the system and that none are redundant.
Reference: [3] <editor> Sergio Bittanti, Alan J. Laub, and Jan C. Willems, editors. </editor> <title> The Riccati Equation. Communications and Control Engineering Series. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: The matrix Riccati equation has been extensively studied in the literature and several methods exist to compute its solutions <ref> [3] </ref>. 2.2 LEARNING MODEL When the system to control is completely known the results described in the previous section solve the LQR problem elegantly. However, in practice, very often the system equations are not exactly known or contains parameters that are not exactly known.
Reference: [4] <author> Claude-Nicolas Fiechter. </author> <title> Efficient reinforcement learning. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: In the simplest and most general case of reinforcement learning, the policy is expressed as a simple mapping from states of the environment into actions. Many well-known algorithms for the reinforcement learning problem, like Q-learning [10], use this "table-based" representation and in a previous paper <ref> [4] </ref> we showed that it is possible to PAC learn such a mapping in time polynomial in the number of states of the environment. Unfortunately, very often in practice the state-space is very large or infinite and table-based methods are inapplicable or extremely inefficient.
Reference: [5] <author> Claude-Nicolas Fiechter. </author> <title> Expected mistake bound model for on-line reinforcement learning. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: During the learning phase the algorithm does not take into consideration the costs associated with the actions and only tries to explore its environment as much as possible to build an accurate model. Using results in <ref> [5] </ref> it is easy to convert this algorithm into an efficient on-line algorithm that performs the task at the same time that it learns, and balances the need for exploration with the constraints of the exploitation.
Reference: [6] <author> Graham C. Goodwin and Kwai Sang Sin. </author> <title> Adaptive Filtering Prediction and Control. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1984. </year>
Reference-contexts: The main difference between our approach and those found in the adaptive control literature is our emphasis on computational and learning efficiency. Results in the control literature typically consider the convergence of an adaptive control scheme to an optimal control in the limit, as time goes to infinity <ref> [6, 8] </ref>. In this paper we are interested in PAC learning results [9]. Specifically we show that our learning algorithm computes an (":ffi)-approximation of an optimal policy in a reasonable (polynomial) amount of time. <p> The idea of using linear regression to estimate the unknown parameters of the systems and to use these estimates to compute a control law as if the parameters were exactly known is not new in adaptive control. It is the approach taken in the self-tuning regulator control scheme <ref> [6, 8] </ref>. The difference is that here the learning algorithm will actively explore the state-space to quickly obtain a good approximation of the unknown parameters. Self-tuning regulators, on the contrary, always choose the control that looks best from an exploitation point of view and only learn "passively".
Reference: [7] <author> L. P. Kaelbling, M. L. Littman, and A. W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: 1 INTRODUCTION Reinforcement learning considers the problem of learning to perform a task in an unknown environment by trial-and-error <ref> [7] </ref>. At each point in time, the learning agent gets some information about the state of the environment from its sensors and, based on this information, selects and performs an action.
Reference: [8] <author> Edoardo Mosca. </author> <title> Optimal, Predictive, and Adaptive Control. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1995. </year>
Reference-contexts: Such linear systems are widely used to model real-world control applications and have been extensively studied in the optimal and adaptive control literature <ref> [2, 1, 8] </ref>. The main difference between our approach and those found in the adaptive control literature is our emphasis on computational and learning efficiency. <p> The main difference between our approach and those found in the adaptive control literature is our emphasis on computational and learning efficiency. Results in the control literature typically consider the convergence of an adaptive control scheme to an optimal control in the limit, as time goes to infinity <ref> [6, 8] </ref>. In this paper we are interested in PAC learning results [9]. Specifically we show that our learning algorithm computes an (":ffi)-approximation of an optimal policy in a reasonable (polynomial) amount of time. <p> The idea of using linear regression to estimate the unknown parameters of the systems and to use these estimates to compute a control law as if the parameters were exactly known is not new in adaptive control. It is the approach taken in the self-tuning regulator control scheme <ref> [6, 8] </ref>. The difference is that here the learning algorithm will actively explore the state-space to quickly obtain a good approximation of the unknown parameters. Self-tuning regulators, on the contrary, always choose the control that looks best from an exploitation point of view and only learn "passively".
Reference: [9] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communication of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: Results in the control literature typically consider the convergence of an adaptive control scheme to an optimal control in the limit, as time goes to infinity [6, 8]. In this paper we are interested in PAC learning results <ref> [9] </ref>. Specifically we show that our learning algorithm computes an (":ffi)-approximation of an optimal policy in a reasonable (polynomial) amount of time.
Reference: [10] <author> C. Watkins. </author> <title> Learning From Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: In the simplest and most general case of reinforcement learning, the policy is expressed as a simple mapping from states of the environment into actions. Many well-known algorithms for the reinforcement learning problem, like Q-learning <ref> [10] </ref>, use this "table-based" representation and in a previous paper [4] we showed that it is possible to PAC learn such a mapping in time polynomial in the number of states of the environment.
References-found: 10

