URL: http://actor.cs.vt.edu/~lhuang/nopno.ps
Refering-URL: http://csgrad.cs.vt.edu/~lhuang/
Root-URL: http://www.cs.vt.edu
Email: kafura@cs.vt.edu  lhuang@cs.vt.edu  
Title: Collective Communication and Communicators in mpi++  
Author: Dr. Dennis Kafura Liya Huang 
Address: VA 24061-0106  VA 24061-0106  
Affiliation: Department of Computer Science Virginia Tech Blacksburg,  Department of Computer Science Virginia Tech Blacksburg,  
Abstract: This paper describes the current version of mpi++, a C++ language binding for MPI, that includes all of the collective services, and services for contexts, groups and communicators as described in Chapter 4 and 5 of the MPI standard. The code for mpi++ has been tested on a Sun Sparc work station and an Intel Paragon. Segments of a mpi++ program implementing a parallel algorithm is introduced to illustrate the Collective class hierarchy. The paper also shows how mpi++ deals with other collective operations (e.g., reduction), attribute caching, groups,and communicators. The class hierarchy of mpi++ is presented and briefly explained. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. V. Bangalore, N. E. Doss, and A. Skjellum. </author> <title> Mpi++: </title> <booktitle> Issues and features. </booktitle> <pages> pages 323-338, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: It provides lightweight interface classes, introduces a minimal use of advanced features of C++. * OOMPI Object Oriented MPI provides a C++ class library that is not a simple one-to-one mapping of MPI function to language binding, and may add, lose, or change MPI-1 specified functionality [6]. * MPI++ <ref> [1] </ref> is a portable C++ interface to MPI that can be used to build numerical libraries to support object-oriented programming. <p> This function performs the global prefix sum of departure time. 1 class DmatrixType_Parameter - 2 public: 3 static int count; 4 ... 5 static MPI_Datatype *typeArray; 6 static void type_array () 7 -typeArray [0]=typeArray <ref> [1] </ref> =... 8 =DOUBLE::type_of ();- ... -; 9 int DmatrixType_Parameter::count=4; 10 ... 11 MPI_Datatype *DmatrixType_Parameter:: 12 typeArray= new MPI_Datatype [4]; 13 typedef Struct&lt;DmatrixType_Parameter&gt; 14 DmatrixType; 15 typedef Contiguous &lt;DmatrixType, 2&gt; 16 DepartureScanType; ... 17 class DepartureTimes :public 18 Reduction&lt;DepartureScanType, DepartureScanType&gt; 19 -public: 20 void 21 user_func (Dmatrix *in,Dmatrix *out,int *len) 22 <p> -; 9 int DmatrixType_Parameter::count=4; 10 ... 11 MPI_Datatype *DmatrixType_Parameter:: 12 typeArray= new MPI_Datatype [4]; 13 typedef Struct&lt;DmatrixType_Parameter&gt; 14 DmatrixType; 15 typedef Contiguous &lt;DmatrixType, 2&gt; 16 DepartureScanType; ... 17 class DepartureTimes :public 18 Reduction&lt;DepartureScanType, DepartureScanType&gt; 19 -public: 20 void 21 user_func (Dmatrix *in,Dmatrix *out,int *len) 22 -double temp; 23 out [0]=out <ref> [1] </ref>; 24 temp=out [1].upper_left+in [1].upper_left; 25 out [1].upper_left=(temp&gt;in [1].lower_left 26 lower_left)?temp:in [1].lower_left;--; ... 27 DepartureTimes depart; 28 Dmatrix scan_msg [2], *glob_sum; 29 scan_msg [1]=D [num_jobs-1]; 30 glob_sum=depart.Scan (scan_msg); In line 27 above, a user-defined MPI reduction operator is created during the construction of a class instance depart of Reduction class template, <p> In line 28, the send data in Scan operation, scan msg is an array of 2 Dmatrix elements. The local sum of departure time in D [num jobs-1] is assigned to scan msg <ref> [1] </ref> (line 29), and then the Scan reduction operation is performed (line 30). During the Scan reduction operation, user func () computes the global prefix sum of departure time (lines 20-26).
Reference: [2] <author> O. Coulaud and E. Dillon. </author> <title> Para++: C++ bindings for message passing libraries user guide. </title> <year> 1995. </year>
Reference-contexts: It provides a parallel programming model without the benefit of compiler support. * PARA++ <ref> [2] </ref> is a generic high-level interface for performing message passing in C++ based on an idea of Stream Message Passing. Its library was originally defined for a domain decomposition method to solve the laplace operator with spectral methods. <p> 15 typedef Contiguous &lt;DmatrixType, 2&gt; 16 DepartureScanType; ... 17 class DepartureTimes :public 18 Reduction&lt;DepartureScanType, DepartureScanType&gt; 19 -public: 20 void 21 user_func (Dmatrix *in,Dmatrix *out,int *len) 22 -double temp; 23 out [0]=out [1]; 24 temp=out [1].upper_left+in [1].upper_left; 25 out [1].upper_left=(temp&gt;in [1].lower_left 26 lower_left)?temp:in [1].lower_left;--; ... 27 DepartureTimes depart; 28 Dmatrix scan_msg <ref> [2] </ref>, *glob_sum; 29 scan_msg [1]=D [num_jobs-1]; 30 glob_sum=depart.Scan (scan_msg); In line 27 above, a user-defined MPI reduction operator is created during the construction of a class instance depart of Reduction class template, where user func () is associated with the operator.
Reference: [3] <author> A. G. Greenberg, B. D. Lubachevsky, and I. Mitrani. </author> <title> Un-boundedly parallel simulations via recurrence relations. </title> <address> pages 1-12, </address> <year> 1990. </year>
Reference-contexts: All the attributes in Collective classes can be inherited by Reduction classes. Parameters needed by reduction functions have the same pattern as in Collective classes. The mpi++ Collective communication classes are illustrated throughout this section by showing the GLM algorithm [8] that simulates a G/G/1 queueing model <ref> [3] </ref>. Two parameters, lambda and mu, characterise the mean interar-rival and service times, respectively. <p> The departure time sequence, D, is a double type of array of num jobs elements. A data structure Dmatrix of 4 double numbers represents one element of D. The computation of local departure time for each event is an operation between two 2x2 matrices of type Dmatrix <ref> [3] </ref>. The following algorithm computes the local prefix sum of departure time sequence D. Note it is not exactly the same algorithm as in [3], rather it is a modified version merely for readability, although it returns the same result as the original algorithm. <p> The computation of local departure time for each event is an operation between two 2x2 matrices of type Dmatrix <ref> [3] </ref>. The following algorithm computes the local prefix sum of departure time sequence D. Note it is not exactly the same algorithm as in [3], rather it is a modified version merely for readability, although it returns the same result as the original algorithm.
Reference: [4] <author> D. G. Kafura and L. Huang. </author> <title> mpi++ : A c++ language binding for mpi. </title> <month> June </month> <year> 1995. </year>
Reference-contexts: The version of mpi++ described in this paper includes all of the MPI services for collective communication as described in Chapter 4 of the MPI standard, and also the services for groups, communicators, and attribute caching (Chapter 5). An earlier paper about mpi++ <ref> [4] </ref> introduced the point-to-point communication services defined in Chapter 3 of the MPI standard. Subsequent versions of mpi++ will incorporate additional MPI services, such as process topologies, environmental management, and profiling interface. <p> The TypeName classes and its descendents are designed to keep information necessary for the Collective classes. The definition of Type-Name class and its subclasses can be found in <ref> [4] </ref>. 1 template&lt;class SendType, class RecvType&gt; 2 class Collective - 3 protected: 4 typedef SendType::Type CsendType; 5 typedef RecvType::Type CrecvType; 6 CrecvType *recvData; 7 int recvCount; 8 Communicator comm; 9 int root, rank, Error; 10 public: 11 Collective (int r=0,Communicator c= 12 Communicator ()); 13 Collective (); 14 int barrier (); <p> the global prefix sum of departure time. 1 class DmatrixType_Parameter - 2 public: 3 static int count; 4 ... 5 static MPI_Datatype *typeArray; 6 static void type_array () 7 -typeArray [0]=typeArray [1] =... 8 =DOUBLE::type_of ();- ... -; 9 int DmatrixType_Parameter::count=4; 10 ... 11 MPI_Datatype *DmatrixType_Parameter:: 12 typeArray= new MPI_Datatype <ref> [4] </ref>; 13 typedef Struct&lt;DmatrixType_Parameter&gt; 14 DmatrixType; 15 typedef Contiguous &lt;DmatrixType, 2&gt; 16 DepartureScanType; ... 17 class DepartureTimes :public 18 Reduction&lt;DepartureScanType, DepartureScanType&gt; 19 -public: 20 void 21 user_func (Dmatrix *in,Dmatrix *out,int *len) 22 -double temp; 23 out [0]=out [1]; 24 temp=out [1].upper_left+in [1].upper_left; 25 out [1].upper_left=(temp&gt;in [1].lower_left 26 lower_left)?temp:in [1].lower_left;--; ... 27
Reference: [5] <author> C. P. Kruskal. </author> <title> Searching, merging, and sorting in parallel computation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(10), </volume> <month> Octo-ber </month> <year> 1983. </year>
Reference: [6] <author> A. Lumsdaine, B. C. McCandless, and J. M. Squyres. </author> <title> C++ class library: Object oriented mpi (oompi). </title> <month> March </month> <year> 1996. </year> <note> CSE Technical Report TR96-10, URL: http://www.cse.nd.edu/ lsc/research/oompi/. </note>
Reference-contexts: It provides lightweight interface classes, introduces a minimal use of advanced features of C++. * OOMPI Object Oriented MPI provides a C++ class library that is not a simple one-to-one mapping of MPI function to language binding, and may add, lose, or change MPI-1 specified functionality <ref> [6] </ref>. * MPI++ [1] is a portable C++ interface to MPI that can be used to build numerical libraries to support object-oriented programming.
Reference: [7] <author> U. </author> <title> of Tennessee. Mpi: A message-passing interface standard, </title> <note> version 1.1. </note> <year> 1995. </year>
Reference-contexts: 1. Introduction The Message Passing Interface (MPI) standard provides a common interface for message-based communication and synchronization among processes executing on distributed memory parallel computers and networks of workstations <ref> [7] </ref>. MPI functionality includes point-to-point and collective communication routines, support for process groups, communication contexts, and application topologies. For application developers to use the MPI services, currently C and FORTRAN are the two available language bindings. Several groups are developing C++ bindings. <p> Collective Communications This section introduces the mpi++ classes that capture the collective message passing services of MPI. An example of computation for a queueing system (a G/G/1 queue)is described to illustrate the Collective class hierarchy. 2.1. Overview of Collective Communication Classes Collective communication involves a group of processes <ref> [7] </ref>. The functions can be categorized as: * Communication and synchronization operations, e.g., barrier, broadcast, gather, scatter. Such functions provide collection communication services across a group of processes. <p> Groups, Attributes, and Communicators This section introduces the classes in mpi++ that capture the group and communicator services defined in Chapter 5 of the MPI standard. 3.1. Overview Groups define an ordered collection of processes, each with a rank <ref> [7] </ref>, a low-level name for inter-process communication. Groups define a scope for process names in point-to-point communication, and collective operations. A communicator encapsulates a group, and user defined attributes [7]. <p> Overview Groups define an ordered collection of processes, each with a rank <ref> [7] </ref>, a low-level name for inter-process communication. Groups define a scope for process names in point-to-point communication, and collective operations. A communicator encapsulates a group, and user defined attributes [7]. There are two kind of communicators: intra-communicators for operations within a single group of processes, and inter-communicators, for point-to-point communication between two groups of processes. The caching facility allows an application to attach arbitrary pieces of information, called attributes, to communicators.
Reference: [8] <author> J. J. Wang and M. Abrams. </author> <title> Massively time-parallel, approximate simulation of loss queueing systems. </title> <year> 1994. </year>
Reference-contexts: All the attributes in Collective classes can be inherited by Reduction classes. Parameters needed by reduction functions have the same pattern as in Collective classes. The mpi++ Collective communication classes are illustrated throughout this section by showing the GLM algorithm <ref> [8] </ref> that simulates a G/G/1 queueing model [3]. Two parameters, lambda and mu, characterise the mean interar-rival and service times, respectively.
References-found: 8

