URL: http://www.ai.mit.edu/people/cohn/SAL95/Contributions/cohn.ps
Refering-URL: http://www.ai.mit.edu/people/cohn/SAL95/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: cohn@psyche.mit.edu  
Title: "What is the best thing to do right now?": getting beyond greedy exploration  
Author: David A. Cohn 
Note: Address as of October:  
Address: Cambridge, MA 02139  One Cambridge Center, Cambridge, MA 02142  
Affiliation: MIT Center for Biological and Computational Learning E10-243, Massachusetts Institute of Technology  Harlequin, Inc.,  
Abstract-found: 0
Intro-found: 1
Reference: <author> Cohn, D. A.; Ghahramani, Z.; and Jordan, M. I. </author> <year> 1995. </year> <title> Active learning with statistical models. </title> <editor> In Tesauro, G.; Touretzky, D.; and Leen, T., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <address> MIT Press. </address> <note> Expanded version available at ftp://psyche.mit.edu/pub/cohn/nips94.ps.Z. </note>
Reference-contexts: I will assume the learner knows nothing of the problem structure, and is using either a neural network or one of the statistical models described in <ref> (Cohn, Ghahramani, & Jordan 1995) </ref>. The advantage of using one of these models is that they allow derivation of a statistical criterion which we can use to guide our exploration. <p> In practice, this approach yields exploration strategies that greatly outperform random search and many good heuristics (Figure 2). when exploring according to the variance-minimizing strategy described here and various competing heuristic strategies <ref> (Cohn, Ghahramani, & Jordan 1995) </ref>. Limitations of the current approaches The current statistical approaches to data selection in the machine learning community (as cited above) suffer two major limitations. First, the estimated loss criterion requires assuming an unbiased learner, whether or not that assumption is justified. <p> In most cases, this assumption is unwarranted, and in some cases can have dire effects (see the expanded version of (Cohn 1994) for an example). For some learning algorithms, there are data selection methods that explicitly account for bias <ref> (Cohn 1995) </ref>, but for many learners, no such methods exist yet. In these notes, I will ignore the bias problem and concentrate on the problem of exploration strategies.
Reference: <author> Cohn, D. A. </author> <year> 1994. </year> <title> Neural network exploration using optimal experiment design. </title> <editor> In Cowan, J.; Tesauro, G.; and Alspector, J., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <address> Mor-gan Kaufmann. </address> <note> Expanded version available as ftp://psyche.mit.edu/pub/cohn/statmodels.ps.Z. </note>
Reference-contexts: Bias assumptions Selecting data to minimize model variance implicitly assumes that the learner is unbiased. In most cases, this assumption is unwarranted, and in some cases can have dire effects (see the expanded version of <ref> (Cohn 1994) </ref> for an example). For some learning algorithms, there are data selection methods that explicitly account for bias (Cohn 1995), but for many learners, no such methods exist yet. In these notes, I will ignore the bias problem and concentrate on the problem of exploration strategies. <p> time step, what action will produce the greatest reduction in loss?" ignoring important parts of the domain because it must take several non-greedy steps to get to them. (right) Experimental evidence suggests that this behavior is not pervasive, and that greedy learners can reliably find efficient trajectories in some domains <ref> (Cohn 1994) </ref>. In input-constrained domains such as our toy arm, greedy search can have potentially pathological behavior. Because reaching unexplored parts of such a domain may require taking several non-greedy steps, a greedy learner may box itself in (see Figure 3).
Reference: <author> Cohn, D. </author> <year> 1995. </year> <title> Minimizing statistical bias with queries. </title> <note> Unpublished; available electronically as ftp://psyche.mit.edu/pub/cohn/bias.ps.Z. </note>
Reference-contexts: I will assume the learner knows nothing of the problem structure, and is using either a neural network or one of the statistical models described in <ref> (Cohn, Ghahramani, & Jordan 1995) </ref>. The advantage of using one of these models is that they allow derivation of a statistical criterion which we can use to guide our exploration. <p> In practice, this approach yields exploration strategies that greatly outperform random search and many good heuristics (Figure 2). when exploring according to the variance-minimizing strategy described here and various competing heuristic strategies <ref> (Cohn, Ghahramani, & Jordan 1995) </ref>. Limitations of the current approaches The current statistical approaches to data selection in the machine learning community (as cited above) suffer two major limitations. First, the estimated loss criterion requires assuming an unbiased learner, whether or not that assumption is justified. <p> In most cases, this assumption is unwarranted, and in some cases can have dire effects (see the expanded version of (Cohn 1994) for an example). For some learning algorithms, there are data selection methods that explicitly account for bias <ref> (Cohn 1995) </ref>, but for many learners, no such methods exist yet. In these notes, I will ignore the bias problem and concentrate on the problem of exploration strategies.
Reference: <author> Dayan, P., and Sejnowski, T. </author> <year> 1995. </year> <title> Exploration bonuses and dual control. </title> <note> Available electronically as ftp://ftp.cs.toronto.edu/pub/dayan/exp.ps.Z. </note>
Reference-contexts: is then optimized, with the action at each step being iteratively perturbed toward optimality, given the assumption that the learner also takes all preceding and subsequent steps, and that all other steps result in their 3 The same idea has been used quite successfully to balance exploration and exploitation; see <ref> (Dayan & Sejnowski 1995) </ref> and the references contained therein for examples. 4 If we are assuming that the model is unbiased, then (2) amounts to assuming that all model predictions will remain unchanged. expected gains.
Reference: <author> Dreyfus, S. </author> <year> 1965. </year> <title> Dynamic Programming and the Calculus of Variations. </title> <publisher> Academic Press. </publisher>
Reference-contexts: In the limit, the process approximates the (unrealizable) Bayesian formulation over trajectories, maximizing over a distribution of gains for the first step, a distribution of distributions for the second, etc. Sequentially-refined exploration has parallels in the field of adaptive control, specifically open-loop optimal feedback control <ref> (Dreyfus 1965) </ref>. In this setting, it may be viewed as open-loop optimal feedback exploration. It is worth noting that, although illustrated here for a domain with constrained inputs, the sequentially-refined approach is equally applicable to domains where sequential inputs are unconstrained.
Reference: <author> Fedorov, V. </author> <year> 1972. </year> <title> Theory of Optimal Experiments. </title> <publisher> Academic Press. </publisher>
Reference-contexts: Gaussian noise, then the expected gain (in terms of model variance) from each example would be independent of the result of the experiment. We could thus compute the expected gains of a complete sequence of actions, and could select an optimal trajectory prior to actually taking any steps (see <ref> (Fedorov 1972) </ref> for details). Greedy designs In most circumstances, it is not possible create an optimal exact design from which to learn. Life (like our robot arm) is highly nonlinear, and noise, even when approximately Gaussian, is rarely i.i.d.
Reference: <author> Geman, S.; Bienenstock; and Doursat, R. </author> <year> 1992. </year> <title> Neural networks and the bias/variance dilemma. </title> <booktitle> Neural Computation 4 </booktitle> <pages> 1-58. </pages>
Reference-contexts: However, in the absence of an external "test set," we can not measure the learner's loss, let alone measure how an action will affect it. We can approach the problem by decomposing the MSE into bias and variance terms <ref> (Geman, Bienenstock, & Doursat 1992) </ref>. If we have reason to be 2 The problems that such constraints pose are illustrated in the next section. lieve that our learner is approximately unbiased, then we can use model variance as an estimate for MSE.
Reference: <author> MacKay, D. J. </author> <year> 1992. </year> <title> Information-based objective functions for active data selection. </title> <booktitle> Neural Computation 4(4) </booktitle> <pages> 590-604. </pages>
Reference: <author> Paass, G., and Kindermann, J. </author> <year> 1995. </year> <title> Bayesian query construction for neural network models. </title> <editor> In Tesauro, G.; Touretzky, D.; and Leen, T., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Salganicoff, M.; Kunin, L.; and Ungar, L. </author> <year> 1994. </year> <title> Active exploration based ID-3 learning for robot grasping. </title> <booktitle> In Workshop on Robot Learning Eleventh International Conference on Machine Learning. </booktitle>
Reference: <author> Sollich, P. </author> <year> 1994. </year> <title> Query construction, entropy and generalization in neural network models. </title> <journal> Physical Review E 49 </journal> <pages> 4637-4651. </pages> <note> Available as ftp://archive.cis.ohio-state.edu/pub/neuroprose/sollich.queries.ps.Z. </note>
Reference-contexts: Because reaching unexplored parts of such a domain may require taking several non-greedy steps, a greedy learner may box itself in (see Figure 3). Al--though experimental results suggest that this pathological behavior is not be pervasive, it is still clear that the greedy approach is not optimal. Sollich <ref> (Sollich 1994) </ref> considers the theoretical relationship between greedy and globally optimal strategies for several simple problems. A hybrid approach: Sequentially-refined design The problem with the greedy approach is that it ignores the effect that immediate actions have on subsequent gains.
References-found: 11

