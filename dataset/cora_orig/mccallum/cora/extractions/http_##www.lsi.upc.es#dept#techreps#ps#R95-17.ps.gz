URL: http://www.lsi.upc.es/dept/techreps/ps/R95-17.ps.gz
Refering-URL: http://www.lsi.upc.es/dept/techreps/1995.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: alquezar@lsi.upc.es, sanfeliu@ic.upc.es  
Title: AUGMENTED REGULAR EXPRESSIONS: A FORMALISM TO DESCRIBE, RECOGNIZE AND LEARN A CLASS OF CONTEXT-SENSITIVE LANGUAGES.  
Author: Rene Alquezar and Alberto Sanfeliu (flfl) 
Affiliation: Dept. de Llenguatges i Sistemes Informatics, Universitat Politecnica de Catalunya (flfl) Institut de Cibernetica, Universitat Politecnica de Catalunya CSIC  
Abstract: In order to extend the potential of application of the syntactic approach to pattern recognition, the efficient use of models capable of describing context-sensitive structural relationships is needed. Moreover, the ability to learn such models from examples is interesting to automate as much as possible the development of applications. In this paper, a new formalism that permits to describe a non-trivial class of context-sensitive languages, the Augmented Regular Expressions (AREs), is introduced. AREs augment the descriptive power of regular expressions by including a set of constraints that involve the number of instances of the operands of the star operations in each string of the language. Likewise, algorithms are given to infer AREs from string examples and to recognize language strings by AREs. The method for learning AREs consists of a regular grammatical inference step, aimed at obtaining a regular superset of the target language, followed by a constraint induction process, which reduces the extension of the inferred language transforming it into a context-sensitive one. Hence, this two-step approach avoids the difficulty of learning context-sensitive grammars directly from the data. The method for recognizing language strings is also splitted in two stages: matching the underlying regular expression and checking that the resulting star instances satisfy the constraints. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. Alquezar and A. Sanfeliu, </author> <title> "Incremental grammatical inference from positive and negative data using unbiased finite state automata," </title> <booktitle> in Proc. of the IAPR Int. Workshop on Structural and Syntactic Pattern Recognition, </booktitle> <address> SSPR'94, Nahariya, Israel, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The most part of the work in GI has been devoted to the problem of inferring regular grammars (or finite-state automata) [15, 21]. Nevertheless, just a few methods have been proposed for regular GI from complete presentation, either in the classical symbolic approach <ref> [1, 14, 22] </ref> or in conjunction with other approaches such as genetic search [9] and recurrent neural networks [2, 12, 28]. The most part of the reported regular GI methods are heuristic techniques that use only positive examples and some a-priori knowledge (e.g. [18, 25]). <p> Method 1 below is a little more detailed description of the learning procedure, with references to the algorithms proposed in the next sections to implement all the processing steps, except the regular grammatical inference step, for which a wide range of algorithms is available <ref> [1, 9, 15, 22, 25, 28] </ref>. This must not be interpreted, however, as if the choice of the regular GI method were irrelevant. <p> Let the subpath of states associated with the substring be p = p 0 :::p r , where we know that p 0 = i, p r = j and 8t 2 <ref> [1; r 1] </ref> : (p t &gt; i) ^ (p t &gt; j). If r = 1 then the term ff n ij is selected as the matched term. <p> The definition of V restricts the allowed values for the star variables to natural numbers, 8k 2 <ref> [1; ns] </ref> : v k 2 N . Consequently, the set of linear relations L is only well-defined when the involved variables take natural numbers as values. <p> A set L of linear relations among the star variables in V . Outputs: A boolean variable satisf y constraints whose value will be TRUE if and only if the star instances SI satisfy all the constraints in L. begin ASI <ref> [1] </ref> := SI f The set SI is placed in the first position of an array ASI just for compatibility with the arguments of the functions determine ancestor and coefficients and sum of star exponents. g f Mark the star variables for which the housing ancestor has not been computed g <p> ; node; vdep id; ASI; 1; common housing anc id; anc id; coef [vdep id]) f returns anc id, the identifier of the housing ancestor of node, and coef [vdep id], the coefficients given by the exponents of v vdep id for each instance of v anc id in ASI <ref> [1] </ref> g housing ancestor of node [vdep id] := anc id if number of actual exponents (coef [vdep id]) &gt; 0 then if housing ancestor of node [vdep id] 6= common housing anc id then satisf y constraints := F ALSE else j := 1; l := length (list of indep <p> ; node; v id; ASI; 1; common housing anc id; anc id; coef [v id]) f returns anc id, the identifier of the housing ancestor of node, and coef [v id], the coefficients given by the exponents of v v id for each instance of v anc id in ASI <ref> [1] </ref> g housing ancestor of node [v id] := anc id end if if housing ancestor of node [v id] 6= common housing anc id then satisf y constraints := F ALSE end if end while if satisf y constraints then row max number := sum of star exponents (ASI; common <p> row max number] do coef f icient := coef [star id][i] f for each instance of star variable v star id write the corresponding row of coefficients of the independent variables g if coef f icient 0 then nrow := nrow + 1 B [nrow] := coef f icient A <ref> [nrow; 1] </ref> := 1 current elem := first element (list of indep star variables) for j in [2 :: column max number] do v id := read value (list of indep star variables; current elem) exponent := coef [v id][i] if exponent &lt; 0 then exponent := 0 f convert unassigned
Reference: 2. <author> R. Alquezar and A. Sanfeliu, </author> <title> "An algebraic framework to represent finite-state machines in single-layer recurrent neural networks," </title> <note> to appear in Neural Computation 7, </note> <year> 1995. </year>
Reference-contexts: Nevertheless, just a few methods have been proposed for regular GI from complete presentation, either in the classical symbolic approach [1, 14, 22] or in conjunction with other approaches such as genetic search [9] and recurrent neural networks <ref> [2, 12, 28] </ref>. The most part of the reported regular GI methods are heuristic techniques that use only positive examples and some a-priori knowledge (e.g. [18, 25]).
Reference: 3. <author> D. Angluin, </author> <title> "Inductive inference of formal languages from positive data," </title> <booktitle> Information & Control 45, </booktitle> <address> pp.117-135, </address> <year> 1980. </year>
Reference-contexts: It is well-known that any enumerable class of recursive languages (context-sensitive and below) can be identified in the limit from complete presentation (both positive and negative data), whereas only certain non-superfinite 2 classes of languages (below regular languages) can be identified from a positive presentation <ref> [3, 13] </ref>. The presentation of a language L by queries is an oracle that answers, for any query s, whether s 2 L or not. The inference methods for this type of presentation yield a description D of L after making some queries about L.
Reference: 4. <author> D. Angluin, </author> <title> "Finding patterns common to a set of strings," </title> <journal> J. Comput. System Science 21, </journal> <volume> pp.46-62, </volume> <year> 1980. </year>
Reference-contexts: Not surprisingly, no other paper following this line of research has been reported since then [21, 31]. Finally, the inductive inference of pattern languages has been studied <ref> [4, 20] </ref>. <p> In fact, L (CSG 4 ) corresponds to the pattern language xx over the binary alphabet = f0; 1g, where the variable x stands for any string in + <ref> [4] </ref>. The ARE (0+1) v 1 (0+1) v 2 with fv 2 = v 1 g cannot express that the substrings associated with the instances of the operands of the stars denoted by v 1 and v 2 are identical. <p> This simple example shows that pattern languages <ref> [4] </ref> are not covered by AREs. On the other hand, it is obvious that the class of pattern languages does not cover the languages represented by AREs. <p> However, it has been demonstrated that not all the context-sensitive languages are covered by AREs. In addition, it has been shown that AREs and pattern languages <ref> [4] </ref> are not comparable.
Reference: 5. <author> D. Angluin, </author> <title> "A note on the number of queries needed to identify regular languages," </title> <booktitle> Information & Control 51, </booktitle> <address> pp.76-87, </address> <year> 1981. </year>
Reference-contexts: The inference methods for this type of presentation yield a description D of L after making some queries about L. A mixed presentation by both examples and queries is also possible, in which the examples are used to infer an initial description to be refined later by queries <ref> [5, 23] </ref>. The most part of the work in GI has been devoted to the problem of inferring regular grammars (or finite-state automata) [15, 21].
Reference: 6. <author> D. Angluin and C.H. Smith, </author> <title> "Inductive inference: theory and methods," </title> <booktitle> ACM Computing Survey 15 (3), </booktitle> <address> pp.237-269, </address> <year> 1983. </year>
Reference-contexts: The reported methods can be classified depending on the class of languages they are able to infer and depending on whether the language is presented by examples or by queries <ref> [6] </ref>.
Reference: 7. <author> D.N. Arden, </author> <title> "Delay logic and finite state machines," </title> <booktitle> in Proc. Second Ann. Symp. on Switching Theory and Logical Design, </booktitle> <month> October </month> <year> 1961, </year> <month> pp.133-151. </month>
Reference-contexts: By selecting a specific algorithm, a deterministic mapping can be established from FSA to REs, this is, a canonical RE R can be chosen for each FSA A, R = (A). The algorithm that is described next is based on a method due to Arden <ref> [7, 17] </ref>, but a final step has been appended to simplify the resulting regular expression. 4.1. A basic algorithm to find a regular expression equivalent to a given FSA.
Reference: 8. <author> S.M. Chou and K.S.Fu, </author> <title> "Inference for transition network grammars," </title> <booktitle> in Proc. of Int. Joint Conf. on Pattern Recognition, 3, </booktitle> <address> CA, </address> <year> 1976, </year> <month> pp.79-84. </month>
Reference-contexts: A drawback of Takada's method is that the classes of learnable context-sensitive languages are quite restricted, since the languages are generated through a sequence of (controlled) universal even linear grammars. On the other hand, an early work by Chou and Fu <ref> [8] </ref> discussed the matter of inferring transition networks (TNs) [32] from positive examples. An extension of the (heuristic) k-tails regular GI method was proposed for learning Basic TNs, thus covering the inference of context-free languages.
Reference: 9. <author> P. Dupont, </author> <title> "Regular grammatical inference from positive and negative samples by genetic search: the GIG method," in Grammatical Inference and Applications, </title> <booktitle> Proc. of the Second Int. Colloquium, </booktitle> <address> ICGI'94, Alicante, Spain, </address> <month> September </month> <year> 1994, </year> <editor> R.C.Carrasco and J.Oncina (eds.), Springer-Verlag, </editor> <booktitle> Lecture Notes in Artificial Intelligence 862, </booktitle> <address> pp.236-245. </address>
Reference-contexts: Nevertheless, just a few methods have been proposed for regular GI from complete presentation, either in the classical symbolic approach [1, 14, 22] or in conjunction with other approaches such as genetic search <ref> [9] </ref> and recurrent neural networks [2, 12, 28]. The most part of the reported regular GI methods are heuristic techniques that use only positive examples and some a-priori knowledge (e.g. [18, 25]). <p> Method 1 below is a little more detailed description of the learning procedure, with references to the algorithms proposed in the next sections to implement all the processing steps, except the regular grammatical inference step, for which a wide range of algorithms is available <ref> [1, 9, 15, 22, 25, 28] </ref>. This must not be interpreted, however, as if the choice of the regular GI method were irrelevant.
Reference: 10. <author> K.S. Fu, </author> <title> Syntactic Pattern Recognition and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: However, in order to be used in a wider class of problems and applications, there is a need for GI methods that cope with the issue of learning context-sensitive languages. This requirement is specially significant in syntactic pattern recognition problems for computer vision <ref> [10] </ref>, where many objects contain symmetries and structural relationships that are not describable by context-free languages. Nonetheless, work on learning context-sensitive languages is extremely scarce in the literature.
Reference: 11. <author> P. Garcia, J. Oncina, </author> <title> "Learning general context-free grammars from positive structural samples and negative strings," </title> <institution> DSIC Research Report, Universidad Politecnica de Valencia, Spain, </institution> <year> 1993. </year>
Reference-contexts: In addition, some GI methods have been suggested to learn proper subclasses of context-free languages, such as the even linear languages, from positive data [24, 29], and a few more algorithms have been proposed to infer general context-free grammars from positive structural examples [19, 26] (or together with negative strings <ref> [11] </ref>). However, in order to be used in a wider class of problems and applications, there is a need for GI methods that cope with the issue of learning context-sensitive languages.
Reference: 12. <author> C.L. Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, Y.C. Lee, </author> <title> "Learning and extracting finite state automata with second-order recurrent neural networks," </title> <booktitle> Neural Computation 4, </booktitle> <address> pp.393-405, </address> <year> 1992. </year>
Reference-contexts: Nevertheless, just a few methods have been proposed for regular GI from complete presentation, either in the classical symbolic approach [1, 14, 22] or in conjunction with other approaches such as genetic search [9] and recurrent neural networks <ref> [2, 12, 28] </ref>. The most part of the reported regular GI methods are heuristic techniques that use only positive examples and some a-priori knowledge (e.g. [18, 25]).
Reference: 13. <author> E.M. Gold, </author> <title> "Language identification in the limit," </title> <booktitle> Information & Control 10, </booktitle> <address> pp.447-474, </address> <year> 1967. </year>
Reference-contexts: It is well-known that any enumerable class of recursive languages (context-sensitive and below) can be identified in the limit from complete presentation (both positive and negative data), whereas only certain non-superfinite 2 classes of languages (below regular languages) can be identified from a positive presentation <ref> [3, 13] </ref>. The presentation of a language L by queries is an oracle that answers, for any query s, whether s 2 L or not. The inference methods for this type of presentation yield a description D of L after making some queries about L.
Reference: 14. <author> E.M. Gold, </author> <title> "Complexity of automaton identification from given data," </title> <booktitle> Information and Control 37, </booktitle> <address> pp.302-320, </address> <year> 1978. </year>
Reference-contexts: The most part of the work in GI has been devoted to the problem of inferring regular grammars (or finite-state automata) [15, 21]. Nevertheless, just a few methods have been proposed for regular GI from complete presentation, either in the classical symbolic approach <ref> [1, 14, 22] </ref> or in conjunction with other approaches such as genetic search [9] and recurrent neural networks [2, 12, 28]. The most part of the reported regular GI methods are heuristic techniques that use only positive examples and some a-priori knowledge (e.g. [18, 25]).
Reference: 15. <author> J. Gregor, </author> <title> "Data-driven inductive inference of finite-state automata," </title> <journal> Int. J. of Pattern Recognition and Artificial Intelligence 8 (1), </journal> <volume> pp.305-322, </volume> <year> 1994. </year>
Reference-contexts: The most part of the work in GI has been devoted to the problem of inferring regular grammars (or finite-state automata) <ref> [15, 21] </ref>. Nevertheless, just a few methods have been proposed for regular GI from complete presentation, either in the classical symbolic approach [1, 14, 22] or in conjunction with other approaches such as genetic search [9] and recurrent neural networks [2, 12, 28]. <p> Method 1 below is a little more detailed description of the learning procedure, with references to the algorithms proposed in the next sections to implement all the processing steps, except the regular grammatical inference step, for which a wide range of algorithms is available <ref> [1, 9, 15, 22, 25, 28] </ref>. This must not be interpreted, however, as if the choice of the regular GI method were irrelevant.
Reference: 16. <author> J.E. Hopfcroft and J.D. Ullman, </author> <title> Introduction to Automata Theory, Languages and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading MA, </address> <year> 1979. </year>
Reference-contexts: It is a better approach to convert the given expressions into their equivalent finite-state automata (FSA) and to test these for equivalence (e.g. by minimization) <ref> [16] </ref>. The following theorem, which proof can be found in [17], is behind the method described in next section to derive regular expressions from FSA. 7 Theorem 3.1. Let Q, P and R be regular expressions over a finite alphabet. <p> Then, if L (P ) does not contain , the equation R = Q + RP has a unique solution given by R = QP fl . 4. Mapping FSA into equivalent regular expressions. It is well known (Kleene's theorem <ref> [16] </ref>) that every language accepted by an FSA can be represented by a regular expression and every language denoted by a regular expression can be recognized by an FSA. Given an FSA A, there can be many equivalent REs R such that L (A) = L (R). <p> Given an FSA A, there can be many equivalent REs R such that L (A) = L (R). Several algorithms have been proposed to find a regular expression that describes the language accepted by a given FSA <ref> [16, 17] </ref>. By selecting a specific algorithm, a deterministic mapping can be established from FSA to REs, this is, a canonical RE R can be chosen for each FSA A, R = (A).
Reference: 17. <author> Z. Kohavi, </author> <title> Switching and Finite Automata Theory, (2nd edition). </title> <publisher> Tata McGraw-Hill, </publisher> <address> New Delhi, India, </address> <year> 1978. </year>
Reference-contexts: It is a better approach to convert the given expressions into their equivalent finite-state automata (FSA) and to test these for equivalence (e.g. by minimization) [16]. The following theorem, which proof can be found in <ref> [17] </ref>, is behind the method described in next section to derive regular expressions from FSA. 7 Theorem 3.1. Let Q, P and R be regular expressions over a finite alphabet. <p> Given an FSA A, there can be many equivalent REs R such that L (A) = L (R). Several algorithms have been proposed to find a regular expression that describes the language accepted by a given FSA <ref> [16, 17] </ref>. By selecting a specific algorithm, a deterministic mapping can be established from FSA to REs, this is, a canonical RE R can be chosen for each FSA A, R = (A). <p> By selecting a specific algorithm, a deterministic mapping can be established from FSA to REs, this is, a canonical RE R can be chosen for each FSA A, R = (A). The algorithm that is described next is based on a method due to Arden <ref> [7, 17] </ref>, but a final step has been appended to simplify the resulting regular expression. 4.1. A basic algorithm to find a regular expression equivalent to a given FSA. <p> Algorithm 1 is such a procedure <ref> [17] </ref>, which is based on solving the following system of symbolic equations: (e n 00 + R 1 ff n (n1)0 + 1 ) : R 1 = R 0 ff n 11 + ::: + R n1 ff n :: :: :: :: ::: :: n1 ) : R n1
Reference: 18. <author> M. Kudo, M.Shimbo, </author> <title> "Efficient regular grammatical inference techniques by the use of partial similarities and their logical relationships", Pattern Recognition 21 (4), </title> <address> pp.401-409, </address> <year> 1988. </year>
Reference-contexts: The most part of the reported regular GI methods are heuristic techniques that use only positive examples and some a-priori knowledge (e.g. <ref> [18, 25] </ref>).
Reference: 19. <author> E. Makinen, </author> <title> "Remarks on the structural grammatical inference problem for context-free grammars," </title> <journal> Information Processing Letters 44, </journal> <volume> pp.125-127, </volume> <year> 1992. </year> <month> 47 </month>
Reference-contexts: In addition, some GI methods have been suggested to learn proper subclasses of context-free languages, such as the even linear languages, from positive data [24, 29], and a few more algorithms have been proposed to infer general context-free grammars from positive structural examples <ref> [19, 26] </ref> (or together with negative strings [11]). However, in order to be used in a wider class of problems and applications, there is a need for GI methods that cope with the issue of learning context-sensitive languages.
Reference: 20. <author> A. Marron and K. Ko, </author> <title> "Identification of pattern languages from examples and queries," </title> <booktitle> Information and Computation 74, </booktitle> <address> pp.91-112, </address> <year> 1987. </year>
Reference-contexts: Not surprisingly, no other paper following this line of research has been reported since then [21, 31]. Finally, the inductive inference of pattern languages has been studied <ref> [4, 20] </ref>.
Reference: 21. <author> L. Miclet, </author> <title> "Grammatical inference," in Syntatic and Structural Pattern Recognition: Theory and Applications, </title> <editor> H.Bunke and A.Sanfeliu, Eds., </editor> <publisher> World Scientific, </publisher> <year> 1990. </year>
Reference-contexts: D is the smallest acceptor among all candidates). This problem is traditionally 1 The term complete presentation is somewhat confusing, since obviously, the complete (often infinite) language L is not presented. 1 referred to as grammatical inference (GI) <ref> [21, 31] </ref>. A GI method is said to identify L in the limit if for larger and larger collections of examples, the descriptions D eventually converge to a correct description for L. <p> The most part of the work in GI has been devoted to the problem of inferring regular grammars (or finite-state automata) <ref> [15, 21] </ref>. Nevertheless, just a few methods have been proposed for regular GI from complete presentation, either in the classical symbolic approach [1, 14, 22] or in conjunction with other approaches such as genetic search [9] and recurrent neural networks [2, 12, 28]. <p> Not surprisingly, no other paper following this line of research has been reported since then <ref> [21, 31] </ref>. Finally, the inductive inference of pattern languages has been studied [4, 20].
Reference: 22. <author> J. Oncina, P. Garcia, </author> <title> "Identifying regular languages in polynomial time," in Advances in Structural and Syntactic Pattern Recognition, </title> <editor> H. Bunke (ed.), World-Scientific, </editor> <address> Singapore, </address> <year> 1992, </year> <month> pp.99-108. </month>
Reference-contexts: The most part of the work in GI has been devoted to the problem of inferring regular grammars (or finite-state automata) [15, 21]. Nevertheless, just a few methods have been proposed for regular GI from complete presentation, either in the classical symbolic approach <ref> [1, 14, 22] </ref> or in conjunction with other approaches such as genetic search [9] and recurrent neural networks [2, 12, 28]. The most part of the reported regular GI methods are heuristic techniques that use only positive examples and some a-priori knowledge (e.g. [18, 25]). <p> Method 1 below is a little more detailed description of the learning procedure, with references to the algorithms proposed in the next sections to implement all the processing steps, except the regular grammatical inference step, for which a wide range of algorithms is available <ref> [1, 9, 15, 22, 25, 28] </ref>. This must not be interpreted, however, as if the choice of the regular GI method were irrelevant.
Reference: 23. <author> T.W. Pao, J.W. Carr, </author> <title> "A solution of the syntactical induction-inference problem for regular languages," </title> <booktitle> Computer Languages 3, </booktitle> <address> pp.53-64, </address> <year> 1978. </year>
Reference-contexts: The inference methods for this type of presentation yield a description D of L after making some queries about L. A mixed presentation by both examples and queries is also possible, in which the examples are used to infer an initial description to be refined later by queries <ref> [5, 23] </ref>. The most part of the work in GI has been devoted to the problem of inferring regular grammars (or finite-state automata) [15, 21].
Reference: 24. <author> V. Radhakrishnan, G. Nagaraja, </author> <title> "Inference of even linear grammars and its application to picture description languages," Pattern Recognition 21 (1), </title> <address> pp.55-62, </address> <year> 1988. </year>
Reference-contexts: The most part of the reported regular GI methods are heuristic techniques that use only positive examples and some a-priori knowledge (e.g. [18, 25]). In addition, some GI methods have been suggested to learn proper subclasses of context-free languages, such as the even linear languages, from positive data <ref> [24, 29] </ref>, and a few more algorithms have been proposed to infer general context-free grammars from positive structural examples [19, 26] (or together with negative strings [11]).
Reference: 25. <author> H. Rulot, E. Vidal, </author> <title> "Modelling (sub)string-length-based constraints through a grammatical inference method," in Pattern Recognition: Theory and Applications, </title> <editor> Devijver and Kittler (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1987, </year> <month> pp.451-459. </month>
Reference-contexts: The most part of the reported regular GI methods are heuristic techniques that use only positive examples and some a-priori knowledge (e.g. <ref> [18, 25] </ref>). <p> Method 1 below is a little more detailed description of the learning procedure, with references to the algorithms proposed in the next sections to implement all the processing steps, except the regular grammatical inference step, for which a wide range of algorithms is available <ref> [1, 9, 15, 22, 25, 28] </ref>. This must not be interpreted, however, as if the choice of the regular GI method were irrelevant.
Reference: 26. <author> Y. Sakakibara, </author> <title> "Efficient learning of context-free grammars from positive structural examples," </title> <booktitle> Information and Computation 97, </booktitle> <address> pp.23-60, </address> <year> 1992. </year>
Reference-contexts: In addition, some GI methods have been suggested to learn proper subclasses of context-free languages, such as the even linear languages, from positive data [24, 29], and a few more algorithms have been proposed to infer general context-free grammars from positive structural examples <ref> [19, 26] </ref> (or together with negative strings [11]). However, in order to be used in a wider class of problems and applications, there is a need for GI methods that cope with the issue of learning context-sensitive languages.
Reference: 27. <author> A. Salomaa, </author> <title> Formal Languages. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Recently, Takada has shown that a hierarchy of language families that are properly contained in the family of context-sensitive languages can be learned using regular GI algorithms [30]. His approach is based on using control sets on grammars (a type of regulated rewrittings <ref> [27] </ref>) and establishing a recursive sequence of controlling grammars that starts on a regular grammar. A drawback of Takada's method is that the classes of learnable context-sensitive languages are quite restricted, since the languages are generated through a sequence of (controlled) universal even linear grammars.
Reference: 28. <author> A. Sanfeliu and R. Alquezar, </author> <title> "Active grammatical inference: a new learning methodology," </title> <booktitle> in Proc. of the IAPR Int. Workshop on Structural and Syntactic Pattern Recognition, </booktitle> <address> SSPR'94, Nahariya, Israel, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Nevertheless, just a few methods have been proposed for regular GI from complete presentation, either in the classical symbolic approach [1, 14, 22] or in conjunction with other approaches such as genetic search [9] and recurrent neural networks <ref> [2, 12, 28] </ref>. The most part of the reported regular GI methods are heuristic techniques that use only positive examples and some a-priori knowledge (e.g. [18, 25]). <p> Even though the target language is context-sensitive, one may try to enter this sample into an FSA learning algorithm and analyse the usefulness of the result 4 . For example, the application of the (regular) active grammatical inference method, recently reported <ref> [28] </ref>, to the given S yielded the deterministic FSA displayed in Fig.1, which accounts for the basic repetitive structure of the model but which over-generalizes a lot, accepting rather arbitrary contours without any length restriction. <p> Method 1 below is a little more detailed description of the learning procedure, with references to the algorithms proposed in the next sections to implement all the processing steps, except the regular grammatical inference step, for which a wide range of algorithms is available <ref> [1, 9, 15, 22, 25, 28] </ref>. This must not be interpreted, however, as if the choice of the regular GI method were irrelevant.
Reference: 29. <author> Y. Takada, </author> <title> "Grammatical inference for even linear languages based on control sets," </title> <note> Information Processing Letters 28 (4), pp.193-199, </note> <year> 1988. </year>
Reference-contexts: The most part of the reported regular GI methods are heuristic techniques that use only positive examples and some a-priori knowledge (e.g. [18, 25]). In addition, some GI methods have been suggested to learn proper subclasses of context-free languages, such as the even linear languages, from positive data <ref> [24, 29] </ref>, and a few more algorithms have been proposed to infer general context-free grammars from positive structural examples [19, 26] (or together with negative strings [11]).
Reference: 30. <author> Y. Takada, </author> <title> "A hierarchy of language families learnable by regular language learners," in Grammatical Inference and Applications, </title> <booktitle> Proc. of the Second Int. Colloquium, </booktitle> <address> ICGI'94, Alicante, Spain, </address> <month> September </month> <year> 1994, </year> <editor> R.C.Carrasco and J.Oncina (eds.), Springer-Verlag, </editor> <booktitle> Lecture Notes in Artificial Intelligence 862, </booktitle> <address> pp.16-24. </address>
Reference-contexts: Nonetheless, work on learning context-sensitive languages is extremely scarce in the literature. Recently, Takada has shown that a hierarchy of language families that are properly contained in the family of context-sensitive languages can be learned using regular GI algorithms <ref> [30] </ref>. His approach is based on using control sets on grammars (a type of regulated rewrittings [27]) and establishing a recursive sequence of controlling grammars that starts on a regular grammar.
Reference: 31. <author> E. Vidal, </author> <title> "Grammatical inference: an introductory survey," in Grammatical Inference and Applications, </title> <booktitle> Proc. of the Second Int. Colloquium, </booktitle> <address> ICGI'94, Alicante, Spain, </address> <month> September </month> <year> 1994, </year> <editor> R.C.Carrasco and J.Oncina (eds.), Springer-Verlag, </editor> <booktitle> Lecture Notes in Artificial Intelligence 862, </booktitle> <address> pp.1-4. </address>
Reference-contexts: D is the smallest acceptor among all candidates). This problem is traditionally 1 The term complete presentation is somewhat confusing, since obviously, the complete (often infinite) language L is not presented. 1 referred to as grammatical inference (GI) <ref> [21, 31] </ref>. A GI method is said to identify L in the limit if for larger and larger collections of examples, the descriptions D eventually converge to a correct description for L. <p> Not surprisingly, no other paper following this line of research has been reported since then <ref> [21, 31] </ref>. Finally, the inductive inference of pattern languages has been studied [4, 20].
Reference: 32. <author> W.A. Woods, </author> <title> "Transition networks grammars for natural language analysis," </title> <type> CACM 13, </type> <institution> pp.591-606, </institution> <year> 1970. </year> <month> 48 </month>
Reference-contexts: On the other hand, an early work by Chou and Fu [8] discussed the matter of inferring transition networks (TNs) <ref> [32] </ref> from positive examples. An extension of the (heuristic) k-tails regular GI method was proposed for learning Basic TNs, thus covering the inference of context-free languages.
References-found: 32

