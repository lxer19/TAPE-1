URL: http://www.cs.utexas.edu/users/vlr/papers/ima97.ps
Refering-URL: http://www.cs.utexas.edu/users/vlr/pub.html
Root-URL: 
Title: A GENERAL PURPOSE SHARED-MEMORY MODEL FOR PARALLEL COMPUTATION  
Author: VIJAYA RAMACHANDRAN 
Date: October 3, 1997  
Note: To appear in the proceedings of IMA Workshop on Parallel Algorithms, Sept. 1996, Minneapolis, MN  
Abstract: We describe a general-purpose shared-memory model for parallel computation, called the qsm [22], which provides a high-level shared-memory abstraction for parallel algorithm design, as well as the ability to be emulated in an effective manner on the bsp, a lower-level, distributed-memory model. We present new emulation results that show that very little generality is lost by not having a `gap parameter' at memory.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Adler, P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> Modeling parallel bandwidth: Local vs. global restrictions. </title> <booktitle> In Proc. 9th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: hashing O (g lg n) time, fi (gn) work w.h.p. qrqw [20] load balancing, max. load L O (g p lg n lg lg L + lg L) time, qrqw [20] fi (gn) work w.h.p. broadcast to n mem. locations fi (g lg n=(lg g)) time, fi (gn) work qsm <ref> [1] </ref> sorting O (g lg n) time, O (gn lg n) work erew [3, 12] simple fast sorting O (g lg n + lg 2 n=(lg lg n)) time, qsm [22] (sample sort) O (gn lg n) work w.h.p. work-optimal sorting O (n * (g + lg n)) time, * &gt;
Reference: [2] <author> A. Aggarwal, A. K. Chandra, and M. Snir. </author> <title> Communication complexity of PRAMs. </title> <journal> Theoretical Computer Science, </journal> <volume> 71(1) </volume> <pages> 3-28, </pages> <year> 1990. </year>
Reference: [3] <author> M. Ajtai, J. Komlos, and E. Szemeredi. </author> <title> Sorting in c lg n parallel steps. </title> <journal> Combinatorica, </journal> <volume> 3(1) </volume> <pages> 1-19, </pages> <year> 1983. </year>
Reference-contexts: balancing, max. load L O (g p lg n lg lg L + lg L) time, qrqw [20] fi (gn) work w.h.p. broadcast to n mem. locations fi (g lg n=(lg g)) time, fi (gn) work qsm [1] sorting O (g lg n) time, O (gn lg n) work erew <ref> [3, 12] </ref> simple fast sorting O (g lg n + lg 2 n=(lg lg n)) time, qsm [22] (sample sort) O (gn lg n) work w.h.p. work-optimal sorting O (n * (g + lg n)) time, * &gt; 0, bsp [17] (sample sort) fi (gn + n lg n) work w.h.p.
Reference: [4] <author> A. Alexandrov, M. F. Ionescu, K. E. Schauser, and C. Sheiman. LogGP: </author> <title> Incorporating long messages into the LogP model | one step closer towards a realistic model for parallel computation. </title> <booktitle> In Proc. 7th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 95-105, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The bsp and logp models have become very popular in recent years, and many algorithms have been designed and analyzed on these models and their extensions (see, e.g., <ref> [4, 8, 17, 25, 27, 36, 47] </ref>). However, algorithms designed for these models tend to have rather complicated performance analyses, because of the number of parameters in the model as well as the need to keep track of the exact memory partition across the processors at each step.
Reference: [5] <author> B. Alpern, L. Carter, and E. Feig. </author> <title> Uniform memory hierarchies. </title> <booktitle> In Proc. 31st IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 600-608, </pages> <month> October </month> <year> 1990. </year>
Reference: [6] <author> Y. Aumann and M. O. Rabin. </author> <title> Clock construction in fully asynchronous parallel systems and PRAM simulation. </title> <booktitle> In Proc. 33rd IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 147-156, </pages> <month> October </month> <year> 1992. </year>
Reference: [7] <author> A. Bar-Noy and S. Kipnis. </author> <title> Designing broadcasting algorithms in the postal model for message-passing systems. </title> <booktitle> In Proc. 4th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 13-22, </pages> <month> June-July </month> <year> 1992. </year>
Reference: [8] <author> A. Baumker and W. Dittrich. </author> <title> Fully dynamic search trees for an extension of the BSP model. </title> <booktitle> In Proc. 8th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 233-242, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: The bsp and logp models have become very popular in recent years, and many algorithms have been designed and analyzed on these models and their extensions (see, e.g., <ref> [4, 8, 17, 25, 27, 36, 47] </ref>). However, algorithms designed for these models tend to have rather complicated performance analyses, because of the number of parameters in the model as well as the need to keep track of the exact memory partition across the processors at each step.
Reference: [9] <author> G. E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference: [10] <author> G. E. Blelloch, P. B. Gibbons, Y. Matias, and M. Zagha. </author> <title> Accounting for memory bank contention and delay in high-bandwidth multiprocessors. </title> <booktitle> In Proc. 7th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 84-94, </pages> <month> July </month> <year> 1995. </year>
Reference: [11] <author> J. L. Carter and M.N. Wegman. </author> <title> Universal classes of hash functions. </title> <journal> J. Comput. Syst.Sci. </journal> <volume> 18 </volume> <pages> 143-154, </pages> <year> 1979. </year>
Reference-contexts: In practice one would distribute the shared memory across the bsp processors using a random hash function from a class of universal hash functions that can be evaluated quickly (see, e.g., <ref> [11, 37, 26] </ref>).
Reference: [12] <author> R. Cole. </author> <title> Parallel merge sort. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17(4) </volume> <pages> 770-785, </pages> <year> 1988. </year>
Reference-contexts: balancing, max. load L O (g p lg n lg lg L + lg L) time, qrqw [20] fi (gn) work w.h.p. broadcast to n mem. locations fi (g lg n=(lg g)) time, fi (gn) work qsm [1] sorting O (g lg n) time, O (gn lg n) work erew <ref> [3, 12] </ref> simple fast sorting O (g lg n + lg 2 n=(lg lg n)) time, qsm [22] (sample sort) O (gn lg n) work w.h.p. work-optimal sorting O (n * (g + lg n)) time, * &gt; 0, bsp [17] (sample sort) fi (gn + n lg n) work w.h.p.
Reference: [13] <author> R. Cole and O. Zajicek. </author> <title> The APRAM: Incorporating asynchrony into the PRAM model. </title> <booktitle> In Proc. 1st ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 169-178, </pages> <month> June </month> <year> 1989. </year>
Reference: [14] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> In Proc. 4th ACM SIGPLAN Symp. on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1993. </year> <month> 12 </month>
Reference-contexts: In recent years, distributed-memory models that characterize the interconnection network abstractly by parameters that capture its performance have gained much attention. An early work along these lines is the CTA [42]. More recently, the bsp model [43, 44] and the logp model <ref> [14] </ref> have gained wide acceptance as general-purpose models of parallel computation. In these models the parallel machine is abstracted as a collection of processors-memory units with no global shared memory.
Reference: [15] <author> C. Dwork, M. Herlihy, and O. Waarts. </author> <title> Contention in shared memory algorithms. </title> <booktitle> In Proc. 25th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 174-183, </pages> <month> May </month> <year> 1993. </year>
Reference: [16] <author> S. Fortune and J. Wyllie. </author> <title> Parallelism in random access machines. </title> <booktitle> In Proc. 10th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <month> May </month> <year> 1978. </year>
Reference: [17] <author> A. V. Gerbessiotis and L. Valiant. </author> <title> Direct bulk-synchronous parallel algorithms. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22 </volume> <pages> 251-267, </pages> <year> 1994. </year>
Reference-contexts: The bsp and logp models have become very popular in recent years, and many algorithms have been designed and analyzed on these models and their extensions (see, e.g., <ref> [4, 8, 17, 25, 27, 36, 47] </ref>). However, algorithms designed for these models tend to have rather complicated performance analyses, because of the number of parameters in the model as well as the need to keep track of the exact memory partition across the processors at each step. <p> (g lg n) time, O (gn lg n) work erew [3, 12] simple fast sorting O (g lg n + lg 2 n=(lg lg n)) time, qsm [22] (sample sort) O (gn lg n) work w.h.p. work-optimal sorting O (n * (g + lg n)) time, * &gt; 0, bsp <ref> [17] </ref> (sample sort) fi (gn + n lg n) work w.h.p.
Reference: [18] <author> P. B. Gibbons. </author> <title> A more practical PRAM model. </title> <booktitle> In Proc. 1st ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 158-168, </pages> <month> June </month> <year> 1989. </year> <title> Full version in The Asynchronous PRAM: A semi-synchronous model for shared memory MIMD machines, </title> <type> PhD thesis, </type> <institution> U.C. Berkeley 1989. </institution>
Reference: [19] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> The Queue-Read Queue-Write PRAM model: Accounting for contention in parallel algorithms. </title> <journal> SIAM Journal on Computing, </journal> <note> 1997. To appear. Preliminary version appears in Proc. </note> <editor> 5th ACM-SIAM Symp. </editor> <booktitle> on Discrete Algorithms, </booktitle> <pages> pages 638-648, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: The work reported in [22] builds on the results in <ref> [19] </ref> where a simple variant of the pram model is described in which the read-write steps are required to be queuing; this model is called the qrqw pram. <p> The cost of a step was the maximum number of requests to any single memory location. A randomized work-preserving emulation of the qrqw pram on a special type of bsp is given in <ref> [19] </ref>, with slowdown only logarithmic in the number of processors 1 . In [22], the qrqw model was extended to the qsm model, which incorporates a gap parameter at processors to capture limitations in bandwidth. <p> The work of a qsm algorithm is its processor-time product. The particular instance of the Queuing Shared Memory model in which the gap parameter, g, equals 1 is essentially the Queue-Read Queue-Write (qrqw) pram model defined in <ref> [19] </ref>. <p> This 4 Summary of Algorithmic Results problem (n = size of input) qsm result 3 source prefix sums, list ranking, etc. 4 O (g lg n) time, fi (gn) 5 work erew linear compaction O ( p g lg n + g lg lg n) time, qrqw <ref> [19] </ref> O (gn) work w.h.p. random permutation O (g lg n) time, fi (gn) work w.h.p. qrqw [20] multiple compaction O (g lg n) time, fi (gn) work w.h.p. qrqw [20] parallel hashing O (g lg n) time, fi (gn) work w.h.p. qrqw [20] load balancing, max. load L O (g <p> On the other hand, if the qsm is enhanced to have unit-cost concurrent memory accesses, this appears to give the model more power than is warranted by the performance of currently available machines. For more detailed discussions on the appropriateness of the queue metric, see <ref> [19, 22] </ref>. * The qsm is a bulk-synchronous model, i.e., a step consists of a sequence of pipe-lined requests to memory, together with a sequence of local operations, and there is global synchronization between successive steps. <p> For a completely asynchronous general-purpose shared-memory model, a promising candidate is the qrqw asynchronous pram [21], augmented with the gap parameter. 11 Acknowledgement I would like to thank Phil Gibbons and Yossi Matias for innumerable discus- sions on queuing shared memory models; this collaboration led to the results in <ref> [19, 20, 21, 22] </ref>.
Reference: [20] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> Efficient low-contention parallel algorithms. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 53(3) </volume> <pages> 417-442, </pages> <year> 1996. </year> <note> Special issue devoted to selected papers from the 1994 ACM Symp. on Parallel Algorithms and Architectures. </note>
Reference-contexts: 3 source prefix sums, list ranking, etc. 4 O (g lg n) time, fi (gn) 5 work erew linear compaction O ( p g lg n + g lg lg n) time, qrqw [19] O (gn) work w.h.p. random permutation O (g lg n) time, fi (gn) work w.h.p. qrqw <ref> [20] </ref> multiple compaction O (g lg n) time, fi (gn) work w.h.p. qrqw [20] parallel hashing O (g lg n) time, fi (gn) work w.h.p. qrqw [20] load balancing, max. load L O (g p lg n lg lg L + lg L) time, qrqw [20] fi (gn) work w.h.p. broadcast <p> fi (gn) 5 work erew linear compaction O ( p g lg n + g lg lg n) time, qrqw [19] O (gn) work w.h.p. random permutation O (g lg n) time, fi (gn) work w.h.p. qrqw <ref> [20] </ref> multiple compaction O (g lg n) time, fi (gn) work w.h.p. qrqw [20] parallel hashing O (g lg n) time, fi (gn) work w.h.p. qrqw [20] load balancing, max. load L O (g p lg n lg lg L + lg L) time, qrqw [20] fi (gn) work w.h.p. broadcast to n mem. locations fi (g lg n=(lg g)) time, fi (gn) work <p> + g lg lg n) time, qrqw [19] O (gn) work w.h.p. random permutation O (g lg n) time, fi (gn) work w.h.p. qrqw <ref> [20] </ref> multiple compaction O (g lg n) time, fi (gn) work w.h.p. qrqw [20] parallel hashing O (g lg n) time, fi (gn) work w.h.p. qrqw [20] load balancing, max. load L O (g p lg n lg lg L + lg L) time, qrqw [20] fi (gn) work w.h.p. broadcast to n mem. locations fi (g lg n=(lg g)) time, fi (gn) work qsm [1] sorting O (g lg n) time, O (gn lg n) work <p> fi (gn) work w.h.p. qrqw <ref> [20] </ref> multiple compaction O (g lg n) time, fi (gn) work w.h.p. qrqw [20] parallel hashing O (g lg n) time, fi (gn) work w.h.p. qrqw [20] load balancing, max. load L O (g p lg n lg lg L + lg L) time, qrqw [20] fi (gn) work w.h.p. broadcast to n mem. locations fi (g lg n=(lg g)) time, fi (gn) work qsm [1] sorting O (g lg n) time, O (gn lg n) work erew [3, 12] simple fast sorting O (g lg n + lg 2 n=(lg lg n)) time, qsm [22] <p> For a completely asynchronous general-purpose shared-memory model, a promising candidate is the qrqw asynchronous pram [21], augmented with the gap parameter. 11 Acknowledgement I would like to thank Phil Gibbons and Yossi Matias for innumerable discus- sions on queuing shared memory models; this collaboration led to the results in <ref> [19, 20, 21, 22] </ref>.
Reference: [21] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> The Queue-Read Queue-Write Asynchronous PRAM model. </title> <note> Theoretical Computer Science: Special Issue on Parallel Processing. To appear. Preliminary version in Euro-Par'96, Lecture Notes in Computer Science, Vol. </note> <month> 1124, </month> <pages> pages 279-292. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: For a completely asynchronous general-purpose shared-memory model, a promising candidate is the qrqw asynchronous pram <ref> [21] </ref>, augmented with the gap parameter. 11 Acknowledgement I would like to thank Phil Gibbons and Yossi Matias for innumerable discus- sions on queuing shared memory models; this collaboration led to the results in [19, 20, 21, 22]. <p> For a completely asynchronous general-purpose shared-memory model, a promising candidate is the qrqw asynchronous pram [21], augmented with the gap parameter. 11 Acknowledgement I would like to thank Phil Gibbons and Yossi Matias for innumerable discus- sions on queuing shared memory models; this collaboration led to the results in <ref> [19, 20, 21, 22] </ref>.
Reference: [22] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> Can a shared-memory model serve as a bridging model for parallel computation? In Proc. </title> <booktitle> 9th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: However, algorithms designed for these models tend to have rather complicated performance analyses, because of the number of parameters in the model as well as the need to keep track of the exact memory partition across the processors at each step. Very recently, in <ref> [22] </ref> the issue of whether there is merit in developing a general-purpose model of parallel computation, starting with a shared-memory framework was explored. Certainly, shared-memory has been a widely-supported abstraction in parallel programming [30]. <p> Certainly, shared-memory has been a widely-supported abstraction in parallel programming [30]. Additionally, the architectures of many parallel machines are either intrinsically shared-memory or support it using suitable hardware. The main issues addressed in <ref> [22] </ref> are the enhancements to be made to a simple shared-memory model such as the pram, and the effectiveness of the resulting model in capturing the essential features of parallel machines along the lines of the bsp and the logp models. The work reported in [22] builds on the results in <p> The main issues addressed in <ref> [22] </ref> are the enhancements to be made to a simple shared-memory model such as the pram, and the effectiveness of the resulting model in capturing the essential features of parallel machines along the lines of the bsp and the logp models. The work reported in [22] builds on the results in [19] where a simple variant of the pram model is described in which the read-write steps are required to be queuing; this model is called the qrqw pram. <p> The cost of a step was the maximum number of requests to any single memory location. A randomized work-preserving emulation of the qrqw pram on a special type of bsp is given in [19], with slowdown only logarithmic in the number of processors 1 . In <ref> [22] </ref>, the qrqw model was extended to the qsm model, which incorporates a gap parameter at processors to capture limitations in bandwidth. It is shown in [22] that the qsm has a random 1 An emulation is work-preserving if the processor-time bound on the emulated machine is the same as that <p> In <ref> [22] </ref>, the qrqw model was extended to the qsm model, which incorporates a gap parameter at processors to capture limitations in bandwidth. It is shown in [22] that the qsm has a random 1 An emulation is work-preserving if the processor-time bound on the emulated machine is the same as that on the machine being emulated, to within a constant factor. <p> Thus, the qsm is a simpler model than either the bsp or the logp models. The qsm has a gap parameter at the processors to capture the limited bandwidth of parallel machines, but it does not have a gap parameter at the memory. This fact is noted in <ref> [22] </ref>, but is not explored further. In this paper we explore this issue by defining a generalization of the qsm that has (different) gap parameters at the processors and at memory locations. We present a work-preserving emulation of this generalized qsm on the bsp, and some related results. <p> The time taken by a bsp algorithm is the sum of the costs of the individual supersteps in the algorithm. 2 The Queuing Shared Memory Model (QSM) In this section, we present the definition of the Queuing Shared Memory model. Definition 2.1 <ref> [22] </ref> The Queuing Shared Memory (qsm) model consists of a number of identical processors, each with its own private memory, communicating by reading and writing locations in a shared memory. Processors execute a sequence of synchronized phases, each consisting of an arbitrary interleaving of the following operations: 1. <p> [20] fi (gn) work w.h.p. broadcast to n mem. locations fi (g lg n=(lg g)) time, fi (gn) work qsm [1] sorting O (g lg n) time, O (gn lg n) work erew [3, 12] simple fast sorting O (g lg n + lg 2 n=(lg lg n)) time, qsm <ref> [22] </ref> (sample sort) O (gn lg n) work w.h.p. work-optimal sorting O (n * (g + lg n)) time, * &gt; 0, bsp [17] (sample sort) fi (gn + n lg n) work w.h.p. <p> Most of these results are the consequence of the following four Observations, all of which are from <ref> [22] </ref>. 3 The time bound stated is the fastest for the given work bound; by Observation 3.1, any slower time is possible within the same work bound. 4 By Observation 3.2 any erew result maps on to the qsm with the work and time both increasing by a factor of g. <p> Then, at a later step, 0 could access this value as a local unit-time computation. On a qsm the corresponding qsm processor 0 Q would need to perform a read on global memory at the later step to access the value, thereby incurring a time cost of g. In <ref> [22] </ref> an explicit computation is given that runs faster on the bsp than on the qsm. <p> As a result an algorithm that is designed using this additional power of the bsp over the qsm may not be that widely applicable. 6 The paper <ref> [22] </ref> also presents a randomized work-preserving emulation of the bsp on the qsm that incurs a slow-down that is only logarithmic in the number of processors. Thus, if a modest slow-down is acceptable, then in fact, any bsp algorithm can be mapped on to the qsm in a work-preserving manner. <p> For completeness, we state here the result regarding the emulation of the bsp on the qsm. The emulation algorithm and the proof of the following theorem can be found in full version of <ref> [22] </ref>. <p> If E (S) &gt; 0, then for any - &gt; 0 Prob (S &gt; (1 + -)E (S)) &lt; (1 + -) (1+-) : We now state and prove the work-preserving emulation result. A similar theorem is proved in <ref> [22] </ref>, which presents an emulation of the qsm on a (d; x)-bsp. <p> In situations where this is an important consideration, one should tailor one's algorithm to the correct d parameter. 5 Discussion In this paper, we have described the qsm model of <ref> [22] </ref>, reviewed algorithmic results for the model, and presented a randomized work-preserving emulation for a generalization of the qsm on the bsp. <p> In such cases there would be no reason for the number of memory banks to equal the number of processors, which is the situation modeled by the bsp and logp models. This point is elaborated in some detail in <ref> [22] </ref>. * The queuing rule for concurrent memory accesses in the qsm is crucial in matching it to real machines. <p> On the other hand, if the qsm is enhanced to have unit-cost concurrent memory accesses, this appears to give the model more power than is warranted by the performance of currently available machines. For more detailed discussions on the appropriateness of the queue metric, see <ref> [19, 22] </ref>. * The qsm is a bulk-synchronous model, i.e., a step consists of a sequence of pipe-lined requests to memory, together with a sequence of local operations, and there is global synchronization between successive steps. <p> For a completely asynchronous general-purpose shared-memory model, a promising candidate is the qrqw asynchronous pram [21], augmented with the gap parameter. 11 Acknowledgement I would like to thank Phil Gibbons and Yossi Matias for innumerable discus- sions on queuing shared memory models; this collaboration led to the results in <ref> [19, 20, 21, 22] </ref>.
Reference: [23] <author> T. Heywood and S. Ranka. </author> <title> A practical hierarchical model of parallel computation: I. The model. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 212-232, </pages> <year> 1992. </year>
Reference: [24] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1992. </year>
Reference-contexts: As a simple model at a high level of abstraction, the pram has served an important role, and most of the basic paradigms for parallel algorithm design as well as the basic ideas underlying the parallel algorithms for many problems have been developed on this model (see, e.g., <ref> [28, 24, 41] </ref>). The other approach that has been used to design parallel algorithms has been to consider distributed-memory models, and tailor the parallel algorithm to a specific interconnection network that connects the processors and memory, e.g., mesh, hypercube, shu*e-exchange, cube-connected cycles, etc. <p> The two problems cited in this line are representatives of the large class of problems for which logarithmic time, linear work erew pram algorithms are known (see, e.g., <ref> [28, 24, 41] </ref>). 5 The use of fi in the work or time bound implies that the result is the best possible, to within a constant factor. 5 Observation 3.1 (Self-simulation) Given a qsm algorithm that runs in time t using p processors, the same algorithm can be made to run
Reference: [25] <author> B. H. H. Juurlink and H. A. G. Wijshoff. </author> <title> The E-BSP Model: Incorporating general locality and unbalanced communication into the BSP Model. </title> <booktitle> In Proc. Euro-Par'96, </booktitle> <pages> pages 339-347, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: The bsp and logp models have become very popular in recent years, and many algorithms have been designed and analyzed on these models and their extensions (see, e.g., <ref> [4, 8, 17, 25, 27, 36, 47] </ref>). However, algorithms designed for these models tend to have rather complicated performance analyses, because of the number of parameters in the model as well as the need to keep track of the exact memory partition across the processors at each step.
Reference: [26] <author> A. Karlin and E. Upfal. </author> <title> Parallel hashing An efficient implementation of shared memory. </title> <journal> J. ACM, </journal> <volume> 35:4, </volume> <pages> pages 876-892, </pages> <year> 1988. </year>
Reference-contexts: In practice one would distribute the shared memory across the bsp processors using a random hash function from a class of universal hash functions that can be evaluated quickly (see, e.g., <ref> [11, 37, 26] </ref>).
Reference: [27] <author> R. Karp, A. Sahay, E. Santos, and K.E. Schauser. </author> <title> Optimal broadcast and summation in the LogP model. </title> <booktitle> In Proc. 5th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 142-153, </pages> <month> June-July </month> <year> 1993. </year>
Reference-contexts: The bsp and logp models have become very popular in recent years, and many algorithms have been designed and analyzed on these models and their extensions (see, e.g., <ref> [4, 8, 17, 25, 27, 36, 47] </ref>). However, algorithms designed for these models tend to have rather complicated performance analyses, because of the number of parameters in the model as well as the need to keep track of the exact memory partition across the processors at each step.
Reference: [28] <author> R. M. Karp and V. Ramachandran. </author> <title> Parallel algorithms for shared-memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A, </booktitle> <pages> pages 869-941. </pages> <publisher> Elsevier Science Publishers B.V., </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1990. </year> <month> 13 </month>
Reference-contexts: As a simple model at a high level of abstraction, the pram has served an important role, and most of the basic paradigms for parallel algorithm design as well as the basic ideas underlying the parallel algorithms for many problems have been developed on this model (see, e.g., <ref> [28, 24, 41] </ref>). The other approach that has been used to design parallel algorithms has been to consider distributed-memory models, and tailor the parallel algorithm to a specific interconnection network that connects the processors and memory, e.g., mesh, hypercube, shu*e-exchange, cube-connected cycles, etc. <p> Thus earlier pram models were classified as erew, crew, and crcw (see, e.g., <ref> [28] </ref>); the ercw pram was studied more recently [33]. The latter two models (crcw and ercw pram) have several variants depending on how a concurrent write is resolved. In all models a step took unit time. <p> The two problems cited in this line are representatives of the large class of problems for which logarithmic time, linear work erew pram algorithms are known (see, e.g., <ref> [28, 24, 41] </ref>). 5 The use of fi in the work or time bound implies that the result is the best possible, to within a constant factor. 5 Observation 3.1 (Self-simulation) Given a qsm algorithm that runs in time t using p processors, the same algorithm can be made to run
Reference: [29] <author> Z. M. Kedem, K. V. Palem, M. O. Rabin, and A. Raghunathan. </author> <title> Efficient program transfor-mations for resilient parallel computation via randomization. </title> <booktitle> In Proc. 24th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 306-317, </pages> <month> May </month> <year> 1992. </year>
Reference: [30] <author> K. Kennedy. </author> <title> A research agenda for high performance computing software. </title> <booktitle> In Developing a Computer Science Agenda for High-Performance Computing, </booktitle> <pages> pages 106-109. </pages> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: Very recently, in [22] the issue of whether there is merit in developing a general-purpose model of parallel computation, starting with a shared-memory framework was explored. Certainly, shared-memory has been a widely-supported abstraction in parallel programming <ref> [30] </ref>. Additionally, the architectures of many parallel machines are either intrinsically shared-memory or support it using suitable hardware.
Reference: [31] <author> F. T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays Trees Hy-percubes. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: There are several results known on embedding one of these networks, the source network, on to another, the target network (see, e.g., <ref> [31] </ref>), so that an efficient algorithm on the source network results in an efficient algorithm on the target network. Neither of the above approaches has been very satisfactory.
Reference: [32] <author> P. Liu, W. Aiello, and S. Bhatt. </author> <title> An atomic model for message-passing. </title> <booktitle> In Proc. 5th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 154-163, </pages> <month> June-July </month> <year> 1993. </year>
Reference: [33] <author> P.D. MacKenzie and V. Ramachandran. </author> <title> ERCW PRAMs and optical communication. </title> <note> Theoretical Computer Science: Special Issue on Parallel Processing. To appear. Preliminary version in Euro-Par'96, Lecture Notes in Computer Science, Vol. </note> <month> 1124, </month> <pages> pages 293-303. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Thus earlier pram models were classified as erew, crew, and crcw (see, e.g., [28]); the ercw pram was studied more recently <ref> [33] </ref>. The latter two models (crcw and ercw pram) have several variants depending on how a concurrent write is resolved. In all models a step took unit time. In the qrqw pram model, concurrent memory accesses were allowed, but a step no longer took unit time.
Reference: [34] <author> B. M. Maggs, L. R. Matheson, and R. E. Tarjan. </author> <title> Models of parallel computation: A survey and synthesis. </title> <booktitle> In Proc. 28th Hawaii International Conf. on System Sciences, pages II: </booktitle> <pages> 61-70, </pages> <month> January </month> <year> 1995. </year>
Reference: [35] <author> Y. Mansour, N. Nisan, and U. Vishkin. </author> <title> Trade-offs between communication throughput and parallel time. </title> <booktitle> In Proc. 26th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 372-381, </pages> <year> 1994. </year>
Reference: [36] <author> W. F. McColl. </author> <title> A BSP realization of Strassen's algorithm. </title> <type> Technical report, </type> <institution> Oxford University Computing Laboratory, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The bsp and logp models have become very popular in recent years, and many algorithms have been designed and analyzed on these models and their extensions (see, e.g., <ref> [4, 8, 17, 25, 27, 36, 47] </ref>). However, algorithms designed for these models tend to have rather complicated performance analyses, because of the number of parameters in the model as well as the need to keep track of the exact memory partition across the processors at each step.
Reference: [37] <author> K. Mehlhorn and U. Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memories. </title> <journal> Acta Informatica, </journal> <volume> 21 </volume> <pages> 339-374, </pages> <year> 1984. </year>
Reference-contexts: In practice one would distribute the shared memory across the bsp processors using a random hash function from a class of universal hash functions that can be evaluated quickly (see, e.g., <ref> [11, 37, 26] </ref>).
Reference: [38] <author> N. Nishimura. </author> <title> Asynchronous shared memory parallel computation. </title> <booktitle> In Proc. 2nd ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 76-84, </pages> <month> July </month> <year> 1990. </year>
Reference: [39] <author> P. Raghavan. </author> <title> Probabilistic construction of deterministic algorithms: approximating packing integer programs. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 37 </volume> <pages> 130-143, </pages> <year> 1988. </year>
Reference-contexts: Despite the apparent mis-match between some of the parameters, we present below, a work-preserving emulation of the qsm (g; d) on the bsp. The proof of the emulation result requires the following result by Raghavan and Spencer. 7 Theorem 4.1 <ref> [39] </ref> Let a 1 ; : : : ; a r be reals in (0; 1]. Let x 1 ; : : : ; x r be independent Bernoulli trials with E (x j ) = j . Let S = P r j=1 a j x j .
Reference: [40] <author> A. G. Ranade. </author> <title> Fluent parallel computation. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> May </month> <year> 1989. </year>
Reference: [41] <author> J. H. Reif, </author> <title> editor. A Synthesis of Parallel Algorithms. </title> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: As a simple model at a high level of abstraction, the pram has served an important role, and most of the basic paradigms for parallel algorithm design as well as the basic ideas underlying the parallel algorithms for many problems have been developed on this model (see, e.g., <ref> [28, 24, 41] </ref>). The other approach that has been used to design parallel algorithms has been to consider distributed-memory models, and tailor the parallel algorithm to a specific interconnection network that connects the processors and memory, e.g., mesh, hypercube, shu*e-exchange, cube-connected cycles, etc. <p> The two problems cited in this line are representatives of the large class of problems for which logarithmic time, linear work erew pram algorithms are known (see, e.g., <ref> [28, 24, 41] </ref>). 5 The use of fi in the work or time bound implies that the result is the best possible, to within a constant factor. 5 Observation 3.1 (Self-simulation) Given a qsm algorithm that runs in time t using p processors, the same algorithm can be made to run
Reference: [42] <author> L. Snyder. </author> <title> Type architecture, shared memory and the corollary of modest potential. </title> <booktitle> Annual Review of CS, </booktitle> <address> I:289-317, </address> <year> 1986. </year>
Reference-contexts: In recent years, distributed-memory models that characterize the interconnection network abstractly by parameters that capture its performance have gained much attention. An early work along these lines is the CTA <ref> [42] </ref>. More recently, the bsp model [43, 44] and the logp model [14] have gained wide acceptance as general-purpose models of parallel computation. In these models the parallel machine is abstracted as a collection of processors-memory units with no global shared memory.
Reference: [43] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: In recent years, distributed-memory models that characterize the interconnection network abstractly by parameters that capture its performance have gained much attention. An early work along these lines is the CTA [42]. More recently, the bsp model <ref> [43, 44] </ref> and the logp model [14] have gained wide acceptance as general-purpose models of parallel computation. In these models the parallel machine is abstracted as a collection of processors-memory units with no global shared memory. <p> Section 5 concludes the paper with a discussion of some of the important features of the qsm. Since we will make several comparisons of the qsm model to the bsp model, we conclude this section by presenting the definition of the Bulk-Synchronous Parallel (bsp) model <ref> [43, 44] </ref>. The bsp model consists of p processor/memory components that communicate by sending point-to-point messages. The interconnection network supporting this communication is characterized by a bandwidth parameter g and a latency parameter L. A bsp computation consists of a sequence of "supersteps" separated by bulk synchronizations. <p> Additionally, by Observation 3.4, for oblivious bsp algorithms there is a very simple optimal step-by-step mapping of the oblivious bsp algorithm on to the qsm. 4 QSM Emulation Results Recall that we defined the Bulk Synchronous Parallel (bsp) model of <ref> [43, 44] </ref> in Section 1. In this section we present a work-preserving emulation of the qsm on the bsp.
Reference: [44] <author> L. G. Valiant. </author> <title> General purpose parallel architectures. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A, </booktitle> <pages> pages 943-972. </pages> <publisher> Elsevier Science Publishers B.V., </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1990. </year> <month> 14 </month>
Reference-contexts: In recent years, distributed-memory models that characterize the interconnection network abstractly by parameters that capture its performance have gained much attention. An early work along these lines is the CTA [42]. More recently, the bsp model <ref> [43, 44] </ref> and the logp model [14] have gained wide acceptance as general-purpose models of parallel computation. In these models the parallel machine is abstracted as a collection of processors-memory units with no global shared memory. <p> Section 5 concludes the paper with a discussion of some of the important features of the qsm. Since we will make several comparisons of the qsm model to the bsp model, we conclude this section by presenting the definition of the Bulk-Synchronous Parallel (bsp) model <ref> [43, 44] </ref>. The bsp model consists of p processor/memory components that communicate by sending point-to-point messages. The interconnection network supporting this communication is characterized by a bandwidth parameter g and a latency parameter L. A bsp computation consists of a sequence of "supersteps" separated by bulk synchronizations. <p> Additionally, by Observation 3.4, for oblivious bsp algorithms there is a very simple optimal step-by-step mapping of the oblivious bsp algorithm on to the qsm. 4 QSM Emulation Results Recall that we defined the Bulk Synchronous Parallel (bsp) model of <ref> [43, 44] </ref> in Section 1. In this section we present a work-preserving emulation of the qsm on the bsp.
Reference: [45] <author> U. Vishkin. </author> <title> A parallel-design distributed-implementation (PDDI) general purpose computer. </title> <journal> Theoretical Computer Science, </journal> <volume> 32 </volume> <pages> 157-172, </pages> <year> 1984. </year>
Reference: [46] <author> J. S. Vitter and E. A. M. Shriver. </author> <title> Optimal disk I/O with parallel block transfer. </title> <booktitle> In Proc. 22nd ACM Symp. on Theory of Computing, </booktitle> <pages> pages 159-169, </pages> <month> May </month> <year> 1990. </year>
Reference: [47] <author> H. A. G. Wijshoff and B. H. H. Juurlink. </author> <title> A quantitative comparison of parallel computation models. </title> <booktitle> In Proc. 8th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 13-24, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: The bsp and logp models have become very popular in recent years, and many algorithms have been designed and analyzed on these models and their extensions (see, e.g., <ref> [4, 8, 17, 25, 27, 36, 47] </ref>). However, algorithms designed for these models tend to have rather complicated performance analyses, because of the number of parameters in the model as well as the need to keep track of the exact memory partition across the processors at each step.
References-found: 47

