URL: http://www.cs.monash.edu.au/~rohan/PAPERS/maxent.ps
Refering-URL: http://www.cs.monash.edu.au/~rohan/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: fdld,jono,rohan,cswg@cs.monash.edu.au  
Title: BAYESIAN ESTIMATION OF THE VON MISES CONCENTRATION PARAMETER  
Author: David L. Dowe, Jonathan J. Oliver, Rohan A. Baxter and Chris S. Wallace, 
Address: 3168, Australia  
Affiliation: Department of Computer Science, Monash University, Clayton, Vic.  
Abstract: The von Mises distribution is a maximum entropy distribution. It corresponds to the distribution of an angle of a compass needle in a uniform magnetic field of direction, , with concentration parameter, . The concentration parameter, , is the ratio of the field strength to the temperature of thermal fluctuations. Previously, we obtained a Bayesian estimator for the von Mises distribution parameters using the information-theoretic Minimum Message Length (MML) principle. Here, we examine a variety of Bayesian estimation techniques by examining the posterior distribution in both polar and Cartesian co-ordinates. We compare the MML estimator with these fellow Bayesian techniques, and a range of Classical estimators. We find that the Bayesian estimators outperform the Classical estimators. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.A. Achcar and A.F.M. Smith. </author> <title> Aspects of reparameterization in approximate Bayesian inference. </title> <booktitle> In Bayesian and Likelihood Methods in Statistics and Econometrics, </booktitle> <pages> pages 439-452. </pages> <publisher> Elsevier Science, </publisher> <address> Amsterdam, </address> <year> 1993. </year>
Reference-contexts: This work raises questions about the effect of parameterisations on Bayesian inference (e.g., <ref> [1, 4] </ref>). We find that the Bayesian estimators outperform the Classical estimators on a series of simulations. 2.
Reference: [2] <author> J.O. Berger. </author> <title> Statistical Decision Theory and Bayesian analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: We consider two priors on [9]: h 2 () = (1 + 2 ) 4.2. The MAP Estimate It is generally known that the mode of the posterior (and hence the MAP estimate) is not invariant under non-linear parameter transformations <ref> [2] </ref>. Therefore, we consider these estimates in both polar co-ordinates (; ), and Cartesian co-ordinates (X; Y ) = (: cos (); : sin ()), since in some sense, both these representations can be considered "natural".
Reference: [3] <author> N.I. Fisher. </author> <title> Statistical Analysis of Circular Data. </title> <publisher> Cambridge University Press, </publisher> <year> 1993. </year>
Reference-contexts: This distribution is a circular analogue of the Gaussian distribution, to which it converges for large and small . Circular distributions and the von Mises distribution in particular are of interest in a wide range of fields, such as biology, geography, geology, geophysics, medicine, meteorology and oceanography <ref> [3] </ref>, and protein dihedral angles [10]. <p> We consider the Bayesian MAP estimate (the posterior mode) and the MML estimate. We consider the following Classical estimates: Maximum Likelihood, Marginalised Maximum Likelihood [7], and an estimator proposed by N.I. Fisher <ref> [3] </ref>. Whereas we were able to obtain an analytical closed form estimator of , all of our estimators for will have to be obtained by numerical methods. 3.1. <p> We then applied the estimation methods previously discussed, namely (A) the Maximum Likelihood estimator (MaxLik), (B) the Marginalised Maximum Likelihood estimator (Schou) [7], (C) an estimator proposed by N.I. Fisher (NF) <ref> [3] </ref>, (D) the MML estimator [9] with the h 2 and h 3 priors (MML h2 and MML h3 ), and (E) the three sensible MAP estimators discussed (MAP ; ; x;y Tables 1 and 2 give the mean absolute error (MAE), the mean squared error (MSE), and the mean Kullback-Leibler
Reference: [4] <author> J.A. Hills and A.F.M. Smith. </author> <title> Parameterization issues in Bayesian inference. </title> <booktitle> In Bayesian Statistics 4, </booktitle> <pages> pages 227-246. </pages> <publisher> Oxford University Press, </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: This work raises questions about the effect of parameterisations on Bayesian inference (e.g., <ref> [1, 4] </ref>). We find that the Bayesian estimators outperform the Classical estimators on a series of simulations. 2.
Reference: [5] <author> K.V. Mardia. </author> <title> Statistics of Directional Data. </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: (; )) In Appendix 1, we derive the Fisher matrix for the von Mises distribution [9]: F (; ) = E ( @ 2 L @@ ) @@ ) E ( @ 2 L # " 0 N A 0 () where A () is defined to be (see Mardia <ref> [5] </ref>): A () = I 1 () = 0 () (4) The determinant of the Fisher matrix is therefore: det (F (; )) = N 2 A ()A 0 () and thus the expression to be maximised is: Exp = h (; ) p (Dj; ) p (5) We maximise this
Reference: [6] <author> J.J. Oliver and R.A. Baxter. </author> <title> MML and Bayesianism: Similarities and differences. </title> <type> Technical report TR 206, </type> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, Australia, </address> <year> 1994. </year> <note> Available on the WWW from http://www.cs.monash.edu.au/ ~ jono. </note>
Reference-contexts: Therefore, we consider these estimates in both polar co-ordinates (; ), and Cartesian co-ordinates (X; Y ) = (: cos (); : sin ()), since in some sense, both these representations can be considered "natural". Oliver and Baxter <ref> [6] </ref> illustrated the manner in which the mode moves given the h 3 prior and the data: D = f 1 ; 2 ; : : : ; 10 g = f279 o ; 143 o ; 307 o ; 153 o ; 35 o ; 203 o ; 325 o <p> For the MML estimates, we use the prior distributions from Section 4.1.. Unlike the MAP estimate, we do not need to consider the parameterisation for the MML estimate, since the MML estimate is invariant under 1-1 differentiable transformations (see [8], [11, p245] or <ref> [6, Section 5.4] </ref>). 4 D.L. Dowe, J.J. Oliver, R.A. Baxter and C.S.
Reference: [7] <author> G. Schou. </author> <title> Estimation of the concentration parameter in von Mises-Fisher distributions. </title> <journal> Biometrika, </journal> <volume> 65 </volume> <pages> 369-377, </pages> <year> 1978. </year>
Reference-contexts: Estimation Methods We consider and compare several Bayesian and Classical approaches to estimating the concentration parameter of the von Mises distribution. We consider the Bayesian MAP estimate (the posterior mode) and the MML estimate. We consider the following Classical estimates: Maximum Likelihood, Marginalised Maximum Likelihood <ref> [7] </ref>, and an estimator proposed by N.I. Fisher [3]. Whereas we were able to obtain an analytical closed form estimator of , all of our estimators for will have to be obtained by numerical methods. 3.1. <p> We then applied the estimation methods previously discussed, namely (A) the Maximum Likelihood estimator (MaxLik), (B) the Marginalised Maximum Likelihood estimator (Schou) <ref> [7] </ref>, (C) an estimator proposed by N.I.
Reference: [8] <author> C.S. Wallace and D.M. Boulton. </author> <title> An invariant Bayes method for point estimation. </title> <journal> Classification Society Bulletin, </journal> <volume> 3(3) </volume> <pages> 11-34, </pages> <year> 1975. </year>
Reference-contexts: For the MML estimates, we use the prior distributions from Section 4.1.. Unlike the MAP estimate, we do not need to consider the parameterisation for the MML estimate, since the MML estimate is invariant under 1-1 differentiable transformations (see <ref> [8] </ref>, [11, p245] or [6, Section 5.4]). 4 D.L. Dowe, J.J. Oliver, R.A. Baxter and C.S.
Reference: [9] <author> C.S. Wallace and D.L. Dowe. </author> <title> MML estimation of the von Mises concentration parameter. </title> <type> Technical report TR 193, </type> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, Australia, </address> <note> 1993; submitted to Australian J. </note> <author> Statistics. </author> <title> Bayesian estimation of the von Mises concentration parameter 7 </title>
Reference-contexts: We consider a range of estimators for the parameters of the von Mises distribution, both Classical and Bayesian, including a Bayesian estimator <ref> [9] </ref> obtained using the information-theoretic Minimum Message Length (MML) principle [11], and a variety of Bayesian estimators obtained by examining the posterior distribution in both polar and Cartesian co-ordinates. This work raises questions about the effect of parameterisations on Bayesian inference (e.g., [1, 4]). <p> We also note that the Bayesian MAP estimate requires a choice of parameterisation, an issue we discuss in Section 4.2.. 4.1. Prior Distributions We assume a prior h () = 1 2 on independent of . We consider two priors on <ref> [9] </ref>: h 2 () = (1 + 2 ) 4.2. The MAP Estimate It is generally known that the mode of the posterior (and hence the MAP estimate) is not invariant under non-linear parameter transformations [2]. <p> Minimising the Message Length is then equivalent to maximising: Exp = h (; ) p (Dj; ) det (F (; )) In Appendix 1, we derive the Fisher matrix for the von Mises distribution <ref> [9] </ref>: F (; ) = E ( @ 2 L @@ ) @@ ) E ( @ 2 L # " 0 N A 0 () where A () is defined to be (see Mardia [5]): A () = I 1 () = 0 () (4) The determinant of the Fisher <p> We then applied the estimation methods previously discussed, namely (A) the Maximum Likelihood estimator (MaxLik), (B) the Marginalised Maximum Likelihood estimator (Schou) [7], (C) an estimator proposed by N.I. Fisher (NF) [3], (D) the MML estimator <ref> [9] </ref> with the h 2 and h 3 priors (MML h2 and MML h3 ), and (E) the three sensible MAP estimators discussed (MAP ; ; x;y Tables 1 and 2 give the mean absolute error (MAE), the mean squared error (MSE), and the mean Kullback-Leibler distance (MKL) for each of <p> Conclusions and Discussion From the results given in Section 6, we draw the following conclusions. Firstly, the Bayesian methods for point estimation outperformed the classical point estimators, very convincingly for small N . An extensive comparison of the MML estimator and the Classical estimators was performed in <ref> [9] </ref>. Secondly, the Bayesian methods were not overly sensitive to the choice of prior.
Reference: [10] <author> C.S. Wallace and D.L. Dowe. </author> <title> Intrinsic classification by MML the Snob program. </title> <booktitle> In Proc. of the 7th Australian Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 37-44. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: Circular distributions and the von Mises distribution in particular are of interest in a wide range of fields, such as biology, geography, geology, geophysics, medicine, meteorology and oceanography [3], and protein dihedral angles <ref> [10] </ref>.

References-found: 10

