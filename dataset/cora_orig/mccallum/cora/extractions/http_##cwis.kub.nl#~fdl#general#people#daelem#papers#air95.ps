URL: http://cwis.kub.nl/~fdl/general/people/daelem/papers/air95.ps
Refering-URL: http://ilk.kub.nl/~ilk/papers/abstracts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Walter.Daelemans@kub.nl (ii) fantal,weijtersg@cs.rulimburg.nl  
Title: IGTree: Using Trees for Compression and Classification in Lazy Learning Algorithms  
Author: Walter Daelemans (i) Antal van den Bosch (ii) Ton Weijters (ii) (i) 
Address: Netherlands  
Affiliation: Computational Linguistics MATRIKS Tilburg University, The Netherlands Maastricht University, The  
Abstract: We describe the IGTree learning algorithm, which compresses an instance base into a tree structure. The concept of information gain is used as a heuristic function for performing this compression. IGTree produces trees that, compared to other lazy learning approaches, reduce storage requirements and the time required to compute classifications. Furthermore, we obtained similar or better generalization accuracy with IGTree when trained on two complex linguistic tasks, viz. letter-phoneme transliteration and part-of-speech-tagging, when compared to alternative lazy learning and decision tree approaches (viz., IB1, information-gain-weighted IB1, and C4.5). A third experiment, with the task of word hyphenation, demonstrates that when the mutual differences in information gain of features is too small, IGTree as well as information-gain-weighted IB1 perform worse than IB1. These results indicate that IGTree is a useful algorithm for problems characterized by the availability of a large number of training instances described by symbolic features with sufficiently differing information gain values. keywords: lazy learning, eager learning, decision trees, information gain, data compression, instance base indexing
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <pages> 37-66. </pages>
Reference-contexts: The problem is usually described (in terms of linguistic rules) as noisy and complex, with many subregularities and (pockets of) exceptions. In other words, apart from a core of generalizations, there is a relatively large periphery of irreg ularities. In lazy learning <ref> (e.g., the IB1 algorithm in Aha, Kibler, and Albert, 1991) </ref>, similarity of a new instance to stored instances is used to find the nearest neighbors of the new instance. The classes associated with the nearest neighbor instances are then used to predict the class of the new instance.
Reference: <author> Aha, D. W. </author> <year> (1992). </year> <title> Generalizing from case studies: A case study. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> 1-10. </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The observation that the problems exhibit a lot of sub-regularity and exceptions may explain why full memory produces better results than an approach in which not all training items are kept in memory <ref> (cf. Aha, 1992) </ref>. Unfortunately, as the prediction function in lazy learning has to compare a test instance to all stored instances, and our linguistic data sets typically contain hundreds of thousands of instances, processing of new instances is prohibitively slow.
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> A case-based approach to knowledge acquisition for domain-specific sentence analysis. </title> <booktitle> In AAAI-93, </booktitle> <pages> 798-803. </pages>
Reference: <author> Daelemans, W. </author> <year> (1995). </year> <title> Memory-based lexical acquisition and processing. </title> <editor> In Stef-fens, P., editor, </editor> <booktitle> Machine Translation and the Lexicon, Lecture Notes in Artificial Intelligence 898. </booktitle> <address> Berlin: </address> <publisher> Springer. </publisher>
Reference: <author> Daelemans, W., Van den Bosch, A. </author> <year> (1992). </year> <title> Generalisation performance of backpropagation learning on a syllabification task. </title> <editor> In M. Drossaers & A. Nijholt (Eds.), </editor> <booktitle> 18 TWLT3: Connectionism and Natural Language Processing. </booktitle> <institution> Enschede: Twente University. </institution>
Reference-contexts: We noticed that IB1, when extended with a simple feature weighting similarity function, sometimes outperforms both connectionist approaches and knowledge-based "linguistic-engineering" 2 approaches (Daelemans and Van den Bosch, 1992, 1994; Van den Bosch and Daele--mans, 1993). The similarity function we introduced in lazy learning <ref> (Daelemans and Van den Bosch, 1992) </ref> consisted simply of multiplying, when comparing two instances, the similarity between the values for each feature with the corresponding information gain for that feature (information gain is also implemented in C4.5, Quinlan, 1993, to guide decision tree building).
Reference: <author> Daelemans, W., Van den Bosch, A. </author> <year> (1994). </year> <title> A language-independent, data-oriented architecture for grapheme-to-phoneme conversion. </title> <booktitle> In Proceedings of ESCA-IEEE Speech Synthesis Conference '94, </booktitle> <address> New York. </address>
Reference: <author> Deng, K. & Moore, A. W. </author> <year> (1995). </year> <title> Multiresolution Instance-Based Learning. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artififical Intelligence. </booktitle> <address> Montreal: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dougherty, J., Kohavi, R. & Sahami, M. </author> <year> (1995). </year> <title> Supervised and unsupervised dis-cretization of continuous features. </title> <booktitle> In Proceedings of the International Conference on Machine Learning '95. </booktitle>
Reference-contexts: No empirical studies addressing this issue have been published yet. Although we developed IGTree to deal with the nominal, unordered features with which we describe our linguistic instances, IGTree can be extended to handle continuous features by means of discretization techniques <ref> (cf. Dougherty, Kohavi, and Sahami, 1995) </ref>. decision trees on a small symbolic dataset. On the basis of size, shape, and number of holes, an object is to be classified as nut, screw, key, pen, or scissors. The instance base contains 12 instances.
Reference: <author> Friedman, J., Bentley, J., and Ari Finkel, R. </author> <year> (1977). </year> <title> An algorithm for finding best matches in logarithmic expected time. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 3(3). </volume>
Reference-contexts: K-d trees <ref> (Friedman, Bentley, and Finkel 1977) </ref> are binary trees that have been proposed for indexing databases of instances (with ordered feature values, e.g., numeric values) for use in k-nearest neighbor approaches.
Reference: <author> Kitano, H. </author> <year> (1993). </year> <title> Challenges of massive parallelism. </title> <booktitle> In IJCAI, </booktitle> <pages> 813-834. </pages>
Reference: <author> Kohavi, R., & Li, C-H. </author> <year> (1995). </year> <title> Oblivious decision trees, graphs, and top-down pruning. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1071-1077. </pages> <address> Montreal: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A tree produced by the IGTree algorithm is oblivious because all nodes at a certain level in the tree test the same feature. The IGTree approach differs in two aspects from other oblivious decision tree (cf. Langley and Sage, 1994) and oblivious decision 6 graph <ref> (cf. Kohavi and Li, 1995) </ref> approaches. First, in IGTree, information gain of features is used to determine the order in which they are expanded in the decision tree. <p> No empirical studies addressing this issue have been published yet. Although we developed IGTree to deal with the nominal, unordered features with which we describe our linguistic instances, IGTree can be extended to handle continuous features by means of discretization techniques <ref> (cf. Dougherty, Kohavi, and Sahami, 1995) </ref>. decision trees on a small symbolic dataset. On the basis of size, shape, and number of holes, an object is to be classified as nut, screw, key, pen, or scissors. The instance base contains 12 instances.
Reference: <author> Langley, P., & Sage, S. </author> <year> (1994). </year> <title> Oblivious decision trees and abstract cases. </title> <editor> In D. W. Aha (Ed.), </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: A tree produced by the IGTree algorithm is oblivious because all nodes at a certain level in the tree test the same feature. The IGTree approach differs in two aspects from other oblivious decision tree <ref> (cf. Langley and Sage, 1994) </ref> and oblivious decision 6 graph (cf. Kohavi and Li, 1995) approaches. First, in IGTree, information gain of features is used to determine the order in which they are expanded in the decision tree.
Reference: <author> Nunn, A. & van Heuven, V. J. </author> <year> (1993). </year> <title> Morphon, lexicon-based text-to-phoneme conversion and phonological rules. </title> <editor> In V. J. van Heuven & L. C. Pols (Eds.), </editor> <title> Analysis and Synthesis of Speech: Strategic Research Towards High-Quality Text-to-Speech Generation. </title> <publisher> Berlin: Mouton de Gruyter. </publisher>
Reference-contexts: Generalization accuracy Standard Memory usage Algorithm on test phonemes deviation (bytes) IGTree 97.07 0.11 77,749 IB1 92.11 0.15 1,573,885 C4.5 97.03 0.14 992,047 As a second illustration of accuracy, we mention the results of a comparison between IGTree trained on a set of 70,000 Dutch word-pronunciation pairs, and Morpa-cum-Morphon <ref> (Nunn and Van Heuven, 1993) </ref>, a state-of-the-art "linguistic-engineering" system for Dutch.
Reference: <author> Omohundro, S. M. </author> <year> (1991). </year> <title> Bumptrees for Efficient Function, Constraint, and Classification Learning. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky (eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 3. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. 19 Rumelhart, </publisher> <editor> D. E., Hinton, G. E., & Williams, R. J. </editor> <year> (1986). </year> <title> Learning internal repre-sentations by error propagation. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1: Foundations. </booktitle> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: A fundamental difference with decision trees concerns the purpose of IGTrees. The goal of Top Down Induction of Decision Trees, as in the state-of-the-art program C4.5 <ref> (Quinlan, 1993) </ref>, is to abstract from the training examples. In contrast, we use decision trees for lossless compression of the training examples. Pruning of the resulting tree in order to derive understandable decision trees or rule sets is therefore not an issue in our approach. <p> Finally, in IGTree, defaults are computed at each node of the tree (i.e., defaults are local), whereas in TDIDT, global defaults are used (although in C4.5, a similar local default assignment procedure is used). In terms of high compression without generalisation performance loss, C4.5rules <ref> (Quinlan, 1993) </ref> appears a strong alternative to IGTree.
Reference: <author> Sejnowski, T. J., Rosenberg, C. S. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages>
Reference: <author> Stanfill, C. and Waltz, D. </author> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29, </volume> <pages> 1212-1228. </pages>
Reference: <author> Van den Bosch, A., Daelemans, W. </author> <year> (1993). </year> <title> Data-oriented methods for grapheme-to-phoneme conversion. </title> <booktitle> In Proceedings of the 6th Conference of the EACL, </booktitle> <pages> 45-53. </pages> <address> Utrecht: OTS. </address>
Reference-contexts: Generalization accuracy Standard Memory usage Algorithm on test phonemes deviation (bytes) IGTree 97.07 0.11 77,749 IB1 92.11 0.15 1,573,885 C4.5 97.03 0.14 992,047 As a second illustration of accuracy, we mention the results of a comparison between IGTree trained on a set of 70,000 Dutch word-pronunciation pairs, and Morpa-cum-Morphon <ref> (Nunn and Van Heuven, 1993) </ref>, a state-of-the-art "linguistic-engineering" system for Dutch. <p> Tested on an identical test set (provided by the developers of Morpa-cum-Morphon), IGTree produced 89.5% correctly transliterated words, whereas Morpa-cum-Morphon only converted 85.3% words correctly <ref> (Van den Bosch and Daele-mans, 1993) </ref>. 3.2 Hyphenation and Part-of-Speech Tagging In order to obtain a better insight into the properties of IGTree, we provide some additional results obtained with IGTree on different datasets.
Reference: <author> Weijters, A., & Hoppenbrouwers, G. </author> <year> (1990). </year> <title> NetSpraak: een neuraal netwerk voor grafeem-foneem-omzetting. Tabu, </title> <booktitle> 20:1, </booktitle> <pages> 1-25. </pages>
Reference-contexts: The connectionist approach was replicated for Dutch in NetSpraak <ref> (Weijters & Hop-penbrouwers, 1990) </ref>. From celex, a lexical data base of English, German, and Dutch, we derived a data base consisting of 20,000 Dutch word-pronunciation pairs.
Reference: <author> Weijters, A. </author> <year> (1991). </year> <title> A simple look-up procedure superior to NETtalk? In Proceedings of the International Conference on Artificial Neural Networks, </title> <address> Espoo, Finland. </address>
Reference: <author> Wess, S., Althoff, K. D., and Derwand, G. </author> <year> (1994). </year> <title> Using k-d trees to improve the retrieval step in case-based reasoning. </title> <editor> In S. Wess, K. D. Althoff, and M. M. Richter (Eds.), </editor> <booktitle> Topics in Case-Based Reasoning. </booktitle> <address> Berlin: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Wess, S. </author> <year> (1995). </year> <title> Fallbasiertes Problemlosen in wissensbasierten Systemen zur Entschei-dungsunterstutzung und Diagnostik. </title> <type> Doctoral Dissertation, </type> <institution> University of Kaiser-slautern. </institution>
Reference: <author> Wolpert, D. H. </author> <year> (1990). </year> <title> Constructing a generalizer superior to NETtalk via a mathematical theory of generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 3, </volume> <pages> 445-452. 20 </pages>
References-found: 23

