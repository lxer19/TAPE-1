URL: http://www.cs.washington.edu/research/jair/volume9/cook98a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/contents/v9.html
Root-URL: http://www.cs.washington.edu
Email: cook@cse.uta.edu  cvarnell@sfasu.edu  
Title: Adaptive Parallel Iterative Deepening Search  
Author: Diane J. Cook R. Craig Varnell Stephen F. 
Address: Box 19015, Arlington, TX 76019 USA  Box 13063, Nacogdoches, TX 75962 USA  
Affiliation: Department of Computer Science and Engineering University of Texas at Arlington  Department of Computer Science  Austin State University  
Note: Journal of Artificial Intelligence Research 9 (1998) 139-166 Submitted 4/98; published 10/98  
Abstract: Many of the artificial intelligence techniques developed to date rely on heuristic search through large spaces. Unfortunately, the size of these spaces and the corresponding computational effort reduce the applicability of otherwise novel and effective algorithms. A number of parallel and distributed approaches to search have considerably improved the performance of the search process. Our goal is to develop an architecture that automatically selects parallel search strategies for optimal performance on a variety of search problems. In this paper we describe one such architecture realized in the Eureka system, which combines the benefits of many different approaches to parallel heuristic search. Through empirical and theoretical analyses we observe that features of the problem space directly affect the choice of optimal parallel search strategy. We then employ machine learning techniques to select the optimal parallel search strategy for a given problem space. When a new search task is input to the system, Eureka uses features describing the search space and the chosen architecture to automatically select the appropriate search strategy. Eureka has been tested on a MIMD parallel processor, a distributed network of workstations, and a single workstation using multithreading. Results generated from fifteen puzzle problems, robot arm motion problems, artificial search spaces, and planning problems indicate that Eureka outperforms any of the tested strategies used exclusively for all problem instances and is able to greatly reduce the search time for these applications. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agrawal, D., Janakiram, V., & Mehrotra, R. </author> <year> (1988). </year> <title> A randomized parallel branch and bound algorithm. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. 69-75. </pages> <institution> The Pennsylvania State University. </institution>
Reference-contexts: Advances in parallel and distributed computing offer potentially large increases in performance to such compute-intensive tasks. In response, a number of parallel approaches have been developed to improve various search algorithms including depth-first search (Kumar & Rao, 1990), branch-and-bound search <ref> (Agrawal, Janakiram, & Mehrotra, 1988) </ref>, A* (Evett, Hendler, Mahanti, & Nau, 1995; Mahapatra & Dutt, 1995), IDA* (Mahanti & Daniels, 1993; Powley, Ferguson, & Korf, 1993; Powley & Korf, 1991), and game tree search (Feldmann, Mysliwietz, & Monien, 1994), as well as to improve the run time of specific applications such
Reference: <author> Anderson, S., & Chen, M. C. </author> <year> (1987). </year> <title> Parallel branch-and-bound algorithms on the hypercube. </title> <booktitle> In Proceedings of the Conference on Hypercube Multiprocessors, </booktitle> <pages> pp. 309-317. </pages>
Reference: <author> Barrett, A., & Weld, D. </author> <year> (1992). </year> <title> Partial order planning: evaluating possible efficiency gains. </title> <type> Tech. rep. CSE TR 92-05-1, </type> <institution> University of Washington. </institution>
Reference-contexts: We generate 20 problem instances for use in the experiments. For our fourth test domain, we integrate our own C-based version of the SNLP nonlinear planner <ref> (Barrett & Weld, 1992) </ref> into Eureka. To conform with the Eureka architecture, the integrated planner utilizes IDA* search instead of the Best-First search method employed by SNLP.
Reference: <author> Bhattacharjee, S., Calvert, K., & Zegura, E. W. </author> <year> (1997). </year> <title> An architecture for active networking. </title> <editor> In Tantawy, A. N. (Ed.), </editor> <booktitle> High Performance Networking, </booktitle> <pages> pp. 265-279. </pages> <publisher> Chapman & Hall. </publisher>
Reference: <author> Cestnik, B. </author> <year> (1990). </year> <title> Estimating probabilities: a crucial task in machine learning. </title> <booktitle> In Proceedings of the Ninth European Conference on Artificial Intelligence, </booktitle> <pages> pp. 174-179. </pages>
Reference-contexts: supplied the data from all of the 15 Puzzle classification experiments described in the previous section as input to versions of C4.5, the ID3 decision tree induction algorithm (Quinlan, 1986), the CN2 sequential covering algorithm (Clark & Niblett, 1989), a backpropagation neural net (Rumelhart & McClelland, 1986), a Bayesian classifier <ref> (Cestnik, 1990) </ref>, and a majority-wins classifier. As with the other experiments, results are based on ten-fold cross-validation. Table 14 shows that the decision tree algorithms performed best on this particular data set. Ultimately, the best machine learning algorithm in this context is the algorithm that consistently yields the best speedup.
Reference: <author> Challou, D., Gini, M., & Kumar, V. </author> <year> (1993). </year> <title> Parallel search algorithms for robot motion planning. </title> <editor> In Geller, J. (Ed.), </editor> <booktitle> Proceedings of the AAAI Symposium on Innovative Applications of Massive Parallelism, </booktitle> <pages> pp. 40-47. </pages>
Reference-contexts: 1995), IDA* (Mahanti & Daniels, 1993; Powley, Ferguson, & Korf, 1993; Powley & Korf, 1991), and game tree search (Feldmann, Mysliwietz, & Monien, 1994), as well as to improve the run time of specific applications such as the fifteen puzzle problem (Kumar & Rao, 1990) and robot arm path planning <ref> (Challou, Gini, & Kumar, 1993) </ref>. In addition to MIMD c fl1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.
Reference: <author> Clark, P., & Niblett, R. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-284. </pages>
Reference-contexts: To test the results of various existing approaches, we supplied the data from all of the 15 Puzzle classification experiments described in the previous section as input to versions of C4.5, the ID3 decision tree induction algorithm (Quinlan, 1986), the CN2 sequential covering algorithm <ref> (Clark & Niblett, 1989) </ref>, a backpropagation neural net (Rumelhart & McClelland, 1986), a Bayesian classifier (Cestnik, 1990), and a majority-wins classifier. As with the other experiments, results are based on ten-fold cross-validation. Table 14 shows that the decision tree algorithms performed best on this particular data set.
Reference: <author> Cook, D. J., & Varnell, R. C. </author> <year> (1997). </year> <title> Maximizing the benefits of parallel search using machine learning. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pp. 559-564. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: In response to this problem, we have developed the Eureka parallel search engine that combines many of these approaches to parallel heuristic search. Eureka <ref> (Cook & Varnell, 1997) </ref> is a parallel IDA* search architecture that merges multiple approaches to task distribution, load balancing, and tree ordering, and can be run on a MIMD shared memory or distributed memory parallel processor, a distributed network of workstations, or a single machine with multithreading. <p> On the other hand, dividing the search space among processors can be more effective when the branching factor is very large and the number of IDA* iterations is relatively small. A compromise between these approaches divides the set of processors into clusters <ref> (Cook, 1997) </ref>. Each cluster is given a unique cost threshold, and the search space is divided between processors within each cluster, as shown in Figure 3.
Reference: <author> Cook, D. J. </author> <year> (1997). </year> <title> A hybrid approach to improving the performance of parallel search. </title> <booktitle> In Parallel Processing for Artificial Intelligence 3, </booktitle> <pages> pp. 120-145. </pages> <publisher> Elsevier. </publisher>
Reference-contexts: In response to this problem, we have developed the Eureka parallel search engine that combines many of these approaches to parallel heuristic search. Eureka <ref> (Cook & Varnell, 1997) </ref> is a parallel IDA* search architecture that merges multiple approaches to task distribution, load balancing, and tree ordering, and can be run on a MIMD shared memory or distributed memory parallel processor, a distributed network of workstations, or a single machine with multithreading. <p> On the other hand, dividing the search space among processors can be more effective when the branching factor is very large and the number of IDA* iterations is relatively small. A compromise between these approaches divides the set of processors into clusters <ref> (Cook, 1997) </ref>. Each cluster is given a unique cost threshold, and the search space is divided between processors within each cluster, as shown in Figure 3.
Reference: <author> Cook, D. J., Hall, L., & Thomas, W. </author> <year> (1993). </year> <title> Parallel search using transformation-ordering iterative-deepening A*. </title> <journal> The International Journal of Intelligent Systems, </journal> <volume> 8 (8), </volume> <pages> 855-873. </pages>
Reference: <author> Cook, D. J., & Lyons, G. </author> <year> (1993). </year> <title> Massively parallel IDA* search. </title> <journal> Journal of Artificial Intelligence Tools, </journal> <volume> 2 (2), </volume> <pages> 163-180. </pages>
Reference: <author> Craig, J. J. </author> <year> (1989). </year> <title> Introduction to robotics. </title> <publisher> Addison-Wesley. </publisher>
Reference: <author> Evett, M., Hendler, J., Mahanti, A., & Nau, D. </author> <year> (1995). </year> <title> PRA*: massively parallel heuristic search. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 25, </volume> <pages> 133-143. </pages>
Reference: <author> Feldmann, R., Mysliwietz, P., & Monien, B. </author> <year> (1994). </year> <title> Studying overheads in massively parallel min/max-tree evaluation. </title> <booktitle> In Proceedings of the Sixth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. </pages> <month> 94-103. </month> <title> Association for Computing Machinery. 164 Adaptive Parallel Iterative Deepening Search Frank, </title> <editor> M., Sukavirija, P., & Foley, J. D. </editor> <year> (1995). </year> <title> Inference bear: designing interactive interfaces through before and after snapshots. </title> <booktitle> In Proceedings of the ACM Symposium on Designing Interactive Systems, </booktitle> <pages> pp. 167-175. </pages> <institution> Association for Computing Machinery. </institution>
Reference-contexts: been developed to improve various search algorithms including depth-first search (Kumar & Rao, 1990), branch-and-bound search (Agrawal, Janakiram, & Mehrotra, 1988), A* (Evett, Hendler, Mahanti, & Nau, 1995; Mahapatra & Dutt, 1995), IDA* (Mahanti & Daniels, 1993; Powley, Ferguson, & Korf, 1993; Powley & Korf, 1991), and game tree search <ref> (Feldmann, Mysliwietz, & Monien, 1994) </ref>, as well as to improve the run time of specific applications such as the fifteen puzzle problem (Kumar & Rao, 1990) and robot arm path planning (Challou, Gini, & Kumar, 1993). In addition to MIMD c fl1998 AI Access Foundation and Morgan Kaufmann Publishers.
Reference: <author> Furuichi, M., Taki, K., & Ichyoshi, N. </author> <year> (1990). </year> <title> A multi-level load balancing scheme for or-parallel exhaustive search programs on the multi-psi. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 100-106. </pages> <institution> Association for Computing Machinery. </institution>
Reference: <author> Kale, L. V., & Saletore, V. A. </author> <year> (1990). </year> <title> Parallel state-space search for a first solution with consistent linear speedups. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 19 (4), </volume> <pages> 251-293. </pages>
Reference-contexts: Thus the serial algorithm will perform disproportionately more search than all processors combined using the parallel algorithm. Each type of parallel search approach described in this paper can yield superlinear speedup under certain conditions. Some algorithms more closely imitate serial search, but at a potential loss of overall performance <ref> (Kale & Saletore, 1990) </ref>. Eureka's selection of strategies in the fifteen puzzle domain does not perform consistently better than using some of the strategies in isolation. One reason for this disappointing performance is the nature of the training data.
Reference: <author> Karypis, G., & Kumar, V. </author> <year> (1992). </year> <title> Unstructured tree search on SIMD parallel computers. </title> <booktitle> In Proceedings of Supercomputing 92, </booktitle> <pages> pp. 453-462. </pages> <publisher> IEEE Computer Society. </publisher>
Reference: <author> Kumar, V., & Rao, V. N. </author> <year> (1990). </year> <title> Scalable parallel formulations of depth-first search. </title> <editor> In Kumar, Kanal, & Gopalakrishan (Eds.), </editor> <booktitle> Parallel Algorithms for Machine Intelligence and Vision, </booktitle> <pages> pp. 1-41. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Advances in parallel and distributed computing offer potentially large increases in performance to such compute-intensive tasks. In response, a number of parallel approaches have been developed to improve various search algorithms including depth-first search <ref> (Kumar & Rao, 1990) </ref>, branch-and-bound search (Agrawal, Janakiram, & Mehrotra, 1988), A* (Evett, Hendler, Mahanti, & Nau, 1995; Mahapatra & Dutt, 1995), IDA* (Mahanti & Daniels, 1993; Powley, Ferguson, & Korf, 1993; Powley & Korf, 1991), and game tree search (Feldmann, Mysliwietz, & Monien, 1994), as well as to improve the <p> (Evett, Hendler, Mahanti, & Nau, 1995; Mahapatra & Dutt, 1995), IDA* (Mahanti & Daniels, 1993; Powley, Ferguson, & Korf, 1993; Powley & Korf, 1991), and game tree search (Feldmann, Mysliwietz, & Monien, 1994), as well as to improve the run time of specific applications such as the fifteen puzzle problem <ref> (Kumar & Rao, 1990) </ref> and robot arm path planning (Challou, Gini, & Kumar, 1993). In addition to MIMD c fl1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. <p> A search algorithm typically stores nodes which have not been fully expanded on an Open list. When giving work to another processor, nodes can be given from the head of the list (deep in the tree), the tail (near the root), or from a sampling of all levels <ref> (Kumar & Rao, 1990) </ref>. A number of approaches have been introduced for reducing processor idle time using a load balancing operation.
Reference: <author> Lieberman, H. </author> <year> (1998). </year> <title> Integrating user interface agents with conventional applications. </title> <booktitle> In Proceedings of the ACM Conference on Intelligent User Interfaces, </booktitle> <pages> pp. 39-46. </pages> <institution> Association for Computing Machinery. </institution>
Reference: <author> Mahanti, A., & Daniels, C. </author> <year> (1993). </year> <title> SIMD parallel heuristic search. </title> <journal> Artificial Intelligence, </journal> <volume> 60 (2), </volume> <pages> 243-281. </pages>
Reference: <author> Mahapatra, N. R., & Dutt, S. </author> <year> (1995). </year> <title> New anticipatory load balancing strategies for parallel A* algorithms. </title> <booktitle> In Proceedings of the DIMACS Series on Discrete Mathematics and Theoretical Computer Science, </booktitle> <pages> pp. 197-232. </pages> <publisher> American Mathematical Society. </publisher>
Reference-contexts: Because one processor may run out of work before others, load balancing is used to activate the idle processor. The first phase of load balancing involves selecting a processor from which to request work. One example is the nearest neighbor approach <ref> (Mahapatra & Dutt, 1995) </ref>; alternative approaches include selecting random processors or allowing a master processor to keep track of processor loads and send the ID of a heavily loaded processor to one that is idle. <p> A number of approaches have been introduced for reducing processor idle time using a load balancing operation. Using the quality equalizing strategy <ref> (Mahapatra & Dutt, 1995) </ref>, processors anticipate idle time by sending out a work request when their load is almost empty, so that they can continue processing remaining nodes while waiting for a response.
Reference: <author> Minton, S. </author> <year> (1996). </year> <title> Automatically configuring constraint satisfaction programs: a case study. </title> <booktitle> Constraints, </booktitle> <volume> 1 (1), </volume> <pages> 7-43. </pages>
Reference: <author> Powley, C., Ferguson, C., & Korf, R. E. </author> <year> (1993). </year> <title> Depth-first heuristic search on a SIMD machine. </title> <journal> Artificial Intelligence, </journal> <volume> 60 (2), </volume> <pages> 199-242. </pages>
Reference: <author> Powley, C., & Korf, R. E. </author> <year> (1991). </year> <title> Single-agent parallel window search. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13 (5), </volume> <pages> 466-477. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: programs for machine learning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Problem attributes are combined with the corresponding classes and are fed as training examples to a machine learning system. We use C4.5 <ref> (Quinlan, 1993) </ref> to induce a decision tree from the pre-classified cases. A rule base is generated for each concept to be learned, corresponding to each of the strategy decisions listed above that need to be made. 4.
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning 1, </journal> <volume> 1 (1), </volume> <pages> 81-106. </pages>
Reference-contexts: To test the results of various existing approaches, we supplied the data from all of the 15 Puzzle classification experiments described in the previous section as input to versions of C4.5, the ID3 decision tree induction algorithm <ref> (Quinlan, 1986) </ref>, the CN2 sequential covering algorithm (Clark & Niblett, 1989), a backpropagation neural net (Rumelhart & McClelland, 1986), a Bayesian classifier (Cestnik, 1990), and a majority-wins classifier. As with the other experiments, results are based on ten-fold cross-validation.
Reference: <author> Rajpal, S. P., & Kumar, S. </author> <year> (1993). </year> <title> Parallel heuristic search algorithms for message passing multiprocessor systems. </title> <journal> Computer Science and Informatics, </journal> <volume> 23 (4), </volume> <pages> 7-18. </pages>
Reference: <author> Rao, V. N., Kumar, V., & Ramesh, K. </author> <year> (1987). </year> <title> A parallel implementation of iterative-deepening-A*. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pp. 178-182. </pages> <note> Morgan Kaufmann. </note> <author> 165 Cook and Varnell Reinefeld, A., & Schnecke, V. </author> <year> (1994). </year> <title> AIDA* asynchronous parallel IDA*. </title> <booktitle> In Proceedings of the Tenth Canadian Conference on Artificial Intelligence, </booktitle> <pages> pp. 295-302. </pages> <booktitle> Canadian Information Processing Society. </booktitle>
Reference: <author> Rumelhart, D. E., & McClelland, J. L. </author> <year> (1986). </year> <title> Parallel distributed processing: exploration in the microstructure of cognition, Volumes 1 and 2. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: the results of various existing approaches, we supplied the data from all of the 15 Puzzle classification experiments described in the previous section as input to versions of C4.5, the ID3 decision tree induction algorithm (Quinlan, 1986), the CN2 sequential covering algorithm (Clark & Niblett, 1989), a backpropagation neural net <ref> (Rumelhart & McClelland, 1986) </ref>, a Bayesian classifier (Cestnik, 1990), and a majority-wins classifier. As with the other experiments, results are based on ten-fold cross-validation. Table 14 shows that the decision tree algorithms performed best on this particular data set.
Reference: <author> Saletore, V. A. </author> <year> (1990). </year> <title> A distributed and adaptive dynamic load balancing scheme for parallel processing of medium-grain tasks. </title> <booktitle> In Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pp. 231-235. </pages>
Reference-contexts: Thus the serial algorithm will perform disproportionately more search than all processors combined using the parallel algorithm. Each type of parallel search approach described in this paper can yield superlinear speedup under certain conditions. Some algorithms more closely imitate serial search, but at a potential loss of overall performance <ref> (Kale & Saletore, 1990) </ref>. Eureka's selection of strategies in the fifteen puzzle domain does not perform consistently better than using some of the strategies in isolation. One reason for this disappointing performance is the nature of the training data.
Reference: <author> Steenkiste, P., Fisher, A., & Zhang, H. </author> <year> (1997). </year> <title> Darwin: resource management for application-aware networks. </title> <type> Tech. rep. </type> <institution> CMU-CS-97-195, Carnegie Mellon University. </institution>
Reference: <author> Suttner, C. </author> <year> (1997). </year> <title> Static partitioning with slackness. </title> <booktitle> In Parallel Processing for Artificial Intelligence 3, </booktitle> <pages> pp. 106-120. </pages> <publisher> Elsevier. </publisher>
Reference: <author> Varnell, R. C. </author> <year> (1997). </year> <title> An architecture for improving the performance of parallel search. </title> <type> Ph.D. thesis, </type> <institution> University of Texas at Arlington. </institution> <month> 166 </month>
Reference-contexts: In response to this problem, we have developed the Eureka parallel search engine that combines many of these approaches to parallel heuristic search. Eureka <ref> (Cook & Varnell, 1997) </ref> is a parallel IDA* search architecture that merges multiple approaches to task distribution, load balancing, and tree ordering, and can be run on a MIMD shared memory or distributed memory parallel processor, a distributed network of workstations, or a single machine with multithreading.
References-found: 33

