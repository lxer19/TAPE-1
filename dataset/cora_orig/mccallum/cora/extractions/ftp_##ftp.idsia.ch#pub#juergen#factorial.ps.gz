URL: ftp://ftp.idsia.ch/pub/juergen/factorial.ps.gz
Refering-URL: http://www.bmc.riken.go.jp/sensor/Allan/ICA/index.html
Root-URL: 
Email: yirgan@cs.colorado.edu  
Title: LEARNING FACTORIAL CODES BY PREDICTABILITY MINIMIZATION (Neural Computation, 4(6):863-879, 1992)  
Author: Jurgen Schmidhuber 
Address: Campus Box 430, Boulder, CO 80309, USA  
Affiliation: Department of Computer Science University of Colorado  
Abstract: I propose a novel general principle for unsupervised learning of distributed non-redundant internal representations of input patterns. The principle is based on two opposing forces. For each representational unit there is an adaptive predictor which tries to predict the unit from the remaining units. In turn, each unit tries to react to the environment such that it minimizes its predictability. This encourages each unit to filter `abstract concepts' out of the environmental input such that these concepts are statistically independent of those upon which the other units focus. I discuss various simple yet potentially powerful implementations of the principle which aim at finding binary factorial codes (Bar-low et al., 1989), i.e. codes where the probability of the occurrence of a particular input is simply the product of the probabilities of the corresponding code symbols. Such codes are potentially relevant for (1) segmentation tasks, (2) speeding up supervised learning, (3) novelty detection. Methods for finding factorial codes automatically implement Occam's razor for finding codes using a minimal number of units. Unlike previous methods the novel principle has a potential for removing not only linear but also non-linear output redundancy. Illustrative experiments show that algorithms based on the principle of predictability minimization are practically feasible. The final part of this paper describes an entirely local algorithm that has a potential for learning unique representations of extended input sequences.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. B. Barlow, T. P. Kaushal, and G. J. Mitchison. </author> <title> Finding minimum entropy codes. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 412-423, </pages> <year> 1989. </year>
Reference: [2] <author> S. Becker. </author> <title> Unsupervised learning procedures for neural networks. </title> <journal> International Journal of Neural Systems, </journal> <volume> 2(1 </volume> & 2):17-33, 1991. 
Reference: [3] <author> F. Foldiak. </author> <title> Forming sparse representations by local anti-hebbian learning. </title> <journal> Biological Cybernetics, </journal> <volume> 64 </volume> <pages> 165-170, </pages> <year> 1990. </year>
Reference: [4] <author> R. Linsker. </author> <title> Self-organization in a perceptual network. </title> <journal> IEEE Computer, </journal> <volume> 21 </volume> <pages> 105-117, </pages> <year> 1988. </year>
Reference: [5] <author> E. Oja. </author> <title> Neural networks, principal components, and subspaces. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(1) </volume> <pages> 61-68, </pages> <year> 1989. </year>
Reference: [6] <author> B. A. Pearlmutter and G. E. Hinton. G-maximization: </author> <title> An unsupervised learning procedure for discovering regularities. </title> <editor> In J. S. Denker, editor, </editor> <booktitle> Neural Networks for Computing: American Institute of Physics Conference Proceedings 151, </booktitle> <volume> volume 2, </volume> <pages> pages 333-338, </pages> <year> 1986. </year>
Reference: [7] <author> D. Prelinger. </author> <note> Diploma thesis, in preparation, 1992. </note> <institution> Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference: [8] <author> J. Rubner and K. Schulten. </author> <title> Development of feature detectors by self-organization: A network model. </title> <journal> Biological Cybernetics, </journal> <volume> 62 </volume> <pages> 193-199, </pages> <year> 1990. </year>
Reference: [9] <author> T. D. Sanger. </author> <title> An optimality principle for unsupervised learning. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <pages> pages 11-19. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference: [10] <author> J. H. Schmidhuber. </author> <title> Learning factorial codes by predictability minimization. </title> <type> Technical Report CU-CS-565-91, </type> <institution> Dept. of Comp. Sci., University of Colorado at Boulder, </institution> <month> December </month> <year> 1991. </year>
Reference: [11] <author> J. H. Schmidhuber. </author> <title> Learning complex, extended sequences using the principle of history compression. </title> <journal> Neural Computation, </journal> <volume> 4(2): </volume> <year> 1992. </year>
Reference: [12] <author> J. H. Schmidhuber. </author> <title> Learning unambiguous reduced sequence descriptions. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, to appear. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference: [13] <author> C. E. Shannon. </author> <title> A mathematical theory of communication (part III). </title> <journal> Bell System Technical Journal, </journal> <volume> XXVII:623-656, </volume> <year> 1948. </year>
Reference: [14] <author> F. M. Silva and L. B. Almeida. </author> <title> A distributed decorrelation algorithm. </title> <editor> In Erol Gelenbe, editor, </editor> <booktitle> Neural Networks, Advances and Applications. </booktitle> <publisher> North-Holland, </publisher> <year> 1991. </year> <note> To appear. </note>
Reference: [15] <author> P. J. Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1974. </year>
Reference: [16] <author> R. S. Zemel and G. E. Hinton. </author> <title> Discovering viewpoint-invariant relationships that characterize objects. </title> <editor> In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 299-305. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
References-found: 16

