URL: ftp://ftp.speech.sri.com/pub/papers/darpa98-lattices.ps
Refering-URL: http://www.speech.sri.com/people/stolcke/publications.html
Root-URL: 
Title: New Developments in Lattice-Based Search Strategies in SRI's Hub4 System  
Author: Fuliang Weng Andreas Stolcke Ananth Sankar 
Address: Menlo Park, California  
Affiliation: Speech Technology and Research Laboratory SRI International  
Abstract: We describe new developments in SRI's lattice-based progressive search strategy. These developments include the implementation of a new bigram lattice algorithm, lattice optimization techniques, and expansion of bigram lattices to trigram lattices. The new bigram lattice generation algorithm is based on generation of backtrace entries using a word-dependent N-best list decoding pass, followed by lattice generation from the backtrace entries. We present an algorithm to reduce the size of the bigram lattices while maintaining all valid paths. This algorithm is shown to reduce the size of the lattice by about 50%, allowing easier processing in later stages such as expansion to trigram lattices. We describe two algorithms to expand bigram lattices to trigram lattices. The first is a conventional method, while the second is a novel approach that results in compact trigram lattices that were found to be a factor of six smaller than lattices created with the conventional approach. Decoding with the new trigram lattices gave a 5% improvement in word error rate as compared to our previous search strategy which used trigram LMs to rescore N-best lists. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. E. Hopcroft and J. D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1979. </year>
Reference-contexts: Here, we explore an alternative way to make the resulting lattices smaller. The idea is to combine identical (sub)paths in the lattices so that the redundant nodes and transitions are removed. Similar research has been reported in the computer science literature <ref> [1] </ref>, where standard algorithms for minimizing deterministic finite state automata are given. More recently, algorithms for minimization of weighted transducers were also developed [2, 9]. These approaches differ from ours in several aspects.
Reference: 2. <author> M. Mohri and M. Riley. </author> <title> Weighted determinization and minimization for large vocabulary speech recognition. </title> <booktitle> In Proc. EU-ROSPEECH, </booktitle> <volume> vol. 1, </volume> <pages> pp. 131134, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: Similar research has been reported in the computer science literature [1], where standard algorithms for minimizing deterministic finite state automata are given. More recently, algorithms for minimization of weighted transducers were also developed <ref> [2, 9] </ref>. These approaches differ from ours in several aspects. First, we are dealing with word lattices, a dual representation of finite state automata, where nodes are states and transitions are labeled with words.
Reference: 3. <author> H. Murveit, J. Butzberger, V. Digalakis, and M. Wein-traub. </author> <title> Large-vocabulary dictation using SRI's DECIPHER speech recognition system: Progressive search techniques. </title> <booktitle> In Proc. ICASSP, </booktitle> <volume> vol. II, </volume> <pages> pp. 319322, </pages> <address> Minneapolis, </address> <year> 1993. </year>
Reference-contexts: 1. Introduction Progressive search techniques have previously been proposed as a method of applying complex knowledge sources in decoding for automatic speech recognition (ASR) <ref> [3] </ref>. The basic idea is to use simple knowledge sources, such as non-crossword acoustic models and bigram LMs to generate a lattice of possible hypotheses. The lattices are then progressively decoded with more complex knowledge sources such as crossword acoustic models or trigram LMs. <p> Our previous bigram word lattice algorithm intended to extract a subset of the full LM for an input utterance, so that the search can be narrowed in later-stage processing <ref> [3] </ref>. Unfortunately, its implementation was too simplistic in that the LM backoff node was retained in the lattice, causing almost any word in the lattice to be hypothesized at each word end, making recognition slow.
Reference: 4. <author> H. Ney and X. Aubert. </author> <title> A word graph algorithm for large vocabulary, continuous speech recognition. </title> <booktitle> In Proc. ICSLP, </booktitle> <pages> pp. 13551358, </pages> <address> Yokohama, </address> <year> 1994. </year> <title> Bigram F0 F1 Subset Lattice Total 1997 Evaluation 14.74 31.96 24.32 Post-evaluation 14.77 32.02 24.36 Table 7: 1-best recognition word error rates of trigram lattices expanded from different bigram lattices. </title>
Reference-contexts: Our new bigram lattice generation algorithm is based on the word-dependent N-best algorithm [8], and is similar to <ref> [4, 5] </ref>. The algorithm assumes that the starting time for a word depends on the preceding word but not on any word before that. Thus, a separate word hypothesis is propagated for each possible predecessor word.
Reference: 5. <author> J. Odell. </author> <title> The Use of Context in Large Vocabulary Speech Recognition. </title> <type> Ph.D. thesis, </type> <institution> Cambridge University Engineering Department, </institution> <address> Cambridge, U.K., </address> <year> 1995. </year>
Reference-contexts: Our new bigram lattice generation algorithm is based on the word-dependent N-best algorithm [8], and is similar to <ref> [4, 5] </ref>. The algorithm assumes that the starting time for a word depends on the preceding word but not on any word before that. Thus, a separate word hypothesis is propagated for each possible predecessor word.
Reference: 6. <author> A. Sankar, L. Heck, and A. Stolcke. </author> <title> Acoustic modeling for the SRI Hub4 partitioned evaluation continuous speech recognition system. </title> <booktitle> In ProceedingsDARPA Speech Recognition Workshop, </booktitle> <pages> pp. 127132, </pages> <address> Chantilly, VA, </address> <year> 1997. </year>
Reference-contexts: The lattices are then progressively decoded with more complex knowledge sources such as crossword acoustic models or trigram LMs. In search strategies we have previously used, such as in <ref> [6] </ref>, non-crossword acoustic models and bigram LMs were used to create word bigram lattices. Adapted acoustic models were then used to produce N-best lists from these lattices. Finally, more complex knowledge sources such as crossword acoustic models and trigram LMs were used to rescore N-best lists. <p> The acoustic models used for recognition are last year's adapted models described in <ref> [6] </ref>. From the first two rows, we can see that the new algorithm gives about a 1.8% relative improvement over the old algorithm when rescored with a trigram LM. However, this improvement did not carry over to a fourgram LM rescoring. <p> Trigram lattices were obtained from bigram lattices using 1800/700 as the forward-backward pruning thresholds and the backoff procedure described in Section 2. Recognition on male F0, F1, and FX conditions of the 1996 Hub4 development data with last year's adapted acoustic models <ref> [6] </ref> was performed on these trigram lattices through conventional expansion. Contrastive results are given in Table 4. The first row shows the results when using the old lattice generation algorithm to generate bigram lattices, generating N-best lists from these lattices, and rescor-ing the N-best hypotheses with a trigram LM.
Reference: 7. <author> A. Sankar, F. Weng, Z. Rivlin, A. Stolcke, and R. R. Gadde. </author> <title> The development of SRI's 1997 Broadcast News transcription system. In Proceedings DARPA Broadcast News Transcription and Understanding Workshop, </title> <address> Lansdowne, VA, </address> <year> 1998. </year>
Reference-contexts: Using 1600/1200 as the forward-backward pruning thresholds, the algorithm reduced lattice sizes by about 50%, as shown in Table 2. However, using an unadapted version of this year's acoustic model <ref> [7] </ref>, we observed no statistically significant difference in recognition accuracy between original and reduced lattices (Table 3). 4. Algorithms for Expansion to Trigram Lattices One of the main purposes of this work is to use trigram LMs at an early stage. <p> Further, Table 5 shows that the size of the trigram lattices from the compact expansion algorithm is only a about a sixth of those from the conventional expansion algorithm. Recognition experiments were carried out using the conventional and compact trigram lattices using this year's unadapted acoustic models <ref> [7] </ref>. Table 6 shows that there is no difference in performance between the conventional and compact trigram lattices. However, as we noted before, the compact lattices can be generated ten times faster than the conventional lattices, and are also six times smaller. <p> Recall the difference between these lattices is the forward-backward pruning thresholds used, with the result that the post evaluation lattices had a lower lattice error rate. Again we used this year's unadapted acoustic models <ref> [7] </ref>. Yet, as shown in Table 7, there was no significant change in performance with these differently pruned lattices. 6. Summary We have reported recent improvements of lattice-based search techniques in our Hub4 system.
Reference: 8. <author> R. Schwartz and S. Austin. </author> <title> A comparison of several approximate algorithms for finding multiple (N -BEST) sentence hypotheses. </title> <booktitle> In Proc. ICASSP, </booktitle> <volume> vol. 1, </volume> <pages> pp. 701704, </pages> <address> Toronto, </address> <year> 1991. </year>
Reference-contexts: To correct this problem, we implemented a new bigram lattice algorithm based on a word-dependent N-best decoding pass <ref> [8] </ref> that creates backtrace entries which are then processed to create a lattice of hypotheses. This results in true lattices that do not contain the backoff node, and have a finite number of paths or hypotheses. We implemented two algorithms to expand the bigram lattices to trigram lattices. <p> Our new bigram lattice generation algorithm is based on the word-dependent N-best algorithm <ref> [8] </ref>, and is similar to [4, 5]. The algorithm assumes that the starting time for a word depends on the preceding word but not on any word before that. Thus, a separate word hypothesis is propagated for each possible predecessor word.
Reference: 9. <author> N. Strom. </author> <title> Automatic Continuous Speech Recognition with Rapid Speaker Adaptation for Human/Machine Interaction. </title> <type> Ph.D. thesis, </type> <address> KTH, Stockholm, </address> <year> 1997. </year>
Reference-contexts: Similar research has been reported in the computer science literature [1], where standard algorithms for minimizing deterministic finite state automata are given. More recently, algorithms for minimization of weighted transducers were also developed <ref> [2, 9] </ref>. These approaches differ from ours in several aspects. First, we are dealing with word lattices, a dual representation of finite state automata, where nodes are states and transitions are labeled with words.
Reference: 10. <author> F. Weng, A. Stolcke, and A. Sankar. </author> <title> Hub4 language modeling using domain interpolation and data clustering. </title> <booktitle> In Proceedings DARPA Speech Recognition Workshop, </booktitle> <pages> pp. 147151, </pages> <address> Chantilly, VA, </address> <year> 1997. </year>
Reference-contexts: Table 1 gives the recognition results on F0, F1, and FX conditions for the male speakers of the 1996 Hub4 development set, using previous and new lattice generation algorithms to generate bigram lattices and rescoring them with last year's Hub4/SWBD/NABN 48K vocabulary trigram and fourgram LMs described in <ref> [10] </ref>. The acoustic models used for recognition are last year's adapted models described in [6]. From the first two rows, we can see that the new algorithm gives about a 1.8% relative improvement over the old algorithm when rescored with a trigram LM.
References-found: 10

