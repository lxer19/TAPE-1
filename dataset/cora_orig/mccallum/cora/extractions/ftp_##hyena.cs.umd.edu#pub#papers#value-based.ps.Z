URL: ftp://hyena.cs.umd.edu/pub/papers/value-based.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/compilers/compilers-pub-abs.html
Root-URL: 
Author: Reinhard v. Hanxleden Ken Kennedy Joel Saltz 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University  
Note: Center for Research on Parallel Computation  
Date: December, 1993  Revised February, 1994.  
Pubnum: CRPC-TR93365-S  
Abstract: Value-Based Distributions in Fortran D: A Preliminary Report 
Abstract-found: 1
Intro-found: 1
Reference: [BB87] <author> M. J. Berger and S. Bokhari. </author> <title> A partitioning strategy for non-uniform problems on multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):570-580, </volume> <year> 1987. </year>
Reference-contexts: They have developed a wealth a different mapping strategies, both value-based <ref> [BB87, HS91, DHU + 93] </ref>, and connectivity based [Sim91], which can be use as mapping strategies for irregular distributions. LPAR is a programming model for implementing numerical algorithms with a local structure, such as Particle-in-Cell or Multigrid, on distributed memory MIMD multiprocessors [BK93].
Reference: [BBO + 83] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus. CHARMM: </author> <title> A program for macromolecular energy, minimization and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4(2) </volume> <pages> 187-217, </pages> <year> 1983. </year>
Reference-contexts: Another example where data are not related by indices, but values, are molecular dynamics programs such as GROMOS [GB88], CHARMM <ref> [BBO + 83] </ref>, or ARGOS [SM90] that are used to simulate biomolecular systems. One important routine common to these programs is the nonbonded force (NBF) routine, which typically accounts for the bulk of the computational work 3 (around 90%). Figure 1 shows an abstracted version of a sequential NBF calculation.
Reference: [BCF + 93] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> A compilation approach for Fortran 90D/HPF compilers on distributed memory MIMD computers. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: A Fortran 90D compiler has been written at Syracuse fl This work is supported by an IBM fellowship, and by the National Aeronautics and Space Administration/the National Science Foundation under grant #ASC-9349459. y Corresponding author. E-mail: reinhard@rice.edu. Phone: (713) 527-8750x2740. Fax: (713) 285-5136. 1 University <ref> [BCF + 93] </ref>. For regular problems, i.e., applications with relatively simple array subscript functions and fixed communication requirements, these compilers have had considerable successes. However, irregular problems, such as molecular dynamics or unstructured mesh codes, have turned out to be significantly harder to parallelize [SH91].
Reference: [BGSZ92] <author> P. Brezany, M. Gerndt, V. Sipkova, and H. Zima. </author> <title> SUPERB support for irregular scientific computations. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <pages> pages 314-321, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: the run-time support may become critical for larger numbers of processors. 14 sizes and distribution strategies. 15 value-based balanced bisection. 16 5.5 Speedups an additional advantage. 6 Related Work The feasibility of automatically generating inspectors/executors for simple loop nests has been demonstrated by the Arf [WSBH91], Kali [KM91] and SUPERB <ref> [BGSZ92] </ref> compilers. Slicing analysis is a technique for generating inspectors and pre-inspectors for multiple levels of indirection, which is another typical characteristic of irregular problems [DSvH93]. Run time iteration graphs can assist in improving load balance and access locality when distributing loop iterations across processors [PSC93a].
Reference: [Bia91] <author> E. S. Biagioni. </author> <title> Scan Directed Load Balancing. </title> <type> PhD thesis, </type> <institution> University of North Carolina at Chapel Hill, </institution> <year> 1991. </year>
Reference-contexts: In case an application has good index-based locality, one might still want to distribute data irregularly to improve load balance, but some or all of the mapping computation can be done on the fly <ref> [Bia91, Han92, BK93, CHMS94] </ref>.) Representing a value-based distribution explicitly requires a large amount of state. A translation table maps global indices i glob into pairs (i loc ; p) of local indices and processor numbers.
Reference: [BK93] <author> S. B. Baden and S. R. Kohn. </author> <title> Portable parallel programming of numerical problems under the LPAR system. </title> <type> Technical Report CS93-330, </type> <institution> Department of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <year> 1993. </year>
Reference-contexts: In case an application has good index-based locality, one might still want to distribute data irregularly to improve load balance, but some or all of the mapping computation can be done on the fly <ref> [Bia91, Han92, BK93, CHMS94] </ref>.) Representing a value-based distribution explicitly requires a large amount of state. A translation table maps global indices i glob into pairs (i loc ; p) of local indices and processor numbers. <p> LPAR is a programming model for implementing numerical algorithms with a local structure, such as Particle-in-Cell or Multigrid, on distributed memory MIMD multiprocessors <ref> [BK93] </ref>. Given a data structure which already reflects the physical locality characteristics of the underlying problem, it distributes data and computation in a load balanced fashion. Value-based distributions were initially proposed as an enhancement of Fortran 77D [Han92].
Reference: [CCH + 88] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: From this annotated program, data-parallel compilers will generate codes in different native Fortran dialects for different parallel architectures. A prototype Fortran 77D compiler targeting MIMD distributed memory machines has been developed at Rice University [HKK + 91, Tse93] as part of the ParaS-cope programming environment <ref> [CCH + 88] </ref>. A Fortran 90D compiler has been written at Syracuse fl This work is supported by an IBM fellowship, and by the National Aeronautics and Space Administration/the National Science Foundation under grant #ASC-9349459. y Corresponding author. E-mail: reinhard@rice.edu. Phone: (713) 527-8750x2740.
Reference: [CHMS94] <author> T. W. Clark, R. v. Hanxleden, J. A. McCammon, and L. R. Scott. </author> <title> Parallelization using spatial decomposition for molecular dynamics. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93356-S. </note>
Reference-contexts: Therefore, after an inital parallelization based on data replication, many researchers choose to distribute the data in a way that considers the actual data dependences specific to their application <ref> [CHMS94] </ref>. These "irregular" distributions are typically harder to debug and manage than regular distributions and present an additional level of complexity beyond general message-passing style programming. <p> In case an application has good index-based locality, one might still want to distribute data irregularly to improve load balance, but some or all of the mapping computation can be done on the fly <ref> [Bia91, Han92, BK93, CHMS94] </ref>.) Representing a value-based distribution explicitly requires a large amount of state. A translation table maps global indices i glob into pairs (i loc ; p) of local indices and processor numbers.
Reference: [CM90] <author> T. W. Clark and J. A. McCammon. </author> <title> Parallelization of a molecular dynamics non-bonded force algorithm for MIMD architectures. </title> <journal> Computers & Chemistry, </journal> <volume> 14(3) </volume> <pages> 219-224, </pages> <year> 1990. </year>
Reference-contexts: Another alternative is to replicate the data and distribute just the computation itself and combine results at the end. This approach has the further advantage of simplicity and robustness, and for relatively small problem sizes and numbers of processors it may actually result in satisfactory performance <ref> [CM90] </ref>. However, one of the main advantages of distributed memory machines, scalability to large problem and machine sizes, has to be compromised under this approach.
Reference: [CMZ92] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Several research projects have aimed at providing a "machine independent parallel programming style" as a more user-friendly and investment-preserving alternative. Here the applications programmer uses a dialect of sequential Fortran and annotates it with high level distribution information. Examples include High Performance Fortran (HPF) [KLS + 94], Vienna Fortran <ref> [CMZ92] </ref>, and Fortran D [FHK + 90], which comes in two flavors, Fortran 77D and Fortran 90D. From this annotated program, data-parallel compilers will generate codes in different native Fortran dialects for different parallel architectures. <p> The potential speedup gained from distributing data and computation across processors is likely to be lost by exceedingly high communication costs when using simple distribution schemes. To allow more general access patterns, index based mapping arrays [WSBH91] or mapping functions <ref> [CMZ92] </ref> can be used. There, however, the programmer has to provide such arrays or functions that explicitly use indices. Another alternative is to replicate the data and distribute just the computation itself and combine results at the end.
Reference: [Dah90] <author> D. Dahl. </author> <title> Mapping and compiled communication on the connection machine system. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The Give-N-Take framework is a generalization of partial redundancy elimination techniques that can be used for analyzing data access requirements and for efficient communication placement in the presence of arbitrary control flow and reference patterns [HK93]. Examples of SIMD specific work are the Communication Compiler <ref> [Dah90] </ref> for the Connection Machine and the loop flattening transformation to allow efficient execution of loop nests with varying inner loop bounds [HK92]. 7 Summary and Conclusions Data-parallel languages, such as HPF or Fortran D, promise a user-friendly, efficient programming environment for parallel machines.
Reference: [DHU + 93] <author> R. Das, Y.-S. Hwang, M. Uysal, J. Saltz, and A. Sussman. </author> <title> Applying the CHAOS/PARTI library to irregular problems in computational chemistry and computational aerodynamics. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <institution> Mississippi State University, Starkville, MS, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: These "irregular" distributions are typically harder to debug and manage than regular distributions and present an additional level of complexity beyond general message-passing style programming. High level library routines, such as CHAOS <ref> [DMS + 92, DHU + 93] </ref>, can assist in tasks like global-to-local name space mappings, communication schedule generation, and schedule based communication. Such libraries are essential for keeping code complexity and programming difficulties in reasonable limits. <p> Therefore, not only storing but also accessing the information adds complexity to the program. As mentioned in Section 2, runtime libraries such as CHAOS can take most of the complexity of this task from the programmer <ref> [DHU + 93] </ref>, but their use still requires explicit managing of the data structures associated with irregular distributions and communications. 3.2 Storing Value-Based Distributed Data Again assuming that x is distributed according to its value, the number of elements of x assigned to each processor typically varies and is not known <p> They have developed a wealth a different mapping strategies, both value-based <ref> [BB87, HS91, DHU + 93] </ref>, and connectivity based [Sim91], which can be use as mapping strategies for irregular distributions. LPAR is a programming model for implementing numerical algorithms with a local structure, such as Particle-in-Cell or Multigrid, on distributed memory MIMD multiprocessors [BK93].
Reference: [DMS + 92] <author> R. Das, D. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives, </title> <booktitle> AIAA-92-0562. In Proceedings of the 30th Aerospace Sciences Meeting. AIAA, </booktitle> <month> January </month> <year> 1992. </year> <month> 18 </month>
Reference-contexts: These "irregular" distributions are typically harder to debug and manage than regular distributions and present an additional level of complexity beyond general message-passing style programming. High level library routines, such as CHAOS <ref> [DMS + 92, DHU + 93] </ref>, can assist in tasks like global-to-local name space mappings, communication schedule generation, and schedule based communication. Such libraries are essential for keeping code complexity and programming difficulties in reasonable limits.
Reference: [DSvH93] <author> R. Das, J. Saltz, and R. v. Hanxleden. </author> <title> Slicing analysis and indirect accesses to distributed ar rays. </title> <editor> In U. Banerjee et al., editor, </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> volume 769, </volume> <pages> pages 152-168. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <month> August </month> <year> 1993. </year> <booktitle> From the Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR. </address> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93319-S. </note>
Reference-contexts: Here we will focus on the compiler technology specific to handling such distributions and its relationship to other aspects of the compiler. For a discussion of other, equally important aspects of compiling irregular applications, such as communication analysis and preprocessing, we refer the reader to other publications <ref> [DSvH93, Han93] </ref>. The rest of this paper is organized as follows. Section 2 introduces value-based locality and illustrates the use of value-based distributions with a kernel taken from a Molecular Dynamics code. Section 3 lists the implications of value-based distributions for the resulting message-passing node program. <p> This schedule is used in the communication statements that gather coordinates (line 69) and add forces across processors (line 81). The inspector is relatively similar to irregular references to regularly distributed arrays, and we refer to other publications for a more detailed discussion <ref> [DSvH93, Han93] </ref>. One additional complication, however, is that a translation table has to be computed based on D:loc2glob (line 55). This implies the need for solving the reaching decompositions problem both intra- and inter-procedurally [HHKT92]. <p> However, the traces may still become too space consuming, in which case more space saving alternatives, such as a hash table combined with name-space translations on the fly, may be used <ref> [DSvH93] </ref>. 4.4 The Actual Computation After distributing data and prefetching off-processor references, the actual computation can be performed (lines 62 : : : 87). <p> Slicing analysis is a technique for generating inspectors and pre-inspectors for multiple levels of indirection, which is another typical characteristic of irregular problems <ref> [DSvH93] </ref>. Run time iteration graphs can assist in improving load balance and access locality when distributing loop iterations across processors [PSC93a].
Reference: [FHK + 90] <author> G. C. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D lan guage specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year> <note> Revised April, </note> <year> 1991. </year>
Reference-contexts: Here the applications programmer uses a dialect of sequential Fortran and annotates it with high level distribution information. Examples include High Performance Fortran (HPF) [KLS + 94], Vienna Fortran [CMZ92], and Fortran D <ref> [FHK + 90] </ref>, which comes in two flavors, Fortran 77D and Fortran 90D. From this annotated program, data-parallel compilers will generate codes in different native Fortran dialects for different parallel architectures.
Reference: [GB88] <author> W. F. van Gunsteren and H. J. C. Berendsen. GROMOS: </author> <title> GROningen MOlecular Simulation soft ware. </title> <type> Technical report, </type> <institution> Laboratory of Physical Chemistry, University of Groningen, </institution> <address> Nijenborgh, The Netherlands, </address> <year> 1988. </year>
Reference-contexts: Another example where data are not related by indices, but values, are molecular dynamics programs such as GROMOS <ref> [GB88] </ref>, CHARMM [BBO + 83], or ARGOS [SM90] that are used to simulate biomolecular systems. One important routine common to these programs is the nonbonded force (NBF) routine, which typically accounts for the bulk of the computational work 3 (around 90%).
Reference: [Han92] <author> R. v. Hanxleden. </author> <title> Compiler support for machine independent parallelization of irregular problems. </title> <type> Technical Report CRPC-TR92301-S, </type> <note> Center for Research on Parallel Computation, </note> <month> November </month> <year> 1992. </year> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR92301-S. </note>
Reference-contexts: In case an application has good index-based locality, one might still want to distribute data irregularly to improve load balance, but some or all of the mapping computation can be done on the fly <ref> [Bia91, Han92, BK93, CHMS94] </ref>.) Representing a value-based distribution explicitly requires a large amount of state. A translation table maps global indices i glob into pairs (i loc ; p) of local indices and processor numbers. <p> Given a data structure which already reflects the physical locality characteristics of the underlying problem, it distributes data and computation in a load balanced fashion. Value-based distributions were initially proposed as an enhancement of Fortran 77D <ref> [Han92] </ref>. A variant of it, based on a GeoCoL (Geometrical, Connectivity and/or Load) data structure, has since then been implemented in a Fortran 90D prototype compiler by Ponnusamy et al. [PSC93a, PSC + 93b].
Reference: [Han93] <author> R. v. Hanxleden. </author> <title> Handling irregular problems with Fortran D | A preliminary report. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Nether-lands, </address> <month> December </month> <year> 1993. </year> <note> D Newsletter #9, available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93339-S. </note>
Reference-contexts: Here we will focus on the compiler technology specific to handling such distributions and its relationship to other aspects of the compiler. For a discussion of other, equally important aspects of compiling irregular applications, such as communication analysis and preprocessing, we refer the reader to other publications <ref> [DSvH93, Han93] </ref>. The rest of this paper is organized as follows. Section 2 introduces value-based locality and illustrates the use of value-based distributions with a kernel taken from a Molecular Dynamics code. Section 3 lists the implications of value-based distributions for the resulting message-passing node program. <p> Note that the same problem occurs when regularly distributed arrays are accessed irregularly and we wish to append buffer space for off-processor data at the end of the array <ref> [Han93] </ref>. 3.3 Mapping Name Spaces Mapping between the global name space of a Fortran D program and the local name space of the node program is an important component of the parallelization process. <p> This schedule is used in the communication statements that gather coordinates (line 69) and add forces across processors (line 81). The inspector is relatively similar to irregular references to regularly distributed arrays, and we refer to other publications for a more detailed discussion <ref> [DSvH93, Han93] </ref>. One additional complication, however, is that a translation table has to be computed based on D:loc2glob (line 55). This implies the need for solving the reaching decompositions problem both intra- and inter-procedurally [HHKT92].
Reference: [HHKT92] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: One additional complication, however, is that a translation table has to be computed based on D:loc2glob (line 55). This implies the need for solving the reaching decompositions problem both intra- and inter-procedurally <ref> [HHKT92] </ref>. Note that the current strategy for collecting off-processor references is to record each individual subscript in a trace array. This trace array is created in global name space (j$glob, see lines 47 : : : 54) and then converted into local name space (j$loc, line 56).
Reference: [HK92] <author> R. v. Hanxleden and K. Kennedy. </author> <title> Relaxing SIMD control flow constraints using loop transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <pages> pages 188-199, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year> <note> ACM Press. Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR92207-S. </note>
Reference-contexts: Examples of SIMD specific work are the Communication Compiler [Dah90] for the Connection Machine and the loop flattening transformation to allow efficient execution of loop nests with varying inner loop bounds <ref> [HK92] </ref>. 7 Summary and Conclusions Data-parallel languages, such as HPF or Fortran D, promise a user-friendly, efficient programming environment for parallel machines. However, one of their weaknesses so far has been the lack of support for irregular problems.
Reference: [HK93] <author> R. v. Hanxleden and K. Kennedy. </author> <title> A code placement framework and its application to communication generation. </title> <type> Technical Report CRPC-TR93337-S, </type> <note> Center for Research on Parallel Computation, October 1993. Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93337-S. </note>
Reference-contexts: The Give-N-Take framework is a generalization of partial redundancy elimination techniques that can be used for analyzing data access requirements and for efficient communication placement in the presence of arbitrary control flow and reference patterns <ref> [HK93] </ref>.
Reference: [HKK + 91] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: From this annotated program, data-parallel compilers will generate codes in different native Fortran dialects for different parallel architectures. A prototype Fortran 77D compiler targeting MIMD distributed memory machines has been developed at Rice University <ref> [HKK + 91, Tse93] </ref> as part of the ParaS-cope programming environment [CCH + 88]. A Fortran 90D compiler has been written at Syracuse fl This work is supported by an IBM fellowship, and by the National Aeronautics and Space Administration/the National Science Foundation under grant #ASC-9349459. y Corresponding author.
Reference: [HS91] <author> R. v. Hanxleden and L. R. Scott. </author> <title> Load balancing on message passing architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 312-324, </pages> <year> 1991. </year>
Reference-contexts: They have developed a wealth a different mapping strategies, both value-based <ref> [BB87, HS91, DHU + 93] </ref>, and connectivity based [Sim91], which can be use as mapping strategies for irregular distributions. LPAR is a programming model for implementing numerical algorithms with a local structure, such as Particle-in-Cell or Multigrid, on distributed memory MIMD multiprocessors [BK93].
Reference: [KLS + 94] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Several research projects have aimed at providing a "machine independent parallel programming style" as a more user-friendly and investment-preserving alternative. Here the applications programmer uses a dialect of sequential Fortran and annotates it with high level distribution information. Examples include High Performance Fortran (HPF) <ref> [KLS + 94] </ref>, Vienna Fortran [CMZ92], and Fortran D [FHK + 90], which comes in two flavors, Fortran 77D and Fortran 90D. From this annotated program, data-parallel compilers will generate codes in different native Fortran dialects for different parallel architectures. <p> These extensions could also be applied directly to the HPF standard <ref> [KLS + 94] </ref>. We were able to limit ourselves to a simple extension of the already existing DISTRIBUTE directive, as was illustrated by the code in Figure 2.
Reference: [KM91] <author> C. Koelbel and P. Mehrotra. </author> <title> Programming data parallel algorithms on distributed memory machines using Kali. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: the scalability of the run-time support may become critical for larger numbers of processors. 14 sizes and distribution strategies. 15 value-based balanced bisection. 16 5.5 Speedups an additional advantage. 6 Related Work The feasibility of automatically generating inspectors/executors for simple loop nests has been demonstrated by the Arf [WSBH91], Kali <ref> [KM91] </ref> and SUPERB [BGSZ92] compilers. Slicing analysis is a technique for generating inspectors and pre-inspectors for multiple levels of indirection, which is another typical characteristic of irregular problems [DSvH93]. Run time iteration graphs can assist in improving load balance and access locality when distributing loop iterations across processors [PSC93a].
Reference: [KMCKC93] <author> U. Kremer, J. Mellor-Crummey, K. Kennedy, and A. Carle. </author> <title> Automatic data layout for distributed memory machines in the D programming environment. </title> <editor> In Christoph W. Kessler, editor, </editor> <title> Automatic Parallelization | New Approaches to Code Generation, Data Distribution, </title> <booktitle> and Performance Prediction, </booktitle> <pages> pages 136-152. </pages> <booktitle> Vieweg Advanced Studies in Computer Science, </booktitle> <publisher> Verlag Vieweg, Wiesbaden, </publisher> <address> Germany, </address> <year> 1993. </year> <note> Also available as technical report CRPC-TR93-298-S, </note> <institution> Rice University. </institution>
Reference-contexts: We will refer to this characteristic as index-based locality. Exactly how arrays should be distributed and aligned in the presence of index-based locality is by no means trivial and still an active field of research <ref> [KMCKC93] </ref>. However, one can generally assume that only regular distributions (like BLOCK, CYCLIC, or BLOCK-CYCLIC) and redistributions have to be considered. For irregular problems, this assumption cannot be made.
Reference: [KMV90] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machines. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: First, an "inspection phase" determines what data have to be communicated within a certain loop and generates a communication schedule. Then, an "execution phase" (repeatedly) performs the actual computation and uses the communication schedule to exchange data <ref> [MSS + 88, KMV90] </ref>. 3.5 A Bootstrapping Problem One characteristic of value-based distributions is that they may pose a certain bootstrapping problem to both the user and the compiler.
Reference: [MSS + 88] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol, and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: First, an "inspection phase" determines what data have to be communicated within a certain loop and generates a communication schedule. Then, an "execution phase" (repeatedly) performs the actual computation and uses the communication schedule to exchange data <ref> [MSS + 88, KMV90] </ref>. 3.5 A Bootstrapping Problem One characteristic of value-based distributions is that they may pose a certain bootstrapping problem to both the user and the compiler.
Reference: [PSC93a] <author> R. Ponnusamy, J. Saltz, and A. Choudhary. </author> <title> Runtime compilation techniques for data partitioning and communication schedule reuse. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 361-370. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year> <note> Technical Report CS-TR-3055 and UMIACS-TR-93-32, University of Maryland, April `93. Available via anonymous ftp from hyena.cs.umd.edu. 19 </note>
Reference-contexts: Slicing analysis is a technique for generating inspectors and pre-inspectors for multiple levels of indirection, which is another typical characteristic of irregular problems [DSvH93]. Run time iteration graphs can assist in improving load balance and access locality when distributing loop iterations across processors <ref> [PSC93a] </ref>. Even though the classic data-parallel paradigms as defined by Fortran D-like languages did not provide any means for directly exploiting value-based locality, application programmers have been taking advantage of this kind of locality when hand-coding in low-level parallel languages (for example with message passing). <p> Value-based distributions were initially proposed as an enhancement of Fortran 77D [Han92]. A variant of it, based on a GeoCoL (Geometrical, Connectivity and/or Load) data structure, has since then been implemented in a Fortran 90D prototype compiler by Ponnusamy et al. <ref> [PSC93a, PSC + 93b] </ref>. The Give-N-Take framework is a generalization of partial redundancy elimination techniques that can be used for analyzing data access requirements and for efficient communication placement in the presence of arbitrary control flow and reference patterns [HK93].
Reference: [PSC + 93b] <author> R. Ponnusamy, J. Saltz, A. Choudhary, Y.-S. Hwang, and G. Fox. </author> <title> Runtime support and compilation methods for user-specified data distributions. </title> <institution> Technical Report CS-TR-3194 and UMIACS-TR-93-135, University of Maryland, </institution> <month> November </month> <year> 1993. </year> <note> Available via anonymous ftp from hyena.cs.umd.edu. </note>
Reference-contexts: Value-based distributions were initially proposed as an enhancement of Fortran 77D [Han92]. A variant of it, based on a GeoCoL (Geometrical, Connectivity and/or Load) data structure, has since then been implemented in a Fortran 90D prototype compiler by Ponnusamy et al. <ref> [PSC93a, PSC + 93b] </ref>. The Give-N-Take framework is a generalization of partial redundancy elimination techniques that can be used for analyzing data access requirements and for efficient communication placement in the presence of arbitrary control flow and reference patterns [HK93].
Reference: [SH91] <author> J. P. Singh and J. L. Hennessy. </author> <title> An empirical investigation of the effectiveness and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Fax: (713) 285-5136. 1 University [BCF + 93]. For regular problems, i.e., applications with relatively simple array subscript functions and fixed communication requirements, these compilers have had considerable successes. However, irregular problems, such as molecular dynamics or unstructured mesh codes, have turned out to be significantly harder to parallelize <ref> [SH91] </ref>. One principal problem with irregular problems is the general lack of access locality when distributing data and computation navely. For example, assume that the value assigned to some array element a (i) depends on some other array element b (j).
Reference: [Sim91] <author> H. Simon. </author> <title> Partitioning of unstructured mesh problems for parallel processing. </title> <booktitle> In Proceedings of the Conference on Parallel Methods on Large Scale Structural Analysis and Physics Applications. </booktitle> <publisher> Pergamon Press, </publisher> <year> 1991. </year>
Reference-contexts: They have developed a wealth a different mapping strategies, both value-based [BB87, HS91, DHU + 93], and connectivity based <ref> [Sim91] </ref>, which can be use as mapping strategies for irregular distributions. LPAR is a programming model for implementing numerical algorithms with a local structure, such as Particle-in-Cell or Multigrid, on distributed memory MIMD multiprocessors [BK93].
Reference: [SM90] <author> T. P. Straatsma and J. Andrew McCammon. ARGOS, </author> <title> a vectorized general molecular dynamics pro gram. </title> <journal> Journal of Computational Chemistry, </journal> <volume> II(8):943-951, </volume> <year> 1990. </year>
Reference-contexts: Another example where data are not related by indices, but values, are molecular dynamics programs such as GROMOS [GB88], CHARMM [BBO + 83], or ARGOS <ref> [SM90] </ref> that are used to simulate biomolecular systems. One important routine common to these programs is the nonbonded force (NBF) routine, which typically accounts for the bulk of the computational work 3 (around 90%). Figure 1 shows an abstracted version of a sequential NBF calculation.
Reference: [Tse93] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: From this annotated program, data-parallel compilers will generate codes in different native Fortran dialects for different parallel architectures. A prototype Fortran 77D compiler targeting MIMD distributed memory machines has been developed at Rice University <ref> [HKK + 91, Tse93] </ref> as part of the ParaS-cope programming environment [CCH + 88]. A Fortran 90D compiler has been written at Syracuse fl This work is supported by an IBM fellowship, and by the National Aeronautics and Space Administration/the National Science Foundation under grant #ASC-9349459. y Corresponding author.
Reference: [WCSM93] <author> Y.-T. Wong, T. W. Clark, J. Shen, and J. A. McCammon. </author> <title> Molecular dynamics simulation of substrate enzyme interactions in the active site channel of superoxide dismutase. </title> <journal> Journal of Molecular Simulation, </journal> <volume> 10(2-6):277-289, </volume> <year> 1993. </year>
Reference-contexts: SOD (superoxide dismutase) is a catalytic enzyme that converts the toxic free-radical, O 4 2 , a byproduct of aerobic respiration, to the neutral molecules O 2 and H 2 O 2 <ref> [WCSM93] </ref>. Our solvated system contained 6968 atoms. We ran the simulation for 30 time steps, on 1, 2, 4, 8, 16, and 32 processors, with the following distribution strategies. 1.
Reference: [WSBH91] <author> J. Wu, J. Saltz, H. Berryman, and S. Hiranandani. </author> <title> Distributed memory compiler design for sparse prob lems. </title> <type> ICASE Report 91-13, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> January </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: The potential speedup gained from distributing data and computation across processors is likely to be lost by exceedingly high communication costs when using simple distribution schemes. To allow more general access patterns, index based mapping arrays <ref> [WSBH91] </ref> or mapping functions [CMZ92] can be used. There, however, the programmer has to provide such arrays or functions that explicitly use indices. Another alternative is to replicate the data and distribute just the computation itself and combine results at the end. <p> A translation table maps global indices i glob into pairs (i loc ; p) of local indices and processor numbers. Often the translation table itself is too large to be fully replicated and is also distributed <ref> [WSBH91] </ref>. Therefore, not only storing but also accessing the information adds complexity to the program. <p> apparent that the scalability of the run-time support may become critical for larger numbers of processors. 14 sizes and distribution strategies. 15 value-based balanced bisection. 16 5.5 Speedups an additional advantage. 6 Related Work The feasibility of automatically generating inspectors/executors for simple loop nests has been demonstrated by the Arf <ref> [WSBH91] </ref>, Kali [KM91] and SUPERB [BGSZ92] compilers. Slicing analysis is a technique for generating inspectors and pre-inspectors for multiple levels of indirection, which is another typical characteristic of irregular problems [DSvH93].
References-found: 36

