URL: http://www.cs.colostate.edu/~ftppub/TechReports/1993/tr-102.ps.Z
Refering-URL: http://www.cs.colostate.edu/~ftppub/
Root-URL: 
Affiliation: Department of Computer Science Colorado State University  
Abstract: The VISA User's Guide Matthew Haines and Wim Bohm Technical Report CS-93-102 March 10, 1993 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Though most parallel programming languages do not provide support for data decompositions [10], there has been some recent effort to remedy this neglect. FortranD [6] and other Fortran extensions <ref> [1, 15] </ref> augment Fortran with statements that allow the programmer to specify a limited number of decompositions, and the compiler then uses this information to generate a data-parallel program with implicit message passing for sending and receiving remote values.
Reference: [2] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Shared memory for distributed memory multiprocessors. </title> <institution> Technical Report Rice COMP TR89-91, Rice University, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: Another approach to simplifying the problem of programming distributed memory machines is to utilize underlying support for a single addressing space. This is most commonly provided by either the operating system (often termed distributed shared memory) or the hardware. These DSM systems <ref> [2, 9, 11] </ref> provide a single addressing space to the compiler so that the programmer can code in a shared-memory fashion, leaving the details of data decomposition and message passing to the operating system.
Reference: [3] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Thus this approach is not language-independent. Other parallel languages either provide similar support for decompositions that are used by the compiler to generate data parallel programs [7, 8, 12], and thus suffer the same problems as the FortranD approach, or lack the ability to control decomposition altogether <ref> [3] </ref>. Another approach to simplifying the problem of programming distributed memory machines is to utilize underlying support for a single addressing space. This is most commonly provided by either the operating system (often termed distributed shared memory) or the hardware.
Reference: [4] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Of the assumptions made, the ability to determine access patterns is by far the most idealistic. This is reflected in the current alternative methods being used for distributing data structures: * The compiler controls the distribution of data structures. This is the approach taken by the parallelizing compiler camp <ref> [4, 15] </ref>. The basic idea is to distribute the data structures 6 according to some distribution function, and then to analyze the array subscripts to deter-mine whether or not, for a particular thread, a given reference is local or remote.
Reference: [5] <author> Matthew Haines and Wim Bohm. </author> <title> On the design of distributed memory sisal. </title> <type> Technical Report CS-92-144, </type> <institution> Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: We implemented this fixed addressing scheme as a VISA runtime option, and found that although translation proceeds at a faster rate, the fixed control parameters often causes mis-alignment with the parallel loops accessing the data structures, resulting in excessive remote references and severely degraded performance <ref> [5] </ref>. Therefore we opt for the ability to avoid latency by providing flexible data distributions that can easily be matched to the access patterns of the parallel loops.
Reference: [6] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Though most parallel programming languages do not provide support for data decompositions [10], there has been some recent effort to remedy this neglect. FortranD <ref> [6] </ref> and other Fortran extensions [1, 15] augment Fortran with statements that allow the programmer to specify a limited number of decompositions, and the compiler then uses this information to generate a data-parallel program with implicit message passing for sending and receiving remote values. <p> Since the programmer may have a better idea as to how the data will be accessed <ref> [6] </ref>, most compilers that perform the data distribution for the programmer will accept these "hints" so that the proper data distribution function can be selected. * The compiler controls the distribution with the help of run-time profiles [13].
Reference: [7] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-Space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Computing, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Thus this approach is not language-independent. Other parallel languages either provide similar support for decompositions that are used by the compiler to generate data parallel programs <ref> [7, 8, 12] </ref>, and thus suffer the same problems as the FortranD approach, or lack the ability to control decomposition altogether [3]. Another approach to simplifying the problem of programming distributed memory machines is to utilize underlying support for a single addressing space.
Reference: [8] <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: Thus this approach is not language-independent. Other parallel languages either provide similar support for decompositions that are used by the compiler to generate data parallel programs <ref> [7, 8, 12] </ref>, and thus suffer the same problems as the FortranD approach, or lack the ability to control decomposition altogether [3]. Another approach to simplifying the problem of programming distributed memory machines is to utilize underlying support for a single addressing space.
Reference: [9] <author> Kai Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> September </month> <year> 1986. </year>
Reference-contexts: Another approach to simplifying the problem of programming distributed memory machines is to utilize underlying support for a single addressing space. This is most commonly provided by either the operating system (often termed distributed shared memory) or the hardware. These DSM systems <ref> [2, 9, 11] </ref> provide a single addressing space to the compiler so that the programmer can code in a shared-memory fashion, leaving the details of data decomposition and message passing to the operating system.
Reference: [10] <author> Cherri M. Pancake and Donna Bergmark. </author> <title> Do parallel languages respond to the needs of scientific programmers. </title> <journal> IEEE Computer, </journal> <volume> 23(12) </volume> <pages> 13-24, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Clearly this is a difficult task for even the most basic application, and, in an ideal world, should not be left to the programmer to solve. Though most parallel programming languages do not provide support for data decompositions <ref> [10] </ref>, there has been some recent effort to remedy this neglect.
Reference: [11] <author> Umakishore Ramachandran, Mustaque Ahamad, and M. Yousef A. Khalidi. </author> <title> Unifying synchronization and data transfer in maintaining coherence of distributed shared memory. </title> <type> Technical Report GIT-CS-88/23, </type> <institution> Georgia Institute of Technology, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: Another approach to simplifying the problem of programming distributed memory machines is to utilize underlying support for a single addressing space. This is most commonly provided by either the operating system (often termed distributed shared memory) or the hardware. These DSM systems <ref> [2, 9, 11] </ref> provide a single addressing space to the compiler so that the programmer can code in a shared-memory fashion, leaving the details of data decomposition and message passing to the operating system.
Reference: [12] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In ACM SIGPLAN, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Thus this approach is not language-independent. Other parallel languages either provide similar support for decompositions that are used by the compiler to generate data parallel programs <ref> [7, 8, 12] </ref>, and thus suffer the same problems as the FortranD approach, or lack the ability to control decomposition altogether [3]. Another approach to simplifying the problem of programming distributed memory machines is to utilize underlying support for a single addressing space.
Reference: [13] <author> Vivek Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> The MIT Press, </publisher> <year> 1989. </year> <note> Research Monographs in Parallel and Distributed Computing. </note>
Reference-contexts: may have a better idea as to how the data will be accessed [6], most compilers that perform the data distribution for the programmer will accept these "hints" so that the proper data distribution function can be selected. * The compiler controls the distribution with the help of run-time profiles <ref> [13] </ref>. Again, this approach attempts to help the automated distribution process, but rather than have the programmer tell the compiler how the data will be accessed, the compiler simply "watches" several characteristic runs and notes the distribution patterns used for those runs.
Reference: [14] <author> Zhiyu Shen, Zhiyuan Li, and Pen-Chung Yew. </author> <title> An emperical study of Fortran programs for paralleliz-ing compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 356-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Symbolic subscript terms with unknown values, coupled subscripts, and nonzero or nonunity coefficients of loop indices often make dependence analysis impossible for even the most sophisticated parallelizing compilers <ref> [14] </ref>. Second, this technique requires that data structure sizes and the number of processors to be used be known at compile time, which restricts the ability to run the application using varying parameters without re-compilation each time.
Reference: [15] <author> H. Zima, H. Bast, and M. Gerndt. </author> <title> Superb: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1986. </year> <month> 18 </month>
Reference-contexts: Though most parallel programming languages do not provide support for data decompositions [10], there has been some recent effort to remedy this neglect. FortranD [6] and other Fortran extensions <ref> [1, 15] </ref> augment Fortran with statements that allow the programmer to specify a limited number of decompositions, and the compiler then uses this information to generate a data-parallel program with implicit message passing for sending and receiving remote values. <p> Of the assumptions made, the ability to determine access patterns is by far the most idealistic. This is reflected in the current alternative methods being used for distributing data structures: * The compiler controls the distribution of data structures. This is the approach taken by the parallelizing compiler camp <ref> [4, 15] </ref>. The basic idea is to distribute the data structures 6 according to some distribution function, and then to analyze the array subscripts to deter-mine whether or not, for a particular thread, a given reference is local or remote.
References-found: 15

