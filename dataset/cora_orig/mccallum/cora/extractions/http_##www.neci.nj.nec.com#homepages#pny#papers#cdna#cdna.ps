URL: http://www.neci.nj.nec.com/homepages/pny/papers/cdna/cdna.ps
Refering-URL: http://www.neci.nj.nec.com/homepages/pny/papers/cdna/main.html
Root-URL: 
Email: Email: loewenst@paul.rutgers.edu  Email: pny@research.nj.nec.com  
Phone: Phone: 201-761-5949 Fax: 609-951-2483  
Title: Significantly Lower Entropy Estimates for Natural DNA Sequences  
Author: David Loewenstern* and Peter N. Yianilos and 
Date: Submitted: November 10, 1996 Revised: September 24, 1997  
Address: New Brunswick, New Jersey 08903  4 Independence Way, Princeton, NJ 08540  Princeton, New Jersey 08544  
Affiliation: Department of Computer Science, Rutgers University,  NEC Research Institute,  Department of Computer Science Princeton University,  
Abstract-found: 0
Intro-found: 1
Reference: [Baum and Eagon1967] <author> L. E. Baum and J. E. Eagon. </author> <title> An inequality with application to statistical estimation for probabalistic functions of a Markov process and to models for ecology. </title> <journal> Bull. AMS, </journal> <volume> 73 </volume> <pages> 360-363, </pages> <year> 1967. </year>
Reference-contexts: The learning task before us is to estimate these parameters by examining the training set. Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models <ref> [Baum and Eagon1967, Baum et al.1970, Baum1972, Poritz1988] </ref>, and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery [Dempster et al.1977, Redner and Walker1984] of essentially the same algorithm and underlying information theoretic inequality.
Reference: [Baum et al.1970] <author> L. E. Baum, T. Petrie, G. Soules, and N. Weiss. </author> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> Ann. Math Stat., </journal> <volume> 35 41 </volume> <pages> 164-171, </pages> <year> 1970. </year>
Reference-contexts: The learning task before us is to estimate these parameters by examining the training set. Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models <ref> [Baum and Eagon1967, Baum et al.1970, Baum1972, Poritz1988] </ref>, and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery [Dempster et al.1977, Redner and Walker1984] of essentially the same algorithm and underlying information theoretic inequality.
Reference: [Baum1972] <author> L. E. Baum. </author> <title> An inequality and associated maximization technique in statistical esti-matation of probabilistic functions of Markov processes. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference-contexts: The learning task before us is to estimate these parameters by examining the training set. Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models <ref> [Baum and Eagon1967, Baum et al.1970, Baum1972, Poritz1988] </ref>, and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery [Dempster et al.1977, Redner and Walker1984] of essentially the same algorithm and underlying information theoretic inequality.
Reference: [Bell et al.1990] <author> Timothy C. Bell, John G. Cleary, and Ian H. Witten. </author> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: Assuming each character (nucleotide) is drawn uniformly at random from the alphabet, and that all positions in the string are independent, we know from elementary information theory <ref> [Cover and Thomas1991, Bell et al.1990] </ref> that an optimal code will devote 2 bits to representing each character. This is the maximum entropy case. <p> result is then even more surprising since it implies that knowledge of the immediate past reduces the entropy estimate by a mere 0:05 bits. 1 Data compression techniques such as Lempel-Ziv (LZ) coding (See their original paper [Ziv and Lempel1977] and the discussion of the considerable work that followed in <ref> [Bell et al.1990] </ref>) may be viewed as entropy estimators, with LZ corresponding to a model that predicts based on a historical context of variable length. It "compresses" humretblas to 2:14 bits per character 2 , which is actually worse than the flat random model we started with. <p> However as context length increases, it becomes increasingly unlikely that a given context has ever been seen. This problem has led to the development of 3 In practice some flattening rule such as that of Laplace is used to avoid zero probabilities. 3 variable length context language models <ref> [Bell et al.1990] </ref> which simply stated, use long contexts when enough earlier observations exists, and otherwise use shorter ones. <p> In [Mantegna et al.1993] conventional fixed multigram entropy estimates are reported for several sequences. Grumbach and Tahi in [Grumbach and Tahi1994] exploit reverse-complements and long matches to achieve improved compression rates. The method is one of pointer substitution and resembles LZ78 (see <ref> [Bell et al.1990] </ref>). The results of this paper substantially improve on Grumbach and Tahi's results for many sequences. <p> For example, Shannon's work estimated the entropy of English text to be roughly 1:3 bits per character (see discussion in <ref> [Bell et al.1990] </ref>). Random text over a 27 letter alphabet (A-Z and space) corresponds to 4:8 bits. Today's statistical models, with no specific knowledge of English syntax or semantics, can achieve roughly 2 bits.
Reference: [Cardon and Stormo1992] <author> Lon Cardon and Gary Stormo. </author> <title> Expectation maximization algorithm for identifying protein-binding sites with variable lengths from unaligned DNA fragments. </title> <journal> JMB, </journal> <volume> 223 </volume> <pages> 159-170, </pages> <year> 1992. </year>
Reference: [Cosmi et al.1990] <author> Carmelina Cosmi, Vincenzo Cuomo, Maria Ragosta, and Maria Macchiato. </author> <title> Characterization of nucleotidic sequences using maximum entropy techniques. </title> <journal> J. Theor. Biol, </journal> (147):423-432, 1990. 
Reference: [Cover and Thomas1991] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: Assuming each character (nucleotide) is drawn uniformly at random from the alphabet, and that all positions in the string are independent, we know from elementary information theory <ref> [Cover and Thomas1991, Bell et al.1990] </ref> that an optimal code will devote 2 bits to representing each character. This is the maximum entropy case. <p> The entropy rate of a stochastic process fX t g is defined as: lim 1 H (X 1 ; X 2 ; : : : ; X t ) (1) where this limit need not in general exist, and H denotes the information theoretic entropy function (see <ref> [Cover and Thomas1991] </ref>.) Given full knowledge of the process, and the limit's existence, the entropy rate is a well-defined attribute of the process. But we know very little of the process underlying the generation of natural DNA, and can merely observe the outcome, i.e. the nucleotide sequence.
Reference: [Dempster et al.1977] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Ser. B (methodological), </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models [Baum and Eagon1967, Baum et al.1970, Baum1972, Poritz1988], and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery <ref> [Dempster et al.1977, Redner and Walker1984] </ref> of essentially the same algorithm and underlying information theoretic inequality. The Baum-Welch algorithm is an optimization technique which locally maximizes the probability of generating the training set from the model by adjusting parameters.
Reference: [Dujon et al.1994] <author> B. Dujon, D. Alexandraki, B. Andre, W. Ansorge, and V. Baladron et al. </author> <title> Complete DNA sequence of yeast chromosome XI. </title> <journal> Nature, </journal> <volume> 369, </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: The observed relationship between %(C + G) and gene density <ref> [Dujon et al.1994] </ref> in yeast is reflected in the discrepancy in H (1) between the coding and non-coding region of Yeast chromosome III. Observe that the H (6) estimate is rarely better than H (4), and in some cases is markedly worse (a consequence of limited sequence length).
Reference: [Farach et al.1995] <author> Martin Farach, Michiel Noordewier, Serap Savari, Larry Shepp, Abraham Wyner, and Jacob Ziv. </author> <title> On the entropy of DNA: Algorithms and measurements based on memory and rapid convergence. </title> <booktitle> In Proceedings of the Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1995. </year> <month> 36 </month>
Reference-contexts: In fact, it has been observed by several authors that coding regions are less compressible than non-coding regions (e.g., <ref> [Mantegna et al.1993, Salamon and Konopka1992, Farach et al.1995] </ref>). So it is clear that two sequences that code for the same polypeptide may nevertheless have large Hamming distance.
Reference: [Gatlin1972] <author> Lila L. Gatlin. </author> <title> Information Theory and the Living System. </title> <publisher> Columbia University Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: It is entirely possible that very different results will be obtained, particularly for coding regions, when much more DNA is available for analysis. It should be noted that H (1) entropy estimates include the known effect of %(C+G) <ref> [Gatlin1972] </ref> on entropy estimation, and biocompress-2 includes the also known effect of long exact repeats and exact complement repeats [Herzel et al.1994]. cdna's generally superior performance indicates that DNA possesses more structure which may be exploited.
Reference: [Grumbach and Tahi1994] <author> Stephane Grumbach and Fariza Tahi. </author> <title> A new challenge for compression algorithms: genetic sequences. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 30(6) </volume> <pages> 875-886, </pages> <year> 1994. </year>
Reference-contexts: The most effective metric looks not only for matches in the forward direction, but also for matches that are reversed and complemented. By "complemented" we mean that A; G are interchanged with T; C. This technique was introduced by Grumbach and Tahi <ref> [Grumbach and Tahi1994] </ref> and in almost all cases slightly 4 improves the rate of compression. We also describe a two-stage model for coding regions in which an amino acid is first predicted based on earlier amino acids, followed by the prediction of a particular codon. <p> In recent years there has been increasing interest in entropy estimates for DNA. In [Mantegna et al.1993] conventional fixed multigram entropy estimates are reported for several sequences. Grumbach and Tahi in <ref> [Grumbach and Tahi1994] </ref> exploit reverse-complements and long matches to achieve improved compression rates. The method is one of pointer substitution and resembles LZ78 (see [Bell et al.1990]). The results of this paper substantially improve on Grumbach and Tahi's results for many sequences. <p> The resulting sequence contains 484; 483 bases and is referred to as our non-redundant data set. 19 5 Experimental Results Our model's performance on the sequences described in section 4 is summarized in table 2. In some cases our results may be compared directly with estimates from <ref> [Grumbach and Tahi1994] </ref>, which are included in the table. Our values for H (4) (the 4-symbol entropy) may be compared with the redundancy estimates of [Mantegna et al.1993] and are in agreement. We have grouped our results by general type (i.e. mammalian, prokaryote, etc.). <p> We have grouped our results by general type (i.e. mammalian, prokaryote, etc.). The H (1); H (4); H (6) columns contain conventional multigram entropy estimates. The cdna column reports our model's cross-validation entropy estimates. Compressive estimates from the biocompress-2 program of <ref> [Grumbach and Tahi1994] </ref> are contained in the following column. Our model's compressive estimates are given in the table's final column, cdna-compress. The compressive estimates are generated by partitioning the sequence into 20 equal segments: s 1 ; s 2 ; :::s 20 .
Reference: [Herzel et al.1994] <author> Hanspeter Herzel, Werner Ebeling, and Armin Schmitt. </author> <title> Entropies of biose-quences: The role of repeats. </title> <journal> Physical Review E, </journal> <volume> 50(6) </volume> <pages> 5061-5071, </pages> <year> 1994. </year>
Reference-contexts: It should be noted that H (1) entropy estimates include the known effect of %(C+G) [Gatlin1972] on entropy estimation, and biocompress-2 includes the also known effect of long exact repeats and exact complement repeats <ref> [Herzel et al.1994] </ref>. cdna's generally superior performance indicates that DNA possesses more structure which may be exploited. Several notions of distance were evaluated and the best performance resulted from considering both Hamming distance to reversed and complemented targets, as well as standard Hamming distance, then selecting the minimum.
Reference: [Herzel1988] <author> H. Herzel. </author> <title> Complexity of symbol sequences. </title> <journal> Syst. Anal. Modl. Simul., </journal> <volume> 5(5) </volume> <pages> 435-444, </pages> <year> 1988. </year>
Reference-contexts: That is, small local distortions do little to alter the overall message. This is discussed further in Section 6. 10 3 Algorithms In this section we further motivate our model and then describe it in formal terms. That natural DNA includes near repeats is well known, and in <ref> [Herzel1988] </ref> the statistics of their occurrence are discussed. We measure nearness using ordinary Hamming distance, i.e. the number of positions at which two equal length strings disagree.
Reference: [Krogh et al.1994] <author> A. Krogh, I. Mian, and D. Haussler. </author> <title> A hidden Markov model that finds genes in Escheria Coli DNA. </title> <journal> Nucleic Acids Research supplement, </journal> <volume> 22 </volume> <pages> 4768-4778, </pages> <year> 1994. </year>
Reference: [Lauc et al.1992] <author> Gordan Lauc, Igor Ilic, and Harija Heffer-Lauc. </author> <title> Entropies of coding and noncod-ing sequences of DNA and proteins. </title> <journal> Biophysical Chemistry, </journal> (42):7-11, 1992. 
Reference: [Loewenstern et al.1995] <author> D. Loewenstern, H. Hirsh, P. N. Yianilos, and M. Noordewier. </author> <title> DNA sequence classification using compression-based induction. </title> <type> Technical Report TR 95-087, </type> <institution> DIMACS, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The success of such methods ultimately rests on the strength of the models they employ. In our own earlier work <ref> [Loewenstern et al.1995] </ref> we were encouraged by experiments that suggested that models based on the recent past could provide a somewhat effective solution to several classification problems. Further work involving incremental improvements to the model and general scheme failed to yield much better performance.
Reference: [Mantegna et al.1993] <author> R.N. Mantegna, S.V. Buldyrev, A.L. Goldberger, S. Havlin, C.-K. Peng, M. Simons, and H.E. Stanley. </author> <title> Linguistic features of noncoding DNA sequences. </title> <journal> Physical Review Letters, </journal> <volume> 73(23) </volume> <pages> 3169-3172, </pages> <year> 1993. </year>
Reference-contexts: A logical next step taken by several investigators, focuses instead on higher order entropy estimates arising from measurements of the frequencies of longer sequences. For natural languages (e.g. English) this step typically leads to significantly lower entropy estimates. But the best resulting estimate for humretblas is roughly 1:90 bits <ref> [Mantegna et al.1993] </ref>, still not impressively different from our 2-bit random starting point. This may be something of a surprise, since such models reflect such known DNA structure as %(C + G) composition and CG suppression. <p> Section 6 concludes with a discussion that suggests why the high entropy levels observed for coding regions may not be so surprising after all. In recent years there has been increasing interest in entropy estimates for DNA. In <ref> [Mantegna et al.1993] </ref> conventional fixed multigram entropy estimates are reported for several sequences. Grumbach and Tahi in [Grumbach and Tahi1994] exploit reverse-complements and long matches to achieve improved compression rates. The method is one of pointer substitution and resembles LZ78 (see [Bell et al.1990]). <p> In some cases our results may be compared directly with estimates from [Grumbach and Tahi1994], which are included in the table. Our values for H (4) (the 4-symbol entropy) may be compared with the redundancy estimates of <ref> [Mantegna et al.1993] </ref> and are in agreement. We have grouped our results by general type (i.e. mammalian, prokaryote, etc.). The H (1); H (4); H (6) columns contain conventional multigram entropy estimates. The cdna column reports our model's cross-validation entropy estimates. <p> In fact, it has been observed by several authors that coding regions are less compressible than non-coding regions (e.g., <ref> [Mantegna et al.1993, Salamon and Konopka1992, Farach et al.1995] </ref>). So it is clear that two sequences that code for the same polypeptide may nevertheless have large Hamming distance.
Reference: [Noordewier1996] <author> M. Noordewier, </author> <month> April </month> <year> 1996. </year> <title> Private Communication. </title>
Reference-contexts: To gather a larger body of coding regions, we obtained a data set of 490 complete human genes. This data set was screened to remove any partial genes, pseudogenes, mutants, copies, or variants of the same gene <ref> [Noordewier1996] </ref>. The resulting sequence contains 484; 483 bases and is referred to as our non-redundant data set. 19 5 Experimental Results Our model's performance on the sequences described in section 4 is summarized in table 2.
Reference: [Poritz1988] <author> A. B. Poritz. </author> <title> Hidden Markov models: a guided tour. </title> <booktitle> In Proc. ICASSP-88, </booktitle> <pages> pages 7-13, </pages> <year> 1988. </year>
Reference-contexts: The learning task before us is to estimate these parameters by examining the training set. Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models <ref> [Baum and Eagon1967, Baum et al.1970, Baum1972, Poritz1988] </ref>, and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery [Dempster et al.1977, Redner and Walker1984] of essentially the same algorithm and underlying information theoretic inequality.
Reference: [Redner and Walker1984] <author> R. A. Redner and H. F. Walker. </author> <title> Mixture densities, maximum likelihood, and the EM algorithm. </title> <journal> SIAM Review, </journal> <volume> 26 </volume> <pages> 195-239, </pages> <year> 1984. </year>
Reference-contexts: Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models [Baum and Eagon1967, Baum et al.1970, Baum1972, Poritz1988], and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery <ref> [Dempster et al.1977, Redner and Walker1984] </ref> of essentially the same algorithm and underlying information theoretic inequality. The Baum-Welch algorithm is an optimization technique which locally maximizes the probability of generating the training set from the model by adjusting parameters.
Reference: [Salamon and Konopka1992] <author> Peter Salamon and Andrzej K. Konopka. </author> <title> A maximum entropy principle for the distribution of local complexity in naturally occurring nucleotide sequences. </title> <journal> Computers Chem., </journal> <volume> 16(2) </volume> <pages> 117-124, </pages> <year> 1992. </year>
Reference-contexts: In fact, it has been observed by several authors that coding regions are less compressible than non-coding regions (e.g., <ref> [Mantegna et al.1993, Salamon and Konopka1992, Farach et al.1995] </ref>). So it is clear that two sequences that code for the same polypeptide may nevertheless have large Hamming distance.
Reference: [Yianilos1997] <author> Peter N. Yianilos. </author> <title> Topics in Computational Hidden State Modeling. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Princeton University, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: A global optimization method would be expected to yield better results in general. In the general case, however, the problem of globally optimizing directed acyclic graph-based models is known to be NP-complete (see <ref> [Yianilos1997, p. 32] </ref>). Our model fits within this class but we have not considered the complexity of its optimization. It is known, however, to exhibit multiple local optima. 6 6 We noted the entropy estimates of humretblas using different randomly generated initial probability distributions.

References-found: 23

