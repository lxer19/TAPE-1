URL: http://www.ai.mit.edu/people/holly/papers/sab92.ps.Z
Refering-URL: http://www.ai.mit.edu/people/holly/papers/papers.html
Root-URL: 
Email: email: holly@ai.mit.edu, las@ai.mit.edu  
Title: An Adaptive Communication Protocol for Cooperating Mobile Robots  
Author: Holly Yanco and Lynn Andrea Stein 
Address: 545 Technology Square Cambridge, MA 02139  
Affiliation: Artificial Intelligence Laboratory Massachusetts Institute of Technology  
Abstract: We describe mobile robots engaged in a cooperative task that requires communication. The robots are initially given a fixed but uninterpreted vocabulary for communication. In attempting to perform their task, the robots learn a private communication language. Different meanings for vocabulary elements are learned in different runs of the experiment. As circumstances change, the robots adapt their lan guage to allow continued success at their task.
Abstract-found: 1
Intro-found: 1
Reference: [Fukuda and Kawauchi, 1990] <author> T. Fukuda and Y. Kawauchi. </author> <title> Communication and distributed intelligence for cellular robotic system CEBOT. </title> <booktitle> In 1990 Japan-USA Symposium on Flexible Automation, </booktitle> <pages> pages 1085-1092, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: It is often an expedient even when one agent can accomplish the task on its own or when all agents have access to the requisite information. Previous work on cooperative behavior among mobile robots has largely assumed a fixed communication language. (See, for example, <ref> [Fukuda and Kawauchi, 1990] </ref>, [Matsumoto et al., 1990], or [Shin and Epstein, 1985].) However, a language created for the robots may not provide the optimal solution. The language itself may not be natural either to the robots or to the task at hand.
Reference: [Kaelbling, 1990] <author> Leslie Pack Kaelbling. </author> <title> Learning in em-bedded systems. </title> <type> Technical Report TR-90-04, </type> <institution> Teleos Research, Palo Alto, California, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: + r end e (s) = if ub (x 0 ; n 0 ) &gt; ub (x 1 ; n 1 ) then return 0 else return 1 where ub (x; n) = n + ff=2 z ff=2 p q n )(1 x z 2 4n z 2 2n rithm <ref> [Kaelbling, 1990, Figure 21] </ref>. robot. Upon receipt of the leader's signal, the follower robot selects and performs an action. If both robots have performed correctly, positive reinforcement (+) is issued. If either robot performs incorrectly, negative reinforcement () is issued. <p> If either robot performs incorrectly, negative reinforcement () is issued. Based on this environmental feedback, the robots learn to select appropriate actions and communication signals. This algorithm is summarized in figure 1. Both the action selection and the signal selection are learned using standard reinforcement learning techniques. (See, e.g., <ref> [Kaelbling, 1990] </ref> or [Sutton, 1992] for overviews of reinforcement learning.) The particular algorithm that we use is adapted from Kaelbling's interval estimation method [1990]. Interval estimation is a relatively simple form of reinforcement: A table of inputs fi actions is maintained.
Reference: [MacLennan, 1990] <author> Bruce MacLennan. </author> <title> Evolution of communication in a population of simple machines. </title> <type> Technical Report CS-90-99, </type> <institution> University of Tennessee, Knoxville, Tennessee, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: Work on the development of communication between groups of autonomous agents has also been done by <ref> [MacLennan, 1990] </ref> and [Werner and Dyer, 1990]. Their research addresses the problem of language learning with genetic algorithms. Language evolves over many generations of the community. Within an individual agent, however, language is fixed over its lifetime.
Reference: [Maes and Brooks, 1990] <author> Pattie Maes and Rodney A. Brooks. </author> <title> Learning to coordinate behaviors. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 796-802, </pages> <address> Boston, Massachusetts, July 1990. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Taking the human out of the loop. One shortcoming of the current work is that a human "instructor" is currently needed to observe the robot's behavior and provide positive or negative reinforcement. In a more natural task, the environment itself should be able to provide that feedback. <ref> [Maes and Brooks, 1990] </ref> describe such an experiment with an individual legged robot that self-reinforces to learn a balanced gait. Because our robots are somewhat fragile and because their effective ability is largely limited to wheeled locomotion, we have not yet attempted such autonomic reinforcement.
Reference: [Martin and Sargent, 1991] <author> Fred Martin and Randy Sargent. </author> <title> The MIT sensor robot: User's guide and technical reference. </title> <month> October </month> <year> 1991. </year>
Reference-contexts: follower robot takes the (environmentally constrained) appropriate action when that signal is received. (The algorithm is summarized in figure 1.) 3 The robots Bert and Ernie, the two robots used in this research, are Sensor Robots designed by Fred Martin at the Media Laboratory at the Massachusetts Institute of Technology <ref> [Martin and Sargent, 1991] </ref>. Each robot is approximately 9 00 l fi 6 00 w fi 4 00 h, with a single circuit board containing most of the computational and sensory resources of the robot.
Reference: [Matsumoto et al., 1990] <author> A. Matsumoto, H. Asama, and Y. Ishida. </author> <title> Communication in the autonomous and decentralized robot system ACTRESS. </title> <booktitle> In Proceedings of the IEEE International Workshop on Intelligent Robots and Systems, </booktitle> <pages> pages 835-840, </pages> <address> Tsuchura, Japan, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: It is often an expedient even when one agent can accomplish the task on its own or when all agents have access to the requisite information. Previous work on cooperative behavior among mobile robots has largely assumed a fixed communication language. (See, for example, [Fukuda and Kawauchi, 1990], <ref> [Matsumoto et al., 1990] </ref>, or [Shin and Epstein, 1985].) However, a language created for the robots may not provide the optimal solution. The language itself may not be natural either to the robots or to the task at hand.
Reference: [Sargent and Martin, 1991] <author> Randy Sargent and Fred Martin. </author> <title> ic: Multi-tasking interactive C for the 6811. </title> <note> ic Version 2.5, </note> <month> October </month> <year> 1991. </year>
Reference-contexts: A 6v battery strapped to the underside of the chassis supplies the power for the robot. The robots are shown in figure 2. The primary computational resource is an onboard Motorola 6811 microprocessor. The programming environment is ic, a multi-tasking interactive C compiler and interpreter developed by Randy Sar-gent <ref> [Sargent and Martin, 1991] </ref>. ic allows a Sensor Robot to be addressed through a serial line from a host computer as well as the downloading of programs for autonomous activity. The work described in this paper was implemented with the robots under autonomous control.
Reference: [Shewchuk, 1991] <author> John P. Shewchuk. </author> <type> Ph.D. thesis proposal. </type> <institution> Department of Computer Science, Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1991. </year>
Reference-contexts: The research described in this paper is aimed towards giving autonomous agents the ability to develop their own language. Our initial work was inspired by that of <ref> [Shewchuk, 1991] </ref>. <p> However, task-based reinforcement is not random noise, and some algorithms will be better suited to the job than others. Experiments such as ours provide a useful testbed for learning algorithms. <ref> [Shewchuk, 1991] </ref> describes efforts to design reinforcement learning algorithms more suited to this sort of problem. Complex tasks. The selected task|coordinated movement|is one in which reinforcement is received at every iteration. In more complex tasks, reinforcement may be received only after completion of a sequence of actions.
Reference: [Shin and Epstein, 1985] <author> K. Shin and M. Epstein. </author> <title> Communication primitives for a distributed multi-robot system. </title> <booktitle> In Proceedings of the IEEE Robotics and Automation Conference, </booktitle> <pages> pages 910-917, </pages> <year> 1985. </year>
Reference-contexts: Previous work on cooperative behavior among mobile robots has largely assumed a fixed communication language. (See, for example, [Fukuda and Kawauchi, 1990], [Matsumoto et al., 1990], or <ref> [Shin and Epstein, 1985] </ref>.) However, a language created for the robots may not provide the optimal solution. The language itself may not be natural either to the robots or to the task at hand.
Reference: [Sutton, 1988] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference: [Sutton, 1992] <author> Richard S. Sutton. </author> <title> Special issue on reinforcement learning. </title> <booktitle> Machine Learning, </booktitle> <pages> 8(3-4), </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Based on this environmental feedback, the robots learn to select appropriate actions and communication signals. This algorithm is summarized in figure 1. Both the action selection and the signal selection are learned using standard reinforcement learning techniques. (See, e.g., [Kaelbling, 1990] or <ref> [Sutton, 1992] </ref> for overviews of reinforcement learning.) The particular algorithm that we use is adapted from Kaelbling's interval estimation method [1990]. Interval estimation is a relatively simple form of reinforcement: A table of inputs fi actions is maintained.
Reference: [Werner and Dyer, 1990] <author> Gregory M. Werner and Michael G. Dyer. </author> <title> Evolution of communication in artificial organisms. </title> <type> Technical Report UCLA-AI-90-06, </type> <institution> University of California, Los Angles, California, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: Work on the development of communication between groups of autonomous agents has also been done by [MacLennan, 1990] and <ref> [Werner and Dyer, 1990] </ref>. Their research addresses the problem of language learning with genetic algorithms. Language evolves over many generations of the community. Within an individual agent, however, language is fixed over its lifetime.
References-found: 12

