URL: ftp://ftp.cs.toronto.edu/pub/radford/nonrev.ps.Z
Refering-URL: http://www.cs.toronto.edu/~radford/publications.html
Root-URL: http://www.cs.toronto.edu
Email: radford@stat.utoronto.ca sph11@cornell.edu  
Title: Analysis of a Non-Reversible Markov Chain Sampler  
Author: Persi Diaconis Susan Holmes Radford M. Neal 
Keyword: Non-reversible Markov chain, Markov chain Monte Carlo, Metropolis algorithm.  
Date: June 1997  
Note: 5  Acknowledgments  
Address: France  
Affiliation: Maths and ORIE Biometrics Unit Dept. of Statistics and Dept. Cornell University Cornell University of Computer Science and and University of Toronto Dept of Mathematics Unite de Biometrie Canada Harvard University INRA-Montpellier  
Abstract: Technical Report BU-1385-M, Biometrics Unit, Cornell University Abstract We analyse the convergence to stationarity of a simple non-reversible Markov chain that serves as a model for several non-reversible Markov chain sampling methods that are used in practice. Our theoretical and numerical results show that non-reversibility can indeed lead to improvements over the diffusive behavior of simple Markov chain sampling schemes. The analysis uses both probabilistic techniques and an explicit diagonalisation. We thank David Aldous, Martin Hildebrand, Brad Mann, and Laurent Saloff-Coste for their help. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adler, S. L. </author> <title> (1981) "Over-relaxation method for the Monte Carlo evaluation of the partition function for multiquadratic actions", </title> <journal> Physical Review D, </journal> <volume> vol. 23, </volume> <pages> pp. 2901-2904. </pages>
Reference-contexts: In a variant of this method due to Horowitz [19], a similar effect is produced using a Markov chain that is carefully designed to be non-reversible. (See [31, 21, 24] for reviews of these methods.) The overrelaxation method <ref> [1] </ref> also employs a non-reversible Markov chain as a way of suppressing diffusive behavior, as discussed in [25]. In this paper, we analyse a non-reversible Markov chain that does a one-dimensional walk, as an abstraction of these practical sampling methods, particularly that of Horowitz [19]. <p> Accept the move to x fl = x + z with probability min <ref> [1; (x fl )=(x)] </ref>. If this move is accepted, the new state becomes (z fl ; x fl ), where z fl is the same as z except that z fl i = z i . If the move is not accepted, the state is unchanged. <p> This approach is potentially advantageous when state probabilities vary substantially over short distances, but these variations tend to can 18 cel over longer distances, as is typically the case for the discretization error in a simulation of Hamiltonian dynamics. Overrelaxation <ref> [1, 25] </ref> is another way of constructing a non-reversible Markov chain, which can avoid diffusive behavior in many situations. <p> This can be seen in the simple case of a multivariate Gaussian distribution with high positive correlations, where non-reversible methods such as Hybrid Monte Carlo [13], Horowitz's method [19], and overrelaxation <ref> [1] </ref> can sample much more efficiently than Gibbs sampling and simple Metropolis methods [25]. A simple non-reversible walk using "lines" in the coordinate directions will not necessarily be adequate for such a situation, however.
Reference: [2] <author> Binder, K. </author> <title> (1979) Monte Carlo Methods in Statistical Physics, </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference-contexts: 1 Introduction Markov chain sampling methods are commonly used in statistics [30, 29], computer science [28], statistical mechanics <ref> [2] </ref>, and quantum field theory [31, 21]. In all these fields, distributions are encountered that are difficult to sample from directly, but for which a Markov chain that converges to the distribution can easily be constructed.
Reference: [3] <author> Conway, J., Sloane, N., Wilks, A. </author> <title> (1989) "Gray codes for reflection groups", </title> <booktitle> Graphs and Combinatorics, </booktitle> <volume> vol. 5, </volume> <pages> pp. 315-325. </pages>
Reference-contexts: We consider three special cases: 1. H = S n . There is only one block in the partition. This must be ordered. One method is to use lexicographical order. A second method uses a Gray code based on transpositions <ref> [3, 9] </ref>. This linearizes the problem so that the method of Section 5.1 can be used. This is not a foolish approach; if the walk is started off at the identity it should be reasonably efficient. 2. H = fid; (1; 2)g.
Reference: [4] <author> Chung, F., Graham, R., Yau, S. T. </author> <title> (1996) "On sampling with Markov chains", Random Structures and Algorithms, </title> <journal> vol. </journal> <volume> 9, </volume> <pages> pp. 55-77. </pages>
Reference-contexts: The walk described above has a diffusive behavior taking an order (Diameter) 2 steps to reach stationarity. This is proved by Chung, Graham, and Yau <ref> [4] </ref> for tables with large row and column sums and by Diaconis and Saloff-Coste [10] for small values of I and J.
Reference: [5] <author> Diaconis P. </author> <title> (1988) Group Representations in Probability and Statistics, </title> <type> Hayward, </type> <institution> California: Institute of Mathematical Statistics. </institution>
Reference-contexts: The problem is to draw samples from , for instance when n = 52. One approach is to use the Metropolis algorithm with base chain random transpositions. This seems to work well even in the uniform case ( = 1). Some analyses and references to background literature appear in <ref> [5] </ref>. To apply the directed method of Section 5.2 we must find a collection of ordered partitions. One natural construction uses the group structure of S n . Let H be a subgroup of S n and P H the partition of S n into cosets of H. <p> This walk is connected. We remark in closing that diffusive behavior does not occur when generating uniformly distributed random permutations by successive transpositions of randomly chosen pairs <ref> [5] </ref>, nor when such random transpositions are used as a Metropolis proposal for sampling from a distribution over permutations of exponential form [11]. 7 Scope and limitations of non-reversible sampling We have shown in this paper that non-reversibility can be a desirable property of a Markov chain sampling method.
Reference: [6] <author> Diaconis P. and Efron, B. </author> <title> (1987) "Probabilistic-geometric theorems arising from the analysis of contingency tables", </title> <editor> in A. E. Gelfand (editor), </editor> <title> Contributions to the Theory and Application of Statistics: A Volume in Honor of Herbert Solomon, </title> <address> Boston: </address> <publisher> Academic Press. </publisher> <pages> 24 </pages>
Reference-contexts: This problem was posed by Diaconis and Efron <ref> [6] </ref> who give statistical motivation. Diaconis and Gangolli [8] give a host of other applications. Even for small I and J, the size of the state space can be huge.
Reference: [7] <author> Diaconis P. and Fill J. </author> <title> (1990) "Strong stationary times via a new form of duality", </title> <journal> Annals of Probability, </journal> <volume> vol. 18, </volume> <pages> pp. 1483-1522. </pages>
Reference-contexts: If T is the first time that a transition chooses from this mixture, then at time T , the process is stationary. Indeed, T is a strong stationary time in the sense of <ref> [7] </ref>; this reference gives results that provide a bound 3 on the total variation. An elementary proof may also be found in [24, Section 3.3]. For general m, we apply the above to K m .
Reference: [8] <author> Diaconis P. and Gangolli, A. </author> <title> (1996) "Rectangular arrays with fixed margins", in Finite Markov Chain Renaissance, </title> <publisher> Springer-Verlag IMA series, </publisher> <pages> pp. 15-42. </pages>
Reference-contexts: This problem was posed by Diaconis and Efron [6] who give statistical motivation. Diaconis and Gangolli <ref> [8] </ref> give a host of other applications. Even for small I and J, the size of the state space can be huge.
Reference: [9] <author> Diaconis, P. and Holmes, S. </author> <title> (1994) "Gray codes for randomization procedures" Statistics and Computing, </title> <journal> vol. </journal> <volume> 4, </volume> <pages> pp. 207-302. </pages>
Reference-contexts: We consider three special cases: 1. H = S n . There is only one block in the partition. This must be ordered. One method is to use lexicographical order. A second method uses a Gray code based on transpositions <ref> [3, 9] </ref>. This linearizes the problem so that the method of Section 5.1 can be used. This is not a foolish approach; if the walk is started off at the identity it should be reasonably efficient. 2. H = fid; (1; 2)g.
Reference: [10] <author> Diaconis P. and Saloff-Coste L. </author> <title> (1992) "Moderate growth and random walk on finite groups", </title> <journal> Geometry and Functional Analysis, </journal> <volume> vol. 4, </volume> <pages> pp. 1-36. </pages>
Reference-contexts: The O 2 (or ` 2 ) distance can be written as O 2 (`) = max X (K ` (x; y) (y)) 2 = max fl fl x 1 fl 2 = kK ` k 2 For these equivalences, see <ref> [10] </ref>. This O 2 distance bounds total variation distance through 4kK ` T V Usually the two distances give essentially the same answers for convergence. <p> The walk described above has a diffusive behavior taking an order (Diameter) 2 steps to reach stationarity. This is proved by Chung, Graham, and Yau [4] for tables with large row and column sums and by Diaconis and Saloff-Coste <ref> [10] </ref> for small values of I and J. One can try to avoid this diffusive behavior by applying the method of Section 5.2 in an obvious way, taking the lines to be determined by a pair of rows and columns and moving along these lines in a directed fashion.
Reference: [11] <author> Diaconis P. and Saloff-Coste L. </author> <title> (1995) "What do we know about the Metropolis algorithm?", </title> <booktitle> Procedings of the 27th Symposium on Theory of Computing, </booktitle> <pages> pp. 112-129. </pages>
Reference-contexts: In all these fields, distributions are encountered that are difficult to sample from directly, but for which a Markov chain that converges to the distribution can easily be constructed. For many such methods (eg, the Metropolis algorithm <ref> [23, 11] </ref>, and the Gibbs sampler [15, 14] with a random scan), the Markov chain constructed is reversible. These methods tend to explore the distribution by means of a diffusive random walk. <p> It is not hard to prove a lower bound showing that they are necessary as well. Thus here the iid Metropolis is slow. The analysis in <ref> [11] </ref> shows that the classical Metropolis algorithm, (and presumably the fiber algorithm as well) reaches stationarity in order nd steps for this example. <p> Let (x) = z p (x). For large n, z a fl ff n jff fl j+d . Thus fl c n d , for c bounded. Now, Liu's result shows that the chain M u reaches stationarity in a bounded number of steps. The analysis in <ref> [11] </ref> shows that the 17 classical Metropolis algorithm requires order n 2 steps to reach stationarity. <p> This is certainly true for exponential peaks, but things are somewhat better for polynomial peaks. For the linear peaks, as in the distribution above, available theory <ref> [11] </ref> shows that order n 2 log n steps are necessary and sufficient for the usual Metropolis chain to reach stationarity. Preliminary work of Hildebrand [18] suggests that order n 2 are necessary and suffice for convergence of the directed algorithm. <p> This walk is connected. We remark in closing that diffusive behavior does not occur when generating uniformly distributed random permutations by successive transpositions of randomly chosen pairs [5], nor when such random transpositions are used as a Metropolis proposal for sampling from a distribution over permutations of exponential form <ref> [11] </ref>. 7 Scope and limitations of non-reversible sampling We have shown in this paper that non-reversibility can be a desirable property of a Markov chain sampling method. This conclusion accords with observations of the behaviour of some practical non-reversible sampling methods [26, 17] and some previous theory (eg, [21]). <p> Another limitation is that the (diameter) 2 convergence time associated with reversible random walks applies to uniform or relatively flat stationary distributions. When the distribution is highly non-uniform, a non-reversible walk might have little or no advantage. For example, available theory <ref> [11] </ref> shows that when a random-walk Metropolis algorithm is used to sample from a distribution on a low dimensional grid having exponential peaks, the walk basically heads directly for the nearest peak. Thus if the stationary distribution is unimodal order diameter steps suffice for stationarity.
Reference: [12] <author> Diaconis P. and Sturmfels B. </author> <title> (1997) "Algebraic algorithms for sampling from conditional distributions", </title> <note> to appear in Annals of Statistics. </note>
Reference-contexts: Consider the 4 fi 4 table below: Black Brunette Red Blonde Brown 68 20 15 5 Blue 119 84 54 29 Hazel 26 17 14 14 Green 7 94 10 16 There are approximately 10 15 tables with these same margins. Diaconis and Sturmfels <ref> [12] </ref> suggested the following algorithm for generating random tables: 21 1. Randomly choose a pair of different rows and a pair of different columns. 2. Choose one of the following two changes to the 2 by 2 square thus defined, with equal prob abilities: + ! + 3. <p> We have done this, and found that the directed method does indeed work much faster than the reversible random walk. A host of other statistical problems can also be solved by an extension of the random walk algorithm given above. We give a general description here; see <ref> [12] </ref> for statistical motivation. Let X = fx 2 N n : Ax = yg, where A is a specified m fi n matrix with non-negative entries, and y is an m-vector with non-negative entries. In applications, X will be finite and non-empty. <p> In applications, X will be finite and non-empty. The problem is to sample from the uniform distribution on X . The random walk approach of <ref> [12] </ref> is defined in terms of a set of Markov Basis vectors, v 1 ; v 2 ; : : : v k 2 Z n , which satisfy: (1) Av i = 0. (2) For any x and x 0 in X , there is a positive integer, `, indices <p> One possibility is an iid Metropolis algorithm, as discussed in Section 5.3. For the contingency table example of Section 6.2, where a direction was specified by a pair of rows and a pair of columns, an alternative, implemented in <ref> [12] </ref>, is to consider the four cells in these row and columns as a 2 fi 2 table and chooses uniformly among all the 2 fi 2 tables with the same margins. <p> This is easy to do, since such a 2 fi 2 table is specified by one entry, which varies between easily computed bounds. A similar comment holds for the more general problems described in <ref> [12] </ref>. Another limitation is that the (diameter) 2 convergence time associated with reversible random walks applies to uniform or relatively flat stationary distributions. When the distribution is highly non-uniform, a non-reversible walk might have little or no advantage.
Reference: [13] <author> Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. </author> <title> (1987) "Hybrid Monte Carlo", </title> <journal> Physics Letters B, </journal> <volume> vol. 195, </volume> <pages> pp. 216-222. </pages>
Reference-contexts: Some Markov chain methods attempt to avoid the inefficiencies of such diffusive exploration. The Hybrid Monte Carlo method <ref> [13] </ref> uses an elaborate Metropolis proposal that can make large changes to the state. <p> Second, the rejection rate can be controlled by adjusting the size of the time step used in simulating the dynamics. A high rejection rate that would lead to frequent reversals of direction can thereby be avoided. Horowitz's method was derived from the "Hybrid Monte Carlo" method <ref> [13] </ref>, in which the dynamics is simulated for many time steps, with a Metropolis acceptance criterion being applied to the final state. <p> This can be seen in the simple case of a multivariate Gaussian distribution with high positive correlations, where non-reversible methods such as Hybrid Monte Carlo <ref> [13] </ref>, Horowitz's method [19], and overrelaxation [1] can sample much more efficiently than Gibbs sampling and simple Metropolis methods [25]. A simple non-reversible walk using "lines" in the coordinate directions will not necessarily be adequate for such a situation, however.
Reference: [14] <author> Gelfand, A. E. and Smith, A. F. M. </author> <title> (1990) "Sampling-based approaches to calculating marginal densities", </title> <journal> Journal of the American Statistical Association, </journal> <volume> vol. 85, </volume> <pages> pp. 398-409. </pages>
Reference-contexts: In all these fields, distributions are encountered that are difficult to sample from directly, but for which a Markov chain that converges to the distribution can easily be constructed. For many such methods (eg, the Metropolis algorithm [23, 11], and the Gibbs sampler <ref> [15, 14] </ref> with a random scan), the Markov chain constructed is reversible. These methods tend to explore the distribution by means of a diffusive random walk.
Reference: [15] <author> Geman, S. and Geman, D. </author> <title> (1984) "Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 6, </volume> <pages> pp. 721-741. </pages>
Reference-contexts: In all these fields, distributions are encountered that are difficult to sample from directly, but for which a Markov chain that converges to the distribution can easily be constructed. For many such methods (eg, the Metropolis algorithm [23, 11], and the Gibbs sampler <ref> [15, 14] </ref> with a random scan), the Markov chain constructed is reversible. These methods tend to explore the distribution by means of a diffusive random walk.
Reference: [16] <author> Gustafson, P. </author> <title> (1997a) "Large hierarchical Bayesian analysis of multivariate survival data", </title> <journal> Biometrics, </journal> <volume> vol. 53, </volume> <pages> pp. 230-242. </pages>
Reference-contexts: The advantage of looking at simpler methods is of course the possibility of more detailed analysis. We briefly discuss here some relationships between the methods of this paper and non-reversible methods that are presently used in quantum field theory [31, 21] and in some statistical applications <ref> [26, 16] </ref>. The one-dimensional walk of Section 5.1 is closely related to the "guided Monte Carlo" method of Horowitz [19]. The context is rather different, however.
Reference: [17] <author> Gustafson, P. </author> <title> (1997b) "A guided walk Metropolis algorithm", </title> <type> preprint. </type>
Reference-contexts: In this paper, we analyse a non-reversible Markov chain that does a one-dimensional walk, as an abstraction of these practical sampling methods, particularly that of Horowitz [19]. Gustafson <ref> [17] </ref> has also recently tried using adaptations of Horowitz's method. We find that the non-reversible walk does indeed converge more rapidly than the corresponding simple random walk. <p> One could then define 15 two Metropolis base chains, one with a drift to the right, one with a drift to the left. This has been tried by Gustafson <ref> [17] </ref>, who found that it produces moderate improvements over random walk Metropolis when used in a component-by-component updating scheme for sampling from a multivariate distribution. One can also use this idea to make directed versions of other reversible chains. <p> This conclusion accords with observations of the behaviour of some practical non-reversible sampling methods <ref> [26, 17] </ref> and some previous theory (eg, [21]). The methods we discuss have some limitations, however. As illustrated in Section 6.1, any local algorithm, including the non-reversible walk, can effectively get stuck when sampling from a multimodal distribution with extreme barriers to movement between peaks.
Reference: [18] <author> Hildebrand, M. </author> <title> (1997) "Rates of Convergence for a Directed Version of the Metropolis Algorithm", </title> <type> preprint. </type>
Reference-contexts: A similar, slightly easier argument suffices for larger c, we omit further details. In Theorem 2 we determined the rate of convergence carefully enough to find the cutoff in the O 2 distance at n 2c (log n + ). Martin Hildebrand <ref> [18] </ref> has shown us preliminary results which imply that with flip rates c=n, and c = c (n) tending to infinity, order cn steps are necessary and suffice for convergence in total variation distance. <p> For the linear peaks, as in the distribution above, available theory [11] shows that order n 2 log n steps are necessary and sufficient for the usual Metropolis chain to reach stationarity. Preliminary work of Hildebrand <ref> [18] </ref> suggests that order n 2 are necessary and suffice for convergence of the directed algorithm. Here, we show some numerical results that are consistent with such asymptotic behavior. We tried using the following three methods to sample from V-shaped distributions: 1.
Reference: [19] <author> Horowitz, A. M. </author> <title> (1991) "A generalized guided Monte Carlo algorithm", </title> <journal> Physics Letters B, </journal> <volume> vol. 268, </volume> <pages> pp. 247-252. </pages>
Reference-contexts: Some Markov chain methods attempt to avoid the inefficiencies of such diffusive exploration. The Hybrid Monte Carlo method [13] uses an elaborate Metropolis proposal that can make large changes to the state. In a variant of this method due to Horowitz <ref> [19] </ref>, a similar effect is produced using a Markov chain that is carefully designed to be non-reversible. (See [31, 21, 24] for reviews of these methods.) The overrelaxation method [1] also employs a non-reversible Markov chain as a way of suppressing diffusive behavior, as discussed in [25]. <p> In this paper, we analyse a non-reversible Markov chain that does a one-dimensional walk, as an abstraction of these practical sampling methods, particularly that of Horowitz <ref> [19] </ref>. Gustafson [17] has also recently tried using adaptations of Horowitz's method. We find that the non-reversible walk does indeed converge more rapidly than the corresponding simple random walk. <p> If (x) is uniform, one can easily see that chain e M with = 1=n reduces to the non-reversible walk of Section 2, which was analysed in Sections 3 and 4. The more general chain described here was abstracted from Horowitz <ref> [19] </ref>, as discussed further in Section 5.4. The same idea can be applied to general state spaces. For example, to sample from (dx), on R, an extended state space consisting of two copies of R could be used. <p> We briefly discuss here some relationships between the methods of this paper and non-reversible methods that are presently used in quantum field theory [31, 21] and in some statistical applications [26, 16]. The one-dimensional walk of Section 5.1 is closely related to the "guided Monte Carlo" method of Horowitz <ref> [19] </ref>. The context is rather different, however. Horowitz's method applies to continuous state spaces (eg, R d ), and assumes that the partial derivatives of the density function with respect to the coordinates can be computed. <p> This can be seen in the simple case of a multivariate Gaussian distribution with high positive correlations, where non-reversible methods such as Hybrid Monte Carlo [13], Horowitz's method <ref> [19] </ref>, and overrelaxation [1] can sample much more efficiently than Gibbs sampling and simple Metropolis methods [25]. A simple non-reversible walk using "lines" in the coordinate directions will not necessarily be adequate for such a situation, however. <p> This is essentially the situation with Horowitz's dynamical method <ref> [19] </ref>. The challenge is to find other such methods, especially for discrete state spaces where dynamical methods cannot be applied.
Reference: [20] <author> Lindvall, T. </author> <title> (1992) Lectures on the Coupling Method, </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: The total variation distance between the distribution after ` steps, from any starting state, and the stationary distribution, , is bounded by the probability that not all the chains will have coupled within ` steps <ref> [20] </ref>. Let X a;k be the position of the chain started at state a after k transitions.
Reference: [21] <author> Kennedy, A. D. </author> <title> (1990) "The theory of hybrid stochastic algorithms", </title> <editor> in P. H. Damgaard, et al. </editor> <booktitle> (editors) Probabilistic Methods in Quantum Field Theory and Quantum Gravity, </booktitle> <address> New York: </address> <publisher> Plenum Press. </publisher>
Reference-contexts: 1 Introduction Markov chain sampling methods are commonly used in statistics [30, 29], computer science [28], statistical mechanics [2], and quantum field theory <ref> [31, 21] </ref>. In all these fields, distributions are encountered that are difficult to sample from directly, but for which a Markov chain that converges to the distribution can easily be constructed. <p> The Hybrid Monte Carlo method [13] uses an elaborate Metropolis proposal that can make large changes to the state. In a variant of this method due to Horowitz [19], a similar effect is produced using a Markov chain that is carefully designed to be non-reversible. (See <ref> [31, 21, 24] </ref> for reviews of these methods.) The overrelaxation method [1] also employs a non-reversible Markov chain as a way of suppressing diffusive behavior, as discussed in [25]. <p> The advantage of looking at simpler methods is of course the possibility of more detailed analysis. We briefly discuss here some relationships between the methods of this paper and non-reversible methods that are presently used in quantum field theory <ref> [31, 21] </ref> and in some statistical applications [26, 16]. The one-dimensional walk of Section 5.1 is closely related to the "guided Monte Carlo" method of Horowitz [19]. The context is rather different, however. <p> This conclusion accords with observations of the behaviour of some practical non-reversible sampling methods [26, 17] and some previous theory (eg, <ref> [21] </ref>). The methods we discuss have some limitations, however. As illustrated in Section 6.1, any local algorithm, including the non-reversible walk, can effectively get stuck when sampling from a multimodal distribution with extreme barriers to movement between peaks.
Reference: [22] <author> Liu, J. </author> <title> (1996) "Metropolized independent sampling with comparisons to rejection sampling and importance sampling", </title> <journal> Statistics and Computing, </journal> <volume> vol. 6, </volume> <pages> pp. 113-119. </pages>
Reference-contexts: Call this iid Metropolis chain M u . Suppose that the state space, X , has N points, and let fl = max x (x). Liu <ref> [22] </ref> shows that kM ` T V 1 ` We consider two examples for which X = f1; : : : ; ng d , for some n and d, and hence N = n d .
Reference: [23] <author> Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. </author> <title> (1953) "Equation of state calculations by fast computing machines", </title> <journal> Journal of Chemical Physics, </journal> <volume> vol. 21, </volume> <pages> pp. 1087-1092. 25 </pages>
Reference-contexts: In all these fields, distributions are encountered that are difficult to sample from directly, but for which a Markov chain that converges to the distribution can easily be constructed. For many such methods (eg, the Metropolis algorithm <ref> [23, 11] </ref>, and the Gibbs sampler [15, 14] with a random scan), the Markov chain constructed is reversible. These methods tend to explore the distribution by means of a diffusive random walk.
Reference: [24] <author> Neal, R. M. </author> <title> (1993) Probabilistic Inference Using Markov Chain Monte Carlo Methods, </title> <type> Tech--nical Report CRG-TR-93-1, </type> <institution> Dept. of Computer Science, University of Toronto, </institution> <note> 144 pages. Available from the author's home page at http://www.cs.utoronto.ca/radford/. </note>
Reference-contexts: The Hybrid Monte Carlo method [13] uses an elaborate Metropolis proposal that can make large changes to the state. In a variant of this method due to Horowitz [19], a similar effect is produced using a Markov chain that is carefully designed to be non-reversible. (See <ref> [31, 21, 24] </ref> for reviews of these methods.) The overrelaxation method [1] also employs a non-reversible Markov chain as a way of suppressing diffusive behavior, as discussed in [25]. <p> Indeed, T is a strong stationary time in the sense of [7]; this reference gives results that provide a bound 3 on the total variation. An elementary proof may also be found in <ref> [24, Section 3.3] </ref>. For general m, we apply the above to K m . To prove 3.4, let T 1 ; T 2 ; : : : be the times that the walks changes sign (ie, when x ! x is chosen, including when x = x = 0).
Reference: [25] <author> Neal, R. M. </author> <title> (1995) "Suppressing random walks in Markov chain Monte Carlo using ordered overrelaxation", </title> <type> Technical Report No. 9508, </type> <institution> Dept. of Statistics, University of Toronto, </institution> <note> 22 pages. Available from the author's home page at http://www.cs.utoronto.ca/radford/. </note>
Reference-contexts: method due to Horowitz [19], a similar effect is produced using a Markov chain that is carefully designed to be non-reversible. (See [31, 21, 24] for reviews of these methods.) The overrelaxation method [1] also employs a non-reversible Markov chain as a way of suppressing diffusive behavior, as discussed in <ref> [25] </ref>. In this paper, we analyse a non-reversible Markov chain that does a one-dimensional walk, as an abstraction of these practical sampling methods, particularly that of Horowitz [19]. Gustafson [17] has also recently tried using adaptations of Horowitz's method. <p> This approach is potentially advantageous when state probabilities vary substantially over short distances, but these variations tend to can 18 cel over longer distances, as is typically the case for the discretization error in a simulation of Hamiltonian dynamics. Overrelaxation <ref> [1, 25] </ref> is another way of constructing a non-reversible Markov chain, which can avoid diffusive behavior in many situations. <p> This can be seen in the simple case of a multivariate Gaussian distribution with high positive correlations, where non-reversible methods such as Hybrid Monte Carlo [13], Horowitz's method [19], and overrelaxation [1] can sample much more efficiently than Gibbs sampling and simple Metropolis methods <ref> [25] </ref>. A simple non-reversible walk using "lines" in the coordinate directions will not necessarily be adequate for such a situation, however.
Reference: [26] <author> Neal, R. M. </author> <booktitle> (1996) Bayesian Learning for Neural Networks (Lecture Notes in Statistics No. 118), </booktitle> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The advantage of looking at simpler methods is of course the possibility of more detailed analysis. We briefly discuss here some relationships between the methods of this paper and non-reversible methods that are presently used in quantum field theory [31, 21] and in some statistical applications <ref> [26, 16] </ref>. The one-dimensional walk of Section 5.1 is closely related to the "guided Monte Carlo" method of Horowitz [19]. The context is rather different, however. <p> This conclusion accords with observations of the behaviour of some practical non-reversible sampling methods <ref> [26, 17] </ref> and some previous theory (eg, [21]). The methods we discuss have some limitations, however. As illustrated in Section 6.1, any local algorithm, including the non-reversible walk, can effectively get stuck when sampling from a multimodal distribution with extreme barriers to movement between peaks.
Reference: [27] <author> Roberts, G. O. and Sahu, S. K. </author> <title> (1997) "Updating schemes, correlation structure, blocking and parameterisation for the Gibbs sampler", </title> <journal> to appear in the Journal of the Royal Statistical Society B. </journal>
Reference-contexts: These methods tend to explore the distribution by means of a diffusive random walk. Some other common methods, such as the Gibbs sampler with a systematic scan, use a Markov chain that is not reversible, but which has diffusive behavior resembling that of a related reversible chain <ref> [27] </ref>. Some Markov chain methods attempt to avoid the inefficiencies of such diffusive exploration. The Hybrid Monte Carlo method [13] uses an elaborate Metropolis proposal that can make large changes to the state.
Reference: [28] <author> Sinclair, A. </author> <title> (1993) Algorithms for Random Generation and Counting: A Markov Chain Approach, </title> <address> Boston: </address> <publisher> Birkhauser. </publisher>
Reference-contexts: 1 Introduction Markov chain sampling methods are commonly used in statistics [30, 29], computer science <ref> [28] </ref>, statistical mechanics [2], and quantum field theory [31, 21]. In all these fields, distributions are encountered that are difficult to sample from directly, but for which a Markov chain that converges to the distribution can easily be constructed.
Reference: [29] <author> Smith, A. F. M. and Roberts, G. O. </author> <title> (1993) "Bayesian computation via the Gibbs sampler and related Markov chain Monte Carlo methods", </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> vol. 55, </volume> <pages> pp. 3-23. </pages>
Reference-contexts: 1 Introduction Markov chain sampling methods are commonly used in statistics <ref> [30, 29] </ref>, computer science [28], statistical mechanics [2], and quantum field theory [31, 21]. In all these fields, distributions are encountered that are difficult to sample from directly, but for which a Markov chain that converges to the distribution can easily be constructed.
Reference: [30] <author> Tierney, L. </author> <title> (1994) "Markov Chains for exploring Posterior Distributions", </title> <journal> Annals of Statistics vol. </journal> <volume> 22, </volume> <pages> pp. 1701-1762 </pages>
Reference-contexts: 1 Introduction Markov chain sampling methods are commonly used in statistics <ref> [30, 29] </ref>, computer science [28], statistical mechanics [2], and quantum field theory [31, 21]. In all these fields, distributions are encountered that are difficult to sample from directly, but for which a Markov chain that converges to the distribution can easily be constructed.
Reference: [31] <author> Toussaint, D. </author> <title> (1989) "Introduction to algorithms for Monte Carlo simulations and their application to QCD", </title> <journal> Computer Physics Communications, </journal> <volume> vol. 56, </volume> <pages> pp. 69-92. 26 </pages>
Reference-contexts: 1 Introduction Markov chain sampling methods are commonly used in statistics [30, 29], computer science [28], statistical mechanics [2], and quantum field theory <ref> [31, 21] </ref>. In all these fields, distributions are encountered that are difficult to sample from directly, but for which a Markov chain that converges to the distribution can easily be constructed. <p> The Hybrid Monte Carlo method [13] uses an elaborate Metropolis proposal that can make large changes to the state. In a variant of this method due to Horowitz [19], a similar effect is produced using a Markov chain that is carefully designed to be non-reversible. (See <ref> [31, 21, 24] </ref> for reviews of these methods.) The overrelaxation method [1] also employs a non-reversible Markov chain as a way of suppressing diffusive behavior, as discussed in [25]. <p> The advantage of looking at simpler methods is of course the possibility of more detailed analysis. We briefly discuss here some relationships between the methods of this paper and non-reversible methods that are presently used in quantum field theory <ref> [31, 21] </ref> and in some statistical applications [26, 16]. The one-dimensional walk of Section 5.1 is closely related to the "guided Monte Carlo" method of Horowitz [19]. The context is rather different, however.
References-found: 31

