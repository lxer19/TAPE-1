URL: http://www.research.att.com/~mkearns/papers/reinforcement.ps.Z
Refering-URL: http://www.research.att.com/~mkearns/
Root-URL: 
Email: mkearns@research.att.com  baveja@cs.colorado.edu  
Title: Near-Optimal Performance for Reinforcement Learning in Polynomial Time  
Author: Michael Kearns Satinder Singh 
Date: February 27, 1998  
Affiliation: AT&T Labs  University of Colorado  
Abstract: We present new algorithms for reinforcement learning and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes. After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give algorithms requiring a number of actions and total computation time that are only polynomial in T and the number of states, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is their explicit handling of the Exploration-Exploitation trade-off. These are the first results in the reinforcement learning literature giving algorithms that provably converge to near-optimal performance in polynomial time for general Markov decision processes. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. </author> <year> (1990). </year> <title> Sequential decision problems and neural networks. </title> <booktitle> In NIPS 2, </booktitle> <pages> pages 686-693, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bertsekas, D. P. & Tsitsiklis, J. N. </author> <year> (1996). </year> <title> Neuro-Dynamic Programming. </title> <address> Belmont, MA: </address> <publisher> Athena Scientific. </publisher>
Reference: <author> Fiechter, C. </author> <year> (1994). </year> <title> Efficient reinforcement learning. </title> <booktitle> In COLT94, </booktitle> <pages> pages 88-97. </pages> <publisher> ACM Press. </publisher>
Reference: <author> Fiechter, C. </author> <year> (1997). </year> <title> Expected mistake bound model for on-line reinforcement learning. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference (ICML97), </booktitle> <pages> pages 116-124. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Gullapalli, V. & Barto, A. G. </author> <year> (1994). </year> <title> Convergence of indirect adaptive asynchronous value iteration algorithms. </title> <booktitle> In NIPS 6, </booktitle> <pages> pages 695-702. </pages> <publisher> Morgan Kauffman. </publisher>
Reference: <author> Jaakkola, T., Jordan, M. I., & Singh, S. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6 (6), </volume> <pages> 1185-1201. </pages>
Reference: <author> Moore, A. W. & Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13 (1). </volume>
Reference-contexts: It is likely that a practical implementation based on the algorithmic ideas given here would enjoy performance on natural problems that is considerably better than the current bounds indicate. We note that as far as experimental work is concerned, the Optimism Under Uncertainty heuristic of <ref> (Moore & Atkeson, 1993) </ref> for the discounted case is perhaps closest in spirit to our E 3 algorithm. 4.6 Eliminating Knowledge of T and opt ( T;* In order to simplify our presentation of the main theorem and the E 3 algorithm, we made the assumption that the learning algorithm knew
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov decision processes : discrete stochastic dynamic programming. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: For the development and exposition, it will be easiest to consider MDP's for which every policy is ergodic, the so-called unichain MDP's <ref> (Puterman, 1994) </ref>. Considering the unichain case simply allows us to discuss the stationary distribution of any policy without cumbersome technical details, and as it turns out, the result for unichains already forces the main technical ideas upon us. <p> This motivates the following definition. 2 In the long version, we relate the notion of *-return mixing time to the standard notion of mixing time to stationary distributions <ref> (Puterman, 1994) </ref>. The important point here is that the *-return mixing time is polynomially bounded by the standard mixing time, but may in some cases be substantially smaller. 4 Definition 3 Let M be a Markov decision process.
Reference: <author> Saul, L. & Singh, S. </author> <year> (1996). </year> <title> Learning curve bounds for Markov decision processes with undiscounted rewards. </title> <note> In COLT96. 15 Schapire, </note> <author> R. E. & Warmuth, M. K. </author> <year> (1994). </year> <title> On the worst-case analysis of temporal-difference learning algorithms. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 266-274. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Singh, S. & Dayan, P. </author> <year> (1998). </year> <title> Analytical mean squared error curves for temporal difference learning. Machine Learning. </title> <publisher> in press. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. & Barto, A. G. </author> <year> (1998). </year> <title> Reinforcement Learning: An Introduction. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Thrun, S. B. </author> <year> (1992). </year> <title> The role of exploration in learning control. In White, </title> <editor> D. A. & Sofge, D. A. (Eds.), </editor> <booktitle> Handbook of Intelligent Control: Neural, Fuzzy and Adaptive Approaches. </booktitle> <address> Florence, Kentucky 41022: </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference: <author> Tsitsiklis, J. </author> <year> (1994). </year> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16 (3), </volume> <pages> 185-202. </pages>
Reference: <author> Watkins, C. J. C. H. & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 279-292. 16 </pages>
References-found: 15

