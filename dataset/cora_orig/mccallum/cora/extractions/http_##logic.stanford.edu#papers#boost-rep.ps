URL: http://logic.stanford.edu/papers/boost-rep.ps
Refering-URL: http://logic.stanford.edu/papers/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: ofer@cs.stanford.edu  solla@nordita.dk  
Title: Some Notes on Schapire's Boosting Algorithm  
Author: Ofer Matan Sara Solla 
Date: May 19, 1995  
Address: CA 94305  DK-2100 Copenhagen, Denmark  
Affiliation: Department of Computer Science Stanford University,  The Niels Bohr Institute  
Abstract: We study Schapire's Boosting Algorithm(SBA) for use in practice. SBA is analyzed in terms of its representation and its search. We show that the SBA representation is a piecewise tiling of the domain and that if the weak learner has low coverage ability, SBA's search may fail to boost or may give a sub-optimal solution. We present a rejection boosting algorithm that trades-off exploration and exploitation: It requires fewer pattern labels at the expense of lower boosting ability.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: This is in contrast with the view of a majority of three hypotheses which is correct only in binary classification problems. Therefore, the representation of SBA is similar not to additive mixture models [7], but to trees and decision lists <ref> [1, 10, 11] </ref>. h 2 's role in this view is a rejector. <p> It is a simple 2 dimensional 2-class concept that the weak learners cannot fit exactly. The weak learners had 2 fully connected hidden layers of the architecture 2x5x5x2. target concept is shown in the top left corner. The domain of the concept is <ref> [1; 1] </ref> 2 . Black areas are labeled +, white areas are labeled . Samples are drawn uniformly from the domain. The second row presents SBA results and the following rows present RBA results for different rejection rates. The columns display the weak learners concepts or their errors.
Reference: [2] <author> Harris Drucker, Robert Schapire, and Patrice Simard. </author> <title> Boosting performance in neural networks. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 4 </volume> <pages> 705-719, </pages> <year> 1993. </year>
Reference-contexts: A weak learning algorithm produces a classifier guaranteed to do better than 50% correct for any target concept and distribution. A strong learning algo-rithm can achieve any level of performance (in the PAC sense). Schapire's method has been applied successfully to an OCR problem learned by backpropagation networks <ref> [2] </ref>. In this paper we analyze Schapire's Boosting Algorithm (SBA) given that we want to use it in practice, where the "weakness" is due to the lack of effective capacity of the learning machine at hand. Our study is done in terms of SBA's (1) representation and (2) search method.
Reference: [3] <author> Y. Freund and R.E. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In EuroColt-95, </booktitle> <year> 1995. </year>
Reference-contexts: We have studied Schapire's Boosting Algorithm and argued that it may fail to boost efficiently under certain conditions. This quality is due to the tiling nature of its representation. Other boosting algorithms such as <ref> [4, 3] </ref> which have a mixture representation are not prone to such problems. We have suggested an alternative of boosting by rejection that uses fewer pattern labels. A good rejection criterion can give comparable results to SBA at a fraction of the labeling cost.
Reference: [4] <author> Yoav Freund. </author> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proceedings of the 5th Workshop on Computational Learning theory, </booktitle> <pages> pages 391-398. </pages> <publisher> Morgan Kaufman, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction Boosting algorithms were introduced in the COLT community as tools to prove the equivalence of weak learning and strong learning <ref> [12, 4] </ref>. A weak learning algorithm produces a classifier guaranteed to do better than 50% correct for any target concept and distribution. A strong learning algo-rithm can achieve any level of performance (in the PAC sense). <p> We have studied Schapire's Boosting Algorithm and argued that it may fail to boost efficiently under certain conditions. This quality is due to the tiling nature of its representation. Other boosting algorithms such as <ref> [4, 3] </ref> which have a mixture representation are not prone to such problems. We have suggested an alternative of boosting by rejection that uses fewer pattern labels. A good rejection criterion can give comparable results to SBA at a fraction of the labeling cost.
Reference: [5] <author> L. K. Hansen and P. Salamon. </author> <title> Neural network ensembles. </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> 12(10) </volume> <pages> 993-10001, </pages> <year> 1990. </year>
Reference-contexts: For example, the capacity of back-propagation networks seem to grow very slowly as additional units are added. In contrast, boosting does not appear to be useful for LM's that have infinite potential capacity (e.g. knearest neighbors and decision trees). 2 SBA resembles work in ensembles <ref> [9, 5] </ref> in that it is using multiple experts. The skewing of the distribution resembles work in Statistics where models are fitted to residuals.
Reference: [6] <author> R.A. Jacobs, M.I. Jordan, S.J. Nowlan, and G.E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <year> 1991. </year> <month> 10 </month>
Reference-contexts: The skewing of the distribution resembles work in Statistics where models are fitted to residuals. In relation to gating networks <ref> [6] </ref>, SBA may be seen as using hard (vs. soft) gates. 2 When can SBA increase coverage ? The ability to cover a target concept by tiling depends on the complexity of the target and of the learner while the actual error depends on the distribution as well.
Reference: [7] <author> M.I. Jordan and R.A. Jacobs. </author> <title> Hierarchical mixtures of experts and the em algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: We view the composite hypothesis h SBA as piecewise covering of the domain (see Figure 1a). This is in contrast with the view of a majority of three hypotheses which is correct only in binary classification problems. Therefore, the representation of SBA is similar not to additive mixture models <ref> [7] </ref>, but to trees and decision lists [1, 10, 11]. h 2 's role in this view is a rejector.
Reference: [8] <author> O. Matan. </author> <note> On-site learning (submitted). Available at http://cit.stanford.edu/papers/on-site.ps, 1995. </note>
Reference-contexts: Rejected patterns are classified by h R2 . This scheme is attractive in that it may use fewer patterns to train, and only patterns that are rejected need to be labeled. This is of importance when labeling costs are high or the system is learning while on-site <ref> [8] </ref>. Boosting in practice can fail due to the inability of the additional weak learner to perform well on the new distribution.
Reference: [9] <author> M. P. Perrone and L. N. Cooper. </author> <title> When networks disagree: Ensemble methods for hybrid neural networks. </title> <editor> In R. J. Mammone, editor, </editor> <booktitle> Neural Networks for speech and Image Processing. </booktitle> <address> Chapman-Hall, </address> <year> 1993. </year>
Reference-contexts: For example, the capacity of back-propagation networks seem to grow very slowly as additional units are added. In contrast, boosting does not appear to be useful for LM's that have infinite potential capacity (e.g. knearest neighbors and decision trees). 2 SBA resembles work in ensembles <ref> [9, 5] </ref> in that it is using multiple experts. The skewing of the distribution resembles work in Statistics where models are fitted to residuals.
Reference: [10] <author> J.R. Quinlan. C4.5: </author> <title> Programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: This is in contrast with the view of a majority of three hypotheses which is correct only in binary classification problems. Therefore, the representation of SBA is similar not to additive mixture models [7], but to trees and decision lists <ref> [1, 10, 11] </ref>. h 2 's role in this view is a rejector.
Reference: [11] <author> Ronald L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: This is in contrast with the view of a majority of three hypotheses which is correct only in binary classification problems. Therefore, the representation of SBA is similar not to additive mixture models [7], but to trees and decision lists <ref> [1, 10, 11] </ref>. h 2 's role in this view is a rejector.
Reference: [12] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Boosting algorithms were introduced in the COLT community as tools to prove the equivalence of weak learning and strong learning <ref> [12, 4] </ref>. A weak learning algorithm produces a classifier guaranteed to do better than 50% correct for any target concept and distribution. A strong learning algo-rithm can achieve any level of performance (in the PAC sense).
References-found: 12

