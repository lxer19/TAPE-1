URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/ANL9341.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts93.htm
Root-URL: http://www.mcs.anl.gov
Title: Distribution Category:  Early Experiences with the IBM SP1 and the High-Performance Switch Edited by  
Author: Mathematics and William Gropp 
Note: This work was supported in part by the Office of Scientific Computing, U.S. Department of Energy, under Contract W-31-109-Eng-38.  
Date: November 1993  
Web: ANL-93/41  
Address: 9700 South Cass Avenue Argonne, IL 60439-4801  
Affiliation: Computer Science (UC-405) ARGONNE NATIONAL LABORATORY  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> James Boyle, Ralph Butler, Terrence Disz, Barnett Glickfeld, Ewing Lusk, Ross Over-beek, James Patterson, and Rick Stevens. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart, and Winston, </publisher> <year> 1987. </year>
Reference-contexts: A version of PETSc that can take advantage of IBM's ESSL (when available) has been developed; the object-oriented nature of PETSc means that users can take advantage of these changes by relinking rather than rewriting their code. 2.10 p4 Contributed by Ewing Lusk The p4 parallel programming system <ref> [1, 2, 3] </ref> currently runs on nearly all existing parallel computers and workstations. It has been used routinely on networks of RS/6000's. It was hoped that the RS/6000 version of p4 could be built unchanged on the SP1. <p> We have developed both sequential and parallel versions of the fastDNAml package; the sequential version is now being distributed through the RDP server, and we have helped a limited set of institutions instal the parallel version (which is based on the p4 package of routines for writing portable parallel programs <ref> [1] </ref>). The phylogenetic tree that we are developing is of major scientific interest. It is the culmination of decades of work to develop the technology, gather the rRNA sequences, and careful align them for analysis.
Reference: [2] <author> Ralph Butler and Ewing Lusk. </author> <title> Monitors, messages, and clusters: The p4 parallel programming system. </title> <journal> Journal of Parallel Computing. </journal> <note> To appear (Also Argonne National Laboratory Mathematics and Computer Science Division preprint P362-0493). </note>
Reference-contexts: A version of PETSc that can take advantage of IBM's ESSL (when available) has been developed; the object-oriented nature of PETSc means that users can take advantage of these changes by relinking rather than rewriting their code. 2.10 p4 Contributed by Ewing Lusk The p4 parallel programming system <ref> [1, 2, 3] </ref> currently runs on nearly all existing parallel computers and workstations. It has been used routinely on networks of RS/6000's. It was hoped that the RS/6000 version of p4 could be built unchanged on the SP1.
Reference: [3] <author> Ralph Butler and Ewing Lusk. </author> <title> User's guide to the p4 parallel programming system. </title> <type> Technical Report ANL-92/17, </type> <institution> Argonne National Laboratory, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: A version of PETSc that can take advantage of IBM's ESSL (when available) has been developed; the object-oriented nature of PETSc means that users can take advantage of these changes by relinking rather than rewriting their code. 2.10 p4 Contributed by Ewing Lusk The p4 parallel programming system <ref> [1, 2, 3] </ref> currently runs on nearly all existing parallel computers and workstations. It has been used routinely on networks of RS/6000's. It was hoped that the RS/6000 version of p4 could be built unchanged on the SP1.
Reference: [4] <author> J. Felsenstein. </author> <title> Phylip manual version 3.3. </title> <type> Technical report, </type> <institution> University of California, Berkeley, Calif., </institution> <year> 1990. </year>
Reference-contexts: Such a phylogenetic tree plays a fundamental role in supporting interpretation of molecular sequence data. Phylogenetic tree inference based on maximum likelihood is appealing from both biological and statistical perspectives. Felsenstein <ref> [4] </ref> has written a computer program that implements such a method. However, maximum likelihood is computationally demanding, so the program had been used only for inferring relationships among small numbers of sequences (up to about 20).
Reference: [5] <author> Ian Foster, Robert Olson, and Steven Tuecke. </author> <title> Productive parallel programming: The PCN approach. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 51-66, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: These provides portability of parallel applications written with MPI from a variety of machines to the SP1, and in particular between the SP1 and workstation networks. 2.8 PCN Contributed by Steven Tuecke PCN <ref> [5, 7] </ref> is a system for developing and executing parallel programs. It comprises a high-level programming language, tools for developing and debugging programs in this language, and interfaces to Fortran and C that allow the reuse of existing code in multilingual parallel programs.
Reference: [6] <author> Ian Foster, Robert Olson, and Steven Tuecke. </author> <title> Programming in Fortran M. </title> <type> Technical Report ANL-93/26, Revision 1, </type> <institution> Argonne National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: The matrices must be symmetric but may have an arbitrary sparsity structure. BlockSolve is a portable package that is compatible with several different message-passing paradigms including EUIH but not EUI. 2.4 Fortran M Contributed by Steven Tuecke Fortran M <ref> [6] </ref> is a small set of extensions to Fortran that supports a modular approach to the construction of sequential and parallel programs. Fortran M programs use channels to plug together processes that may be written in Fortran M or Fortran 77. Processes communicate by sending and receiving messages on channels.
Reference: [7] <author> Ian Foster and Steven Tuecke. </author> <title> Parallel programming with PCN. </title> <type> Technical Report ANL-91/32, Rev. 2, </type> <institution> Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: These provides portability of parallel applications written with MPI from a variety of machines to the SP1, and in particular between the SP1 and workstation networks. 2.8 PCN Contributed by Steven Tuecke PCN <ref> [5, 7] </ref> is a system for developing and executing parallel programs. It comprises a high-level programming language, tools for developing and debugging programs in this language, and interfaces to Fortran and C that allow the reuse of existing code in multilingual parallel programs.
Reference: [8] <author> William Gropp. </author> <title> Early experiences with the IBM SP-1. </title> <type> Technical Report ANL/MCS-TM-177, </type> <institution> Argonne National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: This report describes the applications and programming packages that researchers at Argonne National Laboratory ported to the SP1 after the high-performance switch was delivered. A previous report <ref> [8] </ref> discussed the SP1 without the high-performance switch. This report discusses early experiences with the SP1; most of the tools and applications discussed in this paper have not yet been tuned for the SP1.
Reference: [9] <author> William Gropp and Ewing Lusk. </author> <title> An abstract device definition to support the implementation of a high-level message-passing interface. </title> <type> Technical Report MCS-P342-1193, </type> <institution> Argonne National Laboratory, </institution> <year> 1993. </year> <month> 26 </month>
Reference-contexts: A partial implementation of the standard as it stood in May 1993 was implemented and run on the SP1. In August we met with IBM researchers in Yorktown to propose a new implementation design <ref> [9] </ref>. The new design specifies a low-level device interface on which the MPI can be implemented portably. We have begun doing this, and at this stage (early September 1993) have implemented the basic point-to-point message-passing functionality of MPI.
Reference: [10] <author> William D. Gropp and Barry Smith. </author> <title> Chameleon parallel programming tools users manual. </title> <type> Technical Report ANL-93/23, </type> <institution> Argonne National Laboratory, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Unfortunately, the lack of a standard for message passing has hampered the construction of portable and efficient parallel programs. In an attempt to remedy this problem, a number of groups have developed their own message-passing systems, each with its own strengths and weaknesses. Chameleon <ref> [10] </ref> is a second-generation system of this type. Rather than replacing these existing systems, Chameleon is meant to supplement them by providing a uniform way to access many of these systems.
Reference: [11] <author> William D. Gropp and Barry Smith. </author> <title> Users manual for KSP: Data-structure-neutral codes implementing Krylov space methods. </title> <type> Technical Report ANL-93/30, </type> <institution> Argonne National Laboratory, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: PETSc is designed to match advanced algorithms to new and existing applications by taking an object-oriented approach to the design of the routines. For example, the iterative accelerators that are part of PETSc <ref> [11] </ref> have been designed to allow the user to specify all of the vector operations as well as matrix-vector product and preconditioning. Thus, these iterative methods can be used with nontraditional vectors, such as oct-trees or vectors distributed across a distributed-memory parallel computer. <p> PETSc also includes a number of packages that aid in writing parallel programs. One of these is BlockComm, a package for communicating blocks of data between processors. Another is a parallel general (nonsymmetric) linear system solver using iterative methods <ref> [11] </ref>. Currently, parallel versions of conjugate gradient, conjugate gradient squared, BiCG-Stab, Freund's transpose-free QMR, generalized minimum residual (GMRES), Chan's transpose-free QMR, Chebychev, and Richardson are supported, along with a variety of preconditioners. A parallel nonlinear system solver is also available.
Reference: [12] <author> Michael T. Heath and Jennifer Etheridge Finger. </author> <title> Visualizing performance of parallel programs. </title> <journal> IEEE Software, </journal> <volume> 8(5) </volume> <pages> 29-39, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The second is EUI, IBM's message-passing system. IP and EUI applications may share the switch; multiple IP applications may share both nodes and the switch. IP and EUI run under the "Parallel Operating Environment," or POE. POE includes a number of tools, such as a parallel debugger and ParaGraph-like <ref> [12] </ref> visualization tool (vt). These two transport layers share a common interface to the switch known as lightspeed. feature of this debugger is that it displays the location in the code of each of the processors.
Reference: [13] <author> Mark T. Jones and Paul E. Plassmann. </author> <title> An efficient parallel iterative solver for large sparse linear systems. In Proceedings of the IMA Workshop on Sparse Matrix Computations: Graph Theory Issues & Algorithms, </title> <address> Minneapolis, </address> <year> 1991. </year> <institution> University of Minnesota. </institution>
Reference-contexts: In the POE environment, these scripts can use the high-performance switch. 7 Table 3: Parallel unix commands Unix Parallel cp pcp ls pls find pfind if (`test`) action ppred kill pkill a.out prun ... a.out 2.3 BlockSolve Contributed by Paul Plassmann and Lori Freitag BlockSolve <ref> [13] </ref> is a software library for solving large, sparse systems of linear equations on massively parallel computers. The matrices must be symmetric but may have an arbitrary sparsity structure.
Reference: [14] <author> D. C. Liu and J. Nocedal. </author> <title> On the limited memory BFGS method for large-scale optimization. </title> <journal> Math. Prog., </journal> <volume> 45 </volume> <pages> 503-528, </pages> <year> 1989. </year>
Reference-contexts: SP1 (p4) Processors CPU Elapsed CPU Elapsed CPU Elapsed 1 - 203.89 205.46 203.90 205.29 4 160.26 160.00 37.54 37.61 33.29 112.46 16 43.13 43.00 12.75 12.97 - 3.8 Superconductivity|Vortex Structures Contributed by Mario Palumbo and Paul Plassmann We have developed a parallel code that uses the limited-memory BFGS algorithm <ref> [14] </ref> to find optimal vortex solutions within the three-dimensional anisotropic Ginzburg-Landau model. Our implementation is capable of considering arbitrary field orientation as well as various types of random and correlated disorder.
Reference: [15] <author> G. J. Olsen, R. Overbeek, N. Larsen, and C. R. Woese. </author> <title> The ribosomal database project: Updated description. </title> <journal> Nucleic Acids Res., </journal> <volume> 19 </volume> <pages> 4817-4817, </pages> <year> 1991. </year> <month> 27 </month>
Reference-contexts: It currently does not use any of the switch-based transport mechanisms. This application has consumed by far the largest number of hours on the SP1 since its installation. The Ribosomal Database Project at the University of Illinois at Urbana-Champaign <ref> [15] </ref> distributes alignments of small subunit ribosomal RNA (rRNA) sequences from both prokaryotic microorganisms and eukaryotes. In order to better understand the organisms themselves and the evolution and function of the ribosome, we wished to infer a consistent, high-quality phylogenetic tree relating all of these sequences.
References-found: 15

