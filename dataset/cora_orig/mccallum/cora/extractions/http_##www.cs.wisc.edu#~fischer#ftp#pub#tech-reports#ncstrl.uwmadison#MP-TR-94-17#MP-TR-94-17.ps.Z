URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-94-17/MP-TR-94-17.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-94-17/
Root-URL: http://www.cs.wisc.edu
Title: Operator Splitting Methods for Monotone Affine Variational Inequalities, with a Parallel Application to Optimal Control
Author: Jonathan Eckstein Michael C. Ferris hM q; w xi w B () M q hx; M qi () 
Address: New Brunswick, NJ 08903  Wisconsin, Madison, WI 53706  
Affiliation: School of Business and RUTCOR, Rutgers University,  Computer Sciences Department, University of  
Note: The  B i=1 with i 2 [1; 1), u i 2 (1; 1], and i u i It is well known that when 0 and u 1, the  This work is partially supported by the National Science Foundation under grants CCR-9157632 and CDA-9024618, the Air Force Office of Scientific Research under grant F49620-94-1-0036 and the Department of Energy grant DE-FG03-94ER61915.  
Date: July 30, 1996  
Abstract: This paper applies splitting techniques developed for set-valued maximal monotone operators to monotone affine variational inequalities, including as a special case the classical linear complementarity problem. We give a unified presentation of several splitting algorithms for monotone operators, and then apply these results to obtain two classes of algorithms for affine variational inequalities. The second class resembles classical matrix splitting, but has a novel "under-relaxation" step, and converges under more general conditions. In particular, the convergence proofs do not require the affine operator to be symmetric. We specialize our matrix-splitting-like method to discrete-time optimal control problems formulated as extended linear-quadratic programs in the manner advocated by Rockafellar and Wets. The result is a highly parallel algorithm, which we implement and test on the Connection Machine CM-5 computer family. where M is a given n fi n matrix and q is a vector from &lt; n . We denote this problem avi(M; q; B). A common and important special case occurs when B is a box, that is, This paper is restricted to the monotone case of avi(M; q; B), where M is positive semidefinite, although not necessarily symmetric. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> H. Brezis. Operateurs Maximaux Monotones et Semi-Groupes de Contractions dans les Espaces de Hilbert. </editor> <publisher> North-Holland, </publisher> <year> 1973. </year>
Reference-contexts: Finding a zero of a maximal monotone operator T , that is, some x 2 &lt; n with 0 2 T (x), is a fundamental problem <ref> [1, 30, 39] </ref> that generalizes not only all of lower semicontinuous convex optimization, but also monotone variational inequalities. Some salient facts regarding monotone and maximal monotone operators on &lt; n are collected below. Proposition 1 1. <p> If Q is an invertible matrix, then the operator Q &gt; T Q = (Q 1 x; Q &gt; y) j (x; y) 2 T is (maximal) monotone if and only if T is (maximal) monotone. Proof. Statement 1 is trivial. Statement 2 is shown in <ref> [1, 23] </ref>. For the first part of statement 3, suppose z = x + y = x 0 + y 0 , where (x; y); (x 0 ; y 0 ) 2 T . <p> For further details, refer to [7, Section 3.6]. If we relabel the discretized variables using u <ref> [1] </ref> = u L u [t] = u (t t1 ); t = 2; : : : ; N then the resulting discretized problem is min t=1 p [t] ; u [t] + 2 u [t] ; P [t] u [t] c [t] ; w [t] + V [t] Q [t] <p> For example, letting ffi = (t R t L )=(N 1), we have P <ref> [1] </ref> = P L , U [1] = U L , P [t] = ffi ~ P (t t1 ) and U [t] = U (t t1 ) for t = 2; : : : ; N , c [t] = ffi~c (t t ) for t = 1; : : <p> For example, letting ffi = (t R t L )=(N 1), we have P <ref> [1] </ref> = P L , U [1] = U L , P [t] = ffi ~ P (t t1 ) and U [t] = U (t t1 ) for t = 2; : : : ; N , c [t] = ffi~c (t t ) for t = 1; : : : ; N 1, and c <p> After some simplifications, the discretized saddle-point problem in the variables x [t] = (u [t] ; y [t] ; v [t] ; w [t] ), t = 1; 2; : : : ; N is equivalent to avi (M; q; B), with M = 6 6 6 6 6 M <ref> [1] </ref> L &gt; L [1] M [2] L &gt; L [2] M [3] L &gt; . . . [N1] 3 7 7 7 7 5 q = q [1] q [2] : : : q [N] ; (43) N Y where the submatrices M [t] , L [t] , and q <p> discretized saddle-point problem in the variables x [t] = (u [t] ; y [t] ; v [t] ; w [t] ), t = 1; 2; : : : ; N is equivalent to avi (M; q; B), with M = 6 6 6 6 6 M <ref> [1] </ref> L &gt; L [1] M [2] L &gt; L [2] M [3] L &gt; . . . [N1] 3 7 7 7 7 5 q = q [1] q [2] : : : q [N] ; (43) N Y where the submatrices M [t] , L [t] , and q [t] take the respective <p> 2; : : : ; N is equivalent to avi (M; q; B), with M = 6 6 6 6 6 M <ref> [1] </ref> L &gt; L [1] M [2] L &gt; L [2] M [3] L &gt; . . . [N1] 3 7 7 7 7 5 q = q [1] q [2] : : : q [N] ; (43) N Y where the submatrices M [t] , L [t] , and q [t] take the respective forms M [t] = 6 6 P [t] B &gt; B [t] I I C &gt; 3 7 5 L [t] = 6 6 <p> the respective forms M [t] = 6 6 P [t] B &gt; B [t] I I C &gt; 3 7 5 L [t] = 6 6 D &gt; A [t] 7 7 ; (46) h [t] b &gt; [t] c &gt; i The matrices M [t] are square, with M <ref> [1] </ref> having dimension h L + 2s + m, M [N] having dimension h + 2s + m R , and the rest having dimension h + 2s + m. <p> Although this last property makes all the matrices M [t] identical, except for M <ref> [1] </ref> and M [N] , this characteristic does not appear to make the generated problems particularly easy to solve, and our code takes no advantage of it. Thus, all of out test problems have the dynamical structure that is described in [43]. <p> However, one can isolate the block tridiagonal structure of the problem by taking the approach of Proposition 8 with M 1 = 6 6 4 ffM [2] ffM [N] 7 7 5 M 2 = 6 6 4 <ref> [1] </ref> [2] L [N1] (1 ff)M [N] 7 7 5 and ff being some scalar in the range [0; 1]. If we choose H to be block diagonal conformally with M 1 , then H + M 1 is also block diagonal. <p> tridiagonal structure of the problem by taking the approach of Proposition 8 with M 1 = 6 6 4 ffM [2] ffM [N] 7 7 5 M 2 = 6 6 4 [1] [2] L [N1] (1 ff)M [N] 7 7 5 and ff being some scalar in the range <ref> [0; 1] </ref>. If we choose H to be block diagonal conformally with M 1 , then H + M 1 is also block diagonal. <p> Then H + M 2 takes the form 2 6 6 6 6 6 6 6 6 6 6 6 H 1 H 2 H 3 H 4 <ref> [1] </ref> [1] H 1 A [1] H 2 H 3 H 4 [2] [2] H 1 A [2] H 2 . . . 7 7 7 7 7 7 7 7 7 7 7 5 This matrix is also block diagonal, and the linear system of (31) thus decomposes into many <p> Then H + M 2 takes the form 2 6 6 6 6 6 6 6 6 6 6 6 H 1 H 2 H 3 H 4 <ref> [1] </ref> [1] H 1 A [1] H 2 H 3 H 4 [2] [2] H 1 A [2] H 2 . . . 7 7 7 7 7 7 7 7 7 7 7 5 This matrix is also block diagonal, and the linear system of (31) thus decomposes into many smaller, <p> Then H + M 2 takes the form 2 6 6 6 6 6 6 6 6 6 6 6 H 1 H 2 H 3 H 4 <ref> [1] </ref> [1] H 1 A [1] H 2 H 3 H 4 [2] [2] H 1 A [2] H 2 . . . 7 7 7 7 7 7 7 7 7 7 7 5 This matrix is also block diagonal, and the linear system of (31) thus decomposes into many smaller, independent linear systems. <p> In practice, we also need some procedure for terminating the algorithm. Given that B is a "box" defined by ` 2 <ref> [1; +1) n and u 2 (1; +1] </ref> n as in (2), we may define, for any x; g 2 &lt; n , fl i (x i ; g i ; ` i ; u i ) = max f0; ` i x i ; x i u i ; min
Reference: [2] <author> M. Cao and M. C. Ferris. </author> <title> Lineality removal for copositive-plus normal maps. </title> <journal> Communications on Applied Nonlinear Analysis, </journal> <volume> 2 </volume> <pages> 1-10, </pages> <year> 1995. </year>
Reference-contexts: 2 k (I + cM ) 1 r B (z k ) cq z k B : (26) The following result is obtained by applying Proposition 6 and remarking that avi (M; q; B) must have a solution if M is positive semidefinite and feas (M; q; B) 6= ; <ref> [2] </ref>. 9 Proposition 7 Consider the problem avi (M; q; B) of (1), where M is positive semidefinite, q 2 &lt; n and B is a closed convex set, and assume feas (M; q; B) 6= ;. <p> problem in the variables x [t] = (u [t] ; y [t] ; v [t] ; w [t] ), t = 1; 2; : : : ; N is equivalent to avi (M; q; B), with M = 6 6 6 6 6 M [1] L &gt; L [1] M <ref> [2] </ref> L &gt; L [2] M [3] L &gt; . . . [N1] 3 7 7 7 7 5 q = q [1] q [2] : : : q [N] ; (43) N Y where the submatrices M [t] , L [t] , and q [t] take the respective forms M <p> x [t] = (u [t] ; y [t] ; v [t] ; w [t] ), t = 1; 2; : : : ; N is equivalent to avi (M; q; B), with M = 6 6 6 6 6 M [1] L &gt; L [1] M <ref> [2] </ref> L &gt; L [2] M [3] L &gt; . . . [N1] 3 7 7 7 7 5 q = q [1] q [2] : : : q [N] ; (43) N Y where the submatrices M [t] , L [t] , and q [t] take the respective forms M [t] = 6 6 <p> : : ; N is equivalent to avi (M; q; B), with M = 6 6 6 6 6 M [1] L &gt; L [1] M <ref> [2] </ref> L &gt; L [2] M [3] L &gt; . . . [N1] 3 7 7 7 7 5 q = q [1] q [2] : : : q [N] ; (43) N Y where the submatrices M [t] , L [t] , and q [t] take the respective forms M [t] = 6 6 P [t] B &gt; B [t] I I C &gt; 3 7 5 L [t] = 6 6 D &gt; <p> However, one can isolate the block tridiagonal structure of the problem by taking the approach of Proposition 8 with M 1 = 6 6 4 ffM <ref> [2] </ref> ffM [N] 7 7 5 M 2 = 6 6 4 [1] [2] L [N1] (1 ff)M [N] 7 7 5 and ff being some scalar in the range [0; 1]. <p> However, one can isolate the block tridiagonal structure of the problem by taking the approach of Proposition 8 with M 1 = 6 6 4 ffM <ref> [2] </ref> ffM [N] 7 7 5 M 2 = 6 6 4 [1] [2] L [N1] (1 ff)M [N] 7 7 5 and ff being some scalar in the range [0; 1]. If we choose H to be block diagonal conformally with M 1 , then H + M 1 is also block diagonal. <p> When ff = 1, a much simpler procedure is possible with some further assumptions on H. In 15 addition to H being block diagonal, that is, H = 6 6 4 H <ref> [2] </ref> H [N] 7 7 5 suppose that each H [t] has the block diagonal substructure H [t] = 6 6 4 [t] [t] [t] [t] 7 7 5 with the blocks conforming to those of M [t] . <p> Then H + M 2 takes the form 2 6 6 6 6 6 6 6 6 6 6 6 H 1 H 2 H 3 H 4 [1] [1] H 1 A [1] H 2 H 3 H 4 <ref> [2] </ref> [2] H 1 A [2] H 2 . . . 7 7 7 7 7 7 7 7 7 7 7 5 This matrix is also block diagonal, and the linear system of (31) thus decomposes into many smaller, independent linear systems. <p> Then H + M 2 takes the form 2 6 6 6 6 6 6 6 6 6 6 6 H 1 H 2 H 3 H 4 [1] [1] H 1 A [1] H 2 H 3 H 4 <ref> [2] </ref> [2] H 1 A [2] H 2 . . . 7 7 7 7 7 7 7 7 7 7 7 5 This matrix is also block diagonal, and the linear system of (31) thus decomposes into many smaller, independent linear systems. <p> Then H + M 2 takes the form 2 6 6 6 6 6 6 6 6 6 6 6 H 1 H 2 H 3 H 4 [1] [1] H 1 A [1] H 2 H 3 H 4 <ref> [2] </ref> [2] H 1 A [2] H 2 . . . 7 7 7 7 7 7 7 7 7 7 7 5 This matrix is also block diagonal, and the linear system of (31) thus decomposes into many smaller, independent linear systems. <p> Then for each of these subproblems there will be a final complementary basis Y k [t] . Consider their 16 concatenation Y k : = (Y k <ref> [2] </ref> ; : : : ; Y k [N] ). Eventually, as fy k g and fx k g converge to some solution x fl of avi (M; q; B), Y k should stabilize at some complementary basis corresponding to x fl .
Reference: [3] <author> C. H. Chen and O. L. Mangasarian. </author> <title> Smoothing methods for convex inequalities and linear complementarity problems. </title> <journal> Mathematical Programming, </journal> <volume> 78 </volume> <pages> 51-70, </pages> <year> 1995. </year>
Reference-contexts: = (u [t] ; y [t] ; v [t] ; w [t] ), t = 1; 2; : : : ; N is equivalent to avi (M; q; B), with M = 6 6 6 6 6 M [1] L &gt; L [1] M [2] L &gt; L [2] M <ref> [3] </ref> L &gt; . . . [N1] 3 7 7 7 7 5 q = q [1] q [2] : : : q [N] ; (43) N Y where the submatrices M [t] , L [t] , and q [t] take the respective forms M [t] = 6 6 P [t] <p> For purposes of comparison, we attempted to solve the problems using standard serial codes for affine variational inequalities on a SPARCStation 10/51 workstation with 32 megabytes of RAM. 19 The serial codes consisted of PATH [8], which implements a generalization of the SQP method with a piecewise-linear path search, SMOOTH <ref> [3] </ref>, a differentiable approximation method, and MILES [45], which implements a standard pivotal algorithm of the Lemke class. All the above codes are designed for nonlinear box-constrained variational inequalities, but can be applied to linear problems.
Reference: [4] <author> G. H.-G. Chen. </author> <title> Forward-Backward Splitting Techniques: Theory and Applications. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1994. </year>
Reference-contexts: Therefore, we can apply the contraction case of Proposition 3, and proceed as in case 1. 2 Note that the theory we have just presented covers only Peaceman- and Douglas-Rachford splitting, and does not subsume forward-backward <ref> [4, 14, 31, 49] </ref>, or "double-backward" [19, 31, 22] splitting schemes. These methods have a different and generally less attractive convergence theory. 2 Relating Monotone Affine Variational Inequalities to Monotone Operators We now relate the monotone operator theory we have just presented to monotone affine variational inequality problems. <p> Instead, we chose to maintain the original problem structure, and adopt the approach of (30)-(31), with sup f k g 1 k=0 &lt; 1. An approach based on forward-backward splitting is also possible <ref> [4] </ref>, but is subject to relatively stringent stepsize restrictions. Our approach is motivated by the existence of parallel solvers for block-tridiagonal systems of linear equations having structure like (42).
Reference: [5] <author> R. W. Cottle and G. B. Dantzig. </author> <title> Complementary pivot theory of mathematical programming. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 1 </volume> <pages> 103-125, </pages> <year> 1968. </year>
Reference-contexts: Define k : = q+(M 2 H)x k and suppose we solve each subproblem avi (H [t] + ffM [t] ; k [t] ; B [t] ) making up (30) by a standard pivotal method <ref> [5, 18] </ref>. Then for each of these subproblems there will be a final complementary basis Y k [t] . Consider their 16 concatenation Y k : = (Y k [2] ; : : : ; Y k [N] ).
Reference: [6] <author> R. W. Cottle, J. S. Pang, and R. E. Stone. </author> <title> The Linear Complementarity Problem. </title> <publisher> Academic Press, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference: [7] <author> S. P. Dirkse and M. C. Ferris. MCPLIB: </author> <title> A collection of nonlinear mixed complementarity problems. </title> <journal> Optimization Methods and Software, </journal> <volume> 5 </volume> <pages> 319-345, </pages> <year> 1995. </year>
Reference-contexts: For further details, refer to <ref> [7, Section 3.6] </ref>. <p> We generated test problems having precisely this structure using techniques already developed for MCPLIB <ref> [7] </ref>, which is based on the code written by Wright [50] at the University of Washington.
Reference: [8] <author> S. P. Dirkse and M. C. Ferris. </author> <title> The PATH solver: A non-monotone stabilization scheme for mixed complementarity problems. </title> <journal> Optimization Methods and Software, </journal> <volume> 5 </volume> <pages> 123-156, </pages> <year> 1995. </year>
Reference-contexts: rest of the algorithm might prove considerably more complicated. 3.4 Computational Results on CM-5E Systems For purposes of comparison, we attempted to solve the problems using standard serial codes for affine variational inequalities on a SPARCStation 10/51 workstation with 32 megabytes of RAM. 19 The serial codes consisted of PATH <ref> [8] </ref>, which implements a generalization of the SQP method with a piecewise-linear path search, SMOOTH [3], a differentiable approximation method, and MILES [45], which implements a standard pivotal algorithm of the Lemke class.
Reference: [9] <author> J. Douglas and H. H. Rachford. </author> <title> On the numerical solution of heat conduction problems in two and three space variables. </title> <journal> Transactions of the American Mathematical Society, </journal> <volume> 82 </volume> <pages> 421-439, </pages> <year> 1956. </year>
Reference-contexts: Splitting allows one to "pull apart" the structure of some problems so they can be attacked in a highly parallel manner. This approach is consonant with the general philosophy of splitting methods, dating back at least to 1950's alternating direction methods for banded systems of linear equations <ref> [9, 32] </ref>: one expresses the structure of ones problem as a composition of two less complicated structures, each simple enough to be attacked with the technology at hand.
Reference: [10] <author> A. Drud. CONOPT: </author> <title> A GRG code for large sparse dynamic nonlinear optimization problems. </title> <journal> Mathematical Programming, </journal> <volume> 31 </volume> <pages> 153-191, </pages> <year> 1985. </year> <month> 24 </month>
Reference-contexts: All the above codes are designed for nonlinear box-constrained variational inequalities, but can be applied to linear problems. By reformulating the test problems as quadratic programs [42], we also attempted to solve some of the problems with the generalized reduced gradient nonlinear programming code CONOPT <ref> [10] </ref>. The generated quadratic programs are convex, but they are guaranteed not to be strictly convex. Other techniques for the parallel solution of ELQP problems arising in optimal control have been described in the literature.
Reference: [11] <author> J. Eckstein. </author> <title> Splitting Methods for Monotone Operators, with Applications to Parallel Opti--mization. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Techonology, </institution> <address> Cambridge, MA, </address> <year> 1989. </year> <title> Report LIDS-TH-1877, Laboratory for Information and Decision Systems, </title> <publisher> M.I.T. </publisher>
Reference-contexts: The next statement is standard and can be found in [35, 36]. The result about the maximality of sums of operators is proven in [37]. Statement 6 can be established directly or may be found in <ref> [11, Proposition 3.1 (iv)] </ref>. 2 We now consider another, more familiar class of operators.
Reference: [12] <author> J. Eckstein and D. P. Bertsekas. </author> <title> On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators. </title> <journal> Mathematical Programming, </journal> <volume> 55 </volume> <pages> 293-318, </pages> <year> 1992. </year>
Reference-contexts: In view of Proposition 2, the first case is implied by <ref> [12, Theorem 3] </ref> applied to the maximal monotone operator T = N 1 [N ], where k = 2 k and c k = 1 for all k (note: this theorem generalizes many prior results, including [25, Theorem 3] and [38, Theorem 1]). <p> The more general case 0 &lt; inff k g 1 k=0 supf k g 1 k=0 &lt; 1 is addressed in <ref> [12] </ref>. If one allows Peaceman-Rachford steps, that is, one permits k to equal or approach 1, then N must be a contraction for Proposition 3 to guarantee convergence. <p> Since M 2 is not positive definite, we were constrained to keep k bounded away from 1. Thus, the algorithm implemented is a form of "Douglas-Rachford" (DR) splitting as described in <ref> [12] </ref>. We set to be fairly large because the effort involved in factoring the block tridiagonal matrix M is considerable, and in particular much greater than that needed in the block tridiagonal back solves of step 3. In practice, the algorithm always terminates in step 8.
Reference: [13] <author> M. J. Flynn. </author> <title> Some computer organizations and their effectiveness. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-21:948-960, </volume> <year> 1972. </year>
Reference-contexts: Under the global CMF/CMSSL programming environment, the CM-5 functions essentially synchronously, like a giant array processor or SIMD computer <ref> [13] </ref>. The elemental processing units are not the "processing nodes", but the individual vector arithmetic units (VU's). Each processing node has four of these vector units. The vector units function synchronously, each performing similar operations on different data. The environment is "tuned" principally for speed of operations on long vectors.
Reference: [14] <author> D. Gabay. </author> <title> Applications of the method of multipliers to variational inequalities. </title> <editor> In M. Fortin and R. Glowinski, editors, </editor> <title> Augmented Lagrangian methods: Applications to the Solution of Boundary Value Problems. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1983. </year>
Reference-contexts: Therefore, we can apply the contraction case of Proposition 3, and proceed as in case 1. 2 Note that the theory we have just presented covers only Peaceman- and Douglas-Rachford splitting, and does not subsume forward-backward <ref> [4, 14, 31, 49] </ref>, or "double-backward" [19, 31, 22] splitting schemes. These methods have a different and generally less attractive convergence theory. 2 Relating Monotone Affine Variational Inequalities to Monotone Operators We now relate the monotone operator theory we have just presented to monotone affine variational inequality problems.
Reference: [15] <author> R. W. Hockney and C. R. Jesshope. </author> <title> Parallel Computers 2. Adam Hilger, </title> <address> Bristol, </address> <year> 1988. </year>
Reference-contexts: Similarly, with H block diagonal, the matrix H + M 2 in (31) will be block tridiagonal. Thus, (31) may be solved by block-oriented versions of the parallel cyclic reduction and substructuring methods described in <ref> [15, Section 5.4] </ref> and [16]. Briefly, block cyclic reduction involves using block Gaussian elimination to eliminate every other block of rows. The remaining, uneliminated rows form a block tridiagonal system half the size of the original one.
Reference: [16] <author> S. L. Johnsson. </author> <title> Solving tridiagonal systems on ensemble architectures. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 8 </volume> <pages> 475-489, </pages> <year> 1987. </year>
Reference-contexts: Similarly, with H block diagonal, the matrix H + M 2 in (31) will be block tridiagonal. Thus, (31) may be solved by block-oriented versions of the parallel cyclic reduction and substructuring methods described in [15, Section 5.4] and <ref> [16] </ref>. Briefly, block cyclic reduction involves using block Gaussian elimination to eliminate every other block of rows. The remaining, uneliminated rows form a block tridiagonal system half the size of the original one. <p> If ff &lt; 1, we must factor H + M 2 , which can be done using standard parallel techniques (e.g. block versions of the cyclic reduction method of <ref> [16] </ref>). Computing 0 : = M 2 x 0 requires an exchange of data between processors holding adjacent groups of time steps. 1. Because H is block-diagonal, the computation of k decomposes by the time step t. <p> Due to the algorithm's computational and communication regularity, we chose to implement it using the global data-parallel CM Fortran (CMF) language [47] and the CMSSL collection of numerical/scientific subroutines [48], which already contains sophisticated block-tridiagonal routines based on <ref> [16] </ref>. Under the global CMF/CMSSL programming environment, the CM-5 functions essentially synchronously, like a giant array processor or SIMD computer [13]. The elemental processing units are not the "processing nodes", but the individual vector arithmetic units (VU's). Each processing node has four of these vector units.
Reference: [17] <author> J. Lawrence and J. E. Spingarn. </author> <title> On fixed points of non-expansive piecewise isometric mappings. </title> <journal> Proceedings of the London Mathematical Society, </journal> <volume> 55(3) </volume> <pages> 605-624, </pages> <year> 1987. </year>
Reference-contexts: We now develop some observations from <ref> [17, 23] </ref> that reveal a deep connection between monotone and nonexpansive operators.
Reference: [18] <author> C. E. Lemke. </author> <title> On complementary pivot theory. </title> <editor> In G. B. Dantzig and A. F. Veinott, editors, </editor> <booktitle> Mathematics of the Decision Sciences: Part 1, volume 11 of Lectures in Applied Mathematics, </booktitle> <pages> pages 95-114. </pages> <publisher> American Mathematical Society, </publisher> <address> Providence, </address> <year> 1968. </year>
Reference-contexts: Define k : = q+(M 2 H)x k and suppose we solve each subproblem avi (H [t] + ffM [t] ; k [t] ; B [t] ) making up (30) by a standard pivotal method <ref> [5, 18] </ref>. Then for each of these subproblems there will be a final complementary basis Y k [t] . Consider their 16 concatenation Y k : = (Y k [2] ; : : : ; Y k [N] ).
Reference: [19] <author> P.-L. Lions. </author> <title> Une Methode iterative de resolution d'une inequation variationnelle. </title> <journal> Israel Journal of Mathematics, </journal> <volume> 31(2) </volume> <pages> 204-208, </pages> <year> 1978. </year>
Reference-contexts: Therefore, we can apply the contraction case of Proposition 3, and proceed as in case 1. 2 Note that the theory we have just presented covers only Peaceman- and Douglas-Rachford splitting, and does not subsume forward-backward [4, 14, 31, 49], or "double-backward" <ref> [19, 31, 22] </ref> splitting schemes. These methods have a different and generally less attractive convergence theory. 2 Relating Monotone Affine Variational Inequalities to Monotone Operators We now relate the monotone operator theory we have just presented to monotone affine variational inequality problems.
Reference: [20] <author> P.-L. Lions and B. Mercier. </author> <title> Splitting methods for the sum of two nonlinear operators. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16 </volume> <pages> 964-979, </pages> <year> 1979. </year>
Reference-contexts: Therefore, taking square roots, (17) implies kN (z) N (z 0 )k ff kz z 0 k for every z; z 0 2 dom N. 2 Conditions (11)-(12) are the same as those employed in the rate of convergence results in works such as <ref> [20] </ref> and [21]. 1.2 Rachford-Class Splitting Theory In many cases, the operator N [T ], or essentially equivalently (I + T ) 1 , may be too difficult to evaluate to make direct application of Proposition 3 practical. <p> k (y k+1 a k+1 ) + (1 k )(x k + b k ) = 2 k y k+1 + (1 2 k )x k + b k : The case k 1 is known as "Peaceman-Rachford" splitting, while the case k 1=2 is traditionally known as "Douglas-Rachford" splitting <ref> [20] </ref>. The more general case 0 &lt; inff k g 1 k=0 supf k g 1 k=0 &lt; 1 is addressed in [12]. If one allows Peaceman-Rachford steps, that is, one permits k to equal or approach 1, then N must be a contraction for Proposition 3 to guarantee convergence.
Reference: [21] <author> P. Mahey, S. Oualibouch, and D. T. Pham. </author> <title> Proximal decomposition on the graph of a maximal monotone operator. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 5 </volume> <pages> 454-466, </pages> <year> 1995. </year>
Reference-contexts: Therefore, taking square roots, (17) implies kN (z) N (z 0 )k ff kz z 0 k for every z; z 0 2 dom N. 2 Conditions (11)-(12) are the same as those employed in the rate of convergence results in works such as [20] and <ref> [21] </ref>. 1.2 Rachford-Class Splitting Theory In many cases, the operator N [T ], or essentially equivalently (I + T ) 1 , may be too difficult to evaluate to make direct application of Proposition 3 practical.
Reference: [22] <author> P. Mahey and D. T. Pham. </author> <title> Partial regularization of the sum of two maximal monotone operators. </title> <journal> RAIRO Modelisation et Analyse Numerique, </journal> <volume> 27 </volume> <pages> 375-392, </pages> <year> 1993. </year>
Reference-contexts: Therefore, we can apply the contraction case of Proposition 3, and proceed as in case 1. 2 Note that the theory we have just presented covers only Peaceman- and Douglas-Rachford splitting, and does not subsume forward-backward [4, 14, 31, 49], or "double-backward" <ref> [19, 31, 22] </ref> splitting schemes. These methods have a different and generally less attractive convergence theory. 2 Relating Monotone Affine Variational Inequalities to Monotone Operators We now relate the monotone operator theory we have just presented to monotone affine variational inequality problems.
Reference: [23] <author> G. J. Minty. </author> <title> Monotone (nonlinear) operators in Hilbert space. </title> <journal> Duke Mathematics Journal, </journal> <volume> 29 </volume> <pages> 341-346, </pages> <year> 1962. </year>
Reference-contexts: If Q is an invertible matrix, then the operator Q &gt; T Q = (Q 1 x; Q &gt; y) j (x; y) 2 T is (maximal) monotone if and only if T is (maximal) monotone. Proof. Statement 1 is trivial. Statement 2 is shown in <ref> [1, 23] </ref>. For the first part of statement 3, suppose z = x + y = x 0 + y 0 , where (x; y); (x 0 ; y 0 ) 2 T . <p> Then 0 hx x 0 ; y y 0 i = hx x 0 ; (z x) (z x 0 )i = kx x 0 k , implying x = x 0 and y = y 0 . The rest of the statement follows from statement 2 (see also <ref> [23] </ref> for an equivalent result). The next statement is standard and can be found in [35, 36]. The result about the maximality of sums of operators is proven in [37]. <p> We now develop some observations from <ref> [17, 23] </ref> that reveal a deep connection between monotone and nonexpansive operators.
Reference: [24] <author> K. G. Murty. </author> <title> Linear Complementarity, Linear and Nonlinear Programming. </title> <address> Helderman-Verlag, Berlin, </address> <year> 1988. </year>
Reference: [25] <author> Z. Opial. </author> <title> Weak convergence of the sequence of successive approximations for nonexpansive mappings. </title> <journal> Bulletin of the American Mathematical Society, </journal> <volume> 73 </volume> <pages> 591-597, </pages> <year> 1967. </year>
Reference-contexts: In view of Proposition 2, the first case is implied by [12, Theorem 3] applied to the maximal monotone operator T = N 1 [N ], where k = 2 k and c k = 1 for all k (note: this theorem generalizes many prior results, including <ref> [25, Theorem 3] </ref> and [38, Theorem 1]).
Reference: [26] <author> J. S. Pang. </author> <title> On the convergence of a basic iterative method for the implicit complementarity problem. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 37 </volume> <pages> 149-162, </pages> <year> 1982. </year>
Reference-contexts: The method may be construed as standard matrix splitting (see e.g. <ref> [26, 27, 28] </ref>), with the computation (34) replacing the usual under-relaxation step. Proposition 8 also guarantees convergence when f k g is allowed to approach 1, so long as M , and hence M 2 = M skew (B), is positive definite.
Reference: [27] <author> J. S. Pang. </author> <title> Necessary and sufficient conditions for the convergence of iterative methods for the linear complementarity problem. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 42 </volume> <pages> 1-17, </pages> <year> 1984. </year>
Reference-contexts: The method may be construed as standard matrix splitting (see e.g. <ref> [26, 27, 28] </ref>), with the computation (34) replacing the usual under-relaxation step. Proposition 8 also guarantees convergence when f k g is allowed to approach 1, so long as M , and hence M 2 = M skew (B), is positive definite.
Reference: [28] <author> J. S. Pang. </author> <title> More results on the convergence of iterative methods for the symmetric linear complementarity problem. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 49 </volume> <pages> 107-134, </pages> <year> 1986. </year>
Reference-contexts: The method may be construed as standard matrix splitting (see e.g. <ref> [26, 27, 28] </ref>), with the computation (34) replacing the usual under-relaxation step. Proposition 8 also guarantees convergence when f k g is allowed to approach 1, so long as M , and hence M 2 = M skew (B), is positive definite.
Reference: [29] <author> J. F. A. D. Pantoja and D. Q. Mayne. </author> <title> Sequential quadratic programming algorithm for discrete optimal control problems with control inequality constraints. </title> <journal> International Journal on Control, </journal> <volume> 53 </volume> <pages> 823-836, </pages> <year> 1991. </year>
Reference-contexts: Other techniques for the parallel solution of ELQP problems arising in optimal control have been described in the literature. Wright [51] develops an interior point SQP approach with special adaptation of the (banded) linear algebra for solving the generated subproblems. Pantoja and Mayne <ref> [29] </ref> also use an SQP approach, but exploit the structure at a higher level. Both of these techniques are essentially comparable to the PATH method, but with special implementation to exploit the problem structure. None of these authors report results on the structured ELQP's outlined in Section 3.1.
Reference: [30] <author> D. Pascali and S. Sburlan. </author> <title> Nonlinear Mappings of Monotone Type. </title> <address> Editura Academeie, </address> <year> 1978. </year>
Reference-contexts: Finding a zero of a maximal monotone operator T , that is, some x 2 &lt; n with 0 2 T (x), is a fundamental problem <ref> [1, 30, 39] </ref> that generalizes not only all of lower semicontinuous convex optimization, but also monotone variational inequalities. Some salient facts regarding monotone and maximal monotone operators on &lt; n are collected below. Proposition 1 1.
Reference: [31] <author> G. B. Passty. </author> <title> Ergodic convergence to a zero of the sum of monotone operators in Hilbert space. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 72 </volume> <pages> 383-390, </pages> <year> 1979. </year>
Reference-contexts: Therefore, we can apply the contraction case of Proposition 3, and proceed as in case 1. 2 Note that the theory we have just presented covers only Peaceman- and Douglas-Rachford splitting, and does not subsume forward-backward <ref> [4, 14, 31, 49] </ref>, or "double-backward" [19, 31, 22] splitting schemes. These methods have a different and generally less attractive convergence theory. 2 Relating Monotone Affine Variational Inequalities to Monotone Operators We now relate the monotone operator theory we have just presented to monotone affine variational inequality problems. <p> Therefore, we can apply the contraction case of Proposition 3, and proceed as in case 1. 2 Note that the theory we have just presented covers only Peaceman- and Douglas-Rachford splitting, and does not subsume forward-backward [4, 14, 31, 49], or "double-backward" <ref> [19, 31, 22] </ref> splitting schemes. These methods have a different and generally less attractive convergence theory. 2 Relating Monotone Affine Variational Inequalities to Monotone Operators We now relate the monotone operator theory we have just presented to monotone affine variational inequality problems.
Reference: [32] <author> D. W. Peaceman and H. H. Rachford. </author> <title> The numerical solution of parabolic and elliptic differential equations. </title> <journal> SIAM Journal, </journal> <volume> 3 </volume> <pages> 28-41, </pages> <year> 1955. </year>
Reference-contexts: Splitting allows one to "pull apart" the structure of some problems so they can be attacked in a highly parallel manner. This approach is consonant with the general philosophy of splitting methods, dating back at least to 1950's alternating direction methods for banded systems of linear equations <ref> [9, 32] </ref>: one expresses the structure of ones problem as a composition of two less complicated structures, each simple enough to be attacked with the technology at hand.
Reference: [33] <author> B. T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: The second case follows from direct application of <ref> [33, Lemma 3, page 45] </ref>. 2 Note that 1 2 N [T ] = f (x + y; x) j (x; y) 2 T g = (I + T ) 1 : Thus, if we leave k fixed at 1=2, we are executing the iteration z k+1 (I + T )
Reference: [34] <author> S. M. Robinson. </author> <title> Normal maps induced by linear transformations. </title> <journal> Mathematics of Operations Research, </journal> <volume> 17 </volume> <pages> 691-714, </pages> <year> 1992. </year>
Reference-contexts: Since H is positive definite and M 1 is positive semidefinite, H +M 1 is positive definite, and the "sol" denotes a unique point <ref> [34, Theorem 4.3] </ref> (this result also confirms that y k+1 must exist).
Reference: [35] <author> R. T. Rockafellar. </author> <title> Characterization of the subdifferentials of convex functions. </title> <journal> Pacific Journal of Mathematics, </journal> <volume> 17(3) </volume> <pages> 497-510, </pages> <year> 1966. </year>
Reference-contexts: The rest of the statement follows from statement 2 (see also [23] for an equivalent result). The next statement is standard and can be found in <ref> [35, 36] </ref>. The result about the maximality of sums of operators is proven in [37]. Statement 6 can be established directly or may be found in [11, Proposition 3.1 (iv)]. 2 We now consider another, more familiar class of operators.
Reference: [36] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1970. </year>
Reference-contexts: If T is maximal, then such a representation must exist. 4. If f : &lt; n ! &lt; [ f+1g is a lower semi-continuous convex function, then the subgradient operator @f (see for example <ref> [36] </ref>) is maximal monotone. 5. If T 1 and T 2 are monotone operators, then T 1 + T 2 is also monotone. <p> If T 1 and T 2 are both maximal and ri (dom T 1 ) " ri (dom T 2 ) 6= ;, where "ri" denotes the relative interior (see for example <ref> [36] </ref>), then T 1 + T 2 is also maximal. 6. If Q is an invertible matrix, then the operator Q &gt; T Q = (Q 1 x; Q &gt; y) j (x; y) 2 T is (maximal) monotone if and only if T is (maximal) monotone. Proof. <p> The rest of the statement follows from statement 2 (see also [23] for an equivalent result). The next statement is standard and can be found in <ref> [35, 36] </ref>. The result about the maximality of sums of operators is proven in [37]. Statement 6 can be established directly or may be found in [11, Proposition 3.1 (iv)]. 2 We now consider another, more familiar class of operators.
Reference: [37] <author> R. T. Rockafellar. </author> <title> On the maximality of sums of nonlinear monotone operators. </title> <journal> Transactions of the American Mathematical Society, </journal> <volume> 149 </volume> <pages> 75-88, </pages> <year> 1970. </year>
Reference-contexts: The rest of the statement follows from statement 2 (see also [23] for an equivalent result). The next statement is standard and can be found in [35, 36]. The result about the maximality of sums of operators is proven in <ref> [37] </ref>. Statement 6 can be established directly or may be found in [11, Proposition 3.1 (iv)]. 2 We now consider another, more familiar class of operators.
Reference: [38] <author> R. T. Rockafellar. </author> <title> Monotone operators and the proximal point algorithm. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 14 </volume> <pages> 877-898, </pages> <year> 1976. </year>
Reference-contexts: view of Proposition 2, the first case is implied by [12, Theorem 3] applied to the maximal monotone operator T = N 1 [N ], where k = 2 k and c k = 1 for all k (note: this theorem generalizes many prior results, including [25, Theorem 3] and <ref> [38, Theorem 1] </ref>). <p> + y; x) j (x; y) 2 T g = (I + T ) 1 : Thus, if we leave k fixed at 1=2, we are executing the iteration z k+1 (I + T ) 1 (z k ) ; which is a form of the well-known proximal point algorithm <ref> [38] </ref>. The general form of the proximal point algorithm differs only in allowing iterations of the form z k+1 (I + c k T ) 1 (z k ) ; where fc k g is a sequence of positive scalars bounded away from zero.
Reference: [39] <author> R. T. Rockafellar. </author> <title> Monotone operators and augmented Lagrangian methods in nonlinear programming. </title> <editor> In O. L. Mangasarian, R. R. Meyer, and S. M. Robinson, editors, </editor> <booktitle> Nonlinear Programming 3, </booktitle> <pages> pages 1-26. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1978. </year>
Reference-contexts: Finding a zero of a maximal monotone operator T , that is, some x 2 &lt; n with 0 2 T (x), is a fundamental problem <ref> [1, 30, 39] </ref> that generalizes not only all of lower semicontinuous convex optimization, but also monotone variational inequalities. Some salient facts regarding monotone and maximal monotone operators on &lt; n are collected below. Proposition 1 1.
Reference: [40] <author> R. T. Rockafellar. </author> <title> Linear-quadratic programming and optimal control. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 25 </volume> <pages> 781-814, </pages> <year> 1987. </year>
Reference-contexts: We draw an example of this kind of decomposition from the field of discrete-time optimal control. Our formulation is based on the notion of extended linear-quadratic programming as introduced by Rockafellar <ref> [40, 41] </ref>. 3.1 Discrete-Time Optimal Control as an Extended Linear-Quadratic Problem Many optimal control problems have essentially linear dynamics that evolve over a fixed time interval [t L ; t R ] according to an underlying differential equation dw (t ) = ~ A (t )w (t ) + ~ B <p> Many different penalizations of the state and control "constraints" can be added using particular choices of V and Q. For example, if V = &lt; m + and Q = 0, then VQ (z) = 0 if z 0, and +1 otherwise, effectively generating inequality constraints. <ref> [40] </ref> gives a variety of choices of V and Q, showing how to model terminal state conditions and linear-quadratic regulator problems. <p> Essentially, this dual problem is also a control problem where v are the (discretized) dual control variables and y are the dual state variables, It is well-known that determining a saddle point of (41) is equivalent to solving (40) under a suitably mild qualification <ref> [40] </ref>.
Reference: [41] <author> R. T. Rockafellar. </author> <title> Multistage convex programming and discrete-time optimal control. </title> <journal> Control and Cybernetics, </journal> <volume> 17(2-3):225-245, </volume> <year> 1988. </year>
Reference-contexts: We draw an example of this kind of decomposition from the field of discrete-time optimal control. Our formulation is based on the notion of extended linear-quadratic programming as introduced by Rockafellar <ref> [40, 41] </ref>. 3.1 Discrete-Time Optimal Control as an Extended Linear-Quadratic Problem Many optimal control problems have essentially linear dynamics that evolve over a fixed time interval [t L ; t R ] according to an underlying differential equation dw (t ) = ~ A (t )w (t ) + ~ B
Reference: [42] <author> R. T. Rockafellar and R. J.-B. Wets. </author> <title> A Lagrangian finite generation technique for solving linear-quadratic problems in stochastic programming. </title> <journal> Mathematical Programming Study, </journal> <volume> 28 </volume> <pages> 63-93, </pages> <year> 1986. </year>
Reference-contexts: All the above codes are designed for nonlinear box-constrained variational inequalities, but can be applied to linear problems. By reformulating the test problems as quadratic programs <ref> [42] </ref>, we also attempted to solve some of the problems with the generalized reduced gradient nonlinear programming code CONOPT [10]. The generated quadratic programs are convex, but they are guaranteed not to be strictly convex. <p> There are many other application areas for which the techniques described in this paper would be appropriate. A particular class of applications for future research is the field of linear-quadratic problems arising in stochastic programming <ref> [42, 44] </ref>. Acknowledgements: We would like to thank Paul Bay, formerly of Thinking Machines Corporation, for helping us with the CMSSL block tridiagonal linear system routines.
Reference: [43] <author> R. T. Rockafellar and R. J.-B. Wets. </author> <title> Generalized linear-quadratic problems of deterministic and stochastic optimal control in discrete time. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 28 </volume> <pages> 810-822, </pages> <year> 1990. </year>
Reference-contexts: Thus, all of out test problems have the dynamical structure that is described in <ref> [43] </ref>. Only the parameters referred to within this structure have been randomized. Furthermore, the assumed structure is very general, and includes many standard problems occurring in optimal control. 3.2 Parallel Application of Splitting Consider the application of splitting algorithms to MAVI's with the structure described in (42)-(47).
Reference: [44] <author> R. T. Rockafellar and R. J.-B. Wets. </author> <title> Scenarios and policy aggregation in optimization under uncertainty. </title> <journal> Mathematics of Operations Research, </journal> <volume> 10 </volume> <pages> 119-147, </pages> <year> 1991. </year> <month> 26 </month>
Reference-contexts: There are many other application areas for which the techniques described in this paper would be appropriate. A particular class of applications for future research is the field of linear-quadratic problems arising in stochastic programming <ref> [42, 44] </ref>. Acknowledgements: We would like to thank Paul Bay, formerly of Thinking Machines Corporation, for helping us with the CMSSL block tridiagonal linear system routines.
Reference: [45] <author> T. F. Rutherford. MILES: </author> <title> A mixed inequality and nonlinear equation solver. </title> <type> Working Paper, </type> <institution> Department of Economics, University of Colorado, Boulder, </institution> <year> 1993. </year>
Reference-contexts: solve the problems using standard serial codes for affine variational inequalities on a SPARCStation 10/51 workstation with 32 megabytes of RAM. 19 The serial codes consisted of PATH [8], which implements a generalization of the SQP method with a piecewise-linear path search, SMOOTH [3], a differentiable approximation method, and MILES <ref> [45] </ref>, which implements a standard pivotal algorithm of the Lemke class. All the above codes are designed for nonlinear box-constrained variational inequalities, but can be applied to linear problems.
Reference: [46] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> Connection Machine CM-5 Technical Summary, </note> <year> 1993. </year>
Reference-contexts: The computation and communication requirements of the algorithm are highly regular, with the possible exception of finding y k+1 [t] in step 1. 18 3.3 Implementation for the CM-5 Family We implemented the algorithm on the CM-5 family of parallel computers <ref> [46] </ref>. The modified "fat tree" communication topology of the CM-5 is well-suited to parallel tridiagonal factorization, shift operations, and global reductions.
Reference: [47] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Language Reference Manual, </note> <year> 1994. </year>
Reference-contexts: The modified "fat tree" communication topology of the CM-5 is well-suited to parallel tridiagonal factorization, shift operations, and global reductions. Due to the algorithm's computational and communication regularity, we chose to implement it using the global data-parallel CM Fortran (CMF) language <ref> [47] </ref> and the CMSSL collection of numerical/scientific subroutines [48], which already contains sophisticated block-tridiagonal routines based on [16]. Under the global CMF/CMSSL programming environment, the CM-5 functions essentially synchronously, like a giant array processor or SIMD computer [13].
Reference: [48] <institution> Thinking Machines Corporation, Cambridge, MA. CMSSL for CM Fortran, </institution> <year> 1994. </year>
Reference-contexts: The modified "fat tree" communication topology of the CM-5 is well-suited to parallel tridiagonal factorization, shift operations, and global reductions. Due to the algorithm's computational and communication regularity, we chose to implement it using the global data-parallel CM Fortran (CMF) language [47] and the CMSSL collection of numerical/scientific subroutines <ref> [48] </ref>, which already contains sophisticated block-tridiagonal routines based on [16]. Under the global CMF/CMSSL programming environment, the CM-5 functions essentially synchronously, like a giant array processor or SIMD computer [13]. The elemental processing units are not the "processing nodes", but the individual vector arithmetic units (VU's).
Reference: [49] <author> P. Tseng. </author> <title> Applications of a splitting algorithm to decomposition in convex programming and variational inequalities. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 29 </volume> <pages> 119-138, </pages> <year> 1991. </year>
Reference-contexts: Therefore, we can apply the contraction case of Proposition 3, and proceed as in case 1. 2 Note that the theory we have just presented covers only Peaceman- and Douglas-Rachford splitting, and does not subsume forward-backward <ref> [4, 14, 31, 49] </ref>, or "double-backward" [19, 31, 22] splitting schemes. These methods have a different and generally less attractive convergence theory. 2 Relating Monotone Affine Variational Inequalities to Monotone Operators We now relate the monotone operator theory we have just presented to monotone affine variational inequality problems.
Reference: [50] <author> S. E. Wright. Dynfgm: </author> <title> Dynamic finite generation method. </title> <type> Technical report, </type> <institution> Department of Mathematics, University of Washington, </institution> <address> Seattle, Washington, </address> <year> 1989. </year>
Reference-contexts: We generated test problems having precisely this structure using techniques already developed for MCPLIB [7], which is based on the code written by Wright <ref> [50] </ref> at the University of Washington.
Reference: [51] <author> S. J. Wright. </author> <title> Solution of discrete-time optimal control problems on parallel computers. </title> <journal> Parallel Computing, </journal> <volume> 16 </volume> <pages> 221-238, </pages> <year> 1990. </year>
Reference-contexts: The generated quadratic programs are convex, but they are guaranteed not to be strictly convex. Other techniques for the parallel solution of ELQP problems arising in optimal control have been described in the literature. Wright <ref> [51] </ref> develops an interior point SQP approach with special adaptation of the (banded) linear algebra for solving the generated subproblems. Pantoja and Mayne [29] also use an SQP approach, but exploit the structure at a higher level.
Reference: [52] <author> C. Y. Zhu. </author> <title> On the primal-dual steepest descent algorithm for extended linear-quadratic programming. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 5 </volume> <pages> 114-128, </pages> <year> 1995. </year>
Reference-contexts: Both of these techniques are essentially comparable to the PATH method, but with special implementation to exploit the problem structure. None of these authors report results on the structured ELQP's outlined in Section 3.1. Zhu and Rockafellar <ref> [52, 53] </ref> consider the problem as an ELQP and apply forward-backward splitting techniques exploiting the underlying duality structure. Although they report results for problems similar to the well-conditioned ones that we consider, their implementation is serial.
Reference: [53] <author> C. Y. Zhu and R. T. Rockafellar. </author> <title> Primal-dual projected gradient algorithms for extended linear-quadratic programming. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 3 </volume> <pages> 751-783, </pages> <booktitle> 1993. </booktitle> <volume> 27 28 29 30 31 </volume>
Reference-contexts: Both of these techniques are essentially comparable to the PATH method, but with special implementation to exploit the problem structure. None of these authors report results on the structured ELQP's outlined in Section 3.1. Zhu and Rockafellar <ref> [52, 53] </ref> consider the problem as an ELQP and apply forward-backward splitting techniques exploiting the underlying duality structure. Although they report results for problems similar to the well-conditioned ones that we consider, their implementation is serial.
References-found: 53

