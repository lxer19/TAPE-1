URL: http://www.cs.rutgers.edu/~uli/cs671/bennett-tr95.ps
Refering-URL: http://www.cs.rutgers.edu/~uli/cs671/index.html
Root-URL: http://www.cs.rutgers.edu
Affiliation: Silicon Graphics, Inc.  
Note: The research described herein has been supported by NASA-Ames under grants NAG2-248 and NAGW 419, using equipment supplied by  
Abstract: PERFORMANCE FACTORS FOR SUPERSCALAR PROCESSORS James E. Bennett Michael J. Flynn Technical Report No. CSL-TR-95-661 February 1995 
Abstract-found: 1
Intro-found: 1
Reference: [AKT86] <author> R. D. Acosta, J. Kjelstrup, and H. C. Torng. </author> <title> An instruction issuing approach to enhancing performance in multiple functional unit processors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 35 </volume> <pages> 815-828, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: Various techniques for dynamic scheduling have been proposed in <ref> [WS84, PHS85, AKT86, Soh90, DT92] </ref>. Dynamic scheduling has even been proposed for VLIW processors [Rau93]. This paper inquires into the factors that affect the performance of such a machine.
Reference: [BF94] <author> J. E. Bennett and M. Flynn. </author> <title> Two case studies in latency tolerant architectures. </title> <type> Technical Report CSL-TR-94-639, </type> <institution> Stanford University, Computer Systems Laboratory, </institution> <month> Oc-tober </month> <year> 1994. </year>
Reference-contexts: The value of the counter saturates at 0 and 7 (no wraparound occurs). There is also a 256 entry table of branch target addresses for predicting the destination address of indirect branches. 2.2 The simulation process All the results in this study were obtained using the MXS simulation environment <ref> [BF94] </ref>. Figure 1 shows the simulation framework. The benchmark is first compiled and then it is executed by the simulator. During execution, the simulator maintains counts of various statistics of interest, such as number of loads, stores, cache misses, etc.
Reference: [BP91] <author> M. Butler and Y. Patt. </author> <title> The effect of real data cache behavior on the performance of a microarchitecture that supports dynamic scheduling. </title> <booktitle> In Proc. of the 24th International Symposium on Microarchitecture, </booktitle> <pages> pages 34-41, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Dynamic scheduling has been proposed as an effective technique for tolerating memory latency due to cache misses <ref> [CCMH91, BP91, GGH92] </ref>. To investigate the factors that contribute to good performance in the presence of memory latency, we study the performance for a fixed primary cache organization, as the branch prediction depth varies. <p> is, the organization of the cache (direct mapped vs. set associative) does not appear to impact the performance of the dynamically scheduled processor, other than through the variation in miss rate. 5 Discussion 5.1 Related work Other papers have studied the ability of dynamically scheduled machines to tolerate memory latency <ref> [CCMH91, BP91, GGH92] </ref>. The focus of [GGH92] was scientific applications running on multiprocessors with very long latencies. Static vs. dynamic scheduling is compared in [CCMH91] and the variation in performance with cache size and load latency are studied in [BP91]. <p> The focus of [GGH92] was scientific applications running on multiprocessors with very long latencies. Static vs. dynamic scheduling is compared in [CCMH91] and the variation in performance with cache size and load latency are studied in <ref> [BP91] </ref>. While [BP91] and this paper both study scalar processors running a mix of applications, this paper examines the factors that allow dynamically scheduled machines to tolerate latency. Also [BP91] studied processors with a smaller instruction window and longer memory latencies. <p> The focus of [GGH92] was scientific applications running on multiprocessors with very long latencies. Static vs. dynamic scheduling is compared in [CCMH91] and the variation in performance with cache size and load latency are studied in <ref> [BP91] </ref>. While [BP91] and this paper both study scalar processors running a mix of applications, this paper examines the factors that allow dynamically scheduled machines to tolerate latency. Also [BP91] studied processors with a smaller instruction window and longer memory latencies. <p> vs. dynamic scheduling is compared in [CCMH91] and the variation in performance with cache size and load latency are studied in <ref> [BP91] </ref>. While [BP91] and this paper both study scalar processors running a mix of applications, this paper examines the factors that allow dynamically scheduled machines to tolerate latency. Also [BP91] studied processors with a smaller instruction window and longer memory latencies.
Reference: [CCMH91] <author> P. Chang, W. Chen, S. Mahlke, and W. Hwu. </author> <title> Comparint static and dynamic code scheduling for multiple-instruction-issue processors. </title> <booktitle> In Proc. of the 24th International Symposium on Microarchitecture, </booktitle> <pages> pages 25-33, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Dynamic scheduling has been proposed as an effective technique for tolerating memory latency due to cache misses <ref> [CCMH91, BP91, GGH92] </ref>. To investigate the factors that contribute to good performance in the presence of memory latency, we study the performance for a fixed primary cache organization, as the branch prediction depth varies. <p> is, the organization of the cache (direct mapped vs. set associative) does not appear to impact the performance of the dynamically scheduled processor, other than through the variation in miss rate. 5 Discussion 5.1 Related work Other papers have studied the ability of dynamically scheduled machines to tolerate memory latency <ref> [CCMH91, BP91, GGH92] </ref>. The focus of [GGH92] was scientific applications running on multiprocessors with very long latencies. Static vs. dynamic scheduling is compared in [CCMH91] and the variation in performance with cache size and load latency are studied in [BP91]. <p> The focus of [GGH92] was scientific applications running on multiprocessors with very long latencies. Static vs. dynamic scheduling is compared in <ref> [CCMH91] </ref> and the variation in performance with cache size and load latency are studied in [BP91]. While [BP91] and this paper both study scalar processors running a mix of applications, this paper examines the factors that allow dynamically scheduled machines to tolerate latency.
Reference: [DT92] <author> H. Dwyer and H. C. Torng. </author> <title> An out-of-order superscalar processor with speculative execution and fast, precise interrupts. </title> <booktitle> In Proc. of the 25th International Symposium on Microarchitecture, </booktitle> <pages> pages 272-81, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Various techniques for dynamic scheduling have been proposed in <ref> [WS84, PHS85, AKT86, Soh90, DT92] </ref>. Dynamic scheduling has even been proposed for VLIW processors [Rau93]. This paper inquires into the factors that affect the performance of such a machine.
Reference: [FJ94] <author> K. I. Farkas and N. P. Jouppi. </author> <title> Complexity/performance tradeoffs with non-blocking loads. </title> <booktitle> In Proc. of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 211-22, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The performance is then determined for different assumptions about the bandwidth of the interface between the primary and secondary caches. This portion of the experiment is modeled after the study that Farkas and Jouppi did on scalar processors <ref> [FJ94] </ref>. Then these results are tested by varying the cache size, degree of associativity, etc., in order to determine the effectiveness of this scheme over a range of cache sizes and designs. <p> Because this was a small effect, and since following multiple paths would involve considerable expense to implement in a real machine, the remaining experiments assume that only a single path is followed. 4.3 Varying memory bandwidth In <ref> [FJ94] </ref>, the performance of a scalar processor was studied with varying organizations of a non-blocking cache. The terminology "fc=1", "fc=2", etc., was defined there to mean that the cache could support one outstanding fetch, two outstanding fetches, etc. <p> One good example is Compress, which shows an overall performance increase of 20% when going from one to two outstanding fetches. In contrast, in the scalar case (as reported in <ref> [FJ94] </ref>), there was minimal performance gain 13 Benchmark No lat. 16K 4-way 8K 4-way 8K Direct 8K 4-way 4 cycles 4 cycles 4 cycles 8 cycles compress 3.31 2.95 2.91 2.89 2.38 uncompress 3.17 3.08 2.93 2.85 2.72 espresso 2.97 2.96 2.90 2.80 2.83 sc 2.81 2.69 2.66 2.60 2.51 xlisp
Reference: [GGH92] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Hiding memory latency using dynamic scheduling in shared-memory multiprocessors. </title> <booktitle> In 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 22-33, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Dynamic scheduling has been proposed as an effective technique for tolerating memory latency due to cache misses <ref> [CCMH91, BP91, GGH92] </ref>. To investigate the factors that contribute to good performance in the presence of memory latency, we study the performance for a fixed primary cache organization, as the branch prediction depth varies. <p> is, the organization of the cache (direct mapped vs. set associative) does not appear to impact the performance of the dynamically scheduled processor, other than through the variation in miss rate. 5 Discussion 5.1 Related work Other papers have studied the ability of dynamically scheduled machines to tolerate memory latency <ref> [CCMH91, BP91, GGH92] </ref>. The focus of [GGH92] was scientific applications running on multiprocessors with very long latencies. Static vs. dynamic scheduling is compared in [CCMH91] and the variation in performance with cache size and load latency are studied in [BP91]. <p> The focus of <ref> [GGH92] </ref> was scientific applications running on multiprocessors with very long latencies. Static vs. dynamic scheduling is compared in [CCMH91] and the variation in performance with cache size and load latency are studied in [BP91].
Reference: [HMB + 93] <author> R. Hank, S. Mahlke, R. Bringmann, J. Gyllenhaal, and W. Hwu. </author> <title> Superblock formation using static program analysis. </title> <booktitle> In Proc. of the 26th International Symposium on Microarchitecture, </booktitle> <pages> pages 247-55, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Techniques such as loop unrolling and superblock formation <ref> [HMB + 93] </ref>, for example, ought to work well. Second, the fetch availability is determined by the rate at which the processor can execute the instructions in the instruction window. This, in turn, depends on the amount of parallelism that is present in the instruction window.
Reference: [HP90] <author> J. Hennessy and D. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Mor-gan Kaufmann Publishers, Inc., </publisher> <address> Palo Alto, CA, </address> <year> 1990. </year>
Reference-contexts: These factors are then used to explain the variations in performance that occur with different processor and memory system features. Because of cycle time constraints, the size of the primary cache is likely to remain limited <ref> [HP90] </ref> in future processors. The miss rate will accordingly remain significant. A secondary cache can be used to reduce the cost of primary cache misses, but in a processor capable of executing multiple instructions per cycle, these misses can still have a major impact.
Reference: [KH92] <author> Gerry Kane and Joe Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: The model is parameterized in terms of the bandwidth and latency of its various components. The processor timing is based on a shallow pipeline design, where cache accesses can be completed in a single cycle (see table 1). The timing is similar to the MIPS R3000 <ref> [KH92] </ref>, except for floating point latencies. All operations are assumed to be fully pipelined. Each cycle, up to eight instructions can be fetched, up to eight instructions can begin execution, and up to eight instructions can complete (write their results to the register file).
Reference: [PHS85] <author> Y. N. Patt, W. Hwu, and M. Shebanow. HPS, </author> <title> a new microarchitecture: Rationale and introduction. </title> <booktitle> In Proc. of the 18th Annual Workshop on Microprogramming, </booktitle> <pages> pages 103-108, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Various techniques for dynamic scheduling have been proposed in <ref> [WS84, PHS85, AKT86, Soh90, DT92] </ref>. Dynamic scheduling has even been proposed for VLIW processors [Rau93]. This paper inquires into the factors that affect the performance of such a machine.
Reference: [Rau93] <author> B. R. Rau. </author> <title> Dynamically scheduled VLIW processors. </title> <booktitle> In Proc. of the 26th International Symposium on Microarchitecture, </booktitle> <pages> pages 80-92, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Various techniques for dynamic scheduling have been proposed in [WS84, PHS85, AKT86, Soh90, DT92]. Dynamic scheduling has even been proposed for VLIW processors <ref> [Rau93] </ref>. This paper inquires into the factors that affect the performance of such a machine. The factors of fetch availability, efficiency, and utility were introduced in order to provide some insight into the causes of variation in performance in a dynamically scheduled processor.
Reference: [Smi81] <author> J. E. Smith. </author> <title> A study of branch prediction strategies. </title> <booktitle> In Proc. Eighth Symposium on Computer Architecture, </booktitle> <pages> pages 135-48, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: The number of outstanding requests allowed to the secondary cache varies. The number of unresolved branches allowed varies from 0 to 8 branches. Branch prediction is implemented as a 256 entry table of 3-bit saturating counters. This is a minor variation of the traditional 2-bit branch prediction scheme <ref> [Smi81] </ref>. Bits 2-9 of the branch address are used to index a table of 3-bit counters, and if the value found there is greater than 3, the branch is predicted taken, otherwise it is predicted not taken.
Reference: [Smi84] <author> J. E. Smith. </author> <title> Decoupled access/execute computer architectures. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2 </volume> <pages> 289-308, </pages> <month> November </month> <year> 1984. </year> <month> 21 </month>
Reference-contexts: This benchmark is unusual in that the instruction window becomes full even when no branch prediction is performed. This indicates that the fetch unit is racing ahead of the computations, as in a decoupled architecture <ref> [Smi84] </ref>. This can happen whenever the branches are independent of the results of the computation, and the dependency chain from one branch to the next is shorter than the dependency chain through the computations. 8 9 10 Benchmark Taken W. Take W.
Reference: [Soh90] <author> G. S. Sohi. </author> <title> Instruction issue logic for high-performance, interruptible, multiple func-tional unit, pipelined computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39 </volume> <pages> 349-359, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Various techniques for dynamic scheduling have been proposed in <ref> [WS84, PHS85, AKT86, Soh90, DT92] </ref>. Dynamic scheduling has even been proposed for VLIW processors [Rau93]. This paper inquires into the factors that affect the performance of such a machine.
Reference: [Tho64] <author> J. E. Thornton. </author> <title> Parallel operation in Control Data 6600. </title> <booktitle> In Proc. AFIPS Fall Joint Computer Conference, </booktitle> <volume> number 26, part 2, </volume> <pages> pages 33-40, </pages> <year> 1964. </year>
Reference-contexts: Compiler optimizations that can increase the amount of parallelism available within such a fixed window size, such as loop transformations [WL91], should work well. 6 Conclusion Dynamic scheduling approaches date back to the CDC 6600 <ref> [Tho64] </ref> and the IBM 360/91 [Tom67]. Various techniques for dynamic scheduling have been proposed in [WS84, PHS85, AKT86, Soh90, DT92]. Dynamic scheduling has even been proposed for VLIW processors [Rau93]. This paper inquires into the factors that affect the performance of such a machine.
Reference: [Tom67] <author> R.M. Tomasulo. </author> <title> An efficient algorithm for exploiting multiple arithmetic units. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 11 </volume> <pages> 25-33, </pages> <year> 1967. </year>
Reference-contexts: This is the number of instructions that need to be examined when determining which instructions are ready to begin execution, and includes the instructions which are waiting in reservation stations <ref> [Tom67] </ref>. To allow sufficient scope for the dynamic scheduling, the instruction window is set to 32 for all of the tests in this paper. The primary data cache varies in size and degree of associativity, with a constant line size of 16 bytes. <p> Compiler optimizations that can increase the amount of parallelism available within such a fixed window size, such as loop transformations [WL91], should work well. 6 Conclusion Dynamic scheduling approaches date back to the CDC 6600 [Tho64] and the IBM 360/91 <ref> [Tom67] </ref>. Various techniques for dynamic scheduling have been proposed in [WS84, PHS85, AKT86, Soh90, DT92]. Dynamic scheduling has even been proposed for VLIW processors [Rau93]. This paper inquires into the factors that affect the performance of such a machine.
Reference: [Uht93] <author> A. K. Uht. </author> <title> Extraction of massive instruction level parallelism. </title> <journal> Computer Architecture News, </journal> <volume> 21 </volume> <pages> 5-12, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Fortunately there are relatively few branches in the weakly predicted categories, so the overall branch prediction accuracy is quite high. The suggestion has been made that following multiple paths of execution is a good way to increase instruction level parallelism <ref> [Uht93] </ref>. However for a fixed level of instruction fetch bandwidth, it would seem that the optimal strategy is to fetch only along the most probable path.
Reference: [WL91] <author> M. Wolf and M. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2 </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: This, in turn, depends on the amount of parallelism that is present in the instruction window. Compiler optimizations that can increase the amount of parallelism available within such a fixed window size, such as loop transformations <ref> [WL91] </ref>, should work well. 6 Conclusion Dynamic scheduling approaches date back to the CDC 6600 [Tho64] and the IBM 360/91 [Tom67]. Various techniques for dynamic scheduling have been proposed in [WS84, PHS85, AKT86, Soh90, DT92]. Dynamic scheduling has even been proposed for VLIW processors [Rau93].
Reference: [WS84] <author> S. Weiss and J. E. Smith. </author> <title> Instruction issue logic in pipelined supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 33 </volume> <pages> 1013-1022, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Various techniques for dynamic scheduling have been proposed in <ref> [WS84, PHS85, AKT86, Soh90, DT92] </ref>. Dynamic scheduling has even been proposed for VLIW processors [Rau93]. This paper inquires into the factors that affect the performance of such a machine.
Reference: [YP93] <author> T-Y. Yeh and Y. Patt. </author> <title> A comparison of dynamic branch predictors that use two levels of branch history. </title> <booktitle> In Proc. of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 257-66, </pages> <month> May </month> <year> 1993. </year> <month> 22 </month>
Reference-contexts: As a result, their instruction windows were more frequently saturated, leading to performance that was strongly influenced by memory latency. 18 5.2 Branch prediction Many branch prediction methods have been proposed that are better than the simple scheme used here, for example <ref> [YP93] </ref>. The performance factors introduced in this paper can be used to analyze the likely impact of adopting better branch prediction mechanisms. The factor primarily affected by improved branch predition is fetch utility, so the biggest performance benefits will occur in those benchmarks where fetch utility is a significant factor.
References-found: 21

