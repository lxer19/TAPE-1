URL: http://www.cs.utoronto.ca/~revow/papers/pami.ps.Z
Refering-URL: http://www.cs.utoronto.ca/~revow/
Root-URL: 
Email: (revow@cs.toronto.edu)  (c.k.i.williams@aston.ac.uk)  (hinton@cs.toronto.edu)  
Title: Using generative models for handwritten digit recognition  
Author: Michael Revow Christopher K. I. Williamsy Geoffrey E. Hinton 
Note: IEEE Trans. Pattern Analysis and Machine Intelligence, 18(6), pp 592-606, 1996  
Address: 6 Kings College Road Toronto, Ontario Canada M5S 1A4  Birmingham B4 7ET UK  
Affiliation: Department of Computer Science University of Toronto  yNow at: Department of Computer Science and Applied Mathematics Aston University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. Durbin, R. Szeliski, and A. L. Yuille, </author> <title> "An analysis of the elastic net approach to the travelling salesman problem", </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <pages> pp. 348-358, </pages> <year> 1989. </year> <month> 31 </month>
Reference-contexts: Thus a single deformable model defines an entire probability distribution across shape instances. Following <ref> [1] </ref> and [34] we define the deformation energy, E def , to be the negative log probability of the deformation. E def (X) = 2 1 log jj + const (3) Splines are a convenient method for modeling handwritten digits as it is easy to incorporate topological variations. <p> The control points are labelled 1 through 8. C. Generative Models Although we use our digit models for recognition, it is helpful to consider how we would use them for generating images. The generative model is an elaboration of the probabilistic interpretation of the elastic net given in <ref> [1] </ref>. To generate a noisy image of a particular digit class, run the following procedure: (1) Pick a deformation of the model (i.e. move the control points away from their home locations) to give a particular realization X. This defines the spline in object-based coordinates.
Reference: [2] <author> S. Impedovo, </author> <title> Fundamentals in handwriting recognition, </title> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference: [3] <author> C . Y. Suen, C. Nadal, R. Legault, T. A. Mai, and L. Lam, </author> <title> "Computer recognition of uncon strained handwritten numerals.", </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 80, no. 7, </volume> <pages> pp. 1162-1180, </pages> <month> July </month> <year> 1992. </year>
Reference: [4] <author> G. L. Cash and M Hatamian, </author> <title> "Optical character recognition by the method of moments", Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> vol. 39, </volume> <pages> pp. 291-310, </pages> <year> 1987. </year>
Reference: [5] <author> L. Lam and C. Y. Suen, </author> <title> "Structural classification and relaxation matching of totally un constrained handwritten zip-code numbers", </title> <journal> Pattern Recognition, </journal> <volume> vol. 21, no. 1, </volume> <pages> pp. 19-31, </pages> <year> 1988. </year>
Reference: [6] <author> M. Shridhar and A. Badreldin, </author> <title> "Recognition of isolated and simply connected handwritten numerals", </title> <journal> Pattern Recognition, </journal> <volume> vol. 19, no. 1, </volume> <pages> pp. 1-12, </pages> <year> 1986. </year>
Reference: [7] <author> Y. Le Cun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, </author> <title> "Handwritten digit recognition with a back-propagation network", </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> D. S. Touretzky, Ed., </editor> <booktitle> Denver, 1990, </booktitle> <volume> vol. 2, </volume> <pages> pp. 396-404, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: Running the second stage of the M-step of our fitting procedure gives the control point locations in the object frame. We would expect this type of network to be less susceptible to over-fitting than conventional neural network recognizers <ref> [7] </ref>, [8], [52]. In conventional networks, each training example only provides log 2 10 bits of constraint on the weights of the network because that is the number of bits required to specify the largest output. A network trained to predict instantiation parameters provides much more constraint per training example. <p> This is in contrast 29 to the thousands of free parameters and multiple passes over the training data required by a typical neural net recognizer <ref> [7] </ref>. We have claimed that one advantage of generative models for handwritten character recognition is that instantiation information from one character should be useful for other characters written by the same author.
Reference: [8] <author> J. D. Keeler, D. E. Rumelhart, and W. K. Leow, </author> <title> "Integrated segmentation and recognition of hand-printed numerals", </title> <booktitle> in Advances in Neural Information Processing Systems 3, </booktitle> <editor> R. P. Lippmann, J. E. Moody, and D. S. Touretzky, Eds., </editor> <address> San Mateo, CA, </address> <year> 1991, </year> <pages> pp. 557-563, </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Running the second stage of the M-step of our fitting procedure gives the control point locations in the object frame. We would expect this type of network to be less susceptible to over-fitting than conventional neural network recognizers [7], <ref> [8] </ref>, [52]. In conventional networks, each training example only provides log 2 10 bits of constraint on the weights of the network because that is the number of bits required to specify the largest output. A network trained to predict instantiation parameters provides much more constraint per training example.
Reference: [9] <author> K. Fukushima and N. </author> <title> Wake, "Handwritten alphanumeric character recognition by the neocog nitron", </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 355-365, </pages> <year> 1991. </year>
Reference: [10] <author> D. Lee and S. N. Srihari, </author> <title> "Handprinted digit recognition: A comparison of algorithms", </title> <booktitle> in Third international workshop on frontiers in handwriting recognition, </booktitle> <pages> pp. 153-162. </pages> <address> Buffalo, NY, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: As a simple comparison, if all images are normalized to 16 fi 16, then k-nearest neighbour 16 has a raw error rate of 4.7% on the validation set and 7.08 % on the bs test set. An in depth study by Lee and Srihari <ref> [10] </ref> 17 evaluated 8 algorithms and 5 combination schemes on the bs test set. The results in table 1 are better than 7 out the 8 individual classifiers they used. <p> Varying the rejection threshold in the post-processing neural network allows us to trade off errors against rejects. a 1% error we have to reject 6-10% on the bs test set. Lee and Srihari's <ref> [10] </ref> curves indicate that they have to reject 2.5-12% to achieve the same rate, but these curves are for a different test set. 0 1 2 3 4 5 6 7 8 9 1 1 0 1 0 0 0 2 2 0 0 3 2 0 3 0 0 1 <p> Mixture model dashed curve. tic generative models for object recognition in a realistic domain. While its classification performance is comparable to other state-of-the-art classifiers <ref> [10] </ref>, [50] it is significantly more computationally intensive. With our simulation code running on a R4000 based workstation, a model settles on a typical image in about 1:1 seconds, resulting in a classification rate of about 5:5 images/minute. This is about two orders of magnitude slower than current practical classifiers.
Reference: [11] <author> F. Kimura and M. Shridhar, </author> <title> "Handwritten numerical recognition based on multiple algo rithms", </title> <journal> Pattern Recognition, </journal> <volume> vol. 24, no. 10, </volume> <pages> pp. 969-983, </pages> <year> 1991. </year>
Reference: [12] <author> J. Geist, R. A. Wilkinson, S. Janet, P. J. Grother, B. Hammond, N. W. Larsen, R. M. Klear, M. J. Matsko, C. J. C. Burges, R. Creecy, J. J. Hull, T. P. Vogl, and C. L. Wilson, </author> <title> "NISTIR 5452. The second census optical character recognition systems conference", </title> <type> Tech. Rep., U.S. </type> <institution> National Institute of Standards and Technology, </institution> <year> 1994. </year>
Reference: [13] <author> P. Simard, Y. Le Cun, and J. Denker, </author> <title> "Efficient pattern recognition using a new transformation distance.", </title> <booktitle> in Advances in Neural Information Processing Systems 5, </booktitle> <editor> J. D. Cowan S. J. Hanson and C. L. Giles, </editor> <booktitle> Eds., </booktitle> <pages> pp. 50-58. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: The digit models we propose have a sound generative probabilistic basis and explicitly incorporate much prior knowledge of handwritten digits, for example, that they are made up of strokes and that they are globally invariant to affine transformations, unlike other implementations <ref> [13] </ref> which attempt to achieve only local invariance. III. Matching Elastic Spline Models to Images A. Overview Each of the ten digits has its own elastic model 2 . A digit-image is recognized by choosing the elastic model which best matches the image.
Reference: [14] <author> C. J. C. Burges, O. Matan, Y. Le Cun, J. S. Denker, L. D. Jackel, C. E. Stenard, C. R. Nohl, and J. I. Ben, </author> <title> "Shortest path segmentation: A method for training a neural network to recognize character strings", </title> <journal> IJCNN, </journal> <volume> vol. 3, </volume> <pages> pp. 165-171, </pages> <year> 1992. </year>
Reference-contexts: 1. Conventionally, a recognizer does not help to guide segmentation by dividing the image into significant and irrelevant parts. So a system typically <ref> [14] </ref> tries many candidate segmentations and all the recognizer can indicate is whether a particular segmentation leads to confident recognition.
Reference: [15] <author> Y. Lee, </author> <title> "Handwritten digit recognition using k- nearest-neighbor, radial-basis function, and backpropogation neural networks.", </title> <journal> Neural computation, </journal> <volume> vol. 3, </volume> <pages> pp. 440-449, </pages> <year> 1991. </year>
Reference-contexts: Obviously feature selection and classifier design may be independently described. 2 recognition to refine the segmentation hypothesis. 2. Statistical recognizers can occasionally confidently classify images that do not look anything like a character <ref> [15] </ref>. This can be ameliorated by training the system to reject junk images [16], but it is hard to get a good sample of rare types of junk. 3.
Reference: [16] <author> J. Bromley and J. Denker, </author> <title> "Improving rejection performance on handwritten digits by training with rubbish", </title> <journal> Neural computation, </journal> <volume> vol. 5, no. 3, </volume> <pages> pp. 367-370, </pages> <year> 1993. </year> <month> 32 </month>
Reference-contexts: Obviously feature selection and classifier design may be independently described. 2 recognition to refine the segmentation hypothesis. 2. Statistical recognizers can occasionally confidently classify images that do not look anything like a character [15]. This can be ameliorated by training the system to reject junk images <ref> [16] </ref>, but it is hard to get a good sample of rare types of junk. 3. Systems that do not incorporate any prior knowledge about the shapes of characters must learn all their knowledge from the training examples.
Reference: [17] <author> D. G. Lowe, </author> <title> Perceptual Organization and Visual Recognition, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Hingham, MA, </address> <year> 1985. </year>
Reference-contexts: For handwritten digits we may also want information on the writing style since this is occasionally crucial in disambiguating other digits in the same string. Motivated by the success of model-based shape recognition in overcoming some of these shortcomings <ref> [17] </ref>, we have investigated the use of deformable elastic models for handwritten digit recognition [18]. Models of this general type have been used in computer vision since the early 1970's.
Reference: [18] <author> G. E. Hinton, C. K. I. Williams, and M. D. Revow, </author> <title> "Adaptive elastic models for hand-printed character recognition", </title> <booktitle> in Advances in Neural Information Processing Systems 4, </booktitle> <editor> J. E. Moody, S. J. Hanson, and R. P. Lippmann, Eds. </editor> <publisher> Morgan Kauffmann, </publisher> <year> 1992. </year>
Reference-contexts: Motivated by the success of model-based shape recognition in overcoming some of these shortcomings [17], we have investigated the use of deformable elastic models for handwritten digit recognition <ref> [18] </ref>. Models of this general type have been used in computer vision since the early 1970's. Ullmann [19] discusses the idea of finding a distortion mapping from a test image to a stored template such that there is correspondence between like features rather than exact matches.
Reference: [19] <author> J. R. </author> <title> Ullmann, "Correspondence in character recognition", in Machine Perception of Patterns and Pictures. </title> <booktitle> Institute of Physics, </booktitle> <address> London, U.K., </address> <year> 1972. </year>
Reference-contexts: Motivated by the success of model-based shape recognition in overcoming some of these shortcomings [17], we have investigated the use of deformable elastic models for handwritten digit recognition [18]. Models of this general type have been used in computer vision since the early 1970's. Ullmann <ref> [19] </ref> discusses the idea of finding a distortion mapping from a test image to a stored template such that there is correspondence between like features rather than exact matches.
Reference: [20] <author> B. </author> <title> Widrow, </title> <journal> "The `rubber-mask' technique-I. Pattern Measurement and Analysis", Pattern Recognition, </journal> <volume> vol. 5, </volume> <pages> pp. 175-197, </pages> <year> 1973. </year>
Reference-contexts: Models of this general type have been used in computer vision since the early 1970's. Ullmann [19] discusses the idea of finding a distortion mapping from a test image to a stored template such that there is correspondence between like features rather than exact matches. Widrow <ref> [20] </ref>, also suggests the idea of using rubber templates to achieve fuzzy matches to a variety of natural objects and waveforms. Burr presents an iterative framework for computing elastic matches in dot and grey-scale images [21] and line drawings [22].
Reference: [21] <author> D. J. Burr, </author> <title> "A dynamic model for image registration", </title> <journal> Comput. Graphics Image Process., </journal> <volume> vol. 15, </volume> <pages> pp. 102-112, </pages> <year> 1981. </year>
Reference-contexts: Widrow [20], also suggests the idea of using rubber templates to achieve fuzzy matches to a variety of natural objects and waveforms. Burr presents an iterative framework for computing elastic matches in dot and grey-scale images <ref> [21] </ref> and line drawings [22]. Using a coarse-to-fine matching strategy he shows how an image can be progressively deformed under the influence of misalignment force fields to fit another image. In a later version [23], global size and rotation adjustments were included. <p> Motivated by research on "snakes" [30], a simple approach to the beads in white space problem (section V.B), is to define another energy term, E w to penalize beads spanning white space. This term is similar to the "support measure" [31] or the symmetric matching used in <ref> [21] </ref> and [22]. E w = b=1 N I X P kb (21) A bead only makes a large contribution to this cost when all inked pixels are far from the bead.
Reference: [22] <author> D. J. Burr, </author> <title> "Elastic matching of line drawings", </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 3, no. 6, </volume> <pages> pp. 708-713, </pages> <year> 1981. </year>
Reference-contexts: Widrow [20], also suggests the idea of using rubber templates to achieve fuzzy matches to a variety of natural objects and waveforms. Burr presents an iterative framework for computing elastic matches in dot and grey-scale images [21] and line drawings <ref> [22] </ref>. Using a coarse-to-fine matching strategy he shows how an image can be progressively deformed under the influence of misalignment force fields to fit another image. In a later version [23], global size and rotation adjustments were included. <p> Motivated by research on "snakes" [30], a simple approach to the beads in white space problem (section V.B), is to define another energy term, E w to penalize beads spanning white space. This term is similar to the "support measure" [31] or the symmetric matching used in [21] and <ref> [22] </ref>. E w = b=1 N I X P kb (21) A bead only makes a large contribution to this cost when all inked pixels are far from the bead.
Reference: [23] <author> D. J. Burr, </author> <title> "Matching elastic templates", in Physical and biological processing of images: proceedings of an international symposium organized by the Rank Prize Funds, </title> <editor> O. J. Braddick and A. C. Sleigh, Eds. </editor> <publisher> Springer-Verlag, </publisher> <year> 1983. </year>
Reference-contexts: Burr presents an iterative framework for computing elastic matches in dot and grey-scale images [21] and line drawings [22]. Using a coarse-to-fine matching strategy he shows how an image can be progressively deformed under the influence of misalignment force fields to fit another image. In a later version <ref> [23] </ref>, global size and rotation adjustments were included. The method has been adapted to match tomographic [24] and thermographic images [25]. One weakness with the approach is that it does not allow the amount of deformation to be traded off against the fidelity of the data match.
Reference: [24] <author> M. Moshfeghi, </author> <title> "Elastic matching of multimodality medical images", Computer Vision, Graph ics and Image Processing: Graphical Models and Image Processing, </title> <journal> vol. </journal> <volume> 53, no. 3, </volume> <pages> pp. 271-282, </pages> <year> 1991. </year>
Reference-contexts: Using a coarse-to-fine matching strategy he shows how an image can be progressively deformed under the influence of misalignment force fields to fit another image. In a later version [23], global size and rotation adjustments were included. The method has been adapted to match tomographic <ref> [24] </ref> and thermographic images [25]. One weakness with the approach is that it does not allow the amount of deformation to be traded off against the fidelity of the data match. It also has no principled way of handling noise or missing data.
Reference: [25] <author> M. Varga and R. Hanka, </author> <title> "Dynamic elastic image stretching technique applied to thermographic images", </title> <booktitle> IEE Proceedings, </booktitle> <volume> vol. 137, Pt. 1, no. 3, </volume> <pages> pp. 146-156, </pages> <year> 1990. </year>
Reference-contexts: Using a coarse-to-fine matching strategy he shows how an image can be progressively deformed under the influence of misalignment force fields to fit another image. In a later version [23], global size and rotation adjustments were included. The method has been adapted to match tomographic [24] and thermographic images <ref> [25] </ref>. One weakness with the approach is that it does not allow the amount of deformation to be traded off against the fidelity of the data match. It also has no principled way of handling noise or missing data.
Reference: [26] <editor> R. Bajcsy and S. </editor> <title> Kovacic, "Multiresolution elastic matching", Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> vol. 46, </volume> <pages> pp. 1-21, </pages> <year> 1989. </year>
Reference-contexts: One weakness with the approach is that it does not allow the amount of deformation to be traded off against the fidelity of the data match. It also has no principled way of handling noise or missing data. Bajcsy and co-workers <ref> [26] </ref>, [27] integrate the notion of a trade-off between data fit and deformation in their multiresolution elastic matching scheme for registering an image with respect to a template. They consider a test image to be drawn on an elastic membrane.
Reference: [27] <author> R. Bajcsy, R. Lieberson, and M. Reivich, </author> <title> "A computerized system for the elastic matching of deformed radiographic images to idealized atlas images", </title> <journal> Journal of Computer Assisted Tomography, </journal> <volume> vol. 7, no. 4, </volume> <pages> pp. 618-625, </pages> <year> 1983. </year>
Reference-contexts: One weakness with the approach is that it does not allow the amount of deformation to be traded off against the fidelity of the data match. It also has no principled way of handling noise or missing data. Bajcsy and co-workers [26], <ref> [27] </ref> integrate the notion of a trade-off between data fit and deformation in their multiresolution elastic matching scheme for registering an image with respect to a template. They consider a test image to be drawn on an elastic membrane.
Reference: [28] <author> M. A. Fischler and R. A. Elschlager, </author> <title> "The representation and matching of pictorial structures", </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. C-22, no. 1, </volume> <pages> pp. 67-92, </pages> <year> 1973. </year>
Reference-contexts: The multiresolution approach is attractive as it initially concentrates on achieving large-scale registration between the images with fine-scale matching coming later in the process. Early work by Fischler and Elschlager <ref> [28] </ref> described a model with local (data fit) and global (model deformation) energy terms. Their model is composed of (rigid) features whose spatial arrangement is constrained by springs and hence the deformation is related to the energy required to stretch or compress these springs.
Reference: [29] <author> A. L. Yuille, </author> <title> "Deformable templates for face recognition", </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> vol. 3(1), </volume> <pages> pp. 59-70, </pages> <year> 1991. </year>
Reference-contexts: Their matching procedure works on a coarse scale, but it is scale dependent and degrades in the presence of noise <ref> [29] </ref>. The facial feature model example they used has been extended by Yuille [29], who constructs a more detailed descriptions of the feature shapes and global matching criteria in terms of peak, valley and edge intensities. In addition, the original dynamic programming search was replaced with a gradient method. <p> Their matching procedure works on a coarse scale, but it is scale dependent and degrades in the presence of noise <ref> [29] </ref>. The facial feature model example they used has been extended by Yuille [29], who constructs a more detailed descriptions of the feature shapes and global matching criteria in terms of peak, valley and edge intensities. In addition, the original dynamic programming search was replaced with a gradient method.
Reference: [30] <author> M. Kass, A. Witkin, and D. Terzopoulos, "Snakes: </author> <title> Active contour models", </title> <booktitle> in Proceedings of the First International Conference on Computer Vision, </booktitle> <address> Washington, D. C., 1987, </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: explained by the model (or explicitly attributed to some additional noise process) the matching process tries to ensure that every part of the model is supported by some part of the image and a match may be good even though it leaves large parts of the image unaccounted for. "Snakes" <ref> [30] </ref>, use different shape constraints, but also attempt to match each part of the model to some part of the image rather than vice versa. <p> However, we found that performance is improved by including five additional terms which are easily obtained from the final fits of the model to the image. Motivated by research on "snakes" <ref> [30] </ref>, a simple approach to the beads in white space problem (section V.B), is to define another energy term, E w to penalize beads spanning white space. This term is similar to the "support measure" [31] or the symmetric matching used in [21] and [22].
Reference: [31] <author> A. Lanitis, C. J. Taylor, and T. F. Cootes, </author> <title> "A generic system for classifying variable objects using flexible template matching", </title> <booktitle> in Proceedings of the British Machine Vision Conference, </booktitle> <editor> J. Illingworth, Ed., </editor> <volume> vol. 1, </volume> <pages> pp. 329-338. </pages> <publisher> BMVA Press, </publisher> <year> 1993. </year>
Reference-contexts: Point distribution models <ref> [31] </ref>, recognize the importance of doing both types of matching, i.e. the model must be supported by the data and the model should explain the data. <p> Motivated by research on "snakes" [30], a simple approach to the beads in white space problem (section V.B), is to define another energy term, E w to penalize beads spanning white space. This term is similar to the "support measure" <ref> [31] </ref> or the symmetric matching used in [21] and [22]. E w = b=1 N I X P kb (21) A bead only makes a large contribution to this cost when all inked pixels are far from the bead.
Reference: [32] <author> B. S. Everitt, </author> <title> An introduction to latent variable models, </title> <publisher> Chapman and Hall, </publisher> <year> 1984. </year>
Reference-contexts: Thus knowledge about the shape can be used to refine the segmentation. What we have just described is an instantiation of the general framework of generative or latent variable models <ref> [32] </ref>. The key idea is that the manifest variables are attributable to a smaller number of underlying hidden or latent variables. In our case, the manifest variables are the pixels and the hidden variables are the positions of the elastic model's control points in the image-frame.
Reference: [33] <author> J. D. Foley, A. van Dam, S. K. Feiner, and J. F. Hughes, </author> <title> Computer Graphics principles and practice. Second edition, </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year> <month> 33 </month>
Reference-contexts: An algorithm for the more difficult problem of inferring the hidden variables from the manifest variables is presented in III.D. B. Elastic Spline Models We model each digit with a uniform, cubic B-spline <ref> [33] </ref>. Each model has at most 8 control points 3 . <p> The location of any point 4 , s (b), on the spline can be written as a linear function <ref> [33] </ref> of the control points locations 5 . s (b) = l=1 Because of the local control feature of B-splines some of the coefficients, fl l (b), will be zero.
Reference: [34] <author> U. Grenander, Y. Chow, and D. M. Keenan, </author> <title> Hands: A pattern theoretic study of biological shapes, </title> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Thus a single deformable model defines an entire probability distribution across shape instances. Following [1] and <ref> [34] </ref> we define the deformation energy, E def , to be the negative log probability of the deformation. E def (X) = 2 1 log jj + const (3) Splines are a convenient method for modeling handwritten digits as it is easy to incorporate topological variations.
Reference: [35] <author> S. Edelman, S. Ullman, and T. </author> <title> Flash, "Reading cursive handwriting by alignment of letter prototypes", </title> <journal> International Journal of Computer Vision, </journal> <volume> vol. 5, no. 3, </volume> <pages> pp. 303-331, </pages> <year> 1990. </year>
Reference-contexts: For example, small changes in the relative locations of the control points can turn the loop of a 2 into a cusp or an open bend (figure 1). This advantage of spline models is pointed out in <ref> [35] </ref> where a different kind of spline is used to fit on-line character data by directly locating candidate control points on strokes in the image.
Reference: [36] <author> C. K. I. Williams, M. D. Revow, and G. E. Hinton, </author> <title> "Hand-printed digit recognition using deformable models", in Spatial Vision in Humans and Robots, </title> <editor> L. Harris and M. Jenkin, Eds. </editor> <publisher> Cambridge University Press, </publisher> <year> 1993. </year>
Reference-contexts: This advantage of spline models is pointed out in [35] where a different kind of spline is used to fit on-line character data by directly locating candidate control points on strokes in the image. It is lost (as pointed out in <ref> [36] </ref>) when models based more directly on Durbin and Willshaw's elastic net are employed [37]. 4 The spline is a one dimensional continuous curve parameterized by b. <p> Currently the residual image is accounted for by a simple uniform noise process (equation (8)). An improvement would be to use a more structured noise model. Another possibility is to model the residual images using "flourish models" <ref> [36] </ref>. Before leaving this topic, it should be noted that some regional stylistic peculiarities, for example the middle bar on "crossed" sevens or the top and bottom of European style ones, may be modeled in this manner.
Reference: [37] <author> J. Bertille, </author> <title> "An elastic matching approach applied to digit recognition", </title> <booktitle> in Proceedings of the second international conference on document analysis and recognition, </booktitle> <pages> pp. 82-85. </pages> <publisher> IEEE Computer Society Press. Los Alamitos, </publisher> <year> 1993. </year>
Reference-contexts: It is lost (as pointed out in [36]) when models based more directly on Durbin and Willshaw's elastic net are employed <ref> [37] </ref>. 4 The spline is a one dimensional continuous curve parameterized by b.
Reference: [38] <author> D. J. C MacKay, </author> <title> "Bayesian interpolation", </title> <journal> Neural computation, </journal> <volume> vol. 4, </volume> <pages> pp. 415-447, </pages> <year> 1992. </year>
Reference-contexts: However, the generative model is useful for recognizing digits as explained in the following sections. D. Fitting a model to an image In this section, a Bayesian interpretation of the fitting process is adopted and we demonstrate how using a maximum posterior framework (see, for example <ref> [38] </ref>) yields a practical algorithm. First we need to refine the notation; A superscript O or I is used to qualify if a quantity is in the object or image frame respectively. So X O represents control points locations in the object frame.
Reference: [39] <author> R. Durbin and D. Willshaw, </author> <title> "An analogue approach to the travelling salesman problem", </title> <journal> Nature, </journal> <volume> vol. 326, </volume> <pages> pp. 689-691, </pages> <year> 1987. </year>
Reference-contexts: During the fitting process the variance of the beads will generally decrease and the number of beads increase as the model gets closer to the data and begins to explain its finer structure. The fitting technique resembles the elastic net algorithm of Durbin and Willshaw <ref> [39] </ref> except that our elastic energy function is much more complex and we are also fitting an affine transformation. In early experiments, we used a conjugate gradient method to optimize E tot .
Reference: [40] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin, </author> <title> "Maximum likelihood from incomplete data via the EM algorithm", </title> <journal> Proceedings of the Royal Statistical Society, </journal> <volume> vol. B-39, </volume> <pages> pp. 1-38, </pages> <year> 1977. </year>
Reference-contexts: Unfortunately this method is slow because each conjugate gradient step may require a few evaluations of E tot , each of which is of the order of B fi N operations. Our preferred method is based upon the Expectation Maximization (EM) algorithm <ref> [40] </ref>. This involves the repeated application of a two step procedure which will not increase E tot as ff is adjusted at each application. During the expectation (E) step, the beads are frozen at their current locations and the responsibility that each bead has for each inked pixel is computed.
Reference: [41] <author> X. Meng and D. B. Rubin, </author> <title> "Recent extensions to the EM algorithm", in Bayesian Statistics 4, </title> <editor> J.M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith, </editor> <booktitle> Eds., </booktitle> <pages> pp. 307-320. </pages> <publisher> Oxford University Press, </publisher> <year> 1992. </year>
Reference-contexts: Since all control 8 This is an example of Expectation/Conditional Maximization <ref> [41] </ref> 9 More precisely, the pixel forces on the beads can be transferred onto the control points and at equilibrium there is a balance between these forces and those pulling the control points towards their home locations. 11 1.
Reference: [42] <author> J. Hampshire and A. Waibel, </author> <title> "A novel objective function for improved phoneme recogni tion using time-delay neural networks", </title> <type> Technical Report CMU-CS-89-118, </type> <institution> Carnegie-Mellon, </institution> <address> Pittsburgh, PA, </address> <year> 1989. </year>
Reference-contexts: The maximum mutual information criterion emphasizes correct discrimination rather than correct modeling of the image data, and it generally leads to better discriminative performance <ref> [42] </ref>, although the advantage of discriminative learning vanishes if the generative model is correct and the fitting process produces the true probability of the data given the model [43].
Reference: [43] <author> P. Brown, </author> <title> The Acoustic-Modeling Problem in Automatic Speech Recognition, </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1987, </year> <note> Also published as IBM Research Division Technical Report RC 12750. </note>
Reference-contexts: mutual information criterion emphasizes correct discrimination rather than correct modeling of the image data, and it generally leads to better discriminative performance [42], although the advantage of discriminative learning vanishes if the generative model is correct and the fitting process produces the true probability of the data given the model <ref> [43] </ref>. Early experiments showed that, for our generative models, maximum likelihood learning was just as effective as discriminative learning, perhaps because the generative models are a reasonable approximation to the way in which the data is generated.
Reference: [44] <author> C. K. I Williams, </author> <title> Combining deformable models and neural networks for handprinted digit recognition, </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <year> 1994. </year>
Reference-contexts: This may prove to be useful in strings of digits, where we would expect different instances of the same digit to have similar styles (see section VII). There may even be mutual information between the pairings of local models for different digits (e.g. between a 4 and a 6) <ref> [44] </ref>. B. Generating both the inked and uninked pixels A significant drawback of our generative model is that it does not treat the uninked pixels as evidence.
Reference: [45] <author> P. Dayan, G. E. Hinton, R. M. Neal, and R. S. Zemel, </author> <title> "The Helmholtz machine", </title> <journal> Neural computation, </journal> <volume> vol. 7, no. 7, </volume> <pages> pp. 889-904, </pages> <year> 1995. </year>
Reference-contexts: With this view of the EM algorithm, it is not immediately obvious that partial implementation of the expectation step is justified. However in an alternative interpretation of the EM algorithm <ref> [45] </ref>, the EM algorithm can be viewed as maximizing a joint function, F (P (); ff) of the distribution of the unobserved data and model parameters.
Reference: [46] <author> J. J Hull, </author> <title> "A database for handwritten text recognition research", </title> <journal> IEEE Transactions Pattern Analysis and Machine Intellegince, </journal> <volume> vol. 16, no. 5, </volume> <pages> pp. 550-554, </pages> <year> 1994. </year>
Reference-contexts: VI. Results on isolated digits The performance of the elastic net in recognizing isolated digits has been evaluated on data from the CEDAR CDROM 1 database of Cities, States, ZIP Codes, Digits, and Alphabetic Characters <ref> [46] </ref>. The br training set of binary segmented digits was subdivided into 3 training sets of size 2000, 7000 and 2000 respectively. A validation set of 2000 examples was also generated from the br training set to allow us to investigate different configurations of the post-processing neural network. <p> The CEDAR database also includes 2 test sets. The goodbs (2213 images) set is a subset of the bs (2711 images) set containing only well segmented digits. It is interesting to note that br training data were segmented with the same diligence as the goodbs test data <ref> [46] </ref>. After fitting all the models to a particular image, we wish to evaluate which of the models best "explains" the data. The natural measure is the sum of E fit and E def that is minimized during the fitting process.
Reference: [47] <author> J. S. Bridle, </author> <title> "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition.", in Neuro-computing: algorithms, architectures and applications, </title> <editor> F. Fougelman-Soulie and J. Herault, Eds. </editor> <booktitle> NATO ASI series on systems and computer science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: A similar transformation is used for the final bead variance, while the remaining inputs are simply scaled. Each of the seven input terms for a model is directly connected to the output unit for that model. The output units compete using the "softmax" function <ref> [47] </ref> which guarantees that the 10 output values form a probability distribution. Including biases on the output units 15 , the network has 80 weights and is trained using conjugate gradient to minimize a cross-entropy error function.
Reference: [48] <author> T. Hastie and R. Tibshirani, </author> <title> Handwritten digit recognition via deformable prototypes, </title> <type> Technical Report, </type> <institution> Department of Statistics, University of Toronto, </institution> <year> 1992. </year>
Reference-contexts: We reject classifications in which the maximum output activation is below some threshold T . We tried a variety of architectures for this "post-processing" network. For example, a digit recognition system developed by Hastie and Tibshirani <ref> [48] </ref> suggested that discrimination would be much better if the net was totally connected so that the output unit that represents one digit receives detailed information about the way in which other digit models fit the data.
Reference: [49] <author> T.M. Ha and H. Bunke, </author> <title> "Handwritten numeral recognition by perturbation method", </title> <booktitle> in Proceedings, Fourth International Workshop on Handwriting Recognition, </booktitle> <pages> pp. 97-106. </pages> <year> 1994. </year>
Reference-contexts: The results in table 1 are better than 7 out the 8 individual classifiers they used. Their best single algorithm has a raw error rate of 2.99% while the best combination scheme has a 2.51% error rate. Ha and Bunke <ref> [49] </ref> report error rates of 0:9 2:3% on the goodbs data set using 5 schemes. Unfortunately it is not clear which data was used to finesse the many empirical constants involved. The confusion matrix of errors for the bs set is shown in table 2.
Reference: [50] <author> G. E. Hinton, M. Revow, and P. Dayan, </author> <title> "Recognizing handwritten digits using mixtures of linear models", </title> <booktitle> in Advances in Neural Information Processing Systems 7, </booktitle> <editor> G. Tesauro, D. S. Touretzky, and T. K. Leen, </editor> <booktitle> Eds., </booktitle> <pages> pp. 1015-1022. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1995. </year> <month> 34 </month>
Reference-contexts: Mixture model dashed curve. tic generative models for object recognition in a realistic domain. While its classification performance is comparable to other state-of-the-art classifiers [10], <ref> [50] </ref> it is significantly more computationally intensive. With our simulation code running on a R4000 based workstation, a model settles on a typical image in about 1:1 seconds, resulting in a classification rate of about 5:5 images/minute. This is about two orders of magnitude slower than current practical classifiers.
Reference: [51] <author> C. K. I. Williams, M. Revow, and G. E. Hinton, </author> <title> "Using a neural net to instantiate a deformable model", </title> <booktitle> in Advances in Neural Information Processing Systems 7, </booktitle> <editor> G. Tesauro, D. S. Touretzky, and T. K. Leen, </editor> <booktitle> Eds., </booktitle> <pages> pp. 965-972. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1995. </year>
Reference-contexts: So this suggests another simple method; detect these poor performing models and terminate their searches after just a few iterations. Providing a good starting point for the search can also speedup the search. For example, a doubling of processing speed was achieved <ref> [51] </ref> using the multi-layered backpropagation network described below. An examination of the errors made on the validation set reveals that almost all mis-classifications can be attributed to two problems: local minima in the search space and modeling difficulties. <p> Using the rich set of instantiation parameters supplied by the correct elastic model after it has been fitted, we can train a conventional supervised multi-layer neural network to predict model instantiation parameters from the image <ref> [51] </ref>. Given an input image 18 , the network predicts the locations of the control points in image space for each of the 10 digit models. Running the second stage of the M-step of our fitting procedure gives the control point locations in the object frame.
Reference: [52] <author> A. Gupta, M. V. Nagendraprasad, A. Liu, P. S. P. Wang, and S. Ayyadurai, </author> <title> "An integrated architecture for recognition of totally unconstrained handwritten numerals", </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> vol. 7, no. 4, </volume> <pages> pp. 757-773, </pages> <year> 1993. </year> <month> 35 </month>
Reference-contexts: Running the second stage of the M-step of our fitting procedure gives the control point locations in the object frame. We would expect this type of network to be less susceptible to over-fitting than conventional neural network recognizers [7], [8], <ref> [52] </ref>. In conventional networks, each training example only provides log 2 10 bits of constraint on the weights of the network because that is the number of bits required to specify the largest output. A network trained to predict instantiation parameters provides much more constraint per training example.
References-found: 52

