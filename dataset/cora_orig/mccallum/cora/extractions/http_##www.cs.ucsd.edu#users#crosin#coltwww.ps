URL: http://www.cs.ucsd.edu/users/crosin/coltwww.ps
Refering-URL: http://www.cs.ucsd.edu/users/crosin/
Root-URL: http://www.cs.ucsd.edu
Email: fcrosin,rikg@cs.ucsd.edu  
Title: A Competitive Approach to Game Learning  
Author: Christopher D. Rosin Richard K. Belew 
Address: La Jolla, CA 92093-0114  
Affiliation: Cognitive Computer Science Research Group CSE Department, University of California, San Diego  
Note: (contact author) and  
Abstract: Machine learning of game strategies has often depended on competitive methods that continually develop new strategies capable of defeating previous ones. We use a very inclusive definition of game and consider a framework within which a competitive algorithm makes repeated use of a strategy learning component that can learn strategies which defeat a given set of opponents. We describe game learning in terms of sets H and X of first and second player strategies, and connect the model with more familiar models of concept learning. We show the importance of the ideas of teaching set [20] and specification number [19] k in this new context. The performance of several competitive algorithms is investigated, using both worst-case and randomized strategy learning algorithms. Our central result (Theorem 4) is a competitive algorithm that solves games in a total number of strategies polynomial in lg(jHj), lg(jX j), and k. Its use is demonstrated, including an application in concept learning with a new kind of counterexample oracle. We conclude with a complexity analysis of game learning, and list a number of new questions arising from this work. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Angluin, D. </author> <title> (1990) Negative results for equivalence queries. </title> <booktitle> Machine Learning 5:2. </booktitle>
Reference-contexts: Counterexamples are not sufficient to learn all games, though. Angluin has shown that for some classes in concept learning, an adversary may choose uninformative counterexamples so that the target cannot be exactly learned in polynomial time <ref> [1] </ref>. Below, we show the existence of a game for which counterexamples do not ensure polynomial learnability, with either worst-case strategy learning algorithms or randomized strategy learning algorithms. Example 2 (Generalized Guessing Games) Let H consist of all natural numbers in the range 0 : : : n 1.
Reference: [2] <author> Bshouty, N.H., R. Cleve, S. Kannan, and C. Ta-mon. </author> <title> (1994) Oracles and queries that are sufficient for exact learning. </title> <address> COLT 94. </address>
Reference-contexts: This result shows that, with such a powerful counterexample oracle, any teachable class of hypotheses can be learned (without resorting to hy potheses outside of H <ref> [2] </ref>). 8 Note that fixing k 0 is crucial. If the counterexample oracle were allowed to provide as many counterexamples as passed hypotheses, it could simply provide a separate counterexample to each hypothesis. The learning model would have no more power than equivalence queries.
Reference: [3] <author> Cliff, D. and G. Miller. </author> <title> (1995) Tracking the Red Queen: Measurements of Adaptive Progress in Co-evolutionary Simulations. </title> <booktitle> Third European Conference on Artificial Life. </booktitle>
Reference-contexts: The definition of "game" used in this paper is very inclusive and allows us to also consider domains other than traditional board games, such as evolving sorting networks [4], minimax controller design [16], and discrete approximations to differential games <ref> [3, 18] </ref>. To study this, our framework for game learning relies on the existence of a strategy learning algorithm, that is able to learn strategies which defeat a given set of opponents. The competitive algorithm then repeatedly uses a strategy learning algorithm to discover strong strategies for the game. <p> The idea of measuring coverage of prior opponents has been discussed in the context of empirical game learning systems <ref> [3] </ref>. 6 5.2 Using Worst-case Strategy Learn- ing Algorithms This competitive algorithm performs as well as possible with worst-case strategy learning algorithms. Theorem 3 Assume a game G has maximum transitive chain length `.
Reference: [4] <author> Hillis, W.D. </author> <title> (1991) Co-evolving Parasites Improve Simulated Evolution as an Optimization Procedure. </title> <booktitle> Artificial Life II. </booktitle>
Reference-contexts: Examples include Samuel's classic work on checkers [15], systems using reinforcement learning [21], and using a genetic algorithm [13, 14]. The definition of "game" used in this paper is very inclusive and allows us to also consider domains other than traditional board games, such as evolving sorting networks <ref> [4] </ref>, minimax controller design [16], and discrete approximations to differential games [3, 18]. To study this, our framework for game learning relies on the existence of a strategy learning algorithm, that is able to learn strategies which defeat a given set of opponents.
Reference: [5] <author> Freund, Y., M. Kearns et al. </author> <title> (1995) Efficient Algorithms for Learning to Play Repeated Games Against Computationally Bounded Adversaries. </title> <type> FOCS 36. </type>
Reference-contexts: These are not useful for complex domains in which the number of states is vast, but there have been promising recent results using certain kinds of value function approximators [9]. Several recent papers have discussed adaptive strategies for simple repeated games <ref> [11, 5] </ref>. The goal of such strategies is typically to learn enough about a particular opponent in early games to do well against it in later games.
Reference: [6] <author> Jerrum, M., L. Valiant, and V. Vazirani. </author> <title> (1986) Random Generation of Combinatorial Structures from a Uniform Distribution. </title> <booktitle> Theoretical Computer Science 43:2. </booktitle>
Reference-contexts: The problem solved by the strategy learning algorithm is in N P (a formula containing several copies of f, with different opponents inserted, must be satisfied). Satisfying the (p; q)-randomization criterion is not much more difficult: the method of almost uniform generation <ref> [6] </ref> can be used to do this in randomized polynomial time with an N P oracle. This implies that games can be solved with an N P oracle in expected time polynomial in lg (jHj), lg (jX j), and k using the covering competitive algorithm.
Reference: [7] <author> Littman, M. </author> <title> (1994) Markov Games as a Framework for Multi-agent Reinforcement Learning. </title> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference. </booktitle>
Reference-contexts: Some results in reinforcement learning have been described in terms of game learning <ref> [7] </ref>. But, most of these results show convergence without considering learning time, or prove time bounds that are polynomial in the number of states [10]. Also, proven results usually rely on simple lookup table representations for value functions.
Reference: [8] <author> Mass, W. </author> <title> (1991) On-line Learning with an Oblivious Environment and the Power of Randomization. </title> <address> COLT 91. </address>
Reference-contexts: Since we allow the strategy learning algorithm unlimited access to the game, any game with a solution has a first-player strategy learning algorithm that immediately produces a perfect strategy. Another typical approach for going beyond worst-case algorithms is to use randomized learning algorithms <ref> [8] </ref>. We consider randomized strategy learning algorithms that, when passed a set A of opponents, choose from a distribution over the set of strategies (or strategy sets) that defeat A. Randomization tends to be most helpful when this distribution is required to be uniform.
Reference: [9] <author> Gordon, G.J. </author> <title> (1995) Stable Function Approximation in Dynamic Programming. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning. </booktitle>
Reference-contexts: Also, proven results usually rely on simple lookup table representations for value functions. These are not useful for complex domains in which the number of states is vast, but there have been promising recent results using certain kinds of value function approximators <ref> [9] </ref>. Several recent papers have discussed adaptive strategies for simple repeated games [11, 5]. The goal of such strategies is typically to learn enough about a particular opponent in early games to do well against it in later games.
Reference: [10] <author> Fiechter, C.-N. </author> <title> (1994) Efficient Reinforcement Learning. </title> <address> COLT 94. </address>
Reference-contexts: Some results in reinforcement learning have been described in terms of game learning [7]. But, most of these results show convergence without considering learning time, or prove time bounds that are polynomial in the number of states <ref> [10] </ref>. Also, proven results usually rely on simple lookup table representations for value functions. These are not useful for complex domains in which the number of states is vast, but there have been promising recent results using certain kinds of value function approximators [9].
Reference: [11] <author> Kilian, J., K.J. Lang, and B.A. Pearlmutter. </author> <title> (1994) Playing the Matching-Shoulders Lob-Pass Game with Logarithmic Regret. </title> <address> COLT 94. </address>
Reference-contexts: These are not useful for complex domains in which the number of states is vast, but there have been promising recent results using certain kinds of value function approximators [9]. Several recent papers have discussed adaptive strategies for simple repeated games <ref> [11, 5] </ref>. The goal of such strategies is typically to learn enough about a particular opponent in early games to do well against it in later games.
Reference: [12] <author> Papadimitriou, C. </author> <note> (1994) Computational Complexity. </note>
Reference-contexts: A decision-problem version of game learning can be formulated: is the game a first-player win? That is, if game outcome is given by f (h; x), decide whether 9h8xf (h; x). But this is simply the canonical 2 P -complete decision problem QSAT 2 <ref> [12] </ref>, 6 so game learning is 2 P -complete. The problem solved by the strategy learning algorithm is in N P (a formula containing several copies of f, with different opponents inserted, must be satisfied).
Reference: [13] <author> Pollack, J., A. Blair, and M. Land. </author> <title> (1995) Coevolution of a Backgammon Player. </title> <note> Submitted to Artificial Life V. </note>
Reference-contexts: This is the type of method that we consider. The main intuition is that it can bootstrap the level of play, from initial uninformed strategies to expert players. Examples include Samuel's classic work on checkers [15], systems using reinforcement learning [21], and using a genetic algorithm <ref> [13, 14] </ref>. The definition of "game" used in this paper is very inclusive and allows us to also consider domains other than traditional board games, such as evolving sorting networks [4], minimax controller design [16], and discrete approximations to differential games [3, 18]. <p> Given the continual improvement seen in strategies, it is likely that this condition is being met implicitly (similar results were obtained when this condition was checked explicitly by sampling the outcome of several games between new and old strategies <ref> [13] </ref>). So, our framework might be used to explain the performance of this system. The separation between competitive algorithm and strategy learning algorithm seems natural because strategy learning is an optimization problem with a fixed cost function; this type of optimization has been well-studied. <p> Then, find s 0 t, t 0 s 0 , and so on. This is essentially the competitive algorithm used in Samuel's checkers learning system, and is very similar to that used in a recent backgammon learning system <ref> [13] </ref>. The main problem with this competitive algorithm is that intransitivity may exist, and a particular learner may simply keep choosing strategies in a cycle. Intransitivity has been observed when using this competitive algorithm for backgammon [13]. <p> system, and is very similar to that used in a recent backgammon learning system <ref> [13] </ref>. The main problem with this competitive algorithm is that intransitivity may exist, and a particular learner may simply keep choosing strategies in a cycle. Intransitivity has been observed when using this competitive algorithm for backgammon [13]. Even a randomized learner may get stuck in such a cycle for a long time. The following example demonstrates this. Example 1 (Small Game Trees) This example considers games represented by game trees.
Reference: [14] <author> Rosin, C., and R. Belew. </author> <title> (1995) Finding Opponents Worth Beating: Methods for Competitive Co-evolution. </title> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms. </booktitle>
Reference-contexts: This is the type of method that we consider. The main intuition is that it can bootstrap the level of play, from initial uninformed strategies to expert players. Examples include Samuel's classic work on checkers [15], systems using reinforcement learning [21], and using a genetic algorithm <ref> [13, 14] </ref>. The definition of "game" used in this paper is very inclusive and allows us to also consider domains other than traditional board games, such as evolving sorting networks [4], minimax controller design [16], and discrete approximations to differential games [3, 18].
Reference: [15] <author> Samuel, A. </author> <title> (1963) Some Studies in Machine Learning Using the Game of Checkers. </title> <booktitle> Computers and Thought. </booktitle>
Reference-contexts: This is the type of method that we consider. The main intuition is that it can bootstrap the level of play, from initial uninformed strategies to expert players. Examples include Samuel's classic work on checkers <ref> [15] </ref>, systems using reinforcement learning [21], and using a genetic algorithm [13, 14]. <p> The main reason for making a distinction between first-player and second-player strategies is the existence of a perfect strategy for one player but not the other. As a concrete example of how this framework might be applied, consider Samuel's original work on learning evaluation functions for checkers from self-play <ref> [15] </ref>. Games were played between a fixed Beta player and a learning Alpha player. Alpha would learn from these games via Samuel's reinforcement learning algorithm; this corresponds to the strategy learning algorithm. When Alpha was finally able to defeat Beta, Beta was replaced by Alpha.
Reference: [16] <author> Sebald, A.V., and J. Schlenzig. </author> <title> (1994) Minimax Design of Neural Net Controllers for Highly Uncertain Plants. </title> <journal> IEEE Transactions on Neural Networks Jan. 1994. </journal> <volume> 5: </volume> <pages> 73-82. </pages>
Reference-contexts: The definition of "game" used in this paper is very inclusive and allows us to also consider domains other than traditional board games, such as evolving sorting networks [4], minimax controller design <ref> [16] </ref>, and discrete approximations to differential games [3, 18]. To study this, our framework for game learning relies on the existence of a strategy learning algorithm, that is able to learn strategies which defeat a given set of opponents.
Reference: [17] <author> Chvatal, V. </author> <title> (1979) A greedy heuristic for the set-covering problem. </title> <journal> Mathematics of Operations Research 4: </journal> <pages> 233-235. </pages>
Reference-contexts: Using L 0 2 repeatedly, a set-covering approximation can be used to create a second-player strategy learning algorithm that returns sets of size at most k 0 = k log jHj <ref> [17] </ref>. If a randomized strategy learning algorithm is available that can find k 0 second-player strategies all at once, satisfying the (p; q)-randomization criterion in the space of remaining feasible sets of k 0 second-player strategies, then the roles in Theorem 4 may be reversed, giving the following corollary.
Reference: [18] <author> Sims, K. </author> <title> (1994) Evolving 3D Morphology and Behavior by Competition. </title> <booktitle> Artificial Life IV </booktitle>
Reference-contexts: The definition of "game" used in this paper is very inclusive and allows us to also consider domains other than traditional board games, such as evolving sorting networks [4], minimax controller design [16], and discrete approximations to differential games <ref> [3, 18] </ref>. To study this, our framework for game learning relies on the existence of a strategy learning algorithm, that is able to learn strategies which defeat a given set of opponents. The competitive algorithm then repeatedly uses a strategy learning algorithm to discover strong strategies for the game.
Reference: [19] <author> Anthony, M., G. Brightwell, D. Cohen, and J. Shawe-Taylor. </author> <title> (1992) On Exact Specification by Examples. </title> <address> COLT 92. </address>
Reference-contexts: Define the specification number k for G to be the size of the smallest teaching set. These definitions follow the corresponding ones for concept learning <ref> [19, 20] </ref>. Since we are considering exact learning, we seek bounds on learning time that are polynomial in k, as well as in lg (jHj) and lg (jX j). The following lemma shows that the dependence on k is necessary.
Reference: [20] <author> Goldman, S.A., and M.J. Kearns. </author> <title> (1991) On the Complexity of Teaching. </title> <address> COLT 91. </address>
Reference-contexts: Define the specification number k for G to be the size of the smallest teaching set. These definitions follow the corresponding ones for concept learning <ref> [19, 20] </ref>. Since we are considering exact learning, we seek bounds on learning time that are polynomial in k, as well as in lg (jHj) and lg (jX j). The following lemma shows that the dependence on k is necessary.
Reference: [21] <author> Tesauro, G. </author> <title> (1995) Temporal Difference Learning and TD-Gammon. </title> <journal> CACM 38:3. </journal> <volume> 10 </volume>
Reference-contexts: This is the type of method that we consider. The main intuition is that it can bootstrap the level of play, from initial uninformed strategies to expert players. Examples include Samuel's classic work on checkers [15], systems using reinforcement learning <ref> [21] </ref>, and using a genetic algorithm [13, 14]. The definition of "game" used in this paper is very inclusive and allows us to also consider domains other than traditional board games, such as evolving sorting networks [4], minimax controller design [16], and discrete approximations to differential games [3, 18]. <p> In some empirical work on game learning, the competitive algorithm is not as explicitly defined. For example, Tesauro's backgammon learning system <ref> [21] </ref> uses reinforcement learning without explicitly checking whether new strategies defeat old ones, as a competitive algorithm would do.
References-found: 21

