URL: ftp://ftp.cm.deakin.edu.au/pub/TR/Computing/TR-C93-32.ps.gz
Refering-URL: http://gollum.cm.deakin.edu.au/techreports.html
Root-URL: 
Email: jackie@deakin.edu.au  nsk@deakin.edu.au  
Title: Taxonomy of Real-Time Scheduling 1 Taxonomy of Real-Time Scheduling  
Author: Jackie Silcock and Swamy Kutti 
Note: 1.0 Introduction  
Address: Geelong, Australia.  
Affiliation: School of Computing and Mathematics Deakin University  
Abstract: In real-time systems, the basic criteria are to satisfy both timeliness and correctness of real-time solutions. The former requirement is controlled by both resource and task scheduling at operating system level whereas the latter is achieved at language level. The efficiency of a scheduling strategy depends upon several factors such as hardware configuration, type of real-time application and the complexity of real-time problem. This report presents a classification of scheduling techniques in order to help designers model their real-time systems with appropriate scheduling schemes to meet their timeliness characteristic. It also aims to explore the scheduling properties and scope of research in real-time computing. In general, real-time systems are classified into soft and hard real-time systems. In soft real-time systems, the tasks are expected to perform as fast as possible. The response time can vary quite considerably, but at the same time can satisfy the deadline to produce the required result. In other words, this type of systems has soft requirement of meeting with the deadline property. For example, airline reservation systems, library catalogue systems, banking systems, etc. belong to this category. On the other hand, hard real-time systems have to cope with very stringent requirement of timeliness constraint. If the tasks dont meet the specified deadline, systems of this type would fail to produce results. These systems, in general, are of process control type, e.g. ight control, nuclear reactor control, defense command and control systems, automated manufacturing plants, etc. A common characteristic for both types of real-time systems is the correctness of the results without which the reliability of a real-time system cannot be assured. Although the timeliness and correctness constraints are the basic characteristics of both types of real-time systems, the hard real-time systems specify even more requirements to guarantee the total reliability of a system. Further constraints include the predictability of system behaviour, adaptability and intelligent scheduling. 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> Stankovic, J. A., </author> <title> Misconceptions about Real-Time Computing. </title> <booktitle> Computer 21(10) 10-19, </booktitle> <month> Oct., </month> <year> 1988. </year>
Reference: [2] <author> Stankovic, J. A., Ramamritham, K., </author> <title> Tutorial:Hard Real-TIme Systems. </title> <publisher> Computer Society Press. </publisher>
Reference-contexts: Real-time scheduling is an important area of research in real-time computing as scheduling theory addresses the problem of meeting the specified timing requirements of the system <ref> [2] </ref>. <p> Relatively little work has been carried out on efficient scheduling algorithms for real-time systems and has been limited to the simplest situations <ref> [2] </ref>. Most of the simple schedulers have been implemented along with the kernels of microcomputer based embedded systems. Recent work has extended to more complicated situations such as scheduling tasks with resource constraints, distributed real-time scheduling and transient overload and stochastic execution time. <p> Much more work is required to enable more realistic and general situations to be handled. Theoretical results can assist greatly by providing an insight on the solution to practical problems. However, in many cases theoretical results depend upon many unrealistic assumptions <ref> [2] </ref>. For example, many scheduling techniques make the assumptions, for the basis of many of their decisions, that tasks are independent and do not share any data or resources. This assumption shows a major aw in many of the scheduling algorithms proposed to date.
Reference: [3] <author> Krishna, C.M., Lee, </author> <title> Y.H., Real-Time Systems. </title> <booktitle> Computer, </booktitle> <pages> 10-11, </pages> <month> May, </month> <year> 1991. </year>
Reference: [4] <author> Rajkumar, R., </author> <title> Synchronization in Real-Time Systems.A Priority Inheritance Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference: [5] <author> Bennett, S., </author> <title> Real-Time Computer Control: An Introduction. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference: [6] <author> Stoyenko, W., Stoyenko, A., </author> <title> Constructing Predictable Real Time Systems. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference: [7] <author> Hellerman, H. , Conroy T.F. </author> , <title> Computer System Performance. </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: scheduling schemes. 1.1 What is Real-Time Scheduling? A scheduler, a central part of an operating system kernel, provides data structures for representing tasks run-time characteristics as well as the state of the machine and implementing an algorithm that uniquely specifies which task is to receive next service by a resource <ref> [7] </ref>. Real-time scheduling is an important area of research in real-time computing as scheduling theory addresses the problem of meeting the specified timing requirements of the system [2].
Reference: [8] <author> Cheng, S-C. , Stankovic, J.A., Ramamrithan, K. </author> , <title> Scheduling Algorithms for Hard Real-Time Systems - A Brief Survey. Hard Real-Time Systems[2], </title> <publisher> Computer Society Press, pp.150-173. [9] van Tilborg, </publisher> <editor> A. M., Koob, G.M., </editor> <title> Foundations of Real-Time Computing: Scheduling and Resource Management. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: General Case Heuristic methods can be used for the general case of scheduling problems, designed on a straightforward topological search scheme and the critical path method combined with a heuristic rule. These methods can also be applied to dynamic scheduling problems <ref> [8] </ref>. Efficient algorithms have been found for task sets whose precedence graph is a tree. <p> At any instant the task with the nearest deadline will execute. This can be shown to be globally optimal and full processor utilization is possible. H_DY_C_P Scheduling on Multiprocessors Locke, Tokuda and Jensen <ref> [8] </ref> compared several simple dynamic scheduling policies and found that least-laxity-first and earliest-deadline-first policies are both good heuristic policies which have already been discussed. Scheduling non-preemptive tasks on Centralized Systems (H_DY_C_NP) This section considers dynamic scheduling mechanisms for nonpreemptable tasks.
Reference: [10] <author> Lawler, E.L., </author> <title> Optimal Sequencing of a Single Machine Subject to Precedence Constraints., </title> <journal> Management Science, </journal> <volume> 19, </volume> <year> 1973. </year>
Reference-contexts: The algorithm is then run on the remaining tasks to find the task with the next latest Taxonomy of Real-Time Scheduling 9 deadline to run second last, and so on. In the algorithm proposed by Lawler <ref> [10] </ref> the task which has no successors and the lowest cost function is placed last. The algorithm works backwards from there to find an optimal sequence. If, instead of cost functions, a deadline is associated with each task, the first-to-last rule is used.
Reference: [11] <author> Cheng, S-C., Stankovic, J.A., Ramamritham, K., </author> <title> Dynamic Scheduling of Groups of Tasks with Precedence Constraints in Distributed Hard Real Time Systems. </title> <booktitle> IEEE Real-Time System Symposium, </booktitle> <year> 1986, </year>
Reference-contexts: If the task cannot be scheduled on that node, a distributed scheduling policy decides to which node the task should be migrated. Scheduling precedence constrained tasks on Distributed Systems (H_DY_D_PC) This section considers dynamic scheduling mechanisms for precedence constrained tasks. An algorithm derived by Cheng, Stankovic and Ramamritham <ref> [11] </ref> partitions groups of tasks which have the same deadline into subgroups, which are then scheduled to different nodes to run in parallel. This algorithm attempts to dynamically schedule groups of tasks which have arbitrary precedence constraints on a distributed system.
Reference: [12] <author> Horn, </author> <title> W.A., Some Simple Scheduling Algorithms, </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 21, </volume> <year> 1974 </year>
Reference-contexts: When a burst of requests arrive at the same time, it is the deadline of the tasks that should be taken into consideration. The scheduler, in this case, schedules on the basis of earliest-deadline-first. To implement this scheme, the scheduler must know the deadlines of all the tasks. Horn <ref> [12] </ref> developed an algorithm to schedule the tasks based upon the earliest-deadline-first policy. <p> H_ST_C_MI_P_AT Scheduling on Multiprocessors Horn developed an algorithm for scheduling independent, arbitrary timed, pre-emptable tasks on a multiprocessor. Little appeared to be known about simple minimization algorithms for multimachines at the time this paper was written <ref> [12] </ref>. The assumptions made are: - all machines are identical, and - there is no constraint on finish time. The method which ensures minimum delay is as follows. Assuming that there are m identical machines tasks are ordered according to increasing deadlines.
Reference: [13] <author> Bratley, P., Florian, M., Robillard, P., </author> <title> Scheduling with Earliest Start and Due Date Constraints., </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 18, (4), </volume> <year> 1977. </year>
Reference-contexts: Some systems with less time-critical tasks could adopt this strategy. Moore [32] showed that the earliest-deadline-first policy was optimal for independent, nonpreemptable tasks executing upon a uniprocessor, making the assumption that the set of tasks had the same ready time. Bratley, Florian and Robillard <ref> [13] </ref> developed an enumeration algorithm which implicitly determines the schedule for tasks whose deadlines and ready times are arbitrary. The aim is to determine a feasible schedule, if one exists, for a task set that will minimize the total time elapsed.
Reference: [14] <author> Simons, B., </author> <title> A Fast Algorithm for Multiprocessor Scheduling., </title> <booktitle> Proc. 21st Annual Symposium on Foundation of Computer Science, </booktitle> <year> 1980. </year>
Reference-contexts: Simons <ref> [14] </ref> proposed an algorithm to solve the problem of scheduling independent, nonpreemptable tasks on a multiprocessor.
Reference: [15] <author> Stankovic, J. A., Ramamritham, K., Cheng, S-C., </author> <title> Evaluation of a Flexible Task Scheduling Algorithm for Distributed Hard Real-Time Systems., </title> <journal> IEEE Transactions on Bibliography 18 Computers, </journal> <volume> 34(12) </volume> <pages> 1130-43, </pages> <month> December </month> <year> 1985. </year>
Reference: [16] <author> Liu, C.L. ,Layland, J.W., </author> <title> Scheduling Algorithms for Multiprogramming in a Hard Real-Time Environment., </title> <journal> Journal for the Association for Computing Machinery, </journal> <volume> 20(1) </volume> <pages> 46-61, </pages> <year> 1973. </year>
Reference-contexts: Static scheduling algorithms allow software engineers to reason, at an abstract level, regarding timing correctness of the system <ref> [16] </ref> and to make predictions about the effect of any future changes to the system [9]. The static approach is, however, rigid and extra tasks can only be added with great difficulty. <p> H_ST_C_MI_P_PT Scheduling on Uniprocessors In this case, tasks are assumed to be ordered according to their importance and the scheduler follows strictly the priority value for assigning the resources. This technique may fail when the scheduler is given with periodic tasks of various duration. Liu and Layland <ref> [16] </ref> proposed the rate monotonic priority scheduling approach for scheduling periodic tasks. In this approach, the priority is given according to frequency of tasks requests rather than its importance. <p> The processor utilization factor is the fraction of processor time spent in execution of a task set. Since the rate monotonic assignment is optimal the utilization factor for this assignment method will be greater than or equal to that for and other priority assignment method <ref> [16] </ref>. The least upper bound imposed upon processor utilization by the requirement for real-time guaranteed service can approach ln (2) or approximately 70% for large task sets. Another fixed priority scheme, as discussed by Texiera [27], made the assumption that the period of a task may differ from its deadline. <p> H_DY_C_P Scheduling on Uniprocessors The earliest-deadline-first algorithm was shown by Dertouzos [17] to be optimal. This was extended by Ramamritham and Stankovic [18] to include the run-time cost of a guarantee scheme. Liu and Laylands deadline-driven scheduling algorithm <ref> [16] </ref> is a dynamic scheduling algorithm in which priorities are assigned to tasks according to the deadlines of current requests i.e. the task with the closest deadline will be assigned the highest priority and that with the furthest deadline will be assigned the lowest priority.
Reference: [17] <author> Sha, L., Goodenough, J.B., </author> <title> Real-Time Scheduling Theory and Ada., </title> <journal> IEEE Computer, </journal> <volume> April </volume> 1990 53-62. 
Reference-contexts: Hard real-time scheduling can be further divided into static or dynamic type as shown in Figure 1.1. Static schedules for tasks are calculated off-line using the tasks characteristics, which must be known beforehand. This approach is manageable for simple systems but rapidly becomes too complex for large systems <ref> [17] </ref>. Dynamic schedules are calculated in-situ, tasks characteristics are not previously known and tasks can be dynamically activated. The static approach is cheap and simple to design and build for simple systems. <p> This produces inexible systems which are difficult to understand and maintain and expensive to upgrade <ref> [17] </ref>. The static on-line technique involves the schedule being calculated on-line based on task numbers and priorities which are known and are fixed prior to the start of execution. In the dynamic approach schedules are calculated in-situ using task characteristics which are not known before execution. <p> These algorithms can be divided into those that can be used for uniprocessors and for multiprocessors. H_DY_C_P Scheduling on Uniprocessors The earliest-deadline-first algorithm was shown by Dertouzos <ref> [17] </ref> to be optimal. This was extended by Ramamritham and Stankovic [18] to include the run-time cost of a guarantee scheme.
Reference: [18] <author> Burns A.,Wellings A., </author> <title> Real-Time Systems and their Programming Languages. </title> <publisher> Addison-Wesley Publishing Company. </publisher>
Reference-contexts: Taxonomy of Real-Time Scheduling 3 If the system has both hard and soft deadlines a single layer of priorities would appear to be inadequate, hence task priorities can be manipulated by changing the periods of the tasks according to their importance <ref> [18] </ref>. Hard real-time scheduling can be further divided into static or dynamic type as shown in Figure 1.1. Static schedules for tasks are calculated off-line using the tasks characteristics, which must be known beforehand. This approach is manageable for simple systems but rapidly becomes too complex for large systems [17]. <p> These algorithms can be divided into those that can be used for uniprocessors and for multiprocessors. H_DY_C_P Scheduling on Uniprocessors The earliest-deadline-first algorithm was shown by Dertouzos [17] to be optimal. This was extended by Ramamritham and Stankovic <ref> [18] </ref> to include the run-time cost of a guarantee scheme.
Reference: [19] <author> SofTech Inc., </author> <title> Designing Real-Time Systems in Ada. </title> <note> Final Report 1123-1.SofTech,Inc., </note> <month> January, </month> <year> 1986. </year>
Reference: [20] <author> Freedman, A., Lees, R., </author> <title> Real-Time Computer Systems. Crane, </title> <publisher> Russak & Company, Inc. </publisher>
Reference: [21] <author> Levi, S., Agrawala, A., </author> <title> Real-Time System Design, </title> <publisher> McGraw-Hill Publishing Company. </publisher>
Reference-contexts: is difficult to design as it involves process level reconfiguration [9], but is adaptable and exible. 1.3 Static Scheduling Algorithms A scheduling scheme is static if a priority is assigned to a computation only once (probably before the computation starts executing), while a dynamic scheme allows changing priority with time. <ref> [21] </ref> Algorithms in which the number of tasks in the system and their priorities are known before run-time are known as Static Scheduling Algorithms. <p> Taxonomy of Real-Time Scheduling 5 Horn developed a simple ow problem formulation which permits minimizing maximum lateness, for the general machine case. This scheme can be improved by taking into consideration both deadline and the amount of execution time required by each task before it will miss its deadline <ref> [21] </ref>. The least-slack-time algorithm gives the highest priority to the task with the smallest slack time. Mok [23] showed that this algorithm, like the rate-monotonic algorithm, is an optimal algorithm for scheduling periodic tasks on a single processor.
Reference: [22] <author> Sha, L., Lehoczky, J., Rajkumar, R., </author> <title> Solutions for some Practical Problems in Prioritized Pre-emptive Scheduling. </title> <booktitle> IEEE Real-Time Systems Symposium, </booktitle> <year> 1986. </year>
Reference: [23] <author> Mok, A., </author> <title> Fundamental Design Problems of Distributed Systems For The Hard Real Time Environment. </title> <type> PhD thesis, </type> <institution> M.I.T., </institution> <year> 1983. </year> <editor> [24]Hatley, D., Pirbai I., </editor> <title> Strategies for Real-Time System Specification, </title> <publisher> Dorset House Publishing Co., Inc. </publisher>
Reference-contexts: This scheme can be improved by taking into consideration both deadline and the amount of execution time required by each task before it will miss its deadline [21]. The least-slack-time algorithm gives the highest priority to the task with the smallest slack time. Mok <ref> [23] </ref> showed that this algorithm, like the rate-monotonic algorithm, is an optimal algorithm for scheduling periodic tasks on a single processor. When deadlines of all sporadic tasks are met, then the earliest-deadline-first and least-slack-time approaches are equivalent to rate monotonic scheduling.
Reference: [25] <author> Lehoczky, J.P., Sha, L. and Ding, Y., </author> <title> The rate monotonic scheduling algorithm: exact characterization and average case behaviour. </title> <booktitle> Proceedings of the 10th IEEE Real-Time Systems Symposium. </booktitle> <month> December </month> <year> 1989, </year> <pages> 166-171. </pages>
Reference: [26] <author> Martel, C., </author> <title> Preemptive scheduling with release times, deadlines and due times. </title> <journal> J. ACM, </journal> <volume> 29(3), </volume> <year> 1982. </year>
Reference-contexts: Assuming that there are m identical machines tasks are ordered according to increasing deadlines. Each of the first m tasks is assigned to the first place on each machine, each of the next m tasks to the second place and so on. Martel <ref> [26] </ref> extended Horns approach to consider processors of differing speeds. Scheduling periodic tasks on Centralized Systems (H_ST_C_MI_P_PT) This section considers scheduling mechanisms for preemptable, mutually independent tasks which have periodic timing. These algorithms can be divided again into those that can be used for uniprocessors and for multiprocessors.
Reference: [27] <author> Texiera, T., </author> <title> Static Priority Interrupt Scheduling.,Proc. </title> <booktitle> of the Seventh Texas Conference on Computing Systems, </booktitle> <month> November, </month> <year> 1978. </year>
Reference-contexts: The least upper bound imposed upon processor utilization by the requirement for real-time guaranteed service can approach ln (2) or approximately 70% for large task sets. Another fixed priority scheme, as discussed by Texiera <ref> [27] </ref>, made the assumption that the period of a task may differ from its deadline.
Reference: [28] <author> Lehoczky, J.P., Sha, L., </author> <title> Performance of bus scheduling algorithms., </title> <booktitle> Performance 86, </booktitle> <year> 1986. </year>
Reference-contexts: Another fixed priority scheme, as discussed by Texiera [27], made the assumption that the period of a task may differ from its deadline. Sha et al <ref> [28] </ref> attempted to achieve better processor utilization by modifying the priority of tasks by halving their periods and executing half of the tasks at the reduced period (first half at the task at first reduced period and second half of the tasks at the second reduced period and so on).
Reference: [29] <author> Davari, S., Dhall, S. K., </author> <title> An on line algorithm for real-time tasks allocation., </title> <booktitle> IEEE Real-Time Systems Symposium, </booktitle> <month> December </month> <year> 1986. </year>
Reference-contexts: This method allows the user to take into account the semantic importance of the tasks. H_ST_C_MI_P_PT Scheduling on Multiprocessors Generally tasks are partitioned among the processors and then the rate monotonic scheme or earliest deadline first scheme is used to schedule tasks on the individual processors. Davari and Dhall <ref> [29] </ref> used a bin-packing algorithm to determine a sub-optimal partition pattern for periodic tasks and the earliest-deadline-first scheme was used on the individual processors. A best-fit partition scheme was developed by Bannister and Trivedi [30] that could be used with the rate monotonic scheme or earliest deadline first scheme. <p> A best-fit partition scheme was developed by Bannister and Trivedi [30] that could be used with the rate monotonic scheme or earliest deadline first scheme. Dhall and Liu [31] developed first-fit and next-fit partition schemes which were improved upon by Davari and Dhall <ref> [29] </ref> giving an improved next-fit scheme. Taxonomy of Real-Time Scheduling 7 Scheduling non-preemptive tasks on Centralized Systems (H_ST_C_MI_NP) This section considers scheduling mechanisms for nonpreemptable, mutually independent tasks. These algorithms can be divided into those that can be used for uniprocessors and for multiprocessors.
Reference: [30] <author> Bannister, J. A., Trivedi K. S., </author> <title> Task allocation in fault-tolerant distributed systems., </title> <publisher> Bibliography 19 Acta Informatica, Springer-Verlag, </publisher> <year> 1983. </year>
Reference-contexts: Davari and Dhall [29] used a bin-packing algorithm to determine a sub-optimal partition pattern for periodic tasks and the earliest-deadline-first scheme was used on the individual processors. A best-fit partition scheme was developed by Bannister and Trivedi <ref> [30] </ref> that could be used with the rate monotonic scheme or earliest deadline first scheme. Dhall and Liu [31] developed first-fit and next-fit partition schemes which were improved upon by Davari and Dhall [29] giving an improved next-fit scheme.
Reference: [31] <author> Dhall, S. K., Liu, C.L., </author> <title> On a real-time scheduling problem., Operation Research,26(1),1978. </title>
Reference-contexts: A best-fit partition scheme was developed by Bannister and Trivedi [30] that could be used with the rate monotonic scheme or earliest deadline first scheme. Dhall and Liu <ref> [31] </ref> developed first-fit and next-fit partition schemes which were improved upon by Davari and Dhall [29] giving an improved next-fit scheme. Taxonomy of Real-Time Scheduling 7 Scheduling non-preemptive tasks on Centralized Systems (H_ST_C_MI_NP) This section considers scheduling mechanisms for nonpreemptable, mutually independent tasks.
Reference: [32] <author> Moore, J. M., </author> <title> An n job, one machine sequencing algorithm for minimizing the number of late jobs., </title> <journal> Management Science, </journal> <volume> 15(1)., </volume> <year> 1986. </year>
Reference-contexts: In other words, while completing a task the overhead due to context switching is minimum. However this scheme has a poor performance in terms of the class of problems that can be feasibly scheduled. Some systems with less time-critical tasks could adopt this strategy. Moore <ref> [32] </ref> showed that the earliest-deadline-first policy was optimal for independent, nonpreemptable tasks executing upon a uniprocessor, making the assumption that the set of tasks had the same ready time.
Reference: [33] <author> Blazewicz, J., </author> <title> Scheduling dependent tasks with different arrival times to meet deadlines. Modelling and Performance Evaluation of Computer Systems, </title> <publisher> North-Holland., </publisher> <year> 1976. </year>
Reference-contexts: If, instead of cost functions, a deadline is associated with each task, the first-to-last rule is used. In this rule, tasks are sequenced from first to last by choosing, from the tasks that are currently available, the task with the earliest possible deadline. Blazewicz <ref> [33] </ref> showed that by adjusting the deadlines and ready times according to the precedence constraints, it is possible to use the earliest deadline first policy, taking the precedence into account implicitly, for a set of preemptable tasks with arbitrary ready times and precedence constraints.
Reference: [34] <author> Chandy, K. M., Reynolds, P. F., </author> <title> Scheduling Partially ordered tasks with probabilistic execution times. </title> <booktitle> Proceeding of the Fifth Symposium on Operating System Principles. </booktitle> <year> 1975. </year>
Reference-contexts: These methods can also be applied to dynamic scheduling problems [8]. Efficient algorithms have been found for task sets whose precedence graph is a tree. Chandy and Reynolds <ref> [34] </ref> showed that the highest-level-first policy was optimal for two processor systems, where a tasks level in the tree is the length of the longest path between that task and any leaf task in the tree.
Reference: [35] <author> Muntz, R. R., Coffman, E. G., </author> <title> Preemptive scheduling of real-time tasks on multiprocessor systems., </title> <journal> J. ACM, </journal> <volume> 17(2), </volume> <month> April </month> <year> 1970. </year>
Reference-contexts: Chandy and Reynolds [34] showed that the highest-level-first policy was optimal for two processor systems, where a tasks level in the tree is the length of the longest path between that task and any leaf task in the tree. Muntz and Coffman <ref> [35] </ref> developed a scheduling algorithm to calculate the minimum schedule length for preemptable tasks in a reverse precedence tree. It assigns tasks to the CPU from the root to the leaves in a bottom-up fashion.
Reference: [36] <author> Efe, K., </author> <title> Heuristic models of task assignment scheduling in distributed systems, </title> <booktitle> IEEE Computer., </booktitle> <month> June </month> <year> 1982. </year>
Reference-contexts: Heuristic algorithms have been developed by Efe and Lo. Efe's algorithm <ref> [36] </ref> clusters tasks together and assigns clusters to processors, thus reducing communication costs. The processors' loads are then balanced by task migration. Lo's heuristic algorithm [37] adds a cost function which maximizes the concurrent execution of the tasks and reduces total execution and communication costs.
Reference: [37] <author> Lo, V. M., </author> <title> Heuristic algorithms for task assignment in distributed systems., </title> <booktitle> Proc. International Conference on Distributed Computing Systems., </booktitle> <year> 1984. </year>
Reference-contexts: Heuristic algorithms have been developed by Efe and Lo. Efe's algorithm [36] clusters tasks together and assigns clusters to processors, thus reducing communication costs. The processors' loads are then balanced by task migration. Lo's heuristic algorithm <ref> [37] </ref> adds a cost function which maximizes the concurrent execution of the tasks and reduces total execution and communication costs. Integer programming models have been developed to find optimal scheduling algorithms. Taxonomy of Real-Time Scheduling 10 1.4 Dynamic Scheduling Algorithms (H_DY) Dynamic schedules are calculated in-situ.
Reference: [38] <author> Dertouzos, M., </author> <title> Control robotics: the procedural control of physical processes.,Proc. </title> <booktitle> of the IFIP Congress, </booktitle> <year> 1974. </year>
Reference: [39] <author> Ramamritham, K., Stankovic, J., </author> <title> Dynamic task scheduling in distributed hard real-time systems., </title> <journal> IEEE Software, </journal> <volume> 1(3), </volume> <year> 1984. </year>
Reference: [40] <author> Baker, K., Su, Z.-S., </author> <title> Sequencing with due-dates and early start times to minimize maximum tardiness., </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 21, </volume> <year> 1974. </year>
Reference-contexts: Scheduling non-preemptive tasks on Centralized Systems (H_DY_C_NP) This section considers dynamic scheduling mechanisms for nonpreemptable tasks. Baker and Su <ref> [40] </ref> compared four simple heuristic algorithms which determine the scheduling order according to deadline, ready time, the average of deadline and ready time and both deadline and ready time, and found that the latter two performed better than the previous two.
Reference: [41] <author> Kutti, S., Garner, B. J., </author> <title> Evaluation of knowledge-based kernel for distributed systems, </title> <booktitle> Australian Computer Science Conference (ACSC-13), Editor Prof. </booktitle> <institution> C.S. Wallace, Monash University, Clayton, Victoria, </institution> <month> Australia-Jan </month> <year> 1990. </year>
Reference-contexts: H_ST_C_MI_NP Scheduling on Multiprocessors The zig_zag scheduling scheme is basically a task allocation scheme among the processors. The goal of this technique is to allocate the tasks in such a way that each processor gets some duty cycle. The following assumptions are made in the design of this algorithm <ref> [41] </ref>. -all tasks have the same period; -tasks may have different run times; -processors are identical -deadline (or period) and run time (weight) of each task are known.
Reference: [42] <author> Kutti, S., </author> <title> Taxonomy of Parallel Processing and Definitions, </title> <booktitle> Parallel Computing 2 (1985), </booktitle> <publisher> Elsevier Science Publishers B.V.(North-Holland). </publisher>
Reference-contexts: In these systems the processors are located at a single point. Whether it is a uniprocessor or multiprocessor, the kernel exists in the central memory. Interprocessor communication is via the central memory and can be considered negligible. For more details on the definition of multiprocessors refer to <ref> [42] </ref>. Taxonomy of Real-Time Scheduling 4 Meaning of pre-emption strategy Pre-emption is a technique which extends the effectiveness of priority scheduling to meet the deadlines of tasks in a dynamic manner.
References-found: 40

