URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/damper.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: microcen@batelco.com.bh  tgd@cs.orst.edu  
Title: Achieving High-Accuracy Text-to-Speech with Machine Learning  
Author: Ghulum Bakiri Thomas G. Dietterich 
Date: May 30, 1997.  
Note: Draft of  
Address: Corvallis, Oregon 97331 USA  
Affiliation: Department of Computer Science University of Bahrain Isa Town, Bahrain  Department of Computer Science Oregon State University  
Abstract: In 1987, Sejnowski and Rosenberg developed their famous NETtalk system for English text-to-speech. This chapter describes a machine learning approach to text-to-speech that builds upon and extends the initial NETtalk work. Among the many extensions to the NETtalk system were the following: a different learning algorithm, a wider input "window", error-correcting output coding, a right-to-left scan of the word to be pronounced (with the results of each decision influencing subsequent decisions), and the addition of several useful input features. These changes yielded a system that performs much better than the original NETtalk system. After training on 19,002 words, the system achieves 93.7% correct pronunciation of individual phonemes and 64.8% correct pronunciation of whole words (where the pronunciation must exactly match the dictionary pronunciation to be correct). Based on the judgements of three human participants in a blind assessment study, our system was estimated to have a serious error rate of 16.7% (on whole words) compared to an error rate of 26.1% for the DECTalk3.0 rulebase.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bakiri, G. </author> <year> (1991). </year> <title> Converting English text to speech: A machine learning approach. </title> <type> Tech. rep. </type> <institution> 91-30-2, Department of Computer Science, Oregon State University, Corvallis, OR. </institution> <note> 14 Bose, </note> <author> R. C., & Ray-Chaudhuri, D. K. </author> <year> (1960). </year> <title> On a class of error-correcting binary group codes. </title> <journal> Information and Control, </journal> <volume> 3, </volume> <pages> 68-79. </pages>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: The algorithm halts when all examples at a node fall in the same class. At this point, a leaf node is created and labeled with the class in question. The basic operation of ID3 is quite similar to the CART algorithm developed by <ref> (Breiman, Friedman, Olshen, & Stone, 1984) </ref> and to the tree-growing method developed by (Lucassen, 1983). In our implementation of ID3, we did not employ windowing, 2 early stopping, or any kind of pruning.
Reference: <author> Breiman, L. </author> <year> (1996). </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24 (2), </volume> <pages> 123-140. </pages>
Reference-contexts: In practice, this permits roughly one fourth of the decision trees to make errors on each test example. Another intuition is that ECOC is a form of committee or ensemble method, closely related to the methods of bagging <ref> (Breiman, 1996) </ref> and boosting (Freund & Schapire, 1996; Quinlan, 1996). According to this view, ECOC is an interesting method for constructing a group of diverse yet accurate classifiers.
Reference: <author> Dietterich, T. G., & Bakiri, G. </author> <year> (1995). </year> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 263-286. </pages>
Reference-contexts: Ties are broken in favor of phoneme/stress pairs that occurred more often in the training data. This is called observed decoding. Previous studies have shown that observed decoding improves the performance of ID3 substantially, but that it is still not as good as the performance of backpropagation <ref> (Dietterich et al., 1995) </ref>. Table 4 summarizes the performance of ID3 applied using the NETtalk formulation of the text-to-speech task. Performance is given in percentage correct at various levels of aggregation. The column labeled "Stress" gives the number of stress symbols correctly predicted on the test set. <p> The row labeled "NETtalk" gives the test set performance of backpropagation on a network with 160 hidden units and cross-validated early stopping <ref> (see Dietterich, Hild & Bakiri, 1995) </ref>. % correct (1000-word data set) Decision Tree Data set used Level of Aggregation data (mean) for evaluation Word Letter Phoneme Stress Bit (mean) Leaves Depth Training set 96.6 99.5 99.8 99.6 100.0 269.9 29.3 Test set 12.5 69.6 81.3 79.2 96.3 269.9 29.3 NETtalk 14.3 <p> According to this view, ECOC is an interesting method for constructing a group of diverse yet accurate classifiers. Experimentally, the error-correcting output coding method has been shown to give excellent results with both decision tree and neural network learning algorithms <ref> (Dietterich & Bakiri, 1995) </ref>. We applied BCH code design methods (Bose & Ray-Chaudhuri, 1960; Hocquenghem, 1959; Lin & Costello, 1983) to design good error correcting codes of various lengths for both the separate and combined phoneme/stress configurations.
Reference: <author> Dietterich, T. G., Hild, H., & Bakiri, G. </author> <year> (1995). </year> <title> A comparison of ID3 and backpropagation for English text-to-speech mapping. </title> <journal> Machine Learning, </journal> <volume> 18, </volume> <pages> 51-80. </pages>
Reference-contexts: Ties are broken in favor of phoneme/stress pairs that occurred more often in the training data. This is called observed decoding. Previous studies have shown that observed decoding improves the performance of ID3 substantially, but that it is still not as good as the performance of backpropagation <ref> (Dietterich et al., 1995) </ref>. Table 4 summarizes the performance of ID3 applied using the NETtalk formulation of the text-to-speech task. Performance is given in percentage correct at various levels of aggregation. The column labeled "Stress" gives the number of stress symbols correctly predicted on the test set. <p> The row labeled "NETtalk" gives the test set performance of backpropagation on a network with 160 hidden units and cross-validated early stopping <ref> (see Dietterich, Hild & Bakiri, 1995) </ref>. % correct (1000-word data set) Decision Tree Data set used Level of Aggregation data (mean) for evaluation Word Letter Phoneme Stress Bit (mean) Leaves Depth Training set 96.6 99.5 99.8 99.6 100.0 269.9 29.3 Test set 12.5 69.6 81.3 79.2 96.3 269.9 29.3 NETtalk 14.3 <p> According to this view, ECOC is an interesting method for constructing a group of diverse yet accurate classifiers. Experimentally, the error-correcting output coding method has been shown to give excellent results with both decision tree and neural network learning algorithms <ref> (Dietterich & Bakiri, 1995) </ref>. We applied BCH code design methods (Bose & Ray-Chaudhuri, 1960; Hocquenghem, 1959; Lin & Costello, 1983) to design good error correcting codes of various lengths for both the separate and combined phoneme/stress configurations.
Reference: <author> Dietterich, T., Hild, H., & Bakiri, G. </author> <year> (1990). </year> <title> A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 24-31 Austin, TX. </address> <publisher> Morgan Kaufmann. Digital Equipment Corporation (1985). </publisher> <editor> Dectalk dtc01 owners manual, </editor> <title> third edition. </title> <type> Tech. rep. </type> <institution> EK-DTC01-OM-003, Digital Equipment Corporation, Maynard, </institution> <address> MA. </address>
Reference-contexts: The basic operation of ID3 is quite similar to the CART algorithm developed by (Breiman, Friedman, Olshen, & Stone, 1984) and to the tree-growing method developed by (Lucassen, 1983). In our implementation of ID3, we did not employ windowing, 2 early stopping, or any kind of pruning. Initial experiments <ref> (Dietterich, Hild, & Bakiri, 1990) </ref> showed that these stopping and pruning methods did not improve performance.
Reference: <author> Freund, Y., & Schapire, R. E. </author> <year> (1996). </year> <title> Experiments with a new boosting algorithm. </title> <editor> In Saitta, L. (Ed.), </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 148-156 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hocquenghem, A. </author> <year> (1959). </year> <title> Codes corecteurs d'erreurs. </title> <journal> Chiffres, </journal> <volume> 2, </volume> <pages> 147-156. </pages>
Reference: <author> Lin, S., & Costello, D. J. J. </author> <year> (1983). </year> <title> Error Control Coding: Fundamentals and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Lucassen, J. M. </author> <year> (1983). </year> <title> Discovering phonemic base forms automatically: An information theoretic approach. </title> <type> Tech. rep. </type> <institution> RC-9833, IBM T. J. Watson Research Center, Yorktown Heights, NY. </institution>
Reference-contexts: At this point, a leaf node is created and labeled with the class in question. The basic operation of ID3 is quite similar to the CART algorithm developed by (Breiman, Friedman, Olshen, & Stone, 1984) and to the tree-growing method developed by <ref> (Lucassen, 1983) </ref>. In our implementation of ID3, we did not employ windowing, 2 early stopping, or any kind of pruning. Initial experiments (Dietterich, Hild, & Bakiri, 1990) showed that these stopping and pruning methods did not improve performance. <p> The result of their investigation was summarized Lucassen as follows <ref> (Lucassen, 1983, p. 11) </ref>: The [system] operates on a word from left to right, predicting phones in left-to-right order. This decision was made after preliminary testing failed to indicate any advantage to either direction.
Reference: <author> Lucassen, J. M., & Mercer, R. L. </author> <year> (1984). </year> <title> An information theoretic approach to the automatic determination of phonemic base forms. </title> <booktitle> In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, ICASSP-84, </booktitle> <pages> pp. </pages> <month> 42.5.1-42.5.4. </month>
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess endgames. </title> <booktitle> In Machine learning: An artificial intelligence approach, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 463-482. </pages> <publisher> Tioga Press, </publisher> <address> Palo Alto, CA. </address>
Reference-contexts: While there exist automated methods for making these choices, they typically involve making multiple runs and employing cross-validation or holdout datasets. These increase the computational cost, often by orders of magnitude. To avoid these problems, we chose the ID3 decision tree algorithm <ref> (Quinlan, 1983, 1986) </ref> for all of our experiments. ID3 constructs a decision tree recursively, starting at the root.
Reference: <author> Quinlan, J. R. </author> <year> (1996). </year> <title> Bagging, boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 725-730 Cambridge, MA. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., McClelland, J. L., </editor> & <booktitle> the PDP research group. (Eds.), Parallel distributed processing: Explorations in the microstructure of cognition, Volume 1: Foundations. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: However, many of these combinations cannot occur in practice. Approximately 140 of these combinations are observed in the NETtalk dictionary. The learning algorithm employed by Sejnowski and Rosenberg was the backpropagation algorithm for training feed-forward neural networks <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>. To work well with backpropagation, it was necessary to convert each input and output into a collection of binary feature values. Specifically, each letter was converted to a string of 29 binary features.
Reference: <author> Sejnowski, T. J., & Rosenberg, C. R. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages> <note> 15 Wettschereck, </note> <author> D., & Dietterich, T. G. </author> <year> (1992). </year> <title> Improving the performance of radial basis function networks by learning center locations. </title> <editor> In Moody, J. E., Hanson, S. J., & Lippmann, R. P. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 4, </volume> <pages> pp. 1133-1140. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address> <month> 16 </month>
References-found: 16

