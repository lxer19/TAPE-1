URL: http://www.pmg.lcs.mit.edu/papers/osdi94-opplog.ps.gz
Refering-URL: http://www.pmg.lcs.mit.edu/Thor-papers.html
Root-URL: 
Email: fjames,liubag@lcs.mit.edu  
Title: Opportunistic Log: Efficient Installation Reads in a Reliable Storage Server  
Author: James O'Toole Liuba Shrira 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: In a distributed storage system, client caches managed on the basis of small granularity objects can provide better memory utilization then page-based caches. However, object servers, unlike page servers, must perform additional disk reads. These installation reads are required to install modified objects onto their corresponding disk pages. The opportunistic log is a new technique that significantly reduces the cost of installation reads. It defers the installation reads, removing them from the modification commit path, and manages a large pool of pending installation reads that can be scheduled efficiently. Using simulations, we show that the opportunistic log substantially enhances the I/O performance of reliable storage servers. An object server without the opportunistic log requires much better client caching to outperform a page server. With an opportunistic log, only a small client cache improvement suffices. Our results imply that efficient scheduling of installation reads can substantially improve the performance of large-scale storage systems and therefore introduces a new performance tradeoff between page-based and object-based architectures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Atul Adya. </author> <title> A Distributed Commit Protocol for Optimistic Concurrency Control. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: We assume the need to abort is detected at the server using a validation phase that does not require any disk accesses; see Adya <ref> [1] </ref> for the details. When the client sends a commit request, the server records a commit record in a stable log, as shown in Figure 1. The commit record contains the objects modified by the client. <p> The choice of concurrency control granularity, though very important in absolute terms, is orthogonal to the question of the relative I/O costs of the object and page server architectures. Using our validation techniques <ref> [1] </ref>, either page or object level concurrency control can be used with both object and page servers. 4.3 Server Designs We implemented four reliable server designs within the simulation model. The first section below describes the common features of the reliable server designs.
Reference: [2] <author> M. Carey, M. Franklin, and M. Zaharioudakis. </author> <title> Fine-Grained Sharing in a Page Server OODBMS. </title> <booktitle> In Proceedings of SIGMOD 1994, </booktitle> <year> 1994. </year>
Reference-contexts: The relative merits of this fundamental design choice are still the subject of debate <ref> [2, 3, 4, 7] </ref>. Client caches managed on the basis of small granularity objects can provide better memory utilization than page-based caches because pages are likely to include unneeded data that cannot be discarded from the cache [3, 4, 7, 14, 21]. <p> Conventional file systems do perform installation reads in these and other situations, but because they do not have a log they cannot defer the installation reads and schedule them efficiently. Previous studies <ref> [2, 7] </ref> compared the communication and concurrency control cost of page-based and object-based architectures. Our study is the first to identify the fundamental I/O performance tradeoff between the efficiency of object cache and the cost of installation reads. <p> After the commit is confirmed by the server, the client immediately starts its next transaction. The workload parameters are shown in Table 2. They are based loosely on similar parameters in other studies of object servers <ref> [2, 7] </ref>. We chose not to model the contents of the client cache because only the workload presented to the servers is relevant to our comparisons. Therefore we simply use the FetchRatio parameter to control the workload. <p> Day [4] studies whether to fetch data on a page or an object basis. Cheng and Hurson [3] demonstrated how object server architecture can enable more efficient client cache utilization. Carey et. al. <ref> [2] </ref> studied concurrency control issues in object, page and hybrid servers. None of these studies considered installation reads. However, in our recent work [21] we have investigated further the I/O performance tradeoffs related to the granularity of caching in large-scale object storage systems.
Reference: [3] <author> Jia-bing R. Cheng and A. R. Hurson. </author> <title> On the Performance Issues of Object-Based Buffering. </title> <booktitle> In Proceedings of the Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 30-37, </pages> <year> 1991. </year>
Reference-contexts: The relative merits of this fundamental design choice are still the subject of debate <ref> [2, 3, 4, 7] </ref>. Client caches managed on the basis of small granularity objects can provide better memory utilization than page-based caches because pages are likely to include unneeded data that cannot be discarded from the cache [3, 4, 7, 14, 21]. <p> Client caches managed on the basis of small granularity objects can provide better memory utilization than page-based caches because pages are likely to include unneeded data that cannot be discarded from the cache <ref> [3, 4, 7, 14, 21] </ref>. On the other hand, object servers may need to do extra I/O when a modified object needs to be updated on disk. If the containing page is not in the object server cache, this page has to be read back from the disk. <p> Since installation reads do not occur in page based servers, optimization of installation reads affects the performance tradeoffs between page-based and object-based systems. Several studies have indicated that object caching at the client can provide better utilization of client memory <ref> [3, 4, 14, 21] </ref> than page caching because the cache can hold more objects that are useful to the client. Improved client memory utilization translates into fewer fetch operations, and therefore reduced I/O load at the server. <p> Dewitt et. al. [7] focuses on the question of distributing the functionality of a persistent object system between the client and the server. Day [4] studies whether to fetch data on a page or an object basis. Cheng and Hurson <ref> [3] </ref> demonstrated how object server architecture can enable more efficient client cache utilization. Carey et. al. [2] studied concurrency control issues in object, page and hybrid servers. None of these studies considered installation reads.
Reference: [4] <author> M. Day. </author> <title> Managing a Cache of Swizzled Objects and Surrogates. </title> <type> PhD thesis, </type> <note> MIT-EECS, In preparation. </note>
Reference-contexts: The relative merits of this fundamental design choice are still the subject of debate <ref> [2, 3, 4, 7] </ref>. Client caches managed on the basis of small granularity objects can provide better memory utilization than page-based caches because pages are likely to include unneeded data that cannot be discarded from the cache [3, 4, 7, 14, 21]. <p> Client caches managed on the basis of small granularity objects can provide better memory utilization than page-based caches because pages are likely to include unneeded data that cannot be discarded from the cache <ref> [3, 4, 7, 14, 21] </ref>. On the other hand, object servers may need to do extra I/O when a modified object needs to be updated on disk. If the containing page is not in the object server cache, this page has to be read back from the disk. <p> Since installation reads do not occur in page based servers, optimization of installation reads affects the performance tradeoffs between page-based and object-based systems. Several studies have indicated that object caching at the client can provide better utilization of client memory <ref> [3, 4, 14, 21] </ref> than page caching because the cache can hold more objects that are useful to the client. Improved client memory utilization translates into fewer fetch operations, and therefore reduced I/O load at the server. <p> Several studies have investigated the design choices for persistent object system architecture. Dewitt et. al. [7] focuses on the question of distributing the functionality of a persistent object system between the client and the server. Day <ref> [4] </ref> studies whether to fetch data on a page or an object basis. Cheng and Hurson [3] demonstrated how object server architecture can enable more efficient client cache utilization. Carey et. al. [2] studied concurrency control issues in object, page and hybrid servers. None of these studies considered installation reads.
Reference: [5] <author> W. de Jonge, F. Kaashoek, and W. Hsieh. </author> <title> The Logical Disk: A New Approach to Improving File Systems. </title> <booktitle> In Proc. of the 14th Symposium on Operating Systems Principles, </booktitle> <address> Asheville, NC, </address> <month> December </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: There is substantial previous work on delayed processing of disk write operations [13, 25]. Some methods applied to delayed disk writes involve writing pages at new locations <ref> [5, 8, 22] </ref> and would not work with disk reads. However, standard disk scheduling methods based on head position apply equally well to read operations.
Reference: [6] <author> O. </author> <title> Deux et al. </title> <journal> The Story of O 2 . IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 91-108, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Many persistent object systems use the more traditional page based architecture [11, 19, 12]. Other systems <ref> [6, 15] </ref> use object server architectures but have not specifically addressed the problem of installation reads. There is an enormous literature on centralized and distributed databases (see Jim Gray's book [10] for an excellent survey). Most modern databases use a stable log to store modifications to insure durability of updates.
Reference: [7] <author> David J. DeWitt, Philippe Futtersack, David Maier, and Fer-nando Velez. </author> <title> A Study of Three Alternative Workstation-Server Architectures for Object Oriented Database Systems. </title> <booktitle> In Proceedings of the 16th Conference on Very Large Data Bases, </booktitle> <pages> pages 107-121, </pages> <address> Brisbane, Australia, </address> <year> 1990. </year>
Reference-contexts: The relative merits of this fundamental design choice are still the subject of debate <ref> [2, 3, 4, 7] </ref>. Client caches managed on the basis of small granularity objects can provide better memory utilization than page-based caches because pages are likely to include unneeded data that cannot be discarded from the cache [3, 4, 7, 14, 21]. <p> Client caches managed on the basis of small granularity objects can provide better memory utilization than page-based caches because pages are likely to include unneeded data that cannot be discarded from the cache <ref> [3, 4, 7, 14, 21] </ref>. On the other hand, object servers may need to do extra I/O when a modified object needs to be updated on disk. If the containing page is not in the object server cache, this page has to be read back from the disk. <p> Conventional file systems do perform installation reads in these and other situations, but because they do not have a log they cannot defer the installation reads and schedule them efficiently. Previous studies <ref> [2, 7] </ref> compared the communication and concurrency control cost of page-based and object-based architectures. Our study is the first to identify the fundamental I/O performance tradeoff between the efficiency of object cache and the cost of installation reads. <p> After the commit is confirmed by the server, the client immediately starts its next transaction. The workload parameters are shown in Table 2. They are based loosely on similar parameters in other studies of object servers <ref> [2, 7] </ref>. We chose not to model the contents of the client cache because only the workload presented to the servers is relevant to our comparisons. Therefore we simply use the FetchRatio parameter to control the workload. <p> Our techniques capitalize on this disk scheduling work. We are not aware of any other work besides ours that is directly concerned with efficient scheduling of deferred reads. Several studies have investigated the design choices for persistent object system architecture. Dewitt et. al. <ref> [7] </ref> focuses on the question of distributing the functionality of a persistent object system between the client and the server. Day [4] studies whether to fetch data on a page or an object basis. Cheng and Hurson [3] demonstrated how object server architecture can enable more efficient client cache utilization.
Reference: [8] <author> R. English and A. Stepanov. Loge: </author> <title> a Self-Organizing Disk Controller. </title> <booktitle> In Proceedings of Winter USENIX, </booktitle> <year> 1992. </year>
Reference-contexts: There is substantial previous work on delayed processing of disk write operations [13, 25]. Some methods applied to delayed disk writes involve writing pages at new locations <ref> [5, 8, 22] </ref> and would not work with disk reads. However, standard disk scheduling methods based on head position apply equally well to read operations.
Reference: [9] <author> S. Ghemawat. </author> <title> Disk Management for Object-Oriented Databases. </title> <type> PhD thesis, </type> <note> MIT-EECS, In preparation. </note>
Reference-contexts: The commit record contains the objects modified by the client. A client's transaction is considered committed once its commit entry is present in the log. 1 An alternative log-structured disk organization for objects would also be possible and is considered by Ghemawat <ref> [9] </ref>. Installing After a transaction commits, modifications from the log are applied to the appropriate pages; we call this update process installation. Note that an install may require a disk read if the page to be updated is not currently cached.
Reference: [10] <author> J. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: In this paper, we introduce a simple new approach for organizing I/O that substantially enhances the I/O performance of object servers. A standard technique to make updates atomic is to record them in a stable write-ahead log <ref> [10] </ref>. We keep this log of modifications in memory and defer installation reads from the commit path. This allows the accumulation of a large pool of pending installation reads that can be scheduled efficiently using well-known disk scheduling techniques [13, 25]. <p> The log entries would also be retained in volatile memory for installation processing. The installation processing would be somewhat complicated by the need to bound length of the log, as in a conventional databases with a write-ahead log <ref> [10] </ref>. In this case the opportunistic log might need to use an age-weighted scheduling algorithm. 4 Experimental Evaluation To evaluate our technique we studied the effect of the opportunistic log on the performance of a persistent object system. <p> Many persistent object systems use the more traditional page based architecture [11, 19, 12]. Other systems [6, 15] use object server architectures but have not specifically addressed the problem of installation reads. There is an enormous literature on centralized and distributed databases (see Jim Gray's book <ref> [10] </ref> for an excellent survey). Most modern databases use a stable log to store modifications to insure durability of updates. The log is not used as a source of updates to the database during normal operations, since database code modifies the pages directly in memory.
Reference: [11] <author> M. Hornick and S. Zdonik. </author> <title> A Shared, Segmented Memory System for an Object-Oriented Database, </title> <address> pages 273-285. </address> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Many persistent object systems use the more traditional page based architecture <ref> [11, 19, 12] </ref>. Other systems [6, 15] use object server architectures but have not specifically addressed the problem of installation reads. There is an enormous literature on centralized and distributed databases (see Jim Gray's book [10] for an excellent survey).
Reference: [12] <author> Object Design Inc. </author> <title> An Introduction to Object Store, Release 1.0. </title> <type> Burlington, </type> <institution> Massachusetts, </institution> <year> 1989. </year>
Reference-contexts: Many persistent object systems use the more traditional page based architecture <ref> [11, 19, 12] </ref>. Other systems [6, 15] use object server architectures but have not specifically addressed the problem of installation reads. There is an enormous literature on centralized and distributed databases (see Jim Gray's book [10] for an excellent survey).
Reference: [13] <author> David M. Jacobson and John Wilkes. </author> <title> Disk Scheduling Algorithms Based on Rotational Position. </title> <type> Technical Report HPL-CSP-91-7, </type> <institution> Hewlett-Packard Laboratories, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: We keep this log of modifications in memory and defer installation reads from the commit path. This allows the accumulation of a large pool of pending installation reads that can be scheduled efficiently using well-known disk scheduling techniques <ref> [13, 25] </ref>. How does one evaluate the impact of scheduled installation reads on the overall performance of an object storage system? One way is to study how it affects the performance tradeoffs between object and page-based architectures. <p> Without scheduled installation reads, break even requires a substantial improvement in the client cache performance. With our techniques only a small improvement suffices. Although there exists substantial work on optimizing deferred writes <ref> [13, 20, 25] </ref>, and read-ahead is used to optimize sequential reads, we are the first (to our knowledge) to take advantage of the opportunity to optimize the installation reads that are important in object servers. <p> By delaying the installation and performing installation reads opportunistically on the basis of disk head position, we can expect to dramatically reduce the expected cost of an installation read. There is substantial previous work on delayed processing of disk write operations <ref> [13, 25] </ref>. Some methods applied to delayed disk writes involve writing pages at new locations [5, 8, 22] and would not work with disk reads. However, standard disk scheduling methods based on head position apply equally well to read operations. <p> Much earlier effort has been invested in optimizing disk scheduling and so widening the performance gap between unoptimized and optimized disk access. Read-ahead is widely used to optimize sequential reads [16]. Recent studies by Seltzer, Chen and Ousterhout [25] and by Jacobson and Wilkes <ref> [13] </ref> aggressively take advantage of large memories to efficiently schedule deferred writes. Our techniques capitalize on this disk scheduling work. We are not aware of any other work besides ours that is directly concerned with efficient scheduling of deferred reads.
Reference: [14] <author> Alfans Kemper and Donal Kossman. </author> <title> Dual-Buffering Strategies in Object Bases. </title> <booktitle> In Proceedings of the 20th Conference on Very-Large Databases, </booktitle> <address> Santiago, Chile, </address> <year> 1994. </year>
Reference-contexts: Client caches managed on the basis of small granularity objects can provide better memory utilization than page-based caches because pages are likely to include unneeded data that cannot be discarded from the cache <ref> [3, 4, 7, 14, 21] </ref>. On the other hand, object servers may need to do extra I/O when a modified object needs to be updated on disk. If the containing page is not in the object server cache, this page has to be read back from the disk. <p> Since installation reads do not occur in page based servers, optimization of installation reads affects the performance tradeoffs between page-based and object-based systems. Several studies have indicated that object caching at the client can provide better utilization of client memory <ref> [3, 4, 14, 21] </ref> than page caching because the cache can hold more objects that are useful to the client. Improved client memory utilization translates into fewer fetch operations, and therefore reduced I/O load at the server.
Reference: [15] <author> W. Kim et al. </author> <title> Architecture of the ORION Next-Generation Database System. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 109-124, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Many persistent object systems use the more traditional page based architecture [11, 19, 12]. Other systems <ref> [6, 15] </ref> use object server architectures but have not specifically addressed the problem of installation reads. There is an enormous literature on centralized and distributed databases (see Jim Gray's book [10] for an excellent survey). Most modern databases use a stable log to store modifications to insure durability of updates.
Reference: [16] <author> Samuel J. Leffler, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3bsd UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Much earlier effort has been invested in optimizing disk scheduling and so widening the performance gap between unoptimized and optimized disk access. Read-ahead is widely used to optimize sequential reads <ref> [16] </ref>. Recent studies by Seltzer, Chen and Ousterhout [25] and by Jacobson and Wilkes [13] aggressively take advantage of large memories to efficiently schedule deferred writes. Our techniques capitalize on this disk scheduling work.
Reference: [17] <author> B. Liskov, M. Day, and L. Shrira. </author> <title> Distributed Object Management in Thor. </title> <editor> In M. Tamer Ozsu, Umesh Dayal, and Patrick Valduriez, editors, </editor> <booktitle> Distributed Object Management. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: Pages are the unit of disk transfer and caching at the server. We assume that objects on disk are updated in place. 1 For concreteness in this presentation, we assume optimistic concurrency control and in-memory commit. These features of our object server architecture are derived from the Thor <ref> [17] </ref> persistent object system. Nevertheless, we believe that the new technique we are proposing is independent of these two assumptions.
Reference: [18] <author> B. Liskov, S. Ghemawat, R. Gruber, P. Johnson, L. Shrira, and M. Williams. </author> <title> Replication in the Harp File System. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <year> 1991. </year>
Reference-contexts: An opportunistic server implements the design presented in 3.2 and therefore benefits from both absorption and disk scheduling. A basic server uses the log to defer and schedule disk writes but does not defer or schedule installation reads. This configuration represents an object server with an in-memory log <ref> [18] </ref>. An absorbing server defers installations but does not use intelligent disk scheduling for installation reads. This configuration benefits from absorption only. Section 5.1 presents and analyses these results. <p> However, portability concerns, the assumption that the persistent data fits in primary memory, and the fact that clients fetch data from one disk copy while stable updates are propagated to another disk copy of the database, make RVM performance considerations very different from our work. Harp <ref> [18] </ref> is a replicated NFS server that uses a replicated in-memory write-ahead log to store durable modifications to Unix files. The log defers and propagates the modifications efficiently.
Reference: [19] <author> D. Maier and J. Stein. </author> <title> Development and Implementation of an Object-Oriented DBMS. </title> <editor> In B. Shriver and P. Wegner, editors, </editor> <booktitle> Research Directions in Object-Oriented Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Many persistent object systems use the more traditional page based architecture <ref> [11, 19, 12] </ref>. Other systems [6, 15] use object server architectures but have not specifically addressed the problem of installation reads. There is an enormous literature on centralized and distributed databases (see Jim Gray's book [10] for an excellent survey).
Reference: [20] <author> Jeffrey C. Mogul. </author> <title> A Better Update Policy. </title> <booktitle> In USENIX Summer Conference, </booktitle> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: Without scheduled installation reads, break even requires a substantial improvement in the client cache performance. With our techniques only a small improvement suffices. Although there exists substantial work on optimizing deferred writes <ref> [13, 20, 25] </ref>, and read-ahead is used to optimize sequential reads, we are the first (to our knowledge) to take advantage of the opportunity to optimize the installation reads that are important in object servers.
Reference: [21] <author> James O'Toole and Liuba Shrira. </author> <title> Hybrid Caching for Large-Scale Object Systems. </title> <booktitle> In Proceedings of the 6th Workshop on Persistent Object Systems, </booktitle> <address> Tarascon, France, </address> <month> September </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: Client caches managed on the basis of small granularity objects can provide better memory utilization than page-based caches because pages are likely to include unneeded data that cannot be discarded from the cache <ref> [3, 4, 7, 14, 21] </ref>. On the other hand, object servers may need to do extra I/O when a modified object needs to be updated on disk. If the containing page is not in the object server cache, this page has to be read back from the disk. <p> Since installation reads do not occur in page based servers, optimization of installation reads affects the performance tradeoffs between page-based and object-based systems. Several studies have indicated that object caching at the client can provide better utilization of client memory <ref> [3, 4, 14, 21] </ref> than page caching because the cache can hold more objects that are useful to the client. Improved client memory utilization translates into fewer fetch operations, and therefore reduced I/O load at the server. <p> Cheng and Hurson [3] demonstrated how object server architecture can enable more efficient client cache utilization. Carey et. al. [2] studied concurrency control issues in object, page and hybrid servers. None of these studies considered installation reads. However, in our recent work <ref> [21] </ref> we have investigated further the I/O performance tradeoffs related to the granularity of caching in large-scale object storage systems. We explored a hybrid design that selectively caches at the client either objects or pages, trading fetch reads for (optimized) installation reads.
Reference: [22] <author> M. Rosenblum and J.K. Ousterhout. </author> <title> The Design and Implementation of a Log Structured File System. </title> <booktitle> In Proc. of the 13th Symposium on Operating Systems Principles, </booktitle> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year> <note> ACM. </note>
Reference-contexts: There is substantial previous work on delayed processing of disk write operations [13, 25]. Some methods applied to delayed disk writes involve writing pages at new locations <ref> [5, 8, 22] </ref> and would not work with disk reads. However, standard disk scheduling methods based on head position apply equally well to read operations.
Reference: [23] <author> Chris Ruemmler and John Wilkes. </author> <title> Modelling Disks. </title> <type> Technical Report HPL-93-68rev1, </type> <institution> Hewlett-Packard Laboratories, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Disk The disk modeled by the simulator provides FIFO servicing of requests issued by the server. The disk geometry and other performance characteristics are taken from the HP97560 drive, as described by Wilkes <ref> [23] </ref>. We chose the HP97560 model because it is simple, accurate, and available. We believe that newer and faster drives will make opportunistic I/O even more important because transfer time is decreasing much faster than seek time [23]. <p> other performance characteristics are taken from the HP97560 drive, as described by Wilkes <ref> [23] </ref>. We chose the HP97560 model because it is simple, accurate, and available. We believe that newer and faster drives will make opportunistic I/O even more important because transfer time is decreasing much faster than seek time [23]. That trend should increase the relative value of locality-based optimizations. 5 Simulation Results In the sections that follow we first examine the relative performance of the object server designs. Then we explore the tradeoffs involved in choosing the log size.
Reference: [24] <author> M. Satyanarayanan, H. Mashburn, P. Kumar, D. Steere, and J. Kistler. </author> <title> Lightweight Recoverable Virtual Memory. </title> <booktitle> In Proc. of the 14th Symposium on Operating Systems Principles, </booktitle> <address> Asheville, NC, </address> <month> December </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: Most modern databases use a stable log to store modifications to insure durability of updates. The log is not used as a source of updates to the database during normal operations, since database code modifies the pages directly in memory. Therefore there are no installation reads. RVM <ref> [24] </ref> is a portable package that provides durable updates to virtual memory regions.
Reference: [25] <author> M. Seltzer, P. Chen., and J. Ousterhout. </author> <title> Disk Scheduling Revisited. </title> <booktitle> In Proceedings of Winter USENIX, </booktitle> <year> 1990. </year>
Reference-contexts: We keep this log of modifications in memory and defer installation reads from the commit path. This allows the accumulation of a large pool of pending installation reads that can be scheduled efficiently using well-known disk scheduling techniques <ref> [13, 25] </ref>. How does one evaluate the impact of scheduled installation reads on the overall performance of an object storage system? One way is to study how it affects the performance tradeoffs between object and page-based architectures. <p> Without scheduled installation reads, break even requires a substantial improvement in the client cache performance. With our techniques only a small improvement suffices. Although there exists substantial work on optimizing deferred writes <ref> [13, 20, 25] </ref>, and read-ahead is used to optimize sequential reads, we are the first (to our knowledge) to take advantage of the opportunity to optimize the installation reads that are important in object servers. <p> By delaying the installation and performing installation reads opportunistically on the basis of disk head position, we can expect to dramatically reduce the expected cost of an installation read. There is substantial previous work on delayed processing of disk write operations <ref> [13, 25] </ref>. Some methods applied to delayed disk writes involve writing pages at new locations [5, 8, 22] and would not work with disk reads. However, standard disk scheduling methods based on head position apply equally well to read operations. <p> Some methods applied to delayed disk writes involve writing pages at new locations [5, 8, 22] and would not work with disk reads. However, standard disk scheduling methods based on head position apply equally well to read operations. In particular, Seltzer et. al. <ref> [25] </ref> have shown that when a pool of 1000 operations is available, greedy algorithms can reduce the cost of individual operations to a fraction of the normal random-access cost. 3.1 Basic Tradeoffs In addition to reducing the cost of installation reads, we expect to reduce the number of installation reads. <p> The prototype design will use a greedy scheduler that always selects the delayed installation read that has the shortest positioning time, based on the most recent disk head position. The scheduler uses a branch-and-bound implementation of the shortest positioning time algorithm <ref> [25] </ref>. We estimate that a good branch-and-bound implementation would make approximate shortest positioning time calculations over thousands of disk operations feasible in current disk controllers. Without non-volatile memory, the server would need to write the log to a stable disk to commit transactions. <p> When the number of clean pages in the cache drops below the WriteTrigger threshold, the server writes one dirty page to the disk. The page is selected using the shortest positioning time algorithm <ref> [25] </ref>. In the object-based server, the simulator models the transaction log as a collection of modified objects. A portion of the non-volatile primary memory is statically allocated to hold log entries. The Objects-per-page parameter defines the number of log entries that can be stored per page of memory allocated. <p> These measurements show that increasing the log is initially very valuable as the installation read cost drops rapidly. However, later improvements are smaller and require much greater increases in log size. Of course, this is the effect we expected, as described by Seltzer <ref> [25] </ref>. Metrics for 50 clients, fetch ratio 20% Log tx/s I-Abs. <p> Much earlier effort has been invested in optimizing disk scheduling and so widening the performance gap between unoptimized and optimized disk access. Read-ahead is widely used to optimize sequential reads [16]. Recent studies by Seltzer, Chen and Ousterhout <ref> [25] </ref> and by Jacobson and Wilkes [13] aggressively take advantage of large memories to efficiently schedule deferred writes. Our techniques capitalize on this disk scheduling work. We are not aware of any other work besides ours that is directly concerned with efficient scheduling of deferred reads.
References-found: 25

