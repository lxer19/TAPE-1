URL: http://www.cs.duke.edu/~jsv/Papers/MVW98.wavelet_histogram.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node57.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: matias@math.tau.ac.il  jsv@cs.duke.edu  minw@cs.duke.edu  
Title: Wavelet-Based Histograms for Selectivity Estimation  
Author: Yossi Matias Jeffrey Scott Vitter Min Wang 
Note: Part of this work was done while the author was visiting Bell Laboratories in  Supported in part by Army Research Office MURI grant DAAH04-96-1-0013 and by National Science Foundation research grant CCR-9522047. Supported in part by an IBM Graduate Fellowship and by Army Research Office MURI grant DAAH04-96-1-0013.  
Address: Israel  Murray Hill, NJ.  
Affiliation: Department of Computer Science Tel Aviv University,  Department of Computer Science Duke University  Department of Computer Science Duke University  
Abstract: Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P , we need to estimate the fraction of records in the database that satisfy P . Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations. In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data distributions give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using them in an on-line fashion for selectivity estimation. Our histograms also provide quick approximate answers to OLAP queries when the exact answers are not required. Our method captures the joint distribution of multiple attributes effectively, even when the attributes are correlated. Experiments confirm that our histograms offer substantial improvements in accuracy over random sampling and other previous approaches. fl Also affiliated with Bell Laboratories, Murray Hill, NJ.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Census Bureau Databases. </institution> <note> http://www.census.gov/. </note>
Reference-contexts: In our experiments we use the synthetic data described in [15], which is indicative of various real-life 9 (a) Linear Wavelets (b) Haar Wavelets (c) MaxDiff (V, A) (d) Random Sampling data <ref> [1] </ref>, and the TPC-D benchmark data [22]. Our query sets are obtained by extending the query sets AH defined in Section 6.1 to the multidimensional cases. The main concern of the multidimensional methods is the effectiveness of the histograms in capturing data dependencies.
Reference: [2] <author> D. L. Donoho. </author> <title> Unconditional bases are optimal bases for data compression and statistical estimation. </title> <type> Technical report, </type> <institution> Department of Statistics, Stanford University, </institution> <year> 1992. </year>
Reference-contexts: We can easily derive the cumulated values T C + =f (0; 2); (1; 2); (2; 7); (3; 9)g. We perform a wavelet transform on the one-dimensional "signal" of the extended cumulative frequencies: S = <ref> [2, 2, 7, 9] </ref>: We first average the cumulative frequencies, pairwise, to get the new lower resolution signal with values [2, 8]: That is, the first two values in the original signal (2 and 2) average to 2, and the second two values 7 and 9 average to 8. <p> We perform a wavelet transform on the one-dimensional "signal" of the extended cumulative frequencies: S = [2, 2, 7, 9]: We first average the cumulative frequencies, pairwise, to get the new lower resolution signal with values <ref> [2, 8] </ref>: That is, the first two values in the original signal (2 and 2) average to 2, and the second two values 7 and 9 average to 8. Clearly, some information is lost in this averaging process. <p> We have succeeded in decomposing the original signal into a lower resolution (two-value) version and a pair of detail coefficients. By repeating this process recursively on the averages, we get the full decomposition: Resolution Averages Detail Coefficients 4 <ref> [2, 2, 7, 9] </ref> 1 [5] [6] We define the wavelet transform (also called wavelet decomposition) of the original four-value signal to be the single coefficient representing the overall average of the original signal, followed by the detail coefficients in the order of increasing resolution. <p> Thus, for the one dimensional Haar basis, the wavelet transform of our original cumulative frequencies is given by b S = <ref> [5, 6, 0, 2] </ref>: The individual entries are called the wavelet coefficients. The wavelet decomposition is very efficient com-putationally, requiring only O (N ) time to compute for a signal of N frequencies. No information has been gained or lost by this pro cess. <p> However, for non-orthogonal wavelets like our linear wavelets and for norms other than the 2-norm, no efficient technique is known for how to choose the m best wavelet coefficients, and various approximations have been studied <ref> [2] </ref>. Our experiments show that Method 2 does best overall in terms of accuracy for wavelet-based histograms. Method 1 is easier to compute but does not perform quite as well. 4 On-Line Reconstruction In the query phase, a range query a X b is presented.
Reference: [3] <author> P. B. Gibbons and Y. Matias. </author> <title> New sampling-based summary statistics for improving approximate query answers. </title> <booktitle> In Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data, </booktitle> <address> Seattle, Washington, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: Wavelet-based histograms should serve as an effective synopsis data structure for selectivity estimation in the context of the on-line summary mechanism of <ref> [3] </ref>. We are developing efficient algorithms for maintaining the wavelet-based histograms given insertions and deletions in the underlying relation. Acknowledgments. We gratefully acknowledge earlier discussions with Christos Faloutsos, Suleyman Cenk Sahinalp, Wim Sweldens, and Brani Vidacovic. We especially thank Vishy Poosala for much useful background and information on histogram techniques.
Reference: [4] <author> P. B. Gibbons, Y. Matias, and V. Poosala. </author> <title> Fast incremental maintenance of approximate histograms. </title> <booktitle> In Proceedings of the 23rd VLDB Conference, </booktitle> <address> Athens, Greece, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: A running up-to-date sample can be kept using a backing sample approach <ref> [4] </ref>. We do not consider in this paper the issues dealing with sample size and the er 2 rors caused by sampling.
Reference: [5] <author> P. Haas and A. Swami. </author> <title> Sequential sampling procedures for query size estimation. </title> <booktitle> In Proceedings of the 1992 ACM SIGMOD Conference, </booktitle> <year> 1992. </year>
Reference-contexts: We have succeeded in decomposing the original signal into a lower resolution (two-value) version and a pair of detail coefficients. By repeating this process recursively on the averages, we get the full decomposition: Resolution Averages Detail Coefficients 4 [2, 2, 7, 9] 1 <ref> [5] </ref> [6] We define the wavelet transform (also called wavelet decomposition) of the original four-value signal to be the single coefficient representing the overall average of the original signal, followed by the detail coefficients in the order of increasing resolution. <p> Thus, for the one dimensional Haar basis, the wavelet transform of our original cumulative frequencies is given by b S = <ref> [5, 6, 0, 2] </ref>: The individual entries are called the wavelet coefficients. The wavelet decomposition is very efficient com-putationally, requiring only O (N ) time to compute for a signal of N frequencies. No information has been gained or lost by this pro cess. <p> Poosala et al [16] characterized the types of histograms in previous studies and proposed new types of histograms. They concluded in their experiments that the MaxDiff (V,A) histograms perform best overall. Random sampling can be used for selectivity estimation <ref> [5, 6, 10, 9] </ref>. The simplest way of using random sampling to estimate selectivity is, during the off-line phase, to take a random sample of a certain size (depending on the catalog size limitation) from the relation.
Reference: [6] <author> P. Haas and A. Swami. </author> <title> Sampling-based selectivity for joins using augmented frequent value statistics. </title> <booktitle> In Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data, </booktitle> <month> March </month> <year> 1995. </year>
Reference-contexts: We have succeeded in decomposing the original signal into a lower resolution (two-value) version and a pair of detail coefficients. By repeating this process recursively on the averages, we get the full decomposition: Resolution Averages Detail Coefficients 4 [2, 2, 7, 9] 1 [5] <ref> [6] </ref> We define the wavelet transform (also called wavelet decomposition) of the original four-value signal to be the single coefficient representing the overall average of the original signal, followed by the detail coefficients in the order of increasing resolution. <p> Thus, for the one dimensional Haar basis, the wavelet transform of our original cumulative frequencies is given by b S = <ref> [5, 6, 0, 2] </ref>: The individual entries are called the wavelet coefficients. The wavelet decomposition is very efficient com-putationally, requiring only O (N ) time to compute for a signal of N frequencies. No information has been gained or lost by this pro cess. <p> Poosala et al [16] characterized the types of histograms in previous studies and proposed new types of histograms. They concluded in their experiments that the MaxDiff (V,A) histograms perform best overall. Random sampling can be used for selectivity estimation <ref> [5, 6, 10, 9] </ref>. The simplest way of using random sampling to estimate selectivity is, during the off-line phase, to take a random sample of a certain size (depending on the catalog size limitation) from the relation.
Reference: [7] <author> C.-T. Ho, R. Agrawal, N. Megiddo, and R. Srikant. </author> <title> Range queries in OLAP data cubes. </title> <booktitle> In Proceedings of the 1997 ACM SIGMOD International Conference on Management of Data, </booktitle> <address> Tucson, Arizona, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: We can easily derive the cumulated values T C + =f (0; 2); (1; 2); (2; 7); (3; 9)g. We perform a wavelet transform on the one-dimensional "signal" of the extended cumulative frequencies: S = <ref> [2, 2, 7, 9] </ref>: We first average the cumulative frequencies, pairwise, to get the new lower resolution signal with values [2, 8]: That is, the first two values in the original signal (2 and 2) average to 2, and the second two values 7 and 9 average to 8. <p> We have succeeded in decomposing the original signal into a lower resolution (two-value) version and a pair of detail coefficients. By repeating this process recursively on the averages, we get the full decomposition: Resolution Averages Detail Coefficients 4 <ref> [2, 2, 7, 9] </ref> 1 [5] [6] We define the wavelet transform (also called wavelet decomposition) of the original four-value signal to be the single coefficient representing the overall average of the original signal, followed by the detail coefficients in the order of increasing resolution. <p> The following theorem adopted from <ref> [7] </ref> can be used to compute an estimate S 0 for the result size of the range query: Theorem 5 ([7]) For each 1 j d, let s (j) = 1 if x j = b j ; Then the approximate selectivity for the d-dimensional range query specified above is S
Reference: [8] <author> B. Jawerth and W. Sweldens. </author> <title> An overview of wavelet based multiresolution analyses. </title> <journal> SIAM Rev., </journal> <volume> 36(3) </volume> <pages> 377-412, </pages> <year> 1994. </year>
Reference-contexts: We also implement a decomposition based upon linear wavelets that gives better estimation. To illustrate how Haar wavelets work, we start with a simple example. A detailed treatment of wavelets can be found in any standard reference on the subject (e.g., <ref> [8, 20] </ref>). Suppose that the data distribution T of attribute X is f (0; 2); (2; 5); (3; 2)g. We can easily derive the cumulated values T C + =f (0; 2); (1; 2); (2; 7); (3; 9)g. <p> We perform a wavelet transform on the one-dimensional "signal" of the extended cumulative frequencies: S = [2, 2, 7, 9]: We first average the cumulative frequencies, pairwise, to get the new lower resolution signal with values <ref> [2, 8] </ref>: That is, the first two values in the original signal (2 and 2) average to 2, and the second two values 7 and 9 average to 8. Clearly, some information is lost in this averaging process.
Reference: [9] <author> R. Lipton and J. Naughton. </author> <title> Query size estimation by adaptive sampling. </title> <journal> J. of Comput. Sys. Sci., </journal> <volume> 51 </volume> <pages> 18-25, </pages> <year> 1985. </year>
Reference-contexts: We can easily derive the cumulated values T C + =f (0; 2); (1; 2); (2; 7); (3; 9)g. We perform a wavelet transform on the one-dimensional "signal" of the extended cumulative frequencies: S = <ref> [2, 2, 7, 9] </ref>: We first average the cumulative frequencies, pairwise, to get the new lower resolution signal with values [2, 8]: That is, the first two values in the original signal (2 and 2) average to 2, and the second two values 7 and 9 average to 8. <p> We have succeeded in decomposing the original signal into a lower resolution (two-value) version and a pair of detail coefficients. By repeating this process recursively on the averages, we get the full decomposition: Resolution Averages Detail Coefficients 4 <ref> [2, 2, 7, 9] </ref> 1 [5] [6] We define the wavelet transform (also called wavelet decomposition) of the original four-value signal to be the single coefficient representing the overall average of the original signal, followed by the detail coefficients in the order of increasing resolution. <p> Poosala et al [16] characterized the types of histograms in previous studies and proposed new types of histograms. They concluded in their experiments that the MaxDiff (V,A) histograms perform best overall. Random sampling can be used for selectivity estimation <ref> [5, 6, 10, 9] </ref>. The simplest way of using random sampling to estimate selectivity is, during the off-line phase, to take a random sample of a certain size (depending on the catalog size limitation) from the relation.
Reference: [10] <author> R. Lipton, J. Naughton, and D. Schneider. </author> <title> Practical selectivity estimation through adaptive sampling. </title> <booktitle> In Proceeding of the 1990 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 1-11, </pages> <year> 1990. </year>
Reference-contexts: Poosala et al [16] characterized the types of histograms in previous studies and proposed new types of histograms. They concluded in their experiments that the MaxDiff (V,A) histograms perform best overall. Random sampling can be used for selectivity estimation <ref> [5, 6, 10, 9] </ref>. The simplest way of using random sampling to estimate selectivity is, during the off-line phase, to take a random sample of a certain size (depending on the catalog size limitation) from the relation.
Reference: [11] <author> M. Muralikrishna and D. J. DeWitt. </author> <title> Equi-depth histograms for estimating selectivity factors for multidimensional queries. </title> <booktitle> In Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 28-36, </pages> <year> 1988. </year>
Reference-contexts: If for any reason it's necessary to further reduce the I/O and CPU costs of the precomputation of T , a well-known approach is to use random sampling <ref> [12, 11] </ref>. The idea is to sample s tuples from the relation randomly and compute T for the sample. The sample data distribution is then used as an estimate of the real data distribution. <p> Functional dependencies and various types of correlations among attributes are very common. Making the attribute value independent assumption in these cases results in very inaccurate estimation of the joint data distribution and poor selectivity estimation. 5.1 Previous Approaches Muralikrishna and DeWitt <ref> [11] </ref> use an interesting spatial index partitioning technique for constructing equidepth histograms for multidimensional data. One drawback with this approach is that it considers each dimension only once during the partition. Poosala and Ioanni-dis [15] propose two effective alternatives.
Reference: [12] <author> G. Piatetsky-Shapiro and C. Connell. </author> <title> Accurate estimation of the number of tuples satisfying a condition. </title> <booktitle> In Proceedings of the 1984 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 256-276, </pages> <year> 1984. </year>
Reference-contexts: If for any reason it's necessary to further reduce the I/O and CPU costs of the precomputation of T , a well-known approach is to use random sampling <ref> [12, 11] </ref>. The idea is to sample s tuples from the relation randomly and compute T for the sample. The sample data distribution is then used as an estimate of the real data distribution.
Reference: [13] <author> V. Poosala. </author> <title> Histogram-Based Estimation Techniques in Database Systems. </title> <publisher> Ph. </publisher> <address> D. </address> <institution> dissertation, University of Wisconsin-Madison, </institution> <year> 1997. </year>
Reference-contexts: Several different histograms have been proposed in the literature. Poosala et al <ref> [16, 13] </ref> propose a taxonomy to capture all previously proposed histograms; new histograms types can be derived by combining effective aspects of different 1 histogram methods. Among the histograms discussed in [16, 13], the MaxDiff (V,A) histogram gives best overall performance. <p> Several different histograms have been proposed in the literature. Poosala et al <ref> [16, 13] </ref> propose a taxonomy to capture all previously proposed histograms; new histograms types can be derived by combining effective aspects of different 1 histogram methods. Among the histograms discussed in [16, 13], the MaxDiff (V,A) histogram gives best overall performance. MaxDiff (V,A) uses area as a "source parameter" in order to choose the bucket boundaries; area is defined for each value in the value set V as the product of the value's frequency and its spread. <p> [x 1 ; x 2 ; : : : ; x d ] = 0 if x j = 1 for any 1 j d. 6 Empirical Results In this section we report on some experiments that compare the performance of our wavelet-based technique with those of Poosala et al <ref> [16, 13, 15] </ref> and random sampling. Our synthetic data sets are those from previous studies on histogram formation and from the TPC-D benchmark [22]. They correspond to studies on typical data found on the web. <p> Zipf distributions are described in more detail in <ref> [13] </ref>. Zipf parameter z = 0 corresponds to a perfectly uniform distribution, and as z increases, the distribution becomes exponentially skewed, with a very large number of small values and a very small number of large values.
Reference: [14] <author> V. </author> <title> Poosala. </title> <type> Personal communication, </type> <year> 1997. </year>
Reference-contexts: In [15], larger z values are considered; most of the Zipf frequencies are actually very close to 0, so they are instead boosted up to 1, with the large frequencies correspondingly lowered, thus yielding semi-Zipf distributed frequency sets <ref> [14] </ref>. The relative effectiveness of different histograms is fairly constant over a wide variety of data distributions and query sets that we studied. on the accuracy of different types of histograms for one typical set of experiments.
Reference: [15] <author> V. Poosala and Y. E. Ioannidis. </author> <title> Selectivity estimation without the attribute value independence assumption. </title> <booktitle> In Proceedings of the 23rd VLDB Conference, </booktitle> <address> Athens, Greece, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: One drawback with this approach is that it considers each dimension only once during the partition. Poosala and Ioanni-dis <ref> [15] </ref> propose two effective alternatives. The first approach partitions the joint data distribution into mutually disjointed buckets and approximates the frequency and the value sets in each bucket in a uniform manner. <p> Among this new class of histograms, the multidimensional MaxDiff (V,A) histograms computed using the MHIST-2 algorithm are most accurate and perform better in practice than previous methods <ref> [15] </ref>. The second approach uses the powerful singular value decomposition (SVD) technique from linear algebra, which is limited to handling two dimensions. <p> [x 1 ; x 2 ; : : : ; x d ] = 0 if x j = 1 for any 1 j d. 6 Empirical Results In this section we report on some experiments that compare the performance of our wavelet-based technique with those of Poosala et al <ref> [16, 13, 15] </ref> and random sampling. Our synthetic data sets are those from previous studies on histogram formation and from the TPC-D benchmark [22]. They correspond to studies on typical data found on the web. <p> Data Range Linear Wavelets Haar Wavelets MHIST-2 255 0.3% 1.5% 7% 1023 0.3% 1.6% 6% Table 6: ke abs k 1 =T errors of various two-dimensional histograms for TPC-D data. iff (V, A) histograms computed using the MHIST-2 algorithm <ref> [15] </ref> (which we refer to as MHIST-2 histograms). In our experiments we use the synthetic data described in [15], which is indicative of various real-life 9 (a) Linear Wavelets (b) Haar Wavelets (c) MaxDiff (V, A) (d) Random Sampling data [1], and the TPC-D benchmark data [22]. <p> 255 0.3% 1.5% 7% 1023 0.3% 1.6% 6% Table 6: ke abs k 1 =T errors of various two-dimensional histograms for TPC-D data. iff (V, A) histograms computed using the MHIST-2 algorithm <ref> [15] </ref> (which we refer to as MHIST-2 histograms). In our experiments we use the synthetic data described in [15], which is indicative of various real-life 9 (a) Linear Wavelets (b) Haar Wavelets (c) MaxDiff (V, A) (d) Random Sampling data [1], and the TPC-D benchmark data [22]. Our query sets are obtained by extending the query sets AH defined in Section 6.1 to the multidimensional cases. <p> A higher z value corresponds to fewer very high frequencies, implying stronger dependencies between the attributes. One question raised here is what is the reasonable range for that z value. As in <ref> [15] </ref>, we fix the relation size T to be 10 6 in our experiments. If we assume our joint value set size is n 1 fi n 2 , then in order to get frequencies that are at least 1, the z value cannot be greater than a certain value. <p> In our experiments, we choose various z in the range 0 z 1:5. The value z = 1:5 already corresponds to a highly skewed frequency set; its top three frequencies are 388747, 137443, and 74814, and the 2500th frequency is 3. In <ref> [15] </ref>, larger z values are considered; most of the Zipf frequencies are actually very close to 0, so they are instead boosted up to 1, with the large frequencies correspondingly lowered, thus yielding semi-Zipf distributed frequency sets [14]. <p> The storage space is 210 four-bytes numbers (again, to be in line with the default storage space 10 Zipf z parameter for the frequency set distribution. (a) Zipf parameter z=0.5 (b) Zipf parameter z=1.0 histograms. in <ref> [15] </ref>). It corresponds to using 30 buckets for MHIST-2 histogram (seven numbers per bucket) and keeping 70 wavelet coefficients for wavelet-based based histogram (three numbers per coefficient). The queries used are those from query set A.
Reference: [16] <author> V. Poosala, Y. E. Ioannidis, P. J. Haas, and E. Shekita. </author> <title> Improved histograms for selectivity estimation of range predicates. </title> <booktitle> In Proceedings of the 1996 ACM SIG-MOD International Conference on Management of Data, </booktitle> <address> Montreal, Canada, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The set of equal predicates is the subset of the range predicates that have a = b. The set of one-side range predicates is the special case of range predicates in which a = 1 or b = 1. We adopt the notations in <ref> [16] </ref> to describe the data distributions and various histograms. The domain D = f0, 1, 2, . . . , N 1g of an attribute X is the set of all possible values of X. <p> Several different histograms have been proposed in the literature. Poosala et al <ref> [16, 13] </ref> propose a taxonomy to capture all previously proposed histograms; new histograms types can be derived by combining effective aspects of different 1 histogram methods. Among the histograms discussed in [16, 13], the MaxDiff (V,A) histogram gives best overall performance. <p> Several different histograms have been proposed in the literature. Poosala et al <ref> [16, 13] </ref> propose a taxonomy to capture all previously proposed histograms; new histograms types can be derived by combining effective aspects of different 1 histogram methods. Among the histograms discussed in [16, 13], the MaxDiff (V,A) histogram gives best overall performance. MaxDiff (V,A) uses area as a "source parameter" in order to choose the bucket boundaries; area is defined for each value in the value set V as the product of the value's frequency and its spread. <p> By using area as a source parameter, MaxDiff (V,A) histogram achieves better approximation than previous well-known methods like equidepth histograms. Poosala et al <ref> [16] </ref> also mention other histogram techniques. For example, histograms based on minimizing the variance of the source parameter such as area have a performance similar to that of MaxDiff (V,A), but are computationally more expensive to construct. <p> [x 1 ; x 2 ; : : : ; x d ] = 0 if x j = 1 for any 1 j d. 6 Empirical Results In this section we report on some experiments that compare the performance of our wavelet-based technique with those of Poosala et al <ref> [16, 13, 15] </ref> and random sampling. Our synthetic data sets are those from previous studies on histogram formation and from the TPC-D benchmark [22]. They correspond to studies on typical data found on the web. <p> For simplicity and ease of replication, we use method 1 for thresholding in all our wavelet experiments. 6.1 Experimental Comparison of One Dimensional Methods In this section, we compare the effectiveness of wavelet-based histograms with MaxDiff (V,A) histograms and random sampling. Poosala et al <ref> [16] </ref> characterized the types of histograms in previous studies and proposed new types of histograms. They concluded in their experiments that the MaxDiff (V,A) histograms perform best overall. Random sampling can be used for selectivity estimation [5, 6, 10, 9]. <p> is estimated in the obvious way: If the result size of the query using a sample of size t is s, the selectivity is estimated as sT =t, where T is the size of the relation. 7 Our one-dimensional experiments use the many syn-thetic data distributions described in detail in <ref> [16] </ref>. We use T = 100,000 to 500,000 tuples, and the number n of distinct values of the attribute is between 200 and 500. The distributions from [16] subsume the types of one-dimensional distributions from the TPC-D benchmark. <p> =t, where T is the size of the relation. 7 Our one-dimensional experiments use the many syn-thetic data distributions described in detail in <ref> [16] </ref>. We use T = 100,000 to 500,000 tuples, and the number n of distinct values of the attribute is between 200 and 500. The distributions from [16] subsume the types of one-dimensional distributions from the TPC-D benchmark. We use eight different query sets in our experiments: A: fX b j b 2 Dg. C: fa X b j a; b 2 D, a &lt; bg. <p> In our experiments, all methods are allowed the same amount of storage. The default storage space we use in the experiments is 42 four-byte numbers (to be in line with Poosala et al's experiments <ref> [16] </ref>, which we replicate); the limited storage space corresponds to the practice in database management systems to devote only a very small amount of auxiliary space to each relation for selectivity estimation [18].
Reference: [17] <author> F. P. Preparata and M. I. Shamos. </author> <title> Computational Geometry: An Introduction. </title> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: The O (m) intervals can be stored in linear space in an interval tree data structure <ref> [17] </ref>. Given a domain element, the wavelet coefficients corresponding to the element's frequency can be found in O (log m) time by a stabbing query on the intervals stored in the interval tree. The reconstructed frequency is then the sum of the contributions of each of those wavelet coefficients.
Reference: [18] <author> P. G. Selinger, M. M. Astrahan, D. D. Chamberlin, R. A. Lorie, and T. G. Price. </author> <title> Access path selection in a relational database management system. </title> <booktitle> In Proceedings of the 1979 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 23-34, </pages> <year> 1979. </year>
Reference-contexts: we use in the experiments is 42 four-byte numbers (to be in line with Poosala et al's experiments [16], which we replicate); the limited storage space corresponds to the practice in database management systems to devote only a very small amount of auxiliary space to each relation for selectivity estimation <ref> [18] </ref>. The 42 numbers correspond to using 14 buckets for the MaxDiff (V,A) histogram, keeping m = 21 wavelet coefficients for wavelet-based histograms, and maintaining a random sample of size 42.
Reference: [19] <author> J. M. Shapiro. </author> <title> An embedded wavelet hierarchical image coder. </title> <booktitle> In Proceedings of 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 4, </volume> <pages> pages 657-660, </pages> <address> San Francisco, CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: I/O efficient techniques for computing the wavelet transform and thresholding for high-dimensional data are discussed in [26]. Ongoing work deals with improved space-accuracy tradeoffs by quantizing coefficients using, for example, a generalized zero-tree <ref> [19] </ref> followed by entropy encoding of the quantized coefficients. It may be that other wavelet bases perform better in practice than do the Haar and linear wavelet bases we have considered in this paper, and those possibilities will be considered on real-life data.
Reference: [20] <author> E. J. Stollnitz, T. D. Derose, and D. H. Salesin. </author> <title> Wavelets for Computer Graphics. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: We also implement a decomposition based upon linear wavelets that gives better estimation. To illustrate how Haar wavelets work, we start with a simple example. A detailed treatment of wavelets can be found in any standard reference on the subject (e.g., <ref> [8, 20] </ref>). Suppose that the data distribution T of attribute X is f (0; 2); (2; 5); (3; 2)g. We can easily derive the cumulated values T C + =f (0; 2); (1; 2); (2; 7); (3; 9)g.
Reference: [21] <author> W. Sweldens. </author> <type> Personal communication, </type> <year> 1997. </year>
Reference-contexts: This property has been con firmed by an extensive set of experiments. Previous methods for expressing the histogram as a piecewise smooth function required O (N ) time, although some researchers suspected that O (m log N )-time algorithms were possible <ref> [21] </ref>. We have developed an efficient and practical technique using priority queues that offers a substantial speedup: Theorem 4 The wavelet-based histogram can be transformed into a standard piecewise smooth representation in time O (m fi minflog m; log log N g) and using O (m) space.
Reference: [22] <institution> TPC benchmark D (decision support), </institution> <year> 1995. </year>
Reference-contexts: Our synthetic data sets are those from previous studies on histogram formation and from the TPC-D benchmark <ref> [22] </ref>. They correspond to studies on typical data found on the web. <p> In our experiments we use the synthetic data described in [15], which is indicative of various real-life 9 (a) Linear Wavelets (b) Haar Wavelets (c) MaxDiff (V, A) (d) Random Sampling data [1], and the TPC-D benchmark data <ref> [22] </ref>. Our query sets are obtained by extending the query sets AH defined in Section 6.1 to the multidimensional cases. The main concern of the multidimensional methods is the effectiveness of the histograms in capturing data dependencies. <p> In the experiments, we use the same value set and query set as for Figure 4. The frequency skew is z = 0:5 for (a) and z = 1:0 for (b). We conducted experiments using TPC-D data <ref> [22] </ref>. We report the results for a typical experiment here.
Reference: [23] <author> J. S. Vitter. </author> <title> Random sampling with a reservoir. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 11(1) </volume> <pages> 37-57, </pages> <month> March </month> <year> 1985. </year>
Reference-contexts: The sample data distribution is then used as an estimate of the real data distribution. To obtain the random sample in a single linear pass, the method of choice is the skip-based method [24] when the number of tuples T is known beforehand or the reservoir sampling variant <ref> [23] </ref> when T is unknown. A running up-to-date sample can be kept using a backing sample approach [4]. We do not consider in this paper the issues dealing with sample size and the er 2 rors caused by sampling.
Reference: [24] <author> J. S. Vitter. </author> <title> An efficient algorithm for sequential random sampling. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 13(1) </volume> <pages> 58-67, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: The sample data distribution is then used as an estimate of the real data distribution. To obtain the random sample in a single linear pass, the method of choice is the skip-based method <ref> [24] </ref> when the number of tuples T is known beforehand or the reservoir sampling variant [23] when T is unknown. A running up-to-date sample can be kept using a backing sample approach [4].
Reference: [25] <author> J. S. Vitter. </author> <title> External memory algorithms. </title> <booktitle> In Proceedings of the 1998 ACM Symposium on Principles of Database Systems, </booktitle> <month> June </month> <year> 1998. </year> <type> Invited tutorial. </type>
Reference-contexts: When the cardinality of V is too large, multiple passes through the relation will be required to obtain T , resulting in possibly excessive I/O cost. We can instead use an I/O-efficient external merge sort to compute T and minimize the I/O cost <ref> [25] </ref>. The merge sort process here is different from the traditional one: During the merging process, records with the same attribute value can be combined by summing the frequencies.
Reference: [26] <author> J. S. Vitter, M. Wang, and B. Iyer. </author> <title> Data cube approximation via wavelets. </title> <type> Manuscript, </type> <year> 1998. </year> <month> 12 </month>
Reference-contexts: For high-dimensional data, which is generally larger, the I/O efficiencies of the wavelet decomposition and of thresholding are con sidered in <ref> [26] </ref>. 3.2 Wavelet Decomposition The goal of the wavelet decomposition step is to represent the extended cumulative data distribution T C + at hierarchical levels of detail. First we need to choose wavelet basis functions. <p> Our histograms give improved performance for selectivity estimation compared with random sampling and previous approaches. In <ref> [26] </ref>, a new thresholding method based on a logarithm transform is proposed that dramatically reduces the errors in wavelet-based approximation of high-dimensional data, such as in OLAP data cubes. <p> High-dimensional data can often be much larger than the low-dimensional data considered in this paper, and I/O communication can be a bottleneck. I/O efficient techniques for computing the wavelet transform and thresholding for high-dimensional data are discussed in <ref> [26] </ref>. Ongoing work deals with improved space-accuracy tradeoffs by quantizing coefficients using, for example, a generalized zero-tree [19] followed by entropy encoding of the quantized coefficients.
References-found: 26

