URL: http://www.uni-paderborn.de/fachbereich/AG/agmadh/Scripts/GENERAL/MIT/Goemans-randomized.ps.gz
Refering-URL: http://www.uni-paderborn.de/fachbereich/AG/agmadh/WWW/english/scripts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Randomized Algorithms  
Author: Lecturer: Michel X. Goemans 
Date: September 1994  
Note: 18.415/6.854 Advanced Algorithms  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> N. Alon. </author> <title> Eigenvalues and expanders. </title> <journal> Combinatorica, </journal> <volume> 6 </volume> <pages> 83-96, </pages> <year> 1986. </year>
Reference-contexts: If it is not 0, we know the determinant is not equivalent to 0, so G has a perfect matching. Random-5 Theorem 3 Let the values of the x ij be independently and uniformly distributed in <ref> [1; 2; : : : ; 2n] </ref>, where n = jAj = jBj. Let A 0 be the resulting matrix. <p> We will prove that if we select the costs according to a suitable probability distribution then, with high probability, there exists a unique minimum cost matching. Let c ij be independent, identically distributed random variables with distribution uniform in the interval <ref> [1; : : : ; 2m] </ref>, where m = jEj. The algorithm computes det (A) and claims that there is a perfect matching if and only if det (A) is nonzero. <p> to last line is justified by our selection of m = jEj and the fact that d ij is independent of c ij , so that the probability of c ij being equal to the particular value d ij is either 1 2m iff d ij is in the range <ref> [1; : : : ; 2m] </ref> or 0 otherwise. fl Notice that if we repeat the algorithm with new random c ij 's, then the second trial will be independent of the first run. <p> Their result can be viewed as an isoperimetric inequality, and its derivation is analogous to an isoperi-metric inequality of Cheeger [4] for Riemannian manifolds, or results on expanders by Alon <ref> [1] </ref> and Alon and Milman [2]. Random-17 6 Conductance of Markov chains (Jerrum-Sinclair) Given a set S of states, let C S denote the capacity of S, which is the probability of being in some state in S when steady-state is reached.
Reference: [2] <author> N. Alon and V. Milman. </author> <title> 1 , isoperimetric inequalities for graphs and supercon-centrators. </title> <journal> Journal of Combinatorial Theory B, </journal> <volume> 38 </volume> <pages> 73-88, </pages> <year> 1985. </year>
Reference-contexts: If it is not 0, we know the determinant is not equivalent to 0, so G has a perfect matching. Random-5 Theorem 3 Let the values of the x ij be independently and uniformly distributed in <ref> [1; 2; : : : ; 2n] </ref>, where n = jAj = jBj. Let A 0 be the resulting matrix. <p> Their result can be viewed as an isoperimetric inequality, and its derivation is analogous to an isoperi-metric inequality of Cheeger [4] for Riemannian manifolds, or results on expanders by Alon [1] and Alon and Milman <ref> [2] </ref>. Random-17 6 Conductance of Markov chains (Jerrum-Sinclair) Given a set S of states, let C S denote the capacity of S, which is the probability of being in some state in S when steady-state is reached.
Reference: [3] <author> I. Barany and Z. Furedi. </author> <title> Computing the volume is difficult. </title> <booktitle> In Proceedings of the 18th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 442-447, </pages> <year> 1986. </year>
Reference-contexts: This is interesting because it can also be shown that one cannot even approximate the volume to within an exponential factor of n cn for c &lt; 0:5 in polynomial time, where n is the dimension <ref> [3] </ref>.
Reference: [4] <author> J. Cheeger. </author> <title> A lower bound for the smallest value of the Laplacian. </title> <editor> In R. Gunning, editor, </editor> <booktitle> Problems in analysis, </booktitle> <pages> pages 195-199. </pages> <publisher> Princeton University Press, </publisher> <year> 1970. </year>
Reference-contexts: For this purpose, Jerrum and Sinclair [11] have derived a relationship between the so-called conductance of the MC and max . Their result can be viewed as an isoperimetric inequality, and its derivation is analogous to an isoperi-metric inequality of Cheeger <ref> [4] </ref> for Riemannian manifolds, or results on expanders by Alon [1] and Alon and Milman [2].
Reference: [5] <author> P. Dagum, M. Mihail, M. Luby, and U. Vazirani. </author> <title> Polytopes, permanents and graphs with large factors. </title> <booktitle> In Proceedings of the 29th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 412-422, </pages> <year> 1988. </year>
Reference-contexts: Theorem 20 (Dagum, Luby, Mihail and Vazirani <ref> [5] </ref>) There exists a constant c such that fl (H) c u k Corollary 21 There exists a fully polynomial *-sampling scheme for U k provided that u k ) for some c 0 . Random-25 A preliminary lemma is required.
Reference: [6] <author> M. Dyer, A. Frieze, and R. Kannan. </author> <title> A random polynomial algorithm for approximating the volume of convex bodies. </title> <journal> Journal of the ACM, </journal> <pages> pages 1-17, </pages> <year> 1991. </year> <month> Random-29 </month>
Reference-contexts: Nevertheless, a very nice application of rapidly mixing markov chains is precisely in the computation of the volume of a convex body or region <ref> [6] </ref>. <p> Other applications of Markov Chains There are other uses for Markov Chains in the design of algorithms. Of these the most interesting is for computing the volume of convex bodies. It can be shown that a fully polynomial randomized approximation scheme may be constructed for this problem <ref> [6] </ref>. This is interesting because it can also be shown that one cannot even approximate the volume to within an exponential factor of n cn for c &lt; 0:5 in polynomial time, where n is the dimension [3].
Reference: [7] <author> R. Karp. </author> <title> An introduction to randomized algorithms. </title> <journal> Discrete Applied Mathe--matics, </journal> <volume> 34 </volume> <pages> 165-201, </pages> <year> 1991. </year>
Reference-contexts: For those interested in learning more about randomized algorithms, we strongly recommend the forthcoming book by Motwani and Raghavan. [9]. First we shall describe some basic principles which typically underly the construction of randomized algorithms. The description follows a set of lectures given by R.M. Karp <ref> [7] </ref>. 1. Foiling the adversary. This does not need much explanation since this was the main use of randomization in the context of on-line algorithms. It applies to problems which can be viewed as a game between the algorithm designer and the adversary.
Reference: [8] <author> R. M. Karp and M. O. Rabin. </author> <title> Efficient randomized pattern-matching algorithms. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 31 </volume> <pages> 249-260, </pages> <year> 1987. </year>
Reference-contexts: Fingerprinting This is a technique for representing a large object by a small fingerprint. Under appropriate circumstances, if two objects have the same fingerprint, then there is strong evidence that they are identical. An example is the randomized algorithm for pattern matching by Karp and Rabin <ref> [8] </ref>. Suppose we are given a string of length n such as randomizearandomlyrandomrandomizedrandom and a pattern of size m such as random. The task is to find all the places the pattern appears in the long string. Let us first describe our model of computation.
Reference: [9] <author> R. Motwani and P. Raghavan. </author> <title> Randomized Algorithms. </title> <year> 1994. </year>
Reference-contexts: In these notes, we shall describe other important illustrations of randomized algorithms in other areas of the theory of algorithms. For those interested in learning more about randomized algorithms, we strongly recommend the forthcoming book by Motwani and Raghavan. <ref> [9] </ref>. First we shall describe some basic principles which typically underly the construction of randomized algorithms. The description follows a set of lectures given by R.M. Karp [7]. 1. Foiling the adversary.
Reference: [10] <author> K. Mulmuley, U. Vazirani, and V. Vazirani. </author> <title> Matching is as easy as matrix inversion. </title> <journal> Combinatorica, </journal> <volume> 7(1) </volume> <pages> 105-113, </pages> <year> 1987. </year>
Reference-contexts: This area is covered in details in these notes. 2 Randomized Algorithm for Bipartite Matching We now look at a randomized algorithm by Mulmuley, Vazirani and Vazirani <ref> [10] </ref> for bipartite matching. This algorithm uses randomness to check an identity.
Reference: [11] <author> A. Sinclair and M. Jerrum. </author> <title> Approximate counting, uniform generation and rapidly mixing markov chains. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 93-133, </pages> <year> 1989. </year> <month> Random-30 </month>
Reference-contexts: Returning to our example: In order to calculate how many iterations are needed until we are arbitrarily close to the uniform distribution , we need to evaluate the second eigenvalue. For this purpose, Jerrum and Sinclair <ref> [11] </ref> have derived a relationship between the so-called conductance of the MC and max . <p> This implies that it will take a long time to reach the stationary distribution, so the rate of convergence will be small. We might therefore expect that 2 will be close to 1 if the conductance is small. Theorem 16 (Jerrum-Sinclair <ref> [11] </ref>) For an ergodic MC that is TR, we have 2 &lt; 1 2 =2: Remark 1 There exist corresponding lower bounds expressing that t 2 and 2 1 2.
References-found: 11

