URL: http://www.iscs.nus.sg/~plong/papers/drift.ps
Refering-URL: 
Root-URL: 
Title: Tracking Drifting Concepts By Minimizing Disagreements  
Author: David P. Helmbold and Philip M. Long 
Date: March 24, 1994  
Address: Santa Cruz, CA 95064  
Affiliation: CIS Board UC Santa Cruz  
Abstract: In this paper we consider the problem of tracking a subset of a domain (called the target) which changes gradually over time. A single (unknown) probability distribution over the domain is used to generate random examples for the learning algorithm and measure the speed at which the target changes. Clearly, the more rapidly the target moves, the harder it is for the algorithm to maintain a good approximation of the target. Therefore we evaluate algorithms based on how much movement of the target can be tolerated between examples while predicting with accuracy *. Furthermore, the complexity of the class H of possible targets, as measured by d, its VC-dimension, also effects the difficulty of tracking the target concept. We show that if the problem of minimizing the number of disagreements with a sample from among concepts in a class H can be approximated to within a factor k, then there is a simple tracking algorithm for H which can achieve a probability * of making a mistake if the target movement rate is at most a constant times * 2 =(k(d + k) ln 1 * ), where d is the Vapnik-Chervonenkis dimension of H. Also, we show that if H is properly PAC-learnable, then there is an efficient (randomized) algorithm that with high probability approximately minimizes disagreements to within a factor of 7d + 1, yielding an efficient tracking algorithm for H which tolerates drift rates up to a constant times * 2 =(d 2 ln 1 In addition, we prove complementary results for the classes of halfspaces and axis-aligned hy- perrectangles showing that the maximum rate of drift that any algorithm (even with unlimited computational power) can tolerate is a constant times * 2 =d. 
Abstract-found: 1
Intro-found: 1
Reference: [ABS90] <author> M. Anthony, N. Biggs, and J. Shawe-Taylor. </author> <title> The learnability of formal concepts. </title> <booktitle> The 1990 Workshop on Computational Learning Theory, </booktitle> <pages> pages 246-257, </pages> <year> 1990. </year>
Reference-contexts: We then return the best hypothesis of those produced by A during the various stages. We use the tightest available PAC-learning bounds, due to Anthony, Biggs and Shawe-Taylor <ref> [ABS90] </ref>, to argue that with high probability, a hypothesis consistent with the subsample can't be too bad on the whole sample. Littlestone and Warmuth [LW89] describe a variant of the weighted majority algorithm where the weights are kept above some lower limit. <p> The early steps of our proof are also similar to those in [AW92], but the proofs, and therefore the results, divirge after a time due to the difference in the applications required by the two papers. First, the results of Anthony, Biggs and Shawe-Taylor <ref> [ABS90] </ref> may be applied 4 to obtain the following. Theorem 6 ([ABS90]) Let X be a set and let H be a concept class over X of VC-dimension d. Let D be a probability distribution over H. Choose f 2 H and * &lt; 1=2. <p> opt)) 7d=fl ): 3 The difference between the result trivially obtainable by combining Theorems 12 and 16 of [KL88] and our result is that in the former, the sample is restricted to have the same number of positive and negative examples. 4 For d &gt; 1, use Theorem 2.1 of <ref> [ABS90] </ref> with ffi = 1=2, and for d = 1 a simple argument along the lines of the proof for their Theorem 2.1 suffices. 8 Algorithm Min-Disagreements Inputs: a sample S of m examples; l, the number of iterations to run; d = VCdim (H n ); and desired approximation factor
Reference: [AV79] <author> D. Angluin and L. Valiant. </author> <title> Fast probabilistic algorithms for Hamiltonion circuits and matchings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18(2) </volume> <pages> 155-193, </pages> <year> 1979. </year>
Reference: [AV90] <author> D. Aldous and U. Vazirani. </author> <title> A Markovian extension of Valiant's learning model. </title> <booktitle> Proceedings of the 31st Annual Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 392-396, </pages> <year> 1990. </year>
Reference-contexts: In contrast, we assume that the sequence of targets is chosen by an adversary before any random examples are generated. Aldous and Vazirani <ref> [AV90] </ref> studied a different version of learning in a changing environment. In their model the target concept is fixed, but the examples are generated by a Markov process rather then from a fixed distribution. The conclusions contain potential applications, observations, and a list of open problems.
Reference: [AW92] <author> N. Abe and O. Watanabe. </author> <title> Polynomially sparse variations and reducibility among prediction problems. </title> <journal> IEICE Trans. Inf. & Syst., </journal> <volume> E75-D(4):449-458, </volume> <year> 1992. </year>
Reference-contexts: We use a technique due to Kearns and Li [KL88] and Abe and Watanabe <ref> [AW92] </ref>, working in stages, where at each stage, we subsample according to the distribution which is uniform over the sample, hoping to get a subsample for which there is a consistent hypothesis, so that we can successfully apply A. <p> Furthermore, algorithm Min-Disagreements from Figure 1 is very similar to the Algorithm B given in a recent paper by Abe and Watanabe <ref> [AW92] </ref>, which was described to us some time ago by Abe. The early steps of our proof are also similar to those in [AW92], but the proofs, and therefore the results, divirge after a time due to the difference in the applications required by the two papers. <p> Furthermore, algorithm Min-Disagreements from Figure 1 is very similar to the Algorithm B given in a recent paper by Abe and Watanabe <ref> [AW92] </ref>, which was described to us some time ago by Abe. The early steps of our proof are also similar to those in [AW92], but the proofs, and therefore the results, divirge after a time due to the difference in the applications required by the two papers. First, the results of Anthony, Biggs and Shawe-Taylor [ABS90] may be applied 4 to obtain the following.
Reference: [BEHW89] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> JACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: If we now apply the results of [VC71], however, our analysis indicates that these algorithms are more than a factor of * from the best upper bounds we can prove on the maximum tolerable rate of drift. In the case of learning stationary targets, it has been observed [Vap82] <ref> [BEHW89] </ref> that uniformly good estimates of the quality of hypotheses were not required for learning in the PAC- model [Val84]. Instead, one only needed to bound the probability that an "*-bad" hypothesis was consistent with a sequence of examples. <p> Nevertheless, given reasonable restrictions on the rate of drift there is, with high probability, some hypothesis having very few disagreements with a reasonable sized suffix of a random sequence of examples. Thus, we are able to apply another of the results of <ref> [BEHW89] </ref>, which bounds the probability that any *-bad hypothesis is consistent with all but a fraction *=2 of the examples. The number of examples required to bound this "*-bad but highly consistent" probability by ffi is within a constant of that for the completely consistent case. <p> A slightly modified analysis holds for the case in which the tracking algorithm uses a hypothesis which only approximately minimizes disagreements with a suffix of the examples. In Section 4, we give a general purpose algorithmic transformation turning a randomized polynomial time hypothesis finder A <ref> [BEHW89] </ref> which, with high probability, returns a hypothesis consistent with an input sample, into an algorithm which efficiently approximately minimizes disagreements to within a factor of 7d + 1, where d is the VC-dimension of the target class. <p> Note that ^ er f is the empirical estimate of the error of h obtained when the (unchang-ing) target concept is f . Our first lemma follows immediately from the results of <ref> [BEHW89, Theorem A3.1] </ref>. <p> One wonders whether these results can be improved. Haussler [Hau91] has generalized the results of <ref> [BEHW89] </ref> to apply to learning in many frameworks, one of which is the learning of real valued functions.
Reference: [EHKV89] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L.G. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. Information and Computation, </title> <address> 82(3):247251, </address> <year> 1989. </year>
Reference-contexts: It is easy to see that VCdim (BASIC n ) = n. Our argument for the upper bound on BASIC n uses ideas from earlier arguments giving lower bounds on the probability of a mistake when predicting a stationary target function <ref> [EHKV89] </ref> [HLW90]. The intuition behind the argument is as follows. Suppose there is a water truck rolling down a section of dusty road at 10 kilometers per hour. Either the truck is empty or it is spraying water (unknown to us, but both possibilities are equally likely).
Reference: [Hau91] <author> D. Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <type> Technical Report UCSC-CRL-91-02, </type> <institution> University of California at Santa Cruz, </institution> <year> 1991. </year>
Reference-contexts: It can be shown by modifying the proofs of Section 3, that for c* 3 =(d ln (1=*)), an algorithm can achieve probability of mistake at most t + * for all large enough t [HL91]. One wonders whether these results can be improved. Haussler <ref> [Hau91] </ref> has generalized the results of [BEHW89] to apply to learning in many frameworks, one of which is the learning of real valued functions. <p> Using Haussler's results, the techniques of Section 3 can trivially be extended to apply to uniformly bounded classes of real valued functions (e.g., feed forward neural networks of a particular architecture which has one output node), where, in place of the Vapnik-Chervonenkis dimension, we use Pollard's pseudo-dimension <ref> [Pol84, Hau91] </ref>, and instead of wanting to make the probability of mistake small, we want to make the expectation of the absolute value of the difference between our prediction and the truth small.
Reference: [HL91] <author> D.P. </author> <title> Helmbold and P.M. Long. Tracking drifting concepts using random examples. </title> <booktitle> The 1991 Workshop on Computational Learning Theory, </booktitle> <pages> pages 13-23, </pages> <year> 1991. </year>
Reference-contexts: In their model the target concept is fixed, but the examples are generated by a Markov process rather then from a fixed distribution. The conclusions contain potential applications, observations, and a list of open problems. The results presented here improve on preliminary results described in <ref> [HL91] </ref>. 2 Notation and Mathematical Preliminaries Let N denote the positive integers and Q denote the rationals. Let ln denote that natural logarithm, and log denote the logarithm base 2. <p> It can be shown by modifying the proofs of Section 3, that for c* 3 =(d ln (1=*)), an algorithm can achieve probability of mistake at most t + * for all large enough t <ref> [HL91] </ref>. One wonders whether these results can be improved. Haussler [Hau91] has generalized the results of [BEHW89] to apply to learning in many frameworks, one of which is the learning of real valued functions.
Reference: [HLW88] <author> D. Haussler, N. Littlestone, and M.K. Warmuth. </author> <title> Predicting f0; 1g functions on ran-domly drawn points. </title> <booktitle> Proceedings of the 29th Annual Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 100-109, </pages> <year> 1988. </year>
Reference-contexts: Thus the adversary must choose sequences of functions where each h i is "close" to h i1 . This is made precise in Section 2. Many readers will notice the similarity of our model to the prediction model studied in <ref> [HLW88] </ref> and elsewhere. The key difference is that in our model there is no single target function, but rather a succession of related target functions. Since the learner may receive only a single example before the target changes, it is unreasonable to expect that the hypotheses converge to a target.
Reference: [HLW90] <author> D. Haussler, N. Littlestone, and M.K. Warmuth. </author> <title> Predicting f0; 1g-functions on ran-domly drawn points. </title> <type> Technical Report UCSC-CRL-90-54, </type> <institution> University of California Santa Cruz, Computer Research Laboratory, </institution> <month> December </month> <year> 1990. </year> <note> To appear in Information and Computation. </note>
Reference-contexts: It is easy to see that VCdim (BASIC n ) = n. Our argument for the upper bound on BASIC n uses ideas from earlier arguments giving lower bounds on the probability of a mistake when predicting a stationary target function [EHKV89] <ref> [HLW90] </ref>. The intuition behind the argument is as follows. Suppose there is a water truck rolling down a section of dusty road at 10 kilometers per hour. Either the truck is empty or it is spraying water (unknown to us, but both possibilities are equally likely). <p> The following theorem follows from the bounds for BASIC n via a trivial embedding of BASIC n into HALFSPACES n and a similar embedding of BASIC 2n into BOXES n using a simplified version of prediction preserving reductions [PW90]. The same embeddings were employed in <ref> [HLW90] </ref>. The details are omitted. Theorem 14 For all * &lt; 1=e 2 and n 2 N, HALFSPACES n is not (*; )-trackable when &gt; e 4 * 2 =n, and BOXES n is not (*; )-trackable when &gt; e 4 * 2 =2n.
Reference: [HR90] <author> T. Hagerup and C. Rub. </author> <title> A guided tour of Chernov bounds. </title> <journal> Information Processing Letters, </journal> <volume> 33 </volume> <pages> 305-308, </pages> <year> 1990. </year>
Reference: [KL88] <author> M. Kearns and M. Li. </author> <title> Learning in the presence of malicious errors. </title> <booktitle> Proceedings of the 20th ACM Symposium on the Theory of Computation, </booktitle> <pages> pages 267-279, </pages> <year> 1988. </year> <month> 15 </month>
Reference-contexts: We use a technique due to Kearns and Li <ref> [KL88] </ref> and Abe and Watanabe [AW92], working in stages, where at each stage, we subsample according to the distribution which is uniform over the sample, hoping to get a subsample for which there is a consistent hypothesis, so that we can successfully apply A. <p> is a positive constant c 2 , depending only on k, such that for any 0 &lt; &lt; * where d log 1 ; strategy A 0 (*; )-tracks H. 7 4 Efficiently Approximately Minimizing Disagreements In this section we discuss the application of the techniques of Kearns and Li <ref> [KL88] </ref> to the problem of approximately minimizing disagreements from among the hypotheses in a class H, showing that if there is an efficient algorithm which returns a hypothesis with no disagreements if there is one, then there is an efficient randomized algorithm which with high probability returns a hypothesis that minimizes <p> Results very similar to those described here are implicit in the work of Kearns and Li (Theorems 12 and 16), although some minor modifications are necessary. 3 Also, we make use of the techniques of <ref> [KL88] </ref> in our proof. Furthermore, algorithm Min-Disagreements from Figure 1 is very similar to the Algorithm B given in a recent paper by Abe and Watanabe [AW92], which was described to us some time ago by Abe. <p> S, then Algorithm Min-Disagreements with inputs S,m,l,d,fl finds a hypothesis consistent with all but (fl + 1)opt examples in S with probability at least 1 exp ((l (2q 1)=2e 1=fl )(flopt=9 (m opt)) 7d=fl ): 3 The difference between the result trivially obtainable by combining Theorems 12 and 16 of <ref> [KL88] </ref> and our result is that in the former, the sample is restricted to have the same number of positive and negative examples. 4 For d &gt; 1, use Theorem 2.1 of [ABS90] with ffi = 1=2, and for d = 1 a simple argument along the lines of the proof <p> dimensional space, i.e. f i=1 Corollary 12 There is a constant c &gt; 0 and there are efficient tracking algorithms for each of fHALFSPACES n : n 2 Ng and fBOXES n : n 2 Ng that (*; )-track these classes for n 2 log (1=*) Finally, Kearns and Li <ref> [KL88] </ref> showed that, loosely speaking, significantly improving the factor of approximation of our algorithm for minimizing disagreements for hyperrectangles (in particular, removing the dependence on d) would lead to corresponding improvements on the approximation algorithm for set cover, which has not been significantly improved since the 1970's.
Reference: [KPR90] <author> T. Kuh, T. Petsche, and R. Rivest. </author> <title> Learning time varying concepts. </title> <booktitle> In NIPS 3. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: However, if the target changes k times, then their mistake bound for the weighted majority algorithm goes up by about a factor of k. It is difficult to translate these bounds into our model as our targets potentially change with each example. Kuh, Petsche and Rivest <ref> [KPR90, KPR91] </ref> studied a variety of models in which the target 3 changes over time, including cases in which the target drifts slowly.
Reference: [KPR91] <author> T. Kuh, T. Petsche, and R. Rivest. </author> <title> Mistake bounds of incremental learners when con-cepts drift with applications to feedforward networks. </title> <booktitle> In NIPS 4. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: However, if the target changes k times, then their mistake bound for the weighted majority algorithm goes up by about a factor of k. It is difficult to translate these bounds into our model as our targets potentially change with each example. Kuh, Petsche and Rivest <ref> [KPR90, KPR91] </ref> studied a variety of models in which the target 3 changes over time, including cases in which the target drifts slowly.
Reference: [Lit89] <author> N. Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, </type> <institution> UC Santa Cruz, </institution> <year> 1989. </year>
Reference: [Lon92] <author> P.M. </author> <title> Long. Towards a more comprehensive theory of learning in computers. </title> <type> PhD thesis, </type> <institution> UC Santa Cruz, </institution> <year> 1992. </year>
Reference-contexts: Nevertheless, it remains possible that, via other methods, one might obtain efficient algorithms that track these classes at rates even closer to optimal. In recent work, the constant of approximation has been improved somewhat <ref> [Lon92] </ref>, but the linear dependence on d remains. 5 Upper bounds on the tolerable amount of drift In this section we prove upper bounds on the tolerable amount of drift for two commonly studied concept classes: halfspaces and axis-aligned rectangles.
Reference: [LW89] <author> N. Littlestone and M.K. Warmuth. </author> <title> The weighted majority algorithm. </title> <booktitle> Proceedings of the 30th Annual Symposium on the Foundations of Computer Science, </booktitle> <year> 1989. </year>
Reference-contexts: We use the tightest available PAC-learning bounds, due to Anthony, Biggs and Shawe-Taylor [ABS90], to argue that with high probability, a hypothesis consistent with the subsample can't be too bad on the whole sample. Littlestone and Warmuth <ref> [LW89] </ref> describe a variant of the weighted majority algorithm where the weights are kept above some lower limit. This allows the weighted majority algorithm to recover and adapt to changes in the target.
Reference: [Pol84] <author> D. Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Using Haussler's results, the techniques of Section 3 can trivially be extended to apply to uniformly bounded classes of real valued functions (e.g., feed forward neural networks of a particular architecture which has one output node), where, in place of the Vapnik-Chervonenkis dimension, we use Pollard's pseudo-dimension <ref> [Pol84, Hau91] </ref>, and instead of wanting to make the probability of mistake small, we want to make the expectation of the absolute value of the difference between our prediction and the truth small.
Reference: [PV88] <author> L. Pitt and L.G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference: [PW90] <author> L. Pitt and M.K. Warmuth. </author> <title> Prediction preserving reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 41(3), </volume> <year> 1990. </year>
Reference-contexts: The following theorem follows from the bounds for BASIC n via a trivial embedding of BASIC n into HALFSPACES n and a similar embedding of BASIC 2n into BOXES n using a simplified version of prediction preserving reductions <ref> [PW90] </ref>. The same embeddings were employed in [HLW90]. The details are omitted.
Reference: [Val84] <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11):11341142, </volume> <year> 1984. </year>
Reference-contexts: In the case of learning stationary targets, it has been observed [Vap82] [BEHW89] that uniformly good estimates of the quality of hypotheses were not required for learning in the PAC- model <ref> [Val84] </ref>. Instead, one only needed to bound the probability that an "*-bad" hypothesis was consistent with a sequence of examples. <p> Assume opt 1. Then Pr (algorithm fails) exp 2e 1=fl fl opt 7d=fl exp 7ld opt (2q 1) 18me 1=7 exp ld (2q 1) This completes the proof. 2 We can now take advantage of the following two theorems, which address learning in Valiant's PAC model <ref> [Val84] </ref>. 10 Theorem 9 ([PV88]) If H [ n 2 Q n is properly PAC learnable, then there is a randomized polynomial time algorithm which solves the consistency problem for H.
Reference: [Vap82] <author> V.N. Vapnik. </author> <title> Estimation of Dependencies based on Empirical Data. </title> <publisher> Springer Verlag, </publisher> <year> 1982. </year>
Reference-contexts: If we now apply the results of [VC71], however, our analysis indicates that these algorithms are more than a factor of * from the best upper bounds we can prove on the maximum tolerable rate of drift. In the case of learning stationary targets, it has been observed <ref> [Vap82] </ref> [BEHW89] that uniformly good estimates of the quality of hypotheses were not required for learning in the PAC- model [Val84]. Instead, one only needed to bound the probability that an "*-bad" hypothesis was consistent with a sequence of examples.
Reference: [Vap89] <author> V.N. Vapnik. </author> <title> Inductive principles of the search for empirical dependences (methods based on weak convergence of probability measures). </title> <booktitle> The 1989 Workshop on Computational Learning Theory, </booktitle> <year> 1989. </year>
Reference-contexts: The results presented here improve on preliminary results described in [HL91]. 2 Notation and Mathematical Preliminaries Let N denote the positive integers and Q denote the rationals. Let ln denote that natural logarithm, and log denote the logarithm base 2. After Vapnik <ref> [Vap89] </ref>, we will adopt a naive attitude toward measurability, assuming that every set is measurable, and simply speak of probability distributions on sets. This assumption is not unreasonable, since if a digital computer is to input or output representations of arbitrary set elements, the set must be countable.
Reference: [VC71] <author> V.N. Vapnik and A.Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, </title> <address> 16(2):264280, </address> <year> 1971. </year>
Reference-contexts: They work by either minimizing or approximately minimizing the number of disagreements with the most recent examples, and using the resulting hypothesis to predict the label of the next point. To analyze such algorithms, one might imagine applying the results of Vapnik and Chervonenkis <ref> [VC71] </ref> to show that if for each hypothesis h in the class, we estimate the probability that h will make a mistake on the next trial by considering the fraction of the last t trials on which h made a mistake, none of these estimates will be very far from the <p> The movement of the target prevents us from simply applying the results of <ref> [VC71] </ref>. To remedy this, we first bound the probability that for any hypothesis h, the estimate we obtain is very far from the estimate we would have obtained, had the target not been moving. Then we are ready to apply uniform convergence results. If we now apply the results of [VC71], <p> <ref> [VC71] </ref>. To remedy this, we first bound the probability that for any hypothesis h, the estimate we obtain is very far from the estimate we would have obtained, had the target not been moving. Then we are ready to apply uniform convergence results. If we now apply the results of [VC71], however, our analysis indicates that these algorithms are more than a factor of * from the best upper bounds we can prove on the maximum tolerable rate of drift. <p> They were then able to shave a factor of 1=* off the bound on the number of examples required for learning with accuracy * obtained by simply applying the results of <ref> [VC71] </ref>. However, in our case, there may not be any hypothesis consistent with more than a few of the most recent examples. <p> We will drop the subscripts where there is no possibility of confusion. If X is a set and H is a family of f0; 1g valued functions defined on X, then the Vapnik- Chervonenkis (VC) dimension of H <ref> [VC71] </ref> is maxfjT j : T = ft 1 ; :::; t k g X; f (h (t 1 ); :::; h (t k )) : h 2 Hg = f0; 1g jT j g: We will assume throughout that all classes discussed have at least two elements, and thus have
References-found: 24

