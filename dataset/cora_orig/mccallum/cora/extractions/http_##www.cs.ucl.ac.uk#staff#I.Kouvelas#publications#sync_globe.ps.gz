URL: http://www.cs.ucl.ac.uk/staff/I.Kouvelas/publications/sync_globe.ps.gz
Refering-URL: http://www.cs.ucl.ac.uk/staff/I.Kouvelas/publications/
Root-URL: http://www.cs.ucl.ac.uk
Title: Lip Synchronisation for use over the Internet: Analysis and Implementation  
Author: Isidor Kouvelas, Vicky Hardman and Anna Watson 
Address: Gower Street, London WC1E 6BT, UK  
Affiliation: Department of Computer Science University College London  
Abstract: This paper presents the first implementation of multicast inter-stream synchronisation over the Mbone / Internet. Variable bit-rate video, packet audio with silence supression and the unpredictable Mbone traffic characteristics provide a real test for the design. The paper also describes an efficient novel video tool architecture to provide intra-stream synchronisation, and implementation of inter-stream synchronisation using a local conference bus. Subjective performance results indicate that the efficient implementation is good enough to provide lip synchronisation for multimedia conferencing applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Van Jacobson and Steve McCanne. </author> <title> The LBL audio tool vat. </title> <note> Available from ftp://ftp.ee.lbl.gov/conferencing/vat/, </note> <month> July </month> <year> 1992. </year>
Reference-contexts: Lip synchronisation can be provided by artificially delaying either the audio or the video output, so that words and sounds are matched in time. The Mbone is a multicast overlay over the Internet which provides multiway communication. Existing Mbone audio tools - such as vat <ref> [1] </ref>, nevot [2] and existing Mbone video tools such as vic [3], ivs [4] do not provide support for lip synchronisation. A new audio tool, RAT [5, 6], developed at UCL, together with a modified version of vic [3] provides the first implementation of lip synchronisation over the Mbone.
Reference: [2] <author> Henning Schulzrinne. </author> <title> Guide to NeVoT. </title> <institution> GMD Fokus, </institution> <address> Berlin, Germany, 3.32 edition, </address> <month> September </month> <year> 1995. </year> <note> The software is available from ftp://gaia.cs.umass.edu/pub/hgschulz/nevot. </note>
Reference-contexts: Lip synchronisation can be provided by artificially delaying either the audio or the video output, so that words and sounds are matched in time. The Mbone is a multicast overlay over the Internet which provides multiway communication. Existing Mbone audio tools - such as vat [1], nevot <ref> [2] </ref> and existing Mbone video tools such as vic [3], ivs [4] do not provide support for lip synchronisation. A new audio tool, RAT [5, 6], developed at UCL, together with a modified version of vic [3] provides the first implementation of lip synchronisation over the Mbone. <p> Consequently, audio tools aim to receive 99.9% of the packets, since the delay characteristic shows a long tail for packets that are abnormally delayed <ref> [2, 7] </ref>. The effect of jitter on video is to result in jerky rendering of frames, a degradation which can be tolerated by users.
Reference: [3] <author> Steve McCanne and Van Jacobson. </author> <title> vic: A flexible framework for packet video. </title> <booktitle> In Proc. of ACM Multimedia '95, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: The Mbone is a multicast overlay over the Internet which provides multiway communication. Existing Mbone audio tools - such as vat [1], nevot [2] and existing Mbone video tools such as vic <ref> [3] </ref>, ivs [4] do not provide support for lip synchronisation. A new audio tool, RAT [5, 6], developed at UCL, together with a modified version of vic [3] provides the first implementation of lip synchronisation over the Mbone. <p> Existing Mbone audio tools - such as vat [1], nevot [2] and existing Mbone video tools such as vic <ref> [3] </ref>, ivs [4] do not provide support for lip synchronisation. A new audio tool, RAT [5, 6], developed at UCL, together with a modified version of vic [3] provides the first implementation of lip synchronisation over the Mbone. RAT and vic use the proposed IETF standard real-time protocol, RTP [7], to provide the necessary functionality required for multicast voice and video communication. <p> The requirement of low delay means that a packet audio system must launch small packets frequently, rather than big packets less often [8]. Typical packet sizes used for audio over the Internet vary from 20 to 80ms in length. Mbone video tools, such as vic <ref> [3] </ref>, typically provide slow-scan video (2-10 frames/second), rather than the broadcast rate (25 frames/second), because of the potentially large amounts of bandwidth consumed by video, especially during multiway communication. <p> The effect of jitter on video is to result in jerky rendering of frames, a degradation which can be tolerated by users. Current video tools, such as vic <ref> [3] </ref>, nv [13], and ivs [4], display frames as soon as they are received and decoded. 3 Synchronisation of Audio and Video in a Packet Network The difference between the end-to-end delay of audio and that of video often varies substantially, because of different processing and hardware manipulation times. <p> Inter-media communication is needed in order to transfer the desired playout delays between the audio and video applications. Negotiation can be facilitated by using a local conference bus such as the Conference Control Channel Protocol developed at UCL [17] or the LBL conference bus <ref> [3] </ref>. Both these architectures em ploy multicast loopback facilities to provide communication. <p> This example was chosen as the worst-case scenario, since current video coding algorithms encode the changes between one frame and the next. The decode and render times over 500 frames were collected using vic <ref> [3] </ref> on a lightly loaded SUN Sparc 10 workstation, where H.261 was used as the video encoding algorithm, and CIF sized frames were transmitted. The frames extend over a period of 50 seconds, and contain movement over the whole period.
Reference: [4] <author> Thierry Turletti. </author> <title> The INRIA Videoconferencing System (IVS). </title> <journal> ConneXions The Interoperability Report, </journal> <volume> 8(10) </volume> <pages> 20-24, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The Mbone is a multicast overlay over the Internet which provides multiway communication. Existing Mbone audio tools - such as vat [1], nevot [2] and existing Mbone video tools such as vic [3], ivs <ref> [4] </ref> do not provide support for lip synchronisation. A new audio tool, RAT [5, 6], developed at UCL, together with a modified version of vic [3] provides the first implementation of lip synchronisation over the Mbone. <p> The effect of jitter on video is to result in jerky rendering of frames, a degradation which can be tolerated by users. Current video tools, such as vic [3], nv [13], and ivs <ref> [4] </ref>, display frames as soon as they are received and decoded. 3 Synchronisation of Audio and Video in a Packet Network The difference between the end-to-end delay of audio and that of video often varies substantially, because of different processing and hardware manipulation times.
Reference: [5] <author> Vicky Hardman, Isidor Kouvelas, Martina Angela Sasse, and Anna Watson. </author> <title> Robust-audio over the internet: Analysis and implementation. </title> <note> Research Note RN/96/8, </note> <institution> Dept. of Computer Science, University College London, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: The Mbone is a multicast overlay over the Internet which provides multiway communication. Existing Mbone audio tools - such as vat [1], nevot [2] and existing Mbone video tools such as vic [3], ivs [4] do not provide support for lip synchronisation. A new audio tool, RAT <ref> [5, 6] </ref>, developed at UCL, together with a modified version of vic [3] provides the first implementation of lip synchronisation over the Mbone. RAT and vic use the proposed IETF standard real-time protocol, RTP [7], to provide the necessary functionality required for multicast voice and video communication. <p> Jitter must be removed from audio packet streams, since it renders the speech unintelligible. A voice reconstruction buffer is used to artificially add delay to the audio stream to smooth out the jitter (figure 1). The mechanism is adaptive, since jitter on the Mbone can vary significantly <ref> [5] </ref>. The artificially added delay in the reconstruction buffer is cal culated from the time that packets take to traverse the network [10, 11, 12]. <p> In order to reduce the amount of processing time required, a novel architecture was used, similar to that developed for RAT <ref> [5] </ref>. The design positions the reconstruction buffer before video decoding has taken place, which eliminates the need for the two extra copies of each video frame. The configuration can be seen in figure 5. <p> The header contains a time-stamp that is used to smooth out the effects of network jitter. The time-stamp is media specific and different reference clocks are used for audio and video. Audio uses its own device interface as a clock, since it is available <ref> [5] </ref>. The format of the time-stamps used for the video stream depends on the type of compression used. The time-stamp used with H.261 encoding [20] is based on a 90kHz clock [19].
Reference: [6] <author> Vicky Hardman and Isidor Kouvelas. </author> <title> RAT general architecture. Internal Note IN/96/2, </title> <institution> Dept. of Computer Science, University College London, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: The Mbone is a multicast overlay over the Internet which provides multiway communication. Existing Mbone audio tools - such as vat [1], nevot [2] and existing Mbone video tools such as vic [3], ivs [4] do not provide support for lip synchronisation. A new audio tool, RAT <ref> [5, 6] </ref>, developed at UCL, together with a modified version of vic [3] provides the first implementation of lip synchronisation over the Mbone. RAT and vic use the proposed IETF standard real-time protocol, RTP [7], to provide the necessary functionality required for multicast voice and video communication.
Reference: [7] <author> Henning Schulzrinne, Stephen Casner, Ron Frederick, and Van Jacobson. </author> <title> A transport protocol for real-time applications. Request for comments RFC 1889, </title> <institution> Internet Engineering Task Force, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: A new audio tool, RAT [5, 6], developed at UCL, together with a modified version of vic [3] provides the first implementation of lip synchronisation over the Mbone. RAT and vic use the proposed IETF standard real-time protocol, RTP <ref> [7] </ref>, to provide the necessary functionality required for multicast voice and video communication. The lip synchronisation mechanism developed for RAT uses the time-stamp information from RTP to identify the required artificial time-lag. This paper describes a lip synchronisation technique, which includes inter-system communication. <p> Consequently, audio tools aim to receive 99.9% of the packets, since the delay characteristic shows a long tail for packets that are abnormally delayed <ref> [2, 7] </ref>. The effect of jitter on video is to result in jerky rendering of frames, a degradation which can be tolerated by users. <p> The delay associated with the transmission of the frame across the network and the extra delay needed to cope with network jitter is known if a video reconstruction buffer is used in a similar way to that recommended for audio, RTP <ref> [7] </ref>. The time to decode and render a frame is likely to be variable, but can easily be allowed for if the buffering to smooth network jitter is accomplished after decoding. <p> Playout delay negotiation between the audio and video processes needs to be in a common format, and with a common reference clock. RTCP control messages (figure 7 are transmitted alongside each data stream, and provide individual mapping between the media and Network Time Protocol (NTP) time-stamps <ref> [7, 22] </ref>. The NTP time-stamps therefore give a common reference to the transmitter workstation clock, which can be used to provide inter-stream synchronisation. Figure 8 shows the exchange of RTP and RTCP packets across the network and synchronisation information over a local conference bus, between multiple conference participants.
Reference: [8] <author> Vicky Hardman, Martina Angela Sasse, Mark Handley, and Anna Watson. </author> <title> Reliable audio for use over the internet. </title> <booktitle> In International Networking Conference (INET), </booktitle> <year> 1995. </year>
Reference-contexts: project ReLaTe (Remote Language Teaching over SuperJANET, a BT/JISC funded project), project MERCI (Multimedia European Research Conferencing Integration, European Union Telematics Applications Pro-gramme #1007), and project RAT (Robust-Audio Tool, an EPSRC funded project #GR/K72780). 2 Background Good quality packet audio systems must provide packet loss protection and low end-to-end delay <ref> [8] </ref>. The requirement of low delay means that a packet audio system must launch small packets frequently, rather than big packets less often [8]. Typical packet sizes used for audio over the Internet vary from 20 to 80ms in length. <p> #1007), and project RAT (Robust-Audio Tool, an EPSRC funded project #GR/K72780). 2 Background Good quality packet audio systems must provide packet loss protection and low end-to-end delay <ref> [8] </ref>. The requirement of low delay means that a packet audio system must launch small packets frequently, rather than big packets less often [8]. Typical packet sizes used for audio over the Internet vary from 20 to 80ms in length.
Reference: [9] <author> Stephen Casner. </author> <title> Are you on the MBone? IEEE MultiMedia, </title> <booktitle> Summer 94, </booktitle> <pages> pages 76-79, 94. </pages>
Reference-contexts: The processing time associated with video encoding is also often great, which results in a large delay between video grabbing and rendering. Video frames are usually split into a number of packets for transmission. The Mbone is a shared packet network <ref> [9] </ref>. Routers operate on a first-in first-out basis and statistically multiplex traffic from different sources. The impact of this behavior on real-time traffic is to introduce jitter to the inter-packet timing relationship, a second order effect. Jitter must be removed from audio packet streams, since it renders the speech unintelligible.
Reference: [10] <author> Warren A. Montgomery. </author> <title> Techniques for packet voice synchronization. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> SAC-1(6):1022-1028, </volume> <month> December </month> <year> 1983. </year>
Reference-contexts: The mechanism is adaptive, since jitter on the Mbone can vary significantly [5]. The artificially added delay in the reconstruction buffer is cal culated from the time that packets take to traverse the network <ref> [10, 11, 12] </ref>. A tradeoff exists between the amount of delay that is introduced in the buffer and the percentage of late packets that are received in time [10]. <p> The artificially added delay in the reconstruction buffer is cal culated from the time that packets take to traverse the network [10, 11, 12]. A tradeoff exists between the amount of delay that is introduced in the buffer and the percentage of late packets that are received in time <ref> [10] </ref>. Consequently, audio tools aim to receive 99.9% of the packets, since the delay characteristic shows a long tail for packets that are abnormally delayed [2, 7]. The effect of jitter on video is to result in jerky rendering of frames, a degradation which can be tolerated by users.
Reference: [11] <author> Ramachandran Ramjee, Jim Kurose, Don Towsley, and Henning Schulzrinne. </author> <title> Adaptive playout mechanisms for packetized audio applications in wide-area networks. </title> <booktitle> In Conference on Computer Communications (IEEE Infocom), </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The mechanism is adaptive, since jitter on the Mbone can vary significantly [5]. The artificially added delay in the reconstruction buffer is cal culated from the time that packets take to traverse the network <ref> [10, 11, 12] </ref>. A tradeoff exists between the amount of delay that is introduced in the buffer and the percentage of late packets that are received in time [10].
Reference: [12] <author> Van Jacobson. </author> <title> Multimedia conferencing on the internet, August 1994. </title> <booktitle> SIGCOMM '94 Tutorial. </booktitle>
Reference-contexts: The mechanism is adaptive, since jitter on the Mbone can vary significantly [5]. The artificially added delay in the reconstruction buffer is cal culated from the time that packets take to traverse the network <ref> [10, 11, 12] </ref>. A tradeoff exists between the amount of delay that is introduced in the buffer and the percentage of late packets that are received in time [10].
Reference: [13] <author> Ron Frederick. </author> <title> Experiences with real-time software video compression. </title> <booktitle> In Sixth International Workshop on Packet Video, </booktitle> <address> Portland, Oregon, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: The effect of jitter on video is to result in jerky rendering of frames, a degradation which can be tolerated by users. Current video tools, such as vic [3], nv <ref> [13] </ref>, and ivs [4], display frames as soon as they are received and decoded. 3 Synchronisation of Audio and Video in a Packet Network The difference between the end-to-end delay of audio and that of video often varies substantially, because of different processing and hardware manipulation times.
Reference: [14] <author> Paul W. Jardetzky, Cormac J. Sreenan, and Roger M. Need-ham. </author> <title> Storage and synchronization for distributed continuous media. </title> <journal> Mulimedia Systems, </journal> <volume> 3(3) </volume> <pages> 151-161, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: Consequently, in order to provide lip synchronisation, one or other of the media streams must be delayed at the receiver. Subjective studies have shown that the two streams do not have to be exactly matched, but that a skew of 80-100ms is below the limit of human perception <ref> [14, 15, 16] </ref>. The provision of synchronisation delay for the audio stream can be achieved simply by increasing that provided for voice reconstruction (see above). In contrast, video tools do not usually have a reconstruction buffer. <p> The information can be used to correctly schedule the presentation time of each frame. It is expected that the 20ms jitter associated with the frame presentation time will produce skew that is still well within the limits of human perception <ref> [14, 15, 16] </ref>. 5.2 Subjective Performance Results The subjective performance results show perceived level of syn-chronisation for different frame rates, and for both synchronised and unsynchronised audio and video. The material consisted of audio and video transmission of a speaker counting.
Reference: [15] <author> J. Escobar, D. Deutsch, and C. Partridge. </author> <title> A multi-service flow synchronisation protocol. </title> <type> BBN STC Tech Report, </type> <month> March </month> <year> 1991. </year>
Reference-contexts: Consequently, in order to provide lip synchronisation, one or other of the media streams must be delayed at the receiver. Subjective studies have shown that the two streams do not have to be exactly matched, but that a skew of 80-100ms is below the limit of human perception <ref> [14, 15, 16] </ref>. The provision of synchronisation delay for the audio stream can be achieved simply by increasing that provided for voice reconstruction (see above). In contrast, video tools do not usually have a reconstruction buffer. <p> The information can be used to correctly schedule the presentation time of each frame. It is expected that the 20ms jitter associated with the frame presentation time will produce skew that is still well within the limits of human perception <ref> [14, 15, 16] </ref>. 5.2 Subjective Performance Results The subjective performance results show perceived level of syn-chronisation for different frame rates, and for both synchronised and unsynchronised audio and video. The material consisted of audio and video transmission of a speaker counting.
Reference: [16] <author> L. Lamont, Lian Li, </author> <title> and N.D. Georganas. Synchronization of multimedia data for a multimedia news-on-demand application. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <month> January </month> <year> 1996. </year>
Reference-contexts: Consequently, in order to provide lip synchronisation, one or other of the media streams must be delayed at the receiver. Subjective studies have shown that the two streams do not have to be exactly matched, but that a skew of 80-100ms is below the limit of human perception <ref> [14, 15, 16] </ref>. The provision of synchronisation delay for the audio stream can be achieved simply by increasing that provided for voice reconstruction (see above). In contrast, video tools do not usually have a reconstruction buffer. <p> The information can be used to correctly schedule the presentation time of each frame. It is expected that the 20ms jitter associated with the frame presentation time will produce skew that is still well within the limits of human perception <ref> [14, 15, 16] </ref>. 5.2 Subjective Performance Results The subjective performance results show perceived level of syn-chronisation for different frame rates, and for both synchronised and unsynchronised audio and video. The material consisted of audio and video transmission of a speaker counting.
Reference: [17] <author> Mark Handley, Ian Wakeman, and Jon Crowcroft. </author> <title> The conference control protocol (CCCP): a scalable base for building conference control applications. </title> <booktitle> In SIGCOMM, </booktitle> <pages> pages 275-287, </pages> <address> Cambridge, Massachusetts, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: Inter-media communication is needed in order to transfer the desired playout delays between the audio and video applications. Negotiation can be facilitated by using a local conference bus such as the Conference Control Channel Protocol developed at UCL <ref> [17] </ref> or the LBL conference bus [3]. Both these architectures em ploy multicast loopback facilities to provide communication.
Reference: [18] <author> Isidor Kouvelas and Vicky Hardman. </author> <title> Overcoming workstation scheduling problems in a real-time audio tool. </title> <booktitle> In Usenix Annual Technical Conference, </booktitle> <address> Anaheim, California, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: The audio device buffer (in the device driver) adds an extra delay component to this figure, but this is also known <ref> [18] </ref>. 4.2 Video End-to-end Delay Measurement The end-to-end delay in the video stream is the delay from the instant the camera scans a frame, to the time it appears on the remote screen.
Reference: [19] <author> Thierry Turletti and Christian Huitema. </author> <title> RTP payload format for H.261 video streams. Internet draft (work-in-progress) draft-ietf-avt-h261-01.txt, </title> <month> July </month> <year> 1995. </year>
Reference-contexts: After decoding and rendering, frames can be placed in a play-out buffer and displayed at a pre-calculated playout point. Figure 4 shows the sequence of operations in this arrangement. Unfortunately due to the way in which H.261 <ref> [19] </ref> video frames are decoded, the design involves making two extra copies of a rendered video frame, which is a very significant overhead in terms of processing power. stage. <p> Audio uses its own device interface as a clock, since it is available [5]. The format of the time-stamps used for the video stream depends on the type of compression used. The time-stamp used with H.261 encoding [20] is based on a 90kHz clock <ref> [19] </ref>. Playout delay negotiation between the audio and video processes needs to be in a common format, and with a common reference clock. RTCP control messages (figure 7 are transmitted alongside each data stream, and provide individual mapping between the media and Network Time Protocol (NTP) time-stamps [7, 22].
Reference: [20] <institution> Video codec for audiovisual services at p x 64 kbit/s. ITU-T Recomendation H.261, International Telecommunication Union, </institution> <year> 1993. </year>
Reference-contexts: The configuration can be seen in figure 5. The design assumes that the decoding and rendering of frames varies only slowly with respect to time. Most video encoding algorithms encode the differences between video frames <ref> [20, 21] </ref>, which means that frames from a period containing significant change in the image will take longer to encode and decode. <p> Audio uses its own device interface as a clock, since it is available [5]. The format of the time-stamps used for the video stream depends on the type of compression used. The time-stamp used with H.261 encoding <ref> [20] </ref> is based on a 90kHz clock [19]. Playout delay negotiation between the audio and video processes needs to be in a common format, and with a common reference clock.
Reference: [21] <author> ISO/IEC JTC1/SC2/WG11. MPEG. ISO, </author> <month> September </month> <year> 1990. </year>
Reference-contexts: The configuration can be seen in figure 5. The design assumes that the decoding and rendering of frames varies only slowly with respect to time. Most video encoding algorithms encode the differences between video frames <ref> [20, 21] </ref>, which means that frames from a period containing significant change in the image will take longer to encode and decode.
Reference: [22] <author> David L. Mills. </author> <title> Network time protocol (version 3) specification, implementation and analysis. Request for comments RFC 1305, </title> <institution> Network Working Group, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Playout delay negotiation between the audio and video processes needs to be in a common format, and with a common reference clock. RTCP control messages (figure 7 are transmitted alongside each data stream, and provide individual mapping between the media and Network Time Protocol (NTP) time-stamps <ref> [7, 22] </ref>. The NTP time-stamps therefore give a common reference to the transmitter workstation clock, which can be used to provide inter-stream synchronisation. Figure 8 shows the exchange of RTP and RTCP packets across the network and synchronisation information over a local conference bus, between multiple conference participants.
Reference: [23] <author> Newton Faller. </author> <title> Measuring the latency time of real-time Unix-like operating systems. </title> <type> Technical Report TR-92-037, </type> <institution> International Computer Science Institute, Berkeley, California, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Such traditional time-sharing operating systems do not provide adequate support for real-time applications, such as audio and video <ref> [23] </ref>. Several attempts have been made to modify the operating system schedulers by increasing the number of preemption points in order to provide bounded dispatch latency for applications [24, 25, 26, 27].
Reference: [24] <author> Olof Hagsand and Peter Sjodin. </author> <title> Workstation support for real-time multimedia communication. </title> <booktitle> In Usenix Winter Technical Conference, </booktitle> <address> San Francisco, California, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Such traditional time-sharing operating systems do not provide adequate support for real-time applications, such as audio and video [23]. Several attempts have been made to modify the operating system schedulers by increasing the number of preemption points in order to provide bounded dispatch latency for applications <ref> [24, 25, 26, 27] </ref>. However, as dynamic real-time scheduling is NP hard [28], and these improvements are not available on most Internet hosts, the UNIX round-robin scheduling approach with limited preemption is likely to persist.
Reference: [25] <author> Sandeep Khanna, Michael Sebree, and John Zolnowsky. </author> <title> Re-altime scheduling in sunos 5.0. </title> <booktitle> In Usenix Winter Technical Conference, </booktitle> <year> 1992. </year>
Reference-contexts: Such traditional time-sharing operating systems do not provide adequate support for real-time applications, such as audio and video [23]. Several attempts have been made to modify the operating system schedulers by increasing the number of preemption points in order to provide bounded dispatch latency for applications <ref> [24, 25, 26, 27] </ref>. However, as dynamic real-time scheduling is NP hard [28], and these improvements are not available on most Internet hosts, the UNIX round-robin scheduling approach with limited preemption is likely to persist.
Reference: [26] <author> Sape J. Mullender, Ian M. Leslie, and Derek McAuley. </author> <title> Operating-system support for distributed multimedia. </title> <booktitle> In Usenix Summer Technical Conference, </booktitle> <address> Boston, Mas-sachusetts, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Such traditional time-sharing operating systems do not provide adequate support for real-time applications, such as audio and video [23]. Several attempts have been made to modify the operating system schedulers by increasing the number of preemption points in order to provide bounded dispatch latency for applications <ref> [24, 25, 26, 27] </ref>. However, as dynamic real-time scheduling is NP hard [28], and these improvements are not available on most Internet hosts, the UNIX round-robin scheduling approach with limited preemption is likely to persist.
Reference: [27] <author> Tom Fisher. </author> <title> Real-time scheduling support in Ultrix-4.2 for multimedia communications. </title> <booktitle> In Third International Workshop on network and operating system support for digital audio and video, </booktitle> <pages> pages 282-288, </pages> <address> San Diego, California, </address> <month> November </month> <year> 1992. </year> <journal> IEEE Communications Society. </journal>
Reference-contexts: Such traditional time-sharing operating systems do not provide adequate support for real-time applications, such as audio and video [23]. Several attempts have been made to modify the operating system schedulers by increasing the number of preemption points in order to provide bounded dispatch latency for applications <ref> [24, 25, 26, 27] </ref>. However, as dynamic real-time scheduling is NP hard [28], and these improvements are not available on most Internet hosts, the UNIX round-robin scheduling approach with limited preemption is likely to persist.
Reference: [28] <author> John A. Stankovic, Marco Spurl, Marco Di Natale, and Gior-gio C. Butiazzo. </author> <title> Implications of classical scheduling results for real-time systems. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 16-25, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Several attempts have been made to modify the operating system schedulers by increasing the number of preemption points in order to provide bounded dispatch latency for applications [24, 25, 26, 27]. However, as dynamic real-time scheduling is NP hard <ref> [28] </ref>, and these improvements are not available on most Internet hosts, the UNIX round-robin scheduling approach with limited preemption is likely to persist. Without real-time support, event timings on the receiving workstation become less accurate as the load on the receiving workstation increases [29].
Reference: [29] <author> K. Fall, J. Pasquale, and S. McCanne. </author> <title> Workstation video playback performance with competitive process load. In International Workshop on Network and Operating System Support for for Digital Audio and Video (NOSSDAV), </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <pages> pages 179-182, </pages> <address> Durham, New Hampshire, April 1995. </address> <publisher> Springer. </publisher>
Reference-contexts: Without real-time support, event timings on the receiving workstation become less accurate as the load on the receiving workstation increases <ref> [29] </ref>. The lack of real-time support in a host platform therefore can be expected to have a profound impact on the success of a lip synchronisation mechanism. 5 Implementation Assessment Results The main results in this paper assess the subjective performance of the lip synchronisation implementation.
References-found: 29

