URL: ftp://ftp.cs.man.ac.uk/pub/ai/jls/subset.ps.Z
Refering-URL: http://www.cs.man.ac.uk/~magnus/magnus.html
Root-URL: http://www.cs.man.ac.uk
Title: The Dynamics of a Genetic Algorithm under Stabilizing Selection  
Author: L. Magnus Rattray 
Date: September 12, 1995  
Address: Manchester, Oxford Road Manchester M13 9PL, U.K.  
Affiliation: Computer Science Department, University of  
Abstract: A formalism recently introduced by Prugel-Bennett and Shapiro uses the methods of statistical mechanics to model the dynamics of genetic algorithms. To be of more general interest this formalism must be able to describe problems other than the test cases they consider. In this paper, the technique is applied to the subset sum problem, which is a combinatorial optimization problem with a strongly non-linear energy (fitness) function and many local minima under single spin flip dynamics. It is a problem which exhibits an interesting dynamics, reminiscent of stabilizing selection in population biology. The dynamics are solved under certain simplifying assumptions and are reduced to a set of difference equations for a small number of relevant quantities. The quantities used are the population's cumulants, which describe its shape, and the mean correlation within the population, which measures the microscopic similarity of population members. Including the mean correlation allows a better description of the population than the cumulants alone would provide and represents a new and important extension of the technique. The formalism includes finite population effects and describes problems of realistic size. The theory is shown to agree closely to simulations of a real genetic algorithm and the mean best energy is accurately predicted. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. H. </author> <booktitle> Holland "Adaptation in Natural and Artificial Systems," </booktitle> <address> (Ann Arbor, </address> <publisher> The University of Michigan Press, </publisher> <year> 1975). </year>
Reference-contexts: 1 Introduction Genetic Algorithms (GAs) are general-purpose search techniques which are loosely based on natural selection <ref> [1, 2] </ref>. A population of solutions evolve under the influence of genetic operators, which are roughly analogous to the processes at work in biological populations. GAs are growing in popularity and are being used in a large variety of problem domains (see [3] for examples). <p> Let the set of possibilities be fw 1 ; w 2 : : : ; w N g, chosen from some arbitrary distribution. In this paper, the w i come from a uniform distribution of reals over the interval <ref> [0; 1] </ref>, although the theoretical results are valid for any distribution with a well defined variance in the large N limit. <p> This is as expected, because the population is moving into a region of lower entropy. The expressions can be averaged numerically over the Gaussian noise and the weights, which are distributed uniformly over the interval <ref> [0; 1] </ref>.
Reference: [2] <author> D. E. Goldberg, </author> <title> "Genetic Algorithms in Search, Optimization and Machine Learning," </title> <publisher> (Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989). </year>
Reference-contexts: 1 Introduction Genetic Algorithms (GAs) are general-purpose search techniques which are loosely based on natural selection <ref> [1, 2] </ref>. A population of solutions evolve under the influence of genetic operators, which are roughly analogous to the processes at work in biological populations. GAs are growing in popularity and are being used in a large variety of problem domains (see [3] for examples). <p> This requires an increased selection strength as the GA converges, a method recommended in the GA literature to avoid premature convergence to non-optimal solutions <ref> [2] </ref>.
Reference: [3] <author> L. </author> <title> Davis (editor) "Handbook Of Genetic Algorithms," </title> <publisher> (Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1991). </year>
Reference-contexts: A population of solutions evolve under the influence of genetic operators, which are roughly analogous to the processes at work in biological populations. GAs are growing in popularity and are being used in a large variety of problem domains (see <ref> [3] </ref> for examples). It is the existence of a population of solutions being processed in parallel which makes the GA different from other stochastic search techniques, such as simulated annealing.
Reference: [4] <author> A. Prugel-Bennett, J. L. </author> <title> Shapiro "An Analysis of Genetic Algorithms Using Statistical Mechanics," </title> <journal> Physical Review Letters, </journal> <volume> 72(9) 1305 (1994). </volume>
Reference-contexts: Although there is a large body of theoretical work on GAs, the established theory does not yet provide a complete picture. Prugel-Bennett and Shapiro (PBS) have introduced a formalism for modeling the dynamics of the GA, using methods from statistical mechanics <ref> [4, 5, 6] </ref>. They considered two closely related toy problems; the random field paramagnet and the Ising spin chain, for which the dynamics can be solved exactly [5]. In this paper their formalism is generalized to a harder combinatorial optimization problem, subset sum. <p> where each population member is identical, Q = Q max where, Q max = N i=1 i (9) 4 Selection Roulette wheel selection is used, where the new population is chosen from the old with replacement and the probability of choosing a population member is equal to it's Boltzmann weight <ref> [10, 4] </ref>. This form of fitness scaling keeps the distribution very close to a Gaussian distribution and since the other genetic operators tend to return the population to a Gaussian it is a natural choice for this problem. <p> If energy is a function of the field value, h, then one can generalize the calculation due to PBS <ref> [4] </ref> to calculate the effect of selection on the field distribution's cumulants. <p> This function is not a well defined probability distribution since it is not necessarily positive, but it does have the correct cumulants. B Selection calculation Following PBS <ref> [4] </ref>, one can express the logarithm of the partition function as an integral, hlog Z s i p (h) = Z 1 e t he tZ s i p (h) dt (B.1) Now the average on the right can be taken, he tZ s i p (h) = ff=1 1 !
Reference: [5] <author> A. Prugel-Bennett, J. L. Shapiro, </author> <title> "The Dynamics of a Genetic Algorithm for Simple Random Ising Systems," </title> <institution> Computer Science Dept., University of Manchester, </institution> <address> Oxford Road, Manchester M13 9PL, U.K. </address> <note> (1995) (submitted to Physica D for publication </note> ). 
Reference-contexts: Although there is a large body of theoretical work on GAs, the established theory does not yet provide a complete picture. Prugel-Bennett and Shapiro (PBS) have introduced a formalism for modeling the dynamics of the GA, using methods from statistical mechanics <ref> [4, 5, 6] </ref>. They considered two closely related toy problems; the random field paramagnet and the Ising spin chain, for which the dynamics can be solved exactly [5]. In this paper their formalism is generalized to a harder combinatorial optimization problem, subset sum. <p> Prugel-Bennett and Shapiro (PBS) have introduced a formalism for modeling the dynamics of the GA, using methods from statistical mechanics [4, 5, 6]. They considered two closely related toy problems; the random field paramagnet and the Ising spin chain, for which the dynamics can be solved exactly <ref> [5] </ref>. In this paper their formalism is generalized to a harder combinatorial optimization problem, subset sum. Although strictly np-complete, this is a problem which is quasi-polynomial and in most cases can be solved with traditional algorithms in polynomial time [7]. <p> A full description of the GA's dynamics is very difficult. The number of possible population realizations is astronomical for a typical problem and crossover introduces a strongly non-linear interaction within the population. Following the formalism due to PBS <ref> [5] </ref>, the population is modeled by a small number of macroscopic quantities and anything not trivially related to these quantities is retrieved through a maximum entropy method. This reduces the dynamics to a small number of difference equations describing the average effect of each operator on each relevant quantity. <p> The optimal solution lies in a dense region of solution space, so that the population seems to stabilize around it. This form of selective pressure is known to population biologists as stabilizing selection and is very different from the problems considered by PBS <ref> [5] </ref>, where selection is directional and pushes the population towards the optimal solution, which lies in a sparse region of the solution space. <p> Because of the non-linearity within the energy, the relationship between the string coding and energy is more complex than in the paramagnet and spin chain considered by PBS <ref> [5] </ref>. <p> Notice that the field distribution is always close to a Gaussian, while the energy distribution would clearly become unsymmetrical as the population evolves. In choosing to model the field distribution, one avoids the problem of dealing with a strongly non-Gaussian distribution. 3 The formalism Following PBS <ref> [5] </ref>, the dynamics are described in terms of each operator's average effect on the distribution's cumulants, which describe the population's shape, and the results for each operator are presented in the following three sections. <p> The third and fourth cumulant expressions for crossover and mutation are not presented here, as they are equivalent to the paramagnet result in Ref. <ref> [5] </ref> under a simple change in variables. Unlike the paramagnet or the spin chain, for this problem the mean correlation within the population cannot be deduced from the population's variance and must be treated independently. <p> Unlike the paramagnet or the spin chain, for this problem the mean correlation within the population cannot be deduced from the population's variance and must be treated independently. The correlation expressions are derived in section 7 and represent an important new contribution to the formalism introduced by PBS <ref> [5] </ref>. <p> selection still increases the magnitude of the negative fourth cumulant, related to the kurtosis, which increases the energy (reduces the fitness) of the best population member on average (see appendix D). 5 Crossover The crossover and mutation calculations for the field distribution are equivalent to the paramagnet considered by PBS <ref> [5] </ref> under a simple change of variables, so only an outline of the 6 derivation and results for the first two cumulants will be presented here. Consider two popula- tion members, ff and fi. <p> The higher cumulants are reduced towards their natural value, with the constraint that the allele frequency at each site remains fixed within the population. For the higher cumulants a maximum entropy ansatz is used to calculate any terms not trivially related to the population's cumulants or mean correlation <ref> [5] </ref>. <p> The higher cumulants are also reduced in magnitude <ref> [5] </ref>. The mutation probability, p m , determines the rate at which the population approaches a random population. Sometimes the mutation rate is annealed while the GA evolves, since a high mutation rate seems to be most beneficial at the beginning of the search. <p> This is because crossover involves the interaction of population members, unlike the other two operators. In the paramagnet and spin chain, considered by PBS <ref> [5] </ref>, the mean correlation can be deduced from the population's variance, but this is not possible in general. In order to deal with harder problems it will be necessary to evolve the mean correlation explicitly. <p> This is fairly natural for crossover and mutation, but for selection one must make some assumption about the relationship between a population member's energy and it's microstate the bit string coding. To deal with this issue a maximum entropy method is used, extending the ansatz presented by PBS <ref> [5] </ref>. Crossover does not change the mean correlation, since the allele frequency at each site within the population remains fixed. <p> During the convergent stage of the dynamics, when 1 = G, the increase in correlation under selection is solely due to duplication. This calculation is an important extension of the formalism presented by PBS <ref> [5] </ref>, since it might allow the method to be extended to less trivial problems. The correlation within the population provides information which cannot be derived from the statistics of the population's energies (or, in this case, field values). <p> The theory shows good, although not perfect, agreement to simulations of a real GA averaged over many runs. In this problem, as in the spin chain and paramagnet considered by PBS <ref> [5] </ref>, the higher cumulants are shown to increase convergence as they increase the accumulation of correlations under selection. During the later stages, the increase in correlation is shown to be almost solely due to the duplication required to maintain the population size after selection. <p> and for a continuous distribution this definition can be generalized using a Fourier transform, ^ae (t) = 1 whose logarithm is the generating function for the cumulants, log ^ae (t) = n=0 (it) n (A.2) One can define a function whose moments are correct given a limited number of cumulants <ref> [5] </ref>, p (h) = p exp ( 2 2 n c X n n=2 u n ( p ) (A.3) where u n (x) are normalized Hermite polynomials and n c is the number of cumulants used. <p> For small fi one can expand f (t; fi; fl), f (t; fi; fl) ' 1 t^ae (1; fi; fl) + 2 where, ^ae (n; fi; fl) = 1 Following PBS <ref> [5] </ref> one can exponentiate this expansion and raise to the power of P , f P (t; fi; fl) = e tP ^ae (1;fi;fl) 1 + 2 ! Completing the integral in equation (B.4) one finds, n = lim @ n 1 ( ^ae 2 (1; fi; fl) This can be <p> C Maximum entropy calculation To calculate terms required for the determination of the higher cumulants after crossover and mutation, PBS introduce a maximum entropy calculation <ref> [5] </ref>. In this work this calculation will also be required in order to estimate the increased correlation within the population under selection.
Reference: [6] <author> J. L. Shapiro, A. Prugel-Bennett, L. M. Rattray, </author> <title> "A Statistical Mechanical Formulation of the Dynamics of Genetic Algorithms," </title> <booktitle> Lecture Notes in Computer Science, 865, special edition on Evolutionary Computing edited by T. </booktitle> <editor> C. Fogarty, </editor> <year> (1994). </year>
Reference-contexts: Although there is a large body of theoretical work on GAs, the established theory does not yet provide a complete picture. Prugel-Bennett and Shapiro (PBS) have introduced a formalism for modeling the dynamics of the GA, using methods from statistical mechanics <ref> [4, 5, 6] </ref>. They considered two closely related toy problems; the random field paramagnet and the Ising spin chain, for which the dynamics can be solved exactly [5]. In this paper their formalism is generalized to a harder combinatorial optimization problem, subset sum.
Reference: [7] <author> M. R. Garey, D. S. Johnson, </author> <title> "Computers and Intractability A Guide to the Theory of NP-Completeness," </title> <editor> (W. H. </editor> <publisher> Freeman and Co., </publisher> <address> San Francisco, </address> <year> 1979). </year>
Reference-contexts: In this paper their formalism is generalized to a harder combinatorial optimization problem, subset sum. Although strictly np-complete, this is a problem which is quasi-polynomial and in most cases can be solved with traditional algorithms in polynomial time <ref> [7] </ref>. This is still a toy problem, since stochastic methods will not perform as fl Internet address: rattraym@cs.man.ac.uk. 1 well as these algorithms, but there are closely related strong np-complete problems, such as bin-packing, to which GAs have been applied effectively [8]. <p> of freedom is required; the mean correlation within the population, which is a measure of the similarity between population members. 2 The algorithm 2.1 Subset sum Posed as a question, the subset sum problem asks whether a set of numbers has a subset which exactly sums to a goal value <ref> [7] </ref>. Posed as an optimization problem, one wishes to find the subset which comes as close as possible to the goal value. Let the set of possibilities be fw 1 ; w 2 : : : ; w N g, chosen from some arbitrary distribution.
Reference: [8] <author> B. Kroger, O. Vornberger, </author> <title> "Enumerative vs Genetic Optimization Two Parallel Algorithms for the Bin Packing Problem," </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 594, </volume> <month> 330-362 </month> <year> (1992). </year>
Reference-contexts: This is still a toy problem, since stochastic methods will not perform as fl Internet address: rattraym@cs.man.ac.uk. 1 well as these algorithms, but there are closely related strong np-complete problems, such as bin-packing, to which GAs have been applied effectively <ref> [8] </ref>. The solution of the dynamics for subset sum may be useful in understanding these harder cases. In the simple GA considered here, solutions to a problem are coded as binary strings and each string is assigned an energy, or negative fitness, through some mapping function. <p> This problem exhibits a very different dynamics to most problems and it might be interesting to look at more realistic problems with this type of dynamics, such as bin packing and knapsack problems which have been solved by GAs in the past <ref> [8] </ref>. 13 By evolving the mean correlation, the maximum entropy ansatz has been extended in a way which may prove important if progress is to be made on harder problems.
Reference: [9] <author> J. E. </author> <title> Baker "Reducing bias and inefficiency in the selection algorithm," </title> <booktitle> in Proceedings of the Second International Conference on Genetic Algorithms pages 14-21, </booktitle> <editor> edited by J. J. Grefenstette, </editor> <publisher> (Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1987). </year>
Reference-contexts: The size of this population, P , remains fixed. Under selection, population members are chosen by some process weighted towards the individuals with the lowest energy. Although Baker (or deterministic) selection is generally thought to be more effective <ref> [9] </ref>, we choose to use roulette wheel selection, since this makes the problem more amenable to analysis, allowing an accurate model for finite population effects. The population is then divided into pairs of parents for crossover.
Reference: [10] <author> M. De la Maza, B. </author> <title> Tidor "Increased Flexibility in Genetic Algorithms: The Use of Variable Boltzmann Selective Pressure to Control Propagation," </title> <booktitle> Proceedings of the ORSA CSTS Conference Computer Science and Operations Research: New Developments in their Interfaces, </booktitle> <month> 425-440 </month> <year> (1991). </year>
Reference-contexts: where each population member is identical, Q = Q max where, Q max = N i=1 i (9) 4 Selection Roulette wheel selection is used, where the new population is chosen from the old with replacement and the probability of choosing a population member is equal to it's Boltzmann weight <ref> [10, 4] </ref>. This form of fitness scaling keeps the distribution very close to a Gaussian distribution and since the other genetic operators tend to return the population to a Gaussian it is a natural choice for this problem.
Reference: [11] <author> J. E. Marsden, </author> <title> "Basic Complex Analysis" (W. </title> <editor> H. </editor> <publisher> Freeman and Co., </publisher> <address> San Francisco., </address> <year> 1973). </year>
Reference-contexts: h ff + k fi h fi + t i=1 i + i=1 log (X 2 i =2 + e k ff w i tw 2 i Since the exponent is O (N ), this integral can be computed for large N by the saddle point method (see, for example, <ref> [11] </ref>), ^ae (itN; h ff ; h fi ) = exp [G (k fl fi (t); t)] (43) where the saddle point equations fix the values of k fl ff and k fl @G (k fl fi (t); t) = 0 (44) @k fi ff (t); k fl Define ^ae (itN
Reference: [12] <author> K. Deb, D. E. Goldberg, </author> <title> "Analyzing Deception in Trap Functions," </title> <booktitle> in Foundations of Genetic Algorithms 2, edited by L. </booktitle> <address> D. </address> <publisher> Whitley (Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993). </year>
Reference-contexts: The selection calculation has been generalized to any energy which can be expressed in terms of a field and this could be useful in examining certain toy problems, such as trap functions, which have received some attention from GA theorists (see, for example, <ref> [12] </ref>). The formalism is still limited to describing average behavior and an accurate model for fluctuations would be an improvement, although in the problems considered under this formalism the average behavior seems to express much of the most interesting behavior.
References-found: 12

