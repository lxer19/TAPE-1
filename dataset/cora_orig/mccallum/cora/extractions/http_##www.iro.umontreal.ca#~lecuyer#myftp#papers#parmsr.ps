URL: http://www.iro.umontreal.ca/~lecuyer/myftp/papers/parmsr.ps
Refering-URL: http://www.iro.umontreal.ca/~lecuyer/papers.html
Root-URL: http://www.iro.umontreal.ca
Title: CENTRAL LIMIT THEOREMS FOR STOCHASTIC OPTIMIZATION ALGORITHMS USING INFINITESIMAL PERTURBATION ANALYSIS  
Author: Qian-Yu Tang, Pierre L'Ecuyer, and Han-Fu Chen 
Keyword: Key words. perturbation analysis, stochastic approximation, recursive estimation, queueing theory, central limit theorems  
Address: Montreal, C.P.6128, Succ. Centre-Ville, Montreal, Quebec H3C 3J7, Canada  100080, People's Republic of China.  
Affiliation: Departement d'IRO, Universite de  Laboratory of Systems and Control, Institute of Systems Science, Academia Sinica, Beijing  
Web: 62L20, 93E12, 93E23, 93E30, 60F05, 60K25  
Note: AMS(MOS) Subject classifications.  Qian-Yu Tang and Pierre L'Ecuyer are with  Han-Fu Chen is with the  
Date: July 8, 1998  
Abstract: Central limit theorems are obtained for the "perturbation analysis Robbins-Monro single run" (PARMSR) optimization algorithm, with updates either after every L customers or after every busy period, in the context of the optimization of a GI=GI=1 queue. The PARMSR algorithm is a stochastic approximation (SA) method for the optimization of infinite-horizon models. It is shown that the convergence rate and the asymptotic variance constant of the optimization algorithm, as a function of the total computing budget (i.e., total number of customers), are the same for both updating methods, and independent of L, provided that the step sizes of SA are chosen in the (asymptotically) optimal way for each method. Resume. On obtient des theoremes de la limite centrale pour la solution de l'algorithme "perturbation analysis Robbins-Monro single run" (PARMSR) pour l'optimisation d'une file d'attente GI=GI=1, avec mise a jour apres chaque L clients ou apres chaque cycle regeneratif. L'algorithme PARMSR est une methode d'approximation stochastique (AS) pour l'optimisation de modeles sur horizon infini. Nos resultats impliquent que le taux de convergence et la constante de variance asymptotique de l'algorithme d'optimisation, en fonction du budget total de calcul (i.e., le nombre total de clients), sont les m^emes pour les deux methodes de mise a jour, et independants de L, pourvu que les longueurs de pas de l'AS soient choisis de maniere optimale pour chaque methode. The Work of Q. Y. Tang and P. L'Ecuyer has been supported by NSERC-Canada grants No. OGP0110050 and SMF0169893, and by FCAR-Quebec grant No. 93ER1654. The work of H. F. Chen was partially supported by the National Natural Science Foundation of China. We thank the two anonymous referees, whose comments helped improving the presentation of the results in paper. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Asmussen, </author> <title> Applied Probability and Queues, </title> <address> New York: </address> <publisher> John Wiley and Sons, </publisher> <year> 1987. </year>
Reference-contexts: We note that weaker hypotheses are imposed in Theorem 4.1 than that in Theorem 2.1. The PARMSR algorithm updated after every BP in this section can in principle be extended to regenerative systems in a wide sense (i.e., Harris-recurrent models; see, e.g., Asmussen <ref> [1] </ref>), if explicit regeneration points can be found before the implementation of the algorithm. We do not pursue this generality in this paper. 5 Conclusion Central limit theorems have been established for the PARMSR algorithms updated either after every L customers or after every BP, for the GI=GI=1 queueing systems. <p> If the controlled parameters are fixed at a constant, a queueing system may enter the steady state at finite time (see, e.g., <ref> [1] </ref>). However, with a changing parameter, the system is no longer stationary. In particular, a regenerative system does not necessarily remain regenerative in the case where the control parameters are changed frequently.
Reference: [2] <author> H. F. </author> <title> Chen,"Asymptotic efficient stochastic approximation,"Stochastics and Stochastics Reports, </title> <address> vol.45, pp.1-16, </address> <year> 1993. </year>
Reference-contexts: Our results show that the step sizes of the algorithms play a major role in their limiting behavior. The idea of averaging the iterates in stochastic approximation (see, e.g., <ref> [2] </ref>, [4], [26], [38]), which is devoted to achieving an optimal convergence rate and optimal variance constant for a broader range of step sizes, can also be combined in a natural way with the PARMSR algorithms. <p> to derive that j' n; k j c 0 expfc n X a j g; (6.3) sup n X a i j' n; i j r &lt; 1; 8 r &gt; 0; (6.4) n!1 i=0 n; i+1 = 0 where c 0 and c are some positive constants (see, e.g., <ref> [2, 4, 24, 38] </ref>).
Reference: [3] <author> H. F. Chen, </author> <title> "Stochastic approximation and its new applications," </title> <booktitle> in Proc. 1994 Hong Kong International Workshop on New Directions in Control and Manufacturing, </booktitle> <address> pp.2-12, </address> <year> 1994. </year>
Reference: [4] <author> H. F. Chen and Y. M. Zhu, </author> <title> Stochastic Approximation, </title> <publisher> Shanghai Scientific and Technological Publishers, </publisher> <year> 1996. </year>
Reference-contexts: Proof. Let " n+1 = f n+1 j ( n )f ( n ). It is seen that f" n ; F k n g is a sequence of martingale differences. The rest the proof is easily derived from (6.49) and is standard (see, e.g., <ref> [4, 17, 24] </ref>). 2 We now express the convergence in terms of the computing budget for n SA iterations, which is a random variable N n = k n = P n i=1 j i ( i1 ). <p> Our results show that the step sizes of the algorithms play a major role in their limiting behavior. The idea of averaging the iterates in stochastic approximation (see, e.g., [2], <ref> [4] </ref>, [26], [38]), which is devoted to achieving an optimal convergence rate and optimal variance constant for a broader range of step sizes, can also be combined in a natural way with the PARMSR algorithms. <p> to derive that j' n; k j c 0 expfc n X a j g; (6.3) sup n X a i j' n; i j r &lt; 1; 8 r &gt; 0; (6.4) n!1 i=0 n; i+1 = 0 where c 0 and c are some positive constants (see, e.g., <ref> [2, 4, 24, 38] </ref>).
Reference: [5] <author> E. K. P. Chong and P. J. Ramadge, </author> <title> "Convergence of recursive optimization algorithms using infinitesimal perturbation analysis estimates," Discrete Event Dynamic Systems: </title> <journal> Theory and Appl., </journal> <volume> vol.1, </volume> <pages> pp. 339-372, </pages> <year> 1992. </year>
Reference-contexts: The PARMSR algorithm updated after every BP is composed of (2.1) and (4.1); see, e.g., <ref> [5] </ref>, [10], [20], and [21]. Theorem 4.1 Suppose that conditions (A1)-(A5) and (A7)-(A8) are satisfied with the condition M 1 +ff=2 &lt; 0 replaced by M 1 j ( 0 )+ff=2 &lt; 0 and the condition on 0 replaced by 0 &gt; 2 (p+ 0 +1).
Reference: [6] <author> E. K. P. Chong and P. J. Ramadge, </author> <title> "Optimization of queues using an infinitesimal perturbation analysis-based stochastic algorithm with general updates times," </title> <journal> SIAM J. Contr. Optim., </journal> <volume> vol.31, </volume> <pages> pp. 698-732, </pages> <year> 1993. </year>
Reference: [7] <author> E. K. P. Chong and P. J. Ramadge, </author> <title> "Stochastic optimization of regenerative systems using infinitesimal perturbation analysis," </title> <journal> IEEE Trans. Automat. Contr., </journal> <volume> vol. AC-39, no.7, </volume> <pages> pp. 1400-1410, </pages> <year> 1994. </year>
Reference: [8] <author> Y. S. Chow, </author> <title> "Local convergence of martingale and the law of large numbers," </title> <journal> Ann. Math. Stat., </journal> <volume> vol.36, </volume> <pages> pp. 552-558, </pages> <year> 1965. </year>
Reference: [9] <author> Y. S. Chow and H. Teicher, </author> <title> Probability Theory : Independence, </title> <address> Interchangeability, Martingale, </address> <year> 1988, </year> <title> Second Edition, </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: k m ) j ( k m ); F k m g is a sequence of martingale differences and that E [(j m+1 ( k m ) j ( k m )) 2 jF k m ] E [j 1 (b) 2 ] &lt; 1: By Burkholder's inequality (see, e.g., <ref> [9] </ref> and [14]), there exists a constant B fl 1 such that E 6 0 o (n) X a k m ' 2 1 2 7 B fl 2 o (n) X a 2 n; k m E [(j m+1 ( k m ) j ( k m )) 2 jF <p> We will verify that the conditions of Theorem 9.5.2 in Chow and Teicher <ref> [9] </ref> hold for n X X n; j . As in the proof of Lemma 3.4 (part (ii)) in [35], it is shown that fw j+1 ; F k j ; j 0g is a sequence of martingale differences. <p> 1: For all " &gt; 0, by (6.3) and (6.28) we then have n X E [X 2 " j=0 a k j E [jw j+1 j 3 jF k j ] ! 0; a:s: (6.29) By (6.25), (6.26) and (6.29), the conditions of Theorem 9.5.2 in Chow and Teicher <ref> [9] </ref> are verified. Thus the proof of (6.24) is completed. 2 Lemma 6.3 Suppose that conditions (A1)-(A8) are satisfied. Then oe (n) X p j m+1 X " k m +i ! N (0; oe 2 Proof. <p> X j mj+1 (b)) e a 2ffi 0 Along the same lines of Lemma 6.1, by (6.62) it can be shown that oe (nL+L) X e a k mL e ' 2 n ! 1 j ( 0 ) 0 23 By (6.66) and Theorem 9.5.2 in Chow and Teicher <ref> [9] </ref>, analogous to Lemma 6.2, it is derived that 1 oe (nL+L) X q j m+1 ( e k m ) i=1 d n ! 1 2 ): (6.67) Similar to Lemma 6.3, by (6.67) we can prove 1 oe (nL+L) X q j m+1 X e " k m +i
Reference: [10] <author> M. C. Fu, </author> <title> "Convergence of a stochastic approximation algorithm for the GI/G/1 queue using infinitesimal perturbation analysis," </title> <journal> J. of Optim. Theory and Appl., </journal> <volume> vol.65, </volume> <pages> pp. 149-160, </pages> <year> 1990. </year>
Reference-contexts: The PARMSR algorithm updated after every BP is composed of (2.1) and (4.1); see, e.g., [5], <ref> [10] </ref>, [20], and [21]. Theorem 4.1 Suppose that conditions (A1)-(A5) and (A7)-(A8) are satisfied with the condition M 1 +ff=2 &lt; 0 replaced by M 1 j ( 0 )+ff=2 &lt; 0 and the condition on 0 replaced by 0 &gt; 2 (p+ 0 +1).
Reference: [11] <author> M. C. Fu and Y. C. Ho, </author> <title> "Using perturbation analysis for gradient estimation, averaging, and updating in a stochastic approximation algorithm," </title> <booktitle> Proc. of the Winter Simulation Conference, </booktitle> <pages> pp. 509-517, </pages> <year> 1988. </year>
Reference: [12] <author> P. Glasserman, </author> <title> Gradient Estimation via Perturbation Analysis, </title> <publisher> Kluwer Academic Pub., </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: The transient effects may also induce a change in the order of events, compared with the case where the control parameters are fixed, because the parameter change is not infinitesimal (in contrast to PA, where the order of events remains the same because the parameter change is infinitesimal; see, e.g., <ref> [12, 16, 19] </ref>). In order to broaden the range of application of the PARMSR algorithm updated after every fixed-length observation period, the above issues need to be properly addressed.
Reference: [13] <author> A. Gut, </author> <title> Stopped Random Walks: Limit Theorems and Applications, </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: 1; ffi 0 2 [0; 1); which gives max ja 1ffi 0 k m j ff fl k m ; 8 ffi 0 2 [0; 1): (6.8) By condition (A7) (ii), we have E j 1 (b) 0 &lt; 1; E L 1 (b) 0 &lt; 1; (6.9) see, e.g., <ref> [13] </ref> and [36].
Reference: [14] <author> P. Hall and C. C. Heyde, </author> <title> Martingale Limit Theory and Its Application. </title> <address> New York: </address> <publisher> Acad. Press, </publisher> <year> 1980. </year>
Reference-contexts: ) j ( k m ); F k m g is a sequence of martingale differences and that E [(j m+1 ( k m ) j ( k m )) 2 jF k m ] E [j 1 (b) 2 ] &lt; 1: By Burkholder's inequality (see, e.g., [9] and <ref> [14] </ref>), there exists a constant B fl 1 such that E 6 0 o (n) X a k m ' 2 1 2 7 B fl 2 o (n) X a 2 n; k m E [(j m+1 ( k m ) j ( k m )) 2 jF k m
Reference: [15] <author> Y. C. Ho and X. R. Cao, </author> <title> "Perturbation analysis and optimization of queueing networks," </title> <journal> J. Optimiz. Theory Appl., vol.40, </journal> <volume> pp.559-582, </volume> <year> 1983. </year>
Reference: [16] <author> Y. C. Ho and X. R. Cao, </author> <title> Perturbation Analysis of Discrete Event Dynamic Systems, </title> <publisher> Kluwer Academic Pub., </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: The transient effects may also induce a change in the order of events, compared with the case where the control parameters are fixed, because the parameter change is not infinitesimal (in contrast to PA, where the order of events remains the same because the parameter change is infinitesimal; see, e.g., <ref> [12, 16, 19] </ref>). In order to broaden the range of application of the PARMSR algorithm updated after every fixed-length observation period, the above issues need to be properly addressed.
Reference: [17] <author> H. J. Kushner and D. S. Clark, </author> <title> Stochastic Approximation Methods for Constrained and Unconstrained Systems. </title> <address> New York: </address> <publisher> Springer, </publisher> <year> 1978. </year>
Reference-contexts: Proof. Let " n+1 = f n+1 j ( n )f ( n ). It is seen that f" n ; F k n g is a sequence of martingale differences. The rest the proof is easily derived from (6.49) and is standard (see, e.g., <ref> [4, 17, 24] </ref>). 2 We now express the convergence in terms of the computing budget for n SA iterations, which is a random variable N n = k n = P n i=1 j i ( i1 ).
Reference: [18] <author> H. J. Kushner and F. J. Vazquez-Abad, </author> <title> "Stochastic approximation methods for systems of interest over an infinite horizon," </title> <journal> SIAM J. on Control and Optim., vol.34, </journal> <volume> pp.712-756, </volume> <year> 1996. </year>
Reference: [19] <author> P. L'Ecuyer, </author> <title> "A Unified View of the IPA, SF, and LR Gradient Estimation Techniques," </title> <journal> Management Science, </journal> <volume> Vol. 36, </volume> <pages> pp. 1364-1383, </pages> <year> 1990. </year>
Reference-contexts: The transient effects may also induce a change in the order of events, compared with the case where the control parameters are fixed, because the parameter change is not infinitesimal (in contrast to PA, where the order of events remains the same because the parameter change is infinitesimal; see, e.g., <ref> [12, 16, 19] </ref>). In order to broaden the range of application of the PARMSR algorithm updated after every fixed-length observation period, the above issues need to be properly addressed.
Reference: [20] <author> P. L'Ecuyer and P. W. Glynn, </author> <title> "Stochastic optimization by simulation: convergence proofs for the GI/G/1 queue in steady-state,"Management Sci., </title> <address> vol.40, pp.1562-1578, </address> <year> 1994. </year>
Reference-contexts: The PARMSR algorithm updated after every BP is composed of (2.1) and (4.1); see, e.g., [5], [10], <ref> [20] </ref>, and [21]. Theorem 4.1 Suppose that conditions (A1)-(A5) and (A7)-(A8) are satisfied with the condition M 1 +ff=2 &lt; 0 replaced by M 1 j ( 0 )+ff=2 &lt; 0 and the condition on 0 replaced by 0 &gt; 2 (p+ 0 +1).
Reference: [21] <author> P. L'Ecuyer, N. Giroux, and P. W. Glynn, </author> <title> "Stochastic optimization by simulation: numerical experiments with the M/M/1 queue in steady-state," </title> <institution> Management Sci., vol.40, pp.1245-1261, </institution> <year> 1994. </year>
Reference-contexts: The PARMSR algorithm updated after every BP is composed of (2.1) and (4.1); see, e.g., [5], [10], [20], and <ref> [21] </ref>. Theorem 4.1 Suppose that conditions (A1)-(A5) and (A7)-(A8) are satisfied with the condition M 1 +ff=2 &lt; 0 replaced by M 1 j ( 0 )+ff=2 &lt; 0 and the condition on 0 replaced by 0 &gt; 2 (p+ 0 +1). <p> But their optimal step sizes are different. This agrees with the numerical results given in <ref> [21] </ref>. If both algorithms use the same step sizes a n , of course they have a different limiting behavior, and the algorithm that does best in this situation is the one whose step sizes fa n g match best the optimal step sizes.
Reference: [22] <author> P. L'Ecuyer and G. Yin, </author> <title> "Budget-dependent convergence rate of stochastic approximation," </title> <journal> SIAM Journal on Optimization, </journal> <volume> vol 8, </volume> <pages> pp. 217-247, </pages> <year> 1998. </year>
Reference: [23] <author> M. S. Meketon, </author> <title> "Optimization in simulation: a survey of recent results," </title> <booktitle> Proc. of the 1987 Winter Simulation Conference, </booktitle> <pages> pp. 58-67, </pages> <year> 1987. </year>
Reference: [24] <author> M. B. Nevel'son and R. S. Hasminsky, </author> <title> Stochastic Approximation and Recursive Estimation, </title> <journal> Translated in Amer. Math. Soc., </journal> <volume> Transl. </volume> <pages> Monographs, </pages> <address> vol.24, Providence, </address> <year> 1972. </year>
Reference-contexts: Proof. Let " n+1 = f n+1 j ( n )f ( n ). It is seen that f" n ; F k n g is a sequence of martingale differences. The rest the proof is easily derived from (6.49) and is standard (see, e.g., <ref> [4, 17, 24] </ref>). 2 We now express the convergence in terms of the computing budget for n SA iterations, which is a random variable N n = k n = P n i=1 j i ( i1 ). <p> to derive that j' n; k j c 0 expfc n X a j g; (6.3) sup n X a i j' n; i j r &lt; 1; 8 r &gt; 0; (6.4) n!1 i=0 n; i+1 = 0 where c 0 and c are some positive constants (see, e.g., <ref> [2, 4, 24, 38] </ref>).
Reference: [25] <author> E. Nummelin, </author> <title> "Regeneration in tandem queues". </title> <journal> Adv. Appl. Prob., vol.13, </journal> <volume> pp.221-230, </volume> <year> 1981. </year>
Reference: [26] <author> B. T. Polyak, </author> <title> "New method of stochastic approximation type procedure,"Automation and Remote Control, </title> <address> vol.51, pp.937-946, </address> <year> 1990. </year>
Reference-contexts: Our results show that the step sizes of the algorithms play a major role in their limiting behavior. The idea of averaging the iterates in stochastic approximation (see, e.g., [2], [4], <ref> [26] </ref>, [38]), which is devoted to achieving an optimal convergence rate and optimal variance constant for a broader range of step sizes, can also be combined in a natural way with the PARMSR algorithms.
Reference: [27] <author> H. Robbins and S. Monro, </author> <title> "A stochastic approximation method," </title> <journal> Ann. Math. Statist., </journal> <volume> vol.22, </volume> <pages> pp. 400-407, </pages> <year> 1951. </year>
Reference: [28] <author> K. Sigman, </author> <title> "Notes on the stability of closed queueing networks," </title> <journal> J. Appl. Probab., vol.26, pp.678-682, 1989; "Corrections," J. Appl. Probab., </journal> <volume> vol. 27, p.735, </volume> <year> 1990. </year>
Reference: [29] <author> K. Sigman, </author> <title> "The stability of open queueing networks," </title> <journal> Stoch. Processes and their Appls., vol.35, </journal> <volume> pp.11-25, </volume> <year> 1990. </year>
Reference: [30] <author> W. F. Stout, </author> <title> Almost Sure Convergence, </title> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1974. </year>
Reference: [31] <author> R. Suri, </author> <title> "Perturbation analysis: the state of the art and research issues explained via the GI/G/1 queue," </title> <journal> Proc. IEEE, </journal> <volume> vol. 77, </volume> <pages> pp. 114-137, </pages> <year> 1989. </year>
Reference: [32] <author> R. Suri and Y. T. Leung, </author> <title> "Single run optimization of discrete event simulations | An empirical study using the M/M/1 queue," </title> <journal> IIE Transactions, </journal> <volume> vol.21, </volume> <pages> pp. 35-49, </pages> <year> 1989. </year>
Reference: [33] <author> R. Suri and M. A. Zazanis, </author> <title> "Perturbation analysis gives strongly consistent sensitivity estimates for the M/G/1 queue," </title> <institution> Management Sci., vol.34, pp.39-64, </institution> <year> 1988. </year>
Reference: [34] <author> Q. Y. Tang and H. F. Chen, </author> <title> "Convergence of perturbation analysis based optimization algorithm with fixed number of customers period," Discrete Event Dynamic Systems: </title> <journal> Theory and Appl., vol.4, </journal> <volume> pp.359-375, </volume> <year> 1994. </year>
Reference-contexts: By (A1), (6.9), and Lemma 2 of Tang and Chen <ref> [34] </ref> it follows that a k m j m+1 (b) ! 0; a:s: (6.10) Since 1 X a n = 1, from (6.3) it follows that j' n; k 0 j ! n ! 1 0 for any fixed k 0 . <p> and (6.36) it is seen that E 4 m=0 p 3 E 4 m=0 p m 3 n ! 1 which gives o (n) X a k m j' n; k m j a k m j m+1 (b)V m+1 ! 0: (6.39) By (A6), (6.36) and Lemma 2 of <ref> [34] </ref>, it is derived that a k m ' n; k m a k m j m+1 (b)V m+1 ! 0; a:s:; which implies a k o (n) ' n; k o (n) a k o (n) j o (n)+1 (b)V o (n)+1 ! 0; a:s: (6.40) Combining (6.39) with (6.40) <p> In order to make the result of Theorem 2.1 applicable to the present setting, as in <ref> [34] </ref> and [35], we introduce the following transformations (n) = [ L fi (n1)L+j = fi n; j ; j = 0; 1; :::; L 1; (6.54) e ' n; i = ' (n1); (i1)+1 ; 8 i 1; e ' n; j = e ' n; 0 ; 8 j
Reference: [35] <author> Q. Y. Tang, H. F. Chen, and Z. J. Han, </author> <title> "Convergence rates of Perturbation-Analysis-Robbins-Monro-Single-Run algorithms for single server queues," </title> <journal> IEEE Trans. on Autom. Contr., </journal> <volume> vol. 42, no. 10, </volume> <pages> pp. 1442-1447, </pages> <year> 1997. </year>
Reference-contexts: By (6.10), in order to prove oe (n) X a k m ' 2 P n ! 1 it suffices to prove o (n) X a k m ' 2 P n ! 1 13 By Lemma 3.3 of <ref> [35] </ref>, there exist constant fi 2 &gt; 0 and fl 1 2 (0; ), where is given in condition (A6), such that P fj m+1 6= j m+1 ( k m )jF k m g fi 2 a k m ; 8 m 1; (6.16) where the filtration F k m <p> From (6.19) we have oe (n) X a k m ' 2 P n ! 1 (v). Under conditions (A1)-(A7), by Theorem 2.1 of <ref> [35] </ref>, lim n!1 n = 0 a.s. Since j () is continuous at 0 by condition (A8), it is seen that lim m!1 j ( k m ) = j ( 0 ), a.s. <p> If conditions (A1)-(A8) are satisfied, then for the PARMSR algorithm updated after the service of every customer defined by (2.1)-(2.3) the following result holds oe (n) X p d n ! 1 1 ): (6.23) Proof. As in Lemma 3.1 of <ref> [35] </ref>, it can be shown that p n ! 1 which gives p n ! 1 By the definition (6.2), it is seen that j m=0 a k m ' n; k m w m+1 m=0 a k m ' n; k m w m+1 j p n ! 1 15 <p> We will verify that the conditions of Theorem 9.5.2 in Chow and Teicher [9] hold for n X X n; j . As in the proof of Lemma 3.4 (part (ii)) in <ref> [35] </ref>, it is shown that fw j+1 ; F k j ; j 0g is a sequence of martingale differences. <p> condition (A4), (6.31)-(6.32), it follows from (2.1)-(2.2) that j k m +i k m j j=1 a k m j=1 0 where W m+1 = j m+1 (b)(B 1 + B 2 L 0 p X b s L s As in the proof (part (iv)) of Lemma 3.4 in <ref> [35] </ref>, there exist random variables fV m+1 ; m 0g such that jf k m +i f k m +i ( k m )j a k m V m+1 ; 1 i j m+1 ; (6.35) and that by condition (A7), (3.9) and the Holder inequality the following inequality holds: sup <p> Analogous to (6.27), by condition (A4) we have j i=j m+1 ( k m )+1 (0) 4 (3) By Lemma 3.3 of <ref> [35] </ref> and the Holder inequality, it is derived that E 6 fi fi fi fi m=0 a k m ' n; k m i=j m+1 ( k m )+1 fi fi fi fi 7 E 4 m=0 a k m j' n; k m j E [W m+1 ] P fj <p> = 1=2 in (6.8) we get 1ij m+1 p p 0 j m+1 (b)a k m : (6.45) Similar to (6.27), we can prove j" k m +i j W m+1 ; 8 1 i j m+1 : (6.46) By (6.8), (6.9) and condition (A7), similar to Lemma 3.1 in <ref> [35] </ref>, we can prove lim p (0) 1ij m+1 a k m +i Thus, by (6.45) it is derived that j m=0 i=1 p p ff fl oe (n) X j m+1 X j' n; k m +i ja k m W m+1 j m+1 (b) 0 m=0 i=1 p (0) <p> +j jW m+1 2 0 1 X a k m i=1 j=1 (0) +ff fl oe (n) X 0 i=1 p (0) 1ij m+1 a k m +i1 n ! 1 via a similar argument as in (6.48). 2 Proof of Theorem 2.1: Under conditions (A1)-(A7), from Theorem 2.2 of <ref> [35] </ref> we have j n 0 j = o (a 1=4 After a finite number of steps, algorithm (2.1) will become the usual Robbins-Monro algorithm. <p> In order to make the result of Theorem 2.1 applicable to the present setting, as in [34] and <ref> [35] </ref>, we introduce the following transformations (n) = [ L fi (n1)L+j = fi n; j ; j = 0; 1; :::; L 1; (6.54) e ' n; i = ' (n1); (i1)+1 ; 8 i 1; e ' n; j = e ' n; 0 ; 8 j &lt; 0;
Reference: [36] <author> H. Thorisson, </author> <title> "The queue GI=G=1: finite moments of the cycle variables and uniform rates of convergence," </title> <journal> Stoch. Processes and their Appl., </journal> <volume> vol.19, </volume> <pages> pp. 85-99, </pages> <year> 1985. </year>
Reference-contexts: 0 2 [0; 1); which gives max ja 1ffi 0 k m j ff fl k m ; 8 ffi 0 2 [0; 1): (6.8) By condition (A7) (ii), we have E j 1 (b) 0 &lt; 1; E L 1 (b) 0 &lt; 1; (6.9) see, e.g., [13] and <ref> [36] </ref>.
Reference: [37] <author> R. Wolff, </author> <title> Stochastic Modeling and the Theory of Queues, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference: [38] <author> G. Yin, </author> <title> "On extension of Polyak's averaging approach to stochastic approximation," </title> <journal> Stochastics and Stochastics Reports, </journal> <volume> vol. 36, pp.245-264, </volume> <year> 1991. </year> <month> 26 </month>
Reference-contexts: Our results show that the step sizes of the algorithms play a major role in their limiting behavior. The idea of averaging the iterates in stochastic approximation (see, e.g., [2], [4], [26], <ref> [38] </ref>), which is devoted to achieving an optimal convergence rate and optimal variance constant for a broader range of step sizes, can also be combined in a natural way with the PARMSR algorithms. <p> to derive that j' n; k j c 0 expfc n X a j g; (6.3) sup n X a i j' n; i j r &lt; 1; 8 r &gt; 0; (6.4) n!1 i=0 n; i+1 = 0 where c 0 and c are some positive constants (see, e.g., <ref> [2, 4, 24, 38] </ref>).
References-found: 38

