URL: http://www.bmc.riken.go.jp/sensor/Allan/ICA/files/ciI3Ecs96.ps.gz
Refering-URL: http://www.bmc.riken.go.jp/sensor/Allan/ICA/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: cia@kamo.riken.go.jp  
Title: SELF-ADAPTIVE NEURAL NETWORKS FOR BLIND SEPARATION OF SOURCES  
Author: Andrzej CICHOCKI Shun-ichi AMARI, Masaharu ADACHI, W lodzimierz KASPRZAK 
Address: Hirosawa 2-1, Wako-shi, Saitama 351-01, JAPAN  
Affiliation: Institute of Physical and Chemical Research, RIKEN F.R. Program, Brain Information Processing Group  
Note: 1996 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS VOL. 2, 157-160. IEEE, PISCATAWAY, NJ, 1996. 96CH35876.  
Abstract: Novel on-line learning algorithms with self adaptive learning rates (parameters) for blind separation of signals are proposed. The main motivation for development of new learning rules is to improve convergence speed and to reduce cross-talking, especially for non-stationary signals. Furthermore, we have discovered that under some conditions the proposed neural network models with associated learning algorithms exhibit a random switch of attention, i.e. they have ability of chaotic or random switching or cross-over of output signals in such way that a specified separated signal may appear at various outputs at different time windows. Validity, performance and dynamic properties of the proposed learning algorithms are investigated by computer simulation experiments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Amari, S., </author> <title> Theory of adaptive pattern classifiers, </title> <journal> IEEE Trans. Electr. Comput., </journal> <volume> EC-16, </volume> <year> 1967, </year> <pages> pp. 299-307. </pages>
Reference-contexts: The objective of this paper is twofold. The main task is to develop on-line learning algorithms with local self adaptive learning parameters in order to provide high performance and high convergence speed for real-time separation of nonstationary source signals <ref> [1] </ref>. The second task is to investigate some dynamical properties of the learning models interesting from neuro-biological point of view. Especially, we are interested in the mechanism of selective attention and random (transient) switching of attention. <p> Learning algorithm for learning rates In this paper we propose that each synaptic weight w ij has its own (local) learning rate ij (t) and that this rate is adjusted during the learning process according to a set of differential equations (cf. <ref> [1] </ref>): t 1 dt t 2 dt = ij (t) + ffjv ij (t)j; (11) where t 1 &gt; 0; t 2 &gt; 0 are time constants, ff &gt; 0 is the gain factor, jxj means absolute value of x, and g ij (t) = w ij (t) f i [y
Reference: [2] <author> Amari, S., Cichocki, A. and Yang, H.H., </author> <title> A new learning algorithm for blind signal separation, </title> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, </address> <year> 1996, </year> <note> (in print). </note>
Reference-contexts: Associated learning algorithm For the above described mixing and neural network models we have developed a novel learning rule described by a system of nonlinear differential equations (cf. <ref> [2] </ref>, [6]): dw ij (t) = ij (t) w ij (t) f i [y i (t)] p=1 ) where ij (t) &gt; 0 is a local adaptive learning rate, and f i (y i ) is a local activation function.
Reference: [3] <author> Amari, S., Cichocki, A. and Yang, H.H., </author> <title> Recurrent neural networks for blind separation of sources, </title> <booktitle> Proceedings of NOLTA-95, </booktitle> <address> Las Vegas, USA, Dec.1995, vol.1, pp.37-42. </address>
Reference-contexts: RECURRENT MODEL AND ASSOCIATED ON-LINE LEARNING ALGORITHM 4.1. Recurrent (feedback) neural network model In previous section we have considered a linear feed-forward single layer neural network. In this section we consider a recurrent Amari-Hopfield type neural network described by a set of differential equations <ref> [3] </ref>: t dt n X cw ij (t)y j (t); (i = 1; 2; :::; n) where t &gt; 0 is a time constant. <p> Let us notice that the proposed neural network is fully connected, i.e. it contains self-loop connections with cw ii (t) generally not equal to zero <ref> [3] </ref>. <p> W (t) 6= I; 8t. 4.2. Associated self-adaptive learning algorithm For the fully recurrent model we have developed the following adaptive on-line learning algorithm, written in matrix form (cf. <ref> [3] </ref>): d c W (t) = (t) : fl ( c W (t) + I) f [y (t)]y T (t) I ; (18) where (t) is the n fi n matrix with positive entries and :fl means element-wise multiplication.
Reference: [4] <author> Bell, A. J. and Sejnowski, T.J., </author> <title> An information-maximization approach to blind separation and blind deconvolution, </title> <journal> Neural Computation, </journal> <volume> vol. 7, </volume> <year> 1995, </year> <pages> pp. 1129-1159. </pages>
Reference: [5] <author> Cardoso, J.-F., Belouchrani, A. and Laheld,B., </author> <title> A new composite criterion for adaptive and iterative blind source separation, </title> <booktitle> Proceedings ICASSP-94, vol.4, </booktitle> <address> Ade-laide, Australia, </address> <month> May </month> <year> 1994, </year> <month> pp.273-276. </month>
Reference: [6] <author> Cichocki, A., Unbehauen, R., Moszczynski, L. and Rummert, E., </author> <title> A new on-line adaptive learning algorithm for blind separation of source signals, </title> <booktitle> Proceedings of ISANN-94, </booktitle> <address> Taiwan, </address> <month> Dec. </month> <year> 1994, </year> <pages> pp. 406-411. </pages>
Reference-contexts: Associated learning algorithm For the above described mixing and neural network models we have developed a novel learning rule described by a system of nonlinear differential equations (cf. [2], <ref> [6] </ref>): dw ij (t) = ij (t) w ij (t) f i [y i (t)] p=1 ) where ij (t) &gt; 0 is a local adaptive learning rate, and f i (y i ) is a local activation function.
Reference: [7] <author> Hendin, O., Horn, D. and Hopfield, J.J., </author> <title> Decomposition of a mixture of signals in a model of the olfactory bulb, </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA, </institution> <year> 1994, </year> <pages> pp. 5942-5946 </pages>
Reference-contexts: This switching is sudden and usually it has some irregular transient behavior. Closely related to "cocktail party" problem are models for the olfactory bulb, which perform separation and decomposition of mixed odor inputs from different sources <ref> [7] </ref>. It is well known that animals need to detect and recognize the odor properties and to localize them in space, in order to hunt for food or to flee from danger.
Reference: [8] <author> Jutten, C. and Herault, J., </author> <title> Blind separation of sources, Part I: An adaptive algorithm based on neuro-mimetic architecture, </title> <booktitle> Signal Processing, </booktitle> <volume> vol. 24, </volume> <year> 1991, </year> <pages> 1-20. </pages>
Reference: [9] <author> Oja, E. and Karhunen, J., </author> <title> Signal separation by nonlinear Hebbian learning, </title> <booktitle> Proceedings of ICNN-95, </booktitle> <address> Perth, Australia, </address> <publisher> Dec.1995, </publisher> <pages> pp. 211-217. </pages>
Reference: [10] <author> Matsuoka, K., Ohya, M. and Kawamoto, M., </author> <title> A neural net for blind separation of non-stationary signal sources, Neural Networks, </title> <address> vol.8, </address> <year> 1995, </year> <pages> pp. 411-419. </pages>
Reference: [11] <author> Tong, L., Liu, R., Soon, V.C. and Huang, Y.-F., </author> <title> Indeterminacy and identifiability of blind identification, </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <volume> vol.38, </volume> <year> 1991, </year> <pages> pp. 499-509. </pages>
Reference-contexts: For this problem, there is no way of knowing the original labeling of the sources, hence any permutation of the outputs is also a satisfying solution. Secondly, the output signals can be arbitrary scaled by non-zero scaling factors. This indeterminacy can be expressed mathematically by a matrix equation <ref> [11] </ref> y (t) = Ps (t); (5) where P 2 R nfim is a generalized permutation matrix in which each column and m rows (m n) contain only one nonzero element. Remaining rows should have all elements equal to zero. 3. FEED-FORWARD MODEL AND ITS LEARNING RULES 3.1.
References-found: 11

