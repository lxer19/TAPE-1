URL: http://www.cs.utexas.edu/users/diz/mutual.ps
Refering-URL: http://www.cs.utexas.edu/users/diz/pubs.html
Root-URL: 
Title: Lower Bounds for Randomized Mutual Exclusion  
Author: Eyal Kushilevitz Yishay Mansour Michael O. Rabin David Zuckerman 
Keyword: Mutual Exclusion, Randomized Distributed Algorithms, Markov Chains, Lower Bounds.  
Abstract: We establish, for the first time, lower bounds for randomized mutual exclusion algorithms (with a read-modify-write operation). Our main result is that a constant-size shared variable cannot guarantee strong fairness, even if randomization is allowed. In fact, we prove a lower bound of (log log n) bits on the size of the shared variable, which is also tight. We investigate weaker fairness conditions and derive tight (upper and lower) bounds for them as well. Surprisingly, it turns out that slightly weakening the fairness condition results in an exponential reduction in the size of the required shared variable. Our lower bounds rely on an analysis of Markov chains that may be of interest on its own and may have applications elsewhere. 
Abstract-found: 1
Intro-found: 1
Reference: [AS91] <author> H. Attiya and M. Snir. </author> <title> Better computing on the anonymous ring. </title> <journal> Journal of Algorithms, </journal> <volume> 12:204 238, </volume> <month> June </month> <year> 1991. </year>
Reference-contexts: Another type of argument for randomized lower bounds is through the use of the min-max theorem <ref> [Yao77, AS91] </ref>. For randomized Byzantine agreement, in the case that more than a third of the processes are faulty, a lower bound on the success rate is known [KY84, GY89].
Reference: [Ben83] <author> M. Ben-Or. </author> <title> Another advantage of free choice: Complete asynchronous agreement protocols. </title> <booktitle> In Proc. 6th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 27-30, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: It is a natural tool which is usually used in order to break symmetry between identical processes in a distributed system. Beyond its natural role in symmetry breaking, randomization often increases the computation power (e.g., <ref> [LR81, Ben83] </ref>), significantly decreases computational costs (e.g., [Bra85, FM88]), and helps in simplifying algorithms. For many applications in distributed environments, there is a provable gap between the power of randomized algorithms and their deterministic counterparts.
Reference: [BFJ + 82] <author> J. E. Burns, M. J. Fischer, P. Jackson, N. A. Lynch, and G. L. Peterson. </author> <title> Data requirements for implementation of n-process mutual exclusion using a single shared variable. </title> <journal> JACM, </journal> <volume> 29 </volume> <pages> 183-205, </pages> <year> 1982. </year>
Reference-contexts: The complexity measure here is the size of the shared variable. 1 Any deterministic algorithm requires an (log n) bit shared variable, in order to achieve mutual exclusion (with fairness) between n distinct process, and this bound is tight <ref> [BFJ + 82] </ref>. <p> An important parameter for evaluating the complexity of a mutual exclusion algorithm is the size of the shared variable that is used. As mentioned above, to guarantee only deadlock freeness, a one-bit semaphore is sufficient [Dij65]. Burns et. al. <ref> [BFJ + 82] </ref> define the bounded-waiting property as a fairness criterion.
Reference: [Bra85] <author> G. Bracha. </author> <title> An O(log n) expected rounds randomized byzantine generals protocol. </title> <booktitle> In Proc. 17th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 316-326, </pages> <year> 1985. </year>
Reference-contexts: It is a natural tool which is usually used in order to break symmetry between identical processes in a distributed system. Beyond its natural role in symmetry breaking, randomization often increases the computation power (e.g., [LR81, Ben83]), significantly decreases computational costs (e.g., <ref> [Bra85, FM88] </ref>), and helps in simplifying algorithms. For many applications in distributed environments, there is a provable gap between the power of randomized algorithms and their deterministic counterparts.
Reference: [CIL87] <author> B. Chor, A. Israeli, and M. Li. </author> <title> On process coordination using asynchronous hardware. </title> <booktitle> In Proc. 6th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 86-97, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: Another important example is that of reaching a consensus in an asynchronous distributed system with faults: this is impossible with deterministic protocols, even if the faults are restricted to a single fail-stop fault [FLP85], but is possible with the use of randomized protocols (see <ref> [CIL87] </ref>). The gap between the performance of randomized and deterministic algorithms exists also for the mutual exclusion problem.
Reference: [Dij65] <author> E. Dijkstra. </author> <title> Solution of a problem in concurrent programming control. </title> <journal> CACM, </journal> <volume> 8(9):569, </volume> <year> 1965. </year>
Reference-contexts: The sequence of accesses to the shared variable is determined by a scheduler. The mutual exclusion problem is a classical problem in distributed computing. It was first suggested by Dijkstra <ref> [Dij65] </ref>, who solved the problem using a (one-bit) semaphore. <p> All these solutions guarantee deadlock freedom, together with some notion of fairness. An important parameter for evaluating the complexity of a mutual exclusion algorithm is the size of the shared variable that is used. As mentioned above, to guarantee only deadlock freeness, a one-bit semaphore is sufficient <ref> [Dij65] </ref>. Burns et. al. [BFJ + 82] define the bounded-waiting property as a fairness criterion.
Reference: [FL82] <author> M. Fischer and N. Lynch. </author> <title> A lower bound for the time to assure interactive consistency. </title> <journal> IPL, </journal> <volume> 14(4) </volume> <pages> 183-186, </pages> <year> 1982. </year>
Reference-contexts: For many applications in distributed environments, there is a provable gap between the power of randomized algorithms and their deterministic counterparts. The most renowned example is achieving Byzantine agreement with a linear number of faults; while any deterministic algorithm requires at least a linear number of rounds <ref> [FL82] </ref>, there is a randomized algorithm that performs the same task in a fl An early version of this paper appeared in Proc. of 25th ACM Symp. on Theory of Computation, May 1993, pp. 154-163.
Reference: [FLP85] <author> M. J. Fischer, N. A. Lynch, and M. S. Paterson. </author> <title> Impossibility of distributed consensus with one family faulty process. </title> <journal> Journal of the ACM, </journal> <volume> 32(2) </volume> <pages> 374-382, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: Another important example is that of reaching a consensus in an asynchronous distributed system with faults: this is impossible with deterministic protocols, even if the faults are restricted to a single fail-stop fault <ref> [FLP85] </ref>, but is possible with the use of randomized protocols (see [CIL87]). The gap between the performance of randomized and deterministic algorithms exists also for the mutual exclusion problem.
Reference: [FM88] <author> P. Feldman and S. Micali. </author> <title> Optimal algorithms for byzantine agreement. </title> <booktitle> In Proc. 20th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 148-161, </pages> <year> 1988. </year>
Reference-contexts: It is a natural tool which is usually used in order to break symmetry between identical processes in a distributed system. Beyond its natural role in symmetry breaking, randomization often increases the computation power (e.g., [LR81, Ben83]), significantly decreases computational costs (e.g., <ref> [Bra85, FM88] </ref>), and helps in simplifying algorithms. For many applications in distributed environments, there is a provable gap between the power of randomized algorithms and their deterministic counterparts. <p> Research was done while the author was at the Laboratory for Computer Science, MIT, and supported by an NSF Postdoctoral Fellowship. Current address: Department of Computer Sciences, The University of Texas at Austin, Austin, TX 78712. e-mail: diz@cs.utexas.edu. 1 constant number of rounds <ref> [FM88] </ref>. Another important example is that of reaching a consensus in an asynchronous distributed system with faults: this is impossible with deterministic protocols, even if the faults are restricted to a single fail-stop fault [FLP85], but is possible with the use of randomized protocols (see [CIL87]).
Reference: [GY89] <author> R. L. Graham and A. C. Yao. </author> <title> On the improbability of reaching byzantine agreements. </title> <booktitle> In Proc. 21st ACM Symp. on Theory of Computing, </booktitle> <pages> pages 467-478, </pages> <year> 1989. </year>
Reference-contexts: Another type of argument for randomized lower bounds is through the use of the min-max theorem [Yao77, AS91]. For randomized Byzantine agreement, in the case that more than a third of the processes are faulty, a lower bound on the success rate is known <ref> [KY84, GY89] </ref>. Previous Work and Our Results In the following we give a more detailed description of the mutual exclusion problem and a summary of our results together with previous results related to our work. The setting of the mutual exclusion problem is as follows.
Reference: [IR81] <author> A. Itai and M. Rodeh. </author> <title> The lord of the ring, or probabilistic methods for breaking symmetry in distributed networks. </title> <booktitle> In Proc. 22th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 150-158, </pages> <address> Nashville, Tennessee, </address> <month> October </month> <year> 1981. </year>
Reference-contexts: Few lower bounds are known for randomized distributed algorithms. Many of these lower bounds are based on arguments that arise from the need of information to flow from one side of the network to the other side, or based on the symmetry between different processes <ref> [IR81] </ref>. Another type of argument for randomized lower bounds is through the use of the min-max theorem [Yao77, AS91]. For randomized Byzantine agreement, in the case that more than a third of the processes are faulty, a lower bound on the success rate is known [KY84, GY89].
Reference: [KR92] <author> E. Kushilevitz and M. O. Rabin. </author> <title> Randomized mutual exclusion algorithms revisited. </title> <booktitle> In Proc. 11th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 275-283, </pages> <year> 1992. </year>
Reference-contexts: On the other hand, there is a randomized algorithm requiring only an O (log log n) bit shared variable <ref> [Rab82, KR92] </ref>. 2 It remained an open problem whether the complexity of the randomized algorithm for the mutual exclusion problem can be farther reduced, perhaps even to a constant number of bits. <p> Still, the number of bits is a very natural measure for the size of variables. 2 The first solution for this problem was given in [Rab82]. A flaw in this solution was pointed out by [Sai92]. A new solution, based on ideas of [Rab82], was given in <ref> [KR92] </ref>. 2 competing for the critical section; a process that is waiting for the critical section may wait forever. Since then, numerous solutions were proposed for the mutual exclusion problem. All these solutions guarantee deadlock freedom, together with some notion of fairness. <p> This property can be considered as a probabilistic analogue of the bounded-waiting property. Randomized algorithms having the linear-fairness property that use O (log log n)-bit shared variable are presented in <ref> [Rab82, KR92] </ref>. This is in contrast to the fi (log n)-bit shared variable required by deterministic algorithms. Proving the correctness of such randomized distributed protocols involves many delicate issues. Saias [Sai92] developed a general methodology to prove the correctness of a randomized distributed protocol. <p> No lower bounds for randomized mutual exclusion were known. In fact, in light of the results mentioned earlier, it may seem plausible that a constant-size shared variable is sufficient for mutual exclusion with linear fairness. More than that, it was shown <ref> [Rab82, KR92] </ref> that a constant-size shared variable may be powerful; it suffices for guaranteeing that each of the competing processes will have (1=n) probability to enter the critical section. However, this is independent of m and hence is much weaker. <p> A run is called proper if the subsequence of phases corresponds to every process P i is of the form: REMAINDER, TRYING, CS, EXIT, REMAINDER,... 4 We assume here the same adversary scheduler and the same correctness conditions as in <ref> [KR92] </ref>. 5 note that since we are interested in this work in proving lower bounds, this assumption makes our job more complicated. 4 A scheduler is a (probabilistic) function that on a finite run gives the identity of the next process to access the shared variable. <p> We follow here the solution suggested by <ref> [KR92] </ref>, that requires the "good" chance to be only in the next time step. 2.2 Markov Chains Let S = fs 1 ; s 2 ; : : : ; s k g be a set of k states. <p> The "winners" of the lottery are those processes drawing the maximal drawn number. We use as a black-box the following theorem, implicit in <ref> [KR92] </ref>, that reduces the existence of mutual exclusion algorithms with certain fairness properties to the existence of lotteries that guarantee a certain probability of having a unique winner. More precisely, Theorem 13: [KR92] Let f be a function, n and B be integers. <p> We use as a black-box the following theorem, implicit in <ref> [KR92] </ref>, that reduces the existence of mutual exclusion algorithms with certain fairness properties to the existence of lotteries that guarantee a certain probability of having a unique winner. More precisely, Theorem 13: [KR92] Let f be a function, n and B be integers. <p> By this theorem, in order to prove the existence of mutual exclusion algorithms, it is enough to prove the existence of the appropriate lotteries. For example, the lottery used in <ref> [Rab82, KR92] </ref> assigns a probability of 2 j for each value 1 j &lt; B (B = 4 + log n), and probability 2 B+1 for the value B.
Reference: [KY84] <author> A. Karlin and A. C. Yao. </author> <title> Probabilistic lower bounds for byzantine agreement. </title> <year> 1984. </year>
Reference-contexts: Another type of argument for randomized lower bounds is through the use of the min-max theorem [Yao77, AS91]. For randomized Byzantine agreement, in the case that more than a third of the processes are faulty, a lower bound on the success rate is known <ref> [KY84, GY89] </ref>. Previous Work and Our Results In the following we give a more detailed description of the mutual exclusion problem and a summary of our results together with previous results related to our work. The setting of the mutual exclusion problem is as follows.
Reference: [LR81] <author> D. Lehman and M. O. Rabin. </author> <title> On the advantage of free choice: A symmetric and fully distributed solution to the dining philosophers problem. </title> <booktitle> In Proc. 8th POPL, </booktitle> <pages> pages 133-138, </pages> <year> 1981. </year>
Reference-contexts: It is a natural tool which is usually used in order to break symmetry between identical processes in a distributed system. Beyond its natural role in symmetry breaking, randomization often increases the computation power (e.g., <ref> [LR81, Ben83] </ref>), significantly decreases computational costs (e.g., [Bra85, FM88]), and helps in simplifying algorithms. For many applications in distributed environments, there is a provable gap between the power of randomized algorithms and their deterministic counterparts.
Reference: [Rab82] <author> M. O. Rabin. </author> <title> n-process mutual exclusion with bounded waiting by 4 log 2 n-valued shared variable. </title> <journal> JCSS, </journal> <volume> 25(1) </volume> <pages> 66-75, </pages> <year> 1982. </year>
Reference-contexts: On the other hand, there is a randomized algorithm requiring only an O (log log n) bit shared variable <ref> [Rab82, KR92] </ref>. 2 It remained an open problem whether the complexity of the randomized algorithm for the mutual exclusion problem can be farther reduced, perhaps even to a constant number of bits. <p> Still, the number of bits is a very natural measure for the size of variables. 2 The first solution for this problem was given in <ref> [Rab82] </ref>. A flaw in this solution was pointed out by [Sai92]. A new solution, based on ideas of [Rab82], was given in [KR92]. 2 competing for the critical section; a process that is waiting for the critical section may wait forever. <p> Still, the number of bits is a very natural measure for the size of variables. 2 The first solution for this problem was given in <ref> [Rab82] </ref>. A flaw in this solution was pointed out by [Sai92]. A new solution, based on ideas of [Rab82], was given in [KR92]. 2 competing for the critical section; a process that is waiting for the critical section may wait forever. Since then, numerous solutions were proposed for the mutual exclusion problem. All these solutions guarantee deadlock freedom, together with some notion of fairness. <p> They proved that if deterministic algorithms are used then an (log n)-bit shared variable is required for achieving bounded-waiting, and that this number of bits is also sufficient. Rabin <ref> [Rab82] </ref> suggested the use of randomized algorithms for mutual exclusion, and defined the notion of fairness for such algorithms. <p> This property can be considered as a probabilistic analogue of the bounded-waiting property. Randomized algorithms having the linear-fairness property that use O (log log n)-bit shared variable are presented in <ref> [Rab82, KR92] </ref>. This is in contrast to the fi (log n)-bit shared variable required by deterministic algorithms. Proving the correctness of such randomized distributed protocols involves many delicate issues. Saias [Sai92] developed a general methodology to prove the correctness of a randomized distributed protocol. <p> The key idea in his methodology is that these two ingredients should be made independent. Using his systematic methodology, Saias [Sai92] uncovered the flaw in <ref> [Rab82] </ref>. No lower bounds for randomized mutual exclusion were known. In fact, in light of the results mentioned earlier, it may seem plausible that a constant-size shared variable is sufficient for mutual exclusion with linear fairness. <p> No lower bounds for randomized mutual exclusion were known. In fact, in light of the results mentioned earlier, it may seem plausible that a constant-size shared variable is sufficient for mutual exclusion with linear fairness. More than that, it was shown <ref> [Rab82, KR92] </ref> that a constant-size shared variable may be powerful; it suffices for guaranteeing that each of the competing processes will have (1=n) probability to enter the critical section. However, this is independent of m and hence is much weaker. <p> C i+1 . At first sight, it seems more natural to require that a process that arrives between time C i1 and C i will have a good probability to enter at C i , as defined in <ref> [Rab82] </ref>. 6 The definition can be weakened to require that this will hold with probability one, and all the results of the paper will remain valid. 7 The probability space is defined on prefixes of runs; therefore, the space is finite. <p> By this theorem, in order to prove the existence of mutual exclusion algorithms, it is enough to prove the existence of the appropriate lotteries. For example, the lottery used in <ref> [Rab82, KR92] </ref> assigns a probability of 2 j for each value 1 j &lt; B (B = 4 + log n), and probability 2 B+1 for the value B.
Reference: [Sai92] <author> I. Saias. </author> <title> Proving probabilistic correctness statements: The case of Rabin`s algorithm for mutual exclusion. </title> <booktitle> In Proc. 11th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 263-272, </pages> <year> 1992. </year>
Reference-contexts: Still, the number of bits is a very natural measure for the size of variables. 2 The first solution for this problem was given in [Rab82]. A flaw in this solution was pointed out by <ref> [Sai92] </ref>. A new solution, based on ideas of [Rab82], was given in [KR92]. 2 competing for the critical section; a process that is waiting for the critical section may wait forever. Since then, numerous solutions were proposed for the mutual exclusion problem. <p> Randomized algorithms having the linear-fairness property that use O (log log n)-bit shared variable are presented in [Rab82, KR92]. This is in contrast to the fi (log n)-bit shared variable required by deterministic algorithms. Proving the correctness of such randomized distributed protocols involves many delicate issues. Saias <ref> [Sai92] </ref> developed a general methodology to prove the correctness of a randomized distributed protocol. The main difficulty of such proofs is the need to deal with two separate sources of nondeterminism: the randomness that the protocol generates, and the decisions of the adversary. <p> The main difficulty of such proofs is the need to deal with two separate sources of nondeterminism: the randomness that the protocol generates, and the decisions of the adversary. The key idea in his methodology is that these two ingredients should be made independent. Using his systematic methodology, Saias <ref> [Sai92] </ref> uncovered the flaw in [Rab82]. No lower bounds for randomized mutual exclusion were known. In fact, in light of the results mentioned earlier, it may seem plausible that a constant-size shared variable is sufficient for mutual exclusion with linear fairness. <p> The results and the proofs (with few minor changes) hold for such a definition as well. 5 However, as pointed out by <ref> [Sai92] </ref>, such a statement is circular as the definition of the event C i depends on whether the process enters the critical section or not, and it seems that there is no "acceptable" way to get around this problem.
Reference: [Yao77] <author> A. C. Yao. </author> <title> Probabilistic computations: Toward a unified measure of complexity. </title> <booktitle> In Proc. 18th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 222-227, </pages> <year> 1977. </year> <month> 16 </month>
Reference-contexts: Another type of argument for randomized lower bounds is through the use of the min-max theorem <ref> [Yao77, AS91] </ref>. For randomized Byzantine agreement, in the case that more than a third of the processes are faulty, a lower bound on the success rate is known [KY84, GY89].
References-found: 17

