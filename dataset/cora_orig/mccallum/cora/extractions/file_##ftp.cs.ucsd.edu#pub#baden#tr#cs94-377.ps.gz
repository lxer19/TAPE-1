URL: file://ftp.cs.ucsd.edu/pub/baden/tr/cs94-377.ps.gz
Refering-URL: http://www.cs.ucsd.edu/~sfink/pub.html
Root-URL: http://www.cs.ucsd.edu
Title: Programming with LPARX  
Author: Scott B. Baden Scott R. Kohn Stephen J. Fink 
Note: Presented at the 1994 Intel Supercomputer Users Group Conference, San Diego, California, June 25-29, 1994, IEEE.  
Address: La Jolla, California 92093-0114 USA  
Affiliation: Department of Computer Science and Engineering University of California, San Diego  
Date: June 1994  
Pubnum: CSE Technical Report Number CS94-377  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Wu and G. Fox, </author> <title> "Fortran 90d compiler for distributed memory mimd parallel computers," </title> <type> Tech. Rep. </type> <institution> SCCS-88b, Syracuse University, </institution> <year> 1991. </year>
Reference-contexts: LPARX is a domain-specific, coarse grain data parallel programming model that provides run-time support for irregular dynamic decompositions of data organized into blocks. Such blocking structures are not currently supported by compiled languages such as Fortran 90D <ref> [1] </ref> and HPF [2], and arise in two important classes of applications in which: * irregular spatial data decompositions are required to balance workloads on parallel processors, e.g. particle methods; * the blocks are intrinsic to the numerical algorithm, such as multilevel and adaptive finite dif ference methods.
Reference: [2] <institution> High Performance Fortran Forum, Rice University, Houston, Texas, High Performance Fortran Language Specification, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: LPARX is a domain-specific, coarse grain data parallel programming model that provides run-time support for irregular dynamic decompositions of data organized into blocks. Such blocking structures are not currently supported by compiled languages such as Fortran 90D [1] and HPF <ref> [2] </ref>, and arise in two important classes of applications in which: * irregular spatial data decompositions are required to balance workloads on parallel processors, e.g. particle methods; * the blocks are intrinsic to the numerical algorithm, such as multilevel and adaptive finite dif ference methods.
Reference: [3] <author> V. S. Sunderam, </author> <title> "Pvm: A framework for parallel distributed computing," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> vol. 2, </volume> <pages> pp. 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: implemented as a C ++ class library, re-quires only basic message passing support and is currently running on several MIMD computers|the In-tel Paragon and iPSC/860, Thinking Machines CM-5, Kendall Square Research KSR-1, the nCUBE nCUBE/2| single processor workstations, the Cray C-90 (currently single processor), and networks of workstations under PVM <ref> [3] </ref>. LPARX software may invoke subroutines written in languages other than C ++ such as C or Fortran. 2 Overview In this section, we give an overview of LPARX's facilities. We introduce LPARX's representation of block irregular decompositions and then discuss its underlying coarse grain data parallel programming model.
Reference: [4] <author> R. W. Hockney and J. W. Eastwood, </author> <title> Computer Simulation Using Particles. </title> <publisher> McGraw-Hill, </publisher> <year> 1981. </year>
Reference-contexts: Grid elements may be any user-defined type or class. For example, in addition to representing a finite difference mesh of floating point numbers, a Grid may also be used to implement the spatial data structures <ref> [4] </ref> common in particle calculations. A Grid is defined over a Region and may be manipulated with high level copy operations. Associated with each Grid is its Region; the bounding box over which the Grid's storage is to be defined.
Reference: [5] <author> M. J. Berger and S. H. Bokhari, </author> <title> "A partitioning strategy for nonuniform problems on multiprocessors," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, </volume> <pages> pp. 570-580, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Perform calculations on the Grids in the XArray in parallel using the for all loop. The decomposition in (1) may be managed explicitly by the application, such as in the generation of refinement regions, or through partitioning routines such as recursive bisection <ref> [5] </ref>, uniform block partitioning, and inverse space partitioning [6], which are provided by LPARX's standard library. The assignment of Regions to processors in (2) provides the application flexibility in delegating work to processors. In general, this information will be returned by the routine which renders the partitions in (1). <p> Such a uniform decomposition does not efficiently distribute workloads; for example, no work is assigned to processors p4, p5, p10, and p11. A better method for decomposing non-uniform workloads is shown in Figure 8 (center and right), which gives two irregular block assignments rendered using recursive bisection <ref> [5] </ref>. In these decompositions, each processor receives approximately the same amount of work. Because the distribution of particles changes over time, we must periodically redistribute the workload across processors to maintain load balance.
Reference: [6] <author> J. R. Pilkington and S. B. Baden, </author> <title> "Dynamic nonuniform mesh partitioning with spacefilling curves." </title> <note> To be submitted to IEEE Transactions on Parallel and Distributed Systems, </note> <month> October </month> <year> 1993. </year>
Reference-contexts: The decomposition in (1) may be managed explicitly by the application, such as in the generation of refinement regions, or through partitioning routines such as recursive bisection [5], uniform block partitioning, and inverse space partitioning <ref> [6] </ref>, which are provided by LPARX's standard library. The assignment of Regions to processors in (2) provides the application flexibility in delegating work to processors. In general, this information will be returned by the routine which renders the partitions in (1).
Reference: [7] <author> S. R. Kohn and S. B. Baden, </author> <title> "An implementation of the lpar parallel programming model for scientific computations," </title> <booktitle> in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> (Norfolk, Va.), </address> <pages> pp. 759-766, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: LPARX is careful about how it handles communication; in localized computations such as RBPJ, many of the block copies are likely to be empty. In these cases LPARX avoids unnecessary communication and copying through optimizations built into the run-time system <ref> [7] </ref>. 5 Like XArrays, looping constructs are also strongly typed. There are 1d loops, 2d loops and so on. We use the notation for_X and for_all_X to designate X-dimensional loops, and we drop the X when discussing a looping construct in more general terms.
Reference: [8] <author> S. B. Baden, S. R. Kohn, S. M. Figueira, and S. J. </author> <type> Fink tech. rep., </type> <institution> University of California - San Diego, CSE 0114, </institution> <address> 9500 Gilman Drive, La Jolla, CA 92092-0114, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: The function mpReduceMax () takes the maximum over all the local values and returns this global maximum value. Various forms of reduction are described in the User's Guide <ref> [8] </ref>. Strictly speaking, the reduction functions are part of an external support library packaged with LPARX, called MP++. 5 In this simple example, all the blocks are static, and one can predict the exact communication structure; the O (N 2 ) communication algorithm is naive.
Reference: [9] <author> A. Almgren, T. Buttke, and P. Colella, </author> <title> "A fast vortex method in three dimensions," </title> <booktitle> in Proceedings of the 10th AIAA Computational Fluid Dynamics Conference, (Honolulu, Hawaii), </booktitle> <pages> pp. 446-455, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The computation repeats for the desired number of time steps. The force calculation typically dominates the computation time. In general, each particle may be influenced by every other particle. Rapid N-body summation algorithms <ref> [9] </ref> [10] [11] avoid the need to compute all O (n 2 ) particle-particle interactions directly. Direct interactions are calculated only for those particles lying within a specified cut-off distance r, as shown in Figure 7. The remaining interactions are computed separately. We will focus on the local interactions here.
Reference: [10] <author> J. Carrier, L. Greengard, and V. Rokhlin, </author> <title> "A fast adaptive multipole algorithm for particle simulations," </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <month> September </month> <year> 1988. </year>
Reference-contexts: The computation repeats for the desired number of time steps. The force calculation typically dominates the computation time. In general, each particle may be influenced by every other particle. Rapid N-body summation algorithms [9] <ref> [10] </ref> [11] avoid the need to compute all O (n 2 ) particle-particle interactions directly. Direct interactions are calculated only for those particles lying within a specified cut-off distance r, as shown in Figure 7. The remaining interactions are computed separately. We will focus on the local interactions here.
Reference: [11] <author> P. Tamayo, J. P. Mesirov, and B. M. Boghosian, </author> <title> "Parallel approaches to short range molecular dynamics simulations," </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <address> (Albuquerque, NM), </address> <month> November </month> <year> 1991. </year>
Reference-contexts: The computation repeats for the desired number of time steps. The force calculation typically dominates the computation time. In general, each particle may be influenced by every other particle. Rapid N-body summation algorithms [9] [10] <ref> [11] </ref> avoid the need to compute all O (n 2 ) particle-particle interactions directly. Direct interactions are calculated only for those particles lying within a specified cut-off distance r, as shown in Figure 7. The remaining interactions are computed separately. We will focus on the local interactions here.
Reference: [12] <author> M. J. Berger and P. Colella, </author> <title> "Local adaptive mesh refinement for shock hydrodynamics," </title> <journal> JCP, </journal> <volume> vol. 82, </volume> <pages> pp. 64-84, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Writing this code is straightforward and is explained in LPARX User's Guide. 4.2 Adaptive Mesh Refinement Adaptive Mesh Refinement (AMR) <ref> [12] </ref> is another important class of dynamic, irregular scientific computation. Consider the calculation of the electronic wavefunction for the C 20 H 20 molecule shown in Figure 11, which arises in ab initio molecular dynamics.
Reference: [13] <author> P. N. Hilfinger and P. Colella, "Fidil: </author> <title> A language for scientific programming," </title> <type> Tech. Rep. </type> <institution> UCRL-98057, Lawrence Livermore National Laboratory, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: LPARX supports structural abstraction, which enables the user to manipulate data decompositions and array section specifiers as first class objects. Such abstraction is related to the domain abstraction of the scientific programming language FIDIL <ref> [13] </ref> and is not supported in fine grain data parallel languages like CM Fortran [14] or High Performance Fortran. LPARX provides three new data types: an index set-valued object called Region, a dynamic array called Grid, and a distributed array of coarse grain data objects called XArray.
Reference: [14] <institution> Thinking Machines, Inc., Cambridge, Massachusetts, </institution> <note> CM Fortran User's Guide, </note> <month> July </month> <year> 1990. </year>
Reference-contexts: LPARX supports structural abstraction, which enables the user to manipulate data decompositions and array section specifiers as first class objects. Such abstraction is related to the domain abstraction of the scientific programming language FIDIL [13] and is not supported in fine grain data parallel languages like CM Fortran <ref> [14] </ref> or High Performance Fortran. LPARX provides three new data types: an index set-valued object called Region, a dynamic array called Grid, and a distributed array of coarse grain data objects called XArray. Efficient block copies are provided on Grids to hide interprocessor communication.
Reference: [15] <author> G. Agrawal, A. Sussman, and J. Saltz, </author> <title> "Compiler and runtime support for structured and block structured applications," </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Coarse grain data parallelism is supported with XArrays and a forall loop. Region calculus operations are defined over Regions, such that data dependencies may be expressed in geometric terms independently of the spatial dimension and data decomposition. Block structured PARTI/CHAOS <ref> [15] </ref> provides facilities that are similar to LPARX, though for a different problem domain: irregularly coupled regular meshes, in which the blocks are static and relatively large. LPARX does not apply to unstructured problems such as the finite element method and general sparse matrix problems.
References-found: 15

