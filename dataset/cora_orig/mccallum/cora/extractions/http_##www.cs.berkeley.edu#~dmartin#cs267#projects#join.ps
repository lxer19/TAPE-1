URL: http://www.cs.berkeley.edu/~dmartin/cs267/projects/join.ps
Refering-URL: http://www.cs.berkeley.edu/~dmartin/cs267/projects/
Root-URL: http://www.cs.berkeley.edu
Title: Parallelizing the Index-Nested-Loops Database Join Primitive on a Shared-Nothing Cluster  
Author: Aaron Brown and Christoforos Kozyrakis -abrown,kozyraki-@cs 
Date: May 15, 1998  
Abstract: We describe an optimized parallel implementation of the index-nested-loops database join primitive for the Berkeley NOW, a shared-nothing cluster of commodity workstations. Using a dataset based on that of the TPC-D benchmark, we analyze the speedup of the parallel algorithm and the impact of a restricted memory environment on the individual node performance. We tnd that our variant of the index-nested-loops join algorithm demonstrates superlinear speedup on up to 8 processors, is scalable due to eective overlap of communication, and is not highly sensitive to the amount of memory on each node: for a sample join on a scale 3 TPC-D dataset, the algorithm performs equally well in 32 MB and 128 MB (ignoring OS overhead). We also demonstrate the importance of a carefully-tuned I/O strategy; in our environment, aggressive blocking combined with application-controlled caching via the Solaris 2.6 directio() interface were crucial to performance.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arpaci-Dusseau, A. C., R. H. Arpaci-Dusseau, D. E. Culler, J. M. Hellerstein, D. A. Patterson. </author> <title> High-Performance Sorting on Networks of Workstations. </title> <booktitle> Proceedings of SIGMOD '97, </booktitle> <address> Tucson, Arizona, </address> <month> May, </month> <year> 1997. </year> <month> 18 </month>
Reference-contexts: Finally, the Berkeley NOWSort project provided the inspiration for our approach to generating a parallel shared-nothing version of database join <ref> [1] </ref>. The NOWSort project resulted in a world-record-holding implementation of the sort database primitive in a nearly identical cluster environment to what we used. <p> When combined with an appropriate call to the Solaris I/O hint interface madvise (), this method avoids copying I/O buers and provides direct access to pages of the relation tles. This was an approach used quite eectively in the NOWSort project <ref> [1] </ref>. However, we had much less success due to the dierent nature of the join problem. <p> We also wish to thank Remzi Arpaci-Dusseau for his suggestions and comments on our implementation plan, and for allowing us to reuse some of the code developed for NOWSort <ref> [1] </ref>. Finally, we wish to thank Margo Seltzer for her advice on how to tune the BerkeleyDB library for maximum I/O performance.
Reference: [2] <author> Ballinger, C. </author> <title> Teradata Database Design 101: A primer on Teradata Physi--cal Database Design and its Advantages. </title> <note> NCR/Teradata white paper, August, 1996. http://www3.ncr.com/product/data_warehouse/dbdes101.pdf. </note>
Reference-contexts: Very few commercial database systems implement a fully-parallel shared-nothing relational database engine, choosing instead to focus on shared-memory, shared-I/O-bus architectures (SMPs and CC-NUMAs). One of the few vendors that has embraced shared-nothing parallel database designs is NCR, with their Teradata system <ref> [2] </ref>. Teradata's style of parallel join processing is similar to what we propose and analyze here: relations are striped across all of the nodes, and joins are performed locally where possible.
Reference: [3] <author> Chun, B., A. Mainwaring, D. Culler. </author> <title> Virtual Network Transport protocols for Myrinet. </title> <booktitle> Proceedings of the 5th Hot Interconnects Conference, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: To minimize the size of the index, we used a bit-teld representation to store the list of processors associated with each key. Our communication layer was built on top of Active Messages II <ref> [3] </ref>. We used medium-message transfers of 8K bytes for all data exchange; we did not use bulk transfers due to their complexity and because our preliminary results demonstrated that the message transfer bandwidth was not a performance bottleneck.
Reference: [4] <author> Culler, D. E., A. Arpaci-Dusseau, R. Arpaci-Dusseau, B. Chun, S. Lumetta, A. Mainwaring, R. Martin, C. Yoshikawa, F. Wong. </author> <title> Parallel Computing on the Berkeley NOW. </title> <booktitle> Proceedings of the 9th Joint Symposium on Parallel Processing, </booktitle> <address> Kobe, Japan. </address>
Reference-contexts: The particular join primitive that we chose to implement was index-nested-loops join. The implementation was performed on the Berkeley NOW, a shared-nothing cluster built 1 out of commodity UltraSPARC workstations <ref> [4] </ref>. <p> The sorting step guarantees the best possible locality in accessing the index structure, since the tuples received from the network may be arbitrarily reordered or interleaved. 13 5.3 Implementation We implemented and measured our parallel index-nested-loops join algorithm on the Berkeley NOW <ref> [4] </ref>.
Reference: [5] <author> Culler, D. E., A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <year> 1993. </year>
Reference-contexts: We used medium-message transfers of 8K bytes for all data exchange; we did not use bulk transfers due to their complexity and because our preliminary results demonstrated that the message transfer bandwidth was not a performance bottleneck. We used the Split-C <ref> [5] </ref> library for its optimized barrier () and all_store_sync () implementations, as well as for its parallel-process start-up code; we did not make use of the Split-C compiler or message transfer primitives. We again relied on threads to overlap I/O, computation, and communication.
Reference: [6] <author> DeWitt, D. J., S. Ghandeharizadeh, D. Schneider, A. Bricker, H.-I Hsaio, and R. Rasmussen. </author> <title> The Gamma database machine project. </title> <journal> IEEE TKDE, </journal> <volume> 2(1), </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: They 3 introduce several variants on the index-nested-loops algorithm, including the optimized subsetting-sorting-nested--loops variant of index-nested-loops (the inspiration for our optimized variant described in Section 5.1), and evaluate the variants (along with hybrid hash) using both an analytic model and an implementation within the Gamma parallel database system <ref> [6] </ref>. Their results demonstrate that subsetting-sorting-nested-loops is slower than hybrid hash except where the relation sizes are very dierent or the system memory is large.
Reference: [7] <author> DeWitt, D. J., J. F. Naughton, J. Burger. </author> <title> Nested loops revisited. </title> <booktitle> Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <address> San Diego, CA, USA, </address> <year> 2022 </year> <month> Jan. </month> <year> 1993, </year> <pages> pp. 230 242. </pages>
Reference-contexts: This limitation can be overcome to a large degree by accessing the inner relation in blocks, as described in Section 4.2. Despite claims that index-nested-loops join is generally less ecient than other types of join (particularly hash join) <ref> [7] </ref>, we chose to use it as the algorithm for our join implementation, since we were interested in understanding the impact of a limited main memory on index-nested-loops join. <p> join in the limited memory case; however, the hash join implementation that we planned to use was not made available to us, and thus we were unable to perform the comparison. 3 Related Work There has been much work comparing various join algorithms in both the parallel and single-node cases <ref> [7] </ref> [8] [10]. In general, parallel hybrid hash has been shown to outperform index-nested-loops join in most cases. DeWitt et. al have probably the most comprehensive analysis of parallel, shared-nothing index-nested-loops join [7]. <p> Related Work There has been much work comparing various join algorithms in both the parallel and single-node cases <ref> [7] </ref> [8] [10]. In general, parallel hybrid hash has been shown to outperform index-nested-loops join in most cases. DeWitt et. al have probably the most comprehensive analysis of parallel, shared-nothing index-nested-loops join [7]. They 3 introduce several variants on the index-nested-loops algorithm, including the optimized subsetting-sorting-nested--loops variant of index-nested-loops (the inspiration for our optimized variant described in Section 5.1), and evaluate the variants (along with hybrid hash) using both an analytic model and an implementation within the Gamma parallel database system [6]. <p> The most nave approach is to simply broadcast the entire outer relation to all nodes; each node simply ignores the parts of the relation that it does not need. A more intelligent approach is based on the subsetting nested loops algorithm described by DeWitt et al. <ref> [7] </ref>.
Reference: [8] <author> Graefe, G. </author> <title> Query Evaluation Techniques for Large Databases. </title> <journal> ACM Computing Surveys 25, </journal> <month> 2 (June </month> <year> 1993), </year> <month> 73170. </month>
Reference-contexts: in the limited memory case; however, the hash join implementation that we planned to use was not made available to us, and thus we were unable to perform the comparison. 3 Related Work There has been much work comparing various join algorithms in both the parallel and single-node cases [7] <ref> [8] </ref> [10]. In general, parallel hybrid hash has been shown to outperform index-nested-loops join in most cases. DeWitt et. al have probably the most comprehensive analysis of parallel, shared-nothing index-nested-loops join [7].
Reference: [9] <author> Keeton, K., D. A. Patterson, and J. M. Hellerstein. </author> <title> The Intelligent Disk (IDISK): A Revolutionary Approach to Database Computing Infrastructure, </title> <type> UCB CS White Paper, </type> <month> March </month> <year> 1998. </year> <note> http://www.cs.berkeley.edu/flkkeeton/Papers/idisk98-draft.ps. </note>
Reference-contexts: throughout this evaluation is to determine which archi-tectural parameters have the most impact on parallel join performance; in particular, we focus on the impact of local memory size on performance, a critical factor for the proposed Istore cluster architecture in which processing and memory are integrated into the disks themselves <ref> [9] </ref>. The rest of this paper is organized as follows. In Section 2, we present background information on the database join primitive and its index-nested-loops variant; Section 3 presents related work. In Sections 4 and 5, we describe our single-node and parallel implementations, respectively, and analyze their performance. <p> and memory-size sensitivity of index-nested-loops join remain relevant for modern systems, or whether they will be applicable to the future's more radical cluster architectures (such as Istore clusters of Intelligent Disks, which have the unusual combination of huge disk capacities, fast networks, large amounts of processing power, but small memories <ref> [9] </ref>). Very few commercial database systems implement a fully-parallel shared-nothing relational database engine, choosing instead to focus on shared-memory, shared-I/O-bus architectures (SMPs and CC-NUMAs). One of the few vendors that has embraced shared-nothing parallel database designs is NCR, with their Teradata system [2].
Reference: [10] <author> Mishra, P., and M. H. Eich. </author> <title> Join Processing in Relational Databases. </title> <journal> ACM Computing Surveys 24, </journal> <month> 1 (March </month> <year> 1992), </year> <month> 63113. </month>
Reference-contexts: Conceptually, the join operation is equivalent to taking a Cartesian product of the two relations (concatenating each tuple of the trst relation with every tuple of the second), then discarding all tuple pairs for which the values of the common join teld dier 1 <ref> [10] </ref>. As an example, consider the following join operation taken from the TPC-D benchmark [11]. <p> With the addition of a second mapping step, our parallel implementation supports such joins as well. 2 2.2 Index-nested-loops Join There are many standard algorithms for implementing joins, including hash joins, sort-merge joins, and index-nested-loops joins <ref> [10] </ref>. In this paper, we focus on one of these algorithms, the index-nested-loops join. To understand index-nested-loops join, we must trst consider a simpler join algorithm, nested-loops join. The nested-loops join algorithm follows directly from the detnition of a join as a Cartesian product followed by a discard. <p> the limited memory case; however, the hash join implementation that we planned to use was not made available to us, and thus we were unable to perform the comparison. 3 Related Work There has been much work comparing various join algorithms in both the parallel and single-node cases [7] [8] <ref> [10] </ref>. In general, parallel hybrid hash has been shown to outperform index-nested-loops join in most cases. DeWitt et. al have probably the most comprehensive analysis of parallel, shared-nothing index-nested-loops join [7].
Reference: [11] <author> Transaction Processing Council. </author> <title> TPC Benchmark TM D (Decision Support) Standard Specitcation Revision 1.3.1. </title> <address> http://www.tpc.org/benchmark_specitcations/TPC_D/131.pdf. </address>
Reference-contexts: As an example, consider the following join operation taken from the TPC-D benchmark <ref> [11] </ref>. <p> of directio (), we found that we could completely control the memory footprint of the join application, thus avoiding inappropriate OS I/O policies and extraneous disk swapping. 4.4.2 Dataset For all of our experiments, we used a synthetic dataset consisting of two relations from the TPC-D decision support benchmark suite <ref> [11] </ref>. The two relations chosen were the Customer and Order relations described above in Section 2.1; the tuple size of the Customer relation is 240 bytes, while the tuple size of the Order relation is 152 bytes.
Reference: [12] <institution> Wal-Mart Taps NCR for Data Warehouse Expansion. </institution> <note> http://www.entmag.com/archive/1997/march19/031914.html. 19 </note>
Reference-contexts: Indeed, one of the largest production data warehouses in existence today (a 24 terabyte data warehouse used by WalMart for decision support) is built upon an MPP architecture with 1024 processors distributed across 128 nodes <ref> [12] </ref>. Cluster architectures are not without their drawbacks, however. In particular, the shared-nothing organization of a cluster (in which data storage, processing, and memory are all partitioned disjointly amongst a set of nodes) presents several challenges to the implementor of a cluster-based parallel database system.
References-found: 12

