URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/kubi-phdthesis.ps.gz
Refering-URL: http://cag-www.lcs.mit.edu/alewife/papers/kubi-phdthesis.html
Root-URL: 
Title: Integrated Shared-Memory and Message-Passing Communication in the Alewife Multiprocessor  
Author: by John David Kubiatowicz Anant Agarwal Arthur C. Smith 
Degree: (1993) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  All rights reserved. Signature of Author  Certified by  Associate Professor of Computer Science and Electrical Engineering Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Students  
Date: February 1998  December 15, 1997  
Address: (1987)  
Affiliation: S.B., Massachusetts Institute of Technology  S.M., Massachusetts Institute of Technology  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology, 1998.  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak Ordering ANew Definition. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 214, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: User-Direct Messaging Page 59 send (header, handler, op2, op3, [addr0:len0], [addr1:len1]) ) stio header, output descriptor [0] ; Place header in descriptor stio handler, output descriptor <ref> [1] </ref> ; Place handler in descriptor stio op2, output descriptor [2] ; op2 and op3 are additional operands stio op3, output descriptor [3] ; stio addr0, output descriptor [4] ; addr0 and len0 describe first DMA stio len0, output descriptor [5] ; stio addr1, output descriptor [6] ; addr1 and len1 <p> user nonblocking OP ldio space avail, temp ; Get space available cmp temp, 8 ; We need 4 double-words bl,a %Failure ; Not enough space failure and flags, $NONBLOCK, flags ; Clear flag (only on failure). stio header, output descriptor [0] ; Place header in descriptor stio handler, output descriptor <ref> [1] </ref> ; Place handler in descriptor stio addr0, output descriptor [2] ; addr0 and len0 describe first DMA stio len0, output descriptor [2] ; ipilaunch &lt; 1, 2 &gt; ; Launch 1 explicit double-word and ; 1 memory block via DMA and flags, $NONBLOCK, flags ; Clear flag (end of sequence). <p> For instance, message arrival interrupts are posted immediately upon arrival of the first words of messages. receive ())(header, handler, op2, op3,[addr0:len0],[addr1:len1]) ldio input window [0], header ; Get header from packet ldio input window <ref> [1] </ref>, handler ; Get handler from packet ldio input window [2], op2 ; op2 and op3 are additional operands ldio input window [3], op3 ; stio addr0, storeback address ; Set up first DMA address ipicst &lt; 2, len0 &gt; ; Skip 2 double-words, store len0 double-words to memory stio addr1, <p> This may be exploited to provide a serializing wait for storeback operation that will wait until all previous storeback operations have completed. peek ())(header, handler, op2, op3) ldio input window [0], header ; Get header from packet ldio input window <ref> [1] </ref>, handler ; Get handler from packet ldio input window [2], op2 ; op2 and op3 are additional operands ldio input window [3], op3 ; Implementation of the peek operation: As shown above, in Figure 2-11, the peek operation is a natural consequence of our input interface. <p> Applying basic pipelining ideas, resource utilization can be improved by allowing a processor to transmit more than one memory request at a time. Multiple outstanding transactions can be supported using software prefetch [20, 85], multithreading via rapid context-switching [115, 5], or weak ordering <ref> [1] </ref>. Studies have shown that the utilization of the network, processor, and memory systems can be improved almost in proportion to the number of outstanding transactions allowed [63, 50]. Multithreading may be implemented with either polling or signaling mechanisms. Polling involves retrying memory requests until they are satisfied.
Reference: [2] <author> Sarita V. Adve, Mark D. Hill, Barton P. Miller, and Robert H.B. Netzer. </author> <title> Detecting Data Races on Weak Memory Systems. </title> <booktitle> In Proceedings of the 18th Anual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: Over the years, researchers have developed a number of ways of programming and reasoning about machines that have consistency models that are weaker than sequential consistency <ref> [2, 42] </ref>. Further, one of the justifications for weaker memory models is that they provide better latency tolerance (under some circumstances) than sequential consistency. As a result, the arguments in Page 46 CHAPTER 2. <p> User-Direct Messaging Page 59 send (header, handler, op2, op3, [addr0:len0], [addr1:len1]) ) stio header, output descriptor [0] ; Place header in descriptor stio handler, output descriptor [1] ; Place handler in descriptor stio op2, output descriptor <ref> [2] </ref> ; op2 and op3 are additional operands stio op3, output descriptor [3] ; stio addr0, output descriptor [4] ; addr0 and len0 describe first DMA stio len0, output descriptor [5] ; stio addr1, output descriptor [6] ; addr1 and len1 describe second DMA stio len1, output descriptor [7] ; ipilaunch <p> available cmp temp, 8 ; We need 4 double-words bl,a %Failure ; Not enough space failure and flags, $NONBLOCK, flags ; Clear flag (only on failure). stio header, output descriptor [0] ; Place header in descriptor stio handler, output descriptor [1] ; Place handler in descriptor stio addr0, output descriptor <ref> [2] </ref> ; addr0 and len0 describe first DMA stio len0, output descriptor [2] ; ipilaunch &lt; 1, 2 &gt; ; Launch 1 explicit double-word and ; 1 memory block via DMA and flags, $NONBLOCK, flags ; Clear flag (end of sequence). restartable sequence that may be restarted to the beginning. if <p> Not enough space failure and flags, $NONBLOCK, flags ; Clear flag (only on failure). stio header, output descriptor [0] ; Place header in descriptor stio handler, output descriptor [1] ; Place handler in descriptor stio addr0, output descriptor <ref> [2] </ref> ; addr0 and len0 describe first DMA stio len0, output descriptor [2] ; ipilaunch &lt; 1, 2 &gt; ; Launch 1 explicit double-word and ; 1 memory block via DMA and flags, $NONBLOCK, flags ; Clear flag (end of sequence). restartable sequence that may be restarted to the beginning. if sufficient descriptor space is available. <p> For instance, message arrival interrupts are posted immediately upon arrival of the first words of messages. receive ())(header, handler, op2, op3,[addr0:len0],[addr1:len1]) ldio input window [0], header ; Get header from packet ldio input window [1], handler ; Get handler from packet ldio input window <ref> [2] </ref>, op2 ; op2 and op3 are additional operands ldio input window [3], op3 ; stio addr0, storeback address ; Set up first DMA address ipicst &lt; 2, len0 &gt; ; Skip 2 double-words, store len0 double-words to memory stio addr1, storeback address ; Set up second DMA address ipicst &lt; <p> This may be exploited to provide a serializing wait for storeback operation that will wait until all previous storeback operations have completed. peek ())(header, handler, op2, op3) ldio input window [0], header ; Get header from packet ldio input window [1], handler ; Get handler from packet ldio input window <ref> [2] </ref>, op2 ; op2 and op3 are additional operands ldio input window [3], op3 ; Implementation of the peek operation: As shown above, in Figure 2-11, the peek operation is a natural consequence of our input interface.
Reference: [3] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk Johnson, David Kranz, John Ku-biatowicz, Beng-Hong Lim, Kenneth Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 213, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: User-Direct Messaging Page 59 send (header, handler, op2, op3, [addr0:len0], [addr1:len1]) ) stio header, output descriptor [0] ; Place header in descriptor stio handler, output descriptor [1] ; Place handler in descriptor stio op2, output descriptor [2] ; op2 and op3 are additional operands stio op3, output descriptor <ref> [3] </ref> ; stio addr0, output descriptor [4] ; addr0 and len0 describe first DMA stio len0, output descriptor [5] ; stio addr1, output descriptor [6] ; addr1 and len1 describe second DMA stio len1, output descriptor [7] ; ipilaunch &lt; 2, 4 &gt; ; Launch 2 explicit double-words and ; 2 <p> posted immediately upon arrival of the first words of messages. receive ())(header, handler, op2, op3,[addr0:len0],[addr1:len1]) ldio input window [0], header ; Get header from packet ldio input window [1], handler ; Get handler from packet ldio input window [2], op2 ; op2 and op3 are additional operands ldio input window <ref> [3] </ref>, op3 ; stio addr0, storeback address ; Set up first DMA address ipicst &lt; 2, len0 &gt; ; Skip 2 double-words, store len0 double-words to memory stio addr1, storeback address ; Set up second DMA address ipicst &lt; 0, -1 &gt; ; Store rest of packet via DMA value of <p> that will wait until all previous storeback operations have completed. peek ())(header, handler, op2, op3) ldio input window [0], header ; Get header from packet ldio input window [1], handler ; Get handler from packet ldio input window [2], op2 ; op2 and op3 are additional operands ldio input window <ref> [3] </ref>, op3 ; Implementation of the peek operation: As shown above, in Figure 2-11, the peek operation is a natural consequence of our input interface. Essentially, the head of the next message may be examined simply by loading data from the input window. <p> In the following paragraphs, we will characterize the raw performance of the Alewife network, as well as the uncontended performance of shared-memory and message-passing communication. Kirk Johnson was instrumental in many of these measurements. Also, many of these numbers have been published elsewhere <ref> [3, 16, 60] </ref>. 6.1.1 Performance of the Network In this section, we perform a characterization of the network performance as a whole. <p> Second, we would like to perform a few comparisons between the same program written in shared-memory and message-passing styles. Many of these numbers have been published elsewhere <ref> [3, 28] </ref>, hence this is a summary of the results. One of the key contributers to the numbers presented in Section 6.2.1 was Ricardo Bianchini, who single-handedly ported a large number of the SPLASH [101] benchmarks. <p> Fredric Chong was instrumental in performing explorations of message-passing applications, as well as explorations of the tradeoffs between shared-memory and message-passing performance (see also Chong's PhD thesis [27]). 6.2.1 Performance of Shared-Memory Programs As reported in <ref> [3] </ref>, shared-memory programs perform well on Alewife, proving the viability of the overall memory controller architecture, the shared-memory interface, and the two-case delivery mechanism; the operating-system resulted from the combined effort of a number of individual researchers (David Kranz, David Chaiken, Dan Nussbaum, and Beng-Hong Lim, in addition to this author, <p> In this section, we would like to mention and examine some of these other research projects. 7.3.1 Hardware Integration of Communication Models Recent architectures demonstrate emerging agreement that it is important to integrate message-passing and shared-memory communication in the same hardware platform <ref> [3, 45, 93, 99, 40] </ref>. This current interest began, perhaps, with an Alewife paper on the advantages of integration [58]. However, other machines, such as the BB&N-Butterfly [17] supported integrated shared-memory and message-passing communication (via bulk DMA) long before this. <p> A polling watchdog mode could be implemented in Alewife, if so desired. Direct Network Interfaces: Several machines have provided direct network interfaces. These include the CM-5, the J-machine, iWarp, the *T interface, Alewife, and Wisconsin's CNI <ref> [68, 35, 13, 89, 3, 86] </ref>. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating 7.3.
Reference: [4] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godfrey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <journal> IEEE Micro, </journal> <volume> 13(3):4861, </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: send (header, handler, op2, op3, [addr0:len0], [addr1:len1]) ) stio header, output descriptor [0] ; Place header in descriptor stio handler, output descriptor [1] ; Place handler in descriptor stio op2, output descriptor [2] ; op2 and op3 are additional operands stio op3, output descriptor [3] ; stio addr0, output descriptor <ref> [4] </ref> ; addr0 and len0 describe first DMA stio len0, output descriptor [5] ; stio addr1, output descriptor [6] ; addr1 and len1 describe second DMA stio len1, output descriptor [7] ; ipilaunch &lt; 2, 4 &gt; ; Launch 2 explicit double-words and ; 2 memory blocks via DMA word) array <p> The destination addresses for these stio instructions are small immediate addresses in the CMMU register space (i.e. output descriptor <ref> [4] </ref> represents a particular CMMU register). The resulting descriptor consists of zero or more pairs of explicit operands, followed by zero or more address-length pairs. The address-length pairs describe blocks of data which will be fetched from memory via DMA. <p> The scalar operands are loaded directly from the input window into registers. The scalar values are accessed via ldio instructions. The source addresses for these instructions are small immediate addresses in the CMMU register space (i.e. input window <ref> [4] </ref> represents a particular CMMU register). After loading the operands, the processor initiates a DMA storeback operation to extract the message from the network. To do this, it first writes the address for the beginning of the DMA operation to a CMMU register called storeback address. <p> As mentioned above, Sparcle is best thought of as a standard RISC microprocessor with extensions for multiprocessing <ref> [4] </ref>. One of the aspects of Sparcle that was exploited in Alewife was the fact that it had no on-chip caches, something which is unheard of today. In this sense, Sparcle represents microprocessor implementation technology from two or three generations ago.
Reference: [5] <author> Anant Agarwal, Beng-Hong Lim, David A. Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104114, </pages> <address> Seattle, WA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Although the actual implementation of featherweight threads will be discussed in more detail in the second part of the thesis, we will start with This diagram shows a block-multithreaded processor <ref> [5] </ref> which contains four separate register sets with associated program counters and status registers. Each register set is a hardware context which can hold one active thread. <p> In Alewife, this is done by incorporating synchronization constructs directly into the cache/memory system: each memory word (of 32 bits) has an additional bit (called a full/empty bit <ref> [102, 5] </ref>) that is employed for synchronization. These full/empty bits are stored in memory and cached along with other data. To access these bits, Alewife attaches a synchronization operation field to each load and store operation. <p> [0] ; Place header in descriptor stio handler, output descriptor [1] ; Place handler in descriptor stio op2, output descriptor [2] ; op2 and op3 are additional operands stio op3, output descriptor [3] ; stio addr0, output descriptor [4] ; addr0 and len0 describe first DMA stio len0, output descriptor <ref> [5] </ref> ; stio addr1, output descriptor [6] ; addr1 and len1 describe second DMA stio len1, output descriptor [7] ; ipilaunch &lt; 2, 4 &gt; ; Launch 2 explicit double-words and ; 2 memory blocks via DMA word) array of registers on the CMMU. <p> Applying basic pipelining ideas, resource utilization can be improved by allowing a processor to transmit more than one memory request at a time. Multiple outstanding transactions can be supported using software prefetch [20, 85], multithreading via rapid context-switching <ref> [115, 5] </ref>, or weak ordering [1]. Studies have shown that the utilization of the network, processor, and memory systems can be improved almost in proportion to the number of outstanding transactions allowed [63, 50]. Multithreading may be implemented with either polling or signaling mechanisms.
Reference: [6] <author> Gail Alverson, Robert Alverson, and David Callahan. </author> <title> Exploiting Heterogeneous Parallelism on a Multithreaded Multiprocessor. </title> <booktitle> In Workshop on Multithreaded Computers, Proceedings of Supercomputing '91. ACM Sigraph & IEEE, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: stio handler, output descriptor [1] ; Place handler in descriptor stio op2, output descriptor [2] ; op2 and op3 are additional operands stio op3, output descriptor [3] ; stio addr0, output descriptor [4] ; addr0 and len0 describe first DMA stio len0, output descriptor [5] ; stio addr1, output descriptor <ref> [6] </ref> ; addr1 and len1 describe second DMA stio len1, output descriptor [7] ; ipilaunch &lt; 2, 4 &gt; ; Launch 2 explicit double-words and ; 2 memory blocks via DMA word) array of registers on the CMMU. <p> A few architectures incorporate multiple contexts, pioneered by the HEP [102], switching on every instruction. These machines, including Monsoon [88] and Tera <ref> [6] </ref>, do not have caches and rely on a large number of contexts to hide remote memory latency. In contrast, Alewife's block multithreading technique switches only on synchronization faults and cache misses to re Page 228 CHAPTER 7.
Reference: [7] <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of SIGPLAN '93 Conference on Programming Languages Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: output descriptor [2] ; op2 and op3 are additional operands stio op3, output descriptor [3] ; stio addr0, output descriptor [4] ; addr0 and len0 describe first DMA stio len0, output descriptor [5] ; stio addr1, output descriptor [6] ; addr1 and len1 describe second DMA stio len1, output descriptor <ref> [7] </ref> ; ipilaunch &lt; 2, 4 &gt; ; Launch 2 explicit double-words and ; 2 memory blocks via DMA word) array of registers on the CMMU.
Reference: [8] <author> Kazuhiro Aoyama and Andrew A. Chien. </author> <title> The Cost of Adaptivity and Virtual Lanes in a Wormhole Router. </title> <journal> Journal of VLSI Design, </journal> <volume> 2(4):315333, </volume> <year> 1993. </year>
Reference-contexts: As with all hardware features, virtual channels have a design and performance cost that grows with the number of channels <ref> [26, 8] </ref>. Each virtual channel requires a separate input and/or output queue on each network port. Further, viewing the set of hardware virtual channels as a scarce resource, there are a number of ways that we might want to allocate them other than for deadlock avoidance of the cache-coherence protocol.
Reference: [9] <author> Remzi H. Arpaci, David E. Culler, Arvind Krishnamurthy, Steve G. Steinberg, and Kather-ine Yelick. </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 320331, </pages> <month> June </month> <year> 1995. </year> <note> Page 237 Page 238 BIBLIOGRAPHY </note>
Reference-contexts: Increased integration of computer systems and the mainstreaming of parallel processing challenges both of these assumptions: on-chip network interfaces can have low overhead and parallel programs frequently require coordinated scheduling for predictable, low latencies <ref> [9] </ref>. Alewife provides low latency for applications where latency matters while including low-cost and reasonably efficient buffering as a fallback mode. Page 230 CHAPTER 7. ALL GOOD THINGS : : : 7.3.4 Two-Case Delivery and Deadlock The notion of two-case delivery has appeared in various guises over the years.
Reference: [10] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. </author> <title> Adaptive Software Cache Management for Distributed Shared Memory Architectures. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> June </month> <year> 1990. </year> <note> IEEE. </note>
Reference: [11] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference-contexts: However, when introduce associative locking (and move the locus of locking away from the cache), this additional state is far less prohibitive. Deadlock Problems: Unfortunately, the basic locking mechanism can lead to four distinct types of deadlock, illustrated in Figure 3-10. This figure contains four different waits-for graphs <ref> [11] </ref>, which represent dependencies between transactions. In these graphs, the large italic letters represent transactions: D for data transactions and I for instruction transactions. The superscripts either P or S represent primary or secondary transactions, respectively.
Reference: [12] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathan Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multi-computer. </title> <booktitle> In Proceedings 21st Annual International Symposium on Computer Architecture (ISCA'94), </booktitle> <pages> pages 142153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The *T NI [89] would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine [40] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces: Memory-based interfaces in multicomputers <ref> [12, 18, 97, 99, 104] </ref> and workstations [36, 37, 108, 112] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers.
Reference: [13] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H.T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Susman, J. Sutton, J. Urbanski, and J. Webb. </author> <title> Supporting Systolic and Memory Communication in iWarp. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 7081, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: A polling watchdog mode could be implemented in Alewife, if so desired. Direct Network Interfaces: Several machines have provided direct network interfaces. These include the CM-5, the J-machine, iWarp, the *T interface, Alewife, and Wisconsin's CNI <ref> [68, 35, 13, 89, 3, 86] </ref>. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating 7.3.
Reference: [14] <author> Shekhar Borkar et al. </author> <title> iWarp: An Integrated Solution to High-Speed Parallel Computing. </title> <booktitle> In Proceedings of Supercomputing '88, </booktitle> <month> November </month> <year> 1988. </year>
Reference-contexts: Thus, efficient messaging facilities should permit direct transfer of information from registers to the network interface. Direct register-to-register transmission has been suggested by a number of architects <ref> [14, 35, 113, 46] </ref>. 2. Blocks of data which reside in memory often accompany such header information. Thus, efficient messaging facilities should allow direct memory access (DMA) mechanisms to be invoked inexpensively, possibly on multiple blocks of data. <p> Here, the messaging overheads are low enough that a thin layer of interrupt-driven operating-system software, can synthesize arbi trary network queueing structures in software. 4. Permitting compilers (or users) to generate network communications code has a number of advantages, as discussed in [113], <ref> [14] </ref>, and [47]. Compiler-generated code, however, requires user-level access to the message interface, including access to some form of atomicity mechanism to control the arrival of message interrupts. Given these observations, we can proceed to define a message-passing communication model for Alewife.
Reference: [15] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl. PROTEUS: </author> <title> A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Although simulation technology has improved much in the last decade, it is still not entirely suited to the execution of large problems running on top of a complete operating system with all of the real effects present in a real machine. Execution-driven simulators, such as SimOS [95], Proteus <ref> [15] </ref>, and others have made the greatest strides in that direction.
Reference: [16] <author> Eric Brewer, Fred Chong, Lok Liu, Shamik Sharma, and John Kubiatowicz. </author> <title> Remote Queues: Exposing Message Queues for Optimization and Atomicity. </title> <booktitle> In Proceedings of the Symposium on Parallel Algorithms and Architectures (SPAA'95). ACM, </booktitle> <year> 1995. </year>
Reference-contexts: Register Description output descriptor <ref> [16] </ref> Output descriptor array space available Output descriptor space available space request Output descriptor space requested desc length Current descriptor length traps pending Pending interrupts and flags input window [16] Input packet array window length Size of message input window storeback address Address for next DMA storeback uatomctrl User atomicity control <p> Register Description output descriptor <ref> [16] </ref> Output descriptor array space available Output descriptor space available space request Output descriptor space requested desc length Current descriptor length traps pending Pending interrupts and flags input window [16] Input packet array window length Size of message input window storeback address Address for next DMA storeback uatomctrl User atomicity control (4 bits) overflow timeout Network overflow count in cycles overflow countdown Cycles remaining before overflow atomicity timeout Atomicity timeout in cycles. atomicity countdown Cycles remaining before timeout Table 2-2: <p> In the following paragraphs, we will characterize the raw performance of the Alewife network, as well as the uncontended performance of shared-memory and message-passing communication. Kirk Johnson was instrumental in many of these measurements. Also, many of these numbers have been published elsewhere <ref> [3, 16, 60] </ref>. 6.1.1 Performance of the Network In this section, we perform a characterization of the network performance as a whole. <p> The User-Direct Messaging model and interfaces build on previous work in messaging models and mechanisms. Messaging Models: The User-Direct Messaging (UDM) model is similar to Active Messages [113] and related to Remote Queues (RQ) <ref> [16] </ref> as an efficient building-block for messaging within a protection domain. User-Direct Messaging differs from Active Messages [113] in that it includes explicit control over message delivery for efficiency; thus, for instance, users may choose to receive messages via polling or via interrupts.
Reference: [17] <institution> Inside the Butterfly Plus. BB&N Advanced Computers Inc., </institution> <year> 1987. </year>
Reference-contexts: This current interest began, perhaps, with an Alewife paper on the advantages of integration [58]. However, other machines, such as the BB&N-Butterfly <ref> [17] </ref> supported integrated shared-memory and message-passing communication (via bulk DMA) long before this.
Reference: [18] <author> Greg Buzzard, David Jacobson, Milon Mackey, Scott Marovich, and John Wilkes. </author> <title> An Implementation of the Hamlyn Sender-Managed Interface Architecture. </title> <booktitle> In Proceedings of the Second Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 245259, </pages> <year> 1996. </year>
Reference-contexts: The *T NI [89] would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine [40] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces: Memory-based interfaces in multicomputers <ref> [12, 18, 97, 99, 104] </ref> and workstations [36, 37, 108, 112] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers.
Reference: [19] <author> David Callahan and Ken Kennedy. </author> <title> Compiling Programs for Distributed-Memory Multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <pages> 2(151-169), </pages> <month> October </month> <year> 1988. </year>
Reference: [20] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <title> Software Prefetching. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 4052. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: In either case, many processor cycles may be lost waiting for a response. Applying basic pipelining ideas, resource utilization can be improved by allowing a processor to transmit more than one memory request at a time. Multiple outstanding transactions can be supported using software prefetch <ref> [20, 85] </ref>, multithreading via rapid context-switching [115, 5], or weak ordering [1]. Studies have shown that the utilization of the network, processor, and memory systems can be improved almost in proportion to the number of outstanding transactions allowed [63, 50]. Multithreading may be implemented with either polling or signaling mechanisms.
Reference: [21] <author> David Chaiken. </author> <title> Smart Memory Systems. </title> <journal> CMG Transactions, </journal> <pages> pages 2332, </pages> <month> Winter </month> <year> 1993. </year>
Reference-contexts: Hence, the memory system and network must correctly support many simultaneous requests. Fourth, the boundaries between shared memory and message passing can be explicitly crossed when hardware events are handled by software (such as for the LimitLESS coherence protocol <ref> [23, 21] </ref>); this can introduce another type of simultaneity in which cache-coherence structures are access by both hardware and software. Hence, any solution to the Service-Interleaving Problem involves imposing order on this chaos. In particular, two requirements must be met: 1. All communication operations must complete eventually. 2.
Reference: [22] <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal. </author> <title> Directory-Based Cache-Coherence in Large-Scale Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6):4158, </volume> <month> June </month> <year> 1990. </year> <note> BIBLIOGRAPHY Page 239 </note>
Reference-contexts: First, and foremost, studies of the properties of a number of parallel programs suggest that the worker-set or number of processors simultaneously sharing a given piece of data is usually small <ref> [22] </ref>. Hence, SCI optimizes for the uncommon case rather than the common case pattern of sharing. Further, since it forms linear chains of data sharers, it risks long latencies during invalidation. Attempts to correct latency problems have only lead to more complexity [55, 54].
Reference: [23] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: We have already mentioned that Alewife provides a sequentially-consistent shared-memory space. What is interesting about Alewife's shared memory is that it is partially implemented in software <ref> [23] </ref>. The basic philosophy espoused by the Alewife coherence protocol mirrors that of many other aspects of Alewife: implementation of common cases in hardware and exceptional cases in software. <p> Hence, the memory system and network must correctly support many simultaneous requests. Fourth, the boundaries between shared memory and message passing can be explicitly crossed when hardware events are handled by software (such as for the LimitLESS coherence protocol <ref> [23, 21] </ref>); this can introduce another type of simultaneity in which cache-coherence structures are access by both hardware and software. Hence, any solution to the Service-Interleaving Problem involves imposing order on this chaos. In particular, two requirements must be met: 1. All communication operations must complete eventually. 2. <p> That is, after a first-time fetch of data from a remote node, subsequent accesses of the data are satisfied entirely within the node. The resulting cache coherence problem can be solved using a variety of directory based schemes <ref> [49, 69, 23] </ref>. Unfortunately, even with coherent caches, the cost of remote memory actions can be prohibitive. To fetch data through the interconnection network, the processor transmits a request, then waits for a response. <p> An alternative way of avoiding deadlock during server-interlock periods is to discard excess requests by sending NAK messages back to the requestor. DASH did this [69], Alewife did this <ref> [23] </ref>, and others have done this. While this removes a large amount of complexity due to queueing, it forces requesting nodes to retry their requests, thereby leading to the possibility of livelock 8 . <p> Page 114 CHAPTER 3. THE SERVICE-INTERLEAVING PROBLEM 3.4.2.1 Directory Interlocks In the LimitLESS cache-coherence protocol, some corner-case operations are handled by forwarding them from hardware to software <ref> [23] </ref>. Since software handlers take nearly an order of magnitude longer to process requests than the hardware does 14 , some amount of decoupling is desirable.
Reference: [24] <author> David L. Chaiken. </author> <title> Mechanisms and Interfaces for Software-Extended Coherent Shared Memory. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <year> 1994. </year> <note> MIT/LCS TR-644. </note>
Reference-contexts: Although the active set of software exception handlers may be selected on behalf of a user to improve performance (see, for instance <ref> [24] </ref>), they are always under control of the operating system and, ultimately, transparent to the user. As a consequence, we do not consider the existence of LimitLESS cache-coherence to be a part of the Latency-Tolerant Shared-Memory Model. <p> This simple behavior is of great value for ameliorating the occasional long access latency caused by LimitLESS cache coherence (see Chaiken <ref> [24] </ref>). Finally, special uncached loads and stores can access data directly in the transaction buffer without first transferring it to the primary cache. <p> We will simply summarize these results here and forward the reader to the corresponding references. David Chaiken made extensive use of small and fast messages to complete and enhance the 6.2. Macrobenchmarks and the Alewife Prototype Page 205 Alewife cache coherence protocol <ref> [24] </ref>. The speed of the message-passing interface is evidenced by the fact that fact that Alewife shared memory performs well across a range of applications (previous section). Thus, the success of LimitLESS provides insight into the speed of message-passing mechanisms.
Reference: [25] <author> David Lars Chaiken. </author> <title> Cache Coherence Protocols for Large-Scale Multiprocessors. </title> <type> Technical Report MIT-LCS-TM-489, </type> <institution> Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Once thrashing has been detected, the thrashwait algorithm requests the data for a second time and disables context-switching, causing the processor to wait for the data to arrive. 5 Adapted from Chaiken <ref> [25] </ref>. The pseudo-code notation is borrowed from [30]. Page 102 CHAPTER 3. THE SERVICE-INTERLEAVING PROBLEM Multiple Primary Transactions: Systems requiring two primary transactions can be accommodated by providing two tried-once bits per context, one for instructions and the other for data. Only a single thrash-wait bit is required. <p> For simplicity, the Valid field is shown with two values, VALID and INVALID. The transient states of ARRIVING and HALF VALID are logically grouped with VALID. 19 This message is a negative acknowledgment on the request. See <ref> [25] </ref> for more information. 5.2. The Communications and Memory Management Unit Page 169 Buffer State Note (s) TIP F INV TT Val FM Description A, G 0 0 0 INVALID NORMAL Empty and available for allocation.
Reference: [26] <author> Andrew A. Chien. </author> <title> A Cost and Speed Model for k-ary n-cube Wormhole Routers. In Hot Interconnects, </title> <year> 1993. </year>
Reference-contexts: As with all hardware features, virtual channels have a design and performance cost that grows with the number of channels <ref> [26, 8] </ref>. Each virtual channel requires a separate input and/or output queue on each network port. Further, viewing the set of hardware virtual channels as a scarce resource, there are a number of ways that we might want to allocate them other than for deadlock avoidance of the cache-coherence protocol.
Reference: [27] <author> Fredric T. Chong. </author> <title> Parallel Communication Mechanisms for Sparse, Irregular Applications. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: Fredric Chong was instrumental in performing explorations of message-passing applications, as well as explorations of the tradeoffs between shared-memory and message-passing performance (see also Chong's PhD thesis <ref> [27] </ref>). 6.2.1 Performance of Shared-Memory Programs As reported in [3], shared-memory programs perform well on Alewife, proving the viability of the overall memory controller architecture, the shared-memory interface, and the two-case delivery mechanism; the operating-system resulted from the combined effort of a number of individual researchers (David Kranz, David Chaiken, Dan <p> Finally, Fred Chong wrote a number of fine-grained message-passing applications for Alewife. Among other things, he explored sparse-matrix kernels, achieving speedups in message-passing versions on Alewife that surpassed the best known speedups at the time. These explorations are shown in greater detail in <ref> [27] </ref>. <p> We will restrict our attention to the characteristics of one application (EM3D). This is joint work with Fred Chong (who took most of the actual numbers). See <ref> [28, 27] </ref> for more details and other examples. EM3D from Berkeley models the propagation of electromagnetic waves through three-dimensional objects [80]. It implicit red-black computation on an irregular bipartite graph. EM3D is iterative and is barrier-synchronized between two phases.
Reference: [28] <author> Fredric T. Chong, Rajeev Barua, Fredrik Dahlgren, John Kubiatowicz, and Anant Agarwal. </author> <title> The Sensitivity of Communicaton Mechanisms to Bandwidth and Latency. </title> <booktitle> In Proceedings of the Fourth Annual Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: Second, we would like to perform a few comparisons between the same program written in shared-memory and message-passing styles. Many of these numbers have been published elsewhere <ref> [3, 28] </ref>, hence this is a summary of the results. One of the key contributers to the numbers presented in Section 6.2.1 was Ricardo Bianchini, who single-handedly ported a large number of the SPLASH [101] benchmarks. <p> We will restrict our attention to the characteristics of one application (EM3D). This is joint work with Fred Chong (who took most of the actual numbers). See <ref> [28, 27] </ref> for more details and other examples. EM3D from Berkeley models the propagation of electromagnetic waves through three-dimensional objects [80]. It implicit red-black computation on an irregular bipartite graph. EM3D is iterative and is barrier-synchronized between two phases. <p> It implicit red-black computation on an irregular bipartite graph. EM3D is iterative and is barrier-synchronized between two phases. Communication takes place because data updated within one phase is used by other nodes in the subsequent phase. In <ref> [28] </ref>, EM3D is written in five different versions (three message-passing and two shared-memory versions): 1. Fine-grained message-passing with interrupt-driven active messages (int-mp). This version communicates five double-words at a time. 2. Fine-grained message-passing with polling (poll-mp). This version also commu nicates five double-words at a time. 3. Bulk-transfer message-passing (bulk-mp). <p> This means that we can derive simple experiments to show differences in the fundamental behavior of the mechanisms themselves. Such experiments are reported in detail in <ref> [28] </ref>, but we will show samples of them for EM3D. across the bisection of a 32-node Alewife machine. The unrestricted number is approximately 18 bytes/processor cycle across the bisection (which is why the axis is at 18). Numbers to the left of this represent restricted bandwidth. <p> This was seen in Section 6.2.3 which summarized some of the results of <ref> [28] </ref>. The Stanford FLASH multiprocessor [64] takes the specialized coprocessor approach to providing a plethora of mechanisms. As a later generation project, FLASH tackled a number of multiuser and reliability issues that were not explored by Alewife (although multiuser issues were addressed in FUGU [77, 78], a related project). <p> Very few (if any) existing machines provide the level of parity between communication mechanisms that Alewife provides. Thus, studies of the inherent cost of different communication models on physical hardware are scarce. None-the-less, the Alewife experiments in Section 6.2.3 (and in <ref> [28] </ref>) were strongly influenced by studies from Wisconsin, Stanford, and Maryland. Our comparison of communication mechanisms is similar to Chandra, Larus and Rogers [96], although we have available a larger set of mechanisms and we generalize to a range of system parameters.
Reference: [29] <author> Douglas W. Clark. </author> <title> Large-Scale Hardware Simulation: Modeling and Verification Strategies. </title> <booktitle> In Proceedings of the 25th Anniversary Symposium, </booktitle> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> September </month> <year> 1990. </year> <institution> Carnegie Mellon University. </institution>
Reference-contexts: As a result, all of the Alewife test vectors consisted of code running on the Sparcle processor. Much of the Alewife testing effort was inspired by a paper by Douglas Clark <ref> [29] </ref>. In this paper, he makes a convincing argument for large-scale, complete-system testing. He argues that, since the interfaces between modules in a large system are as likely to be flawed as the modules themselves, time spent testing modules independently is not necessarily the best use of resources.
Reference: [30] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Once thrashing has been detected, the thrashwait algorithm requests the data for a second time and disables context-switching, causing the processor to wait for the data to arrive. 5 Adapted from Chaiken [25]. The pseudo-code notation is borrowed from <ref> [30] </ref>. Page 102 CHAPTER 3. THE SERVICE-INTERLEAVING PROBLEM Multiple Primary Transactions: Systems requiring two primary transactions can be accommodated by providing two tried-once bits per context, one for instructions and the other for data. Only a single thrash-wait bit is required.
Reference: [31] <author> William J. Dally. </author> <title> Virtual-channel Flow Control. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(5):194205, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: The second reason, however, results from an explicit design decision: the use of a single logical network channel. If shared-memory and message-passing network traffic were completely decoupled, through use of multiple networks or virtual channels <ref> [31] </ref>, then dependencies such as shown in Figure 3-1 would not occur: the shared-memory responses would simply bypass the messages, unblocking the pipeline, and hence avoiding deadlock. As with similar deadlocks, we have two components: finite resources (processor load buffers) and interlocking dependencies (interference between shared memory and message passing). <p> Given this topology, no cycles remain in the communication graph; hence, the protocol processor (responsible for the transformation REQ)DATA) is never blocked indefinitely. Although DASH used physically separate networks (with independent wires), virtual channels <ref> [31] </ref> represent a more practical implementation methodology. For the remainder of this discussion, we will use the term logical channel to denote an independent path between nodes; we will have a bit more to say about implementation later. <p> For instance, virtual channels can be used to smooth out the performance of wormhole-routed mesh networks <ref> [31] </ref> and to achieve deadlock freedom in adaptive networks [32]. These particular uses of virtual channels interact multiplicatively with the deadlock avoidance methods that we have been discussing here.
Reference: [32] <author> William J. Dally and Hiromichi Aoki. </author> <title> Deadlock-Free Adaptive Routing in Multicomputer Networks Using Virtual Channels. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(4):466475, </volume> <month> April </month> <year> 1993. </year>
Reference-contexts: We might, for instance, always send DATA messages on a particular set of logical channels that had higher available bandwidth. This approach shares many details with methods for deadlock-free routing; see, for instance, Dally <ref> [33, 32] </ref>. Page 124 CHAPTER 4. THE PROTOCOL DEADLOCK PROBLEM process of decoupling, we must break the network dependencies P x )M x . <p> For instance, virtual channels can be used to smooth out the performance of wormhole-routed mesh networks [31] and to achieve deadlock freedom in adaptive networks <ref> [32] </ref>. These particular uses of virtual channels interact multiplicatively with the deadlock avoidance methods that we have been discussing here.
Reference: [33] <author> William J. Dally and Charles L. Seitz. </author> <title> Deadlock-Free Message Routing in Multiprocessor Interconnection Networks. </title> <journal> IEEE Transactions on Computing, </journal> <volume> C-36(5):547553, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: We might, for instance, always send DATA messages on a particular set of logical channels that had higher available bandwidth. This approach shares many details with methods for deadlock-free routing; see, for instance, Dally <ref> [33, 32] </ref>. Page 124 CHAPTER 4. THE PROTOCOL DEADLOCK PROBLEM process of decoupling, we must break the network dependencies P x )M x .
Reference: [34] <author> W. J. Dally et al. </author> <title> Architecture of a Message-Driven Processor. </title> <booktitle> In Proceedings of the 14th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 189196, </pages> <address> Washington, D.C., </address> <month> June </month> <year> 1987. </year> <note> IEEE. </note>
Reference-contexts: In this process we follow in the footsteps of of people such as Bill Dally who have espoused the view that communication should be viewed as being as fundamental as addition and multiplication <ref> [34] </ref>. This is our solution to challenge of user-level access. The result of this chapter may be summarized by Figure 2-14, which reproduces the middle layers of Figure 1-3 (page 27).
Reference: [35] <author> William J. Dally et al. </author> <title> The J-Machine: A Fine-Grain Concurrent Computer. </title> <booktitle> In Proceedings of the IFIP (International Federation for Information Processing), 11th World Congress, </booktitle> <pages> pages 11471153, </pages> <address> New York, 1989. </address> <publisher> Elsevier Science Publishing. </publisher> <pages> Page 240 BIBLIOGRAPHY </pages>
Reference-contexts: Thus, efficient messaging facilities should permit direct transfer of information from registers to the network interface. Direct register-to-register transmission has been suggested by a number of architects <ref> [14, 35, 113, 46] </ref>. 2. Blocks of data which reside in memory often accompany such header information. Thus, efficient messaging facilities should allow direct memory access (DMA) mechanisms to be invoked inexpensively, possibly on multiple blocks of data. <p> Some modern processors, such as Alewife's Sparcle processor [106], MOSAIC [100], the MDP <ref> [35] </ref>, and the UltraSPARC [111], can respond rapidly to interrupts. In particular, as discussed in Section 2.2, Alewife supports fast interrupt delivery via featherweight threads. This couples with efficient DMA to provide another advantage: virtual queuing. <p> A polling watchdog mode could be implemented in Alewife, if so desired. Direct Network Interfaces: Several machines have provided direct network interfaces. These include the CM-5, the J-machine, iWarp, the *T interface, Alewife, and Wisconsin's CNI <ref> [68, 35, 13, 89, 3, 86] </ref>. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating 7.3.
Reference: [36] <author> Chris Dalton, Greg Watson, David Banks, Costas Calamvokis, Aled Edwards, and John Lumley. </author> <title> Afterburner. </title> <journal> IEEE Network, </journal> <pages> pages 3643, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The M-machine [40] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces: Memory-based interfaces in multicomputers [12, 18, 97, 99, 104] and workstations <ref> [36, 37, 108, 112] </ref> provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. Automatic hardware buffering also deals well with sinking bursts of messages and provides the lowest overhead (by avoiding the processors) when messages are not handled immediately.
Reference: [37] <author> Peter Druschel, Larry L. Peterson, and Bruce S. Davie. </author> <title> Experiences with a High-Speed Network Adaptor: A Software Perspective. </title> <booktitle> In Proceedings of the Conference on Communication Architectures, Protocols and Applications, </booktitle> <pages> pages 213, </pages> <year> 1994. </year>
Reference-contexts: The M-machine [40] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces: Memory-based interfaces in multicomputers [12, 18, 97, 99, 104] and workstations <ref> [36, 37, 108, 112] </ref> provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. Automatic hardware buffering also deals well with sinking bursts of messages and provides the lowest overhead (by avoiding the processors) when messages are not handled immediately.
Reference: [38] <author> Michel Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 21(2):921, </volume> <month> February </month> <year> 1988. </year>
Reference: [39] <author> Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, Ioannis Schoinas, Mark D. Hill James R. Larus, Anne Rogers, and David A. Wood. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Supercomputing 94, </booktitle> <year> 1994. </year>
Reference-contexts: They also found that node-to-network bandwidth was not critical in modern multiprocessors. Our study shows, however, that bandwidth across the bisection of the machine may become a critical cost in supporting shared memory on modern machines. Such costs will make message passing and specialized user-level protocols <ref> [39] </ref> increasingly important as processor speeds increase. Woo et al. [117] compared bulk transfer with shared memory on simulations of the FLASH multiprocessor [64] running the SPLASH [101] suite.
Reference: [40] <author> Marco Fillo, Stephen W. Keckler, W.J. Dally, Nicholas P. Carter, Andrew Chang, Yevgeny Gurevich, and Whay S. Lee. </author> <title> The M-Machine Multicomputer. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 146156, </pages> <address> Ann Arbor, MI, </address> <month> November </month> <year> 1995. </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: cache-coherence, multithreading, fine-grained communication, and integrated message passing and shared memory; multi-user timesharing would likely have represented one too many areas of 24 An alternative to this would be to provide a separate descriptor array for each register context, which is somewhat akin to the approach adopted by the M-machine <ref> [40] </ref> which uses the register file to hold outgoing messages before launch. This, of course, has direct roots in the Alewife design. Page 76 CHAPTER 2. THE USER-LEVEL ACCESS PROBLEM focus. Second, the absence of virtual memory made construction of a multiuser operating-system difficult. <p> In this section, we would like to mention and examine some of these other research projects. 7.3.1 Hardware Integration of Communication Models Recent architectures demonstrate emerging agreement that it is important to integrate message-passing and shared-memory communication in the same hardware platform <ref> [3, 45, 93, 99, 40] </ref>. This current interest began, perhaps, with an Alewife paper on the advantages of integration [58]. However, other machines, such as the BB&N-Butterfly [17] supported integrated shared-memory and message-passing communication (via bulk DMA) long before this. <p> Although it was not discussed outside of MIT, the Alewife microarchitecture was completed before this paper was published; hence, we can 7.3. Related Work Page 227 only conclude that this is a case of convergent evolution. The Typhoon [93] and M-machine <ref> [40] </ref> architectures have approached the integration of message-passing and shared-memory communication by combining minimal virtual-memory style fine-grained data mapping with fast message interfaces to permit software-managed cache coherence. Typhoon includes a coprocessor for handling cache-coherence requests, while the M-machine reserves a hardware thread for special message handling. <p> The CM-5 provides restricted multiprogramming by strict gang scheduling and by context-switching the network with the processors. The *T NI [89] would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine <ref> [40] </ref> receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread.
Reference: [41] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> June </month> <year> 1990. </year> <note> IEEE. </note>
Reference-contexts: Other weaker memory models must add separate, out-of-band synchronization operations; for example, DASH <ref> [41] </ref> and Origin [67] provide release and acquire operations for synchronization; these operations act on locks in a separate lock space, that is independent of the shared-memory data space. <p> The DASH multiprocessor supports release consistency, instead of sequential consistency (and, in fact, was one of the driving forces behind the creation of the release-consistent model <ref> [41] </ref>). For latency tolerance (or amelioration), DASH includes prefetching and a mechanism for depositing data directly in another processor's cache. As discussed in Chapter 4, deadlock issues in DASH were handled through a combination of multiple virtual channels and negative acknowledgments.
Reference: [42] <author> Kourosh Gharachorloo, Sarita V. Adve, Anoop Gupta, John L. Hennessey, and Mark D. Hill. </author> <title> Programming for Different Memory Consistency Models. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 399407, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Over the years, researchers have developed a number of ways of programming and reasoning about machines that have consistency models that are weaker than sequential consistency <ref> [2, 42] </ref>. Further, one of the justifications for weaker memory models is that they provide better latency tolerance (under some circumstances) than sequential consistency. As a result, the arguments in Page 46 CHAPTER 2.
Reference: [43] <author> Erik Hagersten, Anders Landin, and Seif Haridi. </author> <title> DDM A Cache-Only Memory Architecture. </title> <journal> IEEE Computer, </journal> <volume> 25(9):4454, </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: Recently, the SGI Origin [67] has appeared as a commercial offspring of DASH. This machine has taken fast, distributed, hardware cache-coherence to a new level of performance. The KSR1 and DDM <ref> [43] </ref> provide a shared address space through cache-only memory. These machines also allow prefetching. Issues of deadlock are handled in specialized ways in both of these machines: the KSR1 takes advantage of its ring-based network topology, while the DDM exploits its hierarchy of buses.
Reference: [44] <author> R.H. Halstead and T. Fujita. MASA: </author> <title> A Multithreaded Processor Architecture for Parallel Symbolic Computing. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443451, </pages> <address> New York, </address> <month> June </month> <year> 1988. </year> <note> IEEE. </note>
Reference-contexts: Thus, a block-multithreaded architecture ideally executes threads in quantum that are larger than a single cycle (i.e. threads are executed in blocks of time). This is one of the primary distinctions between a block-multithreaded processor and more hardware-intensive methods of multithreading such as present on MASA <ref> [44] </ref>, HEP [102], or Monsoon [88]. These other architectures can switch between a large number of active hardware threads on a cycle-by-cycle basis. 2.2.2 Scheduling for Featherweight Threads of a much larger set of runnable and suspended threads which are maintained by the operating system.
Reference: [45] <author> John Heinlein, Kourosh Gharachorloo, Scott Dresser, and Anoop Gupta. </author> <title> Integration of Message Passing and Shared Memory in the Stanford FLASH Multiprocessor. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 3850. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: Second, both message transmission and reception occur at user level on the computation processor. This is in marked contrast to the network interfaces of a number of other systems that either force message reception to occur at system level <ref> [64, 45] </ref> or via a coprocessor [93, 97]. Third, the interface is highly efficient and uses a uniform packet structure. The message-passing interface in the Alewife machine is designed around four primary observations: 1. <p> In this section, we would like to mention and examine some of these other research projects. 7.3.1 Hardware Integration of Communication Models Recent architectures demonstrate emerging agreement that it is important to integrate message-passing and shared-memory communication in the same hardware platform <ref> [3, 45, 93, 99, 40] </ref>. This current interest began, perhaps, with an Alewife paper on the advantages of integration [58]. However, other machines, such as the BB&N-Butterfly [17] supported integrated shared-memory and message-passing communication (via bulk DMA) long before this.
Reference: [46] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A Tightly-Coupled Processor-Network Interface. </title> <booktitle> In Fifth Internataional Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <address> Boston, </address> <month> October </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: Thus, efficient messaging facilities should permit direct transfer of information from registers to the network interface. Direct register-to-register transmission has been suggested by a number of architects <ref> [14, 35, 113, 46] </ref>. 2. Blocks of data which reside in memory often accompany such header information. Thus, efficient messaging facilities should allow direct memory access (DMA) mechanisms to be invoked inexpensively, possibly on multiple blocks of data.
Reference: [47] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <address> Boston, </address> <month> October </month> <year> 1992. </year> <note> ACM. BIBLIOGRAPHY Page 241 </note>
Reference-contexts: Here, the messaging overheads are low enough that a thin layer of interrupt-driven operating-system software, can synthesize arbi trary network queueing structures in software. 4. Permitting compilers (or users) to generate network communications code has a number of advantages, as discussed in [113], [14], and <ref> [47] </ref>. Compiler-generated code, however, requires user-level access to the message interface, including access to some form of atomicity mechanism to control the arrival of message interrupts. Given these observations, we can proceed to define a message-passing communication model for Alewife.
Reference: [48] <author> C. Holt, M. Heinrich, J. P. Singh, E. Rothberg, and J. Hennessy. </author> <title> The effects of latency, occupancy and bandwidth on the performance of cache-coherent multprocessors. </title> <type> Technical report, </type> <institution> Stanford University, Stanford, California, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: This generalization is similar to the study of latency, occupancy, and bandwidth by Holt et. 7.3. Related Work Page 231 al <ref> [48] </ref>, which focuses exclusively upon shared-memory mechanisms. Although the Alewife machine provides an excellent starting point for the comparison of a large number of communication mechanisms, our results are greatly enhanced by our use of emulation, an approach inspired by the work at Wisconsin [107].
Reference: [49] <author> David V. James, Anthony T. Laundrie, Stein Gjessing, and Gurindar S. Sohi. </author> <title> Distributed-Directory Scheme: Scalable Coherent Interface. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 7477, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: That is, after a first-time fetch of data from a remote node, subsequent accesses of the data are satisfied entirely within the node. The resulting cache coherence problem can be solved using a variety of directory based schemes <ref> [49, 69, 23] </ref>. Unfortunately, even with coherent caches, the cost of remote memory actions can be prohibitive. To fetch data through the interconnection network, the processor transmits a request, then waits for a response.
Reference: [50] <author> Kirk Johnson. </author> <title> The impact of communication locality on large-scale multiprocessor performance. </title> <booktitle> In 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 392402, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Multiple outstanding transactions can be supported using software prefetch [20, 85], multithreading via rapid context-switching [115, 5], or weak ordering [1]. Studies have shown that the utilization of the network, processor, and memory systems can be improved almost in proportion to the number of outstanding transactions allowed <ref> [63, 50] </ref>. Multithreading may be implemented with either polling or signaling mechanisms. Polling involves retrying memory requests until they are satisfied. This is the behavior of simple RISC pipelines (such as Sparcle) which implement non-binding prefetch or context-switching through synchronous memory faults.
Reference: [51] <author> Kirk L. Johnson. </author> <title> High-Performance All-Software Distributed Shared Memory. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: Thus, the success of LimitLESS provides insight into the speed of message-passing mechanisms. Further, Alewife members developed two complete programming systems as user-level libraries on top of the Alewife user-level message-passing system: the C Region Library (CRL) by Kirk Johnson <ref> [52, 51] </ref> and the Multi-Grain Shared-Memory system (MGS) by Donald Ye-ung [120, 119]. Both of these systems make extensive use of the UDM messaging model, complete with software versions of the user-level atomicity mechanisms (the soft runtime system described in Section 6.1.3).
Reference: [52] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-performance all-software distributed shared memory. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Thus, the success of LimitLESS provides insight into the speed of message-passing mechanisms. Further, Alewife members developed two complete programming systems as user-level libraries on top of the Alewife user-level message-passing system: the C Region Library (CRL) by Kirk Johnson <ref> [52, 51] </ref> and the Multi-Grain Shared-Memory system (MGS) by Donald Ye-ung [120, 119]. Both of these systems make extensive use of the UDM messaging model, complete with software versions of the user-level atomicity mechanisms (the soft runtime system described in Section 6.1.3).
Reference: [53] <author> N.P. Jouppi. </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proceedings, International Symposium on Computer Architecture '90, </booktitle> <pages> pages 364373, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: More general schemes might keep track of the context that owns each buffer to prevent premature lock release). The use of a transaction buffer architecture has been presented in several milieux, such as lockup-free caching [59], victim caching <ref> [53] </ref>, and the remote-access cache of the DASH multiprocessor [69]. The need for an associative match on the address stems from several factors. First, protocol traffic is tagged by address rather than by context number. <p> Thus, cached states can persist outside of primary transactions causing the transaction buffer to act like a small second-level cache. Consequently, non-binding prefetches simply return their data to the transaction buffer 20 . Further, cache-lines can be victim cached <ref> [53] </ref> in the transaction buffer; this means that lines that have been replaced from the primary cache are placed into the transaction buffer rather than dumped into the network.
Reference: [54] <author> Stafanos Kaxiras. </author> <title> Kiloprocessor Extensions to SCI. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: to the server-interlock problem that we have been discussing, however, the fact that SCI performs queueing in hardware means that it naturally handles incoming requests during periods of invalidation: SCI simply builds the so-called prepend queue of requesters at the head of a linked list even while interlocked for invalidation <ref> [98, 54] </ref>. 7 In fact, modern pipelines with out-of-order execution often have a limit to the number of outstanding requests for a similar reason. For instance, the R10000 has a maximum limit of four outstanding requests (including prefetches) at any one time [92]. Page 108 CHAPTER 3. <p> Hence, SCI optimizes for the uncommon case rather than the common case pattern of sharing. Further, since it forms linear chains of data sharers, it risks long latencies during invalidation. Attempts to correct latency problems have only lead to more complexity <ref> [55, 54] </ref>. Finally, the fact that sharing chains stretch through individual caches means that expensive, high-speed storage (in the form of cache-tags) is being used to support sharing rather than less expensive DRAM resources.
Reference: [55] <author> Stefanos Kaxiras and James R. Goodman. </author> <title> The GLW Cache Coherence Protocol Extensions for Widely Shared Data. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Hence, SCI optimizes for the uncommon case rather than the common case pattern of sharing. Further, since it forms linear chains of data sharers, it risks long latencies during invalidation. Attempts to correct latency problems have only lead to more complexity <ref> [55, 54] </ref>. Finally, the fact that sharing chains stretch through individual caches means that expensive, high-speed storage (in the form of cache-tags) is being used to support sharing rather than less expensive DRAM resources.
Reference: [56] <author> A. Klaiber and H. Levy. </author> <title> A Comparison of Message Passing and Shared Memory for Data-Parallel Programs. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: It should be noted, however, that Alewife DMA mechanisms were found to greatly enhance applications (in particular the operating system) with less irregular communication patterns or with large blocks of data. Klaiber and Levy <ref> [56] </ref> study the performance of programs which accesses shared-memory or message-passing runtime libraries. These libraries generated traces for shared-memory and message-passing simulators, to generate statistics on message traffic. However, their programs were not fined tuned for any particular architecture, and hence not fair to either.
Reference: [57] <author> Kathleen Knobe, Joan Lukas, and Guy Steele Jr. </author> <title> Data Optimization: Allocation of Arrays to Reduce Communication on SIMD Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2):102118, </volume> <month> August </month> <year> 1990. </year>
Reference: [58] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory; Early Experience. </title> <booktitle> In Principles and Practice of Parallel Programming (PPoPP) 1993, </booktitle> <pages> pages 5463, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year> <note> ACM. Also as MIT/LCS TM-478, </note> <month> January </month> <year> 1993. </year>
Reference-contexts: Thus, efficient messaging facilities should allow direct memory access (DMA) mechanisms to be invoked inexpensively, possibly on multiple blocks of data. This is important for a number of reasons, including rapid task dispatch (where a task-frame or portion of the calling stack may be transmitted along with the continuation) <ref> [58] </ref> and distributed block I/O (where both a buffer-header structure and data may reside in memory). 3. Some modern processors, such as Alewife's Sparcle processor [106], MOSAIC [100], the MDP [35], and the UltraSPARC [111], can respond rapidly to interrupts. <p> There are a number of reasons for this. First, it provides far more mechanism than is actually needed in many cases. As Chapter 6 demonstrates, message passing is useful as a way of bypassing the coherence protocol. In fact, many of the applications of message-passing discussed in <ref> [58] </ref> do not require a complete mechanism. Second, a machine with a single network port cannot fetch dirty source data while in the middle of transmitting a larger packet since this requires the sending of messages. <p> This current interest began, perhaps, with an Alewife paper on the advantages of integration <ref> [58] </ref>. However, other machines, such as the BB&N-Butterfly [17] supported integrated shared-memory and message-passing communication (via bulk DMA) long before this.
Reference: [59] <author> David Kroft. </author> <title> Lockup-Free Instruction Fetch/Prefetch Cache Organization. </title> <booktitle> In Proceedings of the 8th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 8187, </pages> <month> June </month> <year> 1981. </year> <note> Page 242 BIBLIOGRAPHY </note>
Reference-contexts: More general schemes might keep track of the context that owns each buffer to prevent premature lock release). The use of a transaction buffer architecture has been presented in several milieux, such as lockup-free caching <ref> [59] </ref>, victim caching [53], and the remote-access cache of the DASH multiprocessor [69]. The need for an associative match on the address stems from several factors. First, protocol traffic is tagged by address rather than by context number.
Reference: [60] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In Proceedings of the International Supercomputing Conference (ISC) 1993, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year> <note> IEEE. Also as MIT/LCS TM-498, </note> <month> December </month> <year> 1992. </year>
Reference-contexts: It also involves a user-level atomicity mechanism, a DMA mechanism, and fast message interrupts via featherweight threads as discussed in Section 2.2. With the integrated interface in Alewife, a message can be sent with just a few user-level instructions <ref> [60] </ref>. User threads may receive messages either by polling the network or by executing a user-level interrupt handler. As with other aspects of Alewife, the message-passing interface represents a careful balance between hardware and software features. Consequently, scheduling and queueing decisions are made entirely in software. <p> In the following paragraphs, we will characterize the raw performance of the Alewife network, as well as the uncontended performance of shared-memory and message-passing communication. Kirk Johnson was instrumental in many of these measurements. Also, many of these numbers have been published elsewhere <ref> [3, 16, 60] </ref>. 6.1.1 Performance of the Network In this section, we perform a characterization of the network performance as a whole.
Reference: [61] <author> John Kubiatowicz, David Chaiken, Anant Agarwal, Arthur Altman, Jonathan Babb, David Kranz, Beng-Hong Lim, Ken Mackenzie, John Piscitello, and Donald Yeung. </author> <title> The Alewife CMMU: Addressing the Multiprocessor Communications Gap. </title> <booktitle> In HOTCHIPS, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: The Communications and Memory Management Unit Page 159 5.2 The Communications and Memory Management Unit The Alewife Communications and Memory Management Unit (CMMU) <ref> [61] </ref> is the heart of the Alewife machine. Whereas Sparcle facilitates the low overhead communication interfaces of Alewife, the CMMU houses the actual interfaces as well as all of the communications functionality.
Reference: [62] <author> John D. Kubiatowicz. </author> <title> Closing the Window of Vulnerability in Multiphase Memory Transactions: The Alewife Transaction Store. </title> <type> Technical Report TR-594, </type> <institution> MIT, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Such signaling mechanisms would be similar to those used when implementing binding prefetch or out-of-order completion of loads and stores. This section explores the problems involved in closing the window of vulnerability in polled, context-switching processors. For a discussion of signaling methods, see <ref> [62] </ref>. Systems with multiple outstanding requests may view their requests as split-phase transac subject to a window of vulnerability. tions, consisting of decoupled request and response phases. The time between request and response may be composed of a number of factors, including communication delay, protocol delay, and queueing delay. <p> Note that there are a couple of additional pipelining issues with respect to updating these vectors that arise because they are not directly integrated with the integer pipeline; for more information on this, see <ref> [62] </ref>. Page 174 CHAPTER 5. THE HARDWARE ARCHITECTURE OF ALEWIFE If either of the TW bits for the current context is asserted, then the processor pipeline is frozen with context-switching disabled. This policy makes thrash-detection persistent across the servicing of high-availability interrupts.
Reference: [63] <author> Kiyoshi Kurihara, David Chaiken, and Anant Agarwal. </author> <title> Latency Tolerance through Mul-tithreading in Large-Scale Multiprocessors. </title> <booktitle> In Proceedings International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Japan, April 1991. </address> <publisher> IPS Press. </publisher>
Reference-contexts: Multiple outstanding transactions can be supported using software prefetch [20, 85], multithreading via rapid context-switching [115, 5], or weak ordering [1]. Studies have shown that the utilization of the network, processor, and memory systems can be improved almost in proportion to the number of outstanding transactions allowed <ref> [63, 50] </ref>. Multithreading may be implemented with either polling or signaling mechanisms. Polling involves retrying memory requests until they are satisfied. This is the behavior of simple RISC pipelines (such as Sparcle) which implement non-binding prefetch or context-switching through synchronous memory faults. <p> A request to memory is initiated at the same time 8 . By maintaining a separate PC and PSR for each context, a more aggressive processor design could switch contexts much faster. However, even with 14 cycles of overhead and four processor-resident contexts, multithreading can significantly improve system performance <ref> [115, 63] </ref>. Appendix A shows a more detailed use of the Sparcle context-switching mechanisms to implement featherweight threads for user-level active message handlers.
Reference: [64] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA) 1994, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <title> IEEE. [65] l64811 SPARC Integer Unit Technical Manual. LSI Logic Corporation, Milpitas, </title> <address> CA 95035, </address> <year> 1989. </year>
Reference-contexts: Second, both message transmission and reception occur at user level on the computation processor. This is in marked contrast to the network interfaces of a number of other systems that either force message reception to occur at system level <ref> [64, 45] </ref> or via a coprocessor [93, 97]. Third, the interface is highly efficient and uses a uniform packet structure. The message-passing interface in the Alewife machine is designed around four primary observations: 1. <p> format for the coherence directory and 14 Although this number is not particularly fundamental, it is amazing how much time can be taken by such simple things as searching for a software directory in a hash table, manipulating directories at the bit level, etc. 15 The FLASH multiprocessor from Stanford <ref> [64] </ref> takes this approach in a somewhat more conservative fashion: in that machine, software handling of coherence operations is separated from data prefetching but must simultaneously. 3.4. Protocol Reordering Sensitivities Page 115 Table 3-3 lists the four meta-states. <p> It also permits guarantees of operation atomicity and finite completion time. In fact, modern superscalar processors support a service-coupling-like separation of scheduling from execution to maximize the utilization of functional units. As mentioned in Section 7.3 below, the FLASH architecture paper <ref> [64] </ref> has a diagram that seems to reflect service coupling; this appears to be a case of convergent evolution. 1 Excessive paging is prevented by the scheduler. 7.2. <p> This was seen in Section 6.2.3 which summarized some of the results of [28]. The Stanford FLASH multiprocessor <ref> [64] </ref> takes the specialized coprocessor approach to providing a plethora of mechanisms. As a later generation project, FLASH tackled a number of multiuser and reliability issues that were not explored by Alewife (although multiuser issues were addressed in FUGU [77, 78], a related project). <p> This is another cost of flexibility: coherence operations take many more cycles than they would if hard-coded. It is interesting to note that the scheduling architecture adopted by FLASH <ref> [64] </ref> bears resemblance to the service-coupling methodology of Section 5.2.4: requests are queued up at a hardware scheduler (called the inbox), then forwarded as a stream of requests directly to the protocol processor. Results are finally passed to the outbox for post distribution. In fact, Figure 4.2 of [64] has a <p> by FLASH <ref> [64] </ref> bears resemblance to the service-coupling methodology of Section 5.2.4: requests are queued up at a hardware scheduler (called the inbox), then forwarded as a stream of requests directly to the protocol processor. Results are finally passed to the outbox for post distribution. In fact, Figure 4.2 of [64] has a topological similarity to our Figure 5-18 on page 180. Although it was not discussed outside of MIT, the Alewife microarchitecture was completed before this paper was published; hence, we can 7.3. Related Work Page 227 only conclude that this is a case of convergent evolution. <p> Such costs will make message passing and specialized user-level protocols [39] increasingly important as processor speeds increase. Woo et al. [117] compared bulk transfer with shared memory on simulations of the FLASH multiprocessor <ref> [64] </ref> running the SPLASH [101] suite. They found bulk transfer performance to be disappointing due to the high cost of initiating transfer and the difficulty in finding computation to overlap with the transfer.
Reference: [66] <author> Leslie Lamport. </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multi-process Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9), </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: With this functionality, any of a number of different possible synchronization operations may be constructed. The Role of Memory Model: Alewife supports a sequentially consistent memory model <ref> [66] </ref>. Among other things, sequential consistency provides one of the easiest memory models to reason about.
Reference: [67] <author> James Laudon and Daniel Lenoski. </author> <title> The SG Origin: A ccNUMA Highly Scalable Server. </title> <booktitle> In Procedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Other weaker memory models must add separate, out-of-band synchronization operations; for example, DASH [41] and Origin <ref> [67] </ref> provide release and acquire operations for synchronization; these operations act on locks in a separate lock space, that is independent of the shared-memory data space. <p> Each request is given a priority value which starts low and is incremented whenever the request is retried. In this way, requests can be guaranteed to eventually achieve a priority value that is higher than any other simultaneous requesters <ref> [67] </ref>. This technique provides an elegant solution to the multiple-writer livelock, removing it entirely with little hardware complexity or cost in resources. <p> This affords the performance advantages of the unreduced protocol when queue resources are available, while falling back onto the reduced protocol when necessary to avoid deadlock. Interestingly enough, the Origin multiprocessor <ref> [67] </ref> from Silicon Graphics (SGI) makes use of adaptive depth reduction as described here to avoid deadlock with two logical channels and a DASH-like protocol. The base protocol has a dependency depth of three. <p> Thus, the myth that cache-coherence operations must be expensive is just that: a myth. Remote to local access latencies on the order of 2 to 1 or lower are currently achievable (see the Origin <ref> [67] </ref>, for instance). In fact, we can expect that remote to local access latencies will be driven even lower than 2 to 1, given current trends of increasing cache-line sizes and increasing relative DRAM delays. <p> For latency tolerance (or amelioration), DASH includes prefetching and a mechanism for depositing data directly in another processor's cache. As discussed in Chapter 4, deadlock issues in DASH were handled through a combination of multiple virtual channels and negative acknowledgments. Recently, the SGI Origin <ref> [67] </ref> has appeared as a commercial offspring of DASH. This machine has taken fast, distributed, hardware cache-coherence to a new level of performance. The KSR1 and DDM [43] provide a shared address space through cache-only memory. These machines also allow prefetching.
Reference: [68] <author> Charles E. Leiserson, Aahil S. Abuhamdeh, and David C. Douglas et al. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In The Fourth Annual ACM Symposium on Parallel Algorithms and Architectures. ACM, </booktitle> <year> 1992. </year>
Reference-contexts: A polling watchdog mode could be implemented in Alewife, if so desired. Direct Network Interfaces: Several machines have provided direct network interfaces. These include the CM-5, the J-machine, iWarp, the *T interface, Alewife, and Wisconsin's CNI <ref> [68, 35, 13, 89, 3, 86] </ref>. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating 7.3.
Reference: [69] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148159, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: That is, after a first-time fetch of data from a remote node, subsequent accesses of the data are satisfied entirely within the node. The resulting cache coherence problem can be solved using a variety of directory based schemes <ref> [49, 69, 23] </ref>. Unfortunately, even with coherent caches, the cost of remote memory actions can be prohibitive. To fetch data through the interconnection network, the processor transmits a request, then waits for a response. <p> More general schemes might keep track of the context that owns each buffer to prevent premature lock release). The use of a transaction buffer architecture has been presented in several milieux, such as lockup-free caching [59], victim caching [53], and the remote-access cache of the DASH multiprocessor <ref> [69] </ref>. The need for an associative match on the address stems from several factors. First, protocol traffic is tagged by address rather than by context number. <p> An alternative way of avoiding deadlock during server-interlock periods is to discard excess requests by sending NAK messages back to the requestor. DASH did this <ref> [69] </ref>, Alewife did this [23], and others have done this. While this removes a large amount of complexity due to queueing, it forces requesting nodes to retry their requests, thereby leading to the possibility of livelock 8 . <p> Thus, as we can see from Figure 4-2, it is the dependencies introduced by the protocol processors that are responsible for deadlock in cache-coherence protocols. Hence the name: protocol deadlock. 4.1.1 Breaking Cycles With Logical Channels One solution to this particular deadlock, employed by the DASH <ref> [69] </ref> multiprocessor, is to recognize that responses (such as data) can be guaranteed to be sinkable at their destinations: as a precondition to sending a remote request for data, the source of the request can allocate sufficient space to handle the response when it returns. <p> DASH coherence protocol is not quite deadlock free, despite the fact that two different network channels were provided to eliminate deadlock; to avoid physically deadlocking network channels, the memory controller aborts dependencies such as REQ)INV by sending a NAK message back to the source under certain situations involving full queues <ref> [69] </ref>. This solution transforms potential deadlock into a potential livelock, since the request must be subsequently retried 3 .
Reference: [70] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3):6379, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: That myth is the notion that remote memory access must necessarily be expensive. A number of architectures seem to have accepted that a 10 to 1 (or higher) ratio of remote to local access latency is inevitable (for instance, Dash <ref> [70] </ref>). Unfortunately, such high remote to local access latencies greatly limit the maximum parallelism that can be exploited by applications. In fact, with service coupling, this does not have to be true: Alewife supports a 3 to 1 ratio. Figures 5-19 and 5-20 illustrate this point.
Reference: [71] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The dash prototype: Logic overhead and performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1):4161, </volume> <month> Jan </month> <year> 1993. </year> <note> BIBLIOGRAPHY Page 243 </note>
Reference-contexts: Other cache-coherent shared-memory machines provide either full hardware support for shared memory or implement coherence entirely in software. DASH <ref> [71] </ref> is a cache-coherent multiprocessor that uses a full-mapped, directory-based cache coherence protocol implemented entirely in hardware. From the standpoint of timing, the DASH project was a sibling project to Alewife, although the DASH implementation methodology (FP-GAs) produced working prototypes much quicker than Alewife.
Reference: [72] <author> Daniel E. Lenoski and Wolf-Dietrich Weber. </author> <title> Scalable Shared-Memory Multiprocessing. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: Thus, assuming that a sufficient number of logical channels are available, deadlock can be avoided in this way. The presence of invalidations in cache-coherence 5 Lenoski and Weber's book on multiprocessing mentions the reduction of a depth-three DASH protocol to a depth two protocol. See <ref> [72] </ref>. 6 In fact, this proxy message is a version of the coherence directory, describing multiple invalidation messages in a compact form. 4.2. Message Passing and Deadlock Page 125 protocols implies that non-reduced coherence protocols have a minimum dependence depth of three.
Reference: [73] <author> Richard Anton Lethin. </author> <title> Message-Driven Dynamics. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: for hysteresis directly when we explore the 13 Figure 4-7 does not show the demultiplexor that feeds incoming protocol packets to appropriate hardware. 14 As a case in point, the J-machine [87] employed an interrupt on full output queue heuristic that tended to be far too sensitive to network behavior <ref> [73] </ref>. 4.3. Exploiting Two-Case Delivery for Deadlock Removal Page 133 heuristic offset (a measure of the effectiveness of a deadlock detection heuristic) in Section 6.3.5. <p> This result implies that we should always wait out the presence of temporary queue blockages in many cases they will go away. As a case in point, the J-machine, which triggered a network overflow interrupt as soon as either the input or output queue became full <ref> [73] </ref>, introduced a lot of software overhead to avoid non-existent deadlocks. As the heuristic timeout increases, the heuristic offset slowly reduces in magnitude until it be 11 Since these distributions have huge tails, we are not reporting actual minimum values. <p> Future work should include more interesting models for processor requests. In particular, we should include more message-passing-like communication. One possibility for artificial message-passing communication involves the concept of Snakes as used by Rich Lethin in his thesis <ref> [73] </ref>. These multi-hop messages express more of communication freedom present in message-passing algorithms. <p> Perhaps the clear-est example of this was with the J-machine [87], which provided overflow interrupts on both the input and output queues. The resulting system was studied extensively by Rich Lethin in <ref> [73] </ref>. Interestingly enough, the fact that the overflow interrupts were generated immediately after a corresponding queue became full was a source of performance problems in this machine. <p> Certainly, we saw in Section 6.3 that shared-memory programs are less likely to invoke second-case delivery than message-passing programs; in that sense, these results agree. However, the results of <ref> [73] </ref> do not take into account hysteresis, instead characterizing the frequency with which queues are full. We believe that this is often the wrong question (at least for machines that support better detection heuristics than queue full), since temporary blockages are a natural aspect of wormhole-routed networks.
Reference: [74] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2:361376, </volume> <month> July </month> <year> 1991. </year>
Reference: [75] <author> Beng-Hong Lim. </author> <title> Reactive Synchronization Algorithms for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <year> 1995. </year> <note> MIT/LCS TR-664. </note>
Reference: [76] <author> Beng-Hong Lim and Ricardo Bianchini. </author> <title> Limits on the Performance Benefits of Multithread-ing and Prefetching. </title> <booktitle> In Proceedings of the International Conference on the Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Although this is a form of preemptive scheduling, it makes no guarantees of fairness; instead, it Page 48 CHAPTER 2. THE USER-LEVEL ACCESS PROBLEM tends to maximize cache reuse, letting threads continue to run as long as they are not blocked on memory or synchronization <ref> [76] </ref>. This means that uncontrolled context-switching during critical short interrupt handlers would be deadly (not to mention unwise). Thus, context-switch atomicity must be invoked at all times during handlers and many portions of the kernel. <p> The mean length of the computation sections (i.e. the mean of the exponential distribution) is often called the run-length and is very application dependent. We will be exploring a range of run-lengths and sanity checking this range with results from Lim and Bianchini <ref> [76] </ref> in which they measure run-lengths and prefetching behavior from real applications on Alewife. Deadlock Detection: Since our primary interest in this section is in the frequency of deadlock, DeadSIM includes a deadlock-detection algorithm that detects cyclic dependencies between messages. The simulation starts with an empty network. <p> Thus, we will use Alewife parameters for a set of experiments: 2-dimensional mesh, 256-byte network input and output queue sizes, 19-byte queues within the network (an EMRC value), 8-byte request packets, 24-byte response packets, and 2-bytes/cycle of network latency. In <ref> [76] </ref>, Beng-Hong Lim and Ricardo Bianchini explore typical run-lengths for a number of Alewife applications written in both multithreading and prefetched style. They discovered that most of these applications have run-lengths over 100 cycles and can take advantage of no more than four outstanding requests. <p> In fact, with a 100-cycle run-length, a 64-node machine does not experience deadlock even with 12 outstanding requests! With a 40-cycle run-length (less than all measured applications in <ref> [76] </ref>), a 64-node machine does not start experiencing deadlock until it reaches six outstanding requests, and then the mean time to deadlock is at the limits of our 10-million-cycle cutoff. 6.3. How Frequent IS Deadlock? Page 215 Alewife parameters. requests, at a variety of run-lengths.
Reference: [77] <author> Ken Mackenzie, John Kubiatowicz, Anant Agarwal, and M. Frans Kaashoek. FUGU: </author> <title> Implementing Protection and Virtual Memory in a Multiuser, Multimodel Multiprocessor. </title> <note> Technical Memo MIT/LCS/TM-503, </note> <month> October </month> <year> 1994. </year>
Reference-contexts: Further, there were a number of tricky interactions between virtual memory and user-level DMA that the author was not prepared to deal with at the time. Solutions to these problems are discussed in more detail in <ref> [77] </ref>. 4 This is in contrast to the User-Direct Messaging communication model, discussed in Section 2.4, where model and interface must be introduced separately. 2.3. <p> This can be handled with careful scheduling and manipulation of the free-page cache. This is beyond the topic of the current discussion, however. See <ref> [77] </ref>. Thus, the User-Direct Messaging model directly generalizes to a multiuser system with little extra mechanism. <p> This simple This would simplifying the processor/memory interface a bit by removing the potential need for multiple ipicst instructions to be issued on a given message (see <ref> [77] </ref>). 7.2. How Do the Lessons of Alewife Apply Today? Page 225 concept is surprisingly important and not normally present in modern system architectures. <p> The Stanford FLASH multiprocessor [64] takes the specialized coprocessor approach to providing a plethora of mechanisms. As a later generation project, FLASH tackled a number of multiuser and reliability issues that were not explored by Alewife (although multiuser issues were addressed in FUGU <ref> [77, 78] </ref>, a related project). By replacing the equivalent of the Alewife CMMU with a programmable memory controller, the FLASH team gained flexibility to explore a number of different shared-memory models and message-passing communication styles.
Reference: [78] <author> Kenneth Mackenzie, John Kubiatowicz, Matthew Frank, Walter Lee, Victor Lee, Anant Agarwal, and M. Frans Kaashoek. </author> <title> Exploiting Two-Case Delivery for Fast Protected Messaging. </title> <booktitle> In Proceedings of the Fourth Annual Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: The extent to which mismatched message delivery is an uncommon situation is out of the domain of discussion, but is addressed elsewhere <ref> [78] </ref>. Given two-case delivery as a method for multiplexing traffic from different users, we can make use of the User-Direct Messaging model unmodified in a multiuser system. In fact, there are only two issues that must be addressed from an implementation standpoint: 1. <p> This method avoids the explicit overhead incurred through token or credit-based flow control mechanisms. This is explored in greater depth elsewhere <ref> [78] </ref>. Relaunch Mode: The process of packet relaunch (phase two of recovery) can occur in two different ways: either by launching packets to the local memory controller or by passing a pointer to the queued version of each message to its appropriate message handler. <p> Ken Mackenzie's PhD thesis explores this aspect of buffering in more detail [79] as does <ref> [78] </ref>. However, virtual queueing is important even in a single user machine such as Alewife. Let us return to the question of what happens when the queue-level deadlock detection mechanism is invoked. In Section 4.3.1, we introduced two distinct phases of second-case delivery, namely divert and relaunch. <p> These results are encouraging and suggest that two-case delivery was the correct choice for Alewife (perhaps with a better detection heuristic). In <ref> [78] </ref>, two-case delivery is further explored in the context of a multi-user system. However, to understand two-case delivery in more detail, we need the ability to vary parameters in a way that is not possible with any particular hardware platform. <p> Instead, this conclusion proposes the use of two independent network channels as a way of freeing message-passing and shared-memory communication models to work in concert (as opposed to in opposition). Further, two-case delivery is best employed in an environment that can support virtual buffering <ref> [78] </ref>. With virtual buffering, data that is buffered for second-case delivery is placed into the virtual memory of the receiving application, rather than into real memory. Virtual buffering is made effectively infinite by reserving the option to page buffered message traffic to disk 1 . <p> The Stanford FLASH multiprocessor [64] takes the specialized coprocessor approach to providing a plethora of mechanisms. As a later generation project, FLASH tackled a number of multiuser and reliability issues that were not explored by Alewife (although multiuser issues were addressed in FUGU <ref> [77, 78] </ref>, a related project). By replacing the equivalent of the Alewife CMMU with a programmable memory controller, the FLASH team gained flexibility to explore a number of different shared-memory models and message-passing communication styles. <p> Note, that in CNI, hardware places messages directly into buffers, making the process of virtualizing the network interface for multiple users much more difficult. Direct interface designs have mostly ignored issues of multiprogramming and demand paging. In <ref> [78] </ref>, we discuss the use of User-Direct Messaging in the context of a multiuser system (FUGU). The CM-5 provides restricted multiprogramming by strict gang scheduling and by context-switching the network with the processors. <p> This gives a lower-bound on the amount of buffering that must be performed to avoid deadlocking the machine. Although we took a stab at this, much more work is required. Greater exploration of User-Direct Messaging and two-case delivery in the context of a multiuser FUGU system is explored in <ref> [78, 79] </ref>.
Reference: [79] <author> Kenneth M. Mackenzie. </author> <title> The FUGU Scalable Workstation: Architecture and Performance. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <month> February </month> <year> 1998. </year>
Reference-contexts: a way that we will discuss later (i.e. for maintenance of user-level atomicity semantics), it can also be thought of as part of a destination-based flow control methodology: user packets can be buffered in the virtual space of the user at the destination, a method being developed by Ken Mackenzie <ref> [79] </ref>. To make this a complete flow control mechanism, it requires a method to suppress hyper-active users; this can be accomplished through the scheduler. <p> Ken Mackenzie's PhD thesis explores this aspect of buffering in more detail <ref> [79] </ref> as does [78]. However, virtual queueing is important even in a single user machine such as Alewife. Let us return to the question of what happens when the queue-level deadlock detection mechanism is invoked. In Section 4.3.1, we introduced two distinct phases of second-case delivery, namely divert and relaunch. <p> This notion of virtual buffering is examined in detail by Ken Mackenzie in <ref> [79] </ref>. 4.3. Exploiting Two-Case Delivery for Deadlock Removal Page 139 to interactions with other Alewife features. There are four different issues that we would like to discuss here: * Software complexity. Two-case delivery introduces a layer of software which is below even the cache-coherence protocol. * Hardware-level atomicity. <p> Collaborators: Ken Mackenzie was an important contributor to many of the advanced aspects of two-case delivery. In particular, the concept of virtual buffering was developed in great detail by Ken in his thesis <ref> [79] </ref>. Further, Ken was instrumental in transforming two-case delivery from a deadlock-removal mechanism into a universal delivery paradigm. His thesis illustrates the use of two-case delivery to handle everything from page-faults in message handlers to incorrect message delivery in a multiuser multiprocessor. <p> This block is mirrored by the Memory Management and DRAM Control block, that handles the memory-side aspects of the cache coherence protocol, as well as providing 9 However, see Ken Mackenzie's thesis <ref> [79] </ref>. Page 160 CHAPTER 5. THE HARDWARE ARCHITECTURE OF ALEWIFE DRAM refresh and control. At the heart of the CMMU is the Transaction Buffer, which was one of the primary results of Chapter 3. <p> This gives a lower-bound on the amount of buffering that must be performed to avoid deadlocking the machine. Although we took a stab at this, much more work is required. Greater exploration of User-Direct Messaging and two-case delivery in the context of a multiuser FUGU system is explored in <ref> [78, 79] </ref>.
Reference: [80] <author> N. K. Madsen. </author> <title> Divergence preserving discrete surface integral methods for Maxwell's curl equations using non-orthogonal unstructured grids. </title> <type> Technical Report 92.04, </type> <institution> RIACS, </institution> <month> Febru-ary </month> <year> 1992. </year>
Reference-contexts: We will restrict our attention to the characteristics of one application (EM3D). This is joint work with Fred Chong (who took most of the actual numbers). See [28, 27] for more details and other examples. EM3D from Berkeley models the propagation of electromagnetic waves through three-dimensional objects <ref> [80] </ref>. It implicit red-black computation on an irregular bipartite graph. EM3D is iterative and is barrier-synchronized between two phases. Communication takes place because data updated within one phase is used by other nodes in the subsequent phase.
Reference: [81] <author> A. Mainwaring and D. Culler. </author> <title> Active Messages: Organization and Applications Programming Interface (API V2.0). </title> <institution> University of California at Berkeley, Network of Workstations Project White Paper, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: Like RQ, UDM depends on buffering to avoid deadlock rather than on explicit request and reply networks. The Active Message Applications Programming Interface <ref> [81] </ref> is a complete programming system for messaging communication. This API could be efficiently implemented on top of User-Direct Messaging. UDM is thus lighter weight and more general.
Reference: [82] <author> Olivier Maquelin, Guang R. Gao, Herbert H. J. Hum, Kevin Theobald, and Xin-Min Tian. </author> <title> Polling Watchdog: Combining Polling and Interrupts for Efficient Message Handling. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 179188, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Interestingly enough, virtual queueing, as discussed in Section 4.3.6, is an explicit feature of this API; in this specification, virtual queues are referred to as communication endpoints and contain queuing resources for reception of both inter-domain and intra-domain messages The Polling Watchdog <ref> [82] </ref> integrates polling and interrupts for performance improvement. The resulting programming model is interrupt-based in that application code may receive an interrupt at any point; the application cannot rely on atomicity implicit in a polling model.
Reference: [83] <institution> MC88100 RISC Microprocessor User's Manual, </institution> <note> Second Edition. </note> <institution> Motorola, </institution> <year> 1990. </year>
Reference-contexts: The alloc wim register contains information about which of the four Sparcle contexts are currently in use; the 1 There is precedent for persistent supervisor registers, however. See the PowerPC [91] and the MC88100 <ref> [83] </ref>. Page 233 Page 234 APPENDIX A.
Reference: [84] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <institution> COMP TR90-114, Rice University, Houston, Texas, </institution> <month> May </month> <year> 1990. </year> <note> Page 244 BIBLIOGRAPHY </note>
Reference-contexts: In solving this problem, we start by guaranteeing that at least one writer succeeds under all circumstances. This hardware property is sufficient to enable the construction of software queue locks, such as MCS locks <ref> [84] </ref>, to combat the multiple writer situation. Thus, we guarantee memory fairness through a combined hardware/software solution. One justification to this approach is the observation that the single-write guarantee is more than sufficient by itself to guarantee forward progress for many (perhaps most) uses of shared memory. <p> Although this does not fully solve the multiple-writer livelock (since there is no guarantee that such locks will be granted fairly), this hardware property is sufficient to enable construction of software queue locks (e.g. MCS locks <ref> [84] </ref>). Thus, we guarantee memory fairness through this combined hardware/software solution. In Note that the single-writer guarantee is more than sufficient by itself for many (perhaps most) uses of shared memory. Use of Request Priorities: The SGI Origin takes a slightly different approach to eliminating the multiple-writer livelock.
Reference: [85] <author> Todd Mowry and Anoop Gupta. </author> <title> Tolerating Latency Through Software-Controlled Prefetch-ing in Shared-Memory Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2):87106, </volume> <month> June </month> <year> 1991. </year>
Reference-contexts: The system then attempts to fetch data with the appropriate access type. This is called software prefetching, and has a body of literature behind it (see, for instance <ref> [85] </ref>). One of the unfortunate aspects of this interface is that instruction bandwidth is consumed for prefetch instructions; in many cases, however, this is more than offset by the performance gain. <p> In either case, many processor cycles may be lost waiting for a response. Applying basic pipelining ideas, resource utilization can be improved by allowing a processor to transmit more than one memory request at a time. Multiple outstanding transactions can be supported using software prefetch <ref> [20, 85] </ref>, multithreading via rapid context-switching [115, 5], or weak ordering [1]. Studies have shown that the utilization of the network, processor, and memory systems can be improved almost in proportion to the number of outstanding transactions allowed [63, 50]. Multithreading may be implemented with either polling or signaling mechanisms.
Reference: [86] <author> Shubhendu S. Mukherjee, Babak Falsafi, Mark D. Hill, and David A. Wood. </author> <title> Coherent Network Interfaces for Fine-Grain Communication. </title> <booktitle> In Proceedings of the 23rd International Symposium on Computer Architecture, </booktitle> <pages> pages 247258, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: the processor, with output packets pipelined to the output pins for handling by the memory system; in fact, if these windows were made the same size as cache-lines, many mechanisms for moving data for the memory system could potentially be used for handling interfaces 3 . 2 The CNI interface <ref> [86] </ref> is one particularly clever example of correcting for this deficiency; in some sense, User-Direct Messaging degenerates to a CNI-style delivery during second-case delivery (i.e. buffered delivery). 3 Note that providing a storeback descriptor array for describing DMA operations would make the ipicst instruction atomic (in the sense of ipilaunch), more <p> A polling watchdog mode could be implemented in Alewife, if so desired. Direct Network Interfaces: Several machines have provided direct network interfaces. These include the CM-5, the J-machine, iWarp, the *T interface, Alewife, and Wisconsin's CNI <ref> [68, 35, 13, 89, 3, 86] </ref>. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating 7.3. <p> Second, neither the J-machine, nor the CM-5 allow network messages to be transferred through DMA. Third, the J-machine does not provide an atomic message send like Alewife does; this omission complicates the sharing of a single network interface between user code and interrupt handlers. The CNI work <ref> [86] </ref> shows how to partly compensate for a more distant NI by exploiting standard cache-coherence schemes. This assumes that all messages are copied directly into memory buffers by the network hardware, then extracted by the processor.
Reference: [87] <author> M.D. Noakes, D.A.Wallach, and W.J. Dally. </author> <title> The J-Machine Multicomputer: An Architectural Evaluation. </title> <booktitle> In In Proceedings of the 20th Annual International Symposium on Computer Architecture 1993, </booktitle> <pages> pages 224235, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: Page 58 CHAPTER 2. THE USER-LEVEL ACCESS PROBLEM a multi-instruction injection operation and the corresponding interrupt handler must send a message, then the two messages may become intertwined or corrupted. As a case in point, this was a problem with the J-Machine <ref> [87] </ref>, in which message injection was accomplished through instructions that sent data directly into the network (one or two flits at a time). <p> This suggests that hysteresis is an important component of any detection mechanism 14 ; we will illustrate the need for hysteresis directly when we explore the 13 Figure 4-7 does not show the demultiplexor that feeds incoming protocol packets to appropriate hardware. 14 As a case in point, the J-machine <ref> [87] </ref> employed an interrupt on full output queue heuristic that tended to be far too sensitive to network behavior [73]. 4.3. Exploiting Two-Case Delivery for Deadlock Removal Page 133 heuristic offset (a measure of the effectiveness of a deadlock detection heuristic) in Section 6.3.5. <p> Direct NIs can be inefficient unless placed close to the processor. Anticipating 7.3. Related Work Page 229 continued system integration, we place our NI on the processor-cache bus or closer (see discussing above, in Section 7.2). Both the J-machine <ref> [87] </ref> and the CM-5 export hardware message-passing interfaces directly to the user. These interfaces differ from the Alewife interface in several ways. First, in Alewife, messages are normally delivered via an interrupt and dispatched in software, while in the J-machine, messages are queued and dispatched in sequence by the hardware. <p> Page 230 CHAPTER 7. ALL GOOD THINGS : : : 7.3.4 Two-Case Delivery and Deadlock The notion of two-case delivery has appeared in various guises over the years. Perhaps the clear-est example of this was with the J-machine <ref> [87] </ref>, which provided overflow interrupts on both the input and output queues. The resulting system was studied extensively by Rich Lethin in [73]. Interestingly enough, the fact that the overflow interrupts were generated immediately after a corresponding queue became full was a source of performance problems in this machine.
Reference: [88] <author> G. M. Papadopoulos and D.E. Culler. Monsoon: </author> <title> An Explicit Token-Store Architecture. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 8291, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year> <note> IEEE. </note>
Reference-contexts: This is one of the primary distinctions between a block-multithreaded processor and more hardware-intensive methods of multithreading such as present on MASA [44], HEP [102], or Monsoon <ref> [88] </ref>. These other architectures can switch between a large number of active hardware threads on a cycle-by-cycle basis. 2.2.2 Scheduling for Featherweight Threads of a much larger set of runnable and suspended threads which are maintained by the operating system. <p> A few architectures incorporate multiple contexts, pioneered by the HEP [102], switching on every instruction. These machines, including Monsoon <ref> [88] </ref> and Tera [6], do not have caches and rely on a large number of contexts to hide remote memory latency. In contrast, Alewife's block multithreading technique switches only on synchronization faults and cache misses to re Page 228 CHAPTER 7.
Reference: [89] <author> Gregory M. Papadopoulos, G. Andy Boughton, Robert Greiner, and Michael J. Beckerle. </author> <title> *T: Integrated Building Blocks for Parallel Computing. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 624 635. </pages> <publisher> IEEE, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: A polling watchdog mode could be implemented in Alewife, if so desired. Direct Network Interfaces: Several machines have provided direct network interfaces. These include the CM-5, the J-machine, iWarp, the *T interface, Alewife, and Wisconsin's CNI <ref> [68, 35, 13, 89, 3, 86] </ref>. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating 7.3. <p> Direct interface designs have mostly ignored issues of multiprogramming and demand paging. In [78], we discuss the use of User-Direct Messaging in the context of a multiuser system (FUGU). The CM-5 provides restricted multiprogramming by strict gang scheduling and by context-switching the network with the processors. The *T NI <ref> [89] </ref> would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine [40] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread.
Reference: [90] <author> Timothy M. Pinkston and Sugath Warnakulasuriya. </author> <title> On Deadlocks in Interconnection Networks. </title> <booktitle> In Procedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: This simulation uses 64 processors. erty of deadlock formation is the fact that the frequency of deadlock decreases with increasing network dimensionality. This result is similar to results by Timothy Pinkston and Sugath Warnakulasuriya with respect to message deadlocks in networks with adaptive routing <ref> [90] </ref>. It is, perhaps, not entirely surprising given the fact that we are choosing random destinations for our messages. However, it does say something about the formation of deadlocks.
Reference: [91] <editor> PowerPC Microprocessor Family: </editor> <booktitle> The Programming Environments. </booktitle> <institution> IBM Microelectronics and Motorola, </institution> <year> 1994. </year>
Reference-contexts: These registers are considered to be supervisor-only, even though Sparcle does not support protected registers 1 . The alloc wim register contains information about which of the four Sparcle contexts are currently in use; the 1 There is precedent for persistent supervisor registers, however. See the PowerPC <ref> [91] </ref> and the MC88100 [83]. Page 233 Page 234 APPENDIX A.
Reference: [92] <institution> R10000 Microprocessor User's Manual, Ver 2.0. MIPS Technologies/Silicon Graphics, </institution> <year> 1996. </year>
Reference-contexts: For instance, the R10000 has a maximum limit of four outstanding requests (including prefetches) at any one time <ref> [92] </ref>. Page 108 CHAPTER 3. THE SERVICE-INTERLEAVING PROBLEM While SCI solves the server-interlock problem directly, it has several unfortunate properties in addition to its complexity. <p> This relationship between frequency of deadlock and queue resources is, perhaps, obvious. However, two developments make this particularly apropos: (1) Modern, dynamically-scheduled processors tend allow a small number of outstanding requests since they track pending memory operations with special, transaction-buffer-like hardware structures; the R10000 <ref> [92] </ref>, for instance, allows no more than four memory requests to be outstanding at any one time, and (2) The integration of memory and processor technology continues to improve, easily supporting large network queue sizes.
Reference: [93] <author> Steve K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA) 1994, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: Second, both message transmission and reception occur at user level on the computation processor. This is in marked contrast to the network interfaces of a number of other systems that either force message reception to occur at system level [64, 45] or via a coprocessor <ref> [93, 97] </ref>. Third, the interface is highly efficient and uses a uniform packet structure. The message-passing interface in the Alewife machine is designed around four primary observations: 1. <p> In this section, we would like to mention and examine some of these other research projects. 7.3.1 Hardware Integration of Communication Models Recent architectures demonstrate emerging agreement that it is important to integrate message-passing and shared-memory communication in the same hardware platform <ref> [3, 45, 93, 99, 40] </ref>. This current interest began, perhaps, with an Alewife paper on the advantages of integration [58]. However, other machines, such as the BB&N-Butterfly [17] supported integrated shared-memory and message-passing communication (via bulk DMA) long before this. <p> Although it was not discussed outside of MIT, the Alewife microarchitecture was completed before this paper was published; hence, we can 7.3. Related Work Page 227 only conclude that this is a case of convergent evolution. The Typhoon <ref> [93] </ref> and M-machine [40] architectures have approached the integration of message-passing and shared-memory communication by combining minimal virtual-memory style fine-grained data mapping with fast message interfaces to permit software-managed cache coherence. Typhoon includes a coprocessor for handling cache-coherence requests, while the M-machine reserves a hardware thread for special message handling.
Reference: [94] <author> Anne Rogers and Keshav Pingali. </author> <title> Process Decomposition through Locality of Reference. </title> <booktitle> In SIGPLAN '89, Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference: [95] <author> Mendel Rosenblum, Stephen A. Herrod, Emmett Witchel, and Anoop Gupta. </author> <title> Complete Computer Simulation: The SimOS Approach. </title> <booktitle> In IEEE Parallel and Distributed Technology, </booktitle> <month> Fall </month> <year> 1995. </year>
Reference-contexts: Although simulation technology has improved much in the last decade, it is still not entirely suited to the execution of large problems running on top of a complete operating system with all of the real effects present in a real machine. Execution-driven simulators, such as SimOS <ref> [95] </ref>, Proteus [15], and others have made the greatest strides in that direction.
Reference: [96] <author> Satish Chandra and James Lars and Anne Rogers. </author> <title> Where is Time Spent in Message-Passing and Shared-Memory Programs. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 6173, </pages> <address> San Jose, California, </address> <year> 1994. </year> <note> BIBLIOGRAPHY Page 245 </note>
Reference-contexts: Thus, studies of the inherent cost of different communication models on physical hardware are scarce. None-the-less, the Alewife experiments in Section 6.2.3 (and in [28]) were strongly influenced by studies from Wisconsin, Stanford, and Maryland. Our comparison of communication mechanisms is similar to Chandra, Larus and Rogers <ref> [96] </ref>, although we have available a larger set of mechanisms and we generalize to a range of system parameters. This generalization is similar to the study of latency, occupancy, and bandwidth by Holt et. 7.3. Related Work Page 231 al [48], which focuses exclusively upon shared-memory mechanisms. <p> Chandra, Larus and Rogers compare four applications on a simulation of a message-passing machine similar to a CM-5 multiprocessor against a simulation of a hypothetical machine also similar to a CM-5, but extended by shared-memory hardware <ref> [96] </ref>. Their results are a good point of comparison for our emulation results, since both Alewife and the CM-5 are SPARC-based architectures with very similar parameters. They found that message passing performed approximately a factor of two better than shared memory while simulating a network latency of 100 cycles.
Reference: [97] <author> Klaus E. Schauser and Chris J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In Proceedings of the 9th International Symposium on Parallel Processing, </booktitle> <year> 1995. </year>
Reference-contexts: Second, both message transmission and reception occur at user level on the computation processor. This is in marked contrast to the network interfaces of a number of other systems that either force message reception to occur at system level [64, 45] or via a coprocessor <ref> [93, 97] </ref>. Third, the interface is highly efficient and uses a uniform packet structure. The message-passing interface in the Alewife machine is designed around four primary observations: 1. <p> The *T NI [89] would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine [40] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces: Memory-based interfaces in multicomputers <ref> [12, 18, 97, 99, 104] </ref> and workstations [36, 37, 108, 112] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers.
Reference: [98] <institution> Scalable Coherent Interface. IEEE P1596 SCI standard., </institution> <year> 1989. </year>
Reference-contexts: Thus, we have two possible solutions: (1) queue the request for later processing and (2) send some form of negative acknowledgment (NAK) to the requester, forcing a later retry of the request. Some systems (in particular the Scalable Coherent Interface (SCI) <ref> [98] </ref>) make use of the first of these (burying the complexity of this solution in an already complex protocol), while the second is employed by Alewife, Dash, and Origin, among others. 3.3. <p> This is a variant of the argument used to justify the Scalable Coherent Interface (SCI) protocol <ref> [98] </ref>. For SCI, the coherence protocol forms sharing chains through the caches in order to form a distributed coherence directory. <p> to the server-interlock problem that we have been discussing, however, the fact that SCI performs queueing in hardware means that it naturally handles incoming requests during periods of invalidation: SCI simply builds the so-called prepend queue of requesters at the head of a linked list even while interlocked for invalidation <ref> [98, 54] </ref>. 7 In fact, modern pipelines with out-of-order execution often have a limit to the number of outstanding requests for a similar reason. For instance, the R10000 has a maximum limit of four outstanding requests (including prefetches) at any one time [92]. Page 108 CHAPTER 3.
Reference: [99] <author> Steven L. Scott. </author> <title> Synchronization and Communication in the T3E Multiprocessor. </title> <booktitle> In Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2636, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: In this section, we would like to mention and examine some of these other research projects. 7.3.1 Hardware Integration of Communication Models Recent architectures demonstrate emerging agreement that it is important to integrate message-passing and shared-memory communication in the same hardware platform <ref> [3, 45, 93, 99, 40] </ref>. This current interest began, perhaps, with an Alewife paper on the advantages of integration [58]. However, other machines, such as the BB&N-Butterfly [17] supported integrated shared-memory and message-passing communication (via bulk DMA) long before this. <p> Although it remains to be seen, it is this author's feeling that exacerbating the memory bottleneck by placing software in latency-critical paths is undesirable from a performance standpoint. The Cray T3D and T3E <ref> [99] </ref> integrates message passing and hardware support for a shared address space. Message passing in the T3E is flexible and includes extensive support for DMA. <p> The *T NI [89] would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine [40] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces: Memory-based interfaces in multicomputers <ref> [12, 18, 97, 99, 104] </ref> and workstations [36, 37, 108, 112] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers.
Reference: [100] <author> C.L. Seitz, N.J. Boden, J. Seizovic, and W.K. Su. </author> <title> The Design of the Caltech Mosaic C Multicomputer. </title> <booktitle> In Research on Integrated Systems Symposium Proceedings, </booktitle> <pages> pages 122, </pages> <address> Cambridge, MA, 1993. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Some modern processors, such as Alewife's Sparcle processor [106], MOSAIC <ref> [100] </ref>, the MDP [35], and the UltraSPARC [111], can respond rapidly to interrupts. In particular, as discussed in Section 2.2, Alewife supports fast interrupt delivery via featherweight threads. This couples with efficient DMA to provide another advantage: virtual queuing.
Reference: [101] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-92-526, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Many of these numbers have been published elsewhere [3, 28], hence this is a summary of the results. One of the key contributers to the numbers presented in Section 6.2.1 was Ricardo Bianchini, who single-handedly ported a large number of the SPLASH <ref> [101] </ref> benchmarks. <p> Figure 6-6 presents the runtimes (in billions of cycles) of kernels for a number of different applications. Some of these are from the Stanford SPASH benchmark suite <ref> [101] </ref>, while others were written during the course of the Alewife project. Figure 6-7 shows the impact of two-case delivery on these execution times. The vertical axis in this case is in percentage of execution. <p> We implemented a complete system with custom chips, operating systems, and compilers that works and is usable: Ricardo Bianchini visited us from the university of Rochester one summer and was able to learn about Alewife, then port most of the SPLASH <ref> [101] </ref> benchmarks during his stay. In this sense, Alewife was tremendously more successful than ever anticipated. Ultimately, of course, there was a cost. Some members of the research group (this author among them) spent a long time in graduate school as a result. <p> Such costs will make message passing and specialized user-level protocols [39] increasingly important as processor speeds increase. Woo et al. [117] compared bulk transfer with shared memory on simulations of the FLASH multiprocessor [64] running the SPLASH <ref> [101] </ref> suite. They found bulk transfer performance to be disappointing due to the high cost of initiating transfer and the difficulty in finding computation to overlap with the transfer. Although, Alewife's DMA mechanism is cheaper to initiate than theirs, we also found bulk transfer to have performance problems.
Reference: [102] <author> B.J. Smith. </author> <title> Architecture and Applications of the HEP Multiprocessor Computer System. </title> <booktitle> Society of Photo-optical Instrumentation Engineers, </booktitle> <address> 298:241248, </address> <year> 1981. </year>
Reference-contexts: Thus, a block-multithreaded architecture ideally executes threads in quantum that are larger than a single cycle (i.e. threads are executed in blocks of time). This is one of the primary distinctions between a block-multithreaded processor and more hardware-intensive methods of multithreading such as present on MASA [44], HEP <ref> [102] </ref>, or Monsoon [88]. These other architectures can switch between a large number of active hardware threads on a cycle-by-cycle basis. 2.2.2 Scheduling for Featherweight Threads of a much larger set of runnable and suspended threads which are maintained by the operating system. <p> In Alewife, this is done by incorporating synchronization constructs directly into the cache/memory system: each memory word (of 32 bits) has an additional bit (called a full/empty bit <ref> [102, 5] </ref>) that is employed for synchronization. These full/empty bits are stored in memory and cached along with other data. To access these bits, Alewife attaches a synchronization operation field to each load and store operation. <p> The Sequent NUMA-Q machine takes the FLASH approach of providing a programmable memory controller, although this controller is not as well tuned as the FLASH controller resulting in long remote access times. A few architectures incorporate multiple contexts, pioneered by the HEP <ref> [102] </ref>, switching on every instruction. These machines, including Monsoon [88] and Tera [6], do not have caches and rely on a large number of contexts to hide remote memory latency. In contrast, Alewife's block multithreading technique switches only on synchronization faults and cache misses to re Page 228 CHAPTER 7.
Reference: [103] <author> J. E. Smith, G. E. Dermer, B. D. Vanderwarn, S. D. Klinger, C. M. Rozewski, D. L. Fowler, K. R. Scidmore, and J. P. Laudon. </author> <title> The ZS-1 Central Processor. </title> <booktitle> In Proceedings of the 2nd Annual Conference on Architectural Support for Programming Languages and Operatings Systems, </booktitle> <month> October </month> <year> 1987. </year>
Reference-contexts: One obvious solution to this type of problem is to decouple the processor and hardware through queues. Decoupled architectures such as the ZS-1 <ref> [103] </ref> and WM machine [118] were among the first to use queues and register interlocks to decouple load and store operations from computation operations.
Reference: [104] <author> Marc Snir and Peter Hochschild. </author> <title> The Communication Software and Parallel Environment of the IBM SP-2. </title> <type> Technical Report IBM-RC-19812, </type> <institution> IBM, IBM Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: The *T NI [89] would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine [40] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces: Memory-based interfaces in multicomputers <ref> [12, 18, 97, 99, 104] </ref> and workstations [36, 37, 108, 112] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers.
Reference: [105] <author> SPARC Architecture Manual, </author> <year> 1988. </year> <institution> SUN Microsystems, Mountain View, California. </institution>
Reference-contexts: The first of these is entered by the active message interrupt, and the second is entered by a special Trap instruction, shown at the end of ACT MESS ENTRY (), for returning to system level. Some understanding of SPARC instruction set is appropriate for understanding this code (see <ref> [105] </ref>); however, note that all control-flow instructions (branches, jumps, etc) have one delay slot. The high-level code flow during the execution of an active message handler is that we begin execution at ACT MESS ENTRY (). The jmpl instruction at line 20 calls the user handler code.
Reference: [106] <institution> MIT-SPARCLE Specification Version 1.1 (Preliminary). LSI Logic Corporation, Milpitas, </institution> <address> CA 95035, </address> <year> 1990. </year> <title> Addendum to the 64811 specification. </title>
Reference-contexts: Some modern processors, such as Alewife's Sparcle processor <ref> [106] </ref>, MOSAIC [100], the MDP [35], and the UltraSPARC [111], can respond rapidly to interrupts. In particular, as discussed in Section 2.2, Alewife supports fast interrupt delivery via featherweight threads. This couples with efficient DMA to provide another advantage: virtual queuing.
Reference: [107] <author> Steven Reinhart and James Larus and David Wood. </author> <title> A Comparison of Message Passing and Shared Memory for Data-Parallel Programs. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Related Work Page 231 al [48], which focuses exclusively upon shared-memory mechanisms. Although the Alewife machine provides an excellent starting point for the comparison of a large number of communication mechanisms, our results are greatly enhanced by our use of emulation, an approach inspired by the work at Wisconsin <ref> [107] </ref>. Chandra, Larus and Rogers compare four applications on a simulation of a message-passing machine similar to a CM-5 multiprocessor against a simulation of a hypothetical machine also similar to a CM-5, but extended by shared-memory hardware [96].
Reference: [108] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Efficient Support for Multicomputing on ATM Networks. </title> <institution> UW-CSE 93-04-03, University of Washington, </institution> <address> Seattle, WA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: The M-machine [40] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces: Memory-based interfaces in multicomputers [12, 18, 97, 99, 104] and workstations <ref> [36, 37, 108, 112] </ref> provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. Automatic hardware buffering also deals well with sinking bursts of messages and provides the lowest overhead (by avoiding the processors) when messages are not handled immediately.
Reference: [109] <author> Dean M. Tullsen, Susan J. Eggers, and Henry M. Levey. </author> <title> Simultaneous Multithreading: Maximizing On-Chip Parallelism. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392403, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: What does this mean for modern architectures? The UltraSPARC [111] supports register windows of the same type as Sparcle; featherweight threading could be utilized directly on Ultra-SPARC. Further, a recent and intriguing development in the design of dynamic pipelines is the notion of simultaneous multithreading <ref> [109] </ref>. Simultaneous multithreading (SMT) was proposed as a technique for increasing the utilization of functional units in a superscalar pipeline; it is rumored to be present in the early stages of new processor designs (such as the Alpha).
Reference: [110] <author> Ivan Tving. </author> <title> Multiprocessor Interconnection using SCI. </title> <type> PhD thesis, </type> <institution> Technical University of Denmark, Department of Computer Science, </institution> <month> August </month> <year> 1994. </year> <note> Page 246 BIBLIOGRAPHY </note>
Reference-contexts: There are a number of downsides to SCI, not the least of which is the fact that the protocol is quite complicated, requiring extensive automatic validation in order to ensure correctness (see <ref> [110] </ref>, for instance for a look at the set of bugs discovered in the base protocol after it had been out as a tentative standard).
Reference: [111] <author> UltraSPARC Programmer Reference Manual. Sun Microsystems, </author> <year> 1995. </year>
Reference-contexts: See Section 5.1.2. 2 In fact, as the implementation of the UltraSPARC demonstrated, multiple register sets are not an implementation bottleneck as had once been assumed <ref> [111] </ref>. Page 44 CHAPTER 2. <p> Some modern processors, such as Alewife's Sparcle processor [106], MOSAIC [100], the MDP [35], and the UltraSPARC <ref> [111] </ref>, can respond rapidly to interrupts. In particular, as discussed in Section 2.2, Alewife supports fast interrupt delivery via featherweight threads. This couples with efficient DMA to provide another advantage: virtual queuing. <p> Thus, the caching aspects of featherweight threading really requires at least two independent register sets. What does this mean for modern architectures? The UltraSPARC <ref> [111] </ref> supports register windows of the same type as Sparcle; featherweight threading could be utilized directly on Ultra-SPARC. Further, a recent and intriguing development in the design of dynamic pipelines is the notion of simultaneous multithreading [109].
Reference: [112] <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles. ACM, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: The M-machine [40] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces: Memory-based interfaces in multicomputers [12, 18, 97, 99, 104] and workstations <ref> [36, 37, 108, 112] </ref> provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. Automatic hardware buffering also deals well with sinking bursts of messages and provides the lowest overhead (by avoiding the processors) when messages are not handled immediately.
Reference: [113] <author> Thorsten von Eicken, David Culler, Seth Goldstein, and Klaus Schauser. </author> <title> Active Messages: </title>
Reference-contexts: Thus, efficient messaging facilities should permit direct transfer of information from registers to the network interface. Direct register-to-register transmission has been suggested by a number of architects <ref> [14, 35, 113, 46] </ref>. 2. Blocks of data which reside in memory often accompany such header information. Thus, efficient messaging facilities should allow direct memory access (DMA) mechanisms to be invoked inexpensively, possibly on multiple blocks of data. <p> Here, the messaging overheads are low enough that a thin layer of interrupt-driven operating-system software, can synthesize arbi trary network queueing structures in software. 4. Permitting compilers (or users) to generate network communications code has a number of advantages, as discussed in <ref> [113] </ref>, [14], and [47]. Compiler-generated code, however, requires user-level access to the message interface, including access to some form of atomicity mechanism to control the arrival of message interrupts. Given these observations, we can proceed to define a message-passing communication model for Alewife. <p> In contrast, when atomicity is disabled, the existence of an input message causes the cur traction of this message with a user-level interrupt handler. rent thread to be suspended and an independent handler thread to be initiated, much in the style of Active Messages <ref> [113] </ref>. The handler begins execution with atom-icity invoked, at the handler address specified in the message. A handler is assumed to extract one or more messages from the network before exiting, blocking, or disabling atomicity. <p> The User-Direct Messaging model and interfaces build on previous work in messaging models and mechanisms. Messaging Models: The User-Direct Messaging (UDM) model is similar to Active Messages <ref> [113] </ref> and related to Remote Queues (RQ) [16] as an efficient building-block for messaging within a protection domain. User-Direct Messaging differs from Active Messages [113] in that it includes explicit control over message delivery for efficiency; thus, for instance, users may choose to receive messages via polling or via interrupts. <p> Messaging Models: The User-Direct Messaging (UDM) model is similar to Active Messages <ref> [113] </ref> and related to Remote Queues (RQ) [16] as an efficient building-block for messaging within a protection domain. User-Direct Messaging differs from Active Messages [113] in that it includes explicit control over message delivery for efficiency; thus, for instance, users may choose to receive messages via polling or via interrupts. The relationship between User-Direct Messaging and RQ is not surprising given that this author participated in both models.
References-found: 112

