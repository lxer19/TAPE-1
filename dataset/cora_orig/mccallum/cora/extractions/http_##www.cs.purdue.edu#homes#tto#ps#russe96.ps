URL: http://www.cs.purdue.edu/homes/tto/ps/russe96.ps
Refering-URL: http://www.cs.purdue.edu/homes/tto/mypaps.html
Root-URL: http://www.cs.purdue.edu
Email: e-mail: ceco@iscbg.acad.bg  e-mail: luzz@sun2.dmu.dk  
Phone: 2  
Title: Using dense matrix computations in the solution of sparse problems  
Author: Tz. Ostromsky and Z. Zlatev Frederiksborgvej , P. O. 
Address: G.Bonchev str., bl. 25-A, 1113 Sofia, Bulgaria;  Box 358, DK-4000 Roskilde, Denmark;  
Affiliation: 1 Central Laboratory for Parallel Information Processing, Bulgarian Academy of Sciences, Acad.  National Environmental Research Institute,  
Abstract: On many high-speed computers the dense matrix technique is preferable to sparse matrix technique when the matrices are not very large, because the high computational speed compensates fully the disadvantages of using more arithmetic operations and more storage. Dense matrix techniques can still be used if the computations are successively carried out in a sequence of large dense blocks. A method based on this idea will be discussed.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Anderson, E., Bai, Z., Bischof C., Demmel J., Dongarra, J., Du Croz, J., Greenbaum, A., Hammarling, S., McKenney, A., Ostrouchov, S. and Sorensen, </author> <title> D, "LAPACK: Users' guide", </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: The blocks are handled as dense matrices. This is a trade off procedure: it is accepted to perform more computations, but with higher speed. All computations are performed by dense kernels (LAPACK subrou-tines, <ref> [1] </ref> are used at present, but other dense subroutines may also be applied). The dense kernels perform the most time-consuming part of the work; Table 3. Therefore, it should be expected to obtain good results on any computer for which high quality software for dense matrices is available.
Reference: 2. <author> Duff, I. S., Grimes, R. G. and Lewis, J. G., </author> <title> "Sparse matrix test problems", </title> <journal> ACM Trans. Math. Software, </journal> <volume> 15 (1989), </volume> <pages> 1-14. </pages>
Reference-contexts: A few results on POWER CHALLENGE from Silicon Graphics will also be presented. 3.1 Experiments with rectangular Harwell-Boeing matrices Results obtained when the two largest rectangular matrices from <ref> [2] </ref> have been run, are given in Table 2. The new algorithm has been compared with three other algorithms: (i) Totally dense. Matrix A is stored in a full-size two-dimensional array (the empty locations are filled with zeros). LAPACK is directly used to solve the problem. (ii) Partially sparse.
Reference: 3. <author> Duin, A. C. N. van, Hansen, P. C., Ostromsky, Tz., Wijsoff, H. and Zlatev, Z., </author> <title> "Improving the numerical stability and the performance of a parallel sparse solver", </title> <journal> Comput. Math. Applics., </journal> <volume> Vol. 30, No. 12 (1995), </volume> <pages> 81-96. </pages>
Reference-contexts: Results on CRAY C92A by using LAPACK dense subroutines. A new method will be described. The method is based on a reordering algorithm LORA, <ref> [3] </ref>, [4], which allows us to form easily a sequence of relatively large blocks. The blocks are handled as dense matrices. This is a trade off procedure: it is accepted to perform more computations, but with higher speed. <p> The algorithm consists of five steps: (i) reordering by LORA, (ii) scatter, (iii) compute, (iv) gather and (v) deal with the last block. The actions performed during the five steps are discussed in x2.1 - x2.5) 2.1 Using LORA for rectangular matrices LORA ("locally optimized reordering algorithm"; <ref> [3, 4] </ref>) reorders the matrix to a block upper triangular form (Fig. 1) with an important additional requirement to put as many zeros as possible under the diagonal blocks ("the separator"). The complexity of LORA is O (NZ log n) assuming that A has N Z non-zeros and n columns. <p> The complexity of LORA is O (NZ log n) assuming that A has N Z non-zeros and n columns. The complexity can be reduced to O (NZ). The more expensive version allows us to introduce additional criteria, <ref> [3] </ref>, by which the quality of the ordering is improved. A criterion that puts more non-zeros close to the diagonal blocks is applied here. 2.2 Scattering the non-zeros in a two dimensional array The next task is to form large dense block-rows.
Reference: 4. <author> Gallivan, K., Hansen, P. C., Ostromsky, Tz. and Zlatev, Z., </author> <title> "A locally optimized reordering algorithm and its application to a parallel sparse linear system solver", </title> <journal> Computing, </journal> <volume> 54 (1995), </volume> <pages> 39-67. </pages>
Reference-contexts: Results on CRAY C92A by using LAPACK dense subroutines. A new method will be described. The method is based on a reordering algorithm LORA, [3], <ref> [4] </ref>, which allows us to form easily a sequence of relatively large blocks. The blocks are handled as dense matrices. This is a trade off procedure: it is accepted to perform more computations, but with higher speed. <p> The algorithm consists of five steps: (i) reordering by LORA, (ii) scatter, (iii) compute, (iv) gather and (v) deal with the last block. The actions performed during the five steps are discussed in x2.1 - x2.5) 2.1 Using LORA for rectangular matrices LORA ("locally optimized reordering algorithm"; <ref> [3, 4] </ref>) reorders the matrix to a block upper triangular form (Fig. 1) with an important additional requirement to put as many zeros as possible under the diagonal blocks ("the separator"). The complexity of LORA is O (NZ log n) assuming that A has N Z non-zeros and n columns.
Reference: 5. <author> Zlatev, Z., </author> <title> "Computational methods for general sparse matrices", </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht-Toronto-London, </address> <year> 1991. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: This can cause difficulties, because the number of non-zeros per row is in general changed in the Step 3. Therefore some operations (moving rows to the end of the sparse arrays and even performing occasionally garbage collections; <ref> [5] </ref>) that are traditionally used in sparse techniques for general matrices must be carried in Step 4. This extra work can be reduced by (i) dropping small elements and (ii) avoiding the storage of Q i . <p> The PCG method is applicable, because C is symmetric and positive definite. C is never formed explicitly; one works the whole time with A and R. Q, which is normally rather dense, is neither stored nor used in the iterative process (see <ref> [5] </ref>). Dropping is very successful for some matrices (see x3.2), but it should not be used if the matrix is very ill-conditioned. Direct methods may work better in the latter case. <p> Givens rotations are used to produce zero elements. A Givens rotation is performed only if both leading elements of the two rows involved are non-zeros. (iii) Pure sparse. Without using dense matrix technique (the sparse algorithm used is discussed in <ref> [5] </ref>). <p> Computing times (on CRAY C92A) spent for the factorization of 30 matrices of class F2 (ALP HA=1 , N Z=N RflM + 110) with the new algorithm. 3.3 Dropping small elements and using PCG Dropping small elements and computing an approximate QR-factorization is discussed in <ref> [5] </ref>. It can be used (often with a great positive effect) in our new algorithm. The approximate R (obtained by dropping small non-zeros in Step 4) is used as a preconditioner; x2.4 . Sometimes this leads to considerable reductions of the computing time and/or the storage; see Tables 5, 6.
References-found: 5

