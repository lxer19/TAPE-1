URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/ps/KaSa93.ps
Refering-URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/sark_pub.html
Root-URL: 
Email: (karp@hpl.hp.com, vivek@paloalto.vnet.ibm.com)  
Title: Data Merging for Shared-Memory Multiprocessors  
Author: Alan H. Karp Vivek Sarkar 
Address: 1530 Page Mill Road, Palo Alto, California 94304  
Affiliation: IBM Scientific Center  
Abstract: We describe an efficient software cache consistency mechanism for shared memory multiprocessors that supports multiple writers and works for cache lines of any size. Our mechanism relies on the fact that, for a correct program, only the global memory needs a consistent view of the shared data between synchronization points. Our delayed consistency mechanism allows arbitrary use of data blocks between synchronizations. In contrast to other mechanisms, our mechanism needs no modification to the processor hardware or any assistance from the programmer or compiler; the processors can use normal cache management policies. Since no special action is needed to use the shared data, the processors are free to act almost as if they are all running out of a single cache. The global memory units are nearly identical to those on currently available machines. We need to add only a small amount of hardware and/or software to implement our mechanism. The mechanism can even be implemented using network connected workstations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita Adve and Mark Hill. </author> <title> Weak ordering | a new definition. </title> <booktitle> In Proc. of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 1-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: is write-back, i.e., a cache block is written to global memory only when its dirty bit is set and it has been replaced or invalidated. * If the processor stalls on a cache miss, the cache controller can still handle cache management re quests. * A delayed memory consistency model <ref> [1, 13] </ref> is assumed for data that is accessed by multiple processing elements. <p> There has recently been a great deal of interest in delayed consistency protocols. In these schemes, data update messages can be delayed till synchronization points thus making it possible to overlap more useful work with message latencies. The weak ordering model <ref> [1] </ref> can be summarized by three rules: a) accesses to synchronization variables are sequentially consistent, b) no global access can start till any prior access to a synchronization variable has completed, and c) all global accesses must complete before the next access to a synchronization variable is issued.
Reference: [2] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubi-atowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proc. of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Mirage [12] automatically locks pages, its unit of consistency, for a cer tain amount of time. Orca [4] uses reliable broadcast for both invalidate and update protocols. Amber [8] avoids the problem entirely by requiring the programmer to move data between processors. APRIL <ref> [2] </ref> guarantees sequential consistency, hiding the unavoidable delays with very fast context switching. There has recently been a great deal of interest in delayed consistency protocols. In these schemes, data update messages can be delayed till synchronization points thus making it possible to overlap more useful work with message latencies.
Reference: [3] <author> B. M. Bean et al. </author> <title> Bias Filter Memory for Filtering out Unnecesssary Interrogations of Cache Directories in a Multiprocessor System. </title> <type> U.S. Patent #4,142,234, </type> <month> February </month> <year> 1979. </year>
Reference-contexts: The prob lem is that every modification of shared data results in messages even when the other processors have no need to know the update was made. Performance can be improved by processing the invalidate requests before forwarding them to the cache <ref> [3] </ref> or by buffering them [10]. No matter what is done to reduce the impact of these messages on the cache, the large amount of unnecessary network traffic makes it unlikely that these schemes can be used to build scalable systems.
Reference: [4] <author> H. E. Bal and A. S. Tanenbaum. </author> <title> Distributed Programming with Shared Data. </title> <booktitle> In Proc. IEEE CS 1988 International Conference on Computer Languages, </booktitle> <pages> pages 82-91, </pages> <year> 1988. </year>
Reference-contexts: Clouds [18], which uses objects as its unit of consistency, allows the processor to lock the object to avoid cross invalidates. Mirage [12] automatically locks pages, its unit of consistency, for a cer tain amount of time. Orca <ref> [4] </ref> uses reliable broadcast for both invalidate and update protocols. Amber [8] avoids the problem entirely by requiring the programmer to move data between processors. APRIL [2] guarantees sequential consistency, hiding the unavoidable delays with very fast context switching.
Reference: [5] <author> P. Bitar and A. Despain. </author> <title> Multiprocessor Cache Synchronization: Issues, Innovations, Evolution. </title> <booktitle> In Proc. of the 13th Annual Int. Symp. on Comp. Architecture, </booktitle> <pages> pages 424-433, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: A simple approach is to bypass cache on all accesses to shared data. Unfortunately, the time to get to main memory is so large on today's machines, that this approach is not viable. Another simple approach is to keep each shared data element in a distinct cache block <ref> [5] </ref>. This approach can waste a lot of memory and destroy the spatial locality of reference in shared data. Two mechanisms have been proposed that maintain sequential consistency for cached data when the cache block is allowed to contain multiple shared data elements [7].
Reference: [6] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proc. of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: These delayed consistency solutions require modifications to the cache controller of each processor and that extra work be done. In addition, since synchronization variables control when the updates are propagated, they must be treated specially by the memory system. Munin <ref> [6] </ref> is a distributed shared memory system that allows shared memory parallel programs to be executed efficiently on distributed memory multiprocessors. Munin implements a release consistency model by placing a data object directory in each processor. <p> More recently, there has been work done on lazy release consistency (LRC) [15], a new algorithm for implementing release consistency that only performs modifications as needed. This algorithm is an extension of the release consistency algorithm in Munin's write-shared protocol <ref> [6] </ref>. Unlike previous implementations of release consistency that used the weaker ordering constraints only to hide latency, the LRC algorithm can also reduce the number of messages and the amount of data transferred for global memory accesses.
Reference: [7] <author> L. M. Censier and P. Feautrier. </author> <title> A New Solution to Coherence Problems in Multilevel Caches. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> De-cember </month> <year> 1978. </year>
Reference-contexts: B will then flush block 2 allowing A to proceed. Once A is running, it will flush data block 1 allowing B to proceed. 3 Related Work The problem of cache coherence in shared memory multiprocessors has been studied almost as long as there have been such machines <ref> [7, 16] </ref>. This early work concentrated on maintaining sequential consistency [16] in which the result of a run corresponds to some sequential ordering of the data accesses. A simple approach is to bypass cache on all accesses to shared data. <p> This approach can waste a lot of memory and destroy the spatial locality of reference in shared data. Two mechanisms have been proposed that maintain sequential consistency for cached data when the cache block is allowed to contain multiple shared data elements <ref> [7] </ref>. With invalidate on write, the processor writing a cache block sends an invalidate signal to the other processors. With update on write, the modification is sent to the other processors.
Reference: [8] <author> J. S. Chase, F. G. Amador, E. D. Lazowska, H. M. Levy, and R. J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proc. of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Clouds [18], which uses objects as its unit of consistency, allows the processor to lock the object to avoid cross invalidates. Mirage [12] automatically locks pages, its unit of consistency, for a cer tain amount of time. Orca [4] uses reliable broadcast for both invalidate and update protocols. Amber <ref> [8] </ref> avoids the problem entirely by requiring the programmer to move data between processors. APRIL [2] guarantees sequential consistency, hiding the unavoidable delays with very fast context switching. There has recently been a great deal of interest in delayed consistency protocols.
Reference: [9] <author> Myrias Corporation. </author> <title> System Overview. </title> <address> Edmon-ton, Alberta, </address> <year> 1990. </year>
Reference-contexts: Good performance is obtained if the programmer or compiler tags each object with its usage pattern, e.g., write once, write shared, reduction, producer consumer, etc. A similar scheme was used by the Myrias SPS with the copy on write and comparison being done in hardware <ref> [9] </ref>. Both the receive delayed and the send-and-receive delayed protocols need additional hardware support so the cache can know which data is stale [11]. A cache block becomes stale when an invalidate command is received from another processor.
Reference: [10] <author> M. Dubois and C. Scheurich. </author> <title> Memory Access Dependencies in Shared Memory Multiprocessors. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 16(6) </volume> <pages> 660-674, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The prob lem is that every modification of shared data results in messages even when the other processors have no need to know the update was made. Performance can be improved by processing the invalidate requests before forwarding them to the cache [3] or by buffering them <ref> [10] </ref>. No matter what is done to reduce the impact of these messages on the cache, the large amount of unnecessary network traffic makes it unlikely that these schemes can be used to build scalable systems.
Reference: [11] <author> M. Dubois, J. C. Wang, L. A. Barroso, K. Lee, and Y.-S. Chen. </author> <title> Delayed Consistency and its Effects on the Miss Rate of Parallel Programs. </title> <booktitle> In Proc. Supercomputing '91, </booktitle> <pages> pages 197-206, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: A similar scheme was used by the Myrias SPS with the copy on write and comparison being done in hardware [9]. Both the receive delayed and the send-and-receive delayed protocols need additional hardware support so the cache can know which data is stale <ref> [11] </ref>. A cache block becomes stale when an invalidate command is received from another processor. Any write to a stale block sends an invalidate to all other holders of the block and gets a fresh copy from memory.
Reference: [12] <author> B. D. Fleisch and G. J. Popek. </author> <title> Mirage: A Coherent Distributed Shared Memory Design. </title> <booktitle> In Proc. of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 211-223, </pages> <month> Decmeber </month> <year> 1989. </year>
Reference-contexts: Since large blocks are more likely to be falsely shared, it is left to the compiler or programmer to effectively align the data. Clouds [18], which uses objects as its unit of consistency, allows the processor to lock the object to avoid cross invalidates. Mirage <ref> [12] </ref> automatically locks pages, its unit of consistency, for a cer tain amount of time. Orca [4] uses reliable broadcast for both invalidate and update protocols. Amber [8] avoids the problem entirely by requiring the programmer to move data between processors.
Reference: [13] <author> K. Gharachorloo, D. Lenoski, J. Lanudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proc. of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: is write-back, i.e., a cache block is written to global memory only when its dirty bit is set and it has been replaced or invalidated. * If the processor stalls on a cache miss, the cache controller can still handle cache management re quests. * A delayed memory consistency model <ref> [1, 13] </ref> is assumed for data that is accessed by multiple processing elements. <p> If the system invalidates on write, cache blocks can "ping-pong" between two processors modifying distinct words in the block. If the system updates on write, both sending and receiving processors will be slowed because they must process irrelevant messages. Some systems, such as DASH <ref> [13] </ref>, use an invalidate on write protocol and make no special provisions for false sharing. A number of systems have been designed to maintain sequential consistency while reducing the performance impact of false sharing. Ivy [17] uses a write invalidate protocol on page size blocks. <p> In the release consistency model <ref> [13] </ref>, accesses to synchronization variables are categorized as acquire/release operations to further relax the ordering constraints. Rule b) is only applied to acquire operations, and rule c) is only applied to release operations.
Reference: [14] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: The main components of the multiprocessor model outlined in Figure 1 are <ref> [14] </ref>: * Processing Element (PE) | the processing unit that is replicated to obtain multiple processors in a multiprocessor system. Each PE consists of: 1. CPU | the central processing unit which contains control units, execution units, control and data registers, control and data paths, etc. 2. <p> We could also dedicate some PEs to serve as GMUs. We make the following assumptions about the com ponents listed above <ref> [14] </ref>: * The unit of transfer between local memory and global memory is a fixed-size block. We use the term data block to refer to a block in global memory, and the term cache block to refer to a copy of the block in a local memory.
Reference: [15] <author> Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proc. of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: All stale blocks become invalid at a synchronization point. Additional hardware is needed for an invalidation send buffer and support for partial updates to memory. More recently, there has been work done on lazy release consistency (LRC) <ref> [15] </ref>, a new algorithm for implementing release consistency that only performs modifications as needed. This algorithm is an extension of the release consistency algorithm in Munin's write-shared protocol [6]. <p> The basic data merging mechanism has no support for direct update of local memory. Table 1: Comparison between Lazy Release Consistency and Data Merging mechanisms similar to it, Munin.[6] The newer version of Munin using lazy release consistency <ref> [15] </ref> is much more complicated. The only fair comparison would be to our method with some of the extensions described in Section 5. Since we have not fully studied these extensions, we limit our comparison to be between the original Munin proposal and our simplest variant.
Reference: [16] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: B will then flush block 2 allowing A to proceed. Once A is running, it will flush data block 1 allowing B to proceed. 3 Related Work The problem of cache coherence in shared memory multiprocessors has been studied almost as long as there have been such machines <ref> [7, 16] </ref>. This early work concentrated on maintaining sequential consistency [16] in which the result of a run corresponds to some sequential ordering of the data accesses. A simple approach is to bypass cache on all accesses to shared data. <p> Once A is running, it will flush data block 1 allowing B to proceed. 3 Related Work The problem of cache coherence in shared memory multiprocessors has been studied almost as long as there have been such machines [7, 16]. This early work concentrated on maintaining sequential consistency <ref> [16] </ref> in which the result of a run corresponds to some sequential ordering of the data accesses. A simple approach is to bypass cache on all accesses to shared data. Unfortunately, the time to get to main memory is so large on today's machines, that this approach is not viable.
Reference: [17] <author> K. Li and P. Hudak. </author> <title> Memory coherence in Shared Virtual Memory Systems. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Some systems, such as DASH [13], use an invalidate on write protocol and make no special provisions for false sharing. A number of systems have been designed to maintain sequential consistency while reducing the performance impact of false sharing. Ivy <ref> [17] </ref> uses a write invalidate protocol on page size blocks. Since large blocks are more likely to be falsely shared, it is left to the compiler or programmer to effectively align the data.
Reference: [18] <author> U. Ramachandran, M. Ahamad, and M. Y. A. Khalidi. </author> <title> Coherence of Distributed Shared Memory: Unifying Synchronization and Data Transfer. </title> <booktitle> In Proc. of the 1989 Conference on Parallel Processing, </booktitle> <pages> pages II-160-II-169, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Ivy [17] uses a write invalidate protocol on page size blocks. Since large blocks are more likely to be falsely shared, it is left to the compiler or programmer to effectively align the data. Clouds <ref> [18] </ref>, which uses objects as its unit of consistency, allows the processor to lock the object to avoid cross invalidates. Mirage [12] automatically locks pages, its unit of consistency, for a cer tain amount of time. Orca [4] uses reliable broadcast for both invalidate and update protocols.
Reference: [19] <author> Leslie J. Toomey, Emily C. Plachy, Randolf G. Scarborough, Richard J. Sahulka, Jin F. Shaw, and Alfred W. Shannon. </author> <title> IBM Parallel Fortran. </title> <journal> IBM Systems Journal, </journal> <volume> 27(4) </volume> <pages> 416-435, </pages> <year> 1988. </year>
References-found: 19

