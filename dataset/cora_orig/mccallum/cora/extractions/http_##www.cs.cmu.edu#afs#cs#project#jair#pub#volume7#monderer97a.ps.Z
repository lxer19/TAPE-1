URL: http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume7/monderer97a.ps.Z
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/monderer97a.html
Root-URL: 
Email: dov@ie.technion.ac.il  moshet@ie.technion.ac.il  
Title: Dynamic Non-Bayesian Decision Making  
Author: Dov Monderer Moshe Tennenholtz 
Address: Haifa 32000, Israel  
Affiliation: Industrial Engineering and Management Technion Israel Institute of Technology  
Note: Journal of Artificial Intelligence Research 7 (1997) 231-248 Submitted 7/97; published 11/97  
Abstract: The model of a non-Bayesian agent who faces a repeated game with incomplete information against Nature is an appropriate tool for modeling general agent-environment interactions. In such a model the environment state (controlled by Nature) may change arbitrarily, and the feedback/reward function is initially unknown. The agent is not Bayesian, that is he does not form a prior probability neither on the state selection strategy of Nature, nor on his reward function. A policy for the agent is a function which assigns an action to every history of observations and actions. Two basic feedback structures are considered. In one of them the perfect monitoring case the agent is able to observe the previous environment state as part of his feedback, while in the other the imperfect monitoring case all that is available to the agent is the reward obtained. Both of these settings refer to partially observable processes, where the current environment state is unknown. Our main result refers to the competitive ratio criterion in the perfect monitoring case. We prove the existence of an efficient stochastic policy that ensures that the competitive ratio is obtained at almost all stages with an arbitrarily high probability, where efficiency is measured in terms of rate of convergence. It is further shown that such an optimal policy does not exist in the imperfect monitoring case. Moreover, it is proved that in the perfect monitoring case there does not exist a deterministic policy that satisfies our long run optimality criterion. In addition, we discuss the maxmin criterion and prove that a deterministic efficient optimal strategy does exist in the imperfect monitoring case under this criterion. Finally we show that our approach to long-run optimality can be viewed as qualitative, which distinguishes it from previous work in this area.
Abstract-found: 1
Intro-found: 1
Reference: <author> Alon, N., Spencer, J., & Erdos, P. </author> <year> (1992). </year> <title> The Probabilistic Method. </title> <publisher> Wiley-Interscience. </publisher>
Reference-contexts: Let " = ffi 8 . Define B K = t=1 1 " T for all T K : Roughly speaking, B K captures the cases where temporarily good actions are selected in most stages. By (Chernoff, 1952) (see also <ref> (Alon, Spencer, & Erdos, 1992) </ref>), for every T , P t=1 1 ")T e " 2 T Recall that given a set S, S denotes the complement of S.
Reference: <author> Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. </author> <year> (1995). </year> <title> Gambling in a rigged casino: The adversial multi-armed bandit problem. </title> <booktitle> In Proceedings of the 36th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 322-331. </pages>
Reference: <author> Aumann, R., & Maschler, M. </author> <year> (1995). </year> <title> Repeated Games with Incomplete Information. </title> <publisher> The MIT Press. </publisher>
Reference-contexts: See Aumann and Maschler (1995) for a comprehensive survey. Most of the above literature deals with (partially) Bayesian agents. Some of the rare exceptions are cited in Section 6. 5. Notice that the former assumption is very popular in the related game theory literature <ref> (Aumann & Maschler, 1995) </ref>. Many other intermediate monitoring structures may be interesting as well. 6. Such is also the case in the evolving literature on the problem of controlling partially observable Markov decision processes (Lovejoy, 1991; Cassandra, Kaelbling, & Littman, 1994; Monahan, 1982).
Reference: <author> Banos, A. </author> <year> (1968). </year> <title> On pseudo games. </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> 39, </volume> <pages> 1932-1945. </pages>
Reference: <author> Blackwell, D. </author> <year> (1956). </year> <title> An analog of the minimax theorem for vector payoffs. </title> <journal> Pacific Journal of Mathematic, </journal> <volume> 6, </volume> <pages> 1-8. </pages>
Reference: <author> Boutilier, C., & Poole, D. </author> <year> (1996). </year> <title> Computing optimal policies for partially observable decision processes using compact representations. </title> <booktitle> In Proceedings of the 13th National Conference on Artificial Intelligence, </booktitle> <pages> pp. 1168-1175. </pages>
Reference-contexts: Work on POMDP usually assumes that some observations about the current state may be available (following the presentation by Smallwood & Sondik, 1973), although observations about the previous state are discussed as well <ref> (Boutilier & Poole, 1996) </ref>. Recall that in the case of perfect monitoring the previous environment state is revealed, and the immediate reward is revealed in both prefect and imperfect monitoring. It may be useful to consider also situations where some 16.
Reference: <author> Cassandra, A., Kaelbling, L., & Littman, M. </author> <year> (1994). </year> <title> Acting optimally in partially observable stochastic domain. </title> <booktitle> In Proceedings of the 12th National Conference on Artificial Intelligence, </booktitle> <pages> pp. 1023-1028. </pages>
Reference: <author> Chernoff, H. </author> <year> (1952). </year> <title> A measure of the asymptotic efficiency for tests of a hypothesis based on the sum of observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23, </volume> <pages> 493-509. </pages>
Reference-contexts: Let " = ffi 8 . Define B K = t=1 1 " T for all T K : Roughly speaking, B K captures the cases where temporarily good actions are selected in most stages. By <ref> (Chernoff, 1952) </ref> (see also (Alon, Spencer, & Erdos, 1992)), for every T , P t=1 1 ")T e " 2 T Recall that given a set S, S denotes the complement of S.
Reference: <author> Fudenberg, D., & Levine, D. </author> <year> (1997). </year> <title> Theory of learning in games. </title> <publisher> miemo. </publisher>
Reference-contexts: A very partial list includes: (Shapley, 1953; Blackwell, 1956; Luce & Raiffa, 1957), and more recently (Fudenberg & Tirole, 1991; Mertens, Sorin, & Zamir, 1995), and the evolving literature on learning <ref> (e.g., Fudenberg & Levine 1997) </ref>. The incomplete information setup in which the player is ignorant about the game being played was inspired 232 Dynamic Non-Bayesian Decision Making an investor, I , who is investing daily in a certain index of the stock market.
Reference: <author> Fudenberg, D., & Tirole, J. </author> <year> (1991). </year> <title> Game Theory. </title> <publisher> MIT Press. </publisher>
Reference-contexts: When the reward function is not known to the agent we say that the agent has payoff uncertainty and we c fl1997 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. Monderer and Tennenholtz refer to the problem as a problem with incomplete information <ref> (Fudenberg & Tirole, 1991) </ref>. When modeling a problem with incomplete information one must also describe the underlying assumptions on the knowledge of the agent about the reward function.
Reference: <author> Hannan, J. </author> <year> (1957). </year> <title> Approximation to bayes risk in repeated play. </title> <editor> In Dresher, M., Tucker, A., & Wolfe, P. (Eds.), </editor> <title> Contributions to the Theory of Games, </title> <booktitle> vol. III (Annals of Mathematics Studies 39), </booktitle> <pages> pp. 97-139. </pages> <publisher> Princeton University Press. </publisher>
Reference: <author> Harsanyi, J. </author> <year> (1967). </year> <title> Games with incomplete information played by bayesian players, parts i, ii, </title> <booktitle> iii. Management Science, </booktitle> <volume> 14, </volume> <pages> 159-182. </pages>
Reference-contexts: In the other example, when Bob has to make his decision, if the situation is of imperfect monitoring, Bob would be only able to observe the reward for his behavior (e.g., whether by <ref> (Harsanyi, 1967) </ref>. See Aumann and Maschler (1995) for a comprehensive survey. Most of the above literature deals with (partially) Bayesian agents. Some of the rare exceptions are cited in Section 6. 5. Notice that the former assumption is very popular in the related game theory literature (Aumann & Maschler, 1995).
Reference: <author> Hart, S., & Mas-Colell, A. </author> <year> (1997). </year> <title> A simple adaptive procedure leading to correlated equilibrium. Discussion paper 126, Center for Rationality and Interactive Decision Theory, </title> <institution> Hebrew University. </institution>
Reference: <author> Kaelbling, L., Littman, M., & Moore, A. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 237-258. </pages>
Reference-contexts: The history of actions and states determines the immediate reward as well as the next one-shot decision problem. The history of actions and states also determines the next selected state. Work on reinforcement learning in artificial intelligence <ref> (Kaelbling, Littman, & Moore, 1996) </ref> has adopted the view of an agent operating in a probabilistic Bayesian setting, where the agent's last action and the last state determine the next environment state based on a given probability distribution.
Reference: <author> Kreps, D. </author> <year> (1988). </year> <title> Notes on the Theory of Choice. </title> <publisher> Westview press. 247 Monderer and Tennenholtz Lovejoy, W. </publisher> <year> (1991). </year> <title> A survey of algorithmic methods for partially observed markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 (1-4), </volume> <pages> 47-66. </pages>
Reference: <author> Luce, R. D., & Raiffa, H. </author> <year> (1957). </year> <title> Games and Decisions- Introduction and Critical Survey. </title> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: The above-mentioned setting is a classical static setting for decision making, where there is uncertainty about the actual state of nature <ref> (Luce & Raiffa, 1957) </ref>. In this paper we deal with a dynamic setup, in which the agent faces the decision problem D, without knowing the utility function u, over an infinite number of stages, t = 1; 2; : : :.
Reference: <author> Megiddo, N. </author> <year> (1980). </year> <title> On repeated games with incomplete information played by non-bayesian players. </title> <journal> International Journal of Game Theory, </journal> <volume> 9, </volume> <pages> 157-167. </pages>
Reference: <author> Mertens, J.-F., Sorin, S., & Zamir, S. </author> <year> (1995). </year> <title> Repeated games, part a. CORE, </title> <publisher> DP-9420. </publisher>
Reference: <author> Milnor, J. </author> <year> (1954). </year> <title> Games Against Nature. </title> <editor> In Thrall, R. M., Coombs, C., & Davis, R. (Eds.), </editor> <title> Decision Processes. </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: One of these approaches is the maxmin (safety-level) approach. According to this approach the agent would choose an action that maximizes his worst case payoff. Another approach is the competitive ratio approach (or its additive variant, termed the minmax regret decision criterion <ref> (Milnor, 1954) </ref>.
Reference: <author> Monahan, G. </author> <year> (1982). </year> <title> A survey of partially observable markov decision processes: Theory, models and algorithms. </title> <journal> Management Science, </journal> <volume> 28, </volume> <pages> 1-16. </pages>
Reference: <author> Papadimitriou, C., & Yannakakis, M. </author> <year> (1989). </year> <title> Shortest Paths Without a Map. </title> <booktitle> In Automata, Languages and Programming. 16th International Colloquium Proceedings, </booktitle> <pages> pp. 610-620. </pages>
Reference-contexts: In particular, we show that our approach to long-run optimality can be interpreted as 7. The competitive ratio decision criterion has been found to be most useful in settings such as on-line algorithms <ref> (e.g., Papadimitriou & Yanakakis, 1989) </ref>. 234 Dynamic Non-Bayesian Decision Making qualitative, which distinguishes it from previous work in this area. We also discuss some of the connections of our work with work in reinforcement learning. 2.
Reference: <author> Russell, S., & Norvig, P. </author> <year> (1995). </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall. </publisher>
Reference: <author> Savage, L. </author> <year> (1972). </year> <title> The Foundations of Statistics. </title> <publisher> Dover Publications, </publisher> <address> New York. </address>
Reference: <author> Shapley, L. </author> <year> (1953). </year> <title> Stochastic games. </title> <booktitle> Proceeding of the National Academic of Sciences (USA), </booktitle> <volume> 39, </volume> <pages> 1095-1100. </pages>
Reference-contexts: It may be useful to consider also situations where some 16. The results presented in this paper can be extended to the case where there is some randomness in the reward obtained by the agents as well. 17. Likewise, stochastic games <ref> (Shapley, 1953) </ref> can be considered as repeated games against Nature with partial information about Nature's strategy. For that matter one should redefine the concept of state in such games.
Reference: <author> Smallwood, R., & Sondik, E. </author> <year> (1973). </year> <title> The optimal control of partially observable markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21, </volume> <pages> 1071-1088. </pages>
Reference-contexts: Work on POMDP usually assumes that some observations about the current state may be available <ref> (following the presentation by Smallwood & Sondik, 1973) </ref>, although observations about the previous state are discussed as well (Boutilier & Poole, 1996). Recall that in the case of perfect monitoring the previous environment state is revealed, and the immediate reward is revealed in both prefect and imperfect monitoring.
Reference: <author> Sutton, R. </author> <year> (1992). </year> <title> Special issue on reinforcement learning. </title> <booktitle> Machine Learning, </booktitle> <pages> 8 (3-4). </pages>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Comm. ACM, </journal> <volume> 27 (11), </volume> <pages> 1134-1142. </pages>
Reference-contexts: Note that the function c (a; s) depends on the payoff function u and therefore so do the random variables X t and N t . 13. The interested reader may wish to think of our long-run optimality criteria in view of the original work on PAC learning <ref> (Valiant, 1984) </ref>. In our setting, as in PAC learning, we wish to obtain desired behavior, in most situations, with high probability, and relatively fast. 237 Monderer and Tennenholtz with probability one.
Reference: <author> Watkins, C., & Dayan, P. </author> <year> (1992). </year> <title> Technical note: </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 (3-4), </volume> <pages> 279-292. </pages>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning With Delayed Rewards. </title> <type> Ph.D. thesis, </type> <institution> Cambridge University. </institution>
Reference: <author> Wellman, M., & Doyle, J. </author> <year> (1992). </year> <title> Modular utility representation for decision-theoretic planning. </title> <booktitle> In Proceedings of the first international conference on AI planning systems. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Wellman, M. </author> <year> (1985). </year> <title> Reasoning about preference models. </title> <type> Tech. rep. </type> <institution> MIT/LCS/TR-340, Laboratory for Computer Science, MIT. </institution> <month> 248 </month>
References-found: 31

