URL: http://gn.www.media.mit.edu/groups/gn/publications/ObedMSThesis.ps
Refering-URL: http://gn.www.media.mit.edu/groups/gn/publications.html
Root-URL: http://www.media.mit.edu
Title: Producing Semantically Appropriate Gestures in Embodied Language Generation  ARCHITECTURE AND PLANNING,  
Author: by Obed E. Torres Stephen A. Benton 
Degree: IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF MASTER OF SCIENCE at the  1997 All Rights Reserved Signature of Author Program in Media Arts and Sciences  Certified by Justine Cassell  Arts and Sciences Thesis Supervisor Accepted by  Chairman, Department Committee on Graduate Students  
Note: SUBMITTED TO THE PROGRAM IN  AT&T Career Development Professor  Program in  
Date: 1994  February 1998  October 17, 1997  
Address: Puerto Rico,  
Affiliation: B.S. Computer Engineering University of  MEDIA ARTS AND SCIENCES, SCHOOL OF  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Massachusetts Institute of Technology  of Media Arts and Sciences Program in Media  Media Arts and Sciences  
Abstract-found: 0
Intro-found: 1
Reference: <author> Bers, J. </author> <year> (1995). </year> <title> Directing animated creatures through gesture and speech. M.S. </title> <type> thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference-contexts: That is, if information is missing from the content of speech, the interpreter searched for it in the other modalities (Koons 1994). Bers integrated the speech and pantomimic gestures of the user to direct animated creature behaviors such as the manner in which a bee moves its wings <ref> (Bers 1995) </ref>. 2.3. Gesture and Embodied Language Generation Research on autonomous agents, human-computer interfaces and virtual environments has stirred interest in embodied language generation. Traditionally, natural language generation has focused on the production of written text.
Reference: <author> Biederman, I. </author> <year> (1987). </year> <title> Recognition by components: a theory of human image understanding. </title> <journal> Psychological Review 94, </journal> <pages> 115-147. </pages>
Reference: <author> Bolt, R.A. </author> <year> (1980). </year> <title> Put-That-There: voice and gestures at the graphics interface. </title> <journal> Computer Graphics, </journal> <volume> 14(3), </volume> <pages> 262-70. </pages>
Reference-contexts: The Put-That-There system used speech recognition and a space sensing device to integrate the speech of the user with the position of the cursor to resolve references to items in a wall-sized screen <ref> (Bolt 1980) </ref>. In another system, speech and gestures were integrated to allow manipulation of graphical objects (Bolt & Herranz 1987). Koons built a multimodal interpreter of speech, gestures, and eye movements to resolve deictic references to objects in a wall-sized screen (Koons et. al. 1993).
Reference: <author> Bolt, R.A. </author> <year> (1987). </year> <title> The integrated multi-modal interface. </title> <journal> Transactions of the Institute of Electronics, Information and Communication Engineers (Japan), </journal> <volume> J79-D(11), </volume> <pages> 2017-2025. </pages>
Reference-contexts: The Put-That-There system used speech recognition and a space sensing device to integrate the speech of the user with the position of the cursor to resolve references to items in a wall-sized screen (Bolt 1980). In another system, speech and gestures were integrated to allow manipulation of graphical objects <ref> (Bolt & Herranz 1987) </ref>. Koons built a multimodal interpreter of speech, gestures, and eye movements to resolve deictic references to objects in a wall-sized screen (Koons et. al. 1993). In this system the speech modality drives the analysis of the integrated information.
Reference: <author> Bolt, R.A. & Herranz, E. </author> <year> (1992). </year> <title> Two-handed gestures in multi-modal natural dialog. </title> <booktitle> Proceedings of OISI 92, Fifth Annual Symposium on User Interface Software and Technology, </booktitle> <address> Monterey, CA. </address>
Reference: <author> Cassell, J & McNeill, D. </author> <year> (1991). </year> <title> Gesture and the poetics of prose. </title> <journal> Poetics Today, </journal> <volume> 12(3): </volume> <pages> 375-404. </pages>
Reference-contexts: McNeill and Levy show that gesture and speech convey different narrative information (McNeill & Levy 1982). Cassell and McNeill discuss how gesture often represents the point of view of the speaker when this is not present in the context of the speech <ref> (Cassell & McNeill 1991) </ref>. None of these accounts of the complementarity or redundancy of information across modalities identify how to determine the distribution of communicative load. That is, none of them explains how and what information is distributed across speech and gesture.
Reference: <author> Cassell, J., Pelachaud, C., Badler, N.I., Steedman, M., Achorn, B., Beckett, T., Douville, B., Prevost, S. & Stone, M. </author> <year> (1994a). </year> <title> Animated conversation: rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents. </title> <booktitle> Computer Graphics 94. </booktitle>
Reference-contexts: Animated Conversations was the first computational implementation of human-like figures conveying believable and contextually appropriate gestures, facial expressions, and spoken intonation <ref> (Cassell et. al. 1994a) </ref>. The domain of the dialogue was banking transactions. The scenario was a bank teller in a transactional conversation with a customer wanting to withdraw money. The implemented system used the timing of the intonation to determine the placement of gesture (Cassell et. al. 1994b).
Reference: <author> Cassell, J., Steedman, M., Badler, N., Pelachaud, C., Stone, M., Douville, B., Prevost, S., &Achorn, B. </author> <year> (1994b). </year> <title> Modeling the interactions between gesture and speech. </title> <booktitle> In Proceedings of the Cognitive Science Society Annual Conference, </booktitle> <address> Atlanta, GA. </address>
Reference-contexts: Gesture and speech do not always convey the same information about the meaning of a communicative intention. Gesture might depict aspects of meaning that were not described in speech. Kendon has suggested that what is difficult to express in speech may be conveyed by gesture <ref> (Cassell et. 1994b, Kendon 1994) </ref>. Visual information that is elusive to speech or dynamic scene features difficult to convey with speech, such as the simultaneity of two events or the spatial locations of objects, may be expressed by gesture (Cassell et. al. 1994b, Kendon 1994). <p> Visual information that is elusive to speech or dynamic scene features difficult to convey with speech, such as the simultaneity of two events or the spatial locations of objects, may be expressed by gesture <ref> (Cassell et. al. 1994b, Kendon 1994) </ref>. McNeill and Levy show that gesture and speech convey different narrative information (McNeill & Levy 1982). <p> Research on embodied language generation has contributed to the study of the relation of speech and gesture <ref> (Cassell et. al. 1994b, Cassell & Prevost under review) </ref>. Implemented systems for embodied language generation are able to test the placement and realization of gestural phenomena in relation to intonation and information structure (Cassell et. al. 1994b). <p> Implemented systems for embodied language generation are able to test the placement and realization of gestural phenomena in relation to intonation and information structure <ref> (Cassell et. al. 1994b) </ref>. However, there are other aspects of the relation between speech and gesture that a more comprehensive account of the interplay of speech and gesture should address. <p> The domain of the dialogue was banking transactions. The scenario was a bank teller in a transactional conversation with a customer wanting to withdraw money. The implemented system used the timing of the intonation to determine the placement of gesture <ref> (Cassell et. al. 1994b) </ref>. The system generated the distribution of gestures using information structure. Information structure describes the relation between the content of the utterance and the emerging discourse context. Gestures were generated along words or phrases that were marked as rhematic contributions to the discourse.
Reference: <author> Cassell, J. & Prevost S. </author> <year> (1996). </year> <title> Distribution of semantic features across speech and gesture by humans and computers. </title> <booktitle> Proceedings of the Workshop on the Integration of Gesture in Language and Speech. </booktitle> <address> Wilmington, DE. </address>
Reference-contexts: Recently, Cassell and Prevost hypothesized rules to predict when redundant and complementary information are conveyed in speech and gesture <ref> (Cassell & Prevost 1996, Cassell & Prevost under review) </ref>. <p> None of the accounts on the semantic relation between speech and gesture identify how to determine the distribution of communicative load. Research addressing the question of whether gestures are always redundant has dealt with the role of gesture in expressing concepts that are difficult to express in language <ref> (Cassell & Prevost 1996, McNeill 1996) </ref>. Such research has offered insights about certain semantic features expressed in gesture and not in speech, or expressed in both. <p> In their presentation of a framework for the generation of speech and gesture, Cassell and Prevost discuss a preliminary analysis of experimental data in which subjects were shown a segment of a Road Runner cartoon <ref> (Cassell & Prevost 1996, Cassell & Prevost under review) </ref>. Each subject told the story to a naive subject. A partial analysis of this experimental data allowed the formulation of several heuristics on the basis of information structure distinctions to predict when redundant and complementary information occurs. <p> The third section describes the methodological approach used to analyze the transcriptions. 3.1. Experimental Data The data used in this empirical study is a sample of the data collected during a preliminary experiment to examine associations between manner of motion verbs and gesture <ref> (Cassell & Prevost 21 1996) </ref>. All of it was recorded during narrative discourse. The speaker was brought into the laboratory area for video screening and shown segments of a Road Runner cartoon. <p> The Marked Features Hypothesis Cassell and Prevost suggest that redundancy across speech and gesture marks less predictable information <ref> (Cassell & Prevost 1996, Cassell & Prevost under review) </ref>. By appealing to this notion, I claim that a marked feature in a conceptual structure serves as a representation of what is distributed in gesture as redundant information.
Reference: <editor> Cassell, J. McNeill, D. & McCuloguh, K.E. (in press). Speech-gesture mismatches: </editor> <title> evidence for one underlying representation of linguistic and non-linguistic information. </title> <journal> Cognition. </journal>
Reference: <author> Cassell, J. </author> <title> (in press). A framework for gesture generation and interpretation. </title> <editor> In R. Cipolla and A. Pentland (eds.), </editor> <title> Computer Vision in Human-Machine Interaction. </title> <publisher> Cambridge University Press. </publisher>
Reference: <author> Cassell, J. & Prevost S. </author> <title> (under review). Embodied natural language generation: a framework for generating speech and gesture. </title>
Reference: <author> Church, R.B. & Goldin-Meadow, S. </author> <year> (1986). </year> <title> The mismatch between gesture and speech as an index of transitional knowledge. </title> <journal> Cognition, </journal> <volume> 23, </volume> <pages> 43-71. </pages>
Reference-contexts: Gesture can also provide an index to underlying reasoning processes of problem solving that are not present in the content of speech <ref> (Church & Goldin-Meadow 1986, Goldin-Meadow et. al. 1993) </ref>. Gestures are sometimes produced in the absence of a listener although more gestures are produced when a hearer is present (Cassell in press, Cohen & Harrison 1973, Cohen 1977, Rime & Sciaraturea 1991).
Reference: <author> Cohen, A.A. </author> <year> (1977). </year> <title> The communicative function of hand illustrators. </title> <journal> Journal of Communication, </journal> <volume> 27(4): </volume> <pages> 45-63. </pages>
Reference: <author> Cohen, A.A. & Harrison, R.P. </author> <year> (1973). </year> <title> Intentionality in the use of hand illustrators in face-to-face communication situations. </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> 28, </volume> <pages> 276-279. </pages>
Reference: <author> Ekman, P. and W. </author> <title> Friesen (1969). </title> <journal> The repertoire of nonverbal behavioral categories-origins, usages and coding. Semiotica, </journal> <volume> 1 </volume> <pages> 49-48. </pages>
Reference-contexts: is also synchronization between individual gestures and words, so that the most effortful part of the gesture occurs with or just before the most intonationally prominent syllable of the accompanying speech (Kendon 1972, McNeill 1992, Tuite 1993). 12 Ekman and Friesen identified five categories of nonverbal behaviors with semantic components <ref> (Ekman and Friesen 1969) </ref>. What they termed as illustrators, those nonverbal behaviors interpreted as being part of the speech performance, have been the object of further study mainly by Kendon and McNeill in their categorizations of gesture in relation to the content of speech (Ken-don 1980, McNeill 1992).
Reference: <author> Elhadad, M. </author> <year> (1993). </year> <title> FUF: the universal unifier - user manual, version 5.2. </title> <type> Technical Report CUCS-038-91, </type> <institution> Colum-bia University, </institution> <address> New York. </address> <note> 57 Elhadad, </note> <author> M. </author> & <title> Robin J (1996). An overview of SURGE: a reusable comprehensive syntactic realization component. </title> <type> Technical Report 96-03, </type> <institution> Mathematics and Computer Science Department, Ben Gurion University in the Negev, Israel. </institution>
Reference-contexts: The utterance planner extracts semantic information from conceptual structures and distributes their mapping into highly constrained sentential and gestural specifications. The mapping into sentential specifications is a conversion from identified types of sentences to an arrangement of specifications in the FUF (Functional Unification Formalism) attribute-value language format <ref> (Elhadad 1993) </ref>. Appendix C contains a sample of these lexicalized specifications as inputs to the FUF surface generator. FUF is a natural language generator program that uses the technique of unification grammars. Its two main components are a unifier and a linearizer.
Reference: <author> Elhadad, M., McKeown K. & Robin J. </author> <year> (1997). </year> <title> Floating constraints in lexical choice. </title> <booktitle> Computational Linguistics 23 (2), </booktitle> <pages> 195-239. </pages>
Reference: <author> Fillmore, C. J. </author> <year> (1982). </year> <title> Frame semantics. </title> <booktitle> In Linguistic Society of Korea (eds.), Linguistics in the Morning Calm. </booktitle> <publisher> Hanshin Publishing Co. </publisher>
Reference-contexts: Frame semantics defines the meaning of a word with reference to a structured background of experience, belief, and practices that motivate the concept encoded by the word <ref> (Fillmore 1982) </ref>. In frame semantics, the meaning of a lexical item is analyzed in terms of background frames of meaning structures associated to certain lexical and syntactic patterns.
Reference: <author> Goldin-Meadow S., Alibali M., Church B. </author> <year> (1993). </year> <title> Transitions in concept acquisition: using the hand to read the mind. </title> <journal> Psychological Review, </journal> <volume> 100 (2), </volume> <pages> 279-297. </pages>
Reference: <author> Gruber, J.S. </author> <year> (1965). </year> <title> Studies in lexical relations. </title> <type> Ph.D. thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge. </address>
Reference: <author> Halliday, M. </author> <year> (1967). </year> <title> Intonation and grammar in British English. </title> <publisher> Mouton. </publisher>
Reference-contexts: A partial analysis of this experimental data allowed the formulation of several heuristics on the basis of information structure distinctions to predict when redundant and complementary information occurs. Cassell and Prevost used theme and rheme to specify the information structural components of an utterance <ref> (Halliday 1967) </ref>. The thematic part of an utterance represents the chunk of information that links the utterance to the previous discourse and specifies what the utterance is about.
Reference: <author> Jackendoff, R. </author> <year> (1987). </year> <title> On beyond zebra: The relation of linguistic and visual information. </title> <journal> Cognition, </journal> <volume> 26, </volume> <pages> 89-114. </pages>
Reference-contexts: Conceptual semantics assumes that meanings are mentally encoded and decomposable which means that meanings have an internal structure built up from an innate framework of primitives and principles of combination <ref> (Jackendoff 1987) </ref>. Jackendoffs conceptual semantics establish a formal correspondence between external language, seen as an artifact, and internal language, seen as a body of internally encoded information (Jackendoff 1990). It develops a formal treatment of semantic intuitions. It accounts for the distinctions of meaning and semantic relations among words. <p> It makes explicit its relations to relevant results in the studies of perception and provides a rigorous formalism of primitive decompositions that forces analyses to be concrete and renders a testable theory <ref> (Jackendoff 1987, Jackendoff 1990) </ref>. 27 4. Distribution of Semantic Information Across Speech and Gesture Chapter 4 and 5 present hypotheses and discussions of instances in the sample of experimental data support or do not support the proposed hypotheses. There are a few overall remarks about the nature of these discussions. <p> The Natural Actions Hypothesis Jackendoff suggests that natural actions, like natural kinds, are learned more by ostension (This is what it looks like), exemplifying their appearance, than by definition <ref> (Jackendoff 1987) </ref>. By appealing to this notion, I claim that a spatial structure encoding linked to a conceptual structure of a natural action serves as a representation of what is distributed in gesture as complementary information. Jackendoffs conceptual structures encode meaning in an algebraic format. <p> Jackendoff defines a set of correspondence rules between conceptual structures and the spatial structure encoding as an extension of Marrs visual representation model <ref> (Jackendoff 1987, Marr 1982) </ref>. The spatial structure encoding is a 3D model representation where the primitive units are coordinate axes and means for generating simple shapes around them. <p> Cases of implicit arguments in the analyzed sample of experimental data were instances in which the information missing from speech was implicit information about external motion. The distinction of external motion serves to point to the motion of an entity as a whole, its traversal in a region <ref> (Jackendoff 1987) </ref>. Using a reference object, a region is defined to locate the entity in motion. Prepositions, as linguistic elements that express spatial relations, map the reference object to a path. <p> Cases of natural actions in the analyzed sample of experimental data were instances of internal motion information. The distinction of internal motion serves to point to those motions of an entity that are object-centered, internal dispositions such as bouncing, waving or spinning <ref> (Jackendoff 1987, Jackendoff 1990) </ref>. The natural actions hypothesis is supported by 80% of the instances of the analyzed sample of experimental data. One example of these instances is the utterance he unravels, where unravels is an intransitive verb. <p> An appropriate encoding scheme for these abstract semantic representations was adopted from Jackendoffs conceptual semantics because of its provisions for primitives that capture generalizations about action descriptions and reveal what semantic information is missing from, and what semantic information is marked in, the content of speech <ref> (Jackendoff 1987, Jackendoff 1990) </ref>. The utterance planner extracts semantic information from conceptual structures and distributes their mapping into highly constrained sentential and gestural specifications.
Reference: <author> Jackendoff, R. </author> <year> (1990). </year> <title> Semantic structures. </title> <publisher> MIT Press. </publisher>
Reference-contexts: The next chapter briey describes the analytical schemes provided by Talmy and Jackendoff. The work of Talmy and Jackendoff stands out when one considers the claim that when schemes for analyzing concepts of spatial location and motion are appropriately abstracted they can be generalized to parallel semantic fields <ref> (Jackendoff 1990, Gruber 1965) </ref>. This claim is based on the fact that many verbs and prepositions occur in two or more semantic fields and form intuitively related patterns (Jackendoff 1990). Jackendoff illustrates this with four semantic fields: spatial location and motion, possession, ascription of properties, and scheduling (Jackendoff 1990). <p> This claim is based on the fact that many verbs and prepositions occur in two or more semantic fields and form intuitively related patterns <ref> (Jackendoff 1990) </ref>. Jackendoff illustrates this with four semantic fields: spatial location and motion, possession, ascription of properties, and scheduling (Jackendoff 1990). As presented in (4), (5), (6) and (7), each of these semantic fields contains the verbs go or change, be, and keep. <p> This claim is based on the fact that many verbs and prepositions occur in two or more semantic fields and form intuitively related patterns <ref> (Jackendoff 1990) </ref>. Jackendoff illustrates this with four semantic fields: spatial location and motion, possession, ascription of properties, and scheduling (Jackendoff 1990). As presented in (4), (5), (6) and (7), each of these semantic fields contains the verbs go or change, be, and keep. The sentences with the go verb express change. The sentences with the be verb express terminal states. <p> Jackendoffs conceptual semantics establish a formal correspondence between external language, seen as an artifact, and internal language, seen as a body of internally encoded information <ref> (Jackendoff 1990) </ref>. It develops a formal treatment of semantic intuitions. It accounts for the distinctions of meaning and semantic relations among words. It also accounts for the relation between the formal treatment of meaning and the structure of syntax. <p> It situates its rigorous formalisms in an overall psychological framework that integrates linguistic theory with theories of perception and cognition. Jackendoffs conceptual semantics argues for the existence of essential units of conceptual structures as conceptual constituents <ref> (Jackendoff 1990) </ref>. These conceptual constituents belong to ontological categories such as THING, EVENT, STATE, PLACE, and PATH. Major syntactic parts of a sentence map into conceptual constituents. <p> Also, in Talmys work, the articulation of relations between semantic and perceptual components is tacit. On the contrary, Jackendoffs work on conceptual semantics provides general primitives and rigorous principles of combination to form representations of the meaning of linguistic expressions <ref> (Jackendoff 1990) </ref>. It makes explicit its relations to relevant results in the studies of perception and provides a rigorous formalism of primitive decompositions that forces analyses to be concrete and renders a testable theory (Jackendoff 1987, Jackendoff 1990). 27 4. <p> According to Peterson there is a class of natural actions analogous to natural kinds. Jackendoff proposes that conceptual structures for natural actions such as crawling, dancing, and squirming, are linked to a spatial structure encoding of categories of actions <ref> (Jackendoff 1990, Peterson 1987) </ref>. Jackendoff defines a set of correspondence rules between conceptual structures and the spatial structure encoding as an extension of Marrs visual representation model (Jackendoff 1987, Marr 1982). <p> The spatial structure encoding is a 3D model representation where the primitive units are coordinate axes and means for generating simple shapes around them. The Actions of moving figures are represented as sequences of motions of the figure parts of the 3D model <ref> (Jackendoff 1990, Marr and Vaina 1982) </ref>. Conceptual structures of information about natural actions include function-argument structures which are ultimately linked to spatial structure encoding. The spatial structure encoding linked to conceptual structures may serve as a representation of what is distributed in gesture as complementary information. <p> By appealing to this notion, I claim that distinctions of meaning about the trajectory of an object are realized in gesture as the shape of the hand motion. Jackendoffs conceptual structures use the PATH conceptual category to specify the trajectory of a thing in motion <ref> (Jack-endoff 1990, Jackendoff & Landau 1991) </ref>. Language specifies these trajectories using prepositions and verbs that express spatial relations. For example, to run by involves traversing a trajectory that at some point is near to some place. <p> Force dynamics involve the interaction of two protagonists. Jackendoffs conceptual structures use the AFF function, which stands for affect, to encode the dyadic relations of the protagonists according to Talmys theory of force dynamics configurations <ref> (Jackendoff 1990, Talmy 1988) </ref>. If the dyadic relation is one of opposition, one of the protagonists is the agonist and the other is the antagonist. The agonist has a tendency to perform or to not perform an action. The antagonist opposes the tendency.
Reference: <author> Jackendoff, R. & Landau, B. </author> <year> (1991). </year> <title> Spatial language and spatial cognition. </title> <editor> In D. Napoli & J. Kegl (eds.), </editor> <title> Bridges between Psychology and Linguistics. </title> <address> Hillsdale. </address>
Reference-contexts: This type of information has been termed natural kinds <ref> (Jackendoff & Landau 1991) </ref>. Distinctions of meaning among natural kinds are better represented in a terms of spatial structures than in terms of conceptual structures. According to Peterson there is a class of natural actions analogous to natural kinds. <p> Prepositions, as linguistic elements that express spatial relations, map the reference object to a path. The geometry of a path is sometimes specified by prepositions denoting its orientation, such as along, across, or around, or its distribution in a region, such as all over and throughout <ref> (Jackendoff & Landau 1991) </ref>. The implicit arguments hypothesis is supported by 84% of the implicit argument instances of the analyzed sample of experimental data. Two examples of these instances are the coyote comes out and the road runner returns. <p> Many objects are linguistically categorized on the basis of appearance. The translation of object shape descriptions into language is very difficult because language cannot convey all the richness of spatial representations of complex contours and edge patterns <ref> (Jackendoff & Landau 1991) </ref>. The relative sizes and proportions that characterize a shape are geometric notions far more suitably spelled out in a spatial representation than in language. Several researchers have proposed visual representations of the geometric properties of physical objects (Marr 1982, Biederman 1987). <p> The Trajectory Hypothesis Jackendoff and Landau suggest that language is crude in expressing information about distance and orientation, unless the speaker adopts a culturally stipulated system of measurements, which implies that language cannot convey many details of spatial representations <ref> (Jackendoff & Landau 1991) </ref>. By appealing to this notion, I claim that distinctions of meaning about the trajectory of an object are realized in gesture as the shape of the hand motion.
Reference: <author> Katz, J. J. & Fodor, J. A. </author> <year> (1963). </year> <title> The structure of a semantic theory. </title> <booktitle> Language, </booktitle> <volume> 39: </volume> <pages> 170-210. </pages>
Reference: <author> Kendon, A. </author> <year> (1972). </year> <title> Some relationships between body motion and speech. </title> <editor> In A.W. Siegman & B. Pope (eds.), </editor> <title> Studies in Dyadic Communication. </title> <publisher> Pergamon Press. </publisher>
Reference-contexts: This suggests the existence of an intimate link between the production of gesture and speech. Research shows that people produce the majority of gestures during speech articulation (McNeill 1992). These gestures expand and support the content of the accompanying speech <ref> (Kendon 1972, McNeill 1992) </ref>. They can also reinforce speech such as when there is alignment between the most effortful parts of the gestures and intonational prominence (Kendon 1980). <p> They can also reinforce speech such as when there is alignment between the most effortful parts of the gestures and intonational prominence (Kendon 1980). Several researchers have argued that gesture and speech are different communicative expressions of a single mental representation <ref> (Kendon 1972, McNeill 1992, McNeill 1996, Cassell et. al. in press) </ref>. Gesture and speech do not always convey the same information about the meaning of a communicative intention. Gesture might depict aspects of meaning that were not described in speech. <p> researchers, speech and gesture arise from a single underlying representation, then a more comprehensive account of the semantic relation between speech and gesture should explain the expressive demands of that representation; which features are expressed in speech and which in gesture; and how they are expressed in speech and gesture <ref> (Kendon 1972, McNeill 1992, McNeill 1996, Cassell et. al. in press) </ref>. This thesis addresses these research questions within the context of producing semantically appropriate gestures in embodied language generation. In particular, it addresses what content is conveyed in gesture and in which gestural forms. <p> This previous work is related to or supports research on the semantic relation between speech and gesture. 2.1. Gesture and Communication People spontaneously produce gestures while speaking to expand and support accompanying words <ref> (Kendon 1972, McNeill 1992) </ref>. This elaboration and enhancement of the content of speech provides an index to the underlying thematic organization of the discourse and point of view in events (McNeill 1992). <p> There is also synchronization between individual gestures and words, so that the most effortful part of the gesture occurs with or just before the most intonationally prominent syllable of the accompanying speech <ref> (Kendon 1972, McNeill 1992, Tuite 1993) </ref>. 12 Ekman and Friesen identified five categories of nonverbal behaviors with semantic components (Ekman and Friesen 1969). <p> If speech and gesture arise from a single underlying representation, then a more comprehensive account of the semantic relation between speech and gesture should explain the expressive demands of that representation; which features are expressed in speech and which in gesture; and how they are expressed in speech and gesture <ref> (Kendon 1972, McNeill 1992, McNeill 1996, Cas-sell et. al. in press) </ref>. This thesis addresses these research questions within the context of producing semantically appropriate gestures in embodied language generation. In particular, it addresses what content is conveyed in gesture and in which gestural forms.
Reference: <author> Kendon, A. </author> <year> (1980). </year> <title> Gesticulation and speech: two aspect of the process of utterance. In M.R. Key (ed.), The Relation Between Verbal and Nonverbal Communication. </title> <publisher> Mouton. </publisher>
Reference-contexts: These gestures expand and support the content of the accompanying speech (Kendon 1972, McNeill 1992). They can also reinforce speech such as when there is alignment between the most effortful parts of the gestures and intonational prominence <ref> (Kendon 1980) </ref>. Several researchers have argued that gesture and speech are different communicative expressions of a single mental representation (Kendon 1972, McNeill 1992, McNeill 1996, Cassell et. al. in press). Gesture and speech do not always convey the same information about the meaning of a communicative intention.
Reference: <author> Kendon, A. </author> <year> (1986). </year> <title> Current issues in the study of gesture. </title> <editor> In J. Nespoulous, P. Perron & A. Lecours (eds.), </editor> <booktitle> The Biological Foundations of Gestures: Motor and Semiotic Aspects. </booktitle> <address> Hillsdale. </address>
Reference-contexts: A more comprehensive account of the semantic complementarity and redundancy among modalities has important implications for the study of representational processes and the nature of language, and issues such as investigating whether visual ideas are encoded as spatial structures or if representations are modality-specific 8 <ref> (Kendon 1986) </ref>. Recently, Cassell and Prevost hypothesized rules to predict when redundant and complementary information are conveyed in speech and gesture (Cassell & Prevost 1996, Cassell & Prevost under review).
Reference: <author> Kendon, A. </author> <year> (1994). </year> <title> Do gestures communicate: A review. Research on Language and Social Interaction, </title> <type> 27. </type>
Reference-contexts: The Shape Hypothesis Kendon suggests that what is difficult to express in speech may be conveyed by gesture, including spatial information that is elusive to speech <ref> (Kendon 1994, Cassell et. al. 1994b) </ref>. By appealing to this notion, I claim that distinctions of meaning about the shape of an object are realized in gesture as the shape of the hand. Shape is an important element in the criteria for identification of object categories (Landau et. al. 1988). <p> If a gesture accompanies this sentence, it is likely to include a gesture with a curved hand trajectory. 39 5.3. The Interaction Dyad Hypothesis Kendon suggests that what is difficult to be expressed in speech may be conveyed by gesture, including dynamic features elusive to speech <ref> (Kendon 1994, Cassell et. al. 1994b) </ref>. By appealing to this notion, I claim that semantic distinctions about interaction dyads of force dynamics are realized in the arrangement of the number of hands. Force dynamics involve the interaction of two protagonists.
Reference: <author> Koons, D., Sparrell, C. & Thorisson, </author> <title> K (1993). Integrating simultaneous input from speech, gaze and hand gestures. </title>
Reference-contexts: In another system, speech and gestures were integrated to allow manipulation of graphical objects (Bolt & Herranz 1987). Koons built a multimodal interpreter of speech, gestures, and eye movements to resolve deictic references to objects in a wall-sized screen <ref> (Koons et. al. 1993) </ref>. In this system the speech modality drives the analysis of the integrated information. That is, if information is missing from the content of speech, the interpreter searched for it in the other modalities (Koons 1994).
Reference: <editor> In M. Maybury (ed.), </editor> <booktitle> Intelligent MultiMedia Interfaces. </booktitle> <address> Cambridge, MA. </address>
Reference: <author> Koons D. </author> <year> (1994). </year> <title> Capturing and interpreting multi-modal descriptions with multiple representations. </title> <booktitle> AAAI Symposium on Intelligent Multi-Modal Multi-Media Interface Systems. </booktitle> <address> Stanford, CA. </address>
Reference-contexts: In this system the speech modality drives the analysis of the integrated information. That is, if information is missing from the content of speech, the interpreter searched for it in the other modalities <ref> (Koons 1994) </ref>. Bers integrated the speech and pantomimic gestures of the user to direct animated creature behaviors such as the manner in which a bee moves its wings (Bers 1995). 2.3.
Reference: <author> Landau, B., Smith, L., & Jones, S. </author> <year> (1988). </year> <title> The importance of shape in early lexical learning. </title> <journal> Cognitive Development, </journal> <volume> 3, </volume> <pages> 299-321. </pages>
Reference-contexts: By appealing to this notion, I claim that distinctions of meaning about the shape of an object are realized in gesture as the shape of the hand. Shape is an important element in the criteria for identification of object categories <ref> (Landau et. al. 1988) </ref>. Many objects are linguistically categorized on the basis of appearance. The translation of object shape descriptions into language is very difficult because language cannot convey all the richness of spatial representations of complex contours and edge patterns (Jackendoff & Landau 1991).
Reference: <author> Lasher, R. </author> <year> (1981). </year> <title> The cognitive representation of an event involving human motion. </title> <journal> Cognitive Psychology, </journal> <volume> 13, </volume> <pages> 391-406. </pages>
Reference: <author> Marr, D. </author> <year> (1982). </year> <title> Vision. </title> <publisher> Freeman. </publisher>
Reference-contexts: The relative sizes and proportions that characterize a shape are geometric notions far more suitably spelled out in a spatial representation than in language. Several researchers have proposed visual representations of the geometric properties of physical objects <ref> (Marr 1982, Biederman 1987) </ref>. In these representations objects are decomposed into parts and spatial axes that organize the parts of the objects and relations among them (Jack-endoff & Landau 1991).
Reference: <author> Marr, D., & Vaina, L. </author> <year> (1982). </year> <title> Representation and recognition of the movements of shapes. </title> <journal> Proceedings of the Royal Society of London, </journal> <volume> 214, </volume> <pages> 501-524. </pages> <address> 58 McNeill, D. </address> <year> (1996). </year> <title> Language as gesture (gesture as language). </title> <booktitle> In Proceedings of the Workshop on the Integration of Gesture in Language and Speech, </booktitle> <address> Wilmington, DE. </address>
Reference-contexts: The relative sizes and proportions that characterize a shape are geometric notions far more suitably spelled out in a spatial representation than in language. Several researchers have proposed visual representations of the geometric properties of physical objects <ref> (Marr 1982, Biederman 1987) </ref>. In these representations objects are decomposed into parts and spatial axes that organize the parts of the objects and relations among them (Jack-endoff & Landau 1991).
Reference: <author> McNeill, D. </author> <year> (1992). </year> <title> Hand and mind: what gestures reveal about thought. </title> <publisher> University of Chicago Press. </publisher>
Reference-contexts: They occur at the same time as speech and carry a meaning associated with the content of speech. This suggests the existence of an intimate link between the production of gesture and speech. Research shows that people produce the majority of gestures during speech articulation <ref> (McNeill 1992) </ref>. These gestures expand and support the content of the accompanying speech (Kendon 1972, McNeill 1992). They can also reinforce speech such as when there is alignment between the most effortful parts of the gestures and intonational prominence (Kendon 1980). <p> Gesture and Communication People spontaneously produce gestures while speaking to expand and support accompanying words (Kendon 1972, McNeill 1992). This elaboration and enhancement of the content of speech provides an index to the underlying thematic organization of the discourse and point of view in events <ref> (McNeill 1992) </ref>. Gesture can also provide an index to underlying reasoning processes of problem solving that are not present in the content of speech (Church & Goldin-Meadow 1986, Goldin-Meadow et. al. 1993). <p> Gestures generally co-occur with their semantically parallel linguistic units, except in the cases of pauses or syntactically complex speech, in which case the gesture occurs first <ref> (McNeill 1992) </ref>. <p> According to McNeill, 90% of all gestures occur within the context of speech <ref> (McNeill 1992) </ref>. These spontaneous gestures are of four types: iconic, metaphoric, deictic, and beat. A fundamental assumption underlying the division of these categories is that the speaker is mapping an internal representation into patterned hand motor articulations. <p> Fig. 1 he unravels 3.2. Transcription Procedures The procedures for transcribing the samples of iconic gestures are based on McNeills coding scheme <ref> (McNeill 1992) </ref>. The main coding procedures facilitate transcribing experimental data about the hands, motion, and meaning. For the hands, the following aspects were coded: which hands were used, shape of the hand, and palm and finger orientations. <p> Most of the hand shape specifications were approximations to ASL hand shapes (e.g. A, C, 5, O, and G) that are likely to occur in all narratives <ref> (McNeill 1992) </ref>.
Reference: <author> McNeill, D & Levy, E. </author> <year> (1982). </year> <title> Conceptual representations in language and gesture. </title> <editor> In R. J. Jarvella & W. Klein (eds.), </editor> <booktitle> Speech, Place and Action, </booktitle> <pages> 271-295. </pages>
Reference-contexts: McNeill and Levy show that gesture and speech convey different narrative information <ref> (McNeill & Levy 1982) </ref>. Cassell and McNeill discuss how gesture often represents the point of view of the speaker when this is not present in the context of the speech (Cassell & McNeill 1991).
Reference: <author> Miller, G.A. & Johnson-Laird, P. N. </author> <year> (1976). </year> <title> Language and Perception. </title> <publisher> Harvard University Press. </publisher>
Reference-contexts: In componential semantics, the meaning of a lexical item is analyzed in terms of terminal elements of feature decompositions. Procedural semantics defines the meaning of words in terms of a set of instructions related to the usage of the word <ref> (Miller & Johnson-Laird 1976) </ref>. In procedural semantics, the meaning of a lexical item is analyzed in terms of procedure extractions in the process of lexical recognition or production.
Reference: <author> Murakami, K. & Taguchi, H. </author> <year> (1991). </year> <title> Gesture recognition using recurrent neural networks. </title> <booktitle> CHI 91 Proceedings. </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: The finitude of gestural languages makes them suitable to template-based matching. Murakami and Taguchi implemented a gesture recognizer of alphabet gestures in the Japanese Sign Language <ref> (Murakami & Taguchi 1991) </ref>. Vaananen and Bohm also built a system using a neural network recognizer to map specific 14 hand configurations to system commands (Vaananen & Bohm 1993).
Reference: <author> Pelachaud, C., Badler N., & Steedman. </author> <year> (1996). </year> <title> Generating facial expressions for speech. </title> <journal> Cognitive Science, </journal> <volume> 20(1). </volume>
Reference: <author> Peterson, P. </author> <year> (1985). </year> <title> Causation, agency, </title> <booktitle> and natural actions. In Chicago Linguistic Society, 21st Region and Parases-sion on Causative and Agentivity, </booktitle> <institution> Department of Linguistics, University of Chicago. </institution>
Reference: <author> Prevost, S. </author> <year> (1996). </year> <title> An information structural approach to spoken language generation. </title> <booktitle> Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics. </booktitle>
Reference-contexts: Recently, Cassell and Prevost hypothesized rules to predict when redundant and complementary information are conveyed in speech and gesture <ref> (Cassell & Prevost 1996, Cassell & Prevost under review) </ref>. <p> None of the accounts on the semantic relation between speech and gesture identify how to determine the distribution of communicative load. Research addressing the question of whether gestures are always redundant has dealt with the role of gesture in expressing concepts that are difficult to express in language <ref> (Cassell & Prevost 1996, McNeill 1996) </ref>. Such research has offered insights about certain semantic features expressed in gesture and not in speech, or expressed in both. <p> In their presentation of a framework for the generation of speech and gesture, Cassell and Prevost discuss a preliminary analysis of experimental data in which subjects were shown a segment of a Road Runner cartoon <ref> (Cassell & Prevost 1996, Cassell & Prevost under review) </ref>. Each subject told the story to a naive subject. A partial analysis of this experimental data allowed the formulation of several heuristics on the basis of information structure distinctions to predict when redundant and complementary information occurs. <p> The third section describes the methodological approach used to analyze the transcriptions. 3.1. Experimental Data The data used in this empirical study is a sample of the data collected during a preliminary experiment to examine associations between manner of motion verbs and gesture <ref> (Cassell & Prevost 21 1996) </ref>. All of it was recorded during narrative discourse. The speaker was brought into the laboratory area for video screening and shown segments of a Road Runner cartoon. <p> The Marked Features Hypothesis Cassell and Prevost suggest that redundancy across speech and gesture marks less predictable information <ref> (Cassell & Prevost 1996, Cassell & Prevost under review) </ref>. By appealing to this notion, I claim that a marked feature in a conceptual structure serves as a representation of what is distributed in gesture as redundant information.
Reference: <author> Quek, F. </author> <year> (1994). </year> <title> Toward a vision-based hand gesture interface. </title> <booktitle> In Proceedings of the Virtual Reality System Technology Conference, </booktitle> <address> Singapore. </address>
Reference: <author> Rime, B & Schiaraturea, L. </author> <year> (1991). </year> <title> Gesture and speech. In R.S. </title> <editor> Feldman & B. Rime (eds.), </editor> <booktitle> Fundamentals of Nonverbal Behavior. </booktitle> <publisher> Cambridge University Press. </publisher>
Reference-contexts: Research shows that there is more display of gesture when the verbal task entails mental images of action and objects that include visual, spatial, and motoric information <ref> (Rime & Schiaraturea 1991) </ref>. The inputs to this prototype are conceptual representations of a car accident state or event description. As an output it produces an animated descriptive utterance through the coordinated animation of gesture realization in an upper-body figure and synthesis of speech. 6.2.
Reference: <author> Rogers, W. </author> <year> (1978). </year> <title> The contribution of kinesic illustrators toward the comprehension of verbal behavior within utterances. </title> <journal> Human Communication Research, </journal> <volume> 5 </volume> <pages> 54-62. </pages>
Reference: <author> Saint, P. & Viegas, E. </author> <year> (1995). </year> <title> An introduction to lexical semantics from a linguistic and a psycholinguistic perspective. </title> <editor> In P. Saint & E. Viegas (eds.), </editor> <title> Computational Lexical Semantics. </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: In frame semantics, the meaning of a lexical item is analyzed in terms of background frames of meaning structures associated to certain lexical and syntactic patterns. Lexical semantics defines the meaning of a word in terms of representations of the world and its relation to language <ref> (Saint & Viegas 1995) </ref>. In lexical semantics, the meaning of a lexical item is analyzed in terms of the forms of semantic representations and the relations between semantic and syntactic levels of representation.
Reference: <author> Short, J., Williams, E. & Christie, B. </author> <year> (1976). </year> <title> The social psychology of telecommunications. </title> <publisher> Wiley. </publisher>
Reference: <author> Starner T. & Pentland A. </author> <year> (1995). </year> <title> Visual recognition of American Sign Language using Hidden Markov Models. </title> <booktitle> International Workshop on Automatic Face and Gesture Recognition. </booktitle> <address> Zurich, Switzerland. </address>
Reference: <author> Talmy, L. </author> <year> (1985). </year> <title> Lexicalization patterns: semantic structure in lexical form. </title> <editor> In T. Shopen (ed.), </editor> <title> Grammatical categories and the lexicon, volume 3 of Language typology and syntactic description. </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: In lexical semantics, the meaning of a lexical item is analyzed in terms of the forms of semantic representations and the relations between semantic and syntactic levels of representation. Work on lexical semantics by Talmy and Jackendoff has provided schemes for analyzing concepts of spatial location and motion <ref> (Talmy 1985, Jackendoff 1990) </ref>. Talmys work provides important analyses of spatial concepts in relation to other semantic com 18 ponents and cross-language grammars. Jackendoffs conceptual semantics establishes a formal correspondence between external language, seen as an artifact, and internal language, seen as a body of internally encoded information. <p> The domain of meaning entailed examining semantic features associated with sketching motion events using schemes for analyzing from Jackendoffs and Talmys work on lexical semantics <ref> (Talmy 1985, Jackendoff 1990) </ref>. For gestures, examination of the domain of expression involved analyzing the forms of the gestures as instantiated in hand shapes and hand trajectories. For lexical items, examination of the domain of expression involved analyzing motion verbs and the prepositions associated with them. <p> This approach to exploring relations between meaning and form expression has been effectively employed to conduct crosslinguistic characterization of lexicalization patterns and in tackling the computationally complex task of lexical choice for language generation <ref> (Talmy 1985, Elhadad et. al. in press) </ref>. Talmys work on the semantic patterns of motion events offers a scheme of analysis to examine spatial concepts of motion and location (Talmy 1985). It provides important analyses of spatial concepts in relation to other semantic components and cross-language grammars. <p> Talmys work on the semantic patterns of motion events offers a scheme of analysis to examine spatial concepts of motion and location <ref> (Talmy 1985) </ref>. It provides important analyses of spatial concepts in relation to other semantic components and cross-language grammars. According to Talmy, a motion event is a situation in which there is movement and/or the maintenance of a stationary location.
Reference: <author> Talmy, L. </author> <year> (1988). </year> <title> Force dynamics in language and thought. </title> <booktitle> Cognitive Science 12, </booktitle> <pages> 49-100. </pages>
Reference: <author> Thompson, L. & Massaro, D. </author> <year> (1986). </year> <title> Evaluation and integration of speech and pointing gestures during referential understanding. </title> <journal> Journal of Experimental Child Psychology, </journal> <volume> 42: </volume> <pages> 144-168. </pages>
Reference: <author> Thorisson, K. </author> <year> (1996). </year> <title> Communicative humanoids: a computational model of psychosocial dialogue skills. </title> <type> Ph.D. thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge. </address>
Reference-contexts: User testing of embodied human-like agents that exhibit face-to-face conversational behaviors 15 has shown that the presence of nonverbal feedback behaviors increases believability and effectiveness in the interaction <ref> (Thorisson 1996) </ref>. Animated Conversations was the first computational implementation of human-like figures conveying believable and contextually appropriate gestures, facial expressions, and spoken intonation (Cassell et. al. 1994a). The domain of the dialogue was banking transactions.
Reference: <author> Torres, O., Cassell, J. & Prevost, S. </author> <year> (1997). </year> <title> Modeling gaze behavior as a function of discourse structure. </title> <booktitle> First International Workshop on Human-Computer Conversation. </booktitle> <address> Bellagio, Italy. </address>
Reference: <author> Tuite, K. </author> <year> (1993). </year> <title> The production of gesture. </title> <journal> Semiotica, </journal> <volume> 93(1/2). 59 Vaananen & Bohm, </volume> <editor> K. </editor> <year> (1993). </year> <title> Gesture-driven interaction as a human factor in virtual environments - an approach with neural networks. </title> <editor> In R.A. Earnshaw, M.A. Gigante & H. Jones (eds.), </editor> <title> Virtual Reality Systems. </title> <publisher> Academic Press. </publisher>
Reference: <author> Walker M. </author> <year> (1993). </year> <title> Informational redundancy and resource bounds in dialogue. </title> <type> Ph.D thesis, </type> <institution> University of Pennsyl-vania, </institution> <address> Philadelphia. </address>
Reference-contexts: For both of these the missing information is available from the context. The information exhibited by gesture may be an aid for conversants to reduce the processing time to retrieve information that they know or could infer <ref> (Walker 1993) </ref>. In the case of the utterance the coyote comes out, its conceptual representation is of the function-argument structure of type (4) which after argument substitutions takes the form of (10). The gesture accompanying the sentence was a left hand upwardly moving from left waist to center upper chest.
Reference: <author> Williams, E. </author> <year> (1977). </year> <title> Experimental comparisons of face-to-face and mediated communication: A review. </title> <journal> Psychological Bulletin, </journal> <volume> 84, </volume> <pages> 963-976. </pages>
References-found: 58

