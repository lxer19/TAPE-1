URL: http://www.cs.washington.edu/homes/voelker/pubs/gms-sigmet98.ps
Refering-URL: http://www.cs.washington.edu/homes/voelker/vitae.html
Root-URL: http://www.cs.washington.edu
Title: Implementing Cooperative Prefetching and Caching in a Globally-Managed Memory System  
Author: Geoffrey M. Voelker, Eric J. Anderson, Tracy Kimbrel Michael J. Feeley Jeffrey S. Chase Anna R. Karlin, and Henry M. Levy 
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: This paper presents cooperative prefetching and caching the use of network-wide global resources (memories, CPUs, and disks) to support prefetching and caching in the presence of hints of future demands. Cooperative prefetching and caching effectively unites disk-latency reduction techniques from three lines of research: prefetching algorithms, cluster-wide memory management, and parallel I/O. When used together, these techniques greatly increase the power of prefetching relative to a conventional (non-global-memory) system. We have designed and implemented PGMS, a cooperative prefetching and caching system, under the Digital Unix operating system running on a 1.28 Gb/sec Myrinet-connected cluster of DEC Alpha workstations. Our measurements and analysis show that by using available global resources, cooperative prefetching can obtain significant speedups for I/O-bound programs. For example, for a graphics rendering application, our system achieves a speedup of 4.9 over a non-prefetching version of the same program, and a 3.1-fold improvement over that program using local-disk prefetching alone. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acharya, M. Uysal, R. Bennett, A. Mendelson, M. Beynon, J. Hollingsworth, J. Saltz, and A. Sussman. </author> <title> Tuning the performance of I/O-intensive parallel applications. </title> <booktitle> In Proc. of the Fourth Annual Workshop on I/O in Parallel and Distributed Systems, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Several studies of prefetching considered file system resource management for a single processor with a parallel disk array when application-disclosed access patterns (hints) are available. Researchers have shown that many I/O-intensive applications have predictable access patterns and can therefore provide such hints <ref> [27, 1, 23] </ref>, while Mowry et al. [25] showed that the compiler can automatically generate hints. Patterson et al. [27] manage allocation of cache space and I/O bandwidth between multiple processes, a subset of which are hinted; they apply cost-benefit analysis to estimate the impact of alternative buffer allocations.
Reference: [2] <author> Darrell Anderson, Jeffrey S. Chase, Syam Gadde, Andrew J. Gallatin, Kenneth G. Yocum, and Michael J. Feeley. </author> <title> Cheating the I/O bottleneck: Network storage with Trapeze/Myrin et. </title> <booktitle> In Proceedings of the 1998 USENIX Technical Conference, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: The disks from which all experimentes are performed are 7200 RPM ST32171W Seagate Barricuda drives. Pages and file blocks are 8KB, and reading a random 8KB page from disk takes an average of 13ms. For optimum network performance we used Trapeze <ref> [31, 2] </ref> firmware for the Myrinet adapters. Trapeze uses an adaptive message pipelining technique called cut-through delivery [31] to minimize transfer latencies on the network in a manner similar to GMS subpages [20]. <p> In our PGMS experiments, all putpage and getpage operations (including prefetch to local) copy the page once on the client; these copies can be eliminated with an optimization <ref> [2] </ref>. The prefetch to global operation has two components: the time to generate the prefetch request (Request), and the time to initiate the disk request into remote host memory and process it when it completes (Prefetch).
Reference: [3] <author> T. Anderson, M. Dahlin, J. Neefe, D. Patterson, D. Roselli, and R. Wang. </author> <title> Serverless network file systems. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 14(1), </volume> <month> February </month> <year> 1996. </year>
Reference-contexts: A third approach is to stripe files over multiple disks [26], using multiple nodes to access the disks in parallel <ref> [18, 9, 3] </ref>. This paper presents cooperative prefetching and caching the use of network-wide global memory to support prefetching and caching in the presence of optional program-provided hints of future demands. <p> Finally, efforts such as Zebra [18], TickerTAIP [9], and XFS <ref> [3] </ref> use multiple nodes to increase parallelism for remote file access. PGMS differs from these in its use of active prefetching and caching into remote memory. 6 Conclusions This paper presented PGMS, a system using cooperative prefetch-ing and caching in a network-wide global memory system.
Reference: [4] <author> L. Belady. </author> <title> A study of replacement algorithms for a virtual-storage computer. </title> <journal> IBM Systems Journal, </journal> <volume> 5(2), </volume> <year> 1966. </year>
Reference-contexts: It is well known that in a two-level memory hierarchy such as local memory and disk, the optimal replacement strategy is to replace the page whose next reference is furthest in the future <ref> [4] </ref>. The analogous replacement strategy for a three-level memory hierarchy (local memory, global memory, disk) such as PGMS is the Global Longest Forward Distance (GLF D) algorithm, defined formally as follows.
Reference: [5] <author> N. Boden, D. Cohen, R. Felderman, A. Kulawik, C. Seitz, J. Seizovic, and W-K Su. </author> <title> Myrinet a gigabit-per-second local area network. </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: We then examine the performance characteristics of various aspects of PGMS in detail, using a rendering application as an example. 4.1 Experimental testbed All measurements are from 266 MHz DEC AlphaStation 500 (Al-cor) systems running Digital Unix 4.0, connected by a 1.28 Gb/s Myrinet network <ref> [5] </ref>. All nodes in each experiment use M2F-PCI32 (LANai-4) Myrinet adapters attached to a full-crossbar SW8 Myrinet switch. The disks from which all experimentes are performed are 7200 RPM ST32171W Seagate Barricuda drives.
Reference: [6] <author> P. Cao, E. Felten, A. Karlin, and K. Li. </author> <title> A study of integrated prefetching and caching strategies. </title> <booktitle> In Proc. of the ACM SIG-METRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Whether the page is actually prefetched depends on whether a resident page can be found whose next reference is further in the future. For prefetching into global memory (disk-to-global) PGMS uses the Aggressive algorithm of Cao et al. <ref> [6] </ref>. If a page on disk will be referenced earlier than a page in cluster memory, then the disk page is prefetched. To make room, the global eviction policy chooses for replacement the page (in the cluster) whose next reference is furthest in the future. <p> Therefore, in PGMS we use GLF D as the cache replacement algorithm. 2.3.2 Prefetching strategy Effective prefetching into local memory eliminates stall time while minimizing computational overhead. Previous studies of prefetch-ing <ref> [6, 7] </ref> have shown that for a fully-hinted process with a single disk, the Aggressive prefetching algorithm achieves near-optimal reduction in stall time. Unfortunately, Aggressive's early prefetch-ing may result in suboptimal replacements, which can increase the total number of I/Os performed. <p> In contrast to local prefetching, disk stall time is much more important than computational overhead for disk-to-global prefetching, where the prefetching is performed by otherwise idle nodes. By analogy with the problem of prefetching from a single disk into a single memory <ref> [6] </ref>, the problem of prefetching from multiple disks into global memory, under the assumption that disk-resident data is available uniformly on all disks, can be shown to achieve near-optimal reduction in disk stall time. 1 Further, where the pages to be evicted will not be referenced until significantly later, if ever, <p> Cao et al. [7] considered the integration of prefetching, caching, and disk scheduling in the single-disk case; their Aggressive prefetch-ing strategy is provably near-optimal for a single disk <ref> [6] </ref>, but can be suboptimal for data striped across multiple disks [21]. Kim-brel et al. [22] studied combined prefetching and caching strategies for multiple-disk systems executing a single process; their Forestall algorithm adapts the aggressiveness of prefetching to the extent to which performance is limited by I/O stalls.
Reference: [7] <author> P. Cao, E. Felten, A. Karlin, and K. Li. </author> <title> Implementation and performance of integrated application-controlled file caching, prefetching, and disk scheduling. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 14(4), </volume> <month> November </month> <year> 1996. </year>
Reference-contexts: As a result, recent research has focused on reducing disk stall time through several approaches. One approach is the development of algorithms for prefetching data from disk into memory <ref> [7, 27, 22, 29] </ref>, using hints from either programmer fl Kimbrel is at the IBM T.J. Watson Research Center. y Feeley is at the Department of Computer Science, University of British Columbia. z Chase is at the Department of Computer Science, Duke University. <p> It effectively unites techniques from three previous lines of research: prefetching algorithms, including the work of Patterson et al. [27], Cao et al. <ref> [7] </ref>, Kimbrel et al. [22], and Tomkins et al. [29]; the global memory system (GMS) of Feeley et al. [14]; and the use of network nodes for parallel I/O, as in the Zebra system of Hartman and Ouster-hout [18]. <p> Therefore, in PGMS we use GLF D as the cache replacement algorithm. 2.3.2 Prefetching strategy Effective prefetching into local memory eliminates stall time while minimizing computational overhead. Previous studies of prefetch-ing <ref> [6, 7] </ref> have shown that for a fully-hinted process with a single disk, the Aggressive prefetching algorithm achieves near-optimal reduction in stall time. Unfortunately, Aggressive's early prefetch-ing may result in suboptimal replacements, which can increase the total number of I/Os performed. <p> This simplifies the algorithm and conceptual framework. In practice, any prefetching system must allocate buffers among multiple independent processes with differing hint capabilities. Policies for allocating buffers among competing processes on a single node have been extensively studied <ref> [27, 29, 8, 7] </ref>. <p> Patterson et al. [27] manage allocation of cache space and I/O bandwidth between multiple processes, a subset of which are hinted; they apply cost-benefit analysis to estimate the impact of alternative buffer allocations. Cao et al. <ref> [7] </ref> considered the integration of prefetching, caching, and disk scheduling in the single-disk case; their Aggressive prefetch-ing strategy is provably near-optimal for a single disk [6], but can be suboptimal for data striped across multiple disks [21]. <p> Kim-brel et al. [22] studied combined prefetching and caching strategies for multiple-disk systems executing a single process; their Forestall algorithm adapts the aggressiveness of prefetching to the extent to which performance is limited by I/O stalls. Tomkins et al. [29] compared the performance of the LRU-SP algorithm <ref> [8, 7] </ref> and cost-benefit algorithms for allocating I/O and cache resources among prefetching and non-prefetching processes. PGMS builds on several of these results, in particular, Cao et al.'s Aggressive algorithm and Kimbrel et al.'s Forestall algorithm.
Reference: [8] <author> P. Cao, E. Felten, and K. Li. </author> <title> Implementation and performance of application-controlled file caching. </title> <booktitle> In Proc. of the First Symp. on Operating Systems Design and Implementation. USENIX, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: This simplifies the algorithm and conceptual framework. In practice, any prefetching system must allocate buffers among multiple independent processes with differing hint capabilities. Policies for allocating buffers among competing processes on a single node have been extensively studied <ref> [27, 29, 8, 7] </ref>. <p> Kim-brel et al. [22] studied combined prefetching and caching strategies for multiple-disk systems executing a single process; their Forestall algorithm adapts the aggressiveness of prefetching to the extent to which performance is limited by I/O stalls. Tomkins et al. [29] compared the performance of the LRU-SP algorithm <ref> [8, 7] </ref> and cost-benefit algorithms for allocating I/O and cache resources among prefetching and non-prefetching processes. PGMS builds on several of these results, in particular, Cao et al.'s Aggressive algorithm and Kimbrel et al.'s Forestall algorithm.
Reference: [9] <author> P. Cao, S. Lim, S. Venkataraman, and J. Wilkes. </author> <title> The Ticker-TAIP parallel RAID architecture. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 12(3), </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: A third approach is to stripe files over multiple disks [26], using multiple nodes to access the disks in parallel <ref> [18, 9, 3] </ref>. This paper presents cooperative prefetching and caching the use of network-wide global memory to support prefetching and caching in the presence of optional program-provided hints of future demands. <p> Finally, efforts such as Zebra [18], TickerTAIP <ref> [9] </ref>, and XFS [3] use multiple nodes to increase parallelism for remote file access. PGMS differs from these in its use of active prefetching and caching into remote memory. 6 Conclusions This paper presented PGMS, a system using cooperative prefetch-ing and caching in a network-wide global memory system.
Reference: [10] <author> M. J. Carey, D. J. Dewitt, and J. F. Naughton. </author> <title> The oo7 benchmark. </title> <booktitle> In Proceedings of the ACM SIGMOD Internation Conference on Management of Data, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: We measured the following programs: OO7 is an object-oriented database benchmark that builds and traverses a parts-assembly database <ref> [10] </ref>. Our experiments traverse an existing 100MB database mapped into memory, ac cessing approximately 65MB of data. Render is a display engine that renders a computer-generated scene from a pre-computed 178MB database of tracing data [11].
Reference: [11] <author> B. Chamberlain, T. DeRose, D. Salesin, J. Snyder, and D. Lischinski. </author> <title> Fast rendering of complex environments using a spatial hierarchy. </title> <type> Technical Report 95-05-02, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Our experiments traverse an existing 100MB database mapped into memory, ac cessing approximately 65MB of data. Render is a display engine that renders a computer-generated scene from a pre-computed 178MB database of tracing data <ref> [11] </ref>. Our experiments perform a sequence of operations to move the viewpoint progressively closer to the scene without changing the viewpoint angle, accessing approximately 100MB of data.
Reference: [12] <author> D. Comer and J. Griffioen. </author> <title> A new design for distributed systems: The remote memory model. </title> <booktitle> In Proc. of the USENIX Summer Conf., </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: We describe below key research efforts in these areas and briefly contrast our efforts with those. Early studies showed the possibility of remote paging using dedicated or temporary paging servers <ref> [12, 15, 19] </ref>. More recent efforts have examined the use of network memory in more dynamic environments. Dahlin et al. [13] describe the use of remote memory in the XFS file system, which permits file system clients to benefit both from idle memory and shared file pages on other nodes.
Reference: [13] <author> M. Dahlin, R. Wang, T. Anderson, and D. Patterson. </author> <title> Cooperative caching: Using remote client memory to improve file system performance. </title> <booktitle> In Proc. of the Conf. on Operating Systems Design and Implementation. USENIX, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: A second approach is the use of memory on idle network nodes as an additional level of buffer cache <ref> [13, 14, 16] </ref>; this global memory can be accessed much faster than disk over high-speed switched networks. A third approach is to stripe files over multiple disks [26], using multiple nodes to access the disks in parallel [18, 9, 3]. <p> Early studies showed the possibility of remote paging using dedicated or temporary paging servers [12, 15, 19]. More recent efforts have examined the use of network memory in more dynamic environments. Dahlin et al. <ref> [13] </ref> describe the use of remote memory in the XFS file system, which permits file system clients to benefit both from idle memory and shared file pages on other nodes.
Reference: [14] <author> M. Feeley, W. Morgan, F. Pighin, A. Karlin, H. Levy, and C. Thekkath. </author> <title> Implementing global memory management in a workstation cluster. </title> <booktitle> In Proc. of the 15th ACM Sympoisum on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: A second approach is the use of memory on idle network nodes as an additional level of buffer cache <ref> [13, 14, 16] </ref>; this global memory can be accessed much faster than disk over high-speed switched networks. A third approach is to stripe files over multiple disks [26], using multiple nodes to access the disks in parallel [18, 9, 3]. <p> It effectively unites techniques from three previous lines of research: prefetching algorithms, including the work of Patterson et al. [27], Cao et al. [7], Kimbrel et al. [22], and Tomkins et al. [29]; the global memory system (GMS) of Feeley et al. <ref> [14] </ref>; and the use of network nodes for parallel I/O, as in the Zebra system of Hartman and Ouster-hout [18]. <p> For cache replacement, we modify the algorithms used by GMS <ref> [14] </ref> to incorporate prefetching. <p> We now describe the prototype PGMS implementation that approximates the ideal algorithm for cooperative prefetching and caching. In brief, we implemented PGMS by taking the Digital-UNIX-based GMS global memory system <ref> [14] </ref>, adding prefetching support, and then implementing an approximation to the prefetching algorithm presented above. We begin by giving an overview of GMS for background. 3.1 Overview of GMS GMS is a global memory system for a clustered network of workstations. <p> Dahlin et al. [13] describe the use of remote memory in the XFS file system, which permits file system clients to benefit both from idle memory and shared file pages on other nodes. The GMS system described by Feeley et al. <ref> [14] </ref> places a global memory layer underneath both the file and virtual memory systems, which allows both to benefit transparently from old pages in the network. Sarkar and Hartman [28] showed how hints could be used to approximate global information in remote memory systems such as GMS and XFS.
Reference: [15] <author> E. Felten and J. Zahorjan. </author> <title> Issues in the implementation of a remote memory paging system. </title> <type> Technical Report 91-03-09, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: We describe below key research efforts in these areas and briefly contrast our efforts with those. Early studies showed the possibility of remote paging using dedicated or temporary paging servers <ref> [12, 15, 19] </ref>. More recent efforts have examined the use of network memory in more dynamic environments. Dahlin et al. [13] describe the use of remote memory in the XFS file system, which permits file system clients to benefit both from idle memory and shared file pages on other nodes.
Reference: [16] <author> M. Franklin, M. Carey, and M. Livny. </author> <title> Global memory management in client-server DBMS architectures. </title> <booktitle> In Proc. of the 18th VLDB Conf., </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: A second approach is the use of memory on idle network nodes as an additional level of buffer cache <ref> [13, 14, 16] </ref>; this global memory can be accessed much faster than disk over high-speed switched networks. A third approach is to stripe files over multiple disks [26], using multiple nodes to access the disks in parallel [18, 9, 3]. <p> Sarkar and Hartman [28] showed how hints could be used to approximate global information in remote memory systems such as GMS and XFS. Franklin et al. <ref> [16] </ref> evaluate a client-server DBMS system in which the server can fetch requested data from clients' memories. In all of these systems, applications benefit from shared data that exists in global memory, or from evicting pages to global memory. Our work builds on systems like XFS and GMS.
Reference: [17] <author> G. Gibson, D. Nagle, K. Amiri, F. Chang, E. Feinberg, H. Go-bioff, C. Lee, B. Ozceri, E. Riedel, D. Rochberg, and J. Ze-lenka. </author> <title> File server scaling with network-attached secure disks. </title> <booktitle> In Proc. of the ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: For the purposes of defining the algorithm, we make several simplifying assumptions. First, we assume a uniform cluster topology with network page transfer cost (F G ) independent of location. Second, we assume uniform availability of disk-resident data to all nodes (e.g., through a network-attached disk <ref> [17] </ref> or replicated file system [24]) and uniform page transfer time from disk into a node's memory (F D ). For cluster systems using high-speed switched networks, F G will be significantly smaller than F D .
Reference: [18] <author> J. Hartman and J. Ousterhout. </author> <title> The Zebra striped network file system. </title> <booktitle> In Proc. of the 14th ACM Symp. on Operating Systems Principles, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: A third approach is to stripe files over multiple disks [26], using multiple nodes to access the disks in parallel <ref> [18, 9, 3] </ref>. This paper presents cooperative prefetching and caching the use of network-wide global memory to support prefetching and caching in the presence of optional program-provided hints of future demands. <p> algorithms, including the work of Patterson et al. [27], Cao et al. [7], Kimbrel et al. [22], and Tomkins et al. [29]; the global memory system (GMS) of Feeley et al. [14]; and the use of network nodes for parallel I/O, as in the Zebra system of Hartman and Ouster-hout <ref> [18] </ref>. <p> PGMS differs from all of this work, however, in that it performs prefetching in the context of a three-level storage hierarchy (local memory, global memory, and disk), and attempts to benefit from the parallelism available on multiple nodes and disks in the network. Finally, efforts such as Zebra <ref> [18] </ref>, TickerTAIP [9], and XFS [3] use multiple nodes to increase parallelism for remote file access. PGMS differs from these in its use of active prefetching and caching into remote memory. 6 Conclusions This paper presented PGMS, a system using cooperative prefetch-ing and caching in a network-wide global memory system.
Reference: [19] <author> L. Iftode, K. Petersen, and K. Li. </author> <title> Memory servers for mul-ticomputers. </title> <booktitle> In Proc. of the IEEE Spring COMPCON '93, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: We describe below key research efforts in these areas and briefly contrast our efforts with those. Early studies showed the possibility of remote paging using dedicated or temporary paging servers <ref> [12, 15, 19] </ref>. More recent efforts have examined the use of network memory in more dynamic environments. Dahlin et al. [13] describe the use of remote memory in the XFS file system, which permits file system clients to benefit both from idle memory and shared file pages on other nodes.
Reference: [20] <author> Herve A. Jamrozik, Michael J. Feeley, Geoffrey M. Voelker, James Evans II, Anna R. Karlin, Henry M. Levy, and Mary K. Vernon. </author> <title> Reducing network latency using subpages in a global memory environment. </title> <booktitle> In Proceedings of the Seventh ACM Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: For optimum network performance we used Trapeze [31, 2] firmware for the Myrinet adapters. Trapeze uses an adaptive message pipelining technique called cut-through delivery [31] to minimize transfer latencies on the network in a manner similar to GMS subpages <ref> [20] </ref>. Using Trapeze, GMS can perform an 8KB page fault from remote memory in 165s on platforms capable of delivering the full bandwidth of the 33 MHz 32-bit PCI bus. The Alcor is limited to 66 MB/s in the receiving direction, which increases raw page transfer times to 187s.
Reference: [21] <author> T. Kimbrel and A. Karlin. </author> <title> Near-optimal parallel prefetching and caching. </title> <booktitle> In Proc. of the 1996 IEEE Symp. on Foundations of Computer Science, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Cao et al. [7] considered the integration of prefetching, caching, and disk scheduling in the single-disk case; their Aggressive prefetch-ing strategy is provably near-optimal for a single disk [6], but can be suboptimal for data striped across multiple disks <ref> [21] </ref>. Kim-brel et al. [22] studied combined prefetching and caching strategies for multiple-disk systems executing a single process; their Forestall algorithm adapts the aggressiveness of prefetching to the extent to which performance is limited by I/O stalls.
Reference: [22] <author> T. Kimbrel, A. Tomkins, R. Patterson, B. Bershad, P. Cao, E. Felten, G. Gibson, A. Karlin, and K. Li. </author> <title> A trace-drive comparison of algorithms for parallel prefetching and caching. </title> <booktitle> In Proc. of the 2nd Symp. on Operating Systems Design and Implementation. USENIX, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: As a result, recent research has focused on reducing disk stall time through several approaches. One approach is the development of algorithms for prefetching data from disk into memory <ref> [7, 27, 22, 29] </ref>, using hints from either programmer fl Kimbrel is at the IBM T.J. Watson Research Center. y Feeley is at the Department of Computer Science, University of British Columbia. z Chase is at the Department of Computer Science, Duke University. <p> It effectively unites techniques from three previous lines of research: prefetching algorithms, including the work of Patterson et al. [27], Cao et al. [7], Kimbrel et al. <ref> [22] </ref>, and Tomkins et al. [29]; the global memory system (GMS) of Feeley et al. [14]; and the use of network nodes for parallel I/O, as in the Zebra system of Hartman and Ouster-hout [18]. <p> For prefetching decisions we apply a hybrid algorithm, whose goal is to be conservative locally but aggressive with resources on idle nodes. For local prefetching, we adapt the Forestall algorithm of Kimbrel, et al. <ref> [22, 29] </ref>. Forestall analyzes the future reference stream to determine whether the process is I/O constrained; if so, Forestall attempts to prefetch just early enough to avoid stalling. <p> Although these I/Os are overlapped with computation, a significant overhead (the computational overhead of issuing fetches) can result. The Forestall algorithm has been shown in practice to match the reduction in I/O stall achieved by the Aggressive algorithm, while avoiding the computational overhead of performing unnecessary fetches <ref> [22, 29] </ref>. Forestall is therefore the method of choice for local prefetching. In contrast to local prefetching, disk stall time is much more important than computational overhead for disk-to-global prefetching, where the prefetching is performed by otherwise idle nodes. <p> Cao et al. [7] considered the integration of prefetching, caching, and disk scheduling in the single-disk case; their Aggressive prefetch-ing strategy is provably near-optimal for a single disk [6], but can be suboptimal for data striped across multiple disks [21]. Kim-brel et al. <ref> [22] </ref> studied combined prefetching and caching strategies for multiple-disk systems executing a single process; their Forestall algorithm adapts the aggressiveness of prefetching to the extent to which performance is limited by I/O stalls.
Reference: [23] <author> D. Kotz and C. Ellis. </author> <title> Practical prefetching techniques for multiprocessor file systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1), </volume> <month> January </month> <year> 1993. </year>
Reference-contexts: Several studies of prefetching considered file system resource management for a single processor with a parallel disk array when application-disclosed access patterns (hints) are available. Researchers have shown that many I/O-intensive applications have predictable access patterns and can therefore provide such hints <ref> [27, 1, 23] </ref>, while Mowry et al. [25] showed that the compiler can automatically generate hints. Patterson et al. [27] manage allocation of cache space and I/O bandwidth between multiple processes, a subset of which are hinted; they apply cost-benefit analysis to estimate the impact of alternative buffer allocations.
Reference: [24] <author> E. Lee and C. Thekkath. </author> <title> Petal: Distributed virtual disks. </title> <booktitle> In Proc. of the 7th Conf. on Arhcitectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: First, we assume a uniform cluster topology with network page transfer cost (F G ) independent of location. Second, we assume uniform availability of disk-resident data to all nodes (e.g., through a network-attached disk [17] or replicated file system <ref> [24] </ref>) and uniform page transfer time from disk into a node's memory (F D ). For cluster systems using high-speed switched networks, F G will be significantly smaller than F D .
Reference: [25] <author> T. Mowry, A. Demke, and O. Krieger. </author> <title> Automatic compiler-inserted I/O prefetching for out-of-core applications. </title> <booktitle> In Proc. of the 2nd Symp. on Operating Systems Design and Implementation. USENIX, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Voelker was supported in part by a fellowship from Intel Corporation, Ander-son was supported in part by a fellowship from Microsoft Corporation, and Chase was supported in part by NSF CAREER CCR-96-24857. annotated [27] or compiler-annotated <ref> [25] </ref> programs. A second approach is the use of memory on idle network nodes as an additional level of buffer cache [13, 14, 16]; this global memory can be accessed much faster than disk over high-speed switched networks. <p> Several studies of prefetching considered file system resource management for a single processor with a parallel disk array when application-disclosed access patterns (hints) are available. Researchers have shown that many I/O-intensive applications have predictable access patterns and can therefore provide such hints [27, 1, 23], while Mowry et al. <ref> [25] </ref> showed that the compiler can automatically generate hints. Patterson et al. [27] manage allocation of cache space and I/O bandwidth between multiple processes, a subset of which are hinted; they apply cost-benefit analysis to estimate the impact of alternative buffer allocations.
Reference: [26] <author> D. Patterson, G. Gibson, and R. Katz. </author> <title> A case for redundant arrays of inexpensive disks. </title> <booktitle> In Proc. of the 1988 ACM SIG-MOD Conf. on Management of Data, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: A second approach is the use of memory on idle network nodes as an additional level of buffer cache [13, 14, 16]; this global memory can be accessed much faster than disk over high-speed switched networks. A third approach is to stripe files over multiple disks <ref> [26] </ref>, using multiple nodes to access the disks in parallel [18, 9, 3]. This paper presents cooperative prefetching and caching the use of network-wide global memory to support prefetching and caching in the presence of optional program-provided hints of future demands.
Reference: [27] <author> R.H. Patterson, G.A. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proc. of the 15th Symp. on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: As a result, recent research has focused on reducing disk stall time through several approaches. One approach is the development of algorithms for prefetching data from disk into memory <ref> [7, 27, 22, 29] </ref>, using hints from either programmer fl Kimbrel is at the IBM T.J. Watson Research Center. y Feeley is at the Department of Computer Science, University of British Columbia. z Chase is at the Department of Computer Science, Duke University. <p> Voelker was supported in part by a fellowship from Intel Corporation, Ander-son was supported in part by a fellowship from Microsoft Corporation, and Chase was supported in part by NSF CAREER CCR-96-24857. annotated <ref> [27] </ref> or compiler-annotated [25] programs. A second approach is the use of memory on idle network nodes as an additional level of buffer cache [13, 14, 16]; this global memory can be accessed much faster than disk over high-speed switched networks. <p> Our system, called PGMS (Prefetch-ing Global Memory System), integrates all cluster memory use, including VM pages, mapped files, and file system buffers, for both prefetching and non-prefetching applications. It effectively unites techniques from three previous lines of research: prefetching algorithms, including the work of Patterson et al. <ref> [27] </ref>, Cao et al. [7], Kimbrel et al. [22], and Tomkins et al. [29]; the global memory system (GMS) of Feeley et al. [14]; and the use of network nodes for parallel I/O, as in the Zebra system of Hartman and Ouster-hout [18]. <p> This simplifies the algorithm and conceptual framework. In practice, any prefetching system must allocate buffers among multiple independent processes with differing hint capabilities. Policies for allocating buffers among competing processes on a single node have been extensively studied <ref> [27, 29, 8, 7] </ref>. <p> For example, the benefit of the prefetch recommendations made by hinted processes can be compared to the cost of LRU cache replacement decisions for unhinted processes <ref> [27, 29] </ref>. An interesting direction for future research is to analyze these algorithms in the context of a prefetching global memory system. Processes on different nodes will also compete for global memory and prefetching resources. The prefetching system must similarly allocate resources among competing nodes. <p> Several studies of prefetching considered file system resource management for a single processor with a parallel disk array when application-disclosed access patterns (hints) are available. Researchers have shown that many I/O-intensive applications have predictable access patterns and can therefore provide such hints <ref> [27, 1, 23] </ref>, while Mowry et al. [25] showed that the compiler can automatically generate hints. Patterson et al. [27] manage allocation of cache space and I/O bandwidth between multiple processes, a subset of which are hinted; they apply cost-benefit analysis to estimate the impact of alternative buffer allocations. <p> Researchers have shown that many I/O-intensive applications have predictable access patterns and can therefore provide such hints [27, 1, 23], while Mowry et al. [25] showed that the compiler can automatically generate hints. Patterson et al. <ref> [27] </ref> manage allocation of cache space and I/O bandwidth between multiple processes, a subset of which are hinted; they apply cost-benefit analysis to estimate the impact of alternative buffer allocations.
Reference: [28] <author> P. Sarkar and J. Hartman. </author> <title> Efficient cooperative caching using hints. </title> <booktitle> In Proc. of the 2nd Symp. on Operating Systems Design and Implementation. USENIX, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: The GMS system described by Feeley et al. [14] places a global memory layer underneath both the file and virtual memory systems, which allows both to benefit transparently from old pages in the network. Sarkar and Hartman <ref> [28] </ref> showed how hints could be used to approximate global information in remote memory systems such as GMS and XFS. Franklin et al. [16] evaluate a client-server DBMS system in which the server can fetch requested data from clients' memories.
Reference: [29] <author> A. Tomkins, R.H. Patterson, and G. Gibson. </author> <title> Informed multi-process prefetching and caching. </title> <booktitle> In Proc. of the ACM International Conf. on Measurement and Modeling of Computer Systems, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: As a result, recent research has focused on reducing disk stall time through several approaches. One approach is the development of algorithms for prefetching data from disk into memory <ref> [7, 27, 22, 29] </ref>, using hints from either programmer fl Kimbrel is at the IBM T.J. Watson Research Center. y Feeley is at the Department of Computer Science, University of British Columbia. z Chase is at the Department of Computer Science, Duke University. <p> It effectively unites techniques from three previous lines of research: prefetching algorithms, including the work of Patterson et al. [27], Cao et al. [7], Kimbrel et al. [22], and Tomkins et al. <ref> [29] </ref>; the global memory system (GMS) of Feeley et al. [14]; and the use of network nodes for parallel I/O, as in the Zebra system of Hartman and Ouster-hout [18]. <p> For prefetching decisions we apply a hybrid algorithm, whose goal is to be conservative locally but aggressive with resources on idle nodes. For local prefetching, we adapt the Forestall algorithm of Kimbrel, et al. <ref> [22, 29] </ref>. Forestall analyzes the future reference stream to determine whether the process is I/O constrained; if so, Forestall attempts to prefetch just early enough to avoid stalling. <p> Although these I/Os are overlapped with computation, a significant overhead (the computational overhead of issuing fetches) can result. The Forestall algorithm has been shown in practice to match the reduction in I/O stall achieved by the Aggressive algorithm, while avoiding the computational overhead of performing unnecessary fetches <ref> [22, 29] </ref>. Forestall is therefore the method of choice for local prefetching. In contrast to local prefetching, disk stall time is much more important than computational overhead for disk-to-global prefetching, where the prefetching is performed by otherwise idle nodes. <p> This simplifies the algorithm and conceptual framework. In practice, any prefetching system must allocate buffers among multiple independent processes with differing hint capabilities. Policies for allocating buffers among competing processes on a single node have been extensively studied <ref> [27, 29, 8, 7] </ref>. <p> unhinted processes, variability of inter-reference CPU times between different processes, the 1 This is in sharp contrast to the case where different pages reside on different disks, in which case aggressive prefetching can be far from optimal. 2 It should also be noted that in contrast to the results of <ref> [29] </ref>, a page cannot be prefetched into global memory and then evicted before it is referenced: a page chosen for prefetch into global memory is always the then soonest non-resident page to be referenced by any process in the cluster, so the next global fault will not occur until after that <p> For example, the benefit of the prefetch recommendations made by hinted processes can be compared to the cost of LRU cache replacement decisions for unhinted processes <ref> [27, 29] </ref>. An interesting direction for future research is to analyze these algorithms in the context of a prefetching global memory system. Processes on different nodes will also compete for global memory and prefetching resources. The prefetching system must similarly allocate resources among competing nodes. <p> Kim-brel et al. [22] studied combined prefetching and caching strategies for multiple-disk systems executing a single process; their Forestall algorithm adapts the aggressiveness of prefetching to the extent to which performance is limited by I/O stalls. Tomkins et al. <ref> [29] </ref> compared the performance of the LRU-SP algorithm [8, 7] and cost-benefit algorithms for allocating I/O and cache resources among prefetching and non-prefetching processes. PGMS builds on several of these results, in particular, Cao et al.'s Aggressive algorithm and Kimbrel et al.'s Forestall algorithm.
Reference: [30] <author> G. Voelker, H. Jamrozik, M. Vernon, H. Levy, and E. La-zowska. </author> <title> Managing server load in global memory systems. </title> <booktitle> In Proc. of the ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: The objective of this two-pronged scheme is to maintain valuable blocks in local memory, while sacrificing global blocks to speedup prefetching. We make this tradeoff because it has been shown that in a global memory system performance is relatively insensitive to which of the oldest global pages are replaced <ref> [30] </ref>. Therefore, we replace the least valuable global pages in order to reduce stall time through prefetching, without risking local performance.
Reference: [31] <author> Kenneth G. Yocum, Jeffrey S. Chase, Andrew J. Gallatin, and Alvin R. Lebeck. </author> <title> Cut-through delivery in Trapeze: An exercise in low-latency messaging. </title> <booktitle> In Proceedings of the Sixth IEEE International Symposium on High Performance Distributed Computing (HPDC-6), </booktitle> <pages> pages 243252, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: The disks from which all experimentes are performed are 7200 RPM ST32171W Seagate Barricuda drives. Pages and file blocks are 8KB, and reading a random 8KB page from disk takes an average of 13ms. For optimum network performance we used Trapeze <ref> [31, 2] </ref> firmware for the Myrinet adapters. Trapeze uses an adaptive message pipelining technique called cut-through delivery [31] to minimize transfer latencies on the network in a manner similar to GMS subpages [20]. <p> Pages and file blocks are 8KB, and reading a random 8KB page from disk takes an average of 13ms. For optimum network performance we used Trapeze [31, 2] firmware for the Myrinet adapters. Trapeze uses an adaptive message pipelining technique called cut-through delivery <ref> [31] </ref> to minimize transfer latencies on the network in a manner similar to GMS subpages [20]. Using Trapeze, GMS can perform an 8KB page fault from remote memory in 165s on platforms capable of delivering the full bandwidth of the 33 MHz 32-bit PCI bus.
References-found: 31

