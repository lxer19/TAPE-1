URL: http://www.cs.helsinki.fi/~jkivinen/abstracts/kmu-lhrs-92.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~jkivinen/abstracts/kmu-lhrs-92.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: jkivinen@cs.Helsinki.FI mannila@cs.Helsinki.FI ukkonen@cs.Helsinki.FI  
Title: Learning Hierarchical Rule Sets  
Author: Jyrki Kivinen Heikki Mannila Esko Ukkonen 
Keyword: Key words: computational learning theory, decision lists, rules  
Note: AMS (MOS) subject classifications: 68T05, 68Q20, 62G99  
Address: P.O. Box 26 (Teollisuuskatu 23) SF-00014 University of Helsinki, Finland  
Affiliation: Department of Computer Science  
Abstract: We present an algorithm for learning sets of rules that are organized into up to k levels. Each level can contain an arbitrary number of rules "if c then l" where l is the class associated to the level and c is a concept from a given class of basic concepts. The rules of higher levels have precedence over the rules of lower levels and can be used to represent exceptions. As basic concepts we can use Boolean attributes in the infinite attribute space model, or certain concepts defined in terms of substrings. Given a sample of m examples, the algorithm runs in polynomial time and produces a consistent concept representation of size O((log m) k n k ), where n is the size of the smallest consistent representation with k levels of rules. This implies that the algorithm learns in the PAC model. The algorithm repeatedly applies the greedy heuristics for weighted set cover. The weights are obtained from approximate solutions to previous set cover problems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Blum, </author> <title> Learning Boolean functions in an infinite attribute space, </title> <journal> Mach. Learning, </journal> <volume> 9 (1992), </volume> <pages> pp. 373-386. </pages>
Reference-contexts: These representations allow subsequences that consist of a constant number of pieces. They might be useful in biological applications, where different parts of the string may interact with each other. 2 Example 3 In the infinite attribute space approach to Boolean concept learning <ref> [1] </ref>, the instances are value assignments to an infinite number of Boolean variables. In each instance only a finite number of variables can have the value 1. An instance can then be encoded by a string that lists the names of the variables with value 1. <p> -adequate for S + [ S ; for t 2 f +; g do begin Let t i := t for odd and t i := t for even values of i; for r 2 R 0 do if D (r; t 1 ; S) = ; then begin C <ref> [1; r] </ref> := 0; I [1; r] := end else C [1; r] := 1; for i := 2 to k + 1 do begin if i = k + 1 then H := f true g else H := R 0 ; for r 2 H do begin Create a <p> S ; for t 2 f +; g do begin Let t i := t for odd and t i := t for even values of i; for r 2 R 0 do if D (r; t 1 ; S) = ; then begin C <ref> [1; r] </ref> := 0; I [1; r] := end else C [1; r] := 1; for i := 2 to k + 1 do begin if i = k + 1 then H := f true g else H := R 0 ; for r 2 H do begin Create a weighted set cover problem with <p> +; g do begin Let t i := t for odd and t i := t for even values of i; for r 2 R 0 do if D (r; t 1 ; S) = ; then begin C <ref> [1; r] </ref> := 0; I [1; r] := end else C [1; r] := 1; for i := 2 to k + 1 do begin if i = k + 1 then H := f true g else H := R 0 ; for r 2 H do begin Create a weighted set cover problem with the domain D = D (r; <p> max 1; #(L 0 o A j=1 j j n j + #(L 0 0 n1 Y max 1; #(L 0 o A j=1 j j 0 n Y max 1; #(L 0 o A j=1 j j: This result also holds in the case n = 1, because C <ref> [1; r 0 ] </ref> = 0 for all r 0 2 L 0 1 . The greedy algorithm for weighted set cover problem always produces a solution with cost within a factor H (jDj) of the optimal.
Reference: [2] <author> A. Blum, L. Hellerstein, and N. Littlestone, </author> <title> Learning in the presence of finitely or infinitely many irrelevant attributes, </title> <booktitle> Proc. 4th Annual Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1991, </year> <pages> pp. 157-166. </pages>
Reference-contexts: The coefficients defining the linearly separable concept, as well as the number of nonzero coefficients, can be bounded by a polynomial of the number of rules. Hence, Littlestone's Winnow2 algorithm [8] can be used to learn k-level rule sets. By applying the transformation given by Blum et al. <ref> [2] </ref>, this approach can also be made to work with infinite basic concept classes.
Reference: [3] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth, </author> <title> Occam's razor, </title> <journal> Inform. Process. Lett., </journal> <volume> 24 (1987), </volume> <pages> pp. 377-380. </pages>
Reference-contexts: If the basic concept class satisfies some mild assumptions, the algorithm runs in time that is polynomial in the input size, and produces a rule set of size O ((log m) k n k ). Hence, the algorithm is an Occam algorithm <ref> [3] </ref> and therefore also a learning algorithm in the PAC model. The algorithm is based on the greedy approximation technique for weighted set cover problems. <p> Therefore, the size of the representation output by RS k is O (m ff l fi jL c j k ), and RS k is an Occam algorithm. The results of Blumer et al. <ref> [3] </ref> give the following corollary. Corollary 6 Let ff &gt; 0 be an arbitrary constant. Let 0 &lt; " &lt; 1, 0 &lt; ffi &lt; 1, n 2 IN and l 2 IN be arbitrary parameters.
Reference: [4] <author> V. Chvatal, </author> <title> A greedy heuristic for the set-covering problem, </title> <journal> Math. Oper. Res., </journal> <volume> 4 (1979), </volume> <pages> pp. 233-235. </pages>
Reference-contexts: A solution to the problem is a set J I such that [ i2J D i = D. The cost of a solution J is cost (J ) = P i2J cost (i). Chvatal <ref> [4] </ref> has shown that if a solution exists, a greedy algorithm obtains in polynomial time a solution with a cost that exceeds the minimum cost at most by a factor H (jDj) where H (m) = P m i=1 1=i.
Reference: [5] <author> P. G. Kolaitis and M. N. Thakur, </author> <title> Approximation properties of NP minimization classes, </title> <booktitle> Proc. 6th Annual Structure in Complexity Theory Conference, </booktitle> <publisher> IEEE Computer Society, Los Alamitos, </publisher> <year> 1991, </year> <pages> pp. 353-366. </pages>
Reference-contexts: Our algorithm is simple and fairly intuitive. The analysis of the iterative use of the greedy approximation algorithm has also some interest in itself, as it produces a small extension of the approximation results of Kolaitis and Thakur <ref> [5] </ref>. Notation We use jwj to denote the length of a string w and jjW jj = P w2W jwj to denote the total length of strings in a set W .
Reference: [6] <author> H. J. Levesque, </author> <title> Knowledge representation and reasoning, </title> <booktitle> Readings in Artificial Intelligence & Databases, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1989, </year> <pages> pp. 35-51. </pages>
Reference-contexts: Proceedings of the Fourth Annual ACM Workshop on Computational Learning Theory 1 if x 2 c 1 then 1 else if x 2 c 2 then 2 : : : else if x 2 c k then k else . and use knowledge of this form has been widely recognized <ref> [6, 12] </ref>. The hyphenation algorithm of T E X is a commonly used example of this type of representation. It uses five levels of rules (called patterns) [7]. In this paper we give a general algorithm for learning hierarchical sets of rules.
Reference: [7] <author> F. M. Liang, </author> <title> Word Hy-phen-a-tion by Com-put-er, </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1983. </year>
Reference-contexts: The hyphenation algorithm of T E X is a commonly used example of this type of representation. It uses five levels of rules (called patterns) <ref> [7] </ref>. In this paper we give a general algorithm for learning hierarchical sets of rules. The algorithm is applicable to any set of basic rules satisfying certain mild assumptions. For the T E X application, the basic rules are defined in terms of sets of superstrings of given strings. <p> Define (r) to be the set of superstrings of r. That is, w 2 (r) if w = xry for some strings x; y 2 fl . This substring representation is similar to patterns used in T E X to represent allowed and disallowed hyphenations <ref> [7] </ref>. These hyphenation rules constitute a hierarchy with exceptions, similar to the hierarchy we shall soon describe for our setting. 2 A sample is a pair S = (S + ; S ) where S + and S are finite subsets of fl .
Reference: [8] <author> N. Littlestone, </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm, </title> <journal> Mach. Learning, </journal> <volume> 2 (1988), </volume> <pages> pp. 285-318. </pages>
Reference-contexts: The coefficients defining the linearly separable concept, as well as the number of nonzero coefficients, can be bounded by a polynomial of the number of rules. Hence, Littlestone's Winnow2 algorithm <ref> [8] </ref> can be used to learn k-level rule sets. By applying the transformation given by Blum et al. [2], this approach can also be made to work with infinite basic concept classes.
Reference: [9] <author> P. Orponen and R. Greiner, </author> <title> On the sample complexity of finding good search strategies, </title> <booktitle> Proc. 3rd Annual Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1990, </year> <pages> pp. 352-358. </pages>
Reference-contexts: However, in Section 4 we prove a bound for the quality of the approximation produced by our algorithm. For another example of an analysis of this type, see Orponen and Greiner <ref> [9] </ref>. For the rest of this section, we consider an (f; g)-reasonable basic representation class (R; ; ; ). We assume that given strings w 2 fl and r 2 R it can be determined in time h (jwj; jrj) whether w 2 (r) holds.
Reference: [10] <author> L. Pitt and M. K. Warmuth, </author> <title> Prediction-preserving reducibility, </title> <journal> J. Comput. System Sci., </journal> <volume> 41 (1990), </volume> <pages> pp. 430-467. </pages>
Reference-contexts: We assume that the cardinalities of the alphabets and are finite and at least 2. We fix a set R fl , and define a mapping that associates to each string r 2 R a concept (r) fl . The quadruple (R; ; ; ) is a representation class <ref> [10] </ref>. The string r is a representation for the concept (r). We introduce a new symbol true 62 fl and extend by defining (true) = fl . Example 1 Let be a finite alphabet, = , and R = fl .
Reference: [11] <author> R. L. Rivest, </author> <title> Learning decision lists, </title> <journal> Mach. Learning, </journal> <volume> 2 (1987), </volume> <pages> pp. 229-246. </pages>
Reference-contexts: We now consider using rules to partition a set X into the classes + and , or to positive and negative instances. In concept learning, hierarchies of rules and exceptions are often represented as decision lists <ref> [11] </ref>. Given a set c i X and a class i 2 f +; g for i = 1; : : : ; k and a default class , we classify a string x 2 X as in representation for this classifier. <p> We then write the k-level rule set in a list form as (L 1 ; : : : ; L k ; ). A hierarchical rule set can also be written as a decision list, although this can increase the number of levels. Rivest <ref> [11] </ref> has given a learning algorithm for decision lists with an unbounded number of levels. However, the algorithm requires the class of basic concepts to be finite. We give a learning algorithm that also allows certain 2 infinite classes of basic concepts. <p> We now let x 2 k (L) if x is associated in L to level i with i = +. A k-level rule set can be transformed into an ordinary decision list <ref> [11] </ref>, with only one rule per level allowed, simply by putting the rules of level i into consecutive levels of the new list.
Reference: [12] <author> D. S. Touretzky, </author> <title> The Mathematics of Inheritance Systems, </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1986. </year>
Reference-contexts: Proceedings of the Fourth Annual ACM Workshop on Computational Learning Theory 1 if x 2 c 1 then 1 else if x 2 c 2 then 2 : : : else if x 2 c k then k else . and use knowledge of this form has been widely recognized <ref> [6, 12] </ref>. The hyphenation algorithm of T E X is a commonly used example of this type of representation. It uses five levels of rules (called patterns) [7]. In this paper we give a general algorithm for learning hierarchical sets of rules.
References-found: 12

