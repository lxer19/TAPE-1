URL: http://www.cs.utexas.edu/users/plaxton/ps/1998/sicomp.ps
Refering-URL: http://www.cs.utexas.edu/users/plaxton/html/abc.html
Root-URL: 
Title: HYPERCUBIC SORTING NETWORKS  
Author: TOM LEIGHTON AND C. GREG PLAXTON 
Keyword: Key words. parallel sorting, sorting networks, hypercubic machines  
Note: AMS subject classifications. 68P10, 68Q22, 68Q25, 68R05  
Abstract: This paper provides an analysis of a natural d-round tournament over n = 2 d players, and demonstrates that the tournament possesses a surprisingly strong ranking property. The ranking property of this tournament is used to design efficient sorting algorithms for a variety of different models of parallel computation: (i) a comparator network of depth c lg n, c 7:44, that sorts the vast majority of the n! possible input permutations, (ii) an O(lg n)-depth hypercubic comparator network that sorts the vast majority of permuta tions, (iii) a hypercubic sorting network with nearly logarithmic depth, (iv) an O(lg n)-time randomized sorting algorithm for any hypercubic machine (other such algorithms have been previously discovered, but this algorithm has a significantly smaller failure probability than any previously known algorithm), and (v) a randomized algorithm for sorting n O(m)-bit records on an (n lg n)-node omega machine in O(m + lg n) bit steps. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Aiello, F. T. Leighton, B. Maggs, and M. Newman. </author> <title> Fast algorithms for bit-serial routing on a hypercube. </title> <journal> Mathematical Systems Theory, </journal> <volume> 24 </volume> <pages> 253-271, </pages> <year> 1991. </year>
Reference-contexts: T. LEIGHTON AND C. G. PLAXTON Table 1 Type conventions. Symbol Type Symbol Type a; b; d; i; j; k; m; n integer N comparator network c real constant ff; fi binary string f; g; h function * empty string p; q real number in <ref> [0; 1] </ref> permutation u; v; w; z real number set of permutations x; y various 0-1 vector A; B; C set set of 0-1 vectors E probabilistic event o; !; fi; asymptotic symbol X; Y random variable summation symbol D probability distribution fl c ; c ; c defined constant M <p> The only previous result of this kind that does not rely on the AKS sorting network is the recent work of Aiello, Leighton, Maggs, and Newman <ref> [1] </ref>, which provides a randomized bit-serial routing algorithm that runs in optimal time with very high probability on the hypercube. <p> Let R (d) and R (d; a) denote the uniform distributions over (d) and (d; a), respectively. Let R (d; k) denote the uniform distribution over (d; k). For all p in <ref> [0; 1] </ref>, let B (d; p) denote the distribution that assigns probability p k (1 p) pow (d)k to each 0-1 d-vector in (d; k). Note that a random 0-1 d-vector drawn from this distribution corresponds to the sequence of outcomes of d independent, p-biased Bernoulli trials. <p> ) is the probability distribution over (d) that assigns probability p i (resp., p 0 i ) to the d-bit binary string i d1 i 0 , 0 i &lt; pow (d) = n, then define D D 0 if and only if there exist real numbers x ij in <ref> [0; 1] </ref>, 0 i &lt; n, 0 j &lt; n, such that: (i) 0i&lt;n x ij = 1, 0 j &lt; n, i = 0j&lt;n x ij p j , 0 i &lt; n, and (iii) x ij &gt; 0 only if i k j k , 0 k &lt; d. <p> One may easily verify that for each binary string ff, the following conditions hold: (i) ff (0) = 0, (ii) ff (1) = 1, (iii) ff (p) is monotonically increasing for p in <ref> [0; 1] </ref>, (iv) ff (p) is a degree-pow (jffj) polynomial in p. Conditions (i), (ii), and (iii) imply that ff (p) is in [0; 1] for all p in [0; 1]. lemma 5.2. <p> each binary string ff, the following conditions hold: (i) ff (0) = 0, (ii) ff (1) = 1, (iii) ff (p) is monotonically increasing for p in <ref> [0; 1] </ref>, (iv) ff (p) is a degree-pow (jffj) polynomial in p. Conditions (i), (ii), and (iii) imply that ff (p) is in [0; 1] for all p in [0; 1]. lemma 5.2. Output wire j of a 0-1 no-elimination (d; p)-tournament receives a 0 with probability ff (p), where ff = j d1 j 0 . Proof. <p> conditions hold: (i) ff (0) = 0, (ii) ff (1) = 1, (iii) ff (p) is monotonically increasing for p in <ref> [0; 1] </ref>, (iv) ff (p) is a degree-pow (jffj) polynomial in p. Conditions (i), (ii), and (iii) imply that ff (p) is in [0; 1] for all p in [0; 1]. lemma 5.2. Output wire j of a 0-1 no-elimination (d; p)-tournament receives a 0 with probability ff (p), where ff = j d1 j 0 . Proof. <p> We can easily calculate that ff (1=2) 0:796 and fi (1=2) 0:882, suggesting that the player with record ff should be rated above the player with record fi. lemma 5.3. For all binary strings ff and fi, and all p in <ref> [0; 1] </ref>, fiff (p) = ff ( fi (p)): Proof. For ff = *, the result is immediate since * (p) = p. For jffj &gt; 0, we prove the result by induction on jffj. <p> In order to better understand the behavior of the output polynomial ff , it will be useful to study its inverse function. In particular, for any binary string ff, we define ff (z) to be the function such that ff ( ff (p)) = p for all p in <ref> [0; 1] </ref>. Unlike ff , ff is not a polynomial for jffj 1. However, like ff , there is a simple inductive scheme for computing ff . This is demonstrated by the following lemma. lemma 5.4. For all binary strings ff, and all z in [0; 1], * (z) = z; <p> p for all p in <ref> [0; 1] </ref>. Unlike ff , ff is not a polynomial for jffj 1. However, like ff , there is a simple inductive scheme for computing ff . This is demonstrated by the following lemma. lemma 5.4. For all binary strings ff, and all z in [0; 1], * (z) = z; p 1ff (z) = ff (z): Proof. Since * (p) = p for all p in [0; 1], * is the identity function, and thus * is also the identity function. Hence * (z) = z for all z in [0; 1]. <p> This is demonstrated by the following lemma. lemma 5.4. For all binary strings ff, and all z in <ref> [0; 1] </ref>, * (z) = z; p 1ff (z) = ff (z): Proof. Since * (p) = p for all p in [0; 1], * is the identity function, and thus * is also the identity function. Hence * (z) = z for all z in [0; 1]. By Lemma 5.3, we have 0ff (p) = ff ( 0 (p)) HYPERCUBIC SORTING NETWORKS 17 for all p in [0; 1]. <p> and all z in <ref> [0; 1] </ref>, * (z) = z; p 1ff (z) = ff (z): Proof. Since * (p) = p for all p in [0; 1], * is the identity function, and thus * is also the identity function. Hence * (z) = z for all z in [0; 1]. By Lemma 5.3, we have 0ff (p) = ff ( 0 (p)) HYPERCUBIC SORTING NETWORKS 17 for all p in [0; 1]. <p> for all p in <ref> [0; 1] </ref>, * is the identity function, and thus * is also the identity function. Hence * (z) = z for all z in [0; 1]. By Lemma 5.3, we have 0ff (p) = ff ( 0 (p)) HYPERCUBIC SORTING NETWORKS 17 for all p in [0; 1]. <p> The proof that 1ff (z) = p ff (z) proceeds in a similar fashion. By Lemma 5.3, we have 1ff (p) = ff ( 1 (p)) for all p in <ref> [0; 1] </ref>. Setting p = 1ff (z), we find that ff ( 1ff (z) 2 ) = 1ff ( 1ff (z)) = ff ( ff (z)): Since ff is a monotonically increasing function, we have 1ff (z) 2 = ff (z) and thus 1ff (z) = ff (z); as desired. <p> Note that ff (0) = 0 and ff (1) = 1 for all binary strings ff. The following lemma is analogous to Lemma 5.3. lemma 5.5. For all binary strings ff and fi, and all z in <ref> [0; 1] </ref>, fiff (z) = fi ( ff (z)): Proof. Similar to the proof of Lemma 5.3. 18 F. T. LEIGHTON AND C. G. PLAXTON 5.3. Auxiliary Definitions. In this section, we state a number of definitions related to the analysis of the no-elimination tournament. <p> T. LEIGHTON AND C. G. PLAXTON 5.3. Auxiliary Definitions. In this section, we state a number of definitions related to the analysis of the no-elimination tournament. These definitions are used primarily in x5.4 and x5.5, but also appear in subsequent sections. For all x &lt; y in <ref> [0; 1] </ref>, 1, and d 0, let y (1 x) ;(3) ( ff (x); ff (y)) ;(4) X ff:jffj=d h ff (x; y) ;(5) 0&lt;x<y&lt;1 h 0 (x; y) + h 1 (x; y) fl c = inf lg () + 0:822;(7) 1 p c = 2 lg c = 2 <p> y) + h 1 (x; y) fl c = inf lg () + 0:822;(7) 1 p c = 2 lg c = 2 lg lg (4 2 2) 4:260:(9) Informally, we think of (x; y) as a measure of the "distance" between x and y for x &lt; y in <ref> [0; 1] </ref>. The function h ff (x; y) may then be viewed as the fractional decrease in the distance between x and y that results from applying ff to both x and y. <p> Lemma 5.13 shows that for certain small values of ", the difference ff (1 ") ff (") is small for most binary strings ff. lemma 5.8. For all x &lt; y in <ref> [0; 1] </ref>, y x (x; y)=2: Proof. Define (z) = 4 z z for z in [0; 1]. Since d (z) = 4 z 1 for z in [0; 1], we know that (z) is a non-decreasing function of z. <p> Lemma 5.13 shows that for certain small values of ", the difference ff (1 ") ff (") is small for most binary strings ff. lemma 5.8. For all x &lt; y in <ref> [0; 1] </ref>, y x (x; y)=2: Proof. Define (z) = 4 z z for z in [0; 1]. Since d (z) = 4 z 1 for z in [0; 1], we know that (z) is a non-decreasing function of z. Hence, (x; y) (y x) = y (1x) 4 lg e = (y) (x) and thus y x 4 lg e lemma 5.9. <p> For all x &lt; y in <ref> [0; 1] </ref>, y x (x; y)=2: Proof. Define (z) = 4 z z for z in [0; 1]. Since d (z) = 4 z 1 for z in [0; 1], we know that (z) is a non-decreasing function of z. Hence, (x; y) (y x) = y (1x) 4 lg e = (y) (x) and thus y x 4 lg e lemma 5.9. For all x &lt; y in [0; 1], we have: (i) h * (x; y) = <p> d (z) = 4 z 1 for z in <ref> [0; 1] </ref>, we know that (z) is a non-decreasing function of z. Hence, (x; y) (y x) = y (1x) 4 lg e = (y) (x) and thus y x 4 lg e lemma 5.9. For all x &lt; y in [0; 1], we have: (i) h * (x; y) = 1, and (ii) for all binary strings ff and fi, h fiff (x; y) = h fi ( ff (x); ff (y)) h ff (x; y): 20 F. T. LEIGHTON AND C. G. PLAXTON Proof. <p> T. LEIGHTON AND C. G. PLAXTON Proof. Since * is the identity function, h * (x; y) = 1 for all x &lt; y in <ref> [0; 1] </ref>. <p> For all x &lt; y in <ref> [0; 1] </ref>, 1, and d 0, we have H (x; y; d) () d : Proof. For d = 0, the result is immediate since H (x; y; 0) = 1. <p> = (1 2 ") pow ((fl 1) d)=4. (Note that 0 &lt; ffi &lt; 1=4 since (fl; "; d) is an admissible triple.) We remark that, using Lemma 5.4, ff (z) can be computed in O (jffj) arithmetic operations (counting square root as a single operation) for any z in <ref> [0; 1] </ref>. <p> This occurs with probability at least 1 (n + 1) "(d) = 1 pow ( pow (g (d))), as required. lemma 6.2. For all a, b, and d such that 0 b a d, and all " and " 0 in <ref> [0; 1] </ref>, we have Sort D (d; a; " + 2 " 0 ) Sort D (d; a; b; ") + Sort D (d; b; " 0 ) + 2 Merge D (d; b; 1): HYPERCUBIC SORTING NETWORKS 27 Proof. <p> Lemmas 4.3, 4.6, and 6.2 together imply that Sort D (d; a; ") Sort D (d; a; b; ") + O (b)(10) for all a, b, and d such that 0 b a d, and all " in <ref> [0; 1] </ref>. lemma 6.3. For all a, b, and d such that 0 b a d, and all " in [0; 1], we have Sort D (d; a; b + 1; ") Most D (d; a; b; ") + Insert D (d; a b): Proof. <p> Sort D (d; a; ") Sort D (d; a; b; ") + O (b)(10) for all a, b, and d such that 0 b a d, and all " in <ref> [0; 1] </ref>. lemma 6.3. For all a, b, and d such that 0 b a d, and all " in [0; 1], we have Sort D (d; a; b + 1; ") Most D (d; a; b; ") + Insert D (d; a b): Proof. <p> Hence A has a dirty region of size at most 2 pow (b) = pow (b + 1) after stage (e), as desired. lemma 6.4. For all a, b, and d such that 0 b a d, and all fl, ", and " 0 in <ref> [0; 1] </ref>, we have Sort D (d; a; b 0 + 3; " + 2 " 0 ) Sort D (d; a; b; ") + Sort D (d; b + 2; b 0 + 1; " 0 ) + Merge D (d; b b 0 ; 1); where b 0 = <p> 6.4, we find that Sort D (d; a; b (fl c + o (1)) (b + 2)c + 3; pow ( pow (o (b))) + " 0 ) Sort D (d; a; b; " 0 ) + (3 2 fl c + o (1)) b; for all " 0 in <ref> [0; 1] </ref>. <p> Proof. Similar to the proof of Theorem 6.1. lemma 7.2. For all a, b, and d such that 0 b a d, and all " and " 0 in <ref> [0; 1] </ref>, we have Sort h D (d; a; " + 2 " 0 ) Sort h D (d; a; b; ") + Sort h D (d; b; " 0 ) + 2 Merge h D (d; b; 1) + O (a): Proof. Similar to the proof of Lemma 6.2. <p> This accounts for the additive O (a) term on the right-hand side of the inequality. lemma 7.3. For all a, b, and d such that 0 b a d, and all " in <ref> [0; 1] </ref>, we have Sort h D (d; a; b + 1; ") Most h D (d; a; b; ") + Insert h D (d; a b) + O (a): Proof. Similar to the proof of Lemma 6.3. <p> Lemmas 4.3 and 7.2 (with b = bfl ac + 1) now give Sort h D (d; a; pow ( pow (fi (a))) + 2 " 0 ) Sort h D (d; bfl ac + 1; " 0 ) + O (a) for all " 0 in <ref> [0; 1] </ref>. Iteratively applying the preceding inequality, we find that Sort h D (d; a; pow ( pow (fi (b)))) Sort h D (d; b; 0) + O (a) for all a, b, and d such that 0 b a d. <p> ac + 1; O (pow (a) "(a))) = O (a): Lemmas 4.3 and 7.2 now give Sort h D (d; a; O (pow (a) "(a)) + 2 " 0 ) Sort h D (d; bfl (a) ac + 1; " 0 ) + O (a); for all " 0 in <ref> [0; 1] </ref>. Iteratively applying the preceding inequality, we find that Sort h D (d; a; O (pow (b) "(b))) Sort h D (d; b; 0) + O (a -(a)) for all a, b, and d such that 0 b a d. lemma 9.2.
Reference: [2] <author> M. Ajtai, J. Komlos, and E. Szemeredi. </author> <title> Sorting in c log n parallel steps. </title> <journal> Combinatorica, </journal> <volume> 3 </volume> <pages> 1-19, </pages> <year> 1983. </year> <note> 42 F. </note> <author> T. LEIGHTON AND C. G. </author> <note> PLAXTON </note>
Reference-contexts: The source of the difficulty of this particular exercise was subsequently revealed by Ajtai, Komlos, and Szemeredi <ref> [2] </ref>, who provided an optimal O (lg n)-depth construction known as the AKS sorting network. While the AKS sorting network represents a major theoretical breakthrough, it suffers from two significant shortcomings. First, the multiplicative constant hidden within the O-notation is sufficiently large that the result remains impractical. <p> A hypercubic version of the construction is discussed in the next paragraph.) At the expense of allowing the network to fail on a small fraction of the n! possible input permutations, this construction improves upon the asymptotic depth of the best previously known sorting networks by several orders of magnitude <ref> [2, 15] </ref>. We make use of the AKS construction as part of our network. <p> This bound is due to Batcher [4], and follows from repeated application of Lemma 4.3. lemma 4.6. For all a and d such that 0 a d, we have Sort D (d; a; 0) = O (a): Proof. This bound is due to Ajtai, Komlos and Szemeredi <ref> [2] </ref>. Unfortunately, the constant factor associated with the AKS sorting network is impractically large. lemma 4.7.
Reference: [3] <author> M. Ajtai, J. Komlos, and E. Szemeredi. Halvers and expanders. </author> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 686-692, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: A similar technique has recently been used by Ajtai, Komlos, and Szemeredi as part of an improved version of their original sorting network construction <ref> [3] </ref>. The multiplicative constant associated with the new construction is significantly lower than the constant established by Paterson [15], though it remains impractical. The main idea underlying the results of this section may be informally outlined as follows.
Reference: [4] <author> K. E. Batcher. </author> <title> Sorting networks and their applications. </title> <booktitle> In Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <volume> vol. 32, </volume> <pages> pages 307-314, </pages> <year> 1968. </year>
Reference-contexts: Hence the well-known (n lg n) sequential lower bound for comparison-based sorting implies an (lg n) lower bound on the depth of any n-input sorting network. An elegant O (lg 2 n)-depth upper bound is given by Batcher's bitonic sorting network <ref> [4] </ref>. For small values of n, the depth of bitonic sort either matches or is very close to matching that of the best constructions known (a very limited number of which are known to be optimal) [11, x5.3.4]. <p> For all a and d such that 0 &lt; a d, we have Merge D (d; a 1; 1) a; Merge h D (d; a 1; 1) = O (a): Proof. These bounds are established by Batcher's bitonic merge network <ref> [4] </ref>. <p> For all a and d such that 0 a d, we have Insert D (d; a) a; Insert h D (d; a) = O (a): Proof. This bound is also established by Batcher's bitonic merge network <ref> [4] </ref>. <p> The basic idea is to use a tree-like network.) lemma 4.5. For all a and d such that 0 a d, we have Sort h D (d; a; 0) = O (a 2 ): Proof. This bound is due to Batcher <ref> [4] </ref>, and follows from repeated application of Lemma 4.3. lemma 4.6. For all a and d such that 0 a d, we have Sort D (d; a; 0) = O (a): Proof. This bound is due to Ajtai, Komlos and Szemeredi [2].
Reference: [5] <author> V. E. </author> <title> Benes. Optimal rearrangeable multistage connecting networks. </title> <journal> Bell System Technical Journal, </journal> <volume> 43 </volume> <pages> 1641-1656, </pages> <year> 1964. </year>
Reference-contexts: Furthermore, the gate assignments of N can be computed in time polynomial in pow (a). Proof. This is a straightforward consequence of the work of Benes <ref> [5] </ref>. In particular, for a = d, the Benes permutation network corresponds to a hypercubic d-network satisfying properties (i), (iii), and (v).
Reference: [6] <author> H. Chernoff. </author> <title> A measure of the asymptotic efficiency for tests of a hypothesis based on the sum of observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23 </volume> <pages> 493-509, </pages> <year> 1952. </year>
Reference-contexts: Note that, unlike the p fi 's, each q ff is a random variable. Furthermore, the random variable X ff is easily seen to be the sum of pow (b) independent Bernoulli trials, where trial fi has success probability p fi . Thus, a standard Chernoff-type argument <ref> [6] </ref> implies PrfjX ff p pow (b)j # pow (b)g 2 e 2# 2 pow (b) (14) for all # 0. Define a random execution to be ffi-balanced if p ffi q ff p + ffi for all ff.
Reference: [7] <author> R. E. Cypher. </author> <title> Theoretical aspects of VLSI pin limitations. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22 </volume> <pages> 58-63, </pages> <year> 1993. </year>
Reference-contexts: Second, the structure of the network is sufficiently "irregular" that it does not seem to map efficiently to common interconnection schemes. In fact, Cypher has proven that any emulation of the AKS network on the cube-connected cycles requires (lg 2 n) time <ref> [7] </ref>. The latter issue is of significant interest, since a primary motivation for considering the problem of constructing small-depth sorting networks is to obtain a fast parallel sorting algorithm for a general-purpose parallel computer.
Reference: [8] <author> R. E. Cypher and C. G. Plaxton. </author> <title> Techniques for shared key sorting. </title> <type> Technical Report RJ 7347, </type> <institution> Computer Science Department, IBM Almaden Research Center, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: A small improvement to the Sharesort bound is known when polynomial-time "pre-processing" (to compute certain look-up tables) is allowed. In particular, the running time of Sharesort can be improved to O (d (lg d) lg fl d) in that case <ref> [8] </ref>. This improvement has been incorporated into the "(d) bound of Theorem 10.2. If exponential pre-processing is allowed, the running time of Sharesort can be improved further to O (d lg d) [8]. <p> running time of Sharesort can be improved to O (d (lg d) lg fl d) in that case <ref> [8] </ref>. This improvement has been incorporated into the "(d) bound of Theorem 10.2. If exponential pre-processing is allowed, the running time of Sharesort can be improved further to O (d lg d) [8]. However, it is not clear whether the latter result could be used to improve the "(d) bound of Theorem 10.2. (The lack of uniformity can be eliminated through randomization.
Reference: [9] <author> R. E. Cypher and C. G. Plaxton. </author> <title> Deterministic sorting in nearly logarithmic time on the hypercube and related computers. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 47 </volume> <pages> 501-548, </pages> <year> 1993. </year>
Reference-contexts: of our sorting algorithm is non-adaptive in the sense that it can be described solely in terms of oblivious routing and compare-interchange operations; there is no queueing. (The very 4 high probability version is adaptive because it makes use of the Sharesort algorithm of Cypher and Plaxton as a subroutine <ref> [9] </ref>.) Note that the permutation routing problem, in which each processor has a packet of information to send to another processor, and no two packets are destined to the same processor, is trivially reducible to the sorting problem. (The idea is to sort the packets based on their destination addresses.) Hence, <p> The scheme of x7 can also be used to prove Theorem 10.1 below with the function " as defined in Theorem 10.2. In this case, we can dramatically decrease the failure probability by making use of the Sharesort algorithm of Cypher and Plaxton <ref> [9] </ref>. Sharesort is a polynomial-time uniform hypercubic sorting algorithm with worst-case running time O (d lg 2 d) [9]. Note that Sharesort runs in O (d) time on O (d= lg 2 d)- cubes. <p> In this case, we can dramatically decrease the failure probability by making use of the Sharesort algorithm of Cypher and Plaxton <ref> [9] </ref>. Sharesort is a polynomial-time uniform hypercubic sorting algorithm with worst-case running time O (d lg 2 d) [9]. Note that Sharesort runs in O (d) time on O (d= lg 2 d)- cubes. Hence, we can modify the scheme of x7 by cutting off the sorting recurrence at fi (d= lg 2 d)-cubes instead of fi ( p d)-cubes (as allowed by bitonic sort). Unfortunately, 38 F.
Reference: [10] <author> W. Hoeffding. </author> <title> On the distribution of the number of successes in independent trials. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 27 </volume> <pages> 713-721, </pages> <year> 1956. </year>
Reference: [11] <author> D. E. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> volume 3. </volume> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: In fact, any n-input comparator network that sorts the 2 n possible 0-1 vectors of length n is a sorting network. The latter result is known as the 0-1 principle for sorting networks <ref> [11, x5.3.4] </ref>. fl This paper combines results appearing in preliminary form as A (fairly) Simple Circuit that Usually Sorts, in Proceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science, IEEE Computer Society Press, Los Alamitos, CA, 1990, pp. 264-274; and as A Hypercubic Sorting Network with Nearly Logarithmic <p> For small values of n, the depth of bitonic sort either matches or is very close to matching that of the best constructions known (a very limited number of which are known to be optimal) <ref> [11, x5.3.4] </ref>. Thus, one might suspect the depth of Batcher's bitonic sorting network to be optimal to within a constant factor, or perhaps even to within a lower-order additive term. <p> Thus, one might suspect the depth of Batcher's bitonic sorting network to be optimal to within a constant factor, or perhaps even to within a lower-order additive term. Consider Knuth's Exercise 5.3.4.51 <ref> [11] </ref> (posed as an open problem): Prove that the asymptotic value of ^ S (n) is not O (nlg n), where ^ S (n) denotes the minimal size (number of comparator gates) of an n-input sorting network of any depth. <p> In particular, we say that a comparator network is hypercubic if and only if successive levels are connected either by a shu*e or an un-shu*e (inverse shu*e) permutation. (These terms are defined more precisely in x3.) Knuth's Exercise 5.3.4.47 <ref> [11] </ref>, posed as an open problem, may be viewed as asking for the depth complexity of shu*e-only sorting networks, in which every pair of adjacent levels is connected by a shu*e permutation. <p> T. LEIGHTON AND C. G. PLAXTON lemma 4.1. A d-network N is a sorting network if and only if (d) Sort (N ): Proof. This lemma is known as the 0-1 principle for sorting networks, and is proven in <ref> [11, x5.3.4] </ref>. (The proof is given in the context of deterministic networks.
Reference: [12] <author> F. T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, and Hypercubes. </title> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: This correspondence allows any shu*e-only comparator network to be efficiently emulated (i.e., with constant slowdown) on any hypercubic machine. (We remark that "hypercubic machines" are more commonly referred to as "hypercubic networks" <ref> [12, Chapter 3] </ref>. <p> networks" <ref> [12, Chapter 3] </ref>. We prefer the term "hypercubic machines" in the present context only because we use the term "networks" to refer to comparator networks.) However, the class of hypercu-bic machines is most often characterized in terms of efficient emulation of so-called HYPERCUBIC SORTING NETWORKS 3 "normal" hypercube algorithms [12, Chapter 3], which effectively allow the data to either be shu*ed or unshu*ed at each step. (More formally, a hypercube algorithm is "normal" if it satisfies the following two conditions: (i) in any given step of the computation, communication occurs across a single dimension, and (ii) in any pair of <p> In fact, standard reductions <ref> [12, x3.4.3] </ref> allow us to apply our sorting algorithm to efficiently solve a variety of other routing problems as well (e.g., many-to-one routing with combining). <p> in bitonic form. (We remark that in the classic sorting network model, where a given level may contain fewer than pow (d 1) gates, it is possible to HYPERCUBIC SORTING NETWORKS 13 match this depth bound while achieving size pow (d) 1 instead of d pow (d) [14] (see also <ref> [12, x3.5.4] </ref>). The basic idea is to use a tree-like network.) lemma 4.5. For all a and d such that 0 a d, we have Sort h D (d; a; 0) = O (a 2 ): Proof. <p> We then use this coin-tossing network to develop a polynomial-time uniform hypercubic algorithm that sorts every d-vector in O (d) time with high probability. We define a hypercubic algorithm as any normal hypercube algorithm. (See <ref> [12, x3.1.3] </ref>, for example, for a definition of the class of normal hypercube algorithms.) Every depth-a hypercubic sorting d-network corresponds to a (possibly non-uniform) hypercubic sorting algorithm that runs in O (a) time on any pow (d)-processor hy-percubic machine. <p> Omega machines belong to the class of butterfly-like machines discussed in <ref> [12, x3.8.1] </ref>. Observe that there is a close correspondence between an order-d omega machine M and a depth-d shu*e d-network N .
Reference: [13] <author> F. T. Leighton, B. M. Maggs, A. G. Ranade, and S. B. Rao. </author> <title> Randomized routing and sorting on fixed-connection networks. </title> <journal> Journal of Algorithms, </journal> <volume> 17 </volume> <pages> 157-205, </pages> <year> 1994. </year>
Reference-contexts: Probability of failure aside, Flash-sort requires more storage than our algorithm, since it makes use of a fi (lg n)-sized priority queue at each processor. On the other hand, a very high probability sorting algorithm with constant size queues has previously been given by Leighton, Maggs, Ranade, and Rao <ref> [13] </ref>. <p> In fact, standard reductions [12, x3.4.3] allow us to apply our sorting algorithm to efficiently solve a variety of other routing problems as well (e.g., many-to-one routing with combining). Interestingly, all previously known optimal-time algorithms for permutation routing on hypercubic machines <ref> [13, 18, 20] </ref> are randomized, and do not achieve a success probability better than "very high". Thus, the results of x10 provide a permutation routing algorithm for 6 F. T. LEIGHTON AND C. G. PLAXTON Table 1 Type conventions.
Reference: [14] <author> F. T. Leighton and C. G. Plaxton. </author> <title> A (fairly) simple circuit that (usually) sorts. </title> <booktitle> In Proceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 264-274, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: input is already in bitonic form. (We remark that in the classic sorting network model, where a given level may contain fewer than pow (d 1) gates, it is possible to HYPERCUBIC SORTING NETWORKS 13 match this depth bound while achieving size pow (d) 1 instead of d pow (d) <ref> [14] </ref> (see also [12, x3.5.4]). The basic idea is to use a tree-like network.) lemma 4.5. For all a and d such that 0 a d, we have Sort h D (d; a; 0) = O (a 2 ): Proof.
Reference: [15] <author> M. S. Paterson. </author> <title> Improved sorting networks with O(log N) depth. </title> <journal> Algorithmica, </journal> <volume> 5 </volume> <pages> 75-92, </pages> <year> 1990. </year>
Reference-contexts: A hypercubic version of the construction is discussed in the next paragraph.) At the expense of allowing the network to fail on a small fraction of the n! possible input permutations, this construction improves upon the asymptotic depth of the best previously known sorting networks by several orders of magnitude <ref> [2, 15] </ref>. We make use of the AKS construction as part of our network. <p> A similar technique has recently been used by Ajtai, Komlos, and Szemeredi as part of an improved version of their original sorting network construction [3]. The multiplicative constant associated with the new construction is significantly lower than the constant established by Paterson <ref> [15] </ref>, though it remains impractical. The main idea underlying the results of this section may be informally outlined as follows. An n-input network is an m-way merging network if and only if it correctly merges all possible input vectors consisting of m sorted vectors of length n=m.
Reference: [16] <author> C. G. Plaxton. </author> <title> A hypercubic sorting network with nearly logarithmic depth. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 405-416, </pages> <month> May </month> <year> 1992. </year>
Reference: [17] <author> C. G. Plaxton and T. Suel. </author> <title> A lower bound for sorting networks based on the shu*e permutation. </title> <journal> Mathematical Systems Theory, </journal> <volume> 27 </volume> <pages> 491-508, </pages> <year> 1994. </year>
Reference-contexts: Batcher's bitonic sort provides an O (lg n) upper bound for this problem, and recently, Plaxton and Suel <ref> [17] </ref> have established an (lg 2 n= lg lg n) lower bound. (The same lower bound holds for the class of unshu*e-only sorting networks.) From a practical point of view, Knuth's shu*e-only requirement would seem to be overly-restrictive. <p> exist hypercubic sorting networks of depth 2 p Note that this bound is o (lg 1+" n) for any constant " &gt; 0. (See Theorem 9.1 for a more precise form of the upper bound.) Given the aforementioned (lg 2 n= lg lg n) lower bound of Plaxton and Suel <ref> [17] </ref>, our upper bound establishes a surprisingly strong separation between the power of shu*e-only comparator networks and that of hypercubic comparator networks.
Reference: [18] <author> A. G. Ranade. </author> <title> How to emulate shared memory. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 42 </volume> <pages> 307-326, </pages> <year> 1991. </year>
Reference-contexts: In fact, standard reductions [12, x3.4.3] allow us to apply our sorting algorithm to efficiently solve a variety of other routing problems as well (e.g., many-to-one routing with combining). Interestingly, all previously known optimal-time algorithms for permutation routing on hypercubic machines <ref> [13, 18, 20] </ref> are randomized, and do not achieve a success probability better than "very high". Thus, the results of x10 provide a permutation routing algorithm for 6 F. T. LEIGHTON AND C. G. PLAXTON Table 1 Type conventions.
Reference: [19] <author> J. H. Reif and L. G. Valiant. </author> <title> A logarithmic time sort for linear size networks. </title> <journal> Journal of the ACM, </journal> <volume> 34 </volume> <pages> 60-76, </pages> <year> 1987. </year>
Reference-contexts: For example, the Flashsort algorithm of Reif and Valiant <ref> [19] </ref> is in this category. However, none of these algorithms has a success probability better than "very high". Probability of failure aside, Flash-sort requires more storage than our algorithm, since it makes use of a fi (lg n)-sized priority queue at each processor.

References-found: 19

