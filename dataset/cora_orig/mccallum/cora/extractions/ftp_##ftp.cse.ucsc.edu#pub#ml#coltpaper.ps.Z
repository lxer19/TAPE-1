URL: ftp://ftp.cse.ucsc.edu/pub/ml/coltpaper.ps.Z
Refering-URL: http://www.cse.ucsc.edu/~haussler/pubs.html
Root-URL: http://www.cse.ucsc.edu
Title: General Bounds on the Mutual Information Between a Parameter and n Conditionally Independent Observations  
Author: David Haussler Manfred Opper 
Affiliation: UC Santa Cruz  Universitat Wurzburg  
Abstract: Each parameter in an abstract parameter space fi is associated with a different probability distribution on a set Y . A parameter is chosen at random from fi according to some a priori distribution on fi, and n conditionally independent random variables Y n = Y 1 ; : : : Y n are observed with common distribution determined by . We obtain bounds on the mutual information between the random variable fi, giving the choice of parameter, and the random variable Y n , giving the sequence of observations. We also bound the supremum of the mutual information, over choices of the prior distribution on fi. These quantities have applications in density estimation, computational learning theory, universal coding, hypothesis testing, and portfolio selection theory. The bounds are given in terms of the metric and information dimensions of the parameter space fi with respect to the Hellinger distance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amari and N. Murata. </author> <title> Statistical theory of learning curves under entropic loss. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 140-153, </pages> <year> 1993. </year>
Reference-contexts: bounds on the average instantaneous regret for the encoding of the tth observation as a function of t, or equivalently the average additional information gained about the true state of nature from the tth observation, which is related to loss measures more commonly examined in computational learning theory (see e.g. <ref> [20, 1] </ref>). Further applications of the results given here to this and related problems are described in [29, 23]. 2 Basic Definitions and Main Result We use the following notational conventions. <p> The following is a useful lemma in this direction. See e.g. [6] for related results. Lemma 5 For any z, 0 z 1, define b (z) = 2 (1 z) 2 2 <ref> [1=2; 1] </ref>: For all distributions P = fp i g and Q = fq i g, b (s)D H (P; Q) D K (P jjQ) b (r)D H (P; Q); where r = inf i q i q i p i . <p> As the instantaneous information gain is of interest in many areas, including computational learning theory <ref> [20, 1] </ref>, we briefly discuss here how our results may be used to analyze it. Lemma 6 Let a 1 ; a 2 ; : : : be a sequence of real numbers such that a = lim n!1 a n ln n exists (possibly being infi nite). <p> The argument for the case of infinite b is similar. 2 It follows from this lemma that whenever lim I (fi; Y n ) = d 2 <ref> [0; 1] </ref>; for example, as in the cases described in the Gaussian example in Section 3 and in Section 4 of this paper, then either lim n!1 nI (fi; Y n ) = d or this limit does not exist.
Reference: [2] <author> A. Barron. </author> <title> The strong ergodic theorem for densities: generalized Shannon-McMillan-Breiman theorem. </title> <journal> The Annals of Probability, </journal> <volume> 13 </volume> <pages> 1292-1303, </pages> <year> 1985. </year>
Reference-contexts: In the case that fi is finite, results of Renyi [37] show further that the difference I (fi; Y n ) H (fi) converges to zero exponentially fast in n. More general results, including the above corollary, follow from results in Pinsker's book [30] (see also <ref> [2] </ref>). For uncountable fi, I (fi; Y n ) is typically unbounded as n grows. To illustrate Theorem 1 in this case, we calculate the bounds given there for a simple Gaussian case, where an exact formula for the mutual information can be obtained.
Reference: [3] <author> A. Barron. </author> <title> The exponential convergence of posterior probabilities with implications for Bayes estimators of density functions. </title> <type> Technical Report 7, </type> <institution> Dept. of Statistics, U. Ill. Urbana-Champaign, </institution> <year> 1987. </year>
Reference-contexts: Theorem 1 For every n 1, IE fi fl ln IE fi e n 1 H (; fl ) I (fi; Y n ) The upper bound follows from results given in <ref> [3] </ref> and is mentioned there, however we give a simple and direct proof. To the best of our knowledge, the lower bound is new. The proof is given in a series of lemmas and calculations. We begin with the upper bound.
Reference: [4] <author> A. Barron, B. Clarke, and D. Haussler. </author> <title> Information bounds for the risk of bayesian predictions and the redundancy of universal codes. </title> <booktitle> In Proc. International Symposium on Information Theory. </booktitle>
Reference-contexts: These results were extended to the mutual information and minimax risk in [11]. Related lower bounds, which are often quoted, were obtained by Ris-sanen [38], based on certain asymptotic normality assumptions. In this paper we extend work from <ref> [19, 4] </ref>, obtaining bounds on I (fi; Y n ) in more general settings. The approach taken here is to relate the mutual information I (fi; Y n ) directly to certain metric properties of the parameter space fi.
Reference: [5] <author> A. Barron and T. </author> <title> Cover. A bound on the financial value of information. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 34 </volume> <pages> 1097-1100, </pages> <year> 1988. </year>
Reference-contexts: This general setup is fundamental in statistics and related disciplines, including coding and data compression [12], computational learning theory [19, 22, 20, fl Supported by NSF grant IRI-9123692. Email addresses: haussler@cse.ucsc.edu y Supported by Heisenberg fellowship of DFG Email addresses: opper@physik.uni-wuerzburg.de 1, 28, 41], and portfolio selection theory <ref> [5] </ref>. It is usually called density estimation or parameter estimation in statistics, depending on the nature of the parameter space fi. In this paper we explore this setup from an information theoretic point of view.
Reference: [6] <author> L. Birge. </author> <title> Approximation dans les espaces metriques et theorie de l'estimation. </title> <journal> Zeitschrift fuer Wahrscheinlichkeitstheorie und verwandte gebiete, </journal> <volume> 65 </volume> <pages> 181-237, </pages> <year> 1983. </year>
Reference-contexts: Metric entropies defined in terms of the relative entropy and Hellinger distances have been used in the context of density estimation by LeCam [27], Birge <ref> [6, 7] </ref>, Has-minskii and Ibragimov [17], and van de Geer [40], the latter with an explicit goal of applying methods from empirical processes to this problem. <p> The following is a useful lemma in this direction. See e.g. <ref> [6] </ref> for related results.
Reference: [7] <author> L. Birge. </author> <title> On estimating a density using Hellinger distance and some other strange facts. Probability theory and related fields, </title> <booktitle> 71 </booktitle> <pages> 271-291, </pages> <year> 1986. </year>
Reference-contexts: Metric entropies defined in terms of the relative entropy and Hellinger distances have been used in the context of density estimation by LeCam [27], Birge <ref> [6, 7] </ref>, Has-minskii and Ibragimov [17], and van de Geer [40], the latter with an explicit goal of applying methods from empirical processes to this problem.
Reference: [8] <author> L. Birge and P. Massart. </author> <title> Rates of convergence for minimum contrast estimators. Probability Theory and Related Fields, </title> <booktitle> 97 </booktitle> <pages> 113-150, </pages> <year> 1993. </year>
Reference-contexts: These include definitions of dimension via the metric entropy as introduced by Kolmogorov and Tikhomirov in [26] and commonly used in the theory of empirical processes (see e.g. <ref> [13, 31, 16, 8] </ref>), as well as Renyi's information dimension [36, 15, 25] and the associated Posner-Rodemich *-entropy [32, 33, 34, 35].
Reference: [9] <author> B. Clarke. </author> <title> Asymptotic cumulative risk and Bayes risk under entropy loss with applications. </title> <type> PhD thesis, </type> <institution> Dept. of Statistics, University of Ill., </institution> <year> 1989. </year>
Reference-contexts: In this case they were even able to estimate the lower order additive terms in this approximation, which involve the Fisher information and the entropy of the prior. Further related results were given by Efroimovich [14] and Clarke <ref> [9] </ref>. Clarke and Barron gave a detailed analysis, with applications, of the risk (redundancy) of the Bayes method as a function of the true state of nature [10]. These results were extended to the mutual information and minimax risk in [11].
Reference: [10] <author> B. Clarke and A. Barron. </author> <title> Information-theoretic asymptotics of Bayes methods. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 36(3) </volume> <pages> 453-471, </pages> <year> 1990. </year>
Reference-contexts: This leads to the other standard interpretations of the mutual information in terms of density estimation and coding theory. Clarke and Barron discuss further interpretations of this quantity in the context of hypothesis testing and portfolio selection theory <ref> [10] </ref>. <p> Further related results were given by Efroimovich [14] and Clarke [9]. Clarke and Barron gave a detailed analysis, with applications, of the risk (redundancy) of the Bayes method as a function of the true state of nature <ref> [10] </ref>. These results were extended to the mutual information and minimax risk in [11]. Related lower bounds, which are often quoted, were obtained by Ris-sanen [38], based on certain asymptotic normality assumptions.
Reference: [11] <author> B. Clarke and A. Barron. </author> <title> Jefferys' prior is asymptotically least favorable under entropy risk. </title> <journal> J. Statistical Planning and Inference, </journal> <volume> 41 </volume> <pages> 37-60, </pages> <year> 1994. </year>
Reference-contexts: Clarke and Barron gave a detailed analysis, with applications, of the risk (redundancy) of the Bayes method as a function of the true state of nature [10]. These results were extended to the mutual information and minimax risk in <ref> [11] </ref>. Related lower bounds, which are often quoted, were obtained by Ris-sanen [38], based on certain asymptotic normality assumptions. In this paper we extend work from [19, 4], obtaining bounds on I (fi; Y n ) in more general settings. <p> For the lower bound, using Theorem 1 and Fatou's lemma lim inf I (fi; Y n ) lim inf i X P ( j )e n i n!1 X P ( j )e n = i = H (fi): 2 This result generalizes the similar result in <ref> [11] </ref> (Corollary 1) by removing the additional conditions assumed there. In the case that fi is finite, results of Renyi [37] show further that the difference I (fi; Y n ) H (fi) converges to zero exponentially fast in n.
Reference: [12] <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: This general setup is fundamental in statistics and related disciplines, including coding and data compression <ref> [12] </ref>, computational learning theory [19, 22, 20, fl Supported by NSF grant IRI-9123692. Email addresses: haussler@cse.ucsc.edu y Supported by Heisenberg fellowship of DFG Email addresses: opper@physik.uni-wuerzburg.de 1, 28, 41], and portfolio selection theory [5]. <p> j) IE fi fl ln IE fi e P (y n j fl ) = IE fi fl ln IE fi e n K ( fl ;) ; where the last equality follows from the fact that the KL divergence is additive over the product of independent distributions (see e.g. <ref> [12] </ref>). <p> Note that this quantity is nonnegative. When H (fi) is finite it is easily verified that I (fi; Y n ) = H (fi) H (fijY n ) (see e.g. <ref> [12] </ref>), and thus lim sup n!1 I (fi; Y n ) H (fi) in this case as well.
Reference: [13] <author> R. M. Dudley. </author> <title> A course on empirical processes. </title> <booktitle> Lecture Notes in Mathematics, </booktitle> <volume> 1097 </volume> <pages> 2-142, </pages> <year> 1984. </year>
Reference-contexts: These include definitions of dimension via the metric entropy as introduced by Kolmogorov and Tikhomirov in [26] and commonly used in the theory of empirical processes (see e.g. <ref> [13, 31, 16, 8] </ref>), as well as Renyi's information dimension [36, 15, 25] and the associated Posner-Rodemich *-entropy [32, 33, 34, 35].
Reference: [14] <author> S. Y. Efroimovich. </author> <title> Information contained in a sequence of observations. Problems in Information Transmission, </title> <booktitle> 15 </booktitle> <pages> 178-189, </pages> <year> 1980. </year>
Reference-contexts: In this case they were even able to estimate the lower order additive terms in this approximation, which involve the Fisher information and the entropy of the prior. Further related results were given by Efroimovich <ref> [14] </ref> and Clarke [9]. Clarke and Barron gave a detailed analysis, with applications, of the risk (redundancy) of the Bayes method as a function of the true state of nature [10]. These results were extended to the mutual information and minimax risk in [11].
Reference: [15] <author> J. D. Farmer, E. Ott, and J. A. Yorke. </author> <title> The dimension of chaotic attractors. </title> <journal> Physica D, </journal> <volume> 7 </volume> <pages> 153-180, </pages> <year> 1983. </year>
Reference-contexts: These include definitions of dimension via the metric entropy as introduced by Kolmogorov and Tikhomirov in [26] and commonly used in the theory of empirical processes (see e.g. [13, 31, 16, 8]), as well as Renyi's information dimension <ref> [36, 15, 25] </ref> and the associated Posner-Rodemich *-entropy [32, 33, 34, 35]. <p> When dim K (X) = dim K (X), then this value is denoted dim K (X) and called the metric dimension of X. The information dimension <ref> [36, 15, 25] </ref>, volume-scaling dimension, and Laplace transform dimension are defined analogously by substituting H *; , V *; and L *; respectively for K * in the above definition, and are denoted dim H; (X), dim V; (X) and dim L; (X), respectively.
Reference: [16] <author> E. Gine and J. Zinn. </author> <title> Some limit theorems for empirical processes. </title> <journal> Annals of Probability, </journal> <volume> 12 </volume> <pages> 929-989, </pages> <year> 1984. </year>
Reference-contexts: These include definitions of dimension via the metric entropy as introduced by Kolmogorov and Tikhomirov in [26] and commonly used in the theory of empirical processes (see e.g. <ref> [13, 31, 16, 8] </ref>), as well as Renyi's information dimension [36, 15, 25] and the associated Posner-Rodemich *-entropy [32, 33, 34, 35].
Reference: [17] <author> R. Hasminskii and I. Ibragimov. </author> <title> On density estima-tion in the view of Kolmogorov's ideas in approximation theory. </title> <journal> Annals of statistics, </journal> <volume> 18 </volume> <pages> 999-1010, </pages> <year> 1990. </year>
Reference-contexts: Metric entropies defined in terms of the relative entropy and Hellinger distances have been used in the context of density estimation by LeCam [27], Birge [6, 7], Has-minskii and Ibragimov <ref> [17] </ref>, and van de Geer [40], the latter with an explicit goal of applying methods from empirical processes to this problem.
Reference: [18] <author> D. Haussler. </author> <title> A general minimax result for relative entropy. </title> <year> 1995. </year>
Reference-contexts: Theorem 2 If dim K (fi; 1=2 H ) is finite then 2 Here we use the Borel subsets with respect to the topol ogy of weak convergence of measures, as in <ref> [18] </ref>. 1. n!1 ln n dim H;P fi (fi; H ) and lim inf C (fi; Y n ) 1=2 2 Proof: It follows from the lower bound of Theorem 1 that I (fi; Y n ) L p 4=n;P fi 1=2 H ).
Reference: [19] <author> D. Haussler and A. Barron. </author> <title> How well do Bayes methods work for on-line prediction of f+1; 1g values? In Proceedings of the Third NEC Symposium on Computation and Cognition. </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: A simple calculation (see e.g. <ref> [19] </ref>) shows that the Bayes risk is equal to the mutual information I (fi; Y n ) between the random variable fi and the random variable Y n = Y 1 ; : : : ; Y n , which can be interpreted as the average amount of information contained in <p> These results were extended to the mutual information and minimax risk in [11]. Related lower bounds, which are often quoted, were obtained by Ris-sanen [38], based on certain asymptotic normality assumptions. In this paper we extend work from <ref> [19, 4] </ref>, obtaining bounds on I (fi; Y n ) in more general settings. The approach taken here is to relate the mutual information I (fi; Y n ) directly to certain metric properties of the parameter space fi. <p> It is easily verified that n X I (fi; Y t ) = I (fi; Y n ); so the total average instantaneous information gain is the same as the average total information gain, or mu tual information, as expected (see e.g. <ref> [19] </ref>). As the instantaneous information gain is of interest in many areas, including computational learning theory [20, 1], we briefly discuss here how our results may be used to analyze it.
Reference: [20] <author> D. Haussler, M. Kearns, and R. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 84-114, </pages> <year> 1994. </year>
Reference-contexts: bounds on the average instantaneous regret for the encoding of the tth observation as a function of t, or equivalently the average additional information gained about the true state of nature from the tth observation, which is related to loss measures more commonly examined in computational learning theory (see e.g. <ref> [20, 1] </ref>). Further applications of the results given here to this and related problems are described in [29, 23]. 2 Basic Definitions and Main Result We use the following notational conventions. <p> As the instantaneous information gain is of interest in many areas, including computational learning theory <ref> [20, 1] </ref>, we briefly discuss here how our results may be used to analyze it. Lemma 6 Let a 1 ; a 2 ; : : : be a sequence of real numbers such that a = lim n!1 a n ln n exists (possibly being infi nite).
Reference: [21] <author> D. Haussler, M. Kearns, H. S. Seung, and N. Tishby. </author> <title> Rigorous learning curve bounds from statistical mechanics. </title> <booktitle> In Proceedings of the Seventh Annual ACM Workshop on Computational Learning Theory, </booktitle> <year> 1994. </year>
Reference-contexts: It would be interesting to see what general scaling laws can be established in the infinite dimensional case using Theorem 1. The possibility of using Theorem 1 to obtain bounds in the thermodynamic limit of large sample size and large dimension (see e.g. <ref> [21] </ref>) should also be explored. Finally, in practice it is important to get accurate estimates of I (fi; Y n ) for moderate sample size n. We believe that the bounds given in Theorem 1 will be quite close in most practical cases, but this remains to be verified.
Reference: [22] <author> D. Haussler, J. Kivinen, and M. Warmuth. </author> <title> Tight worst-case loss bounds for predicting with expert advice. </title> <type> Technical Report UCSC-CRL-94-36, </type> <institution> University of California at Santa Cruz, Computer and Information Sciences, </institution> <year> 1994. </year>
Reference: [23] <author> D. Haussler and M. Opper. </author> <title> Mutual information and Bayes methods for learning a distribution. </title> <booktitle> In Proc. Workshop on the Theory of Neural Networks: The Statistical Mechanics Perspective. World Scientific, </booktitle> <year> 1995. </year> <note> to appear. </note>
Reference-contexts: Further applications of the results given here to this and related problems are described in <ref> [29, 23] </ref>. 2 Basic Definitions and Main Result We use the following notational conventions. For a random variable X, P X denotes the distribution function for X, and if X has a density then it is denoted p X .
Reference: [24] <author> I. Ibragimov and R. Hasminskii. </author> <title> On the information in a sample about a parameter. </title> <booktitle> In Second Int. Symp. on Information Theory, </booktitle> <pages> pages 295-309, </pages> <year> 1972. </year>
Reference-contexts: Early work by Ibragimov and Hasminskii showed that I (fi; Y n ) (D=2) log n when Y is real-valued and the conditional distributions on Y are a smooth family of densities indexed by real-valued parameter vectors of dimension D, and certain other conditions apply <ref> [24] </ref>. In this case they were even able to estimate the lower order additive terms in this approximation, which involve the Fisher information and the entropy of the prior. Further related results were given by Efroimovich [14] and Clarke [9].
Reference: [25] <author> T. Kawabata and A. Dembo. </author> <title> The rate-distortion dimension of sets and measures. </title> <journal> IEEE Trans. on Info. Th., </journal> <volume> 40 </volume> <pages> 1564-1572, </pages> <year> 1994. </year>
Reference-contexts: These include definitions of dimension via the metric entropy as introduced by Kolmogorov and Tikhomirov in [26] and commonly used in the theory of empirical processes (see e.g. [13, 31, 16, 8]), as well as Renyi's information dimension <ref> [36, 15, 25] </ref> and the associated Posner-Rodemich *-entropy [32, 33, 34, 35]. <p> When dim K (X) = dim K (X), then this value is denoted dim K (X) and called the metric dimension of X. The information dimension <ref> [36, 15, 25] </ref>, volume-scaling dimension, and Laplace transform dimension are defined analogously by substituting H *; , V *; and L *; respectively for K * in the above definition, and are denoted dim H; (X), dim V; (X) and dim L; (X), respectively.
Reference: [26] <author> A. N. Kolmogorov and V. M. Tihomirov. </author> <title> *-entropy and *-capacity of sets in functional spaces. </title> <journal> Amer. Math. Soc. Translations (Ser. </journal> <volume> 2), 17 </volume> <pages> 277-364, </pages> <year> 1961. </year>
Reference-contexts: In Section 4 we obtain new bounds on the mutual information and minimax risk in terms of various abstract notions of the dimension and capacity of the parameter space fi. These include definitions of dimension via the metric entropy as introduced by Kolmogorov and Tikhomirov in <ref> [26] </ref> and commonly used in the theory of empirical processes (see e.g. [13, 31, 16, 8]), as well as Renyi's information dimension [36, 15, 25] and the associated Posner-Rodemich *-entropy [32, 33, 34, 35]. <p> For the following definitions, let (X; ) be a complete separable metric space and be a measure on X defined on the Borel subsets of X. Definition 1 (Metric entropy, also called Kolmogorov *-entropy <ref> [26] </ref>) A partition of X is a collection f i g of Borel subsets of X that are pairwise disjoint and whose union is X. The diameter of a set A X is given by diam (A) = sup x;y2A (x; y). <p> We call H * (X) the Posner-Rodemich *-capacity of X. The volume-scaling and Laplace transform capacities V * (X) and L * (X) are defined analogously. For each of these entropies and capacities, there is a corresponding notion of dimension. Definition 6 The upper and lower metric dimensions <ref> [26] </ref> are defined by dim K (X) = lim sup K * (X) and *!0 ln * respectively. When dim K (X) = dim K (X), then this value is denoted dim K (X) and called the metric dimension of X. <p> Let M * (X) denote the cardinality of the largest finite *-separated subset of X, or 1 if arbitrarily large *-separated subsets exist. It is readily verified that M * (X) D 2* (X) (see e.g. <ref> [26] </ref>). Now let S be an *-separated subset of X of maximal cardinality M * (X). If S is finite, then let be the uniform distribution on S. Then V *; (X) = ln M * (X) ln D 2* (X) = K 2* (X).
Reference: [27] <author> L. LeCam. </author> <title> Asymptotic methods in statistical decision theory. </title> <publisher> Springer, </publisher> <year> 1986. </year>
Reference-contexts: Metric entropies defined in terms of the relative entropy and Hellinger distances have been used in the context of density estimation by LeCam <ref> [27] </ref>, Birge [6, 7], Has-minskii and Ibragimov [17], and van de Geer [40], the latter with an explicit goal of applying methods from empirical processes to this problem.
Reference: [28] <author> N. Littlestone and M. K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference: [29] <author> M. Opper and D. Haussler. </author> <title> General bounds for predictive errors in supervised learning. </title> <booktitle> In Proc. Workshop on the Theory of Neural Networks: The Statistical Mechanics Perspective. World Scientific, </booktitle> <year> 1995. </year> <note> to appear. </note>
Reference-contexts: Further applications of the results given here to this and related problems are described in <ref> [29, 23] </ref>. 2 Basic Definitions and Main Result We use the following notational conventions. For a random variable X, P X denotes the distribution function for X, and if X has a density then it is denoted p X .
Reference: [30] <author> M. S. Pinsker. </author> <title> Information and Information Stability of Random Variables and Processes (Transl.). </title> <type> Holden Day, </type> <year> 1964. </year>
Reference-contexts: In the case that fi is finite, results of Renyi [37] show further that the difference I (fi; Y n ) H (fi) converges to zero exponentially fast in n. More general results, including the above corollary, follow from results in Pinsker's book <ref> [30] </ref> (see also [2]). For uncountable fi, I (fi; Y n ) is typically unbounded as n grows. To illustrate Theorem 1 in this case, we calculate the bounds given there for a simple Gaussian case, where an exact formula for the mutual information can be obtained.
Reference: [31] <author> D. Pollard. </author> <title> Empirical Processes: Theory and Applications, </title> <booktitle> volume 2 of NSF-CBMS Regional Conference Series in Probability and Statistics. </booktitle> <institution> Institute of Math. Stat. and Am. Stat. Assoc., </institution> <year> 1990. </year>
Reference-contexts: These include definitions of dimension via the metric entropy as introduced by Kolmogorov and Tikhomirov in [26] and commonly used in the theory of empirical processes (see e.g. <ref> [13, 31, 16, 8] </ref>), as well as Renyi's information dimension [36, 15, 25] and the associated Posner-Rodemich *-entropy [32, 33, 34, 35].
Reference: [32] <author> E. Posner, E. Rodemich, and H. Rumsey. </author> <title> Epsilon-entropy of stochastic processes. </title> <journal> Ann. Math. Statist., </journal> <volume> 38 </volume> <pages> 1000-1020, </pages> <year> 1967. </year>
Reference-contexts: These include definitions of dimension via the metric entropy as introduced by Kolmogorov and Tikhomirov in [26] and commonly used in the theory of empirical processes (see e.g. [13, 31, 16, 8]), as well as Renyi's information dimension [36, 15, 25] and the associated Posner-Rodemich *-entropy <ref> [32, 33, 34, 35] </ref>.
Reference: [33] <author> E. Posner, E. Rodemich, and H. Rumsey. </author> <title> Epsilon-entropy of gaussian processes. </title> <journal> Ann. Math. Statist., </journal> <volume> 40 </volume> <pages> 1272-1296, </pages> <year> 1969. </year>
Reference-contexts: These include definitions of dimension via the metric entropy as introduced by Kolmogorov and Tikhomirov in [26] and commonly used in the theory of empirical processes (see e.g. [13, 31, 16, 8]), as well as Renyi's information dimension [36, 15, 25] and the associated Posner-Rodemich *-entropy <ref> [32, 33, 34, 35] </ref>.
Reference: [34] <author> E. Posner and R. Rodemich. </author> <title> Epsilon-entropy and data compression. </title> <journal> Ann. Math. Statist., </journal> <volume> 42 </volume> <pages> 2079-2125, </pages> <year> 1971. </year>
Reference-contexts: These include definitions of dimension via the metric entropy as introduced by Kolmogorov and Tikhomirov in [26] and commonly used in the theory of empirical processes (see e.g. [13, 31, 16, 8]), as well as Renyi's information dimension [36, 15, 25] and the associated Posner-Rodemich *-entropy <ref> [32, 33, 34, 35] </ref>. <p> Finally, the last inequality follows from the fact that the entropy of a partition is always at most the logarithm of the cardinality of the partition. In contrast to part (1), the inequality in part (2) is a quite nontrivial result. The proof is given in <ref> [34] </ref> (see also [35], inequality (9.7).) Part (3) is verified as follows.
Reference: [35] <author> E. Posner and R. Rodemich. </author> <title> Epsilon-entropy of proability distributions. </title> <editor> In L. C. et.al., editor, </editor> <title> Sixth Berkeley Sym. </title> <journal> on Math., Stat. and Prob., </journal> <volume> volume 2, </volume> <pages> pages 699-707. </pages> <year> 1972. </year>
Reference-contexts: These include definitions of dimension via the metric entropy as introduced by Kolmogorov and Tikhomirov in [26] and commonly used in the theory of empirical processes (see e.g. [13, 31, 16, 8]), as well as Renyi's information dimension [36, 15, 25] and the associated Posner-Rodemich *-entropy <ref> [32, 33, 34, 35] </ref>. <p> The metric entropy of (X; ) is defined by K * (X) = K * (X; ) = ln D * (X; ): (The metric is omitted when understood from the context here and below.) Definition 2 (Posner-Rodemich *-entropy <ref> [35] </ref>) Let = f i g be a countable partition of X. The entropy of is defined as H () = P i ( i ) ln ( i ). Let P * denote the set of all countable partitions of X of diameter at most *. <p> Finally, the last inequality follows from the fact that the entropy of a partition is always at most the logarithm of the cardinality of the partition. In contrast to part (1), the inequality in part (2) is a quite nontrivial result. The proof is given in [34] (see also <ref> [35] </ref>, inequality (9.7).) Part (3) is verified as follows.
Reference: [36] <author> A. Renyi. </author> <title> On the dimension and entropy of probability distributions. </title> <journal> Acta Math. Acad. Sci. Hung., </journal> <volume> 10 </volume> <pages> 193-215, </pages> <year> 1959. </year>
Reference-contexts: These include definitions of dimension via the metric entropy as introduced by Kolmogorov and Tikhomirov in [26] and commonly used in the theory of empirical processes (see e.g. [13, 31, 16, 8]), as well as Renyi's information dimension <ref> [36, 15, 25] </ref> and the associated Posner-Rodemich *-entropy [32, 33, 34, 35]. <p> When dim K (X) = dim K (X), then this value is denoted dim K (X) and called the metric dimension of X. The information dimension <ref> [36, 15, 25] </ref>, volume-scaling dimension, and Laplace transform dimension are defined analogously by substituting H *; , V *; and L *; respectively for K * in the above definition, and are denoted dim H; (X), dim V; (X) and dim L; (X), respectively.
Reference: [37] <author> A. Renyi. </author> <title> On the amount of information concerning an unknown parameter in a sequence of observations. </title> <journal> Publ. Math. Inst. Hungar. Acad. Sci., </journal> <volume> 9 </volume> <pages> 617-625, </pages> <year> 1964. </year>
Reference-contexts: In the case that fi is finite, results of Renyi <ref> [37] </ref> show further that the difference I (fi; Y n ) H (fi) converges to zero exponentially fast in n. More general results, including the above corollary, follow from results in Pinsker's book [30] (see also [2]).
Reference: [38] <author> J. Rissanen. </author> <title> Stochastic complexity and modeling. </title> <journal> The Annals of Statistics, </journal> <volume> 14(3) </volume> <pages> 1080-1100, </pages> <year> 1986. </year>
Reference-contexts: These results were extended to the mutual information and minimax risk in [11]. Related lower bounds, which are often quoted, were obtained by Ris-sanen <ref> [38] </ref>, based on certain asymptotic normality assumptions. In this paper we extend work from [19, 4], obtaining bounds on I (fi; Y n ) in more general settings.
Reference: [39] <author> K. Symanzik. </author> <title> Proof and refinements of an inequality of feynman. </title> <journal> J.Math. Phys., </journal> <volume> 6:1155-, </volume> <year> 1965. </year>
Reference-contexts: To the best of our knowledge, the lower bound is new. The proof is given in a series of lemmas and calculations. We begin with the upper bound. This requires the following lemma which has been previously utilized in the framework of Statistical Physics <ref> [39] </ref>.
Reference: [40] <author> S. van deGeer. </author> <title> Hellinger-consistency of certain non-parametric maximum likelihood estimators. </title> <journal> Annals of Statistics, </journal> <volume> 21 </volume> <pages> 14-44, </pages> <year> 1993. </year>
Reference-contexts: Metric entropies defined in terms of the relative entropy and Hellinger distances have been used in the context of density estimation by LeCam [27], Birge [6, 7], Has-minskii and Ibragimov [17], and van de Geer <ref> [40] </ref>, the latter with an explicit goal of applying methods from empirical processes to this problem.
Reference: [41] <author> K. Yamanishi. </author> <title> A learning criterion for stochastic rules. </title> <booktitle> Machine Learning, 1992. Special Issue on the Proceedings of the 3nd Workshop on Computational Learning Theory. </booktitle>
References-found: 41

