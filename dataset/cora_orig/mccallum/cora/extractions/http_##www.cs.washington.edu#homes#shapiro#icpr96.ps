URL: http://www.cs.washington.edu/homes/shapiro/icpr96.ps
Refering-URL: http://www.cs.washington.edu/homes/shapiro/papers.html
Root-URL: 
Email: e-mail: bharath@george.ee.washington.edu, shapiro@cs.washington.edu  
Title: 3D Matching using Statistically Significant Groupings  
Author: Bharath Modayur and Linda Shapiro 
Keyword: Object recognition bounded error uncertainty regions Vision programming.  
Address: Seattle WA 98195 U.S.A.  
Affiliation: Intelligent Systems Laboratory Electrical Engineering Department, FT-10 University of Washington  
Abstract: Vision programming is defined as the task of constructing explicit object models to be used in object recognition. These object models specify the features to be used in recognizing the object as well as the exact order in which they have to be used. For 3D recognition, in the absence of grouping information, the number of bases (model feature/image feature correspondences) that must be examined before a match is found is prohibitively large. By exploiting the relationships between features, we can avoid having to consider a potentially large number of bases. The automatic programming framework [5] helps us in ordering model features based on their utilities such as detectability and error rate that are derived from training data. Examining model features in the order specified by this framework leads to minimal numbers of bases being considered before a match is found. In this article, we describe a vision programming approach to matching 3D models to 2D images. Our system considers feature clusters instead of individual features and dynamically orders unmatched feature clusters based on the existing state of the match. The dynamic feature cluster ordering is achieved through the use of a new dynamic cost function. The automatic vision programming framework is general enough to be used by any feature-based recognition system, and in this article, it is shown to lead to dramatic improvements in the performance of a correspondence-based object recognition system [14]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T.D. Alter and D.W. Jacobs. </author> <title> Error propagation in full 3d-from-2d object recognition. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 889-892, </pages> <year> 1994. </year>
Reference-contexts: In the current work, the seed basis b seed and the supporting bases b support are not generated by the matching engine but by the ordering engine as shown in to determine a 3D affine transformation. The projection error regions can be determined using the analytical method outlined in <ref> [1] </ref> or the numerical method reported in [12]. For our experiments, we used the numerical method because of availability at that time. The PERFORM method proceeds by intersecting these projection error regions as with the 2D-2D case described in our earlier work [13], [15].
Reference: [2] <author> R.C. Bolles and R.A. Cain. </author> <title> Recognizing and locating partially visible objects: The local-feature focus method. </title> <journal> Int. J. Robotics Res., </journal> <volume> 1, no.3:57-82, </volume> <year> 1982. </year>
Reference-contexts: The perceptual groupings described in [11] are also model-independent and thus the system does not have to be re-tuned every time a new object is introduced into the model database. The local feature focus methodology proposed by Bolles and Cain <ref> [2] </ref> and used again in the 3DPO system of Bolles and Horaud [3] uses key features to form an initial hypothesis and start the search. Additional features that are consistent with this hypothesis are then added. Maximal sets of mutually consistent hypotheses are then examined further for a match.
Reference: [3] <author> R.C. Bolles and P. Horaud. 3dpo: </author> <title> A three-dimensional part orientation system. </title> <journal> Int. J. Robotics Res., </journal> <volume> 5, 3 </volume> <pages> 3-26, </pages> <year> 1986. </year>
Reference-contexts: The local feature focus methodology proposed by Bolles and Cain [2] and used again in the 3DPO system of Bolles and Horaud <ref> [3] </ref> uses key features to form an initial hypothesis and start the search. Additional features that are consistent with this hypothesis are then added. Maximal sets of mutually consistent hypotheses are then examined further for a match.
Reference: [4] <author> O. Camps, L.G. Shapiro, and R.M. Haralick. Premio: </author> <title> An overview. </title> <booktitle> In IEEE Workshop on Directions in Automated CAD Based Vision, </booktitle> <pages> pages 11-21, </pages> <year> 1991. </year>
Reference-contexts: We derive analytic expressions for a dynamic cost function that is used to reorder feature clusters based on the existing state of a match. There are different ways by which the utility of an object feature can be estimated. The PREMIO system described in <ref> [4] </ref> models the characteristics of the object model and the environment (lighting, sensor, etc.) and 3 uses analytic methods to estimate the appearance of features in an image. The approach taken by [5] is to use real images to determine empirical estimates of feature utility measures.
Reference: [5] <author> C.H. Chen and P.G. Mulgaonkar. </author> <title> Automatic vision programming. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 55 </volume> <pages> 170-183, </pages> <year> 1992. </year>
Reference-contexts: These object models specify the features to be used in recognizing the object as well as the exact order in which they have to be used. The work of Chen and Mulgaonkar reported in <ref> [5] </ref> describes an elegant framework for automatic vision programming. They define a set of feature utility measures and derive rigorous techniques for using those measures to minimize an objective cost function for recognition of objects. <p> The dynamic feature cluster ordering is achieved through the use of a dynamic cost function defined in this work. 2 SRI Automatic Programming An Overview In this section, we briefly review the automatic programming methodology of <ref> [5] </ref> henceforth referred to as the AP methodology. The main idea behind the AP methodology is to order model features in such a way as to optimize an objective cost function. <p> Since matching done in the order prescribed by the methodology minimizes a well-defined cost function, the matching process is deemed optimal. We start with a brief overview of the feature utility measures and the cost function used in <ref> [5] </ref>. <p> An optimal vision program will then be the one that achieves the maximum number of successful recognitions in the minimum amount of time. The cost of matching the model with f as the seed feature was shown in <ref> [5] </ref> to be equal to C (f ) = D (f )(1 q u ) E (f ) fl (T f + (h 1) fl T c ) (1) where E (f ) is the error rate of feature f ; D (f ) is the detectability; q u is the <p> The PREMIO system described in [4] models the characteristics of the object model and the environment (lighting, sensor, etc.) and 3 uses analytic methods to estimate the appearance of features in an image. The approach taken by <ref> [5] </ref> is to use real images to determine empirical estimates of feature utility measures. They also used CAD tools to synthesize images and run image processing algorithms on the synthesized images to estimate feature utility measures. <p> All possible n-clusters are considered for ordering. In practice n is limited to 2 or 3. The ordering of feature clusters is done in such a way as to minimize a cost function. Our definitions of cost function and feature utilities are similar to those used in <ref> [5] </ref>. As the process of matching model features to image features proceeds, the cost function is re-evaluated dynamically and the feature clusters are reordered. Since matching done this way minimizes a well-defined cost function, the matching process is optimal. <p> P N is the probability of a correct instance of C being rejected. The success rate using model cluster C given by (1 P N )D (C) is also the probability that a match will be found using cluster C. The cost function as defined in <ref> [5] </ref> is the ratio of expected execution time to success rate. Thus, we have (C) = (1 P N )D (C) where (C) is the cost function for feature cluster C. <p> D (C) = P (C@ C (x)jM @x) where C is a pose function <ref> [5] </ref> that produces the location/orientation of a feature cluster C, given the location/orientation of the model M . The @ symbol is used to denote occurrence of a model or a feature cluster at a given location. <p> The model cluster C iter is removed from the unmatched model cluster set U . The iteration number iter is incremented. 6. The reliability evaluator computes the reliability <ref> [5] </ref> of the current hypothesis H. If the reliability is below the acceptance threshold, the seed cluster correspondence which started the matching process is assumed to be erroneous and the matching procedure is initiated with a new seed cluster correspondence. 7. <p> At each step, the matching engine outputs the current hypothesis, and the reliability evaluator module determines the reliability of the current hypothesis. 10 The reliability of a hypothesis is defined in terms of detectabilities and error rates of matched and unmatched features <ref> [5] </ref>. If the reliability falls below the reliability threshold (0.99 in our experiments), the hypothesis is rejected and a new hypothesis is initiated.
Reference: [6] <author> C.H. Chen, P.G. Mulgaonkar, and B. Modayur. </author> <title> Automatic vision programs from predicted features. </title> <booktitle> In SPIE Conference on Intelligent Robots and Computer Vision XI, </booktitle> <pages> pages 374-394. </pages> <address> Boston, MA, </address> <year> 1992. </year>
Reference-contexts: Because of the fact that the CAD tools are not yet capable of modeling objects, lighting and sensor characteristics realistically, the feature utilities estimated from CAD-synthesized images were quite different from those that were derived from real images <ref> [6] </ref>. The approach we take is to estimate feature utility measures from a large set of real images. The process of perceptual organization, which detects viewpoint-invariant groupings of image features, has been used in various forms by researchers.
Reference: [7] <author> M. Costa. </author> <title> Feature extraction for appearance-based object recognition. </title> <type> Technical Report 94-12-01, </type> <month> Dec </month> <year> 1994. </year>
Reference-contexts: The training data was obtained using 17 images of a single view class of CURV. Line segments were extracted from the grayscale images using the Burns line finder as before. Ellipses were detected using the method reported in <ref> [7] </ref>. The error rates and detectabilities, determined empirically from the training images, were used to evaluate the cost function as defined in Equation 2. The model feature clusters were then ordered in terms of increasing cost function.
Reference: [8] <author> M.S. Costa and L.G. Shapiro. </author> <title> Scene analysis using appearance-based models and relational indexing. </title> <booktitle> In International symposium on computer vision. Coral Gables, </booktitle> <address> Florida, </address> <month> Nov </month> <year> 1995. </year>
Reference-contexts: The use of elliptical "appearance-based features" improves the performance of the matching algorithm as the error rates of such features are very low. The elliptical features used in our work also had higher detectabilities than line features. The results reported in <ref> [8] </ref> seem to support this trend of higher detectability and lower error rates for many appearance-based features. (i) (ii) (iii) (iv) 6.4 Dynamic Feature Ordering The implicit assumption that static feature ordering relies on is that object feature clusters occur independently of each other. In practice, this is not so.
Reference: [9] <author> J.G. Harris and A.M. Flynn. </author> <title> Object recognition using the connection machine's router. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 134-139, </pages> <year> 1986. </year>
Reference: [10] <author> D. P. Huttenlocher and S. Ullman. </author> <title> Recognizing solid objects by alignment with an image. </title> <journal> International Journal of Computer Vision, </journal> <volume> 5(2) </volume> <pages> 195-212, </pages> <year> 1990. </year>
Reference-contexts: The PERFORM method is based on the alignment principle. That is, a minimal number of model-to-image feature correspondences are used to construct a transformation, which is then used to project the model back onto the image. Additional support is then sought for this transformation. As opposed to <ref> [10] </ref>, our method has an explicit noise model. Incorporation of the noise model allows for model features to be projected over an uncertainty region (called a Projection Error Region or PER). The matching method works by intersecting these projection error regions in image space.
Reference: [11] <author> D.G. Lowe. </author> <title> Three-dimensional object recognition from single two-dimensional images. </title> <journal> Artif. Intell., </journal> <volume> 31 </volume> <pages> 355-395, </pages> <year> 1987. </year>
Reference-contexts: The automatic programming approach differs significantly from the grouping methodologies described in <ref> [11] </ref> by the way in which useful features for matching are selected. Features to be used in matching and their ordering are determined automatically from experimental training data and are not based on the algorithm designer's expertise. <p> The approach we take is to estimate feature utility measures from a large set of real images. The process of perceptual organization, which detects viewpoint-invariant groupings of image features, has been used in various forms by researchers. The seminal work of Lowe <ref> [11] </ref> utilizes the non-accidental, perceptual groupings of linear image features to form hypotheses about object identity and location. Examples of such groupings that are invariant over wide ranges of viewpoints include collinearity, proximity, parallelism, texture properties, and certain symmetries. <p> Examples of such groupings that are invariant over wide ranges of viewpoints include collinearity, proximity, parallelism, texture properties, and certain symmetries. The main motivation behind using perceptual grouping is to cut down the combinatorics of matching by exploring significant hypotheses first. The perceptual groupings described in <ref> [11] </ref> are also model-independent and thus the system does not have to be re-tuned every time a new object is introduced into the model database.
Reference: [12] <author> B. Modayur. </author> <title> Projection error regions under 3d affine transformation. Internal Report ISL-01-07-1994, </title> <journal> July 1994. </journal> <volume> 14 Image # Bases examined Supporting features - Static Supporting features - Dynamic 1 10 70 56 3 39 291 209 5 29 391 160 7 49 448 263 9 65 647 346 11 39 438 212 13 52 465 291 15 49 488 265 17 38 343 214 19 78 671 409 21 75 743 389 features examined for static and dynamic feature orderings. </volume> <booktitle> The number of bases examined remains the same in both cases. </booktitle>
Reference-contexts: The projection error regions can be determined using the analytical method outlined in [1] or the numerical method reported in <ref> [12] </ref>. For our experiments, we used the numerical method because of availability at that time. The PERFORM method proceeds by intersecting these projection error regions as with the 2D-2D case described in our earlier work [13], [15].
Reference: [13] <author> B. Modayur and L. G. Shapiro. </author> <title> Fast parallel object recognition. </title> <booktitle> In International Conference on Pattern Recognition, volume D, </booktitle> <pages> pages 284-289. </pages> <address> Jerusalem, Israel, </address> <year> 1994. </year>
Reference-contexts: Features to be used in matching and their ordering are determined automatically from experimental training data and are not based on the algorithm designer's expertise. The PERFORM matching algorithm was originally developed for use on massively parallel machines <ref> [13] </ref>, [15]. The PERFORM method is based on the alignment principle. That is, a minimal number of model-to-image feature correspondences are used to construct a transformation, which is then used to project the model back onto the image. Additional support is then sought for this transformation. <p> For our experiments, we used the numerical method because of availability at that time. The PERFORM method proceeds by intersecting these projection error regions as with the 2D-2D case described in our earlier work <ref> [13] </ref>, [15]. After an initial hypothesis is generated, the ordering engine provides the matching engine supporting model clusters that can be used to either accept or reject the initial hypothesis. The image grouper provides supporting image clusters that are compatible with the supporting model clusters.
Reference: [14] <author> B. Modayur and L.G. Shapiro. </author> <title> Perform: A fast object recognition method using intersection of projection error regions. </title> <journal> Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <month> Jan </month> <year> 1995. </year>
Reference: [15] <author> B. R. Modayur and L. G. Shapiro. </author> <title> Parallel algorithm for object recognition and its implementation on a mimd machine. </title> <booktitle> In CAMP '95 Computer Architectures for Machine Perception, </booktitle> <pages> pages 313-22. </pages> <address> Como, Italy, </address> <year> 1995. </year>
Reference-contexts: Features to be used in matching and their ordering are determined automatically from experimental training data and are not based on the algorithm designer's expertise. The PERFORM matching algorithm was originally developed for use on massively parallel machines [13], <ref> [15] </ref>. The PERFORM method is based on the alignment principle. That is, a minimal number of model-to-image feature correspondences are used to construct a transformation, which is then used to project the model back onto the image. Additional support is then sought for this transformation. <p> For our experiments, we used the numerical method because of availability at that time. The PERFORM method proceeds by intersecting these projection error regions as with the 2D-2D case described in our earlier work [13], <ref> [15] </ref>. After an initial hypothesis is generated, the ordering engine provides the matching engine supporting model clusters that can be used to either accept or reject the initial hypothesis. The image grouper provides supporting image clusters that are compatible with the supporting model clusters.
Reference: [16] <author> K. Pulli. Tribors: </author> <title> A triplet-based object recognition system. </title> <type> Technical Report 95-01-01, </type> <month> Jan </month> <year> 1995. </year>
Reference-contexts: The model cluster statistics thus determined are used by the image grouper during the online recognition phase to select image clusters <ref> [16] </ref>. 6.2 Testing Phase The feature cost functions that are determined empirically are used to order the model clusters o*ine. During the online recognition phase, the ordering engine illustrated in Figure 1 initially provides the image grouper a seed model cluster. <p> The model 12 cluster statistics thus determined are used by the image grouper during the online recognition phase to select image clusters <ref> [16] </ref>. Four of the test images are shown in Figure 7. The number of bases examined before a match was found was 14, 4, 3, and 8 respectively for the four test images shown.
Reference: [17] <author> I. Rigoutsos and R. Hummel. </author> <title> Massively parallel model matching geometric hashing on the connection machine. </title> <journal> IEEE Computer, </journal> <volume> February:33-42, </volume> <year> 1992. </year>
Reference: [18] <author> L.G. Shapiro and M.S. Costa. </author> <title> Appearance Based 3D Object Recognition. </title> <booktitle> In NSF/ARPA Workshop on 3D Object Representation. </booktitle> <address> New York, </address> <month> December </month> <year> 1994. </year> <month> 15 </month>
Reference-contexts: For the three-dimensional objects used in this work, we acquire images of the same view class. A separate investigation <ref> [18] </ref> concentrates on determining the best candidate view classes using appearance-based features. Our work uses line segment features for 3D polyhedral objects. For non-polyhedral objects, the appearance-based features reported in [18] are used. 5.1 Estimation of Detectability The marginal and joint detectabilities of features and feature clusters are estimated empirically for <p> For the three-dimensional objects used in this work, we acquire images of the same view class. A separate investigation <ref> [18] </ref> concentrates on determining the best candidate view classes using appearance-based features. Our work uses line segment features for 3D polyhedral objects. For non-polyhedral objects, the appearance-based features reported in [18] are used. 5.1 Estimation of Detectability The marginal and joint detectabilities of features and feature clusters are estimated empirically for each set of training images.
References-found: 18

