URL: http://polaris.cs.uiuc.edu/reports/1288.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: AUTOMATIC PARALLELIZATION OF PROLOG PROGRAMS  
Author: BY DAVID CHRISTOPHER SEHR B.S., Butler 
Degree: THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate College of the  
Date: 1988  
Address: 1985 M.S., University of Illinois,  1992 Urbana, Illinois  
Affiliation: University,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <month> March </month> <year> 1986. </year>
Reference-contexts: multiples on one path by renaming and then apply call merger to produce one recursive call site. 5.2 NATURAL LOOPS In order to determine which loops can be transformed into structures similar to a Fortran do loop we begin by identifying a class of strongly connected components called natural loops <ref> [1] </ref>. Each back edge (n; d) in the control flow graph (either a failure edge or a recursive edge) induces a natural loop which consists of those nodes that can reach n without going through d.
Reference: [2] <author> Hassan Ait-Kaci. </author> <title> The WAM: A (real) tutorial. </title> <type> Technical report, </type> <institution> Digital Equipment Corporation, Paris Research Laboratory, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: of programs that do a large amount of program manipulation, however, so it does not seem very profitable to spend a great deal of effort in optimizing program manipulation predicates. 17 1.5 THE WARREN ABSTRACT MACHINE The predominant implementation for Prolog systems is the Warren Abstract Machine or WAM , <ref> [96, 2] </ref>. The WAM is a byte-coded interpreter for a compiled form of Prolog, and was the first efficient method for compiling Prolog for sequential machines. It has since been extended to a variety of parallel execution models [48, 97, 42, 49, 39]. <p> In this section we will present a brief description of the WAM, because it is the model used for the inherent parallelism measurement methods described in Chapter Three. For a much fuller description of the WAM, the reader is referred to the excellent tutorial by Ait-Kaci <ref> [2] </ref>. 1.5.1 WAM ARCHITECTURE system data areas. The argument registers or A registers hold the calling arguments to a function. The number of these registers is implementation specific, but the number 255 seems fairly common.
Reference: [3] <author> Khayri Ali. </author> <title> The muse or-parallel prolog model and its performance. </title> <booktitle> In Proceedings of the 1990 North American Logic Programming Conference, </booktitle> <pages> pages 757-776, </pages> <year> 1990. </year>
Reference-contexts: A number of parallel implementations have been built based on the idea that a system needs to handle the full Prolog language, including the side effect predicates. OR parallel systems were the first to incorporate the full set of Prolog builtins <ref> [74, 3, 87] </ref>. OR parallelism arises from problems that have several alternative candidate solutions, and hence OR parallel execution is able to efficiently extract a great deal of parallelism from problems that are oriented toward combinatorial search. <p> In Chapter Two we present the results obtained by an OR parallel 1 implementation we have developed that incorporates the full set of Prolog side effect predicates. This work and the Aurora [90] and Muse <ref> [3] </ref> systems have shown that an OR parallel system can extract a significant amount of parallelism from Prolog programs, even those containing side effect predicates. <p> This is particularly true of OR parallel systems, where the highest performance implementations are WAM-based <ref> [74, 3] </ref>. There are also a number of extended WAMs [48, 47] in use for extracting AND parallelism. <p> OR parallelism arises from independent traversal of the branches of the OR tree for a program. For programs without side effects OR parallelism is relatively easy to exploit without compile-time analysis <ref> [74, 3] </ref>. With side effects the problem can still be solved without significant static analysis [87], but more efficient solutions may be obtained through static analysis [44]. AND parallelism arises from independent executions of literals within a clause body. <p> Where dependences are inferred between literals, the sequential order will be that which is enforced. 3.2 SEQUENTIAL AND OR TIME The most efficient OR parallel implementations of Prolog to date <ref> [97, 3] </ref> have been based upon the Warren Abstract Machine (WAM) [96]. Because of this, we compute critical path timings in number of WAM instructions executed. The number of instructions is an approximation to execution time, since each type of WAM instruction takes a slightly different time. <p> The model we propose as a suitable execution framework for our transformed programs begins with an OR parallel execution model such as Aurora [74], Muse <ref> [3] </ref>, or the system described in Chapter Two. It could also begin with an AND/OR parallel system such as such as the Reduce-OR Process Model [84], the AND/OR Process Model [26], or Epilog [101].
Reference: [4] <author> Khayri A. M. Ali. </author> <title> A method for implementing cut in parallel execution of prolog. </title> <booktitle> In Proceedings of the 1987 Symposium on Logic Programming. IEEE Computer Society, </booktitle> <year> 1987. </year>
Reference-contexts: First, one group of researchers have proposed new side-effect predicates that have a parallel semantics [74]. The other approach, executing Prolog in parallel while preserving sequential semantics of the side-effect goals, is the one we have taken. Other researchers have also followed this approach. Ali <ref> [4] </ref> presented a method for giving sequential semantics to cut, and Ciepielewski et al. [45] described a method to handle the I/O, cut, and program manipulation predicates. In our previous work [55, 87] we developed a method for handling all of these predicates. <p> Several methods for doing this can be found in [87]. Notice our solution does not require cuts to perform leftmost tests in order to execute. Moreover, it does not block the execution of clauses which come after those containing cuts, as does Ali's method <ref> [4] </ref>. The method proposed by Ciepielewski et al. [45] requires that a cut must be on the leftmost branch within the subtree it prunes in order to be processed. In this case, if branches to the right of the cut are processed, significant speculative parallelism can result.
Reference: [5] <author> F.E. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In R. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers, </booktitle> <pages> pages 1-30. </pages> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: In the previous section we described three transformations for eliminating several sorts of dependences. Two additional transformations that will be useful are distribution <ref> [5] </ref> and statement reordering [78]. Distribution allows us to split a loop with multiple literals in the body (or literals in the body and the left or right bodies), as long as no dependence cycles are broken. <p> In the Prolog framework, statement reordering is also closely related to goal reordering. Many other Fortran transformations should be applicable to Prolog programs, although we have not considered them here. Loop fusion <ref> [5] </ref> may be useful in cases, although it seems difficult in loops with non-determinism. The elegant work of Banerjee [10] on loop reversal, skewing, and interchange is definitely useful for the relatively rare case of nested deterministic loops.
Reference: [6] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and its Application to Program Transformations. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1983. </year> <pages> (UMI 83-14916). </pages>
Reference-contexts: First, two literals are data dependent if they share a variable, occur along the same execution path, and one literal writes the variable. This is the most common sort of dependence, and the one to which the most effort has been devoted both, in procedural languages <ref> [9, 18, 6] </ref> and in Prolog [33]. The second sort of dependence arises from side effect predicates such as input/output, database manipulation, and program manipulation. This sort of dependence occurs less frequently, but has also received a fair amount of attention [36, 55, 87, 44]. <p> We analyze these variables to determine which nodes have dependences between them that cross iterations of the do. With scalar expanded variables and argument cells in functors we can determine the read and write points as above, and then apply Fortran dependence analysis techniques <ref> [9, 18, 6] </ref> to test for dependences. If the test finds that there is a dependence between two nodes in the same iteration, the dependence is said to be loop independent. It is possible that there are dependences that go across iterations of a do loop.
Reference: [7] <author> K.R. Apt and M.H. van Emden. </author> <title> Contributions to the theory of logic programming. </title> <journal> Journal of the ACM, </journal> <volume> 29(3) </volume> <pages> 841-862, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: This sequence of rewriting rules is applied recursively to each predicate upon reading the program into the compiler. 1.3 OPERATIONAL SEMANTICS We refer to the semantics of a Prolog program in this dissertation in terms of the program's SLD tree <ref> [7] </ref>, or OR tree. This is the representation that is most natural for the description of OR parallel execution. Furthermore, the critical path timings of Prolog programs discussed in Chapter Three can be easily defined in terms of paths in the OR tree.
Reference: [8] <author> John Backus. </author> <title> Can programming be liberated from the von neumann style? A functional style and its algebra of programs. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 613-641, </pages> <month> August </month> <year> 1978. </year>
Reference-contexts: The first presents a few techniques used to convert the procedure so that it has a single identified call site. These techniques normalize procedures with non-determinism or multiple recursive call sites. The second section describes finding natural loops in the CFG, which are used to perform recursion splitting <ref> [51, 8] </ref> on non-tail recursive procedures. The next two sections describe the two parts of constructing an iteration 111 space for a do loop, namely identifying induction variables and termination conditions. The fifth section describes the treatment of live variables in recursion splitting and the use of scalar expansion [102].
Reference: [9] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: First, two literals are data dependent if they share a variable, occur along the same execution path, and one literal writes the variable. This is the most common sort of dependence, and the one to which the most effort has been devoted both, in procedural languages <ref> [9, 18, 6] </ref> and in Prolog [33]. The second sort of dependence arises from side effect predicates such as input/output, database manipulation, and program manipulation. This sort of dependence occurs less frequently, but has also received a fair amount of attention [36, 55, 87, 44]. <p> We analyze these variables to determine which nodes have dependences between them that cross iterations of the do. With scalar expanded variables and argument cells in functors we can determine the read and write points as above, and then apply Fortran dependence analysis techniques <ref> [9, 18, 6] </ref> to test for dependences. If the test finds that there is a dependence between two nodes in the same iteration, the dependence is said to be loop independent. It is possible that there are dependences that go across iterations of a do loop. <p> The number of iterations between the source and sink of a dependence is called the dependence distance, and is computed by a number of Fortran techniques <ref> [9] </ref>. Because several of our transformations use dependence distance, we label dependence arcs between nodes within a do with their distance. If the dependence is loop independent, the distance is zero. If the distance is unknown, it is marked as unknown.
Reference: [10] <author> Utpal Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 192-219, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Many other Fortran transformations should be applicable to Prolog programs, although we have not considered them here. Loop fusion [5] may be useful in cases, although it seems difficult in loops with non-determinism. The elegant work of Banerjee <ref> [10] </ref> on loop reversal, skewing, and interchange is definitely useful for the relatively rare case of nested deterministic loops. <p> A third topic for future research in transformations for Prolog is the addition of more transformations to the repertoire. With the addition of multidimension structures, we can use Banerjee's unimodular matrix methods <ref> [10] </ref> to determine optimal loop interchange and skewing. This will allow us to perform efficient vectorization and parallelization as well as enabling the use of compile-time memory management techniques such as blocking, prefetching, and data reuse.
Reference: [11] <author> J. Banning. </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables. </title> <booktitle> In Conference Record of the Sixth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 29-41, </pages> <month> January </month> <year> 1979. </year>
Reference-contexts: This is very conservative, since it produces inexact results in the most commonly occurring cases. Moreover, the likelihood of the procedure being called with the pathological term that necessitates this is very low. Further enhancements in alias analysis such as using <ref> [11, 27, 28, 66, 67] </ref> may yield significant improvements. For each literal l the procedure in Figure 6.2 is used to set up the calling arguments aliases for the procedure.
Reference: [12] <editor> BIM, </editor> <address> 3078 Everberg Belgium. BIM Prolog release 2.4 March 1989. </address>
Reference-contexts: These are byte-coded instructions for the static procedures in the program (those which are not modified by assert and retract). The representation of dynamic procedures is usually a tokenized form of the clauses that is interpreted, although other systems <ref> [12] </ref> compile dynamic procedures into byte-codes. The heap is where run time term instances are constructed. Each time a structure is created as a calling argument by a put structure or set structure the structure is built on the heap, and its subterms are filled in by subsequent instructions. <p> This can in many cases prove useful for the elimination of redundant or needless work, but does incur scheduling complexities, and is often very difficult to compile efficiently <ref> [12] </ref>. Since our expands are performed eagerly, the alteration of the OR tree at run time may cause either additional work to be added (in the case of the assert predicates), or work already done to become useless (when retract removes a clause already used in an expand).
Reference: [13] <author> R.S. Bird. </author> <title> Tabulation techniques for recursive programs. </title> <journal> ACM Computing Surveys, </journal> <volume> 12(4), </volume> <month> December </month> <year> 1980. </year>
Reference-contexts: The transformations we present below only deal with multiple recursions that have one descent function, so this situation must be dealt with. It is possible that these multiple descent functions give rise to recursive overcomputation that could be tabulated according to schemes such as <ref> [13, 24] </ref>. As there does not appear to be any real algorithmic method for applying such transformations we have not considered that here. A pattern matching 121 system could be employed here to do such transformations, although we doubt the generality of such a scheme.
Reference: [14] <author> P. Borgwardt. </author> <title> Parallel prolog using stack segments on shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 1984 International Conference on Logic Programming. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1984. </year>
Reference-contexts: That is the fundamental requirement for such dependences. A great deal of effort has gone into the design of binding environments for OR parallel systems <ref> [14, 97, 98, 71, 43, 23] </ref> and AND/OR parallel systems [56, 25]. This work is all aimed at alleviating such dependences, and as such can be seen as more exotic flavors of scalar expansion [102], and variable renaming [31] and localization [103]. <p> Without destructive assignment all output dependences are removed by the OR parallel binding environment. We assume in what follows that the underlying implementation has used one of these OR parallel binding environments (in the interpreter of Chapter Two, it is hash windows <ref> [14] </ref>). Hence we modify the definition of data dependence to remove those that are handled by the environment. <p> Each node conceptually consists of three parts, as shown in Figure 2.1: a sequential 34 35 binding environment such as that given by Bruynooghe [15], a parallel binding environment (for example, Borgwardt's hash windows <ref> [14] </ref>), and a scheduling queue node. The scheduling queue nodes are linked by a pair of pointers. The first, called next, points to the post order successor in the OR tree, and is used by the scheduler to select nodes for processing. <p> The execution model for doacross requires special handling of the non-determinism that is possible in the body. This is done by using the OR parallel binding environment that underlies the system, probably either binding arrays [97] or hash windows <ref> [14] </ref>. The spine of the recursion is treated by having each iteration produce its local environment and pass it to the next iteration. The binding environment impact of doacross is shown in Figure 6.16. Each iteration starts by allocating its own local binding environment e i .
Reference: [15] <author> Maurice Bruynooghe. </author> <title> The memory management of prolog implementations. In K.L. </title> <editor> Clark and S.A. Tarnlund, editors, </editor> <booktitle> Logic Programming, </booktitle> <pages> pages 83-98. </pages> <publisher> Academic Press, </publisher> <year> 1982. </year>
Reference-contexts: We represent the computation of a Prolog program by a data structure explicitly maintaining the OR tree. Each node conceptually consists of three parts, as shown in Figure 2.1: a sequential 34 35 binding environment such as that given by Bruynooghe <ref> [15] </ref>, a parallel binding environment (for example, Borgwardt's hash windows [14]), and a scheduling queue node. The scheduling queue nodes are linked by a pair of pointers.
Reference: [16] <author> M. Burke, R. Cytron, J. Ferrante, and W. Hsieh. </author> <title> Automatic generation of nested, fork-join parallelism. </title> <journal> Journal of Supercomputing, </journal> <pages> pages 71-88, </pages> <year> 1989. </year>
Reference-contexts: It is also important to note that the ordering requirements are only within a single relation. If two literals involve two different database relations, there is no ordering necessary between them. Techniques used for clause indexing [19, 38] and data privatization <ref> [16, 70, 93] </ref> may be useful in partitioning the database to reduce the amount of serialization required. 15 1.4.4 PROGRAM MANIPULATION The last class of side effects that are dependent upon Prolog's execution order are the program manipulation predicates.
Reference: [17] <author> Michael Burke and Ronald Cytron. </author> <title> Interprocedural dependence analysis and paralleliza-tion. </title> <booktitle> In Proceedings ofthe 1986 Sigplan Compiler Construction Conference, </booktitle> <pages> pages 162-175, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Moreover, if iteration can be identified and exposed through source transformation, the possibility of using existing parallelizing compiler technology is opened. Several projects have studied the parallelization of Fortran do loops <ref> [62, 59, 17] </ref> and a number of techniques have been developed. The problem that concerns us most is how to transform Prolog programs to express iteration in a similar way to the Fortran do loop.
Reference: [18] <author> Michael Burke and Ronald Cytron. </author> <title> Interprocedural dependence analysis and paralleliza-tion. </title> <booktitle> In Proceedings of the SIGPLAN 86 Symposium on Compiler Construction, </booktitle> <pages> pages 162-175, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: First, two literals are data dependent if they share a variable, occur along the same execution path, and one literal writes the variable. This is the most common sort of dependence, and the one to which the most effort has been devoted both, in procedural languages <ref> [9, 18, 6] </ref> and in Prolog [33]. The second sort of dependence arises from side effect predicates such as input/output, database manipulation, and program manipulation. This sort of dependence occurs less frequently, but has also received a fair amount of attention [36, 55, 87, 44]. <p> We analyze these variables to determine which nodes have dependences between them that cross iterations of the do. With scalar expanded variables and argument cells in functors we can determine the read and write points as above, and then apply Fortran dependence analysis techniques <ref> [9, 18, 6] </ref> to test for dependences. If the test finds that there is a dependence between two nodes in the same iteration, the dependence is said to be loop independent. It is possible that there are dependences that go across iterations of a do loop.
Reference: [19] <author> Mats Carlsson. </author> <title> Freeze, indexing, and other implementation issues in the wam. </title> <booktitle> In Proceedings of the Fourth International Conference on Logic Programming, </booktitle> <pages> page 40, </pages> <year> 1987. </year>
Reference-contexts: It is also important to note that the ordering requirements are only within a single relation. If two literals involve two different database relations, there is no ordering necessary between them. Techniques used for clause indexing <ref> [19, 38] </ref> and data privatization [16, 70, 93] may be useful in partitioning the database to reduce the amount of serialization required. 15 1.4.4 PROGRAM MANIPULATION The last class of side effects that are dependent upon Prolog's execution order are the program manipulation predicates. <p> After applying 88 these transformations the CFGs for the program are ready for transformations that convert the procedures to loop form, as we describe in the next chapter. 4.6.1 INDEXING In most Prolog systems the compiler generates instructions to do run-time clause indexing <ref> [19, 38] </ref>. Indexing is a method to improve backtracking behavior by reducing the number of clauses of a procedure that might be applicable at a particular call site. <p> It should be noted that there may be overlap between these partitions, because a clause with a variable in the index argument position of the head will match any calling argument. This is both the reason for a number of proposed methods for selecting index arguments <ref> [19] </ref> and a bit of clause duplication in our method. We will create a duplicate clause for each index class that will match. This creates code growth, but with relatively sharp mode information this does not appear to be excessive. <p> Indexing dispatches execution based on the run-time type of a term, so we begin by defining the term classes that we use for indexing in Figure 4.2. There are six classes in our system, in contrast to the usual four in the WAM <ref> [19] </ref>. The first four classes, number, atom, nil, and var, contain exactly those terms that are in the corresponding abstract term types.
Reference: [20] <author> J. Chang, A. M. Despain, and D. </author> <title> DeGroot. And-parallelism of logic programs based on a static data dependency analysis. </title> <booktitle> In Proceedings of Compcon 85, </booktitle> <year> 1985. </year> <month> 284 </month>
Reference-contexts: A second form of parallelism, namely AND parallelism, arises from the existence of independent sub-problems all of which must be solved to solve one larger problem. To address this sort of problem there have been several proposals for AND parallel execution of Prolog <ref> [26, 58, 101, 20, 36, 47] </ref>. Most of these systems do not treat the full Prolog language (with notable exceptions [36, 47]). The side effect predicates are the most common part omitted. <p> Some have used static data dependence graphs <ref> [58, 101, 20] </ref>, others partially dynamic dependence graphs [36, 47], and still others totally dynamic dependence graphs [26]. Kale [57] also notes that in rare circumstances it may be advantageous to allow concurrent execution of dependent literals. <p> Another use of the method we propose is to compute exact dependences to test the effectiveness of dependence tests. There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static <ref> [58, 20, 101] </ref> to partly dynamic (conditional) [36, 47] to completely dynamic [26]. Kale [57] notes that in rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [84] supports only independent AND parallelism.
Reference: [21] <author> Si-En Chang and Y. Paul Chiang. </author> <title> Restricted and-parallelism execution model with side effects. </title> <booktitle> In Proceedings of the 1989 North American Conference on Logic Programming, </booktitle> <pages> pages 350-368, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: The literals are those that that must be succeeded through to reach the leaf node. Because these paths may share literals, a safety test must be applied, namely that none of the shared literals cause global side effects. This can be done by a simple flow analysis <ref> [37, 21] </ref>. The sequences of literals are ordered according to their corresponding leaves. Each sequence is made into a clause by itself and placed in the correct lexical position, except for the sequence ending in the recursive call. That sequence replaces the downward part of the recursive clause.
Reference: [22] <author> Shyh-Ching Chen. </author> <title> Speedup of Iterative Programs in Multiprocessing Systems. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> January </month> <year> 1975. </year>
Reference-contexts: These dependence cycles may compute recurrences that can be parallelized by algorithm replacements. A number of such recurrence solutions such as tree height reduction [64], parallel banded and triangular matrix solvers <ref> [22] </ref>, and parallel prefix [61] should be in the repertoire of algorithm replacements that are possible by pattern matching. We believe that a number of recurrences, especially those coming from reductions, should be recognizable by such pattern matching.
Reference: [23] <author> A. Ciepielewski and S. Haridi. </author> <title> A formal model for the or-parallel execution of logic programs. </title> <editor> In R. E. A. Mason, editor, </editor> <booktitle> Information Processing. </booktitle> <publisher> Elsevier, Amsterdam, Holland, </publisher> <year> 1986. </year>
Reference-contexts: That is the fundamental requirement for such dependences. A great deal of effort has gone into the design of binding environments for OR parallel systems <ref> [14, 97, 98, 71, 43, 23] </ref> and AND/OR parallel systems [56, 25]. This work is all aimed at alleviating such dependences, and as such can be seen as more exotic flavors of scalar expansion [102], and variable renaming [31] and localization [103].
Reference: [24] <author> Norman Cohen. </author> <title> Characterization and elimination of redundancy in recursive programs. </title> <booktitle> In Proceedings of the Sixth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 143-157, </pages> <year> 1979. </year>
Reference-contexts: The transformations we present below only deal with multiple recursions that have one descent function, so this situation must be dealt with. It is possible that these multiple descent functions give rise to recursive overcomputation that could be tabulated according to schemes such as <ref> [13, 24] </ref>. As there does not appear to be any real algorithmic method for applying such transformations we have not considered that here. A pattern matching 121 system could be employed here to do such transformations, although we doubt the generality of such a scheme.
Reference: [25] <author> John Conery. </author> <title> Binding environments for parallel logic programs in non-shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1987 Symposium on Logic Programming, </booktitle> <pages> pages 457-467. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1987. </year>
Reference-contexts: That is the fundamental requirement for such dependences. A great deal of effort has gone into the design of binding environments for OR parallel systems [14, 97, 98, 71, 43, 23] and AND/OR parallel systems <ref> [56, 25] </ref>. This work is all aimed at alleviating such dependences, and as such can be seen as more exotic flavors of scalar expansion [102], and variable renaming [31] and localization [103]. The OR tree fragment in Figure 1.13 illustrates the dependences in a simple program.
Reference: [26] <author> J.S. Conery and D.F. Kibler. </author> <title> And parallelism and nondeterminism in logic programs. </title> <journal> New Generation Computing, </journal> <volume> 3 </volume> <pages> 43-70, </pages> <year> 1985. </year>
Reference-contexts: A second form of parallelism, namely AND parallelism, arises from the existence of independent sub-problems all of which must be solved to solve one larger problem. To address this sort of problem there have been several proposals for AND parallel execution of Prolog <ref> [26, 58, 101, 20, 36, 47] </ref>. Most of these systems do not treat the full Prolog language (with notable exceptions [36, 47]). The side effect predicates are the most common part omitted. <p> Some have used static data dependence graphs [58, 101, 20], others partially dynamic dependence graphs [36, 47], and still others totally dynamic dependence graphs <ref> [26] </ref>. Kale [57] also notes that in rare circumstances it may be advantageous to allow concurrent execution of dependent literals. <p> There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static [58, 20, 101] to partly dynamic (conditional) [36, 47] to completely dynamic <ref> [26] </ref>. Kale [57] notes that in rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [84] supports only independent AND parallelism. <p> It could also begin with an AND/OR parallel system such as such as the Reduce-OR Process Model [84], the AND/OR Process Model <ref> [26] </ref>, or Epilog [101]. Any of these models can be extended fairly simply with the necessary structures for executing parallel loops. Because of this the results presented in Chapter Eight give performance numbers for both OR and AND/OR parallel implementations augmented with parallel loops.
Reference: [27] <author> K. Cooper. </author> <title> Analyzing aliases of reference formal parameters. </title> <booktitle> In Conference Record of the Twelfth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 281-290, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: This is very conservative, since it produces inexact results in the most commonly occurring cases. Moreover, the likelihood of the procedure being called with the pathological term that necessitates this is very low. Further enhancements in alias analysis such as using <ref> [11, 27, 28, 66, 67] </ref> may yield significant improvements. For each literal l the procedure in Figure 6.2 is used to set up the calling arguments aliases for the procedure.
Reference: [28] <author> K. Cooper and K. Kennedy. </author> <title> Fast interprocedural alias analysis. </title> <booktitle> In Conference Record of the Sixteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 49-59, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: This is very conservative, since it produces inexact results in the most commonly occurring cases. Moreover, the likelihood of the procedure being called with the pathological term that necessitates this is very low. Further enhancements in alias analysis such as using <ref> [11, 27, 28, 66, 67] </ref> may yield significant improvements. For each literal l the procedure in Figure 6.2 is used to set up the calling arguments aliases for the procedure.
Reference: [29] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimizations in the &lt; n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: Second, any routine 204 which calls this version of q must be changed to produce the cons vector used as input and consume one produced as output. If this cannot be done at all call sites, cloning <ref> [29] </ref> can often be used to allow use of cons vectors. As was said before, the caller does not strictly need to be modified to accept the returned list, since we can simply return Sv [[1]].
Reference: [30] <author> Cray Research, Inc. </author> <title> CF77 Compiling System, Volume 4: Parallel Processing Guide, </title> <month> June </month> <year> 1991. </year> <note> Publication SG-3074 5.0. 285 </note>
Reference-contexts: It is a simple extension to the loop type of the same name developed for Fortran [77, 32]. To synchronize cross iteration dependences requires the definition of three synchronization builtin predicates. Two of these are directly taken from Cray Fortran <ref> [30] </ref>, and provide point to point synchronization of data and side effect dependences. The third is original and is used to enforce side effect dependences. 6.4.1 PARALLELISM MODEL The doacross predicate has six arguments, each of which corresponds to an argument in do.
Reference: [31] <author> R. Cytron and J. Ferrante. </author> <title> What's in a name? or the value of renaming for parallelism detection and storage allocation. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 19-27. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1987. </year>
Reference-contexts: This work is all aimed at alleviating such dependences, and as such can be seen as more exotic flavors of scalar expansion [102], and variable renaming <ref> [31] </ref> and localization [103]. The OR tree fragment in Figure 1.13 illustrates the dependences in a simple program. There is a branch point in the OR tree and both subtrees bind the variable V that is in an environment higher in the tree.
Reference: [32] <author> Ronald Cytron. </author> <type> Ph.D. Thesis. PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1986. </year>
Reference-contexts: In that chapter we extend the timing method of Chapter Three to time the resulting programs. Chapter Six presents the methods for parallelizing the loop-converted programs. There we describe two parallel loop forms based on doacross <ref> [77, 32] </ref> and doall [73] loops, and extend the timing method include these parallel constructs. In Chapter Seven we present an analysis of the effectiveness of the transformations. We do this by presenting several benchmarks before and after transformation, and comparing inherent parallelism using the timing method of Chapter Three. <p> Some do literals contain cross-iteration dependences or have non-determinism, and therefore those do literals cannot be converted to doall literals. The next section describes a second parallel form of do, called doacross <ref> [79, 32] </ref>. This form can deal with cross-iteration dependences and non-determinism. It does restrict the scheduling paradigm that must be used, however. Dependences that occur in dos that are transformed into doacrosses must be explicitly synchronized. <p> In this section we present a new builtin predicate, called doacross, that is a parallel form of a do which can be used to execute loops with cross iteration dependences. It is a simple extension to the loop type of the same name developed for Fortran <ref> [77, 32] </ref>. To synchronize cross iteration dependences requires the definition of three synchronization builtin predicates. Two of these are directly taken from Cray Fortran [30], and provide point to point synchronization of data and side effect dependences.
Reference: [33] <author> Saumya K. Debray. </author> <title> Static inference of modes and data dependencies in logic programs. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Arizona, </institution> <month> August </month> <year> 1987. </year> <type> Report 87-24. </type>
Reference-contexts: Moreover, X and Y are tied together so that when X is given the value t in the next goal of the query, Y is as well. Aliasing information cannot be computed exactly at compile time. One compile time approach <ref> [33] </ref> uses abstract interpretation to infer them approximately. <p> This method should be performed after the mode inference presented in Chapter Four, as binding status is used to prune possible aliases. It should, however, be computed before scalar expansion or do conversion. Our method is a variation on that proposed by Debray <ref> [33] </ref> in that our method uses modes to improve the accuracy of the inference. As with mode inference, we use the set of procedures that are specified by the user as callable from outside the program as a starting point. <p> This is the most common sort of dependence, and the one to which the most effort has been devoted both, in procedural languages [9, 18, 6] and in Prolog <ref> [33] </ref>. The second sort of dependence arises from side effect predicates such as input/output, database manipulation, and program manipulation. This sort of dependence occurs less frequently, but has also received a fair amount of attention [36, 55, 87, 44].
Reference: [34] <author> Saumya K. Debray. </author> <title> Automatic mode inference for prolog programs. </title> <journal> Journal of Logic Programming, </journal> <pages> pages 207-229, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Any other structure that contains no variables is of type g-struct. A structure that contains variables is a u-struct. A variable has type var. 79 the simple lattice used by Debray <ref> [34] </ref>. The extensions allow us to get sharper information about list types, since we will be able to transform a number of list recurrences as well as arithmetic recurrences. There are six types that arise from join computations: atomic, g-list, list, ground, nv, and d.
Reference: [35] <author> Saumya K. Debray. </author> <title> A simple code improvement scheme for prolog. </title> <booktitle> In Proceedings of the Sixth International Conference on Logic Programming, </booktitle> <pages> pages 17-32. </pages> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: One compile time approach [33] uses abstract interpretation to infer them approximately. As abstract interpretation is able to compute other information useful to this work (such as call modes, live variables, lifetimes, etc.), this seems a reasonable approach for aliasing. 1.6.2 CONTROL FLOW GRAPHS Debray <ref> [35] </ref> proposes a method for constructing control flow graphs of Prolog programs. His method allows the use of a number of traditional scalar optimizations such as invariant hoisting, induction variable elimination and common sub-expression elimination. <p> Mode inference and specialization 4. Compile-time clause indexing 5. Optimization using builtin information 6. Dead code elimination 7. User predicate determinism control flow graph, including using several transformations to make it more useful for analysis. The control flow graph is similar to the graphs used by Debray <ref> [35] </ref>. <p> In this chapter we describe in much greater detail the construction of the CFG, and present preliminary transformations that make further analysis of procedures easier. A broad class of optimizations can be applied to the CFG, including the traditional scalar optimizations demonstrated by Debray <ref> [35] </ref>. Additionally, compile-time type, determinacy, and failure inference can easily be defined in terms of the CFG and used to improve programs. We begin this chapter by defining more CFG terminology, and then describe the preliminary transformations used to improve such graphs. described in this chapter. <p> and call graph construction Naive construction Mode inference - Builtin optimization Determinism * Loop conversion Induction variables Iteration space construction Scalar expansion Recursion splitting * Dependence graphs Aliases Side effect analysis Constructing graph * Parallelization Eliminating dependences - Doall - Doacross and synchronization Recurrence recognition 209 hit upon by Debray <ref> [35] </ref>: even sequential optimizations for loops are very useful for Prolog, given the right framework. Next the dependence graph of the program is constructed. What we have presented here is a system using very simple mode and aliasing information, but we are still able to get fairly sharp dependence analysis. <p> To address this opportunity we developed a framework for incorporating procedural language transformations into a Prolog compiler. The first step in this direction was an extension to Debray's <ref> [35] </ref> flow graph notation. This allows us to bring together a number of existing Prolog transformations and to construct a control flow graph that is very much like that referred to in procedural transformations.
Reference: [36] <author> D. </author> <title> DeGroot. Restricted and-parallelism. </title> <booktitle> In Proceedings of the International Conference on Fifth Generation Computer Systems, </booktitle> <pages> pages 471-478. </pages> <publisher> North Holland, </publisher> <year> 1984. </year>
Reference-contexts: A second form of parallelism, namely AND parallelism, arises from the existence of independent sub-problems all of which must be solved to solve one larger problem. To address this sort of problem there have been several proposals for AND parallel execution of Prolog <ref> [26, 58, 101, 20, 36, 47] </ref>. Most of these systems do not treat the full Prolog language (with notable exceptions [36, 47]). The side effect predicates are the most common part omitted. <p> To address this sort of problem there have been several proposals for AND parallel execution of Prolog [26, 58, 101, 20, 36, 47]. Most of these systems do not treat the full Prolog language (with notable exceptions <ref> [36, 47] </ref>). The side effect predicates are the most common part omitted. The failure to provide the side effect predicates that exist in sequential Prolog systems makes it difficult to execute off-the-shelf Prolog applications. <p> Some have used static data dependence graphs [58, 101, 20], others partially dynamic dependence graphs <ref> [36, 47] </ref>, and still others totally dynamic dependence graphs [26]. Kale [57] also notes that in rare circumstances it may be advantageous to allow concurrent execution of dependent literals. <p> There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static [58, 20, 101] to partly dynamic (conditional) <ref> [36, 47] </ref> to completely dynamic [26]. Kale [57] notes that in rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [84] supports only independent AND parallelism. <p> The second sort of dependence arises from side effect predicates such as input/output, database manipulation, and program manipulation. This sort of dependence occurs less frequently, but has also received a fair amount of attention <ref> [36, 55, 87, 44] </ref>. In this section we describe the construction of the dependence graph for a procedure. We begin by describing data dependences and their analysis. This starts by combining mode and alias information to determine read and write points for each variable in a procedure.
Reference: [37] <author> D. </author> <title> DeGroot. Restricted and-parallelism and side effects. </title> <booktitle> In Proceedings of the Fourth International Symposium on Logic Programming, </booktitle> <pages> pages 80-89. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1987. </year>
Reference-contexts: The literals are those that that must be succeeded through to reach the leaf node. Because these paths may share literals, a safety test must be applied, namely that none of the shared literals cause global side effects. This can be done by a simple flow analysis <ref> [37, 21] </ref>. The sequences of literals are ordered according to their corresponding leaves. Each sequence is made into a clause by itself and placed in the correct lexical position, except for the sequence ending in the recursive call. That sequence replaces the downward part of the recursive clause.
Reference: [38] <author> Bart Demoen, Andre Marien, and Alain Callebaut. </author> <title> Indexing prolog clauses. </title> <booktitle> In Proceedings of the 1989 North American Conference on Logic Programming, </booktitle> <pages> pages 1001-1012, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: It is also important to note that the ordering requirements are only within a single relation. If two literals involve two different database relations, there is no ordering necessary between them. Techniques used for clause indexing <ref> [19, 38] </ref> and data privatization [16, 70, 93] may be useful in partitioning the database to reduce the amount of serialization required. 15 1.4.4 PROGRAM MANIPULATION The last class of side effects that are dependent upon Prolog's execution order are the program manipulation predicates. <p> After applying 88 these transformations the CFGs for the program are ready for transformations that convert the procedures to loop form, as we describe in the next chapter. 4.6.1 INDEXING In most Prolog systems the compiler generates instructions to do run-time clause indexing <ref> [19, 38] </ref>. Indexing is a method to improve backtracking behavior by reducing the number of clauses of a procedure that might be applicable at a particular call site.
Reference: [39] <author> Tep Dobry. </author> <title> A High Performance Architecture for Prolog. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1990. </year>
Reference-contexts: The WAM is a byte-coded interpreter for a compiled form of Prolog, and was the first efficient method for compiling Prolog for sequential machines. It has since been extended to a variety of parallel execution models <ref> [48, 97, 42, 49, 39] </ref>. In this section we will present a brief description of the WAM, because it is the model used for the inherent parallelism measurement methods described in Chapter Three.
Reference: [40] <author> Rudolf Eigenmann, Jay Hoeflinger, Zhiyuan Li, and David Padua. </author> <title> Experience in the automatic parallelization of four perfect-benchmark programs. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1991. </year> <month> 286 </month>
Reference-contexts: From now on, we will refer to both such variables and as array variables. Subscripts involving induction variables may require significant effort to analyze, just as their Fortran counterparts have generated a great deal of research <ref> [41, 40] </ref>. The transformations applied so far have scalar expanded any do converted procedure's arguments and the variables that are live across the recursive call, and have split the procedure into a downward and upward loop.
Reference: [41] <author> Mohammad Haghighat. </author> <title> Symbolic Dependence Analysis for High Performance Paralleliz--ing Compilers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1990. </year> <note> CSRD report no. 995. </note>
Reference-contexts: From now on, we will refer to both such variables and as array variables. Subscripts involving induction variables may require significant effort to analyze, just as their Fortran counterparts have generated a great deal of research <ref> [41, 40] </ref>. The transformations applied so far have scalar expanded any do converted procedure's arguments and the variables that are live across the recursive call, and have split the procedure into a downward and upward loop.
Reference: [42] <author> S. Haridi and S. Janson. </author> <title> Kernel andorra prolog and its computation model. </title> <booktitle> In Proceedings of the Seventh International Conference on Logic Programming, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: The WAM is a byte-coded interpreter for a compiled form of Prolog, and was the first efficient method for compiling Prolog for sequential machines. It has since been extended to a variety of parallel execution models <ref> [48, 97, 42, 49, 39] </ref>. In this section we will present a brief description of the WAM, because it is the model used for the inherent parallelism measurement methods described in Chapter Three.
Reference: [43] <author> B. Hausman, A. Ciepielewski, and S. Haridi. </author> <title> Or-parallel prolog made efficient on shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1987 Symposium on Logic Programming, </booktitle> <pages> pages 69-79. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1987. </year>
Reference-contexts: That is the fundamental requirement for such dependences. A great deal of effort has gone into the design of binding environments for OR parallel systems <ref> [14, 97, 98, 71, 43, 23] </ref> and AND/OR parallel systems [56, 25]. This work is all aimed at alleviating such dependences, and as such can be seen as more exotic flavors of scalar expansion [102], and variable renaming [31] and localization [103]. <p> One topic that needs to be addressed in the use of cons vectors is the question of OR parallelism in routines using cons vectors. The method we believe will produce the most success is to use a privileged branch binding environment <ref> [43] </ref> for cons vectors. The privileged branch system will allow one designated branch in the OR tree to bind the cons vector directly, and the others must use a non-local binding environment. This allows maximal efficiency along one branch, while making the others pay a cost for cons vector operations.
Reference: [44] <author> Bogumil Hausman. </author> <title> Pruning and Speculative Work in OR-Parallel PROLOG. </title> <type> PhD thesis, </type> <institution> Swedish Institute of Computer Science, </institution> <month> March </month> <year> 1990. </year> <note> Report No. TRITA-CS-9002. </note>
Reference-contexts: For programs without side effects OR parallelism is relatively easy to exploit without compile-time analysis [74, 3]. With side effects the problem can still be solved without significant static analysis [87], but more efficient solutions may be obtained through static analysis <ref> [44] </ref>. AND parallelism arises from independent executions of literals within a clause body. <p> The second sort of dependence arises from side effect predicates such as input/output, database manipulation, and program manipulation. This sort of dependence occurs less frequently, but has also received a fair amount of attention <ref> [36, 55, 87, 44] </ref>. In this section we describe the construction of the dependence graph for a procedure. We begin by describing data dependences and their analysis. This starts by combining mode and alias information to determine read and write points for each variable in a procedure. <p> This reduces the amount of speculative computation at the expense of possibly increasing the time required to produce all solutions. Using a run-time priority scheme such as that implemented in Rolog [86] may address this problem. Another possibility is extending the work of Hausman <ref> [44] </ref> on compile estimation of speculation. Handling failures in the body of the loop is also different for doacross. <p> This is because of the fact that the underlying OR parallel system produces irregular tree-structured computations that are not easily synchronized by point-to-point synchronization. Because of this, the usual solution <ref> [87, 44] </ref> to OR parallelism and side effects is to allow side effects to execute only when they are at the leftmost unprocessed node in the OR tree. <p> A number of questions surrounding the OR parallel execution of Prolog programs remain for future work. First, what is the compiler's role in improving the performance of programs with side effects? Others <ref> [44] </ref> have looked into determining the amount of speculation that is caused by the presence of cuts. We believe that this work could be integrated into our system to provide more efficient scheduling, especially in the presence of the big cut, small cut problem described in Chapter Two.
Reference: [45] <author> Bogumil Hausman, Andrzej Ciepielewski, and Alan Calderwood. </author> <title> Cut and side-effects in or-parallel prolog. </title> <booktitle> In Proceedings of the International Conference on Fifth Generation Computer Systems 1988, </booktitle> <pages> pages 831-840. </pages> <publisher> ICOT, </publisher> <year> 1988. </year>
Reference-contexts: The other approach, executing Prolog in parallel while preserving sequential semantics of the side-effect goals, is the one we have taken. Other researchers have also followed this approach. Ali [4] presented a method for giving sequential semantics to cut, and Ciepielewski et al. <ref> [45] </ref> described a method to handle the I/O, cut, and program manipulation predicates. In our previous work [55, 87] we developed a method for handling all of these predicates. <p> Notice our solution does not require cuts to perform leftmost tests in order to execute. Moreover, it does not block the execution of clauses which come after those containing cuts, as does Ali's method [4]. The method proposed by Ciepielewski et al. <ref> [45] </ref> requires that a cut must be on the leftmost branch within the subtree it prunes in order to be processed. In this case, if branches to the right of the cut are processed, significant speculative parallelism can result.
Reference: [46] <author> Matthew S. Hecht. </author> <title> Flow Analysis of Computer Programs. </title> <publisher> North Holland, </publisher> <address> New York, NY, </address> <year> 1977. </year>
Reference-contexts: For a procedure with more than one recursive call site, there is one pair of cycles induced by each call site. This is complicated to execute as a loop, since whether or not the multiple 112 113 recursive calls are in the same clause, the CFG is irreducible <ref> [46] </ref>. A graph is said to be irreducible if successive application of Hecht's T1 and T2 graph transformations to the original graph fails to produce a graph containing only one vertex. If both recursive calls are in the same clause then a situation like that depicted in Figure 5.2 arises. <p> If both recursive calls are in the same clause then a situation like that depicted in Figure 5.2 arises. This simple procedure has a recursive call site at nodes 2 and 3. Using T1-T2 reduction <ref> [46] </ref> finally produces a graph isomorphic to the second graph in the figure. This graph cannot be reduced any further, and hence the whole graph is irreducible. A similar situation is shown in Figure 5.3 for two recursive calls in different clauses.
Reference: [47] <author> M. V. Hermenegildo. </author> <title> Independent AND-Parallel Prolog and its Architecture. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: A second form of parallelism, namely AND parallelism, arises from the existence of independent sub-problems all of which must be solved to solve one larger problem. To address this sort of problem there have been several proposals for AND parallel execution of Prolog <ref> [26, 58, 101, 20, 36, 47] </ref>. Most of these systems do not treat the full Prolog language (with notable exceptions [36, 47]). The side effect predicates are the most common part omitted. <p> To address this sort of problem there have been several proposals for AND parallel execution of Prolog [26, 58, 101, 20, 36, 47]. Most of these systems do not treat the full Prolog language (with notable exceptions <ref> [36, 47] </ref>). The side effect predicates are the most common part omitted. The failure to provide the side effect predicates that exist in sequential Prolog systems makes it difficult to execute off-the-shelf Prolog applications. <p> This is particularly true of OR parallel systems, where the highest performance implementations are WAM-based [74, 3]. There are also a number of extended WAMs <ref> [48, 47] </ref> in use for extracting AND parallelism. In the implementation of our previous work on side effects in OR parallel Prolog [55, 87] we did not use the WAM, mostly because of the magnitude of constructing the entire compiler/byte-code interpreter system. <p> Some have used static data dependence graphs [58, 101, 20], others partially dynamic dependence graphs <ref> [36, 47] </ref>, and still others totally dynamic dependence graphs [26]. Kale [57] also notes that in rare circumstances it may be advantageous to allow concurrent execution of dependent literals. <p> There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static [58, 20, 101] to partly dynamic (conditional) <ref> [36, 47] </ref> to completely dynamic [26]. Kale [57] notes that in rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [84] supports only independent AND parallelism.
Reference: [48] <author> Manuel Hermenegildo. </author> <title> An abstract machine for restricted and-parallel execution of logic programs. </title> <booktitle> In Proceedings of the Third International Conference on Logic Programming, </booktitle> <pages> pages 25-39. </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: The WAM is a byte-coded interpreter for a compiled form of Prolog, and was the first efficient method for compiling Prolog for sequential machines. It has since been extended to a variety of parallel execution models <ref> [48, 97, 42, 49, 39] </ref>. In this section we will present a brief description of the WAM, because it is the model used for the inherent parallelism measurement methods described in Chapter Three. <p> This is particularly true of OR parallel systems, where the highest performance implementations are WAM-based [74, 3]. There are also a number of extended WAMs <ref> [48, 47] </ref> in use for extracting AND parallelism. In the implementation of our previous work on side effects in OR parallel Prolog [55, 87] we did not use the WAM, mostly because of the magnitude of constructing the entire compiler/byte-code interpreter system.
Reference: [49] <author> B.K. Holmer, B. Sano, M. Carlton, P. VanRoy, R. Haygood, J.M. Pendleton, T. Dobry, W.R. Bush, and A.M. Despain. </author> <title> Fast prolog with an extended general purpose architecture. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: The WAM is a byte-coded interpreter for a compiled form of Prolog, and was the first efficient method for compiling Prolog for sequential machines. It has since been extended to a variety of parallel execution models <ref> [48, 97, 42, 49, 39] </ref>. In this section we will present a brief description of the WAM, because it is the model used for the inherent parallelism measurement methods described in Chapter Three.
Reference: [50] <author> W. Ludwell Harrison III. </author> <title> Compiling lisp for evaluation on a tightly coupled multiprocessor. </title> <type> Technical report, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> March </month> <year> 1986. </year> <journal> CSRD Rpt. </journal> <volume> No. 565. </volume> <pages> 287 </pages>
Reference-contexts: vector, or the vector's first cell must be returned to an unmodified caller. operation in the procedure is changed from a call to length to a call to vector length because the cons vector's length can be computed more efficiently than simply traversing the list (a parallel find first one <ref> [50] </ref> algorithm can be used). Since the cons vector Lv was passed in, there is no need for allocating an expanded scalar for it. The Sv cons vector is an output argument for the procedure, so it is allocated in place of the expanded scalar in the previous version.
Reference: [51] <author> Williams Ludwell Harrison III. </author> <title> The Interprocedural Analysis and Automatic Paralleliza--tion of Scheme Programs. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1989. </year> <note> CSRD Report No. 860. </note>
Reference-contexts: The plethora of techniques developed for Fortran address a number of the same problems that arise in Prolog compilation as well. These techniques have been applied to languages that have pointers and recursion <ref> [51, 68] </ref>, so they clearly are applicable to more than just the Fortran language. The problem in applying these techniques to Prolog is that the usual execution model of Prolog is not very similar to that for procedural languages. <p> The first presents a few techniques used to convert the procedure so that it has a single identified call site. These techniques normalize procedures with non-determinism or multiple recursive call sites. The second section describes finding natural loops in the CFG, which are used to perform recursion splitting <ref> [51, 8] </ref> on non-tail recursive procedures. The next two sections describe the two parts of constructing an iteration 111 space for a do loop, namely identifying induction variables and termination conditions. The fifth section describes the treatment of live variables in recursion splitting and the use of scalar expansion [102].
Reference: [52] <institution> American National Standards Institute. </institution> <type> X3J3 Draft Fortran Standard, </type> <year> 1989. </year>
Reference-contexts: We now know that the induction variable a 1 goes from its initial value down to the smallest integer greater than zero in steps of m a 1 , or 1. The smallest integer greater than zero with stepsize one is obviously one, so to use the Fortran 90 <ref> [52] </ref> triplet notation, this induction variable gives rise to the iteration space (a 1 (0) : 1 : 1). The method for inferring iteration spaces for induction variables with m V greater than zero is equally straightforward. <p> The first argument is an iteration space constructor of the form V = L : U : S, where L, U , and S are integer constant expressions and V is an integer induction variable. These constant expressions are together interpreted in the same way as a Fortran-90 triplet <ref> [52] </ref>. The second argument is the list of variables that are local to an iteration. This would include F 1 and N 1 for the example in Figure 5.28.
Reference: [53] <author> D. Jacobs and A. Langen. </author> <title> Accurate and efficient approximation of variable aliases in logic programs. </title> <booktitle> In Proceedings of the 1989 North American Conference on Logic Programming, </booktitle> <pages> pages 154-165, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: This is conservative in the sense that aliases may be inferred where none actually exists. A number of methods have been presented for inferring aliases from Prolog programs <ref> [53, 99] </ref>. Almost any of these methods can be used to infer the sort of information we need for determining data dependences. To show how such information can be inferred we present here a simple approach for inferring aliases.
Reference: [54] <author> D. N. Jayasimha. </author> <title> Communication and Synchronization in Parallel Computation. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1988. </year> <note> CSRD Report No. 819. </note>
Reference-contexts: This is because of redundant synchronization, and in many cases can be optimized by removing unnecessary synchronization. For the case of post-wait pairs, an adaptation of the work for Fortran dependence elimination <ref> [75, 76, 60, 54, 69] </ref> should be fairly simple. This is especially useful when several dependences are synchronized in the same loop and have similar dependence distances. Block leftmosts can often be reduced by analyzing the CFG.
Reference: [55] <author> L. V. Kale, D. A. Padua, and D. C. Sehr. </author> <title> Or parallel execution of prolog programs with side effects. </title> <journal> The Journal of Supercomputing, </journal> <volume> 2(2) </volume> <pages> 209-223, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: A parallel Prolog system must address these constraints by either serializing accesses to the database [97] or maintaining a record of out-of-order execution and fixing it before problems arise <ref> [55, 87] </ref>. It is also important to note that the ordering requirements are only within a single relation. If two literals involve two different database relations, there is no ordering necessary between them. <p> This is particularly true of OR parallel systems, where the highest performance implementations are WAM-based [74, 3]. There are also a number of extended WAMs [48, 47] in use for extracting AND parallelism. In the implementation of our previous work on side effects in OR parallel Prolog <ref> [55, 87] </ref> we did not use the WAM, mostly because of the magnitude of constructing the entire compiler/byte-code interpreter system. Nonetheless, there is nothing preventing the techniques presented in Chapter Two from being used in an OR parallel WAM. <p> His approach [58] allows such execution, although the implementation [84] does not. 32 CHAPTER 2 OR PARALLEL EXECUTION OF PROLOG We began our study of the parallel execution of Prolog programs by studying the OR parallel execution of Prolog programs with side effects <ref> [55] </ref>. Several other projects have addressed the same problem [74, 97], but to our knowledge our previous work comprises the earliest full treatment of Prolog's side effect predicates. Two approaches have been presented by the community regarding the treatment of side-effect predicates. <p> Other researchers have also followed this approach. Ali [4] presented a method for giving sequential semantics to cut, and Ciepielewski et al. [45] described a method to handle the I/O, cut, and program manipulation predicates. In our previous work <ref> [55, 87] </ref> we developed a method for handling all of these predicates. We also noted that our method is able to exploit more parallelism in programs containing cuts than Ali's approach, and more parallelism in dynamic predicate calls than Aurora Prolog. <p> Because there may have been processors working in this region R, a retract will guide processors out of that region before reclaiming the storage used by R. The scheduling effects of retract are taken up in <ref> [55, 87] </ref>. 2.6 IMPLEMENTATION AND PERFORMANCE Our OR parallel Prolog interpreter, which we have just described, has been implemented in C on the Alliant FX/8, and performance figures have been obtained. The benchmark programs that have been executed are of two basic types. <p> The second sort of dependence arises from side effect predicates such as input/output, database manipulation, and program manipulation. This sort of dependence occurs less frequently, but has also received a fair amount of attention <ref> [36, 55, 87, 44] </ref>. In this section we describe the construction of the dependence graph for a procedure. We begin by describing data dependences and their analysis. This starts by combining mode and alias information to determine read and write points for each variable in a procedure. <p> We began by describing our work that on the OR parallel execution of programs with side effects. This work contained several extensions to our previous work <ref> [55, 87] </ref>, together with experimental data about the implementation. In this work we showed that we can efficiently execute Prolog programs with side effects in an OR parallel fashion. A number of questions surrounding the OR parallel execution of Prolog programs remain for future work.
Reference: [56] <author> L. V. Kale, B. Ramkumar, and W. W. Shu. </author> <title> A memory organisation independent binding environment for and and or parallel execution of logic programs. </title> <booktitle> In Proceedings of the Joint Fifth International Conference/Symposium on Logic Programming. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1988. </year>
Reference-contexts: That is the fundamental requirement for such dependences. A great deal of effort has gone into the design of binding environments for OR parallel systems [14, 97, 98, 71, 43, 23] and AND/OR parallel systems <ref> [56, 25] </ref>. This work is all aimed at alleviating such dependences, and as such can be seen as more exotic flavors of scalar expansion [102], and variable renaming [31] and localization [103]. The OR tree fragment in Figure 1.13 illustrates the dependences in a simple program.
Reference: [57] <author> Laxmikant V. Kale. </author> <title> Parallel Architectures for Problem Solving. </title> <type> PhD thesis, </type> <institution> State University of New York at Stony Brook, </institution> <year> 1985. </year>
Reference-contexts: Some have used static data dependence graphs [58, 101, 20], others partially dynamic dependence graphs [36, 47], and still others totally dynamic dependence graphs [26]. Kale <ref> [57] </ref> also notes that in rare circumstances it may be advantageous to allow concurrent execution of dependent literals. <p> There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static [58, 20, 101] to partly dynamic (conditional) [36, 47] to completely dynamic [26]. Kale <ref> [57] </ref> notes that in rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [84] supports only independent AND parallelism. Epilog [101] also permits dependent AND parallelism, but provides a primitive (CAND) to curtail it.
Reference: [58] <author> Laxmikant V. Kale. </author> <title> Parallel execution of logic programs: the reduce-or process model. </title> <booktitle> In Proceedings of the International Conference on Logic Programming, </booktitle> <pages> pages 616-632, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: A second form of parallelism, namely AND parallelism, arises from the existence of independent sub-problems all of which must be solved to solve one larger problem. To address this sort of problem there have been several proposals for AND parallel execution of Prolog <ref> [26, 58, 101, 20, 36, 47] </ref>. Most of these systems do not treat the full Prolog language (with notable exceptions [36, 47]). The side effect predicates are the most common part omitted. <p> Some have used static data dependence graphs <ref> [58, 101, 20] </ref>, others partially dynamic dependence graphs [36, 47], and still others totally dynamic dependence graphs [26]. Kale [57] also notes that in rare circumstances it may be advantageous to allow concurrent execution of dependent literals. <p> Some have used static data dependence graphs [58, 101, 20], others partially dynamic dependence graphs [36, 47], and still others totally dynamic dependence graphs [26]. Kale [57] also notes that in rare circumstances it may be advantageous to allow concurrent execution of dependent literals. His approach <ref> [58] </ref> allows such execution, although the implementation [84] does not. 32 CHAPTER 2 OR PARALLEL EXECUTION OF PROLOG We began our study of the parallel execution of Prolog programs by studying the OR parallel execution of Prolog programs with side effects [55]. <p> Another use of the method we propose is to compute exact dependences to test the effectiveness of dependence tests. There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static <ref> [58, 20, 101] </ref> to partly dynamic (conditional) [36, 47] to completely dynamic [26]. Kale [57] notes that in rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [84] supports only independent AND parallelism.
Reference: [59] <author> Ken Kennedy. </author> <title> Automatic translation of fortran programs to vector form. </title> <type> Technical report, </type> <institution> Rice University, </institution> <year> 1980. </year> <note> Report 467-029-4. </note>
Reference-contexts: Moreover, if iteration can be identified and exposed through source transformation, the possibility of using existing parallelizing compiler technology is opened. Several projects have studied the parallelization of Fortran do loops <ref> [62, 59, 17] </ref> and a number of techniques have been developed. The problem that concerns us most is how to transform Prolog programs to express iteration in a similar way to the Fortran do loop.
Reference: [60] <author> V. P. Krothapalli and P. Sadayappan. </author> <title> Removal of redundant dependences in doacross loops with constant dependences. </title> <booktitle> In Proceedings of the Third Sigplan Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 51-60, </pages> <month> April </month> <year> 1991. </year> <month> 288 </month>
Reference-contexts: This is because of redundant synchronization, and in many cases can be optimized by removing unnecessary synchronization. For the case of post-wait pairs, an adaptation of the work for Fortran dependence elimination <ref> [75, 76, 60, 54, 69] </ref> should be fairly simple. This is especially useful when several dependences are synchronized in the same loop and have similar dependence distances. Block leftmosts can often be reduced by analyzing the CFG.
Reference: [61] <author> Clyde Kruskal, Larry Rudolph, and Marc Snir. </author> <title> The power of parallel prefix. </title> <editor> In Doug DeGroot, editor, </editor> <booktitle> Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 180-185, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: These dependence cycles may compute recurrences that can be parallelized by algorithm replacements. A number of such recurrence solutions such as tree height reduction [64], parallel banded and triangular matrix solvers [22], and parallel prefix <ref> [61] </ref> should be in the repertoire of algorithm replacements that are possible by pattern matching. We believe that a number of recurrences, especially those coming from reductions, should be recognizable by such pattern matching. It remains as future work which parallel recurrence solvers are useful beyond that for Prolog execution.
Reference: [62] <author> D. Kuck, R. Kuhn, B. Leasure, and M. Wolfe. </author> <title> The structure of an advanced retargetable vectorizer. </title> <editor> In Kai Hwang, editor, </editor> <booktitle> Tutorial on Supercomputers: Designs and Applications, </booktitle> <pages> pages 163-178. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1984. </year>
Reference-contexts: Moreover, if iteration can be identified and exposed through source transformation, the possibility of using existing parallelizing compiler technology is opened. Several projects have studied the parallelization of Fortran do loops <ref> [62, 59, 17] </ref> and a number of techniques have been developed. The problem that concerns us most is how to transform Prolog programs to express iteration in a similar way to the Fortran do loop.
Reference: [63] <author> David Kuck, Edward Davidson, Duncan Lawrie, and Ahmed Sameh. </author> <title> Parallel supercomputing today and the cedar approach. </title> <editor> In J. J. Dongarra, editor, </editor> <booktitle> Experimental Parallel Computing Architectures, </booktitle> <pages> pages 1-20. </pages> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: The doall construct we propose should be efficient on tightly-coupled machines, and we believe that together with the underlying parallel implementation our loops can provide an effective way to execute Prolog on hierarchical memory machines such as Cedar <ref> [63] </ref> and the Alliant FX/2800. This would be done by running loops as lightweight tasks on clusters of processors, and distributing the loop 180 literals across clusters using a distributed memory implementation of the underlying model.
Reference: [64] <author> David J. Kuck. </author> <title> The Structure of Computers and Computations, volume I. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY, </address> <year> 1978. </year>
Reference-contexts: These dependence cycles may compute recurrences that can be parallelized by algorithm replacements. A number of such recurrence solutions such as tree height reduction <ref> [64] </ref>, parallel banded and triangular matrix solvers [22], and parallel prefix [61] should be in the repertoire of algorithm replacements that are possible by pattern matching. We believe that a number of recurrences, especially those coming from reductions, should be recognizable by such pattern matching.
Reference: [65] <author> Manoj Kumar. </author> <title> Measuring parallelism in computation intensive scientific/engineering applications. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(9), </volume> <month> September </month> <year> 1988. </year>
Reference-contexts: This sort of inherent parallelism measurement can provide insights into why a particular execution model achieves a certain performance. It can also help to show potential bottlenecks in programs and dataflow analysis techniques. The inherent parallelism of Fortran programs has been measured by Kumar <ref> [65] </ref>, who determines the critical path of a program by running a specially instrumented program. This technique seems appealing for existing applications, because by adding a small overhead we can gather a large number of execution parameters. In Chapter Three we describe the adaptation of Kumar's technique to Prolog. <p> The OR instrumented version of a typical program takes two to three times as long as its uninstrumented original. The AND/OR instrumented version always takes less than ten times the original execution time. 54 Our AND parallelism estimation method is based upon the work of by Kumar <ref> [65] </ref> in estimating the inherent parallelism in Fortran programs. His method augments the source program with a timestamp for each data item d, which is updated each time d is written. <p> Often segments of a branch can execute simultaneously, and doing so would reduce that critical path time. This is AND parallel execution, and unlike OR parallelism, it requires testing for dependences even in pure Prolog programs. In this section we describe the application of Kumar's <ref> [65] </ref> techniques for Fortran to estimate the best AND/OR parallel execution time. The method we describe extends his work to deal with the dynamic data structures and aliasing present in Prolog. <p> Further work such as that done by Petersen and Padua [80] may prove useful in order to determine whether more effort should be devoted to dependence analysis. Their system is based upon Kumar's <ref> [65] </ref> techniques as is that we described in Chapter Three. Because of this, it should be reasonably straightforward to extend the methods of Chapter Three to perform such an evaluation of Prolog dependence analysis techniques.
Reference: [66] <author> W. Landi. </author> <title> Interprocedural Aliasing in the Presence of Pointers. </title> <type> PhD thesis, </type> <institution> Rutgers University, </institution> <year> 1991. </year>
Reference-contexts: This is very conservative, since it produces inexact results in the most commonly occurring cases. Moreover, the likelihood of the procedure being called with the pathological term that necessitates this is very low. Further enhancements in alias analysis such as using <ref> [11, 27, 28, 66, 67] </ref> may yield significant improvements. For each literal l the procedure in Figure 6.2 is used to set up the calling arguments aliases for the procedure.
Reference: [67] <author> William Landi and Barbara Ryder. </author> <title> Pointer induced aliasing: A problem classification. </title> <booktitle> In Conference Record of the Eighteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1991. </year>
Reference-contexts: This is very conservative, since it produces inexact results in the most commonly occurring cases. Moreover, the likelihood of the procedure being called with the pathological term that necessitates this is very low. Further enhancements in alias analysis such as using <ref> [11, 27, 28, 66, 67] </ref> may yield significant improvements. For each literal l the procedure in Figure 6.2 is used to set up the calling arguments aliases for the procedure.
Reference: [68] <author> James Larus. </author> <title> Restructuring lisp programs for concurrent execution. </title> <booktitle> In Proceedings of the ACM/SIGPLAN PPEALS 1988, </booktitle> <pages> pages 100-110, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: The plethora of techniques developed for Fortran address a number of the same problems that arise in Prolog compilation as well. These techniques have been applied to languages that have pointers and recursion <ref> [51, 68] </ref>, so they clearly are applicable to more than just the Fortran language. The problem in applying these techniques to Prolog is that the usual execution model of Prolog is not very similar to that for procedural languages.
Reference: [69] <author> Z. Li and W. Abu-Sufah. </author> <title> On reducing data synchronizaaation in multiprocessed loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(1):105-109, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: This is because of redundant synchronization, and in many cases can be optimized by removing unnecessary synchronization. For the case of post-wait pairs, an adaptation of the work for Fortran dependence elimination <ref> [75, 76, 60, 54, 69] </ref> should be fairly simple. This is especially useful when several dependences are synchronized in the same loop and have similar dependence distances. Block leftmosts can often be reduced by analyzing the CFG.
Reference: [70] <author> Zhiyuan Li. </author> <title> Array Privatization for Parallel Execution of Loops. </title> <booktitle> In Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <pages> pages 313-322, </pages> <month> July </month> <year> 1992. </year> <month> 289 </month>
Reference-contexts: It is also important to note that the ordering requirements are only within a single relation. If two literals involve two different database relations, there is no ordering necessary between them. Techniques used for clause indexing [19, 38] and data privatization <ref> [16, 70, 93] </ref> may be useful in partitioning the database to reduce the amount of serialization required. 15 1.4.4 PROGRAM MANIPULATION The last class of side effects that are dependent upon Prolog's execution order are the program manipulation predicates.
Reference: [71] <author> Gary Lindstrom. </author> <title> Or-parallelism on applicative architectures. </title> <booktitle> In Proceedings of the Second International Logic Programming Conference, </booktitle> <pages> pages 159-170, </pages> <institution> Uppsala, Sweden, </institution> <year> 1984. </year> <title> Ord & Form. </title>
Reference-contexts: That is the fundamental requirement for such dependences. A great deal of effort has gone into the design of binding environments for OR parallel systems <ref> [14, 97, 98, 71, 43, 23] </ref> and AND/OR parallel systems [56, 25]. This work is all aimed at alleviating such dependences, and as such can be seen as more exotic flavors of scalar expansion [102], and variable renaming [31] and localization [103].
Reference: [72] <author> D. B. Loveman. </author> <title> Program improvement by source-to-source transformation. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 121-145, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Our method for handling these difficulties is to select a branch consistently (say the leftmost one) at each choice point and attempt to parallelize the loop so obtained. This is done by repeatedly applying inline expansion <ref> [72] </ref> to the procedure called by the node at the head of the failure arc leading either from a recursive call site or a success node.
Reference: [73] <author> S.F. Lundstrum and G.H. Barnes. </author> <title> A controllable mimd architecture. </title> <booktitle> In Proceedings of the 1980 International Conference on Parallel Processing, </booktitle> <pages> pages 19-27. </pages> <publisher> IEEE Press, </publisher> <month> August </month> <year> 1980. </year>
Reference-contexts: In that chapter we extend the timing method of Chapter Three to time the resulting programs. Chapter Six presents the methods for parallelizing the loop-converted programs. There we describe two parallel loop forms based on doacross [77, 32] and doall <ref> [73] </ref> loops, and extend the timing method include these parallel constructs. In Chapter Seven we present an analysis of the effectiveness of the transformations. We do this by presenting several benchmarks before and after transformation, and comparing inherent parallelism using the timing method of Chapter Three. <p> We start our discussion of parallel execution by describing a parallel form of the do predicate. This form, called doall after the Burroughs Fortran statement of the same name <ref> [73] </ref>, expresses that all the iterations of the do can execute in parallel. In section three we give a method for deciding whether a do can be transformed into a doall, and show how it is converted.
Reference: [74] <editor> E. Lusk, D.H.D. Warren, and Seif Haridi et al. </editor> <title> The aurora or-parallel prolog system. </title> <booktitle> In Proceedings of the International Conference on Fifth Generation Computer Systems, </booktitle> <pages> pages 819-830. </pages> <publisher> ICOT, </publisher> <year> 1988. </year>
Reference-contexts: A number of parallel implementations have been built based on the idea that a system needs to handle the full Prolog language, including the side effect predicates. OR parallel systems were the first to incorporate the full set of Prolog builtins <ref> [74, 3, 87] </ref>. OR parallelism arises from problems that have several alternative candidate solutions, and hence OR parallel execution is able to efficiently extract a great deal of parallelism from problems that are oriented toward combinatorial search. <p> This is particularly true of OR parallel systems, where the highest performance implementations are WAM-based <ref> [74, 3] </ref>. There are also a number of extended WAMs [48, 47] in use for extracting AND parallelism. <p> OR parallelism arises from independent traversal of the branches of the OR tree for a program. For programs without side effects OR parallelism is relatively easy to exploit without compile-time analysis <ref> [74, 3] </ref>. With side effects the problem can still be solved without significant static analysis [87], but more efficient solutions may be obtained through static analysis [44]. AND parallelism arises from independent executions of literals within a clause body. <p> Several other projects have addressed the same problem <ref> [74, 97] </ref>, but to our knowledge our previous work comprises the earliest full treatment of Prolog's side effect predicates. Two approaches have been presented by the community regarding the treatment of side-effect predicates. First, one group of researchers have proposed new side-effect predicates that have a parallel semantics [74]. <p> Two approaches have been presented by the community regarding the treatment of side-effect predicates. First, one group of researchers have proposed new side-effect predicates that have a parallel semantics <ref> [74] </ref>. The other approach, executing Prolog in parallel while preserving sequential semantics of the side-effect goals, is the one we have taken. Other researchers have also followed this approach. <p> The model we propose as a suitable execution framework for our transformed programs begins with an OR parallel execution model such as Aurora <ref> [74] </ref>, Muse [3], or the system described in Chapter Two. It could also begin with an AND/OR parallel system such as such as the Reduce-OR Process Model [84], the AND/OR Process Model [26], or Epilog [101].
Reference: [75] <author> Samuel P. Midkiff. </author> <title> Automatic generation of synchronization instructions for parallel processors. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1986. </year> <note> CSRD report no. 588. </note>
Reference-contexts: This is because of redundant synchronization, and in many cases can be optimized by removing unnecessary synchronization. For the case of post-wait pairs, an adaptation of the work for Fortran dependence elimination <ref> [75, 76, 60, 54, 69] </ref> should be fairly simple. This is especially useful when several dependences are synchronized in the same loop and have similar dependence distances. Block leftmosts can often be reduced by analyzing the CFG.
Reference: [76] <author> Samuel P. Midkiff. </author> <title> The Dependence Analysis and Synchronization of Parallel Programs. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: This is because of redundant synchronization, and in many cases can be optimized by removing unnecessary synchronization. For the case of post-wait pairs, an adaptation of the work for Fortran dependence elimination <ref> [75, 76, 60, 54, 69] </ref> should be fairly simple. This is especially useful when several dependences are synchronized in the same loop and have similar dependence distances. Block leftmosts can often be reduced by analyzing the CFG.
Reference: [77] <author> D. A. Padua, D. J. Kuck, and D. H. Lawrie. </author> <title> High-Speed Multiprocessors and Compilation Techniques. </title> <journal> Special Issue on Parallel Processing, IEEE Trans. on Computers, </journal> <volume> C-29(9):763-776, </volume> <month> Sept., </month> <year> 1980. </year>
Reference-contexts: In that chapter we extend the timing method of Chapter Three to time the resulting programs. Chapter Six presents the methods for parallelizing the loop-converted programs. There we describe two parallel loop forms based on doacross <ref> [77, 32] </ref> and doall [73] loops, and extend the timing method include these parallel constructs. In Chapter Seven we present an analysis of the effectiveness of the transformations. We do this by presenting several benchmarks before and after transformation, and comparing inherent parallelism using the timing method of Chapter Three. <p> In this section we present a new builtin predicate, called doacross, that is a parallel form of a do which can be used to execute loops with cross iteration dependences. It is a simple extension to the loop type of the same name developed for Fortran <ref> [77, 32] </ref>. To synchronize cross iteration dependences requires the definition of three synchronization builtin predicates. Two of these are directly taken from Cray Fortran [30], and provide point to point synchronization of data and side effect dependences.
Reference: [78] <author> David Padua and Michael Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <pages> pages 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: One would like to be able to expose the set of recurrences that are used to express iteration and transform programs to improve their parallel execution performance. 2 This problem is very similar to the conversion of Fortran if-goto loops to do loops <ref> [78] </ref>. Moreover, if iteration can be identified and exposed through source transformation, the possibility of using existing parallelizing compiler technology is opened. Several projects have studied the parallelization of Fortran do loops [62, 59, 17] and a number of techniques have been developed. <p> Recognizing control recurrences often may require data structure conversions, and such conversions are also described in this chapter. Once cycles have been converted to loop forms we build their dependence graphs and parallelize using methods very similar to those used by fortran compilers <ref> [78, 103] </ref>. This chapter consists of seven sections. The first presents a few techniques used to convert the procedure so that it has a single identified call site. These techniques normalize procedures with non-determinism or multiple recursive call sites. <p> In the previous section we described three transformations for eliminating several sorts of dependences. Two additional transformations that will be useful are distribution [5] and statement reordering <ref> [78] </ref>. Distribution allows us to split a loop with multiple literals in the body (or literals in the body and the left or right bodies), as long as no dependence cycles are broken.
Reference: [79] <author> David A. Padua. </author> <title> Multiprocessors: Discussion of Some Theoretical and Practical Problems. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year> <note> Report No. 79-990. </note>
Reference-contexts: Some do literals contain cross-iteration dependences or have non-determinism, and therefore those do literals cannot be converted to doall literals. The next section describes a second parallel form of do, called doacross <ref> [79, 32] </ref>. This form can deal with cross-iteration dependences and non-determinism. It does restrict the scheduling paradigm that must be used, however. Dependences that occur in dos that are transformed into doacrosses must be explicitly synchronized.
Reference: [80] <author> David A. Padua and Paul M. Petersen. </author> <title> Evaluation of parallelizing compilers. </title> <booktitle> In PACTA '92, International Conference of Parallel Computing and Transputers Applications, </booktitle> <address> Barcelona, Spain, </address> <year> 1992. </year> <note> (to appear). 290 </note>
Reference-contexts: Next the dependence graph of the program is constructed. What we have presented here is a system using very simple mode and aliasing information, but we are still able to get fairly sharp dependence analysis. Further work such as that done by Petersen and Padua <ref> [80] </ref> may prove useful in order to determine whether more effort should be devoted to dependence analysis. Their system is based upon Kumar's [65] techniques as is that we described in Chapter Three.
Reference: [81] <author> Paul Marx Petersen. </author> <title> Evaluation Methods For Parallel Compilation Using Dynamic Anal--ysis. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> January </month> <year> 1993. </year> <note> In preparation. </note>
Reference-contexts: However, we believe they do represent a useful metric for comparing program execution before and after transformation. 214 T 1 = sequential time T c = critical path parallel time T p = 0 T p = T p + dH (i)=pe Both approximations used here were proposed by Petersen <ref> [81] </ref> for use in estimating the inherent parallelism in Fortran programs. We present error bounds for both functions, and have implemented both of them in the timing systems discussed in Chapter Three. <p> This will enable better evaluations of parallel Prolog systems, and potentially give more information for designing new ones. Another direction for this work is measuring higher-level program parameters such as induction variables, dependence analysis effectiveness, and memory optimization strategies such as is currently being done for Fortran <ref> [81, 94] </ref>. One final addition that would be useful is timing estimates assuming that the only source of parallelism is the loop constructs proposed in Chapter Six. This was not implemented in the work we described here, because it requires defining a completely new execution model for the programs.
Reference: [82] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1425-1439, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: First, the do must have empty left and right body arguments, because doall is deterministic. Second, if there must be no loop carried flow dependences. This is because the iterations of a doall loop may be executed in any order, using random scheduling, pre-scheduling, self-scheduling [91], guided self scheduling <ref> [82] </ref> or any other of the myriad scheduling paradigms developed for parallel loops. Starting dependent iterations out of strict ascending order has the possibility of deadlocking on finite numbers of processors. Testing for loop-carried dependences consists of looking for dependences in the dependence graph with distance other than zero.
Reference: [83] <institution> Quintus Computer Systems, Inc., Mountain View, California. </institution> <note> Quintus Prolog Reference Manual, </note> <year> 1988. </year>
Reference-contexts: The next node, N 4 , removes that element from the database. When node N 5 is reached by the interpreter the first element of the data relation contains the elements 2 and 1, in that order. We use the semantics of Quintus Prolog <ref> [83] </ref> regarding changes of the database in the subtree formed by the processing of a recorded/3 literal. In this example, if the left subtree of N 5 had used erase/1 to remove 1 from the data relation, that change would not cause the rightmost subtree not to exist.
Reference: [84] <author> B. Ramkumar and L.V. Kale. </author> <title> Compiled execution of the reduce-or process model on multiprocessors. </title> <booktitle> In Proceedings of the 1989 North American Conference on Logic Programming, </booktitle> <pages> pages 313-331, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Kale [57] also notes that in rare circumstances it may be advantageous to allow concurrent execution of dependent literals. His approach [58] allows such execution, although the implementation <ref> [84] </ref> does not. 32 CHAPTER 2 OR PARALLEL EXECUTION OF PROLOG We began our study of the parallel execution of Prolog programs by studying the OR parallel execution of Prolog programs with side effects [55]. <p> Kale [57] notes that in rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation <ref> [84] </ref> supports only independent AND parallelism. Epilog [101] also permits dependent AND parallelism, but provides a primitive (CAND) to curtail it. The model we have developed includes dynamic, independent AND parallelism, with a strict sequential ordering on dependent literals. <p> It could also begin with an AND/OR parallel system such as such as the Reduce-OR Process Model <ref> [84] </ref>, the AND/OR Process Model [26], or Epilog [101]. Any of these models can be extended fairly simply with the necessary structures for executing parallel loops. Because of this the results presented in Chapter Eight give performance numbers for both OR and AND/OR parallel implementations augmented with parallel loops.
Reference: [85] <author> B.G. Ryder and M.C. Paull. </author> <title> Elimination algorithms for data flow analysis. </title> <journal> ACM Computing Surveys, </journal> <volume> 18(3) </volume> <pages> 277-316, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: Therefore, the determinism of p depends on the determinism of q, and the pair (S (C i ); S (C i )) is added to add-pairs (p,q). approach <ref> [85] </ref> that iterates until the worklist becomes empty. The first part computes the dependence sets used for updating the worklist, and initializes the path-set array that is used to represent the s's in the function description.
Reference: [86] <author> Vikram A. Saletore. </author> <title> Machine Independent Parallel Execution of Speculative Computations. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: This can be changed by having the processor execute l i before b i . This reduces the amount of speculative computation at the expense of possibly increasing the time required to produce all solutions. Using a run-time priority scheme such as that implemented in Rolog <ref> [86] </ref> may address this problem. Another possibility is extending the work of Hausman [44] on compile estimation of speculation. Handling failures in the body of the loop is also different for doacross.
Reference: [87] <author> David C. Sehr. </author> <title> Or parallel execution of prolog programs with side effects. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1988. </year> <type> CSRD Report Number 822. </type>
Reference-contexts: A number of parallel implementations have been built based on the idea that a system needs to handle the full Prolog language, including the side effect predicates. OR parallel systems were the first to incorporate the full set of Prolog builtins <ref> [74, 3, 87] </ref>. OR parallelism arises from problems that have several alternative candidate solutions, and hence OR parallel execution is able to efficiently extract a great deal of parallelism from problems that are oriented toward combinatorial search. <p> When database or program manipulation predicates are used, the structure of the SLD tree is dependent on Prolog's sequential search order. Moreover, when either the cut predicate, the if or if-then-else connectives are used, parts of the SLD tree are not traversed by a sequential system. Some parallel systems <ref> [87] </ref> may traverse parts 8 of these regions in a trade-off of extra work for additional parallelism. This work gives rise to speculative parallelism An SLD tree is a rooted tree with each node N labeled by a conjunction of literals called the query at N . <p> A parallel Prolog system must address these constraints by either serializing accesses to the database [97] or maintaining a record of out-of-order execution and fixing it before problems arise <ref> [55, 87] </ref>. It is also important to note that the ordering requirements are only within a single relation. If two literals involve two different database relations, there is no ordering necessary between them. <p> This is particularly true of OR parallel systems, where the highest performance implementations are WAM-based [74, 3]. There are also a number of extended WAMs [48, 47] in use for extracting AND parallelism. In the implementation of our previous work on side effects in OR parallel Prolog <ref> [55, 87] </ref> we did not use the WAM, mostly because of the magnitude of constructing the entire compiler/byte-code interpreter system. Nonetheless, there is nothing preventing the techniques presented in Chapter Two from being used in an OR parallel WAM. <p> OR parallelism arises from independent traversal of the branches of the OR tree for a program. For programs without side effects OR parallelism is relatively easy to exploit without compile-time analysis [74, 3]. With side effects the problem can still be solved without significant static analysis <ref> [87] </ref>, but more efficient solutions may be obtained through static analysis [44]. AND parallelism arises from independent executions of literals within a clause body. <p> Other researchers have also followed this approach. Ali [4] presented a method for giving sequential semantics to cut, and Ciepielewski et al. [45] described a method to handle the I/O, cut, and program manipulation predicates. In our previous work <ref> [55, 87] </ref> we developed a method for handling all of these predicates. We also noted that our method is able to exploit more parallelism in programs containing cuts than Ali's approach, and more parallelism in dynamic predicate calls than Aurora Prolog. <p> Our method does not require that expand operations produce all the children of a node at once. It is equally well suited to an incremental generation of children by using mechanisms analogous to the choice points of sequential Prolog systems <ref> [87] </ref>. This may be done to limit memory consumption or to avoid large amounts of speculative parallelism. Soft side-effect predicates are those that examine a part of global state (input/output buffers, etc.) of the running program, but do not change the program or scheduling strategy at run time. <p> There may also have been processes working in the pruned region and for the sake of efficiency they should be relocated to a more productive region of the OR tree. Several methods for doing this can be found in <ref> [87] </ref>. Notice our solution does not require cuts to perform leftmost tests in order to execute. Moreover, it does not block the execution of clauses which come after those containing cuts, as does Ali's method [4]. <p> problem also raises the issue of storage reclamation, since it is not true that a region being inaccessible via the scheduling queue is sufficient to say that it is dead. 40 (a) After cut at C1 is performed (b) After both cuts are performed 41 Several solutions are proposed in <ref> [87] </ref>, and other ones should result from observations of the behavior of the system. 2.5 PROCESSING ASSERTA, ASSERTZ AND RETRACT The two assert predicates and retract give the programmer the power to alter the structure of the program at run time. <p> Because there may have been processors working in this region R, a retract will guide processors out of that region before reclaiming the storage used by R. The scheduling effects of retract are taken up in <ref> [55, 87] </ref>. 2.6 IMPLEMENTATION AND PERFORMANCE Our OR parallel Prolog interpreter, which we have just described, has been implemented in C on the Alliant FX/8, and performance figures have been obtained. The benchmark programs that have been executed are of two basic types. <p> The second sort of dependence arises from side effect predicates such as input/output, database manipulation, and program manipulation. This sort of dependence occurs less frequently, but has also received a fair amount of attention <ref> [36, 55, 87, 44] </ref>. In this section we describe the construction of the dependence graph for a procedure. We begin by describing data dependences and their analysis. This starts by combining mode and alias information to determine read and write points for each variable in a procedure. <p> This is because of the fact that the underlying OR parallel system produces irregular tree-structured computations that are not easily synchronized by point-to-point synchronization. Because of this, the usual solution <ref> [87, 44] </ref> to OR parallelism and side effects is to allow side effects to execute only when they are at the leftmost unprocessed node in the OR tree. <p> We began by describing our work that on the OR parallel execution of programs with side effects. This work contained several extensions to our previous work <ref> [55, 87] </ref>, together with experimental data about the implementation. In this work we showed that we can efficiently execute Prolog programs with side effects in an OR parallel fashion. A number of questions surrounding the OR parallel execution of Prolog programs remain for future work.
Reference: [88] <author> Kish Shen. </author> <title> An investigation of the argonne model of or-parallel prolog. </title> <type> Master's thesis, </type> <institution> University of Manchester, </institution> <year> 1986. </year>
Reference-contexts: The method we describe extends his work to deal with the dynamic data structures and aliasing present in Prolog. We believe this framework has the advantage over other methods <ref> [88, 92] </ref> of allowing us to extend it to measure critical path times in programs with user parallelism. A program's dependences can only be exactly determined at execution time, since one execution may have a dependence while another does not.
Reference: [89] <institution> Swedish Institute of Computer Science, </institution> <note> S-164 28 Kista, Sweden. SICStus Prolog User's Manual, </note> <year> 1991. </year>
Reference-contexts: Moreover, each time a get structure instruction attempts to unify a structure with a variable a structure is constructed in the heap. Environments, which is the Prolog form of activation records, and choice points are stored on the stack in most implementations. Other implementations separate these into two stacks <ref> [89] </ref> to allow more efficient memory management. An environment contains the local variables for a clause plus pointers to the continuation's first instruction and environment. A choice point maintains most of the information to perform backtracking.
Reference: [90] <author> Peter Szeredi. </author> <title> Performance analysis of the aurora or-parallel prolog system. </title> <booktitle> In Proceedings of the 1989 North American Conference on Logic Programming, </booktitle> <pages> pages 713-732, </pages> <year> 1989. </year>
Reference-contexts: In Chapter Two we present the results obtained by an OR parallel 1 implementation we have developed that incorporates the full set of Prolog side effect predicates. This work and the Aurora <ref> [90] </ref> and Muse [3] systems have shown that an OR parallel system can extract a significant amount of parallelism from Prolog programs, even those containing side effect predicates. <p> A number of programs exhibited essentially no OR parallelism (e.g. divide10, log10, nreverse, times10). In general, independent AND parallel execution improved the performance of programs already speeded up by OR parallel execution by a small factor (1-6). These programs have all shown reasonable speedups in real OR parallel systems <ref> [90] </ref>. Our results show that there is plenty of parallelism in several of these programs to extend to much larger machines (e.g. consider chat parser, query and zebra). Those with smaller speedups may profit from the introduction of independent AND parallelism.
Reference: [91] <author> P. Tang and P.-C. Yew. </author> <title> Processor self-scheduling for multiple-nested parallel loops. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 528-535, </pages> <month> August </month> <year> 1986. </year> <month> 291 </month>
Reference-contexts: First, the do must have empty left and right body arguments, because doall is deterministic. Second, if there must be no loop carried flow dependences. This is because the iterations of a doall loop may be executed in any order, using random scheduling, pre-scheduling, self-scheduling <ref> [91] </ref>, guided self scheduling [82] or any other of the myriad scheduling paradigms developed for parallel loops. Starting dependent iterations out of strict ascending order has the possibility of deadlocking on finite numbers of processors.
Reference: [92] <author> Evan Tick. </author> <title> Studies in Prolog Architectures. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: The method we describe extends his work to deal with the dynamic data structures and aliasing present in Prolog. We believe this framework has the advantage over other methods <ref> [88, 92] </ref> of allowing us to extend it to measure critical path times in programs with user parallelism. A program's dependences can only be exactly determined at execution time, since one execution may have a dependence while another does not.
Reference: [93] <author> P. Tu and D. Padua. </author> <title> Array privatization for shared and distributed memory machines. </title> <booktitle> In Proc. 2nd Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: It is also important to note that the ordering requirements are only within a single relation. If two literals involve two different database relations, there is no ordering necessary between them. Techniques used for clause indexing [19, 38] and data privatization <ref> [16, 70, 93] </ref> may be useful in partitioning the database to reduce the amount of serialization required. 15 1.4.4 PROGRAM MANIPULATION The last class of side effects that are dependent upon Prolog's execution order are the program manipulation predicates.
Reference: [94] <author> Peng Tu. </author> <title> Array expansion and privatization. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1993. </year> <note> In preparation. </note>
Reference-contexts: This will enable better evaluations of parallel Prolog systems, and potentially give more information for designing new ones. Another direction for this work is measuring higher-level program parameters such as induction variables, dependence analysis effectiveness, and memory optimization strategies such as is currently being done for Fortran <ref> [81, 94] </ref>. One final addition that would be useful is timing estimates assuming that the only source of parallelism is the loop constructs proposed in Chapter Six. This was not implemented in the work we described here, because it requires defining a completely new execution model for the programs.
Reference: [95] <author> Yu-Wen Tung and Dan I. Moldovan. </author> <title> Detection of and parallelism in logic programs. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 984-991, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Furthermore, since all the unifications in node 3 were trivial, the dependences leading into node 3 (6 3 and 8 3) have been combined with 3 4, 3 5 and 3 7. 1.7 SOURCES OF PARALLELISM A number of sources of parallelism have been categorized <ref> [95] </ref> for Prolog programs. Two sources account for the overwhelming majority of the parallelism in real programs: AND and OR parallelism. Other sources include stream, unification, and search parallelism. An important part of deciding how to parallelize programs is an understanding of the importance of various sorts of parallelism.
Reference: [96] <author> David H. D. Warren. </author> <title> An abstract prolog instruction set. </title> <type> Technical report, </type> <institution> SRI International, </institution> <month> October </month> <year> 1983. </year> <note> Technical Note 309. </note>
Reference-contexts: of programs that do a large amount of program manipulation, however, so it does not seem very profitable to spend a great deal of effort in optimizing program manipulation predicates. 17 1.5 THE WARREN ABSTRACT MACHINE The predominant implementation for Prolog systems is the Warren Abstract Machine or WAM , <ref> [96, 2] </ref>. The WAM is a byte-coded interpreter for a compiled form of Prolog, and was the first efficient method for compiling Prolog for sequential machines. It has since been extended to a variety of parallel execution models [48, 97, 42, 49, 39]. <p> Where dependences are inferred between literals, the sequential order will be that which is enforced. 3.2 SEQUENTIAL AND OR TIME The most efficient OR parallel implementations of Prolog to date [97, 3] have been based upon the Warren Abstract Machine (WAM) <ref> [96] </ref>. Because of this, we compute critical path timings in number of WAM instructions executed. The number of instructions is an approximation to execution time, since each type of WAM instruction takes a slightly different time. Variations in execution time come mainly from two sources: argument unification and backward execution.
Reference: [97] <author> David H.D. Warren. </author> <title> The sri model for or parallel execution of prolog | abstract design and implementation. </title> <booktitle> In Proceedings of the 1987 Symposium on Logic Programming, </booktitle> <pages> pages 92-103. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> September </month> <year> 1987. </year>
Reference-contexts: Lastly, if the erase is not processed before the recorded at N 5 , then there would be a spurious rightmost child corresponding to the element 3 in the data relation. A parallel Prolog system must address these constraints by either serializing accesses to the database <ref> [97] </ref> or maintaining a record of out-of-order execution and fixing it before problems arise [55, 87]. It is also important to note that the ordering requirements are only within a single relation. If two literals involve two different database relations, there is no ordering necessary between them. <p> The WAM is a byte-coded interpreter for a compiled form of Prolog, and was the first efficient method for compiling Prolog for sequential machines. It has since been extended to a variety of parallel execution models <ref> [48, 97, 42, 49, 39] </ref>. In this section we will present a brief description of the WAM, because it is the model used for the inherent parallelism measurement methods described in Chapter Three. <p> That is the fundamental requirement for such dependences. A great deal of effort has gone into the design of binding environments for OR parallel systems <ref> [14, 97, 98, 71, 43, 23] </ref> and AND/OR parallel systems [56, 25]. This work is all aimed at alleviating such dependences, and as such can be seen as more exotic flavors of scalar expansion [102], and variable renaming [31] and localization [103]. <p> Several other projects have addressed the same problem <ref> [74, 97] </ref>, but to our knowledge our previous work comprises the earliest full treatment of Prolog's side effect predicates. Two approaches have been presented by the community regarding the treatment of side-effect predicates. First, one group of researchers have proposed new side-effect predicates that have a parallel semantics [74]. <p> Where dependences are inferred between literals, the sequential order will be that which is enforced. 3.2 SEQUENTIAL AND OR TIME The most efficient OR parallel implementations of Prolog to date <ref> [97, 3] </ref> have been based upon the Warren Abstract Machine (WAM) [96]. Because of this, we compute critical path timings in number of WAM instructions executed. The number of instructions is an approximation to execution time, since each type of WAM instruction takes a slightly different time. <p> These programs range over a variety of sizes and purposes. There are several interesting facts to observe from these programs. First, David H. D. Warren's assertion <ref> [97] </ref> that OR parallelism was likely to produce significant speedups on a range of pro grams appears to be borne out. Several programs achieved small speedups from OR parallelism, 67 mostly due to shallow backtracking (e.g flatten, ops8, poly10, qsort, tak, unify). <p> The execution model for doacross requires special handling of the non-determinism that is possible in the body. This is done by using the OR parallel binding environment that underlies the system, probably either binding arrays <ref> [97] </ref> or hash windows [14]. The spine of the recursion is treated by having each iteration produce its local environment and pass it to the next iteration. The binding environment impact of doacross is shown in Figure 6.16.
Reference: [98] <author> David S. Warren. </author> <title> Efficient prolog memory management for flexible control strategies. </title> <booktitle> In Proceedings of the 1984 Logic Programming Symposium, </booktitle> <pages> pages 198-202. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1984. </year>
Reference-contexts: That is the fundamental requirement for such dependences. A great deal of effort has gone into the design of binding environments for OR parallel systems <ref> [14, 97, 98, 71, 43, 23] </ref> and AND/OR parallel systems [56, 25]. This work is all aimed at alleviating such dependences, and as such can be seen as more exotic flavors of scalar expansion [102], and variable renaming [31] and localization [103].
Reference: [99] <author> R. Warren, M. Hermenegildo, and S.K. Debray. </author> <title> On the practicality of global flow analysis of logic programs. </title> <booktitle> In Proceedings of the Fifth International Conference and Symposium on Logic Programming, </booktitle> <pages> pages 684-694, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: This is conservative in the sense that aliases may be inferred where none actually exists. A number of methods have been presented for inferring aliases from Prolog programs <ref> [53, 99] </ref>. Almost any of these methods can be used to infer the sort of information we need for determining data dependences. To show how such information can be inferred we present here a simple approach for inferring aliases.
Reference: [100] <author> William H. Winsborough. </author> <title> Path-dependent reachability analysis for multiple specialization. </title> <booktitle> In Proceedings of the 1989 North American Conference on Logic Programming, </booktitle> <pages> pages 133-153, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: There is, of course, the possibility of pathological programs for which some predicates may have an exponential number of calling modes, but these do not appear common. Before finally processing the flow graphs for a procedure we use procedure specialization <ref> [100] </ref> to open more opportunities for transformation. Each procedure P to be specialized has a set of call modes C.
Reference: [101] <author> Michael Wise. </author> <title> Prolog Multiprocessors. </title> <publisher> Prentice-Hall International Publishers, </publisher> <year> 1986. </year>
Reference-contexts: A second form of parallelism, namely AND parallelism, arises from the existence of independent sub-problems all of which must be solved to solve one larger problem. To address this sort of problem there have been several proposals for AND parallel execution of Prolog <ref> [26, 58, 101, 20, 36, 47] </ref>. Most of these systems do not treat the full Prolog language (with notable exceptions [36, 47]). The side effect predicates are the most common part omitted. <p> Some have used static data dependence graphs <ref> [58, 101, 20] </ref>, others partially dynamic dependence graphs [36, 47], and still others totally dynamic dependence graphs [26]. Kale [57] also notes that in rare circumstances it may be advantageous to allow concurrent execution of dependent literals. <p> Another use of the method we propose is to compute exact dependences to test the effectiveness of dependence tests. There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static <ref> [58, 20, 101] </ref> to partly dynamic (conditional) [36, 47] to completely dynamic [26]. Kale [57] notes that in rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [84] supports only independent AND parallelism. <p> Kale [57] notes that in rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [84] supports only independent AND parallelism. Epilog <ref> [101] </ref> also permits dependent AND parallelism, but provides a primitive (CAND) to curtail it. The model we have developed includes dynamic, independent AND parallelism, with a strict sequential ordering on dependent literals. <p> It could also begin with an AND/OR parallel system such as such as the Reduce-OR Process Model [84], the AND/OR Process Model [26], or Epilog <ref> [101] </ref>. Any of these models can be extended fairly simply with the necessary structures for executing parallel loops. Because of this the results presented in Chapter Eight give performance numbers for both OR and AND/OR parallel implementations augmented with parallel loops.
Reference: [102] <author> Michael J. Wolfe. </author> <title> Techniques for improving the inherent parallelism in programs. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> July </month> <year> 1978. </year> <note> Report No. UIUCDCS-R-78-929. </note>
Reference-contexts: This work is all aimed at alleviating such dependences, and as such can be seen as more exotic flavors of scalar expansion <ref> [102] </ref>, and variable renaming [31] and localization [103]. The OR tree fragment in Figure 1.13 illustrates the dependences in a simple program. There is a branch point in the OR tree and both subtrees bind the variable V that is in an environment higher in the tree. <p> The next two sections describe the two parts of constructing an iteration 111 space for a do loop, namely identifying induction variables and termination conditions. The fifth section describes the treatment of live variables in recursion splitting and the use of scalar expansion <ref> [102] </ref>. The sixth section finally describes converting the identified regions of the procedure into do loop forms. <p> Since all the instances of variables need to be preserved across the two loops, a structure for representing the individual instances of variables is needed. For this purpose we extend the dialect of Prolog that is used as the target of our transformations to include expanded scalars <ref> [102] </ref>, which are arrays of scalar variable instances.

References-found: 102

