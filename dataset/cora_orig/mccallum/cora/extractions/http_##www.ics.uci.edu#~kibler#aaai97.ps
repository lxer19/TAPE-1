URL: http://www.ics.uci.edu/~kibler/aaai97.ps
Refering-URL: http://www.ics.uci.edu/~kibler/
Root-URL: 
Email: fpdatta, kiblerg@ics.uci.edu  
Title: Symbolic Nearest Mean Classifiers  
Author: Piew Datta and Dennis Kibler 
Address: Irvine, CA 92717  
Affiliation: Department of Information and Computer Science University of California  
Abstract: The minimum-distance classifier summarizes each class with a prototype and then uses a nearest neighbor approach for classification. Three drawbacks of the minimum-distance classifier are its inability to work with symbolic attributes, weigh attributes, and learn more than a single prototype for each class. The proposed solutions to these problems include defining the mean for symbolic attributes, providing a weighting metric, and learning several possible prototypes for each class. The learning algorithm developed to tackle these problems, SNMC, increases classification accuracy by 10% over the original minimum-distance classifier and has a higher average generalization accuracy than both C4.5 and PEBLS on 20 domains from the UCI data repository. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., Kibler, D. & Albert M. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine learning, </journal> <volume> volume 6, </volume> <pages> pp 37-66. </pages> <address> Boston, MA: </address> <publisher> Kluwer Publishers. </publisher>
Reference-contexts: Introduction The instance-based <ref> (Aha, Kibler, & Albert, 1991) </ref> or nearest neighbor learning method (Duda & Hart, 1973) is a traditional statistical pattern recognition method for classifying unseen examples. These methods store the training set of examples.
Reference: <author> Auer, P., Holte, R. & Maass, W. </author> <year> (1995). </year> <title> Theory and applications of agnostic PAC-learning with small decision trees. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <address> Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Datta, P. and Kibler, D. </author> <year> (1995). </year> <title> Learning Prototypical Concept Descriptions. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <address> Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cheeseman, P. & Stutz, P. </author> <year> (1996). </year> <title> Bayesian classifi cation (AutoClass): theory and results, in Advances in Knowledge Discovery and Data Mining, </title> <type> U.M. </type>

Reference: <author> Dougherty, J., Kohavi, R., Sahami, M. </author> <year> (1995). </year> <title> Supervised and Unsupervised Discretization of Continuous Features. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <address> Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Duda, R., and Hart P. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: Introduction The instance-based (Aha, Kibler, & Albert, 1991) or nearest neighbor learning method <ref> (Duda & Hart, 1973) </ref> is a traditional statistical pattern recognition method for classifying unseen examples. These methods store the training set of examples. To classify an example from the test set, the closest example in the training set is found and the class of that example is predicted. <p> The two main parameters to this algorithm are the distance metric for finding the closest cluster and k, the number of clusters to create. Euclidean distance is the most commonly applied distance measure, although other distance metrics do exist such as city block distance and Mahalanobis distance <ref> (Duda & Hart 1973) </ref>. Finding k, on the other hand, is a more serious open problem. Duda & Hart, Smyth (1996), Cheese Randomly initialize k clusters. Repeat Compute cluster means.
Reference: <author> Iba, W. & Langley, P. </author> <year> (1992). </year> <title> Induction of one-level decision trees. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning. </booktitle> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1995). </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> MacQueen, J. </author> <year> (1967). </year> <title> Some methods for classification and analysis of multivariate observations. </title> <booktitle> In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability. </booktitle> <volume> Vol. 1. </volume> <publisher> University of Cali-fornia Press, </publisher> <address> Berkeley. </address>
Reference: <author> Murphy, P. and Aha, D. </author> <year> (1994). </year> <title> UCI repository of ma chine learning databases [machine readable data repos itory]. </title> <type> Tech. Rep., </type> <institution> University of California, Irvine. </institution>
Reference-contexts: Table 1 shows the results of 30 runs of this experiment randomly choosing two-thirds of the examples (without replacement) as training examples and the remainder as test examples 2 on 11 domains from the UCI Data Repository <ref> (Murphy & Aha 1994) </ref> containing either a mixture of symbolic and real-value attributes or only real-value attributes. Both the training and test sets retain the original class probabilities.
Reference: <author> Niblack, W. </author> <year> (1986). </year> <title> An Introduction to Digital Image Processing. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: To reduce the number of training examples stored, some researchers ( Duda & Hart, 1973; Aha, Kibler & Albert, 1991; Zhang, 1992; Skalak, 1994; Datta & Kibler, 1995) have stored only a subset of the examples or a generalization of the examples. Duda & Hart (1973) and others <ref> (Niblack, 1986) </ref> discuss a variation of the nearest neighbor classifier termed the minimum-distance classifier. This classifier works similarly to the nearest neighbor classifier except that instead of storing each training example, the mean for each class is stored as a prototype.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Schmidt, W. Levelt, D. & Duin, R. </author> <year> (1994). </year> <title> An experimental comparison of neural classifiers with 'traditional' classifiers. Pattern Recognition in Practice IV: Multiple Paradigms, Comparative Studies, and Hybrid Systems, </title> <booktitle> Proceedings of an International Workshop. </booktitle> <address> Vlieland, The Netherlands, </address> <publisher> Elsevier. </publisher>
Reference: <author> Skalak, D. </author> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Smyth, P. </author> <year> (1996). </year> <title> Clustering using Monte Carlo Cross-Validation. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining. </booktitle> <address> Seattle, Washington. </address>
Reference: <author> Stanfill, C., & Waltz, D. </author> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM. </journal> <volume> vol. 29. no 12. </volume> <pages> pp. 1213-1228. </pages>
Reference: <author> Zhang, J. </author> <year> (1992). </year> <title> Selecting typical instances in instance-based learning. </title> <booktitle> In em Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 17

