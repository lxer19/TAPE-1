URL: http://robotics.stanford.edu/~ronnyk/aaai94.ps
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: ronnyk@CS.Stanford.EDU  
Title: Bottom-Up Induction of Oblivious Read-Once Decision Graphs: Strengths and Limitations  
Author: Ron Kohavi 
Address: Stanford, CA 94305  
Affiliation: Computer Science Department Stanford University  
Note: Appears in AAAI-94  
Abstract: We report improvements to HOODG, a supervised learning algorithm that induces concepts from labelled instances using oblivious, read-once decision graphs as the underlying hypothesis representation structure. While it is shown that the greedy approach to variable ordering is locally optimal, we also show an inherent limitation of all bottom-up induction algorithms, including HOODG, that construct such decision graphs bottom-up by minimizing the width of levels in the resulting graph. We report our empirical experiments that demonstrate the algorithm's generalization power. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bryant, R. E. </author> <year> 1986. </year> <title> Graph-based algorithms for boolean function manipulation. </title> <journal> IEEE Transactions on Computers C-35(8):677-691. </journal>
Reference-contexts: In (Kohavi 1994), Oblivious read-Once Decision Graphs (OODGs) were introduced as an alternative representation structure for supervised classification learning. OODGs retain most of the advantages of decision trees, while overcoming the two problems mentioned above. OODGs are similar to Ordered Binary Decision Diagrams (OBDDs) <ref> (Bryant 1986) </ref>, which have been used in the engineering community to represent state-graph models of systems, allowing verification of finite-state systems with up to 10 120 states (Burch, Clarke, & Long 1991). We refer the reader to (Kohavi 1994) for a discussion of related work.
Reference: <author> Burch, J. R.; Clarke, E. M.; and Long, D. E. </author> <year> 1991. </year> <title> Representing circuits more efficiently in symbolic model checking. </title> <booktitle> In Proceedings of the 28th ACM/IEEE Design Automation Conference, </booktitle> <pages> 403-407. </pages>
Reference-contexts: OODGs are similar to Ordered Binary Decision Diagrams (OBDDs) (Bryant 1986), which have been used in the engineering community to represent state-graph models of systems, allowing verification of finite-state systems with up to 10 120 states <ref> (Burch, Clarke, & Long 1991) </ref>. We refer the reader to (Kohavi 1994) for a discussion of related work. OODGs have a different bias from that of decision trees, and thus some concepts that are hard to represent as trees are easy to represent as OODGs, and vice-versa.
Reference: <author> Friedman, S. J., and Suppowit, K. J. </author> <year> 1990. </year> <title> Finding the optimal variable ordering for binary decision diagrams. </title> <journal> IEEE Transactions on Computers 39(5) </journal> <pages> 710-713. </pages>
Reference-contexts: Ordering the Variables Given the full instance space, it is possible to find the optimal ordering using dynamic programming, by checking 2 n different orderings <ref> (Friedman & Suppowit 1990) </ref>. Since this is impractical in practice, our implementation greedily select the variable that yields the smallest width at the next level, excluding constant nodes. We break ties in favor of minimizing the number of edges.
Reference: <author> Fujita, M.; Matsunaga, Y.; and Kakuda, T. </author> <year> 1991. </year> <title> On variable ordering of binary decision diagrams for the application of multilevel logic synthesis. </title> <booktitle> In Proceedings of the European Conference on Design Automation, </booktitle> <pages> 50-54. </pages> <publisher> IEEE Computing Press. </publisher>
Reference-contexts: The following theorem shows that in a bottom-up construction, where the full instance space is available, the above heuristic is locally optimal and cannot be improved by a single exchange of neighboring vari ables. Such exchanges were done to improve the size of OBDDs when created top-down in <ref> (Fujita, Matsunaga, & Kakuda 1991) </ref>. Theorem 1 If during a bottom-up construction, the variable that creates the smallest width at each level is chosen, no exchange of two adjacent variables will improve the size of the OODG or OBDD.
Reference: <author> John, G.; Kohavi, R.; and Pfleger, K. </author> <year> 1994. </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> 121-129. </pages> <note> Morgan Kaufmann. Available by anonymous ftp from: starry.Stanford.EDU:pub/ronnyk/ml94.ps. </note>
Reference-contexts: The large time requirement in the vote database is due to the large correlations between the attributes, which make many attributes weakly relevant <ref> (John, Kohavi, & Pfleger 1994) </ref>, thus forcing a two-ply lookahead. HOODG does much better on all the artificial data sets, and about the same on the real datasets, except for breast-cancer in the original encoding, where we noted that its myopic hill-climbing forces a ten-way split at the root variable. <p> Tie-breaking heuristics were added to the original HOODG algorithm, improving the accuracy and learning rate. Although limited in its myopic view, the HOODG algorithm performs well, especially if the concept is graph-like (e.g., Monk 1, Monk 2), or has totally irrelevant attributes <ref> (John, Kohavi, & Pfleger 1994) </ref>. The algorithm clearly outperforms other algorithms on the artificial domains tested, and is comparable on real domains, even though it currently does not deal with noise and probably overfits the data.
Reference: <author> Kohavi, R. </author> <year> 1994. </year> <title> Bottom-up induction of oblivious, read-once decision graphs. In Proceedings of the Eu-ropean Conference on Machine Learning. </title> <note> Available by anonymous ftp from starry.Stanford.EDU:pub/ronnyk/euroML94.ps. </note>
Reference-contexts: Both problems reduce the number of instances at lower nodes in the tree|instances needed for statistical significance of tests performed during the tree construction process. In <ref> (Kohavi 1994) </ref>, Oblivious read-Once Decision Graphs (OODGs) were introduced as an alternative representation structure for supervised classification learning. OODGs retain most of the advantages of decision trees, while overcoming the two problems mentioned above. <p> OODGs are similar to Ordered Binary Decision Diagrams (OBDDs) (Bryant 1986), which have been used in the engineering community to represent state-graph models of systems, allowing verification of finite-state systems with up to 10 120 states (Burch, Clarke, & Long 1991). We refer the reader to <ref> (Kohavi 1994) </ref> for a discussion of related work. OODGs have a different bias from that of decision trees, and thus some concepts that are hard to represent as trees are easy to represent as OODGs, and vice-versa. <p> In this paper, we investigate the strength and limitations of inducing OODGs bottom-up using HOODG, a greedy hill-climbing algorithm for inducing OODGs, previously introduced in <ref> (Kohavi 1994) </ref>. We show that on the one hand, a greedy choice of variables always yields an ordering that is locally optimal for fully specified functions and cannot be improved by a single exchange of adjacent variables (a technique sometimes used in the engineering community). <p> We refer the reader to <ref> (Kohavi 1994) </ref> for a more detailed description of these properties. Bottom-Up Construction of OODGs In this section we present an algorithm for constructing a reduced OODG given the full (labelled) instance space. The algorithm is recursive and nondeterministic. <p> If all nodes are constant nodes (the attribute is deemed irrelevant), we do another lookahead step and pick the variable that maximizes the number of irrelevant attributes at the next level. This heuristic is different from the original one proposed in <ref> (Kohavi 1994) </ref>. The main difference is the fact that the minimization is done excluding constant nodes. By ignoring constant nodes, the algorithm scales better when the target concept is decomposable. <p> Our heuristic breaks ties in favor of projection set that has the most instances differing by at most one bit, and given equality, breaks ties in favor of adding the minimum number of new destinations (again, least commitment). These tie-breakers were added to the original heuristic, presented in <ref> (Kohavi 1994) </ref>, after it was noted that there are many cases where they are needed. We have tried different tie-breaking heuristics, and many reasonable heuristics perform better than the arbitrary tie-breaking originally used. <p> The large time requirement in the vote database is due to the large correlations between the attributes, which make many attributes weakly relevant <ref> (John, Kohavi, & Pfleger 1994) </ref>, thus forcing a two-ply lookahead. HOODG does much better on all the artificial data sets, and about the same on the real datasets, except for breast-cancer in the original encoding, where we noted that its myopic hill-climbing forces a ten-way split at the root variable. <p> Tie-breaking heuristics were added to the original HOODG algorithm, improving the accuracy and learning rate. Although limited in its myopic view, the HOODG algorithm performs well, especially if the concept is graph-like (e.g., Monk 1, Monk 2), or has totally irrelevant attributes <ref> (John, Kohavi, & Pfleger 1994) </ref>. The algorithm clearly outperforms other algorithms on the artificial domains tested, and is comparable on real domains, even though it currently does not deal with noise and probably overfits the data.
Reference: <author> Lund, C., and Yannakakis, M. </author> <year> 1993. </year> <title> On the hardness of approximating minimization problems. </title> <booktitle> In ACM Symposium on Theory of Computing. </booktitle>
Reference-contexts: This is a strong negative result, since it is known that the chromatic number of a graph cannot be approximated to within any constant multiplicative factor unless P=NP <ref> (Lund & Yannakakis 1993) </ref>. Note that this result applies to any algorithm that attempts to minimize the width of an OODG at a given level, whether done incrementally as in HOODG, or otherwise.
Reference: <author> Moret, B. M. E. </author> <year> 1982. </year> <title> Decision trees and diagrams. </title> <journal> ACM Computing Surveys 14(4) </journal> <pages> 593-623. </pages>
Reference: <author> Oliver, J. J. </author> <year> 1993. </year> <title> Decision graphs | an extension of decision trees. </title> <booktitle> In Proceedings of the fourth International workshop on Artificial Intelligence and Statistics, </booktitle> <pages> 343-350. </pages>
Reference-contexts: Experimental Results We now turn to a series of experiments that attempt to evaluate the performance of the HOODG algorithm. Table 1 shows the accuracy results 1 for ID3, C4.5 (Quinlan 1993), Oliver's decision graph algorithm, DGRAPH <ref> (Oliver 1993) </ref>, and HOODG, on the following datasets that we generated or retrieved from (?): Monk 1,2 In (Thrun et al. 1991), 24 authors compared 25 machine learning algorithms on problems called the monks problems. In this domain there are six attributes with discrete values.
Reference: <author> Pagallo, G., and Haussler, D. </author> <year> 1990. </year> <title> Boolean feature discovery in empirical learning. </title> <booktitle> Machine Learning 5 </booktitle> <pages> 71-99. </pages>
Reference-contexts: Decision trees provide one structure that is commonly constructed using top-down induction techniques (Quinlan 1993; 1986; Moret 1982). However, the tree structure used to represent the hypothesized target concept suffers from some well-known problems, most notably the replication problem and the fragmentation problem <ref> (Pagallo & Haussler 1990) </ref>. The replication problem forces duplication of subtrees in disjunctive concepts such as (A ^ B) _ (C ^ D); the fragmentation problem causes partitioning of the data into fragments, when a high-arity attribute is tested at a node.
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 81-106. </pages> <note> Reprinted in Shavlik and Dietterich (eds.) Readings in Machine Learning. </note>
Reference-contexts: As noted in (Quinlan 1988), this domain is hard for decision trees also, since 2 The multi-way split problem suggests using a different measure, perhaps similar to Quinlan's gain-ratio <ref> (Quinlan 1986) </ref>. the entropy criteria favors a data bit at the root. The learning curves in Figure 3 show the accuracy of the hypotheses versus the training set size for HOODG and ID3.
Reference: <author> Quinlan, J. R. </author> <year> 1988. </year> <title> An empirical comparison of genetic and decision-tree classifiers. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> 135-141. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: There is no advantage in trying to construct a graph or OODG; moreover, one would expect the oblivious restriction to make the task harder for HOODG, since each data bit would need to be tested on a different level. As noted in <ref> (Quinlan 1988) </ref>, this domain is hard for decision trees also, since 2 The multi-way split problem suggests using a different measure, perhaps similar to Quinlan's gain-ratio (Quinlan 1986). the entropy criteria favors a data bit at the root.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> Los Altos, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Experimental Results We now turn to a series of experiments that attempt to evaluate the performance of the HOODG algorithm. Table 1 shows the accuracy results 1 for ID3, C4.5 <ref> (Quinlan 1993) </ref>, Oliver's decision graph algorithm, DGRAPH (Oliver 1993), and HOODG, on the following datasets that we generated or retrieved from (?): Monk 1,2 In (Thrun et al. 1991), 24 authors compared 25 machine learning algorithms on problems called the monks problems.
Reference: <author> Takenaga, Y., and Yajima, S. </author> <year> 1993. </year> <title> NP-completeness of minimum binary decision diagram identification. </title> <type> Technical Report COMP 92-99, IEICE. </type>
Reference-contexts: We have tried different tie-breaking heuristics, and many reasonable heuristics perform better than the arbitrary tie-breaking originally used. As the following results show, it is unlikely that an algorithm finding the smallest consistent OODG will be found, even for a given ordering. In <ref> (Takenaga & Yajima 1993) </ref>, it was shown that identifying whether there exists an OBDD with k nodes that is consistent with labelled instances is NP-complete, and this result applies to OODGs too.
Reference: <author> Thrun, S.; Bala, J.; Bloedorn, E.; Bratko, I.; Cestnik, B.; Cheng, J.; De Jong, K.; Dzeroski, S.; Fahlman, S.; Fisher, D.; Hamann, R.; Kaufman, K.; Keller, S.; Kononenko, I.; Kreuziger, J.; Michalski, R.; Mitchell, T.; Pachowicz, P.; Reich, Y.; Vafaie, H.; de Weldel, W. V.; Wenzel, W.; Wnek, J.; and Zhang, J. </author> <year> 1991. </year> <title> The monk's problems: A performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Table 1 shows the accuracy results 1 for ID3, C4.5 (Quinlan 1993), Oliver's decision graph algorithm, DGRAPH (Oliver 1993), and HOODG, on the following datasets that we generated or retrieved from (?): Monk 1,2 In <ref> (Thrun et al. 1991) </ref>, 24 authors compared 25 machine learning algorithms on problems called the monks problems. In this domain there are six attributes with discrete values. The Monk 1 problem has a single training set, but it is too easy.
References-found: 15

