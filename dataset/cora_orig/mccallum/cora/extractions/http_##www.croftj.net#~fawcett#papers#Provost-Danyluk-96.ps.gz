URL: http://www.croftj.net/~fawcett/papers/Provost-Danyluk-96.ps.gz
Refering-URL: http://ai.iit.nrc.ca/bibliographies/cost-sensitive.html
Root-URL: 
Email: foster@nynexst.com  andrea@cs.williams.edu  
Title: A Study of Complications in Real-world Machine Learning  
Author: Foster John Provost Andrea Pohoreckyj Danyluk Williams 
Date: January 19, 1996  
Address: 400 Westchester Ave. White Plains, NY 10604  Williamstown, MA 01267  
Affiliation: NYNEX Science Technology, Inc.  College Department of Computer Science  
Abstract: This paper is a case study of a long-term project studying the application of machine learning to the problem of dispatching technicians to fix faults in the local loop of a telephone network. The bottom line of the study is that simple machine learning techniques can increase the effectiveness of local-loop dispatch. However, presenting a convincing argument to that effect has been far from simple. The case study shows how almost all of the complexity in this application of machine learning is related to issues that are peripheral in the machine learning literature. In particular, we had to gather data from multiple sources, use domain knowledge to clean up data, and deal with the fact that an evaluation based on classification accuracy is relatively meaningless without an analysis of cost effectiveness. Our view is that application studies should be helpful in guiding future research. Therefore we conclude by outlining useful directions suggested by our experience on this long-term project. Keywords: ML application, cost-sensitivity 
Abstract-found: 1
Intro-found: 1
Reference: <author> Beers, Y. </author> <year> (1957). </year> <title> An Introduction to the Theory of Error. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company. </publisher>
Reference: <author> Danyluk, A. P. & Provost, F. J. </author> <year> (1993a). </year> <title> Small Disjuncts in Action: Learning to Diagnose Errors in the Local Loop of the Telephone Network. </title> <booktitle> In Proceedings of 13 the Tenth International Conference on Machine Learning, </booktitle> <pages> 81-88. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Several approaches to the problem of tuning MAX have been investigated: (1) The application of inductive learning to generate completely new knowledge bases for specific locations <ref> (Danyluk & Provost, 1993a) </ref>. (2) The application of analytic and inductive learning to modify the existing knowledge base for specific locations (Pazzani & Brunk, 1993; Goodman, 1989). (3) The application of techniques to perform parameter tuning (Merz, et al., 1996). This paper discusses the first of these only. <p> Throughout this paper, we use "the default" as a shorthand for "classifying all cases identically using the most frequently occurring class." The PDER is important because different data sets have different numbers of classes; thus comparisons of absolute error rate are affected. As reported previously <ref> (Danyluk & Provost, 1993a, b) </ref>, in order to evaluate the potential of machine learning in this domain, we used the existing MAX expert system to create a "clean" data set from which to learn. <p> The results of modeling MAX suggest that 400 examples may be too few for effective learning. Analysis of the concept description learned by MAX explains why many examples are needed: very small disjuncts comprise a large portion of the concept description <ref> (Danyluk & Provost, 1993a) </ref>. However, the relatively large increase in PDER with learning is promising with respect to the potential for machine learning to model the behavior of human experts.
Reference: <author> Danyluk, A. P. & Provost, F. J. </author> <year> (1993b). </year> <title> Adaptive Expert Systems: Applying Machine Learning to NYNEX MAX, </title> <booktitle> in Working Notes of the AAAI-93 Workshop on AI in Service and Support: Bridging the Gap Between Research and Applications. </booktitle>
Reference: <author> Dattatreya, G. R., and Kanal, L. N. </author> <year> (1985). </year> <title> Decision Trees in Pattern Recognition. </title>
Reference-contexts: Interested researchers can begin by referencing work in statistics (Duda & Hart, 1973), decision analysis (Henrion, et al., 1991; Keeney, 1982; Weinstein & Fineberg, 1980), and pattern recognition <ref> (Dattatreya & Kanal, 1985) </ref>. Webb (1996) gives an overview of recent work on cost-sensitive machine learning. Acknowledgements Tom Fawcett helped in developing new mappings from field technicians' resolution codes to dispatches. Kim Tabtiang performed the runs reported in Sections 3,4, and 5.
Reference: <editor> In L. N. Kanal and A. Rosenfeld (ed.), </editor> <booktitle> Progress in Pattern Recognition 2, p. </booktitle> <pages> 189-239. </pages>
Reference: <institution> Elsevier Science Publishers B.V. (North-Holland). </institution>
Reference: <author> Duda, R. O., and Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: In light of the the results of Holte (1993) and Pazzani, et al., (1994), we postulate that a cost-sensitive decision stump is a relatively strong "straw-man" for cost-sensitive machine learning work. Interested researchers can begin by referencing work in statistics <ref> (Duda & Hart, 1973) </ref>, decision analysis (Henrion, et al., 1991; Keeney, 1982; Weinstein & Fineberg, 1980), and pattern recognition (Dattatreya & Kanal, 1985). Webb (1996) gives an overview of recent work on cost-sensitive machine learning. Acknowledgements Tom Fawcett helped in developing new mappings from field technicians' resolution codes to dispatches.
Reference: <author> Goodman, R. M. & Smyth, P. </author> <title> (1989) The Induction of Probabilistic Rule Sets the ITRULE Algorithm. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> 129-132. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Henrion, M., Breese, J. S., and Horowitz, E. J. </author> <year> (1991). </year> <title> Decision Analysis and Expert Systems. </title> <journal> AI Magazine, </journal> <volume> 12, </volume> <pages> p. 64-91. </pages>
Reference: <author> Holte, R. C. </author> <title> (1993) Very Simple Classification Rules Perform Well on Most Commonly Used Datasets, </title> <booktitle> Machine Learning:11:1, </booktitle> <pages> 63-90. </pages>
Reference-contexts: Vercode is essentially a summary of the electrical readings produced by MLT. MAX was originally designed with the goal of using additional information to increase the performance of vercode alone. As the results in Table 2 show, the decision stumps <ref> (Holte, 1993) </ref> learned by C4.5 on the field data perform much better than those learned with larger feature sets. 4 Cleaning up the Data By analyzing the different trouble resolutions reported by the field technicians, it becomes clear that machine learning programs would have a difficult time modeling the data.
Reference: <author> Keeney, R. L. </author> <year> (1982). </year> <title> Decision Analysis: An Overview. </title> <journal> Operations Research, </journal> <volume> 30, </volume> <pages> p. 803-838. </pages>
Reference: <author> Merz, C. J., Pazzani, M., & Danyluk, A. P. </author> <year> (1996). </year> <title> Tuning Numeric Parameters to Troubleshoot a Telephone Network Local Loop. </title> <note> IEEE Expert (to appear). </note>
Reference-contexts: The application of inductive learning to generate completely new knowledge bases for specific locations (Danyluk & Provost, 1993a). (2) The application of analytic and inductive learning to modify the existing knowledge base for specific locations (Pazzani & Brunk, 1993; Goodman, 1989). (3) The application of techniques to perform parameter tuning <ref> (Merz, et al., 1996) </ref>. This paper discusses the first of these only. Unless stated otherwise, all results reported in this paper were generated using C4.5 (Quinlan, 1992) with default settings. 3 Results given are after pruning. Numbers of test examples are given with each set of runs.
Reference: <author> Lee, Y. </author> <year> (1995). </year> <title> Learning a Robust Rule Set. </title> <type> Ph.D. Thesis. </type> <institution> Department of Computer Science, University of Pittsburgh. </institution>
Reference: <author> Pazzani, M. J. & Brunk, C. </author> <year> (1993). </year> <title> Finding Accurate Frontiers: A Knowledge-Intensive Approach to Relational Learning. </title> <booktitle> In Proceedings of AAAI-93, </booktitle> <pages> 328-334. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., Brunk C. </author> <year> (1994). </year> <title> Reducing Misclassification Costs. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> 217-225. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Provost, F. </author> <year> (1994). </year> <title> Goal-Directed Inductive Learning: Trading off Accuracy for Reduced Error Cost. </title> <booktitle> In Working Notes of the AAAI-94 Workshop on Goal-Driven Learning, </booktitle> <pages> 94-101. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1992). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: This paper discusses the first of these only. Unless stated otherwise, all results reported in this paper were generated using C4.5 <ref> (Quinlan, 1992) </ref> with default settings. 3 Results given are after pruning. Numbers of test examples are given with each set of runs. All results reported have been averaged over 10 runs with training and test sets chosen randomly.
Reference: <author> Rabinowitz, H., Flamholz, J., Wolin, E., & Euchner, J. </author> <note> (1991) NYNEX MAX: </note>
Reference-contexts: Our view is that application studies help to guide future research. Therefore we conclude by outlining useful directions suggested by our experience on this long-term project. 2 NYNEX MAX and Machine Learning MAX <ref> (Rabinowitz, et al., 1991) </ref> is an expert system developed by NYNEX Science and Technology 1 for the purpose of troubleshooting customer-reported telephone problems. MAX deals specifically with problems in the local loop, the part of the telephone network between the central office and the customer's premises.
References-found: 18

