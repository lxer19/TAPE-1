URL: file://ftp.cs.ucsd.edu/pub/baden/tr/vipp.ps.gz
Refering-URL: http://www.cs.ucsd.edu/groups/hpcl/scg/tr.html
Root-URL: http://www.cs.ucsd.edu
Title: VIPP: Visual Interactive Parallel Performance Tool  
Author: Edmund M. Johnson Scott R. Kohn Scott B. Baden, 
Keyword: Multiprocessors, Load Balancing, Performance Visualization, Particle Methods,  
Note: Education in Parallel Processing.  
Address: Triangle Park NC 27709-2195  La Jolla, CA 92093-0340  9500 Gilman Drive San Diego, CA 92093-0114 USA  
Affiliation: International Business Machines Corporation Research  Chemistry Department, 0340 University of California, San Diego  Department of Computer Science and Engineering, 0114 University of California, San Diego  
Email: Email address: baden@cs.ucsd.edu  
Phone: Phone: (619) 534-8861  
Degree: Assistant Professor  
Abstract: VIPP is an instructional tool for exploring performance tradeoffs that arise when load balancing various particle methods on distributed memory MIMD computers. VIPP enables its user to interactively visualize factors affecting performance including: granularity, geometry, and the relative costs of communication and computation. VIPP employs trace driven, application-level simulations in lieu of low level machine-level behavioral simulations. While such simulations ignore certain performance factors|such as the cache and the processor interconnect|they are ideal for instructional use: they require only a working serial program, and they may be run inexpensively on a low-cost workstation. We present some experiments to illustrate VIPP's capabilities. In addition to use as an instructional tool, VIPP's ability to perform qualitative performance prediction may be helpful in guiding parallel implementations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. B. Baden, </author> <title> Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12 (1991), </volume> <pages> pp. 145-157. </pages>
Reference-contexts: Of the three communication activities, only the first appears to have a significant cost <ref> [17, 11, 1] </ref>; the reamining activities are ignored by VIPP. Partitioning is performed periodically according to a user-specified interval. <p> The serial model is useful in computationally-intensive applications where there may be no advantage to parallelizing the partitioner <ref> [1] </ref>. We implicitly assume that all processors devoted to the computation run at equal speed. Thus, our execution model does not apply to workstation clusters in which the processors are unevely loaded, or where the processor hardware is heterogeneous.
Reference: [2] <author> S. B. Baden and S. R. Kohn, </author> <title> A comparison of load balancing strategies for particle methods running on mimd multiprocessors, </title> <booktitle> in Proc. Fifth SIAM Conf. on Parallel Processing for Sci. </booktitle> <institution> Comput., Houston, TX, </institution> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: Because of the fine-grained nature of the interleaving process, partitions tend to require a significant amount of communication relative to the amount of computation (the surface-to-volume effect) <ref> [2] </ref>. VIPP does not support blocked interleaved decompositions, e.g. HPF-style BLOCK CYCLIC [8], in which the points in the template correspond to a rectangular block of cells.
Reference: [3] <author> M. J. Berger and S. H. Bokhari, </author> <title> A partitioning strategy for nonuniform problems on multiprocessors, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36 (1987), </volume> <pages> pp. 570-580. </pages>
Reference-contexts: 1 Introduction The load balancing problem arises in a number of irregular applications including particle methods [5], finite element methods [19], and adaptive mesh methods <ref> [3] </ref>. In practice, the task of balancing workloads entail entails juggling a diversity of data- and application-dependent performance tradeoffs. We have developed VIPP, an instructional tool for evaluating the complex tradeoffs associated with parallelizing certain types of particle methods. <p> The following six sections discuss these partitioning strategies in more detail. 3.1 Orthogonal Recursive Bisection Orthogonal recursive bisection <ref> [3] </ref> splits the partitioning mesh into two subregions carrying approximately the same amount of work. It then recursively applies itself to the two subregions until the desired number of partitions has been rendered.
Reference: [4] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl, Proteus: </author> <title> A high-performance parallel-architecture simulator, </title> <booktitle> in Proceedings of the ACM SIGMETRICS and Performance '92 Conference, </booktitle> <month> May </month> <year> 1992. </year> <note> A longer form apears as MIT technical report MIT/LCS/TR-516. </note>
Reference-contexts: VIPP permits the user to vary factors such as partitioning strategy and frequency, granularity, number of processors, and system-dependent communication costs. Based on these values, VIPP estimates and displays workload balance, communication overhead, domain partitioning, and parallel efficiency. As compared with VIPP, hardware level simulators such as Proteus <ref> [4] </ref> provide detailed performance information. Other performance measurement tools include Paradyn [14], and Pablo [18]. These tools enable the user to measure running programs on parallel computers and are general purpose. Paradyn is able to attribute performance measurements to specific higher-level language constructs or to specific data structures.
Reference: [5] <author> T. W. Clark, R. V. Hanxleden, J. A. McCammon, and L. R. Scott, </author> <title> Parallelization using spatial decomposition for molecular dynamics, </title> <type> Tech. Rep. </type> <institution> CRPC-TR93356-S, Center for Research on Parallel Computation, Rice University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: 1 Introduction The load balancing problem arises in a number of irregular applications including particle methods <ref> [5] </ref>, finite element methods [19], and adaptive mesh methods [3]. In practice, the task of balancing workloads entail entails juggling a diversity of data- and application-dependent performance tradeoffs. We have developed VIPP, an instructional tool for evaluating the complex tradeoffs associated with parallelizing certain types of particle methods.
Reference: [6] <author> B. Falsafi, A. R. Lebeck, S. K. Reinhardt, I. Schoinas, M. D. Hill, J. R. Larus, A. Rogers, and D. A. Wood, </author> <title> Application-specific protocols for user-level shared memory, </title> <booktitle> in Proc. Supercomputing '94, </booktitle> <address> Wash., D.C., </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: See, for example, results with the Stanford Dash [13] and application-specific cache protocols with the Wisconsin Wind Tunnel <ref> [6] </ref>. Such techniques expose the message passing activity used to maintain cache coherence. 4 Nevertheless, there is no substitute for comparing VIPP's predictions against actual observations of a working parallelized application. 6 3 Partitioning the Problem VIPP enables the programmer to inspect the data partitioning scheme visually.
Reference: [7] <author> G. C. Fox, </author> <title> Hardware and software architectures for irregular problem architectures, in Unstructured Scientific Computation on Scalable Multiprocessors, </title> <editor> P. Mehrotra, J. Saltz, and R. Voigt, eds., </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: VIPP models the parallel execution of our application on such a machine by partitioning the mesh into subproblems and executing the subproblems one per processor in loosely asynchronous fashion <ref> [7] </ref>. VIPP determines the parallel running time of the application by estimating the time spent in local and global computation, and adding this to an estimate of communication and load balancing times.
Reference: [8] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification, </title> <institution> Rice University, Houston, Texas, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Because of the fine-grained nature of the interleaving process, partitions tend to require a significant amount of communication relative to the amount of computation (the surface-to-volume effect) [2]. VIPP does not support blocked interleaved decompositions, e.g. HPF-style BLOCK CYCLIC <ref> [8] </ref>, in which the points in the template correspond to a rectangular block of cells. <p> Thus, it is more appropriate for a flat shared memory. 6 3.6 Uniform Partitioning Uniform partitioning renders HPF-style BLOCK decompositions <ref> [8] </ref>. It is included primarily for completeness, to help demonstrate why non-uniform partitioning is necessary in particle methods. 4 The User Interface In this section we describe the VIPP display window and user interface.
Reference: [9] <author> R. W. Hockney and J. W. Eastwood, </author> <title> Computer Simulation Using Particles, </title> <publisher> McGraw-Hill, </publisher> <year> 1981. </year> <month> 18 </month>
Reference-contexts: VIPP employs trace-driven behavioral simulation to model the performance of various particle methods running on distributed memory parallel computers, including message passing architectures, workstation clusters, and distributed shared memory architectures. It is intended for two-dimensional particle methods that employ a uniform single level mesh to organize the computation <ref> [9] </ref>, but does not support hierarchical representations [23, 20]. VIPP provides a convenient graphical interface that enables the user to interactively browse the load balancing parameter space. <p> The execution model includes data partitioning and communication. The hardware model describes the low-level communication cost. We next discuss each of these three models in detail. 2.1 Application Model VIPP assumes a particle method in which the particles are sorted into a 2-dimensional chaining mesh <ref> [9] </ref> (also called link-cell), and evolve over a period of timesteps. Particles move under mutual interaction according to a problem-dependent force law. Computing the trajectory of the particles involves two steps: a force evaluation step and a time integration step.
Reference: [10] <author> E. Johnson and S. B. Baden, </author> <title> Vipp user and implementer's guide, </title> <type> tech. rep., </type> <institution> University of California - San Diego, CSE 0114, </institution> <address> 9500 Gilman Drive, La Jolla, CA 92092-0114, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: These controls are summarized in Table 2, and described more fully in the accompanying User and Implementor's Guide <ref> [10] </ref>. 5 VIPP in Action VIPP enables the user to explore a parameter space of load balancing alternatives and to note their affect on performance.
Reference: [11] <author> S. R. Kohn, </author> <title> A Parallel Software Infrastructure for Dynamic Block-Irregular Scientific Calculations, </title> <type> PhD thesis, </type> <institution> University of California at San Diego, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Of the three communication activities, only the first appears to have a significant cost <ref> [17, 11, 1] </ref>; the reamining activities are ignored by VIPP. Partitioning is performed periodically according to a user-specified interval.
Reference: [12] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy, </author> <title> The stanford flash multiprocessor, </title> <booktitle> in Proc. 21st International Symposium on Computer Architecture, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994, </year> <pages> pp. 302-313. </pages>
Reference-contexts: Included are conventional message passing MPPs such as the Intel Paragon, certain distributed shared memory architectures such as the Stanford Flash <ref> [12] </ref>, and workstation clusters. VIPP models the parallel execution of our application on such a machine by partitioning the mesh into subproblems and executing the subproblems one per processor in loosely asynchronous fashion [7].
Reference: [13] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam, </author> <title> The Stanford DASH multiprocessor, </title> <journal> IEEE Computer, </journal> <volume> 25 (1992), </volume> <pages> pp. 63-79. </pages>
Reference-contexts: For the variable-length message model, T comm = 1024 + n # For the fixed-length message model, T comm = ~ fi L + 1024 (4) 3 In practice, we may improve performance dramatically by explicitly managing cache locality. See, for example, results with the Stanford Dash <ref> [13] </ref> and application-specific cache protocols with the Wisconsin Wind Tunnel [6].
Reference: [14] <author> B. Miller, M. Callaghan, J. Cargille, J. Hollingsworth, R. Irvin, K. Karavanic, K. Kunchithapadam, and T. Newhall, </author> <title> The paradyn parallel performance measurement tools, </title> <booktitle> IEEE Computer, </booktitle> <month> 28 </month> <year> (1995). </year>
Reference-contexts: Based on these values, VIPP estimates and displays workload balance, communication overhead, domain partitioning, and parallel efficiency. As compared with VIPP, hardware level simulators such as Proteus [4] provide detailed performance information. Other performance measurement tools include Paradyn <ref> [14] </ref>, and Pablo [18]. These tools enable the user to measure running programs on parallel computers and are general purpose. Paradyn is able to attribute performance measurements to specific higher-level language constructs or to specific data structures.
Reference: [15] <author> D. M. Nicol and J. H. Saltz, </author> <title> An analysis of scatter decomposition, </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39 (1990), </volume> <pages> pp. 1337-1345. </pages>
Reference-contexts: Pilkington and Baden have conducted empirical studies [17] in which the highly irregular partitionings were observed to incur modest communication overheads. 3.4 Interleaved or Scattered Decomposition Whereas orthogonal recursive bisection produces coarse grain connected partitions, an interleaved or scattered decomposition method <ref> [15] </ref> renders fine grained partitions. Interleaved decomposition decomposes the partitioning mesh by assigning the bins of the mesh periodically to the processors based on a lexicographical ordering.
Reference: [16] <author> C.-W. Ou, S. Ranka, and G. Fox, </author> <title> Fast mapping and remapping algorithms for irregular and adaptive problems, </title> <booktitle> in 1993 ICPDS, </booktitle> <address> Taipei, Taiwan, </address> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: ISP techniques yield highly balanced partitionings, and have been applied by Singh and Hennessy [20] and Warren and Salmon [23] to hierarchical N-body methods, and by Ou et al. to the Finite Element Method <ref> [16] </ref>. Pilkington and Baden have conducted empirical studies [17] in which the highly irregular partitionings were observed to incur modest communication overheads. 3.4 Interleaved or Scattered Decomposition Whereas orthogonal recursive bisection produces coarse grain connected partitions, an interleaved or scattered decomposition method [15] renders fine grained partitions.
Reference: [17] <author> J. R. Pilkington and S. B. Baden, </author> <title> Partitioning with spacefilling curves, </title> <type> Tech. Rep. </type> <institution> CS94-349, Univ. of Calif., San Diego, Dept. of Computer Science and Engineering, </institution> <year> 1994. </year>
Reference-contexts: Of the three communication activities, only the first appears to have a significant cost <ref> [17, 11, 1] </ref>; the reamining activities are ignored by VIPP. Partitioning is performed periodically according to a user-specified interval. <p> ISP techniques yield highly balanced partitionings, and have been applied by Singh and Hennessy [20] and Warren and Salmon [23] to hierarchical N-body methods, and by Ou et al. to the Finite Element Method [16]. Pilkington and Baden have conducted empirical studies <ref> [17] </ref> in which the highly irregular partitionings were observed to incur modest communication overheads. 3.4 Interleaved or Scattered Decomposition Whereas orthogonal recursive bisection produces coarse grain connected partitions, an interleaved or scattered decomposition method [15] renders fine grained partitions.
Reference: [18] <author> D. A. Reed, R. A. Aydt, R. J. Noe, P. C. Roth, K. A. Shields, B. Schwartz, and L. F. Tavera, </author> <title> Scalable performance analysis: The pablo performance analysis environment, </title> <booktitle> in Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <editor> A. Skjellum, ed., </editor> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year>
Reference-contexts: Based on these values, VIPP estimates and displays workload balance, communication overhead, domain partitioning, and parallel efficiency. As compared with VIPP, hardware level simulators such as Proteus [4] provide detailed performance information. Other performance measurement tools include Paradyn [14], and Pablo <ref> [18] </ref>. These tools enable the user to measure running programs on parallel computers and are general purpose. Paradyn is able to attribute performance measurements to specific higher-level language constructs or to specific data structures. Along these lines Srinivas (Indiana) [21] has developed a tool for visualizing distributed data structures.
Reference: [19] <author> H. D. Simon, </author> <title> Partitioning of unstructured problems for parallel processing, </title> <type> Tech. Rep. </type> <institution> RNR-91-008, NASA Ames Res. Ctr., </institution> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The load balancing problem arises in a number of irregular applications including particle methods [5], finite element methods <ref> [19] </ref>, and adaptive mesh methods [3]. In practice, the task of balancing workloads entail entails juggling a diversity of data- and application-dependent performance tradeoffs. We have developed VIPP, an instructional tool for evaluating the complex tradeoffs associated with parallelizing certain types of particle methods.
Reference: [20] <author> J. P. Singh, C. Holt, J. L. Hennessy, and A. Gupta, </author> <title> A parallel adaptive fast multipole method, </title> <booktitle> in Proc. Supercomputing '93, </booktitle> <year> 1993, </year> <pages> pp. 54-65. </pages>
Reference-contexts: It is intended for two-dimensional particle methods that employ a uniform single level mesh to organize the computation [9], but does not support hierarchical representations <ref> [23, 20] </ref>. VIPP provides a convenient graphical interface that enables the user to interactively browse the load balancing parameter space. <p> We assume that the array is static, so that the mapping phase may be considered as a preprocessing step with time amortized over many subsequent partitions. ISP techniques yield highly balanced partitionings, and have been applied by Singh and Hennessy <ref> [20] </ref> and Warren and Salmon [23] to hierarchical N-body methods, and by Ou et al. to the Finite Element Method [16].
Reference: [21] <author> S. Srinivas, </author> <title> Visualizing distributed data structures, </title> <booktitle> in Proceedings of Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, Virginia, </address> <month> February </month> <year> 1995. </year> <note> Extended version available on the WWW from ftp://ftp.cs.indiana.edu/pub/techreports/TR408.ps.Z. </note>
Reference-contexts: Other performance measurement tools include Paradyn [14], and Pablo [18]. These tools enable the user to measure running programs on parallel computers and are general purpose. Paradyn is able to attribute performance measurements to specific higher-level language constructs or to specific data structures. Along these lines Srinivas (Indiana) <ref> [21] </ref> has developed a tool for visualizing distributed data structures. By comparison to Paradyn and Pablo, VIPP is a domain-specific trace-driven simulator; it does not require a parallelized program or even running hardware, but it is restricted to a specific class of application.
Reference: [22] <author> P. Tang and P. C. Yew, </author> <title> Processor self-scheduling for multiple-nested parallel loops, </title> <booktitle> in Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1986, </year> <pages> pp. 528-535. </pages>
Reference-contexts: Such a strategy trades off communication overhead agains load imbalance by increasing the granularity of the templates, and may be added in a future release of VIPP. 5 3.5 Processor Self-Scheduling Processor self-scheduling <ref> [22] </ref> is included as an "iron man" to establish a theoretical upper bound on how well workloads can be balanced, given that the problem has an inexact solution.
Reference: [23] <author> M. S. Warren and J. K. Salmon, </author> <title> A parallel hashed oct-tree n-body algorithm, </title> <booktitle> in Proc. Supercomputing '93, </booktitle> <year> 1993, </year> <pages> pp. 12-21. 19 </pages>
Reference-contexts: It is intended for two-dimensional particle methods that employ a uniform single level mesh to organize the computation [9], but does not support hierarchical representations <ref> [23, 20] </ref>. VIPP provides a convenient graphical interface that enables the user to interactively browse the load balancing parameter space. <p> We assume that the array is static, so that the mapping phase may be considered as a preprocessing step with time amortized over many subsequent partitions. ISP techniques yield highly balanced partitionings, and have been applied by Singh and Hennessy [20] and Warren and Salmon <ref> [23] </ref> to hierarchical N-body methods, and by Ou et al. to the Finite Element Method [16].
References-found: 23

