URL: http://elib.cs.berkeley.edu/papers/db/database_published.ps.gz
Refering-URL: http://elib.cs.berkeley.edu/papers/db/
Root-URL: 
Title: Special Issue on Query Processing for Non-Standard Data Query Processing in a Parallel Object-Relational Database
Author: .Joseph M. Hellerstein Laura M. Haas, Donald Kossman, Edward L. Wimmers, Jun Yang Surajit Chaudhuri and Luis Gravano 
Note: Bulletin of the Technical Committee on Data Engineering December 1996 Vol. 19 No. 4 Letters Letter from the Editor-in-Chief. .David Lomet 1 Letter from the Special Issue Editor.  Conference and Journal Notices International Conference on Data Engineering back cover  
Abstract: Michael A. Olson, Wei Michael Hong, Michael Ubell, Michael Stonebraker 3 E-ADTs: Turbo-Charging Complex Data . . . . . . Praveen Seshadri, Miron Livny, Raghu Ramakrishnan 11 Storage and Retrieval of Feature Data for a Very Large Online Image Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Chad Carson and Virginia E. Ogle 19 Data Modeling and Querying in the PIQ Image DBMS . . . . . . . . . . Uri Shaft and Raghu Ramakrishnan 28 An Optimizer for Heterogeneous Systems with NonStandard Data and Search Capabilities . . . . . . . . . . . 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Flickner, H. Sawhney, W. Niblack, J. Ashley, et al. </author> <title> Query by image and video content: the QBIC system. </title> <journal> IEEE Computer, </journal> <volume> 28(9) </volume> <pages> 23-32, </pages> <month> Sep </month> <year> 1995. </year>
Reference-contexts: To fully utilize such databases, we must be able to search for images containing interesting objects. Existing image retrieval systems rely on a manual review of each image or on the presumption of a homogeneous collection of similarly-structured images, or they simply search for images using low-level appearance cues <ref> [1, 2, 3, 4, 5] </ref>.
Reference: [2] <author> Jeffrey R. Bach, Charles Fuller, Amarnath Gupta, Arun Hampapur, Bradley Horowitz, Rich Humphrey, Ramesh Jain, and Chiao-fe Shu. </author> <title> The Virage image search engine: An open frameworl for image management. In Storage and Retrieval for Still Image and Video Databases IV. </title> <booktitle> SPIE, </booktitle> <month> Feb </month> <year> 1996. </year>
Reference-contexts: To fully utilize such databases, we must be able to search for images containing interesting objects. Existing image retrieval systems rely on a manual review of each image or on the presumption of a homogeneous collection of similarly-structured images, or they simply search for images using low-level appearance cues <ref> [1, 2, 3, 4, 5] </ref>.
Reference: [3] <author> U. Shaft and R. Ramakrishnan. </author> <title> Content-based queries in image databases. </title> <type> Technical Report 1309, </type> <institution> University of Wisconsin Computer Science Department, </institution> <month> Mar </month> <year> 1996. </year>
Reference-contexts: To fully utilize such databases, we must be able to search for images containing interesting objects. Existing image retrieval systems rely on a manual review of each image or on the presumption of a homogeneous collection of similarly-structured images, or they simply search for images using low-level appearance cues <ref> [1, 2, 3, 4, 5] </ref>.
Reference: [4] <author> Michael Swain and Markus Stricker. </author> <title> The capacity and the sensitivity of color histogram indexing. </title> <type> Technical Report 94-05, </type> <institution> University of Chicago, </institution> <month> Mar </month> <year> 1994. </year>
Reference-contexts: To fully utilize such databases, we must be able to search for images containing interesting objects. Existing image retrieval systems rely on a manual review of each image or on the presumption of a homogeneous collection of similarly-structured images, or they simply search for images using low-level appearance cues <ref> [1, 2, 3, 4, 5] </ref>.
Reference: [5] <author> A.P. Pentland, R.W. Picard, and S. Sclaroff. Photobook: </author> <title> Content-based manipulation of image databases. </title> <journal> Int. Journal of Computer Vision, </journal> <note> to appear. </note>
Reference-contexts: To fully utilize such databases, we must be able to search for images containing interesting objects. Existing image retrieval systems rely on a manual review of each image or on the presumption of a homogeneous collection of similarly-structured images, or they simply search for images using low-level appearance cues <ref> [1, 2, 3, 4, 5] </ref>.
Reference: [6] <author> Virginia E. Ogle and Robert Wilensky. </author> <title> Testbed development for the berkeley digital library project. </title> <journal> D-lib Magazine, </journal> <month> Jul </month> <year> 1996. </year>
Reference-contexts: Bulletin of the IEEE Computer Society Technical Committee on Data Engineering z This work was supported by an NSF Digital Library Grant (IRI 94-11334) and an NSF graduate fellowship for Chad Carson. 19 In support of this research, we have developed a testbed of data <ref> [6] </ref> that as of this writing includes about 65,000 scanned document pages, over 50,000 digital images, and several hundred high-resolution satellite photographs. This data is provided primarily by public agencies in California that desire online access to the data for their own employees or the general public.
Reference: [7] <author> J. Ponce, A. Zisserman, and M. Hebert. </author> <title> Object representation in computer vision|II. </title> <publisher> Springer LNCS no. </publisher> <address> 1144, </address> <year> 1996. </year>
Reference-contexts: However, a few researchers have begun to work on more general object recognition <ref> [7] </ref>. The current focus of our vision research is to identify objects in pictures: animals, trees, flowers, buildings, and other kinds of "things" that users might request. This focus is the direct result of research by the user needs assessment component of the Digital Library project [8].
Reference: [8] <author> Nancy Van House, Mark H. Butler, Virginia Ogle, and Lisa Schiff. </author> <title> User-centered iterative design for digital libraries: </title> <journal> The cypress experience. D-lib Magazine, </journal> <month> Feb </month> <year> 1996. </year>
Reference-contexts: The current focus of our vision research is to identify objects in pictures: animals, trees, flowers, buildings, and other kinds of "things" that users might request. This focus is the direct result of research by the user needs assessment component of the Digital Library project <ref> [8] </ref>. Interviews were conducted at the California Department of Water Resources (DWR), which is a primary source of the images used in the Digital Library project testbed as well as one of its main users.
Reference: [9] <author> J. Malik, D. Forsyth, M. Fleck, H. Greenspan, T. Leung, C. Carson, S. Belongie, and C. Bregler. </author> <title> Finding objects in image databases by grouping. </title> <booktitle> In International Conference on Image Processing (ICIP-96), special session on Images in Digital Libraries, </booktitle> <month> Sep </month> <year> 1996. </year>
Reference-contexts: Thus we begin with low-level color and texture processing to find coherent regions, and then use the properties of these regions and their relationship with one another to group them at progressively higher levels <ref> [9] </ref>.
Reference: [10] <author> G. Wyszecki and W.S. Stiles. </author> <title> Color science: concepts and methods, quantitative data and formulae. </title> <publisher> Wiley, </publisher> <address> second edition, </address> <year> 1982. </year> <month> 27 </month>
Reference-contexts: We look for the following 13 colors in each image: red, orange, yellow, green, blue-green, light blue, blue, purple, pink, brown, white, gray, and black. We chose these colors because they match human perceptual categories and tend to distinguish interesting objects from their backgrounds <ref> [10] </ref>. We use the following algorithm to find these "colored dots": 1. Map the image's hue, saturation, and value (HSV) channels into the 13 perceptual color channels. 21 2.
References-found: 10

