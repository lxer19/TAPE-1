URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS94-13.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: An Efficient Implementation of the Quorum Consensus Protocol  
Author: M. L. Liu D. Agrawal A. El Abbadi 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Debbie A. Agarwal. </author> <title> Personal Communication on Ethernet Performance Measurements. </title> <institution> Department of Electrical and Computer Engineering, University of California, Santa Barbara, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Our choice of parameters for the database and transactions are chosen so as to be comparable to the existing simulation studies such as [8]. The choice of message propagation time is in keeping with [26] and with a recent study <ref> [1] </ref> conducted on Sun workstations interconnected via ethernet and employing Unix UDP protocol. The scale of disk access time relative to message propagation time is in accordance with [15].
Reference: [2] <author> R. Agrawal, M. Carey, and M. Livny. </author> <title> The Performance of Alternative Strategies for Dealing with Deadlocks in Database Management Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(12):1348-1363, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: In the absence of a full implementation, simulation becomes a viable alternative for a comprehensive performance model. Indeed simulation has been employed in many existing studies on protocols for distributed database systems. <ref> [2, 8, 16, 23] </ref>. In our study, we use a simulation model to study the performance of a number of replica control protocols, including the popular primary copy protocol and the less widely accepted quorum consensus protocol. <p> For our model, the strict two-phase locking algorithm [12, 5] is used. Deadlocks avoidance is implemented by using a timeout interval based on the following heuristic <ref> [2] </ref> : T imeoutInterval = (G) + k fl (G) where (G) is the average lock request response time, (G) is the standard deviation of the response time, and k is a weighting factor.
Reference: [3] <author> P.A. Alsberg and J.D.Day. </author> <title> A principle for resilient sharing of distributed resources. </title> <booktitle> Proceedings of the Second International Conference on Software Engineering, </booktitle> <month> October </month> <year> 1976. </year>
Reference-contexts: Hence the write-all approach is modified instead to write into all copies available to the transaction coordinator; such protocols are termed write-all-available protocols. The most commonly known protocol of this genre is the primary copy protocol <ref> [3, 28] </ref>. The protocol is designed for systems where a primary copy of each data object is located at one site, while replicas of the object are distributed among other nodes.
Reference: [4] <author> P. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: The difficulty lies in keeping the replicas consistent with each other in the face of system failures while at the same time maximizing the data availability. The algorithms which address these problems are called replica control algorithms <ref> [4] </ref>. A transaction T originates at a site, called the coordinator, which forwards the operations contained in T to the appropriate database sites, called the participants, where the data is stored. Each operation either reads from or writes to a data item. <p> In a replicated database, the coordinator translates each Read and Write on a data item x in such a manner that an interleaved execution of transactions is equivalent to a serial execution of those transactions on an unreplicated database. This property, known as one-copy serializability <ref> [4] </ref>, is the correctness criteria for transaction execution on a replicated database system. <p> Similarly, each Write (x) is translated into Write (x B 1 ), ..., Write (x B n ) <ref> [4] </ref>. The composition of the read set R = fA 1 ; :::; A m g and the write set W = fB 1 ; :::; B n g is protocol-dependent.
Reference: [5] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: It also does not require special treatment to recover copies, as outdated copies will not have the latest version number and so will not be read but will be over-written. In spite of these advantages, the QC algorithm has some well-known drawbacks <ref> [5] </ref>. First, it requires multiple reads for each read. Second, it requires a large number of copies (2n +1) to tolerate n site failures, and, finally, all copies of x must be known in advance (to determine the weights and hence quorum configurations). <p> At each server site, this component is responsible for accepting transaction requests from the coordinator site. (b) Concurrency Control Manager: At each server site, this component provides concurrency control for transactions that have been submitted to the site. For our model, the strict two-phase locking algorithm <ref> [12, 5] </ref> is used.
Reference: [6] <author> D. Bersekast and R. Gallager. </author> <title> Data Networks. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference-contexts: On the Internet, for example, hosts constantly exchange messages to monitor topological changes: the maintenance of such information is necessary for the purpose of network routing <ref> [6] </ref>. Furthermore, fault-tolerant systems such as the Tandem systems use a scheme where hosts regularly exchange "I'm alive" messages to monitor the status of each other [7].
Reference: [7] <author> D. </author> <title> Davcevand W.A. Burkard. Consistency and recovery control for replicated files. </title> <booktitle> Proc. 10th ACM Symposium of Operating Systems Principles, </booktitle> <pages> pages 87-96, </pages> <year> 1985. </year>
Reference-contexts: Furthermore, fault-tolerant systems such as the Tandem systems use a scheme where hosts regularly exchange "I'm alive" messages to monitor the status of each other <ref> [7] </ref>. Furthermore, many distributed algorithms, such as the primary copy protocol, also make use of a configuration of communicable hosts which is, in their case, managed by the protocol itself.
Reference: [8] <author> M. Carey and M. Livny. </author> <title> Conflict Detection Tradeoffs for Replicated Data. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 16(4) </volume> <pages> 703-746, </pages> <year> 1991. </year>
Reference-contexts: In the absence of a full implementation, simulation becomes a viable alternative for a comprehensive performance model. Indeed simulation has been employed in many existing studies on protocols for distributed database systems. <ref> [2, 8, 16, 23] </ref>. In our study, we use a simulation model to study the performance of a number of replica control protocols, including the popular primary copy protocol and the less widely accepted quorum consensus protocol. <p> For our experiments, the key parameter settings are described in able 6. Our choice of parameters for the database and transactions are chosen so as to be comparable to the existing simulation studies such as <ref> [8] </ref>. The choice of message propagation time is in keeping with [26] and with a recent study [1] conducted on Sun workstations interconnected via ethernet and employing Unix UDP protocol. The scale of disk access time relative to message propagation time is in accordance with [15].
Reference: [9] <author> CACI Products Company. </author> <title> MODSIM II Reference Manual. </title> <address> CACI, La Jolla, CA, </address> <year> 1993. </year> <month> 20 </month>
Reference-contexts: The simulation model described in this paper was implemented using the general-purpose simulation programming language MODSIM II from CACI Products Co., La Jolla, California <ref> [9] </ref>. MODSIM II is a high-level language which supports object-oriented programming and discrete-event simulation. The model is a large, process-based simulation model. Each site is an object, as is each transaction. Methods are defined for these objects to implement the functional modules of the model.
Reference: [10] <author> A. El Abbadi, D. Skeen, and F. Cristian. </author> <title> A nonblocking quorum consensuss protocol for replicated data management. </title> <booktitle> Proceedings 4th ACM Symposium on the Principle of Database Systems, </booktitle> <month> March </month> <year> 1985. </year>
Reference-contexts: Based on the up list, a group of replicas for a data object can consistently determine which of the replica is the current primary replica. On the other hand, the Harp distributed file system [21] uses the concept of view changes, first introduced in <ref> [10] </ref>, to maintain a consistent view of the organization among an individual groups of replicas. In general, an elaborate algorithm is required for the primary copy protocol to guarantee a consistent view of the replica configuration.
Reference: [11] <author> A. El Abbadi and S. Toueg. </author> <title> Maintaining Availability in Partitioned Replicated Databases. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 14(2) </volume> <pages> 264-290, </pages> <year> 1989. </year>
Reference-contexts: QC is an elegant algorithm which provides one-copy-serializability even in the presence of network partitioning. It does so without requiring complicated management of network configurations among the copy sites which is required for other replica control algorithms such as virtual partitions <ref> [11] </ref> or Primary Copy [28, 21]. It also does not require special treatment to recover copies, as outdated copies will not have the latest version number and so will not be read but will be over-written. In spite of these advantages, the QC algorithm has some well-known drawbacks [5].
Reference: [12] <author> K. P. Eswaran, J. N. Gray, R. A. Lorie, and I. L. Traiger. </author> <title> The Notions of Consistency and Predicate Locks in a Database System. </title> <journal> Communications of the ACM, </journal> 19(11) 624-633, November 1976. 
Reference-contexts: At each server site, this component is responsible for accepting transaction requests from the coordinator site. (b) Concurrency Control Manager: At each server site, this component provides concurrency control for transactions that have been submitted to the site. For our model, the strict two-phase locking algorithm <ref> [12, 5] </ref> is used.
Reference: [13] <author> Domenico Ferrari. </author> <title> Computer Systems Performance Evaluation. </title> <publisher> Prentice Hall, </publisher> <year> 1978. </year>
Reference-contexts: Statistics for individual transactions are carried in the data fields of the objects for the transactions, and are accumulated using statistic procedures provided by MODSIM II. In our experiments, the means of the desired measurements are obtained by using the method of batch means <ref> [19, 13, 18] </ref>. The results reported in the paper are within the 90 percent confidence intervals for the quantities measured: the size of the confidence intervals of those measurements was within a few percent of the mean in almost all cases.
Reference: [14] <author> D. K. Gifford. </author> <title> Weighted Voting for Replicated Data. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 16(4) </volume> <pages> 150-159, </pages> <year> 1979. </year>
Reference-contexts: The reconfiguration mechanism entails a two-phase protocol similar to the two-phase commit protocol to attain consensus among the sites or replicas regarding the group configuration. 2.2 The Quorum Consensus Protocol The quorum consensus (QC) algorithm <ref> [14] </ref> is an inherently simple protocol which guarantees data consistency against all non-byzantine failures [17], including network partitioning. QC has received much attention from researchers . In the QC algorithm, a non-negative weight is assigned to each replica of data object x.
Reference: [15] <author> J. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year>
Reference-contexts: The choice of message propagation time is in keeping with [26] and with a recent study [1] conducted on Sun workstations interconnected via ethernet and employing Unix UDP protocol. The scale of disk access time relative to message propagation time is in accordance with <ref> [15] </ref>. For the tractability of the simulation, the number of sites and the database size are set to smaller values than systems in practice.
Reference: [16] <author> D.D.E. Long J.-F. P^aris and A. Glockner. </author> <title> A realistic evaluation of consistency algorithms for replicated files. </title> <booktitle> Proceedings 21st Annual Simulation, </booktitle> <pages> pages 121-130, </pages> <year> 1988. </year>
Reference-contexts: In the absence of a full implementation, simulation becomes a viable alternative for a comprehensive performance model. Indeed simulation has been employed in many existing studies on protocols for distributed database systems. <ref> [2, 8, 16, 23] </ref>. In our study, we use a simulation model to study the performance of a number of replica control protocols, including the popular primary copy protocol and the less widely accepted quorum consensus protocol.
Reference: [17] <author> L. Lamport, R. Shostak, and M. Pease. </author> <title> The Byzantine Generals Problem. </title> <journal> ACM Trans. Prog. Lang. Syst., </journal> <volume> 4(3) </volume> <pages> 382-401, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: The reconfiguration mechanism entails a two-phase protocol similar to the two-phase commit protocol to attain consensus among the sites or replicas regarding the group configuration. 2.2 The Quorum Consensus Protocol The quorum consensus (QC) algorithm [14] is an inherently simple protocol which guarantees data consistency against all non-byzantine failures <ref> [17] </ref>, including network partitioning. QC has received much attention from researchers . In the QC algorithm, a non-negative weight is assigned to each replica of data object x.
Reference: [18] <author> S. Lavenberg. </author> <title> Computer Performance Modeling Handbook. </title> <publisher> Academic Press, </publisher> <year> 1983. </year>
Reference-contexts: Statistics for individual transactions are carried in the data fields of the objects for the transactions, and are accumulated using statistic procedures provided by MODSIM II. In our experiments, the means of the desired measurements are obtained by using the method of batch means <ref> [19, 13, 18] </ref>. The results reported in the paper are within the 90 percent confidence intervals for the quantities measured: the size of the confidence intervals of those measurements was within a few percent of the mean in almost all cases.
Reference: [19] <author> Averill M. Law and David Kelton. </author> <title> Simulation Modeling and Analysis. </title> <publisher> McGraw Hill, </publisher> <year> 1991. </year>
Reference-contexts: Statistics for individual transactions are carried in the data fields of the objects for the transactions, and are accumulated using statistic procedures provided by MODSIM II. In our experiments, the means of the desired measurements are obtained by using the method of batch means <ref> [19, 13, 18] </ref>. The results reported in the paper are within the 90 percent confidence intervals for the quantities measured: the size of the confidence intervals of those measurements was within a few percent of the mean in almost all cases.
Reference: [20] <author> M. L. Liu, D. Agrawal, and A. El Abbadi. </author> <title> The performance of two-phase commit protocols in the presence of site failures. </title> <booktitle> To appear in The 24th Annual International Symposium on Fault-Tolerant Computing, </booktitle> <year> 1994. </year>
Reference-contexts: As mentioned above, analytical properties of a protocol itself is often insufficient to help a designer to exercise his options. Indeed, in our previous study <ref> [20] </ref> we have demonstrated how, in the case of atomic commitment protocols, the dynamic behavior of the system can be counter-intuitive. In this paper, we explore a number of implementation approaches for the quorum consensus protocol by using a simulation model to evaluate their relative performance.
Reference: [21] <author> B. Oki and B. Liskov. Viewstamped Replicataion: </author> <title> A New Primary Copy Method to Support Highly-Available Distributed Systems. </title> <booktitle> Proceedings of the Seventh Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 8-17, </pages> <year> 1988. </year>
Reference-contexts: Unavailable secondary copies receive the update on a deferred basis. The primary-copy protocol is perhaps the most widely accepted replica control protocol in practice: It has been employed in existing implementations such as Distributed INGRES [29, 28] and the HARP distributed file system <ref> [21] </ref>. The protocol's primary appeal is in its conceptual simplicity, and, in environments where the primary copy can be chosen to be the closest to the transaction coordinator, the protocol yields the best response time for read operations. <p> In INGRES [28], the configuration is maintained systemwide in an "up list". Based on the up list, a group of replicas for a data object can consistently determine which of the replica is the current primary replica. On the other hand, the Harp distributed file system <ref> [21] </ref> uses the concept of view changes, first introduced in [10], to maintain a consistent view of the organization among an individual groups of replicas. In general, an elaborate algorithm is required for the primary copy protocol to guarantee a consistent view of the replica configuration. <p> QC is an elegant algorithm which provides one-copy-serializability even in the presence of network partitioning. It does so without requiring complicated management of network configurations among the copy sites which is required for other replica control algorithms such as virtual partitions [11] or Primary Copy <ref> [28, 21] </ref>. It also does not require special treatment to recover copies, as outdated copies will not have the latest version number and so will not be read but will be over-written. In spite of these advantages, the QC algorithm has some well-known drawbacks [5].
Reference: [22] <author> M. T. Ozsu and P. Valduriez. </author> <title> Distributed Database Systems: Where Are We Now? Computer, </title> <booktitle> 24(8) </booktitle> <pages> 68-78, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: 1 Introduction While distributed database systems have been the subject of extensive research, its acceptance in the industry remains tenuous <ref> [22] </ref>. A key reason for this phenomenon is that for such a system to function properly, it must perform correctly in the presence of communication and site failures. Although many protocols which address failures have been proposed, their implementation has not been widespread. <p> Although many protocols which address failures have been proposed, their implementation has not been widespread. For example, few in the the family of replica control protocols have ever been put into practice <ref> [24, 22] </ref> . This lack of acceptance is largely due to (i) the difficulty involved in assessing the performance impact of the protocols, and (ii) the high complexity typically associated with such protocols.
Reference: [23] <author> J.-F. P^aris and D.D.E. </author> <title> Long. The performance of available copy protocols for the management of replicated data. </title> <booktitle> Proceedings 11th Performance Evaluation, </booktitle> <pages> pages 9-30, </pages> <year> 1990. </year>
Reference-contexts: In the absence of a full implementation, simulation becomes a viable alternative for a comprehensive performance model. Indeed simulation has been employed in many existing studies on protocols for distributed database systems. <ref> [2, 8, 16, 23] </ref>. In our study, we use a simulation model to study the performance of a number of replica control protocols, including the popular primary copy protocol and the less widely accepted quorum consensus protocol.
Reference: [24] <author> A. Reuter. </author> <title> Performance and Reliability Issues in Future DBMSs. </title> <booktitle> In Lecture Notes in Computer Science: Database Systems of the 90s, </booktitle> <pages> pages 294-315. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Although many protocols which address failures have been proposed, their implementation has not been widespread. For example, few in the the family of replica control protocols have ever been put into practice <ref> [24, 22] </ref> . This lack of acceptance is largely due to (i) the difficulty involved in assessing the performance impact of the protocols, and (ii) the high complexity typically associated with such protocols.
Reference: [25] <author> R. Schlichting and F. B. Schneider. </author> <title> Fail-Stop Processors: An Approach to Designing Fault-Tolerant Computing Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(3) </volume> <pages> 222-238, </pages> <month> August </month> <year> 1982. </year>
Reference-contexts: Failures do not overlap at one site; that is, a site will not be subject to more than one failure at a time. Failures are uniformly distributed among the server sites. Failures are fail-stop <ref> [25] </ref>: all activities at the failed site cease upon a failure. Key information needed for the recovery of the site is assumed to be retained in stable storage. Site failures are implemented by interrupting all activities occurring at the failed site.
Reference: [26] <author> M. Scott and A. Cox. </author> <title> An Empirical Study of Message-Passing Overhead. </title> <booktitle> In ICCC 7th International Conference on Distriubted Computing Systems, </booktitle> <pages> pages 536-543, </pages> <year> 1987. </year>
Reference-contexts: For our experiments, the key parameter settings are described in able 6. Our choice of parameters for the database and transactions are chosen so as to be comparable to the existing simulation studies such as [8]. The choice of message propagation time is in keeping with <ref> [26] </ref> and with a recent study [1] conducted on Sun workstations interconnected via ethernet and employing Unix UDP protocol. The scale of disk access time relative to message propagation time is in accordance with [15].
Reference: [27] <author> J. Seguin, G. Sergeant, and P. Wilms. </author> <title> A majority consensus algorithm for the consistency of duplicated and distributed information. </title> <booktitle> Proc. IEEE Int. Conf. Distributed Computing Systems, </booktitle> <pages> pages 617-624, </pages> <year> 1979. </year>
Reference-contexts: As the weights assigned to the copies can be seen as "votes" from them, the algorithm is also known as the Voting algorithm. In its simplest form, a quorum is a majority set in which case the algorithm is known as Majority Voting <ref> [27] </ref>. QC is an elegant algorithm which provides one-copy-serializability even in the presence of network partitioning. It does so without requiring complicated management of network configurations among the copy sites which is required for other replica control algorithms such as virtual partitions [11] or Primary Copy [28, 21].
Reference: [28] <author> M. Stonebraker. </author> <title> Concurrency Control and Consistency of Multiple Copies of Data in Distributed INGRESS. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 188-194, </pages> <month> May </month> <year> 1979. </year> <month> 21 </month>
Reference-contexts: Hence the write-all approach is modified instead to write into all copies available to the transaction coordinator; such protocols are termed write-all-available protocols. The most commonly known protocol of this genre is the primary copy protocol <ref> [3, 28] </ref>. The protocol is designed for systems where a primary copy of each data object is located at one site, while replicas of the object are distributed among other nodes. <p> Unavailable secondary copies receive the update on a deferred basis. The primary-copy protocol is perhaps the most widely accepted replica control protocol in practice: It has been employed in existing implementations such as Distributed INGRES <ref> [29, 28] </ref> and the HARP distributed file system [21]. The protocol's primary appeal is in its conceptual simplicity, and, in environments where the primary copy can be chosen to be the closest to the transaction coordinator, the protocol yields the best response time for read operations. <p> Thus partition A may view replica x a as the primary copy, while partition B regards replica x b as the primary. In other implementations, the protocol has been enhanced to tolerate network partitioning by maintaining a systemwide consistent view of the replica configuration. In INGRES <ref> [28] </ref>, the configuration is maintained systemwide in an "up list". Based on the up list, a group of replicas for a data object can consistently determine which of the replica is the current primary replica. <p> QC is an elegant algorithm which provides one-copy-serializability even in the presence of network partitioning. It does so without requiring complicated management of network configurations among the copy sites which is required for other replica control algorithms such as virtual partitions [11] or Primary Copy <ref> [28, 21] </ref>. It also does not require special treatment to recover copies, as outdated copies will not have the latest version number and so will not be read but will be over-written. In spite of these advantages, the QC algorithm has some well-known drawbacks [5].
Reference: [29] <author> M. StoneBraker and E. Neuhold. </author> <title> A distributed data base version of ingres. </title> <booktitle> Proceedings 2nd Berkeley Workshop on Distributed Data Bases and Computer Networks, </booktitle> <month> May </month> <year> 1977. </year> <month> 22 </month>
Reference-contexts: Unavailable secondary copies receive the update on a deferred basis. The primary-copy protocol is perhaps the most widely accepted replica control protocol in practice: It has been employed in existing implementations such as Distributed INGRES <ref> [29, 28] </ref> and the HARP distributed file system [21]. The protocol's primary appeal is in its conceptual simplicity, and, in environments where the primary copy can be chosen to be the closest to the transaction coordinator, the protocol yields the best response time for read operations.
References-found: 29

