URL: ftp://ftp.cs.columbia.edu/pub/CAVE/papers/watanabe/eccv.ps
Refering-URL: http://www.cs.columbia.edu/~nabe/
Root-URL: http://www.cs.columbia.edu
Phone: 2  
Title: Telecentric Optics for Computational Vision  
Author: Masahiro Watanabe and Shree K. Nayar 
Address: 292 Yoshida-cho, Totsuka, Yokohama 244, Japan  New York, NY 10027, USA  
Affiliation: 1 Production Engineering Research Lab., Hitachi Ltd.,  Department of Computer Science, Columbia University,  
Abstract: An optical approach to constant-magnification imaging is described. Magnification variations due to changes in focus setting pose problems for pertinent vision techniques, such as, depth from defocus. It is shown that magnification of a conventional lens can be made invariant to defocus by simply adding an aperture at an analytically derived location. The resulting optical configuration is called "telecentric." It is shown that most commercially available lenses can be turned into telecentric ones. The procedure for calculating the position of the additional aperture is outlined. The photometric and geometric properties of telecentric lenses are discussed in detail. Several experiments have been conducted to demonstrate the effectiveness of telecentricity.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. Born and E. Wolf. </author> <booktitle> Principles of Optics. </booktitle> <address> London:Permagon, </address> <year> 1965. </year>
Reference-contexts: The magnification problem is eliminated in its entirety by the introduction of an optical configuration, referred to as telecentric optics. Though telecen-tricity has been known for long in optics <ref> [1, 8] </ref>, it has not been exploited in the realm of computational vision. With this optics, magnification remains constant despite focus changes. The attractive feature of this solution is that commercially available lenses (used extensively in machine vision) are easily transformed to telecentric ones by adding an extra aperture. <p> This ray is called the principal ray <ref> [1, 8] </ref> in optics. Since its intersection with the sensor plane varies with the position of the sensor plane, image magnification varies with defocus. 2.2 Telecentric Optics Keeping in mind the image formation model shown in Figure 1, we proceed to discuss the constant magnification configuration, called telecentric optics. <p> The thin lens is just a special case when the two principal planes <ref> [1, 6] </ref> coincides. Figure 1 is easily modified for the compound lens case, by replacing the thin lens with the two principal planes, U and U 0 , of the compound lens, as shown in Figure 3 (a). <p> One can tell this by looking at the exit pupil position in the specification sheet provided by the manufacturer. If the exit pupil position is at 1, the lens is image-side telecentric <ref> [1, 8] </ref>. An example is Fujinon's H12fi10.5A. But this does not mean that this zoom lens has magnification that is invariant to focus change, because zoom lenses usually change focus by complex movements of some of the lens components.
Reference: 2. <author> V. M. Bove, Jr. </author> <title> Entropy-based depth from focus. </title> <journal> Journal of Optical Society of America A, </journal> <volume> 10 </volume> <pages> 561-566, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Estimation of image contrast in the presence of these effects could result in erroneous depth estimates. In contrast to depth from focus, depth from defocus uses only two images with different optical settings <ref> [2, 4, 13, 14, 15, 18] </ref>. As it attempts to compute depth from this minimal number of images, it requires all aspects of the image formation process to be precisely modeled. Since the two images taken correspond to different levels of defocus, they are expected to differ in magnification. <p> Given that two images are all we have in the case of defocus analysis, the magnification variations prove particularly harmful during depth computation. As a result, most investigators of depth from defocus have been forced to vary aperture size <ref> [2, 4, 14, 15, 18] </ref> rather than the focus setting (for example, the distance between the sensor and the lens) to induce defocus variations between the two images. Aperture size changes have the advantage of not introducing magnification variations.
Reference: 3. <author> T. Darrell and K. Wohn. </author> <title> Pyramid based depth from focus. </title> <booktitle> Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 504-509, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Further, the necessity to change two physical parameters (focus and zoom) simultaneously tends to increase errors caused by backlashes in the lens mechanism and variations in lens distortion. An alternative approach to the magnification problem is a computational one, commonly referred to as image warping. Darrell and Wohn <ref> [3] </ref> proposed the use of warping to correct image shifts due to magnification changes caused by focusing. This method is simple and effective for some applications, but can prove computationally intensive for real-time ones. <p> Depth from focus uses a sequence of images taken by incrementing the focus setting in small steps. For each pixel, the focus setting that maximizes image contrast is determined. This in turn can be used to compute the depth of the corresponding scene point <ref> [3, 7, 9, 10, 11, 17] </ref>. Magnification variations due to defocus, however, cause additional image variations in the form of translations and scalings. Estimation of image contrast in the presence of these effects could result in erroneous depth estimates.
Reference: 4. <author> J. Ens and P. Lawrence. </author> <title> A matrix based method for determining depth from focus. </title> <booktitle> Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 600-609, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Estimation of image contrast in the presence of these effects could result in erroneous depth estimates. In contrast to depth from focus, depth from defocus uses only two images with different optical settings <ref> [2, 4, 13, 14, 15, 18] </ref>. As it attempts to compute depth from this minimal number of images, it requires all aspects of the image formation process to be precisely modeled. Since the two images taken correspond to different levels of defocus, they are expected to differ in magnification. <p> Given that two images are all we have in the case of defocus analysis, the magnification variations prove particularly harmful during depth computation. As a result, most investigators of depth from defocus have been forced to vary aperture size <ref> [2, 4, 14, 15, 18] </ref> rather than the focus setting (for example, the distance between the sensor and the lens) to induce defocus variations between the two images. Aperture size changes have the advantage of not introducing magnification variations.
Reference: 5. <author> B. K. P. Horn. </author> <title> Focusing. </title> <type> Technical Report Memo 160, </type> <institution> AI Lab., Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, USA, </address> <year> 1968. </year>
Reference-contexts: 1 Introduction The problem of magnification variation due to change in focus setting has significance in machine vision. The classical approach to solving this problem has been to view it as one of camera calibration <ref> [5, 17] </ref>. Willson and Shafer [17] conducted a careful analysis of the interaction between focus and magnification. They proposed a joint calibration approach that measures the relation between zooming and focusing for a given lens.
Reference: 6. <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This above fact results in another remarkable property of the telecentric lens: The image brightness stays constant in a telecentric lens, while, in a conventional lens, brightness decreases as the effective focal length d i increases. This becomes clear by examining the image irradiance equation <ref> [6] </ref>: E = L 4 F e where, E is the irradiance of the sensor plane, L is the radiance of the surface in the direction of the lens and F e is the effective F-number. <p> The thin lens is just a special case when the two principal planes <ref> [1, 6] </ref> coincides. Figure 1 is easily modified for the compound lens case, by replacing the thin lens with the two principal planes, U and U 0 , of the compound lens, as shown in Figure 3 (a).
Reference: 7. <author> R. A. Jarvis. </author> <title> A perspective on range finding techniques for computer vision. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 5(2) </volume> <pages> 122-139, </pages> <month> March </month> <year> 1983. </year>
Reference-contexts: Depth from focus uses a sequence of images taken by incrementing the focus setting in small steps. For each pixel, the focus setting that maximizes image contrast is determined. This in turn can be used to compute the depth of the corresponding scene point <ref> [3, 7, 9, 10, 11, 17] </ref>. Magnification variations due to defocus, however, cause additional image variations in the form of translations and scalings. Estimation of image contrast in the presence of these effects could result in erroneous depth estimates.
Reference: 8. <author> R. Kingslake. </author> <title> Optical System Design. </title> <publisher> Academic Press, </publisher> <year> 1983. </year>
Reference-contexts: The magnification problem is eliminated in its entirety by the introduction of an optical configuration, referred to as telecentric optics. Though telecen-tricity has been known for long in optics <ref> [1, 8] </ref>, it has not been exploited in the realm of computational vision. With this optics, magnification remains constant despite focus changes. The attractive feature of this solution is that commercially available lenses (used extensively in machine vision) are easily transformed to telecentric ones by adding an extra aperture. <p> This ray is called the principal ray <ref> [1, 8] </ref> in optics. Since its intersection with the sensor plane varies with the position of the sensor plane, image magnification varies with defocus. 2.2 Telecentric Optics Keeping in mind the image formation model shown in Figure 1, we proceed to discuss the constant magnification configuration, called telecentric optics. <p> Straightforward geometrical analysis reveals that the ray of light R 0 from any scene point that passes through the center O 0 of aperture A 0 , i.e. the principal ray, emerges parallel to the optical axis on the image side of the lens <ref> [8] </ref>. Furthermore, this parallel ray is the axis of a cone that includes all light rays radiated by the scene point, passed through by A 0 , and intercepted by the lens. <p> One can tell this by looking at the exit pupil position in the specification sheet provided by the manufacturer. If the exit pupil position is at 1, the lens is image-side telecentric <ref> [1, 8] </ref>. An example is Fujinon's H12fi10.5A. But this does not mean that this zoom lens has magnification that is invariant to focus change, because zoom lenses usually change focus by complex movements of some of the lens components.
Reference: 9. <author> A. Krishnan and N. Ahuja. </author> <title> Range estimation from focus using a non-frontal imaging camera. </title> <booktitle> Proc. of AAAI Conf., </booktitle> <pages> pages 830-835, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Depth from focus uses a sequence of images taken by incrementing the focus setting in small steps. For each pixel, the focus setting that maximizes image contrast is determined. This in turn can be used to compute the depth of the corresponding scene point <ref> [3, 7, 9, 10, 11, 17] </ref>. Magnification variations due to defocus, however, cause additional image variations in the form of translations and scalings. Estimation of image contrast in the presence of these effects could result in erroneous depth estimates.
Reference: 10. <author> E. Krotkov. </author> <title> Focusing. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 223-237, </pages> <year> 1987. </year>
Reference-contexts: Depth from focus uses a sequence of images taken by incrementing the focus setting in small steps. For each pixel, the focus setting that maximizes image contrast is determined. This in turn can be used to compute the depth of the corresponding scene point <ref> [3, 7, 9, 10, 11, 17] </ref>. Magnification variations due to defocus, however, cause additional image variations in the form of translations and scalings. Estimation of image contrast in the presence of these effects could result in erroneous depth estimates.
Reference: 11. <author> S. K. Nayar and Y. Nakagawa. </author> <title> Shape from focus. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(8) </volume> <pages> 824-831, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Depth from focus uses a sequence of images taken by incrementing the focus setting in small steps. For each pixel, the focus setting that maximizes image contrast is determined. This in turn can be used to compute the depth of the corresponding scene point <ref> [3, 7, 9, 10, 11, 17] </ref>. Magnification variations due to defocus, however, cause additional image variations in the form of translations and scalings. Estimation of image contrast in the presence of these effects could result in erroneous depth estimates.
Reference: 12. <author> S. K. Nayar and M. Watanabe. </author> <title> Passive bifocal vision sensor. </title> <note> Technical Report (in preparation), </note> <institution> Dept. of Computer Science, Columbia University, </institution> <address> New York, NY, USA, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: In practice, this can be accomplished by using a beam splitter (or for more than two images, a sequence of beam splitters) behind the telecentric lens <ref> [12] </ref>. In a real-time application, all images can be digitized simultaneously and processed in parallel as in [13]. 3 Aperture Placement Although the discussion on telecentricity in section 2 was based on the thin lens model, the results holds true for compound lenses.
Reference: 13. <author> S. K. Nayar, M. Watanabe, and M. Noguchi. </author> <title> Real-time focus range sensor. </title> <booktitle> Proc. of Intl. Conf. on Computer Vision, </booktitle> <pages> pages 995-1001, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Estimation of image contrast in the presence of these effects could result in erroneous depth estimates. In contrast to depth from focus, depth from defocus uses only two images with different optical settings <ref> [2, 4, 13, 14, 15, 18] </ref>. As it attempts to compute depth from this minimal number of images, it requires all aspects of the image formation process to be precisely modeled. Since the two images taken correspond to different levels of defocus, they are expected to differ in magnification. <p> Further, extensive experimentation is conducted to verify the invariance of magnification to defocus in four telecentric lenses that were constructed from commonly used commercial lenses. We have successfully incorporated a telecentric lens into a real-time active range sensor that is based on depth from defocus <ref> [13] </ref>. Recently, we demon strated the application of telecentric optics to passive depth from defocus [16]. <p> In practice, this can be accomplished by using a beam splitter (or for more than two images, a sequence of beam splitters) behind the telecentric lens [12]. In a real-time application, all images can be digitized simultaneously and processed in parallel as in <ref> [13] </ref>. 3 Aperture Placement Although the discussion on telecentricity in section 2 was based on the thin lens model, the results holds true for compound lenses. The thin lens is just a special case when the two principal planes [1, 6] coincides. <p> Fujinon's CF12.5A f/1.4 is an example such a lens, which we converted to telecentric by placing an aperture inside and used to develop a real-time focus range sensor <ref> [13] </ref>. In practice, the exact location where the additional aperture needs to be placed can be determined in the following way. In some cases, the lens manufacturer provides information regarding the front focal position, which is customarily denoted as F in the schematic diagram. <p> This needs to be compensated by using either brighter illumination or a more sensitive image sensor. However, it should be noted that a lens designed from scratch can be made telecentric without substantial reduction in brightness. We have applied telecentric optics to the depth from defocus problem <ref> [13, 16] </ref>. The computed depth maps were found to be far superior to ones computed using a conventional lens. Acknowledgements We thank one of the anonymous reviewers for detailed comments on the paper and Simon Baker for his help in preparing the final manuscript.
Reference: 14. <author> A. Pentland. </author> <title> A new sense for depth of field. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 9(4) </volume> <pages> 523-531, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Estimation of image contrast in the presence of these effects could result in erroneous depth estimates. In contrast to depth from focus, depth from defocus uses only two images with different optical settings <ref> [2, 4, 13, 14, 15, 18] </ref>. As it attempts to compute depth from this minimal number of images, it requires all aspects of the image formation process to be precisely modeled. Since the two images taken correspond to different levels of defocus, they are expected to differ in magnification. <p> Given that two images are all we have in the case of defocus analysis, the magnification variations prove particularly harmful during depth computation. As a result, most investigators of depth from defocus have been forced to vary aperture size <ref> [2, 4, 14, 15, 18] </ref> rather than the focus setting (for example, the distance between the sensor and the lens) to induce defocus variations between the two images. Aperture size changes have the advantage of not introducing magnification variations.
Reference: 15. <author> G. Surya and M. Subbarao. </author> <title> Depth from defocus by changing camera aperture: A spatial domain approach. </title> <booktitle> Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 61-67, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Estimation of image contrast in the presence of these effects could result in erroneous depth estimates. In contrast to depth from focus, depth from defocus uses only two images with different optical settings <ref> [2, 4, 13, 14, 15, 18] </ref>. As it attempts to compute depth from this minimal number of images, it requires all aspects of the image formation process to be precisely modeled. Since the two images taken correspond to different levels of defocus, they are expected to differ in magnification. <p> Given that two images are all we have in the case of defocus analysis, the magnification variations prove particularly harmful during depth computation. As a result, most investigators of depth from defocus have been forced to vary aperture size <ref> [2, 4, 14, 15, 18] </ref> rather than the focus setting (for example, the distance between the sensor and the lens) to induce defocus variations between the two images. Aperture size changes have the advantage of not introducing magnification variations.
Reference: 16. <author> M. Watanabe and S. K. Nayar. </author> <title> Telecentric optics for constant-magnification imaging. </title> <type> Technical Report CUCS-026-95, </type> <institution> Dept. of Computer Science, Columbia University, </institution> <address> New York, NY, USA, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: We have successfully incorporated a telecentric lens into a real-time active range sensor that is based on depth from defocus [13]. Recently, we demon strated the application of telecentric optics to passive depth from defocus <ref> [16] </ref>. <p> This needs to be compensated by using either brighter illumination or a more sensitive image sensor. However, it should be noted that a lens designed from scratch can be made telecentric without substantial reduction in brightness. We have applied telecentric optics to the depth from defocus problem <ref> [13, 16] </ref>. The computed depth maps were found to be far superior to ones computed using a conventional lens. Acknowledgements We thank one of the anonymous reviewers for detailed comments on the paper and Simon Baker for his help in preparing the final manuscript.
Reference: 17. <author> R. G. Willson and S. A. Shafer. </author> <title> Modeling and calibration of automated zoom lenses. </title> <type> Technical Report CMU-RI-TR-94-03, </type> <institution> The Robotics Institute, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, USA, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The problem of magnification variation due to change in focus setting has significance in machine vision. The classical approach to solving this problem has been to view it as one of camera calibration <ref> [5, 17] </ref>. Willson and Shafer [17] conducted a careful analysis of the interaction between focus and magnification. They proposed a joint calibration approach that measures the relation between zooming and focusing for a given lens. <p> 1 Introduction The problem of magnification variation due to change in focus setting has significance in machine vision. The classical approach to solving this problem has been to view it as one of camera calibration [5, 17]. Willson and Shafer <ref> [17] </ref> conducted a careful analysis of the interaction between focus and magnification. They proposed a joint calibration approach that measures the relation between zooming and focusing for a given lens. <p> Depth from focus uses a sequence of images taken by incrementing the focus setting in small steps. For each pixel, the focus setting that maximizes image contrast is determined. This in turn can be used to compute the depth of the corresponding scene point <ref> [3, 7, 9, 10, 11, 17] </ref>. Magnification variations due to defocus, however, cause additional image variations in the form of translations and scalings. Estimation of image contrast in the presence of these effects could result in erroneous depth estimates.
Reference: 18. <author> Y. Xiong and S. A. Shafer. </author> <title> Moment and hypergeometric filters for high precision computation of focus, stereo and optical flow. </title> <type> Technical Report CMU-RI-TR-94-28, </type> <institution> The Robotics Institute, Carnegie Mellon University, Pittsburg, </institution> <address> PA, USA, </address> <month> September </month> <year> 1994. </year> <title> This article was processed using the L a T E X macro package with ECCV'96 style </title>
Reference-contexts: Estimation of image contrast in the presence of these effects could result in erroneous depth estimates. In contrast to depth from focus, depth from defocus uses only two images with different optical settings <ref> [2, 4, 13, 14, 15, 18] </ref>. As it attempts to compute depth from this minimal number of images, it requires all aspects of the image formation process to be precisely modeled. Since the two images taken correspond to different levels of defocus, they are expected to differ in magnification. <p> Given that two images are all we have in the case of defocus analysis, the magnification variations prove particularly harmful during depth computation. As a result, most investigators of depth from defocus have been forced to vary aperture size <ref> [2, 4, 14, 15, 18] </ref> rather than the focus setting (for example, the distance between the sensor and the lens) to induce defocus variations between the two images. Aperture size changes have the advantage of not introducing magnification variations.
References-found: 18

