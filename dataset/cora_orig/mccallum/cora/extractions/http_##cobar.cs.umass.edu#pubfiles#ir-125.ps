URL: http://cobar.cs.umass.edu/pubfiles/ir-125.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/
Root-URL: 
Email: callan@cs.umass.edu  
Title: Learning While Filtering Documents  
Author: Jamie Callan 
Web: www.cs.umass.edu/~callan/  
Address: Amherst, MA 01003-4610, USA  
Affiliation: Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts  
Abstract: This paper examines the problems of learning queries and dissemination thresholds from relevance feedback in a dynamic information filtering environment. It revisits the EG algorithm for learning queries, identifying several problems in using it reliably for information filtering, and providing solutions. It also presents a new algorithm for learning dissemination thresholds automatically, from the same relevance feedback information used to learn queries. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Allan. </author> <title> Incremental relevance feedback. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 270-278, </pages> <address> Zurich, </address> <year> 1996. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: The current state-of-the-art for creating and modifying queries from large amounts of training data is the Rocchio algorithm, augmented with Dynamic Feedback Optimization (DFO) [4]. Rocchio can be implemented as an incremental algorithm <ref> [1] </ref>. However, Dynamic Feedback Optimization is a batch-oriented algorithm that iteratively tests a large number of weight adjustments against an archival database. It is not clear how DFO can be applied in a more interactive environment. <p> This methodology is not ideal for simulating an interactive environment, but it is to some extent an accepted methodology. Using it makes the work described here comparable to work reported at TREC. The default relevance feedback algorithm in InRoute, an incremental Rocchio algorithm <ref> [1] </ref>, was used to learn queries. The default settings were used, which allowed up to 10 query terms to be added to the query.
Reference: [2] <author> J. Allan, J. P. Callan, W. B. Croft, L. Ballesteros, J. Broglio, J. Xu, and H. Shu. </author> <note> INQUERY at TREC-5. </note> <editor> In D. K. Harman and E. M. Voorhees, editors, </editor> <booktitle> The Fifth Text REtrieval Conference (TREC-5), </booktitle> <pages> pages 119-132. </pages> <institution> National Institute of Standards and Technology, </institution> <note> Special Publication 500-238, </note> <year> 1997. </year>
Reference-contexts: However, the algorithm was tested in a batch-oriented manner, in which training documents were examined repeatedly, and in random order. Later work on the TREC-5 Routing task produced less accurate results, raising doubts about the stability of the algorithm <ref> [2] </ref>. There is little published research on learning dissemination thresholds, perhaps because filtering systems have only recently been based on statistical models. <p> learn dissemination thresholds automatically. 3 Learning Term Weights Incrementally With EG Prior research suggested that the EG algorithm might be as effective as Rocchio augmented with Dynamic Feedback Optimization (DFO) [12], but the experiments were not truly incremental, and later work on a different corpus failed to support this result <ref> [2] </ref>. The best one can say is that EG appears to be a promising incremental alternative worth further examination. 3.1 The EG Algorithm A detailed description and analysis of EG is beyond the scope of this paper. <p> A "batch-oriented" Routing query was also created, as a state-of-the-art reference point against which to compare the incremental queries. The Routing query contained 50 additional terms, and 50 proximity operators each for distances 1, 5, and 20 <ref> [2] </ref>. For this comparison only, filtering output, using thresholds, was converted to a ranked list by sorting on document score. The usual Routing output (no thresholds, top 1,000 documents per query) was used. Results were evaluated using the Average Precision metric.
Reference: [3] <author> T. A. H. Bell and A. Moffat. </author> <title> The design of a high performance information filtering system. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 12-20, </pages> <address> Zurich, </address> <year> 1996. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: Some efficiency optimizations designed for large filter ing services cause problems. For example, it might not be acceptable to "compile" a set of profiles to improve filtering speed, if doing so meant that people could not easily change their profiles <ref> [3] </ref>. However, it is a common user-requirement that software must run faster than it does, so optimizations that maintain flexibility and responsiveness are important. The current state-of-the-art for creating and modifying queries from large amounts of training data is the Rocchio algorithm, augmented with Dynamic Feedback Optimization (DFO) [4].
Reference: [4] <author> C. Buckley and G. Salton. </author> <title> Optimization of relevance feedback weights. </title> <booktitle> In Proceedings of the Eighteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 351-357, </pages> <address> Seattle, </address> <year> 1995. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: Routing queries are created by analyzing a large set of example documents that have been labeled relevant and non-relevant. The most accurate algorithms assume that documents can be examined repeatedly, usually in any order, or that potential changes can be tested on an archival collection <ref> [4, 15, 12] </ref>. A related problem is deciding how well a document must satisfy an information need in order to be disseminated. This problem of setting dissemination thresholds is usually addressed with algorithms that assume access to a large set of labeled example documents [7, 8]. <p> However, it is a common user-requirement that software must run faster than it does, so optimizations that maintain flexibility and responsiveness are important. The current state-of-the-art for creating and modifying queries from large amounts of training data is the Rocchio algorithm, augmented with Dynamic Feedback Optimization (DFO) <ref> [4] </ref>. Rocchio can be implemented as an incremental algorithm [1]. However, Dynamic Feedback Optimization is a batch-oriented algorithm that iteratively tests a large number of weight adjustments against an archival database. It is not clear how DFO can be applied in a more interactive environment.
Reference: [5] <author> J. P. Callan. </author> <title> Document filtering with inference networks. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 262-269, </pages> <address> Zurich, </address> <year> 1996. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: This paper addresses the problems of learning filtering queries and their corresponding dissemination thresholds, automatically and incrementally. The work reported here was done with InRoute, a document filtering system based on a Bayesian inference network model of information retrieval and filtering <ref> [5] </ref>. However, the results reported here apply also to most statistical models of IR, including other probabilistic and vector space models. 2 Document Filtering There are many types of document filtering environments, each with different assumptions and requirements [19, 14, 6, 18, 5]. <p> However, the results reported here apply also to most statistical models of IR, including other probabilistic and vector space models. 2 Document Filtering There are many types of document filtering environments, each with different assumptions and requirements <ref> [19, 14, 6, 18, 5] </ref>. We begin by defining what is meant by document filtering in this paper, and discussing how previously published research relates to it. <p> This "slow filtering" approach is only acceptable when information can be disseminated slowly. When information must be disseminated quickly, one must use a filtering system that processes documents individually, as they arrive. Filtering systems such as SIFT [18] and InRoute <ref> [5] </ref> satisfy this requirement. Some efficiency optimizations designed for large filter ing services cause problems. For example, it might not be acceptable to "compile" a set of profiles to improve filtering speed, if doing so meant that people could not easily change their profiles [3]. <p> While it is important to verify that this remains true for the EG algorithm, our first hypothesis is that it does indeed remain true. This hypothesis is tested using In-Route's tf:idf features <ref> [5] </ref>, which incorporate Robertson's tf weighting formula [16] and a scaled idf . Our second hypothesis is that EG is sensitive to the choice of target values intended to signify relevant and nonrelevant documents.
Reference: [6] <author> P. W. Foltz and S. T. Dumais. </author> <title> Personalized information delivery: An analysis of information filtering methods. </title> <journal> Communications of the ACM, </journal> <volume> 35(12) </volume> <pages> 51-60, </pages> <year> 1992. </year>
Reference-contexts: However, the results reported here apply also to most statistical models of IR, including other probabilistic and vector space models. 2 Document Filtering There are many types of document filtering environments, each with different assumptions and requirements <ref> [19, 14, 6, 18, 5] </ref>. We begin by defining what is meant by document filtering in this paper, and discussing how previously published research relates to it. <p> However, these requirements present difficulties for many of the information filtering solutions proposed in the past. For example, it would not be acceptable to store documents for some period of time, then index them and search the index with an information retrieval system <ref> [17, 6] </ref>. This "slow filtering" approach is only acceptable when information can be disseminated slowly. When information must be disseminated quickly, one must use a filtering system that processes documents individually, as they arrive. Filtering systems such as SIFT [18] and InRoute [5] satisfy this requirement.
Reference: [7] <editor> D. Harman, editor. </editor> <booktitle> Proceedings of the Fifth Text REtrieval Conference (TREC-5). National Institute of Standards and Technology Special Publication 500-238, </booktitle> <address> Gaithersburg, MD, </address> <year> 1997. </year>
Reference-contexts: A related problem is deciding how well a document must satisfy an information need in order to be disseminated. This problem of setting dissemination thresholds is usually addressed with algorithms that assume access to a large set of labeled example documents <ref> [7, 8] </ref>. <p> Most methods tested in the TREC-5 and TREC-6 Filtering tracks depended on having access to a large set of training documents, and a cost function defining the user's requirements for precision vs recall <ref> [7, 8] </ref>. It is not clear how well these methods would work in a more interactive environment, particularly initially, when little training data is available. Nor is it clear how these approaches to threshold-learning interact with query learning or idf adjustments. <p> Several batch-oriented solutions require a cost function describing a person's preferences, and then find a threshold that optimizes for it by testing different thresholds on a training set of relevant and nonrelevant documents <ref> [7, 8] </ref>. Our interest is a solution for interactive environments. "Corpus" statistics such as idf may be adjusted as each document passes through the system, making it less clear what a "good" document score might be.
Reference: [8] <editor> D. Harman, editor. </editor> <booktitle> Proceedings of the Sixth Text REtrieval Conference (TREC-6). National Institute of Standards and Technology Special Publication, </booktitle> <address> Gaithersburg, MD, </address> <publisher> (in press). </publisher>
Reference-contexts: A related problem is deciding how well a document must satisfy an information need in order to be disseminated. This problem of setting dissemination thresholds is usually addressed with algorithms that assume access to a large set of labeled example documents <ref> [7, 8] </ref>. <p> Most methods tested in the TREC-5 and TREC-6 Filtering tracks depended on having access to a large set of training documents, and a cost function defining the user's requirements for precision vs recall <ref> [7, 8] </ref>. It is not clear how well these methods would work in a more interactive environment, particularly initially, when little training data is available. Nor is it clear how these approaches to threshold-learning interact with query learning or idf adjustments. <p> Several batch-oriented solutions require a cost function describing a person's preferences, and then find a threshold that optimizes for it by testing different thresholds on a training set of relevant and nonrelevant documents <ref> [7, 8] </ref>. Our interest is a solution for interactive environments. "Corpus" statistics such as idf may be adjusted as each document passes through the system, making it less clear what a "good" document score might be. <p> We hope that one or more of them will be useful to the reader. Table 4 illustrates how each metric behaves on four artificial examples. 4.3 Experimental Methodology The TREC-6 Filtering methodology <ref> [8, 9] </ref> was adopted for these experiments. The filtering system starts with an "Ad-hoc" query, and receives relevance feedback whenever it disseminates a document during the training phase. Queries and thresholds are then frozen and tested on the test set.
Reference: [9] <author> D.A. Hull. </author> <title> The TREC-6 Filtering track: Description and analysis. </title> <editor> In D. K. Harman and E. M. Voorhees, editors, </editor> <booktitle> The Sixth Text REtrieval Conference (TREC-6). </booktitle> <institution> National Institute of Standards and Technology, </institution> <note> Special Publication, (in press). </note>
Reference-contexts: We hope that one or more of them will be useful to the reader. Table 4 illustrates how each metric behaves on four artificial examples. 4.3 Experimental Methodology The TREC-6 Filtering methodology <ref> [8, 9] </ref> was adopted for these experiments. The filtering system starts with an "Ad-hoc" query, and receives relevance feedback whenever it disseminates a document during the training phase. Queries and thresholds are then frozen and tested on the test set.
Reference: [10] <author> J. Kivinen and M. K. Warmuth. </author> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <type> Technical Report UCSC-CRL-94-16, </type> <institution> Baskin Center for Computer Engineering and Information Sciences, University of California, </institution> <address> Santa Cruz, CA, </address> <year> 1994. </year>
Reference-contexts: However, Dynamic Feedback Optimization is a batch-oriented algorithm that iteratively tests a large number of weight adjustments against an archival database. It is not clear how DFO can be applied in a more interactive environment. Prior research suggested that the EG algorithm <ref> [10] </ref> is as effective at creating queries for filtering and routing tasks as Rocchio augmented with DFO [12]. This work was encouraging because EG is an incremental algorithm with well-defined theoretical properties. <p> The best one can say is that EG appears to be a promising incremental alternative worth further examination. 3.1 The EG Algorithm A detailed description and analysis of EG is beyond the scope of this paper. Details and in-depth analysis of the algorithm can be found in <ref> [10] </ref>, while [12] provides a detailed comparison of the Rocchio, EG, and Widrow-Hoff algorithms for IR tasks. We confine our attention here to a study of possible problems in applying EG to information filtering tasks. <p> We confine our attention here to a study of possible problems in applying EG to information filtering tasks. The EG algorithm, like Rocchio, attempts to find a linear classifier that minimizes the magnitude of classification errors <ref> [10] </ref>. One can consider the classifier to be a weight vector that is applied to a set of query terms. EG imposes the restrictions that all weights must be positive, and sum to 1, neither or which is a problem for filtering queries. <p> Feature Type: * Boolean. * tf:idf: Target Values: * Boolean targets f0, 1g. * f0.42, 0.49g, as used in [12]. * fMinFeature, MaxFeatureg, from hypothesis 3. Learning Rate: * R = max i (max j x i;j min j x i;j ), as suggested in <ref> [10] </ref> (called KW, in this paper). * R = 1: There are 12 possible combinations of feature values, but only 7 are unique and make sense. All 7 combinations were tested. The experimental results are summarized in Table 2. The baseline values were obtained with unweighted queries.
Reference: [11] <author> D. D. Lewis. </author> <title> The TREC-5 filtering track. </title> <editor> In D. K. Harman and E. M. Voorhees, editors, </editor> <booktitle> The Fifth Text REtrieval Conference (TREC-5), </booktitle> <pages> pages 75-96, </pages> <address> Gaithersburg, MD, </address> <year> 1997. </year> <institution> National Institute of Standards and Technology, </institution> <note> Special Publication 500-238. </note>
Reference-contexts: These characteristics make it well-suited to most interactive filtering environments. 4.2 Metrics It is generally accepted that metrics for ranked retrieval are not adequate for measuring filtering systems that use dissemination thresholds <ref> [11] </ref>.
Reference: [12] <author> D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka. </author> <title> Training algorithms for linear text classifiers. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 298-306, </pages> <address> Zurich, </address> <year> 1996. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: Routing queries are created by analyzing a large set of example documents that have been labeled relevant and non-relevant. The most accurate algorithms assume that documents can be examined repeatedly, usually in any order, or that potential changes can be tested on an archival collection <ref> [4, 15, 12] </ref>. A related problem is deciding how well a document must satisfy an information need in order to be disseminated. This problem of setting dissemination thresholds is usually addressed with algorithms that assume access to a large set of labeled example documents [7, 8]. <p> It is not clear how DFO can be applied in a more interactive environment. Prior research suggested that the EG algorithm [10] is as effective at creating queries for filtering and routing tasks as Rocchio augmented with DFO <ref> [12] </ref>. This work was encouraging because EG is an incremental algorithm with well-defined theoretical properties. However, the algorithm was tested in a batch-oriented manner, in which training documents were examined repeatedly, and in random order. <p> The incremental Rocchio algorithm, and perhaps EG, can be used. There is little guidance on how to learn dissemination thresholds automatically. 3 Learning Term Weights Incrementally With EG Prior research suggested that the EG algorithm might be as effective as Rocchio augmented with Dynamic Feedback Optimization (DFO) <ref> [12] </ref>, but the experiments were not truly incremental, and later work on a different corpus failed to support this result [2]. <p> The best one can say is that EG appears to be a promising incremental alternative worth further examination. 3.1 The EG Algorithm A detailed description and analysis of EG is beyond the scope of this paper. Details and in-depth analysis of the algorithm can be found in [10], while <ref> [12] </ref> provides a detailed comparison of the Rocchio, EG, and Widrow-Hoff algorithms for IR tasks. We confine our attention here to a study of possible problems in applying EG to information filtering tasks. <p> We note, for example, that Lewis, et al, were careful to choose target values of 0.42 and 0.49, which were roughly comparable to "low" and "high" scores in INQUERY <ref> [12] </ref>. <p> The EG hypotheses can be mapped to three variables, representing feature type, method of setting target values, and method of setting learning rate. Feature Type: * Boolean. * tf:idf: Target Values: * Boolean targets f0, 1g. * f0.42, 0.49g, as used in <ref> [12] </ref>. * fMinFeature, MaxFeatureg, from hypothesis 3. <p> Boolean target values were a poor choice for the tf:idf representation, as expected. The target values of 0.42 and 0.49 performed well, confirming previously published results <ref> [12] </ref>. Target values set dynamically, based on the minimum and maximum feature values, performed poorly with the KW learning rate, but performed well when the learning rate was calculated with R = 1. <p> TREC-6 Filtering queries, trained on TREC FBIS 3 & 4 data, tested on TREC FBIS 6 data. The most consistent combinations of parameter settings were the one corresponding to our hypotheses (tf:idf , Min/Max, R = 1), and the one used in <ref> [12] </ref> (tf:idf , 0.42/0.49, KW). This experiment confirms each of the hypotheses presented in Section 3.2, but also confirms that the KW method of setting learning rates is appropriate when reasonable target values are known a priori.
Reference: [13] <author> N. J. Nilsson. </author> <title> Learning machines. </title> <publisher> McGraw-Hill, </publisher> <year> 1965. </year>
Reference-contexts: This hypothesis is tested using In-Route's tf:idf features [5], which incorporate Robertson's tf weighting formula [16] and a scaled idf . Our second hypothesis is that EG is sensitive to the choice of target values intended to signify relevant and nonrelevant documents. Some algorithms, for example the Perceptron algorithm <ref> [13] </ref>, are insensitive to the values chosen; all that matters is the direction of the error gradient. However, EG follows an exponentiated gradient; small variations in how the gradient is determined might have large effects on algorithm behavior.
Reference: [14] <author> K.H. Packer and D. Soergel. </author> <title> The importance of SDI for current awareness in fields with severe scatter of information. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 30(3) </volume> <pages> 125-135, </pages> <year> 1979. </year>
Reference-contexts: However, the results reported here apply also to most statistical models of IR, including other probabilistic and vector space models. 2 Document Filtering There are many types of document filtering environments, each with different assumptions and requirements <ref> [19, 14, 6, 18, 5] </ref>. We begin by defining what is meant by document filtering in this paper, and discussing how previously published research relates to it.
Reference: [15] <author> S. E. Robertson, S. Walker, M. M. Hancock-Beaulieu, M. Gatford, and A. Payne. </author> <note> Okapi at TREC-4. </note> <editor> In D. K. Harman, editor, </editor> <booktitle> The Fourth Text REtrieval Conference (TREC-4), </booktitle> <pages> pages 73-96, </pages> <address> Gaithersburg, MD, </address> <year> 1996. </year> <institution> National Institute of Standards and Technology, </institution> <note> Special Publication 500-236. </note>
Reference-contexts: Routing queries are created by analyzing a large set of example documents that have been labeled relevant and non-relevant. The most accurate algorithms assume that documents can be examined repeatedly, usually in any order, or that potential changes can be tested on an archival collection <ref> [4, 15, 12] </ref>. A related problem is deciding how well a document must satisfy an information need in order to be disseminated. This problem of setting dissemination thresholds is usually addressed with algorithms that assume access to a large set of labeled example documents [7, 8].
Reference: [16] <author> S.E. Robertson and S. Walker. </author> <title> Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval. </title> <booktitle> In Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 232-241, </pages> <address> Dublin, Ireland, </address> <year> 1994. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: While it is important to verify that this remains true for the EG algorithm, our first hypothesis is that it does indeed remain true. This hypothesis is tested using In-Route's tf:idf features [5], which incorporate Robertson's tf weighting formula <ref> [16] </ref> and a scaled idf . Our second hypothesis is that EG is sensitive to the choice of target values intended to signify relevant and nonrelevant documents.
Reference: [17] <author> M. F. Wyle and H. P. Frei. </author> <title> Retrieving highly dynamic, widely distributed information. </title> <booktitle> In Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 108-115, </pages> <address> Boston, MA, </address> <year> 1989. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: However, these requirements present difficulties for many of the information filtering solutions proposed in the past. For example, it would not be acceptable to store documents for some period of time, then index them and search the index with an information retrieval system <ref> [17, 6] </ref>. This "slow filtering" approach is only acceptable when information can be disseminated slowly. When information must be disseminated quickly, one must use a filtering system that processes documents individually, as they arrive. Filtering systems such as SIFT [18] and InRoute [5] satisfy this requirement.
Reference: [18] <author> T. Yan and H. Garcia-Molina. </author> <title> SIFT A tool for wide-area information dissemination. </title> <booktitle> In Proc. USENIX Winter 1995 Technical Conference, </booktitle> <address> New Orleans, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: However, the results reported here apply also to most statistical models of IR, including other probabilistic and vector space models. 2 Document Filtering There are many types of document filtering environments, each with different assumptions and requirements <ref> [19, 14, 6, 18, 5] </ref>. We begin by defining what is meant by document filtering in this paper, and discussing how previously published research relates to it. <p> This "slow filtering" approach is only acceptable when information can be disseminated slowly. When information must be disseminated quickly, one must use a filtering system that processes documents individually, as they arrive. Filtering systems such as SIFT <ref> [18] </ref> and InRoute [5] satisfy this requirement. Some efficiency optimizations designed for large filter ing services cause problems. For example, it might not be acceptable to "compile" a set of profiles to improve filtering speed, if doing so meant that people could not easily change their profiles [3].
Reference: [19] <author> J. A. Yochum. </author> <title> A high-speed text scanning algorithm utilizing least frequent trigraphs. </title> <booktitle> In Proceedings of the IEEE International Symposium on New Directions in Computing, </booktitle> <pages> pages 114-121, </pages> <address> Trondheim, Norway, 1985. </address> <publisher> IEEE. </publisher>
Reference-contexts: However, the results reported here apply also to most statistical models of IR, including other probabilistic and vector space models. 2 Document Filtering There are many types of document filtering environments, each with different assumptions and requirements <ref> [19, 14, 6, 18, 5] </ref>. We begin by defining what is meant by document filtering in this paper, and discussing how previously published research relates to it.
References-found: 19

