URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3108/3108.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: wak@cs.umd.edu pugh@cs.umd.edu  
Title: Determining Schedules based on Performance Estimation  
Author: Wayne Kelly William Pugh 
Note: This work is supported by an NSF PYI grant CCR-9157384 and by a Packard Fellowship.  
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies Dept. of Computer Science Dept. of Computer Science Univ. of Maryland,  
Date: December, 1993  
Pubnum: UMIACS-TR-93-67  CS-TR-3108  
Abstract: In previous work, we presented a framework for unifying iteration reordering transformations such as loop interchange, loop distribution, loop skewing and statement reordering. The framework provides a uniform way to represent and reason about transformations. However, it does not provide a way to decide which transformation(s) should be applied to a given program. This paper describes a way to make such decisions within the context of the framework. The framework is based on the idea that a transformation can be represented as an affine mapping from the original iteration space to a new iteration space. We show how we can estimate the performance of a program by considering only the mapping from which it was produced. We also show how to produce an lower bound on performance given only a partially specified mapping. Our ability to estimate performance directly from mappings and to do so even for partially specified mappings allows us to efficiently find mappings which will produce good code. 
Abstract-found: 1
Intro-found: 1
Reference: [ADY92] <author> L. Khachiyan A. Darte and Y.Roberts. </author> <title> Linear scheduling is close to optimality. </title> <booktitle> In International Conference on Application Specific Array Processors, </booktitle> <pages> pages 37-46, </pages> <year> 1992. </year>
Reference-contexts: These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without actually performing the transformations and analyzing the resulting code. To overcome these problems, many researchers have proposed a uniform approach to transforming programs <ref> [Ban90, WL91, LMQ91, Fea92, ST92, KP93, ADY92] </ref>. The problem of finding the optimal program equivalent to some other program is undecidable in general, since it reduces to the "program equivalence problem"[Tou84]. Consequently existing systems make various simplifying assumptions. <p> We represent transformations by associating a multi-dimensional mapping with each statement. This allows us to represent a large class of transformations, including any combination of loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering. Most schedule based systems <ref> [LMQ91, Fea92, ADY92] </ref> determine which schedule to apply by using linear programming techniques to optimize a linear function representing execution time. Most of these systems assume that parallelism is the only factor that will affect execution time, i.e. they ignore issues such as simplicity and data locality.
Reference: [Ban90] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proc. of the 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 192-219, </pages> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without actually performing the transformations and analyzing the resulting code. To overcome these problems, many researchers have proposed a uniform approach to transforming programs <ref> [Ban90, WL91, LMQ91, Fea92, ST92, KP93, ADY92] </ref>. The problem of finding the optimal program equivalent to some other program is undecidable in general, since it reduces to the "program equivalence problem"[Tou84]. Consequently existing systems make various simplifying assumptions.
Reference: [Fea92] <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, Part I, One-dimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(5), </volume> <month> Oct </month> <year> 1992. </year> <note> Postscript available as pub.ibp.fr:ibp/reports/masi.92/78.ps.Z. </note>
Reference-contexts: These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without actually performing the transformations and analyzing the resulting code. To overcome these problems, many researchers have proposed a uniform approach to transforming programs <ref> [Ban90, WL91, LMQ91, Fea92, ST92, KP93, ADY92] </ref>. The problem of finding the optimal program equivalent to some other program is undecidable in general, since it reduces to the "program equivalence problem"[Tou84]. Consequently existing systems make various simplifying assumptions. <p> We represent transformations by associating a multi-dimensional mapping with each statement. This allows us to represent a large class of transformations, including any combination of loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering. Most schedule based systems <ref> [LMQ91, Fea92, ADY92] </ref> determine which schedule to apply by using linear programming techniques to optimize a linear function representing execution time. Most of these systems assume that parallelism is the only factor that will affect execution time, i.e. they ignore issues such as simplicity and data locality. <p> Our parallelism granularity estimate for a given statement is the total number of iterations executed by sequential loops outside the outermost parallel loop. If all loops inside the outermost parallel loop are parallel, then our definition of parallelism granularity is equivalent to latency <ref> [Fea92] </ref>. for 10 i = 1, 8 do forall 10 k = 1, 100 do for 10 m = 1, 10 do This will be a good indicator of the amount of overhead incurred through starting up and synchronizing parallel loops and hence closely related to execution time.
Reference: [KM92] <author> K. Kennedy and K.S. McKinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 323-334, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: = 1,m do forall i = 1,n do c (i,j) = c (i,j)+a (i,k)*b (k,j) endfor endfor endfor T 10 : [i; j; k] ! [k; j; i] forall j = 1,p do forall i = 1,n do c (i,j) = c (i,j)+a (i,k)*b (k,j) endfor endfor endfor Many researchers <ref> [WL91, KM92] </ref> have determined experimentally that the best non-blocked 2 mapping for matrix 2 We can represent blocking transformations as mappings, however the ExtendMapping algorithm currently doesn't produce them. 12 multiply is the JKI mapping, closely followed by the KJI mapping.
Reference: [KP93] <author> Wayne Kelly and William Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> Technical Report CS-TR-3193, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without actually performing the transformations and analyzing the resulting code. To overcome these problems, many researchers have proposed a uniform approach to transforming programs <ref> [Ban90, WL91, LMQ91, Fea92, ST92, KP93, ADY92] </ref>. The problem of finding the optimal program equivalent to some other program is undecidable in general, since it reduces to the "program equivalence problem"[Tou84]. Consequently existing systems make various simplifying assumptions. <p> Nodes closer to the leaves correspond to mappings that are more fully specified that those higher in the tree. We use heuristics to limit the branching factor at each node, and use methods described in <ref> [KP93] </ref> to restrict our search to nodes which correspond to legal mappings. The monotone property of the performance estimators allows us to use an admissible heuristic search algorithm [Nil80] to find the optimal node (s) in the search tree. <p> Therefore it is important that the estimates we obtain for partially specified schedules be lower bounds on the estimates for all extensions of these mappings. This is precisely what the monotone property allows us to do. In Section 2 we summarize our earlier work <ref> [KP93] </ref> on developing a framework for unifying reordering transformations. In Section 3 we describe our algorithm for selecting mappings based on performance estimation. In Section 4 we define some terms and concepts used by our performance estimators and in Section 5 we describe the performance estimators we have implemented. <p> In Section 4 we define some terms and concepts used by our performance estimators and in Section 5 we describe the performance estimators we have implemented. In Section 6 we present results based on our implementation and in Section 7 we state our conclusions. 2 The Framework In <ref> [KP93] </ref> we presented a framework for unifying iteration reordering transformations. The framework is designed to provide a uniform way to represent and reason about transformations. 2.1 Iteration spaces A program implicitly specifies an iteration space. <p> Mapping b causes mapping a to be removed from the list iff: 9p p a &gt; MAND p _ 8p s:t: p b p a &gt; DESIR p 11 6 Results The framework described in <ref> [KP93] </ref> and the system described in this paper, have been implemented in our extension of Michael Wolfe's tiny tool. Our extension of tiny is available via anonymous ftp from ftp.cs.umd.edu in the directory pub/omega. In this section we give examples and results based on our implementation.
Reference: [LMQ91] <author> Herve Leverge, Christophe Mauras, and Patrice Quinton. </author> <title> A language-orientied approach to the design of systolic chips. </title> <journal> Journal of VLSI Signal Processing, </journal> <pages> pages 173-182, </pages> <month> Mar </month> <year> 1991. </year>
Reference-contexts: These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without actually performing the transformations and analyzing the resulting code. To overcome these problems, many researchers have proposed a uniform approach to transforming programs <ref> [Ban90, WL91, LMQ91, Fea92, ST92, KP93, ADY92] </ref>. The problem of finding the optimal program equivalent to some other program is undecidable in general, since it reduces to the "program equivalence problem"[Tou84]. Consequently existing systems make various simplifying assumptions. <p> We represent transformations by associating a multi-dimensional mapping with each statement. This allows us to represent a large class of transformations, including any combination of loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering. Most schedule based systems <ref> [LMQ91, Fea92, ADY92] </ref> determine which schedule to apply by using linear programming techniques to optimize a linear function representing execution time. Most of these systems assume that parallelism is the only factor that will affect execution time, i.e. they ignore issues such as simplicity and data locality.
Reference: [LP92] <author> Wei Li and Keshav Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <booktitle> In 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 249-260, </pages> <institution> Yale University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: In fact, the best we can do, if the variable part of the outermost loop is i + j, is one statement stride 0 after level 2, one statement stride O (1) in level 3 and one statement stride 0 after level 3. In their paper on access normalization <ref> [LP92] </ref>, Li and Pingali analyze the following code fragment: for i = 1 to n do for k = max (i-b+1,j-b+1,1) to min (i+b-1,j+b-1,n) do When we apply our methods to this example, we correctly predict that using certain skewed mapping such as i k or j i will produce the
Reference: [Nil80] <author> Nils J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1980. </year>
Reference-contexts: The performance estimators can also take as input, partially specified mappings. The performance estimators were carefully designed so as to satisfy the monotone property <ref> [Nil80] </ref>. A performance estimator P satisfies the monotone property iff: 8S; S 0 S 7! S 0 ) P (S) P (S 0 ) where S 7! S 0 means that mapping S 0 can be obtained by further specifying the partially specified mapping S. <p> We use heuristics to limit the branching factor at each node, and use methods described in [KP93] to restrict our search to nodes which correspond to legal mappings. The monotone property of the performance estimators allows us to use an admissible heuristic search algorithm <ref> [Nil80] </ref> to find the optimal node (s) in the search tree. Admissibility means that we are guaranteed to find the optimal node (s) in the tree despite the fact that we may not examine all of the nodes. <p> This section overviews our implementation of such a system which uses performance estimation to guide the selection of mappings. Figure 3 gives an outline of our algorithm. This algorithm is closely related to the A fl heuristic search algorithm <ref> [Nil80] </ref>.
Reference: [ST92] <author> Vivek Sarkar and Radhika Thekkath. </author> <title> A general framework for iteration-reordering loop transformations. </title> <booktitle> In ACM SIG-PLAN'92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 175-187, </pages> <address> San Francisco, California, </address> <month> Jun </month> <year> 1992. </year>
Reference-contexts: These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without actually performing the transformations and analyzing the resulting code. To overcome these problems, many researchers have proposed a uniform approach to transforming programs <ref> [Ban90, WL91, LMQ91, Fea92, ST92, KP93, ADY92] </ref>. The problem of finding the optimal program equivalent to some other program is undecidable in general, since it reduces to the "program equivalence problem"[Tou84]. Consequently existing systems make various simplifying assumptions.
Reference: [Tou84] <author> George J. Tourlakis. </author> <title> Computability. </title> <publisher> Reston Publishing Company, </publisher> <year> 1984. </year>
Reference: [WL91] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference-contexts: These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without actually performing the transformations and analyzing the resulting code. To overcome these problems, many researchers have proposed a uniform approach to transforming programs <ref> [Ban90, WL91, LMQ91, Fea92, ST92, KP93, ADY92] </ref>. The problem of finding the optimal program equivalent to some other program is undecidable in general, since it reduces to the "program equivalence problem"[Tou84]. Consequently existing systems make various simplifying assumptions. <p> = 1,m do forall i = 1,n do c (i,j) = c (i,j)+a (i,k)*b (k,j) endfor endfor endfor T 10 : [i; j; k] ! [k; j; i] forall j = 1,p do forall i = 1,n do c (i,j) = c (i,j)+a (i,k)*b (k,j) endfor endfor endfor Many researchers <ref> [WL91, KM92] </ref> have determined experimentally that the best non-blocked 2 mapping for matrix 2 We can represent blocking transformations as mappings, however the ExtendMapping algorithm currently doesn't produce them. 12 multiply is the JKI mapping, closely followed by the KJI mapping.
Reference: [Wol89] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Traditionally, optimizing compilers attempt to improve the performance of programs by applying source to source transformations, such as loop interchange, loop skewing and loop distribution <ref> [Wol89] </ref>. Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without actually performing the transformations and analyzing the resulting code.
References-found: 12

