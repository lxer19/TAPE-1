URL: http://www.csi.uottawa.ca/~holte/Publications/ml95.agnostic-pac.ps
Refering-URL: http://www.csi.uottawa.ca/~holte/Learning/other-sites.html
Root-URL: 
Email: pauer@igi.tu-graz.ac.at  holte@csi.uottawa.ca  maass@igi.tu-graz.ac.at  
Title: Theory and Applications of Agnostic PAC-Learning with Small Decision Trees  
Author: Peter Auer Robert C. Holte Wolfgang Maass 
Address: A-8010 Graz, Austria  Ottawa, Canada K1N 6N5  A-8010 Graz, Austria  
Affiliation: Institute for Theoretical Computer Science Technische Universitaet Graz  Computer Science Dept. University of Ottawa  Institute for Theoretical Computer Science Technische Universitaet Graz  
Abstract: We exhibit a theoretically founded algorithm T2 for agnostic PAC-learning of decision trees of at most 2 levels, whose computation time is almost linear in the size of the training set. We evaluate the performance of this learning algorithm T2 on 15 common real-world datasets, and show that for most of these datasets T2 provides simple decision trees with little or no loss in predictive power (compared with C4.5). In fact, for datasets with continuous attributes its error rate tends to be lower than that of C4.5. To the best of our knowledge this is the first time that a PAC-learning algorithm is shown to be applicable to real-world classification problems. Since one can prove that T2 is an agnostic PAC-learning algorithm, T2 is guaranteed to produce close to optimal 2-level decision trees from sufficiently large training sets for any (!) distribution of data. In this regard T2 differs strongly from all other learning algorithms that are considered in applied machine learning, for which no guarantee can be given about their performance on new datasets. We also demonstrate that this algorithm T2 can be used as a diagnostic tool for the investigation of the expressive limits of 2-level decision trees. Finally, T2, in combination with new bounds on the VC-dimension of decision trees of bounded depth that we derive, provides us now for the first time with the tools necessary for comparing learning curves of decision trees for real-world datasets with the theoretical estimates of PAC learning theory. 
Abstract-found: 1
Intro-found: 1
Reference: [AL88] <author> D. Angluin, P. Laird, </author> <title> Learning from noisy examples, </title> <journal> Machine Learning, </journal> <volume> vol. 2, </volume> <year> 1988, </year> <pages> 343 - 370. </pages>
Reference-contexts: make the original version of PAC-learning (where one focuses on hypothesis classes H n and distributions D such that there is some target concept C t 2 H n with Err D [C t ] = 0) more realistic, it has been extended to include certain noise models (see e.g. <ref> [AL88] </ref>, [EK91], [Elo92], [Kea93], [KL93], [Dec93]): the target concept C t 2 H n is either disguised by a large amount of white noise, or by a small (in comparison with the desired error rate of the learner) fraction of arbitrary (even malicious) noise.
Reference: [BB94] <author> M. Bohanec, I. Bratko, </author> <title> Trading Accuracy for Simplicity in Decision Trees, </title> <journal> Machine Learning, </journal> <volume> vol. 15, no. 3, </volume> <year> 1994, </year> <pages> 223 - 250. </pages>
Reference-contexts: Depth (or some other measure of static complexity) is a suitable complexity measure if the tree is to be interpreted by humans. In this case, a simple, although only approximately accurate concept definition may be more useful than a completely accurate definition which involves a lot of detail (p.223 <ref> [BB94] </ref>). From this perspective, C4.5's trees for G2, CR, PI, and SE are clearly too complex. For HD and IO, C4.5's trees are considerably more complex than T2's but only moderately more accurate. For the purposes described in [BB94], T2's trees are the more desirable. <p> than a completely accurate definition which involves a lot of detail (p.223 <ref> [BB94] </ref>). From this perspective, C4.5's trees for G2, CR, PI, and SE are clearly too complex. For HD and IO, C4.5's trees are considerably more complex than T2's but only moderately more accurate. For the purposes described in [BB94], T2's trees are the more desirable. CH and SP are special cases since they absolutely require a complex tree to attain high accuracy. In the framework defined in [BB94], the hypotheses produced by learning algorithms are not used to predict unseen instances, but to summarize the contents of a dataset. <p> For the purposes described in <ref> [BB94] </ref>, T2's trees are the more desirable. CH and SP are special cases since they absolutely require a complex tree to attain high accuracy. In the framework defined in [BB94], the hypotheses produced by learning algorithms are not used to predict unseen instances, but to summarize the contents of a dataset. An intriguing question is whether common real-world classification problems S can in fact be characterized accurately by a simple hypothesis of a given type, see e.g. [Elo94]. <p> It should be noted here that, because OPT <ref> [BB94] </ref> and 1Rw [Hol93] are heuristic measures, these values underestimate the ability of simple rules to summarize a dataset. <p> The average of the Sky2 values in Table 2 is 89.8%, and it is below 85% on only 4 of the datasets. Clearly, T2 is an excellent algorithm for the task of summarizing datasets defined in <ref> [BB94] </ref>. 4 LEARNING CURVES FOR DECISION TREES OF SMALL DEPTH At the heart of learning theory are certain statistical results (due to [Vap82], [BEHW89], [Hau92], [Tal94], [DL95] and others) that provide for any distribution D, and any size of the training set S train drawn according to D, an upper bound
Reference: [BEHW89] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, M. K. War-muth, </author> <title> Learnability and the Vapnik-Chervonenkis dimension, </title> <journal> JACM 36(4), 1989, </journal> <volume> 929 - 965. </volume>
Reference-contexts: Clearly, T2 is an excellent algorithm for the task of summarizing datasets defined in [BB94]. 4 LEARNING CURVES FOR DECISION TREES OF SMALL DEPTH At the heart of learning theory are certain statistical results (due to [Vap82], <ref> [BEHW89] </ref>, [Hau92], [Tal94], [DL95] and others) that provide for any distribution D, and any size of the training set S train drawn according to D, an upper bound for the difference between the true error Err D [ H] of a hypothesis H 2 H that minimizes Err S train [H]
Reference: [BFOS84] <author> L. Breiman, J. H. Friedman, R. A. Olshen, C. J. Stone, </author> <title> Classification and Regression Trees, </title> <publisher> Wadsworth (Belmont, </publisher> <year> 1984). </year>
Reference-contexts: As a special case T2 yields a computational tool for choosing optimal multi-variate splits with regard to the split criterion weighted inaccuracy (wacc). Lubinsky [Lub94] has shown that this split criterion wacc performs for many datasets as well as Gini <ref> [BFOS84] </ref>, if used as a criterion for greedy algorithms that build decision trees of unbounded depth. Another extension of algorithm T2 is considered in the following result, which may be of some practical interest for small datasets with few attributes and d = 3, or even d = 4.
Reference: [BN92] <author> W. Buntine, T. Niblett, </author> <title> A further comparison of splitting rules for decision-tree induction, </title> <journal> Machine Learning, </journal> <volume> vol. 8, </volume> <year> 1992, </year> <pages> 75 - 82. </pages>
Reference-contexts: In applied machine learning concrete datasets from quite diverse application domains are viewed as prototypes for real-world classification problems. The performance of many practical learning algorithms on these datasets is described in a number of interesting comparative studies (see e.g. [Min89], [WGT90], [WK90], [WK91], <ref> [BN92] </ref>, [Hol93]). Each dataset is a list of items (typically between a few dozen and several thousand), each item consisting of n attribute values (typically n &lt; 40) and an associated classification. The attributes might be continuous, ranging over R, or categorical, ranging over some finite set.
Reference: [CT92] <author> D. Cohn, G. Tesauro, </author> <title> How tight are the Vapnik-Chervonenkis bounds, </title> <booktitle> Neural Computation 4, 1992, </booktitle> <volume> 249 - 269. </volume>
Reference-contexts: However, because of the previous lack of algorithms that are sufficiently efficient so that one can actually compute such hypotheses H for interesting hypothesis classes H and real-world data, these theories have so far been tested only an artificial data (see e.g. [SSSD90], <ref> [CT92] </ref>). Our new algorithm T2 now permits us, for the first time, to actually compute a hypothesis H 2 TREE (2) that minimizes Err S train , hence we can evaluate the predictions of this essential piece of PAC-learning theory on real-world classification problems.
Reference: [Dec93] <author> S. E. Decatur, </author> <title> Statistical queries and faulty PAC oracles, </title> <booktitle> Proc. of the 6th ACM Conference on Computational Learning Theory, 1993, </booktitle> <volume> 262 - 268. </volume>
Reference-contexts: PAC-learning (where one focuses on hypothesis classes H n and distributions D such that there is some target concept C t 2 H n with Err D [C t ] = 0) more realistic, it has been extended to include certain noise models (see e.g. [AL88], [EK91], [Elo92], [Kea93], [KL93], <ref> [Dec93] </ref>): the target concept C t 2 H n is either disguised by a large amount of white noise, or by a small (in comparison with the desired error rate of the learner) fraction of arbitrary (even malicious) noise.
Reference: [DGM95] <author> D. P. Dobkin, D. Gunopulos, W. Maass, </author> <title> Computing the maximum bichromatic discrepancy, with applications in computer graphics and machine learning, </title> <note> invited paper for a special issue of the Journal of Computer and System Sciences. </note>
Reference-contexts: These negative results are quite disappointing, since in learning theory one usually views these as the simplest nontrivial hypothesis classes for continuous and boolean attributes respectively. On the other hand one did succeed in designing agnostic PAC-learning algorithms for a few hypothesis classes H n ([KSS92], [Maa93], [Maa94], <ref> [DGM95] </ref>). <p> 2, the associated intervals cannot be optimized independently from each other, and the most delicate part of the algorithm T2 is the reduction of this 2-dimensional optimization problem to a 1-dimensional problem that is more complicated, but which can be solved in O (m log m) computation steps (see [Maa94], <ref> [DGM95] </ref> for other applications of such a method).
Reference: [DL95] <author> L. Devroye, G. Lugosi, </author> <title> Lower bounds in pattern recognition and learning, appears in Pattern recognition. </title>
Reference-contexts: Clearly, T2 is an excellent algorithm for the task of summarizing datasets defined in [BB94]. 4 LEARNING CURVES FOR DECISION TREES OF SMALL DEPTH At the heart of learning theory are certain statistical results (due to [Vap82], [BEHW89], [Hau92], [Tal94], <ref> [DL95] </ref> and others) that provide for any distribution D, and any size of the training set S train drawn according to D, an upper bound for the difference between the true error Err D [ H] of a hypothesis H 2 H that minimizes Err S train [H] and the least <p> These guesses are somewhat unsatisfactory, but arguably the best approximation that we can achieve (without having access to further examples from the distribution D that generated S). [Hau92], [Tal94], <ref> [DL95] </ref> have achieved the best known theoretical bounds for the minimal size m of a training set that is needed to guarantee a certain value (respectively upper bound) for the abovementioned difference ". <p> Ignoring log-factors and the dependency on the confidence parameter ffi, these yield an upper bound m = O " 2 which holds for any distribution D [Tal94], and a lower bound m = W inf H2H Err D [H] " 2 which holds for some distribution D <ref> [DL95] </ref>. Unfortunately these upper and lower bounds differ by large constant factors, and hence one cannot predict from these bounds for concrete distributions D the actual functional dependency of m on ".
Reference: [DP93] <author> A. Danyluk, F. Provost, </author> <title> Small Disjuncts in Action: Learning to Diagnose Errors in the Local Loop of the Telephone Network, </title> <booktitle> Proc. 10th International Conf. Machine Learning (ML'93), </booktitle> <publisher> Morgan Kauf-mann, </publisher> <year> 1993, </year> <pages> 81 - 88. </pages>
Reference-contexts: Unfortunately the version with white noise does not model the situation that one encounters in the here considered real-world classification problems S (e.g. the systematic noise reported in <ref> [DP93] </ref>). On the other hand, in the model with malicious noise the PAC-learner can only achieve error rates that are intolerably large from the point of view of applied machine learning.
Reference: [DSS93] <author> H. Druker, R. Schapire, P. Simard, </author> <title> Improving performance in neural networks using a boosting algorithm, </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> vol. 5, </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1993, </year> <pages> 42-49. </pages>
Reference-contexts: It should be mentioned in this context, that although T2 is apparently the first PAC-learning algorithm that is tested on real-world classification problems, there has previously been already a fruitful migration of various ideas from PAC-learning theory into applications (see e.g. <ref> [DSS93] </ref>). In applied machine learning concrete datasets from quite diverse application domains are viewed as prototypes for real-world classification problems. The performance of many practical learning algorithms on these datasets is described in a number of interesting comparative studies (see e.g. [Min89], [WGT90], [WK90], [WK91], [BN92], [Hol93]).
Reference: [EK91] <author> T. Elomaa, J. Kivinen, </author> <title> Learning decision trees from noisy examples, </title> <type> Report A-1991-3, </type> <institution> Dept. of Computer Science, University of Helsinki (1991). </institution>
Reference-contexts: the original version of PAC-learning (where one focuses on hypothesis classes H n and distributions D such that there is some target concept C t 2 H n with Err D [C t ] = 0) more realistic, it has been extended to include certain noise models (see e.g. [AL88], <ref> [EK91] </ref>, [Elo92], [Kea93], [KL93], [Dec93]): the target concept C t 2 H n is either disguised by a large amount of white noise, or by a small (in comparison with the desired error rate of the learner) fraction of arbitrary (even malicious) noise.
Reference: [Elo92] <author> T. Elomaa, </author> <title> A hybrid approach to decision tree learning, </title> <type> Report C-1992-61, </type> <institution> Dept. of Computer Science, University of Helsinki (1992). </institution>
Reference-contexts: original version of PAC-learning (where one focuses on hypothesis classes H n and distributions D such that there is some target concept C t 2 H n with Err D [C t ] = 0) more realistic, it has been extended to include certain noise models (see e.g. [AL88], [EK91], <ref> [Elo92] </ref>, [Kea93], [KL93], [Dec93]): the target concept C t 2 H n is either disguised by a large amount of white noise, or by a small (in comparison with the desired error rate of the learner) fraction of arbitrary (even malicious) noise.
Reference: [Elo94] <author> T. Elomaa, </author> <title> In defense of C4.5: Notes on learning one-level decision trees, </title> <booktitle> Proc. of the 11th Int. Conf. on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994, </year> <pages> 62 - 69. </pages>
Reference-contexts: An intriguing question is whether common real-world classification problems S can in fact be characterized accurately by a simple hypothesis of a given type, see e.g. <ref> [Elo94] </ref>.
Reference: [Hau92] <author> D. Haussler, </author> <title> Decision theoretic generalizations of the PAC-model for neural nets and other learning applications, </title> <journal> Inf. and Comp., </journal> <volume> vol. 100, </volume> <year> 1992, </year> <pages> 78 - 150. </pages>
Reference-contexts: Of course the expected value of the true error Err D [A (S train )] will in general depend not only on A, but also on the size m of S train and on the underlying distribution D. Agnostic PAC-learning (due to <ref> [Hau92] </ref> and [KSS92]) is the variant of PAC-learning (due to [Val84]) most relevant to practical machine learning. It differs from normal PAC-learning in two important ways. <p> fi 2 f1; : : : ; tg : H (x i n ) 6= x i fi algorithm if A (S train ) can be computed with a number of computation steps that is bounded by a polynomial in the (bit-length) size of the representation of S train . <ref> [Hau92] </ref> and [KSS92] have shown that there is an efficient PAC-learning algorithm for a family of hypothesis classes H n if and only if the VC-dimension of H n grows polynomially in n and there is a polynomial time algorithm that computes for any set S train of items a hypothesis <p> Proof: It is easy to show (see section 4) that for fixed p the VC-dimension of TREE (2; n; p; K) is bounded by a polynomial in n and K. This fact, in combination with Theorem 1, implies according to <ref> [Hau92] </ref> that T2 is an efficient agnostic PAC-learning algorithm. According to Theorem 1 the algorithm T2 outputs a tree T that minimizes the disagreement between T and L, i.e. jfx 2 L : T (x 1 ; : : : ; x n ) 6= x 0 gj. <p> Clearly, T2 is an excellent algorithm for the task of summarizing datasets defined in [BB94]. 4 LEARNING CURVES FOR DECISION TREES OF SMALL DEPTH At the heart of learning theory are certain statistical results (due to [Vap82], [BEHW89], <ref> [Hau92] </ref>, [Tal94], [DL95] and others) that provide for any distribution D, and any size of the training set S train drawn according to D, an upper bound for the difference between the true error Err D [ H] of a hypothesis H 2 H that minimizes Err S train [H] and <p> These guesses are somewhat unsatisfactory, but arguably the best approximation that we can achieve (without having access to further examples from the distribution D that generated S). <ref> [Hau92] </ref>, [Tal94], [DL95] have achieved the best known theoretical bounds for the minimal size m of a training set that is needed to guarantee a certain value (respectively upper bound) for the abovementioned difference ".
Reference: [HKST94] <author> D. Haussler, M. Kearns, H. S. Seung, N. Tishby, </author> <title> Rigorous learning curve bounds from statistical mechanics, </title> <booktitle> Proc. of the 7th Annual ACM Conference on Computational Learning Theory 1994, ACM-Press (1994), </booktitle> <volume> 76 - 87. </volume>
Reference: [Hol93] <author> R. C. Holte, </author> <title> Very simple classification rules perform well on most commonly used datasets, </title> <journal> Machine Learning, </journal> <volume> vol. 11, </volume> <year> 1993, </year> <pages> 63 - 91. </pages>
Reference-contexts: In applied machine learning concrete datasets from quite diverse application domains are viewed as prototypes for real-world classification problems. The performance of many practical learning algorithms on these datasets is described in a number of interesting comparative studies (see e.g. [Min89], [WGT90], [WK90], [WK91], [BN92], <ref> [Hol93] </ref>). Each dataset is a list of items (typically between a few dozen and several thousand), each item consisting of n attribute values (typically n &lt; 40) and an associated classification. The attributes might be continuous, ranging over R, or categorical, ranging over some finite set. <p> Note that TREE (1; n; p; K) is the hypothesis class that is used by Holte's learning algorithm 1R <ref> [Hol93] </ref>. In our experiments we have always chosen K := p + 1. <p> C4.5's hypothesis class includes decision trees of arbitrary depth, but with only binary splits on continuous attributes. Of the fifteen datasets used in the experiments nine (BC,CH,G2,HD,HE,IR,LA,SE,SO) have already been used in <ref> [Hol93] </ref>, and six are new. AP, the appendicitis dataset in [WGT90], was kindly supplied by S. Weiss of Rutgers University. The other five new datasets were obtained from the UCI repository. 4 Table 1 gives the datasets' main characteristics. Size is the total number of examples in the dataset. <p> It should be noted here that, because OPT [BB94] and 1Rw <ref> [Hol93] </ref> are heuristic measures, these values underestimate the ability of simple rules to summarize a dataset.
Reference: [HSV93] <author> K. U. Hoeffgen, H. U. Simon, K. S. Van Horn, </author> <title> Robust trainability of single neurons, </title> <note> preprint (1993) </note>
Reference-contexts: One reason is perhaps that there do exist two remarkable negative results. It has been shown that neither for H n = fhalfspaces over R n g <ref> [HSV93] </ref> nor for H n = fmonomials over n boolean attributesg [KSS92] does there exist an efficient agnostic PAC-learning algorithm (unless RP = N P).
Reference: [Kea93] <author> M. Kearns, </author> <title> Efficient noise-tolerant learning from statistical queries, </title> <booktitle> Proc. of the 25th ACM Symp. on the Theory of Computing, 1993, </booktitle> <volume> 392 - 401. </volume>
Reference-contexts: version of PAC-learning (where one focuses on hypothesis classes H n and distributions D such that there is some target concept C t 2 H n with Err D [C t ] = 0) more realistic, it has been extended to include certain noise models (see e.g. [AL88], [EK91], [Elo92], <ref> [Kea93] </ref>, [KL93], [Dec93]): the target concept C t 2 H n is either disguised by a large amount of white noise, or by a small (in comparison with the desired error rate of the learner) fraction of arbitrary (even malicious) noise.
Reference: [KL93] <author> M. Kearns, M. Li, </author> <title> Learning in the presence of malicious errors, </title> <journal> SIAM J. Comput., </journal> <volume> vol. 22, </volume> <year> 1993, </year> <pages> 807 - 837. </pages>
Reference-contexts: of PAC-learning (where one focuses on hypothesis classes H n and distributions D such that there is some target concept C t 2 H n with Err D [C t ] = 0) more realistic, it has been extended to include certain noise models (see e.g. [AL88], [EK91], [Elo92], [Kea93], <ref> [KL93] </ref>, [Dec93]): the target concept C t 2 H n is either disguised by a large amount of white noise, or by a small (in comparison with the desired error rate of the learner) fraction of arbitrary (even malicious) noise.
Reference: [KSS92] <author> M. J. Kearns, R. E. Schapire, L. M. Sellie, </author> <title> Toward efficient agnostic learning, </title> <booktitle> Proc. of the 5th ACM Workshop on Computational Learning Theory, 1992, </booktitle> <volume> 341 - 352. </volume>
Reference-contexts: Of course the expected value of the true error Err D [A (S train )] will in general depend not only on A, but also on the size m of S train and on the underlying distribution D. Agnostic PAC-learning (due to [Hau92] and <ref> [KSS92] </ref>) is the variant of PAC-learning (due to [Val84]) most relevant to practical machine learning. It differs from normal PAC-learning in two important ways. <p> f1; : : : ; tg : H (x i n ) 6= x i fi algorithm if A (S train ) can be computed with a number of computation steps that is bounded by a polynomial in the (bit-length) size of the representation of S train . [Hau92] and <ref> [KSS92] </ref> have shown that there is an efficient PAC-learning algorithm for a family of hypothesis classes H n if and only if the VC-dimension of H n grows polynomially in n and there is a polynomial time algorithm that computes for any set S train of items a hypothesis H 2 <p> One reason is perhaps that there do exist two remarkable negative results. It has been shown that neither for H n = fhalfspaces over R n g [HSV93] nor for H n = fmonomials over n boolean attributesg <ref> [KSS92] </ref> does there exist an efficient agnostic PAC-learning algorithm (unless RP = N P). These negative results are quite disappointing, since in learning theory one usually views these as the simplest nontrivial hypothesis classes for continuous and boolean attributes respectively.
Reference: [Lub94] <author> D. J. Lubinsky, </author> <title> Bivariate Splits and Consistent Split Criteria in Dichotomous Classification Trees, </title> <institution> HD-Dissertation in Computer Science, Rutgers University (1994). </institution>
Reference-contexts: with b (fi) b possible values (in which case b (fi)+1 edges leave the root of T , labeled by 1, : : : , b (fi), missing), or 2 For a special case of depth 2 decision trees (corners) a O (m log m) algorithm was already given in <ref> [Lub94] </ref> one queries a continuous attribute fi (in which case 3 edges leave the root, labeled by I 1 ; I 2 ; missing, for some partition I 1 ; I 2 of R into two intervals) 3 . <p> However T2 can easily be adapted to optimize instead of the disagreement any other additive split criterion in the sense of Lubinsky <ref> [Lub94] </ref>. In particular it can be used to minimize the total cost of all misclassifications for any given confusion matrix [WK91]. As a special case T2 yields a computational tool for choosing optimal multi-variate splits with regard to the split criterion weighted inaccuracy (wacc). Lubinsky [Lub94] has shown that this split <p> in the sense of Lubinsky <ref> [Lub94] </ref>. In particular it can be used to minimize the total cost of all misclassifications for any given confusion matrix [WK91]. As a special case T2 yields a computational tool for choosing optimal multi-variate splits with regard to the split criterion weighted inaccuracy (wacc). Lubinsky [Lub94] has shown that this split criterion wacc performs for many datasets as well as Gini [BFOS84], if used as a criterion for greedy algorithms that build decision trees of unbounded depth.
Reference: [Maa93] <author> W. Maass, </author> <title> Agnostic PAC-learning of functions on analog neural nets, </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> vol. 6, </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1994, </year> <note> 311-318; journal version to appear in Neural Computation. </note>
Reference-contexts: These negative results are quite disappointing, since in learning theory one usually views these as the simplest nontrivial hypothesis classes for continuous and boolean attributes respectively. On the other hand one did succeed in designing agnostic PAC-learning algorithms for a few hypothesis classes H n ([KSS92], <ref> [Maa93] </ref>, [Maa94], [DGM95]).
Reference: [Maa94] <author> W. Maass, </author> <title> Efficient Agnostic PAC-Learning with Simple Hypotheses, </title> <booktitle> Proc. of the 7th Annual ACM Conference on Computational Learning Theory, </booktitle> <year> 1994, </year> <pages> 67-75. </pages>
Reference-contexts: These negative results are quite disappointing, since in learning theory one usually views these as the simplest nontrivial hypothesis classes for continuous and boolean attributes respectively. On the other hand one did succeed in designing agnostic PAC-learning algorithms for a few hypothesis classes H n ([KSS92], [Maa93], <ref> [Maa94] </ref>, [DGM95]). <p> level 2, the associated intervals cannot be optimized independently from each other, and the most delicate part of the algorithm T2 is the reduction of this 2-dimensional optimization problem to a 1-dimensional problem that is more complicated, but which can be solved in O (m log m) computation steps (see <ref> [Maa94] </ref>, [DGM95] for other applications of such a method).
Reference: [Min89] <author> J. Mingers, </author> <title> An empirical comparison of pruning methods for decision tree induction, </title> <journal> Machine Learning, </journal> <volume> vol. 4, </volume> <year> 1989, </year> <pages> 227 - 243. </pages>
Reference-contexts: In applied machine learning concrete datasets from quite diverse application domains are viewed as prototypes for real-world classification problems. The performance of many practical learning algorithms on these datasets is described in a number of interesting comparative studies (see e.g. <ref> [Min89] </ref>, [WGT90], [WK90], [WK91], [BN92], [Hol93]). Each dataset is a list of items (typically between a few dozen and several thousand), each item consisting of n attribute values (typically n &lt; 40) and an associated classification.
Reference: [Qui92] <author> J. R. Quinlan, C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: In fact, even for extremely successful practical learning algorithms such as C4.5 <ref> [Qui92] </ref> it is relatively easy to construct distributions (respectively datasets) for which they do not perform well in the abovementioned sense, and hence these are not agnostic PAC-learning algorithms. In practice, however, training sets are virtually always smaller than the size needed for these theoretical performance guarantees.
Reference: [SSSD90] <author> D. B. Schwartz, V. K. Samalan, S. A. Solla, J. S. Denker, </author> <title> Exhaustive Learning, </title> <booktitle> Neural Computation 2, 1990, </booktitle> <volume> 374 - 385. </volume>
Reference-contexts: However, because of the previous lack of algorithms that are sufficiently efficient so that one can actually compute such hypotheses H for interesting hypothesis classes H and real-world data, these theories have so far been tested only an artificial data (see e.g. <ref> [SSSD90] </ref>, [CT92]). Our new algorithm T2 now permits us, for the first time, to actually compute a hypothesis H 2 TREE (2) that minimizes Err S train , hence we can evaluate the predictions of this essential piece of PAC-learning theory on real-world classification problems.
Reference: [Tal94] <author> M. Talagrand, </author> <title> Sharper bounds for Gaussian and empirical processes, </title> <journal> Annals of Probability, </journal> <volume> vol. 22, </volume> <year> 1994, </year> <pages> 28-76. </pages>
Reference-contexts: Clearly, T2 is an excellent algorithm for the task of summarizing datasets defined in [BB94]. 4 LEARNING CURVES FOR DECISION TREES OF SMALL DEPTH At the heart of learning theory are certain statistical results (due to [Vap82], [BEHW89], [Hau92], <ref> [Tal94] </ref>, [DL95] and others) that provide for any distribution D, and any size of the training set S train drawn according to D, an upper bound for the difference between the true error Err D [ H] of a hypothesis H 2 H that minimizes Err S train [H] and the <p> These guesses are somewhat unsatisfactory, but arguably the best approximation that we can achieve (without having access to further examples from the distribution D that generated S). [Hau92], <ref> [Tal94] </ref>, [DL95] have achieved the best known theoretical bounds for the minimal size m of a training set that is needed to guarantee a certain value (respectively upper bound) for the abovementioned difference ". <p> Ignoring log-factors and the dependency on the confidence parameter ffi, these yield an upper bound m = O " 2 which holds for any distribution D <ref> [Tal94] </ref>, and a lower bound m = W inf H2H Err D [H] " 2 which holds for some distribution D [DL95].
Reference: [TLS89] <author> N. Tishby, E. Lavin, S. A. Solla, </author> <title> Consistent inference of probabilities in layered networks: Predictions and generalizations, </title> <booktitle> Proc. of IJCNN 1989, </booktitle> <volume> Vol. II, 403 - 409. </volume>
Reference: [Val84] <author> L. G. Valiant, </author> <title> A theory of the learnable, </title> <journal> Comm. of the ACM, </journal> <volume> vol. 27, </volume> <year> 1984, </year> <pages> 1134 - 1142. </pages>
Reference-contexts: 1 INTRODUCTION Numerous articles have been written about the design and analysis of algorithms for PAC-learning, since fl Currently with the University of California at Santa Cruz. Valiant <ref> [Val84] </ref> had introduced the model for probably approximately correct learning in 1984. In applied machine learning an even larger literature exists about the performance of various other learning algorithms on real-world classification tasks. <p> Agnostic PAC-learning (due to [Hau92] and [KSS92]) is the variant of PAC-learning (due to <ref> [Val84] </ref>) most relevant to practical machine learning. It differs from normal PAC-learning in two important ways.
Reference: [Vap82] <author> V. N. Vapnik, </author> <title> Estimations of Dependencies Based on Empirical Data, </title> <publisher> Springer (New York, </publisher> <year> 1982). </year>
Reference-contexts: Clearly, T2 is an excellent algorithm for the task of summarizing datasets defined in [BB94]. 4 LEARNING CURVES FOR DECISION TREES OF SMALL DEPTH At the heart of learning theory are certain statistical results (due to <ref> [Vap82] </ref>, [BEHW89], [Hau92], [Tal94], [DL95] and others) that provide for any distribution D, and any size of the training set S train drawn according to D, an upper bound for the difference between the true error Err D [ H] of a hypothesis H 2 H that minimizes Err S train
Reference: [WGT90] <author> S. M. Weiss, R. Galen, P. V. Tadepalli, </author> <title> Maximizing the predictive value of production rules, Art. </title> <journal> Int., </journal> <volume> vol. 45, </volume> <year> 1990, </year> <pages> 47 - 71. </pages>
Reference-contexts: In applied machine learning concrete datasets from quite diverse application domains are viewed as prototypes for real-world classification problems. The performance of many practical learning algorithms on these datasets is described in a number of interesting comparative studies (see e.g. [Min89], <ref> [WGT90] </ref>, [WK90], [WK91], [BN92], [Hol93]). Each dataset is a list of items (typically between a few dozen and several thousand), each item consisting of n attribute values (typically n &lt; 40) and an associated classification. The attributes might be continuous, ranging over R, or categorical, ranging over some finite set. <p> C4.5's hypothesis class includes decision trees of arbitrary depth, but with only binary splits on continuous attributes. Of the fifteen datasets used in the experiments nine (BC,CH,G2,HD,HE,IR,LA,SE,SO) have already been used in [Hol93], and six are new. AP, the appendicitis dataset in <ref> [WGT90] </ref>, was kindly supplied by S. Weiss of Rutgers University. The other five new datasets were obtained from the UCI repository. 4 Table 1 gives the datasets' main characteristics. Size is the total number of examples in the dataset. Classes is the number of classes in the dataset. <p> Unfortunately very little information of this type is available at this point, since it is usually too time consuming to compute min H2H Err S (H) for interesting classes H of simple hypotheses and common datasets S (for an exception see <ref> [WGT90] </ref> for the case of production rules of length 3 on the dataset AP ). It should be noted here that, because OPT [BB94] and 1Rw [Hol93] are heuristic measures, these values underestimate the ability of simple rules to summarize a dataset.
Reference: [WK90] <author> S. M. Weiss, I. Kapouleas, </author> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods, </title> <booktitle> Proc. of the 11th Int. Joint Conf. on Art. Int. 1990, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> 781 - 787. </pages>
Reference-contexts: In applied machine learning concrete datasets from quite diverse application domains are viewed as prototypes for real-world classification problems. The performance of many practical learning algorithms on these datasets is described in a number of interesting comparative studies (see e.g. [Min89], [WGT90], <ref> [WK90] </ref>, [WK91], [BN92], [Hol93]). Each dataset is a list of items (typically between a few dozen and several thousand), each item consisting of n attribute values (typically n &lt; 40) and an associated classification. The attributes might be continuous, ranging over R, or categorical, ranging over some finite set.
Reference: [WK91] <author> S. M. Weiss, C. A. </author> <title> Kulikowski, Computer Systems that Learn, 1991, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In applied machine learning concrete datasets from quite diverse application domains are viewed as prototypes for real-world classification problems. The performance of many practical learning algorithms on these datasets is described in a number of interesting comparative studies (see e.g. [Min89], [WGT90], [WK90], <ref> [WK91] </ref>, [BN92], [Hol93]). Each dataset is a list of items (typically between a few dozen and several thousand), each item consisting of n attribute values (typically n &lt; 40) and an associated classification. The attributes might be continuous, ranging over R, or categorical, ranging over some finite set. <p> However T2 can easily be adapted to optimize instead of the disagreement any other additive split criterion in the sense of Lubinsky [Lub94]. In particular it can be used to minimize the total cost of all misclassifications for any given confusion matrix <ref> [WK91] </ref>. As a special case T2 yields a computational tool for choosing optimal multi-variate splits with regard to the split criterion weighted inaccuracy (wacc).
References-found: 34

