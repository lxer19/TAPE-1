URL: http://www.ri.cmu.edu/afs/cs/user/kseymore/html/papers/SLT_96_CMU_LM.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs/user/kseymore/html/papers.html
Root-URL: 
Title: Language and Pronunciation Modeling in the CMU 1996 Hub 4 Evaluation  
Author: Kristie Seymore, Stanley Chen, Maxine Eskenazi, and Ronald Rosenfeld 
Address: Pittsburgh, Pennsylvania 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: We describe several language and pronunciation modeling techniques that were applied to the 1996 Hub 4 Broadcast News transcription task. These include topic adaptation, the use of remote corpora, vocabulary size optimization, n-gram cutoff optimization, modeling of spontaneous speech, handling of unknown linguistic boundaries, higher order n-grams, weight optimization in rescoring, and lexical modeling of phrases and acronyms. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> P. Clarkson and A. Robinson. </author> <title> Language model adaptation using mixtures and an exponentially decaying cache. </title> <booktitle> In Proceedings of ICASSP-97, </booktitle> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: TOPIC ADAPTATION We are currently looking at methods of topic adaptation in unrestricted domains, using the BN domain as our testbed due to its semantic richness. Adapting statistical language models using topic information has been successful in the past (for example, <ref> [1, 3, 10] </ref>), but the majority of adaptation attempts have focused either on a one-of-N classification, where a new document is assumed to belong to only one of a (typically small) number of disjoint topic sets, or on coarse topic classification, where only a few topics are defined. <p> Other possible solutions include assigning half of each duplicated story to each leaf, or using supervised clustering to make reasonable decisions. Agglomerative clustering has been used successfully for topic adaptation in a mixture modeling framework <ref> [1, 3] </ref>. However, one advantage of retaining a high number of individual topic clusters, instead of merging the clusters down to a small number, is the ability to make fine distinctions between different subjects and mix unusual topics together that may occur in a future story.
Reference: 2. <author> Mitch Weintraub et al. </author> <title> Fast training and portability. </title> <booktitle> In 1995 Language Modeling Summer Research Workshop: Technical Reports, Center for Language and Speech Processing, </booktitle> <institution> Johns Hopkins University, Baltimore, </institution> <year> 1995. </year>
Reference-contexts: The FP column indicates whether or not filled pauses were predicted from their unigram probabilities, and the Posterior column indicates whether or not the model interpolation weights were weighted by the unigram probability of the last word in the history <ref> [2] </ref>. Rescoring Stories A and B with the topic language score results in a lower word error rate (41.7%) than using the Katz trigram score (42.6%). However, more improvement was obtained by rescoring with the Kneser-Ney trigram model (40.9%).
Reference: 3. <author> R. Iyer and M. Ostendorf. </author> <title> Modeling long distance dependence in language: Topic mixtures vs. dynamic cache models. </title> <booktitle> In Proceedings of the ICSLP, </booktitle> <pages> pages 236239, </pages> <year> 1996. </year>
Reference-contexts: TOPIC ADAPTATION We are currently looking at methods of topic adaptation in unrestricted domains, using the BN domain as our testbed due to its semantic richness. Adapting statistical language models using topic information has been successful in the past (for example, <ref> [1, 3, 10] </ref>), but the majority of adaptation attempts have focused either on a one-of-N classification, where a new document is assumed to belong to only one of a (typically small) number of disjoint topic sets, or on coarse topic classification, where only a few topics are defined. <p> Other possible solutions include assigning half of each duplicated story to each leaf, or using supervised clustering to make reasonable decisions. Agglomerative clustering has been used successfully for topic adaptation in a mixture modeling framework <ref> [1, 3] </ref>. However, one advantage of retaining a high number of individual topic clusters, instead of merging the clusters down to a small number, is the ability to make fine distinctions between different subjects and mix unusual topics together that may occur in a future story.
Reference: 4. <author> Slava M. Katz. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3):400401, </volume> <month> March </month> <year> 1987. </year>
Reference-contexts: Unless otherwise specified, all WER's in this paper were produced using this methodology. 2.5. Smoothing We compared two different smoothing techniques for trigram models: Katz smoothing <ref> [4] </ref> and Kneser-Ney smoothing [5]. Training on 130M words of Broadcast News data, we measured a perplexity of 237 for Katz smoothing and a perplexity of 219 for Kneser-Ney smoothing on the first two-thirds of the development set.
Reference: 5. <author> Reinhard Kneser and Hermann Ney. </author> <title> Improved backing-off for m-gram language modeling. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 181184, </pages> <year> 1995. </year>
Reference-contexts: Unless otherwise specified, all WER's in this paper were produced using this methodology. 2.5. Smoothing We compared two different smoothing techniques for trigram models: Katz smoothing [4] and Kneser-Ney smoothing <ref> [5] </ref>. Training on 130M words of Broadcast News data, we measured a perplexity of 237 for Katz smoothing and a perplexity of 219 for Kneser-Ney smoothing on the first two-thirds of the development set. By adding extra parameters to Kneser-Ney smoothing, 1 we lowered the perplexity to 211. <p> For the right-most point in the graph (corresponding to a training 3 We use Kneser-Ney smoothing <ref> [5] </ref>. set of about 75M words), a 7-gram model has 15% lower perplexity than a trigram model. In addition, from the graph it seems likely this difference will be greater for larger training sets.
Reference: 6. <author> M. Ostendorf, A. Kannan, S. Austin, O. Kimball, R. Schwartz, and J. R. Rohlicek. </author> <title> Integration of diverse recognition methodologies through reevaluation of n-best sentence hypotheses. </title> <booktitle> In Proceedings of the DARPA Workshop on Speech and Natural Language, </booktitle> <pages> pages 8387, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: In order to combine multiple scores effectively, it is necessary to choose appropriate values for the weights w i in equation (1). To do this, we use a similar methodology as developed in <ref> [6] </ref>. We implemented Powell's algorithm as described in Numerical Recipes in C [7, pp. 309-317] to automatically search for optimal weights given a set of N -best lists and the corresponding hypotheses' error rates.
Reference: 7. <author> W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Vetter-ling. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: In order to combine multiple scores effectively, it is necessary to choose appropriate values for the weights w i in equation (1). To do this, we use a similar methodology as developed in [6]. We implemented Powell's algorithm as described in Numerical Recipes in C <ref> [7, pp. 309-317] </ref> to automatically search for optimal weights given a set of N -best lists and the corresponding hypotheses' error rates. We search for the values of w i that minimize the WER of the highest scoring utterance in each N -best list.
Reference: 8. <author> Ronald Rosenfeld. </author> <title> Optimizing lexical and n-gram coverage via judicious use of linguistic data. </title> <booktitle> In Proceedings of Eurospeech 95, </booktitle> <pages> pages 17631766, </pages> <year> 1995. </year>
Reference-contexts: The perplexity of the Hub 4 development set using this model is 231. 2.1. Vocabulary Size Optimization To investigate the effect of vocabulary size on recognition WER, we used the methodology developed in <ref> [8] </ref>. The change in WER produced by an increase in vocabulary size is composed of two main factors: an increase in WER due to the increased acoustic confus-ability between words in the vocabulary, and a decrease in WER due to a decreased out-of-vocabulary (OOV) frequency. <p> This figure suggests that a vocabulary in the range of 40k 60k would be appropriate for the Broadcast News task, at least for F0 and F1. See <ref> [8] </ref> for analogous experiments on the NAB corpus. confusability and OOV rate, F0+F1 condition. 2.2. Linguistic Boundaries In the Hub 4 task, the test data is divided into acoustic segments that do not generally correspond to linguistic segments such as sentences; acoustic segments often contain multiple sentences.
Reference: 9. <author> G. Salton. </author> <title> Developments in automatic text retrieval. </title> <booktitle> Science, </booktitle> <address> 253:974980, </address> <year> 1991. </year>
Reference-contexts: The weight of each word in the vector is given by the (tf fi idf) measure frequently used in information retrieval <ref> [9] </ref>: w ik = tf ik log (N=n k ) (2) The term frequency, tf, is the number of times word k appears in cluster i. <p> Given a new text represented by weight vector D j , the topic similarity between cluster i and the new text can be computed with the following cosine measure <ref> [9] </ref>: sim (D j ; D i ) = k=1 w jk w ik P t P t (3) Equation 3 gives the cosine of the angle between the two vectors representing the two sets of text. It is normalized for vector length, so that large clusters are not favored.
Reference: 10. <author> Satoshi Sekine and Ralph Grisham. </author> <title> NYU language modeling experiments for the 1995 CSR evaluation. </title> <booktitle> In Proceedings of the ARPA Spoken Language Systems Technology Workshop, </booktitle> <year> 1995. </year> <title> Language Score FP Posterior Story A+B WER Oracle (best in list) 34.4% Katz 3-gram no N/A 42.6% Kneser-Ney 3-gram no N/A 41.8% Kneser-Ney 3-gram yes N/A 40.9% Topic no no 42.0% Topic yes no 41.7% Topic no yes 42.1% Topic yes yes 41.7% Table 10: WER's for Stories A and B combined, using different language scores. </title>
Reference-contexts: TOPIC ADAPTATION We are currently looking at methods of topic adaptation in unrestricted domains, using the BN domain as our testbed due to its semantic richness. Adapting statistical language models using topic information has been successful in the past (for example, <ref> [1, 3, 10] </ref>), but the majority of adaptation attempts have focused either on a one-of-N classification, where a new document is assumed to belong to only one of a (typically small) number of disjoint topic sets, or on coarse topic classification, where only a few topics are defined.
References-found: 10

