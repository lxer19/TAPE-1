URL: http://www.cse.ogi.edu/~mak/PS/icslp96.ps
Refering-URL: http://www.cse.ogi.edu/CSLU/personnel/bios/mak.html
Root-URL: http://www.cse.ogi.edu
Email: fmak, barnardg@cse.ogi.edu  
Title: PHONE CLUSTERING USING THE BHATTACHARYYA DISTANCE  for Spoken Language Understanding  
Author: Brian Mak Etienne Barnard 
Address: 20000 N.W. Walker Road, Portland, OR 97006  
Affiliation: Center  Oregon Graduate Institute of Science and Technology  
Abstract: In this paper we study using the classification-based Bhat-tacharyya distance measure to guide biphone clustering. The Bhattacharyya distance is a theoretical distance measure between two Gaussian distributions which is equivalent to an upper bound on the optimal Bayesian classification error probability. It also has the desirable properties of being com-putationally simple and extensible to more Gaussian mixtures. Using the Bhattacharyya distance measure in a data-driven approach together with a novel 2-Level Agglomerative Hierarchical Biphone Clustering algorithm, generalized left/right biphones (BGBs) are derived. A neural-net based phone recognizer trained on the BGBs is found to have better frame-level phone recognition than one trained on generalized biphones (BCGBs) derived from a set of commonly-used broad categories. We further evaluate the new BGBs on an isolated-word recognition task of perplexity 40 and obtain a 16.2% error reduction over the broad-category generalized biphones (BCGBs) and a 41.8% error reduction over the monophones. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> E. Barnard, R. A. Cole, M Fanty, and P. Vermeulen. </author> <title> "Real-World Speech Recognition With Neural Networks". </title> <editor> In R. J. Alspector and T. X. Brown, editors, </editor> <booktitle> Applications of Neural Networks to Telecommunications (IWANNT95), </booktitle> <volume> volume 2, </volume> <pages> pages 186-193. </pages> <publisher> Lawrence Erl-baum Assoc., </publisher> <address> Hillsdale, New Jersey, </address> <year> 1995. </year>
Reference-contexts: One hundred test sets of perplexity 40 are constructed by randomly choosing twenty male speaking names and twenty female speaking names 100 times without replacement. The three phonetic nets (net-MONO, net-BCGB, and net-BGB) are used to recognize each test set of names using Viterbi search with a lexical tree <ref> [1] </ref>. Recognition results are averaged over the 100 test sets as shown in Table 2. The new generalized biphones (BGBs) obtain an error reduction of 42.1% over the monophones, and 16.2% over the broad-category generalized biphones (BCGBs). 6.
Reference: 2. <author> R. A. Cole, M. Noel, T. Lander, and T. Durham. </author> <title> "New Telephone Speech Corpora at CSLU". </title> <booktitle> Proceedings of Eu-rospeech, </booktitle> <pages> pages 821-824, </pages> <month> Sep </month> <year> 1995. </year>
Reference-contexts: It is more meaningful to test the three phone types on a real-world recognition task. Four thousand phonetically transcribed names are selected from the OGI Names Corpus <ref> [2] </ref> with balanced genders. One hundred test sets of perplexity 40 are constructed by randomly choosing twenty male speaking names and twenty female speaking names 100 times without replacement.
Reference: 3. <author> L. Deng, V. Gupta, M. Lennig, P. Kenny, and P. Mermelstein. </author> <title> "Acoustic Recognition Component of an 86,000-Word Speech Recognizer". </title> <booktitle> Proceedings of IEEE ICASSP, </booktitle> <pages> pages 741-744, </pages> <year> 1990. </year>
Reference-contexts: Currently, the most popular choices of subword units are the generalized biphones (GB) or generalized triphones (GT). There are two basic approaches to derive these generalized phones: (1) The knowledge-driven approach employs linguistic knowledge about the coarticulatory influences between neighboring phones. For example, L. Deng et al. <ref> [3] </ref> defined contexts based on broad phonetic categories and classified the articulatory effects on vowels and consonants each to 5 types and derived 25 generalized contexts for each phone. A.
Reference: 4. <author> P. D'Orta, M. Ferretti, and S. Scarci. </author> <title> "Phoneme Classification for Real Time Speech Recognition of Italian". </title> <booktitle> Proceedings of IEEE ICASSP, </booktitle> <pages> pages 81-84, </pages> <year> 1987. </year>
Reference-contexts: This usually uses an information theoretic distance measure commonly employed with Hidden Markov models. Examples are works from D'Orta et al. <ref> [4] </ref>, Juang and Rabiner [10], and Lee [6]. Lee et al. [7] later suggested another tree-based allophone clustering method.
Reference: 5. <author> K. Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, Inc., </publisher> <address> 2nd edition, </address> <year> 1990. </year>
Reference-contexts: It also has the desirable properties of being computationally simple while at the same time being extensible to more Gaussian mixtures. 2. BHATTACHARYYA DISTANCE The Bhattacharyya distance is covered in many texts on statistical pattern recognition (for example, <ref> [5] </ref>).
Reference: 6. <author> K. F. Lee. </author> <title> "Context-Dependent Phonetic Hidden Markov Models for Speaker-Independent Continuous Speech Recognition". </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 38(4) </volume> <pages> 599-609, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: This usually uses an information theoretic distance measure commonly employed with Hidden Markov models. Examples are works from D'Orta et al. [4], Juang and Rabiner [10], and Lee <ref> [6] </ref>. Lee et al. [7] later suggested another tree-based allophone clustering method. All allophones are placed in the root of the decision tree and each node of the tree is associated with a binary question, which is selected from a set derived by a linguistic expert.
Reference: 7. <author> K. F. Lee, S. Hayamizu, H. W. Hon, C. Huang, J. Swartz, and R. Weide. </author> <title> "Allophone Clustering For Continuous Speech Recognition". </title> <booktitle> Proceedings of IEEE ICASSP, </booktitle> <pages> pages 749-752, </pages> <year> 1990. </year>
Reference-contexts: This usually uses an information theoretic distance measure commonly employed with Hidden Markov models. Examples are works from D'Orta et al. [4], Juang and Rabiner [10], and Lee [6]. Lee et al. <ref> [7] </ref> later suggested another tree-based allophone clustering method. All allophones are placed in the root of the decision tree and each node of the tree is associated with a binary question, which is selected from a set derived by a linguistic expert.
Reference: 8. <author> A. Ljolje. </author> <title> "High Accuracy Phone Recognition Using Context Clustering and Quasi-Triphonic Models". </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 8 </volume> <pages> 129-151, </pages> <year> 1994. </year>
Reference-contexts: For example, L. Deng et al. [3] defined contexts based on broad phonetic categories and classified the articulatory effects on vowels and consonants each to 5 types and derived 25 generalized contexts for each phone. A. Ljolje <ref> [8] </ref> used more detailed contextual effects to derive a set of 19 left-context classes and 18 right-context classes. (2) The data-driven approach evaluates all contexts in the training data, and uses some distance measure with a clustering algorithm to split or merge the contexts to a specified number of generalized contexts.
Reference: 9. <author> Y.K. Muthusamy, R.A. Cole, and B. T. Oshika. </author> <title> "The OGI Multi-Language Telephone Speech Corpus". </title> <booktitle> Proceedings of ICSLP, </booktitle> <address> II:895-898, </address> <month> Oct </month> <year> 1992. </year>
Reference-contexts: As an example, Fig. 1 shows a clustering tree of 39 context-independent monophones obtained by using a standard agglomerative hierarchical clustering (AHC) procedure and guided by a Bhattacharyya distance matrix computed over the OGI TS corpus <ref> [9] </ref>. It can be seen that most clusters at the bottom level are well-known confusable pairs: fl, wg, fm, ng, fer, rg, fae, ehg, fch, jhg, fd, tg, fs, zg, and ff, thg. 4.
Reference: 10. <author> L. R. Rabiner, C. H. Lee, B. H. Juang, and J. G. Wilpon. </author> <title> "HMM Clustering for Connected Word Recognition". </title> <booktitle> Proceedings of IEEE ICASSP, </booktitle> <pages> pages 405-408, </pages> <year> 1989. </year>
Reference-contexts: This usually uses an information theoretic distance measure commonly employed with Hidden Markov models. Examples are works from D'Orta et al. [4], Juang and Rabiner <ref> [10] </ref>, and Lee [6]. Lee et al. [7] later suggested another tree-based allophone clustering method. All allophones are placed in the root of the decision tree and each node of the tree is associated with a binary question, which is selected from a set derived by a linguistic expert.
References-found: 10

