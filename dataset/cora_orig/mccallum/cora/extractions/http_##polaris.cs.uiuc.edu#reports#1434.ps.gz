URL: http://polaris.cs.uiuc.edu/reports/1434.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: A Uniform Internal Representation for High-Level and Instruction-Level Transformations  
Author: Eduard Ayguad, Cristina Barrado, Jess Labarta, David Lpez, Susana Moreno, David Padua, and Mateo Valero Departament dArquitectura de Computadors 
Address: Barcelona  
Affiliation: Universitat Politcnica de Catalunya  
Abstract-found: 0
Intro-found: 1
Reference: [ABCC88] <author> F. Allen, M. Burke, P. Charles, R. Cytron and J. Ferrante. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <journal> Journal of Parallel and Distributed Computing. </journal> <volume> Vol.5, </volume> <year> 1988. </year>
Reference-contexts: 1 Introduction Compiler techniques for automatic detection of parallelism are often described in the literature and implemented as source-to-source transformations [Wolf82, Zima91, BENP93]. These implementations are usually experimental translators for the parallelization and vectorization of loops <ref> [ABCC88, PGHL89, BEKG94] </ref>. Low-level operations, such as those needed to compute addresses of arrays and scalar parameters, are usually hidden for these source-to-source translators which only represent internally high-level constructs.
Reference: [BEKG94] <author> B. Blume, R. Eigenmann, K. Faigin, J. Grout, J. Hoeinger, D. Padua, P. Petersen, B. Pottenger, L. Rauchwerger, P. Tu and S. Weatherford. </author> <title> Polaris: The Next Generation in Parallelizing Compilers. </title> <booktitle> Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing. </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction Compiler techniques for automatic detection of parallelism are often described in the literature and implemented as source-to-source transformations [Wolf82, Zima91, BENP93]. These implementations are usually experimental translators for the parallelization and vectorization of loops <ref> [ABCC88, PGHL89, BEKG94] </ref>. Low-level operations, such as those needed to compute addresses of arrays and scalar parameters, are usually hidden for these source-to-source translators which only represent internally high-level constructs. <p> In fact, we have implemented the translation to low level form in both Polaris <ref> [BEKG94] </ref> and Parafrase-2 [PGHL89] at the cost of only six man-months. 21 It is also more general because most of the optimizations needed to generate efficient code, such as common subexpression elimination, are already available in many source-to-source translators.
Reference: [BENP93] <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic Program Parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2), </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Compiler techniques for automatic detection of parallelism are often described in the literature and implemented as source-to-source transformations <ref> [Wolf82, Zima91, BENP93] </ref>. These implementations are usually experimental translators for the parallelization and vectorization of loops [ABCC88, PGHL89, BEKG94]. Low-level operations, such as those needed to compute addresses of arrays and scalar parameters, are usually hidden for these source-to-source translators which only represent internally high-level constructs.
Reference: [BaLA94] <author> C. Barrado, J. Labarta and E. Ayguad. </author> <title> An Efficient Scheduling for Doacross Loops. </title> <booktitle> Proceedings of the ISMM Parallel and Distributed Computing and Systems. </booktitle> <year> 1994. </year>
Reference-contexts: For instance, the evaluation of program characteristics such as inherent parallelism [BoLB93] and implementation of scheduling algorithms <ref> [BaLB94, BaLA94] </ref> that try to attain the available parallelism and improve memory locality would be possible with more detail.
Reference: [BaLB94] <author> C. Barrado, J. Labarta and P Borenzstejn. </author> <booktitle> Implementation of GTS. Proceedings of the Int. Conf. on Parallel ARchitectures and Languages Europe. </booktitle> <year> 1994. </year>
Reference-contexts: For instance, the evaluation of program characteristics such as inherent parallelism [BoLB93] and implementation of scheduling algorithms <ref> [BaLB94, BaLA94] </ref> that try to attain the available parallelism and improve memory locality would be possible with more detail.
Reference: [BoLB93] <author> P. Borensztejn, J. Labarta and C. Barrado. </author> <title> Measures of Parallelism at Compile Time. </title> <booktitle> Proceedings of the 1st EUROMICRO Workshop on Parallel and Distributed Processing. </booktitle> <year> 1993. </year>
Reference-contexts: For instance, the evaluation of program characteristics such as inherent parallelism <ref> [BoLB93] </ref> and implementation of scheduling algorithms [BaLB94, BaLA94] that try to attain the available parallelism and improve memory locality would be possible with more detail.
Reference: [Dixi91] <author> K. Dixit, </author> <title> The SPEC Benchmarks, </title> <journal> Parallel Computing, </journal> <volume> No. 17, </volume> <year> 1991. </year> <month> 25 </month>
Reference-contexts: The optimization of spill code is another area that needs further research effort. Having the compiler whose development is proposed in this report would allow to perform an evaluation of the architectural features and algorithms to exploit them for real programs (benchmarks for the Perfect Club [Poin89], SPEC <ref> [Dixi91] </ref>, and proprietary applications). Otherwise, the evaluation is restricted to small benchmarks and loops such as the Livermore Loops [McMa72]. 7.2 Vector Processors and Decoupled Architectures Another project deals with the study of vector processors and decoupled architectures.
Reference: [Espa94] <author> R. Espasa et al., </author> <title> Quantitative Analysis of Vector Code, </title> <booktitle> Proceedings of the 3rd EUROMICRO Workshop on Parallel and Distributed Processing. </booktitle> <year> 1995. </year>
Reference-contexts: The first part of the project focuses on the evaluation of the Convex architecture including a number of measurements of the characteristics of vector instructions and their frequency <ref> [Espa94] </ref>. It has been necessary to work with the assembly code generated directly by the Convex compiler. This limits the type of analysis that can be done and complicates the instrumentation of the code for dynamic measurements.
Reference: [LlVA95] <author> J. Llosa, M. Valero and E. Ayguad. </author> <title> Non-consistent Dual Register Files to Reduce Register Pressure. </title> <booktitle> Proceedings of the 1st Int. Symposium on High Performance Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: The impact in performance of having a finite number of registers in the register file is also evaluated in terms of increase in memory traffic and slowdown. Two different register organizations are proposed in [LVFA94] and <ref> [LlVA95] </ref>. In [LVFA94], a new organization consisting of a small high-bandwidth multiported register file and a low-bandwidth port-limited register file (called sack) is presented. <p> The sack has a single read/write port and therefore it is cheaper (in area) and faster than the multiported file, so it can contain a high number of physical registers. An algorithm to assign values to the sack is proposed. In <ref> [LlVA95] </ref> a non-consistent dual register file is presented as a new register organization to reduce register pressure. <p> The two subfiles are consistent in the sense that both store exactly the same value in the same registers. This implementation reduces the complexity of the register file. In <ref> [LlVA95] </ref> the authors 23 propose and evaluate an organization where each subfile can be accessed independently of the other and store different values; this gives the freedom of storing some values in the two subfiles in a consistent way or storing some values in just one of the two subfiles.
Reference: [LVAL94] <author> J. Llosa, M. Valero, E. Ayguad and J. Labarta. </author> <title> Register Requirements of Pipelined Loops and its Effects on Performance. </title> <booktitle> Proceedings of the 2nd Int. Workshop on Massive Parallelism. </booktitle> <year> 1994. </year>
Reference-contexts: This does not only include an evaluation of architectural features, but also the evaluation of algorithms to perform low-level scheduling, register allocation and spill code. For instance, <ref> [LVAL94] </ref> studies the register requirements of software pipelined loops for different superscalar and superpipelined configurations. The impact in performance of having a finite number of registers in the register file is also evaluated in terms of increase in memory traffic and slowdown.
Reference: [LVFA94] <author> J. Llosa, M. Valero, J. Fortes and E. Ayguad. </author> <title> Using Sacks to Organize Registers in VLIW Machines. </title> <booktitle> Proceedings of the CONPAR94-VAPP-VI conference. </booktitle> <year> 1994. </year>
Reference-contexts: The impact in performance of having a finite number of registers in the register file is also evaluated in terms of increase in memory traffic and slowdown. Two different register organizations are proposed in <ref> [LVFA94] </ref> and [LlVA95]. In [LVFA94], a new organization consisting of a small high-bandwidth multiported register file and a low-bandwidth port-limited register file (called sack) is presented. <p> The impact in performance of having a finite number of registers in the register file is also evaluated in terms of increase in memory traffic and slowdown. Two different register organizations are proposed in <ref> [LVFA94] </ref> and [LlVA95]. In [LVFA94], a new organization consisting of a small high-bandwidth multiported register file and a low-bandwidth port-limited register file (called sack) is presented.
Reference: [McMa72] <author> F.McMahon, </author> <title> Fortran CPU Performance Analysis. </title> <institution> Lawrence Livermore Laboratories, </institution> <year> 1972. </year>
Reference-contexts: Otherwise, the evaluation is restricted to small benchmarks and loops such as the Livermore Loops <ref> [McMa72] </ref>. 7.2 Vector Processors and Decoupled Architectures Another project deals with the study of vector processors and decoupled architectures. The first part of the project focuses on the evaluation of the Convex architecture including a number of measurements of the characteristics of vector instructions and their frequency [Espa94].
Reference: [PGHL89] <author> C. Polychronopoulos, M. Girkar, M. Haghighat, C. Lee, B. Leung. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <booktitle> Proceedings of the Int. Conf. on Parallel Processing, </booktitle> <volume> Vol. II. </volume> <year> 1989. </year>
Reference-contexts: 1 Introduction Compiler techniques for automatic detection of parallelism are often described in the literature and implemented as source-to-source transformations [Wolf82, Zima91, BENP93]. These implementations are usually experimental translators for the parallelization and vectorization of loops <ref> [ABCC88, PGHL89, BEKG94] </ref>. Low-level operations, such as those needed to compute addresses of arrays and scalar parameters, are usually hidden for these source-to-source translators which only represent internally high-level constructs. <p> In fact, we have implemented the translation to low level form in both Polaris [BEKG94] and Parafrase-2 <ref> [PGHL89] </ref> at the cost of only six man-months. 21 It is also more general because most of the optimizations needed to generate efficient code, such as common subexpression elimination, are already available in many source-to-source translators.
Reference: [Poin89] <author> L. </author> <title> Pointer, </title> <type> Perfect Report: 1, CSRD Report No. 896, </type> <institution> University of Illinois, </institution> <year> 1989. </year>
Reference-contexts: The optimization of spill code is another area that needs further research effort. Having the compiler whose development is proposed in this report would allow to perform an evaluation of the architectural features and algorithms to exploit them for real programs (benchmarks for the Perfect Club <ref> [Poin89] </ref>, SPEC [Dixi91], and proprietary applications). Otherwise, the evaluation is restricted to small benchmarks and loops such as the Livermore Loops [McMa72]. 7.2 Vector Processors and Decoupled Architectures Another project deals with the study of vector processors and decoupled architectures.
Reference: [ScKo86] <author> R. G. Scarborough and H. G. Kolsky. </author> <title> A Vectorizing Fortran Compiler. </title> <journal> IBM Journal of Research and Development. </journal> <volume> Vol. 30, No. 2, </volume> <pages> pp. 163-171. </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: Another approach that also uses a uniform representation for both high-level parallelization and scalar optimizations was the one followed in the IBM Fortran compiler <ref> [ScKo86] </ref>. The main difference with our approach is that this compiler evolved from a traditional back-end compiler which was extended to do some of the high-level transformations usually performed in other systems by a source-to-source translator.
Reference: [Stal94] <author> R.M. Stallman. </author> <title> Using and Porting GNU CC. Free Software Foundation, </title> <year> 1994. </year>
Reference: [TWLP91] <author> S.Tjiang, M.Wolf, M.Lam, K.Pieper and J.Hennessy. </author> <title> Integrating Scalar Optimization and Parallelization. </title> <booktitle> 4th Workshop on Languages and Compilers for Parallel Computing. </booktitle> <year> 1991. </year>
Reference-contexts: The main difference with our approach is that this compiler evolved from a traditional back-end compiler which was extended to do some of the high-level transformations usually performed in other systems by a source-to-source translator. The Stanford University Intermediate Form (SUIF) <ref> [TWLP91] </ref> has also a similar goal as the representation described here. Again the difference is that SUIF extends low-level operations with annotations and many of the transformations have been implemented by extending low-level passes so that they can recognize the high-level annotations.
Reference: [WhDh93] <author> S. White and S. Dhawan. POWER2: </author> <title> Next Generation of the RISC System/6000 Family. IBM RISC System/6000 Technology: Volume II. </title> <institution> IBM Corporation. </institution> <year> 1993. </year>
Reference-contexts: An algorithm to assign values to the sack is proposed. In [LlVA95] a non-consistent dual register file is presented as a new register organization to reduce register pressure. This organization is inspired in the implementation of the register files of some new processors such as the Power2 <ref> [WhDh93] </ref>; the register file is implemented as two register subfiles with the same number of registers, same number of write ports but half the number of read ports into each register subfile.
Reference: [Wolf82] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD Thesis. </type> <institution> University of Illinois. Department of Computer Science. </institution> <year> 1982. </year>
Reference-contexts: 1 Introduction Compiler techniques for automatic detection of parallelism are often described in the literature and implemented as source-to-source transformations <ref> [Wolf82, Zima91, BENP93] </ref>. These implementations are usually experimental translators for the parallelization and vectorization of loops [ABCC88, PGHL89, BEKG94]. Low-level operations, such as those needed to compute addresses of arrays and scalar parameters, are usually hidden for these source-to-source translators which only represent internally high-level constructs.
Reference: [Zima91] <author> H.P. Zima. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press. </publisher> <address> New York, NY. </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Compiler techniques for automatic detection of parallelism are often described in the literature and implemented as source-to-source transformations <ref> [Wolf82, Zima91, BENP93] </ref>. These implementations are usually experimental translators for the parallelization and vectorization of loops [ABCC88, PGHL89, BEKG94]. Low-level operations, such as those needed to compute addresses of arrays and scalar parameters, are usually hidden for these source-to-source translators which only represent internally high-level constructs.
References-found: 20

