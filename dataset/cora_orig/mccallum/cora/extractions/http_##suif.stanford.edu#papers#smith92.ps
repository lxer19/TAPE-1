URL: http://suif.stanford.edu/papers/smith92.ps
Refering-URL: http://suif.stanford.edu/papers/papers.html
Root-URL: 
Title: Efficient Superscalar Performance Through Boosting  
Author: Michael D. Smith, Mark Horowitz, Monica S. Lam 
Affiliation: Computer Systems Laboratory Stanford University  
Abstract: The foremost goal of superscalar processor design is to increase performance through the exploitation of instruction-level parallelism (ILP). Previous studies have shown that speculative execution is required for high instruction per cycle (IPC) rates in non-numerical applications. The general trend has been toward supporting speculative execution in complicated, dynamically-scheduled processors. Performance, though, is more than just a high IPC rate; it also depends upon instruction count and cycle time. Boosting is an architectural technique that supports general speculative execution in simpler, statically-scheduled processors. Boosting labels speculative instructions with their control dependence information. This labelling eliminates control dependence constraints on instruction scheduling while still providing full dependence information to the hardware. We have incorporated boosting into a trace-based, global scheduling algorithm that exploits ILP without adversely affecting the instruction count of a program. We use this algorithm and estimates of the boosting hardware involved to evaluate how much speculative execution support is really necessary to achieve good performance. We find that a statically-scheduled superscalar processor using a minimal implementation of boosting can easily reach the performance of a much more complex dynamically-scheduled superscalar processor. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.V. Aho, R. Sethi, and J.D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Illegal speculative movements are recognizable if we can determine what values are needed when the non-predicted edge of the block is taken. This information is exactly the information that is provided by live variable analysis <ref> [1] </ref>. By checking the live-IN sets of the non-predicted successor blocks against the destination register of the current instruction, an algorithm can determine when a speculative movement is illegal. By using the exception and live variable analysis information, an algorithm can boost instructions only when necessary for correctness.
Reference: [2] <author> D. Bernstein and M. Rodeh. </author> <title> Global Instruction Scheduling for Superscalar Machines. </title> <booktitle> In Proc. ACM SIGPLAN 91 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. 241255, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The next step taken by global scheduling researchers was to extend the neighborhood to include conditional pairs [27] (also called equivalent basic blocks <ref> [2] </ref>). Two basic blocks are equivalent if and only if the execution of one block implies the execution of the other block; equivalence is simply a combination of the move-op and unification transformations of Percolation Scheduling for control-independent basic blocks. <p> The key difference between schedulers is how the available instruction set is generated, and the types of global transformation used. Bernstein and Rodeh <ref> [2] </ref> describe a scheduling algorithm that looks in neighbor and peer basic blocks for available instructions. Neighbor and peer basic blocks are only a small set of the blocks from which instructions are available, and thus, this decision greatly limits the size of the available set.
Reference: [3] <author> D. Bernstein, D. Cohen, and H. Krawczyk. </author> <title> Code Duplication: An Assist for Global Instruction Scheduling. </title> <booktitle> In Proceedings of MICRO-24, </booktitle> <pages> pp. 103113, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: For example, Figure 3 contains part of a CFG in which blocks A and D are equivalent. Let us assume that the machine architecture in this figure uses a delayed branching scheme. Under their definition of availability (called M-ready <ref> [3] </ref>), instruction i 4 is not available for scheduling in the delay slot of the branch at the end of basic block A until instruction i 3 in block B is scheduled.
Reference: [4] <author> D.G. Bradlee, S. J. Eggers, and R.R. Henry. </author> <title> Integrating Register Allocation and Instruction Scheduling for RISCs. </title> <booktitle> In the Proc. Fourth Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 122 131, </pages> <month> April, </month> <year> 1991. </year>
Reference: [5] <author> B. </author> <title> Case. Superscalar Techniques: SuperSPARC vs. </title> <type> 88110. Microprocessor Report, </type> <institution> 5(22):111, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: A growing perception is that dynamically-scheduled superscalar processors are the only effective way to couple instruction scheduling and speculative execution [19]. This perception seems to be supported in the commercial world as superscalar implementations move from dynamic dependence checking (e.g. the Sun Super-SPARC <ref> [5] </ref>) toward more complex dynamic scheduling techniques with support for speculative execution (e.g. the Motorola 88110 [5]). Yet, this hardware-intensive approach has a fundamental problem: these machines analyze only a small window of instructions at a time and use simplistic heuristics for choosing among the available instructions. <p> This perception seems to be supported in the commercial world as superscalar implementations move from dynamic dependence checking (e.g. the Sun Super-SPARC <ref> [5] </ref>) toward more complex dynamic scheduling techniques with support for speculative execution (e.g. the Motorola 88110 [5]). Yet, this hardware-intensive approach has a fundamental problem: these machines analyze only a small window of instructions at a time and use simplistic heuristics for choosing among the available instructions. Thus they are not guaranteed to generate a good instruction schedule.
Reference: [6] <author> P.P. Chang, S.A. Mahlke, W.Y. Chen, N.J. Warter, and W.W. Hwu. </author> <title> IMPACT: An Architectural Framework for Multiple-Instruction-Issue Processors. </title> <booktitle> In the Proc. 18th Int. Symp. on Computer Architecture, </booktitle> <pages> pp. 266275, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Gross and Ward [12] describe some modifications to Trace Scheduling to improve the transformations and optimize the compensation code, but the general algorithm still does not consider the cost of compensation code on the off-traces during the scheduling of the main trace. The IMPACT compiler <ref> [6] </ref> also uses the concept of traces to obtain a scheduling algorithm with a more global calculation of availability. <p> Section 3.2.1 reviews the top-level scheduling process for the first implementation of our algorithm. This implementation uses traces as our global view of the program structure. We found that most conditional branches in non-numerical code are predictable (see Table 1 on page 9 and Chang et al. <ref> [6] </ref>), and therefore, traces are a good, first approximation of the entire availability set. Unlike the original Trace Scheduling algorithm though, we maintain the concept of basic blocks within the trace so that can tightly control the scheduling process and limit the penalties imposed on the less-likely traces.
Reference: [7] <author> R.P. Colwell, R.P. Nix, J.J. ODonnell, D.B. Papworth, P.K. Rodman. </author> <title> A VLIW Architecture for a Trace Scheduling Compiler. </title> <booktitle> In the Proc. Second Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 180192, </pages> <month> October, </month> <year> 1987. </year>
Reference: [8] <author> K. Ebcioglu and A. Nicolau. </author> <title> A Global Resource-Constrained Parallelization Technique. </title> <booktitle> In Proc. 3rd Int. Conf. on Supercomputing, </booktitle> <pages> pp. 154163, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Like the original Trace Scheduling algorithm, IMPACT does not worry about the cost of compensation on the less-likely traces. Ebcioglu and Nicolau <ref> [8] </ref> discuss an approach to instruction scheduling called Percolation Scheduling with resources (PSr) that is more global than Trace Scheduling in its calculation of available instructions.
Reference: [9] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The Program Dependence Graph and Its Use in Optimization. </title> <journal> ACM Trans. Program. Lang. Syst., </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year> <month> 12 </month>
Reference-contexts: We refer to these types of algorithms as neighbor and peer scheduling algorithms. Tokoro [27] discusses some early versions of iterative neighbor and peer scheduling. Region Scheduling [13] is another iterative neighbor and peer scheduling algorithm that uses a program dependence graph <ref> [9] </ref> to determine equivalent basic blocks. All of these algorithms repeatedly apply transformations using a local policy. This type of incremental scheme does not always lead to a good global schedule. More recent work has focused on implementing a global instruction scheduler that uses a global policy during instruction scheduling.
Reference: [10] <author> J.A. Fisher. </author> <title> Trace Scheduling: A Technique for Global Microcode Compaction. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-30(7):478-490, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: The tradeoff is between different types of hardware complexity (e.g. our renameable buffer space versus the tagged data architecture of the non-excepting instruction architectures). 3 Global Instruction Scheduling Global scheduling algorithms, such as Trace Scheduling <ref> [10] </ref> and Percolation Scheduling [20], define a framework within which a compiler can perform code motions across basic block boundaries. This section begins with a discussion of the existing global scheduling algorithms, and explains the tradeoffs involved in construct ing a scheduler. <p> Even though instruction i 4 is executed twice along the path ABD, we have not lengthened the execution time of that path (the delay slot cycle exists whether the scheduler fills it or not), and we have shortened the execution time of ACD (the more frequent path). Trace Scheduling <ref> [10] </ref> was the first attempt at an instruction sched-uler with a more global calculation of availability. Trace Scheduling uses probabilities to select a trace of basic blocks. From this trace, a directed acyclic graph (DAG) is built which contains all of the necessary constraints on code motion within the trace.
Reference: [11] <author> J.R. Goodman and W.C. Hsu. </author> <title> Code Scheduling and Register Allocation in Large Basic Blocks. </title> <booktitle> In Proc. 1988 Int. Conf. on Supercomputing, </booktitle> <pages> pp. 442452, </pages> <month> July </month> <year> 1988. </year>
Reference: [12] <author> T. Gross and M. Ward. </author> <title> The Suppression of Compensation Code. </title> <booktitle> In Advances in Languages and Compilers for Parallel Processing, </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> pp. 260 273, </pages> <year> 1991. </year>
Reference-contexts: This cost can be significant for applications where there is more than one important trace. The insertion of compensation code to maintain semantic correctness in the face of global code motions is referred to as bookkeeping. Gross and Ward <ref> [12] </ref> describe some modifications to Trace Scheduling to improve the transformations and optimize the compensation code, but the general algorithm still does not consider the cost of compensation code on the off-traces during the scheduling of the main trace.
Reference: [13] <author> R. Gupta and M.L. Soffa. </author> <title> Region Scheduling: An Approach for Detecting and Redistributing Parallelism. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 16(4):421431, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: We refer to these types of algorithms as neighbor and peer scheduling algorithms. Tokoro [27] discusses some early versions of iterative neighbor and peer scheduling. Region Scheduling <ref> [13] </ref> is another iterative neighbor and peer scheduling algorithm that uses a program dependence graph [9] to determine equivalent basic blocks. All of these algorithms repeatedly apply transformations using a local policy. This type of incremental scheme does not always lead to a good global schedule.
Reference: [14] <author> P.Y.T. Hsu and E.S. Davidson. </author> <title> Highly Concurrent Scalar Processing. </title> <booktitle> In Proc. 13th Int. Symp. on Computer Architecture, </booktitle> <pages> pp. 386395, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: While these compiler-based approaches have the benefit of much simpler issue hardware, they have been limited in their ability to use speculative execution. To augment these global scheduling algorithms, a number of researchers have proposed architectural techniques (e.g. guarding <ref> [14] </ref> and non-excepting instructions [6][7]) which extend, but still limit, the compilers ability to schedule instructions for speculative execution. Recently we proposed a general architectural mechanism called boosting that provides the compiler with an unconstrained model of speculative execution [23]. <p> To accomplish this goal, we require an architectural technique that removes the control dependence constraints on static instruction scheduling. Guarded instruction architectures <ref> [14] </ref> and non-excepting instruction architectures [6][7] are the most widely accepted of these architectural techniques. Guarded instruction architectures predicate a control dependent operation with its dependent branch condition.
Reference: [15] <author> W.W. Hwu and Y.N. Patt. </author> <title> Checkpoint Repair for Out-of-order Execution Machines. </title> <booktitle> In Proc. 14th Int. Symp. on Computer Architecture, </booktitle> <pages> pp. 1826, </pages> <month> June </month> <year> 1987. </year>
Reference: [16] <author> N.P. Jouppi and D.W. Wall. </author> <title> Available Instruction-Level Parallelism for Superscalar and Superpipelined Machines. </title> <booktitle> In Proc. Third Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 272 282, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: For computer architects, the next step is obvious: drive the IPC above one by executing multiple instructions per cycle. Multiple instruction execution machines exploit instruction-level parallelism through superscalar and/or superpipelined techniques. And as Jouppi and Wall <ref> [16] </ref> point out, these techniques are equivalent in their ability to exploit ILP. In non-numerical applications, the amount of ILP is limited.
Reference: [17] <author> G. Kane. </author> <title> MIPS R2000 RISC Architecture. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1987. </year>
Reference-contexts: To determine what shadow structures are necessary, we must determine what effects are possible for all non-branch 5 instructions in a particular architecture. We use the MIPS R2000 architecture <ref> [17] </ref> as our base architecture. In the MIPS architecture, there are three possible effects from instruction execution: a register is written, a memory location is written, or an exception is signalled. Shadow structures are therefore required for the register file and for the 5.
Reference: [18] <author> R.M. Keller. </author> <title> Look-Ahead Processors. </title> <journal> Computing Surveys, </journal> <volume> 7(4):177195, </volume> <month> December </month> <year> 1975. </year>
Reference-contexts: The key insight in implementing the shadow structures is to realize that the data needs only to logically move on a commit. This logical move is implemented by a technique that is similar to register renaming <ref> [18] </ref>. For each sequential register in the architecture, we physically build a pool of register and counter pairs. These counters contain the logical name of each physical register.
Reference: [19] <author> S. Melvin and Y. Patt. </author> <title> Exploiting Fine-Grained Parallelism Through a Combination of Hardware and Software Techniques. </title> <booktitle> In Proc. 18th Int. Symp. on Computer Architecture, </booktitle> <pages> pp. 287296, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: A growing perception is that dynamically-scheduled superscalar processors are the only effective way to couple instruction scheduling and speculative execution <ref> [19] </ref>. This perception seems to be supported in the commercial world as superscalar implementations move from dynamic dependence checking (e.g. the Sun Super-SPARC [5]) toward more complex dynamic scheduling techniques with support for speculative execution (e.g. the Motorola 88110 [5]).
Reference: [20] <author> A. Nicolau. </author> <title> Percolation Scheduling: A Parallel Compilation Technique. </title> <type> Computer Sciences Technical Report 85-678, </type> <institution> Cornell University, </institution> <month> May </month> <year> 1985. </year>
Reference-contexts: The tradeoff is between different types of hardware complexity (e.g. our renameable buffer space versus the tagged data architecture of the non-excepting instruction architectures). 3 Global Instruction Scheduling Global scheduling algorithms, such as Trace Scheduling [10] and Percolation Scheduling <ref> [20] </ref>, define a framework within which a compiler can perform code motions across basic block boundaries. This section begins with a discussion of the existing global scheduling algorithms, and explains the tradeoffs involved in construct ing a scheduler. <p> The early attempts at global scheduling understood the basic rules for code motion between basic blocks, and they optimized a program by repeatedly moving instructions between dynamically adjacent basic blocks to improve the local schedules. The culmination of these iterative, neighborhood scheduling algorithms is Percolation Scheduling <ref> [20] </ref> which describes a complete set of semantics-preserving transformations for moving any operation between adjacent blocks. The next step taken by global scheduling researchers was to extend the neighborhood to include conditional pairs [27] (also called equivalent basic blocks [2]).
Reference: [21] <author> J.E. Smith and A.R. Pleszkun. </author> <title> Implementation of Precise Interrupts in Pipelined Processors. </title> <booktitle> In Proc. 12th Int. Symp. on Computer Architecture, </booktitle> <pages> pp. 3644, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Thus hardware support for unsafe speculative code motions improves machine performance beyond the best performance of the pure software schemes. superscalar machine. The dynamic scheduler fetches and decodes two instructions per cycle. It uses a total of 30 reservation station locations [28] and a 16-entry reorder buffer <ref> [21] </ref> to implement out-of-order execution with speculation, and it uses a 2048-entry, 4-way set associative branch target buffer to predict branches. It has the same number of functional units as our statically-scheduled machine, but since the dynamically-scheduled machine uses reservation stations, it can issue up to 6 instructions per cycle.
Reference: [22] <author> M.D. Smith, M. Johnson, and M.A. Horowitz. </author> <title> Limits on Multiple Instruction Issue. </title> <booktitle> In Proc. Third Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 290302, </pages> <month> April </month> <year> 1989. </year>
Reference: [23] <author> M.D. Smith, M.S. Lam, and M.A. Horowitz. </author> <title> Boosting Beyond Static Scheduling in a Superscalar Processor. </title> <booktitle> In the Proc. 17th Int. Symp. on Computer Architecture, </booktitle> <pages> pp. 344354, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Recently we proposed a general architectural mechanism called boosting that provides the compiler with an unconstrained model of speculative execution <ref> [23] </ref>. That paper discusses the ideas that lead to the concept of boosting, and it contains a preliminary experiment to justify further research. Since then, we have constructed a complete compiler system and a working hardware model to better understand the capabilities and costs of boosting.
Reference: [24] <author> M.D. Smith. </author> <title> Tracing with pixie. </title> <type> Technical Report CSL-TR-91-497, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: There are three SPEC benchmarks and four UNIX utilities. All of these programs are written in C, and all are run to completion 7 . Our base scalar machine model is the MIPS R2000. All results are derived from a trace-driven simulator based on pixie <ref> [24] </ref>, though we also have a functional simulator that verifies that the hardware is correct and an instruction-level simulator that verifies that the scheduled code is correct.
Reference: [25] <author> M.D. Smith. </author> <type> Ph.D. thesis, </type> <institution> Stanford Univ., </institution> <note> in preparation. </note>
Reference-contexts: Both of these overheads are quite acceptable since most exception processing routines occur infrequently, and when they do occur, they run for a long time. A more in-depth discussion of boosted exception handling can be found in Smith <ref> [25] </ref>. Boosting provides the compiler with a clean and unconstrained model of speculative execution. Boosted instructions are free to move far above their dependent branches, and boosting makes exception handling simple and precise. <p> Our trace-based approach allows for on-demand creation of basic blocks to hold duplicated instructions, and it simplifies the dynamic update of the dependence structures due to a duplication. An in-depth discussion of our algorithm can be found in Smith <ref> [25] </ref>. In summary, our global scheduling algorithm can be thought of as conscientious trace scheduling. It is conscientious because the scheduler is aware of the compensation costs of each code motion, and because the code transformations try to minimize the creation of compensation code.
Reference: [26] <author> S.W.K. Tjiang and J.L. Hennessy. </author> <title> SharlitA Tool for Building Optimizers. </title> <booktitle> In Proc ACM SIGPLAN 92 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. 8293, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The assembly file is generated by our SUIF compiler, and the optimizer in this compiler implements all the standard optimizations <ref> [26] </ref>. Our scheduler uses a branch profile of the program to generate the static branch prediction information needed during scheduling. This branch profile is generated from a 7.
Reference: [27] <author> M. Tokoro, E. Tamura, and T. </author> <title> Takizuka. </title> <journal> Optimization of Microprograms. IEEE Trans. of Computers, </journal> <volume> C-30(7):491 504, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: After reviewing previous work we describe our global scheduling algorithm, which is optimized for the scheduling non-numerical codes on modest superscalar machines. 3.1 Background Global instruction scheduling grew out of the work done on local microcode compaction techniques of the 1970s and early 1980s (see <ref> [27] </ref> for a comprehensive reference list). The early attempts at global scheduling understood the basic rules for code motion between basic blocks, and they optimized a program by repeatedly moving instructions between dynamically adjacent basic blocks to improve the local schedules. <p> The culmination of these iterative, neighborhood scheduling algorithms is Percolation Scheduling [20] which describes a complete set of semantics-preserving transformations for moving any operation between adjacent blocks. The next step taken by global scheduling researchers was to extend the neighborhood to include conditional pairs <ref> [27] </ref> (also called equivalent basic blocks [2]). Two basic blocks are equivalent if and only if the execution of one block implies the execution of the other block; equivalence is simply a combination of the move-op and unification transformations of Percolation Scheduling for control-independent basic blocks. <p> We refer to these types of algorithms as neighbor and peer scheduling algorithms. Tokoro <ref> [27] </ref> discusses some early versions of iterative neighbor and peer scheduling. Region Scheduling [13] is another iterative neighbor and peer scheduling algorithm that uses a program dependence graph [9] to determine equivalent basic blocks. All of these algorithms repeatedly apply transformations using a local policy.
Reference: [28] <author> R.M. Tomasulo. </author> <title> An Efficient Algorithm for Exploiting Multiple Arithmetic Units. </title> <journal> IBM Journal, </journal> <volume> 11(1):2533, </volume> <month> Jan-uary </month> <year> 1967. </year>
Reference-contexts: Thus hardware support for unsafe speculative code motions improves machine performance beyond the best performance of the pure software schemes. superscalar machine. The dynamic scheduler fetches and decodes two instructions per cycle. It uses a total of 30 reservation station locations <ref> [28] </ref> and a 16-entry reorder buffer [21] to implement out-of-order execution with speculation, and it uses a 2048-entry, 4-way set associative branch target buffer to predict branches.
Reference: [29] <author> D.W. Wall. </author> <title> Limits of Instruction-Level Parallelism. </title> <booktitle> In Proc. Fourth Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 176 188, </pages> <month> April </month> <year> 1991. </year>
References-found: 29

