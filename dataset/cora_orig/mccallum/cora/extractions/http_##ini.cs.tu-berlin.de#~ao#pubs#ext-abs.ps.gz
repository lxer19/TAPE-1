URL: http://ini.cs.tu-berlin.de/~ao/pubs/ext-abs.ps.gz
Refering-URL: http://ini.cs.tu-berlin.de/~ao/pubs-e.html
Root-URL: http://ini.cs.tu-berlin.de/~ao/pubs-e.html
Title: Segmentation of Medical Images Using Neural-Network Classifiers  
Author: Arnfried Ossen and Thomas Zamzow Helmut Oswald and Eckart Fleck Universitatsklinikum Rudolf Virchow Cardiology/Angiology and 
Address: Berlin, Germany  Berlin, Germany  Berlin, Germany  
Affiliation: Department of Computer Science Technical University of Berlin  Free University of Berlin  German Heart Center Berlin  
Abstract: We demonstrate that neural-network classifiers are able to significantly enhance low-level segmentation of medical images. Minimum-error-rate or Bayesian a posteriori probabilities of object classes are estimated directly. The neural network was successfully integrated and tested in an existing medical imaging system at the German Heart Center. For a relevant application, Graves' ophthalmopathy, segmentation improvements lead to better therapeutic decision making. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> John S. Bridle. </author> <title> Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. </title> <editor> In David S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems II, </booktitle> <pages> pages 211-217. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: a posteriori probabilities P (c j jT) using neural networks, circumventing the costly modeling phase of local texture distributions. 2 Minimum-error-rate Classification Using Neural Networks One of the more important recent results in the neural network field is that neural networks can directly approximate a posteriori probabilities, see e. g. <ref> [4; 5; 1] </ref>. In addition, the networks can estimate (learn) the underlying distribution given a finite sample (finite training set) of input features and corresponding binary indicator (target) vectors. The frequency of object classes in the training set and their corresponding a priori probabilities should match.
Reference: [2] <author> Haluk Derin and Howard Elliott. </author> <title> Modeling and segmentation of noisy and textured images using Gibbs random fields. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-9(1), </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: Standard texture-based classification methods estimate the last two factors in equation 2 separately. They usually involve extensive modeling efforts for the class-conditional probability density functions p (Tjc j ) <ref> [2] </ref>.
Reference: [3] <author> Jenq-Neng Hwang and Eric Tsung-Yen Chung. </author> <title> Textured image synthesis and segmentation via neural network probabilistic modelling. </title> <type> Technical report, </type> <institution> University of Washing-ton, </institution> <address> Seattle, USA, </address> <year> 1993. </year>
Reference-contexts: Typical training times are less than 10 seconds on standard RISC workstations. Segmentation time depends only linearly on the number of pixels in the area of interest. 3.3 Results Comparative studies in textured image segmentation have been presented by Hwang and Chen <ref> [3] </ref>. Their simulations show that the Neural Network approach is able to segment images of artificial and real-world textures more consistently than other approaches. For medical images, it is harder to generate labeled data for the learning process. In our case, segmentation results are evaluated by experienced radiologists.
Reference: [4] <author> John B. Hampshire II and Barak Pearlmutter. </author> <title> Equivalence proofs for multi-layer perceptron classifiers and the Bayesian discrimant function. </title> <editor> In D. S. Touretzky, J. L. Elman, T. J. Sejnowski, and G. E. Hinton, editors, </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: a posteriori probabilities P (c j jT) using neural networks, circumventing the costly modeling phase of local texture distributions. 2 Minimum-error-rate Classification Using Neural Networks One of the more important recent results in the neural network field is that neural networks can directly approximate a posteriori probabilities, see e. g. <ref> [4; 5; 1] </ref>. In addition, the networks can estimate (learn) the underlying distribution given a finite sample (finite training set) of input features and corresponding binary indicator (target) vectors. The frequency of object classes in the training set and their corresponding a priori probabilities should match.
Reference: [5] <author> Michael Richard and Richard Lippmann. </author> <title> Neural network classifiers estimate Bayesian a posteriori probabilities. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 461-483, </pages> <year> 1991. </year>
Reference-contexts: a posteriori probabilities P (c j jT) using neural networks, circumventing the costly modeling phase of local texture distributions. 2 Minimum-error-rate Classification Using Neural Networks One of the more important recent results in the neural network field is that neural networks can directly approximate a posteriori probabilities, see e. g. <ref> [4; 5; 1] </ref>. In addition, the networks can estimate (learn) the underlying distribution given a finite sample (finite training set) of input features and corresponding binary indicator (target) vectors. The frequency of object classes in the training set and their corresponding a priori probabilities should match.
Reference: [6] <author> Andreas Zell, Harald Beyer, Ralf Hubner, Niels Mache, and Michael Vogt. </author> <title> Efficient parallel simulation of neural networks. </title> <type> Technical report, </type> <institution> Institute for Parallel and Distributed High Performance Systems, University of Stuttgart, Stuttgart, Germany, </institution> <year> 1992. </year> <month> 6 </month>
Reference-contexts: On the training algorithm side, it is possible to choose between several variants of back-propagation including weight decay and an implementation on a massively-parallel machine <ref> [6] </ref>. The training set is chosen interactively by selecting representative texture examples for each object class. It is worth noting that no preprocessing of input data is necessary; raw gray values are presented to the net.
References-found: 6

