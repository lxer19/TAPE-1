URL: http://www.cs.berkeley.edu/~leungt/Research/ECCV96_final.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~leungt/publications.html
Root-URL: 
Email: email: leungt@cs.berkeley.edu, malik@cs.berkeley.edu  
Title: Detecting, localizing and grouping repeated scene elements from an image  
Author: Thomas Leung and Jitendra Malik 
Date: (April 1996)  
Address: Cambridge, England  Berkeley, Berkeley, CA 94720  
Affiliation: Computer Vision,  Department of Electrical Engineering and Computer Sciences University of California at  
Note: Fourth Euro. Conf. on  
Abstract: This paper presents an algorithm for detecting, localizing and grouping instances of repeated scene elements. The grouping is represented by a graph where nodes correspond to individual elements and arcs join spatially neighboring elements. Associated with each arc is an affine map that best transforms the image patch at one location to the other. The approach we propose consists of 4 steps: (1) detecting "interesting" elements in the image; (2) matching elements with their neighbors and estimating the affine transform between them; (3) growing the element to form a more distinctive unit; and (4) grouping the elements. The idea is analogous to tracking in dynamic imagery. In our context, we "track" an element to spatially neighboring locations in one image, while in temporal tracking, one would perform the search in neighboring image frames.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. Armstrong, A. Zisserman, and P. Beardsley. </author> <title> "Euclidean Structure from Uncalibrated Images". </title> <booktitle> In Proc. British Machine Vision Conference, </booktitle> <year> 1994. </year>
Reference-contexts: we have a problem formulation that excludes a certain class of textures|stochastic textures with ill defined texture elements|but considering only repeated elements enables a notion of point to point correspondence that is now exactly like that in motion or stereopsis. 2.2 Reconstruction from repeated structure It has been pointed out <ref> [1, 6, 7, 12] </ref> that structures that repeat in a single image of a scene are equivalent to multiple views of a single instance of a structure. If there are many identical objects available, then one can even calibrate the camera and recover structure up to a similarity ambiguity.
Reference: 2. <author> M. Fleck, D. Forsyth, and C. Bregler. </author> <title> "Finding Naked People". </title> <note> In To appear in ECCV96, </note> <year> 1996. </year>
Reference-contexts: This is to be expected because the grouping process assumes small changes from one element to its neighbor. This offers us a useful segmentation technique where the shirt front, left arm and right arm would emerge as separate groups. Work on human figure recognition <ref> [2] </ref> have shown the usefulness of such segmentation into parts. pattern. Traditionally, this would not be regarded as texture as it does not have significant 2D spatial extent. Instead, it is a distinctive element in the scene.
Reference: 3. <author> W. Forstner. </author> <title> "Image Matching". </title> <editor> In R. Haralic and L. Shapiro, editors, </editor> <title> "Computer and Robot Vision", chapter 16. </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: At each window, we compute the second moment matrix: W = W where r is the gradient operator. The second moment matrix tells us how much spatial intensity variation is present in the window. It has been used before in early vision processing as in <ref> [3, 5] </ref>. The eigenvalues, k 1 and k 2 (assume k 1 &gt; k 2 ), of W represent the amount of "energy" in the two principal directions. <p> A small amount of affine transform is allowed to take into account surface shapes 2 . A rough registration is obtained by finding the neighboring patches where the SSDs to our candidate are small. We then use a differential approach <ref> [3, 8] </ref> to estimate the affine transform which will bring the two patches in better correspondence: Err = (I (x) ~ I (Ax + d)) 2 (5) fA;dg ~ I is the intensity at the neighboring location.
Reference: 4. <author> J. G-arding. </author> <title> "Surface orientation and curvature from differential texture distortion". </title> <booktitle> In Fifth Intl. Conf. Computer Vision, </booktitle> <pages> pages 733-739, </pages> <year> 1995. </year>
Reference-contexts: Knowledge of the affine transforms between corresponding patches, or equivalently knowledge of the point correspondences between fiducial points on the image patches, can be used to recover 3D scene structure. The mathematics for shape recovery can either follow analysis of shape from texture <ref> [4, 10, 11] </ref> or structure from motion, or variations of the ideas therein. In the long term, our primary driving application is recognition. For that the vital importance of grouping has long been noted, and recovery of 3D structural relationships can be useful as well. <p> In principle, this should be chosen based on the scale of the features we are looking for. Another direction of work that will be done is to use the affine transforms and the point correspondences in the patches to infer surface shape. Techniques from shape from texture <ref> [4, 10, 11] </ref> or structure from motion will be useful. (a) (b) Fig. 1. Architectural image. The rectangle in (a) is the unit we found and where the growing process starts. The crosses are the locations of the units grouped together. The segmented region is shown in (b).
Reference: 5. <author> J. G-arding and T. Lindeberg. </author> <title> "Direct Estimation of Local Surface Shape in a Fixating Binocular Vision System". </title> <booktitle> In Proc. Euro. Conf. Computer Vision, </booktitle> <year> 1994. </year>
Reference-contexts: At each window, we compute the second moment matrix: W = W where r is the gradient operator. The second moment matrix tells us how much spatial intensity variation is present in the window. It has been used before in early vision processing as in <ref> [3, 5] </ref>. The eigenvalues, k 1 and k 2 (assume k 1 &gt; k 2 ), of W represent the amount of "energy" in the two principal directions.
Reference: 6. <author> J. Liu, J. Mundy, and E. Walker. </author> <title> "Recognizing Arbitrary Objects from Multiple Projections". </title> <booktitle> In Proc. Asian Conf. Computer Vision, </booktitle> <year> 1993. </year>
Reference-contexts: we have a problem formulation that excludes a certain class of textures|stochastic textures with ill defined texture elements|but considering only repeated elements enables a notion of point to point correspondence that is now exactly like that in motion or stereopsis. 2.2 Reconstruction from repeated structure It has been pointed out <ref> [1, 6, 7, 12] </ref> that structures that repeat in a single image of a scene are equivalent to multiple views of a single instance of a structure. If there are many identical objects available, then one can even calibrate the camera and recover structure up to a similarity ambiguity.
Reference: 7. <author> J. Liu, J. Mundy, and A. Zisserman. </author> <title> "Grouping and Structure Recovery for Images of Objects with Finite Rotational Symmetry". </title> <booktitle> In Proc. Asian Conf. Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: we have a problem formulation that excludes a certain class of textures|stochastic textures with ill defined texture elements|but considering only repeated elements enables a notion of point to point correspondence that is now exactly like that in motion or stereopsis. 2.2 Reconstruction from repeated structure It has been pointed out <ref> [1, 6, 7, 12] </ref> that structures that repeat in a single image of a scene are equivalent to multiple views of a single instance of a structure. If there are many identical objects available, then one can even calibrate the camera and recover structure up to a similarity ambiguity.
Reference: 8. <author> B.D. Lucas and T. Kanade. </author> <title> "An Iterative Image Registration Technique with an Application to Stereo Vision". </title> <booktitle> In Proc. 7th Intl. Joint Conf. on Art. Intell., </booktitle> <year> 1981. </year>
Reference-contexts: A small amount of affine transform is allowed to take into account surface shapes 2 . A rough registration is obtained by finding the neighboring patches where the SSDs to our candidate are small. We then use a differential approach <ref> [3, 8] </ref> to estimate the affine transform which will bring the two patches in better correspondence: Err = (I (x) ~ I (Ax + d)) 2 (5) fA;dg ~ I is the intensity at the neighboring location. <p> Linearizing, A I + A and d 0 + ffid (7) Err (I (x) ~ I (x) r ~ I (x) T Ax r ~ I (x) T ffid) 2 (8) A and ffid can be estimated by solving a system of linear equations. See <ref> [8] </ref> for details. We then iterate by warping the image and refining the affine transform. Similarity of the two patches are measured in a normalized way.
Reference: 9. <author> J. Malik and P. Perona. </author> <title> "Preattentive Texture Discrimination with Early Vision Mechanisms". </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> 7(5) </volume> <pages> 923-932, </pages> <year> 1990. </year>
Reference-contexts: This insight has carried over to the quasilinear filtering models that now dominate the field|spatial averages of filter outputs are considered <ref> [9] </ref>. In our context of finding repeated scene elements, the precise positional re-lationships between the features on a single element do matter: we can talk about precise point to point correspondences between features on one element and another.
Reference: 10. <author> J. Malik and R. Rosenholtz. </author> <title> "Recovering Surface Curvature and Orientation from Texture Distortion: a Least Squares Algorithm and Sensitivity Analysis". </title> <booktitle> In Proc. Third Euro. Conf. Computer Vision, </booktitle> <pages> pages 353-364, </pages> <address> Stockholm, Sweden, </address> <year> 1994. </year>
Reference-contexts: Knowledge of the affine transforms between corresponding patches, or equivalently knowledge of the point correspondences between fiducial points on the image patches, can be used to recover 3D scene structure. The mathematics for shape recovery can either follow analysis of shape from texture <ref> [4, 10, 11] </ref> or structure from motion, or variations of the ideas therein. In the long term, our primary driving application is recognition. For that the vital importance of grouping has long been noted, and recovery of 3D structural relationships can be useful as well. <p> This is simplistic-we will discuss below the similarities and differences between texture processing and our work. First, we talk about the similarities. As pointed out by Malik and Rosen-holtz <ref> [10, 11] </ref>, shape from texture is a cue to 3D shape very similar to binocular stereopsis and structure from motion. All of these cues are based on the information available in multiple perspective views of the same surface in the scene. <p> If there are many identical objects available, then one can even calibrate the camera and recover structure up to a similarity ambiguity. Though arrived at independently, fundamentally this represents a variation of the same idea as expressed by Malik and Rosenholtz in the texture context <ref> [10, 11] </ref>. But, here, the notion of point-to-point correspondence is exactly the same as in the motion/stereopsis context. However, in previous work on this topic, the early vision operations of finding image correspondences have not been addressed. <p> In principle, this should be chosen based on the scale of the features we are looking for. Another direction of work that will be done is to use the affine transforms and the point correspondences in the patches to infer surface shape. Techniques from shape from texture <ref> [4, 10, 11] </ref> or structure from motion will be useful. (a) (b) Fig. 1. Architectural image. The rectangle in (a) is the unit we found and where the growing process starts. The crosses are the locations of the units grouped together. The segmented region is shown in (b).
Reference: 11. <author> J. Malik and R. Rosenholtz. </author> <title> "Computing Local Surface Orientation and Shape from Texture for Curved Surfaces". </title> <note> To appear in the International Journal of Computer Vision, </note> <year> 1996. </year>
Reference-contexts: Knowledge of the affine transforms between corresponding patches, or equivalently knowledge of the point correspondences between fiducial points on the image patches, can be used to recover 3D scene structure. The mathematics for shape recovery can either follow analysis of shape from texture <ref> [4, 10, 11] </ref> or structure from motion, or variations of the ideas therein. In the long term, our primary driving application is recognition. For that the vital importance of grouping has long been noted, and recovery of 3D structural relationships can be useful as well. <p> This is simplistic-we will discuss below the similarities and differences between texture processing and our work. First, we talk about the similarities. As pointed out by Malik and Rosen-holtz <ref> [10, 11] </ref>, shape from texture is a cue to 3D shape very similar to binocular stereopsis and structure from motion. All of these cues are based on the information available in multiple perspective views of the same surface in the scene. <p> If there are many identical objects available, then one can even calibrate the camera and recover structure up to a similarity ambiguity. Though arrived at independently, fundamentally this represents a variation of the same idea as expressed by Malik and Rosenholtz in the texture context <ref> [10, 11] </ref>. But, here, the notion of point-to-point correspondence is exactly the same as in the motion/stereopsis context. However, in previous work on this topic, the early vision operations of finding image correspondences have not been addressed. <p> In principle, this should be chosen based on the scale of the features we are looking for. Another direction of work that will be done is to use the affine transforms and the point correspondences in the patches to infer surface shape. Techniques from shape from texture <ref> [4, 10, 11] </ref> or structure from motion will be useful. (a) (b) Fig. 1. Architectural image. The rectangle in (a) is the unit we found and where the growing process starts. The crosses are the locations of the units grouped together. The segmented region is shown in (b).
Reference: 12. <author> J.L. Mundy and A. Zisserman. </author> <title> "Repeated structures: image correspondence constraints and 3D structure recovery.". </title> <booktitle> In Applications of invariance in computer vision. </booktitle> <year> 1994. </year>
Reference-contexts: we have a problem formulation that excludes a certain class of textures|stochastic textures with ill defined texture elements|but considering only repeated elements enables a notion of point to point correspondence that is now exactly like that in motion or stereopsis. 2.2 Reconstruction from repeated structure It has been pointed out <ref> [1, 6, 7, 12] </ref> that structures that repeat in a single image of a scene are equivalent to multiple views of a single instance of a structure. If there are many identical objects available, then one can even calibrate the camera and recover structure up to a similarity ambiguity.
References-found: 12

