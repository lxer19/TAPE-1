URL: http://www.cs.iastate.edu/tech-reports/TR92-27.ps
Refering-URL: http://www.cs.iastate.edu/tech-reports/catalog.html
Root-URL: http://www.cs.iastate.edu
Title: Analyzing Software Requirements Errors in Safety-Critical, Embedded Systems  
Author: TR - Robyn R. Lutz 
Address: 226 Atanasoff Ames, IA 50011  
Affiliation: Iowa State University of Science and Technology Department of Computer Science  
Date: August 27, 1992  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. A. Addy, </author> <title> "A Case Study on Isolation of Safety-Critical Software," </title> <booktitle> in Proceedings of the Sixth Annual Conference on Computer Assurance, </booktitle> <address> June 24-June 27, </address> <year> 1991 </year> <month> . NIST/IEEE, </month> <year> 1991, </year> <pages> pp. 75-83. </pages>
Reference-contexts: Addy, looking at the types of errors that caused safety problems in a large, real-time control system, concluded that the design complexity inherent in such a system requires hidden interfaces which allow errors in non-critical software to affect safety-critical software <ref> [1] </ref>. This is consistent with Selby and Basili's results when they analyzed 770 software errors during the updating of a library tool [23].
Reference: [2] <author> V. R. Basili, R. W. Selby, and D. H. Hutchens, </author> <title> "Experimentation in Software Engineering, </title> <journal> IEEE Transactions on Software Engineering SE-12, </journal> <volume> 7, </volume> <month> July </month> <year> 1986, </year> <pages> pp. 733-743. 12 </pages>
Reference: [3] <author> A. M. Davis, </author> <title> Software Requirements, Analysis and Specification. </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: The capability to describe dynamic events, the timing of process interactions in distinct computers, decentralized supervisory functions, etc., should be 10 considered in chooosing a formal method <ref> [3, 5, 6, 15, 21, 24] </ref>. 4. Promote informal communication among teams. Many safety-related software errors resulted from one individual or team misunderstanding a requirement or not knowing a fact about the system that member (s) of another development team knew.
Reference: [4] <author> D. E. Eckhardt, et al., </author> <title> "An Experimental Evaluation of Software Redundancy as a Strategy for Improving Reliability," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17, 7, </volume> <month> July </month> <year> 1991, </year> <pages> pp. 692-702. </pages>
Reference-contexts: Eckhardt et al., in a study of software redundancy, analyzed the errors in twenty independent versions of a software component of an inertial navigation system <ref> [4] </ref>. He found that inadequate understanding on the parts of the programmers of the specifications or the underlying coordinate system was a major contributor to the program faults causing coincident failures.
Reference: [5] <author> A. Endres, </author> <title> "An Analysis of Errors and Their Causes in Systems Programs," </title> <journal> IEEE Transactions on Software Engineering, SE-1, </journal> <volume> 2, </volume> <month> June </month> <year> 1975, </year> <pages> pp. 140-149. </pages>
Reference-contexts: Discrepancies between the difficulty of the problem and the means used to solve it may permit hazardous software errors to occur <ref> [5] </ref>. The third step of the error analysis therefore associates a pair of process flaws with each program fault. The first element in the pair identifies a process flaw or inadequacy in the control of the system complexity (e.g., requirements which are not discovered until system testing). <p> The work done by Endres is a direct forerunner of Nakajo and Kume's in that Endres backtracked from the error type to the technical and organizational causes which led to each type of error <ref> [5] </ref>. Moreover, because he studied the system testing of an operating system, the software's interaction with the hardware was a source of concern. Endres noted the difficulty of precisely specifying functional demands on the systems before the programmer had seen their effect on the dynamic behavior of the system. <p> The capability to describe dynamic events, the timing of process interactions in distinct computers, decentralized supervisory functions, etc., should be 10 considered in chooosing a formal method <ref> [3, 5, 6, 15, 21, 24] </ref>. 4. Promote informal communication among teams. Many safety-related software errors resulted from one individual or team misunderstanding a requirement or not knowing a fact about the system that member (s) of another development team knew.
Reference: [6] <author> E. M. Gray and R. H. Thayer, </author> <title> "Requirements," in Aerospace Software Engineering, A Collection of Concepts. </title> <editor> Ed. C. Anderson and M. Dorfman. </editor> <address> Washington: AIAA, </address> <year> 1991, </year> <pages> pp. 89-121. </pages>
Reference-contexts: Similarly, specifying the interfaces-especially the timing and dependency relationships-between the software outputs (e.g., star identification) and system outputs (e.g., closing the shutter on the star scanner) is necessary. <ref> [6, 11] </ref> System-development issues such as timing (real-time activities, interrupt handling, frequency of sensor data), hardware capabilities and limitations (storage capacity, power transients, noise characteristics), communication links (buffer and interface formats), and the expected operating environment (temperature, pressure, radiation) need to be reflected in the software requirements specifications because they are <p> The capability to describe dynamic events, the timing of process interactions in distinct computers, decentralized supervisory functions, etc., should be 10 considered in chooosing a formal method <ref> [3, 5, 6, 15, 21, 24] </ref>. 4. Promote informal communication among teams. Many safety-related software errors resulted from one individual or team misunderstanding a requirement or not knowing a fact about the system that member (s) of another development team knew.
Reference: [7] <institution> ANSI/IEEE Standard Glossary of Software Engineering Terminology. </institution> <address> New York: </address> <publisher> IEEE, </publisher> <year> 1983. </year>
Reference: [8] <author> M. S. Jaffe et al., </author> <title> "Software Requirements Analysis for Real-Time Process-Control Systems," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17, 3, </volume> <month> March </month> <year> 1991, </year> <pages> pp. 241-258. </pages>
Reference-contexts: Many of the safety-related software errors reported in Sect. III involve data objects or processes that would be targeted for special attention using hazard-detection techniques such as those described in <ref> [8, 12] </ref>. Early detection of these safety-critical objects and increased attention to software operations involving them might forestall safety-related software errors involving them. 3. Use formal specification techniques in addition to natural-language software requirements specifications.
Reference: [9] <author> P. Jalote, </author> <title> An Integrated Approach to Software Engineering. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Similarly, standard measures of the internal complexity of modules have limited usefulness in anticipating software errors during system testing It is not the internal complexity of a module but the complexity of the module's connection to its environment that yields the persistent, safety-related errors seen in the embedded systems here <ref> [9] </ref>. V. Conclusion A. Recommendations The results in Sect. III indicate that safety-related software errors tend to be produced by different error mechanisms than non-safety-related software errors. This means that system safety can be directly enhanced by targeting the causes of safety-related errors.
Reference: [10] <author> J. C. Knight, </author> <title> "Testing," in Aerospace Software Engineering, A Collection of Concepts. </title> <editor> Ed. C. Anderson and M. Dorfman. </editor> <address> Washington: AIAA, </address> <year> 1991, </year> <pages> pp. 135-159. </pages>
Reference-contexts: In such systems, the software is often physically and logically distributed among various hardware components of the system. The hardware involved may be not only computers but also sensors, actuators, gyros, and science instruments <ref> [10] </ref>. Specifying the external behavior of the software (its transformation of software inputs into software outputs) only makes sense if the interfaces between the system inputs (e.g., environmental conditions, power transients) and the software inputs (e.g., monitor data) are also specified. <p> Similarly, generating test cases from misunderstood or missing requirements will not test system correctness. Traceability of requirements and automatic test generation from specifications offers only partial validation of complex, embedded systems. Alternative validation and testing methods such as those described in <ref> [10, 12] </ref> offer greater coverage. 6. Include requirements for "defensive design" [17]. Many of the safety-related software errors involve inadequate software responses to extreme conditions or extreme values. Anomalous hardware behavior, unanticipated states, events out of order, and obsolete data are all causes of safety-related software errors on the spacecraft.
Reference: [11] <author> N. G. Leveson, </author> <title> "Safety," in Aerospace Software Engineering, A Collection of Concepts. </title> <editor> Ed. C. Anderson and M. Dorfman. </editor> <address> Washington: AIAA, </address> <year> 1991, </year> <pages> pp. 319-336. </pages>
Reference-contexts: Similarly, specifying the interfaces-especially the timing and dependency relationships-between the software outputs (e.g., star identification) and system outputs (e.g., closing the shutter on the star scanner) is necessary. <ref> [6, 11] </ref> System-development issues such as timing (real-time activities, interrupt handling, frequency of sensor data), hardware capabilities and limitations (storage capacity, power transients, noise characteristics), communication links (buffer and interface formats), and the expected operating environment (temperature, pressure, radiation) need to be reflected in the software requirements specifications because they are <p> Identify safety-critical hazards early in the requirements analysis. These hazards are constraints on the possible designs and factors in any contemplated tradeoffs between safety (which tends to encourage software simplicity) and increased functionality (which tends to encourage software complexity) <ref> [11, 23] </ref>. On the two spacecraft studied here, the need for high reliability led to early identification of safety-critical functions. However, early identification of safety-critical data items (whose incorrect values can cause risk to the system), safety-critical timing dependencies, and safety-critical event orders was not performed as thoroughly. <p> CASE tools offer a possible formal solution to the difficulty of promulgating change without increasing paperwork. The prevalence of safety-related software errors involving misunderstood or missing requirements points up the inadequacy of consistency checks of requirements and code as a means of demonstrating system correctness <ref> [11] </ref>. Code that implements incorrect requirements is incorrect if it fails to provide needed system behavior. Similarly, generating test cases from misunderstood or missing requirements will not test system correctness. Traceability of requirements and automatic test generation from specifications offers only partial validation of complex, embedded systems.
Reference: [12] <author> N. G. Leveson, </author> <title> "Software Safety in Embedded Computer Systems," </title> <journal> Communications of the ACM , Vol. </journal> <volume> 34, No. 2, </volume> <month> Feb </month> <year> 1991, </year> <pages> pp. 35-46. </pages>
Reference-contexts: Leveson listed a set of common assumptions that are often false for control systems, resulting in software errors <ref> [12] </ref>. Among these assumptions are that the software specification is correct, that it is possible to predict realistically the software's execution environment (e.g., the existence of transients), and that it is possible to anticipate and specify correctly the software's behavior under all possible circumstances. <p> Many of the safety-related software errors reported in Sect. III involve data objects or processes that would be targeted for special attention using hazard-detection techniques such as those described in <ref> [8, 12] </ref>. Early detection of these safety-critical objects and increased attention to software operations involving them might forestall safety-related software errors involving them. 3. Use formal specification techniques in addition to natural-language software requirements specifications. <p> Similarly, generating test cases from misunderstood or missing requirements will not test system correctness. Traceability of requirements and automatic test generation from specifications offers only partial validation of complex, embedded systems. Alternative validation and testing methods such as those described in <ref> [10, 12] </ref> offer greater coverage. 6. Include requirements for "defensive design" [17]. Many of the safety-related software errors involve inadequate software responses to extreme conditions or extreme values. Anomalous hardware behavior, unanticipated states, events out of order, and obsolete data are all causes of safety-related software errors on the spacecraft. <p> Run-time safety checks on the validity of input data, watchdog timers, delay timers, software filters, software-imposed initialization conditions, additional exception handling, and assertion checking can be used to combat the many safety-critical software errors involving conditional and omission faults <ref> [12] </ref>. Requirements for error-handling, overflow protection, signal saturation limits, heartbeat and pulse frequency, maximum event duration, and system behavior under unexpected conditions can be added and traced into the design. Many safety-related functional faults involve error-recovery routines being invoked inappropriately because of erroneous limit values or bad data.
Reference: [13] <author> N. G. Leveson and P. R. Harvey, </author> <title> "Analyzing Software Safety," </title> <journal> IEEE Transactions on Software Engineering, SE-9, </journal> <volume> 5, </volume> <month> Sept </month> <year> 1983, </year> <pages> pp. 569-579. </pages>
Reference: [14] <author> Karan L'Heureux, </author> <title> "Software Systems Safety Program RTOP, Phase A Report," Internal Document, </title> <institution> Jet Propulsion Laboratory, </institution> <month> April 19, </month> <year> 1991. </year>
Reference: [15] <author> R. Lutz and J. S. K. Wong, </author> <title> "Detecting Unsafe Error Recovery Schedules," </title> <note> IEEE Transactions on Software Engineering , to appear. </note>
Reference-contexts: The capability to describe dynamic events, the timing of process interactions in distinct computers, decentralized supervisory functions, etc., should be 10 considered in chooosing a formal method <ref> [3, 5, 6, 15, 21, 24] </ref>. 4. Promote informal communication among teams. Many safety-related software errors resulted from one individual or team misunderstanding a requirement or not knowing a fact about the system that member (s) of another development team knew.
Reference: [16] <author> T. Nakajo and H. Kume, </author> <title> "A Case History Analysis of Software Error Cause-Effect Relationships," </title> <journal> IEEE Transactions on Software Engineering 17, </journal> <volume> 8, </volume> <month> Aug </month> <year> 1991, </year> <pages> pp. 830-838. </pages>
Reference-contexts: Nakajo and Kume categorized 670 errors found during the software development of two firmware products for controlling measuring instruments and two software products for instrument measurement programs <ref> [16] </ref>. Over 90% of the errors were either interface or functional faults, similar to the results reported here. However, unlike the results described here, 7 Nakajo and Kume found many more conditional faults. It may be that unit testing finds many of the conditional faults prior to system testing.
Reference: [17] <author> P. G. Neumann, </author> <title> "The Computer-Related Risk of the Year: Weak Links and Correlated Events," </title> <booktitle> in Proceedings of the Sixth Annual Conference on Computer Assurance, </booktitle> <address> June 24-June 27, </address> <year> 1991. </year> <month> NIST/IEEE, </month> <year> 1991, </year> <pages> pp. 5-8. 13 </pages>
Reference-contexts: Traceability of requirements and automatic test generation from specifications offers only partial validation of complex, embedded systems. Alternative validation and testing methods such as those described in [10, 12] offer greater coverage. 6. Include requirements for "defensive design" <ref> [17] </ref>. Many of the safety-related software errors involve inadequate software responses to extreme conditions or extreme values. Anomalous hardware behavior, unanticipated states, events out of order, and obsolete data are all causes of safety-related software errors on the spacecraft.
Reference: [18] <author> P. G. Neumann, </author> <title> "On Hierarchical Design of Computer Systems for Critical Applica--tions," </title> <journal> IEEE Transactions on Software Engineering SE-12, </journal> <volume> 9, </volume> <month> Sept </month> <year> 1986, </year> <pages> pp. 905-920. </pages>
Reference: [19] <author> A. P. Nikora, </author> <title> "Error Discovery Rate by Severity Category and Time to Repair Software Failures for Three JPL Flight Projects," Internal Document, </title> <institution> Jet Propulsion Laboratory, </institution> <year> 1991. </year>
Reference-contexts: Errors discovered in the testing phase take longer to correct (because they tend to be more complicated and difficult to isolate). This is consistent with the results in <ref> [19] </ref> indicating that more severe errors take longer to discover than less severe errors during system-level testing. Furthermore, this effect was found to be more pronounced in more complex (as measured by lines of code) software.
Reference: [20] <author> T. J. Ostrand and E. J. Weyuker, </author> <title> "Collecting and Categorizing Software Error Data in an Industrial Environment," </title> <journal> The Journal of Systems and Software, </journal> <volume> 4, </volume> <year> 1984, </year> <pages> pp. 289-300. </pages>
Reference-contexts: In the safety-critical, embedded software on the spacecraft, the flaw was more often a failure to identify or to understand the requirements. Ostrand and Weyuker categorized 173 errors found during the development and testing of an editor system <ref> [20] </ref>. Only 2% of the errors were found during system testing, reflecting the simplicity and stability of the interfaces and requirements. Most of the errors (61%) were found instead during function testing. <p> The most frequent class of errors, other than coding and clerical, was design errors. All three of the most common design errors-extreme conditions neglected, forgotten cases or steps, and loop control errors- are also common functional faults on the spacecraft. Both the findings presented in <ref> [20, 22] </ref> and in this paper confirm the common experience that early insertion and late discovery of software errors maximizes the time and effort that the correction takes.
Reference: [21] <institution> Proceedings of the Berkeley Workshop on Temporal and Real-Time Specification. </institution> <note> EDS. </note> <author> P. B. Ladkin and F. H. </author> <type> Vogt. </type> <institution> Berkeley, CA: International Computer Science Institute, </institution> <year> 1990, </year> <month> TR-90-060. </month>
Reference-contexts: The capability to describe dynamic events, the timing of process interactions in distinct computers, decentralized supervisory functions, etc., should be 10 considered in chooosing a formal method <ref> [3, 5, 6, 15, 21, 24] </ref>. 4. Promote informal communication among teams. Many safety-related software errors resulted from one individual or team misunderstanding a requirement or not knowing a fact about the system that member (s) of another development team knew.
Reference: [22] <author> N. F. Schneidewind and H.-M. Hoffmann, </author> <title> "An Experiment in Software Error Data Collection and Analysis," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5, 3, </volume> <month> May </month> <year> 1979, </year> <pages> pp. 276-286. </pages>
Reference-contexts: Most of the errors (61%) were found instead during function testing. Over half of these errors were caused by omissions, confirming the findings of the present study that omissions are a major cause of software errors. Schneidewind and Hoffmann <ref> [22] </ref> categorized 173 errors found during the development of four small programs by a single programmer. Again, there were no significant interfaces with hardware and little system testing. The most frequent class of errors, other than coding and clerical, was design errors. <p> The most frequent class of errors, other than coding and clerical, was design errors. All three of the most common design errors-extreme conditions neglected, forgotten cases or steps, and loop control errors- are also common functional faults on the spacecraft. Both the findings presented in <ref> [20, 22] </ref> and in this paper confirm the common experience that early insertion and late discovery of software errors maximizes the time and effort that the correction takes.
Reference: [23] <author> R. W. Selby and V. R. Basili, </author> <title> "Analyzing Error-Prone System Structure," </title> <journal> IEEE Transactions on Software Engineering 17, </journal> <volume> 2, </volume> <month> Febr </month> <year> 1991, </year> <pages> pp. 141-152. </pages>
Reference-contexts: This is consistent with Selby and Basili's results when they analyzed 770 software errors during the updating of a library tool <ref> [23] </ref>. Of the 46 errors documented in trouble reports, 70% were categorized as "wrong" and 28% as "missing." They found that subsystems that were highly interactive with other subsystems had proportionately more errors than less interactive subsystems. <p> Identify safety-critical hazards early in the requirements analysis. These hazards are constraints on the possible designs and factors in any contemplated tradeoffs between safety (which tends to encourage software simplicity) and increased functionality (which tends to encourage software complexity) <ref> [11, 23] </ref>. On the two spacecraft studied here, the need for high reliability led to early identification of safety-critical functions. However, early identification of safety-critical data items (whose incorrect values can cause risk to the system), safety-critical timing dependencies, and safety-critical event orders was not performed as thoroughly.
Reference: [24] <author> J. M. Wing, </author> <title> "A Specifier's Introduction to Formal Methods," </title> <journal> Computer , Vol. </journal> <volume> 23, </volume> <month> Sept </month> <year> 1990, </year> <pages> pp. 8-26. </pages>
Reference-contexts: The capability to describe dynamic events, the timing of process interactions in distinct computers, decentralized supervisory functions, etc., should be 10 considered in chooosing a formal method <ref> [3, 5, 6, 15, 21, 24] </ref>. 4. Promote informal communication among teams. Many safety-related software errors resulted from one individual or team misunderstanding a requirement or not knowing a fact about the system that member (s) of another development team knew.
References-found: 24

