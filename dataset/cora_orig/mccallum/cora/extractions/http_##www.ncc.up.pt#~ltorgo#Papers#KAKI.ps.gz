URL: http://www.ncc.up.pt/~ltorgo/Papers/KAKI.ps.gz
Refering-URL: http://www.ncc.up.pt/~ltorgo/Papers/list_pub.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: ltorgo, pbrazdil@ncc.up.pt  
Title: Knowledge Acquisition via Knowledge Integration  
Author: Pavel B. Brazdil Lus Torgo LIACC 
Address: Rua Campo Alegre, 823 2 4100 Porto, Portugal  
Affiliation: Laboratory of AI and Computer Science University of Porto  
Abstract: In this paper we are concerned with the problem of acquiring knowledge by integration. Our aim is to construct an integrated knowledge base from several separate sources. The need to merge knowledge bases can arise, for example, when knowledge bases are acquired independently from interactions with several domain experts. As opinions of different domain experts may differ, the knowledge bases constructed in this way will normally differ too. A similar problem can also arise whenever separate knowledge bases are generated by learning algorithms. The objective of integration is to construct one system that exploits all the knowledge that is available and has a good performance. The aim of this paper is to discuss the methodology of knowledge integration, describe the implemented system (INTEG.3), and present some concrete results which demonstrate the advantages of this method. 
Abstract-found: 1
Intro-found: 1
Reference: <editor> Bond A,H, and Les Gasser (1988): </editor> <booktitle> Distributed Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Consequently, here we will be concerned with, basically, the following three phases: (1) Generation of independent theories (by consultation or inductive learning), 3 In general systems can be given the ability tp recognize themselves. These are some of the concerns of Distributed Artificial Intelligence (see e.g. <ref> (Bond and Gasser, 1988) </ref>), but they are outside the scope of this paper. (2) Competitive characterisation of the systems theories, (3) Construction of the integrated theory. In phase (1) the systems S 1 n work in an independent manner, and as a result produce theories T 1 n .
Reference: <author> Cestnik, B., Kononenko, I., and Bratko, I. </author> <year> (1987): </year> <title> "ASSISTANT 86 : A Knowledge- Elicitation Tool for Sophisticated Users", in Progress in Machine Learning, </title> <editor> I. Bratko and N. Lavrac (eds.), </editor> <publisher> Sigma Press, </publisher> <address> Wilmslow, England. </address>
Reference: <author> Boose John, Bradshaw J., Kitto C. and Shema D. </author> <title> (19 89): From ETS to Acquinas: Six Years of Knowledge Acquisition Tool Development, </title> <booktitle> in Proc of Third European Workshop on Knowledge-Based Systems, </booktitle> <editor> J. Boose, B. Gaines, and J.G.Ganascia (eds.), </editor> <address> Paris, </address> <month> July </month> <year> 1989. </year>
Reference: <author> Brazdil, P. and Clark, P. </author> <year> (1990): </year> <title> "Learning from Imperfect Data", in Machine Learning, Meta-Reasoning and Logics, </title> <editor> P. Bradzil and K. Konolige (eds.) </editor> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Brazdil, P., Muggleton S. </author> <year> (1990): </year> <title> "Learning in Multi-agent Environment", </title> <booktitle> in Proc. of the Second Workshop on Algorithmic Learning Theory , S. </booktitle> <editor> Arikawa, A. Marouka, and T. Sato, </editor> <publisher> OHMSHA Ltd., </publisher> <address> Tokyo, </address> <month> October </month> <year> 1991. </year>
Reference: <author> Clark, P. and Niblett, T. </author> <year> (1987): </year> <title> "Induction in Noisy Domains", </title> <booktitle> in Progress in Machine Learning , I. </booktitle>
Reference: <editor> Bratko and N. Lavrac (eds.), </editor> <publisher> Sigma Press, </publisher> <address> Wimslow, England. </address>
Reference: <author> Clark, P. and Niblett, T. </author> <year> (1989): </year> <title> "The CN2 Induction Algorithm", </title> <publisher> Machine Learning 3 , Kluwer Academic Publishers. </publisher>
Reference-contexts: The decision tree generated by this system is automatically converted into a rule form which we find more amenable for further manipulation. The inductive rule learning system IRL1 is an incr emental learning prog ram, that was partially inspired by CN2 <ref> (Clark and Niblett, 1989) </ref>. This system incrementally updates the existing rules either using generalization and/ or specialization. More details about this system will be made available in (Torgo, 1990).
Reference: <author> Emde W. </author> <year> (1987): </year> <booktitle> "Non-cumulative Learning in METAXA-3", in Proc. of IJCAI-87 , Mlian, Italy, </booktitle> <pages> pp. 208-212. </pages>
Reference: <author> Emde W. </author> <year> (1989): </year> <title> "An Inference Engine for Repre senting Multiple Theories", in Knowledge Representation and Organization in Machine Learning, </title> <editor> K. Morik (ed.), </editor> <booktitle> pp. </booktitle> <pages> 148-176, </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: But one could argue that whenever the data is noisy, it is wrong to try to generate perfect rules that `hide the problem under the carpet. Knowledge Integration and Theory Revision The BLIP system <ref> (Emde, 89) </ref> is capable of representing several competing theories. Emde points out that the decision whether some fact is consistent with the theory depends much on the viewpoint. A particular fact can contradict one theory, and at the same time can be consistent with another theory.
Reference: <author> Gaines B. </author> <year> (1989): </year> <title> Knowledge Acquisition: the Cintinuum Linking Machine Learni ng and Expertise Transfer, </title> <booktitle> in Proc of Third European Workshop on Knowledge Acquisition for Knowledge-Based Systems, </booktitle> <editor> J. Boose, B. Gaines, and J.G. Ganascia (eds.), </editor> <address> Paris, </address> <month> July </month> <year> 1989. </year>
Reference: <author> Gams M. </author> <year> (1989): </year> <title> "New Measurements Highlig ht the Importance of Redundant Knowledge", </title> <booktitle> in Proceedings of 4th European Working Session on Learning , K. Morik (ed.), </booktitle> <pages> pp. 71-80, </pages> <publisher> Pitman - Morgan Kaufmann. </publisher>
Reference: <author> Genesereth, M. and Nilsson, N. </author> <year> (1987): </year> <booktitle> Logical Foundations of Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann Publishers Inc. </publisher>
Reference: <author> Janikow C.Z. </author> <year> (1989): </year> <title> "The AQ16 Inductive Learn ing Program: Some Experimental Results with AQ16 and Other Symbolic and Nonsymbolic Programs", </title> <type> Rep. </type> <institution> of AI Center, George Mason University. </institution>
Reference-contexts: Knowledge Integration and Incremental Learning Knowledge integration is concerned with issues that are related to those in incremental learning systems, such as ID4 (Schlimmer and Fisher, 1988), ID5 (Utgoff, 1988), AQ16 <ref> (Janikow, 1989) </ref>, for example. Both knowledge integration and incremental learning attempt to construct a theory that explains best the given data. There are some important differences between the two approaches, however.
Reference: <author> Morik K. </author> <year> (1989): </year> <title> "Sloppy Modelling", in Knowledge R epresentation and Organization in Machine Learning, </title> <editor> K. Morik (ed.) </editor> <booktitle> pp. </booktitle> <pages> 107-134, </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: Some of these issues may be rather difficult to resolve. Different agents may not only use different predicate vocabulary in their conceptualization, but also rather different conceptualizations <ref> (Morik, 1989) </ref>. The AI community has been concerned with these problems, but no simple solution is in view. Obviously, we cannot provide a simple solution here, although we intend to work on some of these issues in future.
Reference: <author> Murray K.S and Porter B.W. </author> <year> (1989): </year> <title> "Controlling Sear ch for the Consequences of New Information During Knowledge Integration", </title> <booktitle> in Proc. of 6th International Workshop on Machine Learning , A.M. </booktitle>
Reference: <editor> Segre (ed.), </editor> <address> Ithaca, New York, </address> <publisher> Morgan Kaufmann Inc. </publisher>
Reference: <author> Quinlan J.R., </author> <year> (1986): </year> <title> "The Effect of Noise in Concept Learning", </title> <booktitle> Machine Learning (vol. II) , R. </booktitle>
Reference: <editor> Michalski, J. Carbonell and T. Mitchell (eds.), </editor> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Rada R. and Mili H. </author> <year> (1989): </year> <title> "A Knowledge Intensiv e Learning System for Document Retrieval", in Knowledge Representation and Organization in Machine Learning, </title> <editor> K. Morik (ed.), </editor> <booktitle> pp. </booktitle> <pages> 65-88, </pages> <publisher> Springer Verlag. </publisher>
Reference: <author> Shaw M. & Gaines B. </author> <year> (1989): </year> <title> Knowledge Acquisition: Some Fou ndations, Manual Methods and Future Trends, </title> <booktitle> in Proc. of Third European Workshop on Knowledege Acquisition for Knowledge-Based Systems, </booktitle> <editor> J.Boose, B. Gaines and J.G. Ganascia (eds.), </editor> <address> Paris, </address> <month> July </month> <year> 1989. </year>
Reference: <author> Steels L. and van de Velde W. </author> <year> (1989): </year> <title> "Learni ng in Second Generation Expert Systems", in Machine and Human Learning, </title> <editor> Y. Kodratoff and A. Hutchinson (eds.), </editor> <publisher> Michael Horwood Ltd. </publisher>
Reference: <author> Torgo L. </author> <year> (1990): </year> <title> "Incremental Rule Learning Using Entropy Based Search" Utgoff P.E. (1988): "ID5: An Incremental ID3", </title> <booktitle> in Proc. of 5th International Workshop on Machine Learning, </booktitle> <editor> J. Laird (ed.), Ann Habour, </editor> <publisher> Morgan Kaufmann Inc. </publisher>
Reference-contexts: The inductive rule learning system IRL1 is an incr emental learning prog ram, that was partially inspired by CN2 (Clark and Niblett, 1989). This system incrementally updates the existing rules either using generalization and/ or specialization. More details about this system will be made available in <ref> (Torgo, 1990) </ref>. Different theories needed by the knowledge integration system (INTEG.3) were generated by the inductive learning systems (ITL1 and IRL1) in a series of independent learning tasks.
Reference: <author> Van de Welde W. </author> <year> (1989): </year> <title> "Re(presentation) Issu es in Second Generation Expert Systems", in Knowledge Representation and Organization in Machine Learning , K. </title> <publisher> Morik (ed.), </publisher> <pages> pp. 107-134, </pages> <publisher> Springer Verlag. </publisher>
Reference: <author> Wielinga B.J. & Breuker J.A. </author> <year> (1986): </year> <title> Models of Expe rtise, </title> <booktitle> in Proc. of ECAI-86, </booktitle> <editor> L, Steels (ed.), </editor> <address> Brighton. </address>
Reference-contexts: As Gains (1989) has pointed out each approach has its own difficulties. Experts do often provide a mixture of relevant and irrelevant or erroneous information. The objective of current expertise transfer tools, such as Aquinas, KSS0 or KADS <ref> (Wielinga & Breuker, 1986) </ref> is to help the user to clean up the information provided. The disadvantages of a pure machine learning approach are also quite obvious. The system is too dependent on the data that is provided from outside. This data need not necessarily be complete.
Reference: <author> Zhang J. and Michalski R. </author> <year> (1989): </year> <title> "Rule Optimiz ation via SG-TRUNC Method", </title> <booktitle> in Proceedings of 4th European Working Session on Learning, </booktitle> <editor> K. Morik (ed.), </editor> <booktitle> pp. </booktitle> <pages> 251-262, </pages> <publisher> Pitman - Morgan Kaufmann. </publisher>
References-found: 26

