URL: http://www.isi.edu/natural-language/mt/ijcai95-glosser.ps
Refering-URL: http://www.isi.edu/natural-language/GAZELLE.html
Root-URL: 
Email: vh@cs.columbia.edu  knight@isi.edu  
Title: Unification-Based Glossing  
Author: Vasileios Hatzivassiloglou Kevin Knight 
Address: New York, NY 10027  4676 Admiralty Way Marina del Rey, CA 90292  
Affiliation: Department of Computer Science Columbia University  USC/Information Sciences Institute  
Abstract: We present an approach to syntax-based machine translation that combines unification-style interpretation with statistical processing. This approach enables us to translate any Japanese newspaper article into English, with quality far better than a word-for-word translation. Novel ideas include the use of feature structures to encode word lattices and the use of unification to compose and manipulate lattices. Unification also allows us to specify abstract features that delay target-language synthesis until enough source-language information is assembled. Our statistical component enables us to search efficiently among competing translations and locate those with high English fluency.
Abstract-found: 1
Intro-found: 1
Reference: [ Bahl et al., 1983 ] <author> L. R. Bahl, F. Jelinek, and R. L. Mercer. </author> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI, </journal> <volume> 5(2), </volume> <year> 1983. </year>
Reference-contexts: This tactic is optimal when no disambiguating information is available, since it selects the most likely translation, avoiding rare and very specialized alternatives. In the remainder of this section we discuss how we measure the probability of an English sentence from the probabilities of short sequences of words (n-grams <ref> [ Bahl et al., 1983 ] </ref> ), how we estimate these basic probabilities of n-grams, how we handle problems of sparse data by smoothing our estimates, and how we search the space of translation possibilities efficiently during translation so as to select the best scoring translations. 4.1 The sentence likelihood model <p> Finally, no forward estimates of the viability of a partial path are required (as is, for example, the case in the A* or stack decoder <ref> [ Jelinek et al., 1975; Bahl et al., 1983 ] </ref> algorithm). We first perform a topological sort of the states in the word lattice, so that we can visit each state after all its predecessors have been processed.
Reference: [ Brown et al., 1993 ] <author> P. F. Brown, S. A. Della-Pietra, V. J. Della-Pietra, and R. L. Mercer. </author> <title> The mathematics of statistical machine translation: Parameter estimation. </title> <journal> Computational Linguistics, </journal> <volume> 19(2), </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: However, our use of statistics allowed us to avoid much of the traditional hand-coding, and to produce a competitive MT system in nine months. Other statistical approaches to MT include CANDIDE <ref> [ Brown et al., 1993 ] </ref> , which does not do a syntactic analysis of the source text, and LINGSTAT [ Yamron et al., 1994 ] , which does probabilistic parsing. Both LING-STAT and JAPANGLOSS require syntax because they translate between languages with radically different word orders.
Reference: [ Chow and Schwartz, 1989 ] <author> Y. Chow and R. Schwartz. </author> <title> The N-Best algorithm: An efficient search procedure for finding top N sentence hypotheses. </title> <booktitle> In Proc. DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 199-202, </pages> <year> 1989. </year>
Reference-contexts: Consequently, a method is needed to efficiently search the word lattice and select a small set of highly likely translations. We adopted the N-best algorithm for this purpose <ref> [ Chow and Schwartz, 1989 ] </ref> .
Reference: [ Church and Gale, 1991 ] <author> K. W. Church and W. A. Gale. </author> <title> A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5, </volume> <year> 1991. </year>
Reference: [ Dowty et al., 1981 ] <author> D. R. Dowty, R. Wall, and S. Peters. </author> <title> Introduction to Montague Semantics. </title> <publisher> Reidel, </publisher> <address> Dordrecht, </address> <year> 1981. </year>
Reference-contexts: The semantic representation contains conceptual tokens drawn from the 70,000-term SENSUS ontology [ Knight and Luk, 1994 ] . Semantic analysis proceeds as a bottom-up walk of the parse tree, in the style of Mon-tague and Moore <ref> [ Dowty et al., 1981; Moore, 1989 ] </ref> . Semantics is compositional, with each parse tree node assigned a meaning based on the meanings of its children. Leaf node meanings are retrieved from a semantic lexicon, while meaning composition rules handle internal nodes.
Reference: [ Good, 1953 ] <author> I. J. </author> <title> Good. The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40, </volume> <year> 1953. </year>
Reference: [ Jelinek and Mercer, 1980 ] <author> F. Jelinek and R. L. Mercer. </author> <title> Interpolated estimation of Markov source parameters from sparse data. In Pattern Recognition in Practice. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1980. </year>
Reference-contexts: Church and Gale [ 1991 ] provide empirical evidence that indicates that the enhanced Good-Turing estimator outperforms both simple estimators such as the MLE and (to a lesser extent) other complex estimators such as an enhanced version of the deleted estimation method <ref> [ Jelinek and Mercer, 1980 ] </ref> . We have implemented the basic Good-Turing method for single words, allowing for 130,000 unseen words.
Reference: [ Jelinek et al., 1975 ] <author> F. Jelinek, L. R. Bahl, and R. L. Mercer. </author> <title> Design of a linguistic statistical decoder for the recognition of continuous speech. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 21(3), </volume> <year> 1975. </year>
Reference-contexts: Finally, no forward estimates of the viability of a partial path are required (as is, for example, the case in the A* or stack decoder <ref> [ Jelinek et al., 1975; Bahl et al., 1983 ] </ref> algorithm). We first perform a topological sort of the states in the word lattice, so that we can visit each state after all its predecessors have been processed.
Reference: [ Kaplan and Bresnan, 1982 ] <author> R.M. Kaplan and J. Bres-nan. </author> <title> Lexical-functional grammar: A formal system for grammatical representation. In The Mental Representation of Grammatical Relations. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1982. </year>
Reference-contexts: Semantics computes a conceptual representation while glossing computes a target language word lattice. In this notation, =c (a symbol borrowed from Lexical--Functional Grammar <ref> [ Kaplan and Bresnan, 1982 ] </ref> ) means the feature sequence must already exist in the incoming child constituent, and *XOR* sets up a disjunction of feature constraints, only one of which is allowed to be satisfied. 4 Statistical Language Modeling Our glosser module proposes a number of possible translations for
Reference: [ Knight and Chander, 1994 ] <author> K. Knight and I. Chander. </author> <title> Automated postediting of documents. </title> <booktitle> In Proc. AAAI, </booktitle> <year> 1994. </year>
Reference-contexts: Gaps in the generator's knowledge are filled with statistical techniques <ref> [ Knight and Hatzivassiloglou, 1995; Knight and Chander, 1994 ] </ref> , including a model that can rank potential generator outputs. The English lexicon includes 91,000 roots, comparable in size to the 130,000 roots used in Japanese syntactic analysis.
Reference: [ Knight and Hatzivassiloglou, 1995 ] <author> K. Knight and V. Hatzivassiloglou. </author> <title> Two-level, many-paths generation. </title> <booktitle> In Proc. ACL, </booktitle> <year> 1995. </year>
Reference-contexts: Gaps in the generator's knowledge are filled with statistical techniques <ref> [ Knight and Hatzivassiloglou, 1995; Knight and Chander, 1994 ] </ref> , including a model that can rank potential generator outputs. The English lexicon includes 91,000 roots, comparable in size to the 130,000 roots used in Japanese syntactic analysis. <p> Word lattices can be stored and manipulated as feature structures. 2. The compositional semantic interpreter can serve as a glosser, if we provide new knowledge bases. 3. The statistical model we built for ranking generator outputs <ref> [ Knight and Hatzivassiloglou, 1995 ] </ref> can also be used for glossing. This section describes how we put together an MT system based on these ideas. We concentrate here on the components and knowledge bases, deferring linguis tic and statistical aspects to following sections.
Reference: [ Knight and Luk, 1994 ] <author> K. Knight and S. K. Luk. </author> <title> Building a large-scale knowledge base for machine translation. </title> <booktitle> In Proc. AAAI, </booktitle> <year> 1994. </year>
Reference-contexts: INPUT: fi/fi2+BfiLK-fl INTERLINGUA: ((sem ((instance HAVE-AS-A-GOAL) (senser &lt;1&gt; ((instance COMPANY-BUSINESS) (q-mod ((instance NEW-VIRGIN))))) (phenomenon ((instance FOUND-LAUNCH) (agent &lt;1&gt;) (temporal-locating ((instance MONTH) (index 2)))))))) OUTPUT: The new company plans to establish in February. The semantic representation contains conceptual tokens drawn from the 70,000-term SENSUS ontology <ref> [ Knight and Luk, 1994 ] </ref> . Semantic analysis proceeds as a bottom-up walk of the parse tree, in the style of Mon-tague and Moore [ Dowty et al., 1981; Moore, 1989 ] . <p> All of these KBs, however, are still not enough to drive full semantic throughput. Major missing pieces include a large Japanese semantic lexicon and a set of ontological constraints. We are attacking these problems with a combination of manual and automatic techniques <ref> [ Oku-mura and Hovy, 1994; Knight and Luk, 1994 ] </ref> . Meanwhile, we want to test our current lexicons, rules, and analyzers in an end-to-end MT system. We have therefore modified our KBMT system to include a short-cut path from Japanese to English, which we describe in this paper.
Reference: [ Knight et al., 1994 ] <author> K. Knight, I. Chander, M. Haines, V. Hatzivassiloglou, E. Hovy, M. Iida, S. K. Luk, A. Okumura, R. Whitney, and K. Yamada. </author> <title> Integrating knowledge bases and statistics in MT. </title> <booktitle> In Proc. Conference of the Association for Machine Translation in the Americas (AMTA), </booktitle> <year> 1994. </year>
Reference-contexts: 1 Background JAPANGLOSS <ref> [ Knight et al., 1994; 1995 ] </ref> is a project whose goals are to scale up knowledge-based machine translation (KBMT) techniques to handle Japanese-English newspaper MT, to achieve higher quality output than is currently available, and to develop techniques for rapidly constructing MT systems.
Reference: [ Knight et al., 1995 ] <author> K. Knight, I. Chander, M. Haines, V. Hatzivassiloglou, E. Hovy, M. Iida, S. K. Luk, R. Whitney, and K. Yamada. </author> <title> Filling knowledge gaps in a broad-coverage MT system. </title> <booktitle> In Proc. IJCAI, </booktitle> <year> 1995. </year>
Reference: [ Moore, 1989 ] <author> R. Moore. </author> <title> Unification-based semantic interpretation. </title> <booktitle> In Proc. ACL, </booktitle> <year> 1989. </year>
Reference-contexts: The semantic representation contains conceptual tokens drawn from the 70,000-term SENSUS ontology [ Knight and Luk, 1994 ] . Semantic analysis proceeds as a bottom-up walk of the parse tree, in the style of Mon-tague and Moore <ref> [ Dowty et al., 1981; Moore, 1989 ] </ref> . Semantics is compositional, with each parse tree node assigned a meaning based on the meanings of its children. Leaf node meanings are retrieved from a semantic lexicon, while meaning composition rules handle internal nodes.
Reference: [ Nguyen et al., 1994 ] <author> L. Nguyen, R. Schwartz, Y. Zhao, and G. Zavaliagkos. </author> <title> Is N-Best dead? In Proc. </title> <booktitle> ARPA Human Language Technology Workshop. </booktitle> <institution> Advanced Research Projects Agency, </institution> <year> 1994. </year> [ <editor> NMSU/CRL et al., 1995 ] NMSU/CRL, CMU/CMT, and USC/ISI. </editor> <title> The Pan-gloss Mark III machine translation system. </title> <type> Technical Report CMU-CMT-95-145, </type> <institution> Carnegie Mellon University, 1995. Jointly issued by Computing Research Laboratory (New Mexico State University), Center for Machine Translation (Carnegie Mellon University), Information Sciences Institute (University of Southern California). Edited by S. Nirenburg. </institution>
Reference-contexts: It also offers controlled accuracy (i.e., the extent of sub-optimality can be arbitrarily decreased by the amount of memory made available to the search), and empirical studies <ref> [ Nguyen et al., 1994 ] </ref> have shown that it performs equally well with other, more complicated methods.
Reference: [ Okumura and Hovy, 1994 ] <author> A. Okumura and E. Hovy. </author> <title> Ontology concept association using a bilingual dictionary. </title> <booktitle> In Proc. of the ARPA Human Language Technology Workshop, </booktitle> <year> 1994. </year>
Reference: [ Penman, 1989 ] <author> Penman. </author> <title> The Penman documentation. </title> <type> Technical report, </type> <institution> USC/Information Sciences Institute, </institution> <year> 1989. </year>
Reference-contexts: = COMPANY-BUSINESS)) ((NP -&gt; S NP) ((X2 syn form) = (*NOT* rentaidome)) ((X0 sem instance) = rc-modified-object) ((X0 sem head) = (X2 sem)) ((X0 sem rel-mod) = (X1 sem)) (*OR* (((X1 map subject-role) =c X2)) (((X1 map object-role) =c X2)) (((X1 map object2-role) =c X2)))) Generation is performed by PENMAN <ref> [ Penman, 1989 ] </ref> , which includes a large systemic grammar of English. Gaps in the generator's knowledge are filled with statistical techniques [ Knight and Hatzivassiloglou, 1995; Knight and Chander, 1994 ] , including a model that can rank potential generator outputs.
Reference: [ Shieber, 1986 ] <author> S. Shieber. </author> <title> An Introduction to Unification-Based Approaches to Grammar. </title> <institution> University of Chicago, </institution> <year> 1986. </year> <note> Also, CSLI Lecture Notes Series. </note>
Reference-contexts: We syntactically analyze Japanese text, map it to a semantic representation, then generate English. Figure 1 shows a sample translation. Parsing is bottom-up, driven by an augmented context-free grammar whose format is roughly like that of <ref> [ Shieber, 1986 ] </ref> .
Reference: [ Viterbi, 1967 ] <author> A. J. </author> <title> Viterbi. Error bounds for convolution codes and an asymptotically optimal decoding algorithm. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13 </volume> <pages> 260-269, </pages> <year> 1967. </year> [ <editor> White and O'Connell, 1994 ] J. White and T. O'Connell. </editor> <booktitle> Evaluation in the ARPA machine translation program: 1993 methodology. In Proc. ARPA Human Language Technology Workshop, </booktitle> <year> 1994. </year>
Reference-contexts: Consequently, a method is needed to efficiently search the word lattice and select a small set of highly likely translations. We adopted the N-best algorithm for this purpose [ Chow and Schwartz, 1989 ] . Unlike the widely used Viterbi algorithm <ref> [ Viterbi, 1967 ] </ref> , which only produces a single best scoring path, this algorithm offers the advantage of producing any number of the highest scoring paths in the lattice; these paths can then be rescored with a more extensive (and expensive) method.
Reference: [ Yamron et al., 1994 ] <author> J. Yamron, J. Cant, A. Demedts, T. Dietzel, and Y. Ito. </author> <title> The automatic component of the LINGSTAT machine-aided translation system. </title> <booktitle> In Proc. ARPA Workshop on Human Language Technology, </booktitle> <year> 1994. </year>
Reference-contexts: Other statistical approaches to MT include CANDIDE [ Brown et al., 1993 ] , which does not do a syntactic analysis of the source text, and LINGSTAT <ref> [ Yamron et al., 1994 ] </ref> , which does probabilistic parsing. Both LING-STAT and JAPANGLOSS require syntax because they translate between languages with radically different word orders.
References-found: 21

