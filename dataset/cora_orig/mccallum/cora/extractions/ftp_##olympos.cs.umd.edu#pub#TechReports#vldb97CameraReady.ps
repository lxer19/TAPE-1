URL: ftp://olympos.cs.umd.edu/pub/TechReports/vldb97CameraReady.ps
Refering-URL: http://www.cs.toronto.edu/~mendel/dwbib.html
Root-URL: 
Email: christos@cs.umd.edu  jag@research.att.com  nikos@glue.umd.edu  
Title: Recovering Information from Summary Data  
Author: Christos Faloutsos H. V. Jagadish N. D. Sidiropoulos 
Address: College Park MD 20742  Florham Park, NJ 07932  College Park MD 20742  
Affiliation: Dept. of Computer Science and Inst. of Systems Research Univ. of Maryland  AT&T Laboratories  Inst. for Systems Research University of Maryland  
Abstract: Data is often stored in summarized form, as a histogram of aggregates (COUNTs, SUMs, or AVeraGes) over specified ranges. We study how to estimate the original detail data from the stored summary. We formulate this task as an inverse problem, specifying a well-defined cost function that has to be optimized under constraints. We show that our formulation includes the uniformity and independence assumptions as a special case, and that it can achieve better reconstruction results if we maximize the smoothness as opposed to the uniformity. In our experiments on real and synthetic datasets, the proposed method almost consistently outperforms its competitor, improving the root-mean-square error by up to 20 per cent for stock price data, and up to 90 per cent for smoother data sets. Finally, we show how to apply this theory to a variety of database problems that involve partial information, such as OLAP, data warehousing and histograms in query optimization. fl This research was partially funded by the National Science Foundation under Grants No. EEC-94-02384, IRI-9205273 and IRI-9625428. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 23rd VLDB Conference Athens, Greece, 1997 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Richard A. Becker, John M. Chambers, and Al-lan R. Wilks. </author> <title> The New S Language. Wadsworth & Brooks/Cole Advanced Books & Software, </title> <address> Pacific Grove, CA, </address> <year> 1988. </year> <pages> Page 9 </pages>
Reference-contexts: See Figure ??. * `LYNX' dataset (real): Canadian lynx trappings data per year, 1821-1934, for a total of N =114 samples. This is a well known dataset in population biology it can be found in any time-sequence book (e.g., [2]), as well as on-line through the "S" statistical package <ref> [1] </ref>. Notice that it has a periodicity of 9-10 years. However, it is not very smooth: it has abrupt population explosions, with significantly different peak values each time. See Figure ??. The experiments were designed to answer the fol lowing questions: 1.
Reference: [2] <author> George E.P. Box, Gwilym M. Jenkins, and Gre--gory C. Reinsel. </author> <title> Time Series Analysis: Forecasting and Control. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1994. </year> <note> 3rd Edition. </note>
Reference-contexts: See Figure ??. * `LYNX' dataset (real): Canadian lynx trappings data per year, 1821-1934, for a total of N =114 samples. This is a well known dataset in population biology it can be found in any time-sequence book (e.g., <ref> [2] </ref>), as well as on-line through the "S" statistical package [1]. Notice that it has a periodicity of 9-10 years. However, it is not very smooth: it has abrupt population explosions, with significantly different peak values each time. See Figure ??.
Reference: [3] <author> Chungmin M. Chen and Nick Roussopoulos. </author> <title> Adaptive selectivity estimation using query feedback. </title> <booktitle> Proc. of the ACM-SIGMOD, </booktitle> <pages> pages 161-172, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Ioannidis and Poosala [14] studied the trade-off between high prediction accuracy and ease of maintenance. Their recommendation was that histograms should maintain perfect information about selected attribute values, and assume the uniform distribution for the rest. A recent, adaptive method, has been suggested by Chen and Roussopoulos <ref> [3] </ref>. The idea is to approximate the unknown value distribution with a polynomial, and Page 2 to use query feedback to adjust the coefficients of the polynomial.
Reference: [4] <author> S. Christodoulakis. </author> <title> Implication of certain assumptions in data base performance evaluation. </title> <journal> ACM TODS, </journal> <month> June </month> <year> 1984. </year>
Reference-contexts: Early query optimizers used the uniformity assumption [23], which provably leads to pessimistic results <ref> [4] </ref>. Modern query optimiz-ers typically use histograms [14]. The histogram of an attribute gives the count of records that fall into each pre-determined sub-range ("bucket") of the attribute range. DeWitt and Muralikrishna [20] examined combined histograms for multiple attributes.
Reference: [5] <author> Shaul Dar, H.V. Jagadish, Alon Y. Levy, and Di-vesh Srivastava. </author> <title> Answering SQL queries with aggregation using views. </title> <type> Technical report, </type> <institution> AT&T, </institution> <year> 1995. </year>
Reference-contexts: All of these efforts have focussed on the logical nature of partial or missing information. In our paper, there is little qualitative reasoning and the logical analysis is trivial: the emphasis is on effective numerical estimation. Finally, there is much work on views with aggregates. For instance, <ref> [5] </ref> and [10] consider how to answer queries using aggregate views, and [11] shows how to maintain such views incrementally.
Reference: [6] <author> Heinz W. Engl, Martin Hanke, and Andreas Neubauer. </author> <title> Regularization of Inverse Problems. </title> <publisher> Kluwer, </publisher> <address> Dordrecht, </address> <year> 1996. </year>
Reference-contexts: Since data has been aggregated over incompatible ranges in the two base relations, such a union cannot easily be created. In this paper, we show how to attack this reconstruction problem formally. We formulate this as an inverse problem (cf. <ref> [6] </ref>) so that we can draw upon the vast array of literature on this topic in the field of signal processing. The paper is organized as follows. In Section 2 we present related work on query optimization and statistical databases. The mathematical problem formulation is given in Section 3.
Reference: [7] <author> Christos Faloutsos, H.V. Jagadish, and Nikolaos Sidiropoulos. </author> <title> Information recovery from partial data. </title> <type> Technical Report ISR-TR-97-7, </type> <institution> Inst. for Systems Research, Univ. of Maryland, College Park, MD, </institution> <year> 1997. </year>
Reference-contexts: Under appropriate convexity and continuity conditions, the textbook method for solving both the minimization and the maximization version of such problems is the method of Lagrange multipliers [17]. The details are in a technical report <ref> [7] </ref>. The main question is how to choose the functional F (). The objective is to minimize the expected value of the root-mean-squared error, given what we know a-priori about the distribution of values in the vector. <p> Proof: Omitted, for brevity (see <ref> [7] </ref>). QED 4.2 Linear Regularization In many situations, it is expected that there will only be a small difference between successive elements of the vector. Most population distributions, for large enough populations, would follow this principle. <p> This signal can be recovered from its contiguous non-overlapping partial sums fS k g k2Z , S k = i=b (k1)+1 x (i); 8k 2 Z Proof: Omitted for brevity (see <ref> [7] </ref>). QED Our Theorem guarantees full recovery when its conditions are met, and its proof is constructive, i.e., it specifies a filter that achieves full recovery. However, this means of recovery might impractical, or the conditions of the Theorem might not be exactly satisfied.
Reference: [8] <author> Georg Gottlob and Roberto Zicari. </author> <title> Closed world database opened through null values. </title> <booktitle> In Proc. 14th Int'l Conf. on Very Large Databases, </booktitle> <pages> pages 50-61, </pages> <year> 1988. </year>
Reference-contexts: Ng and Ravishankar [21] also consider multiple summary tables, and propose a matrix-algebra criterion to choose the best combination of summary tables to answer a query. Incomplete information has been studied extensively. For example, see [13] or <ref> [8] </ref>. The use of class structure, and other aggregation mechanisms, to store partial information has been presented in [15], and to respond to queries has been studied in [24]. All of these efforts have focussed on the logical nature of partial or missing information.
Reference: [9] <author> J. Gray, A. Bosworth, A. Layman, and H. Pi-rahesh. </author> <title> Data cube: a relational aggregation operator generalizing group-by, cross-tab, and subtotals. </title> <type> Technical Report No. </type> <institution> MSR-TR-95-22, Mi-crosoft, </institution> <year> 1995. </year>
Reference-contexts: This sort of technique is at the heart of the proposal in [16]. Managing such data well is a necessary prerequisite for effective data mining and decision support. * Statistical databases [18], particularly in conjunction with the DataCube operator <ref> [9, 12] </ref>: For example, consider Census data with income levels, given as summary tables (=histograms), with one histogram for each of several attributes (age, years in school, years in present job, geographic location etc.).
Reference: [10] <author> Ashish Gupta, Venkatesh Harinarayan, and Dal-lan Quass. </author> <title> Generalized projections: A powerful approach to aggregation. </title> <booktitle> In Proc. 21st International Conference on VLDB, </booktitle> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: All of these efforts have focussed on the logical nature of partial or missing information. In our paper, there is little qualitative reasoning and the logical analysis is trivial: the emphasis is on effective numerical estimation. Finally, there is much work on views with aggregates. For instance, [5] and <ref> [10] </ref> consider how to answer queries using aggregate views, and [11] shows how to maintain such views incrementally.
Reference: [11] <author> Ashish Gupta, Inderpal Singh Mumick, and V. S. Subrahmanian. </author> <title> Maintaining views incrementally. </title> <booktitle> In Proc. of ACM SIGMOD, </booktitle> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In our paper, there is little qualitative reasoning and the logical analysis is trivial: the emphasis is on effective numerical estimation. Finally, there is much work on views with aggregates. For instance, [5] and [10] consider how to answer queries using aggregate views, and <ref> [11] </ref> shows how to maintain such views incrementally.
Reference: [12] <author> Venky Harinarayan, Anand Rajaraman, and Jef-frey D. Ullman. </author> <title> Implementing data cubes efficiently. </title> <booktitle> In Proc. ACM SIGMOD, </booktitle> <pages> pages 205-216, </pages> <address> Montreal, Canada, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: This sort of technique is at the heart of the proposal in [16]. Managing such data well is a necessary prerequisite for effective data mining and decision support. * Statistical databases [18], particularly in conjunction with the DataCube operator <ref> [9, 12] </ref>: For example, consider Census data with income levels, given as summary tables (=histograms), with one histogram for each of several attributes (age, years in school, years in present job, geographic location etc.).
Reference: [13] <author> T. Imielinski and W. Lipski. </author> <title> Incomplete information in relational databases. </title> <journal> JACM, </journal> <volume> 31(4), </volume> <month> Octo-ber </month> <year> 1984. </year>
Reference-contexts: Ng and Ravishankar [21] also consider multiple summary tables, and propose a matrix-algebra criterion to choose the best combination of summary tables to answer a query. Incomplete information has been studied extensively. For example, see <ref> [13] </ref> or [8]. The use of class structure, and other aggregation mechanisms, to store partial information has been presented in [15], and to respond to queries has been studied in [24]. All of these efforts have focussed on the logical nature of partial or missing information.
Reference: [14] <author> Yannis E. Ioannidis and Viswanath Poosala. </author> <title> Balancing histogram optimality and practicality for query result size estimation. </title> <booktitle> ACM SIGMOD, </booktitle> <pages> pages 233-244, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: If reasonable guesses could quickly be made with respect to the daily totals, these were much preferred. The error could be estimated by computing over the full base data for selected sample aggregates. Page 1 * Query optimization: DBMSs typically maintain histograms <ref> [14] </ref> reporting the number of tuples for selected attribute-value ranges. Queries may select only specific values, or select ranges that only partially overlap with the value ranges used in the histogram. <p> Early query optimizers used the uniformity assumption [23], which provably leads to pessimistic results [4]. Modern query optimiz-ers typically use histograms <ref> [14] </ref>. The histogram of an attribute gives the count of records that fall into each pre-determined sub-range ("bucket") of the attribute range. DeWitt and Muralikrishna [20] examined combined histograms for multiple attributes. Ioannidis and Poosala [14] studied the trade-off between high prediction accuracy and ease of maintenance. <p> Modern query optimiz-ers typically use histograms <ref> [14] </ref>. The histogram of an attribute gives the count of records that fall into each pre-determined sub-range ("bucket") of the attribute range. DeWitt and Muralikrishna [20] examined combined histograms for multiple attributes. Ioannidis and Poosala [14] studied the trade-off between high prediction accuracy and ease of maintenance. Their recommendation was that histograms should maintain perfect information about selected attribute values, and assume the uniform distribution for the rest. A recent, adaptive method, has been suggested by Chen and Roussopoulos [3].
Reference: [15] <author> H. V. Jagadish. </author> <title> The incinerate data model. </title> <journal> ACM TODS, </journal> <volume> 20(1) </volume> <pages> 71-110, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Incomplete information has been studied extensively. For example, see [13] or [8]. The use of class structure, and other aggregation mechanisms, to store partial information has been presented in <ref> [15] </ref>, and to respond to queries has been studied in [24]. All of these efforts have focussed on the logical nature of partial or missing information. In our paper, there is little qualitative reasoning and the logical analysis is trivial: the emphasis is on effective numerical estimation.
Reference: [16] <author> H. V. Jagadish, Inderpal Singh Mumick, and Avi Silberschatz. </author> <title> View maintenance issues in the chronicle data model. </title> <booktitle> In Proc. ACM PODS, </booktitle> <pages> pages 113-124, </pages> <year> 1995. </year>
Reference-contexts: Thus, older records are either stored in tertiary storage, or discarded altogether. Saving summary data on-line, and providing a reconstruction algorithm, is an attractive alternative. This sort of technique is at the heart of the proposal in <ref> [16] </ref>.
Reference: [17] <author> D. G. Luenberger. </author> <title> Introduction to Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1973. </year>
Reference-contexts: Under appropriate convexity and continuity conditions, the textbook method for solving both the minimization and the maximization version of such problems is the method of Lagrange multipliers <ref> [17] </ref>. The details are in a technical report [7]. The main question is how to choose the functional F (). The objective is to minimize the expected value of the root-mean-squared error, given what we know a-priori about the distribution of values in the vector.
Reference: [18] <author> Francesco M. Malvestuto. </author> <title> A universal-scheme approach to statistical databases containing homge-neous summary tables. </title> <journal> ACM TODS, </journal> <volume> 18(4) </volume> <pages> 678-708, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Saving summary data on-line, and providing a reconstruction algorithm, is an attractive alternative. This sort of technique is at the heart of the proposal in [16]. Managing such data well is a necessary prerequisite for effective data mining and decision support. * Statistical databases <ref> [18] </ref>, particularly in conjunction with the DataCube operator [9, 12]: For example, consider Census data with income levels, given as summary tables (=histograms), with one histogram for each of several attributes (age, years in school, years in present job, geographic location etc.). <p> Related work appeared in statistical databases: Malvestuto <ref> [18] </ref> examined the case of multiple summary tables, and developed algorithms to determine whether a given query can be evaluated to a single number, a range, or not at all.
Reference: [19] <author> B. Mandelbrot. </author> <title> Fractal Geometry of Nature. W.H. </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: It is a pleasant surprise that the Linear Regularization does better even for the `IBM' dataset, which is not smooth at all. In fact, being a stock price movement, it is expected to be a "random walk", which is known to be a "fractal", with fractal dimension 1.5 <ref> [19] </ref>. That is, it is nowhere close to being smooth.
Reference: [20] <author> M. Muralikrishna and David J. DeWitt. </author> <title> Equi-depth histograms for estimating selectivity factors for multi-dimensional queries. </title> <booktitle> Proc. ACM SIGMOD, </booktitle> <pages> pages 28-36, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Early query optimizers used the uniformity assumption [23], which provably leads to pessimistic results [4]. Modern query optimiz-ers typically use histograms [14]. The histogram of an attribute gives the count of records that fall into each pre-determined sub-range ("bucket") of the attribute range. DeWitt and Muralikrishna <ref> [20] </ref> examined combined histograms for multiple attributes. Ioannidis and Poosala [14] studied the trade-off between high prediction accuracy and ease of maintenance. Their recommendation was that histograms should maintain perfect information about selected attribute values, and assume the uniform distribution for the rest.
Reference: [21] <author> Wee-Keong Ng and Chinya V. Ravishankar. </author> <title> Information synthesis in statistical databases. </title> <booktitle> Proc. CIKM, </booktitle> <pages> pages 355-361, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Related work appeared in statistical databases: Malvestuto [18] examined the case of multiple summary tables, and developed algorithms to determine whether a given query can be evaluated to a single number, a range, or not at all. Ng and Ravishankar <ref> [21] </ref> also consider multiple summary tables, and propose a matrix-algebra criterion to choose the best combination of summary tables to answer a query. Incomplete information has been studied extensively. For example, see [13] or [8].
Reference: [22] <author> William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year> <note> 2nd Edition. </note>
Reference-contexts: While the specific error metric used is not likely to be critical, for the sake of specificity we focus on the root-mean-squared error. The theory of inverse problems <ref> [22] </ref> is applicable to the question at hand. Our specific case is typically under-constrained and thus ill-posed. Since the original vector is not known, we cannot use the root-mean-squared error as the objective function. <p> The objective is to minimize the expected value of the root-mean-squared error, given what we know a-priori about the distribution of values in the vector. In the following subsections we describe two popular criteria, namely, Maximum Entropy and Linear Regularization. 4.1 Maximum Entropy (ME) Maximum Entropy (e.g., <ref> [22, sec. 18.7] </ref>) will introduce no additional constraints on the nature of the signal to be estimated. <p> Therefore, the problem becomes: Minimize Eq. 6, subject to the conditions of Eq. 5. The functional of Eq. 6 results in an instance of so-called Linear Regularization (or `Phillips-Twomey method', or `constrained linear inversion method' or `Tikhonov-Miller regularization' <ref> [22] </ref>). <p> This may be prohibitive, particularly since N may often be large. However, in our case, the matrix is of a special form: it is almost tri-diagonal, and specifically, it is singly-bordered tri-diagonal, and the border itself is block-diagonal. In this case, matrix inversion has complexity O (M ) <ref> [22, p. 72] </ref>, that is linear on the matrix side. Since the length of the unknown distribution, N , is significantly greater than the number of batches/constraints n, for all practical purposes we can think of the inversion effort as O (N ). <p> How to do this is shown in Appendix ??. The resulting matrix equation ?? can now be solved using any standard linear algebra package. In particular, we recommend the technique that makes use of the tri-diagonal nature of the matrix <ref> [22, p. 72] </ref>, as discussed earlier. 7 Conclusions The main contribution of this work is a formal approach to the recovery of information from summary data, and, more generally, arbitrary, partial data in the form of constraints.
Reference: [23] <author> P.G. Selinger, D.D. Astrahan, R.A. Chamberlain, R.A. Lorie, and T.G. Price. </author> <title> Access path selection in a relational database management system. </title> <booktitle> Proc. ACM-SIGMOD, </booktitle> <pages> pages 23-34, </pages> <year> 1979. </year>
Reference-contexts: Our conclusions and future research directions are discussed in Section 7. 2 Survey There is a large body of related work on query optimization, where the problem is to "guess" the attribute value distribution, to make selectivity estimates for specified queries. Early query optimizers used the uniformity assumption <ref> [23] </ref>, which provably leads to pessimistic results [4]. Modern query optimiz-ers typically use histograms [14]. The histogram of an attribute gives the count of records that fall into each pre-determined sub-range ("bucket") of the attribute range. DeWitt and Muralikrishna [20] examined combined histograms for multiple attributes.
Reference: [24] <author> Chung-Dak Shum and Richard Muntz. </author> <title> An information-theoretic study on aggregate responses. </title> <booktitle> Proc. of VLDB, </booktitle> <pages> pages 479-490, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Incomplete information has been studied extensively. For example, see [13] or [8]. The use of class structure, and other aggregation mechanisms, to store partial information has been presented in [15], and to respond to queries has been studied in <ref> [24] </ref>. All of these efforts have focussed on the logical nature of partial or missing information. In our paper, there is little qualitative reasoning and the logical analysis is trivial: the emphasis is on effective numerical estimation. Finally, there is much work on views with aggregates.
Reference: [25] <author> Yannis Theodoridis and Timos Sellis. </author> <title> A model for the prediction of r-tree performance. </title> <booktitle> Proc. of ACM PODS, </booktitle> <year> 1996. </year>
Reference-contexts: A recent, adaptive method, has been suggested by Chen and Roussopoulos [3]. The idea is to approximate the unknown value distribution with a polynomial, and Page 2 to use query feedback to adjust the coefficients of the polynomial. Similar approaches have been used for spatial databases: Theodoridis and Sellis <ref> [25] </ref> suggest a coarse discretization of the address space; for each grid cell, they use the average data density, and, making the uniformity assumption for each individual grid-cell, they estimate the performance of an R-tree.
Reference: [26] <author> Andreas S. Weigend and Neil A. Gerschenfeld. </author> <title> Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <publisher> Addison Wesley, </publisher> <year> 1994. </year>
Reference-contexts: We used both the Maximum Entropy method and the Linear Regularization method. The measure of success was the normalized root-mean-square error (RMS), which is a typical measure for forecasting in time series <ref> [26] </ref>. Specifically, we define: RM S = 1=N i=1 (x i x actual;i ) 2 ! 1=2 where x i is the reconstructed value and x actual;i is the actual value at time i. We ran our experiments on a number of real and synthetic datasets. These are discussed below.
Reference: [27] <author> Jennifer Widom. </author> <title> Research problems in data warehousing. </title> <address> CIKM, </address> <month> November </month> <year> 1995. </year> <note> Invited paper. Page 10 </note>
Reference-contexts: Cost estimation for such queries will benefit from an accurate reconstruction of attribute-value occurrences for the queried value (- range). Similarly, range queries on multiple attributes will benefit from an accurate synthesis and extrapolation from the histograms of value distributions for individual attributes. * Data warehousing <ref> [27] </ref>: The idea is that the central site will have meta-data, and condensed information (e.g., summary data) from each participating site, which has detailed information.
References-found: 27

