URL: http://www.cs.nyu.edu/vijayk/papers/cm5-t3d-messaging.ps
Refering-URL: http://www.cs.nyu.edu/vijayk/papers.html
Root-URL: http://www.cs.nyu.edu
Email: fvijayk,achieng@cs.uiuc.edu  
Title: A Comparison of Architectural Support for Messaging in the TMC CM-5 and the Cray T3D  
Author: Vijay Karamcheti and Andrew A. Chien 
Date: June 22-24, 1995.  
Note: In Proceedings  
Address: 1304 W. Springfield Avenue, Urbana, IL 61801  ISCA'95, Santa Margherita, Italy,  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign  of  
Abstract: Programming models based on messaging continue to be an important programming model for parallel machines. Messaging costs are strongly influenced by a machine's network interface architecture. We examine the impact of architectural support for messaging in two machines the TMC CM-5 and the Cray T3D by exploring the design and performance of several messaging implementations. The additional features in the T3D support remote operations: memory access, fetch-and-increment, atomic swaps, and prefetch. Experiments on the CM-5 show that requiring processor involvement for message reception can increase the communication overheads from 60% to 300% for moderate variations in computation grain size at the destination. In contrast, the T3D hardware for remote operations decouples message reception from processor activity, producing high-performance messaging independent of computation grain size or variability. In addition, hardware support for a shared address space in the T3D can be used to solve the output contention problem (output hot spots), producing messaging implementations that are robust over a wide variety of traffic patterns. Atomic swap hardware can be used to build a distributed message queue, enabling a pull messaging scheme where the destination requests data transfer upon receive. This scheme uses prefetches to mask receive latency. While this yields performance robust over output contention, its base cost is competitive only for small messages (up to 64 bytes) because of the high cost of issuing and resolving prefetches in the T3D. Emulation shows that if the interaction costs can be reduced by a factor of eight (250ns to 31ns), perhaps by moving the prefetch queue on chip, and there is a corresponding increase in the prefetch queue size, the pull scheme can give superior performance in all cases. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H. T. Kung et al. </author> <title> Supporting systolic and memory communication in iWarp. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 70-81. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1990. </year>
Reference-contexts: Research on specialized hardware support for messaging has focused primarily on integrating message processing within the processor <ref> [10, 14, 1, 25] </ref>. These approaches are effective in reducing point-to-point costs, but provide no solutions for network and output contention. In contrast, we have investigated messaging atop shared address space primitives and demonstrated that it can deliver performance robust over output contention.
Reference: [2] <author> Eric A. Brewer and Bradley C. Kuszmaul. </author> <title> How to get good performance from the CM-5 data network. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <pages> pages 858-867, </pages> <year> 1994. </year>
Reference-contexts: This approach reflects our experience and that of others <ref> [2, 9] </ref> that network performance under aggregate loads is often quite different from unloaded scenarios. We compare the communication performance characteristics of the Thinking Machines CM-5 and the Cray T3D, using traditional push messaging implementations which transfer data eagerly from source to destination. <p> First, the communication rate is limited to the rate at which the processor can move data across the memory bus. Second, communication operations occupy the processor, preventing the overlap of computation and communication. And finally, because the network requires consistent draining to maintain good performance <ref> [2] </ref>, the processor needs to poll the network interface often. Since each poll requires a significant number of cycles, frequent polling can incur a significant overhead. <p> Researchers have also explored ways of managing the network to improve communication performance. While most of this work deals with single link management [18], others <ref> [2] </ref> have looked at obtaining performance on the CM-5 data network for all-pairs communication traffic. However, their solutions (frequent barriers and packet reordering) consider only permutation traffic patterns with moderate output contention and require all processors to cooperate for achieving high performance.
Reference: [3] <author> H. Levy C. A. Thekkath and E. D. Lazowska. </author> <title> Separating data and control transfer in distributed operating systems. </title> <booktitle> In Proceedings of the Sixth Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <year> 1994. </year>
Reference-contexts: In contrast, we have investigated messaging atop shared address space primitives and demonstrated that it can deliver performance robust over output contention. Research on messaging layer approaches to improve communication costs have typically investigated minimizing kernel interaction <ref> [22, 11, 3] </ref> and the active messages [23] approach of offloading all but the essential operations from the messaging layer. Our study evaluates architectural support for messaging and shows that while the above approaches reduce the point-to-point messaging costs, they are inadequate to prevent performance degradation due to output contention.
Reference: [4] <author> K. Mani Chandy and Carl Kesselman. </author> <title> Compositional C++: Compositional parallel programming. </title> <booktitle> In Fifth Workshop on Compilers and Languages for Parallel Computing, </booktitle> <address> New Haven, Connecticut, </address> <year> 1992. </year>
Reference-contexts: Data transport is the essence of messaging and flow control (buffer management) ensures that there is space for messages at the destination. Flow control is required by general messaging models such as ones supported by PVM [13] or MPI [12], and programming models which produce concurrency dynamically <ref> [6, 4] </ref>. Note that primitives such as PUT/GET only specify data movement flow control is still necessary, the source and destination processors must interact to decide where the message must be buffered. If there is no space, the destination must be able to block the source.
Reference: [5] <author> Andrew Chien, Vijay Karamcheti, and John Plevyak. </author> <title> The Concert system compiler and runtime support for efficient fine-grained concurrent object-oriented programs. </title> <type> Technical Report UIUCDCS-R-93-1815, </type> <institution> Department of Computer Science, University of Illinois, Urbana, Illinois, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: This study was motivated by the high communication costs that we observed in our CM-5 implementation of a concurrent object-oriented language which generates very irregular communication traffic <ref> [5, 16] </ref>. We are currently in the process of porting the implementation to the T3D. Future work will examine the implications of architectural support in the context of traffic patterns generated by large application programs.
Reference: [6] <author> Andrew A. Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: Data transport is the essence of messaging and flow control (buffer management) ensures that there is space for messages at the destination. Flow control is required by general messaging models such as ones supported by PVM [13] or MPI [12], and programming models which produce concurrency dynamically <ref> [6, 4] </ref>. Note that primitives such as PUT/GET only specify data movement flow control is still necessary, the source and destination processors must interact to decide where the message must be buffered. If there is no space, the destination must be able to block the source.
Reference: [7] <author> S. Chittor and R. Enbody. </author> <title> Performance evaluation of mesh-connected wormhole-routed networks for interprocessor communication in mul-ticomputers. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <pages> pages 647-56, </pages> <year> 1990. </year>
Reference-contexts: This study is a beginning, more work needs to be done to deter mine the right set of network interfaces which can provide robust messaging performance. 7 Related work While several studies have measured communication performance on parallel machines <ref> [7, 21, 15] </ref>, they have not explored the relationship between network interface architecture and messaging performance. A great deal of attention has also been focused separately on specialized hardware support for messaging, the design of efficient messaging layers, and network management for improving communication performance.
Reference: [8] <author> Cray Research, Inc. </author> <title> Cray T3D System Architecture Overview, </title> <year> 1993. </year>
Reference-contexts: 1 Introduction In the past decade, parallel machines <ref> [8, 22] </ref> have become significant competitors to vector machines in the quest for highest performance computing. Applications on parallel machines rely on inter-node communication to transfer data, synchronize, and coordinate the parallel computation. This coordination determines the parallelism effectively exploited and hence the achievable machine performance. <p> In all cases, the network fabrics are more than sufficient to support the traffic performance is limited by the network endpoints. We compare the network interface hardware in two commercial machines, the Thinking Machines CM-5 [22] and the Cray T3D <ref> [8] </ref>, examining how the network interface architecture affects the structure and performance of the software messaging layer. 1 In addition to using point-to-point benchmarks, we use more complex multi-party communication patterns, exploring network behavior for a range of patterns of use.
Reference: [9] <author> David Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages an Operating Systems, </booktitle> <pages> pages 164-75, </pages> <year> 1991. </year>
Reference-contexts: This approach reflects our experience and that of others <ref> [2, 9] </ref> that network performance under aggregate loads is often quite different from unloaded scenarios. We compare the communication performance characteristics of the Thinking Machines CM-5 and the Cray T3D, using traditional push messaging implementations which transfer data eagerly from source to destination.
Reference: [10] <author> William J. Dally et al. </author> <title> The J-Machine: A fine-grain concurrent computer. </title> <booktitle> In Information Processing 89, Proceedings of the IFIP Congress, </booktitle> <pages> pages 1147-1153, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Research on specialized hardware support for messaging has focused primarily on integrating message processing within the processor <ref> [10, 14, 1, 25] </ref>. These approaches are effective in reducing point-to-point costs, but provide no solutions for network and output contention. In contrast, we have investigated messaging atop shared address space primitives and demonstrated that it can deliver performance robust over output contention.
Reference: [11] <author> Peter Druschel and Larry L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 189-202. </pages> <publisher> ACM SIGOPS, ACM Press, </publisher> <month> December </month> <year> 1993. </year>
Reference-contexts: In contrast, we have investigated messaging atop shared address space primitives and demonstrated that it can deliver performance robust over output contention. Research on messaging layer approaches to improve communication costs have typically investigated minimizing kernel interaction <ref> [22, 11, 3] </ref> and the active messages [23] approach of offloading all but the essential operations from the messaging layer. Our study evaluates architectural support for messaging and shows that while the above approaches reduce the point-to-point messaging costs, they are inadequate to prevent performance degradation due to output contention.
Reference: [12] <author> Message Passing Interface Forum. </author> <title> The MPI message passing interface standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, </institution> <year> 1994. </year>
Reference-contexts: Data transport is the essence of messaging and flow control (buffer management) ensures that there is space for messages at the destination. Flow control is required by general messaging models such as ones supported by PVM [13] or MPI <ref> [12] </ref>, and programming models which produce concurrency dynamically [6, 4]. Note that primitives such as PUT/GET only specify data movement flow control is still necessary, the source and destination processors must interact to decide where the message must be buffered.
Reference: [13] <author> G. Geist and V. Sunderam. </author> <title> The PVM system: Supercomputer level concurrent computation on a heterogeneous network of workstations. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computers Conference, </booktitle> <pages> pages 258-61, </pages> <year> 1991. </year>
Reference-contexts: In this layer, messages are received and 1 Note that although the T3D has hardware support for a shared address space, a major fraction of its workload uses PVM <ref> [13] </ref>, so messaging performance is a significant issue on the T3D. buffered in local memory without involvement of the receiving pro-cessor. <p> Data transport is the essence of messaging and flow control (buffer management) ensures that there is space for messages at the destination. Flow control is required by general messaging models such as ones supported by PVM <ref> [13] </ref> or MPI [12], and programming models which produce concurrency dynamically [6, 4]. Note that primitives such as PUT/GET only specify data movement flow control is still necessary, the source and destination processors must interact to decide where the message must be buffered.
Reference: [14] <author> D. S. Henry and C. F. Joerg. </author> <title> A tightly-coupled processor-network interface. </title> <booktitle> In Proceedingsof the Fifth International Conferenceon Architectural Support for Programming Languages an Operating Systems, </booktitle> <pages> pages 111-122, </pages> <year> 1992. </year>
Reference-contexts: Research on specialized hardware support for messaging has focused primarily on integrating message processing within the processor <ref> [10, 14, 1, 25] </ref>. These approaches are effective in reducing point-to-point costs, but provide no solutions for network and output contention. In contrast, we have investigated messaging atop shared address space primitives and demonstrated that it can deliver performance robust over output contention.
Reference: [15] <author> R. W. Hockney and E. A. Carmona. </author> <title> Comparison of communication on the Intel iPSC/860 and Touchstone Delta. </title> <journal> Parallel Computing, </journal> (18):1067-1072, 1992. 
Reference-contexts: This study is a beginning, more work needs to be done to deter mine the right set of network interfaces which can provide robust messaging performance. 7 Related work While several studies have measured communication performance on parallel machines <ref> [7, 21, 15] </ref>, they have not explored the relationship between network interface architecture and messaging performance. A great deal of attention has also been focused separately on specialized hardware support for messaging, the design of efficient messaging layers, and network management for improving communication performance.
Reference: [16] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concert efficient runtime support for concurrent object-oriented programming languages on stock hardware. </title> <booktitle> In Proceedings of Supercomputing'93, </booktitle> <year> 1993. </year>
Reference-contexts: This study was motivated by the high communication costs that we observed in our CM-5 implementation of a concurrent object-oriented language which generates very irregular communication traffic <ref> [5, 16] </ref>. We are currently in the process of porting the implementation to the T3D. Future work will examine the implications of architectural support in the context of traffic patterns generated by large application programs.
Reference: [17] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Software overhead in messaging layers: </title> <booktitle> Where does the time go? In Proceedings of the Sixth Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <year> 1994. </year>
Reference-contexts: The destination receives the data packets, re-assembles them into the allocated buffer and invokes the specified user handler on transfer completion (Step 4). Flow control is ensured by the buffer allocation protocol; the destination does not respond until it has space to store the contents of the transfer. See <ref> [17] </ref> for a detailed breakdown of the software messaging costs in CMAM. 3.2 T3D Fetch-and-increment Messaging The FM fetch-and-increment messaging primitives use the fetch-and-increment (F&I) hardware [19] as shown in Figure 4.
Reference: [18] <author> R. Metcalfe and D. Boggs. </author> <title> Ethernet: Distributed packet-switching for local computer networks. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 19(7) </volume> <pages> 395-404, </pages> <year> 1976. </year>
Reference-contexts: Researchers have also explored ways of managing the network to improve communication performance. While most of this work deals with single link management <ref> [18] </ref>, others [2] have looked at obtaining performance on the CM-5 data network for all-pairs communication traffic. However, their solutions (frequent barriers and packet reordering) consider only permutation traffic patterns with moderate output contention and require all processors to cooperate for achieving high performance.
Reference: [19] <author> Robert W. </author> <title> Numrich. The Cray T3D address space and how to use it. </title> <type> Technical report, </type> <institution> Cray Research, Inc., </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: SUPPORT CIRCUITRY CPU MEMORY NETWORK INTERFACE NETWORK DTB ANNEX DATA PREFETCH QUEUE ATOMIC SWAP REGISTERS FETCH-AND INCREMENT INPUT BUFFER OUTPUT BUFFER T3D nodes also support three special memory operations for synchronization and latency hiding - fetch-and-increment, atomic swap, and prefetch <ref> [19] </ref>. First, each processor has a fetch-and-increment register that can be read or written by other processors. Reading the fetch-and-increment register atomically increments the register's contents and returns the original contents to the requesting processor. Second, each processor has a swaperand register which implements atomic swaps with remote memory locations. <p> See [17] for a detailed breakdown of the software messaging costs in CMAM. 3.2 T3D Fetch-and-increment Messaging The FM fetch-and-increment messaging primitives use the fetch-and-increment (F&I) hardware <ref> [19] </ref> as shown in Figure 4. The atomic increment of the F&I register is used to serialize message PROCESSOR PROCESSORMEMORY MEMORY 1 fetch-and-increment +1 extract 4 3 remote stores data transfer SOURCE DESTINATION reception and allocate buffers for incoming messages. <p> minimizes the latency of reception, but can cause contention at network outputs. 3.3 T3D Atomic-swap Messaging PROCESSOR PROCESSORMEMORY MEMORY 1 atomic swap source buffer data remote store to old tail pointer 2 remote loads 3 release buffer 4 SOURCE DESTINATION The FM atomic-swap messaging primitives use the T3D swap hardware <ref> [19] </ref> to implement a distributed message queue. Each processor holds a tail pointer for its distributed message queue in a local memory location.
Reference: [20] <author> G. F. Pfister and V. A. Norton. </author> <title> Hot spot contention and combining in multistage interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):943-948, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: More significantly, output contention produces dramatic degradations in performance, increasing source overheads by another order of magnitude from the all-to-all case. For the message size considered above, the source overheads are 9000s for the hotspot,x16 pattern, and 21550s for the all-to-one pattern. Our results demonstrate the classic hotspot saturation <ref> [20] </ref>, where a single blocked output can back up and significantly degrade communication performance for even unrelated traffic.
Reference: [21] <author> R. Ponnusamy, R. Thakur, A. Choudhary, and G. Fox. </author> <title> Scheduling regular and irregular communication patterns on the CM-5. </title> <booktitle> In Supercomputing '92, </booktitle> <pages> pages 394-402, </pages> <year> 1992. </year>
Reference-contexts: This study is a beginning, more work needs to be done to deter mine the right set of network interfaces which can provide robust messaging performance. 7 Related work While several studies have measured communication performance on parallel machines <ref> [7, 21, 15] </ref>, they have not explored the relationship between network interface architecture and messaging performance. A great deal of attention has also been focused separately on specialized hardware support for messaging, the design of efficient messaging layers, and network management for improving communication performance.
Reference: [22] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 Technical Summary, </title> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction In the past decade, parallel machines <ref> [8, 22] </ref> have become significant competitors to vector machines in the quest for highest performance computing. Applications on parallel machines rely on inter-node communication to transfer data, synchronize, and coordinate the parallel computation. This coordination determines the parallelism effectively exploited and hence the achievable machine performance. <p> In all cases, the network fabrics are more than sufficient to support the traffic performance is limited by the network endpoints. We compare the network interface hardware in two commercial machines, the Thinking Machines CM-5 <ref> [22] </ref> and the Cray T3D [8], examining how the network interface architecture affects the structure and performance of the software messaging layer. 1 In addition to using point-to-point benchmarks, we use more complex multi-party communication patterns, exploring network behavior for a range of patterns of use. <p> In contrast, we have investigated messaging atop shared address space primitives and demonstrated that it can deliver performance robust over output contention. Research on messaging layer approaches to improve communication costs have typically investigated minimizing kernel interaction <ref> [22, 11, 3] </ref> and the active messages [23] approach of offloading all but the essential operations from the messaging layer. Our study evaluates architectural support for messaging and shows that while the above approaches reduce the point-to-point messaging costs, they are inadequate to prevent performance degradation due to output contention.
Reference: [23] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: We consider the cost of implementing an active-messages like interface on both the CM5 and the T3D. In an active-messages interface, data transfers are coupled with invoking a handler at the destination node [24]. On the CM-5, this interface is implemented using the UC Berkeley active messages library (CMAM) <ref> [23] </ref>. <p> In contrast, we have investigated messaging atop shared address space primitives and demonstrated that it can deliver performance robust over output contention. Research on messaging layer approaches to improve communication costs have typically investigated minimizing kernel interaction [22, 11, 3] and the active messages <ref> [23] </ref> approach of offloading all but the essential operations from the messaging layer. Our study evaluates architectural support for messaging and shows that while the above approaches reduce the point-to-point messaging costs, they are inadequate to prevent performance degradation due to output contention.
Reference: [24] <author> Thorsten von Eicken and David E. Culler. </author> <title> Building Communication Paradigms with the CM-5 Active Message layer (CMAM). </title> <institution> University of California, Berkeley, </institution> <address> 2.4 edition, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: We consider the cost of implementing an active-messages like interface on both the CM5 and the T3D. In an active-messages interface, data transfers are coupled with invoking a handler at the destination node <ref> [24] </ref>. On the CM-5, this interface is implemented using the UC Berkeley active messages library (CMAM) [23].
Reference: [25] <author> Colin Whitby-Strevens. </author> <title> The transputer. </title> <booktitle> In Proceedings of 12th International Symposium on Computer Architecture, </booktitle> <year> 1985. </year>
Reference-contexts: Research on specialized hardware support for messaging has focused primarily on integrating message processing within the processor <ref> [10, 14, 1, 25] </ref>. These approaches are effective in reducing point-to-point costs, but provide no solutions for network and output contention. In contrast, we have investigated messaging atop shared address space primitives and demonstrated that it can deliver performance robust over output contention.
References-found: 25

