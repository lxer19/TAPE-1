URL: http://polaris.cs.uiuc.edu/reports/1053.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Detecting Redundant Accesses to Array Data (Extended Version)  
Author: Elana D. Granston Alexander V. Veidenbaum 
Note: Much of this work was completed while Elana D. Granston was at the Center for Supercomputing Research and Development. Support was provided by the Department of Energy under Grant No. DE-FG02-85ER25001, Cray Research Inc., and the Esprit Agency under Grant No. APPARC 6634 BRA III. Support was provided by the NASA Ames Research Center under Grant No. NASA NCC 2-559, and the National Science Foundation under Grant No. NSF 89-20891.  
Date: November, 1991 Revised: October, 1992  
Address: Niels Bohrweg 1, 2333 CA Leiden, The Netherlands  1308 W. Main St., Urbana, Illinois 61801  
Affiliation: High Performance Computing Division Department of Computer Science Leiden University  Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: Alleviating memory access delays is crucial to harnessing the potential of high-performance, hierarchical-memory systems, especially vector and parallel systems. In the numerical applications typically executed on such systems, a significant portion of global memory data traffic and, hence, memory system delays arise from accesses to blocks of array elements. These delays can be reduced by using compile-time information to detect when global read and write accesses are redundant (unnecessary) and can be replaced by accesses to local data copies or eliminated entirely. Eliminating a redundant access can speed up program execution in two ways; first, the slow global memory access is replaced by a faster local access. Second, the time to perform other global memory accesses may be reduced as a result of the decreased traffic level and, hence, contention. In a previous paper, we introduced the notion of redundancy analysis. In this paper, we develop our terminology more fully. We also detail our compile time algorithm for applying combined flow and dependence analysis to sequential and parallel programs to detect redundancies across loop nests and in the presence of con ditionals. Furthermore, we show how this information can be used to eliminate redundancies. 
Abstract-found: 1
Intro-found: 1
Reference: [AB85] <author> James Archibald and Jean-Loup Baer. </author> <title> An Economical Solution to the Cache Coherence Problem. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 355-362, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: By combining flow and dependence analysis, the techniques presented in this paper provide more detailed information on stale data than was previously available [CV88]. Hardware-based directory schemes, such as <ref> [Tan76, CF78, AB85, Smi85] </ref>, provide an alternative to software-based schemes. The success of these hardware-based schemes hinges on being able to minimize global communication overhead. Hence, their performance is improved when redundancies are eliminated, as this reduces communication overhead as well.
Reference: [AK86] <author> Randy Allen and Ken Kennedy. </author> <title> Vector Register Allocation. </title> <type> Technical Report COMP TR86-45, </type> <institution> Rice University, </institution> <year> 1986. </year> <month> Revised March </month> <year> 1987 </year> <month> and August </month> <year> 1988. </year>
Reference-contexts: Aligning accesses across several doall loops can also increase the number of redundancies to expose. Note that, even when such opportunities are detected, insufficient temporal locality may prevent us from capitalizing on them. Hence, redundancy analysis is most beneficial when combined with locality-enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>. 30 Acknowledgements We would like to thank Milind Girkar, Edward Gornish, Sam Midkiff, David Sehr and William Jalby for their insightful comments on this paper.
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: The concept of downwardly reaching varies subtly from the classical definition of reaching <ref> [ASU86] </ref>, where data reach from definitions (writes) only. For our purposes, data can reach from either a read or write reference, since either can produce a locally available data copy. <p> Section 6 covers extensions to handle parallel programs. Our flow-analysis approach is based on that devised by Gross and Steenkiste [GS90], who use interval analysis <ref> [ASU86] </ref> to detect dependences among array data. Forward flow analysis is used to compute downwardly reaching references; backward flow analysis is used to compute upwardly reaching ones. The reasons for restricting ourselves to unconditional redundancies are twofold: first, the overhead for eliminating conditional redundancies is likely to outweigh the benefits. <p> In a structured program, two types of intervals arise: those that roughly correspond to loops, and those for which there are no backward branches to blocks within the interval [RP86, GS90]. These are termed loop intervals and non-loop intervals, respectively. By definition <ref> [ASU86, RP86] </ref>, each interval has one entry block. For structured programs, the program flow graph can be constructed such that each loop interval has, not only one entry block, but one exit block as well. <p> The analog to this in classical flow analysis would be use-definition chains <ref> [ASU86] </ref>. Alternatively, we can compute the analog of definition-use chains, namely the set of redundancy inducers and, for each redundancy inducer, the set of redundancies that it induces. Consider the problem of identifying read redundancy inducers.
Reference: [Bal90] <author> Vasanth Balasundaram. </author> <title> A Mechanism for Keeping Useful Internal Information in Parallel Programming Tools: The Data Access Descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9(2) </volume> <pages> 154-170, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Last, the shape must be represented as a function of the indices of enclosing loops, so that the reference instance corresponding to a particular iteration can be represented. It is straightforward to adapt existing shape representation methods <ref> [Bal90, HK90, Sch89] </ref> and exact dependence analysis techniques [HHL90, KK91, Psa91, MHL91, ETW92] to meet these requirements. Hence, one of 9 UREF (b): Set of data that unconditionally downwardly reach the end of block b from references within b, and the references from which these data reach. <p> (summary blk) contains all data conditionally referenced within the loop: CREF w (summary blk) = ] 1llmax COUT w l (exit blk): The union operators in the above equations are inscribed with a "+" to indicate that these unions ] i min ii max f (i), are implemented by translation <ref> [Bal90] </ref>, whereby the range of the union iteration space "i min : i max " is substituted for the union iteration variable "i" in function f .
Reference: [CD88] <author> Chi-Hung Chi and Hank Dietz. </author> <title> Improving Cache Performance by Selective Cache Bypass. </title> <type> Technical Report TR-EE 88-36, </type> <institution> School of Electrical Engineering, Purdue University, </institution> <month> July </month> <year> 1988. </year>
Reference-contexts: As a final example, redundancy information can be used to affect cache block replacement policies <ref> [CD88, GV91a] </ref>. More specifically, based on this information, the compiler can dynamically assign priorities to global data based on the likelihood of reuse. References that induce redundant reads should be assigned high priority, provided that temporal locality is high.
Reference: [CF78] <author> L. M. Censier and P. Feautrier. </author> <title> A New Solution to Coherence Problems in Multicache Systems. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1112-1118, </pages> <month> November </month> <year> 1978. </year>
Reference-contexts: By combining flow and dependence analysis, the techniques presented in this paper provide more detailed information on stale data than was previously available [CV88]. Hardware-based directory schemes, such as <ref> [Tan76, CF78, AB85, Smi85] </ref>, provide an alternative to software-based schemes. The success of these hardware-based schemes hinges on being able to minimize global communication overhead. Hence, their performance is improved when redundancies are eliminated, as this reduces communication overhead as well.
Reference: [CKM88] <author> Ron Cytron, Steve Karlovsky, and Kevin P. McAuliffe. </author> <title> Automatic Management of Programmable Caches Using Flow Analysis. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 229-238, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: With compiler-directed strategies, for example <ref> [Vei86, CKM88, MB89, CV90, DMCK92] </ref>, the cost of the technique is inversely proportional to the precision of the information available at compile time. By combining flow and dependence analysis, the techniques presented in this paper provide more detailed information on stale data than was previously available [CV88].
Reference: [CKP91] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <title> Software Prefetching. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference: [CP90] <author> David Callahan and Allan Porterfield. </author> <title> Data Cache Performance of Supercomputer Applications. </title> <booktitle> In Supercomputing '90, </booktitle> <pages> pages 564-572, </pages> <year> 1990. </year>
Reference: [CV88] <author> Hoichi Cheong and Alexander V. Veidenbaum. </author> <title> Stale Data Detection and Coherence Enforcement Using Flow Analysis. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 138-145, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: By combining flow and dependence analysis, the techniques presented in this paper provide more detailed information on stale data than was previously available <ref> [CV88] </ref>. Hardware-based directory schemes, such as [Tan76, CF78, AB85, Smi85], provide an alternative to software-based schemes. The success of these hardware-based schemes hinges on being able to minimize global communication overhead. Hence, their performance is improved when redundancies are eliminated, as this reduces communication overhead as well.
Reference: [CV90] <author> Hoichi Cheong and Alexander V. Veidenbaum. </author> <title> Compiler-Directed Cache Management in Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 39-47, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: With compiler-directed strategies, for example <ref> [Vei86, CKM88, MB89, CV90, DMCK92] </ref>, the cost of the technique is inversely proportional to the precision of the information available at compile time. By combining flow and dependence analysis, the techniques presented in this paper provide more detailed information on stale data than was previously available [CV88].
Reference: [DMCK92] <author> Ervan Darnell, John M. Mellor-Crummey, and Ken Kennedy. </author> <title> Automatic Software Cache Coherence Through Vectorization. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 129-138, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: With compiler-directed strategies, for example <ref> [Vei86, CKM88, MB89, CV90, DMCK92] </ref>, the cost of the technique is inversely proportional to the precision of the information available at compile time. By combining flow and dependence analysis, the techniques presented in this paper provide more detailed information on stale data than was previously available [CV88].
Reference: [EJWB90] <author> Christine Eisenbeis, William Jalby, Daniel Windheiser, and Francois Bodin. </author> <title> A Strategy for Array Management in Local Memory. </title> <booktitle> In Workshop on Parallel on Programming Languages and Compilers for Parallel Programming, </booktitle> <year> 1990. </year>
Reference-contexts: Aligning accesses across several doall loops can also increase the number of redundancies to expose. Note that, even when such opportunities are detected, insufficient temporal locality may prevent us from capitalizing on them. Hence, redundancy analysis is most beneficial when combined with locality-enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>. 30 Acknowledgements We would like to thank Milind Girkar, Edward Gornish, Sam Midkiff, David Sehr and William Jalby for their insightful comments on this paper.
Reference: [ETW92] <author> C. Eisenbeis, O. Temam, and H. Wijshoff. </author> <title> On Efficiently Characterizing the Solutions of Linear Diophantine Equations and Its Application to Data Dependence Analysis. </title> <type> Technical report, </type> <institution> Department of Computer Science, </institution> <type> Technical Report RUU-CS-92-01, </type> <institution> Utrecht University, </institution> <month> January </month> <year> 1992. </year> <month> 31 </month>
Reference-contexts: Last, the shape must be represented as a function of the indices of enclosing loops, so that the reference instance corresponding to a particular iteration can be represented. It is straightforward to adapt existing shape representation methods [Bal90, HK90, Sch89] and exact dependence analysis techniques <ref> [HHL90, KK91, Psa91, MHL91, ETW92] </ref> to meet these requirements. Hence, one of 9 UREF (b): Set of data that unconditionally downwardly reach the end of block b from references within b, and the references from which these data reach.
Reference: [FST91] <author> Jeanne Ferrante, Vivek Sarkar, and Wendy Thrash. </author> <title> On Estimating and Enhancing Cache Effectiveness (Extended Abstract). </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Aligning accesses across several doall loops can also increase the number of redundancies to expose. Note that, even when such opportunities are detected, insufficient temporal locality may prevent us from capitalizing on them. Hence, redundancy analysis is most beneficial when combined with locality-enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>. 30 Acknowledgements We would like to thank Milind Girkar, Edward Gornish, Sam Midkiff, David Sehr and William Jalby for their insightful comments on this paper.
Reference: [GJG88] <author> Dennis Gannon, William Jalby, and Kyle Gallivan. </author> <title> Strategies for Cache and Local Memory Management. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 587-616, </pages> <month> October </month> <year> 1988. </year>
Reference: [GS90] <author> Thomas Gross and Peter Steenkiste. </author> <title> Structured Dataflow Analysis for Arrays and Its Use in an Optimizing Compiler. </title> <journal> Software Practice& Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The only exceptions known to the authors are the methods proposed by Gross and Steenkiste <ref> [GS90] </ref> and Rosene [Ros90] where combined flow and dependence analysis is applied to sequential programs. In the remainder of this paper, these ideas are extended to detect redundancies in both sequential and parallel programs. 5 Detecting Redundancies and Their Inducers Redundancy identification proceeds in two steps. <p> Section 6 covers extensions to handle parallel programs. Our flow-analysis approach is based on that devised by Gross and Steenkiste <ref> [GS90] </ref>, who use interval analysis [ASU86] to detect dependences among array data. Forward flow analysis is used to compute downwardly reaching references; backward flow analysis is used to compute upwardly reaching ones. <p> In a structured program, two types of intervals arise: those that roughly correspond to loops, and those for which there are no backward branches to blocks within the interval <ref> [RP86, GS90] </ref>. These are termed loop intervals and non-loop intervals, respectively. By definition [ASU86, RP86], each interval has one entry block. For structured programs, the program flow graph can be constructed such that each loop interval has, not only one entry block, but one exit block as well.
Reference: [GV91a] <author> Elana D. Granston and Alexander V. Veidenbaum. </author> <title> An Integrated Hardware/Software Solution for Effective Management of Local Storage in High-Performance Systems. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 83-90, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: As a final example, redundancy information can be used to affect cache block replacement policies <ref> [CD88, GV91a] </ref>. More specifically, based on this information, the compiler can dynamically assign priorities to global data based on the likelihood of reuse. References that induce redundant reads should be assigned high priority, provided that temporal locality is high.
Reference: [GV91b] <author> Elana D. Granston and Alexander V. Veidenbaum. </author> <title> Detecting Redundant Accesses to Array Data. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 854-865, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: In <ref> [GV91b] </ref>, we introduced the notion of redundancy analysis and presented an overview of a more general compile time redundancy detection algorithm.
Reference: [HHL90] <author> Lorenz Huelsbergen, Douglas Hahn, and James Larus. </author> <title> Exact Data Dependence Analysis Using Data Access Descriptors. </title> <type> Technical Report 945, </type> <institution> Computer Science Department, University of Wisconsin-Madison, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: Last, the shape must be represented as a function of the indices of enclosing loops, so that the reference instance corresponding to a particular iteration can be represented. It is straightforward to adapt existing shape representation methods [Bal90, HK90, Sch89] and exact dependence analysis techniques <ref> [HHL90, KK91, Psa91, MHL91, ETW92] </ref> to meet these requirements. Hence, one of 9 UREF (b): Set of data that unconditionally downwardly reach the end of block b from references within b, and the references from which these data reach.
Reference: [HK90] <author> Paul Havlak and Ken Kennedy. </author> <title> Experience with Interprocedural Analysis of Array Side Effects. </title> <booktitle> In Supercomputing '90, </booktitle> <pages> pages 952-961, </pages> <year> 1990. </year>
Reference-contexts: Last, the shape must be represented as a function of the indices of enclosing loops, so that the reference instance corresponding to a particular iteration can be represented. It is straightforward to adapt existing shape representation methods <ref> [Bal90, HK90, Sch89] </ref> and exact dependence analysis techniques [HHL90, KK91, Psa91, MHL91, ETW92] to meet these requirements. Hence, one of 9 UREF (b): Set of data that unconditionally downwardly reach the end of block b from references within b, and the references from which these data reach.
Reference: [KK91] <author> Apostolos D. Kallis and David Klappholz. </author> <title> Reaching Definitions Analysis on Code Containing Array References. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Last, the shape must be represented as a function of the indices of enclosing loops, so that the reference instance corresponding to a particular iteration can be represented. It is straightforward to adapt existing shape representation methods [Bal90, HK90, Sch89] and exact dependence analysis techniques <ref> [HHL90, KK91, Psa91, MHL91, ETW92] </ref> to meet these requirements. Hence, one of 9 UREF (b): Set of data that unconditionally downwardly reach the end of block b from references within b, and the references from which these data reach.
Reference: [LRW91] <author> Monica Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The Cache Performance of Blocked Algorithms. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference: [MAL92] <author> Dror E. Maydan, Saman P. Amarasinghe, and Monica S. Lam. </author> <title> Data Dependence and Data-Flow Analysis of Arrays. </title> <booktitle> In 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 283-292, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: While techniques exist for determining the set of inducers within a given loop nest that is free of conditionally executed code <ref> [MAL92, Win92] </ref>, the methods presented in this paper will be able to detect inducers at a global level. Alternatively, compile time redundancy information can be used to assist the cache controller in determining whether to use a write-through or write-back approach on an access-by-access basis. <p> In general, dependence analysis techniques rely on pairwise comparison of references. Aside from ensuring that the source of a dependence is executed before its sink, control flow issues are ignored. The analysis technique proposed by Maydan et. al. <ref> [MAL92] </ref> considers a slightly larger set of control flow issues that arise from looping constructs, but is still limited to pairwise comparisons and does not consider conditionally executed statements.
Reference: [MB89] <author> Sang Lyul Min and Jean-Loup Baer. </author> <title> A Timestamp-based Cache Coherence Scheme. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 23-32, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: With compiler-directed strategies, for example <ref> [Vei86, CKM88, MB89, CV90, DMCK92] </ref>, the cost of the technique is inversely proportional to the precision of the information available at compile time. By combining flow and dependence analysis, the techniques presented in this paper provide more detailed information on stale data than was previously available [CV88].
Reference: [McK92] <author> Kathryn S. McKinley. </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <type> Technical Report CRPC-TR92214, </type> <month> April </month> <year> 1992. </year>
Reference: [MHL91] <author> Dror E. Maydan, John L. Hennessy, and Monica S. Lam. </author> <title> Efficient and Exact Dependence Analysis. </title> <booktitle> In ACM SIGPLAN '91 Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 1-14, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Last, the shape must be represented as a function of the indices of enclosing loops, so that the reference instance corresponding to a particular iteration can be represented. It is straightforward to adapt existing shape representation methods [Bal90, HK90, Sch89] and exact dependence analysis techniques <ref> [HHL90, KK91, Psa91, MHL91, ETW92] </ref> to meet these requirements. Hence, one of 9 UREF (b): Set of data that unconditionally downwardly reach the end of block b from references within b, and the references from which these data reach.
Reference: [MR79] <author> E. Morel and C. </author> <title> Renvoise. Global Optimization by Suppression of Partial Redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: Second, the time to perform other global memory accesses may be reduced as a result of the decreased traffic level and, hence, contention. Redundant references to scalar data can be detected and eliminated relatively easily with the use of well-established techniques for eliminating redundant computations <ref> [MR79, RWZ88] </ref>. 1 However, in numerical applications, a significant number of these redundancies may arise from array region accesses. <p> Both reads and writes can induce read redundancies, but only writes can induce write redundancies. More formally, 2 The term partially redundant is used differently from its use in <ref> [MR79] </ref>. 6 * A global read or write reference R fully induces read redundancies if every x referenced at R unconditionally upwardly reaches R from one or more read references, * A global read or write reference R partially induces read redundancies if there exists an x referenced at R that
Reference: [Por89] <author> Allan K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, Technical Report Rice COMP TR89-93, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Aligning accesses across several doall loops can also increase the number of redundancies to expose. Note that, even when such opportunities are detected, insufficient temporal locality may prevent us from capitalizing on them. Hence, redundancy analysis is most beneficial when combined with locality-enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>. 30 Acknowledgements We would like to thank Milind Girkar, Edward Gornish, Sam Midkiff, David Sehr and William Jalby for their insightful comments on this paper.
Reference: [Psa91] <author> Kleanthis Psarris. </author> <title> On Exact Dependence Analysis. </title> <type> Technical Report TR-19916, </type> <institution> Computer Science Department, Ohio University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: Last, the shape must be represented as a function of the indices of enclosing loops, so that the reference instance corresponding to a particular iteration can be represented. It is straightforward to adapt existing shape representation methods [Bal90, HK90, Sch89] and exact dependence analysis techniques <ref> [HHL90, KK91, Psa91, MHL91, ETW92] </ref> to meet these requirements. Hence, one of 9 UREF (b): Set of data that unconditionally downwardly reach the end of block b from references within b, and the references from which these data reach.
Reference: [Ros90] <author> Carl M. Rosene. </author> <title> Incremental Dependence Analysis. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <note> Technical Report COMP TR90-112, </note> <month> March </month> <year> 1990. </year> <month> 32 </month>
Reference-contexts: The only exceptions known to the authors are the methods proposed by Gross and Steenkiste [GS90] and Rosene <ref> [Ros90] </ref> where combined flow and dependence analysis is applied to sequential programs. In the remainder of this paper, these ideas are extended to detect redundancies in both sequential and parallel programs. 5 Detecting Redundancies and Their Inducers Redundancy identification proceeds in two steps.
Reference: [RP86] <author> Barbara G. Ryder and Marvin C. Paull. </author> <title> Elimination Algorithms for Data Flow Analysis. </title> <journal> Computing Surveys, </journal> <volume> 18(3) </volume> <pages> 277-316, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: In a structured program, two types of intervals arise: those that roughly correspond to loops, and those for which there are no backward branches to blocks within the interval <ref> [RP86, GS90] </ref>. These are termed loop intervals and non-loop intervals, respectively. By definition [ASU86, RP86], each interval has one entry block. For structured programs, the program flow graph can be constructed such that each loop interval has, not only one entry block, but one exit block as well. <p> In a structured program, two types of intervals arise: those that roughly correspond to loops, and those for which there are no backward branches to blocks within the interval [RP86, GS90]. These are termed loop intervals and non-loop intervals, respectively. By definition <ref> [ASU86, RP86] </ref>, each interval has one entry block. For structured programs, the program flow graph can be constructed such that each loop interval has, not only one entry block, but one exit block as well. <p> Assume that the upper bounds on the nesting depth, the number of array dimensions, and the number of summary shapes permitted per data descriptor are small constants. Then, the total number of basic and summary blocks is O (S) <ref> [RP86] </ref>. Consequently, the number of steps is also bounded by O (S). At worst, the time to perform a single set operation is O (S V). Because there are only a finite number of set operations per step, the time cost per step is subject to the same upper bound.
Reference: [RWZ88] <author> Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Global Value Numbers and Redundant Computations. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 12-27. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1988. </year>
Reference-contexts: Second, the time to perform other global memory accesses may be reduced as a result of the decreased traffic level and, hence, contention. Redundant references to scalar data can be detected and eliminated relatively easily with the use of well-established techniques for eliminating redundant computations <ref> [MR79, RWZ88] </ref>. 1 However, in numerical applications, a significant number of these redundancies may arise from array region accesses. <p> Depending on how the redundancy information is used, it may be more useful to compute redundancy inducers, namely the references that cause others to be redundant. Section 9 addresses algorithm adaptations for accomplishing this. Section 10 discusses 1 The method proposed in <ref> [RWZ88] </ref> also handles individual array elements when subscript expressions are equivalent. 2 R 0 : A [1:N:2] = R 1 : then A [1:N] = R 2 : else A [1:N:2] = endif Before eliminating redundancy. (a) if R 1 : then A [1:N] = R 2 : else A [1:N:2]
Reference: [Sch89] <author> Dale Schouten. </author> <title> An Overview of Interprocedural Analysis Techniques for High Performance Paralleliz-ing Compilers. </title> <type> Master's thesis, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Last, the shape must be represented as a function of the indices of enclosing loops, so that the reference instance corresponding to a particular iteration can be represented. It is straightforward to adapt existing shape representation methods <ref> [Bal90, HK90, Sch89] </ref> and exact dependence analysis techniques [HHL90, KK91, Psa91, MHL91, ETW92] to meet these requirements. Hence, one of 9 UREF (b): Set of data that unconditionally downwardly reach the end of block b from references within b, and the references from which these data reach.
Reference: [Smi85] <author> A. J. Smith. </author> <title> CPU Cache Consistency with Software Support and Using "One Time Identifiers". </title> <booktitle> In Proceedings of the Pacific Computer Communications Symposium, </booktitle> <pages> pages 22-24, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: By combining flow and dependence analysis, the techniques presented in this paper provide more detailed information on stale data than was previously available [CV88]. Hardware-based directory schemes, such as <ref> [Tan76, CF78, AB85, Smi85] </ref>, provide an alternative to software-based schemes. The success of these hardware-based schemes hinges on being able to minimize global communication overhead. Hence, their performance is improved when redundancies are eliminated, as this reduces communication overhead as well.
Reference: [Tan76] <author> C. K. Tang. </author> <title> Cache System Design in Tightly Coupled Multiprocessors. </title> <booktitle> In AFIPS Conference Proceedings of the National Computer Conference, </booktitle> <pages> pages 749-753, </pages> <year> 1976. </year>
Reference-contexts: By combining flow and dependence analysis, the techniques presented in this paper provide more detailed information on stale data than was previously available [CV88]. Hardware-based directory schemes, such as <ref> [Tan76, CF78, AB85, Smi85] </ref>, provide an alternative to software-based schemes. The success of these hardware-based schemes hinges on being able to minimize global communication overhead. Hence, their performance is improved when redundancies are eliminated, as this reduces communication overhead as well.
Reference: [Vei86] <author> Alexander V. Veidenbaum. </author> <title> A Compiler-assisted Cache Coherence Solution for Multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 1029-1036, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: With compiler-directed strategies, for example <ref> [Vei86, CKM88, MB89, CV90, DMCK92] </ref>, the cost of the technique is inversely proportional to the precision of the information available at compile time. By combining flow and dependence analysis, the techniques presented in this paper provide more detailed information on stale data than was previously available [CV88].
Reference: [Win92] <author> Daniel Windheiser. </author> <title> Data Locality Optimization. </title> <type> PhD thesis, </type> <institution> IRISA INRIA-Rennes, </institution> <year> 1992. </year>
Reference-contexts: For some of these cases, if we can detect the potential set of redundancy inducers, we can use existing software strategies for managing local storage and eliminate redundancies more efficiently <ref> [Win92] </ref>. While techniques exist for determining the set of inducers within a given loop nest that is free of conditionally executed code [MAL92, Win92], the methods presented in this paper will be able to detect inducers at a global level. <p> While techniques exist for determining the set of inducers within a given loop nest that is free of conditionally executed code <ref> [MAL92, Win92] </ref>, the methods presented in this paper will be able to detect inducers at a global level. Alternatively, compile time redundancy information can be used to assist the cache controller in determining whether to use a write-through or write-back approach on an access-by-access basis.
Reference: [WL91] <author> Michael Wolf and Monica Lam. </author> <title> A Data Locality Optimizing Algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 26(6), </volume> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Aligning accesses across several doall loops can also increase the number of redundancies to expose. Note that, even when such opportunities are detected, insufficient temporal locality may prevent us from capitalizing on them. Hence, redundancy analysis is most beneficial when combined with locality-enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>. 30 Acknowledgements We would like to thank Milind Girkar, Edward Gornish, Sam Midkiff, David Sehr and William Jalby for their insightful comments on this paper.
Reference: [Wol87] <author> Michael Joseph Wolfe. </author> <title> Iteration Space Tiling for Memory Hierarchies. </title> <booktitle> In Proceedings of the 3rd SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 357-361, </pages> <year> 1987. </year>
Reference-contexts: Aligning accesses across several doall loops can also increase the number of redundancies to expose. Note that, even when such opportunities are detected, insufficient temporal locality may prevent us from capitalizing on them. Hence, redundancy analysis is most beneficial when combined with locality-enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>. 30 Acknowledgements We would like to thank Milind Girkar, Edward Gornish, Sam Midkiff, David Sehr and William Jalby for their insightful comments on this paper.
Reference: [Wol89] <author> Michael Joseph Wolfe. </author> <title> More Iteration Space Tiling. </title> <booktitle> In Supercomputing '89, </booktitle> <year> 1989. </year> <month> 33 </month>
Reference-contexts: Aligning accesses across several doall loops can also increase the number of redundancies to expose. Note that, even when such opportunities are detected, insufficient temporal locality may prevent us from capitalizing on them. Hence, redundancy analysis is most beneficial when combined with locality-enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>. 30 Acknowledgements We would like to thank Milind Girkar, Edward Gornish, Sam Midkiff, David Sehr and William Jalby for their insightful comments on this paper.
References-found: 41

