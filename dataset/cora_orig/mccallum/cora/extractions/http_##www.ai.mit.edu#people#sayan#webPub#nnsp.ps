URL: http://www.ai.mit.edu/people/sayan/webPub/nnsp.ps
Refering-URL: http://www.ai.mit.edu/people/sayan/pub.html
Root-URL: 
Email: sayan@ai.mit.edu eosuna@ai.mit.edu girosi@ai.mit.edu  
Title: Nonlinear Prediction of Chaotic Time Series Using Support Vector Machines  
Author: Sayan Mukherjee Edgar Osuna Federico Girosi 
Affiliation: Center for Biological and Computational Learning E25-201 Massachusetts Institute of Technology  
Date: 24-26 Sep., 1997)  
Address: Island, FL,  Cambridge, MA 02139  
Note: (to appear in the Proc. of IEEE NNSP'97, Amelia  
Abstract: A novel method for regression has been recently proposed by V. Vapnik et al. [8, 9]. The technique, called Support Vector Machine (SVM), is very well founded from the mathematical point of view and seems to provide a new insight in function approximation. We implemented the SVM and tested it on the same data base of chaotic time series that was used in [1] to compare the performances of different approximation techniques, including polynomial and rational approximation, local polynomial techniques, Radial Basis Functions, and Neural Networks. The SVM performs better than the approaches presented in [1]. We also study, for a particular time series, the variability in performance with respect to the few free parameters of SVM.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Casdagli. </author> <title> Nonlinear prediction of chaotic time-series. </title> <journal> Physica D, </journal> <volume> 35 </volume> <pages> 335-356, </pages> <year> 1989. </year>
Reference-contexts: Our results show that SVM is a very promising regression technique, but in order to assess its reliability and performances more extensive experimentation will need to be done in the future. We begin by applying SVM to several chaotic time series data sets that were used by Casdagli <ref> [1] </ref> to test and compare the performances of different approximation techniques. The SVM is a technique with few free parameters. In absence of a principled way to choose these parameters we performed an experimental study to examine the variability in performance as some of these parameters vary between reasonable limits. <p> In the next section we formulate the problem of time series prediction and see how it is equivalent to a regression problem. In section 3 we briefly review the SVM approach to the regression problem. In section 4, the chaotic time series used to benchmark previous regression methods <ref> [1] </ref> are introduced. Section 5 contains the experimental results and the comparison with the techniques presented in [1]. <p> In section 3 we briefly review the SVM approach to the regression problem. In section 4, the chaotic time series used to benchmark previous regression methods <ref> [1] </ref> are introduced. Section 5 contains the experimental results and the comparison with the techniques presented in [1]. <p> A clear theoretical understanding is still missing and we plan to conduct experimental work to understand their role. 4 Benchmark Time Series We tested the SVM regression technique on the same set of chaotic time series that has been used in <ref> [1] </ref> to test and compare several approximation techniques. 4.1 The Mackey-Glass time series We considered two time series generated by the Mackey-Glass delay-differential equation [4]: dx (t) = 0:1x (t) + 1 + x (t ) 10 ; (7) with parameters = 17; 30 and embedding dimensions m = 4; 6 <p> We denote these two time-series by M G 17 and M G 30 . In order to be con sistent with <ref> [1] </ref> the initial condition for the above equation was x (t) = 0:9 for 0 t , and the sampling rate t = 6. <p> In <ref> [1] </ref> Casdagli considered both this time series, that we will denote by Ikeda 1 , and the one generated by the fourth iterate of this map, which has a more complicated dynamic, and that will be denoted by Ikeda 4 . 4.3 The Lorenz time series We also considered the time <p> The series were generated by numerical integration using a fourth order Runge-Kutta method. 5 Comparison with Other Techniques In this section we report the results of the SVM on the time series presented above, and compare them with the results reported in <ref> [1] </ref> about different approximation techniques (polynomial, rational, local polynomial, Radial Basis Functions with multiquadrics as basis function, and Neural Networks). <p> In all cases N was set to 500, except for the Ikeda 4 , for which N = 100, while M was always set to 1000. The data sets we used were the same that were used in <ref> [1] </ref>. Following [1], denoting by ~ f N the predictor built using N data points, the following quantity was used as a measure of the generalization error of ~ f N : 1 M X (x (nt ) ~ f N ( ~ x n1 )) 2 (10) where Var is <p> In all cases N was set to 500, except for the Ikeda 4 , for which N = 100, while M was always set to 1000. The data sets we used were the same that were used in <ref> [1] </ref>. Following [1], denoting by ~ f N the predictor built using N data points, the following quantity was used as a measure of the generalization error of ~ f N : 1 M X (x (nt ) ~ f N ( ~ x n1 )) 2 (10) where Var is the variance <p> Details of our implementation can be found in [6]. For each series we choose the kernel, K, and parameters of the kernel that gave us the smallest generalization error. This is consistent with the strategy adopted in <ref> [1] </ref>. The results are reported in table (1). The last column of the table contains the results of our experiments, while the rest of the table is from [1] with param-eters and kernels set as in the remaining part of this section. <p> This is consistent with the strategy adopted in <ref> [1] </ref>. The results are reported in table (1). The last column of the table contains the results of our experiments, while the rest of the table is from [1] with param-eters and kernels set as in the remaining part of this section. <p> -6.21 (374) Ikeda 4 -1.05 (14) -1.39 (14) -1.26 -1.60 -2.10 - -2.31 (427) Lor 0:05 -4.62 (6) -4.30 (3) -2.00 -3.48 -3.54 - -4.76 (389) Table 1: Estimated values of log 10 ( ~ f n ) for the SVM algorithm and for various regression algorithms, as reported in <ref> [1] </ref>. The degrees used for the best rational and polynomial regressors are in superscripts beside the estimates. Loc d=1 and Loc d=2 refer to local approximation with polynomials of degree 1 and 2 respectively. <p> Loc d=1 and Loc d=2 refer to local approximation with polynomials of degree 1 and 2 respectively. The numbers in parenthesis near the SVM estimates are the number of support vectors obtained by the algorithm. The Neural Networks results which are missing were also missing in <ref> [1] </ref>. 6 Sensitivity of SVM to Parameters and Em- bedding Dimension In this section we report our observations on how the generalization error and the number of support vectors vary with respect to the free parameters of the SVM and to the choice of the embedding dimension.
Reference: [2] <author> K. Ikeda. </author> <title> Multiple-valued stationary state and its instability of light by a ring cavity system. </title> <journal> Opt. Commun., </journal> <volume> 30:257, </volume> <year> 1979. </year>
Reference-contexts: The series were generated by numerical integration using a fourth order Runge-Kutta method. 4.2 The Ikeda map The Ikeda map <ref> [2] </ref> is a two dimensional time series which is generated iterating the following map: f (x 1 ; x 2 ) = (1 + (x 1 cos ! x 2 sin !); (x 1 sin ! + x 2 cos !)); (8) where ! = 0:4 6:0=(1 + x 2 1
Reference: [3] <author> E.N. Lorenz. </author> <title> Deterministic non-periodic flow. </title> <institution> J. Atoms. Sci., 26:636, </institution> <year> 1969. </year>
Reference-contexts: Ikeda 1 , and the one generated by the fourth iterate of this map, which has a more complicated dynamic, and that will be denoted by Ikeda 4 . 4.3 The Lorenz time series We also considered the time series associated to the variable x of the Lorenz differential equation <ref> [3] </ref>: _x = (y x); _y = rx y xz; _z = xy bz (9) where = 10, b = 8 3 , and r = 28.
Reference: [4] <author> M.C. Mackey and J. Glass. </author> <title> Oscillation and chaos in physiological contol systems. </title> <booktitle> Science, </booktitle> <address> 197:287, </address> <year> 1977. </year>
Reference-contexts: understand their role. 4 Benchmark Time Series We tested the SVM regression technique on the same set of chaotic time series that has been used in [1] to test and compare several approximation techniques. 4.1 The Mackey-Glass time series We considered two time series generated by the Mackey-Glass delay-differential equation <ref> [4] </ref>: dx (t) = 0:1x (t) + 1 + x (t ) 10 ; (7) with parameters = 17; 30 and embedding dimensions m = 4; 6 respectively. We denote these two time-series by M G 17 and M G 30 .
Reference: [5] <author> B.A. Murtagh and M. Saunders. </author> <title> Large-scale linearly constrained optimization. </title> <journal> Mathematical Programming, </journal> <volume> 14 </volume> <pages> 41-72, </pages> <year> 1978. </year>
Reference-contexts: We implemented the SVM using MINOS 5.4 <ref> [5] </ref> as the solver for the Quadratic Programming problem of eq. (6). Details of our implementation can be found in [6]. For each series we choose the kernel, K, and parameters of the kernel that gave us the smallest generalization error. This is consistent with the strategy adopted in [1].
Reference: [6] <author> E. Osuna, R. Freund, and F. Girosi. </author> <title> Support vector machines: Training and applications. A.I. </title> <type> Memo 1602, </type> <institution> MIT A. I. Lab., </institution> <year> 1997. </year> <title> (a) (b) space. Note that there is no overfitting (b) The number of support vectors versus * for the same feature space. line is for a 802 dimensional feature space and the dashed line is for a 10 dimensional feature space. </title>
Reference-contexts: We implemented the SVM using MINOS 5.4 [5] as the solver for the Quadratic Programming problem of eq. (6). Details of our implementation can be found in <ref> [6] </ref>. For each series we choose the kernel, K, and parameters of the kernel that gave us the smallest generalization error. This is consistent with the strategy adopted in [1]. The results are reported in table (1).
Reference: [7] <author> F. Takens. </author> <title> Detecting strange attractors in fluid turbulence. </title> <editor> In D. Rand and L.S. Young, editors, </editor> <title> Dynamical Systems and Turbulence. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: Our goal is to be able to predict the future behavior of the time series x (t). Remarkably, this can be done, at least in principle, without knowledge of the other components of the vector x (t). In fact, Tak-ens embedding theorem <ref> [7] </ref> ensures that, under certain conditions, for almost all t and for some m 2D + 1 there is a smooth map f : R m ! R such that: x (nt ) = f (x ((n 1)t ); x ((n 2)t ); : : : ; x ((n m)t ))
Reference: [8] <author> V. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction In this paper we analyze the performance of a new regression technique called a Support Vector Machine <ref> [8, 9] </ref>. This technique can be seen as a new way to train polynomial, neural network, or Radial Basis Functions regressors. The main difference between this technique and many conventional regression techniques is that it uses the Structural Risk Minimization and not the Empirical Risk Minimization induction principle. <p> Vapnik et al. [9]. 3 Support Vectors Machines for Regression In this section we sketch the ideas behind the Support Vectors Machines (SVM) for regression, a more detailed description can be found in [9] and <ref> [8] </ref>. <p> is a constant and the following robust error function has been defined: j y i f (x i ; c) j * = &lt; 0 if j y i f (x i ; c) j&lt; * j y i f (x i ; c) j otherwise: (4) Vapnik showed in <ref> [8] </ref> that the function that minimizes the functional in eq. (3) depends on a finite number of parameters, and has the following form: f (x; ff; ff fl ) = i=1 i ff i )K (x; x i ) + b; (5) where ff fl i ff i = 0; ff
Reference: [9] <author> V. Vapnik, S.E. Golowich, and A. Smola. </author> <title> Support vector method for function approximation, regression estimation, </title> <booktitle> and signal processing. In Advances in Neural information processings systems 8, </booktitle> <address> San Mateo, CA, 1996. </address> <publisher> Morgan Kaufmann Publishers. (in press). </publisher>
Reference-contexts: 1 Introduction In this paper we analyze the performance of a new regression technique called a Support Vector Machine <ref> [8, 9] </ref>. This technique can be seen as a new way to train polynomial, neural network, or Radial Basis Functions regressors. The main difference between this technique and many conventional regression techniques is that it uses the Structural Risk Minimization and not the Empirical Risk Minimization induction principle. <p> Many regression techniques can be used to solve problems of this type. In this paper we concentrate on the Support Vector algorithm, a novel regression technique developed by V. Vapnik et al. <ref> [9] </ref>. 3 Support Vectors Machines for Regression In this section we sketch the ideas behind the Support Vectors Machines (SVM) for regression, a more detailed description can be found in [9] and [8]. <p> Vapnik et al. <ref> [9] </ref>. 3 Support Vectors Machines for Regression In this section we sketch the ideas behind the Support Vectors Machines (SVM) for regression, a more detailed description can be found in [9] and [8].
Reference: [10] <author> V. N. Vapnik. </author> <title> Statistical learning theory. </title> <editor> J. </editor> <publisher> Wiley, </publisher> <year> 1997. </year>
Reference-contexts: It is known that the VC-dimension satisfies h min ( R 2 (A+1) 2 * 2 ; D) + 1 <ref> [10] </ref>, where R is the radius of the smallest sphere that contains all the data points in the feature space, A is a bound on the norm of the vector of coefficients.
References-found: 10

