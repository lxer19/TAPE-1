URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/decentralized-ai-chapter.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/
Root-URL: http://www.csc.ncsu.edu
Title: In  GROUP ABILITY AND STRUCTURE  
Author: Yves DEMAZEAU and Jean-Pierre M ULLER (Eds.) Munindar P. Singh 
Address: Austin, TX 78712-1188 USA  Austin, TX 78759 USA  
Affiliation: Dept of Computer Sciences University of Texas  and Artificial Intelligence Lab MCC  
Date: 1991  
Note: Decentralized Artificial Intelligence, Volume 2 Elsevier Science Publishers B. V. (North-Holland),  
Abstract: Groups of intelligent agents are the object of study in several subareas of AI, notably, autonomous agents, multiagent planning and action, discourse understanding, and cooperative work. I present a formal theory of the ability of a group of agents that analyzes ability directly in terms of the individual skills of the group's members, and its internal structure or organization. Since this theory does not reduce "know-how," the knowledge of skills, to "know-that," the knowledge of facts, it avoids many of the problems that plague traditional theories of knowledge and action. In using the notion of "strategies" (as described in the paper), this theory presents a realistic two-layered view of action by, and interaction among, groups of agents. This theory has important ramifications in the study of the intentions and beliefs of groups of agents, and of the combination of their expertise. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Philip Agre and David Chapman. Pengi: </author> <title> An implementation of a theory of activity. </title> <booktitle> In AAAI, </booktitle> <pages> pages 268-272, </pages> <year> 1987. </year>
Reference-contexts: While this "situated" theory does not use plans, it can accommodate the richer notion of strategies (see x5). Therefore, it contrasts both with situated theories of know-that [22], and informal, and exclusively reactive, accounts of action <ref> [1] </ref>. 4 The Formal Model The formal model is based on possible worlds. 1 Each possible world has a branching history of times. At each time, environmental processes and agents' actions occur. Agents influence the future by acting, but the outcome also depends on other events. <p> [ I R ) ! meets (r; t 0 ; Y G ; tree)) ^hM; treei j= w;t K how (G; A) _ (8S : S 2 S w;t ^ (9t 0 : hS; t; t 0 i 2 [[tree]] current (Y ) )! (9t 00 ; D : D <ref> [1; . . . ; n] </ref> ^ (8i : i 2 D! hS; t; t 00 i 2 [[tree i ]] current (Y i ) ) ^ (8j; t 000 : j 2 ([1; . . . ; n] n D) ^ hS; t; t 000 i 2 [[tree j ]]
Reference: [2] <author> John L. Austin. </author> <title> How to do Things with Words. </title> <publisher> Clarendon, Oxford, </publisher> <address> UK, </address> <year> 1962. </year>
Reference-contexts: It is instructive to see what kinds of interactions among strategies are possible. Some of these abstract interactions among agents may involve queries, requests for certain (possibly abstract) actions, commands to do certain (possibly abstract) actions|in short, all kinds of illocutionary acts <ref> [2] </ref>. The key point is that all of these 10 involve the flow of abstractly specified information of some sort, be it a command or a declaration, from one member to another. Note that information need not be passed linguistically: nonlinguistic conventions, such as gestures, could serve as well.
Reference: [3] <author> Miroslav Benda, V. Jaganathan, and Rajendra Dodhiawala. </author> <title> On optimal cooperation of knowledge sources. </title> <type> Technical report, </type> <institution> Boeing Advanced Technology Center, Boeing Computer Services, </institution> <address> Seattle, WA, </address> <month> September </month> <year> 1986. </year> <month> 17 </month>
Reference-contexts: I now apply it in the context of a well-known problem in DAI: the pursuit problem. Parts of this section are borrowed from [26] which considers the same problem from the viewpoint of the agents' intentions. The pursuit problem problem was introduced by Benda et al. <ref> [3] </ref>, but has been extensively studied by others [6, 28]. We are given a finite two-dimensional grid of points (see Figure 1). Each point may be occupied by either an agent called `Red' (the "adversary") or up to four `Blue' agents.
Reference: [4] <author> Philip R. Cohen and Hector J. Levesque. </author> <title> On acting together: </title> <booktitle> Joint intentions for intelligent agents. In Workshop on Distributed Artificial Intelligence, </booktitle> <year> 1988. </year>
Reference-contexts: 1 Introduction Several interesting and important subareas of AI involve groups of intelligent agents, with varying degrees of autonomy, who share a fragment of the world and affect one another through their actions. These subareas include autonomous agents, multiagent planning and action, discourse understanding, and cooperative work <ref> [4, 5, 10, 12, 15, 16, 17] </ref>. I present a formal theory of the objective ability or know-how of a group of agents that accounts for its internal structure. <p> Since even individual agents can be treated as groups of subagents [8], this theory also applies in areas such as real-time AI systems and robotics. It also helps clarify the question of the joint intentions of a group of agents <ref> [4, 12, 24, 26] </ref>. A longer term research interest that this paper is a step towards is to formulate a logically well-founded methodology for the design of systems of intelligent agents. <p> A consequence of this approach is that this theory, unlike <ref> [4, 12] </ref>, does not require a group of agents to have mutual beliefs (or common knowledge) to cooperate. <p> Common knowledge is impossible to achieve in most realistic scenarios; e.g., where communication is not (known to be) guaranteed with only a finite delay [9, 13]. 2 Groups Traditional theories of multiagent intentions and action tend to ignore the structure of the group of agents being considered <ref> [4, 8, 12, 24] </ref>, and assume that agents are equally capable and perfectly cooperative. However, the knowledge and skills of a group depend greatly on its structure, and on the knowledge and skills of its members.
Reference: [5] <editor> Y. Demazeau and J-P. Muller, editors. </editor> <booktitle> Decentralized Artificial Intelligence, </booktitle> <address> Amster-dam, Holland, 1990. </address> <publisher> Elsevier Science Publishers B.V. / North-Holland. </publisher>
Reference-contexts: 1 Introduction Several interesting and important subareas of AI involve groups of intelligent agents, with varying degrees of autonomy, who share a fragment of the world and affect one another through their actions. These subareas include autonomous agents, multiagent planning and action, discourse understanding, and cooperative work <ref> [4, 5, 10, 12, 15, 16, 17] </ref>. I present a formal theory of the objective ability or know-how of a group of agents that accounts for its internal structure.
Reference: [6] <author> Edmund H. Durfee and Thomas A. Montgomery. </author> <title> MICE: a flexible testbed for intelligent coordination experiments. </title> <booktitle> In Proc. 9th Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 25-40, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Parts of this section are borrowed from [26] which considers the same problem from the viewpoint of the agents' intentions. The pursuit problem problem was introduced by Benda et al. [3], but has been extensively studied by others <ref> [6, 28] </ref>. We are given a finite two-dimensional grid of points (see Figure 1). Each point may be occupied by either an agent called `Red' (the "adversary") or up to four `Blue' agents.
Reference: [7] <author> E. A. Emerson. </author> <title> Temporal and modal logic. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume B. </booktitle> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1990. </year>
Reference-contexts: They will also be useful in guiding the formulation of design principles for multiagent systems. As usual, `j= p' indicates that p is valid, and `6j= p' that it is invalid. Following Emerson <ref> [7] </ref>, AG means "at all times in all futures," and P "sometimes in the past." These and other temporal operators could easily be given formal definition in the model, if that is desired. 1.
Reference: [8] <author> Ronald Fagin and Joseph Y. Halpern. </author> <title> Belief, awareness, and limited reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 34 </volume> <pages> 39-76, </pages> <year> 1988. </year>
Reference-contexts: I present a formal theory of the objective ability or know-how of a group of agents that accounts for its internal structure. Since even individual agents can be treated as groups of subagents <ref> [8] </ref>, this theory also applies in areas such as real-time AI systems and robotics. It also helps clarify the question of the joint intentions of a group of agents [4, 12, 24, 26]. <p> Common knowledge is impossible to achieve in most realistic scenarios; e.g., where communication is not (known to be) guaranteed with only a finite delay [9, 13]. 2 Groups Traditional theories of multiagent intentions and action tend to ignore the structure of the group of agents being considered <ref> [4, 8, 12, 24] </ref>, and assume that agents are equally capable and perfectly cooperative. However, the knowledge and skills of a group depend greatly on its structure, and on the knowledge and skills of its members.
Reference: [9] <author> Michael J. Fischer and Neil Immerman. </author> <title> Foundations of knowledge for distributed systems. </title> <editor> In Joseph Y. Halpern, editor, </editor> <booktitle> Theoretical Aspects of Reasoning About Knowledge, </booktitle> <pages> pages 171-185, </pages> <year> 1986. </year>
Reference-contexts: Common knowledge is impossible to achieve in most realistic scenarios; e.g., where communication is not (known to be) guaranteed with only a finite delay <ref> [9, 13] </ref>. 2 Groups Traditional theories of multiagent intentions and action tend to ignore the structure of the group of agents being considered [4, 8, 12, 24], and assume that agents are equally capable and perfectly cooperative.
Reference: [10] <editor> Les Gasser and Michael N. Huhns, editors. </editor> <booktitle> Distributed Artificial Intelligence, Volume II. </booktitle> <publisher> Pitman/Morgan Kaufmann, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Several interesting and important subareas of AI involve groups of intelligent agents, with varying degrees of autonomy, who share a fragment of the world and affect one another through their actions. These subareas include autonomous agents, multiagent planning and action, discourse understanding, and cooperative work <ref> [4, 5, 10, 12, 15, 16, 17] </ref>. I present a formal theory of the objective ability or know-how of a group of agents that accounts for its internal structure.
Reference: [11] <author> Barbara Grosz and Candace Sidner. </author> <title> Distributed know-how and acting: Research on collaborative planning. </title> <booktitle> In Workshop on Distributed Artificial Intelligence, </booktitle> <year> 1988. </year>
Reference-contexts: The theory of <ref> [11] </ref> is in the same 3 spirit, since it requires the agents in a group to have mutual beliefs about their "shared plans," and about how they may execute them. 3.2 Problems with the Traditional Reduction There are several problems with this reduction; they become even more urgent when group know-how
Reference: [12] <author> Barbara Grosz and Candace Sidner. </author> <title> Plans for discourse. </title> <editor> In P. Cohen, J. Morgan, and M. Pollack, editors, </editor> <title> SDF Benchmark Series: Intentions in Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Several interesting and important subareas of AI involve groups of intelligent agents, with varying degrees of autonomy, who share a fragment of the world and affect one another through their actions. These subareas include autonomous agents, multiagent planning and action, discourse understanding, and cooperative work <ref> [4, 5, 10, 12, 15, 16, 17] </ref>. I present a formal theory of the objective ability or know-how of a group of agents that accounts for its internal structure. <p> Since even individual agents can be treated as groups of subagents [8], this theory also applies in areas such as real-time AI systems and robotics. It also helps clarify the question of the joint intentions of a group of agents <ref> [4, 12, 24, 26] </ref>. A longer term research interest that this paper is a step towards is to formulate a logically well-founded methodology for the design of systems of intelligent agents. <p> A consequence of this approach is that this theory, unlike <ref> [4, 12] </ref>, does not require a group of agents to have mutual beliefs (or common knowledge) to cooperate. <p> Common knowledge is impossible to achieve in most realistic scenarios; e.g., where communication is not (known to be) guaranteed with only a finite delay [9, 13]. 2 Groups Traditional theories of multiagent intentions and action tend to ignore the structure of the group of agents being considered <ref> [4, 8, 12, 24] </ref>, and assume that agents are equally capable and perfectly cooperative. However, the knowledge and skills of a group depend greatly on its structure, and on the knowledge and skills of its members.
Reference: [13] <author> Joseph Y. Halpern and Yoram O. Moses. </author> <title> Knowledge and common knowledge in a distributed environment (revised version). </title> <type> Technical Report RJ 4421, </type> <institution> IBM, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: Common knowledge is impossible to achieve in most realistic scenarios; e.g., where communication is not (known to be) guaranteed with only a finite delay <ref> [9, 13] </ref>. 2 Groups Traditional theories of multiagent intentions and action tend to ignore the structure of the group of agents being considered [4, 8, 12, 24], and assume that agents are equally capable and perfectly cooperative.
Reference: [14] <editor> C. L. Hamblin. Imperatives. </editor> <publisher> Basil Blackwell Ltd., Oxford, </publisher> <address> UK, </address> <year> 1987. </year>
Reference-contexts: Two observations are in order here. (1) A group (e.g., a sports team) may be considered as a single unstructured monolithic agent from without; i.e., groups are "Hobbesian corporate persons," in Hamblin's term <ref> [14, pp. 60, 240] </ref>. The know-how of a group of agents is the same kind of thing as the know-how of individual agents: the only difference is one of extent: one would typically expect a cooperative group to have "greater" know-how than any of its proper subgroups.
Reference: [15] <author> Carl Hewitt. </author> <title> Organizational knowledge processing. </title> <booktitle> In Workshop on Distributed Artificial Intelligence, </booktitle> <year> 1988. </year>
Reference-contexts: 1 Introduction Several interesting and important subareas of AI involve groups of intelligent agents, with varying degrees of autonomy, who share a fragment of the world and affect one another through their actions. These subareas include autonomous agents, multiagent planning and action, discourse understanding, and cooperative work <ref> [4, 5, 10, 12, 15, 16, 17] </ref>. I present a formal theory of the objective ability or know-how of a group of agents that accounts for its internal structure. <p> The major shortcomings are considered below. 1. Multiagent systems: Traditional theories are based on plans, and typically consider only single agent plans: thus they cannot account for the know-how of a group of agents. However, organizations know how to do certain things, but may not have any explicit plan <ref> [15] </ref>. And, even in organizations, the actual actions are taken by individual agents, whose knowledge may not be easily combined. A reduction to what all members know would be too strong, and that to what just some members know too weak. 2.
Reference: [16] <editor> Michael N. Huhns, editor. </editor> <booktitle> Distributed Artificial Intelligence. </booktitle> <address> Pitman/Morgan Kauf-mann, London, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction Several interesting and important subareas of AI involve groups of intelligent agents, with varying degrees of autonomy, who share a fragment of the world and affect one another through their actions. These subareas include autonomous agents, multiagent planning and action, discourse understanding, and cooperative work <ref> [4, 5, 10, 12, 15, 16, 17] </ref>. I present a formal theory of the objective ability or know-how of a group of agents that accounts for its internal structure.
Reference: [17] <author> Kurt G. Konolige. </author> <title> A first-order formalism of knowledge and action for multi-agent planning. </title> <editor> In J. E. Hayes, D. Mitchie, and Y. Pao, editors, </editor> <booktitle> Machine Intelligence 10, </booktitle> <pages> pages 41-73. </pages> <publisher> Ellis Horwood Ltd., </publisher> <address> Chichester, UK, </address> <year> 1982. </year>
Reference-contexts: 1 Introduction Several interesting and important subareas of AI involve groups of intelligent agents, with varying degrees of autonomy, who share a fragment of the world and affect one another through their actions. These subareas include autonomous agents, multiagent planning and action, discourse understanding, and cooperative work <ref> [4, 5, 10, 12, 15, 16, 17] </ref>. I present a formal theory of the objective ability or know-how of a group of agents that accounts for its internal structure. <p> A longer term research interest that this paper is a step towards is to formulate a logically well-founded methodology for the design of systems of intelligent agents. Traditional formal theories of knowledge and action <ref> [17, 20, 21] </ref> stress the conception of knowledge corresponding to know-that, or the knowledge of facts, rather than know-how, the knowledge of skills. It is assumed that know-how can be reduced to know-that.
Reference: [18] <author> Dexter Kozen and Jerzy Tiurzyn. </author> <title> Logics of program. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science. </booktitle> <publisher> North-Holland Publishing Company, </publisher> <address> Amster-dam, The Netherlands, </address> <year> 1990. </year> <month> 18 </month>
Reference-contexts: It can easily be seen that relative to the standard semantics for the constructs introduced above (e.g., see <ref> [18] </ref>), `Y ' is equivalent to `current (Y ); rest (Y ).' Some other obvious, but useful consequences of these definitions are: Lemma 1: `current (Y )' is always of the form `skip' or `do (A)' or `wait (A)' Lemma 2: `current (current (Y 1 ; Y 2 ))' = `current
Reference: [19] <author> Drew McDermott. </author> <title> A temporal logic for reasoning about processes and plans. </title> <journal> Cog--nitive Science, </journal> <volume> 6(2) </volume> <pages> 101-155, </pages> <year> 1982. </year>
Reference-contexts: S, relative to some SS, is the minimal set that satisfies the following conditions: Eternity: SS S Linear Closure: (8t 00 : t 00 2 S! (8t 0 : t 0 &lt; t 0 &lt; t 00 ! t 0 2 S)) 1 This formal model resembles those of McDermott <ref> [19] </ref> and Shoham [25]. However, those accounts emphasize a language that might be used for representing and reasoning about temporal information. The stress here is on giving the formal definition for know-how.
Reference: [20] <author> Robert C. Moore. </author> <title> A formal theory of knowledge and action. </title> <editor> In Jerry R. Hobbs and Robert C. Moore, editors, </editor> <booktitle> Formal Theories of the Commonsense World, </booktitle> <pages> pages 319-358. </pages> <publisher> Ablex Publishing Company, </publisher> <address> Norwood, NJ, </address> <year> 1984. </year>
Reference-contexts: A longer term research interest that this paper is a step towards is to formulate a logically well-founded methodology for the design of systems of intelligent agents. Traditional formal theories of knowledge and action <ref> [17, 20, 21] </ref> stress the conception of knowledge corresponding to know-that, or the knowledge of facts, rather than know-how, the knowledge of skills. It is assumed that know-how can be reduced to know-that. <p> Traditionally, an agent, x, knows how to achieve A by doing an action (description), P , iff x knows that P achieves A, and x can execute P <ref> [20, 21] </ref>. <p> Quantifying over action descriptions, and in the notation of this paper (with M the model, s the current "situation," and `result' a predicate giving the result of executing a plan in a situation), we obtain <ref> [20, p. 347] </ref>: * M j= s K how (x; A) iff (9P : M j= s can-execute (x; P ) ^ M j= s K that (x, result (P; s)! A)) The agent, x, proves that P will yield A, and always explicitly represents both P and all the knowledge
Reference: [21] <author> Leora Morgenstern. </author> <title> A theory of knowledge and planning. </title> <booktitle> In IJCAI, </booktitle> <year> 1987. </year>
Reference-contexts: A longer term research interest that this paper is a step towards is to formulate a logically well-founded methodology for the design of systems of intelligent agents. Traditional formal theories of knowledge and action <ref> [17, 20, 21] </ref> stress the conception of knowledge corresponding to know-that, or the knowledge of facts, rather than know-how, the knowledge of skills. It is assumed that know-how can be reduced to know-that. <p> Traditionally, an agent, x, knows how to achieve A by doing an action (description), P , iff x knows that P achieves A, and x can execute P <ref> [20, 21] </ref>.
Reference: [22] <author> Stanley J. Rosenschein. </author> <title> Formal theories of knowledge in AI and robotics. </title> <journal> New Generation Computing, </journal> <volume> 3(4), </volume> <year> 1985. </year>
Reference-contexts: While this "situated" theory does not use plans, it can accommodate the richer notion of strategies (see x5). Therefore, it contrasts both with situated theories of know-that <ref> [22] </ref>, and informal, and exclusively reactive, accounts of action [1]. 4 The Formal Model The formal model is based on possible worlds. 1 Each possible world has a branching history of times. At each time, environmental processes and agents' actions occur.
Reference: [23] <author> Gilbert Ryle. </author> <title> The Concept of Mind. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> UK, </address> <year> 1949. </year>
Reference-contexts: Indeed, if agents did not act at all, it would be difficult even to assign any knowledge of facts to them, except by arbitrarily interpreting their internal states <ref> [23] </ref>; e.g., we can say that a sunflower knows that the sun is in a certain direction only because it turns to point that way. A book or a library is not intelligent because neither can act.
Reference: [24] <author> John R. Searle. </author> <title> Collective intentions and actions. </title> <editor> In P. Cohen, J. Morgan, and M. Pollack, editors, </editor> <title> SDF Benchmark Series: Intentions in Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Since even individual agents can be treated as groups of subagents [8], this theory also applies in areas such as real-time AI systems and robotics. It also helps clarify the question of the joint intentions of a group of agents <ref> [4, 12, 24, 26] </ref>. A longer term research interest that this paper is a step towards is to formulate a logically well-founded methodology for the design of systems of intelligent agents. <p> Common knowledge is impossible to achieve in most realistic scenarios; e.g., where communication is not (known to be) guaranteed with only a finite delay [9, 13]. 2 Groups Traditional theories of multiagent intentions and action tend to ignore the structure of the group of agents being considered <ref> [4, 8, 12, 24] </ref>, and assume that agents are equally capable and perfectly cooperative. However, the knowledge and skills of a group depend greatly on its structure, and on the knowledge and skills of its members.
Reference: [25] <author> Yoav Shoham. </author> <title> Reasoning About Change: Time and Causation from the Standpoint of AI. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: some SS, is the minimal set that satisfies the following conditions: Eternity: SS S Linear Closure: (8t 00 : t 00 2 S! (8t 0 : t 0 &lt; t 0 &lt; t 00 ! t 0 2 S)) 1 This formal model resembles those of McDermott [19] and Shoham <ref> [25] </ref>. However, those accounts emphasize a language that might be used for representing and reasoning about temporal information. The stress here is on giving the formal definition for know-how.
Reference: [26] <author> Munindar P. Singh. </author> <title> Group intentions. </title> <booktitle> In 10th Workshop on Distributed Artificial Intelligence, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: Since even individual agents can be treated as groups of subagents [8], this theory also applies in areas such as real-time AI systems and robotics. It also helps clarify the question of the joint intentions of a group of agents <ref> [4, 12, 24, 26] </ref>. A longer term research interest that this paper is a step towards is to formulate a logically well-founded methodology for the design of systems of intelligent agents. <p> They need not even be explicitly represented or computed before action begins. It is tempting to say that G knows how to achieve A, if it can achieve A whenever it so "intends." Strategies are a simple, albeit incomplete, way of analyzing 7 intentions (see <ref> [26] </ref> for a formalization of this idea in the same general framework as this paper). I now let each group have a strategy that it follows in the current situation. <p> I now apply it in the context of a well-known problem in DAI: the pursuit problem. Parts of this section are borrowed from <ref> [26] </ref> which considers the same problem from the viewpoint of the agents' intentions. The pursuit problem problem was introduced by Benda et al. [3], but has been extensively studied by others [6, 28]. We are given a finite two-dimensional grid of points (see Figure 1).
Reference: [27] <author> Munindar P. Singh. </author> <title> Towards a theory of situated know-how. </title> <booktitle> In 9th European Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: Traditional formal theories of knowledge and action [17, 20, 21] stress the conception of knowledge corresponding to know-that, or the knowledge of facts, rather than know-how, the knowledge of skills. It is assumed that know-how can be reduced to know-that. In x3, I briefly review the arguments of <ref> [27] </ref> that know-how is important, and cannot be so reduced. In x2, I argue that the internal structure of groups, also neglected by traditional 1 theories, is important. <p> This definition requires the root of the tree to be done entirely, but allows A to occur fortuitously, before it has been completed. This definition is best applied to models that consist only of "normal" scenarios (see <ref> [27] </ref> for a discussion). It is important to note that the tree need not be represented jointly or severally by the members of the group, and is intended not to be so represented.
Reference: [28] <author> Larry M. Stephens and Matthais Merx. </author> <title> The effect of agent organization on the performance of DAI systems. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <note> 1989. Submitted. </note>
Reference-contexts: Parts of this section are borrowed from [26] which considers the same problem from the viewpoint of the agents' intentions. The pursuit problem problem was introduced by Benda et al. [3], but has been extensively studied by others <ref> [6, 28] </ref>. We are given a finite two-dimensional grid of points (see Figure 1). Each point may be occupied by either an agent called `Red' (the "adversary") or up to four `Blue' agents.
Reference: [29] <author> Eric Werner. </author> <title> Cooperating Agents: A Unified Theory of Communication and Social Structure. </title> <editor> In L. Gasser and M. N. Huhns, editors, </editor> <booktitle> Distributed Artificial Intelligence, </booktitle> <volume> Volume II, </volume> <pages> pages 3-36. </pages> <publisher> Pitman/Morgan Kaufmann, </publisher> <address> London, </address> <year> 1989. </year> <month> 19 </month>
Reference-contexts: The interactions among the members of a group can be seen as determining their respective "roles" in the group. Specifically, strategic interactions are the external correlates of the representational roles of Werner <ref> [29] </ref>, where they are seen as intentions to cooperate. <p> Further work, however, is needed to obtain a more "internal" or architectural view of know-how, and to relate it to the general "roles" that different agents may play in a group <ref> [29] </ref>. An important problem in this framework is to relate abilities and structure to perception and reasoning, especially when reasoning is not thought of as pure symbol-processing.
References-found: 29

