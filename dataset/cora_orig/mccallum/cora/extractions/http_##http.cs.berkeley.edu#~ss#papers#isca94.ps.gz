URL: http://http.cs.berkeley.edu/~ss/papers/isca94.ps.gz
Refering-URL: http://http.cs.berkeley.edu/~ss/papers.html
Root-URL: 
Title: RAID-II: A High-Bandwidth Network File Server RAID-II hardware with a single XBUS controller board delivers
Author: Ann L. Drapeau Ken W. Shirriff John H. Hartman Ethan L. Miller Srinivasan Seshan Randy H. Katz Ken Lutz David A. Patterson Edward K. Lee Peter M. Chen Garth A. Gibson 
Note: The  
Abstract: In 1989, the RAID (Redundant Arrays of Inexpensive Disks) group at U. C. Berkeley built a prototype disk array called RAID-I. The bandwidth delivered to clients by RAID-I was severely limited by the memory system bandwidth of the disk array's host workstation. We designed our second prototype, RAID-II, to deliver more of the disk array bandwidth to file server clients. A custom-built crossbar memory system called the XBUS board connects the disks directly to the high-speed network, allowing data for large requests to bypass the server workstation. RAID-II runs Log-Structured File System (LFS) software to optimize performance for bandwidth-intensive applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Peter M. Chen, Edward K. Lee, Ann L. Drapeau, Ken Lutz, Ethan L. Miller, Srinivasan Seshan, Ken Shirriff, David A. Patterson, and Randy H. Katz. </author> <title> Performance and Design Evaluation of the RAID-II Storage Server. </title> <booktitle> International Parallel Processing Symposium Workshop on I/O in Parallel Computer Systems, </booktitle> <month> April </month> <year> 1993. </year> <note> Also invited for submission to the Journal of Distributed and Parallel Databases, to appear. </note>
Reference-contexts: For busses or backplanes farther away from the CPU, available memory bandwidth drops quickly. Thus, file servers incorporating such workstations perform poorly on bandwidth-intensive applications. The design of hardware and software for our second prototype, RAID-II <ref> [1] </ref>, was motivated by a desire to deliver more of the disk array's bandwidth to the file server's clients. <p> This makes it possible for file server software running on the host to access network headers, file data and metadata in the XBUS memory. 2.3 Hardware Performance In this section, we present raw performance of the RAID-II <ref> [1] </ref> hardware, that is, the performance of the hardware without the overhead of a file system. In Section 3.4, we show how much of this raw hardware performance can be delivered by the file system to clients. and writes.
Reference: [2] <author> Ann L. Chervenak and Randy H. Katz. </author> <title> Perfomance of a Disk Array Prototype. </title> <booktitle> In Proceedings SIGMET-RICS, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: In this paper, we examine the efficient delivery of bandwidth from a file server that includes a disk array. In 1989, the RAID group at U.C. Berkeley built an initial RAID prototype, called RAID-I <ref> [2] </ref>. The prototype was constructed using a Sun 4/280 workstation with 128 megabytes of memory, four dual-string SCSI controllers, 28 5.25-inch SCSI disks and specialized disk striping software.
Reference: [3] <author> Bill Collins. </author> <title> High-Performance Data Systems. </title> <booktitle> In Digest of Papers. Eleventh IEEE Symposium on Mass Storage Systems, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: NAStore, for example, sustains 10 megabytes/second to individual files. RAID-II was designed to perform efficiently on small file accesses as well as on large transfers. A system similar in design philosophy to RAID-II is Los Alamos National Laboratory's High Performance Data System (HPDS) <ref> [3] </ref>. Like RAID-II, HPDS attempts to provide low latency on small transfers and control operations as well as high bandwidth on large transfers. HPDS connects an IBM RAID Level 3 disk array to a HIPPI network; like RAID-II, it has a high-bandwidth data path and a low-bandwidth control path.
Reference: [4] <author> Garth A. Gibson, R. Hugo Patterson, and M. Satya narayanan. </author> <title> Disk Reads with DRAM Latency. </title> <booktitle> Third Workshop on Workstaion Operating Systems, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: Techniques for maximizing reliability are beyond the scope of this paper <ref> [4] </ref>, [16], [6].) For reads, data are read from the disk array into the memory on the XBUS board; from there, data are sent over HIPPI, back to the XBUS board, and into XBUS memory.
Reference: [5] <author> Garth Alan Gibson. </author> <title> Redundant Disk Arrays: Re liable, Parallel Secondary Storage. </title> <type> PhD thesis, </type> <address> U. C. Berkeley, </address> <month> April </month> <year> 1991. </year> <note> Technical Report No. UCB/CSD 91/613. </note>
Reference-contexts: These developments require faster I/O systems to transfer the increasing volume of data. High performance file servers will increasingly incorporate disk arrays to provide greater disk bandwidth. RAIDs, or Redundant Arrays of Inexpensive Disks [13], <ref> [5] </ref>, use a collection of relatively small, inexpensive disks to achieve high performance and high reliability in a secondary storage system.
Reference: [6] <author> Jim Gray, Bob Horst, and Mark Walker. </author> <title> Parity Strip ing of Disc Arrays: Low-Cost Reliable Storage with Acceptable Throughput. </title> <booktitle> In Proceedings Very Large Data Bases, </booktitle> <pages> pages 148-161, </pages> <year> 1990. </year>
Reference-contexts: Techniques for maximizing reliability are beyond the scope of this paper [4], [16], <ref> [6] </ref>.) For reads, data are read from the disk array into the memory on the XBUS board; from there, data are sent over HIPPI, back to the XBUS board, and into XBUS memory.
Reference: [7] <author> John H. Hartman and John K. Ousterhout. </author> <title> The Zebra Striped Network File System. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: These base stations in turn drive the radio transceivers of a pico-cellular network to which hand-held display devices will connect. 5.2 Striping Across File Servers: The Zebra File System Zebra <ref> [7] </ref> is a network file system designed to provide high-bandwidth file access by striping files across multiple file servers. Its use with RAID-II would provide a mechanism for striping high-bandwidth file accesses over multiple network connections, and therefore across multiple XBUS boards.
Reference: [8] <author> John L. Hennessy and Norman P. Jouppi. </author> <title> Computer Technology and Architecture: An Evolving Interaction. </title> <journal> IEEE Computer, </journal> <volume> 24 </volume> <pages> 18-29, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The problems RAID-I experienced are typical of many workstations that are designed to exploit large, fast caches for good processor performance, but fail to support adequate primary memory or I/O bandwidth <ref> [8] </ref>, [12]. In such workstations, the memory system is designed so that only the CPU has a fast, high-bandwidth path to memory. For busses or backplanes farther away from the CPU, available memory bandwidth drops quickly. Thus, file servers incorporating such workstations perform poorly on bandwidth-intensive applications.
Reference: [9] <author> David Hitz. </author> <title> An NFS File Server Appliance. </title> <type> Technical Report Technical Report TR01, </type> <institution> Network Appliance Corporation, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: One way of achieving higher performance than is possible from a UNIX workstation server is to use special-purpose software and hardware. An example is the FAServer NFS file server <ref> [9] </ref>. The FAServer does not run UNIX; instead it runs a special-purpose operating system and file system tuned to provide efficient NFS file service. This reduces the overhead of each NFS operation and allows the FAServer to service many more clients than a UNIX-based server running on comparable hardware.
Reference: [10] <author> Reagan W. Moore. </author> <title> File Servers, Networking, and Supercomputers. </title> <type> Technical Report GA-A20574, </type> <institution> San Diego Supercomputer Center, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Each high-speed disk might transfer at a rate of 10 megabytes/second; typically, data might be striped across 40 of these disks. Data are usually copied onto these high-speed disks before an application is run on the supercomputer <ref> [10] </ref>. These disks do not provide permanent storage for users. The file system is not designed to provide efficient small transfers. The other type of supercomputer file system is the mainframe-managed mass storage system.
Reference: [11] <author> Bruce Nelson. </author> <title> An Overview of Functional Multipro cessing for NFS Network Servers. </title> <type> Technical Report Technical Report 1, </type> <institution> Auspex Engineering, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: The underlying hardware for the FAServer is a standard workstation, however, so the FAServer suffers from the same backplane bottleneck as a UNIX workstation when handling large transfers. Another special-purpose NFS file server is the Auspex NS6000 <ref> [11] </ref>. The Auspex is a functional multi-processor, meaning that the tasks required to process an NFS request are divided among several processors. Each processors runs its own special-purpose software, tailored to the task it must perform.
Reference: [12] <author> John K. Ousterhout. </author> <title> Why Aren't Operating Systems Getting Faster as Fast as Hardware. </title> <booktitle> In Proceedings USENIX Technical Conference, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: The problems RAID-I experienced are typical of many workstations that are designed to exploit large, fast caches for good processor performance, but fail to support adequate primary memory or I/O bandwidth [8], <ref> [12] </ref>. In such workstations, the memory system is designed so that only the CPU has a fast, high-bandwidth path to memory. For busses or backplanes farther away from the CPU, available memory bandwidth drops quickly. Thus, file servers incorporating such workstations perform poorly on bandwidth-intensive applications. <p> It has been shown that the bandwidth improvements in workstation memory and I/O systems have not kept pace with improvements in processor speed <ref> [12] </ref>. One way of achieving higher performance than is possible from a UNIX workstation server is to use special-purpose software and hardware. An example is the FAServer NFS file server [9].
Reference: [13] <author> David A. Patterson, Garth Gibson, and Randy H. </author> <note> Katz. </note>
Reference-contexts: These developments require faster I/O systems to transfer the increasing volume of data. High performance file servers will increasingly incorporate disk arrays to provide greater disk bandwidth. RAIDs, or Redundant Arrays of Inexpensive Disks <ref> [13] </ref>, [5], use a collection of relatively small, inexpensive disks to achieve high performance and high reliability in a secondary storage system. <p> The center rack contains three chassis: the top chassis holds VME disk controller boards; the center chassis contains XBUS controller boards and HIPPI interface boards, and the bottom VME chassis contains the Sun4/280 workstation. configured as a RAID Level 5 <ref> [13] </ref> with one parity group of 24 disks. (This scheme delivers high bandwidth but exposes the array to data loss during dependent failure modes such as a SCSI controller failure. <p> Because it avoids small write operations, the Log-Structured File System avoids a weakness of redundant disk arrays that affects traditional file systems. Under a traditional file system, disk arrays that use large block interleaving (Level 5 RAID <ref> [13] </ref>) perform poorly on small write operations because each small write requires four disk accesses: reads of the old data and parity blocks and writes of the new data and parity blocks.
References-found: 13

