URL: http://www.eecs.umich.edu/~gabandah/IPPS96.ps
Refering-URL: http://www.eecs.umich.edu/~gabandah/research.html
Root-URL: http://www.eecs.umich.edu
Email: gabandah,davidson@eecs.umich.edu  
Title: Modeling the Communication Performance of the IBM SP2  
Author: Gheith A. Abandah Edward S. Davidson 
Address: 1301 Beal Avenue, Ann Arbor, MI 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory, Department of EECS University of Michigan  
Date: April 1996. 1  
Note: To appear in the 10th International Parallel Processing Symposium,  
Abstract: The objective of this paper is to develop models that characterize the communication performance of a message-passing multicomputer by taking the IBM SP2 as a case study. The paper evaluates and models the three aspects of the communication performance: scheduling overhead, message-passing time, and synchronization overhead. Performance models are developed for the basic communication patterns, enabling the estimation of the communication times of a message-passing application. Such estimates facilitate activities such as application tuning, selection of the best available implementation technique, and performance comparisons among different multicomputers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Z. Xu and K. Hwany. </author> <title> Modeling communication overhead: MPI and MPL performance on the IBM SP2 multicomputer. </title> <note> To appear in IEEE Parallel and Distributed Technology, Spring 1996. </note>
Reference-contexts: The available models for message-passing multicomputers, however, either do not cover all the important communication patterns or do not give sufficient information for direct use in tuning a message-passing application. This paper and a similar independent effort <ref> [1] </ref> address this need. Most of the work in developing performance models for message-passing multicomputers is centered around benchmarks. The NAS Parallel Benchmarks (NPB) [2] were developed to study the performance of parallel supercomputers, they consist of five parallel kernels and three simulated applications.
Reference: [2] <author> D. Bailey et al. </author> <title> The NAS parallel benchmarks. </title> <type> Tech. Rep. </type> <institution> RNR-94-007, NASA Ames Research Center, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: This paper and a similar independent effort [1] address this need. Most of the work in developing performance models for message-passing multicomputers is centered around benchmarks. The NAS Parallel Benchmarks (NPB) <ref> [2] </ref> were developed to study the performance of parallel supercomputers, they consist of five parallel kernels and three simulated applications. Together, they mimic the computation and data movement characteristics of large-scale computational fluid dynamics applications.
Reference: [3] <author> R. Hockney and M. Berry. </author> <title> Public international benchmarks for parallel computers. </title> <type> Technical Report 1, </type> <institution> PARKBENCH Committee, </institution> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: The NAS Parallel Benchmarks (NPB) [2] were developed to study the performance of parallel supercomputers, they consist of five parallel kernels and three simulated applications. Together, they mimic the computation and data movement characteristics of large-scale computational fluid dynamics applications. Another suite is PARK-BENCH <ref> [3] </ref> which contains low-level benchmarks for measuring basic computer characteristics, kernels to test typical scientific subroutines, and compact applications to test complete problems. Benchmarks like NPB are useful for comparing different machines, but they do not separate computation performance from communication performance.
Reference: [4] <author> W. H. Mangione-Smith, T. P. Shih, S. G. Abraham, and E. S. Davidson. </author> <title> Approaching a machine-application bound in delivered performance on scientific code. </title> <journal> IEEE Proceedings, </journal> <volume> 81(8) </volume> <pages> 1166-1178, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: The objective of this paper is to develop models that characterize the communication performance of a multicomputer by taking the IBM SP2 as a case study. This characterization, along with the computation performance modeling of the processor node <ref> [4, 5] </ref>, are intended to facilitate: explaining the achieved performance of message-passing applications, developing efficient message-passing applications, and comparing the performance of different multicomputers and application code implementations. The performance models should estimate the execution time of a message-passing application given its high-level source code.
Reference: [5] <author> E. L. Boyd, G. A. Abandah, H.-H. Lee, and E. S. Davidson. </author> <title> Modeling computation and communication performance of parallel scientific applications: A case study of the IBM SP2. </title> <type> Technical Report CSE-TR-236-95, </type> <institution> University of Michigan, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The objective of this paper is to develop models that characterize the communication performance of a multicomputer by taking the IBM SP2 as a case study. This characterization, along with the computation performance modeling of the processor node <ref> [4, 5] </ref>, are intended to facilitate: explaining the achieved performance of message-passing applications, developing efficient message-passing applications, and comparing the performance of different multicomputers and application code implementations. The performance models should estimate the execution time of a message-passing application given its high-level source code. <p> These models have proved to be useful for tuning high-performance message-passing scientific applications. For example, we have used them to analyze the performance of an explicit method finite element application for vehicle crash simulation <ref> [5] </ref>. We have also used them in a research study to evaluate and compare the performance of six broadcast algorithms on the SP2 [15]. In this research we have studied standard broadcast algorithms like One-to-many broadcast (otm), binary-tree broadcast (bt), and recursive-doubling broadcast (rd).
Reference: [6] <author> T. Agerwala, J. Martin, J. Mirza, D. Sadler, D. Dias, and M. Snir. </author> <title> SP2 system architecture. </title> <journal> IBM Syst. J., </journal> <volume> 34(2) </volume> <pages> 152-184, </pages> <year> 1995. </year>
Reference-contexts: Scheduling overhead is addressed in Section 3, message-passing time in Section 4, and synchronization overhead in Section 5. Section 6 presents the conclusions and overviews some uses of the models developed in this paper. 2. IBM SP2 The IBM Scalable POWERparallel SP2 <ref> [6] </ref> connects 2 to 512 RISC System/6000 POWER2 processors [7] via a communication subsystem. This subsystem is based upon a low-latency, high-bandwidth interconnection network called the High Performance Switch (HPS).
Reference: [7] <author> S. W. White and S. Dhawan. POWER2: </author> <title> next generation of the RISC System/6000 family. </title> <journal> IBM J. Res. Dev., </journal> <volume> 38(5) </volume> <pages> 493-502, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Section 6 presents the conclusions and overviews some uses of the models developed in this paper. 2. IBM SP2 The IBM Scalable POWERparallel SP2 [6] connects 2 to 512 RISC System/6000 POWER2 processors <ref> [7] </ref> via a communication subsystem. This subsystem is based upon a low-latency, high-bandwidth interconnection network called the High Performance Switch (HPS).
Reference: [8] <editor> C. B. Stunkel et al. </editor> <booktitle> Architecture and implementation of Vul-can. In The 8th International Parallel Processing Symposium, </booktitle> <pages> pages 268-274, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: When the head of a packet is blocked, the flits are buffered in place. As soon as the output port is free, packet transfer resumes. The switching element of the board is the Vulcan switch chip <ref> [8] </ref>. The HPS operates at 40 MHz, providing a peak bandwidth of 40 MB/sec over each of the two channels of each communication link. The Vulcan switch chip contains 8 receiver modules, 8 transmitter modules, an 8 by 8 crossbar, and a central queue for buffering the blocked flits.
Reference: [9] <author> MPI Forum. </author> <title> MPI: A message-passing interface standard, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: All the experiments were carried out during exclusive reservation (no processes running for other users). The measured data show low standard deviation, usually only a few percent of the measured time. The SP2 supports the MPI <ref> [9] </ref>, PMVe [10], and MPL [11] message-passing libraries. MPI is the emerging Message Passing Interface standard, PVMe is IBM's implementation of the popular PVM [12], and MPL is IBM's proprietary Message Passing Library.
Reference: [10] <author> IBM. </author> <title> IBM AIX PVMe User's Guide and Subroutine Reference, </title> <address> 1.0 edition, </address> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: All the experiments were carried out during exclusive reservation (no processes running for other users). The measured data show low standard deviation, usually only a few percent of the measured time. The SP2 supports the MPI [9], PMVe <ref> [10] </ref>, and MPL [11] message-passing libraries. MPI is the emerging Message Passing Interface standard, PVMe is IBM's implementation of the popular PVM [12], and MPL is IBM's proprietary Message Passing Library.
Reference: [11] <author> IBM. </author> <title> IBM AIX Parallel Environment, Parallel Programming Subroutine Reference, </title> <address> 2.0 edition, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: All the experiments were carried out during exclusive reservation (no processes running for other users). The measured data show low standard deviation, usually only a few percent of the measured time. The SP2 supports the MPI [9], PMVe [10], and MPL <ref> [11] </ref> message-passing libraries. MPI is the emerging Message Passing Interface standard, PVMe is IBM's implementation of the popular PVM [12], and MPL is IBM's proprietary Message Passing Library.
Reference: [12] <author> A. Geist et al. </author> <title> PVM 3 User's Guide and Reference Manual. </title> <institution> Oak Ridge National Laboratory, </institution> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The measured data show low standard deviation, usually only a few percent of the measured time. The SP2 supports the MPI [9], PMVe [10], and MPL [11] message-passing libraries. MPI is the emerging Message Passing Interface standard, PVMe is IBM's implementation of the popular PVM <ref> [12] </ref>, and MPL is IBM's proprietary Message Passing Library. MPL is similar to MPI and contains Fortran and C subroutines that allow managing and querying the task environment, sending and receiving messages, performing collective communication, and implementing barrier synchronization.
Reference: [13] <author> M. Snir, P. Hochschild, D. Frye, and K. Gildea. </author> <title> The communication software and parallel environment of the IBM SP2. </title> <journal> IBM Syst. J., </journal> <volume> 34(2) </volume> <pages> 205-221, </pages> <year> 1995. </year>
Reference-contexts: Non-blocking subroutines, on the other hand, return as soon as possible, with communication processing continuing regardless of the state of the application buffers. For Thin nodes, the performance of the non-blocking subroutines is essentially the same as the blocking routines. MPL has two implementations <ref> [13] </ref>; the Internet Protocol implementation (IP) which allows shared usage of the processor nodes, and the User Space implementation (US) which has better performance, but has a limit of one parallel task per processor node.
Reference: [14] <author> R. Hockney. </author> <title> Performance parameters and benchmarking of supercomputers. </title> <journal> Parallel Computing, </journal> <volume> 17(10) </volume> <pages> 1111-1130, </pages> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: The effective HPS bandwidth is thus 40 fi 232=255 = 36.4 MB/sec. The Point-to-point communication time often can be expressed by a few simple parameters <ref> [14] </ref>; r 1 is the asymptotic transfer rate in MB/sec, and t 0 is the asymptotic zero message length latency in sec.
Reference: [15] <author> G. A. Abandah and E. S. Davidson. </author> <title> Modeling the computation and communication performance of the IBM SP2. </title> <type> Technical Report CSE-TR-258-95, </type> <institution> University of Michigan, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: For example, we have used them to analyze the performance of an explicit method finite element application for vehicle crash simulation [5]. We have also used them in a research study to evaluate and compare the performance of six broadcast algorithms on the SP2 <ref> [15] </ref>. In this research we have studied standard broadcast algorithms like One-to-many broadcast (otm), binary-tree broadcast (bt), and recursive-doubling broadcast (rd). In otm, the sender processor sends the message to all the receivers one at a time. <p> These models can also be used to compare different mul-ticomputers. Developing these models for different mul-ticomputers reveals many aspects of their relative performance. We have conducted one such communication performance comparison <ref> [15] </ref> by developing the performance models for the PVM message-passing library on the IBM SP2 and the Convex SPP-1000 [16]. This study has shown that the two systems have somewhat similar communication performance, with some notable differences.
Reference: [16] <institution> Convex Computer Corporation. </institution> <note> Convex Exemplar Programming Guide, x3.0.0.2 edition, </note> <month> June </month> <year> 1994. </year> <month> 9 </month>
Reference-contexts: Developing these models for different mul-ticomputers reveals many aspects of their relative performance. We have conducted one such communication performance comparison [15] by developing the performance models for the PVM message-passing library on the IBM SP2 and the Convex SPP-1000 <ref> [16] </ref>. This study has shown that the two systems have somewhat similar communication performance, with some notable differences. Synchronization in a multicomputer occurs implicitly whenever the processor nodes transfer data through message passing. This synchronization results in increased overhead when the load balance among the processors is poor.
References-found: 16

