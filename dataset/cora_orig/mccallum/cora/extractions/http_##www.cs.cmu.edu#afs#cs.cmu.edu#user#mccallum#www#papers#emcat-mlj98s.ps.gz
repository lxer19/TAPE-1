URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mccallum/www/papers/emcat-mlj98s.ps.gz
Refering-URL: 
Root-URL: 
Email: knigam@cs.cmu.edu  mccallum@justresearch.com  thrun@cs.cmu.edu  mitchell+@cs.cmu.edu  Editor:  
Title: Learning to Classify Text from Labeled and Unlabeled Documents  
Author: KAMAL NIGAM ANDREW MCCALLUM zy SEBASTIAN THRUN TOM MITCHELL 
Keyword: text classification, Expectation Maximization, combining supervised and unsupervised learning, Bayesian learning  
Address: Pittsburgh, PA 15213  Street, Pittsburgh, PA 15213  
Affiliation: School of Computer Science, Carnegie Mellon University,  
Note: Machine Learning, 1-22 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  Just Research, 4616 Henry  
Abstract: This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is significant because in many important text classification problems obtaining classification labels is expensive, while large quantities of unlabeled documents are readily available. We present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function. We then introduce an algorithm for learning from labeled and unlabeled text, based on the combination of Expectation-Maximization with a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> V. Castelli and T. </author> <title> Cover. On the exponential value of labeled samples. </title> <journal> Pattern Recognition Letters, </journal> <volume> 16 </volume> <pages> 105-111, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: For this knowledge to aid classification, we have to exclude the two extreme cases: jD l j = 0 and jD l j = 1. If there is no labeled data, unlabeled data cannot improve classification, as shown in <ref> [1] </ref>. If there is infinite amounts of labeled data, all parameters can be recovered with probability 1 from the labeled data and the resulting classifier is Bayes-optimal [29]; thus, further unlabeled data cannot improve the classification accuracy. <p> Without 18 NIGAM, MCCALLUM, THRUN AND MITCHELL any labeled data, this permutation cannot be found, and thus, although the parameters are known, classification error is not reduced from random guessing. As shown in <ref> [1] </ref>, with infinite unlabeled data, the classification error approaches the Bayes optimal solution at an exponential rate in the number of labeled examples given. Thus, if infinite amounts of unlabeled data are available, the convergence rate of learning from labeled data is changed by an exponential factor. * Trade-off. <p> Unfortunately, their analysis assumes that unlabeled data alone is sufficient to estimate both parameter vectors; thus, they assume that the target concept can be recovered without any target labels. This assumption is unrealistic. As shown in <ref> [1] </ref>, unlabeled data does not improve the classification results in the absence of labeled data. Shahshahani and Landgrebe's analysis also does not investigate the classification error. Unfortunately, all these results rest on two restrictive assumptions, both of which are usually violated in text classification domains.
Reference: 2. <author> V. Castelli and T. </author> <title> Cover. The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(6) </volume> <pages> 2101-2117, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: Thus, if infinite amounts of unlabeled data are available, the convergence rate of learning from labeled data is changed by an exponential factor. * Trade-off. As shown in <ref> [2] </ref>, labeled data can be exponentially more valuable than unlabeled data in reducing the probability of classification error by non-degenerate Bayesian classifiers.
Reference: 3. <author> Peter Cheeseman and John Stutz. </author> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In U. Fayyad, editor, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <year> 1996. </year>
Reference-contexts: When the data is accurately modeled, gains from using EM are readily seen. One obvious question is how to select the best model representation. Cheeseman and Stutz <ref> [3] </ref> investigate this for clustering tasks with no labeled data, and explicitly compare the probability of the data for different models, and select the best match, with a prior that prefers smaller models. For classification tasks, it may be more beneficial to select this with a more appropriate classification-oriented criteria. <p> Ghahramani and Jordan have used EM with mixture models to fill in missing values [11]. The emphasis of their work was on missing feature values, where we focus on augmenting a very small but complete set of labeled data. The AutoClass project <ref> [3, 12] </ref> investigated the combination of the EM algorithm with an underlying model of a naive Bayes classifier. The emphasis of their research was the discovery of novel clusterings for unsupervised learning over unlabeled data. AutoClass has not been applied to text or classification.
Reference: 4. <author> W. Cohen and Y. Singer. </author> <title> Context-sensitive learning methods for text categorization. </title> <booktitle> In Proceedings of ACM SIGIR Conference, </booktitle> <year> 1997. </year>
Reference-contexts: Approaches differ in their methods for selecting the unlabeled example to request a label. Three such examples are relevance sampling and uncertainty sampling [19, 22], and a "Query By Committee" approach [25]. Several other statistical text classifiers have been used by others in a variety of domains <ref> [39, 14, 4] </ref> However, naive Bayes has a strong probabilistic foundation for EM, and is more efficient for large data sets. The thrust of this paper is to straightforwardly demonstrate the value of unlabeled data; a similar approach could apply unlabeled data to more complex classifiers. 9.
Reference: 5. <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley, </publisher> <year> 1991. </year>
Reference-contexts: of encoding words from class c j using a code that is optimal for the distribution of words in :c j ; the sum of this quantity over all words is the Kullback-Leibler divergence between the distribution of words in c j and the distribution of words in :c j <ref> [5] </ref>.
Reference: 6. <author> M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K. Nigam, and S. Slattery. </author> <title> Learning to extract symbolic knowledge from the World Wide Web. </title> <booktitle> In Proceedings of AAAI-98, </booktitle> <year> 1998. </year>
Reference-contexts: There are statistical text learning algorithms that can be trained to approximately classify documents, given a sufficient set of labeled training examples. These text classification algorithms have been used to automatically catalog news articles [19, 14] and web pages <ref> [6] </ref>, automatically learn the reading interests of users [32, 17], and automatically sort electronic mail [23]. One key difficulty with these current algorithms, and the issue addressed by this paper, is that they require a large, often prohibitive, number of labeled training examples to learn accurately. <p> Documents often fall into overlapping categories. Words within a document are not independent of each other|grammar and topicality ensure this. Despite these violations, empirically, the Naive Bayes classifier does a good job of classifying text documents <ref> [20, 6, 39, 13] </ref>. This paradox is explained by the fact that classification estimation is only a function of the sign (in binary cases) of the function estimation; the function approximation can still be poor while classification accuracy remains high [9, 10]. <p> Best performance was obtained with no feature selection, and by normalizing word counts by document length. Accuracy results are reported as averages of ten test/train splits, with 20% of the documents randomly selected for placement in the test set. The WebKB data set <ref> [6] </ref> contains 8145 web pages gathered from university computer science departments. For four departments, all web pages were included; additionally, there are many pages from an assortment of other universities. The pages are divided into seven categories: student, faculty, staff, course, project, department and other. <p> We did not use stemming or a stoplist; we found that using a stoplist actually hurt performance because, for example, "my" is the fourth-ranked word by information gain, and is an excellent indicator of a student homepage. As done previously <ref> [6] </ref>, we use only the 2000 most informative words, as measured by average mutual information with the class variable. This feature selection method is commonly used for text [39, 16, 13].
Reference: 7. <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM. algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: The specific approach we describe here is based on a combination of two well-known learning algorithms: the naive Bayes classifier [20] and the Expectation Maximization (EM) algorithm <ref> [7] </ref>. The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio [35, 34], regression models [38], k-nearest-neighbor [39] and Support Vector Machines [14]. <p> Demp-ster <ref> [7] </ref> uses this insight in the Expectation Maximization algorithm, which finds a local maximum likelihood ^ by an iterative procedure that recomputes the expected value of z and the maximum likelihood parameterization given z. Note that for the labeled documents z i is already known.
Reference: 8. <author> L. Devroye, L. Gyofri, and G. Lugosi. </author> <title> A Probabilistic Theory of Pattern Recognition. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: In certain asymptotic cases, the relative value of unlabeled data in learning classification is well-understood. * No unlabeled data. First consider the efficiency of estimating from a pool of labeled data only, D l . According to <ref> [8] </ref>, estimating using the maximum likelihood estimator is subject to the following error bound: P (j ^ j &gt; ") 2 m e jD l j" 2 =2 (15) Here ^ DjC and ^ C are the maximum likelihood estimates for . <p> Unfortunately, if this assumption is violated, estimators such as the maximum likelihood estimator may generate poor results. As shown in <ref> [8] </ref>, under such conditions the maximum likelihood estimator can easily fail to minimize the classification error on the training set. LEARNING TO CLASSIFY TEXT FROM LABELED AND UNLABELED DOCUMENTS 19 8. Related Work Two other studies have used EM to combine labeled and unlabeled data for classification [30, 36].
Reference: 9. <author> P. Domingos and M. Pazzani. </author> <title> Beyond independence: Conditions for the optimality of the simple bayesian classifier. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 103-130, </pages> <year> 1997. </year>
Reference-contexts: We will then use this to introduce the classifier and show that unlabeled data can be used to improve classification. The framework follows commonly used assumptions <ref> [20, 9] </ref> about the data|(1) that our text is produced by a mixture model, and (2) that there is a one-to-one correspondence between mixture components and classes. <p> Thus, the motivational result from the previous section still holds|that unlabeled documents can be beneficial. Naive Bayes makes the additional assumption that the probability of seeing a word in a document is independent of its context and its position <ref> [20, 9] </ref>. The learning task is to use a set of training documents in order to form estimates for the parameters of the generative model. Naive Bayes forms Bayes optimal estimates of these parameters, then uses the estimated model to classify new documents. <p> This paradox is explained by the fact that classification estimation is only a function of the sign (in binary cases) of the function estimation; the function approximation can still be poor while classification accuracy remains high <ref> [9, 10] </ref>. The above formulation of naive Bayes assumes a generative model that accounts for the number of times a word appears in a document. This is equivalent to a multinomial event model (without the factorial terms that account for event ordering) [26].
Reference: 10. <author> Jerome H. Friedman. </author> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. Data Mining and Knowledge Discovery, </title> <booktitle> 1 </booktitle> <pages> 55-77, </pages> <year> 1997. </year>
Reference-contexts: This paradox is explained by the fact that classification estimation is only a function of the sign (in binary cases) of the function estimation; the function approximation can still be poor while classification accuracy remains high <ref> [9, 10] </ref>. The above formulation of naive Bayes assumes a generative model that accounts for the number of times a word appears in a document. This is equivalent to a multinomial event model (without the factorial terms that account for event ordering) [26].
Reference: 11. <author> Zoubin Ghahramani and Michael Jordan. </author> <title> Supervised learning from incomplete data via an EM approach. </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS 6). </booktitle> <publisher> Morgan Kauffman Publishers, </publisher> <year> 1994. </year>
Reference-contexts: This section describes how to use EM within the probabilistic framework of the previous section. This is a special case of the more general missing values formulation, as presented by <ref> [11] </ref>. While the theory of why EM works is not particularly simple, the resulting algorithm is very straightforward. Our algorithm is outlined in Table 1. We are given a set of training documents D and the task is to build a classifier of the form in the previous section. <p> Our work is an example of applying EM to fill in missing values|the missing values are the class labels of the unlabeled training examples. Ghahramani and Jordan have used EM with mixture models to fill in missing values <ref> [11] </ref>. The emphasis of their work was on missing feature values, where we focus on augmenting a very small but complete set of labeled data. The AutoClass project [3, 12] investigated the combination of the EM algorithm with an underlying model of a naive Bayes classifier.
Reference: 12. <author> Hanson, Cheeseman, and Stutz. </author> <title> Bayesian classification theory. </title> <type> Technical Report Technical Report FIA-90-12-7-01, </type> <institution> NASA AMES Research Center, </institution> <year> 1991. </year>
Reference-contexts: Ghahramani and Jordan have used EM with mixture models to fill in missing values [11]. The emphasis of their work was on missing feature values, where we focus on augmenting a very small but complete set of labeled data. The AutoClass project <ref> [3, 12] </ref> investigated the combination of the EM algorithm with an underlying model of a naive Bayes classifier. The emphasis of their research was the discovery of novel clusterings for unsupervised learning over unlabeled data. AutoClass has not been applied to text or classification.
Reference: 13. <author> Thorsten Joachims. </author> <title> A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. </title> <booktitle> In International Conference on Machine Learning (ICML), </booktitle> <year> 1997. </year>
Reference-contexts: Documents often fall into overlapping categories. Words within a document are not independent of each other|grammar and topicality ensure this. Despite these violations, empirically, the Naive Bayes classifier does a good job of classifying text documents <ref> [20, 6, 39, 13] </ref>. This paradox is explained by the fact that classification estimation is only a function of the sign (in binary cases) of the function estimation; the function approximation can still be poor while classification accuracy remains high [9, 10]. <p> This is equivalent to a multinomial event model (without the factorial terms that account for event ordering) [26]. This formulation has been used by numerous practitioners of naive Bayes text classification <ref> [19, 15, 13, 24, 31, 28] </ref>. However, there is another formulation of naive Bayes text classification that instead assumes a generative model and document representation where each word in the vocabulary is a binary feature, and is modeled by a Bernoulli trial [33, 21, 15, 18, 16]. <p> We present experimental results with three different text corpora from the domains of UseNet news articles (20 Newsgroups), web pages (WebKB), and newswire articles (Reuters). 3 Datasets and Protocol The 20 Newsgroups data set <ref> [13] </ref>, collected by Ken Lang, consists of 20,017 articles divided almost evenly among 20 different UseNet discussion groups. When words from a stoplist of common short words are removed, there are 62,258 unique words that occur more than once. <p> As done previously [6], we use only the 2000 most informative words, as measured by average mutual information with the class variable. This feature selection method is commonly used for text <ref> [39, 16, 13] </ref>. Accuracy results presented below are an average of twenty test/train splits, again randomly holding out 20% of the documents for testing. The `ModApte' train/test split of the Reuters 21578 Distribution 1.0 data set consists of 12902 articles and 90 topic categories from the Reuters newswire. <p> These numbers are presented at the best vocabulary size for each task, indicated in parentheses. Classifiers for different categories performed best with widely varying vocabulary sizes. This variance of optimal vocabulary size is unsurprising. As previously noted <ref> [13] </ref>, categories like "wheat" and "corn" are known for a strong correspondence between words and categories, while categories like "acq" are known for a more subtle class definition.
Reference: 14. <author> Thorsten Joachims. </author> <title> Text categorization with Support Vector Machines: Learning with many relevant features. </title> <type> Technical Report LS8-Report, </type> <institution> University of Dortmund, </institution> <month> November </month> <year> 1997. </year>
Reference-contexts: There are statistical text learning algorithms that can be trained to approximately classify documents, given a sufficient set of labeled training examples. These text classification algorithms have been used to automatically catalog news articles <ref> [19, 14] </ref> and web pages [6], automatically learn the reading interests of users [32, 17], and automatically sort electronic mail [23]. <p> The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio [35, 34], regression models [38], k-nearest-neighbor [39] and Support Vector Machines <ref> [14] </ref>. EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data. <p> Accuracy results presented below are an average of twenty test/train splits, again randomly holding out 20% of the documents for testing. The `ModApte' train/test split of the Reuters 21578 Distribution 1.0 data set consists of 12902 articles and 90 topic categories from the Reuters newswire. Following several other studies <ref> [14, 25] </ref> we use the 10 most populous classes and build binary classifiers for each class. We use all the words inside the &lt;TEXT&gt; tags, including the title and the dateline, except that we remove the REUTER and &# tags that occur at the top and bottom of every document. <p> Approaches differ in their methods for selecting the unlabeled example to request a label. Three such examples are relevance sampling and uncertainty sampling [19, 22], and a "Query By Committee" approach [25]. Several other statistical text classifiers have been used by others in a variety of domains <ref> [39, 14, 4] </ref> However, naive Bayes has a strong probabilistic foundation for EM, and is more efficient for large data sets. The thrust of this paper is to straightforwardly demonstrate the value of unlabeled data; a similar approach could apply unlabeled data to more complex classifiers. 9.
Reference: 15. <author> T. Kalt and W. B. Croft. </author> <title> A new probabilistic model of text classification and retrieval. </title> <type> Technical Report IR-78, </type> <institution> University of Massachusetts Center for Intelligent Information Retrieval, </institution> <year> 1996. </year> <note> http://ciir.cs.umass.edu/publications/index.shtml. </note>
Reference-contexts: This is equivalent to a multinomial event model (without the factorial terms that account for event ordering) [26]. This formulation has been used by numerous practitioners of naive Bayes text classification <ref> [19, 15, 13, 24, 31, 28] </ref>. However, there is another formulation of naive Bayes text classification that instead assumes a generative model and document representation where each word in the vocabulary is a binary feature, and is modeled by a Bernoulli trial [33, 21, 15, 18, 16]. <p> However, there is another formulation of naive Bayes text classification that instead assumes a generative model and document representation where each word in the vocabulary is a binary feature, and is modeled by a Bernoulli trial <ref> [33, 21, 15, 18, 16] </ref>. Empirical comparisons show that the multinomial formulation yields higher-accuracy classifiers [26]. 8 NIGAM, MCCALLUM, THRUN AND MITCHELL 5.
Reference: 16. <author> Daphne Koller and Mehran Sahami. </author> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <year> 1997. </year>
Reference-contexts: However, there is another formulation of naive Bayes text classification that instead assumes a generative model and document representation where each word in the vocabulary is a binary feature, and is modeled by a Bernoulli trial <ref> [33, 21, 15, 18, 16] </ref>. Empirical comparisons show that the multinomial formulation yields higher-accuracy classifiers [26]. 8 NIGAM, MCCALLUM, THRUN AND MITCHELL 5. <p> As done previously [6], we use only the 2000 most informative words, as measured by average mutual information with the class variable. This feature selection method is commonly used for text <ref> [39, 16, 13] </ref>. Accuracy results presented below are an average of twenty test/train splits, again randomly holding out 20% of the documents for testing. The `ModApte' train/test split of the Reuters 21578 Distribution 1.0 data set consists of 12902 articles and 90 topic categories from the Reuters newswire.
Reference: 17. <author> Ken Lang. Newsweeder: </author> <title> Learning to filter netnews. </title> <booktitle> In International Conference on Machine Learning (ICML), </booktitle> <pages> pages 331-339, </pages> <year> 1995. </year>
Reference-contexts: There are statistical text learning algorithms that can be trained to approximately classify documents, given a sufficient set of labeled training examples. These text classification algorithms have been used to automatically catalog news articles [19, 14] and web pages [6], automatically learn the reading interests of users <ref> [32, 17] </ref>, and automatically sort electronic mail [23]. One key difficulty with these current algorithms, and the issue addressed by this paper, is that they require a large, often prohibitive, number of labeled training examples to learn accurately. <p> Take, for example, the task of learning which newsgroup articles are of interest to a person reading UseNet news, as examined by Lang <ref> [17] </ref>. After reading and classifying about 1000 articles, precision of the learned classifier was about 50% for the top 10% of documents ranked by the classifier.
Reference: 18. <author> Leah S. Larkey and W. Bruce Croft. </author> <title> Combining classifiers in text categorization. </title> <booktitle> In SIGIR-96, </booktitle> <year> 1996. </year>
Reference-contexts: However, there is another formulation of naive Bayes text classification that instead assumes a generative model and document representation where each word in the vocabulary is a binary feature, and is modeled by a Bernoulli trial <ref> [33, 21, 15, 18, 16] </ref>. Empirical comparisons show that the multinomial formulation yields higher-accuracy classifiers [26]. 8 NIGAM, MCCALLUM, THRUN AND MITCHELL 5.
Reference: 19. <author> D. Lewis and Gale. </author> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In Proceedings of ACM SIGIR Conference, </booktitle> <year> 1994. </year> <note> 22 NIGAM, MCCALLUM, THRUN AND MITCHELL </note>
Reference-contexts: There are statistical text learning algorithms that can be trained to approximately classify documents, given a sufficient set of labeled training examples. These text classification algorithms have been used to automatically catalog news articles <ref> [19, 14] </ref> and web pages [6], automatically learn the reading interests of users [32, 17], and automatically sort electronic mail [23]. <p> This is equivalent to a multinomial event model (without the factorial terms that account for event ordering) [26]. This formulation has been used by numerous practitioners of naive Bayes text classification <ref> [19, 15, 13, 24, 31, 28] </ref>. However, there is another formulation of naive Bayes text classification that instead assumes a generative model and document representation where each word in the vocabulary is a binary feature, and is modeled by a Bernoulli trial [33, 21, 15, 18, 16]. <p> Approaches differ in their methods for selecting the unlabeled example to request a label. Three such examples are relevance sampling and uncertainty sampling <ref> [19, 22] </ref>, and a "Query By Committee" approach [25]. Several other statistical text classifiers have been used by others in a variety of domains [39, 14, 4] However, naive Bayes has a strong probabilistic foundation for EM, and is more efficient for large data sets.
Reference: 20. <author> David Lewis and Marc Ringuette. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Third Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 81-93, </pages> <year> 1994. </year>
Reference-contexts: The specific approach we describe here is based on a combination of two well-known learning algorithms: the naive Bayes classifier <ref> [20] </ref> and the Expectation Maximization (EM) algorithm [7]. The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio [35, 34], regression models [38], k-nearest-neighbor [39] and Support Vector Machines [14]. <p> We will then use this to introduce the classifier and show that unlabeled data can be used to improve classification. The framework follows commonly used assumptions <ref> [20, 9] </ref> about the data|(1) that our text is produced by a mixture model, and (2) that there is a one-to-one correspondence between mixture components and classes. <p> Thus, the motivational result from the previous section still holds|that unlabeled documents can be beneficial. Naive Bayes makes the additional assumption that the probability of seeing a word in a document is independent of its context and its position <ref> [20, 9] </ref>. The learning task is to use a set of training documents in order to form estimates for the parameters of the generative model. Naive Bayes forms Bayes optimal estimates of these parameters, then uses the estimated model to classify new documents. <p> Documents often fall into overlapping categories. Words within a document are not independent of each other|grammar and topicality ensure this. Despite these violations, empirically, the Naive Bayes classifier does a good job of classifying text documents <ref> [20, 6, 39, 13] </ref>. This paradox is explained by the fact that classification estimation is only a function of the sign (in binary cases) of the function estimation; the function approximation can still be poor while classification accuracy remains high [9, 10].
Reference: 21. <author> David D. Lewis. </author> <title> An evaluation of phrasal and clustered representations on a text categorization task. </title> <booktitle> In SIGIR-92, </booktitle> <year> 1992. </year>
Reference-contexts: However, there is another formulation of naive Bayes text classification that instead assumes a generative model and document representation where each word in the vocabulary is a binary feature, and is modeled by a Bernoulli trial <ref> [33, 21, 15, 18, 16] </ref>. Empirical comparisons show that the multinomial formulation yields higher-accuracy classifiers [26]. 8 NIGAM, MCCALLUM, THRUN AND MITCHELL 5.
Reference: 22. <author> David D. Lewis. </author> <title> A sequential algorithm for training text classifiers: Corrigendum and additional data. </title> <journal> SIGIR Forum, </journal> <volume> 29(2) </volume> <pages> 13-19, </pages> <year> 1995. </year>
Reference-contexts: Approaches differ in their methods for selecting the unlabeled example to request a label. Three such examples are relevance sampling and uncertainty sampling <ref> [19, 22] </ref>, and a "Query By Committee" approach [25]. Several other statistical text classifiers have been used by others in a variety of domains [39, 14, 4] However, naive Bayes has a strong probabilistic foundation for EM, and is more efficient for large data sets.
Reference: 23. <author> David D. Lewis and Kimberly A. Knowles. </author> <title> Threading electronic mail: A preliminary study. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 33(2) </volume> <pages> 209-217, </pages> <year> 1997. </year>
Reference-contexts: These text classification algorithms have been used to automatically catalog news articles [19, 14] and web pages [6], automatically learn the reading interests of users [32, 17], and automatically sort electronic mail <ref> [23] </ref>. One key difficulty with these current algorithms, and the issue addressed by this paper, is that they require a large, often prohibitive, number of labeled training examples to learn accurately.
Reference: 24. <author> Hang Li and Kenji Yamanishi. </author> <title> Document classification using a finite mixture model. </title> <booktitle> In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1997. </year>
Reference-contexts: This is equivalent to a multinomial event model (without the factorial terms that account for event ordering) [26]. This formulation has been used by numerous practitioners of naive Bayes text classification <ref> [19, 15, 13, 24, 31, 28] </ref>. However, there is another formulation of naive Bayes text classification that instead assumes a generative model and document representation where each word in the vocabulary is a binary feature, and is modeled by a Bernoulli trial [33, 21, 15, 18, 16]. <p> We thank Doug Baker for help formatting the Reuters data set. This research was supported in part by the Darpa HPKB program under contract F30602-97-1-0215. Notes 1. This assumption will be relaxed in Section 6 by making this a one-to-many correspondence. Other work <ref> [24] </ref> relaxes this assumption in a many-to-one fashion. 2. Previous naive Bayes formalizations do not include this document length effect. In the most general case, document length should be modeled and parameterized. 3. All three of these data sets are available on the Internet. See http://www.cs.cmu.edu/~textlearning and http://www.research.att.com/~lewis.
Reference: 25. <author> Liere and Tadepalli. </author> <title> Active learning with committees for text categorization. </title> <booktitle> In AAAI-97, </booktitle> <year> 1997. </year>
Reference-contexts: Accuracy results presented below are an average of twenty test/train splits, again randomly holding out 20% of the documents for testing. The `ModApte' train/test split of the Reuters 21578 Distribution 1.0 data set consists of 12902 articles and 90 topic categories from the Reuters newswire. Following several other studies <ref> [14, 25] </ref> we use the 10 most populous classes and build binary classifiers for each class. We use all the words inside the &lt;TEXT&gt; tags, including the title and the dateline, except that we remove the REUTER and &# tags that occur at the top and bottom of every document. <p> Approaches differ in their methods for selecting the unlabeled example to request a label. Three such examples are relevance sampling and uncertainty sampling [19, 22], and a "Query By Committee" approach <ref> [25] </ref>. Several other statistical text classifiers have been used by others in a variety of domains [39, 14, 4] However, naive Bayes has a strong probabilistic foundation for EM, and is more efficient for large data sets.
Reference: 26. <author> Andrew McCallum and Kamal Nigam. </author> <title> A comparison of event models for naive Bayes text classification. </title> <note> In Submitted to AAAI-98 Workshop on Learning for Text Categorization, 1998. http://www.cs.cmu.edu/~mccallum. </note>
Reference-contexts: The above formulation of naive Bayes assumes a generative model that accounts for the number of times a word appears in a document. This is equivalent to a multinomial event model (without the factorial terms that account for event ordering) <ref> [26] </ref>. This formulation has been used by numerous practitioners of naive Bayes text classification [19, 15, 13, 24, 31, 28]. <p> Empirical comparisons show that the multinomial formulation yields higher-accuracy classifiers <ref> [26] </ref>. 8 NIGAM, MCCALLUM, THRUN AND MITCHELL 5. Using EM to Incorporate Unlabeled Data When naive Bayes is given just a small set of labeled training data, classification accuracy will suffer because variance in the parameter estimates of the generative model will be high.
Reference: 27. <author> Andrew McCallum and Kamal Nigam. </author> <title> Employing em in pool-based active learning for text classification. </title> <note> In Submitted to ICML-98, 1998. http://www.cs.cmu.edu~mccallum. </note>
Reference-contexts: Two other learning task formulations could also benefit from using EM: (1) an active learning approach that uses an explicit model of unlabeled data could incorporate EM iterations at every stage to improve its classification, and to better select for which data to request class labels from a labeler <ref> [27] </ref>; (2) an incremental learning algorithm that re-trains throughout the testing phase could use the unlabeled test data received early in the testing phase in order to improve performance on the later test data.
Reference: 28. <author> Andrew McCallum, Ronald Rosenfeld, Tom Mitchell, and Andrew Ng. </author> <title> Improving text clasification by shrinkage in a hierarchy of classes. </title> <note> In Submitted to ICML-98, 1998. http://www.cs.cmu.edu~mccallum. </note>
Reference-contexts: This is equivalent to a multinomial event model (without the factorial terms that account for event ordering) [26]. This formulation has been used by numerous practitioners of naive Bayes text classification <ref> [19, 15, 13, 24, 31, 28] </ref>. However, there is another formulation of naive Bayes text classification that instead assumes a generative model and document representation where each word in the vocabulary is a binary feature, and is modeled by a Bernoulli trial [33, 21, 15, 18, 16].
Reference: 29. <author> G.J. McLachlan and K.E. Basford. </author> <title> Mixture Models. </title> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: If there is no labeled data, unlabeled data cannot improve classification, as shown in [1]. If there is infinite amounts of labeled data, all parameters can be recovered with probability 1 from the labeled data and the resulting classifier is Bayes-optimal <ref> [29] </ref>; thus, further unlabeled data cannot improve the classification accuracy. Note that our argument does not immediately motivate an algorithm for extracting the information from the unlabeled data. Additionally, it not show that better parameter estimation will yield better classification. <p> Likewise, the classification error converges to the Bayes optimal classifier at the same rate. * Infinite unlabeled data. If infinite amounts of unlabeled data are available, however, the parameters of the mixture components can be recovered from the unlabeled data <ref> [29] </ref>, but not the assignment of mixture components to classes. Thus, the estimation problem reduces to the problem of learning a permutation matrix, which assigns labels to the different mixture components.
Reference: 30. <author> David J. Miller and Hasan S. Uyar. </author> <title> A mixture of experts classifier with learning based on both labelled and unlabelled data. </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS 9), </booktitle> <year> 1997. </year>
Reference-contexts: Previous supervised algorithms for learning to classify from text do not incorporate unlabeled data. A similar approach was used by Miller and Uyar <ref> [30] </ref> for non-text data sources. We adapt this approach for the naive Bayes text classifier and conduct a thorough empirical analysis. We also show theoretically that unlabeled data carries information useful for improving parameter estimation under certain restrictive conditions, and survey results that show that this consequently improves classification. <p> LEARNING TO CLASSIFY TEXT FROM LABELED AND UNLABELED DOCUMENTS 19 8. Related Work Two other studies have used EM to combine labeled and unlabeled data for classification <ref> [30, 36] </ref>. Instead of naive Bayes, Shahshahani and Landgrebe use a mixture of Gaussians; Miller and Uyar use Mixtures of Experts. They demonstrate experimental results on non-text data sets with up to 40 features. In contrast, our textual data sets have three orders of magnitude more features.
Reference: 31. <author> Tom M. Mitchell. </author> <title> Machine Learning. </title> <address> WCB/McGraw-Hill, </address> <year> 1997. </year>
Reference-contexts: This is equivalent to a multinomial event model (without the factorial terms that account for event ordering) [26]. This formulation has been used by numerous practitioners of naive Bayes text classification <ref> [19, 15, 13, 24, 31, 28] </ref>. However, there is another formulation of naive Bayes text classification that instead assumes a generative model and document representation where each word in the vocabulary is a binary feature, and is modeled by a Bernoulli trial [33, 21, 15, 18, 16].
Reference: 32. <author> M. J. Pazzani, J. Muramatsu, and D. Billsus. Syskill & Webert: </author> <title> Identifying interesting Web sites. </title> <booktitle> In AAAI-96, </booktitle> <year> 1996. </year>
Reference-contexts: There are statistical text learning algorithms that can be trained to approximately classify documents, given a sufficient set of labeled training examples. These text classification algorithms have been used to automatically catalog news articles [19, 14] and web pages [6], automatically learn the reading interests of users <ref> [32, 17] </ref>, and automatically sort electronic mail [23]. One key difficulty with these current algorithms, and the issue addressed by this paper, is that they require a large, often prohibitive, number of labeled training examples to learn accurately.
Reference: 33. <author> S. E. Robertson and K. Sparck-Jones. </author> <title> Relevance weighting of search terms. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 27 </volume> <pages> 129-146, </pages> <year> 1976. </year>
Reference-contexts: However, there is another formulation of naive Bayes text classification that instead assumes a generative model and document representation where each word in the vocabulary is a binary feature, and is modeled by a Bernoulli trial <ref> [33, 21, 15, 18, 16] </ref>. Empirical comparisons show that the multinomial formulation yields higher-accuracy classifiers [26]. 8 NIGAM, MCCALLUM, THRUN AND MITCHELL 5.
Reference: 34. <author> J. Rocchio. </author> <title> Relevance feedback in information retrieval. In The SMART Retrieval System:Experiments in Automatic Document Processing, </title> <booktitle> chapter 14, </booktitle> <pages> pages 313-323. </pages> <publisher> Prentice Hall, </publisher> <year> 1971. </year>
Reference-contexts: The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio <ref> [35, 34] </ref>, regression models [38], k-nearest-neighbor [39] and Support Vector Machines [14]. EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data.
Reference: 35. <author> G. Salton. </author> <title> Developments in automatic text retrieval. </title> <journal> Science, </journal> <volume> 253 </volume> <pages> 974-979, </pages> <year> 1991. </year>
Reference-contexts: The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio <ref> [35, 34] </ref>, regression models [38], k-nearest-neighbor [39] and Support Vector Machines [14]. EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data.
Reference: 36. <author> B. Shahshahani and D. Landgrebe. </author> <title> The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon. </title> <journal> IEEE Trans. on Geoscience and Remote Sensing, </journal> <volume> 32(5) </volume> <pages> 1087-1095, </pages> <month> Sept </month> <year> 1994. </year>
Reference-contexts: This result, however, assumes that the parameters of the individual mixture components are known; little is known for the more general case, where unlabeled data can be used to estimate those. Shahshahani and Landgrebe <ref> [36] </ref> investigates the utility of unlabeled data in supervised learning, with quite different results. They analyze the convergence rate under the assumption that unbiased estimators are available for , for both the labeled and the unlabeled data. <p> LEARNING TO CLASSIFY TEXT FROM LABELED AND UNLABELED DOCUMENTS 19 8. Related Work Two other studies have used EM to combine labeled and unlabeled data for classification <ref> [30, 36] </ref>. Instead of naive Bayes, Shahshahani and Landgrebe use a mixture of Gaussians; Miller and Uyar use Mixtures of Experts. They demonstrate experimental results on non-text data sets with up to 40 features. In contrast, our textual data sets have three orders of magnitude more features.
Reference: 37. <author> V. Vapnik. </author> <title> Estimations of dependences based on statistical data. </title> <publisher> Springer Publisher, </publisher> <year> 1982. </year>
Reference-contexts: To calculate the probability of a word given a class, w t jc j , simply count the fraction of times that word occurs in the data for that class, and augment this fraction with Bayes optimal smoothing that primes the count for each word with a "pseudo-occurrence" of one <ref> [37] </ref>. This smoothing is sometimes referred to as the Laplacean prior, and is necessary to prevent probability zero probabilities for infrequently occurring words.
Reference: 38. <author> Yiming Yang and Christopher G. Chute. </author> <title> An application of least squares fit mapping to text information retrieval. </title> <booktitle> In Proceedings of the Sixteenth Annual International ACM SIGIR Conference, </booktitle> <year> 1993. </year>
Reference-contexts: The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio [35, 34], regression models <ref> [38] </ref>, k-nearest-neighbor [39] and Support Vector Machines [14]. EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data.
Reference: 39. <author> Yiming Yang and Jan Pederson. </author> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In ICML-97, </booktitle> <pages> pages 412-420, </pages> <year> 1997. </year>
Reference-contexts: The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio [35, 34], regression models [38], k-nearest-neighbor <ref> [39] </ref> and Support Vector Machines [14]. EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data. <p> Documents often fall into overlapping categories. Words within a document are not independent of each other|grammar and topicality ensure this. Despite these violations, empirically, the Naive Bayes classifier does a good job of classifying text documents <ref> [20, 6, 39, 13] </ref>. This paradox is explained by the fact that classification estimation is only a function of the sign (in binary cases) of the function estimation; the function approximation can still be poor while classification accuracy remains high [9, 10]. <p> As done previously [6], we use only the 2000 most informative words, as measured by average mutual information with the class variable. This feature selection method is commonly used for text <ref> [39, 16, 13] </ref>. Accuracy results presented below are an average of twenty test/train splits, again randomly holding out 20% of the documents for testing. The `ModApte' train/test split of the Reuters 21578 Distribution 1.0 data set consists of 12902 articles and 90 topic categories from the Reuters newswire. <p> Approaches differ in their methods for selecting the unlabeled example to request a label. Three such examples are relevance sampling and uncertainty sampling [19, 22], and a "Query By Committee" approach [25]. Several other statistical text classifiers have been used by others in a variety of domains <ref> [39, 14, 4] </ref> However, naive Bayes has a strong probabilistic foundation for EM, and is more efficient for large data sets. The thrust of this paper is to straightforwardly demonstrate the value of unlabeled data; a similar approach could apply unlabeled data to more complex classifiers. 9.
References-found: 39

