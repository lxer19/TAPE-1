URL: http://www.cs.indiana.edu/pub/dswise/FPLA87.ps.Z
Refering-URL: http://www.cs.indiana.edu/pub/dswise/
Root-URL: http://www.cs.indiana.edu
Email: dswise@iuvax.cs.indiana.edu  
Title: MATRIX ALGEBRA AND APPLICATIVE PROGRAMMING* examples are matrix operations (d 2), and a particularly interesting
Author: David S. Wisey 
Keyword: CR categories and Subject Descriptors: C.1.2 [Multiple Data Stream Architectures (Multiprocessors)]: Array and vector processors, Parallel processors; D.1.1 [Applicative (Functional) Programming Techniques]; G.1.3 [Numerical Linear Algebra]: Sparse and very large systems; E.1 [Data Structures]: Trees; F.2.1 [Numerical Algorithms and Problems]: Computation of fast Fourier transform. General Term: Algorithms.  
Note: The major thesis is that 2 d -ary trees should be used to represent d-dimensional arrays;  
Address: 101 Lindley Hall, Bloomington, IN 47405-4101  
Affiliation: Computer Science Department, Indiana University  
Abstract: The broad problem of matrix algebra is taken up from the perspective of functional programming. A key question is how arrays should be represented in order to admit good implementations of well-known efficient algorithms, and whether functional architecture sheds any new light on these or other solutions. It relates directly to disarming the "aggregate update" problem. * c fl1987 by Springer-Verlag. In Gilles Kahn (ed.) Functional Programming Languages and Computer Architecture, Lecture Notes in Computer Science 274, Berlin: Springer (1987) 134-153. The two appendices that are omitted here are available there. y Research reported herein was sponsored, in part, by the National Science Foundation under Grant Number DCR 84-05241. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S. K. Abdali. & D. D. Saunders. </author> <title> Transitive closure and related semiring properties via eliminants. </title> <booktitle> Theoretical Computer Science 40, 2,3 (1985), </booktitle> <pages> 257-274. </pages>
Reference-contexts: Padding with NIL minimizes the space consumed in padding. The matrix is justified to the southeast, rather than the northwest, so to help with computation of eliminants <ref> [1] </ref>. This prescribes a normal form for quadtrees: no scalar entry is ever 0, four quadrants cannot all be NIL, and if the southwest and northeast are NIL then the northwest and southeast cannot be the same scalar. <p> C C C C : (2) This sketch is necessarily different from Knuth's; where he uses the numerator bc, I use cb to provide for multiplication when c and b are matrices. This detail arises whenever multiplication does not commute <ref> [1] </ref>. In this description, the off-pivot entry d is typical of most of the matrix. Particularly in sparse matrices where either b or c is likely to be zero, whole quadrants of values, d, will not change over a Pivot Step; the decorations in those quadrants do not change either.
Reference: 2. <author> P. J. </author> <title> Denning Parallel computing and its evolution. </title> <journal> Comm. ACM 29, </journal> <month> 12 (December, </month> <year> 1986), </year> <pages> 1163-1167. </pages>
Reference-contexts: Many caching strategies fail under parallelism, and bus or switch contention compounds the problem of wait states. Many problems sufficiently large and important enough to justify parallel computation also exhibit sparseness. Therefore, these results are of great interest in designing parallel algorithms and parallel computers <ref> [2, 16] </ref>. Section 4. Arithmetic Algorithms The recursive definition of quaternary trees molds the recursive structure of programs that manipulate them. Moreover, the bifurcation of tree composition leads naturally to more stable algorithms.
Reference: 3. <author> I. S. Duff. </author> <title> A survey of sparse matrix research. </title> <booktitle> Proc. IEEE 65, </booktitle> <month> 4 (April, </month> <year> 1977), </year> <pages> 500-535. </pages>
Reference-contexts: The second originates with McKellar and Coffman [11], who study the storage of submatrices in a demand-paging environment in order to reduce page faults. They arrive at the square-block decomposition, extended to block-specific algorithms by Fischer and Probert [4]. The third is George's nested dissection method <ref> [3] </ref> for matrix problems. A description is given by George and Liu [6]. The last, a recent problem impacted by this representation, is that of updating an aggregate structure in a functional language. <p> The term sparsity for the complement of this quantity is rarely used. <ref> [3, p. 500] </ref>" Rather, he suggests that sparsity of a matrix has as much to do with the distribution of zero elements as with their relative population. His perspective is reflected in analysis of the total space and access-time for familiar kinds of "sparse" matrices, represented as quadtrees [21]. <p> Particularly in sparse matrices where either b or c is likely to be zero, whole quadrants of values, d, will not change over a Pivot Step; the decorations in those quadrants do not change either. In this way, full (total, complete <ref> [3] </ref>) pivoting may be achieved without the cost of an O (n 2 ) search for selecting every pivot. The Pivot Step algorithm is described elsewhere [20]; the code treats each quadrant of each nontrivial, decorated matrix in one of four ways, two of which are presented in Appendix B.
Reference: 4. <author> P. C. Fischer & R. L. Probert. </author> <title> Storage reorganization techniques for matrix computation in a paging environment. </title> <journal> Comm. ACM 22, </journal> <month> 7 (July, </month> <year> 1979), </year> <pages> 405-415. </pages>
Reference-contexts: The second originates with McKellar and Coffman [11], who study the storage of submatrices in a demand-paging environment in order to reduce page faults. They arrive at the square-block decomposition, extended to block-specific algorithms by Fischer and Probert <ref> [4] </ref>. The third is George's nested dissection method [3] for matrix problems. A description is given by George and Liu [6]. The last, a recent problem impacted by this representation, is that of updating an aggregate structure in a functional language.
Reference: 5. <author> D. P. Friedman & D. S. Wise. </author> <title> Aspects of applicative programming for parallel processing. </title> <journal> IEEE Trans. Comput. </journal> <volume> C-27, </volume> <month> 4 (April, </month> <year> 1978), </year> <pages> 289-296. </pages>
Reference-contexts: The Pivot Step algorithm is described elsewhere [20]; the code treats each quadrant of each nontrivial, decorated matrix in one of four ways, two of which are presented in Appendix B. That Daisy <ref> [5, 9] </ref> program specifies data dependencies, which implies some order of evaluation, not entirely apparent at coding time. Because Daisy is entirely lazy, it is difficult to foresee the order of creation for recursively dependent data structures.
Reference: 6. <author> A. George & J. W-H Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <address> Englewood Cliffs, NJ, </address> <note> Prentice-Hall (1981), Chapter 8. </note>
Reference-contexts: They arrive at the square-block decomposition, extended to block-specific algorithms by Fischer and Probert [4]. The third is George's nested dissection method [3] for matrix problems. A description is given by George and Liu <ref> [6] </ref>. The last, a recent problem impacted by this representation, is that of updating an aggregate structure in a functional language. Hudak [7] proposes specific primitives for sequentially stored aggregates, which are avoided by O'Donnell in his tree-like machine [13].
Reference: 7. <author> P. Hudak. </author> <title> Arrays, non-determinism, side-effects, and parallelism: a functional perspective (Extended Abstract). </title> <booktitle> Proc. LANL/MCC Graph Reduction Workshop Sante Fe, </booktitle> <month> September, </month> <year> 1986, </year> <booktitle> Lecture Notes in Computer Science, </booktitle> <address> New York, </address> <note> Springer (to appear). </note>
Reference-contexts: The third is George's nested dissection method [3] for matrix problems. A description is given by George and Liu [6]. The last, a recent problem impacted by this representation, is that of updating an aggregate structure in a functional language. Hudak <ref> [7] </ref> proposes specific primitives for sequentially stored aggregates, which are avoided by O'Donnell in his tree-like machine [13]. Block decomposition moots sequential storage, by providing access and reconstruction (recopying using shared references) in time and space logarithmic, rather than linear, in the order of the structure.
Reference: 8. <author> S. D. Johnson. </author> <title> "Storage Allocation for List Multiprocessing", </title> <institution> Indiana University Computer Science Dept. </institution> <type> Technical Report No. 168, </type> <month> (March, </month> <year> 1985). </year>
Reference-contexts: The coincidence of regular access patterns to regularly allocated arrays, even from regular offsets within different matrices, is likely to become an ever in-creasing problem with more processors. Randomization available from this kind of heap <ref> [8] </ref> would not prevent the first contention between two algorithms, but it would certainly help prevent a first from being immediately followed by more. Thus, the tree structure would allow many coprocessors to run with less memory contention, and to absorb the cost of repeated path traversals.
Reference: 9. <author> S. D. Johnson. </author> <title> Synthesis of Digital Designs from Recursion Equations, </title> <address> Cambridge, MA, </address> <publisher> M.I.T. Press (1984). </publisher>
Reference-contexts: The Pivot Step algorithm is described elsewhere [20]; the code treats each quadrant of each nontrivial, decorated matrix in one of four ways, two of which are presented in Appendix B. That Daisy <ref> [5, 9] </ref> program specifies data dependencies, which implies some order of evaluation, not entirely apparent at coding time. Because Daisy is entirely lazy, it is difficult to foresee the order of creation for recursively dependent data structures. <p> As this idea gains acceptance, the efficient implementation of a multiprocessor heap will become an ever more significant goal [19]. The fast Fourier transform, itself, is an artifact of hardware for many. Yet, Pease's derivation is done without schematics. Is this just another example of synthesizing com-plicated hardware <ref> [9] </ref> through applicative languages? One challenge to this work is that I have only considered examples that are "naturally recursive," or whose block-decomposition proceeds gracefully.
Reference: 10. <author> D. E. Knuth. </author> <title> The Art of Computer Programming, I, Fundamental Algorithms, 2nd Ed., </title> <address> Reading, MA, </address> <publisher> Addison-Wesley (1975), </publisher> <pages> 299-318 + 401, 556. </pages>
Reference-contexts: The former case occurs particularly often with sparse factors, and annihilates the recursion not only of quadrant multiplication, but also of the addition of quadrant-products that follows. Solutions to linear systems and matrix inversion have been reduced to a Pivot Step algorithm <ref> [10] </ref>, where the "independent" problem of a stable choice for the pivot element folds naturally onto the tree [20].
Reference: 11. <author> A. C. McKellar & E. G. Coffman, Jr. </author> <title> Organizing matrices and matrix operations for paged memory systems. </title> <journal> Comm. ACM 12, </journal> <month> 3 (March, </month> <year> 1969), </year> <pages> 153-165. </pages>
Reference-contexts: This enforced block-decomposition representation of matrices recalls several results from the literature. Binary decomposition of vectors is implicit in the fast Fourier transform, and shows up explicitly in Pease's development [15] of a quadtree decomposition of the discrete Fourier transform matrix. The second originates with McKellar and Coffman <ref> [11] </ref>, who study the storage of submatrices in a demand-paging environment in order to reduce page faults. They arrive at the square-block decomposition, extended to block-specific algorithms by Fischer and Probert [4]. The third is George's nested dissection method [3] for matrix problems.
Reference: 12. <author> H. J. Nussbaumber. </author> <title> Fast Fourier Transforms and Convolution Algorithms, </title> <publisher> Berlin, Springer (1982). </publisher>
Reference-contexts: Fast Fourier Transform This section deals explicitly with a vector algorithm, although it derives the algorithm through matrix manipulation|again using quadrant decomposition. Pease [15] derives the Fast Fourier Transform (FFT) in this way from the ordinary Discrete Fourier Transform. His derivation, as well as the ordinary "butterfly" explication <ref> [12] </ref> of this important algo rithm, however, is characterized by separation of necessary permutations from the FFT, itself. After the derivation is presented, the significance of retaining the permutations in the derivation will be discussed.
Reference: 13. <author> J. T. O'Donnell. </author> <title> An architecture that efficiently updates associative aggregates in applicative programming languages. </title> <editor> In Jean-Pierre Jouannaud (ed.), </editor> <booktitle> Functional Programming Languages and Computer Architecture, Lecture Notes in Computer Science 201, </booktitle> <address> Berlin, </address> <publisher> Springer (1985), </publisher> <pages> 164-189. </pages>
Reference-contexts: A description is given by George and Liu [6]. The last, a recent problem impacted by this representation, is that of updating an aggregate structure in a functional language. Hudak [7] proposes specific primitives for sequentially stored aggregates, which are avoided by O'Donnell in his tree-like machine <ref> [13] </ref>. Block decomposition moots sequential storage, by providing access and reconstruction (recopying using shared references) in time and space logarithmic, rather than linear, in the order of the structure. The cost in space becomes irrelevant, however, when storage management on acyclic structures is free [19].
Reference: 14. <author> F. J. Peters. </author> <title> Parallel pivoting algorithms for sparse symmetric matrices. </title> <booktitle> Parallel Computing 1, </booktitle> <month> 1 (August, </month> <year> 1984), </year> <pages> 99-110. </pages>
Reference-contexts: In fact, the test can be made repeatedly for 2 threshold , 4 threshold , etc. With favorable results from purely local testing of decorations, it is possible to repeat local pivots with confidence of maintaining stability. The suggestions in this section are similar to those of Peters <ref> [14] </ref>, who is concerned with identifying multiple pivotings that do not generate mutual processing conflicts. His method is not necessarily stable. This section, rather, locally reveals multiple, stable pivotings within a quadrant, which are then propagated together to synchronize their interdependence. Section 6.
Reference: 15. <author> M. C. Pease. </author> <title> An adaptation of the fast Fourier transform for parallel processing. </title> <journal> J. ACM 15, </journal> <month> 2 (April, </month> <year> 1968), </year> <pages> 252-264. </pages>
Reference-contexts: This enforced block-decomposition representation of matrices recalls several results from the literature. Binary decomposition of vectors is implicit in the fast Fourier transform, and shows up explicitly in Pease's development <ref> [15] </ref> of a quadtree decomposition of the discrete Fourier transform matrix. The second originates with McKellar and Coffman [11], who study the storage of submatrices in a demand-paging environment in order to reduce page faults. They arrive at the square-block decomposition, extended to block-specific algorithms by Fischer and Probert [4]. <p> This section, rather, locally reveals multiple, stable pivotings within a quadrant, which are then propagated together to synchronize their interdependence. Section 6. Fast Fourier Transform This section deals explicitly with a vector algorithm, although it derives the algorithm through matrix manipulation|again using quadrant decomposition. Pease <ref> [15] </ref> derives the Fast Fourier Transform (FFT) in this way from the ordinary Discrete Fourier Transform. His derivation, as well as the ordinary "butterfly" explication [12] of this important algo rithm, however, is characterized by separation of necessary permutations from the FFT, itself. <p> Nothing here improves on him <ref> [15] </ref>, except perhaps the facility of coding when language primitives provide vector bifurcation. He addresses radices other than two and, indeed, much can also be said in favor of ternary and quinary trees when confronted with an FFT of order 360.
Reference: 16. <author> R. Rettberg & R. Thomas. </author> <title> Contention is no obstacle to shared-memory multiprocessing. </title> <journal> Comm. ACM 29, </journal> <month> 12 (December, </month> <year> 1986), </year> <pages> 1202-1212. </pages>
Reference-contexts: Many caching strategies fail under parallelism, and bus or switch contention compounds the problem of wait states. Many problems sufficiently large and important enough to justify parallel computation also exhibit sparseness. Therefore, these results are of great interest in designing parallel algorithms and parallel computers <ref> [2, 16] </ref>. Section 4. Arithmetic Algorithms The recursive definition of quaternary trees molds the recursive structure of programs that manipulate them. Moreover, the bifurcation of tree composition leads naturally to more stable algorithms.
Reference: 17. <author> V. Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numer. Math. </journal> <volume> 13, </volume> <month> 4 (August, </month> <year> 1969), </year> <pages> 354-356. </pages>
Reference-contexts: Matrix multiplication decomposes two ways (again treating the product as two halves), four ways (the four quadrants of the answer), and eight ways (the eight quadrant products in Strassen's decomposition <ref> [17] </ref> of Gaussian matrix multiplication.) Whenever a factor is either NIL or 1, the product is directly available, either as NIL, or as a shared reference to the other factor.
Reference: 18. <author> D. S. Wise. </author> <title> Representing matrices as quadtrees for parallel processors. </title> <note> Information Processing Letters 20 (May, </note> <year> 1985), </year> <pages> 195-199. </pages>
Reference-contexts: In recognition of this need and as a test of the "applicative thesis" (that functional languages and architectures are necessary to accomplish highly parallel performance), a familiar class of problems, matrix algebra, is under study <ref> [18, 20, 21] </ref>, with the goal of developing pure, functional algorithms to mimic the parallel performance of this class of well-studied programs [22]. <p> This is important in floating-point algorithms which, though stable over multiplicative operations, become unstable with addition and subtraction. The algorithm for matrix addition <ref> [18] </ref> and subtraction decomposes naturally into four quadrant additions, separate and independent processes. Because of their mutual independence, these four are naturally computed in parallel within a shared memory, or distributed to independent processors with private memory.
Reference: 19. <author> D. S. Wise. </author> <title> Design for a Multiprocessing Heap with On-Board Reference Counting. </title> <editor> In Jean-Pierre Jouannaud (ed.), </editor> <booktitle> Functional Programming Languages and Computer Architecture, Lecture Notes in Computer Science 201, </booktitle> <address> Berlin, </address> <publisher> Springer (1985), </publisher> <pages> 289-304. </pages>
Reference-contexts: Block decomposition moots sequential storage, by providing access and reconstruction (recopying using shared references) in time and space logarithmic, rather than linear, in the order of the structure. The cost in space becomes irrelevant, however, when storage management on acyclic structures is free <ref> [19] </ref>. Moreover, as we shall see, many of the necessary algorithms distribute naturally across the tree so that any reconstruction is local to a substructure, rather than global over the aggregate. Section 3. <p> Many of the advantages and of the problems (e.g. "hot spots" in memory space) of sequential addressing are mooted by the kind of algorithms described here. As this idea gains acceptance, the efficient implementation of a multiprocessor heap will become an ever more significant goal <ref> [19] </ref>. The fast Fourier transform, itself, is an artifact of hardware for many. Yet, Pease's derivation is done without schematics.
Reference: 20. <author> D. S. Wise. </author> <title> Parallel decomposition of matrix inversion using quadtrees. </title> <booktitle> Proc. 1986 International Conference on Parallel Processing (IEEE Cat. </booktitle> <volume> No. 86CH2355-6), </volume> <pages> 92-99. </pages>
Reference-contexts: In recognition of this need and as a test of the "applicative thesis" (that functional languages and architectures are necessary to accomplish highly parallel performance), a familiar class of problems, matrix algebra, is under study <ref> [18, 20, 21] </ref>, with the goal of developing pure, functional algorithms to mimic the parallel performance of this class of well-studied programs [22]. <p> Inferring the conventional meaning from such a matrix now requires additional information (viz. its order), but we can proceed quite far without size information; it only becomes critical upon Input or Output. One must acknowledge that the I/O conversions are non-trivial algorithms <ref> [20] </ref>, but because they consume little processor resource|and are restrained, also, by communication bandwidth|we eschew them here. Like floating-point number conversions, they are an irritating impediment to one who would experiment with the algorithms discussed below. <p> Density of a particular matrix is the ratio between the space it occupies, and the space occupied by a dense matrix of the same order. Non-sparsity of a particular matrix is the ratio between the expected time to access a random element (path length if considering quadtrees <ref> [20] </ref>), and the expected access time within a dense matrix of the same order. Sparsity is the difference between one and this non-sparsity measure. Both density and sparsity are measured on a scale from zero to one. <p> Solutions to linear systems and matrix inversion have been reduced to a Pivot Step algorithm [10], where the "independent" problem of a stable choice for the pivot element folds naturally onto the tree <ref> [20] </ref>. Each nonterminal node in the quadtree is decorated with a nonnegative number and two bits: the absolute value of the largest uneliminated value in the tree, and an indicator toward the quadrant in which it resides. <p> In this way, full (total, complete [3]) pivoting may be achieved without the cost of an O (n 2 ) search for selecting every pivot. The Pivot Step algorithm is described elsewhere <ref> [20] </ref>; the code treats each quadrant of each nontrivial, decorated matrix in one of four ways, two of which are presented in Appendix B. That Daisy [5, 9] program specifies data dependencies, which implies some order of evaluation, not entirely apparent at coding time. <p> There are two ways that repeated pivots may be dispatched within PIV without sacrificing any stability. One is implicit in the normalized representation of matrices, and one requires a modification on the Pivot Step as described <ref> [20] </ref>. Consider the case where P IV = a, a nonzero scalar, but some of ROW , COL, OFF are not trivial. That is, a turns up to be a scalar at a level above its siblings.
Reference: 21. <author> D. S. Wise & J. Franco. </author> <title> Costs of quadtree representation of sparsely patterned matrices (in preparation.) </title>
Reference-contexts: In recognition of this need and as a test of the "applicative thesis" (that functional languages and architectures are necessary to accomplish highly parallel performance), a familiar class of problems, matrix algebra, is under study <ref> [18, 20, 21] </ref>, with the goal of developing pure, functional algorithms to mimic the parallel performance of this class of well-studied programs [22]. <p> His perspective is reflected in analysis of the total space and access-time for familiar kinds of "sparse" matrices, represented as quadtrees <ref> [21] </ref>. Significant overhead is caused by the nonterminal nodes of a quadtree, a cost that does not appear in conventional, sequential storage of matrices as vectors at contiguous memory addresses. Closed-form results have been computed for total-space and for expected-depth for dense, symmetric, triangular, banded, and permutation matrices. <p> This is remarkably consistent with Duff's observation that patterning is essential to sparseness; the bit-reversal permutation is characterized by its lack of local patterning! Permutation matrices, in general, measure out this badly <ref> [21] </ref> as quadtrees, so it is fortunate that we already prefer an alternative representation that is not dense: as vectors of integers. This measure of sparsity has not been well studied; it is being defined here for the first time.
Reference: 22. <author> M. F. Young. </author> <title> A functional language and modular arithmetic for scientific computing. </title> <editor> In Jean-Pierre Jouannaud (ed.), </editor> <booktitle> Functional Programming Languages and Computer Architecture, Lecture Notes in Computer Science 201, </booktitle> <address> Berlin, </address> <publisher> Springer (1985), </publisher> <pages> 305-318. </pages>
Reference-contexts: a test of the "applicative thesis" (that functional languages and architectures are necessary to accomplish highly parallel performance), a familiar class of problems, matrix algebra, is under study [18, 20, 21], with the goal of developing pure, functional algorithms to mimic the parallel performance of this class of well-studied programs <ref> [22] </ref>. The salient feature of this work is the use of recursive block decomposition to represent all arrays, forcing a rigorously recursive (functional) decomposition of the usual algorithms along the boundaries of naturally arising subtrees.
References-found: 22

