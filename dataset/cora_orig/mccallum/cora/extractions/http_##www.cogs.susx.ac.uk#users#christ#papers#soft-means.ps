URL: http://www.cogs.susx.ac.uk/users/christ/papers/soft-means.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)273 606755 3239  
Title: Unsupervised Learning with the Soft-Means Algorithm  
Author: Chris Thornton 
Keyword: Relevant areas: automated discovery, neural networks  
Address: Brighton BN1 9QN  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: This note describes a useful adaptation of the `peak seeking' regime used in unsupervised learning processes such as competitive learning and `k-means'. The adaptation enables the learning to capture low-order probability effects and thus to more fully capture the probabilistic structure of the training data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Duda, R. and Hart, P. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: 1 Introduction Unsupervised learning involves discovering the underlying structure of a dataset without knowing how the individual data items are classified. This has been recognized as a hard problem in machine learning <ref> [1] </ref>. Many unsupervised learning methods operate by trying to find the `prototypes' of the dataset. The usual approach here is to search for the density peaks in the distribution of training data.
Reference: [2] <author> Van Ryzin, J. </author> <year> (1977). </year> <title> Classification and Clustering. </title> <publisher> London: Academic Press. </publisher>
Reference-contexts: Many unsupervised learning methods operate by trying to find the `prototypes' of the dataset. The usual approach here is to search for the density peaks in the distribution of training data. This can be done by explicit clustering <ref> [2] </ref>, say, or by some iterative method such as competitive learning [3] or k-means clustering [4, 5]. Methods which seek out density peaks effectively sample the probabilistic structure of the data.
Reference: [3] <author> Rumelhart, D. and Zipser, D. </author> <year> (1986). </year> <title> Feature discovery by competitive learning. </title> <editor> In D. Rumelhart, J. McClelland and the PDP Research Group (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructures of Cognition. </booktitle> <volume> Vol I (pp. </volume> <pages> 151-193). </pages> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Many unsupervised learning methods operate by trying to find the `prototypes' of the dataset. The usual approach here is to search for the density peaks in the distribution of training data. This can be done by explicit clustering [2], say, or by some iterative method such as competitive learning <ref> [3] </ref> or k-means clustering [4, 5]. Methods which seek out density peaks effectively sample the probabilistic structure of the data.
Reference: [4] <author> Darken, C. and Moody, J. </author> <year> (1990). </year> <title> Fast adaptive k-means clustering: some empirical results. </title> <booktitle> Proceedings of IJCNN, </booktitle> <address> San Diego. </address>
Reference-contexts: The usual approach here is to search for the density peaks in the distribution of training data. This can be done by explicit clustering [2], say, or by some iterative method such as competitive learning [3] or k-means clustering <ref> [4, 5] </ref>. Methods which seek out density peaks effectively sample the probabilistic structure of the data. However, they are only sensitive to, and can therefore only exploit, the nth-order structure of the data (where n is the number of inputs variables), i.e., the probabilities associated with complete data items.
Reference: [5] <author> Selim, S. and Ismail, M. </author> <year> (1984). </year> <title> K-means-type algorithms: a generalized convergence theorem and characterization of local optimality. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6, No. </volume> <pages> 1 (pp. 81-87). </pages>
Reference-contexts: The usual approach here is to search for the density peaks in the distribution of training data. This can be done by explicit clustering [2], say, or by some iterative method such as competitive learning [3] or k-means clustering <ref> [4, 5] </ref>. Methods which seek out density peaks effectively sample the probabilistic structure of the data. However, they are only sensitive to, and can therefore only exploit, the nth-order structure of the data (where n is the number of inputs variables), i.e., the probabilities associated with complete data items.
Reference: [6] <author> Everitt, B. </author> <year> (1974). </year> <title> Cluster Analysis. </title> <publisher> London: Heinemann. </publisher>
Reference-contexts: The leaf nodes in the dendrogram are the numbers (subscripts) of individual data items and the internal nodes are the maximum distances for items in the relevant cluster. Note the general homogeneity of the structure and, in particular, how all the internal distances <ref> [6] </ref> for the initial clusters are the same. Of course, the apparent absence of density peaks in a dataset does not mean that it has no probabilistic structure.
Reference: [7] <author> Thrun, S., Bala, J., Bloedorn, E., Bratko, I., Cestnik, B., Cheng, J., De Jong, K., Dzeroski, S., Fisher, D., Fahlman, S., Hamann, R., Kaufman, K., 8 Keller, S., Kononenko, I., Kreuziger, J., Michalski, R., Mitchell, T., Pa--chowicz, P., Reich, Y., Vafaie, H., Van de Welde, W., Wenzel, W., Wnek, J. and Zhang, J. </author> <year> (1991). </year> <title> The MONK's problems a performance comparison of different learning algorithms. </title> <institution> CMU-CS-91-197, School of Computer Science, Carnegie-Mellon University. </institution> <month> 9 </month>
Reference-contexts: In many cases, this approach enables the algorithm to produce performance on supervised learning problems comparable to fully supervised algorithms. For example, consider the algorithm's performance on the benchmark problem known as the `third MONKS problem' <ref> [7] </ref>. The underlying rule for this problem is as follows. (attribute_5 = 3 and attribute_4 = 1) or (attribute_5 != 4 and attribute_2 != 3) (with "!=" denoting inequality). <p> Epoch 2 1 -98:83 93:72 83:38 51:35 62:27 69:12 00:99 7 Using the output-generation procedure described above, these produce a classi-fication accuracy on the testing data of 87%. This is comparable to the accuracy produced by several of the supervised algorithms in the original MONKS study. <ref> [7] </ref> 5 Summary The paper has described an enhancement of the `peak-seeking' regime for unsupervised learning. As far as the author has been able to ascertain, this enhancement has not yet been investigated by the community.
References-found: 7

