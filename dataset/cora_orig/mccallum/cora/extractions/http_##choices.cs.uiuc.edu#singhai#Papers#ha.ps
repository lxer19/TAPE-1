URL: http://choices.cs.uiuc.edu/singhai/Papers/ha.ps
Refering-URL: http://choices.cs.uiuc.edu/singhai/Papers/scalr.html
Root-URL: http://www.cs.uiuc.edu
Email: e-mail: ashish.singhai@uiuc.edu  e-mail: -swee.lim,sanjay.radia-@sun.com  
Title: The SunSCALR Framework for Internet Servers  
Author: Ashish Singhai Swee-Boon Lim Sanjay R. Radia 
Address: 1304 W. Springfield Ave., Urbana, IL 61801  901 San Antonio Road, Palo Alto, CA 94303  
Affiliation: Computer Science, University of Illinois  Network Products Group, Sun Microsystems  
Abstract: Internet servers need to be highly-available, inexpensive, and scalable. These goals are often conflicting and most designs meet, with limited success, only few of them. In this paper we describe the SunSCALR framework that achieves these goals by combining proven technologies, careful system design, and engineering trade-offs. It uses a distributed, self-stabilizing algorithm for status monitoring and failure detection, and IP failover for automatic reconfiguration. SunSCALR provides high-availability against message loss, host crashes, and scheduled downtime, and allows on-the-fly addition and removal of hosts. We present detailed performance of SunSCALR. It can provide 10 second failover latency (i.e., better than 99.999% availability if machines fail for 2 hours/month). SunSCALR based products have been in use within Sun and are also available in the market. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson, D. E. Culler, and D. A. Patter-son. </author> <title> A case for NOW (Networks of Workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 54-64, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Also, vertical scaling compromises service availability if the machine fails. Instead, our initial experience showed us that horizontal scaling by introducing additional machines is a better approach. Others have made a similar observation <ref> [1] </ref>. In fact, many popular Web sites today run internet services on a network of servers using off-the-shelf components.
Reference: [2] <author> A. Arora and M. Gouda. </author> <title> Closure and convergence: A foundation of fault-tolerant computing. </title> <journal> IEEE Transactions on Software Engineering, </journal> 19(10) 1015-1027, 1993. 
Reference-contexts: is collection of individual host states. * Consistency predicate P (DB) | P (DB) evaluates to True if DB is consistent, False otherwise. * Convergence Actions CA (DB) | If P (DB) is False then repeated execution of CA (DB) con verges to a state where P (DB) is True <ref> [2] </ref>. * Test Objects T O | These objects monitor local services and maintain the local status. * Monitor Objects M O | These objects implement a protocol for status exchange and leader election. The leader invokes convergence actions if needed. <p> We draw upon the research in the area of fault tolerance in distributed systems. Our work is non-masking as well as self-stabilizing according to Arora and Gouda <ref> [2] </ref>. Arora and Paduska discuss the I-am-alive protocol, its proof, and properties in [3]. Leader election protocols are discussed in [5, 12].
Reference: [3] <author> A. Arora and D. Paduska. </author> <title> A timing-based schema for stabilizing information exchange in networks. </title> <booktitle> In Proceedings of the 3rd International Confence on Computer Networks, </booktitle> <year> 1995. </year>
Reference-contexts: SunSCALR is implemented in software and does not need service or kernel modification. It detects faults using a self-stabilizing distributed algorithm <ref> [3] </ref> and provides high-availability using IP-failover. The self-stabilizing algorithm allows client visible faults but makes them short-lived. Failed services are automatically restarted (possibly on a different machine) using their original address | the clients using the service at the original address continue to be served. <p> The monitors collectively perform failure detection, system reconfiguration, and load-balancing. Also, without special hardware or application modification, we could not make failures completely trans parent. Instead, we opted for a self-stabilizing algorithm <ref> [3] </ref>, which, while allowing client visible faults, minimizes their duration. In the event of a failure, it leads to system reconfiguration so that the system remains available to clients. During the reconfiguration, however, clients may encounter transient failures. <p> Of course, there can be different definitions for balanced load, e.g., bounded variance. CA (DB), invoked in case of load-imbalance by the leader monitor, would migrate load from heavily-loaded to lightly-loaded machines. The Status Exchange Protocol We use the I-am-alive protocol <ref> [3] </ref> for status exchange. Each host mul-ticasts its status on a shared multicast channel once every time unit. If no message is received from a host for h time units then that host is assumed to have failed. <p> If the worst case transmission delay is ffi time units 3 , the global state at each host is current within (h + 1 + ffi) time units. For details and proofs, see <ref> [3] </ref>. The same protocol also serves as a leader election protocol if the host identities are unique and can be linearly ordered. It simply elects, say, the host with smallest identity as the leader. We use control addresses as host identities since they trivially satisfy the uniqueness and ordering requirements. <p> We draw upon the research in the area of fault tolerance in distributed systems. Our work is non-masking as well as self-stabilizing according to Arora and Gouda [2]. Arora and Paduska discuss the I-am-alive protocol, its proof, and properties in <ref> [3] </ref>. Leader election protocols are discussed in [5, 12].
Reference: [4] <author> William E. Baker, Robert W. Horst, David P. Sonnier, and William J. Watson. </author> <title> A flexible servernet-based fault-tolerant architecture. </title> <booktitle> In Proceedings of the 25th Symposium on Fault Tolerant Computing Systems, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Such systems are suitable for data critical services, e.g., a billing database. Such services can be implemented as SunSCALR specializations with service specific recovery routines. Many projects have addressed server availability by modifying network routers or dispatchers [9, 13, 8] or operating systems <ref> [4] </ref>. These solutions provide an extra level of indirection. But this also means an extra level of interpretation | every incoming packet has to be examined and forwarded. This raises many issues: limited scalability; increased latency; limited flexibility; and modified hardware being single point of failure.
Reference: [5] <author> Michael Barborak, Miroslaw Malek, and An-ton Dahbura. </author> <title> The consensus problem in fault-tolerant computing. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 171-220, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: We draw upon the research in the area of fault tolerance in distributed systems. Our work is non-masking as well as self-stabilizing according to Arora and Gouda [2]. Arora and Paduska discuss the I-am-alive protocol, its proof, and properties in [3]. Leader election protocols are discussed in <ref> [5, 12] </ref>.
Reference: [6] <author> T. Brisco. </author> <title> DNS Support for Load Balancing. </title> <institution> Network Working Group, IETF, </institution> <year> 1995. </year> <note> RFC 1794. </note>
Reference-contexts: If it does not get a response, it assumes an interface failure and may shut down the host after sending an alert. Also, monitors can use redundant interfaces to avoid this scenario. Load Balancing We balance load in two ways: using round-robin DNS <ref> [6] </ref> and DNS zone modification. If load increases beyond a high-water mark, the host withdraws itself from the DNS zone, i.e., stops ad vertising itself as a service provider. This initiates a shift of incoming requests to other machines. <p> Long et. al. [17] have also discussed similar experiments to estimate Internet host availability but they do not classify the failures. Early solutions to the availability and load-balancing for cluster based services consisted of round-robin (RR) and load-balancing (LB) extensions to DNS <ref> [6] </ref>. 7 Smith [24] and Kwan et. al. [16] describe 7 Many popular web sites, e.g., www.ncsa.uiuc.edu, home. netscape.com use these schemes. their experiences with RRDNS for long term load--balancing in UK national Web cache and in NCSA Web server, respectively.
Reference: [7] <author> Anawat Chankhunthod, Peter B. Danzig, Chuck Neerdaels, Michael F. Schwartz, and Kurt J. Wor-rel. </author> <title> A hierarchical internet object cache. </title> <booktitle> In Proceedings of the USENIX Annual Technical Conference, </booktitle> <pages> pages 153-163, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Upon cache miss, the hosts fetch the page from the Internet, there is no sharing across caches. Upon failure, the failed Proxy is relocated on another host. The relocated Proxy can use the cache of the new host. The second configuration, like Harvest <ref> [7] </ref>, uses sibling caching. Upon cache miss, the Proxy queries the sibling Proxies before going out to the Internet. Thus, unlike direct sharing via a shared file system, we share caches indirectly using an inter-cache protocol. The third configuration uses partitioned caches (perhaps using a hashing function on the URLs).
Reference: [8] <institution> Cisco Systems Inc. </institution> <note> LocalDirector (Online Product Information), 1997. Available at: http://www. cisco.com/warp/public/751/lodir/. </note>
Reference-contexts: If one host fails, the other substitutes it. Such systems are suitable for data critical services, e.g., a billing database. Such services can be implemented as SunSCALR specializations with service specific recovery routines. Many projects have addressed server availability by modifying network routers or dispatchers <ref> [9, 13, 8] </ref> or operating systems [4]. These solutions provide an extra level of indirection. But this also means an extra level of interpretation | every incoming packet has to be examined and forwarded.
Reference: [9] <author> O. P. Damani, P. Y. Chung, Y. Huang, C. Kin-tala, and Y. M. Wang. ONE-IP: </author> <title> Techniques for hosting a service on a cluster of machines. </title> <journal> Journal of Computer Networks and ISDN Systems, </journal> <volume> 29(8-13):1019-1027, </volume> <month> September </month> <year> 1997. </year>
Reference-contexts: If one host fails, the other substitutes it. Such systems are suitable for data critical services, e.g., a billing database. Such services can be implemented as SunSCALR specializations with service specific recovery routines. Many projects have addressed server availability by modifying network routers or dispatchers <ref> [9, 13, 8] </ref> or operating systems [4]. These solutions provide an extra level of indirection. But this also means an extra level of interpretation | every incoming packet has to be examined and forwarded.
Reference: [10] <author> Peter Druschel and Gaurav Banga. </author> <title> Lazy receiver processing (LRP): A network subsystem architecture for server systems. </title> <booktitle> In Proceedings of the 2nd USENIX Symposium on Operating System Design and Implementation, </booktitle> <volume> volume 30, </volume> <pages> pages 261-276, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Symbols d and m denote the MTTR for these faults, respectively. Availability with respect to these faults is denoted by A d and A m . The factors (parameters) are: message loss profile, host failure rate, and host timeout, called h. Previous studies <ref> [10, 20] </ref> have found that message loss is usually bursty and can be very severe. To account for this, we consider both mild and severe message loss profiles. The metrics are related to the factors as follows. <p> Also, since we focus, instead of link level, on the application level, we can exploit application features (e.g., stateless protocols) to provide cheaper and simpler solutions. Many researchers have discussed load on network servers <ref> [10, 20] </ref>. They have found that applications may experience severe message loss due to operating system bottlenecks. These results motivated us to test our system under prolonged, bursty loss conditions. Kalyanakrishan et. al. [15] discuss experimental results on Web server availability, including a classification of failures.
Reference: [11] <author> M.J. Fischer, N.A. Lynch, and M. Merritt. </author> <title> Easy impossibility proofs for distributed consensus problems. </title> <journal> Distributed Computing, </journal> <volume> 1(1) </volume> <pages> 26-39, </pages> <year> 1986. </year>
Reference-contexts: This prevents new clients from resolving to the orphaned address and minimizes the extra load on the new host because of the orphan. It also complements the automatic de-installation scheme. Duplicate Addresses Since it is not possible to detect reliably host failures in asynchronous, unreliable networks <ref> [11] </ref>, we may detect false failures. If we falsely conclude that a host has failed, we may reassign its service address (es) to other hosts. This will result in more than one hosts serving the same address. Vector messages (Sec. 3.2, expediting convergence) and larger timeouts help avoid duplicates.
Reference: [12] <author> H. Garcia-Molina. </author> <title> Elections in a distributed system. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-31(1):48-59, </volume> <year> 1982. </year>
Reference-contexts: We draw upon the research in the area of fault tolerance in distributed systems. Our work is non-masking as well as self-stabilizing according to Arora and Gouda [2]. Arora and Paduska discuss the I-am-alive protocol, its proof, and properties in [3]. Leader election protocols are discussed in <ref> [5, 12] </ref>.
Reference: [13] <author> International Business Machines. </author> <title> Interactive Network Dispatcher (online product information), </title> <note> 1997. Aavailable at: http://www.ics.raleigh. ibm.com/netdispatch/. </note>
Reference-contexts: If one host fails, the other substitutes it. Such systems are suitable for data critical services, e.g., a billing database. Such services can be implemented as SunSCALR specializations with service specific recovery routines. Many projects have addressed server availability by modifying network routers or dispatchers <ref> [9, 13, 8] </ref> or operating systems [4]. These solutions provide an extra level of indirection. But this also means an extra level of interpretation | every incoming packet has to be examined and forwarded.
Reference: [14] <author> Raj Jain. </author> <title> The Art of Computer System Performance Analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1991. </year>
Reference-contexts: The rand x profile loses a received message with probability x; and the run x=y profile loses bursts of x packets every y messages. The starting point of the burst is uniformly distributed in [1::(y x)]. Experiment Design In the following, we use the terminology of <ref> [14] </ref>. We consider two types of faults: duplicate addresses and unserved addresses. The metrics are: repair time for duplicate address faults called d; and repair time for unserved address fault called m (see Sec. 4.2). Symbols d and m denote the MTTR for these faults, respectively.
Reference: [15] <author> Mahesh Kalyanakrishnan, R. K. Iyer, and J. U. Patel. </author> <title> Reliability of internet hosts : A case study from the end user's perspective. </title> <booktitle> In Proceedings of the Sixth International Conference on Computer Communications and Networks, </booktitle> <pages> pages 418-423. </pages> <publisher> IEEE, </publisher> <month> September </month> <year> 1997. </year>
Reference-contexts: Many researchers have discussed load on network servers [10, 20]. They have found that applications may experience severe message loss due to operating system bottlenecks. These results motivated us to test our system under prolonged, bursty loss conditions. Kalyanakrishan et. al. <ref> [15] </ref> discuss experimental results on Web server availability, including a classification of failures. Long et. al. [17] have also discussed similar experiments to estimate Internet host availability but they do not classify the failures. <p> We have shown with examples (Web caching and News) how application semantics complement IP failover in providing highly available applications. Our availability measurements consider availability at the server end; but they do not measure unavailability due to network problems between the client and the servers (see <ref> [15] </ref> for details). We have shown that the basic SunSCALR framework has negligible overhead. However, in this study, we have not measured the overhead of test objects. While SunSCALR can handle LAN environments well, we have not experimented with WANs.
Reference: [16] <author> T. T. Kwan, R. E. McGrath, and D. A. Reed. </author> <title> NCSA's world wide web server: Design and performance. </title> <journal> IEEE Computer, </journal> <volume> 28(11) </volume> <pages> 68-74, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Early solutions to the availability and load-balancing for cluster based services consisted of round-robin (RR) and load-balancing (LB) extensions to DNS [6]. 7 Smith [24] and Kwan et. al. <ref> [16] </ref> describe 7 Many popular web sites, e.g., www.ncsa.uiuc.edu, home. netscape.com use these schemes. their experiences with RRDNS for long term load--balancing in UK national Web cache and in NCSA Web server, respectively.
Reference: [17] <author> D. Long, A. Muir, and R. Golding. </author> <title> A longitudinal survey of internet host reliability. </title> <booktitle> In Proceedings of the IEEE Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 2-9, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: They have found that applications may experience severe message loss due to operating system bottlenecks. These results motivated us to test our system under prolonged, bursty loss conditions. Kalyanakrishan et. al. [15] discuss experimental results on Web server availability, including a classification of failures. Long et. al. <ref> [17] </ref> have also discussed similar experiments to estimate Internet host availability but they do not classify the failures.
Reference: [18] <author> Friedemann Mattern. </author> <title> Virtual time and global states of distributed systems. </title> <editor> In M. Cosnard et. al., editor, </editor> <booktitle> Proceedings of Workshop on Parallel and Distributed Algorithms, </booktitle> <pages> pages 215-226. </pages> <address> North-Holland/Elsevier, </address> <year> 1989. </year>
Reference-contexts: If, however, each host multicasts its copy of the DB, it will lead to faster propagation of information and faster convergence, especially in the presence of variable delay and message loss. We call this scheme vector messages since it is similar to vector time-stamps <ref> [18] </ref>. Also, it can handle networks that are not broadcast based. These benefits are achieved at the cost of increasing the message size by a factor of n (the size of the service group). For small n, the increased size does not incur much overhead.
Reference: [19] <author> P. Mockapetris. </author> <title> Domain Names Concepts and Facilities. </title> <institution> Information Sciences Institute, University of Southern California, </institution> <month> November </month> <year> 1987. </year> <note> RFC 1034. </note>
Reference-contexts: Name Service As discussed in Sec. 2, service names map to a list of service addresses. Clients use one of the addresses (mostly the first). The monitors can add and remove service addresses from the DNS <ref> [19] </ref>. They automatically add their preferred addresses. Also, when a monitor starts serving an orphaned address, it withdraws the orphan from the domain. This prevents new clients from resolving to the orphaned address and minimizes the extra load on the new host because of the orphan.
Reference: [20] <author> Jeffrey C. Mogul and K. K. Ramakrishnan. </author> <title> Eliminating receive livelock in an interuupt-driven kernel. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 15(3) </volume> <pages> 217-252, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: Symbols d and m denote the MTTR for these faults, respectively. Availability with respect to these faults is denoted by A d and A m . The factors (parameters) are: message loss profile, host failure rate, and host timeout, called h. Previous studies <ref> [10, 20] </ref> have found that message loss is usually bursty and can be very severe. To account for this, we consider both mild and severe message loss profiles. The metrics are related to the factors as follows. <p> Also, since we focus, instead of link level, on the application level, we can exploit application features (e.g., stateless protocols) to provide cheaper and simpler solutions. Many researchers have discussed load on network servers <ref> [10, 20] </ref>. They have found that applications may experience severe message loss due to operating system bottlenecks. These results motivated us to test our system under prolonged, bursty loss conditions. Kalyanakrishan et. al. [15] discuss experimental results on Web server availability, including a classification of failures.
Reference: [21] <author> J. H. Saltzer, D. P. Reed, and D. D. Clark. </author> <title> End-to-end arguments in system design. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(4) </volume> <pages> 277-288, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: In contrast, SunSCALR is cheaper, more flexible, and without any single point of failure. Some of the above systems provide transparent failover of TCP based services. However, it has been argued in the literature <ref> [21] </ref> that just reliable communication does not guarantee application reliability. For example, just TCP failover will not ensure reliable FTP unless the new FTP server also knows the file name and file pointer for the crashed FTP session.
Reference: [22] <author> R. D. Schlichting and F. B. Schneider. </author> <title> Fail-stop processors: An approach to designing fault-tolerant computing systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(3) </volume> <pages> 222-238, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: All hosts in a service group can provide the service associated with that group. A host can belong to multiple service groups and simultaneously provide many services. Hosts or services can completely or partially fail at any time. We assume fail-stop failures <ref> [22] </ref>, i.e., the failed components do not behave maliciously. We do not assume reliable communication (messages may be lost), bounded message delay, or synchronized clocks. The hosts are identified by their network (IP) addresses, called control addresses. Service groups are identified by their logical names, e.g., www.uiuc.edu.
Reference: [23] <author> Michael D. Schroeder, Andrew D. Birrell, Michael Burrows, Hal Murray, Roger M. Needham, Thomas Rodeheffer, Ed Satterthwaite, and Chuck Thacker. Autonet: </author> <title> a high-speed, self-configuring local area network using point-to-point links. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 9(8), </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: The feed acts in master mode to ensure consistent article numbers. This approach may result in transient inconsistency if a failure happens during data update. 7 Related Work Schroeder et. al. have discussed automatic reconfiguration in Autonet <ref> [23] </ref> to provide continuous link connectivity. They use a distributed algorithm to mask failures. However, since their algorithm is not self-stabilizing, it has to artificially delay failure and repair notifications to achieve steady state.
Reference: [24] <author> Neil G. Smith. </author> <title> The UK national web cache: A state of the art report. </title> <booktitle> In Proceedings of the Fifth International World Wide Web Conference, </booktitle> <month> May </month> <year> 1996. </year> <note> Available at: http://www5conf.inria. fr/fich_html/papers/P45/Overview.html. </note>
Reference-contexts: Long et. al. [17] have also discussed similar experiments to estimate Internet host availability but they do not classify the failures. Early solutions to the availability and load-balancing for cluster based services consisted of round-robin (RR) and load-balancing (LB) extensions to DNS [6]. 7 Smith <ref> [24] </ref> and Kwan et. al. [16] describe 7 Many popular web sites, e.g., www.ncsa.uiuc.edu, home. netscape.com use these schemes. their experiences with RRDNS for long term load--balancing in UK national Web cache and in NCSA Web server, respectively.
References-found: 24

