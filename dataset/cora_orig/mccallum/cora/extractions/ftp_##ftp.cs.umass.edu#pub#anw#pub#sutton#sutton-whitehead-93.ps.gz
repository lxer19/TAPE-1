URL: ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/sutton-whitehead-93.ps.gz
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Email: sutton@gte.com swhitehead@gte.com  
Title: Online Learning with Random Representations  
Author: Richard S. Sutton and Steven D. Whitehead 
Address: 40 Sylvan Road Waltham, MA 02254  
Affiliation: GTE Laboratories Incorporated  
Note: Appeared in Proceedings of the Tenth Int. Conf. on Machine Learning, pp. 314-321, Morgan Kaufmann, 1993.  
Abstract: We consider the requirements of online learning|learning which must be done incrementally and in realtime, with the results of learning available soon after each new example is acquired. Despite the abundance of methods for learning from examples, there are few that can be used effectively for online learning, e.g., as components of reinforcement learning systems. Most of these few, including radial basis functions, CMACs, Ko-honen's self-organizing maps, and those developed in this paper, share the same structure. All expand the original input representation into a higher dimensional representation in an unsupervised way, and then map that representation to the final answer using a relatively simple supervised learner, such as a perceptron or LMS rule. Such structures learn very rapidly and reliably, but have been thought either to scale poorly or to require extensive domain knowledge. To the contrary, some researchers (Rosenblatt, 1962; Gallant & Smith, 1987; Kanerva, 1988; Prager & Fallside, 1988) have argued that the expanded representation can be chosen largely at random with good results. The main contribution of this paper is to develop and test this hypothesis. We show that simple random-representation methods can perform as well as nearest-neighbor methods (while being more suited to online learning), and significantly better than backpropagation. We find that the size of the random representation does increase with the dimensionality of the problem, but not unreasonably so, and that the required size can be reduced substantially using unsupervised-learning techniques. Our results suggest that randomness has a useful role to play in online supervised learning and constructive induction. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Albus, J.S. </author> <title> (1981) Brain, Behavior, </title> <journal> and Robotics. Peter-borough, NH: Byte Books. </journal> <volume> Chapter 6, </volume> <pages> pp. 139-179. </pages>
Reference-contexts: We also present results comparing our approach to backpropagation in this paper. 2 Learning with Expanded Representations The most effective and popular methods for online learning are radial-basis-function networks (e.g., Moody & Darken, 1989), CMAC networks <ref> (Albus, 1981) </ref>, and closely related statistical methods such as Parzen windows (e.g., see Duda & Hart, 1973). Other important online methods are self-organizing maps (Kohonen, 1990), STAGGER (Schlimmer & Granger, 1986), and unsupervised classifiers such as COBWEB (Fisher, 1987), among many others.
Reference: <author> Breiman, L., Friedman, J., Olshen, R., Stone, C.J. </author> <title> (1984) Classification and Regression Trees. </title> <address> Belmont, California: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: We say a method is weakly incremental if it requires relatively little additional memory and computation in order to process one additional example. Most classical machine learning and statistical methods do not meet this standard of incrementality. Methods such as ID3 (Quinlan, 1986), CART <ref> (Breiman et al., 1984) </ref>, INDUCE (Michalski, 1983), and MARS (Friedman, 1988) all take a training set as a whole and perform a computationally intensive process that has to be repeated anew each time examples are added to the training set.
Reference: <author> Duda, R.O., Hart, P.E. </author> <title> (1973) Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: We also present results comparing our approach to backpropagation in this paper. 2 Learning with Expanded Representations The most effective and popular methods for online learning are radial-basis-function networks (e.g., Moody & Darken, 1989), CMAC networks (Albus, 1981), and closely related statistical methods such as Parzen windows <ref> (e.g., see Duda & Hart, 1973) </ref>. Other important online methods are self-organizing maps (Kohonen, 1990), STAGGER (Schlimmer & Granger, 1986), and unsupervised classifiers such as COBWEB (Fisher, 1987), among many others. All of these methods are strictly incremental and able to handle non-stationarity.
Reference: <author> Fisher, D.H. </author> <title> (1987) Knowledge acquisition via incremental conceptual clustering. </title> <booktitle> Machine Learning 2: </booktitle> <pages> 139-172. </pages>
Reference-contexts: Other important online methods are self-organizing maps (Kohonen, 1990), STAGGER (Schlimmer & Granger, 1986), and unsupervised classifiers such as COBWEB <ref> (Fisher, 1987) </ref>, among many others. All of these methods are strictly incremental and able to handle non-stationarity. Moreover, all share a similar two-layer structure: The first layer expands the original input representation into a high dimensional feature space, and the second layer maps that expanded representation to the final answer.
Reference: <author> Foldiak, P. </author> <title> (1990) Forming sparse representations by local anti-Hebbian learning. </title> <booktitle> Biological Cybernetics 64: </booktitle> <pages> 165-170. </pages>
Reference: <author> Friedman, J.H. </author> <title> (1988) Multivariate adaptive regression splines. </title> <type> Technical Report 102, </type> <institution> Stanford Univ. Lab. for Computational Statistics. </institution>
Reference-contexts: Most classical machine learning and statistical methods do not meet this standard of incrementality. Methods such as ID3 (Quinlan, 1986), CART (Breiman et al., 1984), INDUCE (Michalski, 1983), and MARS <ref> (Friedman, 1988) </ref> all take a training set as a whole and perform a computationally intensive process that has to be repeated anew each time examples are added to the training set.
Reference: <author> Gallant, S., Smith, D. </author> <title> (1987) Random cells: an idea whose time has come and gone ... and come again? Proceeding of the IEEE International Conference on Neural Networks. </title>
Reference: <author> Hartman, E., Keeler, J.D. </author> <title> (1991) Predicting the future: Advantages of semilocal units. </title> <booktitle> Neural Computation 3: </booktitle> <pages> 566-578. </pages>
Reference-contexts: Other online learning methods such as radial basis functions are known to have great difficulty with irrelevant inputs <ref> (e.g., Hartman and Keeler, 1991) </ref>. As we show next, irrelevant inputs also cause difficulties for nearest-neighbor methods. This experiment was identical to the first experiment (8 relevant input bits, 8 target prototypes, 5000 steps per run, 20 runs) except that it used 22 irrelevant input bits instead of 8.
Reference: <author> Hinton, G.E. </author> <title> (1989) Connectionist learning procedures. </title> <booktitle> Artificial Intelligence 40: </booktitle> <pages> 185-234. </pages>
Reference-contexts: P m k ), f fl is the target value for the current example, and p is the estimated probability that f fl = 1, given by p = 1 + e P m : (5) This learning rule is a version of the perceptron suited to binary classification problems <ref> (Hinton, 1989) </ref>. If the overall problem were instead one of minimizing squared error, then an LMS or normalized LMS rule would be more appropriate (see Section 7).
Reference: <author> Kanerva, P. </author> <title> (1988) Sparse Distributed Memory. </title> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Klopf, A.H., Gose, E. </author> <title> (1963) An evolutionary pattern recognition network. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 15: </journal> <pages> 247-250. </pages>
Reference: <author> Kohonen, T. </author> <title> (1990) The self-organizing map. </title> <booktitle> Proceedings of the IEEE 78: </booktitle> <pages> 1464-1480. </pages>
Reference-contexts: Other important online methods are self-organizing maps <ref> (Kohonen, 1990) </ref>, STAGGER (Schlimmer & Granger, 1986), and unsupervised classifiers such as COBWEB (Fisher, 1987), among many others. All of these methods are strictly incremental and able to handle non-stationarity.
Reference: <author> Lin, L.-J. </author> <title> (1992) Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <booktitle> Machine Learning 8: </booktitle> <pages> 293-322. </pages>
Reference-contexts: However, as this bound is exponential in the dimensionality of the input space, it is far to high to be useful in practice. This problem has been partially overcome in practice by storing old examples and retraining on them <ref> (Lin, 1992) </ref>, or by learning slowly and training extensively (Tesauro, 1992), but it remains a significant limitation.
Reference: <author> Linsker, R. </author> <title> (1988) Self-organization in a perceptual network. </title> <booktitle> Computer 21: </booktitle> <pages> 105-117. </pages>
Reference: <author> Mahadevan, S. </author> <title> (1992) Enhancing transfer in reinforcement learning by building stochastic models of robot actions. </title> <booktitle> Proceedings ML92: </booktitle> <pages> 290-299. </pages>
Reference: <author> Mahadevan, S., Connell, J. </author> <title> (1992) Automatic programming of behavior-based robots using reinforcement learning. </title> <booktitle> Artificial Intelligence: </booktitle> <pages> 311-365. </pages>
Reference: <author> Michalski, </author> <title> R.S. (1983) A theory and methodology of inductive learning. In Machine Learning: An Artificial Intelligence Approach, R.S. </title> <editor> Michalski, J.G. Carbonell, </editor> & <publisher> T.M. </publisher>
Reference-contexts: We say a method is weakly incremental if it requires relatively little additional memory and computation in order to process one additional example. Most classical machine learning and statistical methods do not meet this standard of incrementality. Methods such as ID3 (Quinlan, 1986), CART (Breiman et al., 1984), INDUCE <ref> (Michalski, 1983) </ref>, and MARS (Friedman, 1988) all take a training set as a whole and perform a computationally intensive process that has to be repeated anew each time examples are added to the training set.
Reference: <editor> Mitchell (Eds.). </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moody, J., Darken, C.J. </author> <title> (1989) Fast learning in networks of locally-tuned processing units. </title> <booktitle> Neural Computation 1: </booktitle> <pages> 281-294. </pages>
Reference-contexts: We also present results comparing our approach to backpropagation in this paper. 2 Learning with Expanded Representations The most effective and popular methods for online learning are radial-basis-function networks <ref> (e.g., Moody & Darken, 1989) </ref>, CMAC networks (Albus, 1981), and closely related statistical methods such as Parzen windows (e.g., see Duda & Hart, 1973). Other important online methods are self-organizing maps (Kohonen, 1990), STAGGER (Schlimmer & Granger, 1986), and unsupervised classifiers such as COBWEB (Fisher, 1987), among many others.
Reference: <author> Moore, A.W., Atkeson, </author> <title> C.G. (1992) An investigation of memory-based function approximators for learning control. </title> <publisher> MIT AI Laboratory Techical Report. </publisher>
Reference: <author> Oja, E. </author> <title> (1983) Subspace Methods of Pattern Recognition. </title> <address> Letchworth, Hertfordshire, UK: </address> <publisher> Research Studies Press. </publisher>
Reference: <author> Prager, R.W., Fallside, F. </author> <title> (1988) The modified Kanerva model for automatic speech recognition. </title> <booktitle> Computer Speech and Language 5: </booktitle> <pages> 257-274. </pages>
Reference: <author> Quinlan, J.R. </author> <title> (1986) Induction of decision trees. </title> <booktitle> Machine Learning 1: </booktitle> <pages> 81-106. </pages>
Reference-contexts: We distinguish two degrees of incrementality. We say a method is weakly incremental if it requires relatively little additional memory and computation in order to process one additional example. Most classical machine learning and statistical methods do not meet this standard of incrementality. Methods such as ID3 <ref> (Quinlan, 1986) </ref>, CART (Breiman et al., 1984), INDUCE (Michalski, 1983), and MARS (Friedman, 1988) all take a training set as a whole and perform a computationally intensive process that has to be repeated anew each time examples are added to the training set.
Reference: <author> Rogers, D. </author> <title> (1990) Predicting weather using a genetic memory: a combination of Kanerva's sparse distributed memory and Holland's genetic algorithm. </title> <booktitle> NIPS-2, </booktitle> <pages> pp. 455-464. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rosenblatt, F. </author> <booktitle> (1962) Principles of Neurodynamics. </booktitle> <address> New York: </address> <publisher> Spartan Books. </publisher>
Reference: <author> Rumelhart, D.E., Zipser, D. </author> <title> (1986) Feature discovery by competitive learning. </title> <booktitle> In: Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> Vol. 1, </volume> <pages> 151-193. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Sanger, </author> <title> T.D. (1991) Optimal hidden units for two-layer nonlinear feedforward neural networks. </title> <journal> Int. J. Pattern Recognition and AI 5: </journal> <pages> 545-561. </pages>
Reference: <author> Schlimmer, J.C., Fisher, D. </author> <title> (1986) A case study of incremental concept induction. </title> <booktitle> AAAI-86: </booktitle> <pages> 496-501. </pages>
Reference-contexts: Weakly incremental methods include extended decision tree methods such as ID4 <ref> (Schlimmer & Fisher, 1986) </ref> and ID5R (Utgoff, 1989), and nearest-neighbor and case-based learning methods. However, the memory and (per-example) computational requirements of all of these methods may increase without bound as more examples are seen, 1 which is problematic for long-lived online applications. <p> We say a method is strictly incremental if its requirements for memory and (per-example) computation do not increase with the number of examples. Strictly incremental methods include STAGGER <ref> (Schlimmer & Granger, 1986) </ref> and most connectionist learning methods. Many weakly incremental methods can be converted into strictly incremental variants, e.g., by retaining only a limited number of examples. <p> Other important online methods are self-organizing maps (Kohonen, 1990), STAGGER <ref> (Schlimmer & Granger, 1986) </ref>, and unsupervised classifiers such as COBWEB (Fisher, 1987), among many others. All of these methods are strictly incremental and able to handle non-stationarity.
Reference: <author> Schlimmer, J.C., Granger, R.H., Jr. </author> <title> (1986) Incremental learning from noisy data. </title> <booktitle> Machine Learning 1: </booktitle> <pages> 317-354. </pages>
Reference-contexts: Weakly incremental methods include extended decision tree methods such as ID4 <ref> (Schlimmer & Fisher, 1986) </ref> and ID5R (Utgoff, 1989), and nearest-neighbor and case-based learning methods. However, the memory and (per-example) computational requirements of all of these methods may increase without bound as more examples are seen, 1 which is problematic for long-lived online applications. <p> We say a method is strictly incremental if its requirements for memory and (per-example) computation do not increase with the number of examples. Strictly incremental methods include STAGGER <ref> (Schlimmer & Granger, 1986) </ref> and most connectionist learning methods. Many weakly incremental methods can be converted into strictly incremental variants, e.g., by retaining only a limited number of examples. <p> Other important online methods are self-organizing maps (Kohonen, 1990), STAGGER <ref> (Schlimmer & Granger, 1986) </ref>, and unsupervised classifiers such as COBWEB (Fisher, 1987), among many others. All of these methods are strictly incremental and able to handle non-stationarity.
Reference: <author> Schwartz, D. </author> <title> (1993) ATM scheduling with queuing delay predictions. </title> <booktitle> Proceedings ML-93. </booktitle>
Reference: <author> Tesauro, G. </author> <title> (1992) Practical issues in temporal difference learning. </title> <booktitle> Machine Learning 8: </booktitle> <pages> 257-278. </pages>
Reference-contexts: However, as this bound is exponential in the dimensionality of the input space, it is far to high to be useful in practice. This problem has been partially overcome in practice by storing old examples and retraining on them (Lin, 1992), or by learning slowly and training extensively <ref> (Tesauro, 1992) </ref>, but it remains a significant limitation.
Reference: <author> Uhr, L., Vossler, C. </author> <title> (1961) A pattern recognition program that generates, evaluates and adjusts its own operators. </title> <booktitle> Proc. of the Western J. Computer Conference, </booktitle> <pages> 555-569. </pages>
Reference: <author> Utgoff, P.E. </author> <title> (1989) Incremental induction of decision trees. </title> <booktitle> Machine Learning 4: </booktitle> <pages> 161-186. </pages>
Reference-contexts: Weakly incremental methods include extended decision tree methods such as ID4 (Schlimmer & Fisher, 1986) and ID5R <ref> (Utgoff, 1989) </ref>, and nearest-neighbor and case-based learning methods. However, the memory and (per-example) computational requirements of all of these methods may increase without bound as more examples are seen, 1 which is problematic for long-lived online applications.
References-found: 33

