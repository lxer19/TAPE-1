URL: http://www.neci.nj.nec.com/homepages/pny/papers/ert/ert.ps
Refering-URL: http://www.neci.nj.nec.com/homepages/pny/papers/ert/main.html
Root-URL: 
Title: Towards EM-style Algorithms for a posteriori Optimization of Normal Mixtures  
Author: Eric S. Ristad Peter N. Yianilos 
Keyword: Expectation Maximization (EM), Normal Mixtures, Gaussian Mixtures, Supervised Learning, Maximize Mutual Information (MMI).  
Date: November 14, 1997  
Abstract: Expectation maximization (EM) provides a simple and elegant approach to the problem of optimizing the parameters of a normal mixture on an unlabeled dataset. To accomplish this, EM iteratively reweights the elements of the dataset until a locally optimal normal mixture is obtained. This paper explores the intriguing question of whether such an EM-style algorithm exists for the related and apparently more difficult problem of finding a normal mixture that maximizes the a posteriori class probabilities of a labeled dataset. We expose a fundamental degeneracy in the relationship between a normal mixture and the a posteriori class probability functions that it induces, and use this degeneracy to prove that reweighting a dataset can almost always give rise to a normal mixture that exhibits any desired class function behavior. This establishes that EM-style approaches are sufficiently expressive for a posteriori optimization problems and opens the way to the design of new algorithms for them. fl Eric Sven Ristad is with the Department of Electrical Engineering and Computer Science University of Illinois at Chicago (email: ristad@eecs.uic.edu), and is partially supported by Young Investigator Award IRI-0258517 from the National Science Foundation. Peter N. Yianilos is with the NEC Research Institute, 4 Independence Way, Princeton, NJ 08540 (email pny@research.nj.nec.com). An earlier version of the paper was distributed as a technical report entitled "On the Strange a Posteriori degeneracy of Normal Mixtures, and Related Reparameterization Theorems", and appeared in the second author's thesis [8]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. E. Baum and J. E. Eagon, </author> <title> An inequality with application to statistical estimation for probabalistic functions of a Markov process and to models for ecology, </title> <journal> Bull. AMS, </journal> <volume> 73 (1967), </volume> <pages> pp. 360-363. </pages>
Reference-contexts: The problem of finding a k-component normal mixture M that maximizes the likelihood Q of an unlabeled dataset s 1 ; : : : ; s n may be approached using the well-known expectation maximization (EM) algorithm <ref> [1, 3, 7] </ref>. EM iteratively re-weights each sample's membership in each of the k mixture components by the posteriori probability of the components given the sample. Normal mixtures are also applied to pattern classification problems.
Reference: [2] <author> P. F. Brown, </author> <title> Acoustic-phonetic modeling problem in automatic speech recognition, </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1987. </year>
Reference-contexts: Each constituent normal density is referred to as a component of the mixture, and m 1 ; : : : ; m k are the mixing coefficients. Normal mixtures have proven useful in several areas including pattern recognition [4] and speech recognition <ref> [6, 2, 5] </ref> along with vector quantization and many others. The problem of finding a k-component normal mixture M that maximizes the likelihood Q of an unlabeled dataset s 1 ; : : : ; s n may be approached using the well-known expectation maximization (EM) algorithm [1, 3, 7]. <p> Here the goal is to optimize the mixture's a posteriori performance, that is to predict the correct labels, not model the observation vectors themselves. This is sometimes called the maximum mutual information (MMI) criterion in the speech recognition literature <ref> [2] </ref> and may be viewed as probabilistic supervised learning. A naive algorithm for this problem segregates the data elements by class, constructs the maximum- likelihood normal density for each class, and combines the resulting densities to form a mixture.
Reference: [3] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin, </author> <title> Maximum-likelihood from incomplete data via the EM algorithm, </title> <journal> J. Royal Statistical Society Ser. B (methodological), </journal> <volume> 39 (1977), </volume> <pages> pp. 1-38. </pages>
Reference-contexts: The problem of finding a k-component normal mixture M that maximizes the likelihood Q of an unlabeled dataset s 1 ; : : : ; s n may be approached using the well-known expectation maximization (EM) algorithm <ref> [1, 3, 7] </ref>. EM iteratively re-weights each sample's membership in each of the k mixture components by the posteriori probability of the components given the sample. Normal mixtures are also applied to pattern classification problems.
Reference: [4] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1973. </year>
Reference-contexts: Each constituent normal density is referred to as a component of the mixture, and m 1 ; : : : ; m k are the mixing coefficients. Normal mixtures have proven useful in several areas including pattern recognition <ref> [4] </ref> and speech recognition [6, 2, 5] along with vector quantization and many others. <p> A convenient visualization of class functions, focuses on decision boundaries, i.e. the surfaces along which classification is ambiguous. Imagery like ours in figure 2, and pages 28-31 of <ref> [4] </ref>, suggest an intuitive relationship between mixture component locations, and the resulting a posteriori class structure and decision surfaces. One imagines each mean to be asserting ownership over some volume of space surrounding it.
Reference: [5] <author> X. D. Huang, Y. Ariki, and M. A. Jack, </author> <title> Hidden Markov Models for Speech Recognition, </title> <publisher> Edinburgh University Press, </publisher> <year> 1990. </year>
Reference-contexts: Each constituent normal density is referred to as a component of the mixture, and m 1 ; : : : ; m k are the mixing coefficients. Normal mixtures have proven useful in several areas including pattern recognition [4] and speech recognition <ref> [6, 2, 5] </ref> along with vector quantization and many others. The problem of finding a k-component normal mixture M that maximizes the likelihood Q of an unlabeled dataset s 1 ; : : : ; s n may be approached using the well-known expectation maximization (EM) algorithm [1, 3, 7].
Reference: [6] <author> L. R. Rabiner, B. H. Juang, S. E. Levinson, and M. M. Sondhi, </author> <title> Recognition of isolated digits using hidden markov models with continuous mixture densities, </title> <journal> AT&T Technical Journal, </journal> <year> (1985). </year>
Reference-contexts: Each constituent normal density is referred to as a component of the mixture, and m 1 ; : : : ; m k are the mixing coefficients. Normal mixtures have proven useful in several areas including pattern recognition [4] and speech recognition <ref> [6, 2, 5] </ref> along with vector quantization and many others. The problem of finding a k-component normal mixture M that maximizes the likelihood Q of an unlabeled dataset s 1 ; : : : ; s n may be approached using the well-known expectation maximization (EM) algorithm [1, 3, 7].
Reference: [7] <author> R. A. Redner and H. F. Walker, </author> <title> Mixture densities, maximum likelihood, and the EM al-gorithm, </title> <journal> SIAM Review, </journal> <volume> 26 (1984), </volume> <pages> pp. 195-239. </pages>
Reference-contexts: The problem of finding a k-component normal mixture M that maximizes the likelihood Q of an unlabeled dataset s 1 ; : : : ; s n may be approached using the well-known expectation maximization (EM) algorithm <ref> [1, 3, 7] </ref>. EM iteratively re-weights each sample's membership in each of the k mixture components by the posteriori probability of the components given the sample. Normal mixtures are also applied to pattern classification problems.
Reference: [8] <author> P. N. Yianilos, </author> <title> Topics in Computational Hidden State Modeling, </title> <type> PhD thesis, </type> <institution> Princeton University, Computer Science Department, Princeton, NJ, </institution> <month> June </month> <year> 1997. </year> <month> 16 </month>
References-found: 8

