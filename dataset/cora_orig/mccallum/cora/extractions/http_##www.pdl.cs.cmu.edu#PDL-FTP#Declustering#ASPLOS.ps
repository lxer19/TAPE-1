URL: http://www.pdl.cs.cmu.edu/PDL-FTP/Declustering/ASPLOS.ps
Refering-URL: http://www.pdl.cs.cmu.edu/PDL-FTP/RAID/ComputingSurveys.biblio.html
Root-URL: 
Title: Parity Declustering for Continuous Operation in Redundant Disk Arrays  
Abstract: We describe and evaluate a strategy for declustering the parity encoding in a redundant disk array. This declustered parity organization balances cost against data reliability and performance during failure recovery. It is targeted at highly-available parity-based arrays for use in continuous-operation systems. It improves on standard parity organizations by reducing the additional load on surviving disks during the reconstruction of a failed disks contents. This yields higher user throughput during recovery, and/or shorter recovery time. We first address the generalized parity layout problem, basing our solution on balanced incomplete and complete block designs. A software implementation of declustering is then evaluated using a disk array simulator under a highly concurrent workload comprised of small user accesses. We show that declustered parity penalizes user response time while a disk is being repaired (before and during its recovery) less than comparable non-declustered (RAID5) organizations without any penalty to user response time in the fault-free state. We then show that previously proposed modifications to a simple, single-sweep reconstruction algorithm further decrease user response times during recovery, but, contrary to previous suggestions, the inclusion of these modifications may, for many configurations, also slow the reconstruction process. This result arises from the simple model of disk access performance used in previous work, which did not consider throughput variations due to positioning delays. 
Abstract-found: 1
Intro-found: 1
Reference: [Bitton88] <author> D. Bitton and J. Gray, </author> <title> Disk Shadowing, </title> <booktitle> Proceedings of the 14th Conference on Very Large Data Bases, </booktitle> <year> 1988, </year> <pages> pp. 331-338. </pages>
Reference-contexts: Most single-failure-correcting disk arrays employ either mirrored or parity-encoded redundancy. In mirroring <ref> [Bitton88, Copeland89, Hsiao90] </ref>, one or more duplicate copies of all data are stored on separate disks.
Reference: [Chen90a] <author> P. Chen, et. al., </author> <title> An Evaluation of Redundant Arrays of Disks using an Amdahl 5890, </title> <booktitle> Proceedings of the ACM SIG-METRICS Conference, </booktitle> <year> 1990, </year> <pages> pp. 74-85. </pages>
Reference-contexts: Mirrored systems, while potentially able to deliver higher throughput than parity-based systems for some workloads <ref> [Chen90a, Gray90] </ref>, increase cost by consuming much more disk capacity for redundancy. In this Mark Holland Department of Electrical and Computer Engineering Carnegie Mellon University Pittsburgh, PA 15213-3890 holland@ece.cmu.edu Garth A. <p> This exception is the result of an optimization that the RAID striping driver employs when requests for one data unit are applied to parity stripes containing only three stripe units <ref> [Chen90a] </ref>.
Reference: [Chen90b] <author> P. Chen and D. Patterson, </author> <title> Maximizing Performance in a Striped Disk Array, </title> <booktitle> Proceedings of ACM SIGARCH Conference, </booktitle> <year> 1990, </year> <pages> pp. 322-331. </pages>
Reference-contexts: A single-failure-correcting redundant disk array consists of a set of disks, a mapping of user data to these disks that yields high throughput <ref> [Chen90b] </ref>, and a mapping of a parity encoding for the arrays data such that data lost when a disk fails can be recovered without taking the system off-line [Lee91]. Most single-failure-correcting disk arrays employ either mirrored or parity-encoded redundancy. <p> The simulation environment We acquired an event-driven disk-array simulator called raidSim <ref> [Chen90b, Lee91] </ref> for our analyses. The simulator was developed for the RAID project at U.C. Berkeley [Katz89]. It consists of four primary components, which are illustrated in Figure 5-1. Each component is described below.
Reference: [Chervenak91] <author> A. Chervenak and R. Katz, </author> <title> Performance of a RAID Prototype, </title> <booktitle> Proceedings of the ACM SIGMETRICS Conference, </booktitle> <year> 1991, </year> <pages> pp. 188-197. </pages>
Reference-contexts: To broaden our results, we intend to explore disk arrays with different stripe unit sizes and user workload characteristics. One important concern that neither our nor Muntz and Luis work considers is the impact of CPU overhead and architectural bottlenecks in the reconstructing system <ref> [Chervenak91] </ref>. For greater control of the reconstruction process we intend to implement throttling of reconstruction and/or user workload as well as a exible prioritization scheme that reduces user response time degradation without starving reconstruction.
Reference: [Copeland89] <author> G. Copeland and T. Keller, </author> <title> A Comparison of High-Availability Media Recovery Techniques, </title> <booktitle> Proceedings of the ACM SIGMOD Conference, </booktitle> <year> 1989, </year> <pages> pp. 98-109. </pages>
Reference-contexts: Most single-failure-correcting disk arrays employ either mirrored or parity-encoded redundancy. In mirroring <ref> [Bitton88, Copeland89, Hsiao90] </ref>, one or more duplicate copies of all data are stored on separate disks. <p> Muntz and Lui use the term clustered where we use the term declustered. Their use may be derived from clustering independent RAIDs into a single array with the same parity overhead. Our use follows the earlier work of Copeland and Keller <ref> [Copeland89] </ref> where redundancy information is declustered over more than the minimal collection of disks. i, and Pi represents the parity unit for parity stripe i. <p> This paper provides analyses upon which these decisions can be based. 3. Related work The idea of improving failure-mode performance by declustering redundancy information originated with mirrored systems <ref> [Copeland89, Hsiao90] </ref>. Copeland and Keller describe a scheme called interleaved declustering which treats primary and secondary data copies differently. Traditionally, mirrored systems allocate one disk as a primary and another as a secondary. Copeland and Keller instead allocate only half of each disk for primary copies.
Reference: [Dibble90] <author> P. Dibble, </author> <title> A Parallel Interleaved File System, </title> <institution> University of Rochester, </institution> <type> Technical Report 334, </type> <year> 1990. </year>
Reference: [Geist87] <author> R. Geist and S. Daniel, </author> <title> A Continuum of Disk Scheduling Algorithms, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 5(1), </volume> <year> 1987, </year> <pages> pp. 77-92. </pages>
Reference-contexts: Head Scheduling: CVSCAN <ref> [Geist87] </ref> Parity Stripe Size: 3, 4, 5, 6, 10, 14, 18, and 21 stripe units Parity Overhead: 33, 25, 20, 17, 10, 7, 6, and 5%, resp.
Reference: [Gibson91] <author> G. Gibson, </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage, </title> <type> Ph.D. Dissertation, </type> <institution> University of Califor-nia, </institution> <note> UCB/CSD 91/613, 1991. Also to be published by MIT Press. </note>
Reference-contexts: Most single-failure-correcting disk arrays employ either mirrored or parity-encoded redundancy. In mirroring [Bitton88, Copeland89, Hsiao90], one or more duplicate copies of all data are stored on separate disks. In parity encoding <ref> [Kim86, Patterson88, Gibson91] </ref>, popularized as Redundant Arrays of Inexpensive Disks (RAID), some subset of the physical blocks in the array are used to store a single-error-correction code (usually parity) computed over subsets of the data.
Reference: [Gibson93] <author> G. Gibson and D. Patterson, </author> <title> Designing Disk Arrays for High Data Reliability, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> to appear in January, </note> <year> 1993. </year>
Reference-contexts: In arrays in which groups of disks have a common failure mode, such as power or data cabling, this criteria should be extended to prohibit the allocation of stripe units from one parity stripe to two or more disks shar ing that common failure mode <ref> [Schulze89, Gibson93] </ref>. 2. Distributed reconstruction. When any disk fails, its user workload should be evenly distributed across all other disks in the array. When replaced or repaired, its reconstruction workload should also be evenly distrib uted. 3. Distributed parity. Parity information should be evenly distributed across the array. <p> Highly available disk arrays require short repair times to assure high data reliability because the mean time until data loss is inversely proportional to mean repair time <ref> [Patterson88, Gibson93] </ref>. However, minimal reconstruction time occurs when user access is denied during reconstruction. Because this cannot take less than the three minutes it takes to read all sectors on our disks, and usually takes much longer, continuous-operation systems require data availability during reconstruction.
Reference: [Gray90] <author> G. Gray, B. Horst, and M. Walker, </author> <title> Parity Striping of Disc Arrays: Low-Cost Reliable Storage with Acceptable Throughput, </title> <booktitle> Proceedings of the 16th Conference on Very Large Data Bases, </booktitle> <year> 1990, </year> <pages> pp. 148-160. </pages>
Reference-contexts: Mirrored systems, while potentially able to deliver higher throughput than parity-based systems for some workloads <ref> [Chen90a, Gray90] </ref>, increase cost by consuming much more disk capacity for redundancy. In this Mark Holland Department of Electrical and Computer Engineering Carnegie Mellon University Pittsburgh, PA 15213-3890 holland@ece.cmu.edu Garth A.
Reference: [Hall86] <author> M. Hall, </author> <title> Combinatorial Theory (2nd Edition), </title> <publisher> Wiley-Interscience, </publisher> <year> 1986. </year>
Reference-contexts: This will be achieved if the number of times that a pair of disks contain stripe units from the same parity stripe is constant across all pairs of disks. Muntz and Lui recognized and suggested that such layouts might be found in the literature for balanced incomplete block designs <ref> [Hall86] </ref>. This paper demonstrates that this can be done and one way to do it. <p> In addition to the exorbitant memory requirement for this table, the layout will not meet our distributed parity or distributed reconstruction criteria because even large disks rarely have more than a few million sectors. For this reason we turn to the theory of balanced incomplete block designs <ref> [Hall86] </ref>. Our goal, then, is to find a small block design on C objects with a tuple size of G. This is a difficult problem for general C and G.
Reference: [Holland92] <author> M. Holland and G. Gibson, </author> <title> Parity Declustering for Continuous Operation in Redundant Disk Arrays, </title> <institution> Carnegie Mel-lon University School of Computer Science Technical Report CMU-CS-92-130, </institution> <year> 1992. </year>
Reference-contexts: All of our results indicate that the performance of an array is not highly sensitive to such small variations in a. The block designs we used in our simulations are given in <ref> [Holland92] </ref>. 4. These block designs (and others) are available via anonymous ftp from niagara.nectar.cs.cmu.edu (128.2.250.200) in the file /usr0/anon/pub/Declustering/BD_database.tar.Z.
Reference: [Hsiao90] <author> H.-I. Hsiao and D. DeWitt, </author> <title> Chained Declustering: A New Availability Strategy for MultiProcessor Database Machines, </title> <booktitle> Proceedings of the 6th International Data Engineering Conference, </booktitle> <year> 1990, </year> <pages> pp. 18-28. </pages>
Reference-contexts: Most single-failure-correcting disk arrays employ either mirrored or parity-encoded redundancy. In mirroring <ref> [Bitton88, Copeland89, Hsiao90] </ref>, one or more duplicate copies of all data are stored on separate disks. <p> This paper provides analyses upon which these decisions can be based. 3. Related work The idea of improving failure-mode performance by declustering redundancy information originated with mirrored systems <ref> [Copeland89, Hsiao90] </ref>. Copeland and Keller describe a scheme called interleaved declustering which treats primary and secondary data copies differently. Traditionally, mirrored systems allocate one disk as a primary and another as a secondary. Copeland and Keller instead allocate only half of each disk for primary copies.
Reference: [IBM0661] <author> IBM Corporation, </author> <title> IBM 0661 Disk Drive Product Description, Model 370, First Edition, Low End Storage Products, </title> <type> 504/114-2, </type> <year> 1989. </year>
Reference-contexts: Table 5-1 (c) shows the characteristics of the 314 MB, 3 1/2 inch diameter IBM 0661 Model 370 (Lightning) disks on which our simulations are based <ref> [IBM0661] </ref>. At the lowest level of abstraction in raidSim is an event-driven simulator, which is invoked to cause simulated time to pass. The striping driver code was originally taken directly from the Sprite source code, with essentially zero modification to accommodate simulation, and all our modifications conform to Sprite constraints.
Reference: [Katz89] <author> R. Katz, et. al., </author> <title> A Project on High Performance I/O Subsystems, </title> <journal> ACM Computer Architecture News, </journal> <volume> Vol. 17(5), </volume> <year> 1989, </year> <pages> pp. 24-31. </pages>
Reference-contexts: The simulation environment We acquired an event-driven disk-array simulator called raidSim [Chen90b, Lee91] for our analyses. The simulator was developed for the RAID project at U.C. Berkeley <ref> [Katz89] </ref>. It consists of four primary components, which are illustrated in Figure 5-1. Each component is described below. At the top level of abstraction is a synthetic reference generator, which is capable of producing user request streams drawn from a variety of distributions.
Reference: [Kim86] <author> M. Kim, </author> <title> Synchronized Disk Interleaving, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 35 (11), </volume> <year> 1986, </year> <pages> pp. 978-988. </pages>
Reference-contexts: We do not recommend on-line failure recovery in an environment that can tolerate off-line recovery because the latter restores high performance and high data reliability more quickly. Redundant disk arrays, proposed for increasing input/- output performance and for reducing the cost of high data reliability <ref> [Kim86, Livny87, Patterson88, Salem86] </ref>, also offer an opportunity to achieve high data availability without sacrificing throughput goals. <p> Most single-failure-correcting disk arrays employ either mirrored or parity-encoded redundancy. In mirroring [Bitton88, Copeland89, Hsiao90], one or more duplicate copies of all data are stored on separate disks. In parity encoding <ref> [Kim86, Patterson88, Gibson91] </ref>, popularized as Redundant Arrays of Inexpensive Disks (RAID), some subset of the physical blocks in the array are used to store a single-error-correction code (usually parity) computed over subsets of the data.
Reference: [Lee90] <author> E. Lee, </author> <title> Software and Performance Issues in the Implementation of a RAID Prototype, </title> <institution> University of California, </institution> <type> Technical Report UCB/CSD 90/573, </type> <year> 1990. </year>
Reference-contexts: We then comment on the generality of our approach. 4.1. Layout goals Previous work on declustered parity has left open the problem of allocating parity stripes in an array. Extending from non-declustered parity layout research <ref> [Lee90, Dib-ble90] </ref>, we have identified six criteria for a good parity layout. The first four of these deal exclusively with relationships between stripe units and parity stripe membership, while the last two make recommendations for the rela tionship between user data allocation and parity stripe organization.
Reference: [Lee91] <author> E. Lee and R. Katz, </author> <title> Performance Consequences of Parity Placement in Disk Arrays, </title> <booktitle> Proceedings of ASPLOS-IV, </booktitle> <year> 1991, </year> <pages> pp. 190-199. </pages>
Reference-contexts: single-failure-correcting redundant disk array consists of a set of disks, a mapping of user data to these disks that yields high throughput [Chen90b], and a mapping of a parity encoding for the arrays data such that data lost when a disk fails can be recovered without taking the system off-line <ref> [Lee91] </ref>. Most single-failure-correcting disk arrays employ either mirrored or parity-encoded redundancy. In mirroring [Bitton88, Copeland89, Hsiao90], one or more duplicate copies of all data are stored on separate disks. <p> Section 8 then covers reconstruction performance, contrasting single-thread and parallel reconstruction, and evaluating alternative reconstruction algorithms. Section 9 concludes the paper with a look at interesting topics for future work. 2. The declustered parity layout policy left-symmetric RAID5 redundant disk array <ref> [Lee91] </ref>. A data stripe unit, or simply a data unit is defined as the minimum amount of contiguous user data allocated to one disk before any data is allocated to any other disk. <p> The simulation environment We acquired an event-driven disk-array simulator called raidSim <ref> [Chen90b, Lee91] </ref> for our analyses. The simulator was developed for the RAID project at U.C. Berkeley [Katz89]. It consists of four primary components, which are illustrated in Figure 5-1. Each component is described below.
Reference: [Livny87] <author> M. Livny, S. Khoshafian, and H. Boral, </author> <title> Multi-Disk Management Algorithms, </title> <booktitle> Proceedings of the ACM SIGMETRICS Conference, </booktitle> <year> 1987, </year> <pages> pp. 69-77. </pages>
Reference-contexts: We do not recommend on-line failure recovery in an environment that can tolerate off-line recovery because the latter restores high performance and high data reliability more quickly. Redundant disk arrays, proposed for increasing input/- output performance and for reducing the cost of high data reliability <ref> [Kim86, Livny87, Patterson88, Salem86] </ref>, also offer an opportunity to achieve high data availability without sacrificing throughput goals.
Reference: [Meador89] <author> W. Meador, </author> <title> Disk Array Systems, </title> <booktitle> Proceedings of COMPCON, </booktitle> <year> 1989, </year> <pages> pp. 143-146. </pages>
Reference: [Menon92] <author> J. Menon and J. Kasson, </author> <title> Methods for Improved Update Performance of Disk Arrays, </title> <booktitle> Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <year> 1992, </year> <pages> pp. 74-83. </pages>
Reference-contexts: Because of this high cost for user writes, our system is not able to sustain 378 user writes of 4 KB per second (this would be 72 4 KB accesses per second per disk). Although using four separate disk accesses for every user write is pessimistic <ref> [Patterson88, Menon92, Rosenblum91] </ref>, it gives our results wider generality than would result from simulating specifically optimized systems. Figures 6-1 and 6-2 show that, except for writes with a = 0.1, fault-free performance is essentially independent of parity declustering.
Reference: [Muntz90] <author> R. Muntz and J. Lui, </author> <title> Performance Analysis of Disk Arrays Under Failure, </title> <booktitle> Proceedings of the 16th Conference on Very Large Data Bases, </booktitle> <year> 1990, </year> <pages> pp. 162-173. </pages>
Reference-contexts: Proceedings of the 5th Conference on Architectural Support for Programming Languages and Operating Systems, 1992. paper, we examine a parity-based redundancy scheme called parity declustering, which provides better performance during on-line failure recovery than more common RAID schemes, without the high capacity overhead of mirroring <ref> [Muntz90] </ref> 1 . Our primary figures of merit in this paper are reconstruction time, which is the wallclock time taken to reconstruct the contents of a failed disk after replacement, and user response time during reconstruction. <p> Section 2 of this paper describes our terminology and presents the declustered parity organization. Section 3 describes related studies, notably the introduction of declus-tering by Muntz and Lui <ref> [Muntz90] </ref>, and explains the motivations behind our investigation. Section 4 presents our parity mapping, which was left as an open problem by Muntz and Lui. <p> One perspective on the concept of parity declustering in redundant disk arrays is demonstrated in Figure 2-2; a logical RAID5 array with G = 4 is distributed over C = 7 &gt; G disks, each containing fewer units. (The way this 2. Muntz and Lui <ref> [Muntz90] </ref> use the term group to denote what we call a parity stripe, but we avoid this usage as it conicts with the Patterson, et. al. definition [Patterson88] as a set of disks, rather than a set of disk blocks. P0 symmetric RAID5 organization. <p> Reconstruction performance The primary purpose for employing parity decluster-ing over RAID5 organizations has been given as the desire to support higher user performance during recovery and shorter recovery periods <ref> [Muntz90] </ref>. In this section we show this to be effective, although we also show that previously proposed optimizations do not always improve reconstruction performance. We also demonstrate that there remains an important trade-off between higher performance during recovery and shorter recovery periods. <p> disk because free reconstruction moves the heads around randomly. 8.3 Comparison to analytic model Muntz and Lui also proposed an analytic expression for reconstruction time in an array employing declustered Simple Algorithms Write Replacement Optimizations Write Replacement Time Read Surviving Disks Read Surviving Disks piggybacking on the reconstruction cycle. parity <ref> [Muntz90] </ref>. Figure 8-7 shows our best attempt to reconcile their model with our simulations. <p> Previous work proposing declustered parity mappings for redundant disk arrays has suggested, without implementation, the use of block designs, has proposed two optimizations to a simple sweep reconstruction algorithm, and has analytically modeled the resultant expected reconstruction time <ref> [Muntz90] </ref>. In addition to extending their proposal to an implementation, our work evaluates their optimizations and reconstruction time models in the context of our software implementation running on a disk-accurate simulator.
Reference: [Ousterhout88] <author> J. Ousterhout, et. al., </author> <title> The Sprite Network Operating System, </title> <booktitle> IEEE Computer, </booktitle> <month> February </month> <year> 1988, </year> <pages> pp. 23-36. </pages>
Reference-contexts: We have restricted our attention to random accesses of size 4 KB to model an OLTP system with an effective buffer cache [Ramakrishnan92]. Each request produced by this generator is sent to a RAID striping driver, which was originally the actual code used by the Sprite operating system <ref> [Ousterhout88] </ref> to implement a RAID device on a set of independent disks. Table 5-1 (b) shows the configuration of our extended version of this striping driver. These upper two levels of raidSim should actually run on a Sprite machine.
Reference: [Patterson88] <author> D. Patterson, G. Gibson, and R. Katz, </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proceedings of the ACM SIGMOD Conference, </booktitle> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: We do not recommend on-line failure recovery in an environment that can tolerate off-line recovery because the latter restores high performance and high data reliability more quickly. Redundant disk arrays, proposed for increasing input/- output performance and for reducing the cost of high data reliability <ref> [Kim86, Livny87, Patterson88, Salem86] </ref>, also offer an opportunity to achieve high data availability without sacrificing throughput goals. <p> Most single-failure-correcting disk arrays employ either mirrored or parity-encoded redundancy. In mirroring [Bitton88, Copeland89, Hsiao90], one or more duplicate copies of all data are stored on separate disks. In parity encoding <ref> [Kim86, Patterson88, Gibson91] </ref>, popularized as Redundant Arrays of Inexpensive Disks (RAID), some subset of the physical blocks in the array are used to store a single-error-correction code (usually parity) computed over subsets of the data. <p> Muntz and Lui [Muntz90] use the term group to denote what we call a parity stripe, but we avoid this usage as it conicts with the Patterson, et. al. definition <ref> [Patterson88] </ref> as a set of disks, rather than a set of disk blocks. P0 symmetric RAID5 organization. <p> Because of this high cost for user writes, our system is not able to sustain 378 user writes of 4 KB per second (this would be 72 4 KB accesses per second per disk). Although using four separate disk accesses for every user write is pessimistic <ref> [Patterson88, Menon92, Rosenblum91] </ref>, it gives our results wider generality than would result from simulating specifically optimized systems. Figures 6-1 and 6-2 show that, except for writes with a = 0.1, fault-free performance is essentially independent of parity declustering. <p> Highly available disk arrays require short repair times to assure high data reliability because the mean time until data loss is inversely proportional to mean repair time <ref> [Patterson88, Gibson93] </ref>. However, minimal reconstruction time occurs when user access is denied during reconstruction. Because this cannot take less than the three minutes it takes to read all sectors on our disks, and usually takes much longer, continuous-operation systems require data availability during reconstruction.
Reference: [Ramakrishnan92] <author> K. Ramakrishnan, P. Biswas, and R. Karedla, </author> <title> Analysis of File I/O Traces in Commercial Computing Environments, </title> <booktitle> Proceedings of the ACM SIGMETRICS Conference, </booktitle> <year> 1992, </year> <pages> pp. 78-90. </pages>
Reference-contexts: Table 5-1 (a) shows the configuration of the workload generator used in our simulations. We have restricted our attention to random accesses of size 4 KB to model an OLTP system with an effective buffer cache <ref> [Ramakrishnan92] </ref>. Each request produced by this generator is sent to a RAID striping driver, which was originally the actual code used by the Sprite operating system [Ousterhout88] to implement a RAID device on a set of independent disks.
Reference: [Reddy91] <author> A. Reddy and P. Bannerjee, </author> <title> Gracefully Degradable Disk Arrays, </title> <booktitle> Proceedings of FTCS-21, </booktitle> <year> 1991, </year> <pages> pp. 401-408. </pages>
Reference-contexts: In this paper we will use balanced incomplete and complete block designs (described in Section 4.2) to achieve better performance during reconstruction. Reddy <ref> [Reddy91] </ref> has also used block designs to improve the recovery-mode performance of an array. His approach generates a layout with properties similar to ours, but is restricted to the case where G = C/2. 4.
Reference: [Rosenblum91] <author> M. Rosenblum and J. Ousterhout, </author> <title> The Design and Implementation of a Log-Structured File System, </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <year> 1991, </year> <pages> pp. 1-15. </pages>
Reference-contexts: Because of this high cost for user writes, our system is not able to sustain 378 user writes of 4 KB per second (this would be 72 4 KB accesses per second per disk). Although using four separate disk accesses for every user write is pessimistic <ref> [Patterson88, Menon92, Rosenblum91] </ref>, it gives our results wider generality than would result from simulating specifically optimized systems. Figures 6-1 and 6-2 show that, except for writes with a = 0.1, fault-free performance is essentially independent of parity declustering.
Reference: [Rudeseal92] <author> A. Rudeseal, </author> <title> private communication, </title> <year> 1992. </year>
Reference-contexts: The addition of piggybacking of writes to the redirection of reads algorithm 8. For comparison, RAID products available today specify on-line reconstruction time to be in the range of one to four hours <ref> [Mea-dor89, Rudeseal92] </ref>. 9. Each reconstruction process acquires the identifier of the next stripe to be reconstructed off of a synchronized list, reconstructs that stripe, and then repeats the process. time: 50% reads, 50% writes.
Reference: [Salem86] <author> K. Salem, H. Garcia-Molina, </author> <title> Disk Striping, </title> <booktitle> Proceedings of the 2nd IEEE Conference on Data Engineering, </booktitle> <year> 1986. </year>
Reference-contexts: We do not recommend on-line failure recovery in an environment that can tolerate off-line recovery because the latter restores high performance and high data reliability more quickly. Redundant disk arrays, proposed for increasing input/- output performance and for reducing the cost of high data reliability <ref> [Kim86, Livny87, Patterson88, Salem86] </ref>, also offer an opportunity to achieve high data availability without sacrificing throughput goals.
Reference: [Schulze89] <author> M. Schulze, G. Gibson, R. Katz, and D. Patterson, </author> <title> How Reliable is a RAID?, </title> <booktitle> Proceedings of COMPCON, </booktitle> <year> 1989, </year> <pages> pp. 118-123. </pages>
Reference-contexts: In arrays in which groups of disks have a common failure mode, such as power or data cabling, this criteria should be extended to prohibit the allocation of stripe units from one parity stripe to two or more disks shar ing that common failure mode <ref> [Schulze89, Gibson93] </ref>. 2. Distributed reconstruction. When any disk fails, its user workload should be evenly distributed across all other disks in the array. When replaced or repaired, its reconstruction workload should also be evenly distrib uted. 3. Distributed parity. Parity information should be evenly distributed across the array.
Reference: [TPCA89] <author> The TPC-A Benchmark: </author> <title> A Standard Specification, Transaction Processing Performance Council, </title> <year> 1989. </year>
Reference-contexts: Response time is important to all customers and critical in database and on-line transaction-processing (OLTP) systems. The specifications for OLTP systems often require some minimal level of responsiveness, for example, one benchmark requires that 90% of the transactions must complete in under two seconds <ref> [TPCA89] </ref>. In a continuous-operation system that requires minutes to hours for the recovery of a failed disk, this rule will apply even during these relatively rare recovery intervals. Our analysis reports on user response time during recovery and presents a simple scheme trading off reconstruction time for user response time.
References-found: 31

