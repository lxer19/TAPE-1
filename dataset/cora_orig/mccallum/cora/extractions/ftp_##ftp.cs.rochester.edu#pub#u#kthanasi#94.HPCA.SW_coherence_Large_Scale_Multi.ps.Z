URL: ftp://ftp.cs.rochester.edu/pub/u/kthanasi/94.HPCA.SW_coherence_Large_Scale_Multi.ps.Z
Refering-URL: http://www.cs.rochester.edu/research/cashmere/97-12_DEC_report/bib.html
Root-URL: 
Email: fkthanasi,scottg@cs.rochester.edu  
Title: Software Cache Coherence for Large Scale Multiprocessors  
Author: Leonidas I. Kontothanassis and Michael L. Scott 
Address: Rochester, Rochester, NY 14627-0226  
Affiliation: Department of Computer Science, University of  
Abstract: Shared memory is an appealing abstraction for parallel programming. It must be implemented with caches in order to perform well, however, and caches require a coherence mechanism to ensure that processors reference current data. Hardware coherence mechanisms for large-scale machines are complex and costly, but existing software mechanisms for message-passing machines have not provided a performance-competitive solution. We claim that an intermediate hardware optionmemory-mapped network interfaces that support a global physical address spacecan provide most of the performance benefits of hardware cache coherence. We present a software coherence protocol that runs on this class of machines and greatly narrows the performance gap between hardware and software coherence. We compare the performance of the protocol to that of existing software and hardware alternatives and evaluate the tradeoffs among various cache-write policies. We also observe that simple program changes can greatly improve performance. For the programs in our test suite and with the changes in place, software coherence is often faster and never more than 55% slower than hardware coherence. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal and others. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <editor> In M. Dubois and S. S. Thakkar, editors, </editor> <booktitle> Scalable Shared Memory Multiprocessors, </booktitle> <pages> pages 239-261. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others [4, 11], but is substantially harder to achieve on large-scale multiprocessors <ref> [1, 15, 19] </ref>. It increases both the cost of the machine and the time and intellectual effort required to bring it to market. Given the speed of advances in microprocessor technology, long development times generally lead to machines with out-of-date processors.
Reference: [2] <author> S. G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Sor computes the steady state temperature of a metal sheet using a banded parallelization of red-black successive overrelaxation on a 640 fi 640 grid. Fft computes an one-dimensional FFT on a 65536-element array of complex numbers, using the algorithm described in <ref> [2] </ref>. Mp3d and water are part of the SPLASH suite [24]. Mp3d is a wind-tunnel airflow simulation. We simulated 40000 particles for 10 steps in our studies. Water is a molecular dynamics simulation computing inter- and intra-molecule forces for a set of water molecules.
Reference: [3] <author> T. E. Anderson, H. M. Levy, B. N. Bershad, and E. D. Lazowska. </author> <title> The Interaction of Architecture and Operating System Design. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: To avoid the complexities of instruction-level simulation of interrupt handlers, we assume a constant overhead for page faults. Table 1 summarizes the default parameters used both in our hardware and software coherence simulations, which are in agreement with those published in <ref> [3] </ref> and in several hardware manuals. Some of the transactions required by our coherence protocols require a collection of the operations shown in table 1 and therefore incur the aggregate cost of their constituents.
Reference: [4] <author> J. Archibald and J. Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> Novem--ber </month> <year> 1986. </year>
Reference-contexts: To perform well, however, shared memory requires the use of caches, which in turn require a coherence mechanism to ensure that copies of data are up-to-date. Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others <ref> [4, 11] </ref>, but is substantially harder to achieve on large-scale multiprocessors [1, 15, 19]. It increases both the cost of the machine and the time and intellectual effort required to bring it to market.
Reference: [5] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: We simulated 40000 particles for 10 steps in our studies. Water is a molecular dynamics simulation computing inter- and intra-molecule forces for a set of water molecules. We used 256 molecules and 3 times steps. Finally appbt is from the NASA parallel benchmarks suite <ref> [5] </ref>. It computes an approximation to Navier-Stokes equations. It was translated to shared memory from the original message-based form by Doug Burger and Sanjay Mehta at the University of Wisconsin.
Reference: [6] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Machines in this non-cache-coherent, nonuniform memory access (NCC-NUMA) class include the Cray Research T3D and the Princeton Shrimp <ref> [6] </ref>. In comparison to hardware-coherent machines, NCC-NUMAs can more easily be built from commodity parts, and can follow improvements in microprocessors and other hardware technologies closely. We present a software coherence protocol for NCC-NUMA machines that scales well to large numbers of processors.
Reference: [7] <author> W. J. Bolosky, M. L. Scott, R. P. Fitzgerald, R. J. Fowler, and A. L. Cox. </author> <title> NUMA Policies and Their Relation to Memory Architecture. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 212-221, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Our use of remote reference to reduce the overhead of coherence management can also be found in work on NUMA memory management <ref> [7, 10, 17] </ref>. However relaxed consistency greatly reduces the opportunities for profitable remote data reference. In fact, early experiments we have conducted with on-line NUMA policies and relaxed consistency have failed badly in their attempt to determine when to use remote reference.
Reference: [8] <author> Y. Chen and A. Veidenbaum. </author> <title> An Effective Write Policy for Software Coherence Schemes. </title> <booktitle> In Proceedings Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: We ran our experiments (see section 4.2) under three different assumptions: write-through caches, write-back caches with per-word hardware dirty bits in the cache, and write-through caches with a write-merge buffer <ref> [8] </ref> that hangs onto recently-written lines 2 Under the same principle a write page-fault on an unmapped page will take the page to the shared state. <p> Specifically, we compare the performance obtained with a write-through cache, a write-back cache, and a write-through cache with a buffer for merging writes <ref> [8] </ref>. The policy is applied on only shared data. Private data uses a write-back policy by default. Write-back caches impose the minimum load on the memory and network, since they write blocks back only on evictions, or when explicitly flushed. <p> A write completes when it is acknowledged by the memory system. With a large amount of write traffic we may have simply replaced waiting for the write-back with waiting for missing acknowledgments. Write-through caches with a write-merge buffer <ref> [8] </ref> employ a small (16 entries in our case) fully associative buffer between the cache and the interconnection network. The buffer merges writes to the same cache line, and allocates a new entry for a write to a non-resident cache line.
Reference: [9] <author> H. Cheong and A. V. Veidenbaum. </author> <title> Compiler-Directed Cache Management in Multiprocessors. </title> <journal> Computer, </journal> <volume> 23(6) </volume> <pages> 39-47, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: (in conjunction with the DARPA Research in Information Science and TechnologyHigh Performance Computing, Software Science and Technology program, ARPA Order no. 8930). 1 We are speaking here of behavior-driven coherencemechanisms that move and replicate data at run time in response to observed patterns of program behavioras opposed to compiler-based techniques <ref> [9] </ref>. Unfortunately, the current state of the art in software coherence for message-passing machines provides performance nowhere close to that of hardware cache coherence. <p> Coherence for distributed memory with per-processor caches can also be maintained entirely by a compiler <ref> [9] </ref>. Under this approach the compiler inserts the appropriate cache flush and invalidation instructions in the code, to enforce data consistency.
Reference: [10] <author> A. L. Cox and R. J. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: The protocol employs a distributed, non-replicated directory data structure that maintains cacheability and sharing information, similar to the coherent map data structure of PLATINUM <ref> [10] </ref>. A page can be in one of the following four states: Uncached No processor has a mapping to the page. This is the initial state for all pages. Shared One or more processors have read-only map pings to the page. <p> Our use of remote reference to reduce the overhead of coherence management can also be found in work on NUMA memory management <ref> [7, 10, 17] </ref>. However relaxed consistency greatly reduces the opportunities for profitable remote data reference. In fact, early experiments we have conducted with on-line NUMA policies and relaxed consistency have failed badly in their attempt to determine when to use remote reference.
Reference: [11] <author> S. J. Eggers and R. H. Katz. </author> <title> Evaluation of the Performance of Four Snooping Cache Coherency Protocols. </title> <booktitle> In Proceedings of the Sixteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 2-15, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: To perform well, however, shared memory requires the use of caches, which in turn require a coherence mechanism to ensure that copies of data are up-to-date. Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others <ref> [4, 11] </ref>, but is substantially harder to achieve on large-scale multiprocessors [1, 15, 19]. It increases both the cost of the machine and the time and intellectual effort required to bring it to market.
Reference: [12] <author> M. D. Hill and J. R. Larus. </author> <title> Cache Considerations for Multiprocessor Programmers. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 97-102, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Gauss enjoys the greatest improvement due to these changes, though noticeable improvements occur in water, appbt and mp3d as well. Data structure alignment and padding is a well-known means of reducing false sharing <ref> [12] </ref>. Since coherence blocks in software coherent systems are large (4K bytes in our case), it is unreasonable to require padding of data structures to that size. However we can often pad data structures to subpage boundaries so that a collection of them will fit exactly in a page.
Reference: [13] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the Nineteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <address> Gold Coast, Aus-tralia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: We have also examined architectural alternatives and program-structuring issues that were not addressed by Petersen and Li. Our work resembles Munin [26] and lazy release consistency <ref> [13] </ref> in its use of delayed write notices, but we take advantage of the globally accessible physical address space for cache fills and write-through, and for access to the coherent map and the local weak lists.
Reference: [14] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <institution> COMP TR93-214, Department of Computer Science, Rice University, </institution> <month> Novem-ber </month> <year> 1993. </year>
Reference-contexts: As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin [26], Treadmarks <ref> [14] </ref>, and the work of Petersen and Li [20, 21], we allow more than one processor to write a page concurrently, and we use a variant of release consistency to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data in their caches.) As in
Reference: [15] <author> Kendall Square Research. </author> <title> KSR1 Principles of Operation. </title> <address> Waltham MA, </address> <year> 1992. </year>
Reference-contexts: Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others [4, 11], but is substantially harder to achieve on large-scale multiprocessors <ref> [1, 15, 19] </ref>. It increases both the cost of the machine and the time and intellectual effort required to bring it to market. Given the speed of advances in microprocessor technology, long development times generally lead to machines with out-of-date processors.
Reference: [16] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The FLASH Multiprocessor. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: In fact, we believe that many of the performance-enhancing features of our software coherence protocol could be of use in programmable cache controllers, such as those being developed by the Flash <ref> [16] </ref> and Typhoon [22] projects. It is not yet clear whether such controllers will be cost effective, but they offer the opportunity to combine many of the advantages of both hardware and software coherence: small coherence blocks and concurrent execution of very complicated protocols.
Reference: [17] <author> R. P. LaRowe Jr. and C. S. Ellis. </author> <title> Experimental Comparison of Memory Management Policies for NUMA Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(4) </volume> <pages> 319-363, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Our use of remote reference to reduce the overhead of coherence management can also be found in work on NUMA memory management <ref> [7, 10, 17] </ref>. However relaxed consistency greatly reduces the opportunities for profitable remote data reference. In fact, early experiments we have conducted with on-line NUMA policies and relaxed consistency have failed badly in their attempt to determine when to use remote reference.
Reference: [18] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: improved, with more major restructuring of access to the space cell data structure, but this would require effort out of keeping with the current study. 4.4 Hardware v. software coherence Figures 7 and 8 compare the performance of our best software protocol to that of a relaxed-consistency DASH-like hardware protocol <ref> [18] </ref> on 16 and 64 processors respectively. The unit line in the graphs represents the running time of each application under a sequentially consistent hardware coherence protocol. In all cases the performance of the software protocol is within 55% 4 of the relaxed consistency hardware protocol.
Reference: [19] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stan-ford Dash Multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others [4, 11], but is substantially harder to achieve on large-scale multiprocessors <ref> [1, 15, 19] </ref>. It increases both the cost of the machine and the time and intellectual effort required to bring it to market. Given the speed of advances in microprocessor technology, long development times generally lead to machines with out-of-date processors. <p> In fact, early experiments we have conducted with on-line NUMA policies and relaxed consistency have failed badly in their attempt to determine when to use remote reference. On the hardware side our work bears resemblance to the Stanford Dash project <ref> [19] </ref> in the use of a relaxed consistency model, and to the Georgia Tech Beehive project [23] in the use of relaxed consistency and per-word dirty bits for successful merging of inconsistent cache lines.
Reference: [20] <author> K. Petersen and K. Li. </author> <title> Cache Coherence for Shared Memory Multiprocessors Based on Virtual Memory Support. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <address> Newport Beach, CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin [26], Treadmarks [14], and the work of Petersen and Li <ref> [20, 21] </ref>, we allow more than one processor to write a page concurrently, and we use a variant of release consistency to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data in their caches.) As in the work of Petersen and Li, we exploit <p> Use of a distributed coherent map and per-processor weak lists enhances scalability by minimizing memory contention and by avoiding the need for processors at acquire points to scan weak list entries in which they have no interest (something that would happen with a centralized weak list <ref> [20] </ref>. However it may make the transition to the weak state very expensive, since a potentially large number of remote memory operations may have to be performed (serially) in order to notify all sharing processors. <p> Finally, in section 4.4, we compare the best of the software results to the corresponding results on sequentially-consistent and release-consistent hardware. 4.1 Software coherence protocol alternatives This section compares our software protocol (presented in section 2) to the protocol devised by Petersen and Li <ref> [20] </ref> (modified to distribute the centralized weak list among the memories of the machine), and to a sequentially consistent page-based cache coherence protocol. <p> This protocol is a straightforward extension of the one proposed by Petersen and Li <ref> [20] </ref> to large scale distributed memory machines. <p> In water, the centralized relaxed consistency protocols are badly beaten by the sequentially consistent software protocol. This agrees to some extent with the results reported by Petersen and Li <ref> [20] </ref>, but the advantage of the sequentially consistent protocol was less pronounced in their work. We believe there are two reasons for our difference in results. First, we have restructured the code to greatly reduce false sharing, thus removing one of the advantages that relaxed consistency has over sequential consistency. <p> We have also run experiments (not shown here) varying several of the architectural constants in out simulations. In all cases software cache coherence maintained performance comparable to that of the hardware alternatives. 5 Related Work Our work is most closely related to that of Petersen and Li <ref> [20] </ref>: we both use the notion of weak pages, and purge caches on acquire operations.
Reference: [21] <author> K. Petersen and K. Li. </author> <title> An Evaluation of Multiprocessor Cache Coherence Based on Virtual Memory Support. </title> <booktitle> In Proceedings of the Eighth International Parallel Processing Symposium, </booktitle> <pages> pages 158-164, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: We present performance results in section 4 and compare our work to other approaches in section 5. We compare our protocol to a variety of existing alternatives, including sequentially-consistent hardware, release-consistent hardware, straightforward sequentially-consistent software, and a coherence scheme for small-scale NCC-NUMAs due to Petersen and Li <ref> [21] </ref>. We also report on the impact of several architectural alternatives on the effectiveness of software coherence. <p> As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin [26], Treadmarks [14], and the work of Petersen and Li <ref> [20, 21] </ref>, we allow more than one processor to write a page concurrently, and we use a variant of release consistency to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data in their caches.) As in the work of Petersen and Li, we exploit
Reference: [22] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-level Shared-Memory. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: In fact, we believe that many of the performance-enhancing features of our software coherence protocol could be of use in programmable cache controllers, such as those being developed by the Flash [16] and Typhoon <ref> [22] </ref> projects. It is not yet clear whether such controllers will be cost effective, but they offer the opportunity to combine many of the advantages of both hardware and software coherence: small coherence blocks and concurrent execution of very complicated protocols.
Reference: [23] <author> G. Shah and U. Ramachandran. </author> <title> Towards Exploiting the Architectural Features of Beehive. </title> <institution> GIT-CC-91/51, College of Computing, Georgia Institute of Technology, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: On the hardware side our work bears resemblance to the Stanford Dash project [19] in the use of a relaxed consistency model, and to the Georgia Tech Beehive project <ref> [23] </ref> in the use of relaxed consistency and per-word dirty bits for successful merging of inconsistent cache lines.
Reference: [24] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Fft computes an one-dimensional FFT on a 65536-element array of complex numbers, using the algorithm described in [2]. Mp3d and water are part of the SPLASH suite <ref> [24] </ref>. Mp3d is a wind-tunnel airflow simulation. We simulated 40000 particles for 10 steps in our studies. Water is a molecular dynamics simulation computing inter- and intra-molecule forces for a set of water molecules. We used 256 molecules and 3 times steps. <p> We were unable to establish the access pattern of appbt from the source code; it uses linear arrays to represent higher dimensional data structures and the computation of offsets often uses several levels of indirection. Mp3d <ref> [24] </ref> has very wide-spread sharing. We modified the program slightly (prior to the current studies) to ensure that colliding molecules belong with high probability to either the same processor or neighboring processors. Therefore the molecule data structure exhibits limited pairwise sharing. The main problem is the space cell data structure.
Reference: [25] <author> J. E. Veenstra and R. J. Fowler. Mint: </author> <title> A Front End for Efficient Simulation of Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Second International Workshop on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS '94), </booktitle> <pages> pages 201-207, </pages> <address> Durham, NC, </address> <month> January - February </month> <year> 1994. </year>
Reference-contexts: Our simulator consists of two parts: a front end, Mint <ref> [25] </ref>, that simulates the execution of the processors, and a back end that simulates the memory system. The front end is the same in all our experiments. It implements the MIPS II instruction set.
Reference: [26] <author> W. Zwaenepoel, J. Bennett, J. Carter, and P. Keleher. Munin: </author> <title> Distributed Shared Memory Using Multi-Protocol Release Consistency. </title> <booktitle> Newsletter of the IEEE Computer Society Technical Committee on Operating Systems and Application Environments, </booktitle> <address> 5(4):11, </address> <month> Winter </month> <year> 1991. </year>
Reference-contexts: We summarize our findings and conclude in section 6. 2 The Software Coherence Protocol In this section we present a scalable protocol for software cache coherence. As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin <ref> [26] </ref>, Treadmarks [14], and the work of Petersen and Li [20, 21], we allow more than one processor to write a page concurrently, and we use a variant of release consistency to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data in their caches.) <p> Delayed write notices were introduced in the Munin distributed shared memory system <ref> [26] </ref>, which runs on networks of workstations and communicates solely via messages. Though the relative values of constants are quite different, experiments indicate (see section 4) that delayed transitions are generally beneficial in our environment as well. <p> We have also examined architectural alternatives and program-structuring issues that were not addressed by Petersen and Li. Our work resembles Munin <ref> [26] </ref> and lazy release consistency [13] in its use of delayed write notices, but we take advantage of the globally accessible physical address space for cache fills and write-through, and for access to the coherent map and the local weak lists.
References-found: 26

