URL: http://iacoma.cs.uiuc.edu/iacoma-papers/numarc.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Email: zzhang,torrella@csrd.uiuc.edu  
Title: Reducing Remote Conflict Misses: NUMA with Remote Cache versus COMA 1  
Author: Zheng Zhang and Josep Torrellas 
Web: http://www.csrd.uiuc.edu/iacoma/  
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: Many future applications for scalable shared-memory multiprocessors are likely to have large working sets that overflow secondary or tertiary caches. Two possible solutions to this problem are to add a very large cache called remote cache that caches remote data (NUMA-RC), or organize the machine as a cache-only memory architecture (COMA). This paper tries to determine which solution is best. To compare the performance of the two organizations for the same amount of total memory, we introduce a model of data sharing. The model uses three data sharing patterns: replication, read-mostly migration, and read-write migration. Replication data is accessed in read-mostly mode by several processors, while migration data is accessed largely by one processor at a time. For large working sets, the weight of the migration data largely determines whether COMA outperforms NUMA-RC. Ideally, COMA only needs to fit the replication data in its extra memory; the migration data will simply be swapped between attraction memories. The remote cache of NUMA-RC, instead, needs to house both the replication and the migration data. However, simulations of seven Splash2 applications show that COMA does not outperform NUMA-RC. This is due to two reasons. First, the extra memory added has more associativity in NUMA-RC than in COMA and, therefore, can be utilized better by the working set in NUMA-RC. Second, COMA memory accesses are more expensive. Of course, our results are affected by the applications used, which have been optimized for a cache-coherent NUMA machine. Overall, since NUMA-RC is cheaper, NUMA-RC is more cost-effective for these applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Goldschmidt. </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: The home first attempts to forward the mastership to one of the sharers of the line if there is one. If that fails, then the home is forced to make space for the line. Finally, the evaluation is performed with execution-driven simulations using the TangoLite tracing environment <ref> [1] </ref>. We have developed a highly-efficient simulation package called Zi-brasim, capable of simulating a number of shared-memory multiprocessor configurations. 3.2 Applications We examine seven applications from the Splash2 suite [10] running on 32 simulated processors. We use the default problem size in all the applications.
Reference: [2] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM A Cache-Only Memory Architecture. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: This approach is used in current machines like Sun's S3.mp [6] and Sequent's STiNG [5]. In this paper, we call this machine organization NUMA-RC, for non-uniform memory access with remote cache (Figure 4-(b)). A second approach is to use a Cache-Only Memory Architecture (COMA) organization (Figure 4-(c)). COMA machines <ref> [2, 4, 7, 8, 9] </ref> organize the distributed memories as caches called attraction memories. Like caches, attraction memories contain data and tags. <p> In the cases where the line was displaced from the secondary cache due to conflicts, chances are that the attraction memory still has a copy and can therefore supply it quickly to the processor. Examples of COMA machines, also called All-cache, are the Swedish Institute of Computer Science's DDM1 <ref> [2] </ref> and Kendall Square Research's KSR1 [7]. One of the overheads of COMA machines is that the attraction memory needs to be larger than the size of the application data to allow for the latter to replicate.
Reference: [3] <author> T. Joe. COMA-F: </author> <title> A Non-Hierarchical Cache Only Memory Architecture. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1995. </year>
Reference-contexts: The algorithm that determines how to assign mastership in the case of a line that is shared by more than one processor is called the data mastership assignment algorithm. The COMA organization that we consider in this paper is the Flat-COMA <ref> [3, 9] </ref> one. In this organization, each individual memory line has a directory entry in an agreed-upon node. This node is chosen based on the physical address of the line and is called the home of the line.
Reference: [4] <author> T. Joe and J. Hennessy. </author> <title> Evaluating the Memory Overhead Required for COMA Architecture. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 82-93, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This approach is used in current machines like Sun's S3.mp [6] and Sequent's STiNG [5]. In this paper, we call this machine organization NUMA-RC, for non-uniform memory access with remote cache (Figure 4-(b)). A second approach is to use a Cache-Only Memory Architecture (COMA) organization (Figure 4-(c)). COMA machines <ref> [2, 4, 7, 8, 9] </ref> organize the distributed memories as caches called attraction memories. Like caches, attraction memories contain data and tags. <p> The overall result is that the homes get polluted with data that they are not using and, consequently, suffer more misses in the future. To alleviate this problem, Joe and Hennessy proposed an algorithm that we call Swapping <ref> [4] </ref>. In the Swapping algorithm, when a processor suffers a remote read miss, the line is loaded in state shared master. If the load forces the processor to displace a master line and there are no sharers, the displaced line is sent to the supplying processor.
Reference: [5] <author> T. Lovett and R. Clapp. STiNG: </author> <title> A CC-NUMA Computer System for the Commercial Marketplace. </title> <booktitle> In Proceedings of he 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 308-317, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Otherwise, the data is requested from the corresponding node and eventually stored in the remote cache before passing it to the processor. The remote cache is kept coherent at all times. This approach is used in current machines like Sun's S3.mp [6] and Sequent's STiNG <ref> [5] </ref>. In this paper, we call this machine organization NUMA-RC, for non-uniform memory access with remote cache (Figure 4-(b)). A second approach is to use a Cache-Only Memory Architecture (COMA) organization (Figure 4-(c)). COMA machines [2, 4, 7, 8, 9] organize the distributed memories as caches called attraction memories.
Reference: [6] <author> A. Nowatzyk, G. Aybay, M. Browne, E. Kelly, M. Parkin, B. Radke, and S. Vishi. </author> <title> The S3.mp Scalable Shared Memory Multiprocessor. </title> <booktitle> In Proceedings of the 1995 International Conference on Parallel Processing, </booktitle> <pages> pages I1-I10, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Otherwise, the data is requested from the corresponding node and eventually stored in the remote cache before passing it to the processor. The remote cache is kept coherent at all times. This approach is used in current machines like Sun's S3.mp <ref> [6] </ref> and Sequent's STiNG [5]. In this paper, we call this machine organization NUMA-RC, for non-uniform memory access with remote cache (Figure 4-(b)). A second approach is to use a Cache-Only Memory Architecture (COMA) organization (Figure 4-(c)).
Reference: [7] <institution> Kendall Square Research. </institution> <type> KSR1 Technical Summary. </type> <address> Waltham, MA, </address> <year> 1992. </year>
Reference-contexts: This approach is used in current machines like Sun's S3.mp [6] and Sequent's STiNG [5]. In this paper, we call this machine organization NUMA-RC, for non-uniform memory access with remote cache (Figure 4-(b)). A second approach is to use a Cache-Only Memory Architecture (COMA) organization (Figure 4-(c)). COMA machines <ref> [2, 4, 7, 8, 9] </ref> organize the distributed memories as caches called attraction memories. Like caches, attraction memories contain data and tags. <p> Examples of COMA machines, also called All-cache, are the Swedish Institute of Computer Science's DDM1 [2] and Kendall Square Research's KSR1 <ref> [7] </ref>. One of the overheads of COMA machines is that the attraction memory needs to be larger than the size of the application data to allow for the latter to replicate.
Reference: [8] <author> A. Saulsbury, T. Wilkinson, J. Carter, and A. Landin. </author> <title> An Argument for Simple COMA. </title> <booktitle> In Proceedings of the 1st International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 276-285, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: This approach is used in current machines like Sun's S3.mp [6] and Sequent's STiNG [5]. In this paper, we call this machine organization NUMA-RC, for non-uniform memory access with remote cache (Figure 4-(b)). A second approach is to use a Cache-Only Memory Architecture (COMA) organization (Figure 4-(c)). COMA machines <ref> [2, 4, 7, 8, 9] </ref> organize the distributed memories as caches called attraction memories. Like caches, attraction memories contain data and tags.
Reference: [9] <author> P. Stenstrom, T. Joe, and A. Gupta. </author> <title> Comparative Performance Evaluation of Cache-Coherent NUMA and COMA Architectures. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This approach is used in current machines like Sun's S3.mp [6] and Sequent's STiNG [5]. In this paper, we call this machine organization NUMA-RC, for non-uniform memory access with remote cache (Figure 4-(b)). A second approach is to use a Cache-Only Memory Architecture (COMA) organization (Figure 4-(c)). COMA machines <ref> [2, 4, 7, 8, 9] </ref> organize the distributed memories as caches called attraction memories. Like caches, attraction memories contain data and tags. <p> The algorithm that determines how to assign mastership in the case of a line that is shared by more than one processor is called the data mastership assignment algorithm. The COMA organization that we consider in this paper is the Flat-COMA <ref> [3, 9] </ref> one. In this organization, each individual memory line has a directory entry in an agreed-upon node. This node is chosen based on the physical address of the line and is called the home of the line. <p> To our knowledge, the only prior work directly related to ours is a comparison of NUMA to COMA architectures by Stenstrom et al <ref> [9] </ref>. Such work, however, used more primitive NUMA and COMA organizations and did not dimension the caches so that the working set overflows them. The conclusion of the work was that each architecture performs best for a subset of the applications.
Reference: [10] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Chracterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposiumj on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: In this paper, we address the issue of which of the two organizations, namely NUMA-RC and COMA, is the most cost-effective one for Splash2-like applications <ref> [10] </ref> with a large working set. To our knowledge, the only prior work directly related to ours is a comparison of NUMA to COMA architectures by Stenstrom et al [9]. <p> Such work, however, used more primitive NUMA and COMA organizations and did not dimension the caches so that the working set overflows them. The conclusion of the work was that each architecture performs best for a subset of the applications. In our analysis, we use seven Splash2 applications <ref> [10] </ref> running on simulated 32-node architectures. The cache sizes are such that the working sets of most of the applications overflow them. To make the comparison fair, the NUMA-RC and COMA organizations examined have the same total amount of memory space. <p> Finally, the evaluation is performed with execution-driven simulations using the TangoLite tracing environment [1]. We have developed a highly-efficient simulation package called Zi-brasim, capable of simulating a number of shared-memory multiprocessor configurations. 3.2 Applications We examine seven applications from the Splash2 suite <ref> [10] </ref> running on 32 simulated processors. We use the default problem size in all the applications.
References-found: 10

