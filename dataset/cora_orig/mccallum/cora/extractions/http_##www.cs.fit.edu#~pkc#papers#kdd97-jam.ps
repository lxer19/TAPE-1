URL: http://www.cs.fit.edu/~pkc/papers/kdd97-jam.ps
Refering-URL: http://www.cs.fit.edu/~pkc/papers/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: wfang@cs.columbia.edu  pkc@cs.fit.edu  
Title: JAM: Java Agents for Meta-Learning over Distributed Databases  
Author: Salvatore Stolfo, Andreas L. Prodromidis Shelley Tselepis, Wenke Lee, Dave W. Fan fsal, andreas, sat, wenke, Philip K. Chan 
Keyword: distributed data mining, intelligent agents, machine learning, fraud and intrusion detection, financial information systems.  
Note: This research is supported by the Intrusion Detection Program (BAA9603) from DARPA (F30602-96 1-0311), NSF (IRI-96-32225 and CDA-96-25374) and NYSSTF (423115-445). Supported in part by IBM.  
Date: March 11, 1997  
Address: New York, NY 10027  Melbourne, FL 32901  
Affiliation: Department of Computer Science Columbia University  Computer Science Florida Institute of Technology  
Abstract: In this paper, we describe the JAM system, a distributed, scalable and portable agent-based data mining system that employs a general approach to scaling data mining applications that we call meta-learning. JAM provides a set of learning programs, implemented either as JAVA applets or applications, that compute models over data stored locally at a site. JAM also provides a set of meta-learning agents for combining multiple models that were learned (perhaps) at different sites. It employs a special distribution mechanism which allows the migration of the derived models or classifier agents to other remote sites. We describe the overall architecture of the JAM system and the specific implementation currently under development at Columbia University. One of JAM's target applications is fraud and intrusion detection in financial information systems. A brief description of this learning task and JAM's applicability are also described. Interested users may download JAM from http://www.cs.columbia.edu/~sal/JAM/PROJECT. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: Agents JAM's extensible plug-and-play architecture allows snap-in learning agents. The learning and meta-learning agents are designed as objects. JAM provides the definition of the parent agent class and every instance agent (i.e. a program that implements any of your favorite learning algorithms ID3, Ripper, Cart <ref> [1] </ref>, Bayes [8], Wpebls [7], CN2 [5], etc.) are then defined as a subclass of this parent class. Among other definitions which are inherited by all agent subclasses, the parent agent class provides a very simple and minimal interface that all subclasses have to comply to.
Reference: [2] <author> P. Chan. </author> <title> An Extensible Meta-Learning Approach for Scalable and Accurate Inductive Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY, </address> <year> 1996. </year>
Reference-contexts: Other parameters include the host of the CFM, the Cross-Validation Fold, the Meta-Learning Fold, the Meta-Learning Level, the names of the local learning agent and the local meta-learning agent, etc. Refer to <ref> [2] </ref> for more information on the meaning and use of these parameters. (Notice that Marmalade has established that Strawberry and Mango are its peer Datasites, having acquired this information from the CFM.) Then, Marmalade partitions the thyroid database (noted as thyroid.1.bld and thyroid.2.bld in the Data Set panel) for the 2-Cross-Validation
Reference: [3] <author> P. Chan and S. Stolfo. </author> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proc. Twelfth Intl. Conf. Machine Learning, </booktitle> <pages> pages 90-98, </pages> <year> 1995. </year>
Reference-contexts: In collaboration with the FSTC we have populated these database sites with records of credit card transactions, provided by different banks, in an attempt to detect and prevent fraud by combining learned patterns and behaviors from independent sources. 7 This meta-learning strategy is denoted class-attribute-combiner as defined in <ref> [3, 4] </ref>. 8 The section detailing the meta-learning strategies in [4] describes the various bounds placed on the meta-training data sets while still producing accurate meta-classifiers. 9 6 Acknowledgements We wish to thank David Wolpert, formerly of TXN and presently at IBM Almaden, Hank Vacarro of TXN, Shaula Yemini of Smarts,
Reference: [4] <author> P. Chan and S. Stolfo. </author> <title> Learning arbiter and combiner trees from partitioned data for scaling machine learning. </title> <booktitle> In Proc. Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 39-44, </pages> <year> 1995. </year>
Reference-contexts: In collaboration with the FSTC we have populated these database sites with records of credit card transactions, provided by different banks, in an attempt to detect and prevent fraud by combining learned patterns and behaviors from independent sources. 7 This meta-learning strategy is denoted class-attribute-combiner as defined in <ref> [3, 4] </ref>. 8 The section detailing the meta-learning strategies in [4] describes the various bounds placed on the meta-training data sets while still producing accurate meta-classifiers. 9 6 Acknowledgements We wish to thank David Wolpert, formerly of TXN and presently at IBM Almaden, Hank Vacarro of TXN, Shaula Yemini of Smarts, <p> these database sites with records of credit card transactions, provided by different banks, in an attempt to detect and prevent fraud by combining learned patterns and behaviors from independent sources. 7 This meta-learning strategy is denoted class-attribute-combiner as defined in [3, 4]. 8 The section detailing the meta-learning strategies in <ref> [4] </ref> describes the various bounds placed on the meta-training data sets while still producing accurate meta-classifiers. 9 6 Acknowledgements We wish to thank David Wolpert, formerly of TXN and presently at IBM Almaden, Hank Vacarro of TXN, Shaula Yemini of Smarts, Inc. and Yechiam Yemini for many useful and insightful discussions.
Reference: [5] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-285, </pages> <year> 1989. </year>
Reference-contexts: The learning and meta-learning agents are designed as objects. JAM provides the definition of the parent agent class and every instance agent (i.e. a program that implements any of your favorite learning algorithms ID3, Ripper, Cart [1], Bayes [8], Wpebls [7], CN2 <ref> [5] </ref>, etc.) are then defined as a subclass of this parent class. Among other definitions which are inherited by all agent subclasses, the parent agent class provides a very simple and minimal interface that all subclasses have to comply to.
Reference: [6] <author> William W. Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Proc. Twelfth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Some machine learning algorithms generate concise and very readable textual outputs, e.g., the rule sets from Ripper <ref> [6] </ref>. It is thus counter-intuitive to translate the text to graph form for display purposes. In such cases, JAM simply pretty formats the text output and displays it in the classifier visualization panel.
Reference: [7] <author> S. Cost and S. Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: Agents JAM's extensible plug-and-play architecture allows snap-in learning agents. The learning and meta-learning agents are designed as objects. JAM provides the definition of the parent agent class and every instance agent (i.e. a program that implements any of your favorite learning algorithms ID3, Ripper, Cart [1], Bayes [8], Wpebls <ref> [7] </ref>, CN2 [5], etc.) are then defined as a subclass of this parent class. Among other definitions which are inherited by all agent subclasses, the parent agent class provides a very simple and minimal interface that all subclasses have to comply to.
Reference: [8] <author> R. Duda and P. Hart. </author> <title> Pattern classification and scene analysis. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1973. </year>
Reference-contexts: Agents JAM's extensible plug-and-play architecture allows snap-in learning agents. The learning and meta-learning agents are designed as objects. JAM provides the definition of the parent agent class and every instance agent (i.e. a program that implements any of your favorite learning algorithms ID3, Ripper, Cart [1], Bayes <ref> [8] </ref>, Wpebls [7], CN2 [5], etc.) are then defined as a subclass of this parent class. Among other definitions which are inherited by all agent subclasses, the parent agent class provides a very simple and minimal interface that all subclasses have to comply to.
Reference: [9] <editor> Usama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth. </editor> <title> The kdd process for extracting useful knowledge from data. </title> <journal> Communications of the ACM, </journal> <volume> 39(11) </volume> <pages> 27-34, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: Figure 2 displays a snapshot of the system during the animated meta-learning process where JAM's GUI moves icons within the panel displaying the construction of a new meta-classifier. Classifier Visualization JAM provides graph drawing tools to help users understand the learned knowledge <ref> [9] </ref>. There are many kinds of classifiers, e.g., a decision tree by ID3, that can be represented as graphs. In JAM we have employed major components of JavaDot [10], an extensible visualization system, to display the classifier and allows the user to analyze 4 the graph.
Reference: [10] <author> Wenke Lee and Naser S. Barghouti. Javadot: </author> <title> An extensible visualization environment. </title> <type> Technical Report CUCS-02-97, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY, </address> <year> 1997. </year> <month> 10 </month>
Reference-contexts: Classifier Visualization JAM provides graph drawing tools to help users understand the learned knowledge [9]. There are many kinds of classifiers, e.g., a decision tree by ID3, that can be represented as graphs. In JAM we have employed major components of JavaDot <ref> [10] </ref>, an extensible visualization system, to display the classifier and allows the user to analyze 4 the graph.
Reference: [11] <author> C. Merz and P. Murphy. </author> <title> UCI repository of machine learning databases [http://www.ics.uci.edu/~mlearn/mlrepository.html]. Dept. </title> <institution> of Info. and Computer Sci., Univ. of California, </institution> <address> Irvine, CA, </address> <year> 1996. </year>
Reference-contexts: The snapshot taken is from "Marmalade's point of view". Initially, Marmalade consults the Datasite configuration file where the owner of the Datasite sets the parameters. In this case, the dataset is a medical database with records <ref> [11] </ref>, noted by thyroid in the Data Set panel. Other parameters include the host of the CFM, the Cross-Validation Fold, the Meta-Learning Fold, the Meta-Learning Level, the names of the local learning agent and the local meta-learning agent, etc.
Reference: [12] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: has established that Strawberry and Mango are its peer Datasites, having acquired this information from the CFM.) Then, Marmalade partitions the thyroid database (noted as thyroid.1.bld and thyroid.2.bld in the Data Set panel) for the 2-Cross-Validation Fold and computes the local classifier, noted by Marmalade.1 (here by calling the ID3 <ref> [12] </ref> learning agent). Next, Marmalade imports the remote classifiers, noted by Strawberry.1 and Mango.1 and begins the meta-learning process. Marmalade employs this meta-classifier to predict the classes of input data items (in this case unlabelled medical records).
Reference: [13] <author> S. Stolfo, W. Fan, W. Lee, A. Prodromidis, and P. Chan. </author> <title> Credit card fraud detection using meta-learning: Issues and initial results. </title> <note> Submitted to 3rd Intl. Conf. Know. Disc. Data Mining, 1997. 11 </note>
Reference-contexts: In the first setting, Bayes combined the three base classifiers with the least correlated error and in the second it combined the four most accurate base classifiers. The experiments, settings, rationale and results have been reported in detail in a companion paper <ref> [13] </ref> also available from http://www.cs.columbia.edu/~sal/JAM/PROJECT. 5 Conclusions We believe the concepts embodied by the term meta-learning provide an important step in developing systems that learn from massive databases and that scale.
References-found: 13

