URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P347.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts93.htm
Root-URL: http://www.mcs.anl.gov
Title: Parallel Bandreduction and Tridiagonalization  
Author: Christian Bischof Mercedes Marques Xiaobai Sun 
Date: 1993.  
Note: Published in the Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, R. Sincovec, Ed., pp. 383-390, SIAM,  
Pubnum: ANL-MCS-P347-0193  
Abstract: This paper presents a parallel implementation of a blocked band reduction algorithm for symmetric matrices suggested by Bischof and Sun. The reduction to tridiagonal or block tridiagonal form is a special case of this algorithm. A blocked double torus wrap mapping is used as the underlying data distribution and the so-called WY representation is employed to represent block orthogonal transformations. Preliminary performance results on the Intel Delta indicate that the algorithm is well-suited to a MIMD computing environment and that the use of a block approach significantly improves performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, A. McKenney, and D. Sorensen, </author> <title> LAPACK User's Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1992. </year>
Reference-contexts: This has a very beneficial effect on architectures b zero out "bulge" d Fig. 3. First Block Bandreduction Step. zero out bulge Fig. 4. Chasing the Bulge. employing a memory hierarchy, since it greatly reduces the amount of data movement required <ref> [8, 4, 5, 1] </ref>. To see how this algorithmic primitive is used, let us consider an example. Figure 3 shows the reduction of d subdiagonals in nb columns of a matrix with initial bandwidth b.
Reference: [2] <author> M. Barnett, R. Littlefield, D. G. Payne, and R. van de Geijn, </author> <title> Efficient communication primitives on mesh architectures with hardware routing, </title> <booktitle> in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientfic Computing, </booktitle> <editor> R. Sincovec, ed., </editor> <address> Philadelphia, </address> <year> 1993, </year> <note> SIAM. </note>
Reference-contexts: We also note that the Chamelon tools currently provide only unoptimized "fan-in/fan-out" broadcast and global sum primitives (see, for example [22, 3]) which are substantially slower than primitives that are optimized for the Intel Delta (e.g. <ref> [2, 16] </ref>). These issues will be addressed in future versions of our code. 4 Preliminary Performance Results In this section, we present preliminary performance results that we have obtained with a double-precision version of our code running on 64 processors of the Intel Delta.
Reference: [3] <author> M. Barnett, D. Payne, and R. van de Geijn, </author> <title> Optimal broadcasting in mesh-connected architectures, </title> <type> Tech. Rep. </type> <institution> TR-91-38, Department of Computer Science, University of Texas at Austin, </institution> <year> 1991. </year> <month> 7 </month>
Reference-contexts: We also note that the Chamelon tools currently provide only unoptimized "fan-in/fan-out" broadcast and global sum primitives (see, for example <ref> [22, 3] </ref>) which are substantially slower than primitives that are optimized for the Intel Delta (e.g. [2, 16]).
Reference: [4] <author> C. H. Bischof, </author> <title> Computing the singular value decomposition on a distributed system of vector processors, </title> <booktitle> Parallel Computing, 11 (1989), </booktitle> <pages> pp. </pages> <month> 171-186. </month> <title> [5] , A pipelined block QR decomposition algorithm., in Parallel Processing for Scientific Computing, </title> <editor> G. Rodrigue, ed., </editor> <address> Philadelphia, 1989, </address> <publisher> SIAM Press, </publisher> <pages> pp. 3-7. </pages>
Reference-contexts: This has a very beneficial effect on architectures b zero out "bulge" d Fig. 3. First Block Bandreduction Step. zero out bulge Fig. 4. Chasing the Bulge. employing a memory hierarchy, since it greatly reduces the amount of data movement required <ref> [8, 4, 5, 1] </ref>. To see how this algorithmic primitive is used, let us consider an example. Figure 3 shows the reduction of d subdiagonals in nb columns of a matrix with initial bandwidth b.
Reference: [6] <author> C. H. Bischof and X. Sun, </author> <title> A divide-and-conquer method for computing complementary invariant subspaces of symmetric matrices, </title> <type> Tech. Rep. </type> <institution> MCS-P286-0192, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year> <title> [7] , A framework for band reduction and tridiagonalization of symmetric matrices, </title> <type> Tech. Rep. </type> <institution> MCS-P298-0392, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: We are also working on incorporating better tuned communication routines in the Chamelon programming system that our implementation builds on. Lastly we mention that, in the end, we plan to exploit the divide-and-conquer nature of the tridiagonalization of matrices with only 0 and 1 eigenvalues <ref> [6] </ref> as it arises in the ISDA eigenvalue solver framework [17]. With the 2D torus wrapped data mapping, this approach would not significantly reduce the communication requirements of the code, but would approximately halve the number of arithmetic operations required.
Reference: [8] <author> C. H. Bischof and C. F. Van Loan, </author> <title> The WY representation for products of Householder matrices, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 8 (1987), </volume> <pages> pp. </pages> <month> s2-s13. </month>
Reference-contexts: with nb columns to upper triangular form 0 via Householder reductions and the accumulation of transformations W and Y such that T = (I W Y ) R ! W and Y are of the same size as T and are accumulated according to what is called "method 1" in <ref> [8] </ref>. PRE: The application of the block orthogonal transformation I W Y to a matrix B (say) from the left, i.e. B (I W Y ) T B: SYM: The application of I W Y to a matrix C (say) from the left and the right, i.e. <p> This has a very beneficial effect on architectures b zero out "bulge" d Fig. 3. First Block Bandreduction Step. zero out bulge Fig. 4. Chasing the Bulge. employing a memory hierarchy, since it greatly reduces the amount of data movement required <ref> [8, 4, 5, 1] </ref>. To see how this algorithmic primitive is used, let us consider an example. Figure 3 shows the reduction of d subdiagonals in nb columns of a matrix with initial bandwidth b.
Reference: [9] <author> J. Demmel, J. Dongarra, R. van de Geijn, and D. Walker, </author> <title> LAPACK for distributed-memory machines: The next generation, </title> <booktitle> in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientfic Computing, </booktitle> <editor> R. Sincovec, ed., </editor> <address> Philadelphia, </address> <year> 1993, </year> <note> SIAM. </note>
Reference-contexts: A scalar wrap mapping and one-dimensional (i.e. row or column oriented distributions) are special cases of this mapping. This mapping also has been selected in other efforts to develop linear algebra basis software for massively parallel machines, for example the ScaLAPACK project <ref> [9, 11] </ref>. We also assume that our underlying hardware is logically configured as a p fi p mesh, and that only one process is active on every processor.
Reference: [10] <author> J. Dongarra and R. van de Geijn, </author> <title> Reduction to condensed form for the eigenvalue problem on distributed-memory architectures, </title> <booktitle> Parallel Computer, 18 (1992), </booktitle> <pages> pp. 973-982. </pages>
Reference-contexts: If the matrix is full, the conventional Householder tridiagonalization approach [13, p. 276] or block variants thereof [12] is the method of choice. These two approaches also underlie the parallel implementations described for example in [15] and <ref> [10] </ref>. The approach described in this paper, on the other hand, follows the band reduction framework suggested by Bischof and Sun [7]. The standard approach, which eliminates all subdiagonals at one time, is a special case, but "piecemeal" approaches are possible as well, as illustrated in Figure 1. <p> This is the same kernel as in the POST step. 3 A Parallel Implementation To distribute the matrix across a distributed-memory machine, we chose a blocked two-dimensional torus wrapping (see, for example <ref> [17, 11, 10] </ref>). A scalar wrap mapping and one-dimensional (i.e. row or column oriented distributions) are special cases of this mapping. This mapping also has been selected in other efforts to develop linear algebra basis software for massively parallel machines, for example the ScaLAPACK project [9, 11].
Reference: [11] <author> J. Dongarra, R. van de Geijn, and D. Walker, </author> <title> A look at scalable dense linear algebra libraries, </title> <type> Tech. Rep. TR CS-92-155, </type> <institution> Computer Science Department, The University of Tennessee, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: This is the same kernel as in the POST step. 3 A Parallel Implementation To distribute the matrix across a distributed-memory machine, we chose a blocked two-dimensional torus wrapping (see, for example <ref> [17, 11, 10] </ref>). A scalar wrap mapping and one-dimensional (i.e. row or column oriented distributions) are special cases of this mapping. This mapping also has been selected in other efforts to develop linear algebra basis software for massively parallel machines, for example the ScaLAPACK project [9, 11]. <p> A scalar wrap mapping and one-dimensional (i.e. row or column oriented distributions) are special cases of this mapping. This mapping also has been selected in other efforts to develop linear algebra basis software for massively parallel machines, for example the ScaLAPACK project <ref> [9, 11] </ref>. We also assume that our underlying hardware is logically configured as a p fi p mesh, and that only one process is active on every processor.
Reference: [12] <author> J. J. Dongarra, S. J. Hammarling, and D. C. Sorensen, </author> <title> Block reduction of matrices to condensed form for eigenvalue computations, </title> <type> Tech. Rep. </type> <institution> MCS-TM-99, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Reduction to tridiagonal form is a major step in eigenvalue computations for symmetric matrices. If the matrix is full, the conventional Householder tridiagonalization approach [13, p. 276] or block variants thereof <ref> [12] </ref> is the method of choice. These two approaches also underlie the parallel implementations described for example in [15] and [10]. The approach described in this paper, on the other hand, follows the band reduction framework suggested by Bischof and Sun [7].
Reference: [13] <author> G. H. Golub and C. F. V. Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1983. </year>
Reference-contexts: 1 Introduction Reduction to tridiagonal form is a major step in eigenvalue computations for symmetric matrices. If the matrix is full, the conventional Householder tridiagonalization approach <ref> [13, p. 276] </ref> or block variants thereof [12] is the method of choice. These two approaches also underlie the parallel implementations described for example in [15] and [10]. The approach described in this paper, on the other hand, follows the band reduction framework suggested by Bischof and Sun [7].
Reference: [14] <author> W. Gropp and B. Smith, Chamelon: </author> <title> Parallel programming tools user manual, </title> <type> tech. rep., </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1993. </year> <note> unpublished draft. </note>
Reference-contexts: Influence of Block Size on (block) Tridiagonalization. on the same row or column of the mesh. Hence, to develop a portable code, and to allow a maintainable implementation of this code, we chose to base our implementation on the Chamelon parallel programming tools <ref> [14] </ref>. Chamelon's primitives (such as broadcast or global summation) support arbitrary process groups, and several such "computational contexts" may be active at any given point in time. This greatly simplifies programming, since, for example, a "broadcast" will automatically involve only the members of this computing context.
Reference: [15] <author> B. Hendrikson and D. Womble, </author> <title> The torus-wrap mapping for dense matrix calculations on massively parallel computers, </title> <type> Tech. Rep. </type> <institution> SAND92-0792, Sandia National Laboratories, </institution> <year> 1992. </year>
Reference-contexts: If the matrix is full, the conventional Householder tridiagonalization approach [13, p. 276] or block variants thereof [12] is the method of choice. These two approaches also underlie the parallel implementations described for example in <ref> [15] </ref> and [10]. The approach described in this paper, on the other hand, follows the band reduction framework suggested by Bischof and Sun [7]. The standard approach, which eliminates all subdiagonals at one time, is a special case, but "piecemeal" approaches are possible as well, as illustrated in Figure 1.
Reference: [16] <author> S. Huss-Lederman, E. Jacobson, A. Tsao, and G. Zhang, </author> <title> Matrix multiplication on the intel touchstone delta, </title> <booktitle> in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientfic Computing, </booktitle> <editor> R. Sincovec, ed., </editor> <address> Philadelphia, </address> <year> 1993, </year> <note> SIAM. </note>
Reference-contexts: We also note that the Chamelon tools currently provide only unoptimized "fan-in/fan-out" broadcast and global sum primitives (see, for example [22, 3]) which are substantially slower than primitives that are optimized for the Intel Delta (e.g. <ref> [2, 16] </ref>). These issues will be addressed in future versions of our code. 4 Preliminary Performance Results In this section, we present preliminary performance results that we have obtained with a double-precision version of our code running on 64 processors of the Intel Delta.
Reference: [17] <author> S. Huss-Lederman, A. Tsao, and G. Zhang, </author> <title> A parallel implementation of the invariant subspace decomposition algorithm for dense symmetric matrices, </title> <booktitle> in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientfic Computing, </booktitle> <editor> R. Sincovec, ed., </editor> <address> Philadelphia, </address> <year> 1993, </year> <note> SIAM. </note>
Reference-contexts: This is the same kernel as in the POST step. 3 A Parallel Implementation To distribute the matrix across a distributed-memory machine, we chose a blocked two-dimensional torus wrapping (see, for example <ref> [17, 11, 10] </ref>). A scalar wrap mapping and one-dimensional (i.e. row or column oriented distributions) are special cases of this mapping. This mapping also has been selected in other efforts to develop linear algebra basis software for massively parallel machines, for example the ScaLAPACK project [9, 11]. <p> Lastly we mention that, in the end, we plan to exploit the divide-and-conquer nature of the tridiagonalization of matrices with only 0 and 1 eigenvalues [6] as it arises in the ISDA eigenvalue solver framework <ref> [17] </ref>. With the 2D torus wrapped data mapping, this approach would not significantly reduce the communication requirements of the code, but would approximately halve the number of arithmetic operations required. Acknowledgements The authors would like to thank Bill Gropp for his help and support with the Chamelon programming system.
Reference: [18] <author> L. Kaufman, </author> <title> Banded eigenvalue solvers on vector machines, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 10 (1984), </volume> <pages> pp. 73-86. </pages>
Reference-contexts: The standard approach, which eliminates all subdiagonals at one time, is a special case, but "piecemeal" approaches are possible as well, as illustrated in Figure 1. The "piecemeal" approach was shown to be attractive in comparison to previously suggested band reduction schemes <ref> [20, 21, 18, 19] </ref> in that it allows tradeoffs between flops and storage. fl This work was supported by the Applied and Computational Mathematics Program, Defense Advanced Research Projects Agency, under contract DM28E04120, and by the Office of Scientific Computing, U.S.
Reference: [19] <author> B. Lang, Parallele Reduktion symmetrischer Bandmatrizen auf Tridiagonalgestalt, </author> <type> PhD thesis, </type> <institution> Universitat Karlsruhe (TH), </institution> <year> 1991. </year>
Reference-contexts: The standard approach, which eliminates all subdiagonals at one time, is a special case, but "piecemeal" approaches are possible as well, as illustrated in Figure 1. The "piecemeal" approach was shown to be attractive in comparison to previously suggested band reduction schemes <ref> [20, 21, 18, 19] </ref> in that it allows tradeoffs between flops and storage. fl This work was supported by the Applied and Computational Mathematics Program, Defense Advanced Research Projects Agency, under contract DM28E04120, and by the Office of Scientific Computing, U.S.
Reference: [20] <author> H. </author> <title> Rutishauser, On Jacobi rotation patterns, </title> <booktitle> in Proc. of Symposia in Applied Mathematics, </booktitle> <volume> Vol. </volume> <month> 15, </month> <title> Experimental Arithmetic, High Speed Computing and Mathematics, </title> <booktitle> 1963, </booktitle> <pages> pp. 219-239. </pages>
Reference-contexts: The standard approach, which eliminates all subdiagonals at one time, is a special case, but "piecemeal" approaches are possible as well, as illustrated in Figure 1. The "piecemeal" approach was shown to be attractive in comparison to previously suggested band reduction schemes <ref> [20, 21, 18, 19] </ref> in that it allows tradeoffs between flops and storage. fl This work was supported by the Applied and Computational Mathematics Program, Defense Advanced Research Projects Agency, under contract DM28E04120, and by the Office of Scientific Computing, U.S.
Reference: [21] <author> H. R. Schwarz, </author> <title> Tridiagonalization of a symmetric band matrix, </title> <journal> Numerische Mathematik, </journal> <volume> 12 (1968), </volume> <pages> pp. 231-241. </pages>
Reference-contexts: The standard approach, which eliminates all subdiagonals at one time, is a special case, but "piecemeal" approaches are possible as well, as illustrated in Figure 1. The "piecemeal" approach was shown to be attractive in comparison to previously suggested band reduction schemes <ref> [20, 21, 18, 19] </ref> in that it allows tradeoffs between flops and storage. fl This work was supported by the Applied and Computational Mathematics Program, Defense Advanced Research Projects Agency, under contract DM28E04120, and by the Office of Scientific Computing, U.S.
Reference: [22] <author> R. van de Geijn, </author> <title> On global combine operations, </title> <type> Tech. Rep. </type> <institution> CS-91-129, Computer Science Department, The University of Tennessee, </institution> <year> 1991. </year> <note> to appear in the Journal on Parallel and Distributed Computing. </note>
Reference-contexts: We also note that the Chamelon tools currently provide only unoptimized "fan-in/fan-out" broadcast and global sum primitives (see, for example <ref> [22, 3] </ref>) which are substantially slower than primitives that are optimized for the Intel Delta (e.g. [2, 16]).
References-found: 20

