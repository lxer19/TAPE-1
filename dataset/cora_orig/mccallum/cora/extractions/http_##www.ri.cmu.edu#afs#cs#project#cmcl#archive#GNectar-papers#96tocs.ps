URL: http://www.ri.cmu.edu/afs/cs/project/cmcl/archive/GNectar-papers/96tocs.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs/usr/prs/WWW/papers.html
Root-URL: 
Title: A High-Speed Network Interface for Distributed-Memory Systems: Architecture and Applications  
Author: Peter Steenkiste 
Note: To appear in ACM Transactions on Computer Systems. This research was supported by the Advanced Research Projects Agency/CSTO monitored by the Space and Naval Warfare Systems Command under contract N00039-93-C-0152. Peter Steenkiste can be reached at steenkiste@cs.cmu.edu. His mailing address is  
Date: October 9, 1996  
Address: Pittsburgh, PA 15213  5000 Forbes Avenue, Pittsburgh, PA 15213.  
Affiliation: School of Computer Science Carnegie Mellon University  School of Computer Science, Carnegie Mellon University,  
Abstract: Distributed-memory systems have traditionally had great difficulty performing network I/O at rates proportional to their computational power. The problem is that the network interface has to support network I/O for a supercomputer, using computational and memory bandwidth resources similar to those of a workstation. As a result, the network interface becomes a bottleneck. In this paper we present an I/O architecture that addresses these problems and supports high-speed network I/O on distributed-memory systems. The key to good performance is to partition the work appropriately between the system and the network interface. Some communication tasks are performed on the distributed-memory parallel system since it is more powerful, and less likely to become a bottleneck than the network interface. Tasks that do not parallelize well are performed on the network interface and hardware support is provided for the most time-critical operations. This architecture has been implemented for the iWarp distributed-memory system and has been used by a number of applications. We describe this implementation, present performance results, and use application examples to validate the main features of the I/O architecture. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Adams. </author> <title> Cray T3D system architecture overview. </title> <institution> Cray Research Inc., </institution> <month> September </month> <year> 1993. </year> <note> Revision 1.C. </note>
Reference-contexts: The data interface will have to be reimplemented using the native communication interface for the system, e.g. Nx on the Paragon [25] or remote put/get on the Cray T3D <ref> [1] </ref>. Using a different communication interface will also affect the implementation of the execution module since it optimizes data transfers. Finally, the system software of most distributed-memory systems supports some form of multi-programming.
Reference: [2] <author> ANSI. </author> <title> High-Performance Parallel Interface Mechanical, Electrical and Signalling Protocol Specification HIPPI-PH. ANSI X3.183-1991, </title> <year> 1991. </year>
Reference-contexts: In the last few years, networks based on the ANSI High-Performance Parallel Interface (HIPPI) protocol <ref> [2] </ref> have become very popular in supercomputer centers, and all commercially available supercomputers provide a HIPPI interface. HIPPI supports a data rate of 800 Mbit/second or 1.6 Gbit/second. In addition to HIPPI, there are a number of high-speed network standards in various stages of development by standards bodies.
Reference: [3] <author> Jaime Jungok Bae and Tatsuya Suda. </author> <title> Survey of traffic control schemes and protocols in atm networks. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 79(2):170189, </address> <month> February </month> <year> 1991. </year>
Reference-contexts: The slow down is more significant with the fast reshufing algorithm, since it uses more bandwidth. This anomaly could be avoided on an interconnect that supports the reservation of bandwidth for critical connections, as is for example being explored in the networking community <ref> [9, 3] </ref>. in the last row of the iWarp system. The iWarp node performs the reshufing and then waits for its turn to send data to the HIB.
Reference: [4] <author> M. Barnett, L. Shuler, R. van de Geijn, S. Gupta, D. G. Payne, and J. Watts. </author> <title> Interprocessor Collective Communication Library (InterCom). </title> <booktitle> In Proceedings of the IEEE Scalable High Performance Computing Conference, </booktitle> <pages> pages 357364. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: First, our target throughput is much higher. Second, we exploit the regularity of the distributions to achieve good reshufing performance; this is important, given our performance goals. Note that the data reorganization is in effect a collective communication operation <ref> [18, 4] </ref>. 9 Applicability to Other Systems We have presented a description of the architecture and implementation of a high-bandwidth network interface for the iWarp system.
Reference: [5] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel i/o. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 452461, </pages> <address> Oregon, </address> <month> November </month> <year> 1993. </year> <month> ACM/IEEE. </month>
Reference-contexts: Second, distributed-memory machines typically support high-bandwidth inter-node communication, and since reshufing parallelizes very well, many links can be used at the same time. As a result, the distributed-memory system can reshufe data efficiently. A similar approach has been proposed for disk I/O <ref> [5] </ref>. The creation of large messages inside the distributed memory-system is done by making the interaction with the network interface a two step process. First, data is reshufed from the distribution that was used by the application into an I/O distribution. <p> Kwan and Terstriep [31] show how the data distribution and data representation on the CM2 is a significant hurdle when doing I/O over the HIPPI interface, and how it limits the throughput. Data reorganization has been proposed to achieve high bandwidth access to disks in distributed-memory systems <ref> [5, 29, 38] </ref>. For example, in [5] the authors study the problem of implementing high-speed file I/O in the Intel Touchstone Delta. They observed that in order to achieve good performance it is important to send large blocks of data to the file servers that are part of the system. <p> Data reorganization has been proposed to achieve high bandwidth access to disks in distributed-memory systems [5, 29, 38]. For example, in <ref> [5] </ref> the authors study the problem of implementing high-speed file I/O in the Intel Touchstone Delta. They observed that in order to achieve good performance it is important to send large blocks of data to the file servers that are part of the system. <p> This optimization is widely applicable and has for example also been used to optimize disk I/O on distributed-memory systems <ref> [5, 29, 38] </ref>. Systems that support very efficient communication, e.g. the remote memory reads and writes on the Cray T3D, may not require data reshufing, but any system with non-negligible per-packet overhead is likely to benefit from data reshufing for fine grain data distributions.
Reference: [6] <author> Shekhar Borkar, Robert Cohn, George Cox, Sha Gleason, Thomas Gross, H. T. Kung, Monica Lam, Brian Moore, Craig Peterson, John Pieper, Linda Rankin, P. S. Tseng, Jim Sutton, John Urbanski, and Jon Webb. </author> <title> iWarp: An integrated solution to high-speed parallel computing. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 330339, </pages> <address> Orlando, Florida, </address> <month> November </month> <year> 1988. </year> <journal> IEEE Computer Society and ACM SIGARCH. </journal>
Reference-contexts: HIPPI supports a data rate of 800 Mbit/second or 1.6 Gbit/second. In addition to HIPPI, there are a number of high-speed network standards in various stages of development by standards bodies. These include ATM (Asynchronous Transfer Mode) [16] and Fibre Channel [26]. Meanwhile, distributed-memory computer systems <ref> [6, 23, 27, 32, 44] </ref> are becoming the architecture of choice for many supercomputer applications. The reason is that they are inherently scalable, and provide relatively inexpensive computing cycles compared with traditional uniprocessor or shared-memory multiprocessor supercomputers. <p> Some of the applications that use the HIPPI interface are described in Section 7. 3 iWarp overview iWarp is a distributed-memory parallel computing system <ref> [6] </ref>. An iWarp cell consists of a single-chip iWarp processor and a local memory. The iWarp processor integrates both a high-speed computation and communication agent in a single component.
Reference: [7] <author> Shekhar Borkar, Robert Cohn, George Cox, Thomas Gross, H.T. Kung, Monica Lam, Margie Levine, Brian Moore, Wire Moore, Craig Peterson, Jim Susman, Jim Sutton, John Urbanski, and Jon Webb. </author> <title> Supporting systolic and memory communication in iWarp. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 7081, </pages> <address> Seattle, </address> <month> May </month> <year> 1990. </year> <month> ACM/IEEE. </month>
Reference-contexts: The communication agent connects the iWarp cell to four neighbors through 40 MByte/second buses; the cells in the iWarp array are configured as a torus. The communication system supports high-speed interprocessor communication for a variety of communication models, including systolic communication and memory communication <ref> [7] </ref>. In systolic communication, the CPU writes data directly onto the interconnect, thus minimizing communication latency. Memory communication is supported through the use of spools, on-chip DMA engines that move data between the local memory and the interconnection network. <p> when we discuss the applications in Section 7. 5.3 The iWarp Streams Software We give a more detailed description of the streams implementation for iWarp. 5.3.1 Data and control interface The data interface between the distributed-memory system and the network interface is based on the PCS and ESPL communication libraries <ref> [24, 7] </ref>. PCS is used to create application-specific connections, and ESPL is a fast spooling library that achieve bandwidths close to the 40 MByte/second link rate, even for short messages. <p> Figure 11 shows for example the overhead per (4 byte) word on iWarp for sending blocks of different sizes over the internal interconnect, using spooling operations <ref> [7] </ref>. The overhead is calculated by dividing the number of cycles needed for the transfer by the number of words in the transfer.
Reference: [8] <author> Claudson Bornstein and Peter Steenkiste. </author> <title> Data reshufing in support of fast I/O for distributed-memory machines. </title> <booktitle> In Proceedings of the Third International Symposium on High-Performance Distributed Computing, </booktitle> <pages> pages 227235, </pages> <address> San Francisco, </address> <month> August </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: This architecture does not imply that each application has to provide the code to transfer data to and from the network interface. For example, libraries can be built for common data distributions (e.g. <ref> [8] </ref>). The components interact through a data interface and a control interface: The data interface transfers data between the network interface and the application on the distributed memory system. <p> The I/O mapping chosen corresponds to a block cyclic distribution. The algorithm used to reshufe data from an arbitrary block-cyclic distribution to the I/O distribution is described in <ref> [8] </ref>. This mapping is only one of many possible I/O mappings. <p> We observe that both algorithms easily support throughputs well in excess of the target HIPPI rates. The reason is that the reshufing algorithms are highly parallel: they exploit both parallelism between rows in the iWarp system and inside each row of iWarp <ref> [8] </ref>. 19 B B B J J J J J J 0 400 800 1200 Throughput (MByte/second) Block size (Bytes) B Fast reshuffle J Slow reshuffle on the system using distributions with different block sizes. The figure shows four results.
Reference: [9] <author> Dave Clark, S. Shenker, and L. Zhang. </author> <title> Supporting real-time applications in an integrated services packet network: Architecture and mechanisms. </title> <booktitle> In Proceedings of the SIGCOMM 92 Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 1426, </pages> <address> Baltimore, </address> <month> August </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: The slow down is more significant with the fast reshufing algorithm, since it uses more bandwidth. This anomaly could be avoided on an interconnect that supports the reservation of bandwidth for critical connections, as is for example being explored in the networking community <ref> [9, 3] </ref>. in the last row of the iWarp system. The iWarp node performs the reshufing and then waits for its turn to send data to the HIB.
Reference: [10] <author> David D. Clark, Van Jacobson, John Romkey, and Howard Salwen. </author> <title> An Analysis of TCP Processing Overhead. </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(6):2329, </volume> <month> June </month> <year> 1989. </year>
Reference-contexts: We also optimized the UDP/TCP/IP implementation itself using standard techniques such as header prediction <ref> [10] </ref>. The above optimizations keep the communication overhead within acceptable bounds. On a DEC Alpha workstation 3000/400, it takes about 300 microseconds from the time a user-level application issues a small write until a packet send request is handed to the network adapter, using the DEC OSF1 implementation of TCP/IP.
Reference: [11] <author> R. L. Clay. </author> <title> Scheduling in the Presence of Uncertainty: Probabilistic Solution of the Assignment Problem. </title> <type> PhD thesis, </type> <institution> Department of Chemical Engineering, Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <year> 1991. </year>
Reference-contexts: Specifically, the iWarp is used to generate a complex probability manifold representing a plant. The generated data is then sent to the C-90 at PSC for analysis, and finally, the C90 and CM2 solve the resulting linear assignment problem using a heterogeneous solver <ref> [11] </ref>. We collected data for several application runs, corresponding to input sizes of 1k, 2k and 4k; the samples generated by iWarp in these runs correspond to 256 MB, 1 GB and 4 GB of data.
Reference: [12] <author> R.L. Clay and G.J. McRae. </author> <title> Solution of Large-Scale Modeling and Optimization Problems Using Heterogeneous Supercomputing Systems. </title> <booktitle> In SuParCup 1991 proceedings, Mannheim, </booktitle> <address> Germany, </address> <year> 1991. </year>
Reference-contexts: The application optimizes a system, for example a chemical 21 HIB plant, using a stochastic model <ref> [12] </ref> of the system. The stochastic optimization problem can be transformed into a deterministic problem using the certainty equivalence transformation [15].
Reference: [13] <author> Robert Clay and Peter Steenkiste. </author> <title> Distributing a chemical process optimization application over a gigabit network. In Proceedings of Supercomputing 95, </title> <note> page To appear. ACM/IEEE, </note> <month> December </month> <year> 1995. </year>
Reference-contexts: The application optimizes a system, for example a chemical 21 HIB plant, using a stochastic model [12] of the system. The stochastic optimization problem can be transformed into a deterministic problem using the certainty equivalence transformation [15]. Our implementation of the stochastic optimization problem <ref> [13] </ref> was distributed across a heterogeneous system: the Intel iWarp at the CMU campus and the Cray C-90 and TM CM2 at the Pittsburgh Supercomputer Center (PSC). Specifically, the iWarp is used to generate a complex probability manifold representing a plant.
Reference: [14] <author> Chris Dalton, Greg Watson, David Banks, Costas Calamvokis, Aled Edwards, and John Lumley. </author> <title> Afterburner. </title> <journal> IEEE Network Magazine, </journal> <volume> 7(4):3643, </volume> <month> July </month> <year> 1993. </year> <month> 27 </month>
Reference-contexts: This has been demonstrated not only with the HIB for distributed-memory systems, but also for workstations <ref> [28, 14] </ref>.
Reference: [15] <author> G.B. Dantzig. </author> <title> Planning Under Uncertainty Using Parallel Computing. </title> <type> Technical Report SOL 87-1, </type> <institution> Department of Operations Research, Stanford University, </institution> <year> 1987. </year>
Reference-contexts: The application optimizes a system, for example a chemical 21 HIB plant, using a stochastic model [12] of the system. The stochastic optimization problem can be transformed into a deterministic problem using the certainty equivalence transformation <ref> [15] </ref>. Our implementation of the stochastic optimization problem [13] was distributed across a heterogeneous system: the Intel iWarp at the CMU campus and the Cray C-90 and TM CM2 at the Pittsburgh Supercomputer Center (PSC). Specifically, the iWarp is used to generate a complex probability manifold representing a plant.
Reference: [16] <author> M. de Prycker. </author> <title> Asynchronous Transfer Mode. </title> <publisher> Ellis Harwood, </publisher> <year> 1991. </year>
Reference-contexts: HIPPI supports a data rate of 800 Mbit/second or 1.6 Gbit/second. In addition to HIPPI, there are a number of high-speed network standards in various stages of development by standards bodies. These include ATM (Asynchronous Transfer Mode) <ref> [16] </ref> and Fibre Channel [26]. Meanwhile, distributed-memory computer systems [6, 23, 27, 32, 44] are becoming the architecture of choice for many supercomputer applications. The reason is that they are inherently scalable, and provide relatively inexpensive computing cycles compared with traditional uniprocessor or shared-memory multiprocessor supercomputers.
Reference: [17] <author> High Performance Fortran Forum. </author> <title> High performance fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Block-cyclic distributions are widely used by applications and are for example supported by languages such as High-Performance Fortran <ref> [17] </ref>. I/O of a distributed data structure becomes harder as the partitioning is finer.
Reference: [18] <author> The MPI Forum. </author> <title> MPI: A Message Passing Interface. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 878883, </pages> <address> Oregon, </address> <month> November </month> <year> 1993. </year> <month> ACM/IEEE. </month>
Reference-contexts: First, our target throughput is much higher. Second, we exploit the regularity of the distributions to achieve good reshufing performance; this is important, given our performance goals. Note that the data reorganization is in effect a collective communication operation <ref> [18, 4] </ref>. 9 Applicability to Other Systems We have presented a description of the architecture and implementation of a high-bandwidth network interface for the iWarp system.
Reference: [19] <author> T. Gross, D. OHallaron, and J. Subhlok. </author> <title> Task parallelism in a high performance fortran framework. </title> <booktitle> IEEE Parallel & Distributed Technology, </booktitle> <address> 2(3):1626, </address> <year> 1994. </year>
Reference-contexts: Program generators can be application-specific (e.g. Apply (image processing) [21] and Assign (signal processing) [36]), or more general (e.g. the Fx parallelizing FORTRAN compiler <ref> [19] </ref>). iWarp systems communicate with the outside world through I/O nodes that are linked into the torus at the edge of the array.
Reference: [20] <author> Thomas Gross and Peter Steenkiste. </author> <title> Architecture implications of high-speed I/O for distributed-memory computers. </title> <booktitle> In Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <pages> pages 176185, </pages> <address> Manchester, England, </address> <month> July </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: Data that is sent or received over the network is typically distributed over the private memories of the nodes. This means that the communication software has to perform scatter and gather operations to collect or distribute the data that makes up the data stream <ref> [20] </ref>. In networking terms, this is an architecture-specific data transformation that is part of the presentation layer. The three processing tasks that are hard to implement efficiently for distributed-memory systems correspond roughly to the transport, session and presentation layers of the OSI network model (Figure 2). <p> As a result, support for striping should be built into the network interface architecture. Depending on the application bandwidth requirements, striping might be needed over multiple internal links, multiple external links, or both <ref> [20] </ref>. 10 Conclusion We presented an architecture for network I/O on distributed memory systems and described its implementation for an iWarp HIPPI interface. We were able to achieve fast I/O rates over the iWarp HIPPI interface for real-world applications.
Reference: [21] <author> Leonard Hamey, John Webb, and I-Chen Wu. </author> <title> Low-Level Vision on Warp and the Apply Programming Model. </title> <editor> In J. Kowalik, editor, </editor> <booktitle> Parallel Computation and Computers for Artificial Intelligence, </booktitle> <pages> pages 185199. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1987. </year>
Reference-contexts: The parallelizing compilers translate a sequential user program into a program for each cell in the system, performing communication and computation concurrently on individual cells to achieve additional efficiency. Program generators can be application-specific (e.g. Apply (image processing) <ref> [21] </ref> and Assign (signal processing) [36]), or more general (e.g. the Fx parallelizing FORTRAN compiler [19]). iWarp systems communicate with the outside world through I/O nodes that are linked into the torus at the edge of the array.
Reference: [22] <author> John P. Hayes, Trevor Mudge, Quentin F. Stout, Stephen Colley, and John Palmer. </author> <title> A Microprocessor-based Hypercube Supercomputer. </title> <journal> IEEE Micro, </journal> <volume> 6(5):617, </volume> <month> October </month> <year> 1986. </year>
Reference-contexts: Input follows the inverse path. This approach to I/O is very common, e.g. the NCube <ref> [22] </ref>, and the Intel iPSC [32] and Paragon [25] machines follow the same approach. 4 Transport protocol processing Protocol processing (e.g. TCP or UDP over IP) is one of the potential bottlenecks in network communication.
Reference: [23] <author> Anthony J. G. Hey. </author> <title> Supercomputing with Transputers Past, Present and Future. </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pages 479489. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: HIPPI supports a data rate of 800 Mbit/second or 1.6 Gbit/second. In addition to HIPPI, there are a number of high-speed network standards in various stages of development by standards bodies. These include ATM (Asynchronous Transfer Mode) [16] and Fibre Channel [26]. Meanwhile, distributed-memory computer systems <ref> [6, 23, 27, 32, 44] </ref> are becoming the architecture of choice for many supercomputer applications. The reason is that they are inherently scalable, and provide relatively inexpensive computing cycles compared with traditional uniprocessor or shared-memory multiprocessor supercomputers.
Reference: [24] <author> S. Hinrichs. </author> <title> Programmed Communcation Service Tool Chain Users Guide. </title> <institution> Carnegie Mellon University, </institution> <note> release 2.8 edition, </note> <year> 1991. </year> <title> Now part of Intel RTS 3.0. </title>
Reference-contexts: when we discuss the applications in Section 7. 5.3 The iWarp Streams Software We give a more detailed description of the streams implementation for iWarp. 5.3.1 Data and control interface The data interface between the distributed-memory system and the network interface is based on the PCS and ESPL communication libraries <ref> [24, 7] </ref>. PCS is used to create application-specific connections, and ESPL is a fast spooling library that achieve bandwidths close to the 40 MByte/second link rate, even for short messages.
Reference: [25] <author> Intel Corporation. </author> <title> Paragon X/PS Product Overview, </title> <month> March </month> <year> 1991. </year>
Reference-contexts: Input follows the inverse path. This approach to I/O is very common, e.g. the NCube [22], and the Intel iPSC [32] and Paragon <ref> [25] </ref> machines follow the same approach. 4 Transport protocol processing Protocol processing (e.g. TCP or UDP over IP) is one of the potential bottlenecks in network communication. <p> We expect to see changes in the implementation of the control and data interfaces, the execution module and the control unit of the stream manager. The data interface will have to be reimplemented using the native communication interface for the system, e.g. Nx on the Paragon <ref> [25] </ref> or remote put/get on the Cray T3D [1]. Using a different communication interface will also affect the implementation of the execution module since it optimizes data transfers. Finally, the system software of most distributed-memory systems supports some form of multi-programming.
Reference: [26] <author> X3T9 I/O Interface. </author> <title> Fibre Channel Physical and Signaling Interface (FC-PH), </title> <address> rev. 2.2 edition, </address> <year> 1992. </year> <title> Working draft proposed American National Standard for Information Systems. </title>
Reference-contexts: HIPPI supports a data rate of 800 Mbit/second or 1.6 Gbit/second. In addition to HIPPI, there are a number of high-speed network standards in various stages of development by standards bodies. These include ATM (Asynchronous Transfer Mode) [16] and Fibre Channel <ref> [26] </ref>. Meanwhile, distributed-memory computer systems [6, 23, 27, 32, 44] are becoming the architecture of choice for many supercomputer applications. The reason is that they are inherently scalable, and provide relatively inexpensive computing cycles compared with traditional uniprocessor or shared-memory multiprocessor supercomputers.
Reference: [27] <author> K. Kaneko, M. Nakajima, Y. Nakakura, J. Nishikawa, I. Okabayashi, and H. Kadota. </author> <title> Processing Element Design for a Parallel Computer. </title> <journal> IEEE Micro, </journal> <volume> 10(2):2638, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: HIPPI supports a data rate of 800 Mbit/second or 1.6 Gbit/second. In addition to HIPPI, there are a number of high-speed network standards in various stages of development by standards bodies. These include ATM (Asynchronous Transfer Mode) [16] and Fibre Channel [26]. Meanwhile, distributed-memory computer systems <ref> [6, 23, 27, 32, 44] </ref> are becoming the architecture of choice for many supercomputer applications. The reason is that they are inherently scalable, and provide relatively inexpensive computing cycles compared with traditional uniprocessor or shared-memory multiprocessor supercomputers.
Reference: [28] <author> Karl Kleinpaste, Peter Steenkiste, and Brian Zill. </author> <title> Software support for outboard buffering and checksumming. </title> <booktitle> In Proceedings of the SIGCOMM 95 Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 8798. </pages> <publisher> ACM, </publisher> <month> August/September </month> <year> 1995. </year>
Reference-contexts: The iWarp processor is responsible for per-packet operations, such as protocol processing, while the CAB provides support for per-byte operations: data transfer, checksumming and buffering. The CAB architecture used on the HIB is similar to the Gigabit Nectar workstation CAB <ref> [42, 28] </ref>, which provides support for per-byte operations for network communication on workstations. The operation of the network interface is similar to that of a sequential system, except that the data source and sink is the distributed-memory system, not the iWarp processors on the interface. <p> This has been demonstrated not only with the HIB for distributed-memory systems, but also for workstations <ref> [28, 14] </ref>. <p> A more detailed discussion on the use of outboard buffering and checksumming, as it was implemented in a Unix protocol stack, can be found in <ref> [28] </ref>. Finally, iWarp nodes have a minimal runtime system that lacks support for utilities such as buffer management and timeouts, functions that are supported by full-edged operating systems.
Reference: [29] <author> D. Kotz. </author> <title> Multiprocessor file system interface. </title> <booktitle> In Proceedings of the Second International Conference on Parallel and Distributed Information Systems, pages 194201. </booktitle> <address> ACM/IEEE, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Kwan and Terstriep [31] show how the data distribution and data representation on the CM2 is a significant hurdle when doing I/O over the HIPPI interface, and how it limits the throughput. Data reorganization has been proposed to achieve high bandwidth access to disks in distributed-memory systems <ref> [5, 29, 38] </ref>. For example, in [5] the authors study the problem of implementing high-speed file I/O in the Intel Touchstone Delta. They observed that in order to achieve good performance it is important to send large blocks of data to the file servers that are part of the system. <p> This optimization is widely applicable and has for example also been used to optimize disk I/O on distributed-memory systems <ref> [5, 29, 38] </ref>. Systems that support very efficient communication, e.g. the remote memory reads and writes on the Cray T3D, may not require data reshufing, but any system with non-negligible per-packet overhead is likely to benefit from data reshufing for fine grain data distributions.
Reference: [30] <author> H. T. Kung and Jaspal Subhlok. </author> <title> A new approach to automatic parallelization of blocked linear algebra computations. </title> <booktitle> In Proceedings of Supercomputing 91, </booktitle> <pages> pages 122129, </pages> <address> Albequerque, </address> <month> November </month> <year> 1991. </year> <note> IEEE. </note>
Reference-contexts: The row-swath partitioning is used by the Adapt image processing environment [46]. The coarse-grain block partitioning is used by several image processing applications and fine-grain block partitioning is used in the iWarp implementation of the LAPACK library <ref> [30] </ref>. These three examples are instances of block-cyclic partitionings: the data set is divided in blocks, which are distributed in a cyclic fashion across either the rows, or the rows and columns of the distributed-memory system.
Reference: [31] <author> Thomas T. Kwan and Jeffrey A. Terstriep. </author> <title> Experiments with a gigabit neuroscience application on the cm-2. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 133142, </pages> <address> Oregon, </address> <month> November </month> <year> 1993. </year> <note> ACM/IEEE. 28 </note>
Reference-contexts: Several projects have looked at the difficulty introduced by the data distribution and representation. Kwan and Terstriep <ref> [31] </ref> show how the data distribution and data representation on the CM2 is a significant hurdle when doing I/O over the HIPPI interface, and how it limits the throughput. Data reorganization has been proposed to achieve high bandwidth access to disks in distributed-memory systems [5, 29, 38].
Reference: [32] <author> Sigurd L. Lillevik. </author> <title> The Touchstone 30 Gigaop DELTA Prototype. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference, </booktitle> <pages> pages 671677. </pages> <publisher> IEEE, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: HIPPI supports a data rate of 800 Mbit/second or 1.6 Gbit/second. In addition to HIPPI, there are a number of high-speed network standards in various stages of development by standards bodies. These include ATM (Asynchronous Transfer Mode) [16] and Fibre Channel [26]. Meanwhile, distributed-memory computer systems <ref> [6, 23, 27, 32, 44] </ref> are becoming the architecture of choice for many supercomputer applications. The reason is that they are inherently scalable, and provide relatively inexpensive computing cycles compared with traditional uniprocessor or shared-memory multiprocessor supercomputers. <p> Input follows the inverse path. This approach to I/O is very common, e.g. the NCube [22], and the Intel iPSC <ref> [32] </ref> and Paragon [25] machines follow the same approach. 4 Transport protocol processing Protocol processing (e.g. TCP or UDP over IP) is one of the potential bottlenecks in network communication.
Reference: [33] <author> Andy Nicholson, Joe Golio, David A. Borman, Jeff Young, and Wayne Roiger. </author> <title> High Speed Networking at Cray Research. </title> <journal> Computer Communication Review, </journal> <volume> 21(1):99110, </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: The reason is that they are inherently scalable, and provide relatively inexpensive computing cycles compared with traditional uniprocessor or shared-memory multiprocessor supercomputers. However, while traditional sequential or shared-memory supercomputers such as the Cray have been able to make good use of the HIPPI bandwidth <ref> [33] </ref>, distributed-memory machines have been much less successful. The network interfaces of distributed-memory machines often have low sustained bandwidth, do not perform network protocol processing, or manage connections inefficiently.
Reference: [34] <author> Doug C. Noll, J. C. Cohen, C. H. Meyer, and W. Schneider. </author> <title> Spiral k-space MRI of cortical activation. Journal of Magnetic Resonance Imaging, </title> <address> 5:4956, </address> <year> 1995. </year>
Reference-contexts: The current application represents the first step in this process: obtaining reconstructed, processed and rendered images based on Magnetic Resonance Imaging (MRI) data in a timely manner <ref> [34] </ref>. Our implementation is mapped on three architectures: the iWarp system 22 HIB at CMU performs pixel classification (brain/non-brain) and surface triangularization [35], the C-90 at PSC performs scalar processing, and the Intel Paragon at CMU performs the rendering.
Reference: [35] <author> Doug C. Noll, Jon A. Webb, and Tom E. Warfel. </author> <title> Parallel Fourier inversion by the scan-line method. </title> <journal> IEEE Transactions on Medical Imaging, </journal> <note> in press, </note> <year> 1995. </year>
Reference-contexts: The current application represents the first step in this process: obtaining reconstructed, processed and rendered images based on Magnetic Resonance Imaging (MRI) data in a timely manner [34]. Our implementation is mapped on three architectures: the iWarp system 22 HIB at CMU performs pixel classification (brain/non-brain) and surface triangularization <ref> [35] </ref>, the C-90 at PSC performs scalar processing, and the Intel Paragon at CMU performs the rendering. The input to the iWarp component of the application consists of 52 MRI slices with pixel values representing the probability of being brain tissue.
Reference: [36] <author> David R. OHallaron. </author> <title> The Assign Parallel Program Generator. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference, </booktitle> <pages> pages 178185. </pages> <publisher> IEEE, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: The parallelizing compilers translate a sequential user program into a program for each cell in the system, performing communication and computation concurrently on individual cells to achieve additional efficiency. Program generators can be application-specific (e.g. Apply (image processing) [21] and Assign (signal processing) <ref> [36] </ref>), or more general (e.g. the Fx parallelizing FORTRAN compiler [19]). iWarp systems communicate with the outside world through I/O nodes that are linked into the torus at the edge of the array.
Reference: [37] <author> Erich Rutche and Matthas Kaiserswerth. </author> <title> TCP/IP on the Parallel Protocol Engine. </title> <booktitle> In Proceedings of the 4th IFIP Conference on High Performance Networks, </booktitle> <pages> pages C2 116, </pages> <address> Liege, Belgium, </address> <month> December </month> <year> 1992. </year> <title> IFIP, </title> <publisher> Elsevier. </publisher>
Reference-contexts: There is potential parallelism between transmit and receive processing (if one is transmitting and receiving at the same time), and ACK and data processing can sometimes proceed in parallel <ref> [37] </ref>, but overall, useful parallelism is limited. For these reasons, it is desirable to have protocol processing performed in a central location, i.e. the network interface. A number of distributed systems use a similar approach [39].
Reference: [38] <author> K. E. Seamons and M. Winslett. </author> <title> Physical schemas for large multidimensional arrays in scientific computing applications. </title> <booktitle> In Seventh International Working Conference on Scientific and Statistical Database Management, </booktitle> <pages> pages 218227. </pages> <publisher> IEEE, </publisher> <month> September </month> <year> 1994. </year>
Reference-contexts: Kwan and Terstriep [31] show how the data distribution and data representation on the CM2 is a significant hurdle when doing I/O over the HIPPI interface, and how it limits the throughput. Data reorganization has been proposed to achieve high bandwidth access to disks in distributed-memory systems <ref> [5, 29, 38] </ref>. For example, in [5] the authors study the problem of implementing high-speed file I/O in the Intel Touchstone Delta. They observed that in order to achieve good performance it is important to send large blocks of data to the file servers that are part of the system. <p> This optimization is widely applicable and has for example also been used to optimize disk I/O on distributed-memory systems <ref> [5, 29, 38] </ref>. Systems that support very efficient communication, e.g. the remote memory reads and writes on the Cray T3D, may not require data reshufing, but any system with non-negligible per-packet overhead is likely to benefit from data reshufing for fine grain data distributions.
Reference: [39] <author> Raj K. Singh, Stephen G. Tell, Shaun J. Bharrat, David Becker, and Vernon L. Chi. </author> <title> A programmable hippi interface for a graphics supercomputer. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 124132, </pages> <address> Oregon, </address> <month> November </month> <year> 1993. </year> <month> ACM/IEEE. </month>
Reference-contexts: For these reasons, it is desirable to have protocol processing performed in a central location, i.e. the network interface. A number of distributed systems use a similar approach <ref> [39] </ref>. One protocol processing task that does parallelize well is the checksum calculation for the Internet protocols, and it could be performed efficiently on the distributed-memory system. Unfortunately, calculating the checksum on the system results in an odd ordering of the checksum calculation relative to protocol processing. <p> While the CBI has the advantage that it can be reused for different systems, it adds a step to the I/O process, which will increase latency and reduce throughput for smaller data transfers. The UNC HIPPI interface for the PixelPlanes system <ref> [39] </ref> is architecturally closer to the HIB, since protocol processing is performed on the network interface itself.
Reference: [40] <author> Peter Steenkiste. </author> <title> Analyzing communication latency using the nectar communication processor. </title> <booktitle> In Proceedings of the SIGCOMM 92 Symposium on Communications Architectures and Protocols, pages 199209, </booktitle> <address> Baltimore, </address> <month> August </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: The network interface can easily become a bottleneck, resulting in poor performance. Another approach is to provide a simpler network interface and to minimize the amount of work that is assigned to it by performing some of the communication tasks on the distributed-memory system itself. Earlier research <ref> [40] </ref> shows that the time spent on sending and receiving network data is distributed over several operations such as copying data, buffer management, protocol processing, and interrupt handling, and different overheads dominate depending on the circumstances (e.g. packet size).
Reference: [41] <author> Peter A. Steenkiste. </author> <title> A systematic approach to host interface design for high-speed networks. </title> <journal> IEEE Computer, </journal> <volume> 26(3):4757, </volume> <month> March </month> <year> 1994. </year>
Reference-contexts: The key to making these operations efficient is to streamline the ow of data during I/O <ref> [41] </ref>, so that the number of times that the data is touched is minimized. Example optimizations include the elimination of redundant data copy operations and the calculation of the checksum while data is being copied.
Reference: [42] <author> Peter A. Steenkiste, Brian D. Zill, H.T. Kung, Steven J. Schlick, Jim Hughes, Bob Kowalski, and John Mullaney. </author> <title> A host interface architecture for high-speed networks. </title> <booktitle> In Proceedings of the 4th IFIP Conference on High Performance Networks, </booktitle> <pages> pages A3 116, </pages> <address> Liege, Belgium, </address> <month> December </month> <year> 1992. </year> <title> IFIP, </title> <publisher> Elsevier. </publisher>
Reference-contexts: The iWarp processor is responsible for per-packet operations, such as protocol processing, while the CAB provides support for per-byte operations: data transfer, checksumming and buffering. The CAB architecture used on the HIB is similar to the Gigabit Nectar workstation CAB <ref> [42, 28] </ref>, which provides support for per-byte operations for network communication on workstations. The operation of the network interface is similar to that of a sequential system, except that the data source and sink is the distributed-memory system, not the iWarp processors on the interface. <p> The data ow on receive is exactly the inverse, and network memory holds incoming data until the application on the distributed-memory system is ready to receive it. A more detailed description of the CAB can be found in <ref> [42] </ref>. A critical requirements for high-bandwidth communication is a high memory bandwidth on the I/O node.
Reference: [43] <author> Richard Thompson. </author> <title> Los Alamos Multiple Crossbar Network Crossbar Interfaces. In Workshop on the Architecture and Implementation of High Performance Communication Subsystems. </title> <publisher> IEEE, </publisher> <month> February </month> <year> 1992. </year>
Reference-contexts: A group at Los Alamos National Labs has developed the CrossBar Interconnect (CBI) <ref> [43] </ref>). The CBI is an outboard protocol processor that performs protocol processing for supercomputers connected to HIPPI networks. It has two full-duplex HIPPI connections: one to the supercomputer and one to the HIPPI network.
Reference: [44] <author> Lewis W. Tucker and George G. Robertson. </author> <title> Architecture and Applications of the Connection Machine. </title> <journal> IEEE Computer, </journal> <volume> 21(8):2638, </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: HIPPI supports a data rate of 800 Mbit/second or 1.6 Gbit/second. In addition to HIPPI, there are a number of high-speed network standards in various stages of development by standards bodies. These include ATM (Asynchronous Transfer Mode) [16] and Fibre Channel [26]. Meanwhile, distributed-memory computer systems <ref> [6, 23, 27, 32, 44] </ref> are becoming the architecture of choice for many supercomputer applications. The reason is that they are inherently scalable, and provide relatively inexpensive computing cycles compared with traditional uniprocessor or shared-memory multiprocessor supercomputers.
Reference: [45] <author> Jon Webb. </author> <title> Latency and Bandwidth Considerations in Parallel Robotics Image Processing. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 230239, </pages> <address> Oregon, </address> <month> November </month> <year> 1993. </year> <month> ACM/IEEE. </month>
Reference-contexts: The critical features in the streams software are the support for irregular data sets and for application-specific data distributions. 7.3 Stereo-Vision In the stereo-vision application developed at CMU by Jon Webb, multi-baseline video images from four cameras are correlated to generate a depth image <ref> [45] </ref>. As part of that application, a digital VCR was implemented to display the four images on a framebuffer.
Reference: [46] <author> Jon A. Webb. </author> <title> Architecture-Independent Global Image Processing. </title> <booktitle> In 10th International Conference on Pattern Recognition, </booktitle> <pages> pages 623628, </pages> <address> Atlantic City, NJ, </address> <month> June </month> <year> 1990. </year> <journal> IEEE. </journal> <volume> 29 </volume>
Reference-contexts: Figure 12 shows some of the data mappings that are used by iWarp applications. The row-swath partitioning is used by the Adapt image processing environment <ref> [46] </ref>. The coarse-grain block partitioning is used by several image processing applications and fine-grain block partitioning is used in the iWarp implementation of the LAPACK library [30].
References-found: 46

