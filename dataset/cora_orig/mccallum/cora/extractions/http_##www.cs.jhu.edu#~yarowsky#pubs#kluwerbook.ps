URL: http://www.cs.jhu.edu/~yarowsky/pubs/kluwerbook.ps
Refering-URL: http://www.cs.jhu.edu/~yarowsky/pubs.html
Root-URL: http://www.cs.jhu.edu
Title: A COMPARISON OF CORPUS-BASED TECHNIQUES FOR RESTORING ACCENTS IN SPANISH AND FRENCH TEXT  
Author: DAVID YAROWSKY 
Address: Baltimore, MD 21218-2694  
Affiliation: Department of Computer Science Johns Hopkins University  
Abstract: This chapter will explore and compare three corpus-based techniques for lexical ambiguity resolution, focusing on the problem of restoring missing accents to Spanish and French text. Many of the ambiguities created by missing accents are differences in part of speech: hence one of the methods considered is an N-gram tagger using Viterbi decoding, such as is found in stochastic part-of-speech taggers. A second technique, Bayesian classification, has been successfully applied to word-sense disambiguation and is well suited for some of the semantic ambiguities which arise from missing accents. The third approach, based on decision lists, combines the strengths of the two other methods, incorporating both local syntactic patterns and more distant collocational evidence, and outperforms them both. The problem of accent restoration is particularly well suited for demonstrating and testing the capabilities of the given algorithms because it requires the resolution of both semantic and syntactic ambiguity, and offers an objective ground truth for automatic evaluation. The problem is also a practical one with immediate application. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Church, K.W. </author> <title> (1988) A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text, </title> <booktitle> in Proceedings of the Second Conference on Applied Natural Language Processing, ACL, </booktitle> <pages> 136-143. </pages>
Reference-contexts: METHOD 2: N-GRAM TAGGER Since most of the ambiguities due to missing accents correspond to differences in parts-of-speech, it is natural to consider the algorithm most commonly applied to the problem of part-of-speech tagging, namely the Markov or N-Gram tagger. This approach was first widely publicized in <ref> (Church, 1988) </ref> and has become the standard in the field 2 . It is not necessary, however, to train a full part-of-speech tagger for Spanish and French to restore accents. Many part-of-speech distinctions have no direct bearing on choice of accent pattern.
Reference: <author> Gale, W., K. Church, and D. </author> <month> Yarowsky </month> <year> (1992), </year> <title> A Method for Disambiguating Word Senses in a Large Corpus, </title> <journal> Computers and the Humanities, </journal> <volume> 26, </volume> <pages> 415-439. </pages>
Reference-contexts: METHOD 3: BAYESIAN CLASSIFIER Bayesian classifiers are particularly well suited for handling highly lexical-ized and longer-distance models of context, two of the central weaknesses of the previous approach. They have been employed successfully in word-sense disambiguation <ref> (Gale, Church and Yarowsky, 1992) </ref>, authorship identification (Mosteller and Wallace, 1964) and person-place classification of proper nouns (Gale, Church and Yarowsky, 1994). <p> Several smoothing methods have been explored here, including those discussed in <ref> (Gale et al., 1992) </ref>.
Reference: <author> Gale, W., K. Church, and D. </author> <month> Yarowsky </month> <year> (1994), </year> <title> Discrimination decisions for 100,000-dimensional spaces. </title> <editor> In A. Zampoli, N. Calzolari and M. Palmer (eds.), </editor> <booktitle> Current Issues in Computational Linguistics: In Honour of Don Walker, </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. </pages> <month> 429-450. </month> <title> Kupiec, Julian (1989) Probabilistic Models of Short and Long Distance Word Dependencies in Running Text, </title> <booktitle> in Proceedings, DARPA Speech and Natural Language Workshop, </booktitle> <address> Philadelphia, </address> <month> February, </month> <pages> pp. 290-295. </pages>
Reference-contexts: They have been employed successfully in word-sense disambiguation (Gale, Church and Yarowsky, 1992), authorship identification (Mosteller and Wallace, 1964) and person-place classification of proper nouns <ref> (Gale, Church and Yarowsky, 1994) </ref>. The basic technique employed is to treat a window of words surrounding each ambiguous word as a document, and ask if there are any measurable differences in the distribution of words found in the contexts surrounding one of its accent patterns relative to the other.
Reference: <author> Leacock, Claudia, </author> <title> Geoffrey Towell and Ellen Voorhees (1993) Corpus-Based Statistical Sense Resolution, </title> <booktitle> in Proceedings, ARPA Human Language Technology Workshop. </booktitle>
Reference: <author> Merialdo, B. </author> <title> (1990) `Tagging Text with a Probabilistic Model, in Proceedings of the IBM RESTORING ACCENTS IN SPANISH AND FRENCH TEXT 21 Natural Language ITL, </title> <address> Paris, France, </address> <pages> pp. 161-172. </pages>
Reference-contexts: Performance is presented in Table 2 (suffix). Another variant of Method 2 that was tested here is to use additional dictionary resources in the spirit of <ref> (Merialdo, 1990) </ref>, specifically with the Collins Spanish-English Dictionary and the Liberman-Tzoukermann morphological transducer (1990) used for extrapolation to inflected forms. In this study, the tags are traditional parts of speech (e.g. adv, adj, spron, pastpart), plus individual tags for important function words (e.g. que, ...). <p> These same words were also used to test all the other methods in this comparative study. RESTORING ACCENTS IN SPANISH AND FRENCH TEXT 7 estimation, and the EM algorithm could be used to refine B-Matrix probabilities iteratively <ref> (Merialdo, 1990) </ref>. However, the goal of this study was not to produce a full part-of-speech tagger, but to improve ambiguity resolution in accent restoration. A cost-benefit analysis could help determine whether additional resources are worth devoting to this approach.
Reference: <editor> Mosteller, Frederick, and David Wallace (1964) Inference and Disputed Authorship: </editor> <booktitle> The Federalist, </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts. </address>
Reference-contexts: METHOD 3: BAYESIAN CLASSIFIER Bayesian classifiers are particularly well suited for handling highly lexical-ized and longer-distance models of context, two of the central weaknesses of the previous approach. They have been employed successfully in word-sense disambiguation (Gale, Church and Yarowsky, 1992), authorship identification <ref> (Mosteller and Wallace, 1964) </ref> and person-place classification of proper nouns (Gale, Church and Yarowsky, 1994).
Reference: <author> Paul, D. B. </author> <title> (1990) Speech Recognition Using Hidden Markov Models, </title> <journal> in The Lincoln Laboratory Journal, </journal> <volume> 3. </volume>
Reference-contexts: Because a word's accent pattern is almost always unambiguous given a suffix and can be described in a table such as in Method 1 above, the disambiguation process is a straightforward application of the channel model. The actual algorithm used in this experiment is described in (Rabiner 1989) and <ref> (Paul 1990) </ref>. The B-matrix emit probabilities are defined as B [T AG; deaccented token] p (deaccented tokenjT AG), with transition probabilities defined analogously. The most probable tag sequence for new test data is recovered using a standard Viterbi decoder, implemented from the description in (Rabiner 1989).
Reference: <author> Rabiner, L. R. </author> <title> (1989) A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, </title> <booktitle> in Proceedings of the IEEE, </booktitle> <volume> 77, </volume> <pages> 257-285. </pages>
Reference-contexts: Because a word's accent pattern is almost always unambiguous given a suffix and can be described in a table such as in Method 1 above, the disambiguation process is a straightforward application of the channel model. The actual algorithm used in this experiment is described in <ref> (Rabiner 1989) </ref> and (Paul 1990). The B-matrix emit probabilities are defined as B [T AG; deaccented token] p (deaccented tokenjT AG), with transition probabilities defined analogously. The most probable tag sequence for new test data is recovered using a standard Viterbi decoder, implemented from the description in (Rabiner 1989). <p> is described in <ref> (Rabiner 1989) </ref> and (Paul 1990). The B-matrix emit probabilities are defined as B [T AG; deaccented token] p (deaccented tokenjT AG), with transition probabilities defined analogously. The most probable tag sequence for new test data is recovered using a standard Viterbi decoder, implemented from the description in (Rabiner 1989). The particular use of function-word and suffix sequences has several advantages, the foremost being that no large-scale lexical resources or annotated corpora are required; raw (accented) text is used for training.
Reference: <author> Rivest, R. L. </author> <title> (1987) Learning Decision Lists, </title> <booktitle> in Machine Learning, </booktitle> <volume> 2, </volume> <pages> 229-246. </pages>
Reference-contexts: METHOD 4: DECISION LISTS The limitations observed above are precisely what has motivated the development of Method 4, a hybrid approach using decision lists, combining the strengths of both Bayesian classifiers and N-gram taggers. This approach was derived from the formal model of decision lists presented in <ref> (Rivest, 1987) </ref>. However, feature conjuncts have been restricted to a much narrower complexity than allowed in the original model namely to word and class trigrams. Early results presented in (Sproat, Hirschberg and Yarowsky, 1992) achieved 97% mean accuracy on the problem of homograph resolution in text-to-speech synthesis 4 .

Reference: <author> Yarowsky, </author> <title> David (1996) Homograph Disambiguation in Speech Synthesis, </title> <note> in J. </note>
Reference: <author> Hirschberg, R. Sproat and J. van Santen (eds.), </author> <title> Progress in Speech Synthesis, Springer-Verlag. INDEX accent restoration, 1-3, 7, 11 Bayesian classification, 1 decision lists, </title> <type> 1, 9, 11-16, 18-20 lexical ambiguity resolution, 1 22 </type>
References-found: 11

