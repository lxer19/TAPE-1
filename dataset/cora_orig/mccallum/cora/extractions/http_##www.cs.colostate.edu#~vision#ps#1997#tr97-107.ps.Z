URL: http://www.cs.colostate.edu/~vision/ps/1997/tr97-107.ps.Z
Refering-URL: http://www.cs.colostate.edu/~vision/html/publications.html
Root-URL: 
Email: ross@cs.colostate.edu  draper@cs.colostate.edu  stevensm@cs.colostate.edu  hanson@cs.umass.edu  siejko@rtc.atk.com  
Phone: Phone: (970) 491-5792 Fax: (970) 491-2466  
Title: A Coregistration Approach to Multisensor Target Recognition with Extensions to Exploit Digital Elevation Map Data  
Author: J. Ross Beveridge Bruce A. Draper Mark R. Stevens Allen Hanson Kris Siejko 
Note: This work was sponsored by the Defense Advanced Research Projects Agency (DARPA) Image Understanding Program under grants DAAH04-93-G-422 and DAAH04-95-1-0447, monitored by the U. S. Army Research Office  
Web: WWW: http://www.cs.colostate.edu  
Date: September 8, 1997  
Address: Techsystems  Fort Collins, CO 80523-1873  
Affiliation: Computer Science  Colorado State University  Colorado State University  Colorado State University  University of Massachusetts  Alliant  Computer Science Department Colorado State University  
Pubnum: Technical Report  Technical Report CS-97-107  
Abstract: This document is the Final Technical Report for Contract DAAH04-93-G-0422 administered by the Army Research Office. This manuscript also appears in a collection of final reports prepared for DARPA titled Reconnaissance, Surveillance, and Target Acquisition for the Unmanned Ground Vehicle published by Morgan Kaufmann Publishers. 
Abstract-found: 1
Intro-found: 1
Reference: [ 1 ] <author> A. Akerman and R. Patton and W. De-lashmit and R. Hummel. </author> <title> Target Identification Using Geometric Hashing and FLIR/LADAR fusion. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 595 - 618, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To limit processing time, we use a less demanding algorithm to generate target and pose hypotheses. Therefore, by reducing the number of possibilities to examine during verification, processing time is reduced. While any number of algorithms might fill this role, including geometric hashing techniques <ref> [ 1 ] </ref> , we have chosen to use an existing boundary probing algorithm [ 11 ] developed by Al-liant Techsystems. 2.5.1 Range Boundary Probing Alliant Techsystem's LADAR Recognition System (LARS) has demonstrated state-of-the-art target identification performance on hundreds of frames of both real and synthetic imagery. <p> Projection is possible because both the intrinsic sensor parameters and the pose of the target are known. The gradient under each line is then estimated and converted to an error normalized to the range <ref> [0; 1] </ref>. Lines with weak gradient estimates are omitted. The range fitness error represents how well the predicted 3-D sampled surface model points fit the actual range data. The error is based on the average distance from each model point to the corresponding nearest Euclidean neighbor. <p> The weighting term 0 ff fit 1 controls the relative importance of the optical and the range data. The terms E fit;o and E fit;r are nor malized between <ref> [0; 1] </ref> based upon the expected amount of noise present in the features 4 , and consequently E fit also falls in this range. This normalization allows comparison of data from two separate sources.
Reference: [ 2 ] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Coregistering 3D Models, Range, and Optical Imagery Using Least-Median Squares Fitting. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 719-722, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Additional details on the algorithm presented here, along with one such possible extension, are described in <ref> [ 3; 2; 50 ] </ref> . 2.9.1 The Least-Squares Fitting Function The best coregistration estimate minimizes a quadratic error of fit between corresponding object model and sensor features.
Reference: [ 3 ] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Coregistration of Range and Optical Images Using Coplanarity and Orientation Constraints. </title> <booktitle> In 1996 Conference on Computer Vision and Patter Recognition, </booktitle> <pages> pages 899 - 906, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Additional details on the algorithm presented here, along with one such possible extension, are described in <ref> [ 3; 2; 50 ] </ref> . 2.9.1 The Least-Squares Fitting Function The best coregistration estimate minimizes a quadratic error of fit between corresponding object model and sensor features. <p> This normalization allows comparison of data from two separate sources. The exact derivation of the least-squares fitting function and the associated iterative update equations used to minimize the non-linear error term are presented in <ref> [ 51; 3 ] </ref> . 2.9.2 Median Filtering Extension Median filtering [ 49 ] handles outliers by fitting to the subset of the data which minimizes the ensemble median error value. It is a robust statistic when there are less than 50% outliers.
Reference: [ 4 ] <author> Dana H. Ballard and Christopher M. Brown. </author> <title> Computer Vision. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Engle-wood Cliffs, New Jersey, </address> <year> 1982. </year>
Reference-contexts: This decoupled fusion is in sharp contrast to the multisensor verification module presented below, for which geometric consistency is maintained through a single consistent manipulation of the multisensor and target geometry. 2.5.2 Avoiding Exhaustive Probing Boundary interval probing algorithms suffer from a problem common to most all template matching <ref> [ 4 ] </ref> approaches: exhaustive search in an explosive space of probes/templates is impractical. What is needed are control strategies to select probes only when they are likely to convey meaningful and helpful information, i.e., when their respective scores will be high.
Reference: [ 5 ] <author> J. Ross Beveridge. </author> <title> Local Search Algorithms for Geometric Object Recognition: Optimal Correspondence and Pose. </title> <type> PhD thesis, </type> <institution> University of Massachusetts at Amherst, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Omission introduces a bias in favor of accounting for as many model features as possible <ref> [ 5 ] </ref> . The fitness error values are summarized below and detailed in [ 44 ] . The optical fitness error represents the fidelity of match between the 3-D edge features and the underlying image. <p> Several different approaches to matching are being investigated for this problem. Foremost are a family of local search algorithms which find, with arbitrarily high probability, the optimal correspondence mapping and geometric transformation between a model and image data <ref> [ 10; 5; 6 ] </ref> . Two other techniques are also being studied. The first is a form of Genetic Algorithm called a `Messy GA' [ 24 ] . <p> Hence, the 15 minutes may be reduced to just under 2 minutes: a much more reasonable number from an operations standpoint. 4.1.4 Matching Using Local Search A complete explanation of local search matching appears in Beveridge's dissertation <ref> [ 5 ] </ref> and 3 D matching results appear in [ 8 ] . A controlled performance analysis of 2-D matching appears in [ 9 ] . To briefly review the approach, an iterative generate-and-test strategy moves from a randomly selected initial match to one that is locally optimal.
Reference: [ 6 ] <author> J. Ross Beveridge, Allen Hanson, and Durga Panda. </author> <title> Model-based Fusion of FLIR, </title> <editor> Color and LADAR. In Paul S. Schenker and Ger-ard T. McKee, editors, </editor> <booktitle> Proceedings: Sensor Fusion and Networked Robotics VIII, Proc. SPIE 2589, </booktitle> <pages> pages 2 - 11, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Several different approaches to matching are being investigated for this problem. Foremost are a family of local search algorithms which find, with arbitrarily high probability, the optimal correspondence mapping and geometric transformation between a model and image data <ref> [ 10; 5; 6 ] </ref> . Two other techniques are also being studied. The first is a form of Genetic Algorithm called a `Messy GA' [ 24 ] .
Reference: [ 7 ] <author> J. Ross Beveridge, Durga P. Panda, and Theodore Yachik. </author> <title> November 1993 Fort Carson RSTA Data Collection Final Report. </title> <type> Technical Report CSS-94-118, </type> <institution> Col-orado State University, </institution> <address> Fort Collins, CO, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Below are high-level descriptions of each and pointers sections in this chapter where each is discussed. Data Collection. Over 400 range, IR and color images of military targets against natural terrain were collected at Fort Carson, Colorado. The imagery and documentation <ref> [ 7 ] </ref> has been approved for unlimited public distribution and is available through our website at http://www.cs.colostate.edu/~vision. (Section 2.3) Target Detection Using Color. <p> The Fort Carson data has been cleared for unlimited public distribution and Colorado State maintains a data distribution homepage (http://www.cs.colostate.edu/~vision). To accompany the data, there is a 50 page report <ref> [ 7 ] </ref> describing each image, vehicle array, and ancillary information such as time of day and weather conditions. Additional information on sensor calibration may be found in [ 33 ] . The Fort Carson data meets all of our project's basic needs for algorithm development and testing. <p> Over 400 range, IR and color images were collected and this imagery has been cleared for unlimited public distribution and Colorado State maintains a data distribution homepage (http://www.cs.colostate.edu/~vision). This homepage also includes a complete data browser for the color imagery. A 50 page report <ref> [ 7 ] </ref> describes each image, vehicles present, and ancillary information such as time of day and weather conditions. <p> A detailed case-by-case breakdown is presented in Table 3. The second column indicates the vehicle shot number and vehicle array as identified in the Fort Carson data collection report <ref> [ 7 ] </ref> . The third column indicates the true target. The next five columns show the performance of the probing system, with the first four being the number of vehicle types returned out of 15 possible trials run. The fifth column shows the best probing output.
Reference: [ 8 ] <author> J. Ross Beveridge and Edward M. Riseman. </author> <title> Optimal Geometric Model Matching Under Full 3D Perspective. Computer Vision and Image Understanding, </title> <note> 61(3):351 - 364, 1995. (short version in IEEE Second CAD-Based Vision Workshop). </note>
Reference-contexts: S. Army Research Office get and scene geometry. This may be thought of as model-based sensor fusion, and contrasts with more traditional approaches that attempt to fuse data based upon low-level cues only [ 20 ] . The roots of our approach lie in past alignment-based object recognition research <ref> [ 41; 31; 8 ] </ref> . In this line of research, the value of varying 3-D object to sensor alignment during recognition has been clearly demonstrated. While this paradigm is popular in many domains, it is surprisingly absent from work on ATR. <p> Hence, the 15 minutes may be reduced to just under 2 minutes: a much more reasonable number from an operations standpoint. 4.1.4 Matching Using Local Search A complete explanation of local search matching appears in Beveridge's dissertation [ 5 ] and 3 D matching results appear in <ref> [ 8 ] </ref> . A controlled performance analysis of 2-D matching appears in [ 9 ] . To briefly review the approach, an iterative generate-and-test strategy moves from a randomly selected initial match to one that is locally optimal. <p> By running multiple trials from independently chosen initial matches, the probability of seeing the best (or near best) at least once may be made arbitrarily high. Past experience has demonstrated 100 tri als is adequate to solve most difficult problems <ref> [ 8; 9 ] </ref> . Another benefit of multiple trials is the structure and frequency of alternative solutions tells us much about the difficulty of a particular problem. Not all possible pairs of line features need be considered in matching.
Reference: [ 9 ] <author> J. Ross Beveridge, Edward M. Riseman, and Christopher Graves. </author> <title> Demonstrating Polynomial Run-Time Growth for Local Search Matching. </title> <booktitle> In Proceedings: International Symposium on Computer Vision, </booktitle> <pages> pages 533 - 538, </pages> <address> Coral Gables, Florida, </address> <month> November </month> <year> 1995. </year> <title> IEEE PAMI TC, </title> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: A controlled performance analysis of 2-D matching appears in <ref> [ 9 ] </ref> . To briefly review the approach, an iterative generate-and-test strategy moves from a randomly selected initial match to one that is locally optimal. A global least-squares fitting process always aligns model and data for any correspondence tested. Thus, global geometry implicitly directs search. <p> By running multiple trials from independently chosen initial matches, the probability of seeing the best (or near best) at least once may be made arbitrarily high. Past experience has demonstrated 100 tri als is adequate to solve most difficult problems <ref> [ 8; 9 ] </ref> . Another benefit of multiple trials is the structure and frequency of alternative solutions tells us much about the difficulty of a particular problem. Not all possible pairs of line features need be considered in matching.
Reference: [ 10 ] <author> J. Ross Beveridge, Rich Weiss, and Ed-ward M. Riseman. </author> <title> Combinatorial Optimization Applied to Variable Scale 2D Model Matching. </title> <booktitle> In Proceedings of the IEEE International Conference on Pattern Recognition 1990, Atlantic City, </booktitle> <pages> pages 18 - 23. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Several different approaches to matching are being investigated for this problem. Foremost are a family of local search algorithms which find, with arbitrarily high probability, the optimal correspondence mapping and geometric transformation between a model and image data <ref> [ 10; 5; 6 ] </ref> . Two other techniques are also being studied. The first is a form of Genetic Algorithm called a `Messy GA' [ 24 ] .
Reference: [ 11 ] <author> James E. Bevington. </author> <title> Laser Radar ATR Algorithms: Phase III Final Report. </title> <type> Technical report, </type> <institution> Alliant Techsystems, Inc., </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Therefore, by reducing the number of possibilities to examine during verification, processing time is reduced. While any number of algorithms might fill this role, including geometric hashing techniques [ 1 ] , we have chosen to use an existing boundary probing algorithm <ref> [ 11 ] </ref> developed by Al-liant Techsystems. 2.5.1 Range Boundary Probing Alliant Techsystem's LADAR Recognition System (LARS) has demonstrated state-of-the-art target identification performance on hundreds of frames of both real and synthetic imagery. <p> What is needed are control strategies to select probes only when they are likely to convey meaningful and helpful information, i.e., when their respective scores will be high. Past work on this general problem has developed hand-coded heuristics for avoiding exhaustive probing <ref> [ 11 ] </ref> and at least one algorithm has developed probe hier archies [ 12 ] to control probe use.
Reference: [ 12 ] <author> Elie Bienstock, Donald Geman, Stuart Ge-man, and Donald E. McClure. </author> <title> Development of laser radar atr algorithms: Phase ii military objects. </title> <type> Technical report, </type> <institution> Mathematical Technologies Inc., </institution> <address> Providence, Rhode Island, </address> <month> October </month> <year> 1990. </year> <title> Prepared under Harry Diamond Laboratories Contract No. </title> <publisher> DAAL02-89-C-0081. </publisher>
Reference-contexts: Past work on this general problem has developed hand-coded heuristics for avoiding exhaustive probing [ 11 ] and at least one algorithm has developed probe hier archies <ref> [ 12 ] </ref> to control probe use. In a recently initiated joint project with Professor Charles Anderson, also at Colorado State University, we have begun to explore the use of a Neural Network (NN) as a device for efficiently selecting which probes to apply and when.
Reference: [ 13 ] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. </author> <title> Classification and regression. </title> <type> Technical report, </type> <institution> Wadsworth International Group, </institution> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: MDTs are a variant on traditional univariate decision trees [ 46 ] (a.k.a. regression trees <ref> [ 13 ] </ref> ), in which a feature space is divided by selecting the feature and threshold value that best divides the target class from the background.
Reference: [ 14 ] <author> C. E. Brodley and P. E. Utgoff. </author> <title> Goal-directed Classification Using Linear Machine Decision Trees. </title> <booktitle> Machine Learning, </booktitle> <year> 1994. </year>
Reference-contexts: so that `pixels on target' values for these sensors would also be comparable to those expected in the 0.5 to 1.0 kilometer range using the RSTA sensor suite 2 . 2.4 Recognition Stage 1: Detecting Targets in Multi-spectral Imagery For the first stage of processing, a new machine learning algorithm <ref> [ 14 ] </ref> is applied to the problem of detecting camouflaged targets in multi-spectral (RGB) images. <p> Examples are shown in Figure 2 (see color plates). This allows the system to use an RGB lookup-table for classification, enabling it to operate in almost real-time on inexpensive commercial hardware. 2.4.3 Multivariate Decision Tree Learning The non-parametric classification technique we use is a multivariate decision tree (MDT) <ref> [ 14 ] </ref> . MDTs are a variant on traditional univariate decision trees [ 46 ] (a.k.a. regression trees [ 13 ] ), in which a feature space is divided by selecting the feature and threshold value that best divides the target class from the background.
Reference: [ 15 ] <author> Shashi Buluswar, Bruce A. Draper, Allen Hanson, and Edward Riseman. </author> <title> Non-parametric Classification of Pixels Under Varying Outdoor Illumination. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 1619-1626, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It should be noted that other non-parametric classifiers could also be used for this task, including back-propagation neural networks. However, as discussed in <ref> [ 15 ] </ref> , the decision surfaces for the apparent color of physical objects in an outdoor scene are well-described as piecewise planar functions in 3-D, and MDTs are therefore appropriate. <p> Over 100 color images of military targets taken on 35mm film and then digitized onto Kodak CD were used in this evaluation. In <ref> [ 15 ] </ref> , the authors evaluated the system at both a pixel and region-of-interest level. <p> Since the MDT system merges overlapping ROIs (i.e., overlapping detection rectangles), and because it is not given any depth information about the scene from which to infer target distance and therefore approximate target size, MDT returns ROIs of varying sizes. Both the author's original evalua tion <ref> [ 15 ] </ref> and the LGA evaluation simply counted the number of true and false ROIs detected, and therefore the system was evaluated less harshly if it returned one large false positive region than if it returned two smaller ones, even if the sum of the areas of the smaller regions
Reference: [ 16 ] <author> J. B. Burns, A. R. Hanson, and E. M. Rise-man. </author> <title> Extracting straight lines. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-8(4):425 - 456, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: For this problem, model and data segments are extracted from the rendered DEM and actual images respectively. An in-house implementation of the Burns algorithm <ref> [ 16 ] </ref> 7 is used to extract the line segment features. High frequency texture in these scenes prevents horizon features from being extracted unless the imagery is first smoothed: a 7x7 smoothing kernel has been used here.
Reference: [ 17 ] <author> T.W. Chen and W.C. Lin. </author> <title> A neural-network approach to csg-based 3-d object recognition. </title> <journal> pami, </journal> <volume> 16(7) </volume> <pages> 719-726, </pages> <month> july </month> <year> 1994. </year>
Reference-contexts: While there is little new about using a NN in the context of ATR <ref> [ 59; 17; 22; 35; 45 ] </ref> , what is novel about our approach is that the NN is being used primarily as a control mechanism rather than a pattern classification tool.
Reference: [ 18 ] <author> C.S. Clark, D.K. Conti, W.O. Eckhardt, T.A. McCulloh, R. Nevatia, and D.Y. Tseng. </author> <title> Matching of Natural Terrain Scenes. </title> <booktitle> In ICPR80, </booktitle> <pages> pages 217-222, </pages> <year> 1980. </year>
Reference-contexts: While the very specific problem of feature matching for orientation correction does not appear to have received much attention in the literature, there has been prior work on terrain feature matching which deserves mention. Matching of line segments representing dominant image features was proposed by Clark <ref> [ 18 ] </ref> . Levitt [ 40 ] proposed a way to select salient landmarks from terrain data [ 40 ] for navigation. Stein [ 21 ] uses panoramic horizon curve matching for vehicle localization.
Reference: [ 19 ] <author> David M. Doria and Daneil P. Huttenlocher. </author> <title> Progress on the Fast Adaptive Target Detection Program. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 719-722, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is true of the LADAR probing algorithm used above for hypothesis generation. Of course, when using LADAR, the range data itself provides the range-to-target estimate. More importantly, the IR detection and recognition systems developed by Lockheed-Martin and Hughes <ref> [ 19 ] </ref> require such an estimate. When working with IR or color imagery, it is less obvious how to generate good range-to-target estimates. Such information is not explicitly present in the optical imagery itself.
Reference: [ 20 ] <author> R. O. Eason and R. C. Gonzalez. </author> <title> Least-Squares Fusion of Multisensor Data. </title> <editor> In Mongi A. Abidi and Rafael C. Gonza-lez, editors, </editor> <booktitle> Data Fusion in Robotics and Machine Intelligence, chapter 9. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: S. Army Research Office get and scene geometry. This may be thought of as model-based sensor fusion, and contrasts with more traditional approaches that attempt to fuse data based upon low-level cues only <ref> [ 20 ] </ref> . The roots of our approach lie in past alignment-based object recognition research [ 41; 31; 8 ] . In this line of research, the value of varying 3-D object to sensor alignment during recognition has been clearly demonstrated. <p> This section describes a least-squares multisensor pose algorithm which, given a set of corresponding model and image features, recovers the associated best coregistration estimate. This algo rithm extends single sensor pose work <ref> [ 28; 36; 20; 38 ] </ref> by imposing constraints on both sensor and object geometry. Our target identification system does not currently include the least-squares multisensor pose algorithm.
Reference: [ 21 ] <author> Fridtjof Stein and Gerard Medioni. </author> <title> Map--based Localization using the Panoramic Horizon. </title> <booktitle> In Proceedings of the 1992 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 2631 - 2637, </pages> <address> Nice, France, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Matching of line segments representing dominant image features was proposed by Clark [ 18 ] . Levitt [ 40 ] proposed a way to select salient landmarks from terrain data [ 40 ] for navigation. Stein <ref> [ 21 ] </ref> uses panoramic horizon curve matching for vehicle localization. Thompson and Sutherland [ 56; 57; 55 ] have built a sophisticated expert system with a domain specific image feature extraction algorithm for abstracting structural terrain descriptions.
Reference: [ 22 ] <author> S. Ghosal and R. Mehrotra. </author> <title> Range surface characterization and segmentation using neural networks. </title> <journal> Pattern Recognition, </journal> <volume> 28(5) </volume> <pages> 711-727, </pages> <month> may </month> <year> 1995. </year>
Reference-contexts: While there is little new about using a NN in the context of ATR <ref> [ 59; 17; 22; 35; 45 ] </ref> , what is novel about our approach is that the NN is being used primarily as a control mechanism rather than a pattern classification tool.
Reference: [ 23 ] <author> F. Glover. </author> <title> Tabu search part i. </title> <journal> ORSA Journal on Computing, </journal> <volume> 1(3):190 - 206, </volume> <year> 1989. </year>
Reference-contexts: The initial scaling of the sampling interval is determined automatically, based upon moment analysis applied to the current model and sensor data sets. A variant on local search, called tabu search, is used to escape from some local optima <ref> [ 23 ] </ref> . Tabu search keeps a limited history and will explore `uphill' for a short duration to climb out of local optima. In this problem, it turns out that the regeneration of predicted target features changes the error landscape after each move.
Reference: [ 24 ] <author> David E. Goldberg, Kalyanmoy Deb, Hillol Kargupta, and Georges Harik. </author> <title> Rapid, accurate optimization of difficult problems using fast messy genetic algorithms. </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proc. 5th International Conference on Genetic Algorithms, </booktitle> <pages> pages 56-64. </pages> <publisher> Morgan-Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Two other techniques are also being studied. The first is a form of Genetic Algorithm called a `Messy GA' <ref> [ 24 ] </ref> . The second is the Haus-dorff matching algorithm developed by Hutten-locher [ 30 ] , who has graciously provided us with the code for this algorithm.
Reference: [ 25 ] <author> M. E. Goss, J. R. Beveridge, M. Stevens, and A. Fuegi. </author> <title> Three-dimensional visualization environment for multisensor data analysis, interpretation, and model-based object recognition. </title> <booktitle> In IS&T/SPIE Symposium on Electronic Imaging: Science & Technology, </booktitle> <pages> pages 283 - 291, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Two generations of interactive 3-D graphical user interfaces have been developed under this project <ref> [ 26; 25; 54 ] </ref> . views from our original RangeView system, and it provides a visual overview of the data, object models, and relationships of interest in our work. A FLIR image of an M60 tank is shown in the upper left corner of the figure.
Reference: [ 26 ] <author> Michael E. Goss, J. Ross Beveridge, Mark Stevens, and Aaron Fuegi. </author> <title> Visualization and Verification of Automatic Target Recognition Results Using Combined Range and Optical Imagery. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 491 - 494, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Two generations of interactive 3-D graphical user interfaces have been developed under this project <ref> [ 26; 25; 54 ] </ref> . views from our original RangeView system, and it provides a visual overview of the data, object models, and relationships of interest in our work. A FLIR image of an M60 tank is shown in the upper left corner of the figure.
Reference: [ 27 ] <author> W. Eric L. </author> <title> Grimson. Object Recognition by Computer: The Role of Geometric Constraints. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Thus, global geometry implicitly directs search. A match error takes account both of spatial fit and omission: how much of the model is un-matched. Search is conducted over a space of correspondence mappings C: C is the powerset of possibly matching features S. Most other algorithms consider one-to-many matches <ref> [ 27 ] </ref> while our C includes many-to-many matches. Without many-to-many mappings, properly match ing piecewise approximations to curves with non-coincident breakpoints is impossible. This point is important here because horizon lines involve such non-coincident breakpoints.
Reference: [ 28 ] <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This section describes a least-squares multisensor pose algorithm which, given a set of corresponding model and image features, recovers the associated best coregistration estimate. This algo rithm extends single sensor pose work <ref> [ 28; 36; 20; 38 ] </ref> by imposing constraints on both sensor and object geometry. Our target identification system does not currently include the least-squares multisensor pose algorithm.
Reference: [ 29 ] <author> Hua Chen and Lawrence B. Wolff. </author> <title> A Polarization Phase-Based Method For Material Classification And Object Recognition in Computer Vision. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 1297 - 1303, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: It should also be noted that although this work was designed for work on RGB images, the general approach is applicable to any multi-spectral image source, including multi-band IR or polarimetric imagery <ref> [ 39; 29 ] </ref> . 2.4.1 Color Complements IR In most ATR systems, targets are detected in 3-5 micron infra-red (IR) images.
Reference: [ 30 ] <author> Daniel P. Huttenlocher, Gregory A. Klander-man, and William J. Rucklidge. </author> <title> Comparing images using the hausdorff distance. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <pages> pages 850 - 862, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Two other techniques are also being studied. The first is a form of Genetic Algorithm called a `Messy GA' [ 24 ] . The second is the Haus-dorff matching algorithm developed by Hutten-locher <ref> [ 30 ] </ref> , who has graciously provided us with the code for this algorithm. <p> However, based upon the performance with larger problems the messy GA should solve the smaller problems in several minutes. 4.1.6 Matching with the Hausdorff Metric A modest effort was made to match horizons using the Hausdorff metric using a software package graciously provided to us by Huttenlocher <ref> [ 30 ] </ref> . Two binary images are input to the system where `on' pixels represent local edges in the imagery. We used local edges extracted from the thresh-olded (sky/ground) rendered terrain and from the grey-scale imagery. <p> As suggested, the optimal matching recovers the scale change between rendered terrain and imagery. In future, perhaps in collaboration with Hutten--locher <ref> [ 30 ] </ref> , we hope to devote the time and energy to conduct a more thorough study. 4.2 Automated Orientation Correction Tests on Demo C Site Data To study the ability of the full system described above to improve vehicle orientation estimates, an early test-of-concept experiment has been conducted for
Reference: [ 31 ] <author> Daniel P. Huttenlocher and Shimon Ull-man. </author> <title> Recognizing Solid Objects by Alignment with an Image. </title> <journal> International Journal of Computer Vision, </journal> <volume> 5(2):195 - 212, </volume> <month> Novem-ber </month> <year> 1990. </year>
Reference-contexts: S. Army Research Office get and scene geometry. This may be thought of as model-based sensor fusion, and contrasts with more traditional approaches that attempt to fuse data based upon low-level cues only [ 20 ] . The roots of our approach lie in past alignment-based object recognition research <ref> [ 41; 31; 8 ] </ref> . In this line of research, the value of varying 3-D object to sensor alignment during recognition has been clearly demonstrated. While this paradigm is popular in many domains, it is surprisingly absent from work on ATR.
Reference: [ 32 ] <author> Jacques G. Verly and Richard T. Lacoss. </author> <title> Automatic Target Recognition for LADAR imagery Using Functional Templates Derived From 3-D CAD Models. </title> <editor> In Oscar Firschein, editor, Reconnaissance, </editor> <title> Surveillance, and Target Acquisition (RSTA) for the Unmanned Ground Vehicle. </title> <publisher> Morgan Kauf-mann, </publisher> <year> 1997. </year>
Reference-contexts: Only one other RSTA group has performed target identification on this dataset, and they report that using a template approach on Range Imagery alone they will reliably solve only 4 out of the 35 test cases <ref> [ 32 ] </ref> . The geometrically precise multisensor identification algorithm is computationally demanding. To reduce processing, focus-of-attention algorithms are used to perform detection and suggest possible target type and pose hypotheses. Each of these upstream processes is itself a major component of our project. <p> We believe this goal has been met. To our knowledge, only one other organization has carried out target identification on this data, and that is the group from MIT Lincoln Laboratory. The Fort Carson dataset has been used in part of the evaluation of their own range-only ATR system <ref> [ 32 ] </ref> . The MIT group has also developed a set of correct-recognition performance curves that allow them to predict the best performance they can expect to achieve for given operating parameters (range, depression angle, noise, etc).
Reference: [ 33 ] <author> Zhongfei Zhang J.R. Beveridge, </author> <title> M.R. Stevens and M.E. Goss. Approximate Image Mappings Between Nearly Boresight Aligned Optical and Range Sensors. </title> <type> Technical Report CS-96-112, </type> <institution> Computer Science, Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: To accompany the data, there is a 50 page report [ 7 ] describing each image, vehicle array, and ancillary information such as time of day and weather conditions. Additional information on sensor calibration may be found in <ref> [ 33 ] </ref> . The Fort Carson data meets all of our project's basic needs for algorithm development and testing. Specifically, it includes Range, IR and Color imagery for military vehicles positioned in natural terrain. <p> The specific problem of interest in the context of RSTA is that of near-boresight-aligned sensors. A detailed study of different sources of uncertainty in alignment for near-boresight-aligned sensors appears in <ref> [ 33 ] </ref> . Briefly, a useful heuristic falls out of this study: over small rotations and restricted depth ranges, sensor-to-sensor rotation may be approximated with simpler coplanar sensor-to-sensor translation. This approximation is illustrated for two sensors in Figure 5. translate relative to the object. <p> This homepage also includes a complete data browser for the color imagery. A 50 page report [ 7 ] describes each image, vehicles present, and ancillary information such as time of day and weather conditions. Additional information on the sensor calibration may be found in <ref> [ 33 ] </ref> . 3.2 How Difficult is the Fort Carson Dataset? The Fort Carson dataset was designed to contain challenging target identification problems requiring advancements to the state-of-the-art in ATR. We believe this goal has been met.
Reference: [ 34 ] <author> D. Judd, D. MacAdam, and G. Wyszecki. </author> <title> Spectral distribution of typical daylight as a function of correlated color temperature. </title> <journal> Journal of the Optical Society of America, </journal> <volume> 54(8) </volume> <pages> 1031-1040, </pages> <year> 1964. </year>
Reference-contexts: Fortunately, shifts in the apparent color of targets are not random; there is a limited range of colors that natural daylight can assume <ref> [ 34 ] </ref> , even given various ratios of sunlight to skylight, and a limited blue-shift created by atmospheric humidity.
Reference: [ 35 ] <author> Alana Katz and Philip Thrift. </author> <title> Hybrid neural network classifiers for automatic target detection. </title> <journal> Expert Systems, </journal> <volume> 10(4):243, </volume> <month> Novem-ber </month> <year> 1993. </year>
Reference-contexts: While there is little new about using a NN in the context of ATR <ref> [ 59; 17; 22; 35; 45 ] </ref> , what is novel about our approach is that the NN is being used primarily as a control mechanism rather than a pattern classification tool.
Reference: [ 36 ] <author> Rakesh Kumar. </author> <title> Determination of Camera Location and Orientation. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 870 - 881, </pages> <address> Los Altos, CA, </address> <month> June </month> <year> 1989. </year> <title> DARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This section describes a least-squares multisensor pose algorithm which, given a set of corresponding model and image features, recovers the associated best coregistration estimate. This algo rithm extends single sensor pose work <ref> [ 28; 36; 20; 38 ] </ref> by imposing constraints on both sensor and object geometry. Our target identification system does not currently include the least-squares multisensor pose algorithm. <p> E fit = ff fit E fit;o + (1 ff fit )E fit;r (6) The constituent parts of E fit are illustrated in features. This term is precisely the point-to-plane error criterion defined by Kumar <ref> [ 36; 38 ] </ref> for computing camera-to-model pose 3 . The second, E fit;r , is the sum-of-squared Euclidean distances between corresponding model and range points.
Reference: [ 37 ] <author> Rakesh Kumar. </author> <title> Model Dependent Inference of 3D Information From a Sequence of 2D Images. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, COINS TR92-04, Amherst, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: The subsets must be at least large enough to cover the degrees of freedom, so at least three optical lines and one range point are needed. However, Kumar <ref> [ 37 ] </ref> found that selecting a minimal number of features caused the solution to be sensitive to the Gaussian noise that we assume is overlaid onto the true data. As a consequence, it is better to select a larger subset to stabilize the optimal pose against noise.
Reference: [ 38 ] <author> Rakesh Kumar and Allen R. Hanson. </author> <title> Robust methods for estimating pose and a sensitivity analysis. </title> <booktitle> CVGIP:Image Understanding, </booktitle> <volume> 11, </volume> <year> 1994. </year>
Reference-contexts: This section describes a least-squares multisensor pose algorithm which, given a set of corresponding model and image features, recovers the associated best coregistration estimate. This algo rithm extends single sensor pose work <ref> [ 28; 36; 20; 38 ] </ref> by imposing constraints on both sensor and object geometry. Our target identification system does not currently include the least-squares multisensor pose algorithm. <p> E fit = ff fit E fit;o + (1 ff fit )E fit;r (6) The constituent parts of E fit are illustrated in features. This term is precisely the point-to-plane error criterion defined by Kumar <ref> [ 36; 38 ] </ref> for computing camera-to-model pose 3 . The second, E fit;r , is the sum-of-squared Euclidean distances between corresponding model and range points.
Reference: [ 39 ] <author> Lawrence B. Wolff. </author> <title> Reflectance Modeling for Object Recognition and Detection in Outdoor Scenes. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 799 - 803, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It should also be noted that although this work was designed for work on RGB images, the general approach is applicable to any multi-spectral image source, including multi-band IR or polarimetric imagery <ref> [ 39; 29 ] </ref> . 2.4.1 Color Complements IR In most ATR systems, targets are detected in 3-5 micron infra-red (IR) images.
Reference: [ 40 ] <author> T.S. Levitt, D.T. Lawton, </author> <title> D.M. Chelberg, and P.C. Nelson. Qualitative landmark-based path planning and following. </title> <booktitle> In AAAI-87, </booktitle> <pages> pages 689-694, </pages> <year> 1987. </year>
Reference-contexts: Matching of line segments representing dominant image features was proposed by Clark [ 18 ] . Levitt <ref> [ 40 ] </ref> proposed a way to select salient landmarks from terrain data [ 40 ] for navigation. Stein [ 21 ] uses panoramic horizon curve matching for vehicle localization. <p> Matching of line segments representing dominant image features was proposed by Clark [ 18 ] . Levitt <ref> [ 40 ] </ref> proposed a way to select salient landmarks from terrain data [ 40 ] for navigation. Stein [ 21 ] uses panoramic horizon curve matching for vehicle localization. Thompson and Sutherland [ 56; 57; 55 ] have built a sophisticated expert system with a domain specific image feature extraction algorithm for abstracting structural terrain descriptions.
Reference: [ 41 ] <author> David G. Lowe. </author> <title> Perceptual Organization and Visual Recognition. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1985. </year>
Reference-contexts: S. Army Research Office get and scene geometry. This may be thought of as model-based sensor fusion, and contrasts with more traditional approaches that attempt to fuse data based upon low-level cues only [ 20 ] . The roots of our approach lie in past alignment-based object recognition research <ref> [ 41; 31; 8 ] </ref> . In this line of research, the value of varying 3-D object to sensor alignment during recognition has been clearly demonstrated. While this paradigm is popular in many domains, it is surprisingly absent from work on ATR.
Reference: [ 42 ] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Interleaving 3D Model Feature Prediction and Matching to Support Multi-Sensor Object Recognition. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 699-706, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We have already developed algorithms to convert these models to a level of detail more appropriate for matching to the given sensor data [ 53; 52 ] . Another system, summarized here and fully described in <ref> [ 42 ] </ref> , has been developed to extract edge and surface information from these models.
Reference: [ 43 ] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Optical Linear Feature Detection Based on Model Pose. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 695-697, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The second, E fit;r , is the sum-of-squared Euclidean distances between corresponding model and range points. The edge features in the optical imagery are found using a model-directed edge extraction technique described in <ref> [ 43 ] </ref> . fine optimal coregistration. 3 After developing this measure, Kumar developed others which are more robust but which also require additional normalization. The weighting term 0 ff fit 1 controls the relative importance of the optical and the range data.
Reference: [ 44 ] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Precise Matching of 3-D Target Models to Mul-tisensor Data. </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 6(1) </volume> <pages> 126-142, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: Omission introduces a bias in favor of accounting for as many model features as possible [ 5 ] . The fitness error values are summarized below and detailed in <ref> [ 44 ] </ref> . The optical fitness error represents the fidelity of match between the 3-D edge features and the underlying image. The process of determining the error begins by projecting the predicted 3-D model edges into the optical imagery.
Reference: [ 45 ] <author> L.I. Perlovsky, J.A. Chernick, and W.H. Schoendorf. </author> <title> Multisensor atr and identification of friend or foe using mlans. Neural Networks, </title> <address> 8(7-8):1185-1200, </address> <year> 1995. </year>
Reference-contexts: While there is little new about using a NN in the context of ATR <ref> [ 59; 17; 22; 35; 45 ] </ref> , what is novel about our approach is that the NN is being used primarily as a control mechanism rather than a pattern classification tool.
Reference: [ 46 ] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: MDTs are a variant on traditional univariate decision trees <ref> [ 46 ] </ref> (a.k.a. regression trees [ 13 ] ), in which a feature space is divided by selecting the feature and threshold value that best divides the target class from the background.
Reference: [ 47 ] <author> Ray Rimey. </author> <title> RSTA Sept94 Data Collection Final Report. </title> <type> Technical report, </type> <institution> Martin Ma-rietta Astronautics, </institution> <address> Denver, CO, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: This site was selected because test imagery taken directly from the SSV is available along with ground truth indicating vehicle position and pointing angle relative to fixed targets <ref> [ 47 ] </ref> . A terrain-rendering system has been developed using Open-GL which simulates the FOV of the CCD sensor used on the SSV. A simple lighting model is used and terrain is rendered from positions at which the vehicle actually acquired imagery.
Reference: [ 48 ] <author> Ray Rimey and Darrell Hougen. </author> <title> Discussion of SSV Orientation Correction with Lockheed-Martin RSTA Group. </title> <type> Personal Correspondence, </type> <year> 1995. </year>
Reference-contexts: Orientation estimates can be off by one or more degrees <ref> [ 48 ] </ref> . The resulting uncertainty precludes terrain guided visual search and target recognition. To correct this uncertainty in pointing angle, the SSV transmits imagery from a sweep of the terrain to the operator work station.
Reference: [ 49 ] <author> Peter. J. Rousseeuw and Annick. M. Leroy. </author> <title> Robust regression & outlier detection. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: This normalization allows comparison of data from two separate sources. The exact derivation of the least-squares fitting function and the associated iterative update equations used to minimize the non-linear error term are presented in [ 51; 3 ] . 2.9.2 Median Filtering Extension Median filtering <ref> [ 49 ] </ref> handles outliers by fitting to the subset of the data which minimizes the ensemble median error value. It is a robust statistic when there are less than 50% outliers.
Reference: [ 50 ] <author> Anthony N. A. Schwickerath. </author> <title> Simultaneous refinement of pose and sensor registration. </title> <type> Master's thesis, </type> <institution> Colorado State University (TechReport CS-97-101), </institution> <year> 1996. </year>
Reference-contexts: Additional details on the algorithm presented here, along with one such possible extension, are described in <ref> [ 3; 2; 50 ] </ref> . 2.9.1 The Least-Squares Fitting Function The best coregistration estimate minimizes a quadratic error of fit between corresponding object model and sensor features.
Reference: [ 51 ] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Model to Multisensor Coregistra-tion with Eight Degrees of Freedom. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 481 - 490, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This normalization allows comparison of data from two separate sources. The exact derivation of the least-squares fitting function and the associated iterative update equations used to minimize the non-linear error term are presented in <ref> [ 51; 3 ] </ref> . 2.9.2 Median Filtering Extension Median filtering [ 49 ] handles outliers by fitting to the subset of the data which minimizes the ensemble median error value. It is a robust statistic when there are less than 50% outliers. <p> Both tests were run on four synthetic models. The models exhibit different geometric characteristics including planarity or lack of planarity, symmetry or lack of symmetry, and few versus many features. Complete results of these tests are reported in <ref> [ 51 ] </ref> . In Test 1, we found that, given perfect image data, the algorithm could reliably recover and correct coregistration given up to a 30 ffi error in orientation. <p> Test II shows that, given modest image noise ( = 1 for both sensors), the final rotation error was within 1 ffi of the correct value. With significantly higher errors, though, ( = 5 for both 5 The weights are the combined threshold and ff fit term described in <ref> [ 51 ] </ref> sensors), coregistration yielded a final rotation er-ror around 5 ffi . 2.9.4 Least-Median Squares on Real Data In our previous work [ 51 ] , instabilities and pathological behavior were found when running coreg-istration on hand-picked features. <p> With significantly higher errors, though, ( = 5 for both 5 The weights are the combined threshold and ff fit term described in <ref> [ 51 ] </ref> sensors), coregistration yielded a final rotation er-ror around 5 ffi . 2.9.4 Least-Median Squares on Real Data In our previous work [ 51 ] , instabilities and pathological behavior were found when running coreg-istration on hand-picked features. This behavior has been traced to outliers present in the handpicked data. To address this issue, median filtering is used to construct outlier-free correspondences.
Reference: [ 52 ] <author> Mark R. Stevens. </author> <title> Obtaining 3D Silhouettes and Sampled Surfaces from Solid Models for use in Computer Vision. </title> <type> Master's thesis, </type> <institution> Colorado State University, </institution> <address> Fort Collins, Col-orado, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: We have already developed algorithms to convert these models to a level of detail more appropriate for matching to the given sensor data <ref> [ 53; 52 ] </ref> . Another system, summarized here and fully described in [ 42 ] , has been developed to extract edge and surface information from these models.
Reference: [ 53 ] <author> Mark R. Stevens, J. Ross Beveridge, and Michael E. Goss. </author> <title> Reduction of BRL/CAD Models and Their Use in Automatic Target Recognition Algorithms. </title> <booktitle> In Proceedings: BRL-CAD Symposium. </booktitle> <institution> Army Research Labs, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: We have already developed algorithms to convert these models to a level of detail more appropriate for matching to the given sensor data <ref> [ 53; 52 ] </ref> . Another system, summarized here and fully described in [ 42 ] , has been developed to extract edge and surface information from these models.
Reference: [ 54 ] <author> Mark R. Stevens, J. Ross Beveridge, and Mike Goss. </author> <title> Visualization of multi-sensor model-based object recognition. </title> <journal> IEEE Transactions on Visualization and Computer Graphics, </journal> <note> (submitted). </note>
Reference-contexts: Two generations of interactive 3-D graphical user interfaces have been developed under this project <ref> [ 26; 25; 54 ] </ref> . views from our original RangeView system, and it provides a visual overview of the data, object models, and relationships of interest in our work. A FLIR image of an M60 tank is shown in the upper left corner of the figure.
Reference: [ 55 ] <author> Sutherland, K.T. and Thompson, W.B. </author> <title> Localizing in Unstructured Environments: Dealing with the Errors. </title> <journal> Robotics and Automation, </journal> <volume> 10 </volume> <pages> 740-754, </pages> <year> 1994. </year>
Reference-contexts: Levitt [ 40 ] proposed a way to select salient landmarks from terrain data [ 40 ] for navigation. Stein [ 21 ] uses panoramic horizon curve matching for vehicle localization. Thompson and Sutherland <ref> [ 56; 57; 55 ] </ref> have built a sophisticated expert system with a domain specific image feature extraction algorithm for abstracting structural terrain descriptions.
Reference: [ 56 ] <author> W.B. Thompson, </author> <title> T.C. Henderson, T.L. Colvin, L.B. Dick, and C.M. Valiquette. Vision-Based Localization. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 491-498, </pages> <address> Los Altos, CA, </address> <year> 1993. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Levitt [ 40 ] proposed a way to select salient landmarks from terrain data [ 40 ] for navigation. Stein [ 21 ] uses panoramic horizon curve matching for vehicle localization. Thompson and Sutherland <ref> [ 56; 57; 55 ] </ref> have built a sophisticated expert system with a domain specific image feature extraction algorithm for abstracting structural terrain descriptions.
Reference: [ 57 ] <author> Thompson, W.B. and Pick, Jr., H.L. </author> <title> Vision-Based Navigation. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 127-131, </pages> <address> Los Altos, CA, </address> <year> 1993. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Levitt [ 40 ] proposed a way to select salient landmarks from terrain data [ 40 ] for navigation. Stein [ 21 ] uses panoramic horizon curve matching for vehicle localization. Thompson and Sutherland <ref> [ 56; 57; 55 ] </ref> have built a sophisticated expert system with a domain specific image feature extraction algorithm for abstracting structural terrain descriptions.
Reference: [ 58 ] <author> U. S. </author> <note> Army Ballistic Research Laboratory. BRL-CAD User's Manual, release 4.0 edition, </note> <month> December </month> <year> 1991. </year>
Reference-contexts: The three key elements in this process are: feature prediction, match evaluation, and local search. Each of these elements is described below. 2.7.1 On-line Model Feature Prediction Highly detailed Constructive Solid Geometry (CSG) models of target vehicles are available in BRL-CAD format <ref> [ 58 ] </ref> . We have already developed algorithms to convert these models to a level of detail more appropriate for matching to the given sensor data [ 53; 52 ] .
Reference: [ 59 ] <author> Allen M. Waxman, Micheal C. Seibert, Alan Gove, David A. Fay, Ann Marie Bernar-don, Carol Lazott, William R. Steele, and Robert K. Cunningham. </author> <title> Neural processing of targets in visible, multispectral ir and sar imagery. Neural Networks, </title> <address> 8(7/8):1029, </address> <month> may </month> <year> 1995. </year> <title> a. Image with ROI boxes overlaid b. Summed values from which ROIs are derived a. </title> <editor> Initial Coregistration b. Refined Coregistration a. Initial Coregistration b. </editor> <publisher> Refined Coregistration </publisher>
Reference-contexts: While there is little new about using a NN in the context of ATR <ref> [ 59; 17; 22; 35; 45 ] </ref> , what is novel about our approach is that the NN is being used primarily as a control mechanism rather than a pattern classification tool.
References-found: 59

