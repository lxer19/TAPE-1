URL: ftp://ftp.idsia.ch/pub/juergen/chunker.ps.gz
Refering-URL: http://www.idsia.ch/~juergen/topics.html
Root-URL: 
Title: LEARNING COMPLEX, EXTENDED SEQUENCES USING THE PRINCIPLE OF HISTORY COMPRESSION (Neural Computation, 4(2):234-242, 1992)  
Author: Jurgen Schmidhuber 
Address: Arcisstr. 21, 8000 Munchen 2, Germany  
Affiliation: Institut fur Informatik Technische Universitat Munchen  
Abstract: Previous neural network learning algorithms for sequence processing are computationally expensive and perform poorly when it comes to long time lags. This paper first introduces a simple principle for reducing the descriptions of event sequences without loss of information. A consequence of this principle is that only unexpected inputs can be relevant. This insight leads to the construction of neural architectures that learn to `divide and conquer' by recursively decomposing sequences. I describe two architectures. The first functions as a self-organizing multi-level hierarchy of recurrent networks. The second, involving only two recurrent networks, tries to collapse a multi-level predictor hierarchy into a single recurrent net. Experiments show that the system can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets.
Abstract-found: 1
Intro-found: 1
Reference: <author> Hochreiter, J. </author> <year> (1991). </year> <type> Diploma thesis. </type> <institution> Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference-contexts: Often a multi-level predictor hierarchy will be the fastest way of learning to deal with sequences with multi-level temporal structure (e.g speech). Experiments have shown that multi-level predictors can quickly learn tasks which are practically unlearnable by conventional recurrent networks, e.g. <ref> (Hochreiter, 1991) </ref>. One disadvantage of a predictor hierarchy, however, is that it is not known in advance how many levels will be needed. Another disadvantage is that levels are explicitly separated from each other. <p> See <ref> (Hochreiter, 1991) </ref> and (Schmidhuber, 1991c) for details. A prediction task with a 20-step time lag was constructed. There were 22 possible input symbols a; x; b 1 ; b 2 ; : : : ; b 20 . The learning systems observed one input symbol at a time.
Reference: <author> Miyata, Y. </author> <year> (1988). </year> <title> An unsupervised PDP learning model for action planning. </title> <booktitle> In Proc. of the Tenth Annual Conference of the Cognitive Science Society, </booktitle> <address> Hillsdale, NJ, </address> <pages> pages 223-229. </pages> <publisher> Erlbaum. </publisher>
Reference-contexts: A hierarchical approach to sequence generation was pursued by <ref> (Miyata, 1988) </ref>. 2 For instance, we might employ the more limited feedforward networks and a `time window' approach.
Reference: <author> Mozer, M. C. </author> <year> (1990). </year> <title> Connectionist music composition based on melodic, stylistic, and psychophysical constraints. </title> <type> Technical Report CU-CS-495-90, </type> <institution> University of Colorado at Boulder. </institution>
Reference-contexts: With many applications, a second drawback of these methods is the following: The longer the time lag between an event and the occurrence of a corresponding error the less information is carried by the corresponding back-propagated error signals. <ref> (Mozer, 1990) </ref> and (Rohwer, 1989) have addressed the latter problem but not the former. <p> For instance, one can imagine situations where A unlearns previously learned predictions because of the third term of its error function. Relative weighting of the different terms in A's error function represents an 4 In contrast, the reduced descriptions referred to by <ref> (Mozer, 1990) </ref> are not unambiguous. vector description (referring to time t) dimension x (t) `normal' environmental input n I d (t) teacher-defined target n D i A (t) = x (t) ffi d (t) A's input n I + n D h A (t) A's hidden activations n H A d
Reference: <author> Robinson, A. J. and Fallside, F. </author> <year> (1987). </year> <title> The utility driven dynamic error propagation network. </title> <type> Technical Report CUED/F-INFENG/TR.1, </type> <institution> Cambridge University Engineering Department. </institution>
Reference-contexts: 1 INTRODUCTION Several approaches to on-line supervised sequence learning have been proposed, including back-propagation through time or BPTT, e.g. (Williams and Peng, 1990), the IID- or RTRL-algorithm <ref> (Robinson and Fallside, 1987) </ref>(Williams and Zipser, 1989), and the recent fast-weight algorithm (Schmidhuber, 1991b)). These approaches are computationally intensive; BPTT is not local in time, RTRL-like algorithms and also their more efficient recent relatives (Schmidhuber, 1991d) are not local in space (Schmidhuber, 1991c). <p> P i denotes the ith level network which is trained to predict its own next input from its previous inputs 1 . We take P i to be a conventional dynamic recurrent neural network <ref> (Robinson and Fallside, 1987) </ref>(Williams and Zipser, 1989)(Williams and Peng, 1990)(Schmidhuber, 1991d); however, it might be some other adaptive sequence processing device as well 2 . At each time step the input of the lowest-level recurrent predictor P 0 is the current external input.
Reference: <author> Rohwer, R. </author> <year> (1989). </year> <title> The `moving targets' training method. </title> <editor> In Kindermann, J. and Linden, A., editors, </editor> <booktitle> Proceedings of `Distributed Adaptive Neural Information Processing', </booktitle> <address> St.Augustin, 24.-25.5,. </address> <publisher> Oldenbourg. </publisher>
Reference-contexts: With many applications, a second drawback of these methods is the following: The longer the time lag between an event and the occurrence of a corresponding error the less information is carried by the corresponding back-propagated error signals. (Mozer, 1990) and <ref> (Rohwer, 1989) </ref> have addressed the latter problem but not the former.
Reference: <author> Schmidhuber, J. H. </author> <year> (1991a). </year> <title> Adaptive decomposition of time. </title> <editor> In Kohonen, T., Makisara, K., Simula, O., and Kangas, J., editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <pages> pages 909-914. </pages> <publisher> Elsevier Science Publishers B.V., North-Holland. </publisher>
Reference-contexts: This will happen if the incoming inputs carry global temporal structure which has not yet been discovered by P s . This method is a simplification and an improvement of the recent chunking method described by <ref> (Schmidhuber, 1991a) </ref>. Often a multi-level predictor hierarchy will be the fastest way of learning to deal with sequences with multi-level temporal structure (e.g speech). Experiments have shown that multi-level predictors can quickly learn tasks which are practically unlearnable by conventional recurrent networks, e.g. (Hochreiter, 1991). <p> Once events become expected, they tend to become `subconscious'. There is an obvious analogy to the chunking algorithm: The chunker's attention is removed from events that become expected; they become `subconscious' (automatized) and give rise to even higher-level `abstractions' of the chunker's `consciousness'. The chunking systems described in <ref> (Schmidhuber, 1991a) </ref>, (Schmidhuber, 1991c) and the current paper try to detect temporal regularities and learn to use them for identifying relevant points in time.
Reference: <author> Schmidhuber, J. H. </author> <year> (1991b). </year> <title> Learning to control fast-weight memories: An alternative to recurrent nets. </title> <type> Technical Report FKI-147-91, </type> <institution> Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference-contexts: 1 INTRODUCTION Several approaches to on-line supervised sequence learning have been proposed, including back-propagation through time or BPTT, e.g. (Williams and Peng, 1990), the IID- or RTRL-algorithm (Robinson and Fallside, 1987)(Williams and Zipser, 1989), and the recent fast-weight algorithm <ref> (Schmidhuber, 1991b) </ref>). These approaches are computationally intensive; BPTT is not local in time, RTRL-like algorithms and also their more efficient recent relatives (Schmidhuber, 1991d) are not local in space (Schmidhuber, 1991c).
Reference: <author> Schmidhuber, J. H. </author> <year> (1991c). </year> <title> Neural sequence chunkers. </title> <type> Technical Report FKI-148-91, </type> <institution> Institut fur Infor-matik, Technische Universitat Munchen. </institution>
Reference-contexts: These approaches are computationally intensive; BPTT is not local in time, RTRL-like algorithms and also their more efficient recent relatives (Schmidhuber, 1991d) are not local in space <ref> (Schmidhuber, 1991c) </ref>. Common to all of these approaches is that they do not try to selectively focus on relevant inputs; they waste efficiency and resources by focussing on every input. <p> In the experiments (presented in section 5) relative weighting was not necessary. 4.2 DETAILS OF THE 2-NET CHUNKING ARCHITECTURE The system described below is the on-line version of a representative of a number of variations of the basic principle described in 4.1. See <ref> (Schmidhuber, 1991c) </ref> for various modifications. Table 1 gives an overview of various time-dependent activation vectors relevant for the description of the algorithm. <p> See (Hochreiter, 1991) and <ref> (Schmidhuber, 1991c) </ref> for details. A prediction task with a 20-step time lag was constructed. There were 22 possible input symbols a; x; b 1 ; b 2 ; : : : ; b 20 . The learning systems observed one input symbol at a time. <p> There is an obvious analogy to the chunking algorithm: The chunker's attention is removed from events that become expected; they become `subconscious' (automatized) and give rise to even higher-level `abstractions' of the chunker's `consciousness'. The chunking systems described in (Schmidhuber, 1991a), <ref> (Schmidhuber, 1991c) </ref> and the current paper try to detect temporal regularities and learn to use them for identifying relevant points in time.
Reference: <author> Schmidhuber, J. H. </author> <year> (1991d). </year> <title> An O(n 3 ) learning algorithm for fully recurrent networks. </title> <type> Technical Report FKI-151-91, </type> <institution> Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference-contexts: These approaches are computationally intensive; BPTT is not local in time, RTRL-like algorithms and also their more efficient recent relatives <ref> (Schmidhuber, 1991d) </ref> are not local in space (Schmidhuber, 1991c). Common to all of these approaches is that they do not try to selectively focus on relevant inputs; they waste efficiency and resources by focussing on every input.
Reference: <author> Williams, R. J. and Peng, J. </author> <year> (1990). </year> <title> An efficient gradient-based algorithm for on-line training of recurrent network trajectories. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 491-501. </pages>
Reference-contexts: 1 INTRODUCTION Several approaches to on-line supervised sequence learning have been proposed, including back-propagation through time or BPTT, e.g. <ref> (Williams and Peng, 1990) </ref>, the IID- or RTRL-algorithm (Robinson and Fallside, 1987)(Williams and Zipser, 1989), and the recent fast-weight algorithm (Schmidhuber, 1991b)). <p> The activations of C's output units are considered as part of its state. Both C and A simultaneously are trained by a conventional algorithm for recurrent networks in an on-line fashion. Both the IID-Algorithm and BPTT are appropriate. In particular, computationally inexpensive variants of BPTT <ref> (Williams and Peng, 1990) </ref> are interesting: There are tasks with hierarchical temporal structure where only a few iterations of `back-propagation back into time' per time step are in principle sufficient to bridge arbitrary time lags (see section 5).
Reference: <author> Williams, R. J. and Zipser, D. </author> <year> (1989). </year> <title> Experimental analysis of the real-time recurrent learning algorithm. </title> <journal> Connection Science, </journal> <volume> 1(1) </volume> <pages> 87-111. </pages>
References-found: 11

