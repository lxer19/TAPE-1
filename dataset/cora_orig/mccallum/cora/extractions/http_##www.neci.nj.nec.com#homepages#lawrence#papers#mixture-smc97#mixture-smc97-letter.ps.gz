URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/mixture-smc97/mixture-smc97-letter.ps.gz
Refering-URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/mixture-smc97/mixture-smc97-6.html
Root-URL: 
Email: flawrence,pny,ingemarg@research.nj.nec.com  
Title: Face Recognition Using Mixture-Distance and Raw Images,  Face Recognition using Mixture-Distance and Raw Images  
Author: Steve Lawrence, Peter Yianilos and Ingemar Cox, Steve Lawrence, Peter Yianilos, Ingemar Cox 
Address: 4 Independence Way, Princeton, NJ 08540  
Affiliation: NEC Research,  
Note: 1997 IEEE International Conference on Systems, Man, and Cybernetics, IEEE Press, Piscataway, NJ, pp. 20162021, 1997. Copyright IEEE.  
Abstract: Earlier work suggests that mixture-distance can improve the performance of feature-based face recognition systems in which only a single training example is available for each individual. In this work we investigate the non-feature-based Eigenfaces technique of Turk and Pentland, replacing Euclidean distance with mixture-distance. In mixture-distance, a novel distance function is constructed based on local second-order statistics as estimated by modeling the training data with a mixture of normal densities. The approach is described and experimental results on a database of 600 people are presented, showing that mixture-distance can reduce the error rate by up to 73.9%. In the experimental setting considered, the results indicate that the simplest form of mixture distance yields considerable improvement. Additional, but less dramatic, improvement was possible with more complex forms. The results show that even in the absence of multiple training examples for each class, it is sometimes possible to infer an improved distance function from a statistical model of the training data. Therefore, researchers using Eigenfaces or similar pattern recognition techniques may find significant advantages by considering alternative distance metrics such as mixture-distance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ingemar J. Cox, Joumana Ghosn, and Peter N. Yianilos. </author> <title> Feature-based face recognition using mixture-distance. </title> <booktitle> In International Conference on Computer Vision and Pattern Recognition, pages 209216. </booktitle> <publisher> IEEE Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Earlier work suggests that mixture-distance [9] can improve the performance of feature-based face recognition systems in which only a single training example is available for each individual <ref> [1] </ref>. In this work we investigate the non-feature-based 1 Eigenfaces technique of Turk and Pentland [8], replacing Euclidean distance with mixture-distance. In mixture-distance, a novel distance function is constructed based on local second-order statistics as estimated by modeling the training data with a mixture of normal densities. <p> This factor can be seen to select between first-order (f = 0) and second-order (f = 1) models. Two methods have been used to classify new feature vectors, as in <ref> [1] </ref>. In hard vector quantization (hard VQ), each training example is assigned to one Gaussian: Y i is assigned to the Gaussian G l where l = argmax j P (G j jY i ). <p> j=1 P (G j jY i ) is the posterior probability and P (QjG j ; Y i ) is computed according to the method described in [9]: P (QjG j ; Y i ) = (2) 2 j j j 2 1 (Q Y i ) T j See <ref> [1] </ref> for intuition behind the formulation of mixture-distance. 5 Results In our experiments we used 600 training images and 131 test images. Because no class labels were available, the test images were chosen manually by 3 scanning the database for duplicate images of the same person. <p> We used mixture models with from 1 to 5 Gaus-sians and a flat mixture of mixtures model which consisted of a combination of the 5 individual mixture models. We tested both the hard VQ and soft VQ versions of mixture-distance. As was also found in <ref> [1] </ref>, we found that these provided similar performance. We therefore show only the hard VQ results in this paper, and note that the hard VQ algorithm is less computationally expensive. figure 4 shows the results using 30 eigenfaces.
Reference: [2] <author> B. Moghaddam and A. Pentland. </author> <title> Face recognition using view-based and modular eigenspaces. In Automatic Systems for the Identification and Inspection of Humans, </title> <booktitle> SPIE, </booktitle> <volume> volume 2257, </volume> <year> 1994. </year>
Reference-contexts: The database contains thousands of images which have been normalized for eye location and distance. We have further normalized these images using histogram equalization. These images differ from those used by Pentland et al. <ref> [2] </ref> in that Pentland et al. use extensive additional normalization for the geometry of the face, translation, lighting, contrast, rotation, and scale. We consider a more difficult task where such extensive normalization is not performed, e.g. due to computational requirements.
Reference: [3] <author> B. Moghaddam and A. Pentland. </author> <title> Probabilistic visual learning for object detection. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 786793, </pages> <year> 1995. </year>
Reference-contexts: The training data is modeled as a mixture of normal densities, which is then used in a particular way to measure distance. We note that mixture models have also been used, in a different manner, for maximum likelihood detection of faces by Moghaddam and Pentland <ref> [3] </ref>. <p> All experiments below were done over these five training sets. The eigenfaces were recalculated for each training set. Figure 2 shows the average face and the first five eigenfaces for a sample 600 class training set. We note that in comparison to the eigenface images presented by <ref> [3] </ref>, the eigen-faces shown appear to reflect more variability in the face images with respect to face size and location, as might be expected due to the less precise normalization.
Reference: [4] <author> A. Pentland, B. Moghaddam, and T. Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <year> 1994. </year>
Reference-contexts: Eigenfaces could also be considered as features. 2 Eigenfaces We used the Eigenfaces code from [7] which implements Eigenfaces as described in <ref> [8, 5, 4] </ref>. The procedure is as follows. Let the set of training images be T 1 ; T 2 ; T 3 ; : : : ; T M where M is the number of training images.
Reference: [5] <author> A. Pentland, T. Starner, N. Etcoff, A. Ma-soiu, O. Oliyide, and M. Turk. </author> <title> Experiments with Eigenfaces. </title> <booktitle> In Looking at People Workshop, International Joint Conference on Artificial Intelligence 1993, </booktitle> <address> Chamberry, France, </address> <year> 1993. </year>
Reference-contexts: Eigenfaces could also be considered as features. 2 Eigenfaces We used the Eigenfaces code from [7] which implements Eigenfaces as described in <ref> [8, 5, 4] </ref>. The procedure is as follows. Let the set of training images be T 1 ; T 2 ; T 3 ; : : : ; T M where M is the number of training images.
Reference: [6] <author> R.A. Redner and H.F. Walker. </author> <title> Mixture densities, maximum likelihood and the EM algorithm. </title> <journal> SIAM Review, </journal> <volume> 26:195239, </volume> <year> 1984. </year>
Reference-contexts: The task of estimating the parameters of a normal mixture model has been extensively studied. We have used the well known expectation-maximization algorithm (EM) <ref> [6] </ref>. We initialize the mean i and the covariance i for each Gaussian G i using the K-means algorithm. The determinants of the covariance matrices and the inverse covariance matrices have to be computed at each iteration of the EM algorithm to evaluate the new value of the log likelihood.
Reference: [7] <author> Thad Starner. Eigenfaces code, </author> <year> 1997. </year>
Reference-contexts: Eigenfaces could also be considered as features. 2 Eigenfaces We used the Eigenfaces code from <ref> [7] </ref> which implements Eigenfaces as described in [8, 5, 4]. The procedure is as follows. Let the set of training images be T 1 ; T 2 ; T 3 ; : : : ; T M where M is the number of training images.
Reference: [8] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> J. of Cognitive Neuroscience, </journal> <volume> 3:7186, </volume> <year> 1991. </year>
Reference-contexts: 1 Introduction Earlier work suggests that mixture-distance [9] can improve the performance of feature-based face recognition systems in which only a single training example is available for each individual [1]. In this work we investigate the non-feature-based 1 Eigenfaces technique of Turk and Pentland <ref> [8] </ref>, replacing Euclidean distance with mixture-distance. In mixture-distance, a novel distance function is constructed based on local second-order statistics as estimated by modeling the training data with a mixture of normal densities. <p> Eigenfaces could also be considered as features. 2 Eigenfaces We used the Eigenfaces code from [7] which implements Eigenfaces as described in <ref> [8, 5, 4] </ref>. The procedure is as follows. Let the set of training images be T 1 ; T 2 ; T 3 ; : : : ; T M where M is the number of training images.
Reference: [9] <author> Peter Yianilos. </author> <title> Metric learning via normal mixtures. </title> <type> Technical report, </type> <institution> NEC Research Institute, </institution> <year> 1995. </year> <month> 6 </month>
Reference-contexts: 1 Introduction Earlier work suggests that mixture-distance <ref> [9] </ref> can improve the performance of feature-based face recognition systems in which only a single training example is available for each individual [1]. In this work we investigate the non-feature-based 1 Eigenfaces technique of Turk and Pentland [8], replacing Euclidean distance with mixture-distance. <p> P (Q) P (QjY i )P (Y i ) k=1 P (QjY k )P (Y k ) where P (QjY i ) = j=1 P (G j jY i ) is the posterior probability and P (QjG j ; Y i ) is computed according to the method described in <ref> [9] </ref>: P (QjG j ; Y i ) = (2) 2 j j j 2 1 (Q Y i ) T j See [1] for intuition behind the formulation of mixture-distance. 5 Results In our experiments we used 600 training images and 131 test images.
References-found: 9

