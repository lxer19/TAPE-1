URL: http://www.is.cs.cmu.edu/papers/speech/1996/ICSLP96.klaus.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Email: ries@cs.cmu.edu finndag@ira.uka.de ahw@cs.cmu.edu  
Title: Class Phrase Models For Language Modeling  
Author: Klaus Ries Finn Dag But Alex Waibel 
Address: Germany  
Affiliation: Interactive System Labs Carnegie Mellon University, USA University of Karlsruhe,  
Abstract: Previous attempts to automatically determine multi-words as the basic unit for language modeling have been successful for extending bigram models [10, 9, 2, 8] to improve the perplexity of the language model and/or the word accuracy of the speech decoder. However, none of these techniques gave improvements over the trigram model so far, except for the rather controlled ATIS task [8]. We therefore propose an algorithm, that minimizes the perplexity improvement of a bigram model directly. The new algorithm is able to reduce the trigram perplexity and also achieves word accuracy improvements in the Verbmobil task. It is the natural counterpart of successful word classification algorithms for language modeling [4, 7] that minimize the leaving-one-out bigram perplexity. We also give some details on the usage of class finding techniques and m-gram models, which can be crucial to successful applications of this technique. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Steven Abney. </author> <title> Corpus-Based Methods in Language and Speech, chapter Part-Of-Speech Tagging and Partial Parsing. ELSNET. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, </address> <year> 1996. </year>
Reference-contexts: This could be achieved by manual dictionary modification, dictionary learning or clustering of senonens. The third application is still very speculative: [6] used mutual information to find linguistically motivated segments, <ref> [1] </ref> calls for grammar inference methods to find simple syntactical finite state grammars. Since the successive joins of basic units produces a possibly large number of types of basic units, the data sparseness problem becomes more serious.
Reference: 2. <author> Sabine Deligne and Frederic Bimbot. </author> <title> Language modeling by variable length sequences: Theoretical formulation and evaluation of multigram. </title> <booktitle> In ICASSP, </booktitle> <year> 1995. </year>
Reference-contexts: After a pair is selected we replace all occurrences of that pair by a new phrase symbol throughout the corpus. In the past most implementations of this idea made use of measures of cooccurrence (except for <ref> [2] </ref>), that have been useful in other domains, and the pair is chosen by maximization on that criterion. <p> One could of course also attempt to minimize the corresponding m-gram perplexity for m &gt; 2, but for reasons of computational tractability we attempt the bigram case only. The monogram case of this criterion is very similar to the multigram model <ref> [2] </ref> using the viterbi-assumption, however, the model evaluation of [2] is not done using the convenient leaving-one-out criterion. The bigram leaving-one-out perplexity criterion (PP) can also reflect information, which is obtained from the context of a phrase. <p> The monogram case of this criterion is very similar to the multigram model <ref> [2] </ref> using the viterbi-assumption, however, the model evaluation of [2] is not done using the convenient leaving-one-out criterion. The bigram leaving-one-out perplexity criterion (PP) can also reflect information, which is obtained from the context of a phrase.
Reference: 3. <author> Allen Gorin. </author> <title> On automated language acquistition. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 97(6) </volume> <pages> 3441-3461, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: A pilot experiment on a tagged Verbmobil corpus has shown that we are able to produce similar perplexity improvements on this type of corpus as well. Yet another application would be a hybrid-salience model, where the phrases are used to enhance the salience of the text <ref> [3] </ref>. 7. Acknowledgments This research was partly funded by grant 413-4001-01IV101S3 from the German Federal Ministry of Education, Science, Research and Technology (BMBF) as a part of the VERBMOBIL project. The views and conclusions contained in this document are those of the authors.
Reference: 4. <author> Reinhard Kneser and Herman Ney. </author> <title> Improved clustering techniques for class-based statistical language modeling. </title> <booktitle> In Eurospeech, </booktitle> <address> Berlin, Germany, </address> <year> 1993. </year>
Reference-contexts: N (w) Taking the logarithm and rearranging the term we get: F ML = w;w 0 X N (w) log N (w) The probabilities should be determined on a separate cross validation set and we will therefore minimize the leaving-one-out bigram perplexity of the resulting model along the lines of <ref> [4, 7] </ref>: X N (w; w 0 ) log (N (w; w 0 ) 1 b) (n + 1)b w where b is an absolute discounting factor, N (; ) is the bigram table, n 1 is the number of bigrams occuring exactly once, n + is the number of bigrams <p> The resulting measure will be called hybrid-X. Under the assumption that A 6= B we can simply go through both the bi- and trigram table once and calculate hA;Bi F LO for all hA; Bi. A similar technique was applied in many implementations of <ref> [4] </ref> and elaborated in [7]. Furthermore the trigram table can be calculated incrementally after a pair hA; Bi is joined from the trigram table for all trigrams that contain hA; Bi. <p> The newly proposed PP compares very favorable. For the small Verbmobil corpus the class-phrases show a much smoother plot than the word-phrases. 3. Data Driven Word Classification The words were classified using unsupervised word classification according to the bigram perplexity criterion <ref> [4] </ref>. Many authors either use a fixed number of classes as [4, 7] or let the criterion decide, how many classes to choose. In the current formulation of [4], the model prior is a uniform distribution. <p> For the small Verbmobil corpus the class-phrases show a much smoother plot than the word-phrases. 3. Data Driven Word Classification The words were classified using unsupervised word classification according to the bigram perplexity criterion [4]. Many authors either use a fixed number of classes as <ref> [4, 7] </ref> or let the criterion decide, how many classes to choose. In the current formulation of [4], the model prior is a uniform distribution. <p> Data Driven Word Classification The words were classified using unsupervised word classification according to the bigram perplexity criterion <ref> [4] </ref>. Many authors either use a fixed number of classes as [4, 7] or let the criterion decide, how many classes to choose. In the current formulation of [4], the model prior is a uniform distribution. We added a Gaussian prior on the number of classes we found, since in most cases the optimal number of classes for the trigram model is higher than the one chosen by the uniform prior.
Reference: 5. <author> Reinhard Kneser and Hermann Ney. </author> <title> Improved backing-off for m-gram language modeling. </title> <booktitle> In ICASSP, </booktitle> <year> 1995. </year>
Reference-contexts: Another expectation would be, that the more restricted domain, again Verbmobil, will profit more from phrase models than the less restricted one. In preex-periments only MI, BB, BP and their hybrid variants as well as PP delivered competitive performance. To train the trigram model we used an improved backoff-model <ref> [5] </ref>. In figure 1 the perplexity results on the Switchboard corpus are shown. As one can see, the perplexity criterion performs the best among all criteria and for the BP criterion one can observe, that using the hybrid model considerably restricts the problems of the original criterion.
Reference: 6. <author> David M. Magerman and Mitchell P. Marcus. </author> <title> Distituent parsing and grammar induction. </title> <publisher> pages 122a-122e. </publisher>
Reference-contexts: The second application could be realized by introducing specialized pronunciation variants for basic units like going to instead of merely concatenating the pronunciations of going and to. This could be achieved by manual dictionary modification, dictionary learning or clustering of senonens. The third application is still very speculative: <ref> [6] </ref> used mutual information to find linguistically motivated segments, [1] calls for grammar inference methods to find simple syntactical finite state grammars. Since the successive joins of basic units produces a possibly large number of types of basic units, the data sparseness problem becomes more serious. <p> In the past most implementations of this idea made use of measures of cooccurrence (except for [2]), that have been useful in other domains, and the pair is chosen by maximization on that criterion. Well known measures are * mutual information <ref> [6] </ref> MI * frequency p (w 1 ; w 2 ) * iterative marking frequency [9] * backward bigram BB: p (w 1 jw 2 ) * backward perplexity BP: p (w 1 ; w 2 ) log (p (w 1 jw 2 )) * Suhotin's measure [11], see also [9]
Reference: 7. <author> Sven Martin, Joerg Liebermann, and Hermann Ney. </author> <title> Algorithms for bigram and trigram clustering. </title> <booktitle> In Eurospeech, </booktitle> <year> 1995. </year>
Reference-contexts: N (w) Taking the logarithm and rearranging the term we get: F ML = w;w 0 X N (w) log N (w) The probabilities should be determined on a separate cross validation set and we will therefore minimize the leaving-one-out bigram perplexity of the resulting model along the lines of <ref> [4, 7] </ref>: X N (w; w 0 ) log (N (w; w 0 ) 1 b) (n + 1)b w where b is an absolute discounting factor, N (; ) is the bigram table, n 1 is the number of bigrams occuring exactly once, n + is the number of bigrams <p> The resulting measure will be called hybrid-X. Under the assumption that A 6= B we can simply go through both the bi- and trigram table once and calculate hA;Bi F LO for all hA; Bi. A similar technique was applied in many implementations of [4] and elaborated in <ref> [7] </ref>. Furthermore the trigram table can be calculated incrementally after a pair hA; Bi is joined from the trigram table for all trigrams that contain hA; Bi. One small sacrifice of this procedure is, that the bigram prediction of hA; Bi after hA; Bi is made as p (hA; BijB). <p> For the small Verbmobil corpus the class-phrases show a much smoother plot than the word-phrases. 3. Data Driven Word Classification The words were classified using unsupervised word classification according to the bigram perplexity criterion [4]. Many authors either use a fixed number of classes as <ref> [4, 7] </ref> or let the criterion decide, how many classes to choose. In the current formulation of [4], the model prior is a uniform distribution.
Reference: 8. <author> Michael K McCandless and James R Glass. </author> <title> Empirical acquisition of language models for speech recognition. </title> <booktitle> In ICSLP, </booktitle> <address> Yokohama, Japan, </address> <year> 1994. </year>
Reference: 9. <author> Klaus Ries, Finn Dag But, and Ye-Yi Wang. </author> <title> Improved language modeling by unsupervised acquisition of structure. </title> <booktitle> In ICASSP, </booktitle> <year> 1995. </year>
Reference-contexts: Well known measures are * mutual information [6] MI * frequency p (w 1 ; w 2 ) * iterative marking frequency <ref> [9] </ref> * backward bigram BB: p (w 1 jw 2 ) * backward perplexity BP: p (w 1 ; w 2 ) log (p (w 1 jw 2 )) * Suhotin's measure [11], see also [9] In contrast to these criteria one can try to maximize the desired criterion directly, which <p> [6] MI * frequency p (w 1 ; w 2 ) * iterative marking frequency <ref> [9] </ref> * backward bigram BB: p (w 1 jw 2 ) * backward perplexity BP: p (w 1 ; w 2 ) log (p (w 1 jw 2 )) * Suhotin's measure [11], see also [9] In contrast to these criteria one can try to maximize the desired criterion directly, which is the perplexity. <p> For the corpora we worked with, however, this technique was sufficiently fast. A procedure with possible applications to very large corpora like Wall Street Journal should not try to scan the whole corpus for each phrase. In the spirit of the iterative marking frequency <ref> [9] </ref> a framework, that scans the corpus less frequently, could look like: 1. Find a potential large (ranked) list of candidate phrases according to hA;Bi F LO or some other criterion. 2. Calculate a bigram table of the corpus, where this list was used to join basic units. 3.
Reference: 10. <author> B. Suhm and A. Waibel. </author> <title> Towards better language models for spontaneous speech. </title> <booktitle> In ICSLP, </booktitle> <address> Yokohama, Japan, </address> <year> 1994. </year>
Reference: 11. <author> B. V. Suhotin. </author> <title> Methode de dechiffrage, outil de recherche en linguistique. </title> <journal> TA Informationes, </journal> <volume> 2 </volume> <pages> 3-43, </pages> <year> 1973. </year>
Reference-contexts: * mutual information [6] MI * frequency p (w 1 ; w 2 ) * iterative marking frequency [9] * backward bigram BB: p (w 1 jw 2 ) * backward perplexity BP: p (w 1 ; w 2 ) log (p (w 1 jw 2 )) * Suhotin's measure <ref> [11] </ref>, see also [9] In contrast to these criteria one can try to maximize the desired criterion directly, which is the perplexity.
References-found: 11

