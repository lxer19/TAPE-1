URL: http://www.cs.cornell.edu/Info/People/rus/papers/Information-agents.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/rus/papers/papers.html
Root-URL: 
Title: Information Retrieval, Information Structure, and Information Agents  
Author: Daniela Rus and Devika Subramanian 
Date: December 1993  
Address: Ithaca, NY 14853  
Affiliation: Department of Computer Science Cornell University  
Abstract: This paper presents a new methodology for building special-purpose software agents that capture and access information in large, heterogeneous, distributed information environments. It allows rapid prototyping of information agents for solving a wide range of retrieval tasks with guarantees on performance. The key idea is to exploit underlying structure at various levels of granularity to build partial models that act as high-level indices with task-specific interpretations. These partial models are constructed using modules called navigators. Information agents are configured by using effective communication protocols to connect structure detectors and navigators. This methodology is applied to the design and implementation of information agents in two contexts: one for retrieving stock market information from scanned copies of newspapers and another for retrieving technical reports from the Internet.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Allan and G. Salton, </author> <title> The identification of text relations using automatic hypertext linking, </title> <booktitle> in the Workshop on Intelligent Hypertext, the ACM Conference on Information Knowledge Management, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: We then examine the tables at a finer level to extract the desired information. Our approach is effective in 1 Conventional word-based systems are also structure-based ICA systems. However, they exploit only one type of structure in the data environment. 2 We use the information retrieval system SMART <ref> [1] </ref> to cluster on this topic. 2 information domains with data represented at a level of detail that is too fine for exhaustive searches to be feasible (e.g., bits, for scanned text), but where there is enough hidden, high-level structure to filter relevant information efficiently. <p> Definition 2.6 Vertical structure: The white space density graph of B is the polygonal line wdg : [0; m] ! <ref> [0; 1] </ref> defined by the points wdg (i) = 1 n j=0 w (B i;j ); 0 i m Definition 2.7 Deviations in vertical structure: Given an error tolerance * v , a block of text has column structure if it occurs between two successive local maxima in the wdg above <p> If repeated timeouts occur, Bib Agent informs the user and asks for the next course of action. The performance of Bib Agent can be improved as follows. For each accessible site, papers can be automatically clustered according to topic and subtopic by using methods proposed in <ref> [15, 1] </ref>. A system like [14] can be used to compile statistics for each topic, for each site. These numbers can then be used to annotate the roadmap constructed by Bib Agent and used to dynamically control search.
Reference: [2] <author> I. Artobolevsky, </author> <title> Mechanisms in Modern Engineering Design, </title> <booktitle> vols. </booktitle> <pages> 1-4, </pages> <publisher> MIR Publishers, </publisher> <address> Moscow, </address> <year> 1979. </year> <title> 15 at the lowest common denominator over usages 32 </title>
Reference-contexts: Such requests can be handled more effectively with indices for higher-level entities like photographs, rather than conventional indices on words and word collections. Another example of a complex ICA task, well beyond the reach of present day systems is: "find a mechanism in Artobelovsky's design encyclopedia <ref> [2] </ref> that converts a uniform rotary motion into a reciprocatory motion. Paper design encyclopedias are typically indexed by the component types in the mechanism, and not by function and behavior.
Reference: [3] <author> N. Belkin and W. Croft, </author> <title> Information filtering and information retrieval: two sides of the same coin?, </title> <journal> Communications of the ACM, </journal> <volume> vol. 35(12), </volume> <pages> pp. 29-38, </pages> <year> 1992. </year>
Reference: [4] <author> M. Blum and D. Kozen, </author> <title> On the power of the compass (or, why mazes are easier to search than graphs), </title> <booktitle> in Proceedings of the Symposium on Foundations of Computer Science, </booktitle> <pages> pp 132-142, </pages> <year> 1978. </year>
Reference-contexts: To do this, we need a formal framework for analyzing information content. Such a framework, based 4 on the notion of information invariants has been discussed in the robotics context by [10, 12] and in the theoretical literature by <ref> [4] </ref>. The central theme of [10, 12] has been to determine what information is required to solve a robot task and to develop a theory for rigorously comparing robot protocols for achieving the same task. <p> If t ij = t i 0 j 0 the data in row i and column j and the data in row i 0 and column j 0 are *-similar. The type matrix for the table in Figure 6 is given in Figure 10. A GCD algorithm <ref> [4] </ref> can be used to determine the type, if any, of the overall matrix and thus to decide whether the matrix represents a table.
Reference: [5] <author> R. Brooks, </author> <title> Elephants don't play chess, Design of Autonomous Agents, </title> <editor> ed. P. Maes, MIT/Elsevier, </editor> <year> 1990. </year>
Reference-contexts: We are inspired by research in four distinct areas: information retrieval and filtering, mobile robotics, knowbots, and automated document structuring. * Mobile Robotics. The analogy between mobile robots in unstructured physical environments and information agents in a rich multi-media data environments is not just metaphorical. We have observed <ref> [5, 6] </ref> that the lessons learned in designing task-directed mobile robots can be imported to the problem of information capture and access. <p> Mobile robots are configured to detect and recover from errors in the task environment. This is typically accomplished by having each module be self-correcting a lesson about organization of complex systems that was <ref> [5, 6] </ref> discovered in the context of mobile robotics and insect intelligences. Information agents need to recover from errors in the segmentation and interpretation of data. Additionally, since our agents work in a distributed, wide-area network environment, it is important that they detect and recover from network failures.
Reference: [6] <author> R. Brooks, </author> <title> A robust layered control system for a mobile robot, </title> <journal> IEEE Journal of Robotics and Automation, </journal> <year> 1986. </year>
Reference-contexts: Information agents roam their environment, aided by their navigators, looking for patterns, recognized by their structure detectors. This approach to the ICA problem bears a strong similarity to the search of physical environments by mobile robots <ref> [6] </ref>. Structure detectors are virtual sensors and navigators are virtual effectors. The detailed structure of an information agent is shown in Figure 2. It is a tree of structure detectors and navigators glued together by communication paths. The root of the tree is a structure detector for the user's query. <p> We are inspired by research in four distinct areas: information retrieval and filtering, mobile robotics, knowbots, and automated document structuring. * Mobile Robotics. The analogy between mobile robots in unstructured physical environments and information agents in a rich multi-media data environments is not just metaphorical. We have observed <ref> [5, 6] </ref> that the lessons learned in designing task-directed mobile robots can be imported to the problem of information capture and access. <p> Mobile robots are configured to detect and recover from errors in the task environment. This is typically accomplished by having each module be self-correcting a lesson about organization of complex systems that was <ref> [5, 6] </ref> discovered in the context of mobile robotics and insect intelligences. Information agents need to recover from errors in the segmentation and interpretation of data. Additionally, since our agents work in a distributed, wide-area network environment, it is important that they detect and recover from network failures.
Reference: [7] <author> J. Canny and K. Goldberg, </author> <title> A "RISC" Paradigm for Industrial Robotics, to appear, </title> <booktitle> Proceedings of the International Conference on Robotics and Automation, </booktitle> <year> 1993. </year>
Reference-contexts: We have observed [5, 6] that the lessons learned in designing task-directed mobile robots can be imported to the problem of information capture and access. Section 2.2 provides a detailed description of the parallel between designing mobile robots and modular information agents. <ref> [7] </ref> argues that industrial robotic systems with simple sensors and effectors coupled with effective geometric algorithms can be both inexpensive and reliable. Their design philosophy matches ours.
Reference: [8] <author> V. Cate, Alex: </author> <title> a global file system, </title> <booktitle> in Proceedings of the Usenix Conference on File Systems, </booktitle> <year> 1992. </year>
Reference-contexts: The query goes to the root node that invokes as navigator the Unix utility cd. Bib Agent is built on top of the Alex <ref> [8] </ref> filesystem which provides users transparent access to files located in all the ftp-sites over the world. Bib Agent can thus use Unix commands like cd and ls to work its way through the directories and subdirectories accessible by anonymous ftp.
Reference: [9] <author> P. Crean, C. Russell, and M. V. Dellon, </author> <title> Overview and Programming Guide to the Mind Image Management Systems, </title> <type> Xerox Technical Report X9000627, </type> <year> 1991. </year>
Reference: [10] <author> B. Donald, </author> <title> Information Invariants in Robotics, </title> <note> to appear, Artificial Intelligence. </note>
Reference-contexts: By this we mean, finding structural cues or substitutes for semantic information at different levels of detail. To do this, we need a formal framework for analyzing information content. Such a framework, based 4 on the notion of information invariants has been discussed in the robotics context by <ref> [10, 12] </ref> and in the theoretical literature by [4]. The central theme of [10, 12] has been to determine what information is required to solve a robot task and to develop a theory for rigorously comparing robot protocols for achieving the same task. <p> To do this, we need a formal framework for analyzing information content. Such a framework, based 4 on the notion of information invariants has been discussed in the robotics context by <ref> [10, 12] </ref> and in the theoretical literature by [4]. The central theme of [10, 12] has been to determine what information is required to solve a robot task and to develop a theory for rigorously comparing robot protocols for achieving the same task. <p> The central theme of [10, 12] has been to determine what information is required to solve a robot task and to develop a theory for rigorously comparing robot protocols for achieving the same task. We believe that the transformational approach for information invariants proposed in <ref> [10] </ref> can be used to compare and analyze information agents configured from structure detectors and navigators, much like real robots. Understanding how to associate information with task-level content and developing a theoretical framework to compare the computational power of various approaches to information retrieval are foundational problems in this area. <p> Their design philosophy matches ours. We were influenced in defining topology-based navigators and structure detectors by the work of <ref> [10, 11, 20] </ref> who consider the problem of determining the information requirements to perform robot tasks using the concept of information invariants and perceptual equivalence classes. * Knowbots.
Reference: [11] <author> B. Donald and J. Jennings, </author> <title> Constructive recognizability for task-directed robot programming, </title> <journal> Journal of Robotics and Autonomous Systems, </journal> <volume> 9(1), </volume> <year> 1992. </year>
Reference-contexts: Their design philosophy matches ours. We were influenced in defining topology-based navigators and structure detectors by the work of <ref> [10, 11, 20] </ref> who consider the problem of determining the information requirements to perform robot tasks using the concept of information invariants and perceptual equivalence classes. * Knowbots.
Reference: [12] <author> B. Donald, J. Jennings, and D. </author> <title> Rus, Information Invariants for Cooperating Autonomous Mobile Robots, </title> <booktitle> in Proceedings of the International Symposium on Robotics Research, </booktitle> <year> 1993. </year>
Reference-contexts: By this we mean, finding structural cues or substitutes for semantic information at different levels of detail. To do this, we need a formal framework for analyzing information content. Such a framework, based 4 on the notion of information invariants has been discussed in the robotics context by <ref> [10, 12] </ref> and in the theoretical literature by [4]. The central theme of [10, 12] has been to determine what information is required to solve a robot task and to develop a theory for rigorously comparing robot protocols for achieving the same task. <p> To do this, we need a formal framework for analyzing information content. Such a framework, based 4 on the notion of information invariants has been discussed in the robotics context by <ref> [10, 12] </ref> and in the theoretical literature by [4]. The central theme of [10, 12] has been to determine what information is required to solve a robot task and to develop a theory for rigorously comparing robot protocols for achieving the same task. <p> retrieval. * For a given class of retrieval queries and data environments, what are the appropriate structural abstractions? * Given a retrieval task, how much structure must be detected? * What information is encoded by a given structure? 3 A detailed example of such an analysis can be found in <ref> [12] </ref>. 5 * What class of partial models can be constructed with a given navigator? * What is the most general agent that can be composed out of given structure detectors and navigators to solve a task class? * Given an agent what class of retrieval tasks is it good for?
Reference: [13] <author> H. Fujisawa, Y. Nakano, and K. Kurino, </author> <title> Segmentation methods for character recognition: from segmentation to document structure analysis, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 80, no. 7, </volume> <year> 1992. </year>
Reference-contexts: We develop a new perimeter tracer block segmentation scheme for any printed electronic document that does not examine pixels in the interior of any identified block. For most documents, this method performs better than area-based schemes. Previous work on classifying and logically relating blocks includes <ref> [40, 39, 13, 28, 42, 26, 41] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [41, 13, 40, 28]. <p> For most documents, this method performs better than area-based schemes. Previous work on classifying and logically relating blocks includes [40, 39, 13, 28, 42, 26, 41]. Methods for discriminating between text blocks and pictures in bitmapped images are presented in <ref> [41, 13, 40, 28] </ref>. A language for representing the hierarchical structure of documents is given in [13]. [40] extracts the logical structure by a bottom-up approach which starts with the finest units of structure and computes aggregates. A similar method is in [26]. <p> Previous work on classifying and logically relating blocks includes [40, 39, 13, 28, 42, 26, 41]. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [41, 13, 40, 28]. A language for representing the hierarchical structure of documents is given in <ref> [13] </ref>. [40] extracts the logical structure by a bottom-up approach which starts with the finest units of structure and computes aggregates. A similar method is in [26].
Reference: [14] <author> L. Gravano, H. Garcia-Molina, and A. Tomasic, </author> <title> The Efficacy of GlOSS for the Text Database Discovery Problem, </title> <type> Technical Report no. </type> <institution> STAN-CS-TN-93-01, Computer Science Department, Stanford University, </institution> <year> 1993. </year>
Reference-contexts: The performance of Bib Agent can be improved as follows. For each accessible site, papers can be automatically clustered according to topic and subtopic by using methods proposed in [15, 1]. A system like <ref> [14] </ref> can be used to compile statistics for each topic, for each site. These numbers can then be used to annotate the roadmap constructed by Bib Agent and used to dynamically control search.
Reference: [15] <author> M. Hearst and C. Plaunt, </author> <title> Subtopic Structuring for Full-Length Document Access, </title> <booktitle> in Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pg. </booktitle> <pages> 59-68, </pages> <year> 1993. </year>
Reference-contexts: If repeated timeouts occur, Bib Agent informs the user and asks for the next course of action. The performance of Bib Agent can be improved as follows. For each accessible site, papers can be automatically clustered according to topic and subtopic by using methods proposed in <ref> [15, 1] </ref>. A system like [14] can be used to compile statistics for each topic, for each site. These numbers can then be used to annotate the roadmap constructed by Bib Agent and used to dynamically control search.
Reference: [16] <author> J. Hopcroft and J. Ullman, </author> <title> Introduction to Automata Theory, Languages, and Computation, </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference: [17] <author> D. Huttenlocher, G. Klanderman, and W. Rucklidge, </author> <title> Comparing images using the Haus-dorff distance, to appear, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence. </journal> <volume> 33 </volume>
Reference: [18] <author> D. Huttenlocher, J. Noh, and W. Rucklidge, </author> <title> Tracking non-rigid objects in complex scenes, </title> <institution> Cornell University Technical Report TR92-1320, </institution> <year> 1992. </year>
Reference: [19] <author> A. Jain and S. Bhattacharjee, </author> <title> Address block location on envelopes using Gabor filters, </title> <journal> Pattern Recognition, </journal> <volume> vol. 25, no. 12, </volume> <year> 1992. </year>
Reference-contexts: Document structuring is usually done in two phases. In the first phase, the location of the blocks on the page is determined. In the second phase, the blocks are classified and the logical layout of the document is calculated. Previous work on block segmentation of documents include <ref> [19, 28, 41] </ref>. The methods are developed in the context of very specific applications; namely, segmenting pages of technical journals [28, 41]and locating address blocks in letters [19]. <p> Previous work on block segmentation of documents include [19, 28, 41]. The methods are developed in the context of very specific applications; namely, segmenting pages of technical journals [28, 41]and locating address blocks in letters <ref> [19] </ref>. The methods (the run-length smoothing algorithm [42] and the recursive XY cuts algorithm [28]) use area-based techniques; that is, every pixel in the document is examined at least once in the process of generating the segmentation.
Reference: [20] <author> J. Jennings and D. </author> <title> Rus, Active model acquisition for near-sensorless manipulation with mobile robots, </title> <booktitle> in Proceedings of the IASTED Conference on Robotics and Automation, </booktitle> <year> 1993. </year>
Reference-contexts: Their design philosophy matches ours. We were influenced in defining topology-based navigators and structure detectors by the work of <ref> [10, 11, 20] </ref> who consider the problem of determining the information requirements to perform robot tasks using the concept of information invariants and perceptual equivalence classes. * Knowbots.
Reference: [21] <author> B. Kahle, </author> <title> Overview of Wide Area Information Servers, WAIS on-line documentation, </title> <year> 1991. </year>
Reference-contexts: Examples of tasks that query and manipulate environments include content-based retrieval of technical reports, access of documents via citations, summaries of stock prices from archives, and retrieval of temporal weather patterns from a weather database. A diverse collection of tools like Wais, Smart, Gopher, Archie and Mosaic <ref> [21, 33, 38] </ref>, have been developed to provide keyword-based access to large text environments, as well as limited manipulation (e.g., display) of non-textual information. Underlying these tools are two assumptions about the nature of ICA tasks and methods for decomposing them.
Reference: [22] <author> R. Kahn and V. Cerf, </author> <title> The World of Knowbots, report to the Corporation for National Research Initiative, </title> <address> Arlington, VA 1988. </address>
Reference-contexts: Then, the traversal phase examines the subset of the data environment extracted by recognition to get the desired information. We implement the generic task decomposition scheme using information agents <ref> [22] </ref>. An information agent is a composition of special-purpose programs called structure detectors and navigators. <p> Structure is an important cue for content and "autonomous" information agents that can travel in the information world are well-suited for searching large, distributed, and changing electronic environments. 1.2 Related work The proposal by Kahn <ref> [22] </ref> for organizing architectures for retrieving information from electronic repositories provides the context for the problem addressed in this paper. We are inspired by research in four distinct areas: information retrieval and filtering, mobile robotics, knowbots, and automated document structuring. * Mobile Robotics. <p> There has been a recent flurry of activity in the area of designing intelligent task-directed agents that live and act within realistic software environments. These are called knowbots by <ref> [22] </ref>. A knowledge-level specification of knowbots is provided in Chapter 4 of [22] which outlines the required capabilities without committing to a specific implementation. <p> There has been a recent flurry of activity in the area of designing intelligent task-directed agents that live and act within realistic software environments. These are called knowbots by <ref> [22] </ref>. A knowledge-level specification of knowbots is provided in Chapter 4 of [22] which outlines the required capabilities without committing to a specific implementation. The modular information agent architecture organized around the idea of high-level structure that is presented here can be treated as a specific implementation proposal for this specification. <p> In common with Kahn and Cerf, our initial applications are "in the retrieval 6 of documents for which a user may only be able to specify an imprecise description". Our agent architecture can also be used to set up Personal Library Systems <ref> [22] </ref> "that selectively view, organize, and update contents" of an electronic library for individual use. * Information Retrieval and Filtering.
Reference: [23] <author> H. Kucera and W. Francis, </author> <title> Computational Analysis of Present Day American English, </title> <publisher> Brown University Press, </publisher> <address> Providence, RI, </address> <year> 1967. </year>
Reference-contexts: The analysis makes the following assumptions: * The average word length that occurs in text is known. For English, <ref> [23] </ref> have determined that the average word length of distinct words is 8.1 characters, but of word occurrences in written text, it is 4.7 characters.
Reference: [24] <author> M. Lesk, </author> <title> The CORE electronic chemistry library, </title> <booktitle> Proceedings of the SIGIR, </booktitle> <year> 1991. </year>
Reference: [25] <author> C. Lewis and D. </author> <title> Rus, Robust table recognition and extraction, </title> <type> forthcoming technical report, </type> <institution> Cornell University. </institution>
Reference-contexts: The layout and lexical structures are still clear. Our goal is to create a structure detector that checks for column and content structure while tolerating irregularities to within specified error bounds. 9 We thank Jim Davis for this idea. 10 This section is based on <ref> [25] </ref>. 15 The measure for the column layout of a block of text is given in terms of a data structure called the white space density graph and denoted by wdg.
Reference: [26] <author> M. Mizuno, Y. Tsuji, T. Tanaka, H. Tanaka, M. Iwashita, and T. Temma, </author> <title> Document recognition system with layout structure generator, </title> <journal> NEC Research and Development, </journal> <volume> vol. 32, no. 3, </volume> <year> 1991. </year>
Reference-contexts: We develop a new perimeter tracer block segmentation scheme for any printed electronic document that does not examine pixels in the interior of any identified block. For most documents, this method performs better than area-based schemes. Previous work on classifying and logically relating blocks includes <ref> [40, 39, 13, 28, 42, 26, 41] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [41, 13, 40, 28]. <p> A language for representing the hierarchical structure of documents is given in [13]. [40] extracts the logical structure by a bottom-up approach which starts with the finest units of structure and computes aggregates. A similar method is in <ref> [26] </ref>. We draw on this work to define parametric structure detectors and navigators for elec tronic document retrieval. 2 Modular Agent Architectures As noted earlier, we view information agents as entities that sense and act in data environments.
Reference: [27] <author> J. Munkres, </author> <title> Topology: A First Course, </title> <publisher> Prentice Hall, </publisher> <year> 1975. </year>
Reference-contexts: The representation of the article in Figure 1 at the paragraph level is such a model. Navigators segment the data in the environment at the appropriate grain size for structure detectors. We model granularity shifts in the descriptions of the data environment using concepts from topology <ref> [27] </ref>.
Reference: [28] <author> G. Nagy, S. Seth, and M. Vishwanathan, </author> <title> A prototype document image analysis system for technical journals, </title> <journal> Computer, </journal> <volume> vol. 25, no. 7, </volume> <year> 1992. </year>
Reference-contexts: Document structuring is usually done in two phases. In the first phase, the location of the blocks on the page is determined. In the second phase, the blocks are classified and the logical layout of the document is calculated. Previous work on block segmentation of documents include <ref> [19, 28, 41] </ref>. The methods are developed in the context of very specific applications; namely, segmenting pages of technical journals [28, 41]and locating address blocks in letters [19]. <p> In the second phase, the blocks are classified and the logical layout of the document is calculated. Previous work on block segmentation of documents include [19, 28, 41]. The methods are developed in the context of very specific applications; namely, segmenting pages of technical journals <ref> [28, 41] </ref>and locating address blocks in letters [19]. The methods (the run-length smoothing algorithm [42] and the recursive XY cuts algorithm [28]) use area-based techniques; that is, every pixel in the document is examined at least once in the process of generating the segmentation. <p> Previous work on block segmentation of documents include [19, 28, 41]. The methods are developed in the context of very specific applications; namely, segmenting pages of technical journals [28, 41]and locating address blocks in letters [19]. The methods (the run-length smoothing algorithm [42] and the recursive XY cuts algorithm <ref> [28] </ref>) use area-based techniques; that is, every pixel in the document is examined at least once in the process of generating the segmentation. We develop a new perimeter tracer block segmentation scheme for any printed electronic document that does not examine pixels in the interior of any identified block. <p> We develop a new perimeter tracer block segmentation scheme for any printed electronic document that does not examine pixels in the interior of any identified block. For most documents, this method performs better than area-based schemes. Previous work on classifying and logically relating blocks includes <ref> [40, 39, 13, 28, 42, 26, 41] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [41, 13, 40, 28]. <p> For most documents, this method performs better than area-based schemes. Previous work on classifying and logically relating blocks includes [40, 39, 13, 28, 42, 26, 41]. Methods for discriminating between text blocks and pictures in bitmapped images are presented in <ref> [41, 13, 40, 28] </ref>. A language for representing the hierarchical structure of documents is given in [13]. [40] extracts the logical structure by a bottom-up approach which starts with the finest units of structure and computes aggregates. A similar method is in [26]. <p> The block segmentation algorithm in Figure 4 finds regions by detecting borders of width d. It traces the perimeters of identified regions. Existing algorithms for block segmentation, for example the run length smoothing [42] and the recursive x-y cuts procedure <ref> [28] </ref>, examine every pixel in the document. Our perimeter tracer, in contrast, only examines pixels that are 7 A B = fa + b j a 2 A; b 2 Bg is the Minkowski sum of sets A and B. <p> Most documents have a rectangular layout produced with a finite set of fonts. Each font size has a characteristic spacing between lines and characters. Our algorithm relies on the following generic typesetting rules. A superset of these conventions are in <ref> [28] </ref>. 1. Printed lines are roughly horizontal. 2. The base lines of characters are aligned. 3. Word spaces are larger than character spaces. 4. Paragraphs are separated by wider spaces than lines within a paragraph, or indentation. 5.
Reference: [29] <author> C. Pearce and C. Nicholas, </author> <title> Generating a dynamic hypertext environment with n-gram analysis, </title> <booktitle> in Proceedings of the ACM Conference on Information Knowledge Management, </booktitle> <pages> pp. 148-153, </pages> <year> 1993. </year>
Reference-contexts: It uses statistical measures that are a successful substitute for semantic information. We view SMART as a special structure-detector and navigator pair, and can analyze the class of retrieval tasks that can be solved with it. A variant of SMART is proposed in <ref> [29] </ref>. Here, words are replaced by sequences of characters (called n-grams) and occurrence statistics are accumulated over n-grams instead of on words. <p> This analysis is beyond the scope of this paper but will be described in a forthcoming publication. The information retrieval systems of <ref> [33, 29] </ref> answer queries over pure text by relying on statistics over character-level entities. We believe that there are other notions of structure that are useful for different classes of retrieval tasks. In this paper, and in [32] we propose retrieval schemes that rely on the geometric layout of documents.
Reference: [30] <author> S. Robertson, </author> <title> The methodology of information retrieval experiment, Information Retrieval Experiment, </title> <editor> in K. Sparck Jones, </editor> <publisher> Ed., </publisher> <pages> pp 9-31, </pages> <publisher> Butterworths, </publisher> <year> 1981. </year>
Reference: [31] <author> D. Rus and D. Subramanian, </author> <title> Multi-media RISSC Informatics: Retrieving Information with Simple Structural Components, </title> <booktitle> in Proceedings of the ACM Conference on Information and Knowledge Management, </booktitle> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: These high level structural units function as beacons or landmarks in the data environment. We call this approach structure-based information capture and access 1 . Structure-based ICA <ref> [31] </ref> decomposes the retrieval problem into recognition of high-level structure and traversal of portions of the data environment with this structure. By structure, we mean any level of abstraction higher than the basic unit of data representation like characters and pixels. <p> The communication and interpretation paths are used for answer assembly, and for error detection and recovery. We construct these agents from a library of robust structure detectors and navigators that we have built <ref> [31] </ref>. Our ultimate goal is to assemble these agents automatically from task specifications using classical planning techniques. 1.1 Research Goals Our goal is to develop and prototype a methodology for conceptual retrieval tasks in large, distributed, heterogeneous multimedia environments. This is challenging for many reasons. <p> The remaining blocks are partitioned at the character level using an OCR structure detector. To maintain the alternating structure of the tree, the ASCII paragraphs generated by the character recognizer are passed through the identity navigator to our table detector <ref> [31] </ref>. The subset of paragraph blocks that are recognized as tables are provided to a row navigator, in order to separate the records.
Reference: [32] <author> D. Rus and K. Summers, </author> <title> Recognition and interpretation of electronic document layout, </title> <note> submitted to the 1994 IEEE Multimedia Conference. </note>
Reference-contexts: The information retrieval systems of [33, 29] answer queries over pure text by relying on statistics over character-level entities. We believe that there are other notions of structure that are useful for different classes of retrieval tasks. In this paper, and in <ref> [32] </ref> we propose retrieval schemes that rely on the geometric layout of documents. In particular, we discuss in detail the class of retrieval tasks whose answer is found in tabular form.
Reference: [33] <author> G. Salton and M. McGill, </author> <title> Introduction to Modern Information Retrieval, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year> <month> 34 </month>
Reference-contexts: Examples of tasks that query and manipulate environments include content-based retrieval of technical reports, access of documents via citations, summaries of stock prices from archives, and retrieval of temporal weather patterns from a weather database. A diverse collection of tools like Wais, Smart, Gopher, Archie and Mosaic <ref> [21, 33, 38] </ref>, have been developed to provide keyword-based access to large text environments, as well as limited manipulation (e.g., display) of non-textual information. Underlying these tools are two assumptions about the nature of ICA tasks and methods for decomposing them. <p> Consider, for example, the class of retrieval tasks specified by keywords in an electronic text environment. A system like SMART <ref> [33] </ref> uses statistics on word occurrences in the system to achieve impressive performance. SMART does not explicitly associate meaning with the words it indexes, relying instead on implicit conventions followed in written texts. It uses statistical measures that are a successful substitute for semantic information. <p> This analysis is beyond the scope of this paper but will be described in a forthcoming publication. The information retrieval systems of <ref> [33, 29] </ref> answer queries over pure text by relying on statistics over character-level entities. We believe that there are other notions of structure that are useful for different classes of retrieval tasks. In this paper, and in [32] we propose retrieval schemes that rely on the geometric layout of documents. <p> Our agent architecture can also be used to set up Personal Library Systems [22] "that selectively view, organize, and update contents" of an electronic library for individual use. * Information Retrieval and Filtering. We can interpret the classical work <ref> [33, 34] </ref> in information retrieval (IR) as follows: the data environment is text, the unit of structure is typically a word, the detectors constructed in the IR literature are pattern-matchers over words, the navigators are word indexes over documents.
Reference: [34] <author> G. Salton and C. Buckley, </author> <title> Improving retrieval performance by relevance feedback, </title> <journal> Jour--nal of American Society for Information Science, </journal> <volume> vol. 41(4), </volume> <pages> pp. 288-297, </pages> <year> 1990. </year>
Reference-contexts: Our agent architecture can also be used to set up Personal Library Systems [22] "that selectively view, organize, and update contents" of an electronic library for individual use. * Information Retrieval and Filtering. We can interpret the classical work <ref> [33, 34] </ref> in information retrieval (IR) as follows: the data environment is text, the unit of structure is typically a word, the detectors constructed in the IR literature are pattern-matchers over words, the navigators are word indexes over documents.
Reference: [35] <author> G. Salton, </author> <title> Automatic Text Processing: the transformation, analysis, and retrieval of information by computer, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference: [36] <author> D. Sankoff and J. Kruskal, </author> <title> Time warps, string edits, and macromolecules: the theory and practice of sequence comparison, </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: Now consider Figure 6. This table lacks the lexical regularity of Table 5, because there are small irregularities in the lexical structure of the columns. To express this more rigorously, let M be a metric for string comparison (we use the Levenshtein metric <ref> [36] </ref>.) Given * &gt; 0, two strings a and b are *-similar if M (a; b) * h . We use * h -typings, defined below, of the regular expressions that correspond to the entries of a column in order to control the imperfections we allow in horizontal structure.
Reference: [37] <author> M. Schwartz and P. Tsirigotis, </author> <title> Experience with a Semantically Cognizant Internet White Pages Directory Tool, </title> <journal> Journal of Internetworking Research and Experience, </journal> <month> March </month> <year> 1991. </year>
Reference-contexts: The physical view captures the page layout of the document. The logical view captures the relation between the pieces of the document. 11 contrast, agents like Netfind <ref> [37] </ref> employ navigators that physically traverse the Internet looking for information. Network failures have to be handled by error detection and recovery schemes for physical navigators.
Reference: [38] <author> M. Schwartz, A. Emtage, B. Khale, and B. Neuman, </author> <title> A comparison of Internet discovery approaches, </title> <journal> Computer Systems, </journal> <volume> 5(4), </volume> <year> 1992. </year>
Reference-contexts: Examples of tasks that query and manipulate environments include content-based retrieval of technical reports, access of documents via citations, summaries of stock prices from archives, and retrieval of temporal weather patterns from a weather database. A diverse collection of tools like Wais, Smart, Gopher, Archie and Mosaic <ref> [21, 33, 38] </ref>, have been developed to provide keyword-based access to large text environments, as well as limited manipulation (e.g., display) of non-textual information. Underlying these tools are two assumptions about the nature of ICA tasks and methods for decomposing them.
Reference: [39] <author> Y. Tanosaki, K. Suzuki, K. Kikuchi, and M. Kurihara, </author> <title> A logical structure analysis system for documents, </title> <booktitle> Proceedings of the second international symposium on interoperable information systems, </booktitle> <year> 1988. </year>
Reference-contexts: We develop a new perimeter tracer block segmentation scheme for any printed electronic document that does not examine pixels in the interior of any identified block. For most documents, this method performs better than area-based schemes. Previous work on classifying and logically relating blocks includes <ref> [40, 39, 13, 28, 42, 26, 41] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [41, 13, 40, 28].
Reference: [40] <author> S. Tsujimoto and H. Asada, </author> <title> Major components of a complete text reading system, </title> <booktitle> in Proceedings of the IEEE, </booktitle> <volume> vol. 80, no. 7, </volume> <year> 1992. </year>
Reference-contexts: We develop a new perimeter tracer block segmentation scheme for any printed electronic document that does not examine pixels in the interior of any identified block. For most documents, this method performs better than area-based schemes. Previous work on classifying and logically relating blocks includes <ref> [40, 39, 13, 28, 42, 26, 41] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [41, 13, 40, 28]. <p> For most documents, this method performs better than area-based schemes. Previous work on classifying and logically relating blocks includes [40, 39, 13, 28, 42, 26, 41]. Methods for discriminating between text blocks and pictures in bitmapped images are presented in <ref> [41, 13, 40, 28] </ref>. A language for representing the hierarchical structure of documents is given in [13]. [40] extracts the logical structure by a bottom-up approach which starts with the finest units of structure and computes aggregates. A similar method is in [26]. <p> Previous work on classifying and logically relating blocks includes [40, 39, 13, 28, 42, 26, 41]. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [41, 13, 40, 28]. A language for representing the hierarchical structure of documents is given in [13]. <ref> [40] </ref> extracts the logical structure by a bottom-up approach which starts with the finest units of structure and computes aggregates. A similar method is in [26].
Reference: [41] <author> D. Wang and S. Srihari, </author> <title> Classification of newspaper image blocks using texture analysis, Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 47, </volume> <year> 1989. </year>
Reference-contexts: Document structuring is usually done in two phases. In the first phase, the location of the blocks on the page is determined. In the second phase, the blocks are classified and the logical layout of the document is calculated. Previous work on block segmentation of documents include <ref> [19, 28, 41] </ref>. The methods are developed in the context of very specific applications; namely, segmenting pages of technical journals [28, 41]and locating address blocks in letters [19]. <p> In the second phase, the blocks are classified and the logical layout of the document is calculated. Previous work on block segmentation of documents include [19, 28, 41]. The methods are developed in the context of very specific applications; namely, segmenting pages of technical journals <ref> [28, 41] </ref>and locating address blocks in letters [19]. The methods (the run-length smoothing algorithm [42] and the recursive XY cuts algorithm [28]) use area-based techniques; that is, every pixel in the document is examined at least once in the process of generating the segmentation. <p> We develop a new perimeter tracer block segmentation scheme for any printed electronic document that does not examine pixels in the interior of any identified block. For most documents, this method performs better than area-based schemes. Previous work on classifying and logically relating blocks includes <ref> [40, 39, 13, 28, 42, 26, 41] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [41, 13, 40, 28]. <p> For most documents, this method performs better than area-based schemes. Previous work on classifying and logically relating blocks includes [40, 39, 13, 28, 42, 26, 41]. Methods for discriminating between text blocks and pictures in bitmapped images are presented in <ref> [41, 13, 40, 28] </ref>. A language for representing the hierarchical structure of documents is given in [13]. [40] extracts the logical structure by a bottom-up approach which starts with the finest units of structure and computes aggregates. A similar method is in [26]. <p> The first level navigator is a block segmenter that takes a bitmap copy (one at a time) and produces its paragraph level partition. The structure detector that follows it eliminates all the blocks that are half-tone pictures using methods discussed in <ref> [41] </ref>. The remaining blocks are partitioned at the character level using an OCR structure detector. To maintain the alternating structure of the tree, the ASCII paragraphs generated by the character recognizer are passed through the identity navigator to our table detector [31].
Reference: [42] <author> K. Wong, R. Casey, and F. Wahl, </author> <title> Document Analysis System, </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 26, no. 6, </volume> <year> 1982. </year>
Reference-contexts: Previous work on block segmentation of documents include [19, 28, 41]. The methods are developed in the context of very specific applications; namely, segmenting pages of technical journals [28, 41]and locating address blocks in letters [19]. The methods (the run-length smoothing algorithm <ref> [42] </ref> and the recursive XY cuts algorithm [28]) use area-based techniques; that is, every pixel in the document is examined at least once in the process of generating the segmentation. <p> We develop a new perimeter tracer block segmentation scheme for any printed electronic document that does not examine pixels in the interior of any identified block. For most documents, this method performs better than area-based schemes. Previous work on classifying and logically relating blocks includes <ref> [40, 39, 13, 28, 42, 26, 41] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [41, 13, 40, 28]. <p> The block segmentation algorithm in Figure 4 finds regions by detecting borders of width d. It traces the perimeters of identified regions. Existing algorithms for block segmentation, for example the run length smoothing <ref> [42] </ref> and the recursive x-y cuts procedure [28], examine every pixel in the document. Our perimeter tracer, in contrast, only examines pixels that are 7 A B = fa + b j a 2 A; b 2 Bg is the Minkowski sum of sets A and B.
Reference: [43] <author> User Manual, </author> <title> Splus Reference Manual, </title> <journal> Statistical Sciences,Inc.,Seattle,Washington, </journal> <year> 1991. </year>
Reference-contexts: This is due to the fact that the lengths of words and of the spacing between them are variable, and their occurrences in a line of text are random. We have tested the independence of the distribution of whitespace by extensive experiments with Splus <ref> [43] </ref>. This implies that the blank spaces of a line have a binomial distribution. 11 0 0.2 0.4 0.6 0.8 1 Column Let B be a block of text of n rows and m columns.
References-found: 43

