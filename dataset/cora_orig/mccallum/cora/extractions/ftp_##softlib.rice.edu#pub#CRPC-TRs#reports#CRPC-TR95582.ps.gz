URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95582.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: COMPUTING GRADIENTS IN LARGE-SCALE OPTIMIZATION USING AUTOMATIC DIFFERENTIATION  
Phone: 60439  
Author: Christian H. Bischof, Ali Bouaricha, Peyvand M. Khademi, Jorge J. More 
Note: (Revised version)  Work supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38, by the National Aerospace Agency under Purchase Order L25935D, and by the National Science Foundation, through the Center for Research on Parallel Computation, under Cooperative Agreement No. CCR-9120008.  
Date: January 1995  June 1995  
Address: 9700 South Cass Avenue Argonne, Illinois  Preprint MCS-P488-0195  
Affiliation: ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> B. M. Averick, R. G. Carter, J. J. Mor e, and G.-L. Xue, </author> <title> The MINPACK-2 test problem collection, </title> <type> Preprint MCS-P153-0692, </type> <institution> Mathematics and Computer Science 18 Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: In fact, the sparsity pattern is a byproduct of the ADIFOR/SparsLinC approach. Section 4 discusses the formulation of large-scale problems in terms of partially separable functions, and outlines the problems from the MINPACK-2 <ref> [1] </ref> collection of large-scale problems that we use to validate our approach. Experimental results with problems from the MINPACK-2 collection on Sun SPARC 10, IBM RS 6000 (model 370), and Cray C90 platforms are presented in Section 5. <p> The selected problems are representative of large-scale optimization problems arising from ap plications in superconductivity, optimal design, combustion, and lubrication. We give only a brief description of two of these problems to illustrate the partially separable structure of these problems. For further information refer to <ref> [1] </ref>. 9 The Ginzburg-Landau (GL2) problem is of the form (4.1), where v : IR 2 7! IR 4 .
Reference: [2] <author> B. M. Averick and J. J. Mor e, </author> <title> Evaluation of large-scale optimization problems on vector and parallel architectures, </title> <journal> SIAM J. Optimization, </journal> <volume> 4 (1994), </volume> <pages> pp. 708-721. </pages>
Reference-contexts: The above ratio can be expected for well-coded gradient computations on scalar architectures but requires special techniques on vector and parallel architectures <ref> [2] </ref>. On vector architectures we can expect the ratio (5.1) to hold only if both the function and the gradient evaluation codes vectorize or if neither code vectorizes.
Reference: [3] <author> B. M. Averick, J. J. Mor e, C. H. Bischof, A. Carle, and A. Griewank, </author> <title> Computing large sparse Jacobian matrices using automatic differentiation, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 15 (1994), </volume> <pages> pp. 285-294. </pages>
Reference-contexts: By exploiting the capability to compute directional derivatives (2.1), compressed Jaco-bian matrices can easily be computed via automatic differentiation (for additional details, see <ref> [3] </ref>): Given the seed matrix S, ADIFOR computes the compressed Jacobian matrix f 0 (x)S. In contrast to the approximation techniques based on the compressed Jacobian matrix approach [13, 15], all columns of the compressed Jacobian matrix are computed at once. <p> We do not elaborate further on this point because this contrast in accuracies between automatic differentiation and function differences shows consistency with previously published work <ref> [3] </ref> on the computation of sparse Jacobian matrices with automatic differentiation. 5.2 Memory Requirements Tables 5.1 and 5.2 present, respectively for the GL2 and MSA problems, the total memory required for the computation of the function as well as the various gradient methods, for the case of n = 160; 000
Reference: [4] <author> C. Bischof, A. Carle, G. Corliss, A. Griewank, and P. Hovland, ADIFOR: </author> <title> Generating derivative codes from Fortran programs, </title> <booktitle> Scientific Programming, 1 (1992), </booktitle> <pages> pp. 11-29. </pages>
Reference-contexts: Preliminary tests of this approach were done by Bischof and El-Khadiri [10]. The results in this paper show that this approach is not only feasible, but highly efficient. A brief review of automatic differentiation, the ADIFOR (Automatic Differentiation of Fortran) tool <ref> [4, 6] </ref>, and the SparsLinC (Sparse Linear Combination) library [5, 6] is 3 provided in the next section. <p> The interpretation overhead associated with 5 using this trace for the purposes of automatic differentiation, as well as its potentially very large size, can be a serious computational bottleneck [30]. Recently, a source transformation approach to automatic differentiation has been explored in the ADIFOR <ref> [4, 6] </ref>, ADIC [9], AMC [16], and Odyssee [29] tools. ADIFOR transforms Fortran 77 code, ADIC transforms ANSI-C code, and AMC and Odyssee transform a subset of Fortran 77.
Reference: [5] <author> C. Bischof, A. Carle, and P. Khademi, </author> <title> Fortran 77 interface specification to the SparsLinC library, </title> <type> Tech. Report ANL/MCS-TM-196, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: The results in this paper show that this approach is not only feasible, but highly efficient. A brief review of automatic differentiation, the ADIFOR (Automatic Differentiation of Fortran) tool [4, 6], and the SparsLinC (Sparse Linear Combination) library <ref> [5, 6] </ref> is 3 provided in the next section. Automatic differentiation techniques rely on the fact that every function, no matter how complicated, is executed on a computer as a potentially long sequence of elementary operations such as additions, multiplications, and elementary functions (e.g., the trigonometric and exponential functions). <p> By default, this operation is implemented as a DO loop; and as long as p is of moderate size and the vectors are dense, this is an efficient way of expressing a vector linear combination. The SparsLinC library <ref> [5, 6] </ref> addresses the situation where the seed matrix S is sparse and most of the vectors involved in the computation of f 0 (x)S are sparse.
Reference: [6] <author> C. Bischof, A. Carle, P. Khademi, and A. Mauer, </author> <title> The ADIFOR 2.0 system for the automatic differentiation of Fortran 77 programs, 1994. </title> <type> Preprint MCS-P481-1194, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, and CRPC-TR94491, Center for Research on Parallel Computation, Rice University. </institution>
Reference-contexts: Preliminary tests of this approach were done by Bischof and El-Khadiri [10]. The results in this paper show that this approach is not only feasible, but highly efficient. A brief review of automatic differentiation, the ADIFOR (Automatic Differentiation of Fortran) tool <ref> [4, 6] </ref>, and the SparsLinC (Sparse Linear Combination) library [5, 6] is 3 provided in the next section. <p> The results in this paper show that this approach is not only feasible, but highly efficient. A brief review of automatic differentiation, the ADIFOR (Automatic Differentiation of Fortran) tool [4, 6], and the SparsLinC (Sparse Linear Combination) library <ref> [5, 6] </ref> is 3 provided in the next section. Automatic differentiation techniques rely on the fact that every function, no matter how complicated, is executed on a computer as a potentially long sequence of elementary operations such as additions, multiplications, and elementary functions (e.g., the trigonometric and exponential functions). <p> The interpretation overhead associated with 5 using this trace for the purposes of automatic differentiation, as well as its potentially very large size, can be a serious computational bottleneck [30]. Recently, a source transformation approach to automatic differentiation has been explored in the ADIFOR <ref> [4, 6] </ref>, ADIC [9], AMC [16], and Odyssee [29] tools. ADIFOR transforms Fortran 77 code, ADIC transforms ANSI-C code, and AMC and Odyssee transform a subset of Fortran 77. <p> By default, this operation is implemented as a DO loop; and as long as p is of moderate size and the vectors are dense, this is an efficient way of expressing a vector linear combination. The SparsLinC library <ref> [5, 6] </ref> addresses the situation where the seed matrix S is sparse and most of the vectors involved in the computation of f 0 (x)S are sparse. <p> This situation arises, for example, in the computation of large sparse Jacobian matrices, since the sparsity of the final Jacobian matrix implies that, with great probability, all intermediate derivative computations involve sparse vectors as well. SparsLinC implements routines for executing the vector linear combination (2.2) using sparse data structures <ref> [6] </ref>. It is fully integrated into ADIFOR and ADIC and provides a mechanism for transparently exploiting sparsity in derivative computations. SparsLinC does not require knowledge of the sparsity structure of the Jacobian matrix; indeed, the sparsity structure of the Jacobian matrix is a byproduct of the derivative computation.
Reference: [7] <author> C. Bischof, L. Green, K. Haigler, and T. Knauff, </author> <title> Parallel calculation of sensitivity derivatives for aircraft design using automatic differentiation, </title> <booktitle> in Proceedings of the 5th AIAA/NASA/USAF/ISSMO Symposium on Multidisciplinary Analysis and Optimization, </booktitle> <pages> AIAA 94-4261, </pages> <institution> American Institute of Aeronautics and Astronautics, </institution> <year> 1994, </year> <pages> pp. 73-84. </pages>
Reference-contexts: The results in Table 5.5 show that the AD approach outperforms the FD approach on scalar architectures. The performance of the various approaches on vector architectures is harder to predict as performance depends on the delicate interplay between the code and the compiler (for examples, see <ref> [7, 11] </ref>). Note that the results in Table 5.5 show that the performance of AD is comparable to that of FD on the Cray C90 for those problems (MSA and ODC) where the function evaluation code fails to vectorize.
Reference: [8] <author> C. Bischof, P. Khademi, and A. Carle, </author> <title> Fast computation of gradients and Ja-cobians by transparent exploitation of sparsity in automatic differentiation, </title> <type> Preprint MCS-P519-0595, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: Certainly, the memory needed for representing the sparse Jacobian matrix has a lower bound of nnz (f 0 (x)). Beyond this, SparsLinC requires additional memory for internal representations as explained in <ref> [8] </ref>. The first column in Tables 5.1-5.3 shows the memory required for running the original function. Memory requirements for the hand-coded MINPACK-2 gradient codes are not shown separately, but are always between a factor of 1.5-2 times the memory requirements of the corresponding function. <p> We also note the large variation in T for the Sparse AD results on the SPARC 10. This results from the way SparsLinC exploits the particular sparsity characteristics of each problem (this issue is explored in <ref> [8] </ref>).
Reference: [9] <author> C. Bischof and A. Mauer, </author> <title> ADIC A tool for the automatic differentiation of C programs, </title> <type> Preprint MCS-P499-0295, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: The interpretation overhead associated with 5 using this trace for the purposes of automatic differentiation, as well as its potentially very large size, can be a serious computational bottleneck [30]. Recently, a source transformation approach to automatic differentiation has been explored in the ADIFOR [4, 6], ADIC <ref> [9] </ref>, AMC [16], and Odyssee [29] tools. ADIFOR transforms Fortran 77 code, ADIC transforms ANSI-C code, and AMC and Odyssee transform a subset of Fortran 77. ADIFOR and ADIC mainly use the forward mode, with the reverse mode at the statement level, while AMC and Odyssee use the reverse mode.
Reference: [10] <author> C. H. Bischof and M. El-Khadiri, </author> <title> Extending compile-time reverse mode and exploiting partial separability in ADIFOR, </title> <type> Tech. Report ANL/MCS-TM-163, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: The approach for computing the gradient of f 0 using (1.3) and (1.4) was proposed by Andreas Griewank and can be viewed as a special case of the results discussed by Griewank [18, Section 2]. Preliminary tests of this approach were done by Bischof and El-Khadiri <ref> [10] </ref>. The results in this paper show that this approach is not only feasible, but highly efficient. A brief review of automatic differentiation, the ADIFOR (Automatic Differentiation of Fortran) tool [4, 6], and the SparsLinC (Sparse Linear Combination) library [5, 6] is 3 provided in the next section.
Reference: [11] <author> A. Bouaricha and J. Mor e, </author> <title> Impact of partial separability on large-scale optimization, </title> <type> Preprint MCS-P487-0195, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995. </year> <month> 19 </month>
Reference-contexts: Partial separability is also used to compute the gradient of f 0 as the sum of the gradients of the element functions f i , but this is just another method for hand-coding the gradient. In a related paper <ref> [11] </ref> we discuss the impact of partial separability on optimization software. <p> In many situations it is desirable to have a tool for the determination of f 0 (x) that does 7 not require knowledge of the sparsity pattern of f 0 (x). This situation arises, for example, while developing interfaces to the solution of large-scale optimization problems <ref> [11] </ref>, where it is desirable to relieve the user of the error-prone task of providing the sparsity pattern. In these situations, a sparse implementation of automatic differentiation, such as provided by the ADIFOR/SparsLinC approach, is the only feasible approach. <p> The results in Table 5.5 show that the AD approach outperforms the FD approach on scalar architectures. The performance of the various approaches on vector architectures is harder to predict as performance depends on the delicate interplay between the code and the compiler (for examples, see <ref> [7, 11] </ref>). Note that the results in Table 5.5 show that the performance of AD is comparable to that of FD on the Cray C90 for those problems (MSA and ODC) where the function evaluation code fails to vectorize.
Reference: [12] <author> T. F. Coleman, B. S. Garbow, and J. J. Mor e, </author> <title> Fortran subroutines for estimating sparse Jacobian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 (1984), </volume> <pages> pp. 346-347. </pages>
Reference-contexts: For example, if a matrix is banded with bandwidth fi or if it can be permuted to a matrix with bandwidth fi, it can be shown [13] that p fi. In our experiments we employ the graph-coloring software described in <ref> [12] </ref> to determine an appropriate partition. In an optimization algorithm we invariably need to compute a sequence frf 0 (x k )g of gradients for some sequence fx k g of iterates. This step requires the computation of a sequence of Jacobian matrices ff 0 (x k )g. <p> Another reason for the high relative cost of computing the graph coloring is that the algorithm we employ (subroutine DSM from Coleman, Garbow, and More <ref> [12] </ref>) is intended to produce graph colorings with a small p by employing several heuristics. The runtime of subroutine DSM could be reduced by a factor of two or more without a substantial increase in p by only using one of the heuristics.
Reference: [13] <author> T. F. Coleman and J. J. Mor e, </author> <title> Estimation of sparse Jacobian matrices and graph coloring problems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 (1983), </volume> <pages> pp. 187-209. </pages>
Reference-contexts: The compressed Jacobian matrix approach has long been used in connection with the determination of sparse Jacobian matrices by differences of function values; see, for example, <ref> [13, 15] </ref>. The compressed Jacobian matrix approach requires the determination of a partitioning of the columns of f 0 (x) into structurally orthogonal columns, that is, columns that do not have a nonzero in the same row position. <p> Because of the structural orthogonality property we can uniquely extract all entries of the original Jacobian matrix from the compressed Jacobian. The partitioning problem can be considered as a graph-coloring problem <ref> [13] </ref>. Given a graph representation of the sparsity structure of f 0 (x), these algorithms produce a partitioning of the columns of f 0 (x) into p structurally orthogonal groups by graph-coloring algorithms for the column-intersection graph associated with f 0 (x). <p> For many sparsity patterns, p is small and independent of n. For example, if a matrix is banded with bandwidth fi or if it can be permuted to a matrix with bandwidth fi, it can be shown <ref> [13] </ref> that p fi. In our experiments we employ the graph-coloring software described in [12] to determine an appropriate partition. In an optimization algorithm we invariably need to compute a sequence frf 0 (x k )g of gradients for some sequence fx k g of iterates. <p> In contrast to the approximation techniques based on the compressed Jacobian matrix approach <ref> [13, 15] </ref>, all columns of the compressed Jacobian matrix are computed at once. In many situations it is desirable to have a tool for the determination of f 0 (x) that does 7 not require knowledge of the sparsity pattern of f 0 (x).
Reference: [14] <author> A. R. Conn, N. I. M. Gould, and P. L. Toint, LANCELOT, </author> <title> Springer Series in Computational Mathematics, </title> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Algorithms and software that take advantage of the partially separable structure have been developed for various problems. See, for example, <ref> [27, 14, 23, 31, 32, 33, 34] </ref>. In these algorithms the partially separable structure is used mainly to approximate the (dense) Hessian matrices r 2 f i (x) by quasi-Newton methods.
Reference: [15] <author> A. R. Curtis, M. J. D. Powell, and J. K. Reid, </author> <title> On the estimation of sparse Jacobian matrices, </title> <journal> J. Inst. Math. Appl., </journal> <volume> 13 (1974), </volume> <pages> pp. 117-119. </pages>
Reference-contexts: The compressed Jacobian matrix approach has long been used in connection with the determination of sparse Jacobian matrices by differences of function values; see, for example, <ref> [13, 15] </ref>. The compressed Jacobian matrix approach requires the determination of a partitioning of the columns of f 0 (x) into structurally orthogonal columns, that is, columns that do not have a nonzero in the same row position. <p> In contrast to the approximation techniques based on the compressed Jacobian matrix approach <ref> [13, 15] </ref>, all columns of the compressed Jacobian matrix are computed at once. In many situations it is desirable to have a tool for the determination of f 0 (x) that does 7 not require knowledge of the sparsity pattern of f 0 (x).
Reference: [16] <author> R. Giering, </author> <title> Adjoint model compiler, manual version 0.2, AMC version 2.04, </title> <type> tech. report, </type> <institution> Max-Planck Institut fur Meteorologie, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: The interpretation overhead associated with 5 using this trace for the purposes of automatic differentiation, as well as its potentially very large size, can be a serious computational bottleneck [30]. Recently, a source transformation approach to automatic differentiation has been explored in the ADIFOR [4, 6], ADIC [9], AMC <ref> [16] </ref>, and Odyssee [29] tools. ADIFOR transforms Fortran 77 code, ADIC transforms ANSI-C code, and AMC and Odyssee transform a subset of Fortran 77. ADIFOR and ADIC mainly use the forward mode, with the reverse mode at the statement level, while AMC and Odyssee use the reverse mode.
Reference: [17] <author> A. Griewank, </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation, Optimization Methods and Software, </title> <booktitle> 1 (1992), </booktitle> <pages> pp. </pages> <month> 35-54. </month> <title> [18] , Some bounds on the complexity of gradients, Jacobians, and Hessians, in Complexity in Nonlinear Optimization, </title> <editor> P. Pardalos, ed., </editor> <publisher> World Scientific Publishers, </publisher> <year> 1993, </year> <pages> pp. 128-161. </pages>
Reference-contexts: The storage requirement of the reverse mode, however, can be a difficulty because of the possible dependence on L ff g+M ff g . Griewank <ref> [17] </ref> suggested a snapshot approach to circumvent this difficulty. There have been various implementations of automatic differentiation; an extensive survey can be found in [25]. In particular, we mention GRESS [24], and PADRE-2 [26] for Fortran programs and ADOL-C [20] for C programs.
Reference: [19] <author> A. Griewank and G. F. Corliss, eds., </author> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1991. </year>
Reference-contexts: By applying the chain rule to the composition of those elementary operations, derivative information can be computed exactly and in a completely mechanical fashion <ref> [19, 28] </ref>. In Section 3 we propose two approaches for the computation of the Jacobian matrix f 0 (x). <p> We emphasize that the accuracy of the gradient in an optimization algorithm is of paramount importance because the gradient is used to determine the search directions. An inaccurate gradient can easily lead to false convergence. 2 The ADIFOR Tool and the SparsLinC Library Automatic differentiation <ref> [19, 28] </ref> (AD) is a chain-rule-based technique for evaluating the derivatives of functions defined by computer programs. AD produces code that, in the absence of floating-point exceptions, computes the values of the analytical derivatives ac 4 curate to machine precision.
Reference: [20] <author> A. Griewank, D. Juedes, and J. Srinivasan, ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++, </title> <type> Preprint MCS-P180-1190, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1990. </year>
Reference-contexts: Griewank [17] suggested a snapshot approach to circumvent this difficulty. There have been various implementations of automatic differentiation; an extensive survey can be found in [25]. In particular, we mention GRESS [24], and PADRE-2 [26] for Fortran programs and ADOL-C <ref> [20] </ref> for C programs. GRESS, PADRE-2, and ADOL-C implement both the forward and reverse modes. In order to save control flow information and intermediate values, these tools generate a trace of the computation by recording the particulars of every operation performed in the code.
Reference: [21] <author> A. Griewank and P. L. Toint, </author> <title> On the unconstrained optimization of partially separable functions, in Nonlinear Optimization 1981, </title> <editor> M. J. D. Powell, ed., </editor> <publisher> Academic Press, </publisher> <year> 1982. </year> <title> [22] , Partitioned variable metric updates for large structured optimization problems, </title> <journal> Numer. Math., </journal> <volume> 39 (1982), </volume> <pages> pp. </pages> <month> 119-137. </month> <title> [23] , Numerical experiments with partially separable optimization problems, in Numerical Analysis: </title> <booktitle> Proceedings Dundee 1983, </booktitle> <editor> D. F. Griffiths, ed., </editor> <booktitle> Lecture Notes in Mathematics 1066, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: CCR-9120008. 2 where f i depends on p i t n variables. This class of functions, introduced by Griewank and Toint <ref> [21, 22] </ref>, plays a fundamental role in the solution of large-scale optimization problems since, as shown by Griewank and Toint, a function f 0 is partially separable if the Hessian matrix r 2 f 0 (x) is sparse.
Reference: [24] <author> J. E. Horwedel, GRESS: </author> <title> A preprocessor for sensitivity studies on Fortran programs, in Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. F. Corliss, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991, </year> <pages> pp. 243-250. 20 </pages>
Reference-contexts: Griewank [17] suggested a snapshot approach to circumvent this difficulty. There have been various implementations of automatic differentiation; an extensive survey can be found in [25]. In particular, we mention GRESS <ref> [24] </ref>, and PADRE-2 [26] for Fortran programs and ADOL-C [20] for C programs. GRESS, PADRE-2, and ADOL-C implement both the forward and reverse modes.
Reference: [25] <author> D. Juedes, </author> <title> A taxonomy of automatic differentiation tools, in Proceedings of the Work--shop on Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. Corliss, eds., </editor> <address> Philadelphia, 1991, </address> <publisher> SIAM, </publisher> <pages> pp. 315-330. </pages>
Reference-contexts: The storage requirement of the reverse mode, however, can be a difficulty because of the possible dependence on L ff g+M ff g . Griewank [17] suggested a snapshot approach to circumvent this difficulty. There have been various implementations of automatic differentiation; an extensive survey can be found in <ref> [25] </ref>. In particular, we mention GRESS [24], and PADRE-2 [26] for Fortran programs and ADOL-C [20] for C programs. GRESS, PADRE-2, and ADOL-C implement both the forward and reverse modes.
Reference: [26] <author> K. Kubota, PADRE2, </author> <title> a FORTRAN precompiler yielding error estimates and second derivatives, in Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. F. Corliss, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991, </year> <pages> pp. 251-262. </pages>
Reference-contexts: Griewank [17] suggested a snapshot approach to circumvent this difficulty. There have been various implementations of automatic differentiation; an extensive survey can be found in [25]. In particular, we mention GRESS [24], and PADRE-2 <ref> [26] </ref> for Fortran programs and ADOL-C [20] for C programs. GRESS, PADRE-2, and ADOL-C implement both the forward and reverse modes. In order to save control flow information and intermediate values, these tools generate a trace of the computation by recording the particulars of every operation performed in the code.
Reference: [27] <author> M. Lescrenier, </author> <title> Partially separable optimization and parallel computing, </title> <journal> Ann. Oper. Res., </journal> <volume> 14 (1988), </volume> <pages> pp. 213-224. </pages>
Reference-contexts: Algorithms and software that take advantage of the partially separable structure have been developed for various problems. See, for example, <ref> [27, 14, 23, 31, 32, 33, 34] </ref>. In these algorithms the partially separable structure is used mainly to approximate the (dense) Hessian matrices r 2 f i (x) by quasi-Newton methods.
Reference: [28] <author> L. B. Rall, </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> vol. 120 of Lecture Notes in Computer Science, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: By applying the chain rule to the composition of those elementary operations, derivative information can be computed exactly and in a completely mechanical fashion <ref> [19, 28] </ref>. In Section 3 we propose two approaches for the computation of the Jacobian matrix f 0 (x). <p> We emphasize that the accuracy of the gradient in an optimization algorithm is of paramount importance because the gradient is used to determine the search directions. An inaccurate gradient can easily lead to false convergence. 2 The ADIFOR Tool and the SparsLinC Library Automatic differentiation <ref> [19, 28] </ref> (AD) is a chain-rule-based technique for evaluating the derivatives of functions defined by computer programs. AD produces code that, in the absence of floating-point exceptions, computes the values of the analytical derivatives ac 4 curate to machine precision.
Reference: [29] <author> N. Rostaing, S. Dalmas, and A. Galligo, </author> <title> Automatic differentiation in Odyssee, </title> <booktitle> Tellus, 45a (1993), </booktitle> <pages> pp. 558-568. </pages>
Reference-contexts: Recently, a source transformation approach to automatic differentiation has been explored in the ADIFOR [4, 6], ADIC [9], AMC [16], and Odyssee <ref> [29] </ref> tools. ADIFOR transforms Fortran 77 code, ADIC transforms ANSI-C code, and AMC and Odyssee transform a subset of Fortran 77. ADIFOR and ADIC mainly use the forward mode, with the reverse mode at the statement level, while AMC and Odyssee use the reverse mode.
Reference: [30] <author> E. Soulie, </author> <title> User's experience with Fortran compilers for least squares problems, in Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. F. Corliss, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991, </year> <pages> pp. 297-306. </pages>
Reference-contexts: The interpretation overhead associated with 5 using this trace for the purposes of automatic differentiation, as well as its potentially very large size, can be a serious computational bottleneck <ref> [30] </ref>. Recently, a source transformation approach to automatic differentiation has been explored in the ADIFOR [4, 6], ADIC [9], AMC [16], and Odyssee [29] tools. ADIFOR transforms Fortran 77 code, ADIC transforms ANSI-C code, and AMC and Odyssee transform a subset of Fortran 77.
Reference: [31] <author> P. L. Toint, </author> <title> Numerical solution of large sets of algebraic nonlinear equations, </title> <journal> Math. Comp., </journal> <volume> 46 (1986), </volume> <pages> pp. </pages> <month> 175-189. </month> <title> [32] , On large scale nonlinear least squares calculations, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 8 (1987), </volume> <pages> pp. 416-435. </pages>
Reference-contexts: Algorithms and software that take advantage of the partially separable structure have been developed for various problems. See, for example, <ref> [27, 14, 23, 31, 32, 33, 34] </ref>. In these algorithms the partially separable structure is used mainly to approximate the (dense) Hessian matrices r 2 f i (x) by quasi-Newton methods.
Reference: [33] <author> P. L. Toint and D. Tuyttens, </author> <title> On large-scale nonlinear network optimization, </title> <journal> Math. Programming, </journal> <volume> 48 (1990), </volume> <pages> pp. 125-159. </pages> <month> [34] , LSNNO: </month> <title> A Fortran subroutine for solving large-scale nonlinear network optimization problems, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 18 (1992), </volume> <pages> pp. 308-328. 21 </pages>
Reference-contexts: Algorithms and software that take advantage of the partially separable structure have been developed for various problems. See, for example, <ref> [27, 14, 23, 31, 32, 33, 34] </ref>. In these algorithms the partially separable structure is used mainly to approximate the (dense) Hessian matrices r 2 f i (x) by quasi-Newton methods.
References-found: 29

