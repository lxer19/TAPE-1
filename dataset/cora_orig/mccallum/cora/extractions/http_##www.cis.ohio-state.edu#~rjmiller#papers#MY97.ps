URL: http://www.cis.ohio-state.edu/~rjmiller/papers/MY97.ps
Refering-URL: http://www.cis.ohio-state.edu/~rjmiller/Teaching/788dir/projects.html
Root-URL: 
Email: rjmiller@cis.ohio-state.edu  yangy@cis.ohio-state.edu  
Title: Association Rules over Interval Data  
Author: R. J. Miller Y. Yang 
Affiliation: Dept of Computer Info. Science Ohio State University  Dept of Computer Info. Science Ohio State University  
Abstract: We consider the problem of mining association rules over interval data (that is, ordered data for which the separation between data points has meaning). We show that the measures of what rules are most important (also called rule interest) that are used for mining nominal and ordinal data do not capture the semantics of interval data. In the presence of interval data, support and confidence are no longer intuitive measures of the interest of a rule. We propose a new definition of interest for association rules that takes into account the semantics of interval data. We developed an algorithm for mining association rules under the new definition and overview our experience using the algorithm on large real-life datasets. 
Abstract-found: 1
Intro-found: 1
Reference: [AIS93] <author> R. Agrawal, T. Imielinksi, and A. Swami. </author> <title> Mining Association Rules between Sets of Items in Large Databases. </title> <booktitle> In Proc. of the ACM SIGMOD Int'l Conf. on Management of Data, </booktitle> <address> Washington, DC, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: ACM SIGMOD 26 (2):452-461, May, 1997 [HCC93]. Such work should not be confused with the definition of rules over quantitative data which is the problem considered in this paper. The term association rule has been used to describe a specific form of such rules <ref> [AIS93, AS94, CNFF96, HS95, MTV94, PCY95, SON95] </ref>. The rule frequency is a measure of how often a rule occurs in a data set. <p> When defined as the fraction of all tuples in a relation that satisfy a rule (specifically, jC 1 ^C 2 j=jrj where r is the set of all tuples considered), frequency is referred to as support <ref> [AIS93] </ref>. The strength of a rule implication is a measure of how often a rule is likely to be true within the data set. <p> For clarity, we will refer to this definition of association rules as classical association rules. Numerous algorithms have been proposed to solve this problem <ref> [AIS93, SON95, AS94, HCC93, HS95, MTV94, PCY95, Toi96] </ref>. The main idea behind many of these is the use of the minimum support threshold to limit the space of rules that needs to be searched [AS94]. <p> Algorithms for discovering classical association rules make different assumptions about the type of data sets to be mined. In general, the data set is a relational table. The domains of the attributes may be restricted to boolean domains <ref> [AIS93, AS94] </ref>. <p> To manage the added complexity of considering large domains, data values 1 Such boolean tables are often represented in an unnormalized form as a list of tuple identifiers paired with a set of values <ref> [AIS93] </ref>. Logically, each value represents an attribute on which the tuple has the value true. 1 may be grouped together and considered only collectively. Specifically, a hierarchy may be defined over the values of a domain (for example, a hierarchy of continent-country-- region-city may be used to group geographic values). <p> sections by developing a series of definitions for the association rule problem culminating in a definition of distance-based association rules that meets all stated goals. 3 Adaptive Solution While numerous algorithms for discovering association rules have been proposed, we give an outline that is common to many of these algorithms <ref> [AIS93, AS94] </ref>. Scan 1 Scan the data once and count the number of occurrences of each data value (that is, the number of tuples containing the value). Call this set candidate 1-itemsets. Set i = 1.
Reference: [AS94] <author> R. Agrawal and R. Srikant. </author> <title> Fast Algorithms for Mining Association Rules in Large Databases. </title> <booktitle> In Proc. of the Int'l Conf. on Very Large Data Bases (VLDB), </booktitle> <address> Santiago, Chile, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: ACM SIGMOD 26 (2):452-461, May, 1997 [HCC93]. Such work should not be confused with the definition of rules over quantitative data which is the problem considered in this paper. The term association rule has been used to describe a specific form of such rules <ref> [AIS93, AS94, CNFF96, HS95, MTV94, PCY95, SON95] </ref>. The rule frequency is a measure of how often a rule occurs in a data set. <p> For clarity, we will refer to this definition of association rules as classical association rules. Numerous algorithms have been proposed to solve this problem <ref> [AIS93, SON95, AS94, HCC93, HS95, MTV94, PCY95, Toi96] </ref>. The main idea behind many of these is the use of the minimum support threshold to limit the space of rules that needs to be searched [AS94]. <p> Numerous algorithms have been proposed to solve this problem [AIS93, SON95, AS94, HCC93, HS95, MTV94, PCY95, Toi96]. The main idea behind many of these is the use of the minimum support threshold to limit the space of rules that needs to be searched <ref> [AS94] </ref>. The algorithms require that the minimum confidence and support be specified a priori by a user. <p> Algorithms for discovering classical association rules make different assumptions about the type of data sets to be mined. In general, the data set is a relational table. The domains of the attributes may be restricted to boolean domains <ref> [AIS93, AS94] </ref>. <p> sections by developing a series of definitions for the association rule problem culminating in a definition of distance-based association rules that meets all stated goals. 3 Adaptive Solution While numerous algorithms for discovering association rules have been proposed, we give an outline that is common to many of these algorithms <ref> [AIS93, AS94] </ref>. Scan 1 Scan the data once and count the number of occurrences of each data value (that is, the number of tuples containing the value). Call this set candidate 1-itemsets. Set i = 1. <p> In Phase II, all clusters that also satisfy the frequency threshold (that is, clusters with sufficient support) are used. These clusters form the set of frequent itemsets that are used as input to the a priori algorithm <ref> [AS94] </ref>. If for some X i there are no frequent clusters, we omit X i from consideration in Phase II. Clusters are combined based on the frequency with which they appear together in the dataset. Frequent i-itemsets (for i &gt; 1) are identified as follows.
Reference: [CNFF96] <author> D. W. Cheung, V. T. Ng, A. W. Fu, and Y. Fu. </author> <title> Efficient Mining of Association Rules in Distributed Databases. </title> <journal> IEEE TKDE, </journal> <note> 1996. To ap-pear. </note>
Reference-contexts: ACM SIGMOD 26 (2):452-461, May, 1997 [HCC93]. Such work should not be confused with the definition of rules over quantitative data which is the problem considered in this paper. The term association rule has been used to describe a specific form of such rules <ref> [AIS93, AS94, CNFF96, HS95, MTV94, PCY95, SON95] </ref>. The rule frequency is a measure of how often a rule occurs in a data set.
Reference: [EKX95] <author> M. Ester, H.-P. Kriegel, and X. Xu. </author> <title> A Database Interface for Clustering in Large Spatial Databases. </title> <booktitle> In Proc. of the Int'l Conf. on Knowledge Discovery & Data Mining, </booktitle> <year> 1995. </year> <month> 9 </month>
Reference-contexts: The equi-depth partitioning of Figure 1 fails to meet both of these criteria. The problem of finding good clusters can be formalized as: find a set of K clusters that minimize a given distance metric (such as the average distance between pairs of points in each cluster) <ref> [KR90, EKX95, NH94, ZRL96] </ref>. Before we give a formal definition, we introduce some notation. Let R = fA 1 ; A 2 ; :::; A m g be a relation schema and r be a relation over R where jRj = m and jrj = n.
Reference: [Eve93] <author> B. S. Everitt. </author> <title> Cluster Analysis. </title> <publisher> Edward Arnold and Halsted Press, </publisher> <address> New York - Toronto, </address> <year> 1993. </year>
Reference-contexts: For interval data, there is a whole field devoted to the discovery and analysis of data groupings that reflect the relative distances between data points <ref> [Eve93] </ref>. Clustering techniques vary tremendously in how they use distances to determine groupings and there is no universal definition of what a cluster is or what properties it must have [Eve93]. <p> a whole field devoted to the discovery and analysis of data groupings that reflect the relative distances between data points <ref> [Eve93] </ref>. Clustering techniques vary tremendously in how they use distances to determine groupings and there is no universal definition of what a cluster is or what properties it must have [Eve93]. We adopt a common form of clustering to identify data groups that are compact (the distance between points within a cluster is small) and isolated (relatively separable from other groups). This latter criteria can be formalized by defining the region around the cluster to be relatively sparse.
Reference: [FPSM91] <author> W. J. Frawley, G. Piatetsky-Shapiro, and C. J. Matheus. </author> <title> Knowledge Discovery in Databases: An Overview. </title> <booktitle> In [PSF91], </booktitle> <year> 1991. </year>
Reference-contexts: The use of inappropriate standardization techniques may completely distort or destroy the clustering properties of the data. It is our assumption, and indeed one of the basic assumptions of data mining, that such a deep understanding of relative data semantics is not generally available across the full dataset <ref> [FPSM91] </ref>. Mining is a first step in trying to understand the data. If a semantically meaningful distance metric across a set of attributes is available, we consider those attributes together and apply clustering to the set of attributes.
Reference: [HCC93] <author> J. Han, Y. Cai, and N. Cercone. </author> <title> Data-Driven Discovery of Quantitative Rules in Relational Databases. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(1) </volume> <pages> 29-40, </pages> <year> 1993. </year>
Reference-contexts: Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publi- cations Dept., ACM Inc., fax +1 (212) 869-0481, or (permissions@acm.org). ACM SIGMOD 26 (2):452-461, May, 1997 <ref> [HCC93] </ref>. Such work should not be confused with the definition of rules over quantitative data which is the problem considered in this paper. The term association rule has been used to describe a specific form of such rules [AIS93, AS94, CNFF96, HS95, MTV94, PCY95, SON95]. <p> For clarity, we will refer to this definition of association rules as classical association rules. Numerous algorithms have been proposed to solve this problem <ref> [AIS93, SON95, AS94, HCC93, HS95, MTV94, PCY95, Toi96] </ref>. The main idea behind many of these is the use of the minimum support threshold to limit the space of rules that needs to be searched [AS94].
Reference: [HF95] <author> J. Han and Y. Fu. </author> <title> Discovery of Multiple-level Association Rules from Large Databases. </title> <booktitle> In Proc. of the Int'l Conf. on Very Large Data Bases (VLDB), </booktitle> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: Specifically, a hierarchy may be defined over the values of a domain (for example, a hierarchy of continent-country-- region-city may be used to group geographic values). This hierarchy may then be used to reduce the space of rules considered <ref> [SA95, HF95] </ref>. Additionally, if an attribute is linearly ordered then values may be grouped into ranges. Instead of considering all values of a Salary attribute, values may be partitioned into ranges (for example, ranges of $10,000 increments) [SA96]. <p> A group may be a semantic generalization of a set of data values (we can store one count for all cars rather than a separate count for Hon- das, Fords, etc.) or an interval of ordered values (ages 20- 30) <ref> [SA95, HF95] </ref>. However, the use of groups reduces the precision of the result.
Reference: [HS95] <author> M. Houtsma and A. Swami. </author> <title> Set-oriented Mining of Association Rules. </title> <booktitle> In Proc. of the Int'l Conf. on Data Engineering, </booktitle> <address> Taipei, Taiwan, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: ACM SIGMOD 26 (2):452-461, May, 1997 [HCC93]. Such work should not be confused with the definition of rules over quantitative data which is the problem considered in this paper. The term association rule has been used to describe a specific form of such rules <ref> [AIS93, AS94, CNFF96, HS95, MTV94, PCY95, SON95] </ref>. The rule frequency is a measure of how often a rule occurs in a data set. <p> For clarity, we will refer to this definition of association rules as classical association rules. Numerous algorithms have been proposed to solve this problem <ref> [AIS93, SON95, AS94, HCC93, HS95, MTV94, PCY95, Toi96] </ref>. The main idea behind many of these is the use of the minimum support threshold to limit the space of rules that needs to be searched [AS94].
Reference: [JD88] <author> A. K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: However, we show that these definitions and algorithms may yield very unintuitive results when applied to interval data where the separation between data values has meaning <ref> [JD88] </ref>. In Section 2, we explore these difficulties. In Section 3, we introduce the notion of an adaptive algorithm for which the quality of the results may be varied dynamically depending on the available memory resources. <p> Although designed for quantitative data, this measure uses only the ordinal properties of the data (a qualitative feature) to group adjacent values into intervals <ref> [JD88] </ref>. The initial partitioning is an equi-depth partitioning where Equi-depth Distance-based Salary No. Interval No. Interval 18K 1 1 [18K, 18K] 31K 2 2 [30K, 31K] 81K 3 3 [80K, 82K] the depth (support) of each partition is determined by the partial completeness level. <p> This quality measure is very appropriate for ordinal data where there is a linear order defined over the data but the separation between values has no meaning (for example, a domain where (1, 2, 3) is semantically equivalent to (1, 20, 300) <ref> [JD88] </ref>). However, the same cannot be said when this quality measure is applied to interval data. In Figure 1, we have given an example of an equi-depth partition of a Salary attribute.
Reference: [KR90] <author> L. Kaufman and P. J. Rousseeuw. </author> <title> Finding Groups in Data: An Introduction to Cluster Analysis. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> NY, </address> <year> 1990. </year>
Reference-contexts: The equi-depth partitioning of Figure 1 fails to meet both of these criteria. The problem of finding good clusters can be formalized as: find a set of K clusters that minimize a given distance metric (such as the average distance between pairs of points in each cluster) <ref> [KR90, EKX95, NH94, ZRL96] </ref>. Before we give a formal definition, we introduce some notation. Let R = fA 1 ; A 2 ; :::; A m g be a relation schema and r be a relation over R where jRj = m and jrj = n. <p> However, the area does not reflect the density or coverage of points within a cluster. We therefore have chosen to use a common measure from Statistics, the average pairwise (intra-cluster distance) or diameter of a cluster <ref> [KR90, ZRL96] </ref>. We use ffi X to denote a distance metric on values in the attribute set X, such as the Euclidean or Manhattan distance.
Reference: [Mil95] <author> G. W. Milligan. </author> <title> Clustering Validation: Results and Implications for Applied Analyses. </title> <editor> In P. Arabie, L. J. Huber, and G. DeSoete, editors, </editor> <booktitle> Clustering and Classification, </booktitle> <pages> pages 345-375. </pages> <publisher> World Scientific Publishing, </publisher> <address> River Edge, NJ, </address> <year> 1995. </year>
Reference-contexts: To cluster over multiple attributes, the scales must be standardized so that distances in the different dimensions are comparable. There are numerous forms of statistical standardization but their use must be predicated on a detailed understanding of the data and relationships between data in different dimensions <ref> [Mil95] </ref>. The use of inappropriate standardization techniques may completely distort or destroy the clustering properties of the data. It is our assumption, and indeed one of the basic assumptions of data mining, that such a deep understanding of relative data semantics is not generally available across the full dataset [FPSM91].
Reference: [MTV94] <author> H. Mannila, H. Toivonen, and A. I. Verkamo. </author> <title> Efficient Algorithms for Discovering Association Rules. </title> <booktitle> In Proc. of the AAAI Workshop on Knowledge Discovery in Databases, </booktitle> <pages> pages 181192, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: ACM SIGMOD 26 (2):452-461, May, 1997 [HCC93]. Such work should not be confused with the definition of rules over quantitative data which is the problem considered in this paper. The term association rule has been used to describe a specific form of such rules <ref> [AIS93, AS94, CNFF96, HS95, MTV94, PCY95, SON95] </ref>. The rule frequency is a measure of how often a rule occurs in a data set. <p> For clarity, we will refer to this definition of association rules as classical association rules. Numerous algorithms have been proposed to solve this problem <ref> [AIS93, SON95, AS94, HCC93, HS95, MTV94, PCY95, Toi96] </ref>. The main idea behind many of these is the use of the minimum support threshold to limit the space of rules that needs to be searched [AS94].
Reference: [NH94] <author> R. T. Ng and J. Han. </author> <title> Efficient and Effective Clustering Methods for Spatila Data Mining. </title> <booktitle> In Proc. of the Int'l Conf. on Very Large Data Bases (VLDB), </booktitle> <year> 1994. </year>
Reference-contexts: The equi-depth partitioning of Figure 1 fails to meet both of these criteria. The problem of finding good clusters can be formalized as: find a set of K clusters that minimize a given distance metric (such as the average distance between pairs of points in each cluster) <ref> [KR90, EKX95, NH94, ZRL96] </ref>. Before we give a formal definition, we introduce some notation. Let R = fA 1 ; A 2 ; :::; A m g be a relation schema and r be a relation over R where jRj = m and jrj = n.
Reference: [PCY95] <author> J. S. Park, M.-S. Chen, and P. S. Yu. </author> <title> An Effective Hash Based Algorithm for Mining Association Rules. </title> <booktitle> In Proc. of the ACM SIGMOD Int'l Conf. on Management of Data, </booktitle> <address> San Jose, CA, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: ACM SIGMOD 26 (2):452-461, May, 1997 [HCC93]. Such work should not be confused with the definition of rules over quantitative data which is the problem considered in this paper. The term association rule has been used to describe a specific form of such rules <ref> [AIS93, AS94, CNFF96, HS95, MTV94, PCY95, SON95] </ref>. The rule frequency is a measure of how often a rule occurs in a data set. <p> For clarity, we will refer to this definition of association rules as classical association rules. Numerous algorithms have been proposed to solve this problem <ref> [AIS93, SON95, AS94, HCC93, HS95, MTV94, PCY95, Toi96] </ref>. The main idea behind many of these is the use of the minimum support threshold to limit the space of rules that needs to be searched [AS94].
Reference: [PS91] <author> G. Piatetsky-Shapiro. </author> <title> Discovery, Analysis, and Presentation of Strong Rules. </title> <booktitle> In [PSF91], </booktitle> <pages> pages 229-248, </pages> <year> 1991. </year>
Reference-contexts: 1 Background Much work in data mining revolves around the discovery of rules within large quantities of data. Rules over relations are of the form C 1 )C 2 where C 1 and C 2 are conditions on tuples of the relation <ref> [PS91] </ref>. <p> rule may be exact, meaning that all tuples that satisfy C 1 also satisfy C 2 , it may be strong, meaning that tuples satisfying C 1 almost always satisfy C 2 or it may be approximate meaning the some of the tuples satisfying C 1 also satisfy C 2 <ref> [PS91] </ref>. Most often, the conditions are restricted to be simple equality predicates restricting attribute values (e.g., Quantity = 10) or conjunctions of such predicates on different attributes. Rules are typically ranked by some measure of interest. <p> Common measures are based on the frequency with which a rule appears in the relation or, for approximate rules, the strength of the rule implication. Less commonly this measure may be based on the complexity of the rule or other parameters <ref> [PS91] </ref>.
Reference: [PSF91] <author> G. Piatetsky-Shapiro and W. J. Frawley. </author> <title> Knowledge Discovery in Databases. </title> <publisher> AAAI Press/MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference: [SA95] <author> R. Srikant and R. Agrawal. </author> <title> Mining Generalized Association Rules. </title> <booktitle> In Proc. of the Int'l Conf. on Very Large Data Bases (VLDB), </booktitle> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: Specifically, a hierarchy may be defined over the values of a domain (for example, a hierarchy of continent-country-- region-city may be used to group geographic values). This hierarchy may then be used to reduce the space of rules considered <ref> [SA95, HF95] </ref>. Additionally, if an attribute is linearly ordered then values may be grouped into ranges. Instead of considering all values of a Salary attribute, values may be partitioned into ranges (for example, ranges of $10,000 increments) [SA96]. <p> A group may be a semantic generalization of a set of data values (we can store one count for all cars rather than a separate count for Hon- das, Fords, etc.) or an interval of ordered values (ages 20- 30) <ref> [SA95, HF95] </ref>. However, the use of groups reduces the precision of the result.
Reference: [SA96] <author> R. Srikant and R. Agrawal. </author> <title> Mining Quantitative Association Rules in Large Relational Tables. </title> <booktitle> In Proc. of the ACM SIGMOD Int'l Conf. on Management of Data, </booktitle> <address> Montreal, Canada, </address> <year> 1996. </year>
Reference-contexts: m = 1) is often abbreviated as X)Y where X = fX 1 ; :::; X n g and Y = fY 1 ; :::; Y m g are sets of attributes. 1 This definition has been generalized to include relational tables over arbitrary domains including qualitative and quantitative domains <ref> [SA96] </ref>. It is clear from the formulation of the association rule problem that the complexity of the search depends not only on the number of attributes, but also on the number of values an attribute may have. <p> Additionally, if an attribute is linearly ordered then values may be grouped into ranges. Instead of considering all values of a Salary attribute, values may be partitioned into ranges (for example, ranges of $10,000 increments) <ref> [SA96] </ref>. These solutions work well for nominal datasets in which the data values represent names with no relative meaning and ordinal data where data values have meaning only in relation to each other. <p> From these examples, we extract three goals that will motivate our algorithms. As noted in <ref> [SA96] </ref>, when data domains become large, the expense of finding association rules involving all possible values may be prohibitive. Using a higher support and confidence thresholds can reduce this cost but may also give very few rules. Semantically interesting rules may not be found. <p> Using a higher support and confidence thresholds can reduce this cost but may also give very few rules. Semantically interesting rules may not be found. To address this problem, Srikant and Agrawal have defined what they call quantitative association rules (QAR) <ref> [SA96] </ref>. A QAR is simply a classical association rule in which the predicates may be equality predicates (Attr = val) or range predicates (val1 Attr val2). To limit the number of ranges (intervals) that must be considered, quality metrics are imposed on what constitutes a "good" interval. <p> Based on this, a measure of K-partial completeness was defined to ensure that intervals are neither too big, nor too small with respect to the set of rules they can generate <ref> [SA96] </ref>. Although designed for quantitative data, this measure uses only the ordinal properties of the data (a qualitative feature) to group adjacent values into intervals [JD88]. The initial partitioning is an equi-depth partitioning where Equi-depth Distance-based Salary No. Interval No. <p> As Srikant and Agrawal point out, their measure of interval quality does not work well on skewed data since it may separate close values that have the same behavior (e.g., participate in the same rules) <ref> [SA96] </ref>. They suggest the need for developing alternative measures that are based on the range of an interval. In Section 4, we develop an alternative quality measure that is based on a more comprehensive consideration of the quantitative properties of an interval or region. <p> First, the formulation of the classical association rule is based on exact set membership. Indeed, it was initially motivated by data sets in which tuples contained simple sets of nominal values from a single unordered domain (also referred to as the boolean association rule problem <ref> [SA96] </ref>). Although, the confidence measure provides some notion of approximation ("60% of tuples satisfy Rule (1)") it does not provide any room for approximation in data values. In particular, it cannot be used to express a rule such as "30 year-old DBAs earn about 40,000". <p> This set may be defined as an interval on a single attribute as done in <ref> [SA96] </ref> or it may be an arbitrary region or neighborhood in a multidimensional space. Obviously, the cost of considering all possible subsets of data values is prohibitive. <p> For this reason, we say that a cluster is "defined on" X and denote the cluster C X . A possible quality measure on a one dimensional cluster is the range or smallest interval containing all points <ref> [SA96] </ref> or on 2 dimensions, the area of the smallest bounding box. However, the area does not reflect the density or coverage of points within a cluster. We therefore have chosen to use a common measure from Statistics, the average pairwise (intra-cluster distance) or diameter of a cluster [KR90, ZRL96]. <p> If we restrict consideration to one dimensional clusters, we can directly use the definition of quantitative association rules <ref> [SA96] </ref>. Let A be an attribute of R with quantitative domain D A . Let I R = f (A; l; u) j A is an attribute of R; l 2 D A ; u 2 D A and l ug. I A 2 I R is called an interval. <p> Notice that I X with non-empty extensions can be written as I A 1 [ ::: [ I A l for some set of distinct attributes A i and intervals I A i . Dfn 4.3 <ref> [SA96] </ref> A quantitative association rule is an implication of the form I X )I Y where I X I R and I Y I R where X " Y = ;. * A rule I X )I Y holds with confidence c if jI X [I Y j=jI X j c.
Reference: [SON95] <author> A. Savasere, E. Omiecinski, and S. Navathe. </author> <title> An Efficient Algorithm for Mining Association Rules in Large Databases. </title> <booktitle> In Proc. of the Int'l Conf. on Very Large Data Bases (VLDB), </booktitle> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: ACM SIGMOD 26 (2):452-461, May, 1997 [HCC93]. Such work should not be confused with the definition of rules over quantitative data which is the problem considered in this paper. The term association rule has been used to describe a specific form of such rules <ref> [AIS93, AS94, CNFF96, HS95, MTV94, PCY95, SON95] </ref>. The rule frequency is a measure of how often a rule occurs in a data set. <p> For clarity, we will refer to this definition of association rules as classical association rules. Numerous algorithms have been proposed to solve this problem <ref> [AIS93, SON95, AS94, HCC93, HS95, MTV94, PCY95, Toi96] </ref>. The main idea behind many of these is the use of the minimum support threshold to limit the space of rules that needs to be searched [AS94].
Reference: [Toi96] <author> H. Toivonen. </author> <title> Sampling Large Databases for Association Rules. </title> <booktitle> In Proc. of the Int'l Conf. on Very Large Data Bases (VLDB), </booktitle> <address> Bombay, India, </address> <year> 1996. </year>
Reference-contexts: For clarity, we will refer to this definition of association rules as classical association rules. Numerous algorithms have been proposed to solve this problem <ref> [AIS93, SON95, AS94, HCC93, HS95, MTV94, PCY95, Toi96] </ref>. The main idea behind many of these is the use of the minimum support threshold to limit the space of rules that needs to be searched [AS94].
Reference: [WM92] <author> W. H. Wolberg and O. Mangasarian. </author> <title> Wisconsin Breast Cancer Database. </title> <editor> In P. M. Murphy and D. W. Aha, editors, </editor> <booktitle> UCI Repository of Machine Learning Databases, </booktitle> <address> Irvine, CA, </address> <year> 1992. </year> <institution> University of California, Department of Information and Computer Science. </institution> <note> http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: In this study, the data set was the Wisconsin Breast Cancer Data (WBCD) obtained from UCI Machine Learning Repository <ref> [WM92] </ref>. We used a subset of the WBCD data with 500 tuples 4 . We used 30 of the 32 total attributes removing the key (tuple identifier) attribute and the binary outcome attribute.
Reference: [ZRL96] <author> T. Zhang, R. Ramakrishnan, and M. Livny. </author> <title> BIRCH: An Efficient Data Clustering Method for Very Large Databases. </title> <booktitle> In Proc. of the ACM SIGMOD Int'l Conf. on Management of Data, </booktitle> <address> Montreal, Canada, </address> <year> 1996. </year>
Reference-contexts: However, the use of groups reduces the precision of the result. Taking our cue from recent work in clustering of large datasets <ref> [ZRL96] </ref>, we redefine the association rule problem to include the operating constraint that given a limited amount of memory, we would like to find association rules at the finest (most detailed) level possible without dramatically increasing IO cost or response time. <p> The equi-depth partitioning of Figure 1 fails to meet both of these criteria. The problem of finding good clusters can be formalized as: find a set of K clusters that minimize a given distance metric (such as the average distance between pairs of points in each cluster) <ref> [KR90, EKX95, NH94, ZRL96] </ref>. Before we give a formal definition, we introduce some notation. Let R = fA 1 ; A 2 ; :::; A m g be a relation schema and r be a relation over R where jRj = m and jrj = n. <p> However, the area does not reflect the density or coverage of points within a cluster. We therefore have chosen to use a common measure from Statistics, the average pairwise (intra-cluster distance) or diameter of a cluster <ref> [KR90, ZRL96] </ref>. We use ffi X to denote a distance metric on values in the attribute set X, such as the Euclidean or Manhattan distance. <p> As noted in Section 3, we have drawn the initial idea for developing an adaptive association rule algorithm from Birch, an adaptive clustering algorithm <ref> [ZRL96, ZRL97] </ref>. In addition to being adaptive, Birch has linear IO cost and provides a mechanism for handling outliers that can be modify to help prune clusters with low frequency. We use Birch to find clusters over each attribute. The clusters are created incrementally and represented by a compact summary. <p> From the CFs of two clusters, the CF of their union and a number of distance metrics (including the diameter of Equa- tion (2) using the Euclidean distance as ffi X ) can be derived <ref> [ZRL96] </ref>. Hence, clusters can be combined and new points added to clusters using only the CFs. A clustering feature (CF) contains the number of tuples, the linear sum of the tuples and the square sum of the tuples of a cluster. <p> The rebuilding is done by re-inserting leaf CF nodes into the tree. Hence, the data or the portion of the data that has already been scanned does not need to be rescanned <ref> [ZRL96] </ref>. With the higher 2 This partitioning is defined by the user given the data semantics. The algorithm is applied to a single partitioning of the attributes. threshold, some clusters are likely to be merged reducing the space required for the tree. <p> Since this is done before all data has been scanned, clusters may be wrongly categorized as outliers. Hence, outliers need to be re-inserted into the complete tree to ensure that they are indeed outliers <ref> [ZRL96] </ref>. 4.3.2 Phase II Combining Clusters to Form Rules All clusters found in Phase I will satisfy the density threshold. In Phase II, all clusters that also satisfy the frequency threshold (that is, clusters with sufficient support) are used. <p> This distance measures the degree of association between C X and C Y . The distance between two clusters can be defined using any of a number of standard statistical measures, including the average inter-cluster distance or the centroid Manhattan distance <ref> [ZRL96] </ref>. We will apply these distance measures to clusters projected on specific attributes. Let Ci = ft i j : 1 j N i g. <p> distance D1 between the two images C1 [X] and C2 [X] is: The average inter-cluster distance D2 is defined as: D2 (C1 [X]; C2 [X]) = i=1 j=1 ffi X (t 1 j [X]) (6) These measures and a number of additional distance met- rics between clusters are defined in <ref> [ZRL96] </ref>. We will use D to refer to a distance metric between clusters when we are not making a distinction between specific measures. For a rule C X )C Y , the larger the distance between C Y [Y ]and C X [Y ] the weaker the implication. <p> For C X = ft 1 ; :::; t N g, the ACF includes the clustering feature (Equation 3) and for all attribute sets Y where Y 6= X: N X t i [Y ]; i=1 2 Like CFs, ACFs may be incrementally maintained. The Additivity Theorem <ref> [ZRL96] </ref> for CFs may be extended to ACFs. An ACF-tree is a CF-tree with the leaf nodes modified to be ACFs. The internal nodes remain CF nodes. An ACF-tree is maintained for each X i . <p> In addition, the tree may need to be rebuilt with a higher threshold if it becomes too large. The time for the rebuilding can be done in O (m (log L n) 2 ) <ref> [ZRL96] </ref>. So the worst case complexity of Phase I is O (m (log L n) 2 ). Phase I - IO In Phase I, we scan the data set once. If the ACF trees grow too large, the thresholds are adjusted to condense the trees.
Reference: [ZRL97] <author> T. Zhang, R. Ramakrishnan, and M. Livny. </author> <title> Data Clustering System BIRCH and Its Applications. </title> <note> Submitted for publication, 1997. 10 </note>
Reference-contexts: As noted in Section 3, we have drawn the initial idea for developing an adaptive association rule algorithm from Birch, an adaptive clustering algorithm <ref> [ZRL96, ZRL97] </ref>. In addition to being adaptive, Birch has linear IO cost and provides a mechanism for handling outliers that can be modify to help prune clusters with low frequency. We use Birch to find clusters over each attribute. The clusters are created incrementally and represented by a compact summary.
References-found: 24

