URL: http://www.eecs.umich.edu/PPP/IPPS93.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Title: KSR1 Multiprocessor: Analysis of Latency Hiding Techniques in a Sparse Solver  
Author: Daniel Windheiser, Eric L. Boyd, Eric Hao, Santosh G. Abraham, Edward S. Davidson 
Address: Ann Arbor, MI 48109-2122  
Affiliation: Department of Electrical Engineering and Computer Science The University of Michigan  
Abstract: This paper analyzes and evaluates some novel latency hiding features of the KSR1 multiprocessor: prefetch and poststore instructions and automatic updates. As a case study, we analyze the performance of an iterative sparse solver which generates irregular communications. We show that automatic updates significantly reduce the amount of communication. Although prefetch and poststore instructions reduce the coherence miss ratios, they do not significantly improve the sparse solver performance due to the overhead in executing these instructions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Inside the TC2000 Computer, BBN Advanced Computers, Inc., </institution> <year> 1990. </year>
Reference-contexts: The majority of these shared-address distributed memory multiprocessors can be classified as NUMA (Non-Uniform Memory Access) systems. In NUMA systems, a node contains a processor, associated caches, a portion of the shared memory, and an interface to the communication interconnect. The interconnect is either a multistage network (BBN TC2000 <ref> [1] </ref>, IBM RP3 [5]) or a low-dimensional mesh-based direct-connect network (Stanford DASH [10]). In NUMA systems, the memory access times are non-uniform depending on whether the location addressed is in the cache, the local memory or a remote memory.
Reference: [2] <author> D. Callahan, K. Kennedy, and A. Porterfield, </author> <title> "Software prefetching," </title> <booktitle> in Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 40-52, </pages> <year> 1991. </year>
Reference-contexts: The analysis and experimental results show that the automatic update feature does significantly increase the local cache hit ratio. Other cache protocols also employ automatic update features, e.g. [6]. 4.2 Effects of Prefetches Prefetch instructions have been proposed as a way to hide large access latencies <ref> [2, 7, 12] </ref>. Prefetches are especially efficient in the case of regular data access patterns since it is possible to determine in advance which data needs to be moved to the processor.
Reference: [3] <author> A. Chatterjee, J. Jin, and J. Volakis, </author> <title> "Application of edge-based finite elements and ABCs to 3-D scattering," </title> <note> To appear in the IEEE Transactions on Antennas and Propagation, </note> <year> 1993. </year>
Reference-contexts: Automatic updates occur for prefetch and poststore transactions, as in read transactions. 3 FEM-ATS: The Sparse Solver 3.1 Application Overview FEM-ATS (Finite Element Method Absorbing Termination Surface) is a radiation modeling application developed at the University of Michigan <ref> [3] </ref>. It determines the frequency response obtained from broadcasting a pulse of electromagnetic radiation at a solid object, where the object and the surrounding space is modeled as a collection of finite elements.
Reference: [4] <author> T. H. Dunigan, </author> <title> "Kendall square multiprocessor: Early experiences and performance," </title> <type> Technical Report ORNL/TM-12065, </type> <institution> Oak Ridge National Laboratory, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: For a single-ring KSR1, up to 32 cells are interconnected by a pipelined unidirectional slotted ring. Peak bandwidth of a single ring is 1 GB/sec; maximum achievable data transfer bandwidth is reported as 731 MB/sec <ref> [4] </ref>. The unit of transfer on the ring is one subpage (128 bytes) plus leading header information.
Reference: [5] <author> G. F. Pfister et al., </author> <title> "The IBM research parallel processor prototype (RP3)," </title> <booktitle> in Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1985. </year>
Reference-contexts: In NUMA systems, a node contains a processor, associated caches, a portion of the shared memory, and an interface to the communication interconnect. The interconnect is either a multistage network (BBN TC2000 [1], IBM RP3 <ref> [5] </ref>) or a low-dimensional mesh-based direct-connect network (Stanford DASH [10]). In NUMA systems, the memory access times are non-uniform depending on whether the location addressed is in the cache, the local memory or a remote memory. Theoretically, to execute a program in parallel, the programmer need only identify inherent parallelism.
Reference: [6] <author> J. R. Goodman and P. J. Woest, </author> <title> "The Wisconsin mul-ticube: A new large-scale cache-coherent multiprocessor," </title> <booktitle> in Proceedings of the Fifteenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 422-431, </pages> <year> 1988. </year>
Reference-contexts: The analysis and experimental results show that the automatic update feature does significantly increase the local cache hit ratio. Other cache protocols also employ automatic update features, e.g. <ref> [6] </ref>. 4.2 Effects of Prefetches Prefetch instructions have been proposed as a way to hide large access latencies [2, 7, 12]. Prefetches are especially efficient in the case of regular data access patterns since it is possible to determine in advance which data needs to be moved to the processor.
Reference: [7] <author> E. H. Gornish, E. D. Granston, and A. V. Veiden-baum, </author> <title> "Compiler-directed data prefetching in multiprocessors with memory hierarchies," </title> <booktitle> in Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pp. 354-368, </pages> <year> 1990. </year>
Reference-contexts: The analysis and experimental results show that the automatic update feature does significantly increase the local cache hit ratio. Other cache protocols also employ automatic update features, e.g. [6]. 4.2 Effects of Prefetches Prefetch instructions have been proposed as a way to hide large access latencies <ref> [2, 7, 12] </ref>. Prefetches are especially efficient in the case of regular data access patterns since it is possible to determine in advance which data needs to be moved to the processor.
Reference: [8] <author> E. Hagersten, A. Landin, and S. Haridi, </author> <title> "DDM | a cache-only memory architecture," </title> <booktitle> Computer, </booktitle> <month> Septem-ber </month> <year> 1992. </year>
Reference-contexts: As a result, a large fraction of main memory redundantly backs up the secondary caches, thus increasing the cost of the system without providing commensurate functionality. Recently, COMA (Cache-Only Memory Architecture) systems have been proposed <ref> [8] </ref>, where each node contains a processor, associated caches, and a network interface, as in the NUMA systems, but the main memory is absent. A larger cache at each node is then more affordable. Data items dynamically move between the caches based on the reference patterns in the parallel program.
Reference: [9] <institution> KSR1 Technical Summary, Kendall Square Research Corporation, </institution> <year> 1992. </year>
Reference-contexts: The solver's irregular communication pattern is related to the performance data measured on the KSR1. These experiments yield insight on how to effectively use the KSR1's latency hiding features. 2 KSR1 Architecture Summary The KSR1, (Figure 1) <ref> [9] </ref>, is built as a group of ALLCACHE engines, connected in a fat tree hierarchy of rings. Today 34 rings can be connected by a single second-level ring for a maximum configuration of 1088 processors.
Reference: [10] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy, </author> <title> "The directory-based cache coherence protocol for the DASH multiprocessor," </title> <booktitle> in Proceedings of the Seventeenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 148-159, </pages> <year> 1990. </year>
Reference-contexts: In NUMA systems, a node contains a processor, associated caches, a portion of the shared memory, and an interface to the communication interconnect. The interconnect is either a multistage network (BBN TC2000 [1], IBM RP3 [5]) or a low-dimensional mesh-based direct-connect network (Stanford DASH <ref> [10] </ref>). In NUMA systems, the memory access times are non-uniform depending on whether the location addressed is in the cache, the local memory or a remote memory. Theoretically, to execute a program in parallel, the programmer need only identify inherent parallelism.
Reference: [11] <author> U. Meier and R. Eigenmann, </author> <title> "Parallelization and performance of conjugate gradient algorithms on the Cedar hierarchical memory multiprocessor," </title> <booktitle> in Proc. of the 3rd ACM Sigplan Symp. on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The dis-cretization of the Maxwell equations leads to a system of complex linear equations which is solved using a diagonal-preconditioned symmetric biconjugate gradient method. A description of the parallelization of the conjugate gradient algorithm for the Cedar multiprocessor system, developed at the University of Illinois, can be found in <ref> [11] </ref>.
Reference: [12] <author> T. Mowry and A. Gupta, </author> <title> "Tolerating latency through software-controlled prefetching in shared-memory multiprocessors," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 12, no. 2, </volume> <pages> pp. 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The analysis and experimental results show that the automatic update feature does significantly increase the local cache hit ratio. Other cache protocols also employ automatic update features, e.g. [6]. 4.2 Effects of Prefetches Prefetch instructions have been proposed as a way to hide large access latencies <ref> [2, 7, 12] </ref>. Prefetches are especially efficient in the case of regular data access patterns since it is possible to determine in advance which data needs to be moved to the processor.
References-found: 12

