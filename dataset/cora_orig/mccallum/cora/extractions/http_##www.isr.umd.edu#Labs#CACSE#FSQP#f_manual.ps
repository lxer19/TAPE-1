URL: http://www.isr.umd.edu/Labs/CACSE/FSQP/f_manual.ps
Refering-URL: http://www.isr.umd.edu/Labs/CACSE/FSQP/fsqp.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: User's Guide for FFSQP Version 3.7: A FORTRAN Code for Solving Constrained Nonlinear (Minimax) Optimization
Author: Jian L. Zhou, Andre L. Tits, and Craig T. Lawrence 
Address: College Park, MD 20742  TR-92-107r2)  
Affiliation: Electrical Engineering Department and Institute for Systems Research University of Maryland,  (Systems Research Center  
Abstract: FFSQP is a set of FORTRAN subroutines for the minimization of the maximum of a set of smooth objective functions (possibly a single one, or even none at all) subject to general smooth constraints (if there is no objective function, the goal is to simply find a point satisfying the constraints). If the initial guess provided by the user is infeasible for some inequality constraint or some linear equality constraint, FFSQP first generates a feasible point for these constraints; subsequently the successive iterates generated by FFSQP all satisfy these constraints. Nonlinear equality constraints are turned into inequality constraints (to be satisfied by all iterates) and the maximum of the objective functions is replaced by an exact penalty function which penalizes nonlinear equality constraint violations only. The user has the option of either requiring that the (modified) objective function decrease at each iteration after feasibility for nonlinear inequality and linear constraints has been reached (monotone line search), or requiring a decrease within at most four iterations (nonmono-tone line search). He/She must provide subroutines that define the objective functions and constraint functions and may either provide subroutines to compute the gradients of these functions or require that FFSQP estimate them by forward finite differences. FFSQP implements two algorithms based on Sequential Quadratic Programming (SQP), modified so as to generate feasible iterates. In the first one (monotone line search), a certain Armijo type arc search is used with the property that the step of one is eventually accepted, a requirement for superlinear convergence. In the second one the same effect is achieved by means of a (nonmonotone) search along a straight line. The merit function used in both searches is the maximum of the objective functions if there is no nonlinear equality constraint. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.Q. Mayne & E. Polak, </author> <title> "Feasible Directions Algorithms for Optimization Problems with Equality and Inequality Constraints," </title> <journal> Math. </journal> <note> Programming 11 (1976) , 67-80. </note>
Reference-contexts: If the initial guess provided by the user is infeasible for nonlinear inequality constraints and linear constraints, FFSQP first generates a point satisfying all these constraints by iterating on the problem of minimizing the maximum of these constraints. Then, using Mayne Polak's scheme <ref> [1] </ref>, nonlinear equality constraints are turned into inequality constraints 2 h j (x) 0; j = 1; : : : ; n e and the original objective function max i2I f ff i (x)g is replaced by the modified objective function f m (x; p) = max ff i (x)g j=1 <p> In [2], an essentially arbitrary feasible descent direction d 1 = d 1 (x) is then computed. 4 Then for a certain scalar = (x) 2 <ref> [0; 1] </ref>, a feasible descent direction d = (1 )d 0 + d 1 is obtained, asymptotically close to d 0 : Finally a second order correction ~ d = ~ d (x; d; H) is computed, involving auxiliary function evaluations at x + d; and an Armijo type search is <p> The main payoff is that the auxiliary function evaluations can be dispensed with, except possibly at the first few iterations. First a direction d 1 = d 1 (x) is computed, which is feasible even at Karush-Kuhn-Tucker points. Then for a certain scalar ` = ` (x) 2 <ref> [0; 1] </ref>; a "local" feasible direction d ` = (1 ` )d 0 + ` d 1 is obtained, and at x + d ` the objective functions are tested and feasibility is checked. <p> j = 1; : : : ; t i n i ha j ; x k + d 1 i = b j ; j = 1; : : : ; t e n e 3 This is a refinement (saving much computation and memory) of the scheme proposed in <ref> [1] </ref>. 9 iii. Set v k = minfC k kd 0 k k 2 ; kd 0 k kg. <p> Set v k = minfC k kd 0 k k 2 ; kd 0 k kg. Define values g g k;j equal to zero if k i v k or equal to the maximum in <ref> [0; 1] </ref> such that g j (x k ) + hrg j (x k ); (1 )d 0 k i v k otherwise. Similarly, define values h k;j for j = 1; : : : ; n e .
Reference: [2] <author> E.R. Panier & A.L. </author> <title> Tits, "On Combining Feasibility, Descent and Superlinear Convergence in Inequality Constrained Optimization," </title> <journal> Math. </journal> <note> Programming 59 (1993) , 261-276. </note>
Reference-contexts: Thus, FFSQP solves the original problem with nonlinear equality constraints by solving a modified optimization problem with only linear constraints and nonlinear inequality constraints. For the transformed problem, it implements algorithms that are described and analyzed in <ref> [2] </ref>, [3] and [4], with some additional refinements. These algorithms are based on a Sequential Quadratic Programming (SQP) iteration, modified so as to generate feasible iterates. The merit function is the objective function. An Armijo-type line search is used to generate an initial feasible point when required. <p> The merit function is the objective function. An Armijo-type line search is used to generate an initial feasible point when required. After obtaining feasibility, either (i) an Armijo-type line search may be used, yielding a monotone decrease of the objective function at each iteration <ref> [2] </ref>; or (ii) a nonmonotone line search (inspired from [5] and analyzed in [3] and [4] in this context) may be selected, forcing a decrease of the objective function within at most four iterations. <p> For the solution of the quadratic programming subproblems, FFSQP is set up to call QLD [7] which is provided with the FFSQP distribution for the user's convenience. 2 Description of the Algorithms The algorithms described and analyzed in <ref> [2] </ref>, [3] and [4] are as follows. <p> In <ref> [2] </ref>, an essentially arbitrary feasible descent direction d 1 = d 1 (x) is then computed. 4 Then for a certain scalar = (x) 2 [0; 1], a feasible descent direction d = (1 )d 0 + d 1 is obtained, asymptotically close to d 0 : Finally a second order <p> Conditions are given in <ref> [2] </ref> on d 1 (), () and ~ d (; ) that result in a globally convergent, locally superlinear convergent algorithm. The algorithm in [3] is somewhat more sophisticated. <p> not accepted, a "global" feasible descent direction d g = (1 g )d 0 + g d 1 is obtained with g = g (x) 2 [0; ` ]: A second order correction ~ d = ~ d (x; d g ; H) is computed the same way as in <ref> [2] </ref>, and a "nonmonotone" search is performed along the arc x + td g + t 2 ~ d: Here the purpose of ~ d is to suitably initialize the sequence for the "four iterate" rule. <p> The details are given below. The analysis in <ref> [2] </ref>, [3] and [4] can be easily extended to these modified algorithms.
Reference: [3] <author> J.F. Bonnans, E.R. Panier, A.L. Tits & J.L. Zhou, </author> <title> "Avoiding the Maratos Effect by Means of a Nonmonotone Line Search. II. Inequality Constrained Problems Feasible Iterates," </title> <journal> SIAM J. Numer. Anal. </journal> <month> 29 </month> <year> (1992) </year> <month> , 1187-1202. </month>
Reference-contexts: Thus, FFSQP solves the original problem with nonlinear equality constraints by solving a modified optimization problem with only linear constraints and nonlinear inequality constraints. For the transformed problem, it implements algorithms that are described and analyzed in [2], <ref> [3] </ref> and [4], with some additional refinements. These algorithms are based on a Sequential Quadratic Programming (SQP) iteration, modified so as to generate feasible iterates. The merit function is the objective function. An Armijo-type line search is used to generate an initial feasible point when required. <p> After obtaining feasibility, either (i) an Armijo-type line search may be used, yielding a monotone decrease of the objective function at each iteration [2]; or (ii) a nonmonotone line search (inspired from [5] and analyzed in <ref> [3] </ref> and [4] in this context) may be selected, forcing a decrease of the objective function within at most four iterations. <p> For the solution of the quadratic programming subproblems, FFSQP is set up to call QLD [7] which is provided with the FFSQP distribution for the user's convenience. 2 Description of the Algorithms The algorithms described and analyzed in [2], <ref> [3] </ref> and [4] are as follows. <p> Conditions are given in [2] on d 1 (), () and ~ d (; ) that result in a globally convergent, locally superlinear convergent algorithm. The algorithm in <ref> [3] </ref> is somewhat more sophisticated. An essential difference is that while feasibility is still required, the requirement of decrease of the max objective value is replaced by the weaker requirement that the max objective value at the new point be lower than its maximum over the last four iterates. <p> Conditions are given in <ref> [3] </ref> on d 1 (), ` (), g (), and ~ d (; ) that result in a globally convergent, locally superlinear convergent algorithm. In [4], the algorithm of [3] is refined for the case of unconstrained minimax problems. The major difference over the algorithm of [3] is that there is <p> Conditions are given in <ref> [3] </ref> on d 1 (), ` (), g (), and ~ d (; ) that result in a globally convergent, locally superlinear convergent algorithm. In [4], the algorithm of [3] is refined for the case of unconstrained minimax problems. The major difference over the algorithm of [3] is that there is no need of d 1 . As in [3], ~ d is required to initialize superlinear convergence. <p> Conditions are given in <ref> [3] </ref> on d 1 (), ` (), g (), and ~ d (; ) that result in a globally convergent, locally superlinear convergent algorithm. In [4], the algorithm of [3] is refined for the case of unconstrained minimax problems. The major difference over the algorithm of [3] is that there is no need of d 1 . As in [3], ~ d is required to initialize superlinear convergence. The FFSQP implementation corresponds to a specific choice of d 1 (), (), ~ d (; ), ` (), and g (), with some modifications as follows. <p> In [4], the algorithm of <ref> [3] </ref> is refined for the case of unconstrained minimax problems. The major difference over the algorithm of [3] is that there is no need of d 1 . As in [3], ~ d is required to initialize superlinear convergence. The FFSQP implementation corresponds to a specific choice of d 1 (), (), ~ d (; ), ` (), and g (), with some modifications as follows. <p> The details are given below. The analysis in [2], <ref> [3] </ref> and [4] can be easily extended to these modified algorithms. Also obvious simplifications are introduced concerning linear constraints: the iterates are allowed (for inequality constraints) or are forced (for equality constraints) to stay on the boundary of these constraints and these constraints are not checked in the line search.
Reference: [4] <author> J.L. Zhou & A.L. </author> <title> Tits, "Nonmonotone Line Search for Minimax Problems," </title> <journal> J. Optim. Theory Appl. </journal> <month> 76 </month> <year> (1993) </year> <month> , 455-476. </month>
Reference-contexts: Thus, FFSQP solves the original problem with nonlinear equality constraints by solving a modified optimization problem with only linear constraints and nonlinear inequality constraints. For the transformed problem, it implements algorithms that are described and analyzed in [2], [3] and <ref> [4] </ref>, with some additional refinements. These algorithms are based on a Sequential Quadratic Programming (SQP) iteration, modified so as to generate feasible iterates. The merit function is the objective function. An Armijo-type line search is used to generate an initial feasible point when required. <p> After obtaining feasibility, either (i) an Armijo-type line search may be used, yielding a monotone decrease of the objective function at each iteration [2]; or (ii) a nonmonotone line search (inspired from [5] and analyzed in [3] and <ref> [4] </ref> in this context) may be selected, forcing a decrease of the objective function within at most four iterations. <p> For the solution of the quadratic programming subproblems, FFSQP is set up to call QLD [7] which is provided with the FFSQP distribution for the user's convenience. 2 Description of the Algorithms The algorithms described and analyzed in [2], [3] and <ref> [4] </ref> are as follows. <p> Conditions are given in [3] on d 1 (), ` (), g (), and ~ d (; ) that result in a globally convergent, locally superlinear convergent algorithm. In <ref> [4] </ref>, the algorithm of [3] is refined for the case of unconstrained minimax problems. The major difference over the algorithm of [3] is that there is no need of d 1 . As in [3], ~ d is required to initialize superlinear convergence. <p> The details are given below. The analysis in [2], [3] and <ref> [4] </ref> can be easily extended to these modified algorithms. Also obvious simplifications are introduced concerning linear constraints: the iterates are allowed (for inequality constraints) or are forced (for equality constraints) to stay on the boundary of these constraints and these constraints are not checked in the line search.
Reference: [5] <author> L. Grippo, F. Lampariello & S. Lucidi, </author> <title> "A Nonmonotone Line Search Technique for Newton's Method," </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 23 (1986) , 707-716. </volume> <pages> 41 </pages>
Reference-contexts: An Armijo-type line search is used to generate an initial feasible point when required. After obtaining feasibility, either (i) an Armijo-type line search may be used, yielding a monotone decrease of the objective function at each iteration [2]; or (ii) a nonmonotone line search (inspired from <ref> [5] </ref> and analyzed in [3] and [4] in this context) may be selected, forcing a decrease of the objective function within at most four iterations.
Reference: [6] <author> C.T. Lawrence & A.L. </author> <title> Tits, "Nonlinear Equality Constraints in Feasible Sequential Quadratic Programming," </title> <note> Optimization Methods and Software 6 (1996) , 265-282. </note>
Reference-contexts: After turning nonlinear equality constraints into inequality constraints, these algorithms are used directly to solve the modified problems. Certain procedures (see <ref> [6] </ref>) are adopted to obtain proper values of p j 's in order to ensure that a solution of the modified problem is also a solution of the original problem, as described below.
Reference: [7] <author> K. Schittkowski, </author> <title> QLD : A FORTRAN Code for Quadratic Programming, User's Guide, </title> <institution> Mathematisches Institut, Universitat Bayreuth, Germany, </institution> <year> 1986. </year>
Reference-contexts: For the solution of the quadratic programming subproblems, FFSQP is set up to call QLD <ref> [7] </ref> which is provided with the FFSQP distribution for the user's convenience. 2 Description of the Algorithms The algorithms described and analyzed in [2], [3] and [4] are as follows.
Reference: [8] <author> M.J.D. Powell, </author> <title> "A Fast Algorithm for Nonlinearly Constrained Optimization Calculations," in Numerical Analysis, Dundee, </title> <booktitle> 1977, Lecture Notes in Mathematics 630, </booktitle> <address> G.A. Watson, </address> <publisher> ed., Springer-Verlag, </publisher> <year> 1978, </year> <pages> 144-157. </pages>
Reference-contexts: Updates. i. If nset &gt; 5n and t k &lt; t, set H k+1 = H 0 and nset = 0. Otherwise, set nset = nset + 1 and compute a new approximation H k+1 to the Hessian of the Lagrangian using the BFGS formula with Powell's modification <ref> [8] </ref>. ii. Set x k+1 = x k + t k d k + t 2 ~ d k . iii. <p> Updates. i. If nset &gt; 5n and t k &lt; t, set H k+1 = H 0 and nset = 0. Otherwise, set nset = nset + 1 and compute a new approximation H k+1 to the Hessian of the Lagrangian using the BFGS formula with Powell's modification <ref> [8] </ref>. ii. If kd 0 k k &gt; d, set C k+1 = maxf0:5C k ; Cg: Otherwise, if g j (x k + d ` k ) 0; j = 1; : : : ; n i , set C k+1 = C k .
Reference: [9] <author> W. Hock & K. Schittkowski, </author> <title> Test Examples for Nonlinear Programming Codes, </title> <booktitle> Lecture Notes in Economics and Mathematical Systems (187), </booktitle> <publisher> Springer Verlag, </publisher> <year> 1981. </year>
Reference-contexts: diagnl di1 dqp error estlam fool fuscmp indexs matrcp matrvc nullvc resign sbout1 sbout2 scaprd shift slope small 7.3 Reserved Common Blocks The following named common blocks are used in FFSQP and QLD: fsqpp1 fsqpp2 fsqpp3 fsqpq1 fsqpq2 fsqplo fsqpqp fsqpus CMACHE 8 Examples The first problem is borrowed from <ref> [9] </ref> (Problem 32). It involves a single objective function, simple bounds on the variables, nonlinear inequality constraints, and linear equality constraints. <p> 0.50000000000000E+00 0.15000000000000E+01 0.25000000000000E+01 33 objmax 0.22051986506559E+00 constraints -0.75000000000000E-01 -0.75000000000000E-01 -0.75000000000000E-01 -0.75000000000000E-01 iteration 7 x 0.42500000000000E+00 0.12750000000000E+01 0.21840763196688E+01 objective max4 0.11421841317792E+00 objmax 0.11310472749825E+00 constraints 0.00000000000000E+00 0.00000000000000E+00 0.00000000000000E+00 -0.26419918997596E+00 d0norm 0.15659306304681E-09 ktnorm 0.20560174107850E-10 ncallf 1141 inform 0 Normal termination: You have obtained a solution !! 34 Our third example is borrowed from <ref> [9] </ref> (Problem 71). <p> Table 1 contains results obtained for some (non-minimax) test problems from <ref> [9] </ref> (the same initial points as in [9] were selected). prob indicates the problem number as in [9], nineqn the number of nonlinear constraints, ncallf the total number of evaluations of the objective function, ncallg the total number of evaluations of the (scalar) nonlinear constraint functions, iter the total number of <p> Table 1 contains results obtained for some (non-minimax) test problems from <ref> [9] </ref> (the same initial points as in [9] were selected). prob indicates the problem number as in [9], nineqn the number of nonlinear constraints, ncallf the total number of evaluations of the objective function, ncallg the total number of evaluations of the (scalar) nonlinear constraint functions, iter the total number of iterations, objective the final value of the <p> Table 1 contains results obtained for some (non-minimax) test problems from <ref> [9] </ref> (the same initial points as in [9] were selected). prob indicates the problem number as in [9], nineqn the number of nonlinear constraints, ncallf the total number of evaluations of the objective function, ncallg the total number of evaluations of the (scalar) nonlinear constraint functions, iter the total number of iterations, objective the final value of the objective, d0norm the norm of SQP direction d 0 at <p> In most cases, eps was selected so as to achieve the same field precision as in <ref> [9] </ref>. Whether FFSQP-AL (0) or FFSQP-NL (1) is used is indicated in column "B". Results obtained on selected minimax problems are summarized in Table 2. <p> All of the above are either unconstrained or linearly constrained minimax problems. Unable to find nonlinearly constrained minimax test problems in the literature, we constructed problems p43m through p117m from problems 43, 84, 113 and 117 in <ref> [9] </ref> by removing certain constraints and including instead additional objectives of the form f i (x) = f (x) + ff i g i (x) where the ff i 's are positive scalars and g i (x) 0: Specifically, p43m is constructed from problem 43 by taking out the first two <p> The results for FFSQP should be identical. 39 of the max of the objective functions. Table 3 contains results of problems with nonlinear equality constraints from <ref> [9] </ref>. <p> as it is set to 10 4 for all of the problems except p46, where it was 5.E-3, and p27, where it was 1.E-3 (increased due to slow convergence). epseqn is the norm requirement on the values of the equality constraints and is chosen close to the corresponding values in <ref> [9] </ref>.
Reference: [10] <author> K. Madsen & H. Schjr-Jacobsen, </author> <title> "Linearly Constrained Minimax Optimization," </title> <journal> Math. </journal> <note> Programming 14 (1978) , 208-223. </note>
Reference-contexts: and linear equality constraints: x 0.10000000000000E+00 0.20000000000000E+00 objectives 0.72000000000000E+01 constraints -0.19990000000000E+01 0.55511151231258E-16 29 iteration 3 x -0.98607613152626E-31 0.10000000000000E+01 objectives 0.10000000000000E+01 constraints -0.10000000000000E+01 0.00000000000000E+00 d0norm 0.13945222387368E-30 ktnorm 0.10609826585190E-29 ncallf 3 ncallg 5 inform 0 Normal termination: You have obtained a solution !! Our second example is taken from example 6 in <ref> [10] </ref>. <p> [11]; cb2, cb3, r-s, wong and colv are from [12; Examples 5.1-5] (the latest test results on problems bard down to wong can be found in [13]); kiw1 and kiw4 are from [14] (results for kiw2 and kiw3 are not reported due to data disparity); mad1 to mad8 are from <ref> [10, Examples 1-8] </ref>; polk1 to polk4 are from [15]. Some of these test problems allow one to freely select the number of variables; problems wats-6 and wats-20 correspond to 6 and 20 variables respectively, and mad8-10, mad8-30 and mad8-50 to 10, 30 and 50 variables respectively.
Reference: [11] <author> G.A. Watson, </author> <title> "The Minimax Solution of an Overdetermined System of Non-linear Equations," </title> <journal> J. Inst. Math. Appl. </journal> <month> 23 </month> <year> (1979) </year> <month> , 167-180. </month>
Reference-contexts: Whether FFSQP-AL (0) or FFSQP-NL (1) is used is indicated in column "B". Results obtained on selected minimax problems are summarized in Table 2. Problems bard, davd2, f&r, hettich, and wats are from <ref> [11] </ref>; cb2, cb3, r-s, wong and colv are from [12; Examples 5.1-5] (the latest test results on problems bard down to wong can be found in [13]); kiw1 and kiw4 are from [14] (results for kiw2 and kiw3 are not reported due to data disparity); mad1 to mad8 are from [10,
Reference: [12] <author> C. Charalambous & A.R. Conn, </author> <title> "An Efficient Method to Solve the Minimax Problem Directly," </title> <journal> SIAM J. Numer. Anal. </journal> <month> 15 </month> <year> (1978) </year> <month> , 162-187. </month>
Reference-contexts: Whether FFSQP-AL (0) or FFSQP-NL (1) is used is indicated in column "B". Results obtained on selected minimax problems are summarized in Table 2. Problems bard, davd2, f&r, hettich, and wats are from [11]; cb2, cb3, r-s, wong and colv are from <ref> [12; Examples 5.1-5] </ref> (the latest test results on problems bard down to wong can be found in [13]); kiw1 and kiw4 are from [14] (results for kiw2 and kiw3 are not reported due to data disparity); mad1 to mad8 are from [10, Examples 1-8]; polk1 to polk4 are from [15].
Reference: [13] <author> A.R. Conn & Y. Li, </author> <title> "An Efficient Algorithm for Nonlinear Minimax Problems," </title> <institution> University of Waterloo, Research Report CS-88-41, Waterloo, </institution> <address> Ontario, N2L 3G1 Canada, </address> <month> November, </month> <year> 1989 </year> . 
Reference-contexts: Results obtained on selected minimax problems are summarized in Table 2. Problems bard, davd2, f&r, hettich, and wats are from [11]; cb2, cb3, r-s, wong and colv are from [12; Examples 5.1-5] (the latest test results on problems bard down to wong can be found in <ref> [13] </ref>); kiw1 and kiw4 are from [14] (results for kiw2 and kiw3 are not reported due to data disparity); mad1 to mad8 are from [10, Examples 1-8]; polk1 to polk4 are from [15].
Reference: [14] <author> K.C. Kiwiel, </author> <title> Methods of Descent in Nondifferentiable Optimization, </title> <booktitle> Lecture Notes in Mathematics #1133, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New-York, Tokyo, </address> <year> 1985. </year>
Reference-contexts: Problems bard, davd2, f&r, hettich, and wats are from [11]; cb2, cb3, r-s, wong and colv are from [12; Examples 5.1-5] (the latest test results on problems bard down to wong can be found in [13]); kiw1 and kiw4 are from <ref> [14] </ref> (results for kiw2 and kiw3 are not reported due to data disparity); mad1 to mad8 are from [10, Examples 1-8]; polk1 to polk4 are from [15].
Reference: [15] <author> E. Polak, D.Q. Mayne & J.E. Higgins, </author> <title> "A Superlinearly Convergent Algorithm for Min-max Problems," </title> <booktitle> Proceedings of the 28th IEEE Conference on Decision and Control, </booktitle> <address> Tampa, Florida (December 1989) </address> . 
Reference-contexts: from [12; Examples 5.1-5] (the latest test results on problems bard down to wong can be found in [13]); kiw1 and kiw4 are from [14] (results for kiw2 and kiw3 are not reported due to data disparity); mad1 to mad8 are from [10, Examples 1-8]; polk1 to polk4 are from <ref> [15] </ref>. Some of these test problems allow one to freely select the number of variables; problems wats-6 and wats-20 correspond to 6 and 20 variables respectively, and mad8-10, mad8-30 and mad8-50 to 10, 30 and 50 variables respectively. All of the above are either unconstrained or linearly constrained minimax problems.

References-found: 15

