URL: ftp://ftp.speech.sri.com/pub/papers/eurospeech97-prosody.ps
Refering-URL: http://www.speech.sri.com/people/stolcke/publications.html
Root-URL: 
Email: fees,stolckeg@speech.sri.com;  becky@raven.bu.edu;  
Phone: 2  
Title: A PROSODY-ONLY DECISION-TREE MODEL FOR DISFLUENCY DETECTION  
Author: Elizabeth Shriberg Rebecca Bates Andreas Stolcke 
Web: http://www.speech.sri.com  http://raven.bu.edu  
Address: Menlo Park, California  Boston, Massachusetts  
Affiliation: 1 Speech Technology and Research Laboratory, SRI International,  Dept. of Electrical Engineering, Boston University,  
Abstract: Speech disfluencies (filled pauses, repetitions, repairs, and false starts) are pervasive in spontaneous speech. The ability to detect and correct disfluencies automatically is important for effective natural language understanding, as well as to improve speech models in general. Previous approaches to disfluency detection have relied heavily on lexical information, which makes them less applicable when word recognition is unreliable. We have developed a disfluency detection method using decision tree classifiers that use only local and automatically extracted prosodic features. Because the model doesn't rely on lexical information, it is widely applicable even when word recognition is unreliable. The model performed significantly better than chance at detecting four disfluency types. It also outperformed a language model in the detection of false starts, given the correct transcription. Combining the prosody model with a specialized language model improved accuracy over either model alone for the detection of false starts. Results suggest that a prosody-only model can aid the automatic detection of disfluencies in spontaneous speech. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Bear, J. Dowding, and E. Shriberg. </author> <title> Integrating multiple knowledge sources for detection and correction of repairs in human-computer dialog. </title> <booktitle> In Proc. ACL, </booktitle> <pages> pp. 5663, </pages> <institution> University of Delaware, Newark, Delaware, </institution> <year> 1992. </year>
Reference-contexts: Automatic detection of disfluencies could also benefit higher-level modeling, for example, the automatic segmentation of speech into sentences [11], and the modeling of discourse or topic structure [13]. 1.2. Why use prosody? Various approaches to automatic disfluency detection have been proposed in past work <ref> [8, 1, 7, 4] </ref>. These studies have focused on task-oriented dialog and have used a combination of lexical and prosodic features. Results have shown a heavy reliance on lexical information, although prosodic information was also useful when constrained by the lexical information.
Reference: [2] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: Decision Trees and Language Models For our prosodic model, we chose decision trees because they can be inspected to determine the role of features and feature combinations in classification. We used CART-style decision trees <ref> [2] </ref>, a widely used data modeling algorithm convenient for replication of results. The decision trees (DTs) take a collection of acoustic features X as input and predict disfluency events D by asking questions of the features. The DT outputs posterior probability estimates P (DjX).
Reference: [3] <author> J. J. Godfrey, E. C. Holliman, and J. McDaniel. </author> <title> SWITCHBOARD: Telephone speech corpus for research and development. </title> <booktitle> In Proc. ICASSP, </booktitle> <volume> vol. I, </volume> <pages> pp. 517520, </pages> <address> San Francisco, </address> <year> 1992. </year>
Reference-contexts: METHOD 2.1. Data Speech data consisted of more than 1000 conversations from the Switchboard corpus of human-human telephone dialogs on prescribed topics <ref> [3] </ref>. The data set represents 364 different speakers (45% male, 55% female). Data were divided into randomly selected independent training (500,000 words) and test (60,000 words) sets, with no speaker overlap.
Reference: [4] <author> P. A. Heeman and J. Allen. </author> <title> Detecting and correcting speech repairs. </title> <booktitle> In Proc. ACL, </booktitle> <pages> pp. 295302, </pages> <address> New Mexico State University, Las Cruces, NM, </address> <year> 1994. </year>
Reference-contexts: Automatic detection of disfluencies could also benefit higher-level modeling, for example, the automatic segmentation of speech into sentences [11], and the modeling of discourse or topic structure [13]. 1.2. Why use prosody? Various approaches to automatic disfluency detection have been proposed in past work <ref> [8, 1, 7, 4] </ref>. These studies have focused on task-oriented dialog and have used a combination of lexical and prosodic features. Results have shown a heavy reliance on lexical information, although prosodic information was also useful when constrained by the lexical information.
Reference: [5] <institution> LVCSR Hub 5 Workshop, Linthicum Heights, MD, </institution> <year> 1996. </year>
Reference-contexts: In past work, a correct transcription was assumeda reasonable approach given that error rates for the corpora used were quite low (typically under 5% word error rate). For more natural speech corpora, even state-of-the-art systems are much less accurate (e.g., about 40% WER for Switchboard as of 1996 <ref> [5] </ref>). Thus, while prosody played a lesser role in studies based on correct transcriptions, it could be an important knowledge source for detecting disfluencies when word hypotheses are less reliable. <p> at various levels of resolution, including * Word transcripts * Hand-labeled disfluency annotations and sentence segmentations prepared by the Linguistic Data Consortium (LDC) [6] * Phone-level time marks produced by forced alignment of the word transcripts using the SRI Decipher (TM) speech recognizer, as used in the 1996 LVCSR evaluations <ref> [5] </ref> * Raw acoustic measurements for the prosodic features described below, such as fundamental frequency (F0) and signal-to-noise ratio (SNR) values. To limit computation and to facilitate integration with a language model, our datapoints consisted of each inter-word boundary, as determined by the forced alignments.
Reference: [6] <author> M. Meteer et al. </author> <title> Dysfluency annotation stylebook for the Switchboard corpus. Linguistic Data Consortium, </title> <note> 1995. Revised June 1995 by Ann Taylor. </note>
Reference-contexts: We prepared a speech database that combined information from various sources, and at various levels of resolution, including * Word transcripts * Hand-labeled disfluency annotations and sentence segmentations prepared by the Linguistic Data Consortium (LDC) <ref> [6] </ref> * Phone-level time marks produced by forced alignment of the word transcripts using the SRI Decipher (TM) speech recognizer, as used in the 1996 LVCSR evaluations [5] * Raw acoustic measurements for the prosodic features described below, such as fundamental frequency (F0) and signal-to-noise ratio (SNR) values. <p> From this we can obtain another posterior probability estimate P (DjW ) = P (D; W )=P (W ). The LM used was a disfluency N-gram model of the type used in [12], and was trained on 1.4 million words of Switchboard transcripts, hand-annotated for disfluencies by LDC <ref> [6] </ref>. Finally, we want to combine the DT classifier based on acoustic information with the LM classifier based on word information for a combined estimate.
Reference: [7] <author> C. H. Nakatani and J. Hirschberg. </author> <title> A corpus-based study of repair cues in spontaneous speech. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 95(3):16031616, </volume> <year> 1994. </year>
Reference-contexts: Automatic detection of disfluencies could also benefit higher-level modeling, for example, the automatic segmentation of speech into sentences [11], and the modeling of discourse or topic structure [13]. 1.2. Why use prosody? Various approaches to automatic disfluency detection have been proposed in past work <ref> [8, 1, 7, 4] </ref>. These studies have focused on task-oriented dialog and have used a combination of lexical and prosodic features. Results have shown a heavy reliance on lexical information, although prosodic information was also useful when constrained by the lexical information.
Reference: [8] <author> D. O'Shaughnessy. </author> <title> Correcting complex false starts in spontaneous speech. </title> <booktitle> In Proc. ICASSP, </booktitle> <volume> vol. I, </volume> <pages> pp. 349352, </pages> <address> Adelaide, Australia, </address> <year> 1994. </year>
Reference-contexts: Automatic detection of disfluencies could also benefit higher-level modeling, for example, the automatic segmentation of speech into sentences [11], and the modeling of discourse or topic structure [13]. 1.2. Why use prosody? Various approaches to automatic disfluency detection have been proposed in past work <ref> [8, 1, 7, 4] </ref>. These studies have focused on task-oriented dialog and have used a combination of lexical and prosodic features. Results have shown a heavy reliance on lexical information, although prosodic information was also useful when constrained by the lexical information.
Reference: [9] <author> E. Shriberg and A. Stolcke. </author> <title> Word predictability after hesitations: A corpus-based study. </title> <booktitle> In Proc. ICSLP, </booktitle> <volume> vol. 3, </volume> <pages> pp. 18681871, </pages> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: For example, work on statistical language modeling has shown that perplexity is reduced if disfluencies are removed from the N-gram context [12]. Additional analyses suggest that speakers hesitate before less-predictable words; thus, transition probabilities should be dynamically adjusted in the vicinity of hesitations <ref> [9] </ref>. Automatic detection of disfluencies could also benefit higher-level modeling, for example, the automatic segmentation of speech into sentences [11], and the modeling of discourse or topic structure [13]. 1.2. Why use prosody? Various approaches to automatic disfluency detection have been proposed in past work [8, 1, 7, 4].
Reference: [10] <author> E. E. Shriberg. </author> <title> Preliminaries to a Theory of Speech Disfluencies. </title> <type> Ph.D. thesis, </type> <institution> Department of Psychology, University of California, Berkeley, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: Therefore, results apply only to disfluencies in which no word fragments were involved. (Based on hand-labeled data for a subset of Switchboard <ref> [10] </ref>, we estimate this set to comprise about 80% of all disfluencies). Approximation (1) holds if words and acoustic features are chosen to be largely independent of one another, given D, i.e., P (W jD; X) P (W jD).
Reference: [11] <author> A. Stolcke and E. Shriberg. </author> <title> Automatic linguistic segmentation of conversational speech. </title> <booktitle> In Proc. ICSLP, </booktitle> <volume> vol. 2, </volume> <pages> pp. 10051008, </pages> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: Additional analyses suggest that speakers hesitate before less-predictable words; thus, transition probabilities should be dynamically adjusted in the vicinity of hesitations [9]. Automatic detection of disfluencies could also benefit higher-level modeling, for example, the automatic segmentation of speech into sentences <ref> [11] </ref>, and the modeling of discourse or topic structure [13]. 1.2. Why use prosody? Various approaches to automatic disfluency detection have been proposed in past work [8, 1, 7, 4]. These studies have focused on task-oriented dialog and have used a combination of lexical and prosodic features.
Reference: [12] <author> A. Stolcke and E. Shriberg. </author> <title> Statistical language modeling for speech disfluencies. </title> <booktitle> In Proc. ICASSP, </booktitle> <volume> vol. 1, </volume> <pages> pp. 405408, </pages> <address> Atlanta, </address> <year> 1996. </year>
Reference-contexts: Recent studies suggest that disfluency detection is also relevant at other levels of speech processing. For example, work on statistical language modeling has shown that perplexity is reduced if disfluencies are removed from the N-gram context <ref> [12] </ref>. Additional analyses suggest that speakers hesitate before less-predictable words; thus, transition probabilities should be dynamically adjusted in the vicinity of hesitations [9]. <p> From this we can obtain another posterior probability estimate P (DjW ) = P (D; W )=P (W ). The LM used was a disfluency N-gram model of the type used in <ref> [12] </ref>, and was trained on 1.4 million words of Switchboard transcripts, hand-annotated for disfluencies by LDC [6]. Finally, we want to combine the DT classifier based on acoustic information with the LM classifier based on word information for a combined estimate.
Reference: [13] <author> M. Swerts, A. Wichmann, and R.-J. Beun. </author> <title> Filled pauses as markers of discourse structure. </title> <booktitle> In Proc. ICSLP, </booktitle> <volume> vol. 2, </volume> <pages> pp. 10331036, </pages> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: Additional analyses suggest that speakers hesitate before less-predictable words; thus, transition probabilities should be dynamically adjusted in the vicinity of hesitations [9]. Automatic detection of disfluencies could also benefit higher-level modeling, for example, the automatic segmentation of speech into sentences [11], and the modeling of discourse or topic structure <ref> [13] </ref>. 1.2. Why use prosody? Various approaches to automatic disfluency detection have been proposed in past work [8, 1, 7, 4]. These studies have focused on task-oriented dialog and have used a combination of lexical and prosodic features.
References-found: 13

