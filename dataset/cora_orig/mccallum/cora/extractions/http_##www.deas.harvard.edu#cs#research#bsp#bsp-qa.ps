URL: http://www.deas.harvard.edu/cs/research/bsp/bsp-qa.ps
Refering-URL: http://www.deas.harvard.edu/csecse/research/bsp/bspc.html
Root-URL: 
Email: skill@qucis.queensu.ca  fJonathan.Hill, Bill.McCollg@comlab.ox.ac.uk  
Title: Programming Research Group QUESTIONS AND ANSWERS ABOUT BSP  
Author: D.B. Skillicorn Jonathan M.D. Hill and W.F. McColl 
Address: Canada  Oxford, U.K.  Wolfson Building, Parks Road, Oxford OX1 3QD  
Affiliation: Department of Computing and Information Science Queen's University, Kingston,  Computing Laboratory University of Oxford  Oxford University Computing Laboratory  
Pubnum: PRG-TR-15-96  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> A Beguelin, J Dongarra, A Geist, R Manchek, and V Sunderam. </author> <title> Recent enhancements to PVM. </title> <journal> International Journal of Supercomputing Applications and High Performance Computing, </journal> <volume> 95. </volume>
Reference-contexts: These data show that, for today's parallel computers, it is often better to reduce the number of supersteps, even at the expense of requiring more communication. 18 How does BSPLib compare with other communication systems such as PVM 21 T3D In recent years, the PVM message-passing library <ref> [1, 2, 9] </ref> has been widely implemented and widely used. In that respect, the goal of source code portability in parallel computing has already been achieved by PVM.
Reference: [2] <author> Adam Beguelin, Jack Dongarra, Al Geist, Robert Manchek, and Vaidy Sunderam. </author> <title> A users' guide to PVM parallel virtual machine. </title> <type> Technical Report CS-91-136, </type> <institution> University of Tennessee, </institution> <month> July </month> <year> 1991. </year> <month> 35 </month>
Reference-contexts: These data show that, for today's parallel computers, it is often better to reduce the number of supersteps, even at the expense of requiring more communication. 18 How does BSPLib compare with other communication systems such as PVM 21 T3D In recent years, the PVM message-passing library <ref> [1, 2, 9] </ref> has been widely implemented and widely used. In that respect, the goal of source code portability in parallel computing has already been achieved by PVM.
Reference: [3] <author> G. Bilardi, K.T. Herley, A. Pietracaprina, G. Pucci, and P. Spirakis. </author> <title> BSP vs LogP. </title> <booktitle> In Proceedings of the 8th Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 25-32, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: However, it is natural to ask whether or not the more "flexible" LogP model enables a designer to produce a more efficient algorithm or program for some particular problem, at the expense of a more complex style of programming. Recent results show that this is not the case. In <ref> [3] </ref> it is shown that the BSP and LogP models can efficiently simulate one another, and that there is therefore no loss of performance in using the more-structured BSP programming style. 20 How is BSP related to the PRAM model? The BSP model can be regarded as a generalisation of the
Reference: [4] <author> R.P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21, No.2:201-206, </volume> <month> April </month> <year> 1974. </year>
Reference-contexts: A superstep is shown in Figure 1. We will use p to denote the virtual parallelism of a program, that is the number of processes it uses. If the target parallel computer has fewer processors than the virtual parallelism, an extension of Brent's theorem <ref> [4] </ref> can be used to transform a BSP program into a slimmer version. 4 How does communication work? 3 Most parallel programming systems handle communication, both conceptually and in terms of implementation, at the level of individual actions: memory-to-memory transfers, sends and receives, or active messages.
Reference: [5] <author> P.I. Crumpton and M.B. Giles. </author> <title> Multigrid aircraft computations using the OPlus parallel library. In Parallel Computational Fluid Dynamics: Implementation and Results using Parallel Computers. </title> <booktitle> Proceedings Parallel CFD '95, </booktitle> <pages> pages 339-346, </pages> <address> Pasadena, CA, USA, </address> <month> June </month> <year> 1995. </year> <month> Elsevier/North-Holland. </month>
Reference-contexts: Computational fluid dynamics applications of BSP include: (a) an implementation of a BSP version of the OPlus library for solving 3D multigrid viscous flows, used for computation of flows around aircraft or complex parts of aircraft in a project with Rolls Royce <ref> [5] </ref>, (b) a BSP version of FLOW3D, a computational fluid dynamics code, (c) oil reservoir modelling in the presence of discontinuities and anisotropies in a project with Schlumberger Geoquest Ltd.
Reference: [6] <author> D. E. Culler, R. M. Karp, D. A. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subra-monian, and T. von Eicken. </author> <title> Logp: Towards a realistic model of parallel computation. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: (a) a simple programming discipline (based on supersteps) that makes it easier to determine the correctness of programs, (b) a cost model for performance analysis and prediction which is simpler and compositional, and (c) more efficient implementations on many machines. 19 How is BSP related to the LogP model? LogP <ref> [6] </ref> differs from BSP in three ways: * It uses a form of message passing based on pairwise synchronisation. * It adds an extra parameter representing the overhead involved in sending a message.
Reference: [7] <author> H.G. Dietz, T. Muhammad, J.B. Sponaugle, and T. Mattox. </author> <title> PAPERS: Purdue's adapter for parallel execution and rapid synchronization. </title> <type> Technical Report TR-EE-94-11, </type> <institution> Purdue School of Electrical Engineering, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Certain architecture provide a special hardware mechanism for barrier synchronisation across all of the processors. For example the Cray T3D provides an add-and-broadcast tree, and work at Purdue <ref> [7] </ref> has created generic, fast, and cheap barrier synchronisation hardware for a wide range of architectures. Sharing this single synchronisation resource among several concurrent subsets that may wish to use it at any time seems difficult. We are currently exploring this issue.
Reference: [8] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sunderam. </author> <title> PVM 3 Users Guide and Reference Manual. </title> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee 37831, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: All existing BSP languages are imperative, but there is no intrinsic reason why this need be so. BSP can be expressed in a wide variety of programming languages and systems. For example, BSP programs could be written using existing communication libraries such as PVM <ref> [8] </ref>, MPI [24], or Cray's SHMEM. All that is required is that they provide non-blocking communication mechanisms and a way to implement barrier synchronisation.
Reference: [9] <author> G. A. Geist. PVM3: </author> <title> Beyond network computing. </title> <editor> In J. Volkert, editor, </editor> <booktitle> Parallel Computation, Lecture Notes in Computer Science 734, </booktitle> <pages> pages 194-203. </pages> <publisher> Springer, </publisher> <year> 1993. </year>
Reference-contexts: These data show that, for today's parallel computers, it is often better to reduce the number of supersteps, even at the expense of requiring more communication. 18 How does BSPLib compare with other communication systems such as PVM 21 T3D In recent years, the PVM message-passing library <ref> [1, 2, 9] </ref> has been widely implemented and widely used. In that respect, the goal of source code portability in parallel computing has already been achieved by PVM.
Reference: [10] <author> M. Goudreau, K. Lang, S. Rao, T. Suel, and T. Tsantilas. </author> <title> Towards efficiency and portability: Programming the BS model. </title> <booktitle> In Proceedings of the 8th Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 1-12, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: The most common approach to BSP programming is SPMD imperative programming using Fortran or C, with BSP functionality provided by library calls. Two BSP libraries have been in use for some years: the Oxford BSP Library [25] and the Green BSP Library <ref> [10, 11] </ref>. A standard has recently been agreed for a library called BSPLib [12]. The BSPLib contains operations for delimiting supersteps, and two variants of communication, one based on direct-memory transfer, and the other on buffered message passing. Other BSP languages have been developed.
Reference: [11] <author> Mark W. Goudreau, Kevin Lang, Satish B. Rao, and Thanasis Tsantilas. </author> <title> The Green BSP Library. </title> <type> Technical Report 95-11, </type> <institution> University of Central Florida, </institution> <month> August, </month> <year> 1995. </year>
Reference-contexts: The most common approach to BSP programming is SPMD imperative programming using Fortran or C, with BSP functionality provided by library calls. Two BSP libraries have been in use for some years: the Oxford BSP Library [25] and the Green BSP Library <ref> [10, 11] </ref>. A standard has recently been agreed for a library called BSPLib [12]. The BSPLib contains operations for delimiting supersteps, and two variants of communication, one based on direct-memory transfer, and the other on buffered message passing. Other BSP languages have been developed.
Reference: [12] <author> M.W. Goudreau, J.M.D. Hill, K. Lang, W.F. McColl, S.D. Rao, D.C. Stefanescu, T. Suel, and T. Tsantilas. </author> <title> A proposal for a BSP Worldwide standard. </title> <note> BSP Worldwide, http:// www.bsp-worldwide.org/, April 1996. </note>
Reference-contexts: Two BSP libraries have been in use for some years: the Oxford BSP Library [25] and the Green BSP Library [10, 11]. A standard has recently been agreed for a library called BSPLib <ref> [12] </ref>. The BSPLib contains operations for delimiting supersteps, and two variants of communication, one based on direct-memory transfer, and the other on buffered message passing. Other BSP languages have been developed. These include GPL [21], and Opal [18].
Reference: [13] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Programming. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1994. </year>
Reference-contexts: PVM and all other message-passing systems based on pairwise, rather than barrier, synchronisation also suffer from having no simple analytic cost model for performance prediction, and no simple means of examining the global state of a computation for debugging. MPI <ref> [13] </ref> has been proposed as a new standard for those who want to write portable message-passing programs in Fortran and C. At the level of point-to-point communications (send, receive etc.), MPI is similar to PVM, and the same comparisons apply.
Reference: [14] <author> J. He, Q. Miller, and L. Chen. </author> <title> Algebraic laws for BSP programming. </title> <note> In Proceedings of Europar '96. Springer-Verlag Lecture Notes in Computer Science, to appear 1996. </note>
Reference-contexts: Modelling the latency of deep storage hierarchies fits naturally into BSP's approach to the latency of communication, and investigations of extensions to the BSP cost model applicable to databases are underway. 25 Does BSP have a formal semantics? 25 Several formal semantics for BSP have been developed. The paper <ref> [14] </ref> shows how these may be used to give algebraic laws for developing BSP programs.
Reference: [15] <author> Jonathan M. D. Hill, Paul I. Crumpton, and David A. Burgess. </author> <title> Theory, practice, and a tool for BSP performance prediction. </title> <booktitle> In EuroPar'96, Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1996. </year>
Reference-contexts: The BSP model goes some way towards alleviated this problem if cost analysis is used to guide program development. Unfortunately, in large-scale problems, cost analysis is rarely used at the time of program development. The role of current BSP tools <ref> [15] </ref> is to aid programmers in understanding the intensional properties of their programs by 19 graphically providing profiling and cost information. <p> This is done by routing a fixed-sized h-relation (a over-sampling of 10 iterations is performed for each h-relation) using first a single message of size h; then two messages of size h=2; through to h=4 messages of size 4 words. Figures 9, 10, and 11 show communication profiles <ref> [15] </ref> for the benchmark program running on the Cray T3D and IBM SP2. Each figure contains two graphs. The upper graph contains a breakdown of the communication patterns that arise in each superstep of the benchmark.
Reference: [16] <author> C.A.R. Hoare and J. </author> <title> He. Unified Theories of Programming. </title> <note> Prentice-Hall International, to appear 1996. 36 </note>
Reference-contexts: The paper [14] shows how these may be used to give algebraic laws for developing BSP programs. BSP is used as a semantics case study in a forthcoming book <ref> [16] </ref>. 26 Will BSP influence the design of architectures for the next generation of parallel computers? The contribution of BSP to architecture design is that it clarifies those factors that are most important for performance on problems without locality.
Reference: [17] <author> R.W. Hockney. </author> <title> Performance parameters and benchmarking of supercomputers. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1111-1130, </pages> <year> 1991. </year>
Reference-contexts: However, a superstep in which very little total communication occurs may still deviate from the cost model because of the effects of startup costs for message transmission. Miller refined the cost model [26] using a technique of Hockney <ref> [17] </ref> to model the effect of message granularity on communication cost.
Reference: [18] <author> Simon Knee. </author> <title> Program development and performance prediction on BSP machines using Opal. </title> <type> Technical Report PRG-TR-18-94, </type> <institution> Oxford University Computing Laboratory, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: A standard has recently been agreed for a library called BSPLib [12]. The BSPLib contains operations for delimiting supersteps, and two variants of communication, one based on direct-memory transfer, and the other on buffered message passing. Other BSP languages have been developed. These include GPL [21], and Opal <ref> [18] </ref>. GPL is a first attempt to develop an MIMD language permitting synchronisation of subsets of executing processes. Opal is an object-based BSP language. 8 11 How easy is it to program using the BSPLib library? The BSPLib library provides the operations shown in Table 1.
Reference: [19] <author> C.H. Koelbel, D.B. Loveman, R.S. Schreiber, G.L. Steele Jr., and M.E. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1994. </year>
Reference-contexts: A number of interesting programming languages and elegant theories have been developed in support of the data-parallel style of programming, see e.g. [30]. High Performance Fortran <ref> [19] </ref> is a good example of a practical data-parallel language. Data parallelism is particularly appropriate for problems in which locality is crucial. The BSP approach in principle offers a more flexible and general style of programming than is provided by data parallelism.
Reference: [20] <author> W. F. McColl. </author> <title> Scalable computing. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Computer Science Today: Recent Trends and Developments, volume 1000 of Lecture Notes in Computer Science, </booktitle> <pages> pages 46-61. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: This is optimal in time and memory requirement. A more sophisticated algorithm due to McColl and Valiant <ref> [20] </ref> has BSP complexity Block and Broadcast MM cost = n 3 =p + (n 2 =p 2=3 )g + l requiring memory at each processor of size n 2 =p 2=3 . This is optimal in time, communication, and supersteps, but requires more memory at each processor. <p> Information about it can be found at the web site http://www.bsp-worldwide.org/. A standard for the BSPLib has been agreed. BSP Worldwide organises semiannual workshops on BSP. Other general papers about BSP are <ref> [20, 31] </ref>. There are groups of BSP researchers at: * Oxford | http://www.comlab.ox.ac.uk/oucl/groups/bsp * Harvard | http://das-www.harvard.edu/cs/research/bsp.html 26 * Utrecht | http://www.math.ruu.nl/people/bisseling.html * Carleton | http://www.scs.carleton.ca/~palepu/BSP.html * Central Florida | http://longwood.cs.ucf.edu/csdept/faculty/goudreau.html as well as individuals working on BSP at a number of other universities.
Reference: [21] <author> W. F. McColl and Q. Miller. </author> <title> The GPL language: Reference manual. </title> <type> Technical report, </type> <institution> ESPRIT GEPPCOM Project, Oxford university Computing Laboratory, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: A standard has recently been agreed for a library called BSPLib [12]. The BSPLib contains operations for delimiting supersteps, and two variants of communication, one based on direct-memory transfer, and the other on buffered message passing. Other BSP languages have been developed. These include GPL <ref> [21] </ref>, and Opal [18]. GPL is a first attempt to develop an MIMD language permitting synchronisation of subsets of executing processes. Opal is an object-based BSP language. 8 11 How easy is it to program using the BSPLib library? The BSPLib library provides the operations shown in Table 1.
Reference: [22] <author> W.F. McColl. </author> <title> General purpose parallel computing. </title> <editor> In A.M. Gibbons and P. Spirakis, editors, </editor> <booktitle> Lectures on Parallel Computation, Cambridge International Series on Parallel Computation, </booktitle> <pages> pages 337-391. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference-contexts: move from the straightforward world of parallel algorithms to the much more complex world of parallel software systems. 2 What is Bulk Synchronous Parallelism? Bulk Synchronous Parallelism is a style of parallel programming developed for general-purpose parallelism, that is parallelism across all application areas and a wide range of architectures <ref> [22] </ref>. Its goals are more ambitious than most parallel-programming systems which are aimed at particular kinds of applications, or work well only on particular classes of parallel architectures [23]. BSP's most fundamental properties are that: * It is simple to write. BSP programs are much the same as sequential programs.
Reference: [23] <author> W.F. McColl. </author> <title> Special purpose parallel computing. </title> <editor> In A.M. Gibbons and P. Spirakis, editors, </editor> <booktitle> Lectures on Parallel Computation, Cambridge International Series on Parallel Computation, </booktitle> <pages> pages 261-336. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference-contexts: Its goals are more ambitious than most parallel-programming systems which are aimed at particular kinds of applications, or work well only on particular classes of parallel architectures <ref> [23] </ref>. BSP's most fundamental properties are that: * It is simple to write. BSP programs are much the same as sequential programs. Only a bare minimum of extra information needs to be supplied to describe the use of paral lelism. * It is independent of target architectures. <p> For example two BSP algorithms for matrix multiplication have been developed. The first, a block parallelization of the standard n 3 algorithm <ref> [23] </ref>, has BSP complexity Block MM cost = n 3 =p + (n 2 =p 1=2 )g + p 1=2 l requiring memory at each processor of size n 2 =p. This is optimal in time and memory requirement.
Reference: [24] <author> Message Passing Interface Forum. </author> <title> MPI: A message passing interface. </title> <booktitle> In Proc. Supercomputing '93, </booktitle> <pages> pages 878-883. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year>
Reference-contexts: All existing BSP languages are imperative, but there is no intrinsic reason why this need be so. BSP can be expressed in a wide variety of programming languages and systems. For example, BSP programs could be written using existing communication libraries such as PVM [8], MPI <ref> [24] </ref>, or Cray's SHMEM. All that is required is that they provide non-blocking communication mechanisms and a way to implement barrier synchronisation.
Reference: [25] <author> Richard Miller. </author> <title> A library for Bulk Synchronous Parallel programming. </title> <booktitle> In Proceedings of the BCS Parallel Processing Specialist Group workshop on General Purpose Parallel Computing, </booktitle> <pages> pages 100-108, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: The most common approach to BSP programming is SPMD imperative programming using Fortran or C, with BSP functionality provided by library calls. Two BSP libraries have been in use for some years: the Oxford BSP Library <ref> [25] </ref> and the Green BSP Library [10, 11]. A standard has recently been agreed for a library called BSPLib [12]. The BSPLib contains operations for delimiting supersteps, and two variants of communication, one based on direct-memory transfer, and the other on buffered message passing. Other BSP languages have been developed.
Reference: [26] <author> Richard Miller. </author> <title> Two approaches to architecture-independent parallel computation. </title> <type> D.Phil thesis, </type> <institution> Oxford University Computing Laboratory, </institution> <address> Wolfson Building, Parks Road, Oxford OX1 3QD, </address> <year> 1994. </year>
Reference-contexts: However, a superstep in which very little total communication occurs may still deviate from the cost model because of the effects of startup costs for message transmission. Miller refined the cost model <ref> [26] </ref> using a technique of Hockney [17] to model the effect of message granularity on communication cost.
Reference: [27] <author> P.B. Monk, A.K. Parrott, and P.J. Wesson. </author> <title> A parallel finite element method for electromagnetic scattering. </title> <journal> COMPEL, </journal> <volume> 13, Supp.A:237-242, </volume> <year> 1994. </year>
Reference-contexts: Computational electromagnetics applications of BSP <ref> [27] </ref> include: (a) 3D modelling of electromagnetic interactions with complex bodies using unstructured 3D meshes, in a project with British Aerospace, (b) parallelisation of the TOSCA, SCALA, and ELEKTRA codes, and demonstrations on problems such as design of electric motors and permanent magnets for MRI imaging, (c) a parallel implementation of
Reference: [28] <author> M. Nibhanupudi, C. Norton, and B. Szymanski. </author> <title> Plasma simulation on networks of workstations using the bulk synchronous parallel model. </title> <booktitle> In Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications, </booktitle> <address> Athens, GA, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: There is also work involving parallelising the MERLIN code in a project with Lloyds Register of Shipping and Ford Motor Company. BSP has also been applied to plasma simulation at Rensselaer Polytechnic Institute in New York <ref> [28] </ref>. 13 What do BSP programs look like? Most BSP programs for real problems are large and it is impractical to include their source here. Instead we include some small example programs to show how the BSPLib interface can be used. <p> These include broadcast, scatter, gather, total exchange, reduction, and scan. These standard communication patterns are also provided for BSP in a 22 higher-level library. There has been one attempt to compare BSP performance with MPI <ref> [28] </ref> on a network of workstations. The results show that performance differences are very small, of the order of a few percent.
Reference: [29] <author> J Reed, K Parrott, and T Lanfear. </author> <title> Portability, predictability and performance for parallel computing: </title> <journal> BSP in practice. Concurrency: Practice and Experience, </journal> <note> to appear. </note>
Reference-contexts: ELEKTRA codes, and demonstrations on problems such as design of electric motors and permanent magnets for MRI imaging, (c) a parallel implementation of a time domain electromagnetic code ParEMC3d with absorbing boundary conditions, (d) parallelisation of the EMMA-T2 code for calculating electromagnetic properties of microstrips, wires and cables, and antennae <ref> [29] </ref>. There is also work involving parallelising the MERLIN code in a project with Lloyds Register of Shipping and Ford Motor Company.
Reference: [30] <editor> D.B. Skillicorn. </editor> <booktitle> Foundations of Parallel Programming. Cambridge Series in Parallel Computation. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1994. </year>
Reference-contexts: A number of interesting programming languages and elegant theories have been developed in support of the data-parallel style of programming, see e.g. <ref> [30] </ref>. High Performance Fortran [19] is a good example of a practical data-parallel language. Data parallelism is particularly appropriate for problems in which locality is crucial. The BSP approach in principle offers a more flexible and general style of programming than is provided by data parallelism.
Reference: [31] <author> Leslie G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year> <month> 37 </month>
Reference-contexts: Worse still, the technology required to provide the abstraction is the least likely to be of a commodity nature, and hence even more expensive. The Bulk Synchronous Parallel (BSP) model <ref> [31] </ref> provides software developers with an attractive escape route from the world of architecture-dependent parallel software. The emergence of the model has also coincided with the convergence of commercial parallel machine designs to a standard architectural form with which it is very compatible. <p> message-combining scheme. standard model to be within y% accuracy of the cost attributed by the model that includes message granularity, then: 100 h 0 g 1 = h 0 g (h 0 ) = n 1=2 + 1 h 0 g 1 (2) where h 0 words is Valiant's parameter <ref> [31] </ref> that measures the minimum size of h-relation to achieve n 1=2 throughput. <p> Information about it can be found at the web site http://www.bsp-worldwide.org/. A standard for the BSPLib has been agreed. BSP Worldwide organises semiannual workshops on BSP. Other general papers about BSP are <ref> [20, 31] </ref>. There are groups of BSP researchers at: * Oxford | http://www.comlab.ox.ac.uk/oucl/groups/bsp * Harvard | http://das-www.harvard.edu/cs/research/bsp.html 26 * Utrecht | http://www.math.ruu.nl/people/bisseling.html * Carleton | http://www.scs.carleton.ca/~palepu/BSP.html * Central Florida | http://longwood.cs.ucf.edu/csdept/faculty/goudreau.html as well as individuals working on BSP at a number of other universities.
References-found: 31

