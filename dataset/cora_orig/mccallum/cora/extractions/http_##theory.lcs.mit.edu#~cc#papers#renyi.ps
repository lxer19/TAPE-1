URL: http://theory.lcs.mit.edu/~cc/papers/renyi.ps
Refering-URL: http://theory.lcs.mit.edu/~cc/publications.html
Root-URL: 
Email: cachin@acm.org  
Title: Smooth Entropy and Renyi Entropy  
Author: Christian Cachin 
Note: In Proceedings of EUROCRYPT '97 (W. Fumy, ed.), Lecture Notes in Computer Science, vol. 1233, pp. 193-208, Springer,  
Date: May 20, 1997  1997.  
Address: CH-8092 Zurich, Switzerland  
Affiliation: Department of Computer Science ETH Zurich  
Abstract: The notion of smooth entropy allows a unifying, generalized formulation of privacy amplification and entropy smoothing. Smooth entropy is a measure for the number of almost uniform random bits that can be extracted from a random source by probabilistic algorithms. It is known that the Renyi entropy of order at least 2 of a random variable is a lower bound for its smooth entropy. On the other hand, an assumption about Shannon entropy (which is Renyi entropy of order 1) is too weak to guarantee any non-trivial amount of smooth entropy. In this work we close the gap between Renyi entropy of order 1 and 2. In particular, we show that Renyi entropy of order ff for any 1 &lt; ff &lt; 2 is a lower bound for smooth entropy, up to a small parameter depending on ff, the alphabet size and the failure probability. The results have applications in cryptography for unconditionally secure protocols such as quantum key agreement, key agreement from correlated information, oblivious transfer, and bit commitment.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. H. Bennett, F. Bessette, G. Brassard, L. Salvail, and J. Smolin, </author> <title> "Experimental quantum cryptography," </title> <journal> Journal of Cryptology, </journal> <volume> vol. 5, no. 1, </volume> <pages> pp. 3-28, </pages> <year> 1992. </year>
Reference-contexts: Introduced in 1985 [3, 4] and later generalized [2], it has become a key component of unconditionally secure cryptographic protocols with such various purposes as key agreement from correlated fl Supported by the Swiss National Science Foundation, grant no. 20-42105.94. 1 information [16], key agreement over quantum channels <ref> [1, 5] </ref>, oblivious transfer [6], and bit commitment [10]. Privacy amplification, for short, is a process that allows two parties to distill a secret key from common information about which an adversary has partial knowledge.
Reference: [2] <author> C. H. Bennett, G. Brassard, C. Crepeau, and U. M. Maurer, </author> <title> "Generalized privacy amplification," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 41, </volume> <pages> pp. 1915-1923, </pages> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: However, entropy smoothing does not consider the auxiliary random bits as a resource, unlike extractors used in theoretical computer science [17]. In cryptography, entropy smoothing is known as privacy amplification. Introduced in 1985 [3, 4] and later generalized <ref> [2] </ref>, it has become a key component of unconditionally secure cryptographic protocols with such various purposes as key agreement from correlated fl Supported by the Swiss National Science Foundation, grant no. 20-42105.94. 1 information [16], key agreement over quantum channels [1, 5], oblivious transfer [6], and bit commitment [10]. <p> Examples are pseudorandom generation [11, 14], derandom-ization of algorithms [15], hardness results in computational learning theory [13], and computing with degenerate, weak random sources [20]. A survey of these applications is given by Nisan [17]. Bennett et al. <ref> [4, 2] </ref> and Impagliazzo et al. [12] independently analyzed entropy smoothing by universal hash functions [8] and showed that the length of the almost uniform output depends on the Renyi entropy of order 2 of the input. <p> On the other hand, it is known that a lower bound in terms of Renyi entropy of order 1 (which is equivalent to entropy in the sense of Shannon) is not sufficient to extract a non-trivial amount of uniform bits <ref> [2] </ref>. In this work, we close this gap and prove a lower bound on smooth entropy in terms of Renyi entropy of order ff for any ff between 1 and 2. <p> In particular, log jX j H ff (X) 0 for ff 0 and H (X) H ff (X) for ff &gt; 1. 3 Review of Smooth Entropy and Privacy Amplification Smooth entropy [7] is an abstraction and a generalized formulation of privacy amplification <ref> [2] </ref> and entropy smoothing [12, 14]. As an information measure, smooth entropy is defined operationally with respect to an application scenario (similar to channel capacity [9]). Its value cannot be computed immediately for a given probability distribution. <p> G of functions X ! Y such that for all distinct x 1 ; x 2 2 X , there are at most jGj=jYj functions g in G such that g (x 1 ) = g (x 2 ). 4 Privacy amplification is fundamental for many unconditionally secure cryptographic proto-cols <ref> [2] </ref>. Assume Alice and Bob share a random variable W , while an eavesdropper Eve knows a correlated random variable V that summarizes her knowledge about W . <p> Using an authentic public channel, which is susceptible to eavesdropping but immune to tampering, Alice and Bob wish to agree on a function g such that Eve knows nearly nothing about g (W ). The following theorem by Bennett et al. <ref> [2] </ref> shows that if Alice and Bob choose g at random from a universal hash function G : W ! Y for suitable Y, then Eve's information about Y = g (W ) is negligible. Theorem 1 (Privacy Amplification Theorem [2]). <p> The following theorem by Bennett et al. <ref> [2] </ref> shows that if Alice and Bob choose g at random from a universal hash function G : W ! Y for suitable Y, then Eve's information about Y = g (W ) is negligible. Theorem 1 (Privacy Amplification Theorem [2]). Let X be a random variable over the alphabet X with Renyi entropy H 2 (X), let G be the random variable corresponding to the random choice (with uniform distribution) of a member of a universal hash function G : X ! Y, and let Y = G (X). <p> Note that Shannon entropy cannot be used as a lower bound for smooth entropy. This was observed by Bennett et al. <ref> [2] </ref> and is illustrated in the following example. Example 1. Suppose that everything we know about a random variable X is H (X) t. <p> Suppose side information that increases the Renyi entropy is made available by an imaginary oracle. This increase can be exploited to prove lower bounds on smooth entropy that are much tighter than Renyi entropy of order 2. Side information of this kind was introduced by Bennett et al. <ref> [2] </ref> and is called spoiling knowledge because it leads to less information about the output of the smoothing process. 5 We examine side information that induces an event A such that P [A] is at least 1 * and H 2 (XjA) is large. <p> 2 log jX j jX j 1 + c = 2 log c + log jX j 1 + c &lt; 2 log c 5 A Bound Using Renyi Entropy of Order ff &gt; 1 The connection between entropy smoothing and Renyi entropy was established independently by Bennett et al. <ref> [2] </ref> and Impagliazzo et al. [12]. The Privacy Amplification Theorem shows that Renyi entropy of order 2 is a lower bound for smooth entropy. <p> The graph shows that, together with Theorem 8, Renyi entropy of order ff close to 1 can yield much better bounds on smooth entropy than Renyi entropy of order 2. for fi t n. (With fi = 2 this is the example from <ref> [2] </ref>.) The lower bound on (X) by Renyi entropy of order 2 is weak because H 2 (X) &lt; n=fi. However, H (X fi ) is very close to n bits. to H (X fi ) n.
Reference: [3] <author> C. H. Bennett, G. Brassard, and J.-M. Robert, </author> <title> "How to reduce your enemy's information," </title> <booktitle> in Advances in Cryptology | CRYPTO '85 (H. </booktitle> <editor> C. Williams, ed.), </editor> <volume> vol. </volume> <booktitle> 218 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 468-476, </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: However, entropy smoothing does not consider the auxiliary random bits as a resource, unlike extractors used in theoretical computer science [17]. In cryptography, entropy smoothing is known as privacy amplification. Introduced in 1985 <ref> [3, 4] </ref> and later generalized [2], it has become a key component of unconditionally secure cryptographic protocols with such various purposes as key agreement from correlated fl Supported by the Swiss National Science Foundation, grant no. 20-42105.94. 1 information [16], key agreement over quantum channels [1, 5], oblivious transfer [6], and
Reference: [4] <author> C. H. Bennett, G. Brassard, and J.-M. Robert, </author> <title> "Privacy amplification by public discussion," </title> <journal> SIAM Journal on Computing, </journal> <volume> vol. 17, </volume> <pages> pp. 210-229, </pages> <month> Apr. </month> <year> 1988. </year>
Reference-contexts: However, entropy smoothing does not consider the auxiliary random bits as a resource, unlike extractors used in theoretical computer science [17]. In cryptography, entropy smoothing is known as privacy amplification. Introduced in 1985 <ref> [3, 4] </ref> and later generalized [2], it has become a key component of unconditionally secure cryptographic protocols with such various purposes as key agreement from correlated fl Supported by the Swiss National Science Foundation, grant no. 20-42105.94. 1 information [16], key agreement over quantum channels [1, 5], oblivious transfer [6], and <p> Examples are pseudorandom generation [11, 14], derandom-ization of algorithms [15], hardness results in computational learning theory [13], and computing with degenerate, weak random sources [20]. A survey of these applications is given by Nisan [17]. Bennett et al. <ref> [4, 2] </ref> and Impagliazzo et al. [12] independently analyzed entropy smoothing by universal hash functions [8] and showed that the length of the almost uniform output depends on the Renyi entropy of order 2 of the input.
Reference: [5] <author> G. Brassard and C. Crepeau, </author> <title> "25 years of quantum cryptography," </title> <journal> SIGACT News, </journal> <volume> vol. 27, no. 3, </volume> <pages> pp. 13-24, </pages> <year> 1996. </year>
Reference-contexts: Introduced in 1985 [3, 4] and later generalized [2], it has become a key component of unconditionally secure cryptographic protocols with such various purposes as key agreement from correlated fl Supported by the Swiss National Science Foundation, grant no. 20-42105.94. 1 information [16], key agreement over quantum channels <ref> [1, 5] </ref>, oblivious transfer [6], and bit commitment [10]. Privacy amplification, for short, is a process that allows two parties to distill a secret key from common information about which an adversary has partial knowledge.
Reference: [6] <author> G. Brassard and C. Crepeau, </author> <title> "Oblivious transfers and privacy amplification." </title> <booktitle> Proceedings of EUROCRYPT '97, </booktitle> <year> 1997. </year>
Reference-contexts: 1985 [3, 4] and later generalized [2], it has become a key component of unconditionally secure cryptographic protocols with such various purposes as key agreement from correlated fl Supported by the Swiss National Science Foundation, grant no. 20-42105.94. 1 information [16], key agreement over quantum channels [1, 5], oblivious transfer <ref> [6] </ref>, and bit commitment [10]. Privacy amplification, for short, is a process that allows two parties to distill a secret key from common information about which an adversary has partial knowledge. The two parties do not know anything about the adversary's knowledge except that it satisfies a general bound.
Reference: [7] <author> C. Cachin and U. Maurer, </author> <title> "Smoothing probability distributions and smooth entropy." </title> <note> Preprint (abstract to appear in Proceedings of International Symposium on Information Theory, ISIT 97), </note> <year> 1997. </year>
Reference-contexts: 1 Introduction Entropy smoothing is the process of converting an arbitrary random source into a source with smaller alphabet and almost uniform distribution. Smooth entropy is an information measure that has been proposed recently <ref> [7] </ref> to quantify the number of almost uniform bits that can be extracted by a probabilistic algorithm from any member of a set of random variables. <p> In particular, log jX j H ff (X) 0 for ff 0 and H (X) H ff (X) for ff &gt; 1. 3 Review of Smooth Entropy and Privacy Amplification Smooth entropy <ref> [7] </ref> is an abstraction and a generalized formulation of privacy amplification [2] and entropy smoothing [12, 14]. As an information measure, smooth entropy is defined operationally with respect to an application scenario (similar to channel capacity [9]). Its value cannot be computed immediately for a given probability distribution.
Reference: [8] <author> J. L. Carter and M. N. Wegman, </author> <title> "Universal classes of hash functions," </title> <journal> Journal of Computer and System Sciences, </journal> <volume> vol. 18, </volume> <pages> pp. 143-154, </pages> <year> 1979. </year>
Reference-contexts: A survey of these applications is given by Nisan [17]. Bennett et al. [4, 2] and Impagliazzo et al. [12] independently analyzed entropy smoothing by universal hash functions <ref> [8] </ref> and showed that the length of the almost uniform output depends on the Renyi entropy of order 2 of the input. Privacy amplification can therefore be applied if the two parties assume a lower bound on the Renyi entropy of order 2 of the adversary's knowledge about their information. <p> The failure probability * can be integrated into the uniformity parameter (s) for certain nonuniformity measures such as L 1 distance. The principal method for extracting smooth entropy is based on universal hashing. A universal hash function <ref> [8] </ref> is a set G of functions X ! Y such that for all distinct x 1 ; x 2 2 X , there are at most jGj=jYj functions g in G such that g (x 1 ) = g (x 2 ). 4 Privacy amplification is fundamental for many unconditionally
Reference: [9] <author> T. M. Cover and J. A. Thomas, </author> <title> Elements of Information Theory. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: The main result is proved in Section 5, and Section 6 contains the derivation of the tighter bound in terms of the profile. 2 Preliminaries We assume that the reader is familiar with the notion of entropy and the basic concepts of information theory <ref> [9] </ref>. We repeat some fundamental definitions in this section and introduce the notation. All logarithms in this paper are to the base 2. The cardinality of a set S is denoted by jSj. A random variable X induces a probability distribution P X over an alphabet X . <p> As an information measure, smooth entropy is defined operationally with respect to an application scenario (similar to channel capacity <ref> [9] </ref>). Its value cannot be computed immediately for a given probability distribution. This contrasts with other entropy measures such as Shannon or Renyi entropy that are defined formally in terms of a probability distribution. Consider a random variable X.
Reference: [10] <author> C. Crepeau, </author> <title> "Efficient cryptographic protocols based on noisy channels." </title> <booktitle> Proceedings of EUROCRYPT '97, </booktitle> <year> 1997. </year>
Reference-contexts: later generalized [2], it has become a key component of unconditionally secure cryptographic protocols with such various purposes as key agreement from correlated fl Supported by the Swiss National Science Foundation, grant no. 20-42105.94. 1 information [16], key agreement over quantum channels [1, 5], oblivious transfer [6], and bit commitment <ref> [10] </ref>. Privacy amplification, for short, is a process that allows two parties to distill a secret key from common information about which an adversary has partial knowledge. The two parties do not know anything about the adversary's knowledge except that it satisfies a general bound.
Reference: [11] <author> J. H-astad, R. Impagliazzo, L. A. Levin, and M. Luby, </author> <title> "Construction of a pseudo-random generator from any one-way function," </title> <type> Tech. Rep. 91-068, </type> <institution> International Computer Science Institute (ICSI), Berkeley, </institution> <year> 1991. </year> <month> 13 </month>
Reference-contexts: Apart from the applications in cryptography, entropy smoothing is also at the core of many constructions in complexity theory. Examples are pseudorandom generation <ref> [11, 14] </ref>, derandom-ization of algorithms [15], hardness results in computational learning theory [13], and computing with degenerate, weak random sources [20]. A survey of these applications is given by Nisan [17].
Reference: [12] <author> R. Impagliazzo, L. A. Levin, and M. Luby, </author> <title> "Pseudo-random generation from one-way func-tions," </title> <booktitle> in Proc. 21st Annual ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pp. 12-24, </pages> <year> 1989. </year>
Reference-contexts: Examples are pseudorandom generation [11, 14], derandom-ization of algorithms [15], hardness results in computational learning theory [13], and computing with degenerate, weak random sources [20]. A survey of these applications is given by Nisan [17]. Bennett et al. [4, 2] and Impagliazzo et al. <ref> [12] </ref> independently analyzed entropy smoothing by universal hash functions [8] and showed that the length of the almost uniform output depends on the Renyi entropy of order 2 of the input. <p> In particular, log jX j H ff (X) 0 for ff 0 and H (X) H ff (X) for ff &gt; 1. 3 Review of Smooth Entropy and Privacy Amplification Smooth entropy [7] is an abstraction and a generalized formulation of privacy amplification [2] and entropy smoothing <ref> [12, 14] </ref>. As an information measure, smooth entropy is defined operationally with respect to an application scenario (similar to channel capacity [9]). Its value cannot be computed immediately for a given probability distribution. <p> j 1 + c = 2 log c + log jX j 1 + c &lt; 2 log c 5 A Bound Using Renyi Entropy of Order ff &gt; 1 The connection between entropy smoothing and Renyi entropy was established independently by Bennett et al. [2] and Impagliazzo et al. <ref> [12] </ref>. The Privacy Amplification Theorem shows that Renyi entropy of order 2 is a lower bound for smooth entropy.
Reference: [13] <author> M. Kharitonov, </author> <title> "Cryptographic hardness of distribution-specific learning," </title> <booktitle> in Proc. 25th Annual ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pp. 372-381, </pages> <year> 1993. </year>
Reference-contexts: Apart from the applications in cryptography, entropy smoothing is also at the core of many constructions in complexity theory. Examples are pseudorandom generation [11, 14], derandom-ization of algorithms [15], hardness results in computational learning theory <ref> [13] </ref>, and computing with degenerate, weak random sources [20]. A survey of these applications is given by Nisan [17].
Reference: [14] <author> M. Luby, </author> <title> Pseudorandomness and Cryptographic Applications. </title> <publisher> Princeton University Press, </publisher> <year> 1996. </year>
Reference-contexts: Apart from the applications in cryptography, entropy smoothing is also at the core of many constructions in complexity theory. Examples are pseudorandom generation <ref> [11, 14] </ref>, derandom-ization of algorithms [15], hardness results in computational learning theory [13], and computing with degenerate, weak random sources [20]. A survey of these applications is given by Nisan [17]. <p> The k-th moment inequality for any real-valued random variable X, any integer k &gt; 0, and t 2 R + is P [jXj t] t k : (1) Another useful bound for any real-valued random variable X, any t 2 R + , and any r 2 R is <ref> [14] </ref> P [X r] E [e (Xr)t ]: (2) The (Shannon) entropy of a random variable X with probability distribution P X and al phabet X is defined as H (X) = x2X The conditional entropy of X conditioned on a random variable Y is H (XjY ) = y2Y where <p> In particular, log jX j H ff (X) 0 for ff 0 and H (X) H ff (X) for ff &gt; 1. 3 Review of Smooth Entropy and Privacy Amplification Smooth entropy [7] is an abstraction and a generalized formulation of privacy amplification [2] and entropy smoothing <ref> [12, 14] </ref>. As an information measure, smooth entropy is defined operationally with respect to an application scenario (similar to channel capacity [9]). Its value cannot be computed immediately for a given probability distribution.
Reference: [15] <author> M. Luby and A. Wigderson, </author> <title> "Pairwise independence and derandomization," </title> <type> Tech. Rep. 95-035, </type> <institution> International Computer Science Institute (ICSI), Berkeley, </institution> <year> 1995. </year>
Reference-contexts: Apart from the applications in cryptography, entropy smoothing is also at the core of many constructions in complexity theory. Examples are pseudorandom generation [11, 14], derandom-ization of algorithms <ref> [15] </ref>, hardness results in computational learning theory [13], and computing with degenerate, weak random sources [20]. A survey of these applications is given by Nisan [17].
Reference: [16] <author> U. M. Maurer, </author> <title> "Secret key agreement by public discussion from common information," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 39, </volume> <pages> pp. 733-742, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Introduced in 1985 [3, 4] and later generalized [2], it has become a key component of unconditionally secure cryptographic protocols with such various purposes as key agreement from correlated fl Supported by the Swiss National Science Foundation, grant no. 20-42105.94. 1 information <ref> [16] </ref>, key agreement over quantum channels [1, 5], oblivious transfer [6], and bit commitment [10]. Privacy amplification, for short, is a process that allows two parties to distill a secret key from common information about which an adversary has partial knowledge.
Reference: [17] <author> N. Nisan, </author> <title> "Extracting randomness: How and why | a survey," </title> <booktitle> in Proc. 11th Annual IEEE Conference on Computational Complexity, </booktitle> <year> 1996. </year>
Reference-contexts: The inclusion of randomized extraction functions is the main difference between entropy smoothing and "pure" random number generation in information theory [19], where no additional random sources are available. However, entropy smoothing does not consider the auxiliary random bits as a resource, unlike extractors used in theoretical computer science <ref> [17] </ref>. In cryptography, entropy smoothing is known as privacy amplification. <p> Examples are pseudorandom generation [11, 14], derandom-ization of algorithms [15], hardness results in computational learning theory [13], and computing with degenerate, weak random sources [20]. A survey of these applications is given by Nisan <ref> [17] </ref>. Bennett et al. [4, 2] and Impagliazzo et al. [12] independently analyzed entropy smoothing by universal hash functions [8] and showed that the length of the almost uniform output depends on the Renyi entropy of order 2 of the input.
Reference: [18] <author> A. Renyi, </author> <title> "On measures of entropy and information," </title> <booktitle> in Proc. 4th Berkeley Symposium on Mathematical Statistics and Probability, </booktitle> <volume> vol. 1, </volume> <pages> (Berkeley), pp. 547-561, </pages> <institution> Univ. of Calif. Press, </institution> <year> 1961. </year>
Reference-contexts: and p log p D (P X kP Y ) = x2X P X (x) : (3) The Renyi entropy of order ff of a random variable X with alphabet X is H ff (X) = 1 ff X P X (x) ff for ff 0 and ff 6= 1 <ref> [18] </ref>. Because the limiting case of Renyi entropy for ff ! 1 is Shannon entropy, we can extend the definition to H 1 (X) = H (X).
Reference: [19] <author> S. Vembu and S. Verdu, </author> <title> "Generating random bits from an arbitrary source: Fundamental limits," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 41, </volume> <pages> pp. 1322-1332, </pages> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: The inclusion of randomized extraction functions is the main difference between entropy smoothing and "pure" random number generation in information theory <ref> [19] </ref>, where no additional random sources are available. However, entropy smoothing does not consider the auxiliary random bits as a resource, unlike extractors used in theoretical computer science [17]. In cryptography, entropy smoothing is known as privacy amplification.
Reference: [20] <author> D. Zuckerman, </author> <title> "Simulating BPP using a general weak random source," </title> <journal> Algorithmica, </journal> <volume> vol. 16, </volume> <pages> pp. 367-391, </pages> <year> 1996. </year> <note> Preliminary version presented at 32nd FOCS (1991). 14 </note>
Reference-contexts: Apart from the applications in cryptography, entropy smoothing is also at the core of many constructions in complexity theory. Examples are pseudorandom generation [11, 14], derandom-ization of algorithms [15], hardness results in computational learning theory [13], and computing with degenerate, weak random sources <ref> [20] </ref>. A survey of these applications is given by Nisan [17].
References-found: 20

