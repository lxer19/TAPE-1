URL: http://www.cs.cmu.edu/~mhb/papers/icml98-submitted.ps.gz
Refering-URL: http://www.cs.cmu.edu/~mhb/papers/
Root-URL: 
Email: mhb@cs.cmu.edu veloso@cs.cmu.edu  
Title: Reusing Learned Policies Between Similar Problems  
Author: Mike Bowling Manuela Veloso 
Keyword: reinforcement learning, subproblem reuse, bounding policy optimality  
Note: Contact: Mike Bowling, mhb@cs.cmu.edu, (412) 268-3069.  
Address: Pittsburgh, PA 15213-3890  
Affiliation: Computer Science Department Carnegie Mellon University  
Abstract: We are interested in being able to leverage policy learning in complex problems upon policies learned for similar problems. This capability is particularly important in robot learning, where gathering data is expensive and time-consuming, and prohibits directly applying reinforcement learning. In this case, we would like to be able to transfer knowledge from a simulator, which may have an inaccurate or crude model of the robot and environment. We observed that when applying a policy learned in a simulator, some parts of the policy effectively apply to the real robots while other parts do not. We then explored learning a complex problem by reusing only parts of the solutions of similar problems. Empirical experiments of learning when part of the policy is fixed show that the complete task is learned faster, but the resulting policy is suboptimal. One of the main contributions of this paper is a theorem and its proof, which states the degree of suboptimality of a policy that is fixed over a subproblem, can be determined without the need for optimally solving the complete problem. We formally define a subproblem and build upon the value equivalence of the boundary states of the subproblem to prove the bound on suboptimality. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Craig Boutilier. </author> <title> Macros and subproblems in reinforcement learning. In personal communication, </title> <year> 1998. </year>
Reference-contexts: Applying our theoretical results. Finally, the suboptimality bound as given by our theorem could be combined with methods that find structure in reinforcement learning problems. The bound can provide a measure to evaluate the effectiveness of the constructed abstractions or the loss in optimality of using the abstractions <ref> [1, 2, 3] </ref>. 6 Conclusion In this paper, we introduced and proved a tight bound for the suboptimality of a learned policy that is fixed over a subproblem.
Reference: [2] <author> Thomas Dean and Shieu-Hong Lin. </author> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <year> 1995. </year> <month> 14 </month>
Reference-contexts: Applying our theoretical results. Finally, the suboptimality bound as given by our theorem could be combined with methods that find structure in reinforcement learning problems. The bound can provide a measure to evaluate the effectiveness of the constructed abstractions or the loss in optimality of using the abstractions <ref> [1, 2, 3] </ref>. 6 Conclusion In this paper, we introduced and proved a tight bound for the suboptimality of a learned policy that is fixed over a subproblem.
Reference: [3] <author> Doina Precup, Richard S. Sutton, and Satinder P. Singh. </author> <title> Planning with closed-loop macro actions. </title> <booktitle> In Working notes of the 1997 AAAI Fall Symposium on Model-directed Autonomous Systems, </booktitle> <year> 1997. </year>
Reference-contexts: Applying our theoretical results. Finally, the suboptimality bound as given by our theorem could be combined with methods that find structure in reinforcement learning problems. The bound can provide a measure to evaluate the effectiveness of the constructed abstractions or the loss in optimality of using the abstractions <ref> [1, 2, 3] </ref>. 6 Conclusion In this paper, we introduced and proved a tight bound for the suboptimality of a learned policy that is fixed over a subproblem.
Reference: [4] <author> Sebastian Thrun and Anton Schwartz. </author> <title> Finding structure in reinforcement learning. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 385392. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Two simulated domains were investigated. The first is the simulator used for the robot soccer problem that was just described. In this domain, our similar problem was to alter the desired direction for the robot to strike the ball. The second domain is a discrete grid-navigation problem, presented in <ref> [4] </ref>, and depicted in figure 2. The agent can move in any of the eight grid directions. But, with a probability of 0.1, its action is ignored and it is moved to a random neighbor. <p> Combining both plan retrieval and plan adaptation is shown to produce interesting results (e.g., [6]). In our case, it may be possible to learn these subproblems, which is the goal of the SKILLS algorithm <ref> [4] </ref>. We would have to slightly constrain the state construction of SKILLS to comply to our definition of subproblem. In particular, by our definition of subproblem, states need to be spatially adjacent so that boundaries can be identified. This extension is part of our current research agenda.
Reference: [5] <author> Manuela Veloso, Peter Stone, Kwun Han, and Sorin Achim. CMUnited: </author> <title> A team of robotic soccer agents collaborating in an adversarial environment. </title> <editor> In Hiroaki Kitano, editor, RoboCup-97: </editor> <title> The First Robot World Cup Soccer Games and Conferences. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year> <note> A shorter version appeared in the First International Workshop on RoboCup, IJCAI-97, </note> <month> August, </month> <year> 1997. </year>
Reference-contexts: We analyze the value of the bounds in particular situations, and verify the bound for our results from Section 3. Section 5 discusses some related work, and Section 6 concludes. 2 A Motivating Example Our work is in the context of robotic soccer <ref> [5] </ref>. The robots use several hardwired behaviors to select actions to achieve individual and team goals. However there are situations for which finding the appropriate analytical behavior is not trivial, and we would like to apply learning to these cases. Figure 1 illustrates an example of one of these problems.
Reference: [6] <author> Manuela M. Veloso. </author> <title> Flexible strategy learning: Analogical replay of problem solving episodes. </title> <booktitle> In Proceedings of AAAI-94, the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 595600, </pages> <address> Seattle, WA, </address> <month> August </month> <year> 1994. </year> <note> AAAI Press. 15 </note>
Reference-contexts: These two cases create different research questions; one involves how to change the plan, and the other involves defining plan similarity. Combining both plan retrieval and plan adaptation is shown to produce interesting results (e.g., <ref> [6] </ref>). In our case, it may be possible to learn these subproblems, which is the goal of the SKILLS algorithm [4]. We would have to slightly constrain the state construction of SKILLS to comply to our definition of subproblem.
References-found: 6

