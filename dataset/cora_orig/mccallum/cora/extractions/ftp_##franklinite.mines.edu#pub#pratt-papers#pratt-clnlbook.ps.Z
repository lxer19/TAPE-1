URL: ftp://franklinite.mines.edu/pub/pratt-papers/pratt-clnlbook.ps.Z
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/caruana/pub/transferbib.html
Root-URL: 
Title: Experiments on the Transfer of Knowledge between Neural Networks Reprinted from: Computational Learning Theory and
Author: S. Hanson, G. Drastal and R. Rivest, Lorien Y. Pratt 
Address: New Brunswick, NJ  
Affiliation: Rutgers University Computer Science Department,  
Date: August 13, 1994  
Note: editors. MIT Press, 1994, Chapter 19. Pages 523-560.  pratt@paul.rutgers.edu,  08903. This work was performed in part while the author was supported by a research assistantship from Siemens, Inc.. Partial support was also provided by DOE #DE-FG02-91ER61129, through subcontract #097P753 from the University of Wisconsin and by Bellcore.  
Abstract: This chapter describes three studies which address the question of how neural network learning can be improved via the incorporation of information extracted from other networks. This general problem, which we call network transfer, encompasses many types of relationships between source and target networks. Our focus is on the utilization of weights from source networks which solve a subproblem of the target network task, with the goal of speeding up learning on the target task. We demonstrate how the approach described here can improve learning speed by up to ten times over learning starting with random weights. 
Abstract-found: 1
Intro-found: 1
Reference: [ Atlas et al., 1990a ] <author> L. E. Atlas, J. Connor, D. Park, A. Lippman, R. Cole, M. El-Sharkawi, R. J. Marks, Y. Muthusamy, and M. Rudnick. </author> <title> A performance comparison of trained multi-layer perceptrons and trained classification trees. </title> <booktitle> In IEEE International Conference on Systems, Man, and Cybernetics, </booktitle> <address> Cambridge, Massachusetts, </address> <month> November 14-17, </month> <year> 1990. </year>
Reference-contexts: decision trees and statistical techniques for classifier induction in a number of recent papers ( [ Weiss and Kulikowski, 1991 ] , [ Shavlik et al., 1991 ] , [ Atlas et al., 1990b ] , [ Atlas et al., 1990c ] , [ Cole et al., 1990 ] , <ref> [ Atlas et al., 1990a ] </ref> , [ Dietterich et al., 1990 ] , [ Fisher and McKusick, 1989 ] , [ Weiss and Kapouleas, 1989 ] , [ Mooney et al., 1989 ] , and see [ Quinlan, 1992 ] (this volume)).
Reference: [ Atlas et al., 1990b ] <author> Les Atlas, Ronald Cole, Yeshwant Muthasamy, Alan Lippman, Jerome Con-nor, Dong Park, Mohamed El-Sharkawi, and Robert J. Marks II. </author> <title> A performance comparison of trained multi-layer perceptrons and trained classification trees. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> 31 Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 622-629, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction Back-propagation neural networks [ Rumelhart et al., 1987 ] have been compared to decision trees and statistical techniques for classifier induction in a number of recent papers ( [ Weiss and Kulikowski, 1991 ] , [ Shavlik et al., 1991 ] , <ref> [ Atlas et al., 1990b ] </ref> , [ Atlas et al., 1990c ] , [ Cole et al., 1990 ] , [ Atlas et al., 1990a ] , [ Dietterich et al., 1990 ] , [ Fisher and McKusick, 1989 ] , [ Weiss and Kapouleas, 1989 ] , [ Mooney <p> The run that started at 25% never exceeded a 40% score, even after many training runs. Because of the need to control for this variation, one possible future direction for this work is to explore algorithms which automatically and adaptively determine the values of these parameters (e.g.: <ref> [ Atlas et al., 1990b ] </ref> , [ Barnard and Cole, 1989 ] , [ Brent, 1990 ] , [ Chan and Fallside, 1987 ] ). In comparison to the study described in the previous section, here it is clear that fine tuning did improve network performance considerably.
Reference: [ Atlas et al., 1990c ] <author> Les Atlas, Ronald Cole, Yeshwant Muthusamy, A. Lippman, Jerome Connor, D. Park, M. El-Sharkawi, and Robert J. Marks II. </author> <title> Performance comparisons between backpropagation networks and classification trees on three real-world applications. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(10) </volume> <pages> 1614-1619, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Introduction Back-propagation neural networks [ Rumelhart et al., 1987 ] have been compared to decision trees and statistical techniques for classifier induction in a number of recent papers ( [ Weiss and Kulikowski, 1991 ] , [ Shavlik et al., 1991 ] , [ Atlas et al., 1990b ] , <ref> [ Atlas et al., 1990c ] </ref> , [ Cole et al., 1990 ] , [ Atlas et al., 1990a ] , [ Dietterich et al., 1990 ] , [ Fisher and McKusick, 1989 ] , [ Weiss and Kapouleas, 1989 ] , [ Mooney et al., 1989 ] , and see
Reference: [ Barnard and Cole, 1989 ] <author> Etienne Barnard and Ronald A. Cole. </author> <title> A neural-net training program based on conjugate-gradient optimization. </title> <type> Technical Report CSE 89-014, </type> <institution> Oregon Graduate Center, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: Because of the need to control for this variation, one possible future direction for this work is to explore algorithms which automatically and adaptively determine the values of these parameters (e.g.: [ Atlas et al., 1990b ] , <ref> [ Barnard and Cole, 1989 ] </ref> , [ Brent, 1990 ] , [ Chan and Fallside, 1987 ] ). In comparison to the study described in the previous section, here it is clear that fine tuning did improve network performance considerably. <p> The model of transfer used here decouples initial weight determination from learning. Therefore, the learning algorithm can probably be changed (for example to Conjugate Gradient <ref> [ Barnard and Cole, 1989 ] </ref> ) without changes to the transfer process.
Reference: [ Brent, 1990 ] <author> Richard P. Brent. </author> <title> Fast training algorithms for multi-layer neural nets. </title> <type> Technical Report NA-90-03, </type> <institution> Numerical Analysis Project, Computer Science Department, Stanford University, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Because of the need to control for this variation, one possible future direction for this work is to explore algorithms which automatically and adaptively determine the values of these parameters (e.g.: [ Atlas et al., 1990b ] , [ Barnard and Cole, 1989 ] , <ref> [ Brent, 1990 ] </ref> , [ Chan and Fallside, 1987 ] ). In comparison to the study described in the previous section, here it is clear that fine tuning did improve network performance considerably.
Reference: [ Chan and Fallside, 1987 ] <author> L.W. Chan and F. Fallside. </author> <title> An adaptive training algorithm for back propagation networks. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 2 </volume> <pages> 205-218, </pages> <year> 1987. </year>
Reference-contexts: Because of the need to control for this variation, one possible future direction for this work is to explore algorithms which automatically and adaptively determine the values of these parameters (e.g.: [ Atlas et al., 1990b ] , [ Barnard and Cole, 1989 ] , [ Brent, 1990 ] , <ref> [ Chan and Fallside, 1987 ] </ref> ). In comparison to the study described in the previous section, here it is clear that fine tuning did improve network performance considerably.
Reference: [ Cole et al., 1990 ] <author> R. A. Cole, Y. K. Muthusamy, L. Atlas, T. Leen, and M. Rudnick. </author> <title> Speaker-independent vowel recognition: Comparison of backpropagation and trained classification trees. </title> <booktitle> In Proceedings of the Twenty-Third Annual Hawaii International Conference on System Sciences, Kailua-Kona, Hawaii, </booktitle> <month> January 2-5, </month> <pages> pages 132-141, </pages> <year> 1990. </year>
Reference-contexts: al., 1987 ] have been compared to decision trees and statistical techniques for classifier induction in a number of recent papers ( [ Weiss and Kulikowski, 1991 ] , [ Shavlik et al., 1991 ] , [ Atlas et al., 1990b ] , [ Atlas et al., 1990c ] , <ref> [ Cole et al., 1990 ] </ref> , [ Atlas et al., 1990a ] , [ Dietterich et al., 1990 ] , [ Fisher and McKusick, 1989 ] , [ Weiss and Kapouleas, 1989 ] , [ Mooney et al., 1989 ] , and see [ Quinlan, 1992 ] (this volume)).
Reference: [ Dietterich et al., 1990 ] <author> Tom G. Dietterich, Hermann Hild, and Ghulum Bakiri. </author> <title> A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <type> Technical Report (unnumbered), </type> <institution> Department of Computer Science, Oregon State University, Corvallis, </institution> <address> OR 97331-3902, </address> <year> 1990. </year>
Reference-contexts: induction in a number of recent papers ( [ Weiss and Kulikowski, 1991 ] , [ Shavlik et al., 1991 ] , [ Atlas et al., 1990b ] , [ Atlas et al., 1990c ] , [ Cole et al., 1990 ] , [ Atlas et al., 1990a ] , <ref> [ Dietterich et al., 1990 ] </ref> , [ Fisher and McKusick, 1989 ] , [ Weiss and Kapouleas, 1989 ] , [ Mooney et al., 1989 ] , and see [ Quinlan, 1992 ] (this volume)). <p> Several studies have recognized the strong representational similarities between the two formalisms ( [ Utgoff, 1990 ] , [ Shavlik et al., 1991 ] , [ Sankar and Mammone, 1990 ] , <ref> [ Dietterich et al., 1990 ] </ref> ). The study of network transfer has the potential to exploit this relationship to improve learning speed and/or network performance. from neural networks, insertion of rules into networks, and the utilization of learned decision trees.
Reference: [ Fisher and McKusick, 1989 ] <author> Douglas H. Fisher and Kathleen B. McKusick. </author> <title> An empirical comparison of ID3 and back-propagation. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 788-793, </pages> <address> Detroit, MI, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: ( [ Weiss and Kulikowski, 1991 ] , [ Shavlik et al., 1991 ] , [ Atlas et al., 1990b ] , [ Atlas et al., 1990c ] , [ Cole et al., 1990 ] , [ Atlas et al., 1990a ] , [ Dietterich et al., 1990 ] , <ref> [ Fisher and McKusick, 1989 ] </ref> , [ Weiss and Kapouleas, 1989 ] , [ Mooney et al., 1989 ] , and see [ Quinlan, 1992 ] (this volume)).
Reference: [ Fisher et al., 1987 ] <author> W. M. Fisher, V. Zue, J. Bernstein, and D. Pallett. </author> <title> An acoustic-phonetic data base. </title> <journal> J. Acoust. Soc. Am. </journal> <volume> Suppl., 81(1):S92, </volume> <month> Spring </month> <year> 1987. </year>
Reference-contexts: HIDDEN UNITS 46 OUTPUT UNITS 147 INPUT UNITS SPANNING 35ms 147 INPUT UNITS SPANNING 65ms 30 HIDDEN UNITS 46 OUTPUT UNITS 147 INPUT UNITS SPANNING 125ms 30 HIDDEN UNITS 46 OUTPUT UNITS Signal Preprocessing The training set for these experiments was a portion of the DARPA acoustic-phonetic (A-P) corpus ( <ref> [ Fisher et al., 1987 ] </ref> , also called the TI-MIT corpus), which contains recordings of continuous utterances from 630 speakers, along with transcriptions specifying the phonemes uttered.
Reference: [ Fu, 1991 ] <author> LiMin Fu. </author> <title> Rule learning by searching on adapted nets. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> pages 590-595, </pages> <address> Anaheim, CA, </address> <year> 1991. </year>
Reference-contexts: Target Network Trained Network Single-source Transfer Multiple-Source Transfer (Includes Problem Decomposition) Continued Back Propagation Training . . . Input Hidden Output Source Network (s) Insertion [Towell et. al., 1990] Symbolic Representation Expert Knowledge TRANSFER Learned Decision Tree <ref> [Fu, 1991] </ref> Extraction 1.1 Types of source/target relationships in transfer: We distinguish between two general classes of relationships between source and target networks: 4 Related problem: The source network learns a problem that is related to the target.
Reference: [ Hanson and Pratt, 1989 ] <author> Stephen Jose Hanson and Lorien Y. Pratt. </author> <title> Comparing biases for minimal network construction with back-propagation. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <pages> pages 177-185. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year>
Reference-contexts: These entry points include: * Network topology (cf. [ Lang, 1990 ] , [ Towell et al., 1990 ] ) * Input unit values (preprocessing) * Node activation functions (cf. [ Moody and Darken, 1989 ] ) * Objective function <ref> [ Hanson and Pratt, 1989 ] </ref> , [ Lang, 1990 ] * Target unit values (postprocessing) * Training data order (cf. [ Kruschke, 1990 ] ) * Initial network weights [ Towell et al., 1990 ] * Learning parameters ; ff (many studies, where these parameters are chosen based on their
Reference: [ Hataoka and Waibel, 1989 ] <author> Nobuo Hataoka and Alex H. Waibel. </author> <title> Speaker-independent phoneme recognition on TIMIT database using integated time-delay neural networks (TDNNs). </title> <type> Technical Report CMU-CS-89-190, </type> <institution> Carnegie-Mellon University, </institution> <month> November </month> <year> 1989. </year> <month> 32 </month>
Reference-contexts: In this and the next section we borrow a decomposition technique introduced by Waibel et al. [ 1989 ] , who showed improved learning speed and performance in networks for spoken consonant recognition (see also [ Waibel et al., 1987 ] , [ Waibel, 1989 ] , and <ref> [ Hataoka and Waibel, 1989 ] </ref> .) 3.1 Motivation for Problem Decomposition It has often been observed that small back-propagation networks train quickly, but large ones seem to require exponentially longer time ( [ Tesauro, 1987 ] , [ Tesauro and Janssens, 1988 ] ).
Reference: [ Judd, 1988 ] <author> Stephen Judd. </author> <title> Learning in neural networks. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 2-8. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference: [ Kamm and Singhal, 1990 ] <author> C. A. Kamm and S. Singhal. </author> <title> Effect of neural network input span on phoneme classification. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, 1990, </booktitle> <volume> volume 1, </volume> <pages> pages 195-200, </pages> <address> San Diego, 1990. </address> <publisher> IEEE. </publisher>
Reference-contexts: We demonstrate an order-of-magnitude speed-up in training time for this task. This work was done as part of the Bellcore AP-net project <ref> [ Kamm and Singhal, 1990 ] </ref> . The general goal of the AP-net project is to develop a system for large vocabulary, speaker-independent recognition of continuous speech. This task is divided into three stages, illustrated in Figure 9. <p> Section 4.3 discusses how network performance was measured. Experimental results are given in Section 4.4. 4.1 Previous work: Non-decomposed AP-net networks Networks with Varying Input Durations In a previous experiment designed to study the effect of network input span on phoneme classification performance <ref> [ Kamm and Singhal, 1990 ] </ref> , 19 Use Word-Tuple Probabilities to select likely Sentence Candidates Search Vocabulary for Hypothesized Phoneme Strings to produce Word Candidates Dictionary of Phonemic Transcriptions of Vocabulary Words STAGE 3 Recognized Words or Sentences STAGE 2 Neural Network Output Activation Vector i e a f sil <p> These two modifications are described in more detail in <ref> [ Kamm and Singhal, 1990 ] </ref> . The networks were trained using back-propagation parameters (learning rate) = 0.1 and ff (momentum) = 0:1. <p> Specifically, the proportion of each phoneme in the corpus (or in the subnetwork phoneme set) is multiplied by each hit rate and false alarm score and the result is summed. These normalized averaged scores are generally about 5% higher than the scores reported in <ref> [ Kamm and Singhal, 1990 ] </ref> , reflecting the fact that these networks tended to have higher performance on the more frequently occurring phoneme classes. 5 4.4 Results Table 2 summarizes the conditions used to train networks in this study.
Reference: [ Kolen and Pollack, 1990 ] <author> John F. Kolen and Jordan Pollack. </author> <title> Scenes from exclusive-or: Back propagation is sensitive to initial conditions. </title> <booktitle> In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, </booktitle> <pages> page 868, </pages> <address> Cambridge, MA, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: Our experience in this project supports the fact that the careful selection of network training parameters is critical for successful training (and performance) (see also <ref> [ Kolen and Pollack, 1990 ] </ref> ). For one particular set of parameters, glue training achieved 25% AHR on the training data after three epochs while for another set of parameters, glue training achieved 48% AHR after three epochs.
Reference: [ Kruschke, 1990 ] <author> John K. Kruschke. ALCOVE: </author> <title> A connectionist model of category learning. </title> <type> Technical Report 19, </type> <institution> Indiana University Cognitive Science Department, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: ] , [ Towell et al., 1990 ] ) * Input unit values (preprocessing) * Node activation functions (cf. [ Moody and Darken, 1989 ] ) * Objective function [ Hanson and Pratt, 1989 ] , [ Lang, 1990 ] * Target unit values (postprocessing) * Training data order (cf. <ref> [ Kruschke, 1990 ] </ref> ) * Initial network weights [ Towell et al., 1990 ] * Learning parameters ; ff (many studies, where these parameters are chosen based on their success on previous problems) Understanding of the classification domain and experience with networks for related tasks are usually used to determine
Reference: [ Lang, 1990 ] <author> Kevin Lang. </author> <title> Variable resolution learning techniques for speech recognition. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Many recent research efforts can be viewed as falling into this latter category. Knowledge can be injected at a number of different points into back-propagation networks. These entry points include: * Network topology (cf. <ref> [ Lang, 1990 ] </ref> , [ Towell et al., 1990 ] ) * Input unit values (preprocessing) * Node activation functions (cf. [ Moody and Darken, 1989 ] ) * Objective function [ Hanson and Pratt, 1989 ] , [ Lang, 1990 ] * Target unit values (postprocessing) * Training data <p> These entry points include: * Network topology (cf. <ref> [ Lang, 1990 ] </ref> , [ Towell et al., 1990 ] ) * Input unit values (preprocessing) * Node activation functions (cf. [ Moody and Darken, 1989 ] ) * Objective function [ Hanson and Pratt, 1989 ] , [ Lang, 1990 ] * Target unit values (postprocessing) * Training data order (cf. [ Kruschke, 1990 ] ) * Initial network weights [ Towell et al., 1990 ] * Learning parameters ; ff (many studies, where these parameters are chosen based on their success on previous problems) Understanding of the
Reference: [ Langley, 1989 ] <author> Pat Langley. </author> <title> Unifying Themes in Empirical and Explanation-Based Learning. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <publisher> Cornell University, </publisher> <pages> pages 2-4, </pages> <address> Ithica, NY, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Knowledge may or may not be expressed in a logical formalism <ref> [ Langley, 1989 ] </ref> . 3 Unfortunately, formalized knowledge about neural network task domains isn't often available. An alternative is to simply use the weights generated by a back-propagation network trained on one task to bias learning on a related task.
Reference: [ Lee and Hon, 1989 ] <author> Kai-Fu Lee and Hsiao-Wuen Hon. </author> <title> Speaker-independent phone recognition using Hidden Markov Models. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 37(11) </volume> <pages> 1641-1648, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: It is important to be able to compare the performance of the AP-net network against those of other methods for performing the same task. A recent Hidden Markov Model (HMM) phoneme recognition system <ref> [ Lee and Hon, 1989 ] </ref> had a 46% error rate on the DARPA A-P corpus, measured as the number of phonemes chosen incorrectly. AP-net generates an output activation vector for each successive 5-ms frame of speech (based on 35ms, 65ms, or 125ms of context information).
Reference: [ Moody and Darken, 1989 ] <author> John Moody and Christian J. Darken. </author> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 281-294, </pages> <year> 1989. </year>
Reference-contexts: Knowledge can be injected at a number of different points into back-propagation networks. These entry points include: * Network topology (cf. [ Lang, 1990 ] , [ Towell et al., 1990 ] ) * Input unit values (preprocessing) * Node activation functions (cf. <ref> [ Moody and Darken, 1989 ] </ref> ) * Objective function [ Hanson and Pratt, 1989 ] , [ Lang, 1990 ] * Target unit values (postprocessing) * Training data order (cf. [ Kruschke, 1990 ] ) * Initial network weights [ Towell et al., 1990 ] * Learning parameters ; ff
Reference: [ Mooney et al., 1989 ] <author> Raymond J. Mooney, J. W. Shavlik, G. G. Towell, and A. Gove. </author> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 775-780, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: , [ Atlas et al., 1990b ] , [ Atlas et al., 1990c ] , [ Cole et al., 1990 ] , [ Atlas et al., 1990a ] , [ Dietterich et al., 1990 ] , [ Fisher and McKusick, 1989 ] , [ Weiss and Kapouleas, 1989 ] , <ref> [ Mooney et al., 1989 ] </ref> , and see [ Quinlan, 1992 ] (this volume)). Although these studies describe some problems for which neural networks achieve superior performance to other methods, they also show fairly consistently that longer training times are required.
Reference: [ Morgan and Bourlard, 1990 ] <author> N. Morgan and H. Bourlard. </author> <title> Generalization and parameter estimation in feedforward nets: Some experiments. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 630-637. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: This won't necessarily happen in all decomposed networks there may be local minima that are optimal for subproblems but are not near an optimum solution for the full problem. Also, generalization tests <ref> [ Morgan and Bourlard, 1990 ] </ref> indicate that performance can be degraded in a network that overfits its data, so a careful methodology is necessary to obtain the best sub-networks, as well as the best combined network.
Reference: [ Nowlan, 1990 ] <author> Steven J. Nowlan. </author> <title> Competing experts: An experimental investigation of associative mixture models. </title> <type> Technical Report CRG-TR-90-5, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <year> 1990. </year>
Reference-contexts: The choice of network decomposition on the phoneme recognition task was more principled. It should also be fruitful to explore automated methods for guiding problem decomposition using domain knowledge. Automatic methods for determining decompositions which only use information within the training data have also been recently explored <ref> [ Nowlan, 1990 ] </ref> . * When training data is impoverished (noisy, incorrect, or partial), it may be possible to achieve a performance improvement by using transferred weights.
Reference: [ Pratt and Kamm, 1991 ] <author> Lorien Y. Pratt and Candace A. Kamm. </author> <title> Improving a phoneme classification neural network through problem decomposition. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (IJCNN-91), </booktitle> <pages> pages 821-826, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1991. </year> <note> IEEE. </note>
Reference-contexts: This chapter expands upon, and includes portions of, work previously presented in <ref> [ Pratt and Kamm, 1991 ] </ref> and [ Pratt et al., 1991 ] . 2 Pilot study: Weight Magnitudes and Network Initialization In this section, we present results of a study that explores the dynamics of networks which have had some of their weights pre-set non-randomly.
Reference: [ Pratt et al., 1991 ] <author> Lorien Y. Pratt, Jack Mostow, and Candace A. Kamm. </author> <title> Direct transfer of learned information among neural networks. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> pages 584-589, </pages> <address> Anaheim, CA, </address> <year> 1991. </year> <month> 33 </month>
Reference-contexts: This chapter expands upon, and includes portions of, work previously presented in [ Pratt and Kamm, 1991 ] and <ref> [ Pratt et al., 1991 ] </ref> . 2 Pilot study: Weight Magnitudes and Network Initialization In this section, we present results of a study that explores the dynamics of networks which have had some of their weights pre-set non-randomly.
Reference: [ Pratt, 1991 ] <author> Lorien Y. Pratt. </author> <title> Discriminability-based transfer between neural networks, </title> <booktitle> 1991. </booktitle> <address> (sub-mitted, NIPS-91). </address>
Reference-contexts: Note that weights represent partial, but largely correct, knowledge. Our focus here is on transfer from subnetworks; related problem transfer is explored in <ref> [ Pratt, 1991 ] </ref> . There are two possible formulations of the subnetwork!target network transfer problem: 1. We are given a trained subnetwork, to utilize as we choose as part of a larger network. 2. We are given a large network learning task, which has not been pre-trained.
Reference: [ Quinlan, 1986 ] <author> J. R. Quinlan. </author> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: network for a related problem, how can its weights be converted to the initial weights of a new target network learning task in order to bias further learning? The study of transfer in neural networks also has a broader scope, incorporating the question of how information from learned decision trees <ref> [ Quinlan, 1986 ] </ref> can be utilized by neural networks. In both formalisms, it is possible to represent the basic decision-making unit as a linear separation of feature space. Therefore, learned information is interchangeable distinctions made by decision trees can be used in neural networks and vice versa.
Reference: [ Quinlan, 1992 ] <author> J. R. Quinlan. </author> <title> Comparing connectionist and symbolic learning methods. In Computational Learning Theory and Natural Learning Systems, Constraints and Prospects. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Atlas et al., 1990c ] , [ Cole et al., 1990 ] , [ Atlas et al., 1990a ] , [ Dietterich et al., 1990 ] , [ Fisher and McKusick, 1989 ] , [ Weiss and Kapouleas, 1989 ] , [ Mooney et al., 1989 ] , and see <ref> [ Quinlan, 1992 ] </ref> (this volume)). Although these studies describe some problems for which neural networks achieve superior performance to other methods, they also show fairly consistently that longer training times are required. Furthermore, Judd [ 1988 ] has shown that neural network learning is NP-complete.
Reference: [ Robinson, 1989 ] <author> Anthony John Robinson. </author> <title> Dynamic Error Propagation Networks. </title> <type> PhD thesis, </type> <institution> Cambridge University, Engineering Department, </institution> <month> June </month> <year> 1989. </year>
Reference: [ Rumelhart et al., 1987 ] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In David E. Rumelhart and James L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press: Bradford Books, </publisher> <year> 1987. </year>
Reference-contexts: 1 Introduction Back-propagation neural networks <ref> [ Rumelhart et al., 1987 ] </ref> have been compared to decision trees and statistical techniques for classifier induction in a number of recent papers ( [ Weiss and Kulikowski, 1991 ] , [ Shavlik et al., 1991 ] , [ Atlas et al., 1990b ] , [ Atlas et al., 1990c <p> Therefore we also studied the effect of varying the magnitudes of weights from subnetworks to attempt to understand the effect that they have on further training. We can predict this effect to an extent by examining the input-to-hidden weight update equation used by back-propagation <ref> [ Rumelhart et al., 1987 ] </ref> : w ij = y j (1 y j ) (1) k | -z - x i |-z (1) Here, j indexes hidden units, i indexes input units, k indexes output units, x i is the activation of input unit i, y j is the <p> Each training data target was either 0 or 1. A manual search for values of and ff (10 pairs tried, on networks with random initial weights) resulted in locally optimal values of = 1:5, ff = :9, which were used for all experiments. Standard backpropagation <ref> [ Rumelhart et al., 1987 ] </ref> was used, with a sigmoidal activation function, training after every pattern presentation, and training data presented sequentially. A number of experiments were performed, summarized in Table 1. <p> The output unit corresponding to this phoneme received a target activation of 0:9; all other output units received target activations of 0:1. A modified version of the back-propagation algorithm <ref> [ Rumelhart et al., 1987 ] </ref> was used for training.
Reference: [ Sankar and Mammone, 1990 ] <author> A. Sankar and R. J. Mammone. </author> <title> A fast learning algorithm for tree neural networks. </title> <booktitle> 1990 Conference on Information Sciences and Systems, </booktitle> <address> Princeton, NJ, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: Therefore, learned information is interchangeable distinctions made by decision trees can be used in neural networks and vice versa. Several studies have recognized the strong representational similarities between the two formalisms ( [ Utgoff, 1990 ] , [ Shavlik et al., 1991 ] , <ref> [ Sankar and Mammone, 1990 ] </ref> , [ Dietterich et al., 1990 ] ). The study of network transfer has the potential to exploit this relationship to improve learning speed and/or network performance. from neural networks, insertion of rules into networks, and the utilization of learned decision trees.
Reference: [ Schroeder et al., 1979 ] <author> M. R. Schroeder, B. S. Atal, and J. L. Hall. </author> <title> Optimizing digital speech codes by exploiting masking properties of the human ear. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 66 </volume> <pages> 1647-1652, </pages> <year> 1979. </year>
Reference-contexts: The use of multiple input spans can be viewed as decomposing the problem 4 Barks rescale frequency to reflect equal intervals along the basilar membrane in the cochlea. The scaling is also approximately linearly related to pitch perception. See <ref> [ Schroeder et al., 1979 ] </ref> . 22 in terms of the input units. The problem was further decomposed by assigning the output units so that different subnets included different (but overlapping) subsets of output classes.
Reference: [ Shavlik et al., 1991 ] <author> J. W. Shavlik, R. J. Mooney, and G. G. Towell. </author> <title> Symbolic and neural net learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6(2) </volume> <pages> 111-143, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Back-propagation neural networks [ Rumelhart et al., 1987 ] have been compared to decision trees and statistical techniques for classifier induction in a number of recent papers ( [ Weiss and Kulikowski, 1991 ] , <ref> [ Shavlik et al., 1991 ] </ref> , [ Atlas et al., 1990b ] , [ Atlas et al., 1990c ] , [ Cole et al., 1990 ] , [ Atlas et al., 1990a ] , [ Dietterich et al., 1990 ] , [ Fisher and McKusick, 1989 ] , [ Weiss <p> Therefore, learned information is interchangeable distinctions made by decision trees can be used in neural networks and vice versa. Several studies have recognized the strong representational similarities between the two formalisms ( [ Utgoff, 1990 ] , <ref> [ Shavlik et al., 1991 ] </ref> , [ Sankar and Mammone, 1990 ] , [ Dietterich et al., 1990 ] ).
Reference: [ Shavlik, 1992 ] <author> Jude Shavlik. </author> <title> Integrating explanatory and neural approaches to machine learning. In Computational Learning Theory and Natural Learning Systems, Constraints and Prospects. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference: [ Simon, 1981 ] <editor> H. Simon. </editor> <booktitle> The sciences of the artificial (2nd ed.). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1981. </year>
Reference-contexts: Therefore, breaking a larger problem into pieces may speed up the learning process. The idea of problem decomposition has also been widely studied for AI problems (cf. <ref> [ Simon, 1981 ] </ref> ). 1.2 Types of subnetworks We can distinguish between two general classes of source subnetworks, corresponding to horizontal and vertical decompositions of the target network. In horizontal decomposition, subnetworks are determined by dividing the network between layers (Figure 2).
Reference: [ Sontag, 1989 ] <author> Eduardo Sontag. </author> <title> Sigmoids distinguish more efficiently than heavisides. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 470-472, </pages> <year> 1989. </year>
Reference-contexts: The consequence of viewing activation functions as thresholds is that it may appear that more hidden units are necessary to solve a problem than are strictly required (see <ref> [ Sontag, 1989 ] </ref> ). Consider a space of n dimensions, where n is the number of input units. As shown for n = 2 in values.
Reference: [ Tesauro and Janssens, 1988 ] <author> Gerald Tesauro and Robert Janssens. </author> <title> Scaling relationships in backpropagation learning: dependence on predicate order. </title> <type> Technical Report CCSR-88-1, </type> <institution> Center for Complex Systems Research, University of Illinois at Urbana-Champaign, </institution> <month> February </month> <year> 1988. </year>
Reference-contexts: Waibel et al., 1987 ] , [ Waibel, 1989 ] , and [ Hataoka and Waibel, 1989 ] .) 3.1 Motivation for Problem Decomposition It has often been observed that small back-propagation networks train quickly, but large ones seem to require exponentially longer time ( [ Tesauro, 1987 ] , <ref> [ Tesauro and Janssens, 1988 ] </ref> ). One solution to this problem is to only train small networks. Many real-world problems can be readily decomposed into smaller sub-problems. Smaller networks can be trained on these sub-problems and then combined later into larger networks to solve the original problem.
Reference: [ Tesauro, 1987 ] <author> Gerald Tesauro. </author> <title> Scaling relationships in back-propagation learning: dependence on training set size. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 367-372, </pages> <year> 1987. </year>
Reference-contexts: We may then choose to decompose the task, training smaller networks for sub-tasks before joining them to solve the larger task. The experiments reported here bear on both of these approaches. The second approach is motivated by many empirical studies (cf. <ref> [ Tesauro, 1987 ] </ref> ) which indicate that neural network training is particularly slow for large networks. Therefore, breaking a larger problem into pieces may speed up the learning process. <p> consonant recognition (see also [ Waibel et al., 1987 ] , [ Waibel, 1989 ] , and [ Hataoka and Waibel, 1989 ] .) 3.1 Motivation for Problem Decomposition It has often been observed that small back-propagation networks train quickly, but large ones seem to require exponentially longer time ( <ref> [ Tesauro, 1987 ] </ref> , [ Tesauro and Janssens, 1988 ] ). One solution to this problem is to only train small networks. Many real-world problems can be readily decomposed into smaller sub-problems.
Reference: [ Towell et al., 1990 ] <author> Geoffrey G. Towell, Jude W. Shavlik, and Michiel O. Noordewier. </author> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of AAAI-90, </booktitle> <pages> pages 861-866. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1990. </year> <month> 34 </month>
Reference-contexts: Many recent research efforts can be viewed as falling into this latter category. Knowledge can be injected at a number of different points into back-propagation networks. These entry points include: * Network topology (cf. [ Lang, 1990 ] , <ref> [ Towell et al., 1990 ] </ref> ) * Input unit values (preprocessing) * Node activation functions (cf. [ Moody and Darken, 1989 ] ) * Objective function [ Hanson and Pratt, 1989 ] , [ Lang, 1990 ] * Target unit values (postprocessing) * Training data order (cf. [ Kruschke, 1990 <p> * Input unit values (preprocessing) * Node activation functions (cf. [ Moody and Darken, 1989 ] ) * Objective function [ Hanson and Pratt, 1989 ] , [ Lang, 1990 ] * Target unit values (postprocessing) * Training data order (cf. [ Kruschke, 1990 ] ) * Initial network weights <ref> [ Towell et al., 1990 ] </ref> * Learning parameters ; ff (many studies, where these parameters are chosen based on their success on previous problems) Understanding of the classification domain and experience with networks for related tasks are usually used to determine values for each of these choices. <p> For substantially different source and target network tasks, careful magnitude tuning may be necessary. Furthermore, for a technique like that described in <ref> [ Towell et al., 1990 ] </ref> , which uses weight sets obtained by means other than prior network training, more attention to weight magnitudes may be helpful in dealing with potentially incorrect initial weights. * The network decomposition on the vowel recognition task was chosen arbitrarily. <p> Although this question has been explored in related contexts <ref> [ Towell et al., 1990 ] </ref> , [ Waibel et al., 1989 ] , an important open issue is whether direct network transfer produces significant performance improvement over randomly initialized networks. * Note that transfer (as well as the other techniques shown in Figure 1) determines a set of initial weights,
Reference: [ Utgoff, 1990 ] <author> Paul. E. Utgoff. </author> <title> Perceptron trees: A case study in hybrid concept representations. </title> <journal> Connection Science, </journal> <volume> 1(4), </volume> <year> 1990. </year>
Reference-contexts: Therefore, learned information is interchangeable distinctions made by decision trees can be used in neural networks and vice versa. Several studies have recognized the strong representational similarities between the two formalisms ( <ref> [ Utgoff, 1990 ] </ref> , [ Shavlik et al., 1991 ] , [ Sankar and Mammone, 1990 ] , [ Dietterich et al., 1990 ] ).
Reference: [ Waibel et al., 1987 ] <author> Alexander Waibel, T. Hanazawa, Geoff Hinton, Kiyohiro Shikano, and Kevin Lang. </author> <title> Phoneme recognition using time-delay neural networks. </title> <type> Technical Report TR-I-0006, </type> <institution> ATR Interpreting Telephony Research Laboratories, </institution> <address> Japan, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: In this and the next section we borrow a decomposition technique introduced by Waibel et al. [ 1989 ] , who showed improved learning speed and performance in networks for spoken consonant recognition (see also <ref> [ Waibel et al., 1987 ] </ref> , [ Waibel, 1989 ] , and [ Hataoka and Waibel, 1989 ] .) 3.1 Motivation for Problem Decomposition It has often been observed that small back-propagation networks train quickly, but large ones seem to require exponentially longer time ( [ Tesauro, 1987 ] ,
Reference: [ Waibel et al., 1989 ] <author> Alexander Waibel, Hidefumi Sawai, and Kiyohiro Shikano. </author> <title> Modularity and scaling in large phonemic neural networks. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 37(12) </volume> <pages> 1888-1898, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: These are markedly lower than the scores at the end of glue training. Substituting further glue training for fine tuning did not cause such a drop in performance (nor did performance improve substantially) Note that this study extends the results of <ref> [ Waibel et al., 1989 ] </ref> in that it uses a network with one hidden layer instead of two. It also uses different training data than [ Waibel et al., 1989 ] 's consonant recognition task. Under these different conditions, our experiments also show improved learning speed from problem decomposition. <p> for fine tuning did not cause such a drop in performance (nor did performance improve substantially) Note that this study extends the results of <ref> [ Waibel et al., 1989 ] </ref> in that it uses a network with one hidden layer instead of two. It also uses different training data than [ Waibel et al., 1989 ] 's consonant recognition task. Under these different conditions, our experiments also show improved learning speed from problem decomposition. <p> The networks in <ref> [ Waibel et al., 1989 ] </ref> had two layers of hidden units, while those in Section 3 had only a single hidden layer. In both cases, subnetworks used connections from all input units; decomposition was achieved by dividing the output units into non-overlapping subsets. <p> Although this question has been explored in related contexts [ Towell et al., 1990 ] , <ref> [ Waibel et al., 1989 ] </ref> , an important open issue is whether direct network transfer produces significant performance improvement over randomly initialized networks. * Note that transfer (as well as the other techniques shown in Figure 1) determines a set of initial weights, which are then used for further training.
Reference: [ Waibel, 1989 ] <author> Alexander Waibel. </author> <title> Modular construction of time-delay neural networks for speech recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 39-46, </pages> <year> 1989. </year>
Reference-contexts: In this and the next section we borrow a decomposition technique introduced by Waibel et al. [ 1989 ] , who showed improved learning speed and performance in networks for spoken consonant recognition (see also [ Waibel et al., 1987 ] , <ref> [ Waibel, 1989 ] </ref> , and [ Hataoka and Waibel, 1989 ] .) 3.1 Motivation for Problem Decomposition It has often been observed that small back-propagation networks train quickly, but large ones seem to require exponentially longer time ( [ Tesauro, 1987 ] , [ Tesauro and Janssens, 1988 ] ).
Reference: [ Weigend et al., 1990 ] <author> Andreas S. Weigend, Bernardo A. Huberman, and David E. Rumelhart. </author> <title> Predicting the future: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 193-209, </pages> <year> 1990. </year>
Reference-contexts: An input pattern was considered irrelevant to a particular subnetwork if its target label was not in the set of four or three vowels for which that subnetwork was responsible. We used an "oversized" network training technique <ref> [ Weigend et al., 1990 ] </ref> to control for overfitting: subnetwork training, glue training, and fine tuning were continued until the performance on the test set ceased to improve.
Reference: [ Weiss and Kapouleas, 1989 ] <author> Sholom M. Weiss and Ioannis Kapouleas. </author> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 781-787, </pages> <address> Detroit, MI, </address> <year> 1989. </year> <title> IJCAI, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: , [ Shavlik et al., 1991 ] , [ Atlas et al., 1990b ] , [ Atlas et al., 1990c ] , [ Cole et al., 1990 ] , [ Atlas et al., 1990a ] , [ Dietterich et al., 1990 ] , [ Fisher and McKusick, 1989 ] , <ref> [ Weiss and Kapouleas, 1989 ] </ref> , [ Mooney et al., 1989 ] , and see [ Quinlan, 1992 ] (this volume)). Although these studies describe some problems for which neural networks achieve superior performance to other methods, they also show fairly consistently that longer training times are required.
Reference: [ Weiss and Kulikowski, 1991 ] <author> Sholom M. Weiss and Casimir A. </author> <title> Kulikowski. Computer Systems that Learn. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year> <month> 35 </month>
Reference-contexts: 1 Introduction Back-propagation neural networks [ Rumelhart et al., 1987 ] have been compared to decision trees and statistical techniques for classifier induction in a number of recent papers ( <ref> [ Weiss and Kulikowski, 1991 ] </ref> , [ Shavlik et al., 1991 ] , [ Atlas et al., 1990b ] , [ Atlas et al., 1990c ] , [ Cole et al., 1990 ] , [ Atlas et al., 1990a ] , [ Dietterich et al., 1990 ] , [ Fisher <p> This is not a reliable estimate of the true error rate of this system; a more rigorous analysis, perhaps using a leaving-one-out strategy <ref> [ Weiss and Kulikowski, 1991 ] </ref> should be performed to verify these results. * We used an oversized network training scheme for the vowel recognition task, stopping training just before a degrading generalization score indicated overfitting.
References-found: 47

