URL: http://www-ccs.cs.umass.edu/~kamath/DSEJ.ps
Refering-URL: http://www-ccs.cs.umass.edu/db/crew.html
Root-URL: 
Email: e-mail: fkamath,krithig@cs.umass.edu  
Title: Correctness Issues in Workflow Management  
Author: Mohan Kamath and Krithi Ramamritham 
Address: Amherst MA 01003, USA.  
Affiliation: Department of Computer Science, University of Massachusetts,  
Abstract: Workflow Management is a technique to integrate and automate the execution of steps that comprise a complex process, e:g:; a business process. Workflow Management Systems (WFMSs) primarily evolved from industry to cater to the growing demand for office automation tools among businesses. Coincidentally, database researchers developed several extended transaction models to handle similar applications. Although the goals of both the communities were the same, the issues they focused on were different. The workflow community primarily focused on modeling aspects to accurately capture the data and control flow requirements between the steps that comprise a workflow, while the database community focused on correctness aspects to ensure data consistency of sub-transactions that comprise a transaction. However, we now see a confluence of some of the ideas, with additional features being gradually offered by WFMSs. This paper provides an overview of correctness in workflow management. Correctness is an important aspect of WFMSs and a proper understanding of the available concepts and techniques by WFMS developers and workflow designers will help in building workflows that are flexible enough to capture the requirements of real world applications and robust enough to provide the necessary correctness and reliability properties. We first enumerate the correctness issues that have to be considered to ensure data consistency. Then we survey techniques that have been proposed or are being used in WFMSs for ensuring correctness of workflows. These techniques emerge from the areas of workflow management, extended transaction models, multidatabases and transactional workflows. Finally, we present some open issues related to correctness of workflows in the presence of concurrency and failures.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Alonso, D. Agrawal, A. El Abbadi, M. Kamath, R. Guenthoer, and C. Mohan. </author> <title> Advanced Transaction Models in Workflow Contexts. </title> <booktitle> In Proc. of Intl. Conference on Data Engineering (ICDE), </booktitle> <year> 1996. </year>
Reference-contexts: It has also been demonstrated that the semantics of some of the ETMs can be implemented using y In this paper, general workflows are those that integrate independently developed applications. Most commercial WFMSs have been supporting only such workflows. 3 workflow models <ref> [1] </ref>. Another closely related area is that of multidatabases or federated databases [6, 34] where several techniques have been developed for handling concurrent transactions whose sub-transactions access data from autonomous databases in the presence of failures. <p> The step will be scheduled for execution by the scheduler again. There are issues of idempotency and we will discuss this scenario under logical failures. 5.2. Logical Failures Logical failures (also termed as semantic failures in <ref> [1] </ref>) occur when a step cannot be executed successfully. This can happen for a variety of reasons.
Reference: [2] <author> G. Alonso, M Kamath, D. Agrawal, A. El Abbadi, R. Gunthor, and C. Mohan. </author> <title> Failure Handling in Large Scale Workflow Management Systems. </title> <type> Technical Report RJ 9913(87293), </type> <institution> IBM Almaden Research Center, </institution> <year> 1994. </year>
Reference-contexts: This implies that another workflow engine should be able to take over control of execution of all the workflows that were being handled by the failed workflow engine. This can be achieved via a clustered workflow engine architecture described in <ref> [2] </ref> where several workflow engines share a workflow database. In this scheme, if a workflow engine fails, the workflow instances controlled by it are handled by other workflow engines in the same cluster. It can be observed that the workflow database is a crucial component in achieving forward recovery.
Reference: [3] <author> P. C. Attie, M. P. Singh, A. Sheth, and M. Rusinkiewicz. </author> <title> Specifying and Enforcing Intertask Dependencies. </title> <booktitle> In Proc. Intl' Conf. on Very Large Data Bases, </booktitle> <pages> page 134, </pages> <address> Dublin, Ireland, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: In particular, the scheduler has to determine which steps have completed, which steps have failed and which steps have to be compensated. Thus the scheduler has to deal with enormous state information especially when several thousand instances of workflows are executing concurrently. Scheduling related issues are discussed further in <ref> [3, 40, 43, 42, 20] </ref>. In section 4, we presented the execution atomicity requirements in the presence of concurrent workflows as discussed in [44, 5, 40].
Reference: [4] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley Series in Computer Science. Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: Execution Atomicity of Workflows Traditional transactions use serializability <ref> [4] </ref> as the correctness criterion. Hence the notion of execution atomicity is that none of the changes made by the transaction are externally visible (to other transactions) before it commits.
Reference: [5] <author> Y. Breitbart, A. Deacon, H.J. Schek, A. Sheth, and Weikum. G. </author> <title> Merging Application-Centric and Data-Centric Approaches to Support Transaction-Oriented Multi-system Workflows. </title> <journal> ACM SIGMOD Record, </journal> <volume> 22(3), </volume> <year> 1993. </year>
Reference-contexts: A step compatibility based approach is suggested in <ref> [5] </ref> to control the interleaving of 8 steps from different workflows. They consider an example of a loan processing request where the execution order of two steps, `risk evaluation' and `risk update' is critical, i:e: given any two workflow instances, these steps from two workflows must be executed serializably. <p> Compatible steps are allowed to interleave in any manner, i:e:; there are no restrictions on how they are scheduled. Whenever the scheduler recognizes that two steps are incompatible the steps are scheduled such that their serializability is assured. Authors of <ref> [5] </ref> confine themselves to compatibility of steps of the same workflow. In general however, the scheduler must handle workflow instances of different workflow schemas whose steps may access the same data at a remote resource manager. <p> Scheduling related issues are discussed further in [3, 40, 43, 42, 20]. In section 4, we presented the execution atomicity requirements in the presence of concurrent workflows as discussed in <ref> [44, 5, 40] </ref>. The invariant based approach of ConTracts [44] can be used to ensure executability of steps when other steps from concurrent workflows can access the same data item. However data inconsistency can be caused due to improper interleaving of two or more steps from different workflows. <p> However data inconsistency can be caused due to improper interleaving of two or more steps from different workflows. A different approach is needed to handle such situations. In <ref> [5] </ref>, the interleaving dependency is specified using a compatibility matrix and the scheduler refers to this matrix to ensure correctness. Although the compatibility matrix is defined for only one workflow, in a real system, data sharing occurs between steps of different workflow schemas.
Reference: [6] <author> Y. Breitbart, H. Garcia-Molina, and A. Silberschatz. </author> <title> Overview of Multidatabase Transaction Management. </title> <journal> The VLDB Journal, </journal> <volume> 1(2) </volume> <pages> 181-240, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Most commercial WFMSs have been supporting only such workflows. 3 workflow models [1]. Another closely related area is that of multidatabases or federated databases <ref> [6, 34] </ref> where several techniques have been developed for handling concurrent transactions whose sub-transactions access data from autonomous databases in the presence of failures. Some of these techniques have also been used for improving the correctness properties offered by transactional workflows [40].
Reference: [7] <author> Q. Chen and U. Dayal. </author> <title> A Transactional Nested Process Management System. </title> <booktitle> In Proc. of Intl. Conference on Data Engineering (ICDE), </booktitle> <year> 1996. </year>
Reference-contexts: To provide workflow wide concurrency for accesses to objects without allowing other workflows to observe the changes, a transactional nested process management system has been described in <ref> [7] </ref>. It provides flexibility in the way objects are committed. A step can delegate the responsibility of committing and aborting operations on certain objects to an ancestor either through an intermediate ancestor or directly. This model improves the concurrency within a workflow compared to the closed nested model [37]. <p> Techniques for failure-handling in a nested hierarchy of workflows is discussed in <ref> [7] </ref>. The rollback of a step at a given level may or may not affect its parent step. If it 13 does, then the parent is to be rolled back and the procedure is repeated until a parent step is reached that does not need to be rolled back.
Reference: [8] <author> C. T. Davies. </author> <title> Data Processing Spheres of Control. </title> <journal> IBM Systems Journal, </journal> <volume> 17(2) </volume> <pages> 179-198, </pages> <year> 1978. </year>
Reference-contexts: Similarly the workflow community has borrowed ideas from ETMs, (e:g:; spheres of joint compensation [30] motivated by spheres of control <ref> [8] </ref> and Sagas [14]) in an effort to improve the correctness properties offered by WFMSs. It has also been demonstrated that the semantics of some of the ETMs can be implemented using y In this paper, general workflows are those that integrate independently developed applications.
Reference: [9] <editor> A. Elmagarmid, editor. </editor> <title> Transaction Models for Advanced Database Applications. </title> <publisher> Morgan-Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: However, correctness aspects have largely been ignored. The database community also sensed the need for developing transaction processing systems to handle the needs of new applications like design and office automation. Realizing the limitations of the traditional transaction model for handling long duration applications, several extended transaction models (ETMs) <ref> [9] </ref> were proposed that relax the ACID (Atomicity, Consistency, Isolation and Durability) properties in various ways. Specifically, the focus was on correctness aspects so as to ensure data consistency of sub-transactions that comprise a transaction.
Reference: [10] <author> A. K. Elmagarmid, Y. Leu, W. Litwin, and M. Rusinkiewicz. </author> <title> A Multidatabase Transaction Model for InterBase. </title> <booktitle> In Proc. Int'l. Conf. on Very Large Data Bases, </booktitle> <pages> page 507, </pages> <address> Brisbane, Australia, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: The correctness criterion requires that steps belonging to the same execution-atomic unit of a workflow have compatible serialization orders at all local sites they access. A variation of the same scheme has been used to defined FT-serializability [24] as a correctness criterion for concurrent execution of Flex transactions <ref> [10] </ref> to implement telecommunication workflows. 9 5. Failure Atomicity of Workflows Failure atomicity requirements of a workflow govern how and what changes made by the steps of a workflow are made persistent depending on the success of failure of a workflow.
Reference: [11] <author> Leymann F. and Roller D. </author> <title> Business Process Management With FlowMark. </title> <booktitle> In Proceedings of IEEE CompCon, </booktitle> <pages> pages 230-234, </pages> <year> 1994. </year>
Reference-contexts: Workflow Management Systems (WFMSs) provide support for modeling, executing and monitoring the workflows. WFMSs allow the composition of large applications from smaller independently developed applications. Several prototype and commercial WFMSs have been developed and deployed <ref> [11, 33, 22, 23, 15, 36] </ref>. The workflow community primarily focused on modeling aspects of workflows, so as to accurately capture (i) the data and control flow requirements between the steps that comprise a workflow and (ii) the organizational hierarchy and staff assignments. <p> A step definition consists of what tools/programs are to be used for executing the step. Each step has a set of input and output parameters. To check that a step is started and completed correctly, a start and finish condition can be associated with it <ref> [11] </ref>. There are two types of 4 directed arcs that connect the steps | data flow arcs and control flow arcs. A data flow arc maps an output parameter of a step to input parameters of one or more steps. <p> To facilitate this, the notion of committed acceptable and aborted acceptable termination states for a workflow have been proposed [40]. Given a transactional workflow specification, the set of acceptable states can be systematically determined using event algebra [42]. There are other notions such as dead-path elimination in FlowMark <ref> [11] </ref> which helps the workflow scheduler determine when a workflow is done. These techniques help the scheduler in determining when a workflow is considered complete or in an acceptable state.
Reference: [12] <author> A. A. Farrag and M. T. Ozsu. </author> <title> Using Semantic Knowledge of Transactions to Increase Concurrency. </title> <journal> ACM Trans. on Database Sys., </journal> <volume> 14(4):503, </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: A few other schemes have been suggested in the context of transactional workflows. We discuss them in the rest of this section. These schemes are in some sense motivated by the concept of relative atomicity [13, 31] and breakpoints <ref> [12] </ref> discussed in the context of semantics based concurrency control in transactions for relaxing the serializability requirements by exploiting the semantics of the objects and transactions accessing the objects. A step compatibility based approach is suggested in [5] to control the interleaving of 8 steps from different workflows.
Reference: [13] <author> H. Garcia-Molina. </author> <title> Using Semantic Knowledge for Transaction Processing in a Distributed Database. </title> <journal> ACM Trans. on Database Sys., </journal> <volume> 8(2):186, </volume> <month> June </month> <year> 1983. </year>
Reference-contexts: A few other schemes have been suggested in the context of transactional workflows. We discuss them in the rest of this section. These schemes are in some sense motivated by the concept of relative atomicity <ref> [13, 31] </ref> and breakpoints [12] discussed in the context of semantics based concurrency control in transactions for relaxing the serializability requirements by exploiting the semantics of the objects and transactions accessing the objects.
Reference: [14] <author> H. Garcia-Molina and K. Salem. Sagas. </author> <booktitle> In Proc. 1987 SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 249-259, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Similarly the workflow community has borrowed ideas from ETMs, (e:g:; spheres of joint compensation [30] motivated by spheres of control [8] and Sagas <ref> [14] </ref>) in an effort to improve the correctness properties offered by WFMSs. It has also been demonstrated that the semantics of some of the ETMs can be implemented using y In this paper, general workflows are those that integrate independently developed applications.
Reference: [15] <author> D. Georgakopolous, M. Hornick, and A. Sheth. </author> <title> An Overview of Workflow Management: From 17 Process Modelling to Workflow Automation Infrastructure. </title> <booktitle> Distributed and Parallel Databases, </booktitle> <volume> 3(2) </volume> <pages> 119-152, </pages> <year> 1995. </year>
Reference-contexts: Workflow Management Systems (WFMSs) provide support for modeling, executing and monitoring the workflows. WFMSs allow the composition of large applications from smaller independently developed applications. Several prototype and commercial WFMSs have been developed and deployed <ref> [11, 33, 22, 23, 15, 36] </ref>. The workflow community primarily focused on modeling aspects of workflows, so as to accurately capture (i) the data and control flow requirements between the steps that comprise a workflow and (ii) the organizational hierarchy and staff assignments. <p> To ensure that the dependencies are satisfied, the system needs to determine the state of the objects themselves. Since DOM has mechanisms to track object accesses, dependencies can be enforced. Additional details of how TSME can be used for workflows can be found in <ref> [17, 15] </ref>. Concurrent execution of transactional workflows in discussed in [40], where a multidatabase approach is used to determine the correctness requirements for concurrent workflows. It views a workflow as a global transaction executing local transactions at different sites. <p> If all steps can be compensated then the entire workflow can be rolled back. However, in certain cases if vital steps of a workflow have already been executed, then it may not be possible to compensate them. An alternative action may be necessary. For example <ref> [15] </ref> describes an order cancellation scenario in a telecommunication service order provisioning workflow. Since certain facilities may already have been allocated for the customer, undoing the effect is complex and several choices are possible depending on which facilities have already been allocated.
Reference: [16] <author> D. Georgakopoulos, M. Hornick, P. Krychniak, and F. Manola. </author> <title> Specification and Management of Extended Transactions in a Programmable Transaction Environment. </title> <booktitle> In Proc. IEEE Int'l. Conf. on Data Eng., </booktitle> <pages> page 462, </pages> <address> Houston, TX, </address> <month> February </month> <year> 1994. </year>
Reference-contexts: The compatibility matrix must now be extended to include steps from all the workflow schemas. When this is the case, the number of step incompatibilities can be high and hence the approach may have to be refined. We return to this issue later in section 6. The TSME system <ref> [16, 17] </ref>, provides facilities for specifying workflow correctness requirements along with the workflow schemas using the Distributed Object Management (DOM) infrastructure [32]. Using the transaction specification language, dependencies can be specified between steps.
Reference: [17] <author> D. Georgakopoulos, M. F. Hornick, and F. Manola. </author> <title> Customizing Transaction Models and Mechanisms in a Programmable Environment Supporting Reliable Workflow Automation. </title> <journal> IEEE Trans. on Knowledge and Data Eng., </journal> <year> 1995. </year>
Reference-contexts: The compatibility matrix must now be extended to include steps from all the workflow schemas. When this is the case, the number of step incompatibilities can be high and hence the approach may have to be refined. We return to this issue later in section 6. The TSME system <ref> [16, 17] </ref>, provides facilities for specifying workflow correctness requirements along with the workflow schemas using the Distributed Object Management (DOM) infrastructure [32]. Using the transaction specification language, dependencies can be specified between steps. <p> To ensure that the dependencies are satisfied, the system needs to determine the state of the objects themselves. Since DOM has mechanisms to track object accesses, dependencies can be enforced. Additional details of how TSME can be used for workflows can be found in <ref> [17, 15] </ref>. Concurrent execution of transactional workflows in discussed in [40], where a multidatabase approach is used to determine the correctness requirements for concurrent workflows. It views a workflow as a global transaction executing local transactions at different sites.
Reference: [18] <author> D. Georgakopoulos, M. Rusinkiewicz, and A. Sheth. </author> <title> On Serializability of Multidatabase Transactions through Forced Local Conflicts. </title> <booktitle> In Proc. IEEE Int'l. Conf. on Data Eng., </booktitle> <pages> page 314, </pages> <address> Kobe, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Concurrent execution of transactional workflows in discussed in [40], where a multidatabase approach is used to determine the correctness requirements for concurrent workflows. It views a workflow as a global transaction executing local transactions at different sites. Then it applies a relaxation to the global serializability requirements <ref> [18] </ref> for workflows using the correctness criterion of M-serializability defined in [39]. The workflow is divided into disjoint execution-atomic units, each consisting of related steps. The correctness criterion requires that steps belonging to the same execution-atomic unit of a workflow have compatible serialization orders at all local sites they access.
Reference: [19] <author> J. Gray. </author> <title> The Transaction Concept: Virtues and Limitations. </title> <booktitle> In Proc. Int'l. Conf. on Very Large Data Bases, </booktitle> <pages> page 144, </pages> <address> Cannes, France, </address> <month> September </month> <year> 1981. </year>
Reference-contexts: Now the workflow will try to achieve the objective using the second choice and so on. However, it is important that steps that have been executed on paths that were unsuccessful be undone. This is usually achieved by compensating <ref> [19] </ref> the steps. Consider another scenario where a customer places an order and later cancels it. The action to be taken to handle a cancellation will very much depend on the state of the workflow at the time of cancellation.
Reference: [20] <author> R. Gunthor. </author> <title> The Dependency Manager. In RIDE-NDS (Interoperability of Nontraditional Database Systems), </title> <address> New Orleans, Louisiana, </address> <year> 1996. </year>
Reference-contexts: In particular, the scheduler has to determine which steps have completed, which steps have failed and which steps have to be compensated. Thus the scheduler has to deal with enormous state information especially when several thousand instances of workflows are executing concurrently. Scheduling related issues are discussed further in <ref> [3, 40, 43, 42, 20] </ref>. In section 4, we presented the execution atomicity requirements in the presence of concurrent workflows as discussed in [44, 5, 40].
Reference: [21] <author> D. Hollingsworth. </author> <title> Workflow Management Reference Model, 1994. The Workflow Management Coalition, </title> <note> Accessible via: http://www.aiai.ed.ac.uk/WfMC/. </note>
Reference-contexts: This provides flexibility since any person with that designation can execute the step rather than someone specific. All the modeling activities are performed via a workflow definition tool which is often GUI based. 2.2. Execution Support model <ref> [21] </ref> of the Worflow Management Coalition (WfMC). The definitions of workflows, steps and staff designations are all stored persistently in an underlying database commonly referred to as the workflow database. This database also stores the states of the workflows that are in progress.
Reference: [22] <author> M. Hsu. </author> <title> Special Issue on Workflow and Extended Transaction Systems. </title> <journal> Bulletin of the Technical Committee on Data Engineering, IEEE, </journal> <volume> 16(2), </volume> <year> 1993. </year>
Reference-contexts: Workflow Management Systems (WFMSs) provide support for modeling, executing and monitoring the workflows. WFMSs allow the composition of large applications from smaller independently developed applications. Several prototype and commercial WFMSs have been developed and deployed <ref> [11, 33, 22, 23, 15, 36] </ref>. The workflow community primarily focused on modeling aspects of workflows, so as to accurately capture (i) the data and control flow requirements between the steps that comprise a workflow and (ii) the organizational hierarchy and staff assignments.
Reference: [23] <author> M. </author> <title> Hsu. </title> <journal> Special Issue on Workflow Systems. Bulletin of the Technical Committee on Data Engineering, IEEE, </journal> <volume> 18(1), </volume> <year> 1995. </year>
Reference-contexts: Workflow Management Systems (WFMSs) provide support for modeling, executing and monitoring the workflows. WFMSs allow the composition of large applications from smaller independently developed applications. Several prototype and commercial WFMSs have been developed and deployed <ref> [11, 33, 22, 23, 15, 36] </ref>. The workflow community primarily focused on modeling aspects of workflows, so as to accurately capture (i) the data and control flow requirements between the steps that comprise a workflow and (ii) the organizational hierarchy and staff assignments.
Reference: [24] <author> W.W. Jin, M. Rusinkiewicz, L. Ness, and A. Sheth. </author> <title> Concurrency Control and Recovery of Multidatabase Workflows in Telecommunication Applications. </title> <booktitle> In Proc. ACM SIGMOD Conf., </booktitle> <pages> pages 456-459, </pages> <address> Washington DC, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The correctness criterion requires that steps belonging to the same execution-atomic unit of a workflow have compatible serialization orders at all local sites they access. A variation of the same scheme has been used to defined FT-serializability <ref> [24] </ref> as a correctness criterion for concurrent execution of Flex transactions [10] to implement telecommunication workflows. 9 5.
Reference: [25] <author> M. Kamath, G. Alonso, R. Gunthor, and C. Mohan. </author> <title> Providing High Availability in Workflow Management Systems. </title> <booktitle> In Proceedings of the Fifth International Conference on Extending Database Technology (EDBT-96), </booktitle> <address> Avignon, France, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: Hence it is necessary to use fault tolerance techniques to replicate the state of the workflow database so that the workflows are continued from their present states even 10 if there is a failure. Techniques and algorithms to achieve this efficiently are described in <ref> [25] </ref>. Application agents that supervise the execution of programs for performing the individual steps can also fail. This can cause problems for achieving forward recovery.
Reference: [26] <author> M. Kamath and K. Ramamritham. </author> <title> Modeling, Correctness & Systems Issues in Supporting Advanced Database Applications using Workflow Management Systems. </title> <type> Technical Report TR 95-50, </type> <institution> University of Massachusetts, Computer Science Dept., </institution> <year> 1995. </year>
Reference-contexts: This problem can be ameliorated by logging significant events that happen at the agent. Hence every application agent should have logging facilities so that when a program completes, its return status code and data are logged <ref> [26] </ref>. Later the agent can pass on the results to the workflow engine. Again if the agent fails permanently, there is no solution to handle the situation. The step will be scheduled for execution by the scheduler again.
Reference: [27] <author> M. Kamath and K. Ramamritham. </author> <title> Mechanisms for Ensuring Correctness of Workflows in the presence of Concurrency and Failures. </title> <note> Technical Report In preparation, </note> <institution> University of Massachusetts, Computer Science Dept., </institution> <year> 1996. </year>
Reference-contexts: These issues are being addressed in the context of multiple workflows in <ref> [27] </ref>. 7. Summary Workflow management offers a powerful technique to integrate and automate the different tasks of an enterprise. <p> It should be emphasized that the schemes studied in this paper are primarily concerned with transactional workflows, and additional research needs to be done to incorporate non-transactional objects and executions. Some of these issues are being 16 addressed in <ref> [27] </ref>. A proper understanding of the concepts and techniques related to preserving correctness in WFMSs by both WFMS developers and workflow designers is necessary.
Reference: [28] <author> H. F. Korth, E. Levy, and A. Silberschatz. </author> <title> A Formal Approach to Recovery by Compensating Transactions. </title> <booktitle> In Proc. Int'l. Conf. on Very Large Data Bases, </booktitle> <pages> page 95, </pages> <address> Brisbane, Australia, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: However the effect of rollbacks in one workflow on the forward/rollback execution of other concurrent workflows has received little attention. There has been some work studying the data consistency issues when compensations are performed in the presence of concurrent transactions <ref> [28] </ref>. This work has been later extended to deal with sub-transactions from global transactions in a multidatabase environment [29, 35].
Reference: [29] <author> E. Levy, H. Korth, and A. Silberschatz. </author> <title> A Theory of Relaxed Atomicity. </title> <booktitle> In Proc. of ACM SIGACTS-SIGOPS Symposium on Principles of Distributed Computing, </booktitle> <year> 1991. </year>
Reference-contexts: There has been some work studying the data consistency issues when compensations are performed in the presence of concurrent transactions [28]. This work has been later extended to deal with sub-transactions from global transactions in a multidatabase environment <ref> [29, 35] </ref>. They consider steps of three types | compensatable (steps whose effects can be undone), retriable (steps that are guaranteed to be successful when tried repeatedly) and pivot (steps that are neither compensatable nor retriable).
Reference: [30] <author> F. Leymann. </author> <title> Supporting Business Transactions via Partial Backward Recovery in Workflow Management. </title> <booktitle> In Proc. of BTW'95, </booktitle> <address> Dresden, Germany, 1995. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: The database community has applied some correctness concepts like isolation and failure handling requirements from transactions (including ETMs) to generaly workflows to create transactional workflows [41], whose steps primarily correspond to database transactions. Similarly the workflow community has borrowed ideas from ETMs, (e:g:; spheres of joint compensation <ref> [30] </ref> motivated by spheres of control [8] and Sagas [14]) in an effort to improve the correctness properties offered by WFMSs. <p> The execution support in the WFMS must ensure that the failure atomicity requirements are satisfied when a step fails or a workflow is terminated. One such facility has been developed for FlowMark in <ref> [30] </ref> and is based in the notion of spheres of joint compensation. A collection of steps in a workflow is grouped into a sphere S such that either all the steps of S complete successfully or all of them are compensated. Thus a sphere is basically a failure-atomic unit. <p> Interleaving dependencies are also important when multiple such workflows execute concurrently and a different approach is needed. The failure atomicity requirements of workflows focus on the correctness of individual workflows in the presence of failures. For example, in section 5, we discussed the techniques presented in <ref> [30] </ref> that ensure partial rollback requirements of individual workflows. However the effect of rollbacks in one workflow on the forward/rollback execution of other concurrent workflows has received little attention. There has been some work studying the data consistency issues when compensations are performed in the presence of concurrent transactions [28].
Reference: [31] <author> N. A. Lynch. </author> <title> "multilevel atomicity: A new correctness criterion for database concurrency control". </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(4), </volume> <month> December </month> <year> 1983. </year>
Reference-contexts: A few other schemes have been suggested in the context of transactional workflows. We discuss them in the rest of this section. These schemes are in some sense motivated by the concept of relative atomicity <ref> [13, 31] </ref> and breakpoints [12] discussed in the context of semantics based concurrency control in transactions for relaxing the serializability requirements by exploiting the semantics of the objects and transactions accessing the objects.
Reference: [32] <author> F. Manola, S. Heiler, D. Georgakopoulos, M. Hornick, and M. Brodie. </author> <title> Distributed Object Management. </title> <booktitle> In Int. J.of Intelligent and Cooperative Information Systems 1, </booktitle> <volume> 1, </volume> <month> March </month> <year> 1992. </year> <month> 18 </month>
Reference-contexts: We return to this issue later in section 6. The TSME system [16, 17], provides facilities for specifying workflow correctness requirements along with the workflow schemas using the Distributed Object Management (DOM) infrastructure <ref> [32] </ref>. Using the transaction specification language, dependencies can be specified between steps. Other than the state dependencies that specify a workflow structure, correctness dependencies can be specified to ensure one or more of the following: serializability, temporal correctness or cooperative correctness.
Reference: [33] <author> D.R. McCarthy and S.K. </author> <title> Sarin. </title> <journal> Workflow and Transactions in InConcert. Bulletin of the Technical Committee on Data Engineering, </journal> <volume> 16(2), </volume> <month> June </month> <year> 1993. </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: Workflow Management Systems (WFMSs) provide support for modeling, executing and monitoring the workflows. WFMSs allow the composition of large applications from smaller independently developed applications. Several prototype and commercial WFMSs have been developed and deployed <ref> [11, 33, 22, 23, 15, 36] </ref>. The workflow community primarily focused on modeling aspects of workflows, so as to accurately capture (i) the data and control flow requirements between the steps that comprise a workflow and (ii) the organizational hierarchy and staff assignments. <p> Below we survey some of the solutions that have been proposed. 7 The simplest form of support for controlling concurrent access to data from steps within a workflow or from different workflows is provided in WFMSs like InConcert <ref> [33] </ref> via check-in and check-out. This scheme is suitable for workflows in engineering environments such as CAD/CAM and CASE where decisions to access objects are more ad hoc.
Reference: [34] <author> S. Mehrotra, R. Rastogi, Y. Breitbart, H. F. Korth, and A. Silberschatz. </author> <title> The Concurrency Control Problem in Multidatabases: Characteristics and Solutions. </title> <booktitle> In Proc. ACM SIGMOD Conf., </booktitle> <pages> page 288, </pages> <address> San Diego, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Most commercial WFMSs have been supporting only such workflows. 3 workflow models [1]. Another closely related area is that of multidatabases or federated databases <ref> [6, 34] </ref> where several techniques have been developed for handling concurrent transactions whose sub-transactions access data from autonomous databases in the presence of failures. Some of these techniques have also been used for improving the correctness properties offered by transactional workflows [40].
Reference: [35] <author> S. Mehrotra, R. Rastogi, H. F. Korth, and A. Silberschatz. </author> <title> A Transaction Model for Multidatabase Systems. </title> <booktitle> In Proc. 12th Int'l. Conf. on Distr. Computing Sys., </booktitle> <pages> page 56, </pages> <address> Yokohama, Japan, </address> <month> June 9-12 </month> <year> 1992. </year>
Reference-contexts: There has been some work studying the data consistency issues when compensations are performed in the presence of concurrent transactions [28]. This work has been later extended to deal with sub-transactions from global transactions in a multidatabase environment <ref> [29, 35] </ref>. They consider steps of three types | compensatable (steps whose effects can be undone), retriable (steps that are guaranteed to be successful when tried repeatedly) and pivot (steps that are neither compensatable nor retriable). <p> They consider steps of three types | compensatable (steps whose effects can be undone), retriable (steps that are guaranteed to be successful when tried repeatedly) and pivot (steps that are neither compensatable nor retriable). A criterion called serializability with respect to compensation (SRC) is defined in <ref> [35] </ref> which precludes a multidatabase transaction from observing the changes made by another transaction only at some of the sites even though they conflict at more sites.
Reference: [36] <author> C. Mohan. </author> <title> State of the Art in Workflow Management Systems Research and Products, </title> <booktitle> 1996. Tutorial presented at ACM SIGMOD International Conference on Management of Data, </booktitle> <year> 1996. </year>
Reference-contexts: Workflow Management Systems (WFMSs) provide support for modeling, executing and monitoring the workflows. WFMSs allow the composition of large applications from smaller independently developed applications. Several prototype and commercial WFMSs have been developed and deployed <ref> [11, 33, 22, 23, 15, 36] </ref>. The workflow community primarily focused on modeling aspects of workflows, so as to accurately capture (i) the data and control flow requirements between the steps that comprise a workflow and (ii) the organizational hierarchy and staff assignments.
Reference: [37] <author> J. E. B. Moss. </author> <title> Nested transactions: An approach to reliable distributed computing. </title> <type> Technical report, PhD Thesis, </type> <institution> MIT, </institution> <address> Cambridge, MA, </address> <month> April </month> <year> 1981. </year>
Reference-contexts: However, ETMs require all the activities of a task to be transactional and enforce tight integration between the sub-transactions which are too restrictive for many applications. Hence ETMs have not been incorporated into commercial products but for some exceptions like nested transactions <ref> [37] </ref>. Fortunately, in the last few years there has been a confluence of the two approaches. The database community has applied some correctness concepts like isolation and failure handling requirements from transactions (including ETMs) to generaly workflows to create transactional workflows [41], whose steps primarily correspond to database transactions. <p> It provides flexibility in the way objects are committed. A step can delegate the responsibility of committing and aborting operations on certain objects to an ancestor either through an intermediate ancestor or directly. This model improves the concurrency within a workflow compared to the closed nested model <ref> [37] </ref>. For example, if a step commits its operations to a top-level step, its results are still internal to the workflow but they are accessible to all other steps within the workflow.
Reference: [38] <author> A. Reuter and F. Schwenkreis. </author> <title> ConTracts A Low-Level Mechanism for Building General-Purpose Workflow Management Systems. </title> <journal> Bulletin of the Technical Committee on Data Engineering, </journal> <volume> 18(1), </volume> <month> March </month> <year> 1995. </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: This type of execution atomicity is ideal for workflows in engineering environments where more sophistication is required rather than the simple check-in and check-out model. In ConTracts <ref> [44, 38] </ref>, invariant based synchronization is used to support the executability of a ConTract (workflow). This addresses the problem we discussed earlier in section 3. <p> Earlier in our discussion on execution atomicity, we described how the invariant based approach is used in ConTracts to reduce the duration of locking. Now we discuss how the same approach can be used to ensure compensability <ref> [38] </ref>. The assumption that is made is that the prerequisites to execute a compensation-step are known when the corresponding step has been executed. Hence after the execution of a step, constraints can be established on shared resources such that the executability of the compensation-step is guaranteed.
Reference: [39] <author> M. Rusinkiewicz, A. Cichocki, and P. Krychniak. </author> <title> Towards a Model for Multidatabase Transactions. </title> <journal> International Journal of Intelligent and Cooperative Information Systems, </journal> <volume> 1(3) </volume> <pages> 579-617, </pages> <year> 1992. </year>
Reference-contexts: It views a workflow as a global transaction executing local transactions at different sites. Then it applies a relaxation to the global serializability requirements [18] for workflows using the correctness criterion of M-serializability defined in <ref> [39] </ref>. The workflow is divided into disjoint execution-atomic units, each consisting of related steps. The correctness criterion requires that steps belonging to the same execution-atomic unit of a workflow have compatible serialization orders at all local sites they access. <p> In [40], the M-serializability criterion described in the context of multidatabase system <ref> [39] </ref> is used for handling interleaving of concurrent workflows. Here the system ensures that the execution order of conflicting steps belonging to the same execution-atomic units of two workflows have the same serialization order at every local site.
Reference: [40] <author> M. Rusinkiewicz and A. Sheth. </author> <title> Specification and Execution of Transactional Workflows. </title> <editor> In W. Kim, editor, </editor> <title> Modern Database Systems: The Object Model, Interoperability and Beyond. </title> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: Some of these techniques have also been used for improving the correctness properties offered by transactional workflows <ref> [40] </ref>. All these developments contributed to an increase in the robustness and reliability offered by WFMSs. This paper provides an overview of correctness issues in workflow management. <p> Since DOM has mechanisms to track object accesses, dependencies can be enforced. Additional details of how TSME can be used for workflows can be found in [17, 15]. Concurrent execution of transactional workflows in discussed in <ref> [40] </ref>, where a multidatabase approach is used to determine the correctness requirements for concurrent workflows. It views a workflow as a global transaction executing local transactions at different sites. <p> To facilitate this, the notion of committed acceptable and aborted acceptable termination states for a workflow have been proposed <ref> [40] </ref>. Given a transactional workflow specification, the set of acceptable states can be systematically determined using event algebra [42]. There are other notions such as dead-path elimination in FlowMark [11] which helps the workflow scheduler determine when a workflow is done. <p> In particular, the scheduler has to determine which steps have completed, which steps have failed and which steps have to be compensated. Thus the scheduler has to deal with enormous state information especially when several thousand instances of workflows are executing concurrently. Scheduling related issues are discussed further in <ref> [3, 40, 43, 42, 20] </ref>. In section 4, we presented the execution atomicity requirements in the presence of concurrent workflows as discussed in [44, 5, 40]. <p> Scheduling related issues are discussed further in [3, 40, 43, 42, 20]. In section 4, we presented the execution atomicity requirements in the presence of concurrent workflows as discussed in <ref> [44, 5, 40] </ref>. The invariant based approach of ConTracts [44] can be used to ensure executability of steps when other steps from concurrent workflows can access the same data item. However data inconsistency can be caused due to improper interleaving of two or more steps from different workflows. <p> Specifically, using information about the input and output parameters of a step and utilizing data/control flow information within workflows, it is possible to reduce the number of steps that might be blocked to ensure correctness, thus allowing more schedules. In <ref> [40] </ref>, the M-serializability criterion described in the context of multidatabase system [39] is used for handling interleaving of concurrent workflows. Here the system ensures that the execution order of conflicting steps belonging to the same execution-atomic units of two workflows have the same serialization order at every local site.
Reference: [41] <author> A. Sheth and M. Rusinkiewicz. </author> <title> On Transactional Workflows. </title> <journal> Bulletin of the Technical Committee on Data Engineering, </journal> <volume> 16(2), </volume> <month> June </month> <year> 1993. </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: Fortunately, in the last few years there has been a confluence of the two approaches. The database community has applied some correctness concepts like isolation and failure handling requirements from transactions (including ETMs) to generaly workflows to create transactional workflows <ref> [41] </ref>, whose steps primarily correspond to database transactions. Similarly the workflow community has borrowed ideas from ETMs, (e:g:; spheres of joint compensation [30] motivated by spheres of control [8] and Sagas [14]) in an effort to improve the correctness properties offered by WFMSs.
Reference: [42] <author> M. P. Singh. </author> <title> Synthesizing Distributed Constrained Events from Transactional Workflow Specifications. </title> <booktitle> In Proc. of Intl. Conference on Data Engineering (ICDE), </booktitle> <year> 1996. </year>
Reference-contexts: To facilitate this, the notion of committed acceptable and aborted acceptable termination states for a workflow have been proposed [40]. Given a transactional workflow specification, the set of acceptable states can be systematically determined using event algebra <ref> [42] </ref>. There are other notions such as dead-path elimination in FlowMark [11] which helps the workflow scheduler determine when a workflow is done. These techniques help the scheduler in determining when a workflow is considered complete or in an acceptable state. <p> In particular, the scheduler has to determine which steps have completed, which steps have failed and which steps have to be compensated. Thus the scheduler has to deal with enormous state information especially when several thousand instances of workflows are executing concurrently. Scheduling related issues are discussed further in <ref> [3, 40, 43, 42, 20] </ref>. In section 4, we presented the execution atomicity requirements in the presence of concurrent workflows as discussed in [44, 5, 40].
Reference: [43] <author> M.P. Singh, L.G. Meredith, C. Tomlison, </author> <title> and P.C. Attie. An Algebraic Approach for Workflow Scheduling. </title> <type> Technical Report Carnot-049-94, </type> <institution> MCC, </institution> <year> 1994. </year>
Reference-contexts: In particular, the scheduler has to determine which steps have completed, which steps have failed and which steps have to be compensated. Thus the scheduler has to deal with enormous state information especially when several thousand instances of workflows are executing concurrently. Scheduling related issues are discussed further in <ref> [3, 40, 43, 42, 20] </ref>. In section 4, we presented the execution atomicity requirements in the presence of concurrent workflows as discussed in [44, 5, 40].
Reference: [44] <author> H. Waechter and A. Reuter. </author> <title> The ConTract Model. </title> <editor> In Ahmed K. Elmagarmid, editor, </editor> <title> Database Transaction Models for Advanced Applications, </title> <booktitle> chapter 7, </booktitle> <pages> pages 219-263. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, </address> <year> 1992. </year>
Reference-contexts: This type of execution atomicity is ideal for workflows in engineering environments where more sophistication is required rather than the simple check-in and check-out model. In ConTracts <ref> [44, 38] </ref>, invariant based synchronization is used to support the executability of a ConTract (workflow). This addresses the problem we discussed earlier in section 3. <p> Scheduling related issues are discussed further in [3, 40, 43, 42, 20]. In section 4, we presented the execution atomicity requirements in the presence of concurrent workflows as discussed in <ref> [44, 5, 40] </ref>. The invariant based approach of ConTracts [44] can be used to ensure executability of steps when other steps from concurrent workflows can access the same data item. However data inconsistency can be caused due to improper interleaving of two or more steps from different workflows. <p> Scheduling related issues are discussed further in [3, 40, 43, 42, 20]. In section 4, we presented the execution atomicity requirements in the presence of concurrent workflows as discussed in [44, 5, 40]. The invariant based approach of ConTracts <ref> [44] </ref> can be used to ensure executability of steps when other steps from concurrent workflows can access the same data item. However data inconsistency can be caused due to improper interleaving of two or more steps from different workflows. A different approach is needed to handle such situations. <p> For example, due to data sharing between workflows, the execution of a pivot step in one workflow can affect the rollback of a concurrent workflow. The invariant based approach for ensuring compensatability (ability to rollback steps of a workflow) <ref> [44] </ref> is useful in situations where it suffices to ensure that the constraints hold irrespective of the type of step accessing the data. However, when workflows containing pivot steps execute concurrently a different approach is needed.
Reference: [45] <author> X/Open, </author> <title> editor. Distributed Transaction Processing: The XA Specification. </title> <publisher> X/Open Company Limited, </publisher> <address> UK, </address> <year> 1992. </year>
Reference-contexts: However, this if difficult to achieve due to the following problems. The first problem is that not all local resource managers provide the two-phase commit interface and even if they do, most do not yet conform with the XA interface standard proposed by X/Open <ref> [45] </ref>. The second problem exists because of legacy programs. Since transactions are bundled somewhere in the legacy code, it is not clear how many 12 transactions each of these programs contain and what is the status of each when a failure occurs.
Reference: [46] <author> A. Zhang, M. Nodine, B. Bhargava, and O. Bukhres. </author> <title> Ensuring Relaxed Atomicity for Flexible Transactions in Multidatabase Systems. </title> <booktitle> In Proc. 1994 SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 67-78, </pages> <year> 1994. </year>
Reference-contexts: To ensure that a workflow terminates in a proper state, it is necessary to precisely define whether the effect of a completed step should persist or be undone. Consider for example a workflow where one of several alternative paths <ref> [46] </ref> can be chosen at a decision point to achieve the same objective. After executing a few steps in the first choice, it may not be possible to complete that path due to a logical failure at a step (i:e:; 11 that path cannot be used to meet the objective). <p> Using alternative subtransactions and the notion of semi-atomicity (global transaction is allowed to commit different parts at different times), more resiliency can be achieved in handling the failure of sub-transactions with respect to an individual flexible transaction 15 <ref> [46] </ref>. In this approach, after the execution of a pivot, alternative functional paths are executed such that one of them will commit and the effects of unsuccessful paths are completely undone (compensated). But the requirements in the presence of concurrent workflows can be very complex.
References-found: 46

