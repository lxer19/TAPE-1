URL: http://www.cs.helsinki.fi/~tirri/erudit97.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~tirri/publications.html
Root-URL: 
Title: A Bayesian Approach to Discretization  
Author: Petri Kontkanen, Petri Myllymaki, Tomi Silander, Henry Tirri 
Address: Italy,  P.O.Box  FIN-00014 University of Helsinki, Finland  
Affiliation: (Bari,  Complex Systems Computation Group (CoSCo)  Department of Computer Science  
Note: To be presented at the European Symposium on Intelligent Techniques  
Email: Email: cosco@cs.Helsinki.FI,  
Phone: 26,  Phone: +358 9 708 44173, Fax: +358 9 708 44441  
Web: URL: http://www.cs.Helsinki.FI/research/cosco/  
Date: March 1997).  
Abstract: The performance of many machine learning algorithms can be substantially improved with a proper discretization scheme. In this paper we describe a theoretically rigorous approach to discretization of continuous attribute values, based on a Bayesian clustering framework. The method produces a probabilistic scoring metric for different discretizations, and it can be combined with various types of learning algorithms working on discrete data. The approach is validated by demonstrating empirically the performance improvement of the Naive Bayes classifier when Bayesian discretization is used instead of the standard equal frequency interval discretization. 
Abstract-found: 1
Intro-found: 1
Reference: [Dempster et al., 1977] <author> Dempster, A., Laird, N., and Rubin, D. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39(1) </volume> <pages> 1-38. </pages>
Reference-contexts: The optimal discretization is thus found by searching for a clustering Q i giving the maximum value for the posterior. In our empirical tests in Section 3 we used the well-known K-means algorithm, which is a special case in the more general family of Expectation-Maximization (EM) algorithms <ref> [Dempster et al., 1977] </ref>. This algorithm can be outlined as follows: 1. Generate a random initial clustering Q (0) 2. Determine the corresponding MAP model ^ fi (t) . 3. Cluster the data using model ^ fi (t) . 4. Iterate until convergence. 5.
Reference: [Dougherty et al., 1995] <author> Dougherty, J., Kohavi, R., and Sahami, M. </author> <year> (1995). </year> <title> Supervised and unsupervised dis-cretization of continuous features. </title> <editor> In Prieditis, A., editor, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 194-202. </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Consequently, in order to be able to use such algorithms, a discretization process is needed. Continuous variable discretization has received significant attention recently [Elomaa and Rousu, 1996, Fayyad and Irani, 1993, Friedman and Goldszmidt, 1996]. A classification and a comparison between some of the approaches is presented in <ref> [Dougherty et al., 1995] </ref>. Intuitively, a good discretization is a scheme in which the discrete values represent properly the information in the original continuous values with respect to the task in question. <p> The results suggest that the Bayesian discretization scheme improves the predictive accuracy with both the MAP and evidence inference. 4 CONCLUSION As discussed in <ref> [Dougherty et al., 1995] </ref>, the performance of many machine learning algorithms can be substantially improved with a proper discretization scheme. In this paper we have described a theoretically rigorous probabilistic approach to discretization of continuous attribute values.
Reference: [Elomaa and Rousu, 1996] <author> Elomaa, T. and Rousu, J. </author> <year> (1996). </year> <title> General and efficient multisplitting of numerical attributes. </title> <type> Technical Report C-1996-82, </type> <institution> University of Helsinki, Department of Computer Science. </institution>
Reference: [Fayyad and Irani, 1993] <author> Fayyad, U. and Irani, K. </author> <year> (1993). </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1022-1027, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [Friedman and Goldszmidt, 1996] <author> Friedman, N. and Goldszmidt, M. </author> <year> (1996). </year> <title> Discretizing continuous attributes while learning Bayesian networks. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference (to appear). </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: One theoretically sound approach to good prediction performance is to form the predictive distribution of the attribute in question (see the discussion and empirical results in [Tirri et al., 1996]). Therefore the Bayesian framework offers a natural means for discretization that matches the probabilistic prediction. In <ref> [Friedman and Goldszmidt, 1996] </ref> this Bayesian discretization approach was left as an open problem. A solution to this problem is outlined in Section 2. <p> A natural scoring metric is in this case the posterior P (T jD i ) / P (D i jT )P (T ). However, specification of appropriate likelihood and prior densities is not an easy task, as observed already in <ref> [Friedman and Goldszmidt, 1996] </ref>. We outline here a solution to this problem. Instead of considering all the threshold value sets T , the basic idea is to investigate all the possible clusterings of the sample D i , i.e., assignments of the sample values to the intervals. <p> In this paper we have described a theoretically rigorous probabilistic approach to discretization of continuous attribute values. The paper answers the open problem suggested recently in <ref> [Friedman and Goldszmidt, 1996] </ref>, the problem of developing a scoring metric for discretization based on the Bayesian approach, as opposed to the MDL-principle [Rissanen, 1989] used in [Friedman and Goldszmidt, 1996]. <p> The paper answers the open problem suggested recently in <ref> [Friedman and Goldszmidt, 1996] </ref>, the problem of developing a scoring metric for discretization based on the Bayesian approach, as opposed to the MDL-principle [Rissanen, 1989] used in [Friedman and Goldszmidt, 1996]. The method described here is based on a clustering approach, where instead of trying to explicitly determine good threshold values for discretization, the idea is to evaluate (in the Bayesian setting) different partitionings of the data, which then implicitly determine the threshold values.
Reference: [Kontkanen et al., 1997] <author> Kontkanen, P., Myllymaki, P., Silander, T., Tirri, H., and Grunwald, P. </author> <year> (1997). </year> <title> Comparing predictive inference methods for discrete domains. </title> <booktitle> In Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 311-318, </pages> <address> Ft. Lauderdale, Florida. </address>
Reference-contexts: Choose the final threshold values T . The detailed derivation of the update formulas involves somewhat complicated notation, and is omitted here. 3 EMPIRICAL RESULTS In order to validate our approach, we implemented the Bayesian discretization algorithm and evaluated the prediction performance of the Naive Bayes classifier (see e.g., <ref> [Kontkanen et al., 1997] </ref>) using both the equal frequency interval discretization and the Bayesian discretization described above. As an illustrative example, Figures 1 and 2 present results with one of the commonly used public domain classification data sets from the UC Irvine data repository, the Australian data set 1 . <p> This discretization was then used for generating the initial clustering Q (0) i for the K-means algorithm, and the algorithm was run until convergence. After each iteration, the resulting discretized dataset was used for constructing Bayesian predictive distributions, as showed in <ref> [Kontkanen et al., 1997] </ref>. The predictive accuracy of the resulting models was tested by using 10-fold crossvalidation with both the log-score (Figure 1) and the 0/1-score (Figure 2). <p> The MAP curves were obtained by using the single MAP model for the discretized dataset (MAP inference), whereas the EV curves were obtained by integrating over all possible models using the formulas presented in <ref> [Kontkanen et al., 1997] </ref> (evidence inference). The results suggest that the Bayesian discretization scheme improves the predictive accuracy with both the MAP and evidence inference. 4 CONCLUSION As discussed in [Dougherty et al., 1995], the performance of many machine learning algorithms can be substantially improved with a proper discretization scheme.
Reference: [Rissanen, 1989] <author> Rissanen, J. </author> <year> (1989). </year> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing Company, </publisher> <address> New Jersey. </address>
Reference-contexts: In this paper we have described a theoretically rigorous probabilistic approach to discretization of continuous attribute values. The paper answers the open problem suggested recently in [Friedman and Goldszmidt, 1996], the problem of developing a scoring metric for discretization based on the Bayesian approach, as opposed to the MDL-principle <ref> [Rissanen, 1989] </ref> used in [Friedman and Goldszmidt, 1996].
Reference: [Tirri et al., 1996] <author> Tirri, H., Kontkanen, P., and Myllymaki, P. </author> <year> (1996). </year> <title> Probabilistic instance-based learning. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 507-515. </pages> <publisher> Morgan Kaufmann Publishers. </publisher> <pages> 4 </pages>
Reference-contexts: One theoretically sound approach to good prediction performance is to form the predictive distribution of the attribute in question (see the discussion and empirical results in <ref> [Tirri et al., 1996] </ref>). Therefore the Bayesian framework offers a natural means for discretization that matches the probabilistic prediction. In [Friedman and Goldszmidt, 1996] this Bayesian discretization approach was left as an open problem. A solution to this problem is outlined in Section 2.
References-found: 8

