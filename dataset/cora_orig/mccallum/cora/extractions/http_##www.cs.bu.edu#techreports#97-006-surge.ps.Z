URL: http://www.cs.bu.edu/techreports/97-006-surge.ps.Z
Refering-URL: http://cs-www.bu.edu/techreports/Home.html
Root-URL: 
Email: fbarford,crovellag@cs.bu.edu  
Title: Generating Representative Web Workloads for Network and Server Performance Evaluation  
Author: Paul Barford and Mark Crovella 
Date: REVISED December 31, 1997  
Address: 111 Cummington St, Boston, MA 02215  
Affiliation: Computer Science Department Boston University  
Pubnum: BU-CS-97-006  
Abstract: One role for workload generation is as a means for understanding how servers and networks respond to variation in load. This enables management and capacity planning based on current and projected usage. This paper applies a number of observations of Web server usage to create a realistic Web workload generation tool which mimics a set of real users accessing a server. The tool, called Surge (Scalable URL Reference Generator) generates references matching empirical measurements of 1) server file size distribution; 2) request size distribution; 3) relative file popularity; 4) embedded file references; 5) temporal locality of reference; and 6) idle periods of individual users. This paper reviews the essential elements required in the generation of a representative Web workload. It also addresses the technical challenges to satisfying this large set of simultaneous constraints on the properties of the reference stream, the solutions we adopted, and their associated accuracy. Finally, we present evidence that Surge exercises servers in a manner significantly different from other Web server benchmarks.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Virgilio Almeida, Azer Bestavros, Mark Crovella, and Adriana de Oliveira. </author> <booktitle> Characterizing reference locality in the WWW. In Proceedings of 1996 International Conference on Parallel and Distributed Information Systems (PDIS '96), </booktitle> <pages> pages 92-103, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: The distribution of popularity has a strong effect on the behavior of caches (e.g., buffer caches in the file system), since popular files will typically tend to remain in caches. Popularity distribution for files on Web servers has been shown to commonly follow Zipf's Law <ref> [21, 1, 6] </ref>. Zipf's Law states that if files are ordered from most popular to least popular, then the number of references to a file (P ) tends to be inversely proportional to its rank (r). That is: P = kr 1 for some positive constant k. <p> Assume that the files are stored in a push-down stack. Considering each request in order, move the requested file to the top of the stack, pushing other files down. The depth in the stack at which each requested file is found is the request's stack distance <ref> [1, 13] </ref>. Stack distance serves to capture temporal locality because small stack distances correspond to requests for the same file that occur close to each other in the reference stream. <p> Thus the distribution of stack distances for a Web request sequence serves as a measure of the temporal locality present in the sequence. Typical distributions for request sequences arriving at Web servers have been studied in <ref> [1] </ref>; results show that these are well modeled using lognormal distributions. Since the lognormal distribution has most of its mass concentrated at small values, this is indicative that significant temporal locality is often present in Web request sequences. 6) OFF Times. <p> Temporal locality can be analyzed using the distribution of stack distances. This distribution is an indication of temporal locality because stack distance measures the number of intervening references between references to the same document <ref> [1] </ref>. To obey temporal locality, the sequence of document requests must be arranged such that when cast in a push-down stack the resulting distribution of stack distances will match empirically measured distributions.
Reference: [2] <author> M.F. Arlitt and C.L. Williamson. </author> <title> Web server workload characterization: The search for invariants. </title> <booktitle> In Proceeding of the ACM SIGMETRICS '96 Conference, </booktitle> <address> Philadelphia, PA, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: This indicates that it is important to pay attention to those prop-erties of Web reference streams that have been shown to exhibit high variability, such as request interarrivals [8, 10] and file sizes <ref> [5, 2] </ref>. The second unusual feature of Web workloads is that network traffic due to Web requests can be self-similar|i.e, traffic can show significant variability over a wide range of scales. <p> In order to exercise the server's filesystem properly, it is important that the collection of files stored on the server agree in distribution with empirical measurements. As noted in <ref> [5, 2] </ref>, these distributions can be heavy-tailed, meaning that the server's filesystem must deal with highly variable file sizes. 2) Request Sizes. In order to exercise the network properly, the set of files transferred should match empirical size measurements. We call this set the requests. <p> File Sizes. Completion of the file size model began with the assumption that the heavy tailed characterization of the distribution as described in <ref> [5, 2] </ref> was accurate. The model was then developed as a hybrid consisting of a new distribution for the body, combined with a Pareto distribution for the upper tail.
Reference: [3] <author> Henry Braun. </author> <title> A simple method for testing goodness of fit in the presence of nuisance parameters. </title> <journal> Journal of the Royal Statistical Society, </journal> <year> 1980. </year>
Reference-contexts: The failure of the A 2 test was due primarily to the fact that a fairly large data set was used for the test; this is a common problem with EDF tests [7, 17]. Therefore a method of testing goodness of fit using random sub-samples (as described in <ref> [17, 3] </ref>) was used, which indicated a good fit between the EDF of file sizes and the lognormal distribution. Censoring techniques were employed to determine where to split between the lognormal distribution for the body and the heavy tailed distribution in the tail.
Reference: [4] <institution> The Standard Performance Evaluation Corporation. Specweb96. </institution> <note> http://www.specbench.org/org/web96/. </note>
Reference-contexts: Third, we describe the issues involved in incorporating these models into a single output reference stream, and how we resolved those issues. The final result, Surge, shows a number of properties that are quite different from a common Web workload generator: SPECweb96 <ref> [4] </ref>. We configured Surge and SPECweb96 to exercise a server at equal rates measured in terms of bytes transferred per second. Under these conditions, Surge places much higher demands on the server's CPU, and the typical number of open connections on the server is much higher. <p> Each UE exhibits long periods of activity followed by long periods of inactivity. This is quite different from approaches typically taken in other Web workload generators. Most other workload generators send requests without preserving consistent properties of OFF times <ref> [4, 15, 19] </ref>. The general approach used in these workload generators is simply to make requests for files from a server as quickly as possible. We will show in Section 5 that ignoring OFF times destroys self-similarity in the resulting traffic at high loads.
Reference: [5] <author> M.E. Crovella and A. Bestavros. </author> <title> Self-similarity in world wide web traffic: Evidence and possible causes. </title> <booktitle> In Proceedings of the 1996 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: This indicates that it is important to pay attention to those prop-erties of Web reference streams that have been shown to exhibit high variability, such as request interarrivals [8, 10] and file sizes <ref> [5, 2] </ref>. The second unusual feature of Web workloads is that network traffic due to Web requests can be self-similar|i.e, traffic can show significant variability over a wide range of scales. <p> In order to exercise the server's filesystem properly, it is important that the collection of files stored on the server agree in distribution with empirical measurements. As noted in <ref> [5, 2] </ref>, these distributions can be heavy-tailed, meaning that the server's filesystem must deal with highly variable file sizes. 2) Request Sizes. In order to exercise the network properly, the set of files transferred should match empirical size measurements. We call this set the requests. <p> We call this set the requests. The distribution of requests can be quite different from the distribution of files because requests can result in any individual file being transferred multiple times, or not at all. Again, empirical measurements suggest that the set of request sizes can show heavy tails <ref> [5] </ref>. Note that the difference between the sets we call file sizes and request sizes corresponds to differences in distribution between what is stored in the filesystem of the server and what is transferred over the network from the server. 3) Popularity. <p> As described in the previous section, accurate modeling of inactive OFF times is necessary to capture the bursty nature of an individual Web user's requests. Proper characterization of active OFF times is necessary to replicate the transfer of Web objects. Previous work has measured OFF times <ref> [5] </ref> and the related question of interarrival times of HTTP requests at a server [8]. In summary, the notion of user equivalents along with the six distributions discussed in this section constitute the definition of the Surge model for Web workload generation. <p> File Sizes on the server were identified as a key characteristic of Web workloads in Section 2. File sizes on servers were studied in <ref> [5] </ref> but that work concentrated only on the tail of the distribution. For Surge we need a model for the distribution that is accurate for the body as well as the tail. Active OFF Times have not been specifically addressed in other studies. <p> For Surge we need a model for the distribution that is accurate for the body as well as the tail. Active OFF Times have not been specifically addressed in other studies. Client OFF times in general are described in <ref> [5] </ref>. However, no attempt to describe Active OFF time as a separate distribution was made; thus a model for Active OFF time was required. Finally, a model for the number of Embedded References in a document was required. <p> File Sizes. Completion of the file size model began with the assumption that the heavy tailed characterization of the distribution as described in <ref> [5, 2] </ref> was accurate. The model was then developed as a hybrid consisting of a new distribution for the body, combined with a Pareto distribution for the upper tail. <p> Network Impacts. Finally we consider differences in network traffic generated by the two work-loads. As described in Section 1 we are particularly interested in whether the two workloads generate traffic that is self-similar [12] since this property has been shown to be present in Web traffic <ref> [5] </ref> and has significant implications for network performance [9, 16]. Self-similarity in the network traffic context refers to the scaling of variability (burstiness).
Reference: [6] <author> C.A. Cunha, A. Bestavros, </author> <title> and M.E. Crovella. Characteristics of www client-based traces. </title> <type> Technical Report TR-95-010, </type> <institution> Boston University Department of Computer Science, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: The distribution of popularity has a strong effect on the behavior of caches (e.g., buffer caches in the file system), since popular files will typically tend to remain in caches. Popularity distribution for files on Web servers has been shown to commonly follow Zipf's Law <ref> [21, 1, 6] </ref>. Zipf's Law states that if files are ordered from most popular to least popular, then the number of references to a file (P ) tends to be inversely proportional to its rank (r). That is: P = kr 1 for some positive constant k. <p> In addition to the distribution of stack distances, a method for request sequence generation must distribute references to each file as evenly as possible throughout the entire sequence. 4 Solutions Adopted in Surge 4.1 New Distributional Models The BU client trace data sets discussed in <ref> [6] </ref> were used to develop the three models required to complete Surge. These traces record the behavior of users accessing the Web over a period of two months. In general, it is desirable that a large number of datasets from different environments be examined to determine representative distributions.
Reference: [7] <author> R. B. D'Agostino and M. A. Stephens, </author> <title> editors. Goodness-of-Fit Techniques. </title> <publisher> Marcel Dekker, Inc., </publisher> <year> 1986. </year>
Reference-contexts: These methods, however, do not distinguish between two closely fitting distributions nor do they provide any level of confidence in the fit of the model. To address this drawback one can use goodness-of-fit tests <ref> [7, 17] </ref>. However, these tests also present a number of problems. Methods which place data into bins (such as Chi-Squared tests) suffer from inaccuracies due to the choice of bin size. <p> Thus, Surge has been developed as a highly parameterizable tool. To develop these models, we used standard statistical methods, similar to those used in [17]. We used the Anderson-Darling (A 2 ) empirical distribution function (EDF) test for goodness of fit 7 (recommended in <ref> [7, 17] </ref>) and the 2 test (described in [18]) to compare how well analytic models describe an empirical data set. File Sizes. Completion of the file size model began with the assumption that the heavy tailed characterization of the distribution as described in [5, 2] was accurate. <p> However, the A 2 showed no significance in terms of goodness of fit. The failure of the A 2 test was due primarily to the fact that a fairly large data set was used for the test; this is a common problem with EDF tests <ref> [7, 17] </ref>. Therefore a method of testing goodness of fit using random sub-samples (as described in [17, 3]) was used, which indicated a good fit between the EDF of file sizes and the lognormal distribution.
Reference: [8] <author> S. Deng. </author> <title> Empirical model of WWW document arrivals at access link. </title> <booktitle> In Proceedings of the 1996 IEEE International Conference on Communication, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: This indicates that it is important to pay attention to those prop-erties of Web reference streams that have been shown to exhibit high variability, such as request interarrivals <ref> [8, 10] </ref> and file sizes [5, 2]. The second unusual feature of Web workloads is that network traffic due to Web requests can be self-similar|i.e, traffic can show significant variability over a wide range of scales. <p> Proper characterization of active OFF times is necessary to replicate the transfer of Web objects. Previous work has measured OFF times [5] and the related question of interarrival times of HTTP requests at a server <ref> [8] </ref>. In summary, the notion of user equivalents along with the six distributions discussed in this section constitute the definition of the Surge model for Web workload generation. These characteristics represent a large set of contraints on the properties of the workload generated by Surge.
Reference: [9] <author> A. Erramilli, O. Narayan, and W. Willinger. </author> <title> Experimental queueing analysis with long-range dependent packet traffic. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 4(2) </volume> <pages> 209-223, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: The second unusual feature of Web workloads is that network traffic due to Web requests can be self-similar|i.e, traffic can show significant variability over a wide range of scales. Self-similarity in traffic has been shown to have a significant negative impact on network performance <ref> [9, 16] </ref>, so it is an important feature to capture in a synthethic workload. To capture these properties in a workload generator, one of two approaches could be used: a trace-based approach or an analytic approach. <p> As described in Section 1 we are particularly interested in whether the two workloads generate traffic that is self-similar [12] since this property has been shown to be present in Web traffic [5] and has significant implications for network performance <ref> [9, 16] </ref>. Self-similarity in the network traffic context refers to the scaling of variability (burstiness). A timeseries X t ; t = 1; 2; ::: is said to be exactly second-order self-similar if X t = m H i=m (t1)+1 where d = means equality in distribution.
Reference: [10] <author> A. Feldmann. </author> <title> Modelling characteristics of tcp connections. </title> <type> Technical report, </type> <institution> AT&T Laboratories, </institution> <year> 1996. </year>
Reference-contexts: This indicates that it is important to pay attention to those prop-erties of Web reference streams that have been shown to exhibit high variability, such as request interarrivals <ref> [8, 10] </ref> and file sizes [5, 2]. The second unusual feature of Web workloads is that network traffic due to Web requests can be self-similar|i.e, traffic can show significant variability over a wide range of scales.
Reference: [11] <author> N. Johnson, S. Kotz, and N. Balakrishnan. </author> <title> Discrete Univariate Distributions. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1995. </year>
Reference: [12] <author> W.E. Leland, M.S. Taqqu, W. Willinger, and D.V. Wilson. </author> <title> On the self-similar nature of ethernet traffic (extended version). </title> <journal> IEEE/ACM Transactions on Networking, </journal> <pages> pages 2 1-15, </pages> <year> 1994. </year>
Reference-contexts: Network Impacts. Finally we consider differences in network traffic generated by the two work-loads. As described in Section 1 we are particularly interested in whether the two workloads generate traffic that is self-similar <ref> [12] </ref> since this property has been shown to be present in Web traffic [5] and has significant implications for network performance [9, 16]. Self-similarity in the network traffic context refers to the scaling of variability (burstiness).
Reference: [13] <author> R. Mattson, J. Gecsei, D. Slutz, and I. Traiger. </author> <title> Evaluation techniques and storage hierarchies. </title> <journal> IBM Systems Journal, </journal> <volume> 9 </volume> <pages> 78-117, </pages> <year> 1970. </year>
Reference-contexts: Assume that the files are stored in a push-down stack. Considering each request in order, move the requested file to the top of the stack, pushing other files down. The depth in the stack at which each requested file is found is the request's stack distance <ref> [1, 13] </ref>. Stack distance serves to capture temporal locality because small stack distances correspond to requests for the same file that occur close to each other in the reference stream.
Reference: [14] <author> J.C. Mogul. </author> <title> Network behavior of a busy web server and its clients. </title> <type> Technical Report WRL 95/5, </type> <institution> DEC Western Research Laboratory, </institution> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference-contexts: First, empirical studies of operating Web servers have shown that they experience highly variable demands, which is exhibited as variability in CPU loads and number of 1 Supported in part by NSF Grants CCR-9501822 and CCR-9706685 and by Hewlett-Packard Laboratories. 1 open connections (e.g., see <ref> [14] </ref>). This indicates that it is important to pay attention to those prop-erties of Web reference streams that have been shown to exhibit high variability, such as request interarrivals [8, 10] and file sizes [5, 2]. <p> For this reason we restrict ourselves to runs no greater than 30 minutes. Note that even over intervals as short as these, Web servers can show high variability in many system metrics <ref> [14] </ref>. The workload generated by SPECweb96 depends on two user-defined parameters: a target number of HTTP operations per second, and the number of threads used to make requests. <p> Note that figures in the two rows use different scales on the y axis. This figure shows the large fluctuations in the number of open connections for the Surge workload. Maintaining and servicing a large number of open connections is computationally expensive, as pointed out in <ref> [14] </ref>. Clearly the difference in the number of active connections is likely to be a contributing factor in the CPU utilization differences seen between the two workloads (Figure 5). 13 lower: SPECweb96).
Reference: [15] <institution> University of Minnesota. </institution> <note> Gstone version 1. http://web66.coled.umn.edu/gstone/info.html. 17 </note>
Reference-contexts: Each UE exhibits long periods of activity followed by long periods of inactivity. This is quite different from approaches typically taken in other Web workload generators. Most other workload generators send requests without preserving consistent properties of OFF times <ref> [4, 15, 19] </ref>. The general approach used in these workload generators is simply to make requests for files from a server as quickly as possible. We will show in Section 5 that ignoring OFF times destroys self-similarity in the resulting traffic at high loads. <p> Note that in this respect, the source behavior of SPECweb96 is similar to other Web workload generators whose general goals are to make requests from servers as quickly as possible <ref> [15, 19] </ref>.
Reference: [16] <author> Kihong Park, Gi Tae Kim, and Mark E. Crovella. </author> <title> On the relationship between file sizes, transport protocols, and self-similar network traffic. </title> <booktitle> In Proceedings of the Fourth International Conference on Network Protocols (ICNP'96), </booktitle> <pages> pages 171-180, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: The second unusual feature of Web workloads is that network traffic due to Web requests can be self-similar|i.e, traffic can show significant variability over a wide range of scales. Self-similarity in traffic has been shown to have a significant negative impact on network performance <ref> [9, 16] </ref>, so it is an important feature to capture in a synthethic workload. To capture these properties in a workload generator, one of two approaches could be used: a trace-based approach or an analytic approach. <p> As described in Section 1 we are particularly interested in whether the two workloads generate traffic that is self-similar [12] since this property has been shown to be present in Web traffic [5] and has significant implications for network performance <ref> [9, 16] </ref>. Self-similarity in the network traffic context refers to the scaling of variability (burstiness). A timeseries X t ; t = 1; 2; ::: is said to be exactly second-order self-similar if X t = m H i=m (t1)+1 where d = means equality in distribution.
Reference: [17] <author> Vern Paxson. </author> <title> Empirically-derived analytic models of wide-area tcp connections. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <year> 1994. </year>
Reference-contexts: These methods, however, do not distinguish between two closely fitting distributions nor do they provide any level of confidence in the fit of the model. To address this drawback one can use goodness-of-fit tests <ref> [7, 17] </ref>. However, these tests also present a number of problems. Methods which place data into bins (such as Chi-Squared tests) suffer from inaccuracies due to the choice of bin size. <p> Thus, Surge has been developed as a highly parameterizable tool. To develop these models, we used standard statistical methods, similar to those used in <ref> [17] </ref>. We used the Anderson-Darling (A 2 ) empirical distribution function (EDF) test for goodness of fit 7 (recommended in [7, 17]) and the 2 test (described in [18]) to compare how well analytic models describe an empirical data set. File Sizes. <p> Thus, Surge has been developed as a highly parameterizable tool. To develop these models, we used standard statistical methods, similar to those used in [17]. We used the Anderson-Darling (A 2 ) empirical distribution function (EDF) test for goodness of fit 7 (recommended in <ref> [7, 17] </ref>) and the 2 test (described in [18]) to compare how well analytic models describe an empirical data set. File Sizes. Completion of the file size model began with the assumption that the heavy tailed characterization of the distribution as described in [5, 2] was accurate. <p> However, the A 2 showed no significance in terms of goodness of fit. The failure of the A 2 test was due primarily to the fact that a fairly large data set was used for the test; this is a common problem with EDF tests <ref> [7, 17] </ref>. Therefore a method of testing goodness of fit using random sub-samples (as described in [17, 3]) was used, which indicated a good fit between the EDF of file sizes and the lognormal distribution. <p> The failure of the A 2 test was due primarily to the fact that a fairly large data set was used for the test; this is a common problem with EDF tests [7, 17]. Therefore a method of testing goodness of fit using random sub-samples (as described in <ref> [17, 3] </ref>) was used, which indicated a good fit between the EDF of file sizes and the lognormal distribution. Censoring techniques were employed to determine where to split between the lognormal distribution for the body and the heavy tailed distribution in the tail.
Reference: [18] <author> S. Pederson and M. Johnson. </author> <title> Estimating model discrepancy. </title> <journal> Technometrics, </journal> <year> 1990. </year>
Reference-contexts: To develop these models, we used standard statistical methods, similar to those used in [17]. We used the Anderson-Darling (A 2 ) empirical distribution function (EDF) test for goodness of fit 7 (recommended in [7, 17]) and the 2 test (described in <ref> [18] </ref>) to compare how well analytic models describe an empirical data set. File Sizes. Completion of the file size model began with the assumption that the heavy tailed characterization of the distribution as described in [5, 2] was accurate.
Reference: [19] <author> Gene Trent and Mark Sake. Webstone: </author> <title> The first generation in http server benchmarking, </title> <month> February </month> <year> 1995. </year> <title> Silicon Graphics White Paper. </title>
Reference-contexts: Each UE exhibits long periods of activity followed by long periods of inactivity. This is quite different from approaches typically taken in other Web workload generators. Most other workload generators send requests without preserving consistent properties of OFF times <ref> [4, 15, 19] </ref>. The general approach used in these workload generators is simply to make requests for files from a server as quickly as possible. We will show in Section 5 that ignoring OFF times destroys self-similarity in the resulting traffic at high loads. <p> Note that in this respect, the source behavior of SPECweb96 is similar to other Web workload generators whose general goals are to make requests from servers as quickly as possible <ref> [15, 19] </ref>.
Reference: [20] <author> Walter Willinger, Murad S. Taqqu, Robert Sherman, and Daniel V. Wilson. </author> <title> Self-similarity through high-variability: Statistical analysis of Ethernet LAN traffic at the source level. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 5(1) </volume> <pages> 71-86, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: Hence variability in the resulting aggregate traffic is reduced as well. In contrast, the traffic generated by Surge comes from constant-variability sources; it is the number of such sources that increases as workload intensity increases. Theoretical work <ref> [20] </ref> has shown that when transfer durations are heavy-tailed (as they are in Surge) this condition is sufficient for the generation of self-similar traffic. As a result we expect that Surge will generally produce self-similar network traffic under conditions of both high and low workload intensity.
Reference: [21] <author> G. K. Zipf. </author> <title> Human Behavior and the Principle of Least-Effort. </title> <publisher> Addison-Wesley, </publisher> <address> Cambridge, MA, </address> <year> 1949. </year> <month> 18 </month>
Reference-contexts: The distribution of popularity has a strong effect on the behavior of caches (e.g., buffer caches in the file system), since popular files will typically tend to remain in caches. Popularity distribution for files on Web servers has been shown to commonly follow Zipf's Law <ref> [21, 1, 6] </ref>. Zipf's Law states that if files are ordered from most popular to least popular, then the number of references to a file (P ) tends to be inversely proportional to its rank (r). That is: P = kr 1 for some positive constant k.
References-found: 21

