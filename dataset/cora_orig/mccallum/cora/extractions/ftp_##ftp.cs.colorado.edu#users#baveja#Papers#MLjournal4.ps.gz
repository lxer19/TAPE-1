URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/MLjournal4.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Email: baveja@cs.colorado.edu  dayan@ai.mit.edu  
Title: Analytical Mean Squared Error Curves for Temporal Difference Learning  
Author: SATINDER SINGH PETER DAYAN Editor: Andrew G. Barto 
Keyword: reinforcement learning, temporal difference, Monte Carlo, MSE, bias, variance, eligibility trace, Markov reward process  
Address: Boulder, CO 80309-0430  Cambridge, MA 02139  
Affiliation: Department of Computer Science University of Colorado  Department of Brain and Cognitive Sciences Massachusetts Institute of Technology  
Note: 1-38 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: We provide analytical expressions governing changes to the bias and variance of the lookup table estimators provided by various Monte Carlo and temporal difference value estimation algorithms with o*ine updates over trials in absorbing Markov reward processes. We have used these expressions to develop software that serves as an analysis tool: given a complete description of a Markov reward process, it rapidly yields an exact mean-square-error curve, the curve one would get from averaging together sample mean-square-error curves from an infinite number of learning trials on the given problem. We use our analysis tool to illustrate classes of mean-square-error curve behavior in a variety of example reward processes, and we show that although the various temporal difference algorithms are quite sensitive to the choice of step-size and eligibility-trace parameters, there are values of these parameters that make them similarly competent, and generally good. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barnard, E. </author> <year> (1993). </year> <title> Temporal-difference methods and Markov models. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23 (2), </volume> <pages> 357-365. </pages>
Reference: <author> Barto, A. G. and Duff, M. </author> <year> (1994). </year> <title> Monte Carlo matrix inversion and reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 687-694, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Barto, A. G., Sutton, R. S., and Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13, </volume> <pages> 835-846. </pages> <note> 38 S. </note> <author> SINGH AND P. DAYAN Bucklew, J. A. </author> <year> (1990). </year> <title> Large Deviation Techniques in Decision, Simulation and Estimation. </title> <address> New York: </address> <publisher> Wiley-Interscience. </publisher>
Reference: <author> Dayan, P. </author> <year> (1992). </year> <title> The convergence of TD() for general . Machine Learning, </title> <type> 8 (3/4), </type> <pages> 341-362. </pages>
Reference: <author> Dayan, P. and Sejnowski, T. </author> <year> (1994). </year> <title> TD() converges with probability 1. </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <pages> 295-301. </pages>
Reference: <author> Haussler, D., Kearns, M., Seung, H. S., and Tishby, N. </author> <year> (1994). </year> <title> Rigorous learning curve bounds from statistical mechanics. </title> <booktitle> In Proceedings of the 7th Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 76-87, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: For most such algorithms, a theory of asymptotic convergence with probability one is available under suitable conditions on algorithm parameters. However, what is not available is a theory of learning behavior of the kind that is available in some supervised learning problems <ref> (e.g., Haussler et al., 1994) </ref>.
Reference: <author> Jaakkola, T., Jordan, M. I., and Singh, S. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6 (6), </volume> <pages> 1185-1201. </pages>
Reference: <author> Saul, L. K. and Singh, S. </author> <year> (1996). </year> <title> Learning curves bounds for Markov decision processes with undiscounted rewards. </title> <booktitle> In Proceedings of COLT. </booktitle>
Reference: <author> Singh, S. and Sutton, R. S. </author> <year> (1996). </year> <title> Reinforcement learning with replacing eligibility traces. </title> <journal> Machine Learning, </journal> <volume> Vol. 22, </volume> <pages> 123-158. </pages>
Reference-contexts: Monte Carlo (MC) Monte Carlo algorithms use the terminal payoff that results from a trial to define the ffi in Equation 1. Therefore in MC algorithms the estimated value of one state is unaffected by the estimated value of any other state. We study two MC algorithms <ref> (Singh & Sutton, 1996) </ref>: first-visit MC: v i (t) = v i (t 1) + ff (t)K i (t) (r (t) v i (t 1)) ; and (3) every-visit MC: v i (t) = v i (t 1) + ff (t) i (t) (r (t) v i (t 1)) : (4) <p> Of course, in practice the knowledge required to choose optimal, or even greedy, ff and is not available and so for practical choices of ff and , the differences may be more prominent <ref> (e.g., Singh & Sutton, 1996) </ref>. Higher 20 S. SINGH AND P. DAYAN a) b) c) g) h) i) 5 state Markov reward process whose cyclicity (probability of revisits) is controlled by parameters and c. The parameter c was fixed to 0:9. Initial bias (fi) was controlled separately.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: Temporal Difference (TD) The main difference between TD algorithms <ref> (Sutton, 1988) </ref> and MC algorithms is that the former update the value of a state based not only on the terminal payoff but also on the the estimated values of the intervening states.
Reference: <author> Tsitsiklis, J. </author> <year> (1994). </year> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16 (3), </volume> <pages> 185-202. </pages>
Reference: <author> Wasow, W. R. </author> <year> (1952). </year> <title> A note on the inversion of matrices by random walks. </title> <journal> Math. Tables Other Aids Comput., </journal> <volume> 6, </volume> <pages> 78-81. </pages>
Reference-contexts: 1. Introduction Many different algorithms have been developed for predicting the expected outcome, or value, of uncontrolled Markov reward processes: Monte Carlo (MC) algorithms <ref> (e.g., Wasow, 1952) </ref> and maximum-likehood (ML) algorithms (e.g., Kumar & Varaiya, 1986) in statistics and control, and temporal difference (TD) algorithms (Sutton, 1988; Barto et al., 1983) in machine learning. For most such algorithms, a theory of asymptotic convergence with probability one is available under suitable conditions on algorithm parameters.
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> Ph.D Thesis, </type> <institution> Cambridge Univ., </institution> <address> Cambridge, England. </address>
Reference: <author> Widrow, B. and Stearns, S. D. </author> <year> (1985). </year> <title> Adaptive Signal Processing. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: We can calculate the value of ff at which this ceases being true, a value we call the largest feasible ff. Just like the LMS algorithm <ref> (Widrow & Stearns, 1985) </ref>, these algorithms converge at best with probability 1 to an *-ball around v fl for a constant finite step-size. This amounts to the MSE converging to a fixed value which is determined by Equation 10.
References-found: 14

