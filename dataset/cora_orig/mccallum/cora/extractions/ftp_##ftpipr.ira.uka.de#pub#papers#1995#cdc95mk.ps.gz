URL: ftp://ftpipr.ira.uka.de/pub/papers/1995/cdc95mk.ps.gz
Refering-URL: ftp://ftpipr.ira.uka.de/.public_html/papersna.html
Root-URL: 
Email: fkaiserjreteyjdillmanng@ira.uka.de  
Title: Designing Neural Networks for Adaptive Control  
Author: Michael Kaiser, Albert Retey, Rudiger Dillmann 
Address: D-76128 Karlsruhe, Germany  
Affiliation: Institute for Real-Time Computer Systems Robotics University of Karlsruhe,  
Note: In: 34th IEEE International Conference on Decision and Control, New Orleans, USA, 1995.  
Abstract: This paper discusses the design of neural networks to solve specific problems of adaptive control. In particular, it investigates the influence of typical problems arising in real-world control tasks as well as techniques for their solution that exist in the framework of neurocontrol. Based on this investigation, a systematic design method is developed. The method is exemplified for the development of an adaptive force controller for a robot manipulator. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. T. Miller, R. S. Sutton, and P. J. Werbos, </author> <title> Neural networks for control, </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Artificial neural networks have attracted increasing attention for both identification and control of dynamical systems <ref> [1, 2] </ref>. They have been found to be capable of performing difficult control tasks, especially in robotics [3]. However, setting up a neural controller for adaptive control is in general a difficult task. <p> ( ~u i ; ~y j ) between a measurement component ~y j and a control action ~u i can be defined as D ( ~u i ; ~y j ) := (1 E ( ~u i ) ) 2 with D ( ~u i ; ~y j ) 2 <ref> [0; 1] </ref>. E ( ~u i ) is the entropy of ~u i , i.e., the initial uncertainty about the value of ~u i . <p> The actual adaptation can take place both in an indirect and in a direct way <ref> [1] </ref>. 4.1 Choosing an adaptation scheme In indirect adaptation, a model of the plant is used to determine desired control actions. <p> This method requires less initial knowledge about the control task, however, due to the necessary exploration, it does not allow for optimal control as long as adaptation is performed. One possibility to realize direct adaptation is to employ direct reinforcement learning methods <ref> [10, 1] </ref>. Especially formalisms such as Gullapalli's Stochastic Real Valued units (SRV units, [11]) are suitable, since they allow for learning continuous functions that are needed for controlling real systems. <p> If the model is incomplete or incorrect, direct methods provide a mean to perform efficient adaptation as well. In both cases, methods exist that are able to deal with delayed feedback <ref> [1] </ref>.
Reference: [2] <author> K. S. Narendra and S. Mukhopadhyay, </author> <title> "Adaptive control of nonlinear multivariable systems using neural networks", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, </volume> <year> 1994. </year>
Reference-contexts: 1 Introduction Artificial neural networks have attracted increasing attention for both identification and control of dynamical systems <ref> [1, 2] </ref>. They have been found to be capable of performing difficult control tasks, especially in robotics [3]. However, setting up a neural controller for adaptive control is in general a difficult task. <p> In adaptive neurocontrol, indirect adaptation often employs a feedforward network for learning and representing the plant model <ref> [2] </ref>. For the actual adaptation, the difference between the desired and the obtained feedback (e.g., the difference between the measured and the desired output of the plant) is considered as the network output error. By error backpropagation, the corresponding errors in the input neurons of the model network are obtained.
Reference: [3] <author> C. Torras, </author> <title> "Neural learning for robot control", </title> <booktitle> in Proceedings of the ECAI '94, </booktitle> <address> Amsterdam, The Netherlands, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Artificial neural networks have attracted increasing attention for both identification and control of dynamical systems [1, 2]. They have been found to be capable of performing difficult control tasks, especially in robotics <ref> [3] </ref>. However, setting up a neural controller for adaptive control is in general a difficult task. The off-line design involves selecting the type and the structure of the network as well as finding a suitable set of learning parameters.
Reference: [4] <author> A. Y. Zomaya and T. M. Nabhan, </author> <title> "Centralized and decentralized neuro-adaptive controllers", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 6, </volume> <year> 1993. </year>
Reference-contexts: Later on, the existing controller is replaced by the network, or the network produces a correction signal that is added to the output of the original controller <ref> [4] </ref>. However, conventional controllers are mostly designed based on a simplified model of the plant to be controlled.
Reference: [5] <author> H. R. Berenji, </author> <title> "A reinforcement learning based architecture for fuzzy logic control", </title> <journal> Int. Journal of Approximate Reasoning, </journal> <volume> vol. 6, </volume> <pages> pp. 267 - 292, </pages> <year> 1992. </year>
Reference-contexts: These examples can be recorded during normal operation, or they are generated specifically for teaching purposes [8] (Fig. 1 Also, knowledge in the form of rules can be encoded in neural networks using local representations. See <ref> [5] </ref> for an example. 2). There are, however, several issues that must be taken into account when following this approach. First, d and p of equation (4) must be identified in order to train the neural controller with valid data.
Reference: [6] <author> M. Kaiser, </author> <title> "Time-delay neural networks for control", </title> <booktitle> in Proceedings of the Symposium on Robot Control '94 (SYROCO '94), </booktitle> <address> Capri, Italy, </address> <year> 1994. </year>
Reference-contexts: Instead, one would like to transfer the qualitative knowledge expressed by equation (5) into a network whose neurons provide nonlinear transfer functions, such that during adaptation the simplified model can gradually be exchanged by a more complex one 1 . In <ref> [6] </ref>, we have presented a method that performs this kind of transfer. Starting from qualitative background knowledge expressed through an equation such as (5), the method generates a time-delay neural network [7] that preserves the given function but allows to use nonlinear transfer functions.
Reference: [7] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang, </author> <title> "Phoneme recognition using time-delay neural networks", </title> <journal> IEEE Transactions on acoustics, speech and signal processing, pp. </journal> <volume> 328 - 339, </volume> <month> March </month> <year> 1989. </year>
Reference-contexts: In [6], we have presented a method that performs this kind of transfer. Starting from qualitative background knowledge expressed through an equation such as (5), the method generates a time-delay neural network <ref> [7] </ref> that preserves the given function but allows to use nonlinear transfer functions.
Reference: [8] <author> M. Kaiser, A. Retey, and R. Dillmann, </author> <title> "Robot skill acquisition via human demonstration", </title> <booktitle> in Proceedings of the International Conference on Advanced Robotics (ICAR '95), </booktitle> <year> 1995. </year>
Reference-contexts: These examples can be recorded during normal operation, or they are generated specifically for teaching purposes <ref> [8] </ref> (Fig. 1 Also, knowledge in the form of rules can be encoded in neural networks using local representations. See [5] for an example. 2). There are, however, several issues that must be taken into account when following this approach.
Reference: [9] <author> W. H. Schiffmann and H. W. Geffers, </author> <title> "Adaptive control of dynamic systems by backpropagation networks", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 6, </volume> <year> 1993. </year>
Reference-contexts: For an error-minimizing learning rule, it is sufficient to know the signs of the errors made by the controller, i.e., to know the signs of the system's Jacobian ((@y=@u)) for adaptation. For some tasks, this qualitative knowledge can be provided a-priori, such that model learning is not necessary <ref> [9] </ref>. with wrong model. Desired force was -4N. The quality of indirect adaptation depends on the quality of the model that is used to calculate the changes to the controller's output. Fig. 6 shows the "performance" of a neural controller adapted on the basis of a wrong model.
Reference: [10] <author> A. G. Barto, R. S. Sutton, and C. W. Anderson, </author> <title> "Neuronlike elements that can solve difficult learning control problems", </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <pages> pp. 835-846, </pages> <year> 1983. </year>
Reference-contexts: This method requires less initial knowledge about the control task, however, due to the necessary exploration, it does not allow for optimal control as long as adaptation is performed. One possibility to realize direct adaptation is to employ direct reinforcement learning methods <ref> [10, 1] </ref>. Especially formalisms such as Gullapalli's Stochastic Real Valued units (SRV units, [11]) are suitable, since they allow for learning continuous functions that are needed for controlling real systems. <p> Therefore, for on-line learning neural networks using local representations such as radial-basis function networks [12] or box networks <ref> [10] </ref> are advantageous. They train only the output neurons' weights and do not adapt the hidden layer which in general represents a partition of the input space. The problem of using local representations is that the number of hidden units can grow exponentially with the input space dimension. <p> Partitioning the input space can also serve to realize incremental learning capabilities in multilayer perceptrons, e.g., by using unchangable random weights in the hidden force control task learned by a network employing a box representation like the one used in <ref> [10] </ref>. Examples were presented in sequence. layers or the storage of selected training data in a database for batch learning.
Reference: [11] <author> V. Gullapalli, J. A. Franklin, and H. Benbrahim, </author> <title> "Acquiring robot skills via reinforcement learning", </title> <journal> IEEE Control Systems Magazine, </journal> <volume> vol. 14, no. 1, </volume> <pages> pp. 13 - 24, </pages> <year> 1994. </year>
Reference-contexts: One possibility to realize direct adaptation is to employ direct reinforcement learning methods [10, 1]. Especially formalisms such as Gullapalli's Stochastic Real Valued units (SRV units, <ref> [11] </ref>) are suitable, since they allow for learning continuous functions that are needed for controlling real systems.
Reference: [12] <author> J. Moody and C. Darken, </author> <title> "Learning with localized receptive fields", </title> <booktitle> in Proceedings of the Connectionist Models Summer School, </booktitle> <year> 1988. </year>
Reference-contexts: Therefore, for on-line learning neural networks using local representations such as radial-basis function networks <ref> [12] </ref> or box networks [10] are advantageous. They train only the output neurons' weights and do not adapt the hidden layer which in general represents a partition of the input space.
Reference: [13] <author> C. Baroglio, A. Giordana, M. Kaiser, M. Nuttin, and R. Piola, </author> <title> "Learning controllers for industrial robots", </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference-contexts: The problem of using local representations is that the number of hidden units can grow exponentially with the input space dimension. Therefore, a careful initial clustering of the input space based on examples <ref> [13] </ref> or a reduction by means of hashing [14] is necessary.
Reference: [14] <author> J.S. Albus, </author> <title> "A new approach to manipulator control: The cerebellar model articulation controller", Dynamic Systems, </title> <booktitle> Measurement and Control, </booktitle> <year> 1975. </year>
Reference-contexts: The problem of using local representations is that the number of hidden units can grow exponentially with the input space dimension. Therefore, a careful initial clustering of the input space based on examples [13] or a reduction by means of hashing <ref> [14] </ref> is necessary. Partitioning the input space can also serve to realize incremental learning capabilities in multilayer perceptrons, e.g., by using unchangable random weights in the hidden force control task learned by a network employing a box representation like the one used in [10].
References-found: 14

