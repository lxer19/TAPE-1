URL: http://www.cs.umd.edu/users/cml/work/exp/EFoCS-25-97.ps.gz
Refering-URL: http://www.cs.umd.edu/users/cml/work/exp/
Root-URL: 
Title: A Comparison of Tool-Based and Paper-Based Software Inspection  
Author: F. Macdonald and J. Miller 
Keyword: Software inspection, tool support, controlled experiment  
Date: April 1997  
Pubnum: RR/97/203 [EFoCS-25-97]  
Abstract: Software inspection is an effective method of defect detection. Recent research activity has considered the development of tool support to further increase the efficiency and effectiveness of inspection, resulting in a number of prototype tools being developed. However, no comprehensive evaluations of these tools have been carried out to determine their effectiveness in comparison with traditional paper-based inspection. This issue must be addressed if tool-supported inspection is to become an accepted alternative to, or even replace, paper-based inspection. This paper describes a controlled experiment comparing the effectiveness of tool-supported software inspection with paper-based inspection, using a new prototype software inspection tool known as ASSIST (Asynchronous/Synchronous Software Inspection Support Tool). 43 students used ASSIST and paper-based inspection to inspect two C++ programs of approximately 150 lines. The subjects performed both individual inspection and a group collection meeting, representing a typical inspection process. It was found that subjects performed equally well with tool-based inspection as with paper-based, measured in terms of the number of defects found, the number of false positives reported, and meeting gains and losses.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> John T. Baldwin. </author> <title> An abbreviated C++ code inspection checklist. </title> <note> Available on the WWW, URL: http://www.ics.hawaii.edu/ johnson/FTR/Bib/Baldwin92.html, </note> <year> 1992. </year>
Reference-contexts: All programs used compiled with no errors or warnings using CC under SunOS 4.1.3. The checklist used was derived from a C code checklist by Marick [21], a C++ checklist by Baldwin <ref> [1] </ref> (derived from the aforementioned C checklist), the C++ checklist from [15] and a generic code checklist from [9].
Reference: [2] <author> Jack Barnard and Art Price. </author> <title> Managing code inspection information. </title> <journal> IEEE Software, </journal> <volume> 11(2) </volume> <pages> 56-69, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: For each program, a specification written in a similar style to that of the Kamsties and Lott material was also prepared, along with appropriate lists of library functions. There is no clear consensus on the optimal inspection rate. For example, both Barnard and Price <ref> [2] </ref> and Russell [26] quote a recommendation of 150 lines of code per hour. On the other hand, Gilb and Graham [12], recommend inspecting between 0.5 and 1.5 pages per hour, translating to between 30 and 90 lines of code. All conclude that lower rates improve defect detection.
Reference: [3] <author> Victor R. Basili and Richard W. Selby. </author> <title> Comparing the effectiveness of software testing strategies. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 13(12) </volume> <pages> 1278-1296, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: One program (analyse.cc, 147 lines, 12 faults) was based on the idea of a simple statistical analysis program given in [6]. The second program (graph.cc, 143 lines, 12 faults) was written from a specification for a Fortran graph plotting program, originally found in <ref> [3] </ref>. For each program, a specification written in a similar style to that of the Kamsties and Lott material was also prepared, along with appropriate lists of library functions. There is no clear consensus on the optimal inspection rate.
Reference: [4] <author> C. Alan Boneau. </author> <title> The effects of violations of assumptions underlying the t test. </title> <journal> Psychological Bulletin, </journal> <volume> 57(1) </volume> <pages> 49-64, </pages> <year> 1960. </year>
Reference-contexts: Again, the null hypothesis as applied to groups must be accepted. Under further investigation, the data from the individual phase of the graph.cc inspection failed the the Levene test for homogeneity of variances. However, the robustness of the F test is well documented. For example, Boneau <ref> [4] </ref> has studied the effects of violating the assumptions which underlie the t test, and generalised these results to the F test. Provided samples sizes are sufficient (around 15) and virtually equal, and a two-tailed test is used, non-homogeneity of variances should not cause difficulties.
Reference: [5] <author> L. R. Brothers, V. Sembugamoorthy, and A. E. Irgon. </author> <title> Knowledge-based code inspection with ICICLE. </title> <booktitle> In Innovative Applications of Artificial Intelligence 4: Proceedings of IAAI-92, </booktitle> <year> 1992. </year>
Reference-contexts: In the case of ICICLE <ref> [5] </ref>, the only published evaluation comes in the form of `lessons learned'. In the case of Scrutiny, in addition to lessons learned [14], the authors also claim that tool-based inspection is as effective as paper-based, but there is no quantifiable evidence to support this claim [13].
Reference: [6] <author> H.M. Deitel and P.J. Deitel. </author> <title> C: How to Program. </title> <booktitle> Prentice-Hall International, second edition, </booktitle> <year> 1994. </year>
Reference-contexts: One program (analyse.cc, 147 lines, 12 faults) was based on the idea of a simple statistical analysis program given in <ref> [6] </ref>. The second program (graph.cc, 143 lines, 12 faults) was written from a specification for a Fortran graph plotting program, originally found in [3].
Reference: [7] <author> Andrew Dillon. </author> <title> Reading from paper versus screens: a critical review of the empirical literature. </title> <journal> Ergonomics, </journal> <volume> 35(10) </volume> <pages> 1297-1326, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The second problem concerns reading text from a screen. There have been a number of studies comparing reading from screen versus reading from paper, and Dillon <ref> [7] </ref> provides a good review of these. He suggests that evidence points to a 20-30% reduction in speed when reading from screen compared to reading from paper, although the different experimental factors make it difficult to reach a certain conclusion.
Reference: [8] <author> E. P. Doolan. </author> <title> Experience with Fagan's inspection method. </title> <journal> Software-Practice and Experience, </journal> <volume> 22(2) </volume> <pages> 173-182, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Experience reports extolling the virtues of inspection are easily found. For example, Gilb and Graham [12] present a number of success stories from a variety of projects. More quantitative evidence of the effectiveness of inspections has also been reported: Doolan <ref> [8] </ref> reports industrial experience indicating a 30 times return on investment for every hour spent inspecting software requirement specifications. Russell [26] reports a similar return of 33 hours of maintenance saved for every hour of inspection invested. Despite the benefits, inspection remains an expensive process.
Reference: [9] <author> Robert G. Ebenau and Susan H. Strauss. </author> <title> Software Inspection Process. </title> <publisher> McGraw-Hill, </publisher> <year> 1994. </year>
Reference-contexts: The checklist used was derived from a C code checklist by Marick [21], a C++ checklist by Baldwin [1] (derived from the aforementioned C checklist), the C++ checklist from [15] and a generic code checklist from <ref> [9] </ref>. From the C and C++ checklists we removed items which we considered to be irrelevant (for example, none of our programs made extensive use of macros), along with esoteric items, such as those dealing with threaded programming and signals.
Reference: [10] <author> Allen L. Edwards. </author> <title> Statistical Methods. </title> <publisher> Holt, Rinehart and Winston, Inc, </publisher> <address> second edition, </address> <year> 1967. </year>
Reference-contexts: Provided samples sizes are sufficient (around 15) and virtually equal, and a two-tailed test is used, non-homogeneity of variances should not cause difficulties. A similar conclusion is presented Edwards <ref> [10] </ref>. As a safeguard, the Kruskall-Wallis non-parametric test was applied to all four sets of data, and gave results similar to those for the parametric tests, with no significance.
Reference: [11] <author> Michael E. Fagan. </author> <title> Design and code inspections to reduce errors in program development. </title> <journal> IBM Systems Journal, </journal> <volume> 15(3) </volume> <pages> 182-211, </pages> <year> 1976. </year>
Reference-contexts: 1 Introduction Software inspection, originally described by Michael Fagan in 1976 <ref> [11] </ref>, is well-known as an effective defect finding technique. Experience reports extolling the virtues of inspection are easily found. For example, Gilb and Graham [12] present a number of success stories from a variety of projects.
Reference: [12] <author> T. Gilb and D. Graham. </author> <title> Software Inspection. </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year> <month> 15 </month>
Reference-contexts: 1 Introduction Software inspection, originally described by Michael Fagan in 1976 [11], is well-known as an effective defect finding technique. Experience reports extolling the virtues of inspection are easily found. For example, Gilb and Graham <ref> [12] </ref> present a number of success stories from a variety of projects. More quantitative evidence of the effectiveness of inspections has also been reported: Doolan [8] reports industrial experience indicating a 30 times return on investment for every hour spent inspecting software requirement specifications. <p> There is no clear consensus on the optimal inspection rate. For example, both Barnard and Price [2] and Russell [26] quote a recommendation of 150 lines of code per hour. On the other hand, Gilb and Graham <ref> [12] </ref>, recommend inspecting between 0.5 and 1.5 pages per hour, translating to between 30 and 90 lines of code. All conclude that lower rates improve defect detection. Each practical session lasted 2 hours, giving an inspection rate of around 70 lines per hour. <p> We felt that a short checklist covering the major points to consider would be more effective than a much longer, more detailed checklist which the subjects would struggle to cover in the time allowed. This follows practice recommended by Gilb and Graham <ref> [12] </ref>. The checklist is presented in Appendix B. 2.4 Instrumentation For paper-based inspection, each student was given an individual defect report form like that in Appendix C. For group meetings, the scribe was given a group defect report form like that in Appendix D. <p> For example, the process used did not involve the author presenting an overview of the product, and a rework phase was not used. However, the detection/collection approach to inspection is a standard process <ref> [12] </ref>. These threats are typical of many empirical studies, e.g. [25, 16].
Reference: [13] <author> John W. Gintell, John Arnold, Michael Houde, Jacek Kruszelnicki, Roland McKenney, and Gerard Memmi. Scrutiny: </author> <title> A collaborative inspection and review system. </title> <booktitle> In Proceedings of the Fourth European Software Engineering Conference, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: In the case of Scrutiny, in addition to lessons learned [14], the authors also claim that tool-based inspection is as effective as paper-based, but there is no quantifiable evidence to support this claim <ref> [13] </ref>. Knight and Myers [17] describe two experiments involving their InspeQ inspection tool, designed to support their phased inspection method. The first simply provided information on the feasibility of their method, and on the usability of the associated toolset.
Reference: [14] <author> John W. Gintell, Michael B. Houde, and Roland F. McKenney. </author> <title> Lessons learned by building and using Scrutiny, a collaborative software inspection system. </title> <booktitle> In Proceedings of the Seventh International Workshop on Computer Aided Software Engineering, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: In the case of ICICLE [5], the only published evaluation comes in the form of `lessons learned'. In the case of Scrutiny, in addition to lessons learned <ref> [14] </ref>, the authors also claim that tool-based inspection is as effective as paper-based, but there is no quantifiable evidence to support this claim [13]. Knight and Myers [17] describe two experiments involving their InspeQ inspection tool, designed to support their phased inspection method. <p> However, most common displays are not capable of displaying three such windows (of sufficient size to be useful) at the same time. The screen may also be cluttered with other windows necessary for operation of the tool, such as has been described in Scrutiny <ref> [14] </ref>. Contrast this with paper-based inspection, where inspectors are free to find as large a workspace as required and to spread all the documents around in a manner comfortable to their working method. The second problem concerns reading text from a screen.
Reference: [15] <author> Watts S. Humphrey. </author> <title> A Discipline for Software Engineering. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: All programs used compiled with no errors or warnings using CC under SunOS 4.1.3. The checklist used was derived from a C code checklist by Marick [21], a C++ checklist by Baldwin [1] (derived from the aforementioned C checklist), the C++ checklist from <ref> [15] </ref> and a generic code checklist from [9]. From the C and C++ checklists we removed items which we considered to be irrelevant (for example, none of our programs made extensive use of macros), along with esoteric items, such as those dealing with threaded programming and signals.
Reference: [16] <author> Erik Kamsties and Christopher M. Lott. </author> <title> An empirical evaluation of three defect-detection techniques. </title> <type> Technical Report ISERN-95-02, </type> <institution> International Software Engineering Research Network, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Since the subjects were competent in C++, material in that language was chosen for the experiment. The decision was further ratified by the availability of high quality material from a local replication of Kamsties and Lott's defect detection experiment <ref> [16] </ref>. For the training materials, a selection of programs originally used in Kamsties and Lott's experiment were used, since each program had an appropriate specification, a list of library functions used by the program and a comprehensive fault list. <p> For example, the process used did not involve the author presenting an overview of the product, and a rework phase was not used. However, the detection/collection approach to inspection is a standard process [12]. These threats are typical of many empirical studies, e.g. <ref> [25, 16] </ref>. These threats can be reduced by performing repeated experimentation with other subjects, programs and processes. 9 Program analyse.cc graph.cc Section 1 2 1 2 Method Tool Paper Paper Tool Groups 7 7 7 7 Mean 10.86 10.71 9.57 8.86 St. Dev. 0.69 0.95 1.27 1.07 St.
Reference: [17] <author> John C. Knight and E. Ann Meyers. </author> <title> An improved inspection technique. </title> <journal> Communications of the ACM, </journal> <volume> 36(11) </volume> <pages> 51-61, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: In the case of Scrutiny, in addition to lessons learned [14], the authors also claim that tool-based inspection is as effective as paper-based, but there is no quantifiable evidence to support this claim [13]. Knight and Myers <ref> [17] </ref> describe two experiments involving their InspeQ inspection tool, designed to support their phased inspection method. The first simply provided information on the feasibility of their method, and on the usability of the associated toolset. The second experiment involved inspection of C code, but provided no comparison with paper-based inspection.
Reference: [18] <author> F. Macdonald. </author> <title> ASSIST V1.1 User Manual. </title> <type> Technical Report EFoCS-22-96 [RR/96/199], </type> <institution> Department of Computer Science, University of Strathclyde, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: This research has produced an inspection process modelling language known as IPDL (Inspection Process Definition Language) <ref> [18] </ref>, which is capable of describing any current and future inspection processes, along with the materials and personnel involved in those processes.
Reference: [19] <author> F. Macdonald, J. Miller, A. Brooks, M. Roper, and M. Wood. </author> <title> A review of tool support for software inspection. </title> <booktitle> In Proceedings of the Seventh International Workshop on Computer Aided Software Engineering, </booktitle> <pages> pages 340-349, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: A comprehensive review of these can be found in <ref> [19, 20] </ref>. Generally, these tools allow the inspection team to browse and annotate the product (the document under inspection) on-line, and may support discussion of these annotations during meetings. Although existing systems present innovative approaches to supporting software inspection, in general they suffer from a number of shortcomings.
Reference: [20] <author> F. Macdonald, J. Miller, A. Brooks, M. Roper, and M. Wood. </author> <title> Automating the software inspection process. </title> <journal> Automated Software Engineering: An International Journal, </journal> 3(3/4):193-218, August 1996. 
Reference-contexts: A comprehensive review of these can be found in <ref> [19, 20] </ref>. Generally, these tools allow the inspection team to browse and annotate the product (the document under inspection) on-line, and may support discussion of these annotations during meetings. Although existing systems present innovative approaches to supporting software inspection, in general they suffer from a number of shortcomings.
Reference: [21] <author> Brian Marick. </author> <title> A question catalog for code inspections. </title> <note> Available via anonymous FTP from cs.uiuc.edu as /pub/testing/inspect.ps, </note> <year> 1992. </year>
Reference-contexts: All programs used compiled with no errors or warnings using CC under SunOS 4.1.3. The checklist used was derived from a C code checklist by Marick <ref> [21] </ref>, a C++ checklist by Baldwin [1] (derived from the aforementioned C checklist), the C++ checklist from [15] and a generic code checklist from [9].
Reference: [22] <author> Vahid Mashayekhi. </author> <title> Distribution and Asynchrony in Software Engineering. </title> <type> PhD thesis, </type> <institution> University of Minnesota, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: The first simply provided information on the feasibility of their method, and on the usability of the associated toolset. The second experiment involved inspection of C code, but provided no comparison with paper-based inspection. Mashayekhi <ref> [22] </ref> reports on the implementation of three prototype tools, with the aim of investigating distribution and asynchrony in software engineering. Again, no comparisons with paper-based inspection are made, except in the case of group meetings, where comparable meeting losses are found using both the tool and paper-based methods.
Reference: [23] <author> J. Miller, J. Daly, M. Wood, M. Roper, and A. Brooks. </author> <title> Statistical power and its subcomponents | missing and misunderstood concepts in empirical software engineering research. </title> <journal> Information and Software Technology, </journal> <note> 1997. To appear. </note>
Reference: [24] <author> J. Miller, M. Wood, M. Roper, and A. Brooks. </author> <title> Further experiences with scenarios and checklists. </title> <type> Technical Report EFoCS-20-96, </type> <institution> Department of Computer Science, University of Strathclyde, </institution> <year> 1996. </year>
Reference-contexts: Hence, at these reduced effect sizes the experiment is unable to reliably prove that no effect exists. See Miller et al.[23] for a fuller discussion of the role of statistical power in experimental design. 6 2.3 Materials Having previous experience of running defect detection experiments <ref> [24] </ref>, it was decided that the most appropriate material to inspect would be C++ code. A number of factors influenced this decision. Initially, source code was chosen as the appropriate material due to ease with which defects in code can be defined.
Reference: [25] <author> A. A. Porter, L. G. Votta, and V. R. Basili. </author> <title> Comparing detection methods for software requirements inspections: A replicated experiment. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(6) </volume> <pages> 563-575, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: For example, the process used did not involve the author presenting an overview of the product, and a rework phase was not used. However, the detection/collection approach to inspection is a standard process [12]. These threats are typical of many empirical studies, e.g. <ref> [25, 16] </ref>. These threats can be reduced by performing repeated experimentation with other subjects, programs and processes. 9 Program analyse.cc graph.cc Section 1 2 1 2 Method Tool Paper Paper Tool Groups 7 7 7 7 Mean 10.86 10.71 9.57 8.86 St. Dev. 0.69 0.95 1.27 1.07 St.
Reference: [26] <author> Glen W. Russell. </author> <title> Experience with inspection in ultralarge-scale developments. </title> <journal> IEEE Software, </journal> <volume> 8(1) </volume> <pages> 25-31, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: More quantitative evidence of the effectiveness of inspections has also been reported: Doolan [8] reports industrial experience indicating a 30 times return on investment for every hour spent inspecting software requirement specifications. Russell <ref> [26] </ref> reports a similar return of 33 hours of maintenance saved for every hour of inspection invested. Despite the benefits, inspection remains an expensive process. <p> If rigour is lacking, feedback from the process cannot be used to improve both the inspection process and the overall software development process. Yet rigour can be difficult to achieve due to incomplete or insufficiently detailed descriptions of the process <ref> [26] </ref>, such that actual practice varies depending on the interpretation. <p> For each program, a specification written in a similar style to that of the Kamsties and Lott material was also prepared, along with appropriate lists of library functions. There is no clear consensus on the optimal inspection rate. For example, both Barnard and Price [2] and Russell <ref> [26] </ref> quote a recommendation of 150 lines of code per hour. On the other hand, Gilb and Graham [12], recommend inspecting between 0.5 and 1.5 pages per hour, translating to between 30 and 90 lines of code. All conclude that lower rates improve defect detection.
Reference: [27] <author> Danu Tjahjono. </author> <title> Comparing the cost effectiveness of group synchronous review method and individual asynchronous review method using CSRS: Results of pilot study. </title> <type> Technical Report ICS-TR-95-07, </type> <institution> University of Hawaii, </institution> <month> January </month> <year> 1995. </year> <month> 16 </month>
Reference-contexts: Finally, CSRS (Collaborative Software Review System) has been used to compare the cost effectiveness of a group-based review method with that of an individual-based review method <ref> [27] </ref>. Again, since both methods are tool-based, there is no indication of the relative merits of tool-based and paper-based inspection. Although the evaluations described above attempt to measure, in various ways, the effectiveness of tool support, the fundamental question "Is tool-based software inspection as effective as paper-based inspection?" remains unanswered.
References-found: 27

