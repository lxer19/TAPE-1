URL: ftp://cse.ogi.edu/pub/chcc/Publications/acl97.ps
Refering-URL: http://www.cse.ogi.edu/~johnston/
Root-URL: http://www.cse.ogi.edu
Email: fjohnston,pcohen,dmcgee,oviatt,jay,irag@cse.ogi.edu  
Title: Unification-based Multimodal Integration  
Author: Michael Johnston, Philip R. Cohen, David McGee, Sharon L. Oviatt, James A. Pittman, Ira Smith 
Address: PO BOX 91000, Portland, OR 97291, USA.  
Affiliation: Center for Human Computer Communication Department of Computer Science and Engineering Oregon Graduate Institute,  
Abstract: Recent empirical research has shown conclusive advantages of multimodal interaction over speech-only interaction for map-based tasks. This paper describes a multimodal language processing architecture which supports interfaces allowing simultaneous input from speech and gesture recognition. Integration of spoken and gestural input is driven by unification of typed feature structures representing the semantic contributions of the different modes. This integration method allows the component modalities to mutually compensate for each others' errors. It is implemented in Quick-Set, a multimodal (pen/voice) system that enables users to set up and control dis tributed interactive simulations.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bolt, R. A., </author> <year> 1980. </year> <title> "Put-That-There":Voice and gesture at the graphics interface. </title> <journal> Computer Graphics, 14.3:262-270. </journal>
Reference-contexts: We will demonstrate how, in our system, multimodal integration allows speech input to compensate for errors in gesture recognition and vice versa. Systems capable of integration of speech and gesture have existed since the early 80's. One of the first such systems was the "Put-That-There" system <ref> (Bolt 1980) </ref>. However, in the sixteen years since then, research on multimodal integration has not yielded a reusable scalable architecture for the construction of multimodal systems that integrate gesture and voice.
Reference: <author> Brison, E., and N. Vigouroux. </author> <title> (unpublished ms.). Multimodal references: A generic fusion process. </title> <institution> URIT-URA CNRS. Universit Paul Sabatier, Toulouse, France. </institution>
Reference: <author> Calder, J. </author> <year> 1987. </year> <title> Typed unification for natural language processing. </title> <editor> In E. Klein and J. van Benthem, editors, </editor> <booktitle> Categories, Polymorphisms, and Unifica--tion, </booktitle> <pages> pages 65-72. </pages> <institution> Centre for Cognitive Science, University of Edinburgh, Edinburgh. </institution>
Reference: <author> Carpenter, R. </author> <year> 1990. </year> <title> Typed feature structures: Inheritance, (In)equality, </title> <editor> and Extensionality. In W. Daelemans and G. Gazdar, editors, </editor> <booktitle> Proceedings of the ITK Workshop: Inheritance in Natural Language Processing, </booktitle> <pages> pages 9-18, </pages> <institution> Tilburg. Institute for Language Technology and Artificial Intelligence, Tilburg University, Tilburg. </institution>
Reference: <author> Carpenter, R. </author> <year> 1992. </year> <title> The logic of typed feature structures. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England. </address>
Reference-contexts: Either spoken or typed linguistic expressions are the driving force of interpretation. We present an approach to multimodal integration which overcomes these limiting factors. A wide base of continuous gestural input is supported and integration may be driven by either mode. Typed feature structures <ref> (Carpenter 1992) </ref> are used to provide a clearly defined and well understood common meaning representation for the modes, and multimodal integration is accomplished through unification. 2 Quickset: A Multimodal Interface for Distributed Interactive Simulation The initial application of our multimodal interface architecture has been in the development of the QuickSet system,
Reference: <author> Cheyer, A., and L. Julia. </author> <year> 1995. </year> <title> Multimodal maps: An agent-based approach. </title> <booktitle> In International Conference on Cooperative Multimodal Communication (CMC/95), </booktitle> <pages> pages 24-26, </pages> <month> May </month> <year> 1995. </year> <title> Eind-hoven, </title> <address> The Netherlands. </address>
Reference: <author> Clarkson, J. D., and J. Yi. </author> <year> 1996. </year> <title> LeatherNet: A synthetic forces tactical training system for the USMC commander. </title> <booktitle> In Proceedings of the Sixth Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <pages> pages 275-281. </pages> <institution> Institute for simulation and training. </institution> <note> Technical Report IST-TR-96-18. </note>
Reference-contexts: QuickSet provides a portal into LeatherNet 3 , a simulation system used for the training of US Marine Corps platoon leaders. LeatherNet simulates training exercises using the ModSAF simulator (Courte-manche and Ceranowicz 1995) and supports 3D visualization of the simulated exercises using Com-mandVu <ref> (Clarkson and Yi 1996) </ref>. SRI International's CommandTalk provides a unimodal spoken interface to LeatherNet (Moore et al 1997). QuickSet is a distributed system consisting of a collection of agents that communicate through the Open Agent Architecture 4 (Cohen et al 1994).
Reference: <author> Cohen, P. R. </author> <year> 1991. </year> <title> Integrated interfaces for decision support with simulation. </title> <editor> In B. Nelson, W. D. Kel-ton, and G. M. Clark, editors, </editor> <booktitle> Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 1066-1072. </pages> <publisher> ACM, </publisher> <address> New York. </address>
Reference: <author> Cohen, P. R. </author> <year> 1992. </year> <title> The role of natural language in a multimodal interface. </title> <booktitle> In Proceedings of UIST'92, </booktitle> <pages> pages 143-149. </pages> <publisher> ACM Press, </publisher> <address> New York. </address>
Reference: <author> Cohen, P. R., A. Cheyer, M. Wang, and S. C. Baeg. </author> <year> 1994. </year> <title> An open agent architecture. </title> <booktitle> In Working Notes of the AAAI Spring Symposium on Software Agents (March 21-22, </booktitle> <publisher> Stanford University, Stanford, California), </publisher> <pages> pages 1-8. </pages>
Reference-contexts: SRI International's CommandTalk provides a unimodal spoken interface to LeatherNet (Moore et al 1997). QuickSet is a distributed system consisting of a collection of agents that communicate through the Open Agent Architecture 4 <ref> (Cohen et al 1994) </ref>. It runs on both desktop and hand-held PCs under Windows 95, communicating over wired and wireless LANs (respectively), or modem links. The wireless hand-held unit is a 3-lb Fujitsu Stylistic 1000 (Figure 2).
Reference: <author> Courtemanche, A. J., and A. Ceranowicz. </author> <year> 1995. </year> <title> ModSAF development status. </title> <booktitle> In Proceedings of the Fifth Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <pages> pages 3-13, </pages> <address> May 9-11, Orlando, Florida. </address> <institution> University of Central Florida, Florida. </institution>
Reference: <author> King, P. </author> <year> 1989. </year> <title> A logical formalism for head-driven phrase structure grammar. </title> <type> Ph.D. Thesis, </type> <institution> University of Manchester, </institution> <address> Manchester, England. </address>
Reference: <author> Koons, D. B., C. J. Sparrell, and K. R. Thorisson. </author> <year> 1993. </year> <title> Integrating simultaneous input from speech, gaze, </title> <editor> and hand gestures. In M. T. Maybury, editor, </editor> <booktitle> Intelligent Multimedia Interfaces, </booktitle> <pages> pages 257-276. </pages> <publisher> AAAI Press/ MIT Press, </publisher> <address> Cambridge, Mas-sachusetts. </address>
Reference-contexts: are four major limiting factors in previous approaches to multimodal integration: (i) The majority of approaches limit the bandwidth of the gestural mode to simple deictic pointing gestures made with a mouse (Neal and Shapiro 1991, Cohen 1991, Cohen 1992, Brison and Vigouroux (ms.), Wauchope 1994) or with the hand <ref> (Koons et al 1993 1 ) </ref>. (ii) Most previous approaches have been primarily speech-driven 2 , treating gesture as a secondary dependent mode (Neal and Shapiro 1991, Co-hen 1991, Cohen 1992, Brison and Vigouroux (ms.), Koons et al 1993, Wauchope 1994).
Reference: <author> Moore, R. C., J. Dowding, H. Bratt, J. M. Gawron, Y. Gorfu, and A. </author> <note> Cheyer 1997. CommandTalk: </note>
Reference-contexts: As an illustrative example, in the distributed simulation application we describe in this paper, one user task is to add a "phase line" to a map. In the existing unimodal interface for this application <ref> (CommandTalk, Moore 1997) </ref>, this is accomplished with a spoken utterance such as `CREATE A LINE FROM COORDINATES NINE FOUR THREE NINE THREE ONE TO NINE EIGHT NINE NINE FIVE ZERO AND CALL IT PHASE LINE GREEN'. <p> LeatherNet simulates training exercises using the ModSAF simulator (Courte-manche and Ceranowicz 1995) and supports 3D visualization of the simulated exercises using Com-mandVu (Clarkson and Yi 1996). SRI International's CommandTalk provides a unimodal spoken interface to LeatherNet <ref> (Moore et al 1997) </ref>. QuickSet is a distributed system consisting of a collection of agents that communicate through the Open Agent Architecture 4 (Cohen et al 1994). It runs on both desktop and hand-held PCs under Windows 95, communicating over wired and wireless LANs (respectively), or modem links.
References-found: 14

