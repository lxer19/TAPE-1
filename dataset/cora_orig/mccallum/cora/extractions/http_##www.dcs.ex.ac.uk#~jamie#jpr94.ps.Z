URL: http://www.dcs.ex.ac.uk/~jamie/jpr94.ps.Z
Refering-URL: http://www.dcs.ex.ac.uk/~jamie/
Root-URL: http://www.dcs.ex.ac.uk
Title: Connectionist Syntactic Parsing Using Temporal Variable Binding  
Author: James Henderson 
Affiliation: Department of Computer Science University of Pennsylvania  
Abstract: Recent developments in connectionist architectures for symbolic computation have made it possible to investigate parsing in a connectionist network while still taking advantage of the large body of work on parsing in symbolic frameworks. The work discussed here investigates syntactic parsing in the temporal synchrony variable binding model of symbolic computation in a connectionist network. This computational architecture solves the basic problem with previous connectionist architectures, while keeping their advantages. However, the architecture does have some limitations, which impose constraints on parsing in this architecture. Despite these constraints, the architecture is computationally adequate for syntactic parsing. In addition, the constraints make some significant linguistic predictions. These arguments are made using a specific parsing model. The extensive use of partial descriptions of phrase structure trees is crucial to the ability of this model to recover the syntactic structure of sentences within the constraints imposed by the architecture.
Abstract-found: 1
Intro-found: 1
Reference: <author> Berwick, R. and Weinberg, A. </author> <year> (1984). </year> <title> The Grammatical Basis of Linguistic Performance. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: These constraints imply that the syntactic parser must parse deterministically. This constraint was first proposed by Marcus (Marcus, 1980), and has been argued for by several researchers since ((Church 1980), (Marcus, et.al. 1983), <ref> (Berwick, Weinberg, 1984) </ref>). It requires that the parser deterministically pursue a single analysis. This means that multiple analyses can't be pursued in parallel, and that once the parser commits to an aspect of the analysis it can't retract that commitment.
Reference: <author> Chomsky, N. </author> <year> (1959). </year> <title> On certain formal properties of grammars. </title> <journal> Information and Control, </journal> <volume> 2 </volume> <pages> 137-167. </pages>
Reference-contexts: These limitations impose computational constraints on syntactic parsing in this architecture. Interestingly, most of these constraints have previously been proposed based on linguistic and psychological data. The parser must have a bounded memory <ref> (Chomsky, 1959) </ref>, and in particular can only store information about a bounded number of things (Miller, 1956). The parser cannot explicitly represent disjunction, which in conjunction with the requirement that the parser's output be incrementally interpretable, means the parser must be deterministic in the sense proposed by Marcus (Marcus, 1980). <p> Other levels of representation, such as predicate-argument structure, may not be subject to the monotonicity constraint. 6 The first constraint is an example of a bounded memory requirement. It has generally been assumed that at some level of abstraction the syntactic parser has a bounded memory <ref> (Chomsky, 1959) </ref>. Church (Church, 1980) showed that this constraint applies at a level which takes into consideration performance constraints, such as restrictions on the depth of center embedding and on the availability of phrases for posthead modification.
Reference: <author> Church, K. </author> <year> (1980). </year> <title> On memory limitations in natural language processing. </title> <type> Master's thesis, </type> <institution> Mas-sachusetts Institute of Technology. </institution> <note> MIT LCS Technical Report 245. </note> <author> de Smedt, K. and Kempen, G. </author> <year> (1991). </year> <title> Segment grammar: A formalism for incremental sentence generation. In Paris, </title> <editor> C., Swartout, W., and Mann, W., editors, </editor> <booktitle> Natural Language Generation in Artificial Intelligence and Computational Linguistics, </booktitle> <pages> pages 329-349. </pages> <publisher> Kluwer Academic Publishers, Boston/Dordrecht/London. </publisher>
Reference-contexts: Other levels of representation, such as predicate-argument structure, may not be subject to the monotonicity constraint. 6 The first constraint is an example of a bounded memory requirement. It has generally been assumed that at some level of abstraction the syntactic parser has a bounded memory (Chomsky, 1959). Church <ref> (Church, 1980) </ref> showed that this constraint applies at a level which takes into consideration performance constraints, such as restrictions on the depth of center embedding and on the availability of phrases for posthead modification. <p> However, it is well known that the set of nodes which are available for such modification is severely restricted <ref> (Church, 1980) </ref>. Because of these performance constraints, the parser's bounded memory is not likely to be a problem for maintaining local ambiguities.
Reference: <author> Elman, J. L. </author> <year> (1991). </year> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 195-225. </pages>
Reference-contexts: Like recurrent connectionist networks, these rules may compute sequentially in time, and the order of input items is represented by presenting the input sequentially in time. Such networks have been used to parse simple syntactic constructions <ref> (Elman, 1991) </ref> and to model the interaction of syntactic and semantic constraints (St. John, McClelland, 1992), but because they are unable to capture all the relevant generalizations, they have been unable to handle the full diversity and complexity of natural language syntax.
Reference: <author> Fillmore, C., Kay, P., and O'Connor, M. </author> <year> (1988). </year> <title> Regularity and idiomaticity in grammatical constructions: The case of let alone. </title> <booktitle> Language, </booktitle> <volume> 64 </volume> <pages> 501-538. </pages>
Reference-contexts: As such it is similar to other unification based or constraint based grammar formalisms. These include Description Theory (Marcus, et al., 1983), Head-Driven Phrase Structure Grammar (Pollard, Sag, 1987), Construction Grammar <ref> (Fillmore, et al., 1988) </ref>, and Segment Grammar (de Smedt, Kempen, 1991), among others. Like these other formalisms, SUG allows multiple kinds of grammatical features to be specified independently of each other.
Reference: <author> Gibson, E. </author> <year> (1991). </year> <title> A Computational Theory of Human Linguistic Processing: Memory Limitations and Processing Breakdown. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: To test NNEP's ability to handle local ambiguities, the data from the chapters on ambiguity resolution in <ref> (Gibson, 1991) </ref> were used. (Gibson, 1991) is particularly well suited for this purpose because Gibson surveys the literature on ambiguity resolution and discusses the relevant data. <p> To test NNEP's ability to handle local ambiguities, the data from the chapters on ambiguity resolution in <ref> (Gibson, 1991) </ref> were used. (Gibson, 1991) is particularly well suited for this purpose because Gibson surveys the literature on ambiguity resolution and discusses the relevant data. To 14 While each of these areas deserve a more detailed analysis, a broad and shallow test is appropriate for this stage of the investigation. <p> The test of NNEP's ability to handle center embedded sentences used the data from the chapters on processing overload in <ref> (Gibson, 1991) </ref>. Again, (Gibson, 1991) is particularly well suited for this purpose because it surveys the literature. In addition, some example sentences involving nested ditransitive verbs were constructed and used. <p> The test of NNEP's ability to handle center embedded sentences used the data from the chapters on processing overload in <ref> (Gibson, 1991) </ref>. Again, (Gibson, 1991) is particularly well suited for this purpose because it surveys the literature. In addition, some example sentences involving nested ditransitive verbs were constructed and used.
Reference: <author> Henderson, J. </author> <year> (1990). </year> <title> Structure unification grammar: A unifying framework for investigating natural language. </title> <type> Technical Report MS-CIS-90-94, </type> <institution> University of Pennsylvania, </institution> <address> Philadelphia, PA. </address>
Reference: <author> Henderson, J. </author> <year> (1994). </year> <title> Description Based Parsing in a Connectionist Network. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <address> Philadelphia, PA. </address> <note> Technical Report MS-CIS-94-46. </note>
Reference-contexts: This other constraint limits the storage of relationships between variables, and is related to the way the locality constraint on rules limits the processing of relationships between variables. See (Shastri, Ajjanagadde, 1993) for a discussion of this constraint, and <ref> (Henderson, 1994) </ref> for a discussion of how it applies to the work presented here. 5 whole (as was illustrated in section 2), but computations which require the manipulation of pairs of variables (or triples, etc.) cannot be directly implemented. <p> The locality constraint on rules also has significant consequences, and these consequences will be mentioned, but they will not be the focus of discussion. See <ref> (Henderson, 1994) </ref> for an extensive discussion of these issues. 4 The Parsing Model The argument for the adequacy and linguistic significance of the Shastri and Ajjanagadde connectionist architecture is made using a specific example of a parser implemented in this architecture. <p> Thus this alternative has not been pursued, although perhaps the constrained use of such predicates would be feasible. 12 This discussion is a slight simplification. In the complete model <ref> (Henderson, 1994) </ref>, the parser can wait for information about the immediately following word in cases where it isn't sure which grammar entry to pick. <p> Most of this section discusses these phenomena and the empirical tests which have been performed on them. To argue that an architecture makes linguistically significant predictions, it is simply necessary to provide examples of such predictions. These results will be outlined in the discussion of the tests. See <ref> (Henderson, 1994) </ref> for an extensive discussion of these results. While it is important to pay particular attention to phenomena where the limitations of the architecture are likely to be significant, it is also necessary to guard against errors in the identification of these phenomena.
Reference: <author> John, M. S. and McClelland, J. </author> <year> (1992). </year> <title> Parallel constraint satisfaction as a comprehension mechanism. </title> <editor> In Reilly, R. and Sharkey, N., editors, </editor> <booktitle> Connectionist Approaches to Natural Language Processing, </booktitle> <pages> pages 97-136. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hove, U.K. </address> <note> 19 Kroch, </note> <author> A. </author> <year> (1989). </year> <title> Assymetries in long distance extraction in a tree adjoining grammar. </title> <editor> In Baltin, M. and Kroch, A., editors, </editor> <title> Alternative Conceptions of Phrase Structure. </title> <publisher> University of Chicago Press. </publisher>
Reference-contexts: Like recurrent connectionist networks, these rules may compute sequentially in time, and the order of input items is represented by presenting the input sequentially in time. Such networks have been used to parse simple syntactic constructions (Elman, 1991) and to model the interaction of syntactic and semantic constraints <ref> (St. John, McClelland, 1992) </ref>, but because they are unable to capture all the relevant generalizations, they have been unable to handle the full diversity and complexity of natural language syntax.
Reference: <author> Marcus, M. </author> <year> (1980). </year> <title> A Theory of Syntactic Recognition for Natural Language. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference-contexts: The parser cannot explicitly represent disjunction, which in conjunction with the requirement that the parser's output be incrementally interpretable, means the parser must be deterministic in the sense proposed by Marcus <ref> (Marcus, 1980) </ref>. Also, rules implemented in the network can only test and modify information about a single phrase structure node and the phrase structure tree as a whole. This locality constraint prevents rules which manipulate pairs of nodes, which has some significant linguistic implications that have not previously been investigated. <p> Another interesting correlation with previously proposed computational constraints on natural language is due to the restriction on disjunction and the requirement for incremental monotonic output. These constraints imply that the syntactic parser must parse deterministically. This constraint was first proposed by Marcus <ref> (Marcus, 1980) </ref>, and has been argued for by several researchers since ((Church 1980), (Marcus, et.al. 1983), (Berwick, Weinberg, 1984)). It requires that the parser deterministically pursue a single analysis.
Reference: <author> Marcus, M., Hindle, D., and Fleck, M. </author> <year> (1983). </year> <title> D-theory: Talking about talking about trees. </title> <booktitle> In Proceedings of the 21st Annual Meeting of the ACL, </booktitle> <address> Cambridge, MA. </address>
Reference-contexts: Despite the computational constraints imposed by the S&A architecture, the architecture is powerful enough for syntactic parsing. This argument is made using a specific parsing model which has been implemented in the architecture and tested on a broad range of natural language phenomena. Following Description Theory <ref> (Marcus, et al., 1983) </ref>, this parsing model uses partial descriptions of phrase structure trees to allow deterministic parsing. <p> These constraints imply that the syntactic parser must parse deterministically. This constraint was first proposed by Marcus (Marcus, 1980), and has been argued for by several researchers since ((Church 1980), <ref> (Marcus, et.al. 1983) </ref>, (Berwick, Weinberg, 1984)). It requires that the parser deterministically pursue a single analysis. This means that multiple analyses can't be pursued in parallel, and that once the parser commits to an aspect of the analysis it can't retract that commitment. <p> First, because the parser must be deterministic, the representation should allow the parser to avoid saying what it doesn't know. Following Description Theory <ref> (Marcus, et al., 1983) </ref>, partial descriptions of phrase structure trees are used to satisfy this requirement. Partial descriptions allow the parser to underspecify phrase structure information, rather than either overcommitting or using a disjunction of more completely specified alternatives. <p> Structure Unification Grammar (SUG) is a formalization of accumulating partial information about the phrase structure of a sentence until a complete description of the sentence's phrase structure tree is constructed. As such it is similar to other unification based or constraint based grammar formalisms. These include Description Theory <ref> (Marcus, et al., 1983) </ref>, Head-Driven Phrase Structure Grammar (Pollard, Sag, 1987), Construction Grammar (Fillmore, et al., 1988), and Segment Grammar (de Smedt, Kempen, 1991), among others. Like these other formalisms, SUG allows multiple kinds of grammatical features to be specified independently of each other.
Reference: <author> Miller, G. A. </author> <year> (1956). </year> <title> The magical number seven plus or minus two. </title> <journal> Psychological Review, </journal> <volume> 63 </volume> <pages> 81-96. </pages>
Reference-contexts: These limitations impose computational constraints on syntactic parsing in this architecture. Interestingly, most of these constraints have previously been proposed based on linguistic and psychological data. The parser must have a bounded memory (Chomsky, 1959), and in particular can only store information about a bounded number of things <ref> (Miller, 1956) </ref>. The parser cannot explicitly represent disjunction, which in conjunction with the requirement that the parser's output be incrementally interpretable, means the parser must be deterministic in the sense proposed by Marcus (Marcus, 1980). <p> Miller proposed a bound of seven plus or minus two on the number of things which can be stored in short term memory <ref> (Miller, 1956) </ref>, and this result has been replicated for a surprising number of tasks. <p> In fact, the maximum number of nonterminals required was nine, given the compact phrase structure representation used here. This is interesting because nine is the maximum of the robust bound on human short term memory of seven plus or minus two <ref> (Miller, 1956) </ref>. The data structures which are used to handle the locality constraint on rules also result in some bounds.
Reference: <author> Pesetsky, D. </author> <year> (1982). </year> <title> Paths and Categories. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address> <note> [distributed by MIT Working Papers in Linguistics, </note> <institution> Dept of Linguistics and Philosophy, MIT]. </institution>
Reference-contexts: Thus by accounting for this phenomena with a computational constraint, the competence theory of long distance dependencies can be simplified. This explanation for wh- island constraints is also interesting in that it subsumes Pesetsky's path containment condition <ref> (Pesetsky, 1982) </ref>. In summary, all the data in (Kroch, 1989) was correctly categorized, mostly by adopting the same analyses, and some of the phenomena were predicted by the computational constraints imposed by the S&A architecture.
Reference: <author> Pollard, C. and Sag, I. A. </author> <year> (1987). </year> <title> Information-Based Syntax and Semantics. Vol 1: Fundamentals. Center for the Study of Language and Information, </title> <address> Stanord, CA. </address>
Reference-contexts: As such it is similar to other unification based or constraint based grammar formalisms. These include Description Theory (Marcus, et al., 1983), Head-Driven Phrase Structure Grammar <ref> (Pollard, Sag, 1987) </ref>, Construction Grammar (Fillmore, et al., 1988), and Segment Grammar (de Smedt, Kempen, 1991), among others. Like these other formalisms, SUG allows multiple kinds of grammatical features to be specified independently of each other.
Reference: <author> Rumelhardt, D. E., McClelland, J. L., </author> <title> and the PDP Reseach group (1986). Parallel Distributed Processing: </title> <journal> Explorations in the microstructure of cognition, </journal> <volume> Vol 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Since these predicate units represent a feature decomposition of the types of nodes, and these features are only interpreted within this module, the predicate units are analogous to hidden units in Parallel Distributed Processing <ref> (Rumelhardt, McClelland, 1986) </ref> networks. The network's input units are just a stand-in for the word recognition component of the system. There is one input unit per word.
Reference: <author> Shastri, L. and Ajjanagadde, V. </author> <year> (1993). </year> <title> From simple associations to systematic reasoning: A connectionist representation of rules, variables, and dynamic bindings using temporal synchrony. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 16 </volume> <pages> 417-451. </pages>
Reference-contexts: Recent work on how to support symbolic computation within a connectionist computational architecture has made the combination of these abilities possible, but these architectures have limitations. This article discusses one such computational architecture, proposed by Shastri and Ajjanagadde <ref> (Shastri, Ajjanagadde, 1993) </ref>, and the implications of its limitations for syntactic parsing. These limitations do not prevent syntactic parsing, and they make some significant linguistic predictions. <p> Because this article concentrates on the bounded memory and determinism constraints, these results will only be outlined here. 2 The Connectionist Architecture The Shastri and Ajjanagadde connectionist computational architecture has several characteristics which make it well suited for investigating natural language parsing. As argued in <ref> (Shastri, Aj 2 janagadde, 1993) </ref>, the architecture is biologically motivated, supports the massively parallel use of knowledge, supports evidential reasoning, has psychologically plausible limitations, and supports symbolic computation. <p> Thus predications can only be stored for a bounded number of variables. From biological evidence this bound is at most ten, probably a little less <ref> (Shastri, Ajjanagadde, 1993) </ref>. For this investigation the bound will be assumed to be ten. Another significant limitation of the S&A architecture is that it has no explicit representation of logical connectives. Thus only the default logical connective can be used. <p> This other constraint limits the storage of relationships between variables, and is related to the way the locality constraint on rules limits the processing of relationships between variables. See <ref> (Shastri, Ajjanagadde, 1993) </ref> for a discussion of this constraint, and (Henderson, 1994) for a discussion of how it applies to the work presented here. 5 whole (as was illustrated in section 2), but computations which require the manipulation of pairs of variables (or triples, etc.) cannot be directly implemented. <p> The bound given above is precisely the same form of constraint, and although here I'm assuming ten things can be stored, Miller's results are within the resolution of the biological arguments which were used to derive that bound. See <ref> (Shastri, Ajjanagadde, 1993) </ref> for a more extensive discussion of this relationship. Another interesting correlation with previously proposed computational constraints on natural language is due to the restriction on disjunction and the requirement for incremental monotonic output. These constraints imply that the syntactic parser must parse deterministically. <p> Due to the scope of these topics, they will have to be left for future work. 6 Conclusion This article has discussed syntactic parsing using a model of symbolic computation in a connectionist network recently proposed by Shastri and Ajjanagadde <ref> (Shastri, Ajjanagadde, 1993) </ref>. This connectionist model of computation extends previous connectionist architectures by using temporal synchrony variable binding to represent the identities of entities in a way that allows rules to generalize over entities.
Reference: <author> Smolensky, P. </author> <year> (1990). </year> <title> Tensor product variable binding and the representation of symbolic structures in connectionist systems. </title> <journal> Artificial Intelligence, 46(1-2):159-216. </journal>
Reference-contexts: Thus the rules of the parser inherently generalize across phrase structure constituents. It is this ability to capture generalizations which distinguishes the S&A architecture from other solutions to the variable binding problem, such as <ref> (Smolensky, 1990) </ref>. An example of computation in the S&A architecture is also given in figure 1.
Reference: <author> Steedman, M. </author> <year> (1987). </year> <title> Combinatory grammars and parasitic gaps. </title> <booktitle> Natural Language and Linguistic Theory, </booktitle> <volume> 5 </volume> <pages> 403-439. </pages>
Reference-contexts: In particular, NNEP cannot parse coordinations, or gapping in comparatives, and it cannot make many disambiguation decisions. As argued above, this architecture is well suited for doing disambiguation. I expect that the relationship between SUG and Combinatory Categorial Grammar <ref> (Steedman, 1987) </ref> will make the analyses of coordination and gapping easier for this parsing model than for most phrase structure based parsers.
Reference: <author> Stevenson, S. </author> <year> (1994). </year> <title> Competition and disambiguation in a hybrid network model of human parsing. </title> <journal> Journal of Psycholinguistic Research, </journal> <volume> 23(6). </volume> <pages> 20 </pages>
Reference-contexts: The input activation provided to the grammar units by the above patterns is used to choose what parser action to perform next. The nature of the arbitration network that should be used to make this disambiguation decision has not been significantly addressed in this work, but see <ref> (Stevenson, 1994) </ref> for an investigation of these issues. Once this choice has been made, the grammar unit for the chosen parser action fires in the phase of the chosen site.
References-found: 19

