URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1993/tr-93-082.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1993.html
Root-URL: http://www.icsi.berkeley.edu
Title: All-to-all Broadcast on the CNS-1  
Author: Silvia M. Muller 
Note: The CNS-1 (Connectionist Network Supercomputer) project is a collaboration of the  
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  Germany.  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  University of California at Berkeley and the International Computer Science Institute ICSI and CS Department, University of Saarland,  
Pubnum: TR-93-082  
Email: E-mail smueller@icsi.berkeley.edu  smueller@cs.uni-sb.de  
Phone: 1-510-642-4274 FAX 1-510-643-7684  
Web: or  
Date: December 1993  
Abstract: This study deals with the all-to-all broadcast on the CNS-1. We determine a lower bound for the run time and present an algorithm meeting this bound. Since this study points out a bottleneck in the network interface, we also analyze the performance of alternative interface designs. Our analyses are based on a run time model of the network. 
Abstract-found: 1
Intro-found: 1
Reference: [ABC + 93] <author> K. Asanovic, J. Beck, T. Callahan, J. Feldman, B. Irissou, B. Kingsbury, P. Kohn, J. Lazzaro, N. Morgan, D. Stoutamire, and J. Wawrzynek. </author> <title> CNS-1 architecture specification. </title> <type> Technical Report TR-93-021, </type> <institution> International Computer Science Institute and UC Berkeley, </institution> <year> 1993. </year>
Reference-contexts: Without loss of generality, we assume that a vector element is four bytes. If the elements are only one byte, four elements can be packed into one word. This packing can be done with some loads from and stores to the on-chip data cache. The CNS as described in <ref> [ABC + 93, AC93] </ref> is still in design. We therefore base our analyses on the run time model developed in [Mul93]. In the next section we sketch the network and its timing, but for more details we refer to [ABC + 93, AC93]. 2 Network overview The CNS has a cylindrical <p> The CNS as described in <ref> [ABC + 93, AC93] </ref> is still in design. We therefore base our analyses on the run time model developed in [Mul93]. In the next section we sketch the network and its timing, but for more details we refer to [ABC + 93, AC93]. 2 Network overview The CNS has a cylindrical topology. This is a mesh with wraparound in only one dimension. Each ring holds 32 nodes. Neighboring nodes are connected by a bidirectional link, with a capacity of 8b 1 per CPU cycle and direction. <p> The computation time C min does not include memory access times. Including memory access times, the computation will hardly be faster than 8 C min , at least with the RDRAM memory system presented in <ref> [ABC + 93, AC93] </ref>. The total run time then adds up to about 12 C min . 7 Conclusion The bandwidth between the processor and the network is the same as the bandwidth between two network nodes. That results in a processor-bound CNS.
Reference: [AC93] <author> K. Asanovic and T. Callahan. </author> <title> Torrent Architecture Manual. </title> <institution> International Computer Science Institute and UC Berkeley, </institution> <year> 1993. </year> <title> Internal document, </title> <journal> revisions 1.5/1.9. </journal> <volume> 9 </volume>
Reference-contexts: Without loss of generality, we assume that a vector element is four bytes. If the elements are only one byte, four elements can be packed into one word. This packing can be done with some loads from and stores to the on-chip data cache. The CNS as described in <ref> [ABC + 93, AC93] </ref> is still in design. We therefore base our analyses on the run time model developed in [Mul93]. In the next section we sketch the network and its timing, but for more details we refer to [ABC + 93, AC93]. 2 Network overview The CNS has a cylindrical <p> The CNS as described in <ref> [ABC + 93, AC93] </ref> is still in design. We therefore base our analyses on the run time model developed in [Mul93]. In the next section we sketch the network and its timing, but for more details we refer to [ABC + 93, AC93]. 2 Network overview The CNS has a cylindrical topology. This is a mesh with wraparound in only one dimension. Each ring holds 32 nodes. Neighboring nodes are connected by a bidirectional link, with a capacity of 8b 1 per CPU cycle and direction. <p> The computation time C min does not include memory access times. Including memory access times, the computation will hardly be faster than 8 C min , at least with the RDRAM memory system presented in <ref> [ABC + 93, AC93] </ref>. The total run time then adds up to about 12 C min . 7 Conclusion The bandwidth between the processor and the network is the same as the bandwidth between two network nodes. That results in a processor-bound CNS.
Reference: [Cal93] <author> T. Callahan. </author> <title> CNS-1 Networks, October 1993. </title> <booktitle> Talk at the International Com--puter Science Institute, </booktitle> <address> Berkeley CA. </address>
Reference: [FL91] <author> P. Fraigniaud and E. Lazard. </author> <title> Methods and Problems of Communication in Usual Networks. </title> <institution> Ecole Normale Superieure de Lyon, France, </institution> <year> 1991. </year> <title> Internal document, </title> <month> October. </month>
Reference: [GM94] <author> B. A. Gomes and S. M. Muller. </author> <title> A performance analysis of CNS on sparse connectionist networks. </title> <type> Technical report, </type> <institution> International Computer Science Institute and UC Berkeley, 1993/94. </institution>
Reference-contexts: a high all-to-all broadcast performance. 6 Performance impact We now analyze how the broadcast time influences the performance of the CNS, when evaluating the activations in a sparse neural network with half a million units having an average of 500 connections per unit. 7 Communication In the parallelization described in <ref> [GM94] </ref>, the weights and activations are equally spread over the p processors but the processors need all activations for their computation. The activations are only one byte wide, and so four of them can be packed into one memory word. <p> That does not include time for main memory accesses and so it is a very optimistic lower run time bound for the computation. On the other hand, the analysis in <ref> [GM94] </ref> gives an upper bound of C max = 5 10 8 ' cycles. This run time is 24 times slower than the best case, but it is very difficult to vectorize the indexed memory accesses, the most timeconsuming part of the code.
Reference: [Mul93] <author> S. M. Muller. </author> <title> A performance analysis of the CNS-1 on large, dense backpropagation networks. </title> <type> Technical Report TR-93-046, </type> <institution> International Computer Science Institute, Berkeley, </institution> <year> 1993. </year> <month> 10 </month>
Reference-contexts: This packing can be done with some loads from and stores to the on-chip data cache. The CNS as described in [ABC + 93, AC93] is still in design. We therefore base our analyses on the run time model developed in <ref> [Mul93] </ref>. In the next section we sketch the network and its timing, but for more details we refer to [ABC + 93, AC93]. 2 Network overview The CNS has a cylindrical topology. This is a mesh with wraparound in only one dimension. Each ring holds 32 nodes. <p> In the worst case, it takes 28 cycles to load vlr words into the CPU and 23 cycles to write them back. The values are based on the memory model presented in <ref> [Mul93] </ref>. Message overhead and memory accesses can largely be overlapped. After scheduling the load (store), the processor can execute the receive (send). Message overhead and memory accesses therefore keep the processor busy for no more than max (28 + 23; 21) = 51 cycles.
References-found: 6

