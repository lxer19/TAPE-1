URL: http://www.cs.arizona.edu/people/udi/CAS.ps
Refering-URL: http://www.cs.arizona.edu/people/udi/
Root-URL: http://www.cs.arizona.edu
Email: udi@cs.arizona.edu  
Title: A TEXT COMPRESSION SCHEME THAT ALLOWS FAST SEARCHING DIRECTLY IN THE COMPRESSED FILE  
Author: Udi Manber 
Note: TO APPEAR IN CACM  
Date: March 1993 (Revised September 1994)  
Address: Tucson, AZ 85721  
Affiliation: Department of Computer Science University of Arizona  
Abstract: A new text compression scheme is presented in this paper. The main purpose of this scheme is to speed up string matching by searching the compressed file directly. The scheme requires no modification of the string-matching algorithm, which is used as a black box; any string-matching procedure can be used. Instead, the pattern is modified; only the outcome of the matching of the modified pattern against the compressed file is decompressed. Since the compressed file is smaller than the original file, the search is faster both in terms of I/O time and processing time than a search in the original file. For typical text files, we achieve about 30% reduction of space and slightly less of search time. A 30% space saving is not competitive with good text compression schemes, and thus should not be used where space is the predominant concern. The intended applications of this scheme are files that are searched often, such as catalogs, bibliographic files, and address books. Such files are typically not compressed, but with this scheme they can remain compressed indefinitely, saving space while allowing faster search at the same time. A particular application to an information retrieval system that we developed is also discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [AB92a] <author> Amir, A, and G. Benson, </author> <title> `Two-dimensional periodicity and its application,'' </title> <booktitle> Proc. of the 3rd Symp. on Discrete Algorithms, </booktitle> <address> Orlando Florida (January 1992), </address> <pages> pp. 440-452. 11 </pages>
Reference-contexts: This makes it much easier for programs that handle text to adapt to handle our compressed text. Searching in compressed files was studied, in a theoretical framework, by several authors. Eilam-Tsoreff and Vishkin [EV88] looked at run-length compression, Amir, Landau and Vishkin [ALV92] and Amir and Benson <ref> [AB92a, AB92b] </ref> looked at two-dimensional matching, and very recently Amir, Benson and Farach [ABF94] looked at LZW compression. While these authors have looked at some theoretical questions, the practical side has not been sufficiently addressed yet.
Reference: [AB92b] <author> Amir, A, and G. Benson, </author> <title> ``Efficient two dimensional compressed matching,'' </title> <booktitle> Proc. of the Data Compression Conference, </booktitle> <address> Snowbird Utah (March 1992), </address> <pages> pp. 279-288. </pages>
Reference-contexts: This makes it much easier for programs that handle text to adapt to handle our compressed text. Searching in compressed files was studied, in a theoretical framework, by several authors. Eilam-Tsoreff and Vishkin [EV88] looked at run-length compression, Amir, Landau and Vishkin [ALV92] and Amir and Benson <ref> [AB92a, AB92b] </ref> looked at two-dimensional matching, and very recently Amir, Benson and Farach [ABF94] looked at LZW compression. While these authors have looked at some theoretical questions, the practical side has not been sufficiently addressed yet.
Reference: [ABF94] <author> Amir, A, G. Benson, and M. Farach, </author> <title> ``Let sleeping files lie: pattern matching in Z-compressed files,'' </title> <booktitle> Proc. of the 5rd Symp. on Discrete Algorithms, </booktitle> <month> (January </month> <year> 1994), </year> <note> to appear. </note>
Reference-contexts: Searching in compressed files was studied, in a theoretical framework, by several authors. Eilam-Tsoreff and Vishkin [EV88] looked at run-length compression, Amir, Landau and Vishkin [ALV92] and Amir and Benson [AB92a, AB92b] looked at two-dimensional matching, and very recently Amir, Benson and Farach <ref> [ABF94] </ref> looked at LZW compression. While these authors have looked at some theoretical questions, the practical side has not been sufficiently addressed yet. It is not clear, for example, whether in practice the compressed search in [ABF94] will indeed be faster than a regular decompression followed by a fast search. <p> Benson [AB92a, AB92b] looked at two-dimensional matching, and very recently Amir, Benson and Farach <ref> [ABF94] </ref> looked at LZW compression. While these authors have looked at some theoretical questions, the practical side has not been sufficiently addressed yet. It is not clear, for example, whether in practice the compressed search in [ABF94] will indeed be faster than a regular decompression followed by a fast search. As far as we know [Fa93], no implementation of these algorithms exist. In this paper we concentrate on practical issues in this area. <p> In this paper we concentrate on practical issues in this area. In particular, we compare our algorithms to Boyer-Moore pattern matching, which is typically 5-7 times faster in practice (for natural language texts) than the Knuth-Morris-Pratt approach (which is taken in <ref> [ABF94] </ref>). Speeding up the fast Boyer-Moore searching is a real challenge. For this reason we also rejected Huffman's and other bit-level compression, because we want to keep the search on a byte level for efficiency. Several schemes have been suggested for compression of full-text information-retrieval systems ([KBD89], [WBN91], [MZ92]).
Reference: [AC75] <author> Aho, A. V., and M. J. Corasick, </author> <title> ``Efficient string matching: an aid to bibliographic search'', </title> <journal> Communications of the ACM, </journal> <month> 18 (June </month> <year> 1975), </year> <pages> pp. 333-340. </pages>
Reference-contexts: With our scheme the newsfeed can come in compressed, resulting in faster filtering. (Although we discussed searching single patterns here, our scheme will work for searching multiple patterns with either the Aho-Corasick algorithm used in fgrep <ref> [AC75] </ref> or with agrep's algorithm [WM94].) 5. Future Work A natural improvement attempt to our compression scheme is to consider more than just two characters at a time. This is typical of fixed dictionary schemes (e.g., [BCW90]).
Reference: [BER76] <author> Bitner J. R., G. Erlich, and E. M. Reingold, </author> <title> ``Efficient generation of the binary reflected Gray code and its applications,'' </title> <journal> Communications of the ACM, </journal> <month> 19 (September </month> <year> 1976), </year> <pages> pp. 517-521. </pages>
Reference-contexts: The running times are in seconds on a DEC 5000/240. To verify that the solutions obtained from our algorithm are not too far from optimal, we also implemented a deterministic algorithm that guarantees the best solution. This algorithm is exponential, but by using binary reflected Gray codes <ref> [BER76] </ref> it was possible to run experiments for up to 30 vertices, which is not too far from real data. In all our tests the solution found by the random algorithm was indeed the optimal.
Reference: [BCW90] <author> Bell, T. G., J. G. Cleary, and I. H. Witten, </author> <title> Text Compression, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ (1990). </address>
Reference-contexts: Both are very fast. This kind of compression is not as good as more modern techniques such as the Lempel-Ziv based algorithms [ZL77, We84] or context-modeling algorithms <ref> [BCW90] </ref>. However, context-dependent techniques cannot perform string matching without keeping track 3 of the compression mechanism at the same time, because the compression depends on previous text. <p> Future Work A natural improvement attempt to our compression scheme is to consider more than just two characters at a time. This is typical of fixed dictionary schemes (e.g., <ref> [BCW90] </ref>). The problem is to find some restrictions on the common strings that will prevent overlaps and will be easy to maintain. The same approach may also be used for approximate matching, although the problem is more difficult.
Reference: [BM77] <author> Boyer R. S., and J. S. Moore, </author> <title> ``A fast string searching algorithm,'' </title> <journal> Communications of the ACM, </journal> <month> 20 (October </month> <year> 1977), </year> <pages> pp. 762-772. </pages>
Reference-contexts: is typically an order of magnitude slower than string matching. (We may save IO time by reading less with a better compression scheme, but the extra processing time will exceed the savings.) Speeding up string matching is a difficult goal because string matching is already extremely efficient; with Boyer-Moore filtering <ref> [BM77] </ref>, the matching procedure skips many characters and only a fraction of the text is looked at. As we will show, a pattern-substitution method can be modified to allow a direct search in the compressed file.
Reference: [EV88] <author> Eilam-Tsoreff T., and U. Vishkin, </author> <title> ``Matching patterns in a string subject to multilinear transformations,'' </title> <booktitle> Proc. of the Int. Workshop on Sequences, Combinatorics, Compression, Security, and Transmission, Salerno, </booktitle> <address> Italy (June 1988). </address>
Reference-contexts: This makes it much easier for programs that handle text to adapt to handle our compressed text. Searching in compressed files was studied, in a theoretical framework, by several authors. Eilam-Tsoreff and Vishkin <ref> [EV88] </ref> looked at run-length compression, Amir, Landau and Vishkin [ALV92] and Amir and Benson [AB92a, AB92b] looked at two-dimensional matching, and very recently Amir, Benson and Farach [ABF94] looked at LZW compression. While these authors have looked at some theoretical questions, the practical side has not been sufficiently addressed yet.
Reference: [Fa93] <author> Farach M., </author> <title> private communication (October 1993). </title>
Reference-contexts: It is not clear, for example, whether in practice the compressed search in [ABF94] will indeed be faster than a regular decompression followed by a fast search. As far as we know <ref> [Fa93] </ref>, no implementation of these algorithms exist. In this paper we concentrate on practical issues in this area. In particular, we compare our algorithms to Boyer-Moore pattern matching, which is typically 5-7 times faster in practice (for natural language texts) than the Knuth-Morris-Pratt approach (which is taken in [ABF94]).
Reference: [GJ79] <author> Garey M. R., and D. S. Johnson, </author> <title> Computers and Intractability, A Guide to the Theory of NP-completeness, </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco, CA, </address> <year> 1979. </year>
Reference-contexts: The formulation above is a very clean abstraction of our problem, but unfortunately, it is also a very difficult problem. It turns out that the problem is NP-complete even for unit weights and undirected graphs (by a reduction from the Bipartite Subgraph problem <ref> [GJ79, p. 196] </ref>). But, on the other hand, the graphs we are dealing with are not too large. The important edges correspond to common pairs and not too many characters appear in common pairs. In fact, the number of unique characters overall is quite limited.
Reference: [GM94] <author> B. Gopal, and U. Manber, </author> <title> ``A Fixed-Dictionary Approach to Fast Searching in Compressed Files,'' </title> <note> submitted for publication. </note>
Reference-contexts: The scheme presented in this paper can be used for the exact matching part. Recently we implemented a different scheme that achieves better compression (45-50%) and faster search, but requires a much larger dictionary <ref> [GM94] </ref>. It essentially translates many words into short representations and uses a fixed dictionary for the translation. A search first translates the pattern (assuming it is a complete word) into its encoding and then searches for the encoding directly.
Reference: [Je76] <author> Jewell G. C., </author> <title> ``Text compaction for information retrieval systems,'' </title> <journal> IEEE SMC Newsletter, </journal> <month> 5 (February </month> <year> 1976). </year>
Reference-contexts: We then discuss how the search is done and present experimental results for different search routines. We conclude with a discussion of possible applications. 2. The Compression Algorithm The basis of the compression is a very simple pattern-substitution method that has been used by Snyderman and Hunt [SH70], Jewell <ref> [Je76] </ref>, and most likely reinvented by many others. The goal is to substitute common pairs of characters with special symbols that are still encoded in one byte. A byte allows 256 possible encodings, but typical text uses much fewer.
Reference: [KBD89] <author> Klein, S.T., A. Bookstein, and S. Deerwester, </author> <title> ``Storing text retrieval systems on CD-ROM: compression and encryption considerations,'' </title> <journal> ACM Trans. on Information Systems, </journal> <month> 7 (July </month> <year> 1989), </year> <pages> pp. 230-245. </pages>
Reference: [MW94] <author> Manber U. and S. Wu, ``GLIMPSE: </author> <title> A Tool to Search Through Entire File Systems,'' </title> <booktitle> Usenix Winter 1994 Technical Conference, </booktitle> <address> San Francisco (January 1994), </address> <pages> pp. 23-32. </pages>
Reference-contexts: It is not suitable, however, for very large information bases. We will assume throughout the paper that the search is sequential. Sequential search occurs in many other applications, and in particular, it plays a key role in an information-retrieval system that we designed <ref> [MW94] </ref>. We discuss this in section 4. We also assume that we are dealing with English text and the ASCII character set. We first present the idea behind the compression scheme, and then describe in detail the algorithms involved and their implementation. <p> Applications Our scheme can be used for many applications that require searching. We highlight two possible applications of this scheme. In <ref> [MW94] </ref> we present a design of a multi-level information retrieval system called glimpse (available by anonymous ftp from cs.arizona.edu directory glimpse).
Reference: [MZ92] <author> Moffat A., and J. Zobel, </author> <title> ``Coding for compression in full-text retrieval systems,'' </title> <booktitle> Proc. IEEE Data Compression Conference (1992), </booktitle> <pages> pp. 72-81. </pages>
Reference-contexts: Speeding up the fast Boyer-Moore searching is a real challenge. For this reason we also rejected Huffman's and other bit-level compression, because we want to keep the search on a byte level for efficiency. Several schemes have been suggested for compression of full-text information-retrieval systems ([KBD89], [WBN91], <ref> [MZ92] </ref>). Moffat and Zobel [MZ92] is the most promising of the three. They first build a regular inverted index and then use a Huffman-encoding type compression (word based) for the text and a suffix compression for the inverted index. <p> For this reason we also rejected Huffman's and other bit-level compression, because we want to keep the search on a byte level for efficiency. Several schemes have been suggested for compression of full-text information-retrieval systems ([KBD89], [WBN91], <ref> [MZ92] </ref>). Moffat and Zobel [MZ92] is the most promising of the three. They first build a regular inverted index and then use a Huffman-encoding type compression (word based) for the text and a suffix compression for the inverted index.
Reference: [SH70] <author> Snyderman, M. and B. Hunt, </author> <title> ``The myriad virtues of text compression,'' </title> <journal> Datamation, </journal> <volume> 16 (1970), </volume> <pages> pp. 36-40. </pages>
Reference-contexts: We then discuss how the search is done and present experimental results for different search routines. We conclude with a discussion of possible applications. 2. The Compression Algorithm The basis of the compression is a very simple pattern-substitution method that has been used by Snyderman and Hunt <ref> [SH70] </ref>, Jewell [Je76], and most likely reinvented by many others. The goal is to substitute common pairs of characters with special symbols that are still encoded in one byte. A byte allows 256 possible encodings, but typical text uses much fewer.
Reference: [WBN91] <author> Witten, I. H., T. C. Bell, and C. G. Nevill, </author> <title> ``Models for compression in full-text retrieval systems,'' </title> <booktitle> Proc. of the Data Compression Conference, </booktitle> <address> Snowbird, Utah (April 1991), </address> <pages> pp. 23-32. </pages>
Reference-contexts: Speeding up the fast Boyer-Moore searching is a real challenge. For this reason we also rejected Huffman's and other bit-level compression, because we want to keep the search on a byte level for efficiency. Several schemes have been suggested for compression of full-text information-retrieval systems ([KBD89], <ref> [WBN91] </ref>, [MZ92]). Moffat and Zobel [MZ92] is the most promising of the three. They first build a regular inverted index and then use a Huffman-encoding type compression (word based) for the text and a suffix compression for the inverted index.
Reference: [We84] <author> Welch, T. A., </author> <title> ``A technique for high-performance data compression,'' </title> <booktitle> IEEE Computer, </booktitle> <month> 17 (June </month> <year> 1984), </year> <pages> pp. 8-19. </pages>
Reference-contexts: The compression consists of substituting each of the common pairs with the special byte allocated for it, and decompression is achieved by reversing this procedure. Both are very fast. This kind of compression is not as good as more modern techniques such as the Lempel-Ziv based algorithms <ref> [ZL77, We84] </ref> or context-modeling algorithms [BCW90]. However, context-dependent techniques cannot perform string matching without keeping track 3 of the compression mechanism at the same time, because the compression depends on previous text. <p> It is therefore possible to adapt a text editor to read and edit a compressed file directly. Our compressed files save about 30% space. If searching is not needed, there are much better compression algorithms, such as UNIX compress, based on the Lempel-Ziv-Welch algorithm <ref> [We84] </ref>, which can achieve 50-60% reduction. We tested whether compressing our compressed files further with UNIX compress lead to the same compression rates as compressing the original file, and found that we got better compression rates.
Reference: [WM92] <author> Wu S., and U. Manber, </author> <title> ``Fast Text Searching Allowing Errors,'' </title> <booktitle> Communications of the ACM 35 (October 1992), </booktitle> <pages> pp. 83-91. 12 </pages>
Reference-contexts: We present the results for three large texts and two representative search programs. The first program is fgrep, which does not use Boyer-Moore filtering, and the second is our own agrep <ref> [WM92] </ref>, which does use it and is much faster as a result. We selected 100 random words from the dictionary (the effectiveness of a Boyer-Moore search depends on the pattern), and ran 100 searches for each combination of search and text. We give the average running times in Figure 4. <p> Therefore, we cannot simply match the compressed pattern to the compressed file. However, there are efficient techniques for reducing an approximate matching problem to a different exact matching problem. A simple example <ref> [WM92] </ref> is to divide the pattern into k +1 parts such that any k errors will leave at least one part intact. An exact search is performed for all parts, and the output, which is hopefully much smaller, is then filtered appropriately.
Reference: [WM94] <author> Wu S., and U. Manber, </author> <title> ``A Fast Algorithm for Multi-Pattern Searching'' to appear in Software, </title> <journal> Practice & Experience. </journal>
Reference-contexts: With our scheme the newsfeed can come in compressed, resulting in faster filtering. (Although we discussed searching single patterns here, our scheme will work for searching multiple patterns with either the Aho-Corasick algorithm used in fgrep [AC75] or with agrep's algorithm <ref> [WM94] </ref>.) 5. Future Work A natural improvement attempt to our compression scheme is to consider more than just two characters at a time. This is typical of fixed dictionary schemes (e.g., [BCW90]).
Reference: [ZL77] <author> Ziv, J. and A. Lempel, </author> <title> ``A universal algorithm for sequential data compression,'' </title> <journal> IEEE Trans. on Information Theory, </journal> <month> IT-23 (May </month> <year> 1977). </year> <pages> pp. </pages> <month> 337-343. </month> <title> About the author Udi Manber is a Professor of Computer Science at the University of Arizona. He received his Ph.D in Computer Science from the University of Washington in 1982. His research interests include design of algorithms, pattern matching, computer networks, and software tools. He can be reached at udi@cs.arizona.edu. </title>
Reference-contexts: The compression consists of substituting each of the common pairs with the special byte allocated for it, and decompression is achieved by reversing this procedure. Both are very fast. This kind of compression is not as good as more modern techniques such as the Lempel-Ziv based algorithms <ref> [ZL77, We84] </ref> or context-modeling algorithms [BCW90]. However, context-dependent techniques cannot perform string matching without keeping track 3 of the compression mechanism at the same time, because the compression depends on previous text.
References-found: 21

