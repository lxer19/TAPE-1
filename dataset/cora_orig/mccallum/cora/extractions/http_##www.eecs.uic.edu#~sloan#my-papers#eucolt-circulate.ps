URL: http://www.eecs.uic.edu/~sloan/my-papers/eucolt-circulate.ps
Refering-URL: http://www.eecs.uic.edu/~sloan/papers.html
Root-URL: 
Phone: 2  
Title: Learning from Incomplete Boundary Queries Using Split Graphs and Hypergraphs (Extended Abstract)  
Author: Robert H. Sloan ? and Gyorgy Turan ?? 
Note: on Artificial Intelligence, Hungarian Academy of Sciences To appear in Proc. EuroCOLT '97, which will be held in March 1997 in Jerusalem.  
Address: 851 S. Morgan St. Rm 1120, Chicago, IL 60607-7053, USA  
Affiliation: 1 Dept. of EE Computer Science, University of Illinois at Chicago  Dept. of Mathematics, Stat., Computer Science, University of Illinois at Chicago, Research Group  
Abstract: We consider learnability with membership queries in the presence of incomplete information. In the incomplete boundary query model introduced by Blum et al. [7], it is assumed that membership queries on instances near the boundary of the target concept may receive a "don't know" answer. We show that zero-one threshold functions are efficiently learnable in this model. The learning algorithm uses split graphs when the boundary region has radius 1, and their generalization to split hypergraphs (for which we give a split-finding algorithm) when the boundary region has constant radius greater than 1. We use a notion of indistinguishability of concepts that is appropriate for this model.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> Apr. </month> <year> 1988. </year>
Reference-contexts: The first step is the appropriate noisy variation of the "reduce" algorithm that is common to many algorithms using membership queries to learn some form of monotone DNF (e.g., <ref> [1] </ref>). Given that 1 n has IBQ 1, we set v to 1 n . Now, so long as v has some child to which a membership query responds 1, we replace v by that child and repeat.
Reference: 2. <author> D. Angluin and M. Krik~is. </author> <title> Learning with malicious membership queries and exceptions. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 57-66. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Email: U11557@uicvm.uic.edu. picks the subset of queries that receive a "don't know" response, and Angluin and Krik~is studied a model where an adversary picks a subset of queries that receive the wrong response <ref> [2, 3, 22] </ref>. A practical motivation for studying noisy membership queries comes from the experiments of Lang and Baum [15], which confirm the intuition that erroneous answers to membership queries are more frequent close to the boundary of the target concept.
Reference: 3. <author> D. Angluin, M. Krik~is, R. H. Sloan, and G. Turan. </author> <title> Malicious omissions and errors in answers to membership queries. </title> <journal> Machine Learning. </journal> <note> To appear. </note>
Reference-contexts: Email: U11557@uicvm.uic.edu. picks the subset of queries that receive a "don't know" response, and Angluin and Krik~is studied a model where an adversary picks a subset of queries that receive the wrong response <ref> [2, 3, 22] </ref>. A practical motivation for studying noisy membership queries comes from the experiments of Lang and Baum [15], which confirm the intuition that erroneous answers to membership queries are more frequent close to the boundary of the target concept. <p> In general, there will be exponentially many instances in the boundary region even for constant r. Let us briefly compare this model to the limited membership query model <ref> [3, 22] </ref>. On the one hand, in the limited membership query model, the oracle is allowed to respond fl on any instance it chooses, not only on boundary instances. <p> In particular, our algorithm for learning monotone DNF from limited membership plus equivalence queries could receive an exponential number of fl responses (in the "Up" phase of that algorithm <ref> [3, 22] </ref>) even if the fl responses are restricted to a boundary region of radius 1. Thus the two models appear to be incomparable. 2.1 Terminology for f0; 1g n As do others (e.g., [5, 10]), we will view f0; 1g n as a partially ordered set.
Reference: 4. <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: For PAC learning, some of the the models considered are random noise affecting only the labels of examples, arbitrary noise affecting labels, arbitrary malicious noise, and, for instance space f0; 1g n , various models involving random noise affecting the instance bits <ref> [4, 11, 21, 23] </ref>. There are several recent approaches dealing with noise in query-based models of learning, in particular, for learning with membership and equivalence queries.
Reference: 5. <author> D. Angluin and D. K. </author> <title> Slonim. Randomly fallible teachers: learning monotone DNF with an incomplete membership oracle. </title> <journal> Machine Learning, </journal> <volume> 14(1) </volume> <pages> 7-26, </pages> <year> 1994. </year>
Reference-contexts: Sakakibara [20] proposed a model where membership queries receive the wrong answer at random, but the noise is not persistent for any one instance, so repeated queries can overcome that noise. Angluin and Slonim <ref> [5] </ref> introduced a model where a random subset of the membership queries asked persistently receive the answer, "(I) don't know." Queries that may receive that response are referred to as incomplete. We previously studied the case where an adversary ? Partially supported by NSF grant CCR-9314258. <p> Thus the two models appear to be incomparable. 2.1 Terminology for f0; 1g n As do others (e.g., <ref> [5, 10] </ref>), we will view f0; 1g n as a partially ordered set. The top element is the vector 1 n and the bottom element is 0 n .
Reference: 6. <author> M. Anthony, G. Brightwell, D. Cohen, and J. Shawe-Taylor. </author> <title> On exact specification by examples. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 311-318. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: On the other hand, weighted monotone threshold functions, that is, monotone halfspaces in f0; 1g n , cannot be learned with polynomially many membership queries <ref> [6] </ref>. The (noise-free) membership query algorithm for learning zero-one threshold functions [14] is as follows. Starting with the all-ones vector 1 n , turn components to 0 as long as this still gives a positive example of the target concept.
Reference: 7. <author> A. Blum, P. Chalasani, S. A. Goldman, and D. K. </author> <title> Slonim. Learning with unreliable boundary queries. </title> <booktitle> In Proc. 8th Annu. Conf. on Comput. Learning Theory, </booktitle> <pages> pages 98-107. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1995. </year>
Reference-contexts: A practical motivation for studying noisy membership queries comes from the experiments of Lang and Baum [15], which confirm the intuition that erroneous answers to membership queries are more frequent close to the boundary of the target concept. Blum et al. <ref> [7] </ref> formulated interesting models that try to capture this phenomenon by assuming that the answers to the membership queries are all correct except in a neighborhood of a fixed radius of the boundary of the target concept. (A point is close to the boundary of the target concept if its classification <p> In this paper, we provide a step in this direction. We consider the incomplete boundary query model, which is the milder of the two models proposed in <ref> [7] </ref>, and we discuss learning from membership queries alone (thus, we do not use equivalence queries). Blum et al. [7] gave a learning algorithm for the intersection of two halfspaces in Euclidean space in the UBQ model. <p> In this paper, we provide a step in this direction. We consider the incomplete boundary query model, which is the milder of the two models proposed in <ref> [7] </ref>, and we discuss learning from membership queries alone (thus, we do not use equivalence queries). Blum et al. [7] gave a learning algorithm for the intersection of two halfspaces in Euclidean space in the UBQ model. For the Boolean case, they considered learning subclasses of monotone DNF in a modified version of the UBQ model with only one-sided error. <p> Fix a target concept C. An ordinary or complete membership query on instance v, denoted MQ (v), returns C (v). In this paper we study the incomplete boundary membership oracle, proposed by Blum et al. <ref> [7] </ref>. For the instance space f0; 1g , the distance to the boundary of instance v is the Hamming distance to the nearest instance w such that C (w) 6= C (v). <p> The requirement for successful learning in this noise model is that we must output a hypothesis that agrees with the target concept on all instances that are not in the boundary region of the target concept <ref> [7] </ref>. Moreover, the learner is allowed only a number of queries bounded by a polynomial in n, regardless of the frequency of fl responses. In general, there will be exponentially many instances in the boundary region even for constant r.
Reference: 8. <author> N. Bshouty, T. Hancock, L. Hellerstein, and M. Karpinski. </author> <title> An algorithm to learn read-once threshold formulas, and transformations between learning models. </title> <journal> Computational Complexity, </journal> <volume> 4 </volume> <pages> 37-61, </pages> <year> 1994. </year>
Reference-contexts: In particular, Heged-us showed that this class can be learned with O (n) membership queries [14]. In fact, one can even learn the larger class of read-once compositions of zero-one threshold functions (i.e., zero-one threshold formulas) with polyno-mially many membership queries <ref> [8] </ref>. On the other hand, weighted monotone threshold functions, that is, monotone halfspaces in f0; 1g n , cannot be learned with polynomially many membership queries [6]. The (noise-free) membership query algorithm for learning zero-one threshold functions [14] is as follows. <p> Can the results of this paper be extended to read-once disjunctions of zero-one threshold functions, or, more generally, to the class of zero-one threshold formulas <ref> [8] </ref>, and to the nonmonotone versions of these classes? Another problem is to extend the result of this paper to unreliable boundary queries, that is, to the case of lies instead of "don't know"s.
Reference: 9. <author> S. Foldes and P. L. Hammer. </author> <title> Split graphs. </title> <journal> Congressus Numerantium, </journal> <volume> 19 </volume> <pages> 311-315, </pages> <year> 1977. </year>
Reference-contexts: A similar idea can be used to simulate the second stage of the algorithm. The dichotomy can be formulated conveniently in terms of split graphs for the case of radius 1, and in terms of split hypergraphs in the general case. Split graphs were defined by Foldes and Hammer <ref> [9] </ref> (see also [12, 17].). Split graphs have nice characterizations and efficient algorithms. The general case of split hypergraphs does not appear to have been studied before. <p> Split graphs have a forbidden induced subgraph characterization: Split graphs are exactly those graphs which do not contain cycles of length 4 or 5, or two independent edges as induced subgraphs <ref> [9] </ref>. Peled [18] asked if split hypergraphs have a forbidden induced subgraph characterization. Another question concerns the complexity of finding a split in a split hypergraph. The algorithm described in Section 3 is far from optimal for r = 1.
Reference: 10. <author> S. A. Goldman and H. D. Mathias. </author> <title> Learning k-term DNF formulas with an incomplete membership oracle. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 77-84. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Thus the two models appear to be incomparable. 2.1 Terminology for f0; 1g n As do others (e.g., <ref> [5, 10] </ref>), we will view f0; 1g n as a partially ordered set. The top element is the vector 1 n and the bottom element is 0 n .
Reference: 11. <author> S. A. Goldman and R. H. Sloan. </author> <title> Can PAC learning algorithms tolerate random attribute noise? Algorithmica, </title> <booktitle> 14 </booktitle> <pages> 70-84, </pages> <year> 1995. </year>
Reference-contexts: For PAC learning, some of the the models considered are random noise affecting only the labels of examples, arbitrary noise affecting labels, arbitrary malicious noise, and, for instance space f0; 1g n , various models involving random noise affecting the instance bits <ref> [4, 11, 21, 23] </ref>. There are several recent approaches dealing with noise in query-based models of learning, in particular, for learning with membership and equivalence queries.
Reference: 12. <author> M. C. Golumbic. </author> <title> Algorithmic Graph Theory and Perfect Graphs. </title> <booktitle> Computer Science and Applied Mathematics. </booktitle> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: The dichotomy can be formulated conveniently in terms of split graphs for the case of radius 1, and in terms of split hypergraphs in the general case. Split graphs were defined by Foldes and Hammer [9] (see also <ref> [12, 17] </ref>.). Split graphs have nice characterizations and efficient algorithms. The general case of split hypergraphs does not appear to have been studied before. We give an algorithm to determine whether an r-uniform hypergraph is a split hypergraph, and if so, to output a split, in polynomial time.
Reference: 13. <author> Q. P. Gu and A. Maruoka. </author> <title> Learning monotone boolean functions by uniformly distributed examples. </title> <journal> SIAM J. Comput., </journal> <volume> 21 </volume> <pages> 587-599, </pages> <year> 1992. </year>
Reference-contexts: An instance v 2 f0; 1g n belongs to the concept T h k (y) if at least k of the variables (y 1 ; : : : ; y m ) have value 1 in v. Zero-one threshold functions are a well-studied class <ref> [13, 14, 16, 19] </ref>. In particular, Heged-us showed that this class can be learned with O (n) membership queries [14]. In fact, one can even learn the larger class of read-once compositions of zero-one threshold functions (i.e., zero-one threshold formulas) with polyno-mially many membership queries [8].
Reference: 14. <author> T. Heged-us. </author> <title> On training simple neural networks and small-weight neurons. </title> <booktitle> In Computational Learning Theory: Eurocolt '93, volume New Series Number 53 of The Institute of Mathematics and its Applications Conference Series, </booktitle> <pages> pages 69-82, </pages> <address> Oxford, 1994. </address> <publisher> Oxford University Press. </publisher>
Reference-contexts: An instance v 2 f0; 1g n belongs to the concept T h k (y) if at least k of the variables (y 1 ; : : : ; y m ) have value 1 in v. Zero-one threshold functions are a well-studied class <ref> [13, 14, 16, 19] </ref>. In particular, Heged-us showed that this class can be learned with O (n) membership queries [14]. In fact, one can even learn the larger class of read-once compositions of zero-one threshold functions (i.e., zero-one threshold formulas) with polyno-mially many membership queries [8]. <p> Zero-one threshold functions are a well-studied class [13, 14, 16, 19]. In particular, Heged-us showed that this class can be learned with O (n) membership queries <ref> [14] </ref>. In fact, one can even learn the larger class of read-once compositions of zero-one threshold functions (i.e., zero-one threshold formulas) with polyno-mially many membership queries [8]. <p> On the other hand, weighted monotone threshold functions, that is, monotone halfspaces in f0; 1g n , cannot be learned with polynomially many membership queries [6]. The (noise-free) membership query algorithm for learning zero-one threshold functions <ref> [14] </ref> is as follows. Starting with the all-ones vector 1 n , turn components to 0 as long as this still gives a positive example of the target concept.
Reference: 15. <author> K. J. Lang and E. B. Baum. </author> <title> Query learning can work poorly when a human oracle is used. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <address> Beijing, </address> <year> 1992. </year>
Reference-contexts: A practical motivation for studying noisy membership queries comes from the experiments of Lang and Baum <ref> [15] </ref>, which confirm the intuition that erroneous answers to membership queries are more frequent close to the boundary of the target concept.
Reference: 16. <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: An instance v 2 f0; 1g n belongs to the concept T h k (y) if at least k of the variables (y 1 ; : : : ; y m ) have value 1 in v. Zero-one threshold functions are a well-studied class <ref> [13, 14, 16, 19] </ref>. In particular, Heged-us showed that this class can be learned with O (n) membership queries [14]. In fact, one can even learn the larger class of read-once compositions of zero-one threshold functions (i.e., zero-one threshold formulas) with polyno-mially many membership queries [8].
Reference: 17. <author> N. V. R. Mahadev and U. N. Peled. </author> <title> Threshold Graphs and Related Topics, </title> <booktitle> volume 56 of Annals of Discrete Mathematics. </booktitle> <publisher> Elsevier Science B.V., </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1995. </year>
Reference-contexts: The dichotomy can be formulated conveniently in terms of split graphs for the case of radius 1, and in terms of split hypergraphs in the general case. Split graphs were defined by Foldes and Hammer [9] (see also <ref> [12, 17] </ref>.). Split graphs have nice characterizations and efficient algorithms. The general case of split hypergraphs does not appear to have been studied before. We give an algorithm to determine whether an r-uniform hypergraph is a split hypergraph, and if so, to output a split, in polynomial time. <p> We note here two useful facts about split graphs. First, there is a linear time algorithm to determine whether a graph is a split graph, and output a split if so (see <ref> [17] </ref>). Second, any split graph has at most jV j + 1 distinct splits. An r-uniform hypergraph H has the form H = (V; E), where V is a set and E is a set of r-element subsets of V .
Reference: 18. <author> U. Peled. </author> <type> Personal Communication. </type>
Reference-contexts: Split graphs have a forbidden induced subgraph characterization: Split graphs are exactly those graphs which do not contain cycles of length 4 or 5, or two independent edges as induced subgraphs [9]. Peled <ref> [18] </ref> asked if split hypergraphs have a forbidden induced subgraph characterization. Another question concerns the complexity of finding a split in a split hypergraph. The algorithm described in Section 3 is far from optimal for r = 1.
Reference: 19. <author> L. Pitt and L. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> J. ACM, </journal> <volume> 35 </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference-contexts: An instance v 2 f0; 1g n belongs to the concept T h k (y) if at least k of the variables (y 1 ; : : : ; y m ) have value 1 in v. Zero-one threshold functions are a well-studied class <ref> [13, 14, 16, 19] </ref>. In particular, Heged-us showed that this class can be learned with O (n) membership queries [14]. In fact, one can even learn the larger class of read-once compositions of zero-one threshold functions (i.e., zero-one threshold formulas) with polyno-mially many membership queries [8].
Reference: 20. <author> Y. Sakakibara. </author> <title> On learning from queries and counterexamples in the presence of noise. </title> <journal> Inform. Proc. Lett., </journal> <volume> 37 </volume> <pages> 279-284, </pages> <year> 1991. </year>
Reference-contexts: There are several recent approaches dealing with noise in query-based models of learning, in particular, for learning with membership and equivalence queries. Sakakibara <ref> [20] </ref> proposed a model where membership queries receive the wrong answer at random, but the noise is not persistent for any one instance, so repeated queries can overcome that noise.
Reference: 21. <author> R. H. Sloan. </author> <title> Four types of noise in data for PAC learning. </title> <journal> Inform. Proc. Lett., </journal> <volume> 54 </volume> <pages> 157-162, </pages> <year> 1995. </year>
Reference-contexts: For PAC learning, some of the the models considered are random noise affecting only the labels of examples, arbitrary noise affecting labels, arbitrary malicious noise, and, for instance space f0; 1g n , various models involving random noise affecting the instance bits <ref> [4, 11, 21, 23] </ref>. There are several recent approaches dealing with noise in query-based models of learning, in particular, for learning with membership and equivalence queries.
Reference: 22. <author> R. H. Sloan and G. Turan. </author> <title> Learning with queries but incomplete information. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 237-245. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Email: U11557@uicvm.uic.edu. picks the subset of queries that receive a "don't know" response, and Angluin and Krik~is studied a model where an adversary picks a subset of queries that receive the wrong response <ref> [2, 3, 22] </ref>. A practical motivation for studying noisy membership queries comes from the experiments of Lang and Baum [15], which confirm the intuition that erroneous answers to membership queries are more frequent close to the boundary of the target concept. <p> In general, there will be exponentially many instances in the boundary region even for constant r. Let us briefly compare this model to the limited membership query model <ref> [3, 22] </ref>. On the one hand, in the limited membership query model, the oracle is allowed to respond fl on any instance it chooses, not only on boundary instances. <p> In particular, our algorithm for learning monotone DNF from limited membership plus equivalence queries could receive an exponential number of fl responses (in the "Up" phase of that algorithm <ref> [3, 22] </ref>) even if the fl responses are restricted to a boundary region of radius 1. Thus the two models appear to be incomparable. 2.1 Terminology for f0; 1g n As do others (e.g., [5, 10]), we will view f0; 1g n as a partially ordered set. <p> It would also be of interest to look at the relationship between the incomplete boundary query model and the incomplete membership query model <ref> [22] </ref>, and to prove lower bounds for the number of incomplete boundary queries. Can zero-one threshold functions be learned with less than (n 3 ) incomplete boundary queries? There are some combinatorial questions related to split hypergraphs.
Reference: 23. <author> L. G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings of the 9th International Joint Conference on Artificial Intelligence, </booktitle> <volume> vol. 1, </volume> <pages> pages 560-566, </pages> <address> Los Angeles, California, </address> <year> 1985. </year> <title> International Joint Committee for Artificial Intelligence. This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: For PAC learning, some of the the models considered are random noise affecting only the labels of examples, arbitrary noise affecting labels, arbitrary malicious noise, and, for instance space f0; 1g n , various models involving random noise affecting the instance bits <ref> [4, 11, 21, 23] </ref>. There are several recent approaches dealing with noise in query-based models of learning, in particular, for learning with membership and equivalence queries.
References-found: 23

