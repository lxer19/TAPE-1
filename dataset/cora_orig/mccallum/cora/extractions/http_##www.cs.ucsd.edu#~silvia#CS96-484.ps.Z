URL: http://www.cs.ucsd.edu/~silvia/CS96-484.ps.Z
Refering-URL: http://www.cs.ucsd.edu/~silvia/
Root-URL: http://www.cs.ucsd.edu
Email: -silvia,berman-@cs.ucsd.edu  
Title: TR UCSD CS96-484 Mapping Parallel Applications to Distributed Heterogeneous Systems  
Author: Silvia M. Figueira and Francine Berman 
Address: San Diego  
Affiliation: Department of Computer Science and Engineering University of California,  
Abstract: Fast networks have made it possible to coordinate distributed heterogeneous CPU, memory and storage resources to provide a powerful platform for executing high-performance applications. However, the performance of parallel applications on such systems is highly dependent on the mapping of application tasks to machines. In this paper, we propose a mapping strategy for applications formed by multiple tasks targeted to heterogeneous platforms. We first define a mapping model, the match-tree, which reects the data movement and conversion costs of distributed algorithms and allows for alternative implementations of individual tasks on different machines. We then define the find-mapping and split-partition algorithms, based on the match-tree model, to determine the best allocation of tasks to resources in heterogeneous systems. We illustrate the use of these algorithms with a sample distributed application.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Anglano, J. Schopf, R. Wolski, and F. Berman, Zoom: </author> <title> A Hierarchical Representation for Heterogeneous Applications, </title> <institution> University of California, San Diego - Technical Report CS95-451. </institution>
Reference-contexts: 1 Introduction In the last decade, distributed heterogeneous systems have emerged as a powerful plat form for executing high-performance applications. Current experience shows that many high-performance distributed applications are composed of a few coarse-grained tasks and execute on heterogeneous systems formed by a few machines <ref> [1, 15, 17] </ref>. These machines can be MIMD MPPs, visualization engines, SIMD MPPs, workstations, and/or vector computers. For many of these applications, both heterogeneity and parallelism in the code are exploited [4, 10, 13, 14]. <p> As an example, consider the application formed by three tasks (A, B, and C) shown by Figure 1. This is the ow-graph for one iteration of a General Circulation Model (GCM) <ref> [1, 12, 13] </ref>, which can be used to study the nonlinear interaction and feedback between components of a climate system. Task A (an Atmospheric/Physics task) generates data to be used by tasks B (an Atmospheric/Dynamics task) and C (an Ocean Simulation), which can execute concurrently. <p> For now, assume that both M 1 and M 2 have a single processor. Suppose further that task A can only execute on machine M 1 , but both tasks B and C can 4. This is indeed how the heterogeneous implementation is performed in practice <ref> [1, 12, 13] </ref>. For instance, in the UCLA implementation of GCM, machine M 1 is a Cray C90 and machine M 2 is an Intel Paragon. TR - UCSD - CS96-484 6 run on M 1 or M 2 .
Reference: [2] <author> A. Beguelin, J. J. Dongarra, G. A. Geist, R. Manchek, and V. S. Sunderam, </author> <title> Graphical Development Tools for Network-Based Concurrent Supercomputing, </title> <booktitle> Proceedings of Supercomputing91, </booktitle> <pages> pp. 435-444. </pages>
Reference-contexts: These are enhancements which may decrease the effect of the communication costs on the overall time and must be reected in the performance estimation of a candidate mapping. Most mapping strategies in the literature assume a distributed model in which execution sites are time-shared (e.g. <ref> [2, 5, 9, 11, 18] </ref>). In practice, an execution site in a heterogeneous system may be an MPP (such as the Intel Paragon or the Cray T3D) or a dedicated cluster 3 (such as the IBM SP-2 or the DEC Alpha Farm).
Reference: [3] <author> S. H. Bokhari, </author> <title> On the Mapping Problem, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-30, </volume> <pages> pp. 207-214, </pages> <month> March </month> <year> 1981. </year>
Reference-contexts: TR - UCSD - CS96-484 2 an MPP is used to compute distinct local minima whereas a vector computer is used to compute a convex quadratic underestimator function by solving a linear program. The mapping of tasks to machines in distributed systems is known to be NP-hard <ref> [3, 8] </ref>. One would assume that for heterogeneous applications, typically composed of a few tasks and targeted to 2 or 3 machines, the mapping problem would be easy - in particular, exhaustive search would solve the problem.
Reference: [4] <author> J. Demmel and S. Smith, </author> <title> Parallelizing a Global Atmospheric Chemical Tracer Model, </title> <booktitle> Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pp. 718-725, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: These machines can be MIMD MPPs, visualization engines, SIMD MPPs, workstations, and/or vector computers. For many of these applications, both heterogeneity and parallelism in the code are exploited <ref> [4, 10, 13, 14] </ref>. For instance, in a molecular structure determina tion application which computes the global minimum of a potential energy function [14], 1. Supported by a scholarship from CAPES and UFRJ (Brazil). 2. Supported in part by NSF contract number ASC-9301788.
Reference: [5] <author> H. G. Dietz, W. E. Cohen, and B. K. Grant, </author> <title> Would you run it here... Or there? (AHS: Automatic Heterogeneous Supercomputing, </title> <booktitle> Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <volume> vol. II, </volume> <pages> pp. 217-221, </pages> <year> 1993. </year>
Reference-contexts: These are enhancements which may decrease the effect of the communication costs on the overall time and must be reected in the performance estimation of a candidate mapping. Most mapping strategies in the literature assume a distributed model in which execution sites are time-shared (e.g. <ref> [2, 5, 9, 11, 18] </ref>). In practice, an execution site in a heterogeneous system may be an MPP (such as the Intel Paragon or the Cray T3D) or a dedicated cluster 3 (such as the IBM SP-2 or the DEC Alpha Farm).
Reference: [6] <author> M. A. Driscoll and W. R. Daasch, </author> <title> Accurate Predictions of Parallel Program Execution Time, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 25, </volume> <pages> pp. 16-30, </pages> <year> 1995. </year>
Reference-contexts: These can be provided by the user or obtained from a model such as Driscoll and Daaschs <ref> [6] </ref>, who have developed a strategy to estimate these amounts for MPPs. 4 n K TR - UCSD - CS96-484 9 Our split-partition algorithm utilizes a function time (K, n K ), which provides the time to execute task K with n K nodes.
Reference: [7] <author> S. M. Figueira and F. Berman, </author> <title> Modeling the Effects of Contention on the Performance of Heterogeneous Applications, </title> <booktitle> To appear in Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: The label A 1 -B, C-2 represents data communication from task A on machine M 1 to machine M 2 for both tasks B and C in some order. In production systems, contention effects (synthesized into a slowdown factor <ref> [7] </ref>) must be included in the communication and computation costs (or estimates) associated with each edge in the match-tree. When concurrent tasks share an MPP or dedicated cluster, the effects of contention between multiple non-communicating tasks are typically negligible.
Reference: [8] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness, </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: TR - UCSD - CS96-484 2 an MPP is used to compute distinct local minima whereas a vector computer is used to compute a convex quadratic underestimator function by solving a linear program. The mapping of tasks to machines in distributed systems is known to be NP-hard <ref> [3, 8] </ref>. One would assume that for heterogeneous applications, typically composed of a few tasks and targeted to 2 or 3 machines, the mapping problem would be easy - in particular, exhaustive search would solve the problem.
Reference: [9] <author> D. A. Hensgen, T. Kidd, and E. Keith, </author> <title> Adding Rescheduling to and Integrating Condor with SmartNet, </title> <booktitle> Proceedings of the Heterogeneous Computing Workshop, </booktitle> <pages> pp. 4-11, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: These are enhancements which may decrease the effect of the communication costs on the overall time and must be reected in the performance estimation of a candidate mapping. Most mapping strategies in the literature assume a distributed model in which execution sites are time-shared (e.g. <ref> [2, 5, 9, 11, 18] </ref>). In practice, an execution site in a heterogeneous system may be an MPP (such as the Intel Paragon or the Cray T3D) or a dedicated cluster 3 (such as the IBM SP-2 or the DEC Alpha Farm).
Reference: [10] <author> A. Kuppermann and M. Wu, </author> <title> Quantum Reaction Dynamics on a Gigabit/Sec Network, </title> <booktitle> Proceedings of the Gigabit Testbed Maxijam, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: These machines can be MIMD MPPs, visualization engines, SIMD MPPs, workstations, and/or vector computers. For many of these applications, both heterogeneity and parallelism in the code are exploited <ref> [4, 10, 13, 14] </ref>. For instance, in a molecular structure determina tion application which computes the global minimum of a potential energy function [14], 1. Supported by a scholarship from CAPES and UFRJ (Brazil). 2. Supported in part by NSF contract number ASC-9301788.
Reference: [11] <author> V. M. Lo, </author> <title> Heuristic Algorithms for Task Assignment in Distributed Systems, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, no. 11, </volume> <pages> pp. 1384-1397, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: These are enhancements which may decrease the effect of the communication costs on the overall time and must be reected in the performance estimation of a candidate mapping. Most mapping strategies in the literature assume a distributed model in which execution sites are time-shared (e.g. <ref> [2, 5, 9, 11, 18] </ref>). In practice, an execution site in a heterogeneous system may be an MPP (such as the Intel Paragon or the Cray T3D) or a dedicated cluster 3 (such as the IBM SP-2 or the DEC Alpha Farm).
Reference: [12] <author> C. R. Mechoso, J. D. Farrara, J. A. Spahr, </author> <title> Running a Climate Model in a Heterogeneous, Distributed Environment, </title> <booktitle> Proceedings of the Third IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pp. 79-84, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: As an example, consider the application formed by three tasks (A, B, and C) shown by Figure 1. This is the ow-graph for one iteration of a General Circulation Model (GCM) <ref> [1, 12, 13] </ref>, which can be used to study the nonlinear interaction and feedback between components of a climate system. Task A (an Atmospheric/Physics task) generates data to be used by tasks B (an Atmospheric/Dynamics task) and C (an Ocean Simulation), which can execute concurrently. <p> For now, assume that both M 1 and M 2 have a single processor. Suppose further that task A can only execute on machine M 1 , but both tasks B and C can 4. This is indeed how the heterogeneous implementation is performed in practice <ref> [1, 12, 13] </ref>. For instance, in the UCLA implementation of GCM, machine M 1 is a Cray C90 and machine M 2 is an Intel Paragon. TR - UCSD - CS96-484 6 run on M 1 or M 2 .
Reference: [13] <author> C. R. Mechoso, C. Ma, J. D. Farrara, J. A. Spahr, and R. W. Moore, </author> <title> Distribution of a Climate Model across High-Speed Networks, </title> <booktitle> Proceedings of the Supercomputing91, </booktitle> <pages> pp. 253-260, </pages> <year> 1991. </year>
Reference-contexts: These machines can be MIMD MPPs, visualization engines, SIMD MPPs, workstations, and/or vector computers. For many of these applications, both heterogeneity and parallelism in the code are exploited <ref> [4, 10, 13, 14] </ref>. For instance, in a molecular structure determina tion application which computes the global minimum of a potential energy function [14], 1. Supported by a scholarship from CAPES and UFRJ (Brazil). 2. Supported in part by NSF contract number ASC-9301788. <p> As an example, consider the application formed by three tasks (A, B, and C) shown by Figure 1. This is the ow-graph for one iteration of a General Circulation Model (GCM) <ref> [1, 12, 13] </ref>, which can be used to study the nonlinear interaction and feedback between components of a climate system. Task A (an Atmospheric/Physics task) generates data to be used by tasks B (an Atmospheric/Dynamics task) and C (an Ocean Simulation), which can execute concurrently. <p> For now, assume that both M 1 and M 2 have a single processor. Suppose further that task A can only execute on machine M 1 , but both tasks B and C can 4. This is indeed how the heterogeneous implementation is performed in practice <ref> [1, 12, 13] </ref>. For instance, in the UCLA implementation of GCM, machine M 1 is a Cray C90 and machine M 2 is an Intel Paragon. TR - UCSD - CS96-484 6 run on M 1 or M 2 .
Reference: [14] <author> A. Phillips, J. Rosen, and V. Walke, </author> <title> Molecular Structure Determination by Convex Global Underestimation of Local Energy Minima, </title> <institution> University of Minnesota Supercomputer Institute Research Report UMSI 94/126, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: These machines can be MIMD MPPs, visualization engines, SIMD MPPs, workstations, and/or vector computers. For many of these applications, both heterogeneity and parallelism in the code are exploited <ref> [4, 10, 13, 14] </ref>. For instance, in a molecular structure determina tion application which computes the global minimum of a potential energy function [14], 1. Supported by a scholarship from CAPES and UFRJ (Brazil). 2. Supported in part by NSF contract number ASC-9301788. <p> For many of these applications, both heterogeneity and parallelism in the code are exploited [4, 10, 13, 14]. For instance, in a molecular structure determina tion application which computes the global minimum of a potential energy function <ref> [14] </ref>, 1. Supported by a scholarship from CAPES and UFRJ (Brazil). 2. Supported in part by NSF contract number ASC-9301788.
Reference: [15] <author> J. Schopf. </author> <title> Behavior of Heterogeneous Applications. Parallel Computation Lab, </title> <institution> University of California, </institution> <address> San Diego. </address> <note> Manuscript in preparation. </note>
Reference-contexts: 1 Introduction In the last decade, distributed heterogeneous systems have emerged as a powerful plat form for executing high-performance applications. Current experience shows that many high-performance distributed applications are composed of a few coarse-grained tasks and execute on heterogeneous systems formed by a few machines <ref> [1, 15, 17] </ref>. These machines can be MIMD MPPs, visualization engines, SIMD MPPs, workstations, and/or vector computers. For many of these applications, both heterogeneity and parallelism in the code are exploited [4, 10, 13, 14].
Reference: [16] <author> V. Sunderam, </author> <title> PVM: A Framework for Parallel Distributed Computing, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> vol. 2, </volume> <editor> n. </editor> <volume> 4, </volume> <pages> pp. 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: The Alpha Farm used is a cluster of eight DEC Alpha 3000/400 workstations located at the San Diego Supercomputer Center. The Alphas are connected via a dedicated GIGAswitch and com municate using PVM <ref> [16] </ref>. n K execution time number of nodes n K n GE 4= TR - UCSD - CS96-484 10 When multiple tasks are executing concurrently, the overall time to execute them is the time to execute the longest one.
Reference: [17] <institution> Virtual Environments and Distributed Computing at SC95. </institution> <note> GII Testbed and HPC Challenge Applications on the I-WAY. Edited by Holly Korab and Maxine D. Brown. A publication of ACM/IEEE Supercomputing95. TR - UCSD - CS96-484 19 </note>
Reference-contexts: 1 Introduction In the last decade, distributed heterogeneous systems have emerged as a powerful plat form for executing high-performance applications. Current experience shows that many high-performance distributed applications are composed of a few coarse-grained tasks and execute on heterogeneous systems formed by a few machines <ref> [1, 15, 17] </ref>. These machines can be MIMD MPPs, visualization engines, SIMD MPPs, workstations, and/or vector computers. For many of these applications, both heterogeneity and parallelism in the code are exploited [4, 10, 13, 14]. <p> Since in practice the number of tasks for heterogeneous applications in practice is typically and the number of machines used is typically <ref> [17] </ref>, and (although, in practice, ). Note that we cannot execute the algorithm without a value for all paths.
Reference: [18] <author> L. Tao, B. Narahari, and Y. C. Zhao, </author> <title> Heuristics for Mapping Parallel Computations to Heterogeneous Parallel Architectures, </title> <booktitle> Proceedings of the Heterogeneous Computing Workshop, </booktitle> <pages> pp. 36-41, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: These are enhancements which may decrease the effect of the communication costs on the overall time and must be reected in the performance estimation of a candidate mapping. Most mapping strategies in the literature assume a distributed model in which execution sites are time-shared (e.g. <ref> [2, 5, 9, 11, 18] </ref>). In practice, an execution site in a heterogeneous system may be an MPP (such as the Intel Paragon or the Cray T3D) or a dedicated cluster 3 (such as the IBM SP-2 or the DEC Alpha Farm).
References-found: 18

