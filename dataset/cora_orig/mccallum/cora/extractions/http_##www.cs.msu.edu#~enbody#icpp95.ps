URL: http://www.cs.msu.edu/~enbody/icpp95.ps
Refering-URL: http://www.cs.msu.edu/~enbody/
Root-URL: http://www.cs.msu.edu
Email: crs@msu.edu, enbody@cps.msu.edu  wallach@convex.com, funkhous@convex.com  
Title: AUTOMATIC SELF-ALLOCATING THREADS (ASAT) ON THE CONVEX EXEMPLAR  
Author: Charles Severance and Richard Enbody Steve Wallach and Brad Funkhouser 
Address: East Lansing, MI 48824-1027  Richardson, TX 75083-3851  
Affiliation: Department of Computer Science Michigan State University  Convex Computer Corp.  
Abstract: Parallel processing systems have an advantage over traditional supercomputers in price/performance, but traditional supercomputers retain a significant advantage over parallel processing systems in the area of flexibility. Traditional supercomputers can easily handle a mix of interactive, batch, scalar, vector, parallel, and large memory jobs simultaneously while maintaining high utilization. Often parallel processing systems do not effectively support dynamic sharing of the resources of a system which results in low overall utilization given a mix of jobs. This paper describes the implementation of thread balancing software intended to allow the sharing of a large parallel processing system under a variety of load conditions. The system under consideration for this study is the Convex Exemplar scalable, parallel-processing system. While this paper is focused on the performance of this technique on the Convex Exemplar, it is a general technique which should prove to be useful on many shared memory parallel processors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Liu, V. Saletore, T. Lewis, </author> <title> "Scheduling Parallel Loops with Variable Length Iteration Execution Times of Parallel Computers," </title> <booktitle> Proc. of ISMM 5th Int. Conf. on Parallel and Dist. Systems, </booktitle> <year> 1992. </year>
Reference-contexts: A number of scheduling techniques have been proposed and implemented. An excellent survey of these techniques is presented in [2]. These techniques include Pure Self Scheduling (PSS), Chunk Self Scheduling (CSS), Guided Self Scheduling (GSS) [4], Trapezoidal Self Scheduling (TSS) [7], and Safe Self Scheduling (SSS) <ref> [1] </ref>. The basic approach of these techniques is to partition the iterations of a parallel loop among a number of executing threads in a parallel process. The goal is to have balanced execution times on the processors while minimizing the overhead for partitioning the iterations.
Reference: [2] <author> J. Liu, V. Saletore, </author> <title> "Self Scheduling on Distributed-Memory Machines," </title> <journal> IEEE Supercomputing'93, </journal> <pages> pp. 814-823, </pages> <year> 1993. </year>
Reference-contexts: PREVIOUS WORK The general topic of scheduling for parallel loops is one that is well studied. A number of scheduling techniques have been proposed and implemented. An excellent survey of these techniques is presented in <ref> [2] </ref>. These techniques include Pure Self Scheduling (PSS), Chunk Self Scheduling (CSS), Guided Self Scheduling (GSS) [4], Trapezoidal Self Scheduling (TSS) [7], and Safe Self Scheduling (SSS) [1].
Reference: [3] <author> J. C. Mogul and A. Borg, </author> <title> The Effect of Context Switches on Cache Performance, </title> <institution> DEC Western Research Laboratory TN-16, </institution> <month> Dec., </month> <year> 1990. </year> <note> http://www.research.digital.com/wrl/techreports /abstracts/TN-16.html </note>
Reference-contexts: In addition, they provide an excellent survey of related work. As the speed of the CPU's has increased, the problem of a context switch corrupting cache has become an increasing performance impact. In <ref> [3] </ref>, when a compute-bound process was context switched on a cache-based system, the performance of the application was significantly impacted for the next 100,000 cycles after the process regained the CPU. The context switch still had a small negative impact on performance up to 400,000 cycles after the context switch. <p> It is important to note that this phenomenon is not unique to the Exemplar architecture. This performance effect is due to the nature of cache-based parallel processors. It is well known that cache misses can dramatically affect the performance of today's high performance RISC processors <ref> [3] </ref>. In a multi-threaded, parallel processing system experiencing thread imbalance, the probability of cache misses is very high. Other RISC, cache-based parallel-processing systems exhibit the same behavior under thread imbalance [5].
Reference: [4] <author> C. Polychronopoulos, D. J. Kuck, </author> <title> "Guided Self Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers," </title> <journal> IEEE Transactions on Computers, </journal> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: A number of scheduling techniques have been proposed and implemented. An excellent survey of these techniques is presented in [2]. These techniques include Pure Self Scheduling (PSS), Chunk Self Scheduling (CSS), Guided Self Scheduling (GSS) <ref> [4] </ref>, Trapezoidal Self Scheduling (TSS) [7], and Safe Self Scheduling (SSS) [1]. The basic approach of these techniques is to partition the iterations of a parallel loop among a number of executing threads in a parallel process.
Reference: [5] <author> C. Severance, R. Enbody, </author> <title> Software-Based, Automatic, Self-Adjusting Threads (ASAT) for Parallel Supercomputers, </title> <institution> MSU Computer Science Dept. Tech. </institution> <note> Report CPS-94-17, http://clunix.msu.edu/~crs/papers/asat sgi. </note>
Reference-contexts: The concept and benefits of ASAT are not limited to the Convex Exemplar. Any cache-based parallel processor will have similar problems in dealing with thread imbalance. A previous study of the benefits of Automatic Self-Allocating Threads (ASAT) for the SGI Challenge was done in <ref> [5] </ref>. DYNAMIC LOAD ON THE C-240 Since our concept is based on ASAP, we begin by examining the Convex C-Series' [9] hardware support for dynamic thread management. As the load changes dynamically, the number of processors and threads assigned to an application changes. <p> It is well known that cache misses can dramatically affect the performance of today's high performance RISC processors [3]. In a multi-threaded, parallel processing system experiencing thread imbalance, the probability of cache misses is very high. Other RISC, cache-based parallel-processing systems exhibit the same behavior under thread imbalance <ref> [5] </ref>. <p> Such applications are often hand coded with explicit parallelism. Most applications which use compiler-generated parallelism will not exhibit this behavior. EXEMPLAR IMPLEMENTATION The primary feature of the Exemplar which allowed us to solve the implementation problems identified earlier on an SGI <ref> [5] </ref> is the hardware-based, real-time clock which is accessible in user space without a system call. With a high-resolution, low-overhead clock, ASAT on the Exemplar uses a timed barrier call to detect thread imbalance.
Reference: [6] <author> A. Tucker and A. </author> <title> Gupta , "Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multiprocessors," </title> <booktitle> ACM SOSP Conf., </booktitle> <year> 1989, </year> <editor> p. </editor> <volume> 159 - 166. </volume>
Reference-contexts: For the purpose of this paper, we call this technique Fixed Thread Scheduling (FTS). The FTS approach is reasonable for many of the existing parallel processing systems as long as each application has dedicated resources. In <ref> [6] </ref> the problem of matching the overall systemwide number of threads to the number of proces sors was studied on an Encore Multimax. Their ap-plications used explicit parallelism and a thread library. The thread library communicated with a central server to insure overall thread balance.
Reference: [7] <author> T. Tzen and L. Ni, </author> <title> "Dynamic Loop Scheduling on Shared-Memory Multiprocessors," </title> <booktitle> Int. Conf. on Parallel Processing, </booktitle> <year> 1991, </year> <pages> pp 247-250. </pages>
Reference-contexts: A number of scheduling techniques have been proposed and implemented. An excellent survey of these techniques is presented in [2]. These techniques include Pure Self Scheduling (PSS), Chunk Self Scheduling (CSS), Guided Self Scheduling (GSS) [4], Trapezoidal Self Scheduling (TSS) <ref> [7] </ref>, and Safe Self Scheduling (SSS) [1]. The basic approach of these techniques is to partition the iterations of a parallel loop among a number of executing threads in a parallel process.
Reference: [8] <institution> Convex Computer Corporation, Convex Exemplar Architecture, Document DHW-014, Convex Press, Richardson, TX, </institution> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: The advantage of ASAP is that the parallel job soaks up excess cycles very efficiently whenever the cycles are available. DYNAMIC LOAD ON THE EXEMPLAR The target environment for Automatic Self-Allocating Threads (ASAT) in this study is the Convex Exemplar <ref> [8] </ref>. The Convex Exemplar provides a global cache-coherent memory which is distributed among several "nodes." Each node contains two to eight PA-RISC CPUs which access a common node memory via a crossbar switch. These nodes are connected together using a cache-coherent interconnect.
Reference: [9] <institution> Convex Computer Corp., Convex Architecture Reference Manual (C-Series), Document DHW-300, Convex Press, Richardson, TX, </institution> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: In many situations, the cache impact dominated the overall cost of a context switch. Other dynamic, run-time, thread management techniques which are geared toward compiler detected parallelism include: Automatic Self-Adjusting Processors (ASAP) from Convex <ref> [9] </ref> and Autotasking on Cray Research [10] computers. Convex ASAP is based on hardware extensions to the architecture and requires very little run-time library support. Cray's Autotasking is a software based approach and is supported in the run-time library and the operating system scheduler. <p> A previous study of the benefits of Automatic Self-Allocating Threads (ASAT) for the SGI Challenge was done in [5]. DYNAMIC LOAD ON THE C-240 Since our concept is based on ASAP, we begin by examining the Convex C-Series' <ref> [9] </ref> hardware support for dynamic thread management. As the load changes dynamically, the number of processors and threads assigned to an application changes. The Convex C-Series can adjust the number of threads within a very small number of hardware instructions because thread management is built into the ASAP hardware.
Reference: [10] <author> Cray Research, </author> <title> CF77 Compiling System, Volume 4: Parallel Processing Guide. </title>
Reference-contexts: In many situations, the cache impact dominated the overall cost of a context switch. Other dynamic, run-time, thread management techniques which are geared toward compiler detected parallelism include: Automatic Self-Adjusting Processors (ASAP) from Convex [9] and Autotasking on Cray Research <ref> [10] </ref> computers. Convex ASAP is based on hardware extensions to the architecture and requires very little run-time library support. Cray's Autotasking is a software based approach and is supported in the run-time library and the operating system scheduler. The Cray Autotasking product is similar to our work.
References-found: 10

