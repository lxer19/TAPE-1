URL: http://www-mi.gsf.de/scherf/EUBAFES_ICANN97.ps.Z
Refering-URL: http://wwwbrauer.informatik.tu-muenchen.de/~scherf/
Root-URL: 
Email: e-mail: scherf@gsf.de  
Phone: 2  
Title: Improving RBF Networks by the Feature Selection Approach EUBAFES  
Author: M. Scherf W. Brauer 
Address: D-85754 Neuherberg,  D-80290 Munchen  
Affiliation: 1 GSF National Research Center for Environment and Health, medis Institute  Institut fur Informatik Technische Universitat Munchen,  
Abstract: The curse of dimensionality is one of the severest problems concerning the application of RBF networks. The number of RBF nodes and therefore the number of training examples needed grows exponentially with the intrinsic dimensionality of the input space. One way to address this problem is the application of feature selection as a data preprocessing step. In this paper we propose a two-step approach for the determination of an optimal feature subset: First, all possible feature-subsets are reduced to those with best discrimination properties by the application of the fast and robust filter technique EUBAFES. Secondly we use a wrapper approach to judge, which of the pre-selected feature subsets leads to RBF networks with least complexity and best classification accuracy. Experiments are undertaken to show the improvement for RBF networks by our feature selection approach. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Berthold M.R., Diamond J., </author> <title> Boosting the Performance of RBF Networks with Dynamic Decay Adjustment, </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <month> vol.7 </month> <year> (1995) </year>
Reference-contexts: To obtain a local minimum with binary feature weights we shrink the searchspace for the feature weights to a hypercube <ref> [0; 1] </ref> Q , i.e. w q 2 [0; 1] holds for every feature weight. <p> To obtain a local minimum with binary feature weights we shrink the searchspace for the feature weights to a hypercube <ref> [0; 1] </ref> Q , i.e. w q 2 [0; 1] holds for every feature weight. <p> We will give some examples in the next section. 3 Experimental Results We applied EUBAFES to a number of real world data sets, taken from the UCI Repository of machine learning and used the RBF-DDA neural network approach, introduced by <ref> [1] </ref> as classifier. The reason for the choice of this approach is its ability to automatically determine the number of RBF units. Thus we are able to compare the complexity of different networks by the number of RBF units. In every experiment we chose t 2]0; 1].
Reference: 2. <author> John G., Kohavi R., Pfleger K., </author> <title> Irrelevant Features and the Subset Selection Problem, </title> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, (1993) 121-129 William W. Cohne and Haym Hirsh </booktitle>
Reference-contexts: Concerning the criterion function, <ref> [2] </ref> distinguishes wrapper and filter approaches: Wrapper approaches use the accuracy of a classifier to decide the superiority of a feature subset whereas filter approaches use a criterion function which is independent of classifier accuracy. <p> Although the combination of filter and wrapper techniques in context with RBF networks is a new approach, feature selection is a wide and well examined area in statistics and machine learning (see <ref> [2] </ref> for a detailed overview). A feature selection approach, related to EUBAFES is RELIEF which was originally introduced by [3] and extended to handle noisy, incomplete and multiclass data sets by [4].
Reference: 3. <author> Kira K., Rendell L.: </author> <title> A practical approach to feature selection. </title> <booktitle> Proceedings of the International Conference on Machine Learning, </booktitle> <address> Aberdeen, (1992) 249-256,Sleeman D., Edwards P. </address> <publisher> Morgan Kaufmann </publisher>
Reference-contexts: A feature selection approach, related to EUBAFES is RELIEF which was originally introduced by <ref> [3] </ref> and extended to handle noisy, incomplete and multiclass data sets by [4]. RELIEF coincides with EUBAFES regarding the goal to reinforce similarities between instances in the same and deteriorate similarities of instances in different classes.
Reference: 4. <author> Kononenko I., </author> <title> Estimation attributes: </title> <booktitle> Analysis and extensions of RELIEF in Proceedings of the European Conference on Machine Learning, </booktitle> <address> Catana, Italy (1994) 171-182 Springer Verlag" </address>
Reference-contexts: A feature selection approach, related to EUBAFES is RELIEF which was originally introduced by [3] and extended to handle noisy, incomplete and multiclass data sets by <ref> [4] </ref>. RELIEF coincides with EUBAFES regarding the goal to reinforce similarities between instances in the same and deteriorate similarities of instances in different classes. However RELIEF does not give feature subsets but continuous feature weights and uses a different metric and optimisation technique.
Reference: 5. <author> Moody J. and Darken C.J., </author> <title> Fast learning in networks with locally-tuned processing units Neural Computation, </title> <type> 1, </type> <year> (1989) </year> <month> 281-294 </month>
Reference-contexts: 1 Introduction Neural networks with localised receptive fields (RBF networks) <ref> [5] </ref> are very popular as classifiers due to their interpretability and fast parameter estimation. Nevertheless a serious disadvantage of RBF networks is the way of addressing the curse of dimensionality.
Reference: 6. <author> Scherf M., Brauer W., </author> <title> Feature Selection by Means of a Feature Weighting Approach. </title> <type> Technical Report No. </type> <institution> FKI-221-97, Forschungsberichte kunstliche Intelligenz, Insti-tut fur Informatik, Technische Universitat Munchen (1997) </institution>
Reference-contexts: RELIEF coincides with EUBAFES regarding the goal to reinforce similarities between instances in the same and deteriorate similarities of instances in different classes. However RELIEF does not give feature subsets but continuous feature weights and uses a different metric and optimisation technique. In <ref> [6] </ref> we compare both approaches in more detail.
Reference: 7. <author> Wettschereck D., Aha D.W., Mohori T., </author> <title> A Review and Empirical Evaluation of Feature Weighting Methods for a Class of Lazy Learning Algorithms. Artificial Intelligence Review (to appear) (1997) This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: We address this problem by a feature weighting technique 3 , i.e. we assign a continuous weight to every feature and optimise a criterion function with respect to the weights in a 3 see <ref> [7] </ref> for an overview of feature weighting approaches way that the feature weights are either set to one or zero when the optimisation process terminates.
References-found: 7

