URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/global-nips6.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/proc.html
Root-URL: http://www.iro.umontreal.ca
Title: Globally Trained Handwritten Word Recognizer using Spatial Representation, Convolutional Neural Networks and Hidden Markov Models  
Author: Yoshua Bengio Yann Le Cun Donnie Henderson 
Address: Montreal, Qc H3C-3J7  Holmdel NJ 07733  Holmdel NJ 07733  
Affiliation: Dept. Informatique et Recherche Operationnelle Universite de Montreal  AT&T Bell Labs  AT&T Bell Labs  
Abstract: We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style. The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm. Words are then coded into low resolution "annotated images" where each pixel contains information about trajectory direction and curvature. The recognizer is a convolution network which can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bengio, Y., R. De Mori and G. Flammia and R. Kompe. </author> <year> 1992. </year> <title> Global Optimization of a Neural Network-Hidden Markov Model Hybrid. </title> <journal> IEEE Transactions on Neural Networks v.3, nb.2, pp.252-259. </journal>
Reference-contexts: Experiments described in the next section have shown important reductions in error rates when training with this word-level criterion instead of just training the network separately for each character. Similar combinations of neural networks with HMMs or dynamic programming have been proposed in the past, for speech recognition problems <ref> (Bengio et al 92) </ref>. 6 Experimental Results In a first set of experiments, we evaluated the generalization ability of the neural network classifier coupled with the word normalization preprocessing and AMAP input representation. All results are in writer independent mode (different writers in training and testing).
Reference: <author> Burges, C., O. Matan, Y. Le Cun, J. Denker, L. Jackel, C. Stenard, C. </author> <note> Nohl and J. </note>
Reference: <author> Ben. </author> <year> 1992. </year> <title> Shortest Path Segmentation: A Method for Training a Neural Network to Recognize character Strings. </title> <booktitle> Proc. IJCNN'92 (Baltimore), </booktitle> <pages> pp. 165-172, </pages> <month> v.3. </month>
Reference-contexts: Experiments described in the next section have shown important reductions in error rates when training with this word-level criterion instead of just training the network separately for each character. Similar combinations of neural networks with HMMs or dynamic programming have been proposed in the past, for speech recognition problems <ref> (Bengio et al 92) </ref>. 6 Experimental Results In a first set of experiments, we evaluated the generalization ability of the neural network classifier coupled with the word normalization preprocessing and AMAP input representation. All results are in writer independent mode (different writers in training and testing).
Reference: <author> Guyon, I., Albrecht, P., Le Cun, Y., Denker, J. S., and Weissman, H. </author> <title> 1991 design of a neural network character recognizer for a touch terminal. </title> <journal> Pattern Recognition, </journal> <volume> 24(2) </volume> <pages> 105-119. </pages>
Reference-contexts: Trajectories are normalized, and local geometrical or dynamical features are sometimes extracted. The recognition is performed using curve matching (Tappert 90), or other classification techniques such as Neural Networks <ref> (Guyon et al 91) </ref>. While, as stated earlier, these representations have several advantages, their dependence on stroke ordering and individual writing styles makes them difficult to use in high accuracy, writer independent systems that integrate the segmentation with the recognition.
Reference: <author> Le Cun, Y. </author> <year> 1986. </year> <title> Learning Processes in an Asymmetric Threshold Network. </title> <editor> In Bienenstock, E., Fogelman-Soulie, F., and Weisbuch, G., editors, </editor> <booktitle> Disordered systems and biological organization, </booktitle> <pages> pages 233-240, </pages> <address> Les Houches, France. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: MLCNNs are feed-forward neural networks whose architectures are tailored for minimizing the sensitivity to translations, rotations, or distortions of the input image. They are trained with a variation of the Back-Propagation algorithm <ref> (Rumelhart et al 86, Le Cun 86) </ref>. The units in MCLNNs are only connected to a local neighborhood in the previous layer. Each unit can be seen as a local feature detector whose function is determined by the learning procedure.
Reference: <author> Le Cun, Y. </author> <year> 1989. </year> <title> Generalization and Network Design Strategies. </title> <editor> In Pfeifer, R., Schreter, Z., Fogelman, F., and Steels, L., editors, </editor> <booktitle> Connectionism in Perspective, </booktitle> <address> Zurich, Switzerland. Elsevier. </address> <note> an extended version was published as a technical report of the University of Toronto. </note>
Reference-contexts: Unlike many other representations (such as global features), AMAPs can be computed for complete words without requiring segmentation. 4 Convolutional Neural Networks Image-like representations such as AMAPs are particularly well suited for use in combination with Multi-Layer Convolutional Neural Networks (MLCNN) <ref> (Le Cun 89, Le Cun et al 90) </ref>. MLCNNs are feed-forward neural networks whose architectures are tailored for minimizing the sensitivity to translations, rotations, or distortions of the input image. They are trained with a variation of the Back-Propagation algorithm (Rumelhart et al 86, Le Cun 86).
Reference: <author> Le Cun, Y., Matan, O., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hub-bard, W., Jackel, L. D., and Baird, H. S. </author> <year> 1990. </year> <title> Handwritten Zip Code Recognition with Multilayer Networks. </title> <editor> In IAPR, editor, </editor> <booktitle> Proc. of the International Conference on Pattern Recognition, </booktitle> <address> Atlantic City. </address> <publisher> IEEE. </publisher>
Reference: <author> Matan, O., Burges, C. J. C., LeCun, Y., and Denker, J. S. </author> <year> 1992. </year> <title> Multi-Digit Recognition Using a Space Displacement Neural Network. </title> <editor> In Moody, J. M., Han-son, S. J., and Lippman, R. P., editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> volume 4. </volume> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Instead of producing a single output vector, SDNNs produce a series of output vectors. The outputs detects and recognize characters at different (and overlapping) locations on the input. These multiple-input, multiple-output MLCNN are called Space Displacement Neural Networks (SDNN) <ref> (Matan et al 92) </ref>.
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <booktitle> In Parallel distributed processing: Explorations in the microstructure of cognition, </booktitle> <volume> volume I, </volume> <pages> pages 318-362. </pages> <publisher> Bradford Books, </publisher> <address> Cam-bridge, MA. </address>
Reference-contexts: MLCNNs are feed-forward neural networks whose architectures are tailored for minimizing the sensitivity to translations, rotations, or distortions of the input image. They are trained with a variation of the Back-Propagation algorithm <ref> (Rumelhart et al 86, Le Cun 86) </ref>. The units in MCLNNs are only connected to a local neighborhood in the previous layer. Each unit can be seen as a local feature detector whose function is determined by the learning procedure.
Reference: <author> Schenkel, M., Guyon, I., Weissman, H., and Nohl, C. </author> <year> 1993. </year> <title> TDNN Solutions for Recognizing On-Line Natural Handwriting. </title> <booktitle> In Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Tappert, C., Suen, C., and Wakahara, T. </author> <year> 1990. </year> <title> The state of the art in on-line handwriting recognition. </title> <journal> IEEE Trans. PAMI, </journal> <volume> 12(8). </volume>
Reference-contexts: Trajectories are normalized, and local geometrical or dynamical features are sometimes extracted. The recognition is performed using curve matching <ref> (Tappert 90) </ref>, or other classification techniques such as Neural Networks (Guyon et al 91). While, as stated earlier, these representations have several advantages, their dependence on stroke ordering and individual writing styles makes them difficult to use in high accuracy, writer independent systems that integrate the segmentation with the recognition.
References-found: 11

