URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/fall-symp-91-longer.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/
Root-URL: http://www.csc.ncsu.edu
Email: msingh@cs.utexas.edu msingh@dfki.uni-kl.de  
Title: Social and Psychological Commitments in Multiagent Systems  
Author: Munindar P. Singh 
Address: Postfach 2080 D-6750 Kaiserslautern Germany  Austin, TX 78712 USA  
Affiliation: DFKI (German Research Center for AI)  and Center for Cognitive Science and Dept of Computer Sciences University of Texas  
Date: November 1991  14 June, 1991  
Note: Longer version of a paper that appears in AAAI Fall Symposium on Knowledge and Action at Social and Organiza- tional Levels,  This research was supported by DFKI.  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Michael E. Bratman. </author> <title> Intention, Plans and Practical Reason. </title> <publisher> Harvard University Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1987. </year>
Reference-contexts: The agent who makes such commitments is socially liable for not acting up to them. Another, quite different, sense of commitment has also been drawing attention recently. This is the kind of commitment that Bratman <ref> [1, ch. 2] </ref> and Harman [7, p. 94] invoke when they say that an agent is "committed" to his intentions. This is also the notion used in [2, p. 217] and studied in [12]. <p> Intentions for future actions are an important concept in AI. Perhaps their salient property is that they involve a commitment on the part of agents. This view has been gaining ground in Philosophy and AI (e.g., see <ref> [1, ch. 2] </ref>, [7, p. 94] and [2, p. 217]).
Reference: [2] <author> Philip R. Cohen and Hector J. Levesque. </author> <title> Intention is choice with commitment. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 213-261, </pages> <year> 1990. </year>
Reference-contexts: Another, quite different, sense of commitment has also been drawing attention recently. This is the kind of commitment that Bratman [1, ch. 2] and Harman [7, p. 94] invoke when they say that an agent is "committed" to his intentions. This is also the notion used in <ref> [2, p. 217] </ref> and studied in [12]. As I will explain shortly, this sense of commitment also arises under the guise of the "epistemic entrenchment" of an agent's beliefs as in [4] and [7]. <p> Intentions for future actions are an important concept in AI. Perhaps their salient property is that they involve a commitment on the part of agents. This view has been gaining ground in Philosophy and AI (e.g., see [1, ch. 2], [7, p. 94] and <ref> [2, p. 217] </ref>). The idea here is that an agent who has an intention is in some way committed to it|not only does he intend to achieve the relevant condition right now, but would also intend to achieve it later, even as the circumstances changed, perhaps for the worse.
Reference: [3] <author> Daniel C. Dennett. </author> <title> The Intentional Stance. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: The notion of agency is intimately connected to ascriptions of belief and intention|if something can be considered an agent, it can and must be ascribed at least some relevant beliefs and intentions. That such ascriptions are legitimate has also been argued by McCarthy [9] and Dennett <ref> [3] </ref>: one only has to realize that groups are, at worst, artificial and complex physical systems. One can do better, however, since the "mental properties" of groups depend on their members and on how they are structured.
Reference: [4] <author> Peter Gardenfors. </author> <title> Knowledge in Flux. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: This is also the notion used in [2, p. 217] and studied in [12]. As I will explain shortly, this sense of commitment also arises under the guise of the "epistemic entrenchment" of an agent's beliefs as in <ref> [4] </ref> and [7]. I call the first kind of commitment social commitment or S-commitment and the second kind psychological commitment or P-commitment. These concepts are the subject of this paper. The question: can a group be the locus of believing and intending? may be raised. <p> Agents can also be committed towards their beliefs, this commitment corresponding to the latter's doxastic or epistemic entrenchment <ref> [4, ch. 3] </ref>.
Reference: [5] <author> Barbara Grosz and Candace Sidner. </author> <title> Plans for discourse. </title> <editor> In P. Cohen, J. Morgan, and M. Pollack, editors, </editor> <title> SDF Benchmark Series: Intentions in Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: His not doing so would be a dereliction of duty. 5 Commitments and coordination: In traditional construals of group action, the agents in the group are required to have mutual beliefs about the action of the group <ref> [8, 5] </ref>. Here, it proposed that no such mutual beliefs are necessary. When two agents make social commitments to each other, they are already in a position to perform their joint action.
Reference: [6] <author> Joseph Y. Halpern and Yoram O. Moses. </author> <title> Knowledge and common knowledge in a distributed environment (revised version). </title> <type> Technical Report RJ 4421, </type> <institution> IBM, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: No number of epicycles added to such a theory can repair this problem|as long as there are mutual beliefs, they will be unstable. Indeed, the requirement of mutual belief may be taken as a purported proof that joint action is impossible (e.g., as in <ref> [6] </ref>). A far simpler definition for coordination can be motivated: the involved agents merely have to have the appropriate S-commitments to one another.
Reference: [7] <author> Gilbert Harman. </author> <title> Change in View. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The agent who makes such commitments is socially liable for not acting up to them. Another, quite different, sense of commitment has also been drawing attention recently. This is the kind of commitment that Bratman [1, ch. 2] and Harman <ref> [7, p. 94] </ref> invoke when they say that an agent is "committed" to his intentions. This is also the notion used in [2, p. 217] and studied in [12]. <p> This is also the notion used in [2, p. 217] and studied in [12]. As I will explain shortly, this sense of commitment also arises under the guise of the "epistemic entrenchment" of an agent's beliefs as in [4] and <ref> [7] </ref>. I call the first kind of commitment social commitment or S-commitment and the second kind psychological commitment or P-commitment. These concepts are the subject of this paper. The question: can a group be the locus of believing and intending? may be raised. <p> Intentions for future actions are an important concept in AI. Perhaps their salient property is that they involve a commitment on the part of agents. This view has been gaining ground in Philosophy and AI (e.g., see [1, ch. 2], <ref> [7, p. 94] </ref> and [2, p. 217]).
Reference: [8] <author> H. J. Levesque, P. R. Cohen, and J. T. Nunes. </author> <title> On acting together. </title> <booktitle> In AAAI-90, </booktitle> <year> 1990. </year>
Reference-contexts: His not doing so would be a dereliction of duty. 5 Commitments and coordination: In traditional construals of group action, the agents in the group are required to have mutual beliefs about the action of the group <ref> [8, 5] </ref>. Here, it proposed that no such mutual beliefs are necessary. When two agents make social commitments to each other, they are already in a position to perform their joint action.
Reference: [9] <author> John McCarthy. </author> <title> Ascribing mental qualities to machines. </title> <editor> In Martin Ringle, editor, </editor> <booktitle> Philosophical Perspectives in Artificial Intelligence. </booktitle> <publisher> Harvester Press, </publisher> <year> 1979. </year> <note> Page nos. from a revised version, issued as a report in 1987. </note>
Reference-contexts: The notion of agency is intimately connected to ascriptions of belief and intention|if something can be considered an agent, it can and must be ascribed at least some relevant beliefs and intentions. That such ascriptions are legitimate has also been argued by McCarthy <ref> [9] </ref> and Dennett [3]: one only has to realize that groups are, at worst, artificial and complex physical systems. One can do better, however, since the "mental properties" of groups depend on their members and on how they are structured.
Reference: [10] <author> Munindar P. Singh. </author> <title> Group intentions. </title> <booktitle> In 10th Workshop on Distributed Artificial Intelligence, </booktitle> <month> October </month> <year> 1990. </year>
Reference: [11] <author> Munindar P. Singh. </author> <title> Group ability and structure. </title> <editor> In Y. Demazeau and J.-P. Muller, editors, </editor> <booktitle> Decentralized Artificial Intelligence, </booktitle> <volume> Volume 2. </volume> <publisher> Elsevier Science Publishers B.V. / North-Holland, Amsterdam, Holland, </publisher> <year> 1991. </year>
Reference: [12] <author> Munindar P. Singh. </author> <title> Intentions, commitments and rationality. </title> <booktitle> In Meeting of the Cognitive Science Society, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: This is the kind of commitment that Bratman [1, ch. 2] and Harman [7, p. 94] invoke when they say that an agent is "committed" to his intentions. This is also the notion used in [2, p. 217] and studied in <ref> [12] </ref>. As I will explain shortly, this sense of commitment also arises under the guise of the "epistemic entrenchment" of an agent's beliefs as in [4] and [7]. I call the first kind of commitment social commitment or S-commitment and the second kind psychological commitment or P-commitment.
Reference: [13] <author> Munindar P. Singh. </author> <title> Towards a formal theory of communication for multiagent systems. </title> <booktitle> In International Joint Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1991. </year> <month> 5 </month>
Reference-contexts: Interaction protocols between different agents they may be defined so that the interacting agents have the relevant commitments. This generalizes the idea of <ref> [13] </ref> that participating agents have the requisite intentions and knowhow. This idea can also help differentiate communication from other interactions such as resource conflicts.
References-found: 13

