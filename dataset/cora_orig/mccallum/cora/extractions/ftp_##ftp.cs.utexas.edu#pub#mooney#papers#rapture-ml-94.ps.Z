URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/rapture-ml-94.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: mahoney@cs.utexas.edu, mooney@cs.utexas.edu  
Title: Comparing Methods for Refining Certainty-Factor Rule-Bases  
Author: J. Jeffrey Mahoney and Raymond J. Mooney 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences The University of Texas  
Note: Appears in Proceedings of The 11th International Conference on Machine Learning(ML-94)  
Abstract: This paper compares two methods for refining uncertain knowledge bases using propositional certainty-factor rules. The first method, implemented in the Rapture system, employs neural-network training to refine the certainties of existing rules but uses a symbolic technique to add new rules. The second method, based on the one used in the Kbann system, initially adds a complete set of potential new rules with very low certainty and allows neural-network training to filter and adjust these rules. Experimental results indicate that the former method results in significantly faster training and produces much simpler refined rule bases with slightly greater accuracy.
Abstract-found: 1
Intro-found: 1
Reference: <author> Berenji, H. </author> <year> (1990). </year> <title> Refinement of approximate reasoning-based controllers by reinforcement learning. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> 475-479. </pages> <address> Evanston, IL. </address>
Reference-contexts: In addition, techniques for inducing Bayesian networks from data (Cooper and Her-skovits, 1992) could potentially be used to refine the underlying causal structure as well. Finally, there has also been some recent work on combining symbolic and neural-network methods to revise fuzzy-logic controllers <ref> (Berenji, 1990) </ref>. 7 CONCLUSIONS This paper has demonstrated some advantages to combining symbolic and neural-network methods for refining uncertain knowledge bases. Specifically, a symbolic method for adding new rules to a certainty-factor knowledge base was compared to a neural-network method based on Kbann.
Reference: <author> Buchanan, G., and Shortliffe, E., </author> <title> editors (1984). Rule-Based Expert Systems:The MYCIN Experiments of the Stanford Heuristic Programming Project. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Co. </publisher>
Reference-contexts: However, most of this research focuses on revising logical, Horn-clause, domain theories. This paper, by contrast, focuses on methods for refining uncertain knowledge bases employing rules with certainty factors <ref> (Buchanan and Shortliffe, 1984) </ref>. Since many applications require uncertain reasoning, developing refinement methods for such knowledge bases is an important extension. Rapture is a theory refinement system that combines symbolic and neural-network methods to revise a propositional certainty-factor rule base (Mahoney and Mooney, 1993). <p> We have also included graphs of Rapture-Kbann with and without weight decay for comparison. Although weight-decay helps, the resulting rule base is still overly complex. 4.2 MYCIN RESULTS Experiments were also run on a version of the Mycin knowledge-base <ref> (Buchanan and Shortliffe, 1984) </ref>, which was designed to provide consultative advice on diagnosis and therapy for infectious diseases. This domain consists of 115 examples of solved cases (patients) of infectious diseases drawn from the Stan-ford Medical Center. Ten diseases are included with this data set.
Reference: <author> Cooper, G. G., and Herskovits, E. </author> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347. </pages>
Reference-contexts: Although Schwalb's approach to revising Bayesian networks is intractable in the general case (Schwalb, 1993), it may be useful for networks with limited fan-in; and perhaps similar, more efficient, heuristic methods could be developed for more complex networks. In addition, techniques for inducing Bayesian networks from data <ref> (Cooper and Her-skovits, 1992) </ref> could potentially be used to refine the underlying causal structure as well.
Reference: <author> Fahlman, S., and Lebiere, C. </author> <year> (1989). </year> <booktitle> The cascade-correlation learning architecture. In Advances in Neural Information Processing Systems 2, </booktitle> <pages> 524-532. </pages> <address> Denver, </address> <publisher> CO. </publisher>
Reference-contexts: By contrast, Rapture uses information gain to add new input features only as needed. There has been a number of methods for growing neural-network architectures sufficient to classify a set of training examples, e.g. cascade correlation <ref> (Fahlman and Lebiere, 1989) </ref>, the upstart algorithm (Frean, 1990), and the tiling algorithm (Mezard and Nadal, 1989).
Reference: <author> Feldman, R. </author> <year> (1993). </year> <title> Probabilistic Revision of Logical Domain Theories. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY. </address>
Reference-contexts: However, his method creates a neural network whose size is exponential in the fan-in of the Bayesian network, does not address the issue of adding new features or hidden units, and was not tested on revising actual knowledge bases. PTR <ref> (Feldman, 1993) </ref> revises a theory expressed as a collection of Horn-clause rules including numerical parameters representing the expert's confidence in the accuracy of the rule.
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 198-209. </pages>
Reference-contexts: This cycle of CFBP followed by feature addition continues until either 100% training accuracy is achieved, or there are no gains in either classification accuracy or network error. In the latter case, the algorithm resorts to a modified version of the UPSTART algorithm <ref> (Frean, 1990) </ref>, a neural-network technique for adding new hidden units. Beneath every output unit that contains incorrectly classified examples, two new hidden units are built. <p> By contrast, Rapture uses information gain to add new input features only as needed. There has been a number of methods for growing neural-network architectures sufficient to classify a set of training examples, e.g. cascade correlation (Fahlman and Lebiere, 1989), the upstart algorithm <ref> (Frean, 1990) </ref>, and the tiling algorithm (Mezard and Nadal, 1989).
Reference: <author> Fu, L.-M. </author> <year> (1989). </year> <title> Integration of neural heuristics into knowledge-based inference. </title> <journal> Connection Science, </journal> <volume> 1(3) </volume> <pages> 325-339. </pages>
Reference-contexts: They report only modest improvements in the accuracy of the same Mycin rule base used in our experiments, increasing accuracy from 26:8% to 36:0%. Rapture has the advantage of being able to adjust certainty factors and add rules in addition to deleting rules. Fu <ref> (Fu, 1989) </ref> and Lacher (Lacher, 1992) have also used backpropagation techniques to revise certainty factors. Unlike Rapture, Fu's method does not implement complete CFBP, but rather uses it only on every other layer of the network, and uses a different hill-climbing method on the alternate layers.
Reference: <author> Ginsberg, A., Weiss, S. M., and Politakis, P. </author> <year> (1988). </year> <title> Automatic knowledge based refinement for classification systems. </title> <journal> Artificial Intelligence, </journal> <volume> 35 </volume> <pages> 197-226. </pages>
Reference-contexts: Further, weight decay had no effect upon generalization accuracy. 5 RELATED WORK Although most research in theory refinement has focussed on revising logical theories, there have been several other projects on revising uncertain knowledge bases. This section reviews these and other related projects. Seek2 <ref> (Ginsberg et al., 1988) </ref> revises rule bases containing M-of-N rules, also known as choice-component rules. It uses specific heuristics to revise the threshold, M, of individual rules in order to improve performance on the training data.
Reference: <author> Heckerman, D. </author> <year> (1986). </year> <title> Probabilistic interpretations for Mycin's certainty factors. </title> <editor> In Kanal, L. N., and Lemmer, J. F., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> 167-196. </pages> <address> Amsterdam: </address> <publisher> North Holland. </publisher>
Reference-contexts: Although they have proven quite useful in practice, certainty factors have frequently been criticized as ad hoc and restrictive (Shafer and J. Pearl, 1990). Actually, certainty factors have been shown to have a clear probabilistic semantics, but only under very restrictive independence assumptions <ref> (Heckerman, 1986) </ref>. Nevertheless, the basic revision framework in Rapture should be applicable to other uncertain reasoning formalisms such as Bayesian networks (Pearl, 1988), Dempster-Shafer theory (Shafer, 1976), or fuzzy logic (Zadeh, 1965).
Reference: <author> Hinton, G., and Sejnowski, T. </author> <year> (1986). </year> <title> Learning and relearning in boltzmann machines. </title> <editor> In Rumelhart, D. E., and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. I, </volume> <pages> 282-317. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Once built, the network is trained using backpropagation, and links whose weights fall below a given threshold are deleted. To help minimize the size of the network, weight-decay <ref> (Hinton and Sejnowski, 1986) </ref> is utilized. By adjusting each weight in the network slightly towards zero after each weight update, links that are not contributing to the network are eliminated. After training, symbolic rules can be extracted from the network.
Reference: <author> Koppel, M., Feldman, R., and Segre, A. M. </author> <year> (1994). </year> <title> Bias-driven revision of logical domain theories. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 1-50. </pages>
Reference: <author> Lacher, R. </author> <year> (1992). </year> <title> Node error assignment in expert networks. </title> <editor> In Kandel, A., and Langholz, G., editors, </editor> <booktitle> Hybrid Architectures for Intelligent Systems, </booktitle> <pages> 29-48. </pages> <address> Boca Raton, FL: </address> <publisher> CRC Press, Inc. </publisher>
Reference-contexts: They report only modest improvements in the accuracy of the same Mycin rule base used in our experiments, increasing accuracy from 26:8% to 36:0%. Rapture has the advantage of being able to adjust certainty factors and add rules in addition to deleting rules. Fu (Fu, 1989) and Lacher <ref> (Lacher, 1992) </ref> have also used backpropagation techniques to revise certainty factors. Unlike Rapture, Fu's method does not implement complete CFBP, but rather uses it only on every other layer of the network, and uses a different hill-climbing method on the alternate layers.
Reference: <author> Ma, Y., and Wilkins, D. C. </author> <year> (1991). </year> <title> Improving the performance of inconsistent knowledge bases via combined optimization method. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> 23-27. </pages> <address> Evanston, IL. </address>
Reference-contexts: It uses specific heuristics to revise the threshold, M, of individual rules in order to improve performance on the training data. Unlike Rapture, Seek2 can not modify real-valued weights and contains no means for adding new rules. Ma and Wilkins <ref> (Ma and Wilkins, 1991) </ref> have developed methods for improving the accuracy of a certainty-factor knowledge base by deleting rules. They report only modest improvements in the accuracy of the same Mycin rule base used in our experiments, increasing accuracy from 26:8% to 36:0%.
Reference: <author> Mahoney, J. J., and Mooney, R. J. </author> <year> (1993). </year> <title> Combining connectionist and symbolic learning to refine certainty-factor rule-bases. Connection Science, </title> <publisher> 5(3-4):339-364. </publisher>
Reference-contexts: Since many applications require uncertain reasoning, developing refinement methods for such knowledge bases is an important extension. Rapture is a theory refinement system that combines symbolic and neural-network methods to revise a propositional certainty-factor rule base <ref> (Mahoney and Mooney, 1993) </ref>. It combines the ability of neural network methods to effectively adjust numerical parameters with the ability of symbolic methods to make concise structural changes. <p> The resulting rulebase is generally much simpler than the revised network; however, there is no guarantee that the two representations are identical. 3 RAPTURE 3.1 THE RAPTURE ALGORITHM This section summarizes the refinement algorithm used by Rapture which is outlined in Figure 1. Further details can be found in <ref> (Mahoney and Mooney, 1993) </ref>. After acquiring a probabilistic rule-base from an expert, the rules are mapped into an equivalent network. The certainty factors of the rules are mapped to weights on connections between nodes of the network.
Reference: <author> Mezard, M., and Nadal, J. </author> <year> (1989). </year> <title> Learning in feed-forward layered networks: The tiling algorithm. </title> <journal> Journal of Physics, A22(12):2191-2203. </journal>
Reference-contexts: By contrast, Rapture uses information gain to add new input features only as needed. There has been a number of methods for growing neural-network architectures sufficient to classify a set of training examples, e.g. cascade correlation (Fahlman and Lebiere, 1989), the upstart algorithm (Frean, 1990), and the tiling algorithm <ref> (Mezard and Nadal, 1989) </ref>.
Reference: <author> Michalski, R. S., and Chilausky, S. </author> <year> (1980). </year> <title> Learning by being told and learning from examples: An experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean disease diagnosis. </title> <journal> Journal of Policy Analysis and Information Systems, </journal> <volume> 4(2) </volume> <pages> 126-161. </pages>
Reference-contexts: However, these methods also employ full-connectivity to all input features. 6 FUTURE WORK We are currently in the process of comparing Rapture, Rapture-Kbann, and other inductive learning and theory refinement algorithms on additional tasks such as soybean disease diagnosis <ref> (Michalski and Chilausky, 1980) </ref> and recognizing DNA splice-junctions (Noordewier et al., 1991) in order to determine Rapture's relative performance in these domains. Unfortunately, in the existing experiments, hidden-unit addition has not proven particularly useful since Rapture is generally able to reach convergence using only backpropagation and feature addition.
Reference: <author> Noordewier, M. O., Towell, G. G., and Shavlik, J. W. </author> <year> (1991). </year> <title> Training knowledge-based neural networks to recognize genes in DNA sequences. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> vol. </volume> <pages> 3. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: However, these methods also employ full-connectivity to all input features. 6 FUTURE WORK We are currently in the process of comparing Rapture, Rapture-Kbann, and other inductive learning and theory refinement algorithms on additional tasks such as soybean disease diagnosis (Michalski and Chilausky, 1980) and recognizing DNA splice-junctions <ref> (Noordewier et al., 1991) </ref> in order to determine Rapture's relative performance in these domains. Unfortunately, in the existing experiments, hidden-unit addition has not proven particularly useful since Rapture is generally able to reach convergence using only backpropagation and feature addition.
Reference: <author> O'Neill, M., and Chiafari, F. </author> <year> (1989). </year> <title> Escherichia coli promoters. </title> <journal> Journal of Biological Chemistry, </journal> <volume> 264 </volume> <pages> 5531-5534. </pages>
Reference: <author> Opitz, D. W., and Shavlik, J. W. </author> <year> (1993). </year> <title> Heuristically expanding knowledge-based neural networks. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial intelligence, </booktitle> <pages> 512-517. </pages> <address> Chamberry, France. </address>
Reference-contexts: Unlike certainty factors in Rapture, these values do not represent the strength, or amount of evidence suggested by the rule, but rather the user's confidence that the rule is correct. TopGen <ref> (Opitz and Shavlik, 1993) </ref> is a method for adding new hidden units to a Kbann-network. By keeping track of of the false negative and false positives for which each node is responsible, the algorithm locates areas of the network requiring additional units.
Reference: <author> Ourston, D., and Mooney, R. </author> <year> (1990). </year> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> 815-820. </pages> <address> De-troit, MI. </address>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo,CA: </address> <publisher> Morgan Kaufmann, Inc. </publisher>
Reference-contexts: Pearl, 1990). Actually, certainty factors have been shown to have a clear probabilistic semantics, but only under very restrictive independence assumptions (Heckerman, 1986). Nevertheless, the basic revision framework in Rapture should be applicable to other uncertain reasoning formalisms such as Bayesian networks <ref> (Pearl, 1988) </ref>, Dempster-Shafer theory (Shafer, 1976), or fuzzy logic (Zadeh, 1965). Although Schwalb's approach to revising Bayesian networks is intractable in the general case (Schwalb, 1993), it may be useful for networks with limited fan-in; and perhaps similar, more efficient, heuristic methods could be developed for more complex networks.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106. </pages>
Reference-contexts: During this process, rules whose certainty falls below a certain threshold are deleted. If simply adjusting the certainty of existing rules is insufficient to correctly classify all of the training examples, Rapture uses a version of ID3's information gain heuristic <ref> (Quinlan, 1986) </ref> to determine new features that best discriminate the misclassified examples and includes them in new rules which are then added to the knowledge base. Backpropagation is then used again to adjust the certainties of these new rules.
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, J. R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., and McClel-land, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. I, </volume> <pages> 318-362. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: It combines the ability of neural network methods to effectively adjust numerical parameters with the ability of symbolic methods to make concise structural changes. Specifically, it first uses a modified version of backpropagation <ref> (Rumelhart et al., 1986) </ref> to adjust the certainty factors of existing rules in order to improve classification performance on a set of training examples. During this process, rules whose certainty falls below a certain threshold are deleted.
Reference: <author> Schwalb, E. </author> <year> (1993). </year> <title> Compiling Bayesian networks into neural networks. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 291-297. </pages> <address> Amherst, MA. </address>
Reference-contexts: However, the current publications on these two projects do not address the problem of altering the network architecture (i.e. adding new rules) and do not present results on revising actual expert knowl edge bases. Schwalb <ref> (Schwalb, 1993) </ref> has shown how the parameters of Bayesian networks can also be refined by mapping them into neural networks and performing backpropagation. <p> Nevertheless, the basic revision framework in Rapture should be applicable to other uncertain reasoning formalisms such as Bayesian networks (Pearl, 1988), Dempster-Shafer theory (Shafer, 1976), or fuzzy logic (Zadeh, 1965). Although Schwalb's approach to revising Bayesian networks is intractable in the general case <ref> (Schwalb, 1993) </ref>, it may be useful for networks with limited fan-in; and perhaps similar, more efficient, heuristic methods could be developed for more complex networks. In addition, techniques for inducing Bayesian networks from data (Cooper and Her-skovits, 1992) could potentially be used to refine the underlying causal structure as well.
Reference: <author> Shafer, G. </author> <year> (1976). </year> <title> A Mathematical Theory of Evidence. </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton University Press. </publisher>
Reference-contexts: Pearl, 1990). Actually, certainty factors have been shown to have a clear probabilistic semantics, but only under very restrictive independence assumptions (Heckerman, 1986). Nevertheless, the basic revision framework in Rapture should be applicable to other uncertain reasoning formalisms such as Bayesian networks (Pearl, 1988), Dempster-Shafer theory <ref> (Shafer, 1976) </ref>, or fuzzy logic (Zadeh, 1965). Although Schwalb's approach to revising Bayesian networks is intractable in the general case (Schwalb, 1993), it may be useful for networks with limited fan-in; and perhaps similar, more efficient, heuristic methods could be developed for more complex networks.
Reference: <author> Shafer, G., and J. Pearl, e. </author> <year> (1990). </year> <title> Readings in Uncertain Reasoning. </title> <address> San Mateo,CA: </address> <publisher> Morgan Kauf-mann, Inc. </publisher>
Reference-contexts: We hope to discover the relative advantages and disadvantages of probabilistic sum as a combination function compared to the normal linear-threshold. Although they have proven quite useful in practice, certainty factors have frequently been criticized as ad hoc and restrictive <ref> (Shafer and J. Pearl, 1990) </ref>. Actually, certainty factors have been shown to have a clear probabilistic semantics, but only under very restrictive independence assumptions (Heckerman, 1986).
Reference: <author> Towell, G., and Shavlik, J. </author> <year> (1993). </year> <title> Extracting refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 71-102. </pages>
Reference: <author> Towell, G. G. </author> <year> (1991). </year> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madi-son, WI. </institution>
Reference: <author> Towell, G. G., Shavlik, J. W., and Noordewier, M. O. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based artificial neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> 861-866. </pages> <address> Boston, MA. </address>
Reference-contexts: in Rapture, nodes and links are eliminated from the network whenever their weights drop below 0:01. 4 EXPERIMENTAL RESULTS 4.1 DNA PROMOTER-RECOGNITION RESULTS A prokaryotic promoter is a short DNA sequence that precedes the beginnings of genes, and are locations where the protein RNA polymerase binds to the DNA structure <ref> (Towell et al., 1990) </ref>. A theory designed to recognize such strings composed of DNA-nucleotides was given to Rapture for revision. The data set consists of 106 examples, 53 positive and 53 negative.
Reference: <author> Wogulis, J., and Pazzani, M. </author> <year> (1993). </year> <title> A methodology for evaluating theory revision systems: Results with Audrey II. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial intelligence, </booktitle> <pages> 1128-1134. </pages> <address> Chambery, France. </address>
Reference: <author> Zadeh, L. </author> <year> (1965). </year> <title> Fuzzy sets. </title> <journal> Information and Control, </journal> <volume> 8 </volume> <pages> 338-353. </pages>
Reference-contexts: Actually, certainty factors have been shown to have a clear probabilistic semantics, but only under very restrictive independence assumptions (Heckerman, 1986). Nevertheless, the basic revision framework in Rapture should be applicable to other uncertain reasoning formalisms such as Bayesian networks (Pearl, 1988), Dempster-Shafer theory (Shafer, 1976), or fuzzy logic <ref> (Zadeh, 1965) </ref>. Although Schwalb's approach to revising Bayesian networks is intractable in the general case (Schwalb, 1993), it may be useful for networks with limited fan-in; and perhaps similar, more efficient, heuristic methods could be developed for more complex networks.
References-found: 31

