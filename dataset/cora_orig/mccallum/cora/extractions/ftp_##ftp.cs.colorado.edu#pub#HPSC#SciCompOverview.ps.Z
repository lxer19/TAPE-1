URL: ftp://ftp.cs.colorado.edu/pub/HPSC/SciCompOverview.ps.Z
Refering-URL: http://www.cs.colorado.edu/current/courses/materials.hpsc.html
Root-URL: http://www.cs.colorado.edu
Title: An Overview of Scientific Computing  High Performance Scientific  
Author: Lloyd Fosdick Elizabeth Jessup 
Address: Boulder  
Affiliation: University of Colorado at  
Date: September 28, 1995  
Pubnum: Computing  
Abstract-found: 0
Intro-found: 1
Reference: [Almasi & Gottlieb 89] <author> ALMASI, GEORGE S. AND ALLAN GOTTLIEB. </author> <year> [1989]. </year> <title> Highly Parallel Computing. </title> <publisher> The Benjamin/Cummings Publishing Company, Inc., </publisher> <address> Redwood City, CA, 1st edition. </address>
Reference-contexts: The intent was, as we said earlier, to get you interested to study it further. Here are a few suggestions for further reading. Books by Hockney and Jesshope [Hockney & Jesshope 88], Almasi and Gottlieb <ref> [Almasi & Gottlieb 89] </ref>, and Leighton [Leighton 92] discuss parallel computers, their architectures, and programming. For the broad history of computing see books by Williams [Williams 85] and Augarten [Augarten 84].
Reference: [Augarten 84] <author> AUGARTEN, STAN. </author> <year> [1984]. </year> <title> Bit by Bit. Ticknor & Fields, </title> <address> New York, NY. </address>
Reference-contexts: Here are a few suggestions for further reading. Books by Hockney and Jesshope [Hockney & Jesshope 88], Almasi and Gottlieb [Almasi & Gottlieb 89], and Leighton [Leighton 92] discuss parallel computers, their architectures, and programming. For the broad history of computing see books by Williams [Williams 85] and Augarten <ref> [Augarten 84] </ref>. For the history of scientific computing see the book by Nash [Nash 90] The work on the greenhouse effect by Washington and Bettge is described in [Washington & Bettge 90]. The federal high-performance computing program is described in [HPCC 89].
Reference: [Carriero & Gelernter 89] <author> CARRIERO, N. AND D. GELERNTER. </author> <month> [April </month> <year> 1989]. </year> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458. </pages>
Reference-contexts: Programming these computers at a low level, that is, specifying the individual send and receive commands, is difficult and various languages have been developed attempting to simplify it: Linda, developed at Yale University by Gelernter <ref> [Carriero & Gelernter 89] </ref>, and DINO, developed at the University of Colorado by Schnabel, Rosing, and Weaver [Rosing et al 91], are two examples.
Reference: [DEC 91] <institution> Digital Equipment Corporation, </institution> <address> Palo Alto, CA 94301. </address> <month> [Dec </month> <year> 1991]. </year> <title> DECstation 5000 Model 240: </title> <type> Technical Overview. </type>
Reference-contexts: The memory subsystem, I/O controller, etc. operate with a 25 MHz clock <ref> [DEC 91] </ref>. It uses a MIPS 6 processor and floating-point coprocessor. Measurements of the time for elementary arithmetic operations give the results shown in table 4. The system can have from 8 to 480 Mbytes of DRAM memory 7 with a bandwidth of 100 Mbytes/sec.
Reference: [Dongarra 94] <author> DONGARRA, J. J. </author> <year> [1994]. </year> <title> Performance of various computers using standard linear equations software. </title> <type> Technical Report CS-89-85, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN 37831. </institution> <note> netlib version as of November 1, </note> <year> 1994. </year>
Reference-contexts: In table 1 there is a short list of high-performance workstations with the name of the machine, followed by the name of the manufacturer, followed by the performance in Mflops on the LINPACK Benchmark. <ref> [Dongarra 94] </ref>. This benchmark is based on the speed of solving a system of 100 simultaneous linear equations using software from LINPACK [Dongarra et al 79]. The peak performance of workstations in this table could be as much as five to ten times the LINPACK performance number. <p> The peak performance of workstations in this table could be as much as five to ten times the LINPACK performance number. Table 2 provides a short list of supercomputers, with the name of the machine series, the name of the manufacturer, and the Theoretical Peak Performance <ref> [Dongarra 94] </ref>. 4 As with the workstations, the machines in this list come in various models and configurations; the performance data is for the largest system listed in Dongarra's report.
Reference: [Dongarra et al 79] <author> DONGARRA, J. J., C. B. MOLER, J. R. BUNCH, AND G. W. STEWART. </author> <year> [1979]. </year> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA. </address>
Reference-contexts: This benchmark is based on the speed of solving a system of 100 simultaneous linear equations using software from LINPACK <ref> [Dongarra et al 79] </ref>. The peak performance of workstations in this table could be as much as five to ten times the LINPACK performance number.
Reference: [Geist et al 95] <author> GEIST, AL, ADAM BEGUELIN, JACK DONGARRA, WEICHING JIANG, ROBERT MANCHEK, AND VAIDY SUNDERAM. </author> <year> [1995]. </year> <title> PVM - Parallel Virtual Machine: A Users' Guide and Tutorial for Network Parallel Computing. Scientific and Engineering Computation. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Furthermore, it can serve as as a platform for a heterogeneous multiprocessor consisting of, say, a Paragon, a CM-5, and a group of workstations. The PVM software and documentation can be obtained by anonymous ftp at netlib.att.com, and the PVM manual is now available as a book <ref> [Geist et al 95] </ref>. From the programmers point of view, PVM consists of a library of procedures for C programs or Fortran programs to support interprocessor communication by message passing.
Reference: [Gropp et al 94] <author> GROPP, WILLIAM, EWING LUSK, AND ANTHONY SKJELLUM. </author> <year> [1994]. </year> <title> Using MPI: Portable parallel programming with the message passing interface. Scientific and engineering computation. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Experience with PVM and a number of related CUBoulder : HPSC Course Notes Overview 27 works has led to an unofficial standard for message passing known as MPI (Message-Passing Interface), created by the Message Passing Interface Forum <ref> [Gropp et al 94] </ref>. It specifies the syntax and semantics for message-passing procedures callable from C or Fortran programs. 6 Further reading This has been a very brief treatment of the important and rapidly developing field of scientific computing.
Reference: [Hockney & Jesshope 88] <author> HOCKNEY, R. W. AND C. R. JESSHOPE. </author> <year> [1988]. </year> <title> Parallel Computers 2. Adam Hilger, </title> <publisher> Bristol. </publisher>
Reference-contexts: The intent was, as we said earlier, to get you interested to study it further. Here are a few suggestions for further reading. Books by Hockney and Jesshope <ref> [Hockney & Jesshope 88] </ref>, Almasi and Gottlieb [Almasi & Gottlieb 89], and Leighton [Leighton 92] discuss parallel computers, their architectures, and programming. For the broad history of computing see books by Williams [Williams 85] and Augarten [Augarten 84].
Reference: [HPCC 89] <institution> Executive Office of the President, Office of Science and Technology, </institution> <address> Washington, D.C. </address> <month> [Sep </month> <year> 1989]. </year> <title> The Federal High Performance CUBoulder : HPSC Course Notes Overview 29 Computing Program. This is often referred to as the Bromley Report. The program is now called the High Performance Computing and Communications (HPCC) program. </title>
Reference-contexts: For the history of scientific computing see the book by Nash [Nash 90] The work on the greenhouse effect by Washington and Bettge is described in [Washington & Bettge 90]. The federal high-performance computing program is described in <ref> [HPCC 89] </ref>. A description of High Performance Fortran is in [Koelbel et al 94].
Reference: [Kane 88] <author> KANE, GERRY. </author> <year> [1988]. </year> <title> MIPS RISC Architecture. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: Multiply and divide operations can be overlapped to some extent with other operations but not with each other; e.g., it is possible to execute a double precision floating-point addition and multiplication together in 5 cycles. For more information on these processors, see <ref> [Kane 88] </ref>. 5 Supercomputers The word "supercomputer" came into use in the late 1960s when radically new and powerful computers began to emerge from university and commercial laboratories. Since then it has symbolized the most powerful computers of the time.
Reference: [Koelbel et al 94] <author> KOELBEL, CHARLES H., DAVID B. LOVEMAN, ROBERT S. SCHREIBER, JR. GUY L. STEELE, AND MARY E. ZOSEL. </author> <year> [1994]. </year> <title> The High Performance Fortran Handbook. Scientific and Engineering Computation. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: A group known as the High Performance Fortran Forum (HPFF) has been developing a version of Fortran, High Performance Fortran (HPF), which aims to support parallel programming (see <ref> [Koelbel et al 94] </ref>). Parallel computers in which individual processors execute instructions asynchronously but share a common address space include machines like the Cray Y-MP and C-90, the IBM SP1 and SP2, the Silicon Graphics Challenge, and the Convex Exemplar. They are often referred to as shared-memory computers. <p> For the history of scientific computing see the book by Nash [Nash 90] The work on the greenhouse effect by Washington and Bettge is described in [Washington & Bettge 90]. The federal high-performance computing program is described in [HPCC 89]. A description of High Performance Fortran is in <ref> [Koelbel et al 94] </ref>. A simple introduction to the Internet is given in the book by Krol [Krol 92]; see also the August 1994 issue of the Communications of the ACM for a number of articles on the Internet and related topics.
Reference: [Krol 92] <author> KROL, ED. </author> <year> [1992]. </year> <title> The Whole Internet: User's Guide and Catalog. </title> <publisher> OReilly & Associates, Inc., </publisher> <address> Sebastapol, CA. </address>
Reference-contexts: The federal high-performance computing program is described in [HPCC 89]. A description of High Performance Fortran is in [Koelbel et al 94]. A simple introduction to the Internet is given in the book by Krol <ref> [Krol 92] </ref>; see also the August 1994 issue of the Communications of the ACM for a number of articles on the Internet and related topics. More complete descriptions of the architecture, performance, and applications of supercomputers can be found in the following chapters of this book.
Reference: [Leighton 92] <author> LEIGHTON, F. THOMSON. </author> <year> [1992]. </year> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA. </address>
Reference-contexts: The intent was, as we said earlier, to get you interested to study it further. Here are a few suggestions for further reading. Books by Hockney and Jesshope [Hockney & Jesshope 88], Almasi and Gottlieb [Almasi & Gottlieb 89], and Leighton <ref> [Leighton 92] </ref> discuss parallel computers, their architectures, and programming. For the broad history of computing see books by Williams [Williams 85] and Augarten [Augarten 84].
Reference: [Leiner 94] <author> LEINER, BARRY M. </author> <year> [1994]. </year> <title> Internet technology. </title> <journal> Communications of the ACM, 37(8):32. </journal>
Reference-contexts: The Internet is the worldwide system linking many smaller networks running the TCP/IP protocol. In May 1994 the Internet consisted of 31,000 networks, connecting over two million computers, and was growing at the phenomenal rate of one new network every 10 minutes <ref> [Leiner 94] </ref>. The National Science Foundation Network (NSFNET) is one of the most important components of the Internet, linking the the supercomputing centers in a network known as the backbone, which can be reached from other networks linking universities and other research organizations.
Reference: [Nash 90] <author> NASH, STEPHEN G., editor. </author> <year> [1990]. </year> <title> A History of Scientific Computing. </title> <publisher> ACM Press History Series. Addison-Wesley Publishing Company, </publisher> <address> Reading, MA. </address>
Reference-contexts: For the broad history of computing see books by Williams [Williams 85] and Augarten [Augarten 84]. For the history of scientific computing see the book by Nash <ref> [Nash 90] </ref> The work on the greenhouse effect by Washington and Bettge is described in [Washington & Bettge 90]. The federal high-performance computing program is described in [HPCC 89]. A description of High Performance Fortran is in [Koelbel et al 94].
Reference: [Patterson 85] <author> PATTERSON, DAVID A. </author> <month> [Jan </month> <year> 1985]. </year> <title> Reduced instruction set computers. </title> <journal> Communications of the ACM, </journal> <volume> 28(1) </volume> <pages> 8-21. </pages>
Reference-contexts: In a ten year period the speed of these machines has increased by about two orders of magnitude. In the following paragraphs we discuss some of the architectural features now found in popular workstations. 4.1 RISC architecture Many workstations use a RISC (Reduced Instruction Set Computer) architecture <ref> [Patterson 85] </ref>. It is characterized by a relatively small set of simple instructions, pipelined instruction execution, and cache memory. The principal goal of this architecture is an execution speed of one instruction per clock cycle: with a 40 MHz clock an execution speed of 40 mips 5 is the goal.
Reference: [Rosing et al 91] <author> ROSING, M., R. SCHNABEL, AND R. WEAVER. </author> <year> [1991]. </year> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 32-40. </pages>
Reference-contexts: computers at a low level, that is, specifying the individual send and receive commands, is difficult and various languages have been developed attempting to simplify it: Linda, developed at Yale University by Gelernter [Carriero & Gelernter 89], and DINO, developed at the University of Colorado by Schnabel, Rosing, and Weaver <ref> [Rosing et al 91] </ref>, are two examples. A group known as the High Performance Fortran Forum (HPFF) has been developing a version of Fortran, High Performance Fortran (HPF), which aims to support parallel programming (see [Koelbel et al 94]).
Reference: [Washington & Bettge 90] <author> WASHINGTON, WARREN M. AND THOMAS W. BETTGE. </author> <month> [May/Jun </month> <year> 1990]. </year> <title> Computer simulation of the greenhouse effect. </title> <booktitle> Computers in Physics, </booktitle> <pages> pages 240-246. </pages>
Reference-contexts: For the broad history of computing see books by Williams [Williams 85] and Augarten [Augarten 84]. For the history of scientific computing see the book by Nash [Nash 90] The work on the greenhouse effect by Washington and Bettge is described in <ref> [Washington & Bettge 90] </ref>. The federal high-performance computing program is described in [HPCC 89]. A description of High Performance Fortran is in [Koelbel et al 94].
Reference: [Williams 85] <author> WILLIAMS, MICHAEL R. </author> <year> [1985]. </year> <title> A History of Computing Technology. Computational Mathematics. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Engle-wood Cliffs, NJ. </address> <note> CUBoulder : HPSC Course Notes </note>
Reference-contexts: Here are a few suggestions for further reading. Books by Hockney and Jesshope [Hockney & Jesshope 88], Almasi and Gottlieb [Almasi & Gottlieb 89], and Leighton [Leighton 92] discuss parallel computers, their architectures, and programming. For the broad history of computing see books by Williams <ref> [Williams 85] </ref> and Augarten [Augarten 84]. For the history of scientific computing see the book by Nash [Nash 90] The work on the greenhouse effect by Washington and Bettge is described in [Washington & Bettge 90]. The federal high-performance computing program is described in [HPCC 89].
References-found: 20

