URL: http://www.cs.purdue.edu/homes/spaf/tech-reps/sjc.ps
Refering-URL: http://www.cs.purdue.edu/homes/spaf/students.html
Root-URL: http://www.cs.purdue.edu
Title: Scheduling Support Mechanisms for Autonomous, Heterogeneous, Distributed Systems  
Author: by Stephen Joel Chapin 
Degree: A Thesis Submitted to the Faculty of  In Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy  
Date: December 1993  
Affiliation: Purdue University  
Abstract-found: 0
Intro-found: 1
Reference: <institution> 126 BIBLIOGRAPHY </institution>
Reference: [ADD82] <author> G. R. Andrews, D. P. Dobkin, and P. J. Downey. </author> <title> Distributed allocation with pools of servers. </title> <booktitle> In Proceedings of the Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 73-83. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1982. </year>
Reference-contexts: e H t r g n o s u o o y v r e d c l b e x e s b e Stankovic and Sidhu [SS84] Y Y Y N EAC x P N Stankovic [Sta85b] Y N Y N N x N N Andrews et al. <ref> [ADD82] </ref> Y N Y x E x Y N Majumdar and Green [MG80] Y Y Y N N x N N Bonomi [Bon90] Y N N N N x N N Bonomi and Kumar [BK90] Y N N Y N x N N Greedy Load-Sharing [Cho90] Y N Y N N <p> This method uses a system of rewards and penalties as a feedback mechanism to tune the policy. The second method uses bidding and one-time assignment in a real-time environment, similar to that in [SS84]. Andrews, et al. <ref> [ADD82] </ref> describes a bidding method with dynamic reassignment based on three types of servers: free, preferred, and retentive. Free server allocation will choose any available server from an identical pool.
Reference: [AF87] <author> Y. Artsy and R. Finkel. </author> <title> Simplicity, Efficiency, and Functionality in Designing a Process Migration Facility. </title> <booktitle> In The 2nd Israel Conference on Computer Systems, </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: Locus, Charlotte, and work by Bryant and Finkel Locus uses the basic checkpoint-transfer-restart algorithm, with the optimization that read-only segments that already exist on the target machine are not copied [PW85]. The Charlotte distributed operating system uses the basic algorithm, with the addition of message endpoint forwarding <ref> [AF87, FA89] </ref>. Bryant and Finkel [BF81] concentrates on developing stable process migration methods.
Reference: [Agg93a] <author> Distributed Builds: </author> <title> The Next Step in the Evolution of Programming Tools. Aggregate Computing, </title> <publisher> Inc., </publisher> <address> Minneapolis, MN, </address> <year> 1993. </year>
Reference-contexts: A local task executes on the host where it originated, without going through the global scheduling system. A foreign task originates at a host different from the one on which it executes. NetShare NetShare is a distributed systems construction product of Aggregate Computing, Inc. <ref> [Agg93c, Agg93b, Agg93a] </ref>. NetShare comprises services that provide resource 16 management and task execution on a heterogeneous local-area network. NetShare has two main components, the Resource Management Subsystem and the Task Management Subsystem.
Reference: [Agg93b] <institution> NetMake Technology Summary. Aggregate Computing, Inc., Minneapolis, MN, </institution> <year> 1993. </year>
Reference-contexts: A local task executes on the host where it originated, without going through the global scheduling system. A foreign task originates at a host different from the one on which it executes. NetShare NetShare is a distributed systems construction product of Aggregate Computing, Inc. <ref> [Agg93c, Agg93b, Agg93a] </ref>. NetShare comprises services that provide resource 16 management and task execution on a heterogeneous local-area network. NetShare has two main components, the Resource Management Subsystem and the Task Management Subsystem.
Reference: [Agg93c] <institution> Using the NetShare SDK to Build a Distributed Application: A Technical Discussion. Aggregate Computing, Inc., Minneapolis, MN, </institution> <year> 1993. </year>
Reference-contexts: A local task executes on the host where it originated, without going through the global scheduling system. A foreign task originates at a host different from the one on which it executes. NetShare NetShare is a distributed systems construction product of Aggregate Computing, Inc. <ref> [Agg93c, Agg93b, Agg93a] </ref>. NetShare comprises services that provide resource 16 management and task execution on a heterogeneous local-area network. NetShare has two main components, the Resource Management Subsystem and the Task Management Subsystem. <p> Clients query the database through the CSRL, and receive a set of matching records in response. A sample call to the CSRL, which appears in <ref> [Agg93c] </ref>, is: select UNIX_HOST if ((UNIX_HOST:LOAD_5 &lt; 1.0) && (UNIX_HOST:USERS == 0)) order by (UNIX_HOST:LOAD_5) This call queries the database for hosts running the Unix operating system, with a five-minute load average less than 1.0, and no active users.
Reference: [AHU74] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1974. </year> <note> ISBN 0-201-00029-6. </note>
Reference-contexts: As is demonstrated by nyneve, a child may have multiple parents. Children with the same parent are called siblings. This usage corresponds to the definitions of son, father , brother , proper ancestor , and proper descendant from <ref> [AHU74] </ref>. The term neighbor refers to one of a node's parents, children, or siblings. <p> The kk operator returns the number of elements in the set, e.g. kfa; b; cgk = 3: 4.3 Tree-structured Systems This section defines and proves properties of update semantics for tree-structured systems. We use standard definitions from graph notation, as found in <ref> [AHU74] </ref>. A tree is a graph G = (V; E) with the following three properties: 1. There is exactly one vertex, called the root, that has no parent. 2. Every vertex except the root has exactly one parent. 3. <p> This is known to be an NP-complete problem (see <ref> [AHU74] </ref>). The second part of the definition ensures that the primary links selected by the set covering form a spanning tree. The remainder of this section describes and analyzes a method for determining if a graph is viable. First, each node must be able to compute its Ro x set.
Reference: [BF81] <author> R. M. Bryant and R. A. Finkel. </author> <title> A stable distributed scheduling algorithm. </title> <booktitle> In Proceedings of the International Conference on Distributed Computing Systems, </booktitle> <pages> pages 314-323. </pages> <publisher> IEEE, </publisher> <month> April </month> <year> 1981. </year>
Reference-contexts: The Charlotte distributed operating system uses the basic algorithm, with the addition of message endpoint forwarding [AF87, FA89]. Bryant and Finkel <ref> [BF81] </ref> concentrates on developing stable process migration methods. <p> N Greedy Load-Sharing [Cho90] Y N Y N N X Y N Gao, et al. [GLR84] (BAR) Y N Y N N x N N Stankovic [Sta84] Y N Y N N x P N Chou and Abraham [CA83] Y N Y N N x Y N Bryant and Finkel <ref> [BF81] </ref> Y N Y N N x Y N Chow and Kohler [CK79] Y N N Y N x N N Casey [Cas81] (dipstick) Y N Y N E x N N (bidding) Y N Y N E x N N (adaptive learning) Y N Y N N Y Y N <p> Stankovic [Sta84] gives three variants of load-balancing algorithms based on point-to-point communication that compare the local load to the load on remote processors. Chou and Abraham [CA83] describes a class of load-redistribution algorithms for processor-failure recovery in distributed systems. The work presented in Bryant and Finkel <ref> [BF81] </ref> combines load balancing, dynamic reassignment, and probabilistic scheduling to ensure stability under task migration. This method uses neighbor-to-neighbor communication and forced acceptance to load balance between pairs of machines.
Reference: [BH91a] <author> A. M. Bond and J. H. Hine. DRUMS: </author> <title> A Distributed Performance Information Service. </title> <booktitle> In 14th Australian Computer Science Conference, </booktitle> <address> Sydney, Australia, </address> <month> February </month> <year> 1991. </year>
Reference-contexts: There is no provision for extending the description mechanism, although the presence of default host, user, and application descriptions provides some generality. Because of 20 its centralized decision making, Load balancer violates the scalability and execution autonomy requirements. DRUMS and ANY DRUMS 1 <ref> [BH91a, BH91b, Bon91] </ref> is a distributed information collection and management system developed at Victoria University, Wellington. DRUMS has three main components: local system state monitors (rstat+), processes that collect information from a set of hosts (collector), and a replicated centralized information manager (database).
Reference: [BH91b] <author> A. M. Bond and J. H. Hine. DRUMS: </author> <title> A Distributed Statistical Server for STARS. </title> <booktitle> In Winter USENIX, </booktitle> <address> Dallas, Texas, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: There is no provision for extending the description mechanism, although the presence of default host, user, and application descriptions provides some generality. Because of 20 its centralized decision making, Load balancer violates the scalability and execution autonomy requirements. DRUMS and ANY DRUMS 1 <ref> [BH91a, BH91b, Bon91] </ref> is a distributed information collection and management system developed at Victoria University, Wellington. DRUMS has three main components: local system state monitors (rstat+), processes that collect information from a set of hosts (collector), and a replicated centralized information manager (database).
Reference: [BJ87] <author> K. P. Birman and T. A. Joseph. </author> <title> Reliable communication in the presence of failures. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(1) </volume> <pages> 47-76, </pages> <month> February </month> <year> 1987. </year> <month> 127 </month>
Reference-contexts: Examples of reliable protocols include TCP [Pos80b] or a member of the File Transport Protocol (FTP) family [PR85, Sol92, Lot84]. The ISIS system implements levels of service ranging from unreliable messaging protocols at the lowest level to reliable multicast protocols <ref> [BJ87] </ref>. The choice of protocol depends on the critical characteristic of the channel. For the update channel, timeliness is critical, and reliable protocols typically have higher overhead and delay than unreliable protocols. For the task channel, a reliable protocol ensures delivery of the task and associated data.
Reference: [BJ91] <author> G. J. Bergmann and J. M. Jagadeesh. </author> <title> An MIMD Parallel Processing Programming System with Automatic Resource Allocation. </title> <booktitle> In Proceedings of the ISMM International Workshop on Parallel Computing, </booktitle> <pages> pages 301-304, </pages> <address> Trani, Italy, </address> <month> September 10-13 </month> <year> 1991. </year>
Reference-contexts: MICROS [WV80] Y Y Y N N Y Y N Klappholz and Park [KP84] (DRS) Y N Y N N x Y N Reif and Spirakis [RS82] Y N Y N N x N N Ousterhout, et al. [OSS80] Y Y N N N x N N Bergmann and Jagadeesh <ref> [BJ91] </ref> Y N N N N x N N Drexl [Dre90] Y N N Y N x x N Hochbaum and Shmoys [HS88] Y N N Y N x x N Hsu, et al. [HWK89] Y N N Y N x x N Stone [Sto77] Y N N Y N x <p> The centralized scheduler limits the scalability of this approach. 2.4.4 Static Algorithms All the algorithms in this section are static, and as such, are centralized and without support for autonomy. Bergmann and Jagadeesh <ref> [BJ91] </ref> describes a simple centralized scheme using a heuristic approach to schedule a task force on a set of homogeneous processors. The processors are tightly-coupled and have shared memory. The algorithm generates an initial mapping, then uses a bounded probabilistic approach to move towards the optimal solution. <p> The performance evaluation involved the execution of three distinct algorithms from the literature, and comparisons of their performance with a theoretical optimum. The algorithms were the Greedy Load-Sharing Algorithm (Greedy) from [Cho90], Arrival-Balanced Scheduling (ABS) from [Bla92], and a variation of the BOS algorithm (BOS) from <ref> [BJ91] </ref>. In all cases, the scheduling modules used an update frequency of one quanta. The Greedy and ABS algorithms are dynamic, in that they schedule tasks as they arrive at the system.
Reference: [BK90] <author> F. Bonomi and A. Kumar. </author> <title> Adaptive Optimal Load Balancing in a Nonhomogeneous Multiserver System with a Central Job Scheduler. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(10) </volume> <pages> 1232-1250, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: x P N Stankovic [Sta85b] Y N Y N N x N N Andrews et al. [ADD82] Y N Y x E x Y N Majumdar and Green [MG80] Y Y Y N N x N N Bonomi [Bon90] Y N N N N x N N Bonomi and Kumar <ref> [BK90] </ref> Y N N Y N x N N Greedy Load-Sharing [Cho90] Y N Y N N X Y N Gao, et al. [GLR84] (BAR) Y N Y N N x N N Stankovic [Sta84] Y N Y N N x P N Chou and Abraham [CA83] Y N Y N <p> RTRM uses broadcast communication of a non-extensible description, which limits scalability, and does not support autonomy. 32 Bonomi [Bon90] discusses properties of the Join the Shortest Queue (JSQ) heuristic for load balancing, and presents a heuristic that performs better, based on queuing theory. Bonomi and Kumar <ref> [BK90] </ref> presents an adaptive heuristic based on stochastic splitting of task forces and shows that in a least-squares sense, the heuristic balances the server idle times. Chowdhury [Cho90] describes the Greedy load-sharing algorithm. The Greedy algorithm uses system load to decide where a job should be placed.
Reference: [Bla92] <author> B. A. Blake. </author> <title> Assignment of Independent Tasks to Minimize Completion Time. </title> <journal> Software-Practice and Experience, </journal> <volume> 22(9) </volume> <pages> 723-734, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: The remainder of this section contains a brief description of each method, with a discussion of its place in the taxonomy and its individual properties. 2.4.1 Dynamic, Distributed, Cooperative, Suboptimal Algorithms All of the algorithms in this section are dynamic, distributed, cooperative, suboptimal, and heuristic. Blake <ref> [Bla92] </ref> describes four suboptimal, heuristic algorithms. Under the first algorithm, Non-Scheduling (NS), a task is run where it is submitted. The second algorithm is Random Scheduling (RS), wherein a processor is selected at random and is forced to run a task. <p> of distributed scheduling survey, part I M e t h o d P l c M c a i m i t i u e H t r g n o s u o o y v r e d c l b e x e s b e Blake <ref> [Bla92] </ref> (NS) Y N Y N C Y Y x (ABS) Y N Y N N N Y x (EBS) Y N Y N N N Y x Condor [BLL92] Y Y P N A x N N Remote Unix [Lit87] Y Y P N A x N N Butler [Nic87] <p> Medusa uses static assignment and centralized decision making it a combined policy and mechanism. It does not support autonomy, nor is the mechanism scalable. In addition to the four distributed algorithms already mentioned, Blake <ref> [Bla92] </ref> describes a fifth method called Continual Balanced Scheduling (CBS), that uses a centralized scheduler. Each time a task arrives, CBS generates a mapping within two time quanta of the optimum, and causes tasks to be migrated accordingly. <p> As an example, the MESSIAHS prototype includes a default handler for schedule request messages. The administrator customizes the scheduling policy by writing a filter routine. Figure 5.6 lists the code for Arrival Balanced Scheduling <ref> [Bla92] </ref>, figure 5.7 lists the code for the greedy algorithm, and figure 5.8 lists the code for the BOS algorithm. The next chapter analyzes the performance of these implementations. The implementations of three algorithms demonstrate that the underlying mechanisms are easy to use. <p> The performance evaluation involved the execution of three distinct algorithms from the literature, and comparisons of their performance with a theoretical optimum. The algorithms were the Greedy Load-Sharing Algorithm (Greedy) from [Cho90], Arrival-Balanced Scheduling (ABS) from <ref> [Bla92] </ref>, and a variation of the BOS algorithm (BOS) from [BJ91]. In all cases, the scheduling modules used an update frequency of one quanta. The Greedy and ABS algorithms are dynamic, in that they schedule tasks as they arrive at the system. <p> The inter-task delay was selected from a uniform distribution between zero and the arrival rate parameter; e.g. for an inter-task delay parameter of three, tasks would arrive separated by zero, one, two, or three quanta with approximately equal frequency. This is the same model used in <ref> [Bla92] </ref>. 104 The Greedy algorithm is classified as dynamic, distributed, non-cooperative, suboptimal, and heuristic. The Greedy algorithm rates the load on a host as the number of tasks the host has accepted for execution, without regard to processor speed. Figure 6.2 lists the decision filter for the Greedy algorithm. <p> A Poisson distribution is accepted as closely modeling job behavior in computer systems (see <ref> [Bla92, Fin88] </ref>). A Poisson distribution has a single parameter, , and is defined as f (x) = x! The parameter is both the mean and the variance of the distribution. Figure 6.5 displays Poisson distributions with values of 3, 5, and 10.
Reference: [BLL92] <author> A. Bricker, M. Litzkow, and M. Livny. </author> <title> Condor Technical Summary. </title> <type> Technical Report 1069, </type> <institution> Department of Computer Science, University of Wisconsin-Madison, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: DRUMS does not provide extensible description mechanisms, and therefore does not follow the principle of generality. Remote UNIX, Condor, Butler, and Distributed Batch Remote Unix and its successor, Condor, were developed at the University of Wisconsin <ref> [Lit87, BLL92] </ref>. Butler was developed as part of the Andrew project 22 at Carnegie-Mellon University [Nic87], and Distributed Batch was developed at the MITRE Corporation [GSS89]. <p> n o s u o o y v r e d c l b e x e s b e Blake [Bla92] (NS) Y N Y N C Y Y x (ABS) Y N Y N N N Y x (EBS) Y N Y N N N Y x Condor <ref> [BLL92] </ref> Y Y P N A x N N Remote Unix [Lit87] Y Y P N A x N N Butler [Nic87] Y Y P Y A x N N MITRE [GSS89] Y Y P P A x N N Casavant and Kuhl [CK84] Y Y Y N E x P <p> The first application demonstrates the task revocation facility as used by a general-purpose distributed batch system. The second application implements a load-balancing algorithm. 5.4.1 Distributed Batch The mitre distributed batch [GSS89], Condor <ref> [BLL92] </ref>, and Remote Unix [Lit87] systems support general-purpose distributed processing for machines running the Unix operating system. Recall that Condor has some support for execution autonomy. In particular, Condor includes a limited policy expression mechanism, with predefined variables and functions, without the possibility of extending the system or task description.
Reference: [BMK88] <author> D. R. Boggs, J. C. Mogul, and C. A. Kent. </author> <title> Measured Capacity of an Ethernet: Myths and Reality. </title> <type> Technical Report 88/4, </type> <institution> Digital Equipment Corporation, Western Research Laboratory, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: The update channel is unidirectional; the recipient of an update message returns no information through the update channel. The update channel makes no attempt to ensure reliability. If a reliable message passing mechanism exists, it may be used. As noted by Boggs, et al. in <ref> [BMK88] </ref>, networks are generally reliable under normal use. Timely delivery of data is more important than reliable delivery; late information is likely to be out-of-date, and therefore of little value. Reliable protocols generally have higher communication overhead than unreliable protocols.
Reference: [Bon90] <author> F. Bonomi. </author> <title> On Job Assignment for a Parallel System of Processor Sharing Queues. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(7) </volume> <pages> 858-869, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: s b e Stankovic and Sidhu [SS84] Y Y Y N EAC x P N Stankovic [Sta85b] Y N Y N N x N N Andrews et al. [ADD82] Y N Y x E x Y N Majumdar and Green [MG80] Y Y Y N N x N N Bonomi <ref> [Bon90] </ref> Y N N N N x N N Bonomi and Kumar [BK90] Y N N Y N x N N Greedy Load-Sharing [Cho90] Y N Y N N X Y N Gao, et al. [GLR84] (BAR) Y N Y N N x N N Stankovic [Sta84] Y N Y N <p> RTRM uses broadcast communication of a non-extensible description, which limits scalability, and does not support autonomy. 32 Bonomi <ref> [Bon90] </ref> discusses properties of the Join the Shortest Queue (JSQ) heuristic for load balancing, and presents a heuristic that performs better, based on queuing theory.
Reference: [Bon91] <author> A. M. Bond. </author> <title> A Distributed Service Based on Migrating Servers. </title> <type> Technical Report CS-TR-91/4, </type> <institution> Computer Science Department, Victoria University, Wellington, </institution> <address> New Zealand, </address> <year> 1991. </year>
Reference-contexts: There is no provision for extending the description mechanism, although the presence of default host, user, and application descriptions provides some generality. Because of 20 its centralized decision making, Load balancer violates the scalability and execution autonomy requirements. DRUMS and ANY DRUMS 1 <ref> [BH91a, BH91b, Bon91] </ref> is a distributed information collection and management system developed at Victoria University, Wellington. DRUMS has three main components: local system state monitors (rstat+), processes that collect information from a set of hosts (collector), and a replicated centralized information manager (database).
Reference: [Bou] <author> S. R. Bourne. </author> <title> An Introduction to the UNIX Shell. 4.3 BSD UNIX documentation. </title>
Reference-contexts: As an alternative to MIL, other interface languages could be implemented. PERL [WS90] and Python [vR92] are interpreted languages with efficient implementations. Both are intended for combining traditional imperative programming languages, such as C [KR90], with command-line interpreters, such as the Bourne Shell <ref> [Bou] </ref>. Tcl [Ous93] is a command language that supplies a library of parsing functions that could be integrated into the MESSIAHS scheduling module in place of MIL.
Reference: [BPY90] <author> M. Bowman, L. L. Peterson, and A. Yeatts. Univers: </author> <title> An Attribute-Based Name Server. </title> <journal> Software-Practice and Experience, </journal> <volume> 20(4) </volume> <pages> 403-424, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: The RIS finds the matching set of hosts, and returns the set, sorted by five-minute load average. The syntax and use of the resource management mechanism is similar to that found in the Univers <ref> [BPY90] </ref> and Profile [Pet88] naming systems. The client uses the Task Management Subsystem (TMS) to schedule the individual tasks for execution. The TMS is composed of the Task Servers (TS) and the Client Side Task Library (CSTL). <p> Aggregate types such as records, arrays, or unions may be available but are not guaranteed to be present. The extension mechanism is similar in concept to the attribute-based descriptions of the Profile [Pet88] and Univers <ref> [BPY90] </ref> systems developed at the University of Arizona. We have simplified and tailored the concept to our more limited purposes. The mechanism is the same for extending both task and system description vectors, but the meaning of the fields is different.
Reference: [CA83] <author> T. C. K. Chou and J. A. Abraham. </author> <title> Load redistribution under failure in distributed systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(9):799-808, </volume> <month> September </month> <year> 1983. </year>
Reference-contexts: N Bonomi and Kumar [BK90] Y N N Y N x N N Greedy Load-Sharing [Cho90] Y N Y N N X Y N Gao, et al. [GLR84] (BAR) Y N Y N N x N N Stankovic [Sta84] Y N Y N N x P N Chou and Abraham <ref> [CA83] </ref> Y N Y N N x Y N Bryant and Finkel [BF81] Y N Y N N x Y N Chow and Kohler [CK79] Y N N Y N x N N Casey [Cas81] (dipstick) Y N Y N E x N N (bidding) Y N Y N E x <p> The first algorithm balances arrival rates, with the assumption that all jobs take the same time. The second algorithm balances unfinished work. Stankovic [Sta84] gives three variants of load-balancing algorithms based on point-to-point communication that compare the local load to the load on remote processors. Chou and Abraham <ref> [CA83] </ref> describes a class of load-redistribution algorithms for processor-failure recovery in distributed systems. The work presented in Bryant and Finkel [BF81] combines load balancing, dynamic reassignment, and probabilistic scheduling to ensure stability under task migration. This method uses neighbor-to-neighbor communication and forced acceptance to load balance between pairs of machines.
Reference: [Cas81] <author> L. M. Casey. </author> <title> Decentralised scheduling. </title> <journal> Austrialian Computer Journal, </journal> <volume> 13(2) </volume> <pages> 58-63, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: x N N Stankovic [Sta84] Y N Y N N x P N Chou and Abraham [CA83] Y N Y N N x Y N Bryant and Finkel [BF81] Y N Y N N x Y N Chow and Kohler [CK79] Y N N Y N x N N Casey <ref> [Cas81] </ref> (dipstick) Y N Y N E x N N (bidding) Y N Y N E x N N (adaptive learning) Y N Y N N Y Y N 29 Table 2.4 Summary of distributed scheduling survey, part III M e t h o d P l c M c a <p> This method uses neighbor-to-neighbor communication and forced acceptance to load balance between pairs of machines. Chow and Kohler [CK79] presents load-balancing strategies using a centralized job controller, based on analysis of queuing theory models of heterogeneous distributed systems. Casey <ref> [Cas81] </ref> gives an earlier and less complete version of the Casavant and Kuhl taxonomy, with the term centralised replacing non-distributed and decentralised substituting for distributed.
Reference: [Che90] <author> B. Cheswick. </author> <title> The Design of a Secure Internet Gateway. </title> <booktitle> In USENIX Summer Conference, </booktitle> <pages> pages 233-237, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Garfinkel and Spafford [GS91] define this type of behavior as a firewall. Cheswick discusses the the construction of a secure packet router embodying the firewall concept in <ref> [Che90] </ref>. 3.2.2.6 The Extension Mechanism It is impossible to predefine the complete set of characteristics used by all present and future scheduling algorithms. Therefore, the description vectors include an extension mechanism that allows users to customize the description of a system or task.
Reference: [Cho90] <author> S. Chowdhury. </author> <title> The Greedy Load Sharing Algorithm. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 93-99, </pages> <year> 1990. </year> <month> 128 </month>
Reference-contexts: N N Andrews et al. [ADD82] Y N Y x E x Y N Majumdar and Green [MG80] Y Y Y N N x N N Bonomi [Bon90] Y N N N N x N N Bonomi and Kumar [BK90] Y N N Y N x N N Greedy Load-Sharing <ref> [Cho90] </ref> Y N Y N N X Y N Gao, et al. [GLR84] (BAR) Y N Y N N x N N Stankovic [Sta84] Y N Y N N x P N Chou and Abraham [CA83] Y N Y N N x Y N Bryant and Finkel [BF81] Y N Y <p> Bonomi and Kumar [BK90] presents an adaptive heuristic based on stochastic splitting of task forces and shows that in a least-squares sense, the heuristic balances the server idle times. Chowdhury <ref> [Cho90] </ref> describes the Greedy load-sharing algorithm. The Greedy algorithm uses system load to decide where a job should be placed. <p> The true guard in the revocation filter rule (line 10) matches any available task, and the value portion of the rule assigns an equal priority to all tasks under consideration. 5.4.2 Load Balancing Several researchers have investigated load balancing and sharing policies for distributed systems, such as those described in <ref> [Cho90] </ref>, [ELZ85], and [Puc88]. The Greedy Load-Sharing Algorithm [Cho90], makes decisions based on a local optimum. When a user submits a task for execution, the receiving system attempts to place the task with a less busy neighbor, according to a weighting function. <p> (line 10) matches any available task, and the value portion of the rule assigns an equal priority to all tasks under consideration. 5.4.2 Load Balancing Several researchers have investigated load balancing and sharing policies for distributed systems, such as those described in <ref> [Cho90] </ref>, [ELZ85], and [Puc88]. The Greedy Load-Sharing Algorithm [Cho90], makes decisions based on a local optimum. When a user submits a task for execution, the receiving system attempts to place the task with a less busy neighbor, according to a weighting function. If no suitable neighbor is found, the task is accepted for local execution. <p> The performance evaluation involved the execution of three distinct algorithms from the literature, and comparisons of their performance with a theoretical optimum. The algorithms were the Greedy Load-Sharing Algorithm (Greedy) from <ref> [Cho90] </ref>, Arrival-Balanced Scheduling (ABS) from [Bla92], and a variation of the BOS algorithm (BOS) from [BJ91]. In all cases, the scheduling modules used an update frequency of one quanta. The Greedy and ABS algorithms are dynamic, in that they schedule tasks as they arrive at the system.
Reference: [CK79] <author> Y. C. Chow and W. H. Kohler. </author> <title> Models for dynamic load balancing in a heterogeneous multiple processor system. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(5):354-361, </volume> <month> May </month> <year> 1979. </year>
Reference-contexts: Gao, et al. [GLR84] (BAR) Y N Y N N x N N Stankovic [Sta84] Y N Y N N x P N Chou and Abraham [CA83] Y N Y N N x Y N Bryant and Finkel [BF81] Y N Y N N x Y N Chow and Kohler <ref> [CK79] </ref> Y N N Y N x N N Casey [Cas81] (dipstick) Y N Y N E x N N (bidding) Y N Y N E x N N (adaptive learning) Y N Y N N Y Y N 29 Table 2.4 Summary of distributed scheduling survey, part III M e <p> The work presented in Bryant and Finkel [BF81] combines load balancing, dynamic reassignment, and probabilistic scheduling to ensure stability under task migration. This method uses neighbor-to-neighbor communication and forced acceptance to load balance between pairs of machines. Chow and Kohler <ref> [CK79] </ref> presents load-balancing strategies using a centralized job controller, based on analysis of queuing theory models of heterogeneous distributed systems. Casey [Cas81] gives an earlier and less complete version of the Casavant and Kuhl taxonomy, with the term centralised replacing non-distributed and decentralised substituting for distributed.
Reference: [CK84] <author> T. L. Casavant and J. G. Kuhl. </author> <title> Design of a loosely-coupled distributed multiprocessing network. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 42-45. </pages> <publisher> IEEE, </publisher> <month> August </month> <year> 1984. </year>
Reference-contexts: Y N N N Y x Condor [BLL92] Y Y P N A x N N Remote Unix [Lit87] Y Y P N A x N N Butler [Nic87] Y Y P Y A x N N MITRE [GSS89] Y Y P P A x N N Casavant and Kuhl <ref> [CK84] </ref> Y Y Y N E x P N Ghafoor and Ahmad [GA90] Y Y Y N E Y P N Stankovic [Sta81, Sta85a] Y N Y N N x N x Ramamritham and Stankovic [RS84] Y N Y N E x N x Wave Scheduling [VW84] Y Y Y N <p> Price and Salama [PS90] Y N N Y N x x N Ramakrishnan et al. [RCD91] Y N N Y N x x N Sarkar [Sar89] Y N N Y N x x N Sarkar and Hennessey [SH86b] Y N N Y N x x N 30 Casavant and Kuhl <ref> [CK84] </ref> describes a distributed task execution environment for UNIX System 7, with the primary goal of load balancing without altering the user interface to the operating system. As such, the system combines mechanism and policy. This system supports execution autonomy, but not communication autonomy or administrative autonomy.
Reference: [CK88] <author> T. L. Casavant and J. G. Kuhl. </author> <title> A Taxonomy of Scheduling in General-Purpose Distributed Computing Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(2) </volume> <pages> 141-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Within a distributed system, there are two levels of task scheduling: the association of tasks with processors (global scheduling), and the choice of which task to execute among those available on a processor (local scheduling) <ref> [CK88] </ref>. This dissertation concentrates on developing support for global scheduling. Webster's Dictionary defines autonomous as "having the power of self-government," or as "responding, reacting, or developing independently of the whole." Thus, 3 an autonomous system makes local policy decisions and can act without the permission of any external authority. <p> None of these systems provide support for all four aspects of autonomy. These systems do not meet the requirements of generality, in part because they have no extensible description mechanisms. 23 2.3 A Taxonomy of Scheduling Algorithms Casavant and Kuhl <ref> [CK88] </ref> proposes a taxonomy for scheduling algorithms in distributed systems, which is reproduced in figure 2.4. Section 2.4 surveys related work in the area of scheduling algorithms, and classifies those algorithms in terms of this taxonomy. <p> However, none of these meet all the requirements for generality, scalability, autonomy, data soundness, and non-interference set forth in section 1.1. 37 A taxonomy of scheduling policies, originally proposed by Casavant and Kuhl <ref> [CK88] </ref>, was applied to a broad spectrum of scheduling algorithms from the literature. These surveyed algorithms were also analyzed with respect to the the design principles set forth in chapter 1.1 to determine what capabilities the algorithms require of the underlying mechanism. 38 3. <p> Static algorithms consider only the system topography, not the state, when calculating the mapping. Dynamic algorithms take the current system state as input, and the resultant mapping depends on the state (see <ref> [CK88] </ref>). Figure 5.1 depicts the structure of an MIL program.
Reference: [CLL85] <author> M. J. Carey, M. Livny, and H. Lu. </author> <title> Dynamic Task Allocation in a Distributed Database System. </title> <booktitle> In Distributed Computing Systems, </booktitle> <pages> pages 282-291. </pages> <publisher> IEEE, </publisher> <year> 1985. </year>
Reference-contexts: This dissertation defines a task as a consumer of resources. Examples of tasks include the conventional model of a computationally intensive unit in a larger program, as well as a set of database queries (see Carey, et al. <ref> [CLL85] </ref>), output requests for printers, and data transfers over a communication network. For simplicity of description, this dissertation restricts further discussions to the conventional model of placing computational tasks on processors. A task force (as defined in [VW84]) comprises a group of cooperating tasks for solving a single problem.
Reference: [Com84] <author> D. E. Comer. </author> <title> Operating System Design: </title> <booktitle> the XINU Approach, </booktitle> <volume> volume 1. </volume> <publisher> Prentice-Hall, </publisher> <year> 1984. </year> <note> ISBN 0-13-637539-1. </note>
Reference-contexts: This paper also lists three methods for load balancing: Dipstick, Bidding, and Adaptive Learning, then describes a load-balancing system whereby each processor includes a two-byte status update with each message sent. The Dipstick method is the same as the traditional watermark processing found in 33 many operating systems <ref> [Com84] </ref>. The Adaptive Learning algorithm uses a feedback mechanism based on the run queue length at each processor. Hwang, et al. [HCG + 82] describes a specialized implementation of a distributed Unix project for a network of Digital Equipment Corporation machines, and an associated load-balancing strategy.
Reference: [Com91] <author> D. E. Comer. </author> <title> Internetworking with TCP/IP, volume I, Principles, Protocols, and Architecture. </title> <publisher> Prentice Hall, </publisher> <address> second edition, </address> <year> 1991. </year> <note> ISBN 0-13-468505-9. </note>
Reference-contexts: An update cycle occurs when an update vector that describes a system is incorporated into another system's update vector and subsequently advertised back to the original system. Such behavior causes an ever-increasing overestimation of system resources, analogous to the count to infinity problem in network routing protocols (see Comer <ref> [Com91, chapter 15] </ref>). For any system, there are three sets of systems that could pass it updates: its children, its parents, and its siblings within the hierarchy.
Reference: [Con58] <author> M. E. Conway. </author> <title> Proposal for an UNCOL. </title> <journal> Communications of the ACM, </journal> <volume> 1(3), </volume> <year> 1958. </year>
Reference-contexts: External program representations, analogous to external data representations, have been proposed. The Open Software Foundation has proposed ANDF (Architecture Neutral Distribution Format [Mac93]) and an associated implementation technology, TDF (Ten15 Distribution Format [Pee92]), as standards for intermediate program representation for the OSF/1 operating system. UNCOL <ref> [Con58] </ref> is an earlier effort at such a standard. While each of these addresses some aspects of supporting architecture-independent program representation, none of them is wholly satisfactory. The specification of a unified, external program representation is an open problem.
Reference: [CS92] <author> S. J. Chapin and E. H. Spafford. </author> <title> Scheduling Support for an Internetwork of Heterogeneous, Autonomous Processors. </title> <type> Technical Report TR-92-006, </type> <institution> Department of Computer Sciences, Purdue University, </institution> <month> January </month> <year> 1992. </year>
Reference: [CS93a] <author> S. J. Chapin and E. H. Spafford. </author> <title> An Overview of the MESSIAHS Distributed Scheduling Support System. </title> <type> Technical Report TR-93-011 (su-percedes TR-93-004), </type> <institution> Department of Computer Sciences, Purdue University, </institution> <month> January </month> <year> 1993. </year>
Reference: [CS93b] <author> D. E. Comer and D. L. Stevens. </author> <title> Internetworking with TCP/IP, volume III, Client-Server Programming and Applications. </title> <publisher> Prentice Hall, </publisher> <address> first edition, </address> <year> 1993. </year> <note> ISBN 0-13-474222-2. </note>
Reference-contexts: The former approach, called asymmetric conversion, requires O (n 2 ) different conversion modules, while the latter approach, called symmetric conversion, requires O (n) distinct modules (see Comer and Stevens <ref> [CS93b, chapter 19] </ref>). The approach of symmetric data conversion is generally preferred because of the relative ease of adding new data formats to the distributed system. ISO X.409, Abstract Syntax Notation One (ASN.1) is the international standard for external data representation [fS87a, fS87b], and specifies an explicit encoding.
Reference: [DELO89] <author> W. Du, A. K. Elmagarmid, Y. Leu, and S. D. Ostermann. </author> <title> Effects of Local Autonomy on Global Concurrency Control in Heterogeneous Distributed Database Systems. </title> <booktitle> In Second International Conference on Data and Knowledge Systems for Manufacturing and Engineering, </booktitle> <pages> pages 113-120. </pages> <publisher> IEEE, </publisher> <year> 1989. </year>
Reference: [DO91] <author> F. Douglis and J. Ousterhout. </author> <title> Transparent process migration: Design alternatives and the sprite implementation. </title> <journal> Software-Practice and Experience, </journal> <volume> 21(8) </volume> <pages> 757-785, </pages> <month> August </month> <year> 1991. </year> <month> 129 </month>
Reference-contexts: The numbering preserves that of the basic algorithm. Steps 2a through 2e indicate substeps of the transfer process. The DEMOS/MP approach is typical of migration mechanisms that attempt to optimize various aspects of the transfer step. Sprite The Sprite operating system achieves transparent process migration <ref> [OCD + 88, DO91] </ref>. Sprite uses the basic checkpoint-transfer-restart algorithm, but simplifies the transfer process because of the use of backing files. Instead of paging to local storage, Sprite pages to ordinary files in the network file system.
Reference: [Dre90] <author> A. Drexl. </author> <title> Job-Prozessor-Scheduling fur heterogene Computernetz-werke (Job-Processor Scheduling for Heterogeneous Computer Networks). </title> <journal> Wirtschaftsinformatik, </journal> <volume> 31(4) </volume> <pages> 345-351, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Klappholz and Park [KP84] (DRS) Y N Y N N x Y N Reif and Spirakis [RS82] Y N Y N N x N N Ousterhout, et al. [OSS80] Y Y N N N x N N Bergmann and Jagadeesh [BJ91] Y N N N N x N N Drexl <ref> [Dre90] </ref> Y N N Y N x x N Hochbaum and Shmoys [HS88] Y N N Y N x x N Hsu, et al. [HWK89] Y N N Y N x x N Stone [Sto77] Y N N Y N x x N Lo [Lo88] Y N N Y N x <p> The processors are tightly-coupled and have shared memory. The algorithm generates an initial mapping, then uses a bounded probabilistic approach to move towards the optimal solution. Drexl <ref> [Dre90] </ref> describes a stochastic scheduling algorithm for heterogeneous systems. The algorithm uses one-time assignment, and uses a probability-based penalty function to produce schedules within an acceptable range. Hochbaum and Shmoys [HS88] describes a polynomial-time, approximate, enumerative scheduling technique for processors with different processing speeds, called the dual-approximation algorithm.
Reference: [DRS89] <author> B. F. Dubach, R. M. Rutherford, and C. M. Shub. </author> <title> Process-Originated Migration in a Heterogeneous Environment. </title> <booktitle> In Proceedings of the Computer Science Conference. ACM, </booktitle> <year> 1989. </year>
Reference-contexts: Machines that share a common object code format and instruction set can share object files without translation. Various attempts have been made that have resulted in O (n 2 ) solutions to the problem. Essick [EI87], and Shub, et al. <ref> [DRS89, Shu90] </ref> devise multiple-architecture task representations. Both representations combine machine code for multiple architectures in a single program. External program representations, analogous to external data representations, have been proposed.
Reference: [DZ83] <author> J. D. Day and H. Zimmermann. </author> <title> The OSI Reference Model. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 71(12) </volume> <pages> 1334-1340, </pages> <month> December </month> <year> 1983. </year>
Reference-contexts: If an efficient implementation of a reliable messaging protocol exists, such as in later versions of ISIS, then it could be used for the update channel. Section 3.2.2.2 discusses the requirements of the channels in more detail. In terms of the OSI seven-layer model <ref> [DZ83] </ref>, the machine-dependent layer of the scheduling module contains parts of the session and presentation layers, and provides access to the network and transport layers below. Functions of the OSI application layer appear in the higher layers of the schedule module.
Reference: [EI87] <author> R. B. Essick IV. </author> <title> The Cross-Architecture Procedure Call. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1987. </year> <note> Report No. UIUCDCS-R-87-1340. </note>
Reference-contexts: As of yet, no one has specified an architecture-independent and operating-system-independent program representation. Machines that share a common object code format and instruction set can share object files without translation. Various attempts have been made that have resulted in O (n 2 ) solutions to the problem. Essick <ref> [EI87] </ref>, and Shub, et al. [DRS89, Shu90] devise multiple-architecture task representations. Both representations combine machine code for multiple architectures in a single program. External program representations, analogous to external data representations, have been proposed.
Reference: [ELZ85] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> A Comparison of Receiver-Initiated and Sender-Initiated Dynamic Load Sharing. </title> <type> Technical Report 85-04-01, </type> <institution> University of Washington, Department of Computer Science, </institution> <month> April </month> <year> 1985. </year>
Reference-contexts: Bidding algorithms advertise work to be done and wait for responses from available processors. Load balancing policies attempt to distribute the workload so that processor utilization is approximately equal for all processors in the system. Eager, et al. <ref> [ELZ85] </ref> discusses load-balancing algorithms using task migration in detail and classifies them according to whether they are sender-initiated or receiver-initiated. Under sender-initiated load balancing, the busy processor finds an idle processor to receive a task. With receiver-initiated load balancing, an idle processor locates an overloaded processor and requests a task. <p> true guard in the revocation filter rule (line 10) matches any available task, and the value portion of the rule assigns an equal priority to all tasks under consideration. 5.4.2 Load Balancing Several researchers have investigated load balancing and sharing policies for distributed systems, such as those described in [Cho90], <ref> [ELZ85] </ref>, and [Puc88]. The Greedy Load-Sharing Algorithm [Cho90], makes decisions based on a local optimum. When a user submits a task for execution, the receiving system attempts to place the task with a less busy neighbor, according to a weighting function.
Reference: [Ens78] <author> P. H. Enslow, Jr. </author> <title> What Is a "Distributed" Data Processing System? IEEE Computer, </title> <booktitle> 11(1) </booktitle> <pages> 13-21, </pages> <month> January </month> <year> 1978. </year>
Reference-contexts: Distributed systems communicate by passing messages over an external communications channel. Such systems are often called loosely-coupled systems, in contrast to tightly-coupled parallel machines that communicate through shared memory [HB84]. Coupling represents only one quality of distributed systems. Enslow <ref> [Ens78] </ref> defines four aspects of distribution: hardware distribution, data distribution, processing distribution, and control distribution. Most distributed systems, 2 especially those that assign programs to processors for execution, fail to exploit control distribution fully.
Reference: [EV87] <author> F. Eliassen and J. Veijalainen. </author> <title> Language Support for Multidatabase Transactions in a Cooperative, Autonomous Environment. </title> <booktitle> In TENCON '87, </booktitle> <pages> pages 277-281, </pages> <address> Seoul, </address> <year> 1987. </year> <booktitle> IEEE Regional Conference. </booktitle>
Reference-contexts: Their department may be part of a regional site, which is, in turn, part of a nationwide organization. No single entity, from the user to the large organization, has complete control over all the computers it may wish to use. Garcia-Molina and Kogan [GMK88], and Eliassen and Veijalainen <ref> [EV87] </ref> have examined autonomy in distributed systems and devised taxonomies for different types of autonomy. The scheme proposed by Eliassen and Veijalainen is more general but less detailed than that proposed by Garcia-Molina.
Reference: [FA89] <author> R. Finkel and Y. Artsy. </author> <title> The Process Migration Mechanism of Charlotte. </title> <journal> IEEE Computer Society Technical Committee on Operating Systems Newsletter, </journal> <volume> 3(1) </volume> <pages> 11-14, </pages> <year> 1989. </year>
Reference-contexts: Locus, Charlotte, and work by Bryant and Finkel Locus uses the basic checkpoint-transfer-restart algorithm, with the optimization that read-only segments that already exist on the target machine are not copied [PW85]. The Charlotte distributed operating system uses the basic algorithm, with the addition of message endpoint forwarding <ref> [AF87, FA89] </ref>. Bryant and Finkel [BF81] concentrates on developing stable process migration methods.
Reference: [Fin88] <author> R. A. Finkel. </author> <title> An Operating Systems Vade Mecum. </title> <publisher> Prentice Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <note> second edition, 1988. ISBN 0-13-637950-8. </note>
Reference-contexts: A Poisson distribution is accepted as closely modeling job behavior in computer systems (see <ref> [Bla92, Fin88] </ref>). A Poisson distribution has a single parameter, , and is defined as f (x) = x! The parameter is both the mean and the variance of the distribution. Figure 6.5 displays Poisson distributions with values of 3, 5, and 10.
Reference: [FR90] <author> D. G. Feitelson and L. Rudolph. </author> <title> Distributed Hierarchical Control for Parallel Processing. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This bookkeeping could quickly consume the processing power of the system, and little or 9 no productive work would be accomplished (see <ref> [WV80, FR90] </ref>). Again, there is a tradeoff between the accuracy of a system description and its size. A solution is to compress multiple system descriptions into one, thus saving space while preserving much of the descriptive information. <p> same researcher, so it is natural to place them within the same virtual system. nyneve is under administrative control of two research projects, the Xinu project and the Renaissance project, and therefore belongs to two virtual systems. 40 This hierarchical structure is similar to that presented in Feitelson and Rudolph <ref> [FR90] </ref>, which describes Distributed Hierarchical Control. Under Distributed Hierarchical Control, the system uses a hierarchically-structured multiprocessor as the master processing element in a larger multiprocessor.
Reference: [Fre] <author> Freedman Sharp and Associates Inc., </author> <title> Calgary, Alberta, Canada. Load Balancer v3.3: Automatic Job Queuing and Load Distribution over Heterogeneous UNIX Networks. </title>
Reference-contexts: Execution autonomy is compromised because the policy expression mechanism is completely under the control of the application program; the acceptance of a task for execution is based solely on the import quota of the target machine. 19 Load Balancer Load Balancer <ref> [Fre] </ref>, a product of Freedman Sharp and Associates Inc., is a batch queuing and load sharing system for Unix operating systems. Load Balancer is similar to NetShare in that Load Balancer has a single resource manager (lbmasterd), a per-node local task manager (lblocald), centralized decision making, and limited autonomy support.
Reference: [fS87a] <author> International Organization for Standardization. </author> <title> Information Processing Systems | Open Systems Interconnection | Specification of Basic Specification of Abstract Syntax Notation One (ASN.1). International Standard number 8824, ISO, </title> <month> May </month> <year> 1987. </year> <month> 130 </month>
Reference-contexts: The approach of symmetric data conversion is generally preferred because of the relative ease of adding new data formats to the distributed system. ISO X.409, Abstract Syntax Notation One (ASN.1) is the international standard for external data representation <ref> [fS87a, fS87b] </ref>, and specifies an explicit encoding. Explicit encoding embeds type information in the data stream, and a host with no prior knowledge of the data structure can interpret the data.
Reference: [fS87b] <author> International Organization for Standardization. </author> <title> Information Processing Systems | Open Systems Interconnection | Specification of Basic Encoding Rules for Abstract Syntax Notation One (ASN.1). International Standard number 8825, ISO, </title> <month> May </month> <year> 1987. </year>
Reference-contexts: The approach of symmetric data conversion is generally preferred because of the relative ease of adding new data formats to the distributed system. ISO X.409, Abstract Syntax Notation One (ASN.1) is the international standard for external data representation <ref> [fS87a, fS87b] </ref>, and specifies an explicit encoding. Explicit encoding embeds type information in the data stream, and a host with no prior knowledge of the data structure can interpret the data.
Reference: [GA90] <author> A. Ghafoor and I. Ahmad. </author> <title> An Efficient Model of Dynamic Task Scheduling for Distributed Systems. </title> <booktitle> In Computer Software and Applications Conference, </booktitle> <pages> pages 442-447. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: A x N N Remote Unix [Lit87] Y Y P N A x N N Butler [Nic87] Y Y P Y A x N N MITRE [GSS89] Y Y P P A x N N Casavant and Kuhl [CK84] Y Y Y N E x P N Ghafoor and Ahmad <ref> [GA90] </ref> Y Y Y N E Y P N Stankovic [Sta81, Sta85a] Y N Y N N x N x Ramamritham and Stankovic [RS84] Y N Y N E x N x Wave Scheduling [VW84] Y Y Y N E x P N Ni and Abani [NA81] (LED) Y N Y <p> As such, the system combines mechanism and policy. This system supports execution autonomy, but not communication autonomy or administrative autonomy. Ghafoor and Ahmad <ref> [GA90] </ref> describes a bidding system that combines mechanism and policy. A module called an Information Collector/Dispatcher runs on each node and monitors the local load and that of the node's neighbors.
Reference: [GLR84] <author> C. Gao, J. W. S. Liu, and M. Railey. </author> <title> Load balancing algorithms in homogeneous distributed systems. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 302-306. </pages> <publisher> IEEE, </publisher> <month> August </month> <year> 1984. </year>
Reference-contexts: Y N Majumdar and Green [MG80] Y Y Y N N x N N Bonomi [Bon90] Y N N N N x N N Bonomi and Kumar [BK90] Y N N Y N x N N Greedy Load-Sharing [Cho90] Y N Y N N X Y N Gao, et al. <ref> [GLR84] </ref> (BAR) Y N Y N N x N N Stankovic [Sta84] Y N Y N N x P N Chou and Abraham [CA83] Y N Y N N x Y N Bryant and Finkel [BF81] Y N Y N N x Y N Chow and Kohler [CK79] Y N N <p> This algorithm is non-cooperative in the sense that decisions are made for the local good, but it is cooperative because scheduling assignments are always accepted and all systems are working towards a global load balancing policy. Gao, et al. <ref> [GLR84] </ref> describes two load-balancing algorithms using broadcast information. The first algorithm balances arrival rates, with the assumption that all jobs take the same time. The second algorithm balances unfinished work.
Reference: [GMK88] <author> H. Garcia-Molina and B. Kogan. </author> <title> Node Autonomy in Distributed Systems. </title> <booktitle> In ACM International Symposium on Databases in Parallel and Distributed Systems, </booktitle> <pages> pages 158-166, </pages> <address> Austin, TX, </address> <month> December </month> <year> 1988. </year>
Reference-contexts: Their department may be part of a regional site, which is, in turn, part of a nationwide organization. No single entity, from the user to the large organization, has complete control over all the computers it may wish to use. Garcia-Molina and Kogan <ref> [GMK88] </ref>, and Eliassen and Veijalainen [EV87] have examined autonomy in distributed systems and devised taxonomies for different types of autonomy. The scheme proposed by Eliassen and Veijalainen is more general but less detailed than that proposed by Garcia-Molina.
Reference: [GS91] <author> S. Garfinkel and E. Spafford. </author> <title> Practical UNIX Security. </title> <publisher> O'Reilly and Associates, </publisher> <year> 1991. </year> <note> ISBN 0-937175-72-2. </note>
Reference-contexts: Proxy transfer is used when the destination is inside a virtual system that prohibits outside systems from directly accessing its members. In this case, the task is delivered to the encapsulating virtual system, which is then responsible for forwarding the task to its destination. Garfinkel and Spafford <ref> [GS91] </ref> define this type of behavior as a firewall. Cheswick discusses the the construction of a secure packet router embodying the firewall concept in [Che90]. 3.2.2.6 The Extension Mechanism It is impossible to predefine the complete set of characteristics used by all present and future scheduling algorithms.
Reference: [GSS89] <author> C. A. Gantz, R. D. Silverman, and S. J. Stuart. </author> <title> A Distributed Batching System for Parallel Processing. </title> <journal> Software-Practice and Experience, </journal> <year> 1989. </year>
Reference-contexts: Taken collectively, the aggregate computing resources are sufficient to solve difficult problems such as large-number factoring or climate simulation. When each machine is used in isolation, several limitations appear. Gantz, Silverman, and Stuart <ref> [GSS89] </ref> and Litzkow [Lit87] show that equipment in a workstation-based environment is underutilized, with processor utilization as low as 30%. <p> Remote UNIX, Condor, Butler, and Distributed Batch Remote Unix and its successor, Condor, were developed at the University of Wisconsin [Lit87, BLL92]. Butler was developed as part of the Andrew project 22 at Carnegie-Mellon University [Nic87], and Distributed Batch was developed at the MITRE Corporation <ref> [GSS89] </ref>. All of these systems attempt to increase utilization and share load across a set of Unix-based workstations, but are less complete systems than NetShare or Load Balancer. Therefore, this section groups these systems together and gives a brief description of each. <p> (ABS) Y N Y N N N Y x (EBS) Y N Y N N N Y x Condor [BLL92] Y Y P N A x N N Remote Unix [Lit87] Y Y P N A x N N Butler [Nic87] Y Y P Y A x N N MITRE <ref> [GSS89] </ref> Y Y P P A x N N Casavant and Kuhl [CK84] Y Y Y N E x P N Ghafoor and Ahmad [GA90] Y Y Y N E Y P N Stankovic [Sta81, Sta85a] Y N Y N N x N x Ramamritham and Stankovic [RS84] Y N Y <p> The first application demonstrates the task revocation facility as used by a general-purpose distributed batch system. The second application implements a load-balancing algorithm. 5.4.1 Distributed Batch The mitre distributed batch <ref> [GSS89] </ref>, Condor [BLL92], and Remote Unix [Lit87] systems support general-purpose distributed processing for machines running the Unix operating system. Recall that Condor has some support for execution autonomy.
Reference: [HB84] <author> K. Hwang and F. Briggs. </author> <title> Multiprocessor Systems Architectures. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: A solution to these limitations is to conglomerate the separate processors into a distributed system. Distributed systems communicate by passing messages over an external communications channel. Such systems are often called loosely-coupled systems, in contrast to tightly-coupled parallel machines that communicate through shared memory <ref> [HB84] </ref>. Coupling represents only one quality of distributed systems. Enslow [Ens78] defines four aspects of distribution: hardware distribution, data distribution, processing distribution, and control distribution. Most distributed systems, 2 especially those that assign programs to processors for execution, fail to exploit control distribution fully.
Reference: [HCG + 82] <author> K. Hwang, W. J. Croft, G. H. Goble, B. W. Wah, F. A. Briggs, W. R. Simmons, and C. L. Coates. </author> <title> A UNIX-based local computer network with load balancing. </title> <journal> Computer, </journal> <volume> 15(4) </volume> <pages> 55-65, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: scheduling survey, part III M e t h o d P l c M c a i m i t i u e H t r g n o s u o o y v r e d c l b e x e s b e Hwang et al. <ref> [HCG + 82] </ref> Y Y Y Y N x N N MICROS [WV80] Y Y Y N N Y Y N Klappholz and Park [KP84] (DRS) Y N Y N N x Y N Reif and Spirakis [RS82] Y N Y N N x N N Ousterhout, et al. [OSS80] Y <p> The Dipstick method is the same as the traditional watermark processing found in 33 many operating systems [Com84]. The Adaptive Learning algorithm uses a feedback mechanism based on the run queue length at each processor. Hwang, et al. <ref> [HCG + 82] </ref> describes a specialized implementation of a distributed Unix project for a network of Digital Equipment Corporation machines, and an associated load-balancing strategy. This project supports neither autonomy nor scalability. Wittie and Van Tilborg [WV80] describes MICROS and MICRONET.
Reference: [Hed88] <author> C. Hedrick. </author> <title> Routing information protocol. </title> <type> RFC 1058, </type> <institution> Network Information Center, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: Equation 4.17 employs the technique of a split horizon update found in network routing protocols <ref> [Hed88, Mal93] </ref>. 4.5.2 Proofs of General Rules This section proves rule 4.17 globally complete and correct, given that a spanning tree has been imposed on the graph.
Reference: [HPS71] <author> P. Hoel, S. Port, and C. Stone. </author> <title> Introduction to Probability Theory. Series in Statistics. </title> <publisher> Houghton Mi*in Company, </publisher> <year> 1971. </year> <note> ISBN 0-395-04636-x. </note>
Reference-contexts: A Poisson distribution has a single parameter, , and is defined as f (x) = x! The parameter is both the mean and the variance of the distribution. Figure 6.5 displays Poisson distributions with values of 3, 5, and 10. A result of the Central Limit Theorem (see <ref> [HPS71, chapter 7] </ref>) is that, for a continuous distribution with mean , and standard deviation , a sample S n of size n, and an error limit c, P fi fi n fi fi where ffi = p and is the normal distribution evaluated at ffi. 107 The variance of a
Reference: [HS88] <author> D. Hochbaum and D. Shmoys. </author> <title> A Polynomial Approximation Scheme for Scheduling on Uniform Processors: Using the Dual Approximation Approach. </title> <journal> SIAM Journal of Computing, </journal> <volume> 17(3) </volume> <pages> 539-551, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: N Reif and Spirakis [RS82] Y N Y N N x N N Ousterhout, et al. [OSS80] Y Y N N N x N N Bergmann and Jagadeesh [BJ91] Y N N N N x N N Drexl [Dre90] Y N N Y N x x N Hochbaum and Shmoys <ref> [HS88] </ref> Y N N Y N x x N Hsu, et al. [HWK89] Y N N Y N x x N Stone [Sto77] Y N N Y N x x N Lo [Lo88] Y N N Y N x x N Price and Salama [PS90] Y N N Y N x <p> The algorithm generates an initial mapping, then uses a bounded probabilistic approach to move towards the optimal solution. Drexl [Dre90] describes a stochastic scheduling algorithm for heterogeneous systems. The algorithm uses one-time assignment, and uses a probability-based penalty function to produce schedules within an acceptable range. Hochbaum and Shmoys <ref> [HS88] </ref> describes a polynomial-time, approximate, enumerative scheduling technique for processors with different processing speeds, called the dual-approximation algorithm. The algorithm solves a relaxed form of the bin-packing problem to produce a schedule within a parameterized factor, *, of optimal. <p> The algorithm also uses *-relaxation similar to the dual-approximation algorithm of Hochbaum and Shmoys <ref> [HS88] </ref>. Sarkar [Sar89] and Sarkar and Hennessey [SH86b] describe the GR graph representation and static partitioning and scheduling algorithms for single-assignment programs based on the SISAL language. In GR, nodes represent tasks and edges represent communication. The algorithm consists of four steps: cost assignment, graph expansion, internalization, and processor assignment.
Reference: [HWK89] <author> C. C. Hsu, S. D. Wang, and T. S. Kuo. </author> <title> Minimization of task turnaround time for distributed systems. </title> <booktitle> In Proceedings of the 13th Annual International Computer Software and Applications Conference, </booktitle> <year> 1989. </year>
Reference-contexts: N Ousterhout, et al. [OSS80] Y Y N N N x N N Bergmann and Jagadeesh [BJ91] Y N N N N x N N Drexl [Dre90] Y N N Y N x x N Hochbaum and Shmoys [HS88] Y N N Y N x x N Hsu, et al. <ref> [HWK89] </ref> Y N N Y N x x N Stone [Sto77] Y N N Y N x x N Lo [Lo88] Y N N Y N x x N Price and Salama [PS90] Y N N Y N x x N Ramakrishnan et al. [RCD91] Y N N Y N x <p> The algorithm solves a relaxed form of the bin-packing problem to produce a schedule within a parameterized factor, *, of optimal. That is, the total run time is bounded by (1 + *) times the optimal run time. Hsu, et al. <ref> [HWK89] </ref> describes an approximation technique called the critical sink underestimate method. The task force is represented as a directed acyclic graph, 35 with vertices representing tasks and edges representing execution dependencies. If an edge (ff; fi) appears in the graph, then ff must execute before fi.
Reference: [JLHB88] <author> E. Jul, H. Levy, N. Hutchinson, and A. Black. </author> <title> Fine-Grained Mobility in the Emerald System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year> <month> 131 </month>
Reference: [KMS93] <author> A. H. Karp, K. Miura, and H. Simon. </author> <title> 1992 Gordon Bell Prize Winners. </title> <journal> IEEE Computer, </journal> <volume> 26(1) </volume> <pages> 77-82, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Although the combined resources of several machines might solve the problem at hand, users of this equipment often find that the resources local to each machine, such as memory, disk space, and processing power, are not sufficient to execute large programs (see Karp, Miura, and Simon <ref> [KMS93] </ref> for examples). Certain scientific application programs have distinct components, best suited for massively-parallel machines, vector-processing supercomputers, or graphics-visualization workstations.
Reference: [Knu73] <author> D. E. Knuth. </author> <title> The Art of Computer Programming, Volume III: Searching and Sorting. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year> <note> ISBN 0-201-03803-X. </note>
Reference-contexts: The hash tables use double hashing as described in Knuth <ref> [Knu73, pp. 521-526] </ref> for efficiency. The sys lookup () and task lookup () routines search the tables for a particular task or system. The sys first (), sys next (), task first (), and task next () routines iterate over the tables, returning successive description vectors with each call.
Reference: [KP84] <author> D. Klappholz and H. C. Park. </author> <title> Parallelized process scheduling for a tightly-coupled MIMD machine. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 315-321. </pages> <publisher> IEEE, </publisher> <month> August </month> <year> 1984. </year>
Reference-contexts: t r g n o s u o o y v r e d c l b e x e s b e Hwang et al. [HCG + 82] Y Y Y Y N x N N MICROS [WV80] Y Y Y N N Y Y N Klappholz and Park <ref> [KP84] </ref> (DRS) Y N Y N N x Y N Reif and Spirakis [RS82] Y N Y N N x N N Ousterhout, et al. [OSS80] Y Y N N N x N N Bergmann and Jagadeesh [BJ91] Y N N N N x N N Drexl [Dre90] Y N N <p> MICROS uses hierarchical structuring and data summaries within a tree structured system. All scheduling takes place in a master/slave relationship, so autonomy is not supported. 2.4.2 Dynamic Non-cooperative Algorithms Klappholz and Park <ref> [KP84] </ref> describes Deliberate Random Scheduling (DRS) as a probabilistic, one-time assignment method to accomplish load balancing in heavily-loaded systems. Under DRS, when a task is spawned, a processor is randomly selected from the set of ready processors, and the task is assigned to the selected processor.
Reference: [KR90] <author> B. W. Kernighan and D. M. Ritchie. </author> <title> The C Programming Language. </title> <publisher> Prentice Hall, </publisher> <address> second edition, </address> <year> 1990. </year>
Reference-contexts: As an alternative to MIL, other interface languages could be implemented. PERL [WS90] and Python [vR92] are interpreted languages with efficient implementations. Both are intended for combining traditional imperative programming languages, such as C <ref> [KR90] </ref>, with command-line interpreters, such as the Bourne Shell [Bou]. Tcl [Ous93] is a command language that supplies a library of parsing functions that could be integrated into the MESSIAHS scheduling module in place of MIL.
Reference: [Lam78] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: Therefore, the scheduling mechanisms should communicate only necessary data to minimize their interference with the running of application programs. The scheduling module should also minimize the use of memory, disk and other shared resources. data soundness An ideal support mechanism supplies complete, perfectly accurate information to scheduling algorithms. Lamport <ref> [Lam78] </ref> discusses information dissemination latency in distributed systems, and shows that it is impossible to know the state of the entire distributed system instantaneously. The best that can be achieved is an estimate of the state at some point in the past.
Reference: [Lit87] <author> M. J. Litzkow. </author> <title> Remote UNIX: Turning Idle Workstations Into Cycle Servers. </title> <booktitle> In USENIX Summer Conference, </booktitle> <pages> pages 381-384, </pages> <address> 2560 Ninth Street, Suite 215, Berkeley, CA 94710, 1987. </address> <publisher> USENIX Association. </publisher>
Reference-contexts: Taken collectively, the aggregate computing resources are sufficient to solve difficult problems such as large-number factoring or climate simulation. When each machine is used in isolation, several limitations appear. Gantz, Silverman, and Stuart [GSS89] and Litzkow <ref> [Lit87] </ref> show that equipment in a workstation-based environment is underutilized, with processor utilization as low as 30%. <p> DRUMS does not provide extensible description mechanisms, and therefore does not follow the principle of generality. Remote UNIX, Condor, Butler, and Distributed Batch Remote Unix and its successor, Condor, were developed at the University of Wisconsin <ref> [Lit87, BLL92] </ref>. Butler was developed as part of the Andrew project 22 at Carnegie-Mellon University [Nic87], and Distributed Batch was developed at the MITRE Corporation [GSS89]. <p> c l b e x e s b e Blake [Bla92] (NS) Y N Y N C Y Y x (ABS) Y N Y N N N Y x (EBS) Y N Y N N N Y x Condor [BLL92] Y Y P N A x N N Remote Unix <ref> [Lit87] </ref> Y Y P N A x N N Butler [Nic87] Y Y P Y A x N N MITRE [GSS89] Y Y P P A x N N Casavant and Kuhl [CK84] Y Y Y N E x P N Ghafoor and Ahmad [GA90] Y Y Y N E Y <p> The first application demonstrates the task revocation facility as used by a general-purpose distributed batch system. The second application implements a load-balancing algorithm. 5.4.1 Distributed Batch The mitre distributed batch [GSS89], Condor [BLL92], and Remote Unix <ref> [Lit87] </ref> systems support general-purpose distributed processing for machines running the Unix operating system. Recall that Condor has some support for execution autonomy. In particular, Condor includes a limited policy expression mechanism, with predefined variables and functions, without the possibility of extending the system or task description.
Reference: [Lo88] <author> V. M. Lo. </author> <title> Heuristic Algorithms for Task Assignment in Distributed Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(11) </volume> <pages> 1384-1397, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: N N x N N Drexl [Dre90] Y N N Y N x x N Hochbaum and Shmoys [HS88] Y N N Y N x x N Hsu, et al. [HWK89] Y N N Y N x x N Stone [Sto77] Y N N Y N x x N Lo <ref> [Lo88] </ref> Y N N Y N x x N Price and Salama [PS90] Y N N Y N x x N Ramakrishnan et al. [RCD91] Y N N Y N x x N Sarkar [Sar89] Y N N Y N x x N Sarkar and Hennessey [SH86b] Y N N Y <p> The algorithm relates task assignment to commodity flows in networks, and shows that deriving a Max Flow/Min Cut provides an optimal mapping. Lo <ref> [Lo88] </ref> describes a method based on Stone's Max Flow/Min Cut algorithm for scheduling in heterogeneous systems. This method utilizes a set of heuristics to map from a general system representation to a two-processor system so that Stone's work applies.
Reference: [Lot84] <author> M. Lottor. </author> <title> Simple file transfer protocol. </title> <type> RFC 913, </type> <institution> Network Information Center, </institution> <month> September </month> <year> 1984. </year>
Reference-contexts: For the datagram service, an unreliable protocol such as the User Datagram Protocol (UDP) [Pos80a] is acceptable. Examples of reliable protocols include TCP [Pos80b] or a member of the File Transport Protocol (FTP) family <ref> [PR85, Sol92, Lot84] </ref>. The ISIS system implements levels of service ranging from unreliable messaging protocols at the lowest level to reliable multicast protocols [BJ87]. The choice of protocol depends on the critical characteristic of the channel.
Reference: [Mac93] <author> S. Macrakis. </author> <title> The structure of ANDF: Principles and examples. </title> <type> Technical report, </type> <institution> Open Software Foundation, </institution> <year> 1993. </year>
Reference-contexts: Essick [EI87], and Shub, et al. [DRS89, Shu90] devise multiple-architecture task representations. Both representations combine machine code for multiple architectures in a single program. External program representations, analogous to external data representations, have been proposed. The Open Software Foundation has proposed ANDF (Architecture Neutral Distribution Format <ref> [Mac93] </ref>) and an associated implementation technology, TDF (Ten15 Distribution Format [Pee92]), as standards for intermediate program representation for the OSF/1 operating system. UNCOL [Con58] is an earlier effort at such a standard. While each of these addresses some aspects of supporting architecture-independent program representation, none of them is wholly satisfactory.
Reference: [Mal93] <author> G. Malkin. </author> <note> RIP version 2-carrying additional information. RFC 1388, </note> <institution> Network Information Center, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Equation 4.17 employs the technique of a split horizon update found in network routing protocols <ref> [Hed88, Mal93] </ref>. 4.5.2 Proofs of General Rules This section proves rule 4.17 globally complete and correct, given that a spanning tree has been imposed on the graph.
Reference: [MG80] <author> S. Majumdar and M. L. Green. </author> <title> A distributed real time resource manager. </title> <booktitle> In Proceedings of the IEEE Symposium on Distributed Data Acquisition, Computing, and Control, </booktitle> <pages> pages 185-193, </pages> <year> 1980. </year>
Reference-contexts: v r e d c l b e x e s b e Stankovic and Sidhu [SS84] Y Y Y N EAC x P N Stankovic [Sta85b] Y N Y N N x N N Andrews et al. [ADD82] Y N Y x E x Y N Majumdar and Green <ref> [MG80] </ref> Y Y Y N N x N N Bonomi [Bon90] Y N N N N x N N Bonomi and Kumar [BK90] Y N N Y N x N N Greedy Load-Sharing [Cho90] Y N Y N N X Y N Gao, et al. [GLR84] (BAR) Y N Y N <p> Preferred server allocation asks for a server with a particular characteristic, but will take any server if none is available with the characteristic. Retentive server allocation asks for particular characteristics, and if no matching server is found, a server, busy or free, must fulfill the request. Majumdar and Green <ref> [MG80] </ref> discusses the Real Time Resource Manager, a load-balancing system running on multiple VAX 11/780 computers.
Reference: [MS70] <author> R. A. Meyer and L. H. Seawright. </author> <title> A Virtual Machine Time-Sharing System. </title> <journal> IBM Systems Journal, </journal> <volume> 9(3) </volume> <pages> 199-218, </pages> <year> 1970. </year>
Reference-contexts: At the lowest level of grouping, each virtual system typically consists of a subset of the capabilities of a single machine. In this way, virtual systems combine aspects of multicomputers [Spa86] and virtual machines (see <ref> [MS70, SM79] </ref>, which describe the IBM CP/67 and VM/370 operating systems). Virtual machines present the user with a subset of the capabilities of the physical machine. Multicomputers represent the capabilities of multiple machines as a single collected virtual computer.
Reference: [NA81] <author> L. M. Ni and K. Abani. </author> <title> Nonpreemptive load balancing in a class of local area networks. </title> <booktitle> In Proceedings of the Computer Networking Symposium, </booktitle> <pages> pages 113-118. </pages> <publisher> IEEE, </publisher> <month> December </month> <year> 1981. </year>
Reference-contexts: N Ghafoor and Ahmad [GA90] Y Y Y N E Y P N Stankovic [Sta81, Sta85a] Y N Y N N x N x Ramamritham and Stankovic [RS84] Y N Y N E x N x Wave Scheduling [VW84] Y Y Y N E x P N Ni and Abani <ref> [NA81] </ref> (LED) Y N Y N N x N N (SQ) Y N Y N N Y Y x 28 Table 2.3 Summary of distributed scheduling survey, part II M e t h o d P l c M c a i m i t i u e H t r <p> Van Tilborg and Wittie [VW84] presents Wave Scheduling for hierarchical virtual machines. The task force is recursively subdivided and the processing flows through the virtual machine like a wave, hence the name. Wave Scheduling combines a non-extensible mechanism with policy, and assumes the processors are homogeneous. Ni and Abani <ref> [NA81] </ref> presents two dynamic methods for load balancing on systems connected by local area networks: Least Expected Delay and Shortest Queue. Least Expected Delay assigns the task to the host with the smallest expected completion time, as estimated from data describing the task and the processors. <p> Shortest Queue assigns the task to the host with the fewest number of waiting jobs. These two methods are not scalable because they use information broadcasting to ensure complete 31 information at all nodes. <ref> [NA81] </ref> also presents an optimal stochastic strategy using mathematical programming. The method described in Stankovic and Sidhu [SS84] uses task clusters and distributed groups. Task clusters are sets of tasks with heavy inter-task communication that should be on the same host.
Reference: [Nic87] <author> D. A. Nichols. </author> <title> Using Idle Workstations in a Shared Computing Environment. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 5-12. </pages> <publisher> ACM, </publisher> <year> 1987. </year> <month> 132 </month>
Reference-contexts: Remote UNIX, Condor, Butler, and Distributed Batch Remote Unix and its successor, Condor, were developed at the University of Wisconsin [Lit87, BLL92]. Butler was developed as part of the Andrew project 22 at Carnegie-Mellon University <ref> [Nic87] </ref>, and Distributed Batch was developed at the MITRE Corporation [GSS89]. All of these systems attempt to increase utilization and share load across a set of Unix-based workstations, but are less complete systems than NetShare or Load Balancer. <p> [Bla92] (NS) Y N Y N C Y Y x (ABS) Y N Y N N N Y x (EBS) Y N Y N N N Y x Condor [BLL92] Y Y P N A x N N Remote Unix [Lit87] Y Y P N A x N N Butler <ref> [Nic87] </ref> Y Y P Y A x N N MITRE [GSS89] Y Y P P A x N N Casavant and Kuhl [CK84] Y Y Y N E x P N Ghafoor and Ahmad [GA90] Y Y Y N E Y P N Stankovic [Sta81, Sta85a] Y N Y N N
Reference: [Nil80] <author> N. J. </author> <title> Nilson. </title> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga Publishing Company, </publisher> <year> 1980. </year>
Reference-contexts: The third method, simulated annealing, starts with a mapping and uses probability-based functions to move towards an optimal mapping. Ramakrishnan, et al. [RCD91] presents a refinement of the A* algorithm 2 that can be used either to find optimal mappings or to find approximate mappings. The 2 See Nilson <ref> [Nil80, chapter 2] </ref>. 36 algorithm uses several heuristics based on the sum of communication costs for a task, the task's estimated mean processing cost, a combination of communication costs and mean processing cost, and the difference between the minimum and maximum processing costs for a task.
Reference: [OCD + 88] <author> J. K. Ousterhout, A. R. Cherenson, F. Douglis, M. N. Nelson, and B. B. Welch. </author> <title> The Sprite Network Operating System. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The numbering preserves that of the basic algorithm. Steps 2a through 2e indicate substeps of the transfer process. The DEMOS/MP approach is typical of migration mechanisms that attempt to optimize various aspects of the transfer step. Sprite The Sprite operating system achieves transparent process migration <ref> [OCD + 88, DO91] </ref>. Sprite uses the basic checkpoint-transfer-restart algorithm, but simplifies the transfer process because of the use of backing files. Instead of paging to local storage, Sprite pages to ordinary files in the network file system.
Reference: [OSS80] <author> J. K. Ousterhout, D. A. Scelza, and Pradeep S. Sindhu. </author> <title> Medusa: An experiment in distributed operating system structure. </title> <journal> Communications of the ACM, </journal> <volume> 23(2) </volume> <pages> 92-105, </pages> <month> February </month> <year> 1980. </year>
Reference-contexts: al. [HCG + 82] Y Y Y Y N x N N MICROS [WV80] Y Y Y N N Y Y N Klappholz and Park [KP84] (DRS) Y N Y N N x Y N Reif and Spirakis [RS82] Y N Y N N x N N Ousterhout, et al. <ref> [OSS80] </ref> Y Y N N N x N N Bergmann and Jagadeesh [BJ91] Y N N N N x N N Drexl [Dre90] Y N N Y N x x N Hochbaum and Shmoys [HS88] Y N N Y N x x N Hsu, et al. [HWK89] Y N N Y <p> The use of broadcast communication to keep all resource providers updated with the status of computations in progress limits the scalability of this algorithm. 34 2.4.3 Dynamic Non-distributed Algorithms Ousterhout, et al. <ref> [OSS80] </ref> describes Medusa, a distributed operating system for the Cm* multiprocessor. Medusa uses static assignment and centralized decision making it a combined policy and mechanism. It does not support autonomy, nor is the mechanism scalable.
Reference: [Ost93] <author> S. D. Ostermann. </author> <title> Reliable Messaging Protocols. </title> <type> Ph.D. Dissertation, </type> <institution> Pur-due University, </institution> <year> 1993. </year>
Reference-contexts: Methods of avoiding overestimation of system resources are discussed in chapter 4. 3.2.2.4 The Control Channel The control channel is intended to be a bidirectional, reliable, message-based channel, such as the Simple Reliable Message Protocol <ref> [Ost93] </ref> or the Reliable Datagram Protocol [PH90, VHS84]. A control message consists of a header, including an ID number for the message and a message type, and data that depends on the type of the message.
Reference: [Ous93] <author> J. K. Ousterhout. </author> <title> An Introduction to Tcl and Tk. </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: As an alternative to MIL, other interface languages could be implemented. PERL [WS90] and Python [vR92] are interpreted languages with efficient implementations. Both are intended for combining traditional imperative programming languages, such as C [KR90], with command-line interpreters, such as the Bourne Shell [Bou]. Tcl <ref> [Ous93] </ref> is a command language that supplies a library of parsing functions that could be integrated into the MESSIAHS scheduling module in place of MIL.
Reference: [Pee92] <author> Dr. N. E. Peeling. </author> <title> TDF specification, issue 2.0 revision 1. </title> <type> Technical report, </type> <institution> Defense Research Agency, </institution> <address> Worcestershire, United Kingdom, </address> <month> De-cember </month> <year> 1992. </year>
Reference-contexts: Both representations combine machine code for multiple architectures in a single program. External program representations, analogous to external data representations, have been proposed. The Open Software Foundation has proposed ANDF (Architecture Neutral Distribution Format [Mac93]) and an associated implementation technology, TDF (Ten15 Distribution Format <ref> [Pee92] </ref>), as standards for intermediate program representation for the OSF/1 operating system. UNCOL [Con58] is an earlier effort at such a standard. While each of these addresses some aspects of supporting architecture-independent program representation, none of them is wholly satisfactory.
Reference: [Pet88] <author> L. Peterson. </author> <title> The Profile Naming Service. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(4) </volume> <pages> 341-364, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: The RIS finds the matching set of hosts, and returns the set, sorted by five-minute load average. The syntax and use of the resource management mechanism is similar to that found in the Univers [BPY90] and Profile <ref> [Pet88] </ref> naming systems. The client uses the Task Management Subsystem (TMS) to schedule the individual tasks for execution. The TMS is composed of the Task Servers (TS) and the Client Side Task Library (CSTL). <p> Aggregate types such as records, arrays, or unions may be available but are not guaranteed to be present. The extension mechanism is similar in concept to the attribute-based descriptions of the Profile <ref> [Pet88] </ref> and Univers [BPY90] systems developed at the University of Arizona. We have simplified and tailored the concept to our more limited purposes. The mechanism is the same for extending both task and system description vectors, but the meaning of the fields is different.
Reference: [PH90] <author> C. Partridge and R. Hinden. </author> <title> Version 2 of the Reliable Data Protocol (RDP). </title> <type> RFC 1151, </type> <institution> Network Information Center, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: Methods of avoiding overestimation of system resources are discussed in chapter 4. 3.2.2.4 The Control Channel The control channel is intended to be a bidirectional, reliable, message-based channel, such as the Simple Reliable Message Protocol [Ost93] or the Reliable Datagram Protocol <ref> [PH90, VHS84] </ref>. A control message consists of a header, including an ID number for the message and a message type, and data that depends on the type of the message. The following defined control message types correspond to message events: request messages, reply messages, query messages, and status messages.
Reference: [PM83] <author> M. L. Powell and B. P. Miller. </author> <title> Process migration in DEMOS/MP. </title> <booktitle> In Proceedings of the 9th Symposium on Operating Systems Principles (Operating Systems Review), </booktitle> <pages> pages 110-119. </pages> <publisher> ACM SIGOPS, </publisher> <month> October </month> <year> 1983. </year>
Reference-contexts: There are several factors that complicate the mechanism. For example, open files and communication endpoints must be replicated on the destination system to achieve transparent migration. In general, any location-dependent aspects of the process impede migration. DEMOS/MP Powell and Miller <ref> [PM83] </ref> discusses process migration in the DEMOS/MP distributed operating system. DEMOS/MP is a message-based operating system, and all interactions between processes occur via communications-based system calls. DEMOS/MP splits the transfer step into six substeps, yielding an eight-step migration mechanism.
Reference: [Pos80a] <author> J. B. Postel. </author> <title> User Datagram Protocol. </title> <type> RFC 768, </type> <institution> Network Information Center, </institution> <month> August </month> <year> 1980. </year>
Reference-contexts: The lowest layer provides access to an datagram-oriented service for the advertisement of system state information, and a reliable protocol for task and data transfer. For the datagram service, an unreliable protocol such as the User Datagram Protocol (UDP) <ref> [Pos80a] </ref> is acceptable. Examples of reliable protocols include TCP [Pos80b] or a member of the File Transport Protocol (FTP) family [PR85, Sol92, Lot84]. The ISIS system implements levels of service ranging from unreliable messaging protocols at the lowest level to reliable multicast protocols [BJ87].
Reference: [Pos80b] <author> J.B. Postel. </author> <title> DoD standard Transmission Control Protocol. </title> <type> RFC 761, </type> <institution> Network Information Center, </institution> <month> January </month> <year> 1980. </year>
Reference-contexts: The lowest layer provides access to an datagram-oriented service for the advertisement of system state information, and a reliable protocol for task and data transfer. For the datagram service, an unreliable protocol such as the User Datagram Protocol (UDP) [Pos80a] is acceptable. Examples of reliable protocols include TCP <ref> [Pos80b] </ref> or a member of the File Transport Protocol (FTP) family [PR85, Sol92, Lot84]. The ISIS system implements levels of service ranging from unreliable messaging protocols at the lowest level to reliable multicast protocols [BJ87]. The choice of protocol depends on the critical characteristic of the channel.
Reference: [PR85] <author> J. B. Postel and J. K. Reynolds. </author> <title> File transfer protocol. </title> <type> RFC 959, </type> <institution> Network Information Center, </institution> <month> October </month> <year> 1985. </year>
Reference-contexts: For the datagram service, an unreliable protocol such as the User Datagram Protocol (UDP) [Pos80a] is acceptable. Examples of reliable protocols include TCP [Pos80b] or a member of the File Transport Protocol (FTP) family <ref> [PR85, Sol92, Lot84] </ref>. The ISIS system implements levels of service ranging from unreliable messaging protocols at the lowest level to reliable multicast protocols [BJ87]. The choice of protocol depends on the critical characteristic of the channel.
Reference: [PS90] <author> C. C. Price and M. A. Salama. </author> <title> Scheduling of Precedence-Constrained Tasks on Multiprocessors. </title> <journal> Computer Journal, </journal> <volume> 33(3) </volume> <pages> 219-229, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: x x N Hochbaum and Shmoys [HS88] Y N N Y N x x N Hsu, et al. [HWK89] Y N N Y N x x N Stone [Sto77] Y N N Y N x x N Lo [Lo88] Y N N Y N x x N Price and Salama <ref> [PS90] </ref> Y N N Y N x x N Ramakrishnan et al. [RCD91] Y N N Y N x x N Sarkar [Sar89] Y N N Y N x x N Sarkar and Hennessey [SH86b] Y N N Y N x x N 30 Casavant and Kuhl [CK84] describes a distributed <p> Lo [Lo88] describes a method based on Stone's Max Flow/Min Cut algorithm for scheduling in heterogeneous systems. This method utilizes a set of heuristics to map from a general system representation to a two-processor system so that Stone's work applies. Price and Salama <ref> [PS90] </ref> describes three heuristics for assigning precedence-constrained tasks to a network of identical processors. With the first heuristic, the tasks are sorted in increasing order of communication, and then are iteratively assigned so as to minimize total communication time.
Reference: [Puc88] <author> M. F. Pucci. </author> <title> Design Considerations for Process Migration and Automatic Load Balancing. </title> <type> Technical report, </type> <institution> Bell Communications Research, </institution> <year> 1988. </year>
Reference-contexts: in the revocation filter rule (line 10) matches any available task, and the value portion of the rule assigns an equal priority to all tasks under consideration. 5.4.2 Load Balancing Several researchers have investigated load balancing and sharing policies for distributed systems, such as those described in [Cho90], [ELZ85], and <ref> [Puc88] </ref>. The Greedy Load-Sharing Algorithm [Cho90], makes decisions based on a local optimum. When a user submits a task for execution, the receiving system attempts to place the task with a less busy neighbor, according to a weighting function.
Reference: [PW85] <author> G. Popek and B. Walker, </author> <title> editors. The Locus Distributed System Architecture. </title> <publisher> The MIT Press, </publisher> <year> 1985. </year> <month> 133 </month>
Reference-contexts: Locus, Charlotte, and work by Bryant and Finkel Locus uses the basic checkpoint-transfer-restart algorithm, with the optimization that read-only segments that already exist on the target machine are not copied <ref> [PW85] </ref>. The Charlotte distributed operating system uses the basic algorithm, with the addition of message endpoint forwarding [AF87, FA89]. Bryant and Finkel [BF81] concentrates on developing stable process migration methods.
Reference: [RCD91] <author> S. Ramakrishnan, I. H. Cho, and L. Dunning. </author> <title> A Close Look at Task Assignment in Distributed Systems. </title> <booktitle> In INFOCOM '91, </booktitle> <pages> pages 806-812, </pages> <address> Miami, FL, </address> <month> April </month> <year> 1991. </year> <note> IEEE. </note>
Reference-contexts: x x N Hsu, et al. [HWK89] Y N N Y N x x N Stone [Sto77] Y N N Y N x x N Lo [Lo88] Y N N Y N x x N Price and Salama [PS90] Y N N Y N x x N Ramakrishnan et al. <ref> [RCD91] </ref> Y N N Y N x x N Sarkar [Sar89] Y N N Y N x x N Sarkar and Hennessey [SH86b] Y N N Y N x x N 30 Casavant and Kuhl [CK84] describes a distributed task execution environment for UNIX System 7, with the primary goal of <p> The second heuristic creates pairs of tasks that communicate, sorts the pairs in decreasing order of communication, then groups the pairs into clusters. The third method, simulated annealing, starts with a mapping and uses probability-based functions to move towards an optimal mapping. Ramakrishnan, et al. <ref> [RCD91] </ref> presents a refinement of the A* algorithm 2 that can be used either to find optimal mappings or to find approximate mappings.
Reference: [RS82] <author> J. Reif and P. Spirakis. </author> <title> Real time resrouce allocation in distributed systems. </title> <booktitle> In Proceedings of the Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 84-94. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1982. </year>
Reference-contexts: d c l b e x e s b e Hwang et al. [HCG + 82] Y Y Y Y N x N N MICROS [WV80] Y Y Y N N Y Y N Klappholz and Park [KP84] (DRS) Y N Y N N x Y N Reif and Spirakis <ref> [RS82] </ref> Y N Y N N x N N Ousterhout, et al. [OSS80] Y Y N N N x N N Bergmann and Jagadeesh [BJ91] Y N N N N x N N Drexl [Dre90] Y N N Y N x x N Hochbaum and Shmoys [HS88] Y N N Y <p> DRS dictates a priority scheme for time-slicing, and is thus a mixture of local and global scheduling. There is no administrative autonomy or execution autonomy with this system, because DRS is intended for tightly-coupled machines. Reif and Spirakis <ref> [RS82] </ref> presents a Resource Granting System (RGS) based on probabilities and using broadcast communication. This work assumes the existence of either an underlying handshaking mechanism or of shared variables to negotiate task placement.
Reference: [RS84] <author> K. Ramamritham and J. A. Stankovic. </author> <title> Dynamic task scheduling in distributed hard real-time systems. </title> <booktitle> In Proceedings of the International Conference on Distributed Computing Systems, </booktitle> <pages> pages 96-107. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1984. </year>
Reference-contexts: N N MITRE [GSS89] Y Y P P A x N N Casavant and Kuhl [CK84] Y Y Y N E x P N Ghafoor and Ahmad [GA90] Y Y Y N E Y P N Stankovic [Sta81, Sta85a] Y N Y N N x N x Ramamritham and Stankovic <ref> [RS84] </ref> Y N Y N E x N x Wave Scheduling [VW84] Y Y Y N E x P N Ni and Abani [NA81] (LED) Y N Y N N x N N (SQ) Y N Y N N Y Y x 28 Table 2.3 Summary of distributed scheduling survey, part <p> Stankovic [Sta81, Sta85a] describe methods for homogeneous systems based on Bayesian decision theory. There is no support for autonomy, nor are the methods scalable because they require full knowledge of all nodes in the system. Ramamritham and Stankovic <ref> [RS84] </ref> presents a distributed scheduling algorithm for hard real-time systems. This work supports a form of execution autonomy that guarantees a hard real-time deadline. A node can choose to accept a task and guarantee its completion by a deadline, or to decline the task.
Reference: [Sar89] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Y N x x N Stone [Sto77] Y N N Y N x x N Lo [Lo88] Y N N Y N x x N Price and Salama [PS90] Y N N Y N x x N Ramakrishnan et al. [RCD91] Y N N Y N x x N Sarkar <ref> [Sar89] </ref> Y N N Y N x x N Sarkar and Hennessey [SH86b] Y N N Y N x x N 30 Casavant and Kuhl [CK84] describes a distributed task execution environment for UNIX System 7, with the primary goal of load balancing without altering the user interface to the operating <p> The algorithm also uses *-relaxation similar to the dual-approximation algorithm of Hochbaum and Shmoys [HS88]. Sarkar <ref> [Sar89] </ref> and Sarkar and Hennessey [SH86b] describe the GR graph representation and static partitioning and scheduling algorithms for single-assignment programs based on the SISAL language. In GR, nodes represent tasks and edges represent communication. The algorithm consists of four steps: cost assignment, graph expansion, internalization, and processor assignment.
Reference: [SH86a] <author> V. Sarkar and J. Hennessy. </author> <title> Partitioning Parallel Programs for Macro-Dataflow. </title> <booktitle> In ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 202-211, </pages> <month> August </month> <year> 1986. </year>
Reference: [SH86b] <author> Vivek Sarkar and John Hennessy. </author> <title> Compile-time Partitioning and Scheduling of Parallel Programs. </title> <journal> SIGPLAN Notices, </journal> <volume> 21(7) </volume> <pages> 17-26, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: x x N Lo [Lo88] Y N N Y N x x N Price and Salama [PS90] Y N N Y N x x N Ramakrishnan et al. [RCD91] Y N N Y N x x N Sarkar [Sar89] Y N N Y N x x N Sarkar and Hennessey <ref> [SH86b] </ref> Y N N Y N x x N 30 Casavant and Kuhl [CK84] describes a distributed task execution environment for UNIX System 7, with the primary goal of load balancing without altering the user interface to the operating system. As such, the system combines mechanism and policy. <p> The algorithm also uses *-relaxation similar to the dual-approximation algorithm of Hochbaum and Shmoys [HS88]. Sarkar [Sar89] and Sarkar and Hennessey <ref> [SH86b] </ref> describe the GR graph representation and static partitioning and scheduling algorithms for single-assignment programs based on the SISAL language. In GR, nodes represent tasks and edges represent communication. The algorithm consists of four steps: cost assignment, graph expansion, internalization, and processor assignment.
Reference: [Shu90] <author> C. M. Shub. </author> <title> Native Code Process-Originated Migration in a Heterogeneous Environment. </title> <booktitle> In Proceedings of the Computer Science Conference. ACM, </booktitle> <year> 1990. </year>
Reference-contexts: Machines that share a common object code format and instruction set can share object files without translation. Various attempts have been made that have resulted in O (n 2 ) solutions to the problem. Essick [EI87], and Shub, et al. <ref> [DRS89, Shu90] </ref> devise multiple-architecture task representations. Both representations combine machine code for multiple architectures in a single program. External program representations, analogous to external data representations, have been proposed.
Reference: [SM79] <author> L. H. Seawright and R. A. MacKinnon. </author> <title> VM/370|A Study of Multiplicity and Usefulness. </title> <journal> IBM Systems Journal, </journal> <volume> 18(1) </volume> <pages> 4-17, </pages> <year> 1979. </year>
Reference-contexts: At the lowest level of grouping, each virtual system typically consists of a subset of the capabilities of a single machine. In this way, virtual systems combine aspects of multicomputers [Spa86] and virtual machines (see <ref> [MS70, SM79] </ref>, which describe the IBM CP/67 and VM/370 operating systems). Virtual machines present the user with a subset of the capabilities of the physical machine. Multicomputers represent the capabilities of multiple machines as a single collected virtual computer.
Reference: [Sol92] <author> K. Sollins. </author> <title> The TFTP protocol (revision 2). </title> <type> RFC 1350, </type> <institution> Network Information Center, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: For the datagram service, an unreliable protocol such as the User Datagram Protocol (UDP) [Pos80a] is acceptable. Examples of reliable protocols include TCP [Pos80b] or a member of the File Transport Protocol (FTP) family <ref> [PR85, Sol92, Lot84] </ref>. The ISIS system implements levels of service ranging from unreliable messaging protocols at the lowest level to reliable multicast protocols [BJ87]. The choice of protocol depends on the critical characteristic of the channel.
Reference: [Spa86] <author> E. H. Spafford. </author> <title> Kernel Structures for a Distributed Operating System. </title> <type> PhD thesis, </type> <institution> Georgia Institute of Technology, </institution> <year> 1986. </year>
Reference-contexts: Within the University, National Lab, and Industry virtual systems are other virtual systems, giving a hierarchical structure. At the lowest level of grouping, each virtual system typically consists of a subset of the capabilities of a single machine. In this way, virtual systems combine aspects of multicomputers <ref> [Spa86] </ref> and virtual machines (see [MS70, SM79], which describe the IBM CP/67 and VM/370 operating systems). Virtual machines present the user with a subset of the capabilities of the physical machine. Multicomputers represent the capabilities of multiple machines as a single collected virtual computer.
Reference: [SS84] <author> J. A. Stankovic and I. S. Sidhu. </author> <title> An adaptive bidding algorithm for processes, clusters and distributed groups. </title> <booktitle> In Proceedings of the International Conference on Distributed Computing Systems, </booktitle> <pages> pages 49-59. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1984. </year>
Reference-contexts: scheduling survey, part II M e t h o d P l c M c a i m i t i u e H t r g n o s u o o y v r e d c l b e x e s b e Stankovic and Sidhu <ref> [SS84] </ref> Y Y Y N EAC x P N Stankovic [Sta85b] Y N Y N N x N N Andrews et al. [ADD82] Y N Y x E x Y N Majumdar and Green [MG80] Y Y Y N N x N N Bonomi [Bon90] Y N N N N x <p> These two methods are not scalable because they use information broadcasting to ensure complete 31 information at all nodes. [NA81] also presents an optimal stochastic strategy using mathematical programming. The method described in Stankovic and Sidhu <ref> [SS84] </ref> uses task clusters and distributed groups. Task clusters are sets of tasks with heavy inter-task communication that should be on the same host. Distributed groups also have inter-task communication, but execute faster when spread across separate hosts. <p> This method uses a system of rewards and penalties as a feedback mechanism to tune the policy. The second method uses bidding and one-time assignment in a real-time environment, similar to that in <ref> [SS84] </ref>. Andrews, et al. [ADD82] describes a bidding method with dynamic reassignment based on three types of servers: free, preferred, and retentive. Free server allocation will choose any available server from an identical pool.
Reference: [Sta81] <author> J. A. Stankovic. </author> <title> The analysis of a decentralized control algorithm for job scheduling utilizing bayesian decision theory. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 333-340. </pages> <publisher> IEEE, </publisher> <year> 1981. </year>
Reference-contexts: N A x N N Butler [Nic87] Y Y P Y A x N N MITRE [GSS89] Y Y P P A x N N Casavant and Kuhl [CK84] Y Y Y N E x P N Ghafoor and Ahmad [GA90] Y Y Y N E Y P N Stankovic <ref> [Sta81, Sta85a] </ref> Y N Y N N x N x Ramamritham and Stankovic [RS84] Y N Y N E x N x Wave Scheduling [VW84] Y Y Y N E x P N Ni and Abani [NA81] (LED) Y N Y N N x N N (SQ) Y N Y N <p> The system passes a task between nodes until either a node accepts the task or the task reaches its transfer limit, in which case the current node accepts the task. This algorithm assumes homogeneous processors and has limited support for execution autonomy. Stankovic <ref> [Sta81, Sta85a] </ref> describe methods for homogeneous systems based on Bayesian decision theory. There is no support for autonomy, nor are the methods scalable because they require full knowledge of all nodes in the system. Ramamritham and Stankovic [RS84] presents a distributed scheduling algorithm for hard real-time systems.
Reference: [Sta84] <author> J. A. Stankovic. </author> <title> Simulations of three adaptive, decentralized controlled, job scheduling algorithms. </title> <booktitle> Computer Networks, </booktitle> <address> 8(3):199|217, </address> <month> June </month> <year> 1984. </year> <month> 134 </month>
Reference-contexts: x N N Bonomi [Bon90] Y N N N N x N N Bonomi and Kumar [BK90] Y N N Y N x N N Greedy Load-Sharing [Cho90] Y N Y N N X Y N Gao, et al. [GLR84] (BAR) Y N Y N N x N N Stankovic <ref> [Sta84] </ref> Y N Y N N x P N Chou and Abraham [CA83] Y N Y N N x Y N Bryant and Finkel [BF81] Y N Y N N x Y N Chow and Kohler [CK79] Y N N Y N x N N Casey [Cas81] (dipstick) Y N Y <p> Gao, et al. [GLR84] describes two load-balancing algorithms using broadcast information. The first algorithm balances arrival rates, with the assumption that all jobs take the same time. The second algorithm balances unfinished work. Stankovic <ref> [Sta84] </ref> gives three variants of load-balancing algorithms based on point-to-point communication that compare the local load to the load on remote processors. Chou and Abraham [CA83] describes a class of load-redistribution algorithms for processor-failure recovery in distributed systems.
Reference: [Sta85a] <author> J. A. Stankovic. </author> <title> An application of bayesian decision theory to decentralized control of job scheduling. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(2):117-130, </volume> <month> February </month> <year> 1985. </year>
Reference-contexts: N A x N N Butler [Nic87] Y Y P Y A x N N MITRE [GSS89] Y Y P P A x N N Casavant and Kuhl [CK84] Y Y Y N E x P N Ghafoor and Ahmad [GA90] Y Y Y N E Y P N Stankovic <ref> [Sta81, Sta85a] </ref> Y N Y N N x N x Ramamritham and Stankovic [RS84] Y N Y N E x N x Wave Scheduling [VW84] Y Y Y N E x P N Ni and Abani [NA81] (LED) Y N Y N N x N N (SQ) Y N Y N <p> The system passes a task between nodes until either a node accepts the task or the task reaches its transfer limit, in which case the current node accepts the task. This algorithm assumes homogeneous processors and has limited support for execution autonomy. Stankovic <ref> [Sta81, Sta85a] </ref> describe methods for homogeneous systems based on Bayesian decision theory. There is no support for autonomy, nor are the methods scalable because they require full knowledge of all nodes in the system. Ramamritham and Stankovic [RS84] presents a distributed scheduling algorithm for hard real-time systems.
Reference: [Sta85b] <author> J. A. Stankovic. </author> <title> Stability and distributed scheduling algorithms. </title> <booktitle> In Proceedings of the 1985 ACM Computer Science Conference, </booktitle> <pages> pages 47-57. </pages> <publisher> ACM, </publisher> <month> March </month> <year> 1985. </year>
Reference-contexts: P l c M c a i m i t i u e H t r g n o s u o o y v r e d c l b e x e s b e Stankovic and Sidhu [SS84] Y Y Y N EAC x P N Stankovic <ref> [Sta85b] </ref> Y N Y N N x N N Andrews et al. [ADD82] Y N Y x E x Y N Majumdar and Green [MG80] Y Y Y N N x N N Bonomi [Bon90] Y N N N N x N N Bonomi and Kumar [BK90] Y N N Y <p> Task clusters are sets of tasks with heavy inter-task communication that should be on the same host. Distributed groups also have inter-task communication, but execute faster when spread across separate hosts. This method is a bidding strategy, and uses non-extensible system and task description messages. Stankovic <ref> [Sta85b] </ref> lists two scheduling methods. The first is adaptive with dynamic reassignment, and is based on broadcast messages and stochastic learning automata. This method uses a system of rewards and penalties as a feedback mechanism to tune the policy.
Reference: [Staly] <institution> Standard Performance Evaluation Corporation. </institution> <note> The SPEC Newsletter, published quarterly. </note>
Reference-contexts: Node state declarations are parameters that affect system state. The four node state parameters are specint92, specfp92, recalc timeout, and revocation timeout. The specint92 and specfp92 parameters list the speed of the host in terms of the SPEC benchmarks <ref> [Staly] </ref>. The recalc timeout and revocation timeout parameters determine the timeout periods for the associated events. 5.3.3 Data Combination and Filters MIL provides a mechanism to combine description vectors.
Reference: [Sto77] <author> H. S. Stone. </author> <title> Multiprocessor Scheduling with the Aid of Network Flow Algorithms. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-3(1):85-93, </volume> <month> January </month> <year> 1977. </year>
Reference-contexts: x N N Bergmann and Jagadeesh [BJ91] Y N N N N x N N Drexl [Dre90] Y N N Y N x x N Hochbaum and Shmoys [HS88] Y N N Y N x x N Hsu, et al. [HWK89] Y N N Y N x x N Stone <ref> [Sto77] </ref> Y N N Y N x x N Lo [Lo88] Y N N Y N x x N Price and Salama [PS90] Y N N Y N x x N Ramakrishnan et al. [RCD91] Y N N Y N x x N Sarkar [Sar89] Y N N Y N x <p> The mapping is derived through an enumerative state space search with pruning, which results in an underestimate of the running time for a partially mapped computation, and hence, the name critical sink underestimate. Stone <ref> [Sto77] </ref> describes a method for optimal assignment on a two-processor system based on a Max Flow/Min Cut algorithm for sources and sinks in a weighted directed graph. A maximum flow is one that moves the maximum quantity of goods along the edges from sources to sinks.
Reference: [Sto93] <author> H. S. Stone. </author> <title> High-Performance Computer Architecture. </title> <publisher> Addison-Wesley, </publisher> <address> third edition, </address> <year> 1993. </year> <note> ISBN 0201526883. </note>
Reference-contexts: The granularity of a task, as defined in <ref> [Sto93] </ref>, is the ratio of a task's computation time (R) to its communication time (C). Coarse-grained tasks have a high value of R=C, and include CPU-bound programs such as traditional scientific computations. Medium-grained tasks include text processing and program compilation. Chapter 6 lists experimental results that validate these assumptions.
Reference: [Stu88] <author> M. Stumm. </author> <title> The Design and Implementation of a Decentralized Scheduling Facility for a Workstation Cluster. </title> <booktitle> In Proceedings of the 2nd IEEE Conference on Computer Workstations, </booktitle> <pages> pages 12-22. </pages> <publisher> IEEE, </publisher> <month> March </month> <year> 1988. </year>
Reference-contexts: Centralization of information precludes scalability (see <ref> [Stu88] </ref>), and should therefore be avoided. non-interference High monitoring overhead and message traffic can adversely affect performance within the system. Therefore, the scheduling mechanisms should communicate only necessary data to minimize their interference with the running of application programs.
Reference: [Sun87] <author> XDR: </author> <title> External Data Representation Standard. </title> <publisher> Sun Microsystems Inc., </publisher> <month> June </month> <year> 1987. </year> <note> RFC 1014. </note>
Reference-contexts: Explicit encoding embeds type information in the data stream, and a host with no prior knowledge of the data structure can interpret the data. The XDR (eXternal Data Representation) standard <ref> [Sun87] </ref> specifies an implicit encoding for data types, which means that no type information is embedded in the data stream.
Reference: [TLC85] <author> M. M. Theimer, K. A. Lantz, and D. R. Cheriton. </author> <title> Preemptable Remote Execution Facilities for the V-System. </title> <booktitle> In Proceedings of the Tenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 2-12, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: The V System The V system uses a technique called precopying, wherein the memory is copied while the process continues to execute <ref> [TLC85] </ref>. After the memory is precopied, the process is stopped and any altered pages are copied again. This reduces the amount of time a process is frozen. 15 Accent The Accent operating system uses a lazy approach to transfer [Zay87].
Reference: [VHS84] <author> D. Velten, R. Hinden, and J. Sax. </author> <title> Reliable Data Protocol. </title> <type> RFC 908, </type> <institution> Network Information Center, </institution> <month> July </month> <year> 1984. </year>
Reference-contexts: Methods of avoiding overestimation of system resources are discussed in chapter 4. 3.2.2.4 The Control Channel The control channel is intended to be a bidirectional, reliable, message-based channel, such as the Simple Reliable Message Protocol [Ost93] or the Reliable Datagram Protocol <ref> [PH90, VHS84] </ref>. A control message consists of a header, including an ID number for the message and a message type, and data that depends on the type of the message. The following defined control message types correspond to message events: request messages, reply messages, query messages, and status messages.
Reference: [vR92] <author> G. van Rossum. </author> <title> Python Reference Manual. </title> <institution> Dept. CST, CWI, </institution> <address> 1098 SJ Amsterdam, The Netherlands, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: In addition, substantial performance advantages could result from tuning of 123 the interpreter to eliminate the excessive symbol table manipulation present in the current version. As an alternative to MIL, other interface languages could be implemented. PERL [WS90] and Python <ref> [vR92] </ref> are interpreted languages with efficient implementations. Both are intended for combining traditional imperative programming languages, such as C [KR90], with command-line interpreters, such as the Bourne Shell [Bou].
Reference: [VW84] <author> A. M. Van Tilborg and L. D. Wittie. </author> <title> Wave scheduling|decentralized scheduling of task forces in multicomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-33(9):835-844, </volume> <month> September </month> <year> 1984. </year>
Reference-contexts: For simplicity of description, this dissertation restricts further discussions to the conventional model of placing computational tasks on processors. A task force (as defined in <ref> [VW84] </ref>) comprises a group of cooperating tasks for solving a single problem. Within a distributed system, there are two levels of task scheduling: the association of tasks with processors (global scheduling), and the choice of which task to execute among those available on a processor (local scheduling) [CK88]. <p> N Casavant and Kuhl [CK84] Y Y Y N E x P N Ghafoor and Ahmad [GA90] Y Y Y N E Y P N Stankovic [Sta81, Sta85a] Y N Y N N x N x Ramamritham and Stankovic [RS84] Y N Y N E x N x Wave Scheduling <ref> [VW84] </ref> Y Y Y N E x P N Ni and Abani [NA81] (LED) Y N Y N N x N N (SQ) Y N Y N N Y Y x 28 Table 2.3 Summary of distributed scheduling survey, part II M e t h o d P l c M <p> This work supports a form of execution autonomy that guarantees a hard real-time deadline. A node can choose to accept a task and guarantee its completion by a deadline, or to decline the task. Van Tilborg and Wittie <ref> [VW84] </ref> presents Wave Scheduling for hierarchical virtual machines. The task force is recursively subdivided and the processing flows through the virtual machine like a wave, hence the name. Wave Scheduling combines a non-extensible mechanism with policy, and assumes the processors are homogeneous.
Reference: [WM85] <author> Y. T. Wang and R. J. T. Morris. </author> <title> Load sharing in distributed systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(3):204-217, </volume> <month> March </month> <year> 1985. </year>
Reference-contexts: Under sender-initiated load balancing, the busy processor finds an idle processor to receive a task. With receiver-initiated load balancing, an idle processor locates an overloaded processor and requests a task. Wang and Morris <ref> [WM85] </ref> presents a similar taxonomy using the names source-initiated for sender-initiated and server-initiated for receiver-initiated load balancing. Probabilistic algorithms operate in one of two methods. The first method makes scheduling choices based on statistics rather than exact information.
Reference: [WS90] <author> L. Wall and R. L. Schwarz. </author> <title> Programming perl. </title> <publisher> O'Reilly & Associates, </publisher> <year> 1990. </year> <note> ISBN 0-93717564-1. </note>
Reference-contexts: In addition, substantial performance advantages could result from tuning of 123 the interpreter to eliminate the excessive symbol table manipulation present in the current version. As an alternative to MIL, other interface languages could be implemented. PERL <ref> [WS90] </ref> and Python [vR92] are interpreted languages with efficient implementations. Both are intended for combining traditional imperative programming languages, such as C [KR90], with command-line interpreters, such as the Bourne Shell [Bou].
Reference: [WV80] <author> L. D. Wittie and A. M. Van Tilborg. MICROS, </author> <title> a distributed operating system for MICRONET, a reconfigurable network computer. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(12):1133-1144, </volume> <month> December </month> <year> 1980. </year> <month> 135 </month>
Reference-contexts: This bookkeeping could quickly consume the processing power of the system, and little or 9 no productive work would be accomplished (see <ref> [WV80, FR90] </ref>). Again, there is a tradeoff between the accuracy of a system description and its size. A solution is to compress multiple system descriptions into one, thus saving space while preserving much of the descriptive information. <p> c M c a i m i t i u e H t r g n o s u o o y v r e d c l b e x e s b e Hwang et al. [HCG + 82] Y Y Y Y N x N N MICROS <ref> [WV80] </ref> Y Y Y N N Y Y N Klappholz and Park [KP84] (DRS) Y N Y N N x Y N Reif and Spirakis [RS82] Y N Y N N x N N Ousterhout, et al. [OSS80] Y Y N N N x N N Bergmann and Jagadeesh [BJ91] Y <p> Hwang, et al. [HCG + 82] describes a specialized implementation of a distributed Unix project for a network of Digital Equipment Corporation machines, and an associated load-balancing strategy. This project supports neither autonomy nor scalability. Wittie and Van Tilborg <ref> [WV80] </ref> describes MICROS and MICRONET. MICROS is the load-balancing operating system for MICRONET, which is a reconfigurable and extensible network of 16 LSI-11 nodes. MICROS uses hierarchical structuring and data summaries within a tree structured system. <p> Under Distributed Hierarchical Control, the system uses a hierarchically-structured multiprocessor as the master processing element in a larger multiprocessor. In this system, lower levels in the control tree pass condensed and abstracted description information up to higher levels, where scheduling decisions are made (see also <ref> [WV80] </ref>). 3.1.1 Representation and System Structure Directed acyclic graphs (DAGs) can represent virtual systems. The graph for figure 3.1 is in figure 3.2. Each vertex, or node, within the graph marks the root of an administrative hierarchy, and appears as a virtual system to nodes outside that administrative domain.
Reference: [WW90] <author> C. M. Wang and S. D. Wang. </author> <title> Structured Partitioning of Concurrent Programs for Execution on Multiprocessors. </title> <journal> Parallel Computing, </journal> <volume> 16 </volume> <pages> 41-57, </pages> <year> 1990. </year>

References-found: 118

