URL: http://www.cse.unsw.edu.au/~achim/PostScripts/Hoffmann-ECAI94.ps
Refering-URL: http://www.cse.unsw.edu.au/~achim/index.html
Root-URL: http://www.cse.unsw.edu.au
Email: Email: achim@cs.unsw.oz.au  
Title: Exploiting Causal Domain Knowledge for Learning to Control Dynamic Systems  
Author: Achim G. Hoffmann 
Address: P.O.Box 1, Sydney 2052, Australia,  
Affiliation: School of Computer Science and Engineering, University of New South Wales  
Abstract: This paper introduces a simple yet effective method for using causal domain knowledge for learning to control dynamic systems. Elementary qualitative causal dependencies of the domain are exploited in order to dramatically speed up the learning of reliable control strategies from a simulation model of the system. The reliability of the obtained control strategies is strengthened as well. The effectiveness of the method has experimentally been studied at the problem of learning to balance a pole. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. W. Anderson. </author> <title> Strategy learning with multilayer connectionist representations. </title> <booktitle> In Proceedings of the 4 th International Conference on Machine Learning, </booktitle> <pages> pages 103-114. </pages> <publisher> Mor-gan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: An example is the classical pole balancing problem. To overcome this shortcoming, different methods for coping with nonlinear systems have been proposed. Among them are Fuzzy Control approaches, e.g. [4], symbolic Machine Learning approaches, e.g. [5] as well as Neural Networks <ref> [1] </ref>, and Genetic Algorithms [11]. All these approaches face the problem of credit assignment: A control strategy can only be evaluated as a whole. If a strategy proves unsatisfactory, it is not clear how to alter the strategy to obtain improved performance. <p> All these approaches face the problem of credit assignment: A control strategy can only be evaluated as a whole. If a strategy proves unsatisfactory, it is not clear how to alter the strategy to obtain improved performance. Although systems using various approaches have been designed, e.g. <ref> [5, 7, 1, 9] </ref>, which manage to learn to balance a pole, it is unclear how well these approaches work for other and in particular for more complex systems. The mentioned learning approaches basically consider the system to be controlled as a black box. <p> Upon that data, the tested control strategy is altered in order to generate the next candidate being tested. This idea has been implemented in a number of systems, e.g. in <ref> [3, 1] </ref> and many others. BOXES [5] is one of the very early approaches of that kind. BOXES learns a state-action table which specifies a control action for each possible system state. For that purpose, the possible system states are discretized. <p> An experimental case study The new approach has been tested on the problem of learning to balance a pole. See figure 1. The system being controlled is described by the four system parameters x, _x, , and _ . The simulator used the equations as in Anderson <ref> [1] </ref>. <p> But this number may possibly reduce considerably by improving the use of the genetic algorithm. The results using Neural Networks reported in <ref> [1] </ref> show very slow learning behavior, which is not competitive in terms of learning speed. The work on Fuzzy Control in [4] seems to deal with pole balancing on an unbounded track which is a much easier task.
Reference: [2] <author> I. Bratko. </author> <title> Deriving qualitative control for dynamic systems. </title> <editor> In K. Furukawa and S. Muggleton, editors, </editor> <booktitle> Machine Intelligence and Inductive Learning. </booktitle> <institution> Oxford University Press. (new series of Machine Intelligence), </institution> <note> to appear. </note>
Reference-contexts: This paper proposes to use coarse qualitative knowledge about the effect of control actions on the system being controlled to substantially improve the performance of learning. Recently, the problem of forming the class of considered control strategies by qualitative knowledge has been addressed, e.g. <ref> [10, 2, 11] </ref>. In these investigations, not exclusively but mainly qualitative models about the system to be controlled were used. In the following, qualitative knowledge about possible control strategies is exploited without stating anything about the physics of the system.
Reference: [3] <author> M. E. Connel and P. E. Utgoff. </author> <title> Learning to control a dynamic physical system. </title> <booktitle> In Proceeedings AAAI 87, </booktitle> <pages> pages 456-459, </pages> <year> 1987. </year>
Reference-contexts: Upon that data, the tested control strategy is altered in order to generate the next candidate being tested. This idea has been implemented in a number of systems, e.g. in <ref> [3, 1] </ref> and many others. BOXES [5] is one of the very early approaches of that kind. BOXES learns a state-action table which specifies a control action for each possible system state. For that purpose, the possible system states are discretized.
Reference: [4] <author> C. C. Lee. </author> <title> A self-learning rule-based controller employing approximate reasoning and neural net concepts. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 6 </volume> <pages> 71-93, </pages> <year> 1991. </year>
Reference-contexts: This limitation is perceived as an important issue because many practical systems are nonlinear when described in a `natural way'. An example is the classical pole balancing problem. To overcome this shortcoming, different methods for coping with nonlinear systems have been proposed. Among them are Fuzzy Control approaches, e.g. <ref> [4] </ref>, symbolic Machine Learning approaches, e.g. [5] as well as Neural Networks [1], and Genetic Algorithms [11]. All these approaches face the problem of credit assignment: A control strategy can only be evaluated as a whole. <p> But this number may possibly reduce considerably by improving the use of the genetic algorithm. The results using Neural Networks reported in [1] show very slow learning behavior, which is not competitive in terms of learning speed. The work on Fuzzy Control in <ref> [4] </ref> seems to deal with pole balancing on an unbounded track which is a much easier task.
Reference: [5] <author> D. Michie and R. A. Chambers. </author> <title> BOXES: An experiment in adaptive control. </title> <editor> In E. Dale and D. Michie, editors, </editor> <booktitle> Machine Intelligence, </booktitle> <pages> pages 137-152. </pages> <publisher> Edinburgh: Oliver and Boyd, </publisher> <year> 1968. </year>
Reference-contexts: An example is the classical pole balancing problem. To overcome this shortcoming, different methods for coping with nonlinear systems have been proposed. Among them are Fuzzy Control approaches, e.g. [4], symbolic Machine Learning approaches, e.g. <ref> [5] </ref> as well as Neural Networks [1], and Genetic Algorithms [11]. All these approaches face the problem of credit assignment: A control strategy can only be evaluated as a whole. If a strategy proves unsatisfactory, it is not clear how to alter the strategy to obtain improved performance. <p> All these approaches face the problem of credit assignment: A control strategy can only be evaluated as a whole. If a strategy proves unsatisfactory, it is not clear how to alter the strategy to obtain improved performance. Although systems using various approaches have been designed, e.g. <ref> [5, 7, 1, 9] </ref>, which manage to learn to balance a pole, it is unclear how well these approaches work for other and in particular for more complex systems. The mentioned learning approaches basically consider the system to be controlled as a black box. <p> Upon that data, the tested control strategy is altered in order to generate the next candidate being tested. This idea has been implemented in a number of systems, e.g. in [3, 1] and many others. BOXES <ref> [5] </ref> is one of the very early approaches of that kind. BOXES learns a state-action table which specifies a control action for each possible system state. For that purpose, the possible system states are discretized. The four system parameters span a space of 4-dimensional boxes. <p> Performance of BOXES is usually measured by the number of trials needed for balancing the pole for a given initial system state. Average numbers lie between 75 [6] and 557 <ref> [5] </ref> depending on the exact strategy for altering the control decision in a particular box. Although these approaches proved to work for the pole balancing problem, it is not clear whether and how far they represent methods for coping with the credit assignment problem in general. <p> the cart to the right and vice versa. * Qualitative constraints: If applying the force to the left is appropriate in a system state S 0 = hx 0 ; _x 0 ; 0 ; _ 0 i to bal-1 This is basically the same data structure as in BOXES <ref> [5] </ref>, although the number of distinguished intervals for each system pa rameter may be much larger. Machine Learning 435 Achim G.
Reference: [6] <author> C. Sammut. </author> <title> Recent progress with BOXES. </title> <editor> In K. Furukawa and S. Muggleton, editors, </editor> <booktitle> Machine Intelligence and Inductive Learning. </booktitle> <institution> Oxford University Press. (new series of Machine Intelligence), </institution> <note> to appear. </note>
Reference-contexts: A typical discretization of the system states considers 3 values for each of x, _x and _ and 6 values for . Performance of BOXES is usually measured by the number of trials needed for balancing the pole for a given initial system state. Average numbers lie between 75 <ref> [6] </ref> and 557 [5] depending on the exact strategy for altering the control decision in a particular box. Although these approaches proved to work for the pole balancing problem, it is not clear whether and how far they represent methods for coping with the credit assignment problem in general. <p> Moreover, the number of `learning steps' counted is somewhat arbitrary, since a group of learning steps could be encapsulated into one `bigger step', as it is done in the presented approach to a certain degree. E.g. approaches like BOXES as reported in <ref> [6] </ref> do not use explicit domain knowledge. However, they rely on a meaningful division of the pa Machine Learning 436 Achim G. <p> The `infinite' indicates the case, where the division is too coarse and no successful control strategy can be found. rameter ranges into a small number of intervals and use implicitly the `knowledge' that a division of parameters into 3 respectively 6 intervals is sufficient. The performance reported in <ref> [6] </ref> is quite impressive (75 trials in average for obtaining a working control strategy for a single initial system state, though the reliability that the control strategy is working for new initial states as well, however, appeared somewhat `fragile'.
Reference: [7] <author> C. Sammut. </author> <title> Experimental results from an evaluation of algorithms that learn to control dynamic systems. </title> <booktitle> In Proceedings of the 5 th International Conference on Machine Learning, </booktitle> <pages> pages 437-443. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: All these approaches face the problem of credit assignment: A control strategy can only be evaluated as a whole. If a strategy proves unsatisfactory, it is not clear how to alter the strategy to obtain improved performance. Although systems using various approaches have been designed, e.g. <ref> [5, 7, 1, 9] </ref>, which manage to learn to balance a pole, it is unclear how well these approaches work for other and in particular for more complex systems. The mentioned learning approaches basically consider the system to be controlled as a black box.
Reference: [8] <author> C. Sammut and D. Michie. </author> <title> Controlling a "black box" simulation of a space craft. </title> <journal> AI Magazine, </journal> (12):56-63, 1991. 
Reference-contexts: The pole balancing problem is a popular domain for case studies on controlling nonlinear dynamic systems. It is not only an attractive benchmark. It shows also similarities with control problems of practical importance, such as satellite altitude control <ref> [8] </ref>. A recent survey on approaches for learning to balance a pole can be found in [11]. The pole balancing system, see figure 1, consists of a pole which is hinged on a cart. The pole can swing in the verti c fl 1994 Achim G.
Reference: [9] <author> O. G. Selfridge, R. S. Sutton, and A. G. Barto. </author> <title> Training and tracking in robotics. </title> <booktitle> In Proceeedings 9 th IJCAI'85. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1985. </year>
Reference-contexts: All these approaches face the problem of credit assignment: A control strategy can only be evaluated as a whole. If a strategy proves unsatisfactory, it is not clear how to alter the strategy to obtain improved performance. Although systems using various approaches have been designed, e.g. <ref> [5, 7, 1, 9] </ref>, which manage to learn to balance a pole, it is unclear how well these approaches work for other and in particular for more complex systems. The mentioned learning approaches basically consider the system to be controlled as a black box.
Reference: [10] <author> T. Urbancic and I. Bratko. </author> <title> Learning to control dynamic systems. </title> <editor> In D. Michie and D. Spiegelhalter, editors, </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <type> Ellis Horwood. </type> <note> to appear. Machine Learning 437 Achim G. Hoffmann </note>
Reference-contexts: This paper proposes to use coarse qualitative knowledge about the effect of control actions on the system being controlled to substantially improve the performance of learning. Recently, the problem of forming the class of considered control strategies by qualitative knowledge has been addressed, e.g. <ref> [10, 2, 11] </ref>. In these investigations, not exclusively but mainly qualitative models about the system to be controlled were used. In the following, qualitative knowledge about possible control strategies is exploited without stating anything about the physics of the system.
Reference: [11] <author> A. Varsek, T. Urbancic, and B. Filipic. </author> <title> Genetic algorithms in controller design and tuning. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <note> SMC-23(6):1330-1339. Machine Learning 438 Achim G. Hoffmann </note>
Reference-contexts: An example is the classical pole balancing problem. To overcome this shortcoming, different methods for coping with nonlinear systems have been proposed. Among them are Fuzzy Control approaches, e.g. [4], symbolic Machine Learning approaches, e.g. [5] as well as Neural Networks [1], and Genetic Algorithms <ref> [11] </ref>. All these approaches face the problem of credit assignment: A control strategy can only be evaluated as a whole. If a strategy proves unsatisfactory, it is not clear how to alter the strategy to obtain improved performance. <p> This paper proposes to use coarse qualitative knowledge about the effect of control actions on the system being controlled to substantially improve the performance of learning. Recently, the problem of forming the class of considered control strategies by qualitative knowledge has been addressed, e.g. <ref> [10, 2, 11] </ref>. In these investigations, not exclusively but mainly qualitative models about the system to be controlled were used. In the following, qualitative knowledge about possible control strategies is exploited without stating anything about the physics of the system. <p> It is not only an attractive benchmark. It shows also similarities with control problems of practical importance, such as satellite altitude control [8]. A recent survey on approaches for learning to balance a pole can be found in <ref> [11] </ref>. The pole balancing system, see figure 1, consists of a pole which is hinged on a cart. The pole can swing in the verti c fl 1994 Achim G. Hoffmann ECAI 94. 11th European Conference on Artificial Intelligence Edited by A. <p> The performance reported in [6] is quite impressive (75 trials in average for obtaining a working control strategy for a single initial system state, though the reliability that the control strategy is working for new initial states as well, however, appeared somewhat `fragile'. The experiments reported in <ref> [11] </ref> rely on the same `knowledge' that a division into small number of intervals is sufficient. In addition, they used a qualitative model of the pole-and-cart system and a genetic algorithm for optimizing the control strategy.
References-found: 11

