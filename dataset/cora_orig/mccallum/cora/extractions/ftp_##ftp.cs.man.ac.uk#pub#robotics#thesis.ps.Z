URL: ftp://ftp.cs.man.ac.uk/pub/robotics/thesis.ps.Z
Refering-URL: http://www.cs.man.ac.uk/robotics/lopapers.html
Root-URL: 
Title: Experiments in Competence Acquisition for Autonomous Mobile Robots  
Author: Ulrich Nehmzow Ph.D. 
Note: c flUlrich Nehmzow 1992  
Date: 1992  
Affiliation: University of Edinburgh  
Abstract-found: 0
Intro-found: 0
Reference: [Albus 84] <author> J.S. Albus, </author> <title> Robotics, </title> <editor> in: M. Brady et al. (eds.), </editor> <booktitle> Robotics and Artificial Intelligence, </booktitle> <pages> pp. 65-93, </pages> <publisher> Springer Verlag Berlin, </publisher> <address> Heidelberg, New York, </address> <year> 1984. </year>
Reference: [Allman 77] <author> John Allman, </author> <title> Evolution of the Visual System in Early Primates, </title> <booktitle> in: Progress in Psychobiology and Physiological Psychology 7, </booktitle> <address> pp.1-53, </address> <year> 1977. </year>
Reference: [Arkin 89] <author> R. Arkin, </author> <title> Motor Schema Based Mobile Robot Navigation, </title> <journal> Intern. J. Robotics Research, </journal> <month> August </month> <year> 1989. </year>
Reference-contexts: It uses a generalised potential field method and a dynamic path planning algorithm to navigate round obstacles. Potential field methods ([Khatib 85], [Krogh & Thorpe 86], <ref> [Arkin 89] </ref>) determine the path between the current position of the robot and the desired position by applying an analogy from physics: 22 goal locations exert an (imaginary) attractive force, whilst obstacles exert repulsive forces on the robot.
Reference: [ -Astrom 89] <author> Karl Johan -Astrom, </author> <title> Toward Intelligent Control, </title> <journal> IEEE Control Systems Magazine, </journal> <month> April </month> <year> 1989. </year>
Reference: [Babloyantz 91] <author> Agnessa Babloyantz (ed.), </author> <title> Self-Organization, Emerging Properties, and Learning, </title> <publisher> Plenum Press, </publisher> <address> New York 1991. </address>
Reference: [Barhen et al. 89] <author> J. Barhen, W.B. </author> <title> Bress & C.C. Jorgensen, Applications of Concurrent Neuromorphic Algorithms for Autonomous Robots, </title> <editor> in Eck-miller and von der Malsburg (eds.), </editor> <booktitle> Neural Computers, </booktitle> <publisher> Springer, </publisher> <address> Berlin, Heidelberg, New York, </address> <year> 1989. </year>
Reference: [Barlow 89] <author> H.B. Barlow, </author> <title> Unsupervised Learning, </title> <booktitle> Neural Computation 1, </booktitle> <pages> 295-311, </pages> <publisher> MIT 1989. </publisher>
Reference-contexts: The question, therefore, was: could this scheme be altered so that it would still perform the same task, i.e. distinguish between "interesting" and "uninteresting" locations, but require fewer thresholds and, as a bonus, be computationally cheaper? As <ref> [Barlow 89] </ref> argues, redundancy is what drives unsupervised learning in the biological world: "Redundancy is the part of our sensory experiences that distinguishes [information] from noise; the knowledge it gives us about the patterns and regularities in sensory stimuli must be what drives unsupervised learning. . . .
Reference: [Barraquand & Latombe 90] <author> Jerome Barraquand and Jean-Claude Latombe, </author> <title> A Monte-Carlo Algorithm for Path Planning with many Degrees of Freedom, </title> <booktitle> Proc. IEEE Robotics and Automation, </booktitle> <year> 1990. </year>
Reference-contexts: (which represents the whole environment of the robot in terms of abutting triangles, [Lee & Preparata 84]) to the given map and then converting this Delaunay triangulation into a Voronoi diagram (the straight-line dual to the Delaunay triangulation, [Lee & Drysdale 81]), the latter being a more efficient representation 1 <ref> [Barraquand & Latombe 90] </ref> overcome these problems by using random motions, [Praler & Milios 90] by applying a highly parallel, localised algorithm. 23 than the former 2 .
Reference: [Barto et al. 83] <author> Andrew G. Barto, Richard S. Sutton and Charles W. An-derson, </author> <title> Neuronlike Adaptive Elements that can Solve Difficult Learning Control Problems, in: </title> <journal> IEEE Trans. on systems, man and cybernetics, </journal> <volume> Vol. SMC-13, No. 5,pp. 834 - 846, </volume> <month> September/October </month> <year> 1983. </year>
Reference: [Barto 90] <author> Andrew G. Barto, </author> <title> Connectionist Learning for Control, </title> <editor> in [Miller et al. </editor> <volume> 90]. </volume>
Reference: [Beale & Jackson 90] <author> R. Beale and T. Jackson, </author> <title> Neural Computing: An Introduction, Adam Hilger, </title> <address> Bristol, Philadelphia and New York, </address> <year> 1990. </year> <month> 199 </month>
Reference-contexts: That the Perceptron is a fast learner is proven in the following section. 4.1.7 Convergence in the Perceptron The following derivation of the convergence speed of the Perceptron is largely based upon <ref> [Beale & Jackson 90, pp.54ff] </ref>, but has been aug 5 A magic number is a parameter that influences the success or failure of an algorithm and that is set by the experimenter, using his ingenuity to determine its value. 88 mented by the example shown in figure 4.7.
Reference: [Beer 90] <author> Randall D. Beer, </author> <year> 1990. </year> <title> Intelligence as Adaptive Behaviour, </title> <publisher> Aca--demic Press. </publisher>
Reference: [Braitenberg 84] <author> Valentino Braitenberg, </author> <title> Vehicles, </title> <publisher> MIT Press, </publisher> <address> Cambridge Mass. and London, England, </address> <year> 1984. </year>
Reference: [Brooks 85] <author> Rodney Brooks, </author> <title> A Robust Layered Control System for a Mobile Robot, </title> <publisher> MIT AI Memo 864, </publisher> <year> 1985. </year>
Reference-contexts: On the one hand there is the analytical approach ([Nehmzow et al. 89]): the overall control task is broken up by the designer into fundamental subtasks, which are then implemented. This is a top down approach. <ref> [Brooks 85] </ref> calls this the functional decomposition: the control task is decomposed into a series of functional units; sensor signals percolate through these units until they generate an ac 20 avoid objects wander explore build maps identify objects plan changes perception modelling planning task execution motor control Sensors Actuators ? ? <p> How higher level behaviours can subsume lower level behaviours is explained by an example given in <ref> [Brooks 85] </ref>. First a controller is built that achieves level 0 behaviour (avoid contacts with objects). This controller uses a number of modules, one of these is the Runaway module 26 that takes its input from the Feelforce module.
Reference: [Brooks 86] <author> Rodney Brooks, </author> <title> Achieving Artificial Intelligence through Building Robots, </title> <publisher> MIT AI Memo 899, </publisher> <month> May </month> <year> 1986. </year>
Reference-contexts: In general it is dependent on the actual domain how quantisation can successfully be used. [Maes & Brooks 90] present a six-legged robot | Genghis | which, again by reinforcement learning, learns to coordinate its leg movements so that a walking behaviour is achieved. Unlike <ref> [Brooks 86] </ref>, who determines the arbitration between behaviours by hand, in Genghis the `relevance' of a particular behaviour is determined through a statisti 6 This process is described in chapter 4, beginning on page 77. 7 An example of a set of instinct-rules (the ones used to generate wall following behaviour)
Reference: [Brooks 87] <author> Rodney Brooks, </author> <title> Planning is just a way of avoiding figuring out what to do next, </title> <note> MIT working paper 303, </note> <month> September </month> <year> 1987. </year>
Reference: [Brooks et al. 88] <author> Rodney Brooks, Jonathan Connell and Peter Ning, Her-bert: </author> <title> A Second Generation Mobile Robot, </title> <publisher> MIT AI Memo 1016. </publisher> <month> January </month> <year> 1988. </year>
Reference: [Brooks 90a] <author> Rodney Brooks, </author> <title> Challenges for Complete Creature Architectures, in: Simulation of Adaptive Behaviour, </title> <publisher> MIT Press Cambridge Mass. and London, </publisher> <address> England, </address> <year> 1990. </year>
Reference: [Brooks 90b] <author> Rodney Brooks, </author> <title> Elephants Don't Play Chess, </title> <booktitle> Robotics and Autonomous Systems 6, </booktitle> <pages> 3-15, </pages> <publisher> North-Holland 1990. </publisher>
Reference-contexts: The Subsumption Architecture The most prominent advocate of robotics following the new paradigm is perhaps Rodney Brooks at MIT. He disagrees with "classical" Artificial Intelligence (AI) approaches, based on the symbol system hypothesis ([Simon 69]), which, according to <ref> [Brooks 90b] </ref>, states that intelligence operates on a system of symbols which represent entities in the world. Rather, he supports an approach based on the physical grounding hypothesis, which states that in order to build an intelligent system it is necessary to have its representations grounded in the real world. <p> The Runaway module is subsumed. A good overview of robots with subsumption architecture based controllers built at MIT is given in <ref> [Brooks 90b] </ref>. The simplest robots have just three thresholded infrared sensors (Tom and Jerry), running a three layer subsumption program. Herbert, on the other hand, had thirty infrared proximity sensors, a laser light striping system and a magnetic compass ([Connell 89, Brooks 91b, Brooks et al. 88]).
Reference: [Brooks 91a] <author> Rodney Brooks, </author> <title> Artificial Life and Real Robots, </title> <booktitle> Proceedings of 1st European Conference on Artificial Life 1991, </booktitle> <publisher> MIT Press Cambridge Mass. and London, </publisher> <address> England, </address> <year> 1991. </year>
Reference-contexts: The conclusion is: for mobile robotics it is crucial that the learning algorithm is fast (on the slow speed of reinforcement learning see also <ref> [Brooks 91a] </ref>). The fact that reinforcement learning can be extremely slow is shown by other researchers, too. [Prescott & Mayhew] simulate the AIVRU mobile robot and use a reinforcement learning algorithm similar to the one described by [Watkins 89]. <p> Steels, for example, presents a simulation in which intelligent overall behaviour is achieved through the interaction of simple agents with a simple environment ([Steels 89]). Similar considerations apply to robotics, the factors governing this process are not yet clearly understood (see also <ref> [Brooks 91a] </ref>). From this I drew the conclusion that in order to conduct research in robot behaviour as outlined in section 1.1, the appropriate means to do this would be to conduct experiments with robots 7 . However, there is a place for faithful simulation.
Reference: [Brooks 91b] <author> Rodney Brooks, </author> <title> Intelligence without Reason, </title> <booktitle> IJCAI 91, </booktitle> <pages> pp. 569-595. </pages>
Reference-contexts: The subsumption architecture thus is a distributed control architecture, without internal world model in the classical AI sense (it does have internal representations of states of the finite state machines, though; but it does not have traditional AI representation schemes, nor does it have explicit representations of goals, <ref> [Brooks 91b] </ref>). How higher level behaviours can subsume lower level behaviours is explained by an example given in [Brooks 85]. First a controller is built that achieves level 0 behaviour (avoid contacts with objects). <p> This has been verified experimentally, too. Although input vectors often vary at identical locations, locations are nevertheless identified correctly. That simple navigation is possible without any map-like structure at all was shown in <ref> [Brooks 91b, 6.2] </ref>. Herbert, the can collecting robot, finds its way back to the starting location by using rules like "when passing through a door southbound, turn left". <p> Experimentation, on the other hand, does not suffer from the problem of over-simplification, because by the nature of it all features of both robot and particular environment are there. As <ref> [Brooks 91b] </ref> puts it: "It is no longer possible to argue in conference papers that the simulated perceptual system is realistic, or that problems of uncertainty in action will not be significant. Instead, physical experiments can be done simply and repeatedly. <p> This property is important in robotics | certain competences, such as for example obstacle avoidance, have to be learned very quickly, because the robot's ability to stay operational crucially depends on them. In addition to this, <ref> [Brooks 91b] </ref> identifies as a further problem that the learning rate of the backpropagation network has to be set by hand in most cases. Consequently, most examples of robot controllers having a learning capability use techniques other than backpropagation networks ([Brooks 91b]).
Reference: [Canny 86] <author> John F. Canny, </author> <title> A Computational Approach to Edge Detection, in: </title> <journal> IEEE PAMI 8(6), </journal> <pages> pp. 679-698, </pages> <year> 1986. </year>
Reference: [Cartwright & Collett 83] <author> B.A. Cartwright and T.S.Collett, </author> <title> Landmark Learning in Bees, in: </title> <journal> Journal of Comparative Physiology 151, </journal> <pages> pp. 521-543, </pages> <year> 1983. </year>
Reference-contexts: The mechanism for location recognition described above bears some resemblance to the way bees recognise places. As <ref> [Cartwright & Collett 83] </ref> have found in their experiments, honey bees (Apis mellifera) use nearby landmarks to guide their way to a food source. "[Bees] do not find their way using anything analogous to a floor plan or map of the spatial layout of landmarks and food source. <p> How this could be used for map interpretation is an open question, some ideas as to how it could be achieved are presented in section 7.2. <ref> [Cartwright & Collett 83] </ref> also state that "the bee's guidance system is immune to a considerable amount of noise". To a small extent that could be observed in the experiments conducted here, too.
Reference: [Churchland 86] <author> Patricia Smith Churchland, </author> <title> Neurophilosophy, </title> <publisher> MIT Press Cambridge Mass. and London, </publisher> <address> England, </address> <year> 1986. </year>
Reference: [Clark 87] <author> Andy Clark, </author> <year> 1987. </year> <title> Being there: Why Implementation Matters to Cognitive Science, </title> <journal> Artificial Intelligence Review, </journal> <volume> Vol 1, </volume> <pages> pp. 231-244. </pages>
Reference: [Clark et al. 88] <author> Sharon A. Clark, Terry Allard, William M. Jenkins and Michael M. Merzenich, </author> <title> Receptive Fields in the Body Surface Map in Adult Cortex Defined by Temporally Correlated Inputs, </title> <booktitle> Nature 332 (31), </booktitle> <pages> pp. 444-445, </pages> <month> March </month> <year> 1988. </year> <month> 200 </month>
Reference: [Collett 87] <author> T.S.Collett, </author> <title> Insect Maps, </title> <journal> TINS, </journal> <volume> Vol. 10 No. 4, </volume> <year> 1987. </year>
Reference: [Connell 89] <author> Jonathan Hudson Connell, </author> <title> A Colony Architecture for an Artificial Creature, </title> <publisher> MIT AI TR-1151, </publisher> <year> 1989. </year>
Reference: [Daskalakis 91] <author> Nikolas Daskalakis, </author> <title> Learning Sensor-Action Coupling in Lego Robots, </title> <type> MSc Thesis, </type> <institution> Department of Artificial Intelligence, Edin-burgh University, </institution> <year> 1991. </year>
Reference-contexts: Similarly, in Genghis', Alder's and Cairngorm's case the search space is small enough to give the robot positive feedback at an early stage. Also, like in Obelix', Alder's and Cairngorm's case the input space of Genghis' learning algorithm is small. <ref> [Daskalakis 91] </ref> has replicated and extended the experiments in motor competence acquisition discussed in this thesis in chapter 4, using Lego robots ([Donnett & Smithers 91]), which are faster than Alder, and have different sensors, too (infrared and tactile sensors). <p> Some work has already been done on this question of portability and scalability: <ref> [Daskalakis 91] </ref> successfully implemented an instinct-rule based self-organising controller as described in chapter 4 on Lego robots ([Donnett & Smithers 91]), which have more sensors than Alder and Cairngorm, and which are also faster.
Reference: [De Almeida & Melin 89] <author> R. De Almeida and C. Melin, </author> <title> Exploration of Unknown Environments by a Mobile Robot, </title> <editor> in [Kanade et al. </editor> <volume> 89]. </volume>
Reference: [Dickmanns & Christians 89] <author> E. Dickmanns and Th. Christians, </author> <title> Relative 3D-State Estimation for Autonomous Visual Guidance of Road Vehicles, </title> <editor> in [Kanade et al. </editor> <volume> 89]. </volume>
Reference: [Donnett & Smithers 91] <author> Jim Donnett and Tim Smithers, </author> <title> Lego Vehicles: A Technology for Studying Intelligent Systems, </title> <booktitle> in [SAB 91]. </booktitle>
Reference: [Durbin & Willshaw 87] <author> R. Durbin and D. Willshaw, </author> <title> An Analogue Approach to the Travelling Salesman Problem, Using an Elastic Net Method, </title> <booktitle> Nature (326) No. </booktitle> <volume> 6114, </volume> <pages> pp. 689-691, </pages> <year> 1987. </year>
Reference: [Fikes et al. 72] <author> R.E. Fikes, P.E. Hart and N.J. Nilsson, </author> <title> Learning and Executing Generalized Robot Plans, </title> <journal> AI, </journal> <volume> 3 (1972), </volume> <pages> pp. 251-288. </pages>
Reference-contexts: Although being able to generalise and use once learned plans subsequently, Shakey was unable to cope with situations that could not be expressed in terms of its predefined operators. (More details about the use of these triangle tables are given in <ref> [Fikes et al. 72] </ref>).
Reference: [Flynn & Brooks 88] <author> Anita M. Flynn and Rodney A. Brooks, </author> <title> MIT Mobile Robots What's next?, </title> <booktitle> in: Proc. IEEE Conf. on Robotics and Automation, </booktitle> <year> 1988. </year>
Reference: [Freund et al. 91] <author> E. Freund, R. Mayr, F. Dierks, U. Judaschke, U. Kernebeck, B. Lammen, </author> <title> Safety Aspects for Autonomous Robot Systems, </title> <note> in [Schmidt 91]. </note>
Reference-contexts: Sensors give inconsistent readings, actuators depend on changeable factors such as supply current and voltage, wear, temperature, humidity and others. `Avoiding Obstacles' is by no means a bygone research topic | it is still a widely discussed topic, see for example <ref> [Freund et al. 91] </ref> and [IEEE 91]). Assume a robot had two whiskers mounted at the front of the vehicle and two independent motors that allowed the robot to move forward and backward as well as turn left or right. Such a robot is shown in figure 4.8.
Reference: [Frohlich et al. 91] <author> C. Frohlich, F. Freyberger, G. Karl and G. Schmidt, </author> <title> Multisensor System for an Autonomous Robot Vehicle, </title> <note> in [Schmidt 91]. </note>
Reference: [Giralt et al. 84] <author> G.G. Giralt, R. Chatila and M. Vaisset, </author> <title> An Integrated Navigation and Motion Control System for Autonomous Multisensory Mobile Robots, </title> <booktitle> in: Robotics Research, 1st international symposium, </booktitle> <pages> pp. 191 - 214, </pages> <year> 1984. </year>
Reference: [Goto & Stentz 87] <author> Y. Goto, A. Stentz, </author> <title> The CMU System for Mobile Robot Navigation, </title> <booktitle> in: Proc. IEEE Conf. on Robotics and Automation, </booktitle> <year> 1987. </year>
Reference: [Gould 82] <author> James L. Gould, Ethology: </author> <title> The Mechanisms and Evolution of Behavior, W.W. </title> <publisher> Norton and Co., </publisher> <address> New York 1982. </address>
Reference-contexts: Similar techniques can be found in biology: pigeons, for example, extract the changes in air pressure generated by changes in altitude of a few feet by ignoring the total strength and only measuring differences around some mean, <ref> [Gould 82] </ref>. The important difference between the experiments described here and the experiments described in the previous section is that here input vectors are only generated when a significant motor action is performed, not when any motor action is performed.
Reference: [Gould & Gould 88] <author> James L. Gould and Carol Grant Gould, </author> <title> The Honey Bee, </title> <editor> p. </editor> <volume> 106, </volume> <publisher> Scientific American Library, </publisher> <address> New York, </address> <year> 1988. </year> <month> 201 </month>
Reference: [Hasler et al. 78] <author> Arthur D. </author> <note> Hasler, A.T. Scholz and R.M. Horrall, Olfactory Imprinting and Homing in Salmon, American Scientist 66 (1978) pp. 347-55; quoted in [Gould 82]. </note>
Reference: [Hebb 49] <author> D.O. Hebb, </author> <title> The Organization of Behavior, </title> <publisher> Wiley, </publisher> <address> New York 1949. </address>
Reference-contexts: and build on any earlier learning, instead of replacing it, so that much early learning tends to be permanent; and finally, that the learning of the mature animal owes its efficiency to the slow and inefficient learning that has gone before, but may also be limited and canalized by it. <ref> [Hebb 49] </ref> 6.1 Introduction Growing from "weak to strong" ([McGonigle 91]) is one of the key issues in animal learning; this should apply to intelligent robotics as well.
Reference: [Hertz et al. 91] <author> John Hertz, Anders Krogh and Richard Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison Wesley 1991. </publisher>
Reference-contexts: After several `epochs', i.e. presentations of input vectors to the network, typical dissimilar responses appear for dissimilar input vectors. How much `several' means cannot easily be answered in the case of Kohonen's self-organising feature map; <ref> [Hertz et al. 91, p. 242ff] </ref> and [Ritter 88, p.40ff] discuss the convergence properties of the map in detail. <p> One criterion for determining the required dimensionality of the network is the dimensionality of the world the agent is to operate in. In Alder's and Cairngorm's case the world is effectively one-dimensional, because the robots follow the wall of their enclosure to generate the input signals to the SOFM. <ref> [Hertz et al. 91, pp.239f] </ref> give the example of a robot arm moving in three-dimensional space, avoiding obstacles, and suggest that "here a three-dimensional output array would clearly be appropriate". [Ritter 88, sect.9.1] discusses the use of SOFMs to learn the required torque for feedforward control of a simulated robot manipulator
Reference: [Hubel 79] <author> David H. Hubel, </author> <title> The Visual Cortex of Normal and Deprived Monkeys, </title> <journal> American Scientist, </journal> <volume> 67 No 5, </volume> <pages> pp. 532-543, </pages> <year> 1979. </year>
Reference: [IEEE 91] <editor> Proc. </editor> <booktitle> IEEE Conference on Robotics and Automation, Sacra-mento 1991, </booktitle> <pages> pp. 674-701, 898-925. </pages>
Reference-contexts: Of these tasks, obstacle avoidance is the one referred to most often in the literature, because it is such a fundamentally important competence. Collision avoidance is a research topic in its own right, see for example <ref> [IEEE 91] </ref>, where a whole section is dedicated to this topic. <p> Sensors give inconsistent readings, actuators depend on changeable factors such as supply current and voltage, wear, temperature, humidity and others. `Avoiding Obstacles' is by no means a bygone research topic | it is still a widely discussed topic, see for example [Freund et al. 91] and <ref> [IEEE 91] </ref>). Assume a robot had two whiskers mounted at the front of the vehicle and two independent motors that allowed the robot to move forward and backward as well as turn left or right. Such a robot is shown in figure 4.8.
Reference: [Kaelbling 90] <author> Leslie Pack Kaelbling, </author> <title> Learning in Embedded Systems, </title> <type> Stan-ford Technical Report TR-90-04, </type> <month> June </month> <year> 1990. </year>
Reference-contexts: Without learning, the agent runs into obstacles in 26.5% of all simulation steps, after 50,000 learning steps (!) this rate drops to 3.25%. <ref> [Kaelbling 90] </ref> compares several algorithms and their performances in a simulated robot domain. The agent has to stay away from obstacles (negative reinforcement is applied if it hits an obstacle), it receives positive reinforcement if it moves near a light source. <p> As all reinforcement learning schemes, this scheme can only work if the search space in which suitable actions are sought is small. <ref> [Kaelbling 90] </ref> writes that in experiments with the mobile robot Spanky, whose task it was to move towards a light source, the robot only learned to do this successfully if it was helped in the beginning, so that some positive feedback was received. <p> than experimentation; 2. simulation allows to bypass the sensory perception problem (noisy sensors; sensor interpretation in general) and the motor control problem (inaccurate actuators); and 3. simulation is easier to construct because "the relevant information" can be provided directly and does not have to be deduced from noisy sensor data. <ref> [Kaelbling 90, p. 171] </ref> supports the first point: "The . . . problem [of conducting experiments with robots] is that it takes a long time to conduct the experiments. . . .
Reference: [Kaelbling 91] <author> Leslie Pack Kaelbling, </author> <title> An Adaptable Mobile Robot, </title> <booktitle> Proceedings of 1st European Conference on Artificial Life 1991, </booktitle> <publisher> MIT Press Cam-bridge Mass. and London, </publisher> <address> England, </address> <year> 1991. </year>
Reference: [Kacandes et al. 89] <author> Peter Kacandes, Achim Langen and Hans-Jurgen War-necke, </author> <title> A Combined Generalized Potential Fields/Dynamic Path Planning Approach to Collision Avoidance for a Mobile Autonomous Robot Operating in a Constrained Environment, </title> <editor> in [Kanade et al. </editor> <volume> 89]. </volume>
Reference: [Kampmann & Schmidt 89] <author> Peter Kampmann and Gunther Schmidt, </author> <title> Multilevel Motion Planning for Mobile Robots Based on a Topologically Structured World Model, </title> <editor> in [Kanade et al. </editor> <volume> 89]. </volume>
Reference: [Kampmann & Schmidt 91] <author> Peter Kampmann and Gunther Schmidt, </author> <title> Indoor Navigation of Mobile Robots by Use of Learned Maps, </title> <note> in [Schmidt 91]. </note>
Reference: [Kanade et al. 89] <editor> T. Kanade, F.C.A. Groen and L.O. Hertzberger (eds.), </editor> <booktitle> Intelligent Autonomous Systems 2, Proceedings of IAS 2, </booktitle> <address> ISBN 90-800410-1-7, Amsterdam 1989. </address>
Reference: [Khatib 85] <author> O. Khatib, </author> <title> Real-Time Obstacle Avoidance for Manipulators and Mobile Robots, </title> <journal> IEEE Robotics and Automation 1985. </journal>
Reference: [Kohonen 82a] <author> Teuvo Kohonen, </author> <title> Clustering, Taxonomy and Topological Maps of Patterns, </title> <booktitle> Proc. 6th intern. Conf. on Pattern Recognition, </booktitle> <month> Octo-ber </month> <year> 1982. </year>
Reference-contexts: Moreover, the map attains a format which conforms to the signal distribution", <ref> [Kohonen 82a] </ref>. Self-organising feature maps provide a suitable means to reduce the dimensionality of high dimensional sensor input spaces, and are thus able to abstract this input information.
Reference: [Kohonen 82b] <editor> Teuvo Kohonen Self-Organized Formation of Topologically Correct Feature Maps, </editor> <booktitle> Biological Cybernetics 43, </booktitle> <address> pp.59-69, </address> <year> 1982. </year> <month> 202 </month>
Reference-contexts: This simulated manipulator has a two-dimensional workspace and the dimensionality of the network used is two-dimensional as well (15 x 24 units). <ref> [Kohonen 82b] </ref> presents a mapping of a one-dimensional input space (the response signals of twenty different bandpass filters to a single audio tone) onto a one-dimensional network of ten units. A topology-preserving tonotopic map develops.
Reference: [Kohonen 88] <author> Teuvo Kohonen, </author> <title> Self Organization and Associative Memory, </title> <publisher> Springer Verlag, </publisher> <address> Berlin, Heidelberg, New York, 2nd edition, </address> <year> 1988. </year>
Reference-contexts: Units giving non-zero outputs of 1 could also be used. 3 See <ref> [Kohonen 88, p.132, Fig. 5.11a] </ref>. 130 Alder. The shape of a ring helps avoid border effects. The behaviour of this network is as previously described in section 2.4.2, the neighbour-hood within which weight vectors are updated is 2 cells (constant over time).
Reference: [Knieriemen & v.Puttkamer 91] <author> T. Knieriemen and E. von Puttkamer, </author> <title> Real-Time Control in an Autonomous Mobile Robot, </title> <note> in [Schmidt 91]. </note>
Reference-contexts: Consequently, work has been done on the autonomous acquisition of maps, rather than using predefined maps. [Knieriemen 91], <ref> [Knieriemen & v.Puttkamer 91] </ref> present one example of this. In a three-stage process, MOBOT III constructs a world model, based on the readings from its 3D laser range camera. <p> By constructing a map, for example in the way used by <ref> [Knieriemen & v.Puttkamer 91, Knieriemen 91] </ref>, the problems that oc 30 cur when something changes can be avoided. In such a case the map is simply changed as well. Obviously the ability of the robot to reliably navigate depends on the quality of the map.
Reference: [Knieriemen 91] <author> Thomas Knieriemen, </author> <title> Autonome Mobile Roboter, </title> <publisher> BI Wis-senschaftsverlag, Mannheim, </publisher> <year> 1991. </year>
Reference-contexts: Consequently, work has been done on the autonomous acquisition of maps, rather than using predefined maps. <ref> [Knieriemen 91] </ref>, [Knieriemen & v.Puttkamer 91] present one example of this. In a three-stage process, MOBOT III constructs a world model, based on the readings from its 3D laser range camera. <p> By constructing a map, for example in the way used by <ref> [Knieriemen & v.Puttkamer 91, Knieriemen 91] </ref>, the problems that oc 30 cur when something changes can be avoided. In such a case the map is simply changed as well. Obviously the ability of the robot to reliably navigate depends on the quality of the map.
Reference: [Koren & Borenstein 91] <author> Yoram Koren and Johann Borenstein, </author> <title> Potential Field Methods and their inherent Limitations for Mobile Robot Navigation, </title> <journal> IEEE Robotics and Automation 1991. </journal>
Reference: [Krogh & Thorpe 86] <author> B. Krogh and C. Thorpe, </author> <title> Integrated Path Planning and Dynamic Steering Control for Autonomous Vehicles, </title> <journal> IEEE Robotics and Automation 1986. </journal>
Reference-contexts: It uses a generalised potential field method and a dynamic path planning algorithm to navigate round obstacles. Potential field methods ([Khatib 85], <ref> [Krogh & Thorpe 86] </ref>, [Arkin 89]) determine the path between the current position of the robot and the desired position by applying an analogy from physics: 22 goal locations exert an (imaginary) attractive force, whilst obstacles exert repulsive forces on the robot.
Reference: [Lampinen 91] <institution> Jukko Lampinen, Lappeenranta Institute of Technology, </institution> <type> personal communication. </type>
Reference: [Lee & Drysdale 81] <author> D. Lee and T. Drysdale, </author> <title> Generalization of Voronoi Diagrams in the Plane, </title> <journal> SIAM J. Comput. </journal> <volume> 10, </volume> <year> 1981, </year> <pages> pp. 73-87. </pages>
Reference-contexts: space and obstacle space are determined by first applying a Delaunay triangulation (which represents the whole environment of the robot in terms of abutting triangles, [Lee & Preparata 84]) to the given map and then converting this Delaunay triangulation into a Voronoi diagram (the straight-line dual to the Delaunay triangulation, <ref> [Lee & Drysdale 81] </ref>), the latter being a more efficient representation 1 [Barraquand & Latombe 90] overcome these problems by using random motions, [Praler & Milios 90] by applying a highly parallel, localised algorithm. 23 than the former 2 .
Reference: [Lee & Preparata 84] <author> D. Lee and F. Preparata, </author> <title> Computational Geometry | A Survey, </title> <journal> IEEE Trans. on Comp. </journal> <volume> 33, </volume> <year> 1984, </year> <pages> pp. 1071-1101. </pages>
Reference-contexts: The robot is equipped with a predefined map which is updated by the robot's sensor readings. Free motion space and obstacle space are determined by first applying a Delaunay triangulation (which represents the whole environment of the robot in terms of abutting triangles, <ref> [Lee & Preparata 84] </ref>) to the given map and then converting this Delaunay triangulation into a Voronoi diagram (the straight-line dual to the Delaunay triangulation, [Lee & Drysdale 81]), the latter being a more efficient representation 1 [Barraquand & Latombe 90] overcome these problems by using random motions, [Praler & Milios
Reference: [Levi 87] <author> Paul Levi, </author> <title> Principles of Planning and Control Concepts for Autonomous Mobile Robots, </title> <booktitle> in: Proc. IEEE Conf. on Robotics and Automation, </booktitle> <address> p. 874, </address> <year> 1987. </year>
Reference: [Linsker 88] <author> R. Linsker, </author> <title> Self-Organization in a Perceptual Network, </title> <booktitle> Computer, </booktitle> <month> March </month> <year> 1988, </year> <pages> 105-117. </pages>
Reference-contexts: p p p p ` p p p @ @ @ @ @ @ @ @ @I Z Z P P a a a A A C O B X X X E C , , fl fl imising the information content of the output signal ([Hertz et al. 91], <ref> [Linsker 88] </ref>). In figure 2.4 this is shown, OA is the first principal component direction, OB that of the second principal component (after [Linsker 88]). <p> a A A C O B X X X E C , , fl fl imising the information content of the output signal ([Hertz et al. 91], <ref> [Linsker 88] </ref>). In figure 2.4 this is shown, OA is the first principal component direction, OB that of the second principal component (after [Linsker 88]). The self-organising feature map after Kohonen structures the input signal space along the direction of the first principal component; there are networks that can also extract higher principal components from the input data ([Hertz et al. 91, p.206]).
Reference: [Long-Ji 91] <author> Lin Long-Ji, </author> <title> Self-Improving Reactive Agents: Case studies of Reinforcement Learning Frameworks, </title> <booktitle> in: </booktitle> <address> [SAB 91]. </address>
Reference: [McGonigle & Chalmers 77] <author> Brendan McGonigle and Magaret Chalmers, </author> <title> Are Monkeys Logical, </title> <journal> Nature, </journal> <volume> vol 267, </volume> <pages> pp 694-696, </pages> <year> 1977. </year>
Reference: [McGonigle 91] <author> Brendan McGonigle, </author> <title> Incrementing Intelligent Systems by Design, </title> <booktitle> in [SAB 91]. </booktitle>
Reference-contexts: The whole learning process is seen as a staged process with genetically programmed steps, built upon each other and depending on each other. There seems to be biological evidence to support this view. <ref> [McGonigle 91] </ref>: "[A] staged view of human cognitive growth has received strong support from research in the neurosciences. [Thatcher et al. 87], using EEG phase and coherence measures, have picked up a pattern of growth spurts and consolidations in the development of cortico-cortico connections between hemispheres in human brain development seen <p> many of the important cognitive leaps forward in development are the main consequence of such changes in the nervous system and are thus by `design' and not a consequence of earlier antecedent behaviours." This says that being intelligent cannot be learned, but is the result of an appropriate (genetic) "program". <ref> [McGonigle 91] </ref> describes the process of incrementing intelligent systems by design as a process along two orthogonal trajectories.
Reference: [McGonigle & Chalmers 92] <author> Brendan McGonigle and Magaret Chalmers, </author> <title> Intelligent systems: A Cognitive Analysis, </title> <note> Columbia Press, in preparation, </note> <year> 1992. </year>
Reference: [Maes & Brooks 90] <author> Pattie Maes and Rodney Brooks, </author> <title> Learning to Coordinate Behaviors, </title> <booktitle> Proc. AAAI 1990. </booktitle>
Reference-contexts: I have used quantisation as well, for example to use Alder's sonar sensor for obstacle avoidance (see page 105), or in constructing the input vector for the robots' mapbuilding process (see page 156). In general it is dependent on the actual domain how quantisation can successfully be used. <ref> [Maes & Brooks 90] </ref> present a six-legged robot | Genghis | which, again by reinforcement learning, learns to coordinate its leg movements so that a walking behaviour is achieved. <p> This example may seem far fetched and unlikely to occur in reality, but the wiring of sensors can be error prone if a large number of sensors is to be connected. If the robot can determine the best wiring autonomously, malfunctions due to wrong sensor connections can be avoided. <ref> [Maes & Brooks 90] </ref>: "For more complicated robots prewiring solutions become either too difficult or impractical. . . . <p> Little work has been done to date on actual implementations of learning algorithms on mobile robots, two examples of related work are presented in [Mahadevan & Connell 91] and <ref> [Maes & Brooks 90] </ref>. Compared to these, and compared to related simulations of reinforcement learning architectures ([Kaelbling 90, Prescott & Mayhew, Sutton 91]), the self-organising controller presented in this thesis and implemented on Alder and Cairngorm offers extremely fast learning.
Reference: [Mahadevan & Connell 91] <author> Sridhar Mahadevan and Jonathan Connell, </author> <title> Automatic Programming of Behavior-based Robots using Reinforcement Learning, </title> <booktitle> 9th National Conference on Artificial Intelligence, AAAI 1991. </booktitle> <pages> 203 </pages>
Reference-contexts: For robotics applications this is 37 unrealistic and extremely limiting. Neither of these conditions are as-sumed for the work reported in this thesis. Perhaps not surprisingly, to date there are only very few implementations of reinforcement learning controllers on real robots. Two examples are described below. <ref> [Mahadevan & Connell 91] </ref> present a mobile robot (called Obelix) which uses reinforcement learning (Q-learning) to acquire a box-pushing skill. In order to overcome the credit assignment problem 5 , the overall task of box-pushing is divided into three subtasks: box-finding, box-pushing and unwedging. <p> This 9-bit input vector is used as an input to the Q-learning algorithm. The possible motor actions of Obelix are restricted to five: forward, left turn, right turn, sharp left turn and sharp right turn. <ref> [Mahadevan & Connell 91] </ref> present very interesting results that confirm the findings mentioned earlier, as well as observations made in the experiments reported in this thesis. <p> a confirmation of findings mentioned earlier, but two other aspects are very interesting in relation to the work presented here. 5 How does one correctly assign credit or blame to an action when its consequences unfold over time and interact with the consequences of other actions ([Barto 90])? 38 Firstly, <ref> [Mahadevan & Connell 91] </ref> use a hierarchy of independent be--haviours, expressed as behavioural modules within the subsumption architecture. A similar hierarchy is implicitly present in the so-called instinct-rules that govern the process of motor competence acquisition of Alder and Cairngorm 6 . <p> Little work has been done to date on actual implementations of learning algorithms on mobile robots, two examples of related work are presented in <ref> [Mahadevan & Connell 91] </ref> and [Maes & Brooks 90]. Compared to these, and compared to related simulations of reinforcement learning architectures ([Kaelbling 90, Prescott & Mayhew, Sutton 91]), the self-organising controller presented in this thesis and implemented on Alder and Cairngorm offers extremely fast learning.
Reference: [Malcolm & Smithers 88] <author> Chris Malcolm and Tim Smithers, </author> <title> Programming Assembly Robots in Terms of Task Achieving Behavioural Modules: First Experimental Results, Proc. Intern. Adv. Robotics Programme, Second Workshop on Manipulators, Sensors and Steps towards Mobility, </title> <month> Manch-ester </month> <year> 1988. </year>
Reference: [Malcolm et al. 89] <author> Chris Malcolm, Tim Smithers and John Hallam, </author> <title> An Emerging Paradigm in Robot Architecture, </title> <editor> in [Kanade et al. </editor> <volume> 89]. </volume>
Reference-contexts: avoid objects wander explore build maps identify objects plan changes perception modelling planning task execution motor control Sensors Actuators ? ? ? Sensors Actuators fl fl fl fl @ C C C CCW A AU B B B A A Qs 3 fi fi fi fiffi sition ([Brooks 85], after <ref> [Malcolm et al. 89] </ref>). tion. Planning the robot's path round an obstacle is an example for an analytical approach. On the other hand, a synthetical approach ([Nehmzow et al. 89]) combines independent modules to achieve the global control task. <p> The distinction between analytical and synthetical approach is only one aspect of a new, emerging paradigm in robotics. <ref> [Malcolm et al. 89] </ref> characterise this new paradigm by the following set of features: * Intelligence is emergent through the interaction of the right in gredients, it is not implemented, * sensing and acting are tightly coupled and not dealt with inde pendently, * parallelism across the entire range of the <p> This world model can then be used to plan the robot's actions in the world, to monitor them and to correct them ("Sense, think, act" cycle, <ref> [Malcolm et al. 89] </ref>). In this thesis I have shown that many of the tasks that are achieved by today's mobile robots can be implemented without any explicit world model and without planning. I have described experiments in competence acquisition, conducted with the mobile robots Alder and Cairngorm.
Reference: [Malcolm 91] <author> Chris Malcolm, </author> <type> personal communication, </type> <year> 1991. </year>
Reference-contexts: I have programmed "Alder" (a robot) to behave that way and, as expected, the robot turns away from solitary obstacles. Something surprising happens, though, when the robot is put in a dead end, as shown in figure 1.3: instead of turning left and right forever, possibly 3 <ref> [Malcolm 91] </ref> reports that a particular assembly robot "reliably" failed on Mon-day mornings, because of slightly smaller dimensions due to being cold! 8 are not present in the mere collection of them. 9 MOTOR MOTOR & L L L fi fi fi fi - & % u reversing occasionally, the robot
Reference: [Marr & Hildreth 80] <author> David C. Marr and Ellen C. Hildreth, </author> <title> Theory of Edge Detection, in: </title> <journal> Proc. R. Soc. </journal> <volume> London 207, </volume> <pages> pp. 187-217, </pages> <year> 1980. </year>
Reference: [Martin et al. 90] <author> Fred Martin, Mitchel Resnick and Brian Silverman, Braitenberg Bricks: </author> <title> A Lego-based Creature-Construction Kit, </title> <booktitle> Second Artificial Life Conference, </booktitle> <month> February </month> <year> 1990, </year> <institution> Center for nonlinear studies, Santa Fe Institute, </institution> <address> Santa Fe, New Mexico. </address>
Reference: [Mataric 91] <editor> Maja Mataric, </editor> <title> Navigating with a Rat Brain: </title>
Reference-contexts: Such a topological map represents topological relationships (for example neighbourhood relationships) between locations in the robot's world, but it does not represent the exact distances and angles between these locations. The demands on sensor accuracy are therefore less if a topological map is to be constructed. <ref> [Mataric 91] </ref> presents a navigational system that uses such a topological representation of the world. Using three layers of competence in a subsumption architecture based controller, the mobile robot Toto explores its environment by following boundaries on its left hand side. <p> to be mapped are detected: in Toto, mapable features are identified by their type and the compass heading of the robot, whereas in Alder and Cairngorm they are recognised by the history of preceding events (either previous landmarks or previously executed motor actions), i.e., by the arriving at a location. <ref> [Mataric 91] </ref> states that each landmark is associated with a motion directive, indicating the required motion to reach the next landmark, thus addressing the question of map interpretation. These motion directives are derived from the compass bearing associated with that landmark.
References-found: 77

