URL: http://www.icsi.berkeley.edu/ftp/global/pub/techreports/1994/tr-94-028.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/ftp/global/pub/techreports/1994/
Root-URL: http://www.icsi.berkeley.edu
Title: On the parallel complexity of Gaussian Elimination with Pivoting  
Author: M. Leoncini 
Date: August 1994  
Pubnum: TR-94-028  
Abstract: Consider the Gaussian Elimination algorithm with the well-known Partial Pivoting strategy for improving numerical stability (GEPP). Vavasis proved that the problem of determining the pivot sequence used by GEPP is log space-complete for P, and thus inherently sequential. Assuming P 6= NC, we prove here that either the latter problem cannot be solved in parallel time O(N 1=2* ) or all the problems in P admit polynomial speedup. Here N is the order of the input matrix and * is any positive constant. This strengthens the P-completeness result mentioned above. We conjecture that the result proved in this paper holds for the stronger bound O(N 1* ) as well, and provide supporting evidence to the conjecture. Note that this is equivalent to assert the asymptotic optimality of the naive parallel algorithm for GEPP (modulo P 6= NC). fl Dipartimento di Informatica, Universita di Pisa, Pisa (Italy). Part of this work was done while the author was visiting the "International Computer Science Institute", Berkeley, CA. Support to the author's research has been given by the ESPRIT Basic Research Action, Project 9072 "GEPPCOM", and by the M.U.R.S.T. 40% and 60% funds. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Borodin, J. von zur Gathen, and J. Hopcroft. </author> <title> Fast parallel matrix and GCD computations. </title> <journal> Inform. and Control, </journal> <volume> 52 </volume> <pages> 241-256, </pages> <year> 1982. </year> <month> 22 </month>
Reference-contexts: These include linear system solution, determinant and (with some special care) rank computation. It is well-know that, for matrices of order n, the parallel arithmetic complexity of the latter problems is O (log 2 n) (see <ref> [8, 1, 2, 12] </ref>). However, the algorithms that achieve this bound are not regarded as practical ones by the numerical analysis community. The reason lies in part in the large number of processors required, but mostly because they are considered numerically unstable.
Reference: [2] <author> B. Codenotti and M. Leoncini. </author> <title> Parallel Complexity of Linear System Solution. </title> <publisher> World Scientific Pu. Co., </publisher> <address> Singapore, </address> <year> 1991. </year>
Reference-contexts: These include linear system solution, determinant and (with some special care) rank computation. It is well-know that, for matrices of order n, the parallel arithmetic complexity of the latter problems is O (log 2 n) (see <ref> [8, 1, 2, 12] </ref>). However, the algorithms that achieve this bound are not regarded as practical ones by the numerical analysis community. The reason lies in part in the large number of processors required, but mostly because they are considered numerically unstable.
Reference: [3] <author> A. Condon, </author> <title> A Theory of Strict P-completeness. </title> <booktitle> Proc. STACS 92, Lecture Notes in Computer Science, </booktitle> <volume> 577 </volume> <pages> 33-44. </pages>
Reference-contexts: They introduce the class EP, of problems solvable with constant inefficiency, and the class SP, of problems solvable with polynomial inefficiency. We clearly do not know whether these new classes of problems actually differ from NC. However, <ref> [3] </ref> proves that there are P-complete problems that appear to have a bound on the amount of achievable speedup. Such problems are said strictly T (n)-complete for P, for some complexity function T (n). <p> The first problem complete for P in the stricter sense outlined above is the Square Circuit Value Problems, with T (n) = n 1=2 <ref> [3] </ref>. The technique used to prove this result is a generic reduction from an arbitrary RAM computation. <p> As for the space, this will be the number k = k (n) of registers used. In order to determine the slowdown incurred by RAM simulations on the rRAM model, we begin with a lemma on memory compaction which is an easy adaptation of a result in <ref> [3] </ref>. Lemma 1 A RAM with space demand s (n) can be restricted to access only cells whose addresses are O (s (n)) on input of length n, with only a loss of a factor O (log s (n)) in the running time. <p> Clearly this implies an O (log 2 s (n)) slowdown in the running time of the rRAM program with respect to the original RAM program. 6 The last step consists in forcing the computations to be oblivious. Condon <ref> [3] </ref> proves the following result. Lemma 3 Any RAM with running time t (n) can be simulated by an oblivious RAM with running time O (t (n)). The simulation does not make use of the indirect addressing instructions, and thus holds also in case of our rRAM.
Reference: [4] <author> A. Condon. </author> <type> Personal communication. </type>
Reference-contexts: The result we obtain is an easy one. However, it appears to be difficult to obtain stronger bounds <ref> [4, 10] </ref>.
Reference: [5] <author> S.A. </author> <title> Cook A taxonomy of problems with fast parallel algorithms. </title> <journal> Information and Control, </journal> <volume> 64 </volume> <pages> 2-22, </pages> <year> 1985. </year>
Reference-contexts: There is now a rich literature on the complexity class NC (see <ref> [5, 14] </ref> for surveys and [15] for a general critique). Recently, there has been much interest in identifying problems that, though probably not in NC, admit at least polynomial speedup. Vitter and Simons [18] identified a number of such problems (see also [15]).
Reference: [6] <author> S.A. Cook and R.A. Reckhow. </author> <title> Time Bounded Random Access Machines. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 7 </volume> <pages> 354-375, </pages> <year> 1973. </year>
Reference-contexts: For instance, if M were the Turing Machine, then to observe polynomial speedup it would be sufficient in many cases just to pick a PRAM with a single processor. The sequential model we will use is the classic RAM introduced by Cook and Reckhow <ref> [6] </ref>. This is a natural model of computation, and one widely adopted in the study of concrete algorithms. In this latter setting, one often assumes that the cost of performing an operation is a fixed constant, independent of the length of the operand (s) involved. <p> In this latter setting, one often assumes that the cost of performing an operation is a fixed constant, independent of the length of the operand (s) involved. This is the well-known unit cost criterion. However, for complexity-theoretic investigations, the logarithmic cost criterion (which is the one adopted in <ref> [6] </ref>) is usually regarded as more precise than the unit cost criterion. 3 The instruction set of the RAM is showed in Table 1. The table also shows the execution times charged to each instruction under the logarithmic cost criterion. The function l () is defined as follows (see [6]): l <p> in <ref> [6] </ref>) is usually regarded as more precise than the unit cost criterion. 3 The instruction set of the RAM is showed in Table 1. The table also shows the execution times charged to each instruction under the logarithmic cost criterion. The function l () is defined as follows (see [6]): l (i) = dlog 2 jije if jij 2 1 otherwise. <p> As for the space, which will play an important role in our reduction, the RAM introduced in <ref> [6] </ref> adopted a logarithmic cost criterion as well. According to such criterion, a cost is charged only to those registers that are accessed at some point during the computation. <p> Clearly, in view of (1) and the fact that is the machine precision, the number m 2 L 3 t (n) is a "machine zero". Equation (1) has implications on the word length. As already pointed out in Section 3, Cook and Reckhow <ref> [6] </ref> proved that the largest integer that can be generated by a t (n) time bounded RAM has magnitude 2 O ( p t (n)) . This implies that a word length polynomial in the input size is sufficient.
Reference: [7] <author> T.H. Cormen, C.E. Leiserson, and R.L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press/McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: However, we have sequential algorithms running in time O (n 2 ) and using O (n) space, provided that the number m of edges is O (n) and the capacities on the edges are small size integers (see, e.g. the textbook <ref> [7] </ref>). Therefore we have s (n) = (t (n)) 1 1 2 , i.e. * = 1=2 in the argument of the above paragraph. Acknowledgements It is a pleasure to acknowledge the helpful comments and conversations that I had with Bruno Codenotti and Anne Condon on the subject of this paper.
Reference: [8] <author> L. Csanky. </author> <title> Fast parallel matrix inversion algorithms. </title> <journal> SIAM J. Comput., </journal> <volume> 5 </volume> <pages> 618-623, </pages> <year> 1976. </year>
Reference-contexts: These include linear system solution, determinant and (with some special care) rank computation. It is well-know that, for matrices of order n, the parallel arithmetic complexity of the latter problems is O (log 2 n) (see <ref> [8, 1, 2, 12] </ref>). However, the algorithms that achieve this bound are not regarded as practical ones by the numerical analysis community. The reason lies in part in the large number of processors required, but mostly because they are considered numerically unstable.
Reference: [9] <author> P.W. Dymond. </author> <title> Indirect Addressing and the Time Relationships of Some Models of Sequential Computation. </title> <journal> Comp. and Math. with Appl., </journal> <volume> 5 </volume> <pages> 193-209, </pages> <year> 1979. </year>
Reference-contexts: Our sole concern is to understand how exactly we loose if we eliminate the indirect addressing instructions from the set of Table 1. This is an interesting question per se, but unfortunately one that has not yet received a satisfying answer. Dymond <ref> [9] </ref> studied thoroughly this problem. He introduced the Augmented Counter Machine (ACM) model and studied its relationships with the RAM (among the others). An ACM, with k registers, can be viewed as a RAM without indirect addressing capabilities, but further restricted to add and subtract small amounts only.
Reference: [10] <author> P.W. Dymond. </author> <type> Personal communication through E-mail. </type>
Reference-contexts: Therefore, no fast simulation of the RAM by an ACM is possible. There must always be polynomial slowdown in view of the above result and the RAM time hierarchy proved by Cook and Reckhow. 4 Extending the simulation above to ACMs with full addition and subtraction seems possible <ref> [10] </ref>. If we accept this, we also accept that there must be polynomial slowdown in the simulation of unrestricted RAMs by RAMs without indirection capabilities. <p> The result we obtain is an easy one. However, it appears to be difficult to obtain stronger bounds <ref> [4, 10] </ref>.
Reference: [11] <author> P. van Emde Boas, </author> <title> Machine Models and Simulations. </title> <editor> In: J. van Leeuwen (ed.). </editor> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <volume> Vol. </volume> <editor> A. </editor> <publisher> The MIT Press/Elsevier, </publisher> <year> 1990, </year> <pages> 3-66. </pages>
Reference: [12] <author> J. Von zur Gathen, </author> <title> Parallel Linear Algebra. </title> <editor> In: J. Reif (ed.), </editor> <title> Synthesis of Parallel Algorithm, </title> <publisher> Morgan and Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1993, </year> <pages> 573-617. </pages>
Reference-contexts: These include linear system solution, determinant and (with some special care) rank computation. It is well-know that, for matrices of order n, the parallel arithmetic complexity of the latter problems is O (log 2 n) (see <ref> [8, 1, 2, 12] </ref>). However, the algorithms that achieve this bound are not regarded as practical ones by the numerical analysis community. The reason lies in part in the large number of processors required, but mostly because they are considered numerically unstable.
Reference: [13] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: the ratio pT p =T , where T is the sequential running time, p is the number of processors and T p is the parallel time with p processors. 1 as GEPP), and it is of fundamental importance in computational linear algebra and in the broad field of scientific computing <ref> [13] </ref>. In fact, when the LU decomposition is known, many problems can be solved with little additional cost. These include linear system solution, determinant and (with some special care) rank computation. <p> To this end, we first introduce some basic terminology and recall the fundamental ideas behind Gaussian Elimination with Partial Pivoting (for more details on the latter, we refer the reader to the classic reference <ref> [13] </ref>). * For h = 1; : : : ; N +1, we let M (h) stand for the the matrix resulting from the execution of the first h 1 pivot steps. Thus M (1) = M (A n ; I), while M (N+1) is the final (triangular) matrix.
Reference: [14] <author> R.M. Karp and V. Ramachandran. </author> <title> Parallel Algorithms for Shared-Memory Machines. </title> <editor> In: J. van Leeuwen (ed.). </editor> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <volume> Vol. </volume> <editor> A. </editor> <publisher> The MIT Press/Elsevier, </publisher> <year> 1990, </year> <pages> 869-941. </pages>
Reference-contexts: There is now a rich literature on the complexity class NC (see <ref> [5, 14] </ref> for surveys and [15] for a general critique). Recently, there has been much interest in identifying problems that, though probably not in NC, admit at least polynomial speedup. Vitter and Simons [18] identified a number of such problems (see also [15]). <p> From this fact we obtain a large degree of freedom in the choice of the parallel computation model. In fact, it is well-known (see, e.g., <ref> [14] </ref>) that the running times on the various PRAMs are related by polylogarithmic factors. The same is true for PRAMs and uniform boolean circuits. In this paper we will adopt the (say, CREW) PRAM as our parallel computation model.
Reference: [15] <author> C.P. Kruskal, L. Rudolph, and M. Snir, </author> <title> A complexity theory of efficient parallel algorithms. </title> <journal> Theoretical Computer Science, </journal> <volume> 71 </volume> <pages> 95-132, </pages> <year> 1990. </year>
Reference-contexts: There is now a rich literature on the complexity class NC (see [5, 14] for surveys and <ref> [15] </ref> for a general critique). Recently, there has been much interest in identifying problems that, though probably not in NC, admit at least polynomial speedup. Vitter and Simons [18] identified a number of such problems (see also [15]). <p> rich literature on the complexity class NC (see [5, 14] for surveys and <ref> [15] </ref> for a general critique). Recently, there has been much interest in identifying problems that, though probably not in NC, admit at least polynomial speedup. Vitter and Simons [18] identified a number of such problems (see also [15]). In addition, finding parallel algorithms that achieve only polynomial speedup can be interesting even for problems in NC. <p> Assuming P 6= NC, one could try to classify problems in P NC with respect to the achievable speedup. For instance, [18] consider the class PC of problems that can be sped up by more than a constant factor. On the other hand, <ref> [15] </ref> focus on the problems that admit polynomial speedup, and classify these further with respect to their inefficiency 1 . They introduce the class EP, of problems solvable with constant inefficiency, and the class SP, of problems solvable with polynomial inefficiency.
Reference: [16] <editor> F. Meyer auf der Heide. </editor> <title> Lower bounds for solving linear Diophantine equations on Random Access Machines. </title> <journal> J. ACM, </journal> <volume> 32 </volume> <pages> 929-937, </pages> <year> 1985. </year>
Reference: [17] <author> S.A. Vavasis. </author> <title> Gaussian Elimination with Pivoting is P-complete. </title> <journal> SIAM J. Disc. Math., </journal> <volume> 2 </volume> <pages> 413-423, </pages> <year> 1989. </year>
Reference-contexts: Achieving numerical accuracy in finite precision computations seems to require a lot more of control than that provided by NC algorithms. Such control must be implemented using conditional statements which are in general hard to parallelize. Vavasis <ref> [17] </ref> proved that answering simple questions about the behavior of GEPP, such as whether a certain row i will be used to eliminate a given column j, is a log space-complete problem for the class P, and thus hardly in NC. <p> C C C C C C C C C C C C C C C C 20 5 A hardness result for Gaussian elimination Using the simulation of Section 4, we are now ready to prove our main Lemma. According to <ref> [17] </ref>, we formulate Gaussian Elimination as a language recognition problem in the following way: Given a matrix A and indexes i and j, will the Gaussian Elimination algorithm with Partial Pivoting use the pivot in (initial) row i to eliminate column j? Lemma 6 (Main Lemma) Let t (n) and s
Reference: [18] <author> J.S. Vitter and R.A. Simons. </author> <title> New classes for parallel complexity: a study of unification and other complete problems. </title> <journal> IEEE Trans. Comput., </journal> <volume> 35 </volume> <pages> 403-418, </pages> <year> 1986. </year> <month> 23 </month>
Reference-contexts: There is now a rich literature on the complexity class NC (see [5, 14] for surveys and [15] for a general critique). Recently, there has been much interest in identifying problems that, though probably not in NC, admit at least polynomial speedup. Vitter and Simons <ref> [18] </ref> identified a number of such problems (see also [15]). In addition, finding parallel algorithms that achieve only polynomial speedup can be interesting even for problems in NC. <p> Together with the algorithmic interest, there is an obvious interest in finding complexity results. Assuming P 6= NC, one could try to classify problems in P NC with respect to the achievable speedup. For instance, <ref> [18] </ref> consider the class PC of problems that can be sped up by more than a constant factor. On the other hand, [15] focus on the problems that admit polynomial speedup, and classify these further with respect to their inefficiency 1 .
Reference: [19] <author> J. Wiedermann. </author> <title> Normalizing and Accelerating RAM Computations and the Problem of Reasonable Space Measures. </title> <booktitle> Proc. of 17th ICALP, Lecture Notes in Computer Science, </booktitle> <volume> 433 </volume> <pages> 125-138, </pages> <year> 1990. </year>
Reference-contexts: This implies that a word length polynomial in the input size is sufficient. We observe, however, that this bound can be greatly improved by using the following result, due to Wiedermann. Lemma 5 <ref> [19] </ref> A RAM R working within time t (n) can be simulated, with only constant slowdown, by a RAM R' that uses integers of length O (log t (n)) and addresses of value O (t 2 (n)).
References-found: 19

