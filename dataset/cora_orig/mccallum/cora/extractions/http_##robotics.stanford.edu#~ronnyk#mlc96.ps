URL: http://robotics.stanford.edu/~ronnyk/mlc96.ps
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: http://www.robotics.stanford.edu
Email: ronnyk@engr.sgi.com  sommda@engr.sgi.com  jamesd@eng.sun.com  
Title: Data Mining using MLC A Machine Learning Library in C http://www.sgi.com/Technology/mlc  
Author: Ron Kohavi Dan Sommerfield James Dougherty 
Address: 2011 N. Shoreline Blvd Mountain View, CA 94043-1389  2011 N. Shoreline Blvd Mountain View, CA 94043-1389  MS-USUN03-306 2550 Garcia Ave. Mountain View, CA 94043  
Affiliation: Data Mining and Visualization Silicon Graphics, Inc.  Data Mining and Visualization Silicon Graphics, Inc.  Platform Group Sun Microsystems  
Note: Appeared in Tools With AI 1996, pages 234-245 Received the IEEE TAI Ramamoorthy Best Paper Award Minor modifications on 22-Nov-1996  
Abstract: Data mining algorithms including machine learning, statistical analysis, and pattern recognition techniques can greatly improve our understanding of data warehouses that are now becoming more widespread. In this paper, we focus on classification algorithms and review the need for multiple classification algorithms. We describe a system called MLC ++ , which was designed to help choose the appropriate classification algorithm for a given dataset by making it easy to compare the utility of different algorithms on a specific dataset of interest. MLC ++ not only provides a workbench for such comparisons, but also provides a library of C ++ classes to aid in the development of new algorithms, especially hybrid algorithms and multi-strategy algorithms. Such algorithms are generally hard to code from scratch. We discuss design issues, interfaces to other programs, and visualization of the resulting classifiers. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1992), </year> <title> `Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms', </title> <journal> International Journal of Man-Machine Studies 36(1), </journal> <pages> 267-287. </pages>
Reference: <author> Aha, D. W. </author> <note> (to appear), AI review journal: Special issue on lazy learning. </note>
Reference: <author> Ali, K. M. </author> <year> (1996), </year> <title> Learning Probabilistic Relational Concept Descriptions, </title> <type> PhD thesis, </type> <institution> University of Califor-nia, </institution> <address> Irvine. http://www.ics.uci.edu/ali. </address>
Reference: <author> Auer, P., Holte, R. & Maass, W. </author> <year> (1995), </year> <title> Theory and applications of agnostic PAC-learning with small decision trees, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Breiman, L. </author> <year> (1994), </year> <institution> Bagging predictors, Technical Report Statistics Department, University of California at Berkeley. </institution>
Reference: <author> Clark, P. & Boswell, R. </author> <year> (1991), </year> <title> Rule induction with CN2: Some recent improvements, </title> <editor> in Y. Kodratoff, ed., </editor> <booktitle> `Proceedings of the fifth European conference (EWSL-91)', </booktitle> <publisher> Springer Verlag, </publisher> <pages> pp. 151-163. </pages>
Reference: <author> Clark, P. & Niblett, T. </author> <year> (1989), </year> <title> `The CN2 induction algorithm', </title> <booktitle> Machine Learning 3(4), </booktitle> <pages> 261-283. </pages>
Reference: <author> Cost, S. & Salzberg, S. </author> <year> (1993), </year> <title> `A weighted nearest neighbor algorithm for learning with symbolic features', </title> <booktitle> Machine Learning 10(1), </booktitle> <pages> 57-78. </pages>
Reference: <author> Dasarathy, B. V. </author> <year> (1990), </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques, </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California. </address>
Reference-contexts: Nearest-neighbor The classical nearest-neighbor with options for weight setting, normalizations, and editing <ref> (Dasarathy 1990, Aha 1992, Wettschereck 1994) </ref>. Naive-Bayes A simple induction algorithm that assumes a conditional independence model of attributes given the label (Domingos & Pazzani 1996, Langley, Iba & Thompson 1992, Duda & Hart 1973, Good 1965). OODG Oblivous read-Once Decision Graph induction al gorithm described in Kohavi (1995c).
Reference: <author> Domingos, P. & Pazzani, M. </author> <year> (1996), </year> <title> Beyond independence: conditions for the optimality of the simple bayesian classifier, </title> <editor> in L. Saitta, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Thirteenth International Conference', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 105-112. </pages>
Reference: <author> Dougherty, J., Kohavi, R. & Sahami, M. </author> <year> (1995), </year> <title> Supervised and unsupervised discretization of continuous features, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 194-202. </pages>
Reference-contexts: Discretizers replace real-valued attributes in the database with discrete-valued attributes for algorithms which can only use discrete values. MLC ++ provides several discretization methods <ref> (Dougherty, Kohavi & Sahami 1995) </ref>. Programming with MLC ++ generally requires little coding. Because MLC ++ contains many well-tested modules and utility classes, the bulk of MLC ++ programming is determining how to use the existing code base to implement new functionality.
Reference: <author> Duda, R. & Hart, P. </author> <year> (1973), </year> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley. </publisher>
Reference: <author> Efron, B. & Tibshirani, R. </author> <year> (1995), </year> <title> Cross-validation and the bootstrap: Estimating the error rate of a prediction rule, </title> <type> Technical Report 477, </type> <institution> Stanford University, Statistics department. </institution>
Reference: <author> Fayyad, U. M., Piatetsky-Shapiro, G. & Smyth, P. </author> <year> (1996), </year> <title> From data mining to knowledge discovery: An overview, in `Advances in Knowledge Discovery and Data Mining', </title> <publisher> AAAI Press and the MIT Press, </publisher> <pages> chapter 1, pp. 1-34. </pages>
Reference-contexts: Other classifiers, such as nearest-neighbor algorithms and other lazy algorithms (see Aha (to appear) for details), are usually fast to train but slow in classification. Given these factors, one can define a utility function to rank different algorithms <ref> (Fayyad, Piatetsky-Shapiro & Smyth 1996) </ref>. The last step is to test drive the algorithms and note their utility for your specific domain problem.
Reference: <author> Friedman, J., Kohavi, R. & Yun, Y. </author> <year> (1996), </year> <title> Lazy decision trees, </title> <booktitle> in `Proceedings of the Thirteenth National Conference on Artificial Intelligence', </booktitle> <publisher> AAAI Press and the MIT Press. </publisher>
Reference: <author> Good, I. J. </author> <year> (1965), </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods, </title> <publisher> M.I.T. Press. </publisher>
Reference: <author> Hertz, J., Krogh, A. & Palmer, R. G. </author> <year> (1991), </year> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison Wesley. </publisher>
Reference: <author> Holte, R. C. </author> <year> (1993), </year> <title> `Very simple classification rules perform well on most commonly used datasets', </title> <booktitle> Machine Learning 11, </booktitle> <pages> 63-90. </pages>
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <booktitle> in `Machine Learning: Proceedings of the Eleventh International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 121-129. </pages>
Reference-contexts: Feature selection methods run a search using the inducer itself to determine which attributes 3 in the database are useful for learning. The wrapper ap-proach to feature selection automatically tailors the selection to the inducer being run <ref> (John, Kohavi & Pfleger 1994) </ref>. A voting wrapper runs an algorithm on different portions of the dataset and lets them vote on the predicted class (Wolpert 1992, Breiman 1994, Perrone 1993, Ali 1996).
Reference: <author> Kohavi, R. </author> <year> (1995a), </year> <title> The power of decision tables, </title> <editor> in N. Lavrac & S. Wrobel, eds, </editor> <booktitle> `Proceedings of the Eu-ropean Conference on Machine Learning', Lecture Notes in Artificial Intelligence 914, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <pages> pp. 174-189. </pages>
Reference-contexts: A Perceptron (single neuron) might be a compact classifier, yet given an instance, it may be hard to understand the labelling process. Alternatively, a decision table <ref> (Kohavi 1995a) </ref> may be very large, yet labelling each instance is triv ial: one simply looks it up in the table. Training and classification time The time it takes to classify versus the training time. Some classifiers, such as neural networks are fast to classify but slow to train. <p> The following inducers are created as combinations of others: IDTM Induction of Decision Tables with Majority. A feature subset selection wrapper on top of decision tables <ref> (Kohavi 1995a, Kohavi 1995c) </ref>. C4.5-auto Automatic parameter setting for C4.5 (Kohavi & John 1995). FSS Naive-Bayes Feature subset selection on top of Naive Bayes (Kohavi & Sommerfield 1995). NBTree A decision tree hybrid with Naive-Bayes at the leaves (Kohavi 1996).
Reference: <author> Kohavi, R. </author> <year> (1995b), </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> `Proceedings of the 14th International Joint Conference on Artificial Intelligence', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 1137-1143. </pages>
Reference-contexts: The two most important wrappers in MLC ++ are accuracy estimators and feature selectors. Accuracy estimators use any of a range of methods, such as holdout, cross-validation, or bootstrap to estimate the performance of an inducer <ref> (Kohavi 1995b) </ref>. Feature selection methods run a search using the inducer itself to determine which attributes 3 in the database are useful for learning. The wrapper ap-proach to feature selection automatically tailors the selection to the inducer being run (John, Kohavi & Pfleger 1994).
Reference: <author> Kohavi, R. </author> <year> (1995c), </year> <title> Wrappers for Performance Enhancement and Oblivious Decision Graphs, </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Science department. STAN-CS-TR-95-1560, ftp://starry.stanford.edu/pub/ronnyk/teza.ps. </institution>
Reference: <author> Kohavi, R. </author> <year> (1996), </year> <title> Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid, </title> <booktitle> in `Proceedings of the Second International Conference on Knowledge Discovery and Data Mining', </booktitle> <address> p. </address> <note> to appear. 11 Kohavi, </note> <author> R. & John, G. </author> <year> (1995), </year> <title> Automatic parameter selec-tion by minimizing estimated error, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kauf-mann Publishers, Inc., </publisher> <pages> pp. 304-312. </pages>
Reference-contexts: A feature subset selection wrapper on top of decision tables (Kohavi 1995a, Kohavi 1995c). C4.5-auto Automatic parameter setting for C4.5 (Kohavi & John 1995). FSS Naive-Bayes Feature subset selection on top of Naive Bayes (Kohavi & Sommerfield 1995). NBTree A decision tree hybrid with Naive-Bayes at the leaves <ref> (Kohavi 1996) </ref>. The ability to create hybrid algorithms and wrapped algorithms is a very important and powerful approach for mul-tistrategy learning.
Reference: <author> Kohavi, R., John, G., Long, R., Manley, D. & Pfleger, K. </author> <year> (1994), </year> <title> MLC++: A machine learning library in C++, </title> <booktitle> in `Tools with Artificial Intelligence', </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 740-743. </pages> <address> http://www.sgi.com/Technology/mlc. </address>
Reference-contexts: Feature selection methods run a search using the inducer itself to determine which attributes 3 in the database are useful for learning. The wrapper ap-proach to feature selection automatically tailors the selection to the inducer being run <ref> (John, Kohavi & Pfleger 1994) </ref>. A voting wrapper runs an algorithm on different portions of the dataset and lets them vote on the predicted class (Wolpert 1992, Breiman 1994, Perrone 1993, Ali 1996).
Reference: <author> Kohavi, R. & Sommerfield, D. </author> <year> (1995), </year> <title> Feature subset selection using the wrapper model: Overfitting and dynamic search space topology, </title> <booktitle> in `The First International Conference on Knowledge Discovery and Data Mining', </booktitle> <pages> pp. 192-197. </pages>
Reference-contexts: The following inducers are created as combinations of others: IDTM Induction of Decision Tables with Majority. A feature subset selection wrapper on top of decision tables (Kohavi 1995a, Kohavi 1995c). C4.5-auto Automatic parameter setting for C4.5 <ref> (Kohavi & John 1995) </ref>. FSS Naive-Bayes Feature subset selection on top of Naive Bayes (Kohavi & Sommerfield 1995). NBTree A decision tree hybrid with Naive-Bayes at the leaves (Kohavi 1996). The ability to create hybrid algorithms and wrapped algorithms is a very important and powerful approach for mul-tistrategy learning. <p> A feature subset selection wrapper on top of decision tables (Kohavi 1995a, Kohavi 1995c). C4.5-auto Automatic parameter setting for C4.5 (Kohavi & John 1995). FSS Naive-Bayes Feature subset selection on top of Naive Bayes <ref> (Kohavi & Sommerfield 1995) </ref>. NBTree A decision tree hybrid with Naive-Bayes at the leaves (Kohavi 1996). The ability to create hybrid algorithms and wrapped algorithms is a very important and powerful approach for mul-tistrategy learning. <p> Discretizers replace real-valued attributes in the database with discrete-valued attributes for algorithms which can only use discrete values. MLC ++ provides several discretization methods <ref> (Dougherty, Kohavi & Sahami 1995) </ref>. Programming with MLC ++ generally requires little coding. Because MLC ++ contains many well-tested modules and utility classes, the bulk of MLC ++ programming is determining how to use the existing code base to implement new functionality.
Reference: <author> Kononenko, I. </author> <year> (1993), </year> <title> `Inductive and bayesian learning in medical diagnosis', </title> <booktitle> Applied Artificial Intelligence 7, </booktitle> <pages> 317-337. </pages>
Reference-contexts: In such domains, a certain class of learning algorithms might outperform others. For example, Naive-Bayes seems to be a good performer in medical domains <ref> (Kononenko 1993) </ref>. Quinlan (1994) identifies families of parallel and sequential domains and claims that neural-networks are likely to perform well in parallel domains, while decision-tree algorithms are likely to perform well in sequential domains.
Reference: <author> Koutsofios, E. & North, S. C. </author> <year> (1994), </year> <title> Drawing graphs with dot. </title> <note> Available by anonymous ftp from research.att.com:dist/drawdag/dotdoc.ps.Z. </note>
Reference-contexts: Finally, the Conversion utility changes multi-valued nominal attributes to local or binary encodings which may be more useful for nearest-neighbor or neural-network algorithms. 3.4 Visualization Some induction algorithms support visual output of the classifiers. All graph-based algorithms support can display two-dimensional representations of their graphs using dot and dotty <ref> (Koutsofios & North 1994) </ref>. Dotty is also capable of showing extra information at each node, such as class distributions. Figure 1 shows such a graph. The decision tree algorithms, such as ID3, may generate output for Silicon Graphics' MineSet TM product.
Reference: <author> Langley, P., Iba, W. & Thompson, K. </author> <year> (1992), </year> <title> An analysis of bayesian classifiers, </title> <booktitle> in `Proceedings of the tenth national conference on artificial intelligence', </booktitle> <publisher> AAAI Press and MIT Press, </publisher> <pages> pp. 223-228. </pages>
Reference: <author> Littlestone, N. </author> <year> (1988), </year> <title> `Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm', </title> <booktitle> Machine Learning 2, </booktitle> <pages> 285-318. </pages>
Reference: <author> Michalski, R. S. </author> <year> (1978), </year> <title> A planar geometric model for representing multidimensional discrete spaces and multiple-valued logic functions, </title> <type> Technical Report UIUCDCS-R-78-897, </type> <institution> University of Illinois at Urbaba-Champaign. </institution>
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1996), </year> <note> UCI repository of machine learning databases. http://www.ics.uci.edu/mlearn/MLRepository.html. </note>
Reference-contexts: In the second part of the paper we describe the MLC ++ system and its dual role as a system for end-users and algorithm developers. We show a large comparison of 17 algorithms on eight large datasets for the UC Irvine repository <ref> (Murphy & Aha 1996) </ref>. A study of this magnitude would be extremely hard to conduct without such a tool. With MLC ++ , it was mostly a matter of CPU cycles and a few scripts to parse the output. <p> Converting data formats, learning the different algorithms, and running many of them is usually out of the question. MLC ++ , however, can easily provide such a comparison. In this section we present a comparison on some large datasets from the UC Irvine repository <ref> (Murphy & Aha 1996) </ref>. We also take this opportunity to correct some misunderstandings of results in Holte (1993). Table 1 shows the basic characteristics of the chosen domains. Instances with unknown values were removed from the original datasets.
Reference: <author> Murthy, S. K., Kasif, S. & Salzberg, S. </author> <year> (1994), </year> <title> `A system for the induction of oblique decision trees', </title> <journal> Journal of Artificial Intelligence Research 2, </journal> <pages> 1-33. </pages>
Reference: <author> Naeher, S. </author> <year> (1996), </year> <title> LEDA: A Library of Efficient Data Types and Algorithms, </title> <institution> 3.3 edn, Max-Planck-Institut fuer In-formatik, </institution> <address> IM Stadtwald, D-66123 Saarbruecken, FRG. http://www.mpi-sb.mpg.de/LEDA/leda.html. </address>
Reference-contexts: MCore classes not only provide a wide range of functions, they provide solid tests of these functions, reducing the time it takes to build more complex operations. All general-purpose classes used within the full library are from MCore, with the exception of graph classes from the LEDA library <ref> (Naeher 1996) </ref>. Although the classes in MCore were built for use by the MLC ++ library, they are not tied to machine learning and may be used as a general purpose library. MCore is a separate library which may be linked independently of the whole library. <p> Moreover, most PC-based compilers still cannot compile MLC ++ . We hope that this will change as 32-bit operating systems mature and compilers improve. We quickly chose LEDA <ref> (Naeher 1996) </ref> for graph manipulation and algorithms, and GNU's libg++, which provided some basic data structures. Unfortunately, the GNU library was found to be deficient in many respects. It hasn't kept up with the emerging C ++ standards (e.g., constness issues, templates) and we slowly rewrote all the classes used.
Reference: <author> Perrone, M. </author> <year> (1993), </year> <title> Improving regression estimation: averaging methods for variance reduction with extensions to general convex measure optimization, </title> <type> PhD thesis, </type> <institution> Brown University, Physics Dept. </institution>
Reference: <author> Quinlan, J. R. </author> <year> (1986), </year> <title> `Induction of decision trees', </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106. </pages> <note> Reprinted in Shavlik and Diet-terich (eds.) Readings in Machine Learning. </note>
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, California. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1994), </year> <title> Comparing connectionist and symbolic learning methods, </title> <editor> in S. J. Hanson, G. A. Drastal & R. L. Rivest, eds, </editor> <title> `Computational Learning Theory and Natural Learning Systems', Vol. I: Constraints and Prospects, </title> <publisher> MIT Press, </publisher> <pages> chapter 15, pp. 445456. </pages>
Reference: <author> Schaffer, C. </author> <year> (1994), </year> <title> A conservation law for generalization performance, </title> <booktitle> in `Machine Learning: Proceedings of the Eleventh International Conference', </booktitle> <publisher> Morgan Kauf-mann Publishers, Inc., </publisher> <pages> pp. 259-265. </pages>
Reference-contexts: This result, sometimes called the No Free Lunch Theorem or Conservation Law <ref> (Wolpert 1994, Schaffer 1994) </ref>, assumes that all possible targets are equally likely.
Reference: <author> Stroustroup, B. </author> <year> (1994), </year> <title> The Design and Evolution of C ++ , Addison-Wesley Publishing Company. </title>
Reference-contexts: We assumed that GNU's g++ compiler would catch up. In 1987 Mike Tiemann gave a most animated and interesting presentation of how the GNU C ++ compiler he was building would do just about everything and put all other C ++ compiler writers out of business <ref> (Stroustroup 1994) </ref>. Sadly, the GNU C ++ compiler is still weaker than most commercial grade compilers and (at least as of 1995) cannot handle templates well enough. Moreover, most PC-based compilers still cannot compile MLC ++ .
Reference: <author> Taylor, C., Michie, D. & Spiegalhalter, D. </author> <year> (1994), </year> <title> Machine Learning, Neural and Statistical Classification, </title> <publisher> Paramount Publishing International. </publisher>
Reference-contexts: The height of each bar represents the evidence in favor of a class given that a single attribute is set to a specific value. Figure 3 shows a snapshot of the visualization. 3.5 A Global Comparison Statlog <ref> (Taylor, Michie & Spiegalhalter 1994) </ref> was a large project that compared about 20 different learning algorithms on 20 datasets. The project, funded by the ESPRIT program of the European Community, lasted from October 4 5 1990 to June 1993.
Reference: <author> Thrun et al. </author> <year> (1991), </year> <title> The Monk's problems: A performance comparison of different learning algorithms, </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Weiss, S. M. & Kulikowski, C. A. </author> <year> (1991), </year> <title> Computer Systems that Learn, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA. </address>
Reference-contexts: For example, whether a customer will be able to pay a loan or whether he or she will respond to a yet another credit card offer. Using methods such as holdout, bootstrap, and cross-validation <ref> (Weiss & Kulikowski 1991, Efron & Tibshirani 1995, Kohavi 1995b) </ref>, one can estimate the future prediction accuracy on unseen data quite well in practice. Comprehensibility The ability for humans to understand the data and the classification rules induced by the learning algorithm.
Reference: <author> Wettschereck, D. </author> <year> (1994), </year> <title> A Study of Distance-Based Machine Learning Algorithms, </title> <type> PhD thesis, </type> <institution> Oregon State University. </institution>
Reference: <author> Wnek, J. & Michalski, R. S. </author> <year> (1994), </year> <title> `Hypothesis-driven constructive induction in AQ17-HCI : A method and experiments', </title> <booktitle> Machine Learning 14(2), </booktitle> <pages> 139-168. </pages>
Reference: <author> Wolpert, D. H. </author> <year> (1992), </year> <title> `Stacked generalization', </title> <booktitle> Neural Networks 5, </booktitle> <pages> 241-259. </pages>
Reference-contexts: The wrapper ap-proach to feature selection automatically tailors the selection to the inducer being run (John, Kohavi & Pfleger 1994). A voting wrapper runs an algorithm on different portions of the dataset and lets them vote on the predicted class <ref> (Wolpert 1992, Breiman 1994, Perrone 1993, Ali 1996) </ref>. A discretization wrapper pre-discretizes the data, allowing algorithms that do not support continuous features (or those that do not handle them well) to work properly.
Reference: <author> Wolpert, D. H. </author> <year> (1994), </year> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework, </title> <editor> in D. H. Wolpert, ed., </editor> <title> `The Mathemtatics of Generalization', </title> <publisher> Addison Wes-ley. </publisher> <pages> 12 </pages>
Reference-contexts: This result, sometimes called the No Free Lunch Theorem or Conservation Law <ref> (Wolpert 1994, Schaffer 1994) </ref>, assumes that all possible targets are equally likely.
References-found: 46

