URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/93/tr1171.ps.Z
Refering-URL: http://www.cs.wisc.edu/~arch/uwarch/tech_reports/tech_reports.html
Root-URL: 
Email: larus@cs.wisc.edu  
Title: CICO: A Practical Shared-Memory Programming Performance Model  
Author: James R. Larus, Satish Chandra, David A Wood 
Date: August 16, 1993  
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Pubnum: 608-262-9519  
Abstract: A programming performance model provides a programmer with feedback on the cost of program operations and is a necessary basis to write efficient programs. Many shared-memory performance models do not accurately capture the cost of interprocessor communication caused by non-local memory references, particularly in computers with caches. This paper describes a simple and practical programming performance model|called check-in, check-out (CICO)|for cache-coherent, shared-memory parallel computers. CICO consists of two components. The first is a collection of annotations that a programmer adds to a program to elucidate the communication arising from shared-memory references. The second is a model that calculates the communication cost of these annotations. An annotation's cost models the cost of the memory references that it summarizes and serves as a metric to compare alternative implementations. Several examples demonstrate that CICO accurately predicts cache misses and identifies changes that improve program performance.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> June </month> <year> 1990. </year>
Reference: [2] <author> Anant Agarwal, Richard Simoni, Mark Horowitz, and John Hennessy. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <year> 1988. </year>
Reference-contexts: More complex performance models account for interconnection network topology [7]. 2 CICO Shared-Memory Performance Model Unlike message-passing programs, communication in shared-memory programs is not easily identifiable. In cache-coherent, shared-memory computers, interprocessor communication occurs when a memory reference misses in a cache and the hardware coherence protocol <ref> [2, 5] </ref> requests a copy of the referenced datum from another processor, which may cause outstanding copies to be invalidated. <p> Simple hardware entirely handles programs conforming to the CICO model by updating the pointer/counter and forwarding data to a requesting processor. Programs not conforming to the model run correctly, but cause traps to software trap handlers that perform more complex operations, similar to MIT Alewife <ref> [2] </ref>. Dir 1 SW performs better if programs flush unneeded blocks from caches with check-ins. If these blocks remain in a processor's cache, subsequent memory references would cause the protocol to trap to software to handle the exchange of messages necessary to invalidate copies of a block.
Reference: [3] <author> Alok Aggarwal, Ashok K. Chandra, and Marc Snir. </author> <title> On Communication Latency in PRAM Computations. </title> <booktitle> In Proceedings of the First ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 11-21, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: These assumptions simplify analysis, but do not reflect real computers, particularly those with caches. For this reason, PRAM extensions model non-uniform memory and processor asynchrony <ref> [3, 10, 33, 20] </ref>. The new models are more descriptive and complex than traditional PRAM models, but still do not accurately describe cache-coherent parallel computers. Perhaps this paper will help inspire PRAM extensions that describe this important type of computers.
Reference: [4] <author> Bowen Alpern, Larry Carter, Ephraim Feig, and Ted Selker. </author> <title> The Uniform Memory Hierarchy Model of Computation. </title> <note> Submitted for publication, </note> <year> 1992. </year>
Reference-contexts: Instead, they spent six person--months running experiments to understand this 1,500 line program. Alpern et al. describe a performance model, called the uniform memory hierarchy (UMH), for hierarchical uniprocessor memories <ref> [4] </ref>. This model is an abstract, but detailed description of a computer's memory hierarchy, starting at registers and continuing through virtual memory's paging storage. Unlike CICO, UMH models all levels in a uniprocessor's memory hierarchy and is concerned with the bandwidth between each level.
Reference: [5] <author> J. Archibald and J.-L. Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <year> 1986. </year>
Reference-contexts: More complex performance models account for interconnection network topology [7]. 2 CICO Shared-Memory Performance Model Unlike message-passing programs, communication in shared-memory programs is not easily identifiable. In cache-coherent, shared-memory computers, interprocessor communication occurs when a memory reference misses in a cache and the hardware coherence protocol <ref> [2, 5] </ref> requests a copy of the referenced datum from another processor, which may cause outstanding copies to be invalidated.
Reference: [6] <author> C. Gordon Bell. Multis: </author> <title> A New Class of Multiprocessor Computers. </title> <journal> Science, </journal> <volume> 228 </volume> <pages> 462-466, </pages> <year> 1985. </year>
Reference-contexts: A major issue in these systems is keeping cached copies consistent as processors modify memory locations. Solutions to this cache coherent problem distinguishes several classes of parallel computers. Multis <ref> [6] </ref> are bus-based computers in which all processors watch memory accesses occurring on a shared bus and modify their caches appropriately.
Reference: [7] <author> Mark Bromley, Steven Heller, Tim McNerney, and Guy L. Steele Jr. </author> <title> Fortran at Ten Gigaflops: The Connection Machine Convolution Compiler. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 145-156, </pages> <month> June </month> <year> 1991. </year> <month> 17 </month>
Reference-contexts: Although crude, this model identifies communication as a major bottleneck and suggests effective optimizations such as combining multiple sends into a single message and overlapping communication and computation. More complex performance models account for interconnection network topology <ref> [7] </ref>. 2 CICO Shared-Memory Performance Model Unlike message-passing programs, communication in shared-memory programs is not easily identifiable.
Reference: [8] <author> J. Cheong and A.V. Veidenbaum. </author> <title> A Cache Coherence Scheme With Fast Selective Invalidation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 299-307, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Since they are declarative, not imperative, they do not affect a program's semantics or a cache's operation. The programmer uses annotations to describe and reason about cache behavior, not to implement software cache coherence (cf. <ref> [8, 13, 31] </ref>). The advantage of summarizing cache behavior with these annotations is that the programmer only needs to study the communication at the annotations and need not reason about every memory reference.
Reference: [9] <author> David R. Cheriton, Hendrik A. Goosen, and Philip Machanick. </author> <title> Restructuring a Parallel Simulation to Improve Cache Behavior in a Shared-Memory Multiprocessor: A First Experience. </title> <booktitle> In International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pages 109-118, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: CICO draws from the strengths of both paradigms. It, like message passing, focuses on optimizing communication and data reuse, but still retains the flexibility and load-balancing of shared memory. Cheriton et al. describe the changes they made to MP3D [37], a three-dimensional particle simulator to improve its cache performance <ref> [9] </ref>. The changes eliminated false sharing by changing the data structures to increase their "processor affinity" and hence reduce communication. The changes increased execution speed by a factor of 3-4 on small-scale multiprocessors.
Reference: [10] <author> Richard Cole and Ofer Zajicek. </author> <title> The APRAM: Incorporating Asynchrony into the PRAM Model. </title> <booktitle> In Proceedings of the First ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 169-178, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: These assumptions simplify analysis, but do not reflect real computers, particularly those with caches. For this reason, PRAM extensions model non-uniform memory and processor asynchrony <ref> [3, 10, 33, 20] </ref>. The new models are more descriptive and complex than traditional PRAM models, but still do not accurately describe cache-coherent parallel computers. Perhaps this paper will help inspire PRAM extensions that describe this important type of computers.
Reference: [11] <author> R.C. Covington, S. Madala, V. Mehta, J.R. Jump, and J.B. Sinclair. </author> <title> The Rice Parallel Processing Testbed. </title> <booktitle> In Proceedings of the 1988 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4-11, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: The protocol's operation (and cost), in turn, depends on the memory's state. An accurate, but slow and complex method of calculating the cost of a memory access is to simulate a particular machine in detail and record which statements cause interprocessor communication <ref> [11, 14, 35] </ref>. Simulation, because it is so time-consuming, is generally limited to studying short programs with small data sets. For many programmers, a more attractive approach would trade accuracy for simplicity.
Reference: [12] <author> David Culler, Richard Karp, David Patterson, Abhijit Sahay, Klaus Erik Schauser, Eunice Santos, Rahesh Subramonian, and Thorsten von Eicken. </author> <title> LogP: Toward a Realistic Model of Parallel Computation. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Perhaps this paper will help inspire PRAM extensions that describe this important type of computers. Culler et al. described another model of parallel computation, called LogP, that is closer to actual parallel computers <ref> [12] </ref>. Like CICO, LogP is an abstraction of real machines intended to provide programmers with insight into potential bottlenecks without falling into excessive detail. The LogP model is based on four parameters that describe the cost of communication in a parallel computer. LogP, however, differs from CICO in several ways.
Reference: [13] <author> Ron Cytron, Steve Karlovsky, and Kevin P. McAuliffe. </author> <title> Automatic Management of Programmable Caches. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages 229-238, </pages> <month> Aug 188. </month>
Reference-contexts: Since they are declarative, not imperative, they do not affect a program's semantics or a cache's operation. The programmer uses annotations to describe and reason about cache behavior, not to implement software cache coherence (cf. <ref> [8, 13, 31] </ref>). The advantage of summarizing cache behavior with these annotations is that the programmer only needs to study the communication at the annotations and need not reason about every memory reference.
Reference: [14] <author> Helen Davis, Stephen R. Goldschmidt, and John Hennessy. </author> <title> Multiprocessor Simulation and Tracing Using Tango. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II99-107, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The protocol's operation (and cost), in turn, depends on the memory's state. An accurate, but slow and complex method of calculating the cost of a memory access is to simulate a particular machine in detail and record which statements cause interprocessor communication <ref> [11, 14, 35] </ref>. Simulation, because it is so time-consuming, is generally limited to studying short programs with small data sets. For many programmers, a more attractive approach would trade accuracy for simplicity.
Reference: [15] <author> Susan J. Eggers and Randy H. Katz. </author> <title> A Characterization of Sharing in Parallel Programs and its Application to Coherency Protocol Evaluation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 373-382, </pages> <year> 1988. </year>
Reference-contexts: Shared-memory communication is the interprocessor message traffic caused by cache misses and invalidations. One part is the coherence traffic when processors exchange values or appear to exchange values because of false sharing <ref> [15] </ref>. The other part is the conflict and capacity misses caused by caches of finite size and associativity [23]. A programmer or compiler can reduce both aspects of shared-memory communication by modifying a program and its data structures to use caches more effectively. <p> A program is cache-block race-free if it contains no data races and no unsynchronized false sharing <ref> [15] </ref>. Consequently, when two processors access the same cache block, they must execute a synchronization event between their accesses. These rules place CICO annotations in race-free programs so as to capture the interprocessor communication caused by a cache-coherence protocol.
Reference: [16] <author> Faith E. Fich and Prabhakar L Ragde. </author> <title> Relations Between Concurrent-Write Models of Parallel Computation. </title> <booktitle> In Proceedings of Principals of Distributed Computing, </booktitle> <pages> pages 179-190, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: The discrepancy is even larger on more recent machines|for example, 100 times on Stanford DASH [27]|and is likely to continue increasing. Theoretical models, such as PRAMs <ref> [17, 16, 38] </ref>, abstract too far from real machines to provide a practical programming model, and consequently do not model features of real machines, such as caches. <p> Many theoretical models of shared-memory computers exist. One of the most well-known is parallel random access machine (PRAM) model [17]. A PRAM has a single globally-addressable memory and n processors that operate in lock-step read, compute, write cycles. Although PRAM models handle simultaneous reads or writes differently <ref> [16, 38] </ref>, they all assume that memory accesses are unit cost and that synchronization is unnecessary (because processors run in lock-step). These assumptions simplify analysis, but do not reflect real computers, particularly those with caches. For this reason, PRAM extensions model non-uniform memory and processor asynchrony [3, 10, 33, 20].
Reference: [17] <author> Stephen Fortune and James Wyllie. </author> <title> Parallelism in Random Access Machines. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <year> 1978. </year>
Reference-contexts: The discrepancy is even larger on more recent machines|for example, 100 times on Stanford DASH [27]|and is likely to continue increasing. Theoretical models, such as PRAMs <ref> [17, 16, 38] </ref>, abstract too far from real machines to provide a practical programming model, and consequently do not model features of real machines, such as caches. <p> The extent to which CICO accurately captures shared-memory costs needs to be more fully explored, but the examples in Section 3 show that CICO is useful even with simple assumptions. Many theoretical models of shared-memory computers exist. One of the most well-known is parallel random access machine (PRAM) model <ref> [17] </ref>. A PRAM has a single globally-addressable memory and n processors that operate in lock-step read, compute, write cycles. Although PRAM models handle simultaneous reads or writes differently [16, 38], they all assume that memory accesses are unit cost and that synchronization is unnecessary (because processors run in lock-step).
Reference: [18] <author> Hans Michael Gerndt. </author> <title> Automatic Parallelization for Distributed-Memory Multiprocessor Systems. </title> <type> PhD thesis, </type> <institution> Rheinischen Friedrich-Wilhelms-Universitat, </institution> <year> 1989. </year>
Reference-contexts: These references cause traps, which invoke system software that copies pages of data among processors. Another approach is to have compiled code provide a shared address space on a message-passing machine. Compilers, such as Vienna Fortran <ref> [18, 41] </ref> and Fortran D [24], implement a shared address space by detecting remote memory references (primarily at compile time) and sending data between processors with explicit messages. Purely software solutions perform poorly for many programs, so many forms of shared-memory hardware have been proposed and built.
Reference: [19] <author> Hans Michael Gerndt and Hans Peter Zima. </author> <title> Optimizing Communications in SUPERB. </title> <type> Technical Report ACPC/TR 90-3, </type> <institution> ACPC - Austrian Center for Parallel Computation, University of Vienna, </institution> <year> 1990. </year>
Reference-contexts: In other programs, however, these factors greatly complicate message-passing programming. By exposing the underlying hardware primitives, message passing operations identify in-terprocessor communication and facilitates performance modeling. A simple model, which is commonly used by compilers <ref> [19, 34] </ref>, attributes a uniformly high cost to messages, independent of their size or destination. Although crude, this model identifies communication as a major bottleneck and suggests effective optimizations such as combining multiple sends into a single message and overlapping communication and computation.
Reference: [20] <author> Phillip B. Gibbons. </author> <title> A More Practical PRAM Model. </title> <booktitle> In Proceedings of the First ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 158-168, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: These assumptions simplify analysis, but do not reflect real computers, particularly those with caches. For this reason, PRAM extensions model non-uniform memory and processor asynchrony <ref> [3, 10, 33, 20] </ref>. The new models are more descriptive and complex than traditional PRAM models, but still do not accurately describe cache-coherent parallel computers. Perhaps this paper will help inspire PRAM extensions that describe this important type of computers.
Reference: [21] <author> Mark D. Hill and James R. Larus. </author> <title> Cache Considerations for Programmers of Multiprocessors. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 97-102, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Unfortunately, no practical and accurate programming performance model exists for shared memory. The most common model is naive shared memory, which assumes all memory references are equally cheap (or costly). Even on a simple, shared-bus multiprocessor, such as the Sequent Symmetry, this model does not accurately capture hardware behavior <ref> [21] </ref>. For example, on this computer, a memory reference that hits in the local cache is roughly 20 times faster than a reference to main memory. The discrepancy is even larger on more recent machines|for example, 100 times on Stanford DASH [27]|and is likely to continue increasing. <p> However, from the description, it is unclear whether UMH adequately describes cache-coherent shared memory and whether the details of the lower levels are necessary to reason about shared-memory references. Hill and Larus describe a sequence of simple, qualitative models for cache-coherent, shared-bus computers (Multis) <ref> [21] </ref>. The sequence of models forms an increasingly precise description of the cache-coherence protocols used in Multis. The models enable a programmer to predict how the hardware performs for different patterns of memory references.
Reference: [22] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 262-273, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: This paper describes in detail the Check-In, Check-Out (CICO) model for reasoning about shared-memory communication in cache-coherence parallel computers. The model has been briefly mentioned elsewhere <ref> [22] </ref>. The model is less precise, but far easier to understand and employ than a detailed description of a particular cache-coherence protocol. <p> The advantage of summarizing cache behavior with these annotations is that the programmer only needs to study the communication at the annotations and need not reason about every memory reference. Moreover, the same annotations can also function as directives to a memory system designed to exploit them <ref> [22] </ref>. In this case, these annotations provide a mechanism by which 4 a program can inform the memory system of upcoming memory references, so the mem-ory system can anticipate them to improve performance. <p> We used CICO to analyze matrix multiplication and modified the code as described above. We then ran the original and modified programs on a detailed simulation of a 32 processor machine running the original Dir 1 SW protocol <ref> [22] </ref>. The simulation ran on the Wisconsin Wind Tunnel, which is a detailed and accurate parallel architecture simulator that runs on a Thinking Machines CM-5 computer [35]. <p> They can be passed as directives to a memory system to improve program performance. For example, Dir 1 SW <ref> [22] </ref> 14 is a minimal directory protocol that adds little complexity to the hardware of a message-passing machine, but efficiently supports programs written within the CICO model [40]. <p> The paper also presented a few rules of thumb (e.g, avoid false sharing) and explained their rationale in terms of the model. Hill et al. described a preliminary version of CICO and showed that simple directory hardware could use the annotations to improve program performance <ref> [22, 40] </ref>. The earlier version of CICO was described in terms of Dir 1 SW hardware, did not provide a cost model, and was used to reduce Dir 1 SW traps rather than improve program performance.
Reference: [23] <author> Mark D. Hill and Alan Jay Smith. </author> <title> Evaluating Associativity in CPU Caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-38(12):1612-1630, </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: One part is the coherence traffic when processors exchange values or appear to exchange values because of false sharing [15]. The other part is the conflict and capacity misses caused by caches of finite size and associativity <ref> [23] </ref>. A programmer or compiler can reduce both aspects of shared-memory communication by modifying a program and its data structures to use caches more effectively. However, to make such a change, a programmer or compiler must understand why shared-memory communication occurs and be able to evaluate alternative program organizations. <p> To distinguish the two uses of annotations, we will call them directives when they pass information at run time. 2. Processor caches are fully associative. Uniprocessor cache misses occur because caches have finite size and associativity <ref> [23] </ref>. Unlike capacity misses due to finite size, conflict misses due to limited associativity are difficult to understand and predict because they depend on the relative location of objects in memory. 3.
Reference: [24] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD Distributed-Memory Machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: These references cause traps, which invoke system software that copies pages of data among processors. Another approach is to have compiled code provide a shared address space on a message-passing machine. Compilers, such as Vienna Fortran [18, 41] and Fortran D <ref> [24] </ref>, implement a shared address space by detecting remote memory references (primarily at compile time) and sending data between processors with explicit messages. Purely software solutions perform poorly for many programs, so many forms of shared-memory hardware have been proposed and built.
Reference: [25] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 63-74, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The annotations for A and C execute N 3 =P times, checking out 1=b blocks each time. The total number of checked-out blocks is (2N 3 + N 2 )=b. Blocking (also called tiling) is a well-known technique for reducing communication in this algorithm <ref> [25] </ref>. It breaks the problem into multiplications of smaller submatrices. Because the submatrices fit in the cache, array elements are repeatedly accessed locally rather than remotely fetched. <p> The first check-out accesses T 2 =b blocks and the other two check-out's access T =b blocks. The program accesses a total of (2N 3 =T + P N 2 )=b blocks. Blocking speeds matrix multiplication on real computers <ref> [25] </ref> by reducing the number of cache blocks accessed by a factor of 2=T , in the limit. 3.1.1 Experimental Results This section contains preliminary results from some experiments run to test the CICO model. We used CICO to analyze matrix multiplication and modified the code as described above.
Reference: [26] <author> Thomas J. LeBlanc and Evangelos P. Markatos. </author> <title> Shared Memory Vs. Message Passing in Shared-Memory Multiprocessors. </title> <booktitle> In Fourth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <address> page ?, Dallas, TX, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Shared memory in parallel computers provides the valuable abstraction of a shared address space in which processors communicate by reading and writing memory locations. This shared name space simplifies many programs by making data structures processor-independent, which facilitates load balancing <ref> [26] </ref>; allowing pointer-based data structures, which are necessary for sophisticated algorithms; and freeing programs from per-processor memory limits, which permits fl Presented at: Workshop on Portability and Performance for Parallel Processing, Southampton University, England, July 13-15, 1993. To appear: Ferrante & Hey eds., Portability and Performance for Parallel Processors. <p> The CICO model identifies a program's communication and encourages a programmer to reformulate the program in a similar manner, but still retains the underlying shared-memory paradigm. LeBlanc and Markatos identified another advantage of shared memory in their comparison of shared-memory and message-passing programs <ref> [26] </ref>. They showed that programs with load imbalances are better formulated as shared-memory rather than message-passing programs because tasks and data are more easily moved dynamically in a shared address space. Their programs were written under a naive shared-memory model that was unconcerned with locality.
Reference: [27] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Solutions to this cache coherent problem distinguishes several classes of parallel computers. Multis [6] are bus-based computers in which all processors watch memory accesses occurring on a shared bus and modify their caches appropriately. Directory-based computers|such as Stanford DASH <ref> [27] </ref> and MIT Alewife [1]|eliminate the non-scalable bus by having hardware|and sometimes software|maintain a directory that records which processors hold copies of a cache block. A cache-coherence protocol uses the directories to serialize conflicting updates and to invalidate copies at updates.
Reference: [28] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hen--nessy. </author> <title> The DASH Prototype: Logic Overhead and Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 41-61, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: For example, in the Stanford DASH computer, a read from the local cache can take a single cycle, while a remote memory read requires from 34 to 132 cycles <ref> [28] </ref>. Unfortunately, the behavior and associated cost of a memory reference is difficult to predict. On a parallel computer, the state of a processor's cache depends not only on a processor's previous memory references, but also on other processors' reference patterns.
Reference: [29] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Shared memory is implemented either with software, hardware, or a combination of the two. Purely software systems, such as Li's shared virtual memory <ref> [29] </ref>, implement a shared address space without shared-memory hardware by using the virtual-memory hardware available in most machines to detect non-local memory references. These references cause traps, which invoke system software that copies pages of data among processors.
Reference: [30] <author> Calvin Lin and Lawrence Snyder. </author> <title> A Comparison of Programming Models for Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II-163-170, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Lin and Snyder reported an experiment in which programs written for a non-shared-memory (message-passing) machine ran faster on a shared-memory computer than shared-memory programs <ref> [30] </ref>. Ngo and Snyder reported similar results for more complex programs running on a wide variety of shared-memory computers [32].
Reference: [31] <author> Sang Lyul Min and Jean-Loup Baer. </author> <title> A Timestamp-based Cache Coherence Scheme. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing (Vol. I Architecture), </booktitle> <pages> pages I-23-32, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Since they are declarative, not imperative, they do not affect a program's semantics or a cache's operation. The programmer uses annotations to describe and reason about cache behavior, not to implement software cache coherence (cf. <ref> [8, 13, 31] </ref>). The advantage of summarizing cache behavior with these annotations is that the programmer only needs to study the communication at the annotations and need not reason about every memory reference.
Reference: [32] <author> Ton A. Ngo and Lawrence Snyder. </author> <title> On the Influence of Programming Models on Shared Memory Computer Performance. </title> <booktitle> In Scalable High Performance Computing Conference (SHPCC '92), </booktitle> <address> page ?, April 1992. </address>
Reference-contexts: Programmers who are unaware of true costs use hardware ineffectively because they unnecessarily invoke expensive operations and overuse resources, which causes performance-limiting bottlenecks. For example, in comparing shared-memory and non-shared-memory algorithms, Ngo and Snyder ran an LU factorization of a matrix on three shared-memory computers <ref> [32] </ref>. The naive shared-memory version of the program allowed any processor to update any portion of the matrix. Another version of the program was structured like a message-passing program. It partitioned the matrix among the processors so only one processor updated each portion. <p> Lin and Snyder reported an experiment in which programs written for a non-shared-memory (message-passing) machine ran faster on a shared-memory computer than shared-memory programs [30]. Ngo and Snyder reported similar results for more complex programs running on a wide variety of shared-memory computers <ref> [32] </ref>. The primary difference between the shared-memory and non-shared-memory programs was that the latter programs partitioned shared data so each processor used its cache more effectively and communicated with other processors less frequently than in the shared-memory programs.
Reference: [33] <author> Naomi Nishimura. </author> <title> Asynchronous Shared Memory Parallel Computation. </title> <booktitle> In Proceedings of the Second ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 76-84, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: These assumptions simplify analysis, but do not reflect real computers, particularly those with caches. For this reason, PRAM extensions model non-uniform memory and processor asynchrony <ref> [3, 10, 33, 20] </ref>. The new models are more descriptive and complex than traditional PRAM models, but still do not accurately describe cache-coherent parallel computers. Perhaps this paper will help inspire PRAM extensions that describe this important type of computers.
Reference: [34] <author> Keshav Pingali and Anne Rogers. </author> <title> Compiling for Locality. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II-142-146, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: In other programs, however, these factors greatly complicate message-passing programming. By exposing the underlying hardware primitives, message passing operations identify in-terprocessor communication and facilitates performance modeling. A simple model, which is commonly used by compilers <ref> [19, 34] </ref>, attributes a uniformly high cost to messages, independent of their size or destination. Although crude, this model identifies communication as a major bottleneck and suggests effective optimizations such as combining multiple sends into a single message and overlapping communication and computation.
Reference: [35] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The protocol's operation (and cost), in turn, depends on the memory's state. An accurate, but slow and complex method of calculating the cost of a memory access is to simulate a particular machine in detail and record which statements cause interprocessor communication <ref> [11, 14, 35] </ref>. Simulation, because it is so time-consuming, is generally limited to studying short programs with small data sets. For many programmers, a more attractive approach would trade accuracy for simplicity. <p> The simulation ran on the Wisconsin Wind Tunnel, which is a detailed and accurate parallel architecture simulator that runs on a Thinking Machines CM-5 computer <ref> [35] </ref>.
Reference: [36] <author> Randall Rettberg and Robert Thomas. </author> <title> Contention is no Obstacle to Shared-Memory Multiprocessing. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1202-1212, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Purely software solutions perform poorly for many programs, so many forms of shared-memory hardware have been proposed and built. Non-uniform access machines, such as the BBN Butterfly <ref> [36] </ref>, physically partition memory among processors, which results in sharply higher costs for remote accesses. However, most shared-memory computers, even if memory is partitioned, use caches to keep copies of a memory location close to the processors that are actively accessing it.
Reference: [37] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In the unblocked code, CICO's prediction is very close to the measured value and clearly demonstrate the importance of blocking this algorithm. 3.2 Water Water is one of the Stanford Splash shared-memory benchmarks <ref> [37] </ref>. It is an N-body molecular dynamics code that calculates the forces and potentials among a collection of water molecules. Its principal data structure is a vector of N water molecules. Each of the P processors is statically assigned to compute forces and potentials for N=P contiguous molecules. <p> These experiments assumed a modest 100 cycle delay to obtain a remote cache block. Longer latencies in future machines will reduce performance even more unless cache misses can be reduced. 3.3 Mp3d Mp3d is another Stanford Splash shared-memory benchmarks <ref> [37] </ref>. It performs a Monte Carlo simulation of a rarefied fluid flow. The simulation traces molecules through a three-dimensional space array of unit-sized cells and uses the accumulated information in each cell to compute the fluid flow. <p> CICO draws from the strengths of both paradigms. It, like message passing, focuses on optimizing communication and data reuse, but still retains the flexibility and load-balancing of shared memory. Cheriton et al. describe the changes they made to MP3D <ref> [37] </ref>, a three-dimensional particle simulator to improve its cache performance [9]. The changes eliminated false sharing by changing the data structures to increase their "processor affinity" and hence reduce communication. The changes increased execution speed by a factor of 3-4 on small-scale multiprocessors. <p> We also plan to explore more accurate cost models for particular processors, to see when the increased accuracy is beneficial. Acknowledgements Mark D. Hill provided encouragement and many helpful comments on this paper. Tom Reps and Elizabeth Shriver provided many helpful comments on this paper. Singh et al. <ref> [37] </ref> performed an invaluable service by writing and distributing the SPLASH benchmarks. Michael Wolf provided the mm benchmark. Shubhendu S. Mukherjee restructured mp3d as described above.
Reference: [38] <author> Marc Snir. </author> <title> On Parallel Search. </title> <booktitle> In Proceedings of Principals of Distributed Computing, </booktitle> <pages> pages 242-253, </pages> <month> August </month> <year> 1982. </year>
Reference-contexts: The discrepancy is even larger on more recent machines|for example, 100 times on Stanford DASH [27]|and is likely to continue increasing. Theoretical models, such as PRAMs <ref> [17, 16, 38] </ref>, abstract too far from real machines to provide a practical programming model, and consequently do not model features of real machines, such as caches. <p> Many theoretical models of shared-memory computers exist. One of the most well-known is parallel random access machine (PRAM) model [17]. A PRAM has a single globally-addressable memory and n processors that operate in lock-step read, compute, write cycles. Although PRAM models handle simultaneous reads or writes differently <ref> [16, 38] </ref>, they all assume that memory accesses are unit cost and that synchronization is unnecessary (because processors run in lock-step). These assumptions simplify analysis, but do not reflect real computers, particularly those with caches. For this reason, PRAM extensions model non-uniform memory and processor asynchrony [3, 10, 33, 20].
Reference: [39] <author> Lawrence Snyder. </author> <title> Type Architectures, Shared Memory, and the Corollary of Modest Potential. </title> <booktitle> Annual Review of Computer Science, </booktitle> <pages> pages 289-317, </pages> <year> 1986. </year>
Reference-contexts: Snyder argued that an abstract model of a class of computers, which he called a type architecture (and we call a programming model), should be fundamental bridge between programming languages and programmers and computers <ref> [39] </ref>. His principal requirement for a type architecture is that it should accurately reflect the cost of operations on real machines. In Snyder's terms, the combination of shared memory and CICO can be considered a type architecture for cache-coherent shared-memory computers.
Reference: [40] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-168, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: They can be passed as directives to a memory system to improve program performance. For example, Dir 1 SW [22] 14 is a minimal directory protocol that adds little complexity to the hardware of a message-passing machine, but efficiently supports programs written within the CICO model <ref> [40] </ref>. It uses a single pointer/counter field to either identify the processor holding a writable copy of a cache block or to count the number of outstanding readable copies. Simple hardware entirely handles programs conforming to the CICO model by updating the pointer/counter and forwarding data to a requesting processor. <p> The paper also presented a few rules of thumb (e.g, avoid false sharing) and explained their rationale in terms of the model. Hill et al. described a preliminary version of CICO and showed that simple directory hardware could use the annotations to improve program performance <ref> [22, 40] </ref>. The earlier version of CICO was described in terms of Dir 1 SW hardware, did not provide a cost model, and was used to reduce Dir 1 SW traps rather than improve program performance.
Reference: [41] <author> Hans Zima and Barbara Chapman. </author> <title> Compiling for Distributed-Memory Systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 264-287, </pages> <month> February </month> <year> 1993. </year> <month> 19 </month>
Reference-contexts: These references cause traps, which invoke system software that copies pages of data among processors. Another approach is to have compiled code provide a shared address space on a message-passing machine. Compilers, such as Vienna Fortran <ref> [18, 41] </ref> and Fortran D [24], implement a shared address space by detecting remote memory references (primarily at compile time) and sending data between processors with explicit messages. Purely software solutions perform poorly for many programs, so many forms of shared-memory hardware have been proposed and built.
References-found: 41

