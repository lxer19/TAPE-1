URL: http://cag-www.lcs.mit.edu/~rinard/techreport/TRCS96-09.ps
Refering-URL: http://cag-www.lcs.mit.edu/~rinard/techreport/index.html
Root-URL: 
Email: (martin@cs.ucsb.edu)  (pedro@cs.ucsb.edu)  
Title: Semantic Foundations of Commutativity Analysis  
Author: Martin C. Rinard Pedro Diniz 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: This paper presents the semantic foundations of commu-tativity analysis, an analysis technique for automatically parallelizing programs written in a sequential, imperative programming language. Commutativity analysis views the computation as composed of operations on objects. It then analyzes the program at this granularity to discover when operations commute (i.e. generate the same result regardless of the order in which they execute). If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code. This paper shows that the basic analysis and tranfor-mations that form the foundation of the parallelization technique are sound. It also presents performance results from two automatically parallelized programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic program parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 211-243, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Current parallelizing compilers preserve the semantics of the original serial program by preserving the data dependences <ref> [1, 12] </ref>. They analyze the program to identify independent pieces of computation (two pieces of computation are independent if neither writes a piece of memory that the other accesses), then generate code that executes independent pieces concurrently.
Reference: [2] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(NlogN) force-calculation algorithm. </title> <booktitle> Nature, </booktitle> <pages> pages 446-449, </pages> <month> December </month> <year> 1976. </year>
Reference-contexts: It also performs several code generation optimizations that significantly extend the code generation strategy discussed in Section 6.8. We present a complete treatment of both of these extensions elsewhere [18, 6]. We used the compiler to automatically parallelize two applications: the Barnes-Hut hierarchical N-body code <ref> [2] </ref> and Water, which evaluates forces and potentials in a system of water molecules in the liquid state.
Reference: [3] <author> P. Barth, R. Nikhil, and Arvind. M-structures: </author> <title> Extending a parallel, non-strict, functional language with state. </title> <booktitle> In Proceedings of the Fifth ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 538-568. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: Finally, preserving the data dependences for programs that periodically update shared data structures can artificially limit the amount of exposed concurrency, since tasks must delay updates until they are sure that each update will not change the relative order of reads and writes to the shared data structure <ref> [3, 16] </ref>. This paper presents the semantic foundations of a new analysis technique called commutativity analysis [17, 19]. Instead of preserving the relative order of individual reads and writes to single words of memory, commutativity analysis views the computation as composed of operations on objects. <p> The limiting factor on the speedup is contention for shared objects updated by multiple operations. Barnes-Hut (8192 bodies) Barnes-Hut (16384 bodies) Water (343 molecules) Water (512 molecules) 13 8 Related Work Other researchers have recognized the value of including support for commuting operations in parallel computing systems <ref> [23, 3, 22] </ref>. These systems, however, focus on exploiting commuting operations and rely on some external mechanism, typically the programmer, to specify when the operations actually commute. The techniques presented in this paper, on the other hand, automatically detect commuting operations.
Reference: [4] <author> W. Blume and R. Eigenmann. </author> <title> Symbolic range propagation. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <pages> pages 357-363, </pages> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: Other researchers have successfully applied similar expression manipulation techniques in other analysis contexts <ref> [4] </ref>. Even with exponential running time, there are some expressions that always denote the same value but do not have the same simplified form.
Reference: [5] <author> D. Callahan. </author> <title> Recognizing and parallelizing bounded recurrences. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: The techniques presented in this paper, on the other hand, automatically detect commuting operations. Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [7, 15, 5] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [6] <author> P. Diniz and M. Rinard. </author> <title> Lock coarsening: Eliminating lock overhead in automatically parallelized object-based programs. </title> <type> Technical Report TRCS96-07, </type> <institution> Dept. of Computer Science, University of California at Santa Barbara, </institution> <month> May </month> <year> 1996. </year> <title> operations to leave the object in states that are equivalent with respect to the rest of the computation. </title> <type> 14 </type>
Reference-contexts: It also performs several code generation optimizations that significantly extend the code generation strategy discussed in Section 6.8. We present a complete treatment of both of these extensions elsewhere <ref> [18, 6] </ref>. We used the compiler to automatically parallelize two applications: the Barnes-Hut hierarchical N-body code [2] and Water, which evaluates forces and potentials in a system of water molecules in the liquid state.
Reference: [7] <author> A. Fisher and A. Ghuloum. </author> <title> Parallelizing complex scans and reductions. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Program Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The algorithm builds the table by recursively traversing the outer conditional expressions to identify the minimal conjunctions of basic terms that select each maximal conditional-free subexpression as the value of the original expression. It is possible to further simplify the table using logic minimization techniques as proposed in <ref> [7] </ref>. To compare two expressions for equality the algorithm performs a simple recursive isomorphism test. <p> The techniques presented in this paper, on the other hand, automatically detect commuting operations. Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [7, 15, 5] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [8] <author> W.L. Harrison. </author> <title> The interprocedural analysis and automatic parallelization of Scheme programs. </title> <journal> Lisp and Symbolic Computation, </journal> 2(3/4):179-396, October 1989. 
Reference-contexts: While researchers have been able to develop reasonably effective algorithms for loop nests that manipulate dense matrices using affine access functions [13], there has been little progress towards the successful automatic analysis of programs that manipulate pointer-based data structures. Researchers have attempted to build totally automatic systems <ref> [8] </ref>, but the most promising approaches require the programmer to provide annotations that specify information about the global topology of the manipulated data structures [9]. A second, more fundamental limitation of data-dependence based approaches is an inability to parallelize computations that manipulate graphs.
Reference: [9] <author> L. Hendren, J. Hummel, and A. Nicolau. </author> <title> Abstractions for recursive pointer data structures: Improving the analysis and transformation of imperative programs. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Researchers have attempted to build totally automatic systems [8], but the most promising approaches require the programmer to provide annotations that specify information about the global topology of the manipulated data structures <ref> [9] </ref>. A second, more fundamental limitation of data-dependence based approaches is an inability to parallelize computations that manipulate graphs. The aliases inherently present in these data structures preclude the static discovery of independent pieces of code, forcing the compiler to generate serial code.
Reference: [10] <author> G. Huet. </author> <title> Confluent reductions: Abstract properties and applications to term rewriting systems. </title> <journal> Journal of the ACM, </journal> <volume> 27(4) </volume> <pages> 797-821, </pages> <year> 1980. </year>
Reference-contexts: implies * not hm; fr-&gt;op (o)gi ) (i.e. there is no infinite parallel execution), and * hm; fr-&gt;op (o)gi ) ) hm 2 ; ;i implies m 1 = m 2 Proof Sketch: If all of the invoked operations commute then the transition system is confluent, which guarantees deterministic execution <ref> [10] </ref>. Lemma 3 characterizes the situation when the computation may not terminate. It says that if all of the operations invoked in the parallel executions commute, then it is possible to take any two partial parallel executions and extend them to identical states. <p> ) ) hm 2 ; p 2 i implies 9m 0 2 M; p 2 mst (A) : hm 1 ; p 1 i ) ) hm 0 ; pi and Proof Sketch: If all of the invoked operations commute then the transition system is confluent, which guarantees deterministic execution <ref> [10] </ref>. An immediate corollary of these two lemmas is that if the serial computation terminates, then all parallel computations terminate with identical memories.
Reference: [11] <author> O. Ibarra, P. Diniz, and M. Rinard. </author> <title> On the complexity of commutativity analysis. </title> <type> Technical Report TRCS95-18, </type> <institution> Dept. of Computer Science, University of California at Santa Barbara, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: Even with exponential running time, there are some expressions that always denote the same value but do not have the same simplified form. Given that it is, in general, undecid 11 able to determine if two operations commute <ref> [11] </ref>, such imprecision is an inherent limitation of any expression manipulation algorithm. Another limitation of the presented algorithms is that they require commuting operations to leave the receiver object in identical states in both execution orders.
Reference: [12] <author> D. Kuck, Y. Muraoka, and S. Chen. </author> <title> On the number of operations simultaneously executable in Fortran-like programs and their resulting speedup. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-21(12):1293-1310, </volume> <month> December </month> <year> 1972. </year>
Reference-contexts: 1 Introduction Current parallelizing compilers preserve the semantics of the original serial program by preserving the data dependences <ref> [1, 12] </ref>. They analyze the program to identify independent pieces of computation (two pieces of computation are independent if neither writes a piece of memory that the other accesses), then generate code that executes independent pieces concurrently.
Reference: [13] <author> D. Maydan, J. Hennessy, and M. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: A significant limitation of this approach is the difficulty of performing dependence analysis that is precise enough to expose substantial amounts of concurrency. While researchers have been able to develop reasonably effective algorithms for loop nests that manipulate dense matrices using affine access functions <ref> [13] </ref>, there has been little progress towards the successful automatic analysis of programs that manipulate pointer-based data structures.
Reference: [14] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy task creation: a technique for increasing the granularity of parallel programs. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 185-197, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The parallel visit method acquires the lock before accessing the receiver and releases the lock before invoking any methods. A straightforward applica 2 tion of lazy task creation techniques <ref> [14] </ref> can increase the granularity of the resulting parallel computation. class graph - lock mutex; boolean mark; int val, sum; graph *left; graph *right; -; graph::visit (int s) - this-&gt;parallel_visit (s); wait (); - graph::parallel_visit (int p) - mutex.acquire (); graph *l = left; graph *r = right; int v =
Reference: [15] <author> S. Pinter and R. Pinter. </author> <title> Program optimization and paral-lelization using idioms. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Orlando, FL, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: The techniques presented in this paper, on the other hand, automatically detect commuting operations. Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [7, 15, 5] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [16] <author> M. Rinard. </author> <title> The Design, Implementation and Evaluation of Jade, a Portable, Implicitly Parallel Programming Language. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: Finally, preserving the data dependences for programs that periodically update shared data structures can artificially limit the amount of exposed concurrency, since tasks must delay updates until they are sure that each update will not change the relative order of reads and writes to the shared data structure <ref> [3, 16] </ref>. This paper presents the semantic foundations of a new analysis technique called commutativity analysis [17, 19]. Instead of preserving the relative order of individual reads and writes to single words of memory, commutativity analysis views the computation as composed of operations on objects.
Reference: [17] <author> M. Rinard and P. Diniz. </author> <title> Commutativity analysis: A new analysis framework for parallelizing compilers. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Program Language Design and Implementation, </booktitle> <address> Philadel-phia, PA, </address> <month> May </month> <year> 1996. </year> <note> (An extended version of this document can be found at the authors' web address: http://www.cs.ucsb.edu/~fmartin,pedrog). </note>
Reference-contexts: This paper presents the semantic foundations of a new analysis technique called commutativity analysis <ref> [17, 19] </ref>. Instead of preserving the relative order of individual reads and writes to single words of memory, commutativity analysis views the computation as composed of operations on objects.
Reference: [18] <author> M. Rinard and P. Diniz. </author> <title> Commutativity analysis: A new analysis framework for parallelizing compilers. </title> <type> Technical Report TRCS96-08, </type> <institution> Dept. of Computer Science, University of California at Santa Barbara, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: It also performs several code generation optimizations that significantly extend the code generation strategy discussed in Section 6.8. We present a complete treatment of both of these extensions elsewhere <ref> [18, 6] </ref>. We used the compiler to automatically parallelize two applications: the Barnes-Hut hierarchical N-body code [2] and Water, which evaluates forces and potentials in a system of water molecules in the liquid state. <p> We briefly present several performance results; we provide a more complete description of the applications and the experimental methodology elsewhere <ref> [18] </ref>. on two input data sets; this graph plots the running time of the sequential version running with no parallelization overhead divided by the the running time of the automatically parallel version as a function of the number of processors executing the parallel computation.
Reference: [19] <author> M. Rinard and P. Diniz. </author> <title> Commutativity analysis: A technique for automatically parallelizing pointer-based computations. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <address> Honolulu, HI, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: This paper presents the semantic foundations of a new analysis technique called commutativity analysis <ref> [17, 19] </ref>. Instead of preserving the relative order of individual reads and writes to single words of memory, commutativity analysis views the computation as composed of operations on objects.
Reference: [20] <author> D. Scales and M. S. Lam. </author> <title> An efficient shared memory system for distributed memory machines. </title> <type> Technical Report CSL-TR-94-627, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Programmers who parallelize computations by hand have always used commuting operations. For example, four (Water, MP3D, LocusRoute and Cholesky) of the six parallel applications in the SPLASH benchmark suite [21] and three of the four parallel applications described in <ref> [20] </ref> violate the data dependences of the original serial program and rely on commuting operations for their correct execution.
Reference: [21] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Programmers who parallelize computations by hand have always used commuting operations. For example, four (Water, MP3D, LocusRoute and Cholesky) of the six parallel applications in the SPLASH benchmark suite <ref> [21] </ref> and three of the four parallel applications described in [20] violate the data dependences of the original serial program and rely on commuting operations for their correct execution.
Reference: [22] <author> J. Solworth and B. Reagan. </author> <title> Arbitrary order operations on trees. </title> <booktitle> In Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The limiting factor on the speedup is contention for shared objects updated by multiple operations. Barnes-Hut (8192 bodies) Barnes-Hut (16384 bodies) Water (343 molecules) Water (512 molecules) 13 8 Related Work Other researchers have recognized the value of including support for commuting operations in parallel computing systems <ref> [23, 3, 22] </ref>. These systems, however, focus on exploiting commuting operations and rely on some external mechanism, typically the programmer, to specify when the operations actually commute. The techniques presented in this paper, on the other hand, automatically detect commuting operations.
Reference: [23] <author> G. Steele. </author> <title> Making asynchronous parallelism safe for the world. </title> <booktitle> In Proceedings of the Seventeenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 218-231, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: The limiting factor on the speedup is contention for shared objects updated by multiple operations. Barnes-Hut (8192 bodies) Barnes-Hut (16384 bodies) Water (343 molecules) Water (512 molecules) 13 8 Related Work Other researchers have recognized the value of including support for commuting operations in parallel computing systems <ref> [23, 3, 22] </ref>. These systems, however, focus on exploiting commuting operations and rely on some external mechanism, typically the programmer, to specify when the operations actually commute. The techniques presented in this paper, on the other hand, automatically detect commuting operations.
Reference: [24] <author> W. Weihl. </author> <title> Commutativity-based concurrency control for abstract data types. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1488-1505, </pages> <month> December </month> <year> 1988. </year> <month> 15 </month>
Reference-contexts: Knowledge of the algebraic properties therefore enhances the effectiveness of, rather than enables, the presented techniques. Research performed in the context of database concur-rency control has shown that it is possible to take advantage of commuting operations to increase the amount of con-currency available in transaction processing systems <ref> [24] </ref>. The presented approach is based on using abstract specifications of the behavior of the operations to expose the commutativity. It is the responsibility of the programmer to ensure that the actual implementation preserves the semantics of the specification.
References-found: 24

