URL: http://www.cs.jhu.edu/~goodrich/pubs/sort.ps
Refering-URL: http://www.cs.jhu.edu/~goodrich/pubs/index.html
Root-URL: http://www.cs.jhu.edu
Title: Sorting on a Parallel Pointer Machine with Applications to Set Expression Evaluation  
Author: Michael T. Goodrich and S. Rao Kosaraju 
Keyword: Categories and Subject Descriptors: E.1 [Data Structures]: arrays, lists; F.2.2. [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems|sorting and searching General Terms: Algorithms, Theory, Verification Additional Key Words and Phrases: parallel algorithms, PRAM, pointer machine, linking automaton, expression evaluation, mergesort, cascade merging  
Address: Baltimore, Maryland  
Affiliation: Johns Hopkins University,  
Abstract: We present optimal algorithms for sorting on parallel CREW and EREW versions of the pointer machine model. Intuitively, one can view our methods as being based on a parallel mergesort using linked lists rather than arrays (the usual parallel data structure). We also show how to exploit the "locality" of our approach to solve the set expression evaluation problem, a problem with applications to database querying and logic-programming, in O(log n) time using O(n) processors. Interestingly, this is an asymptotic improvement over what seems possible using previous techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Abrahamson, K., Dadoun, N., Kirkpatrick, D. G., and Przytycka, T. </author> <title> A simple parallel tree contraction algorithm. </title> <editor> J. </editor> <booktitle> Algorithms 10 (1989), </booktitle> <pages> 287-302. </pages>
Reference-contexts: We do not know of any previous parallel algorithms for this problem, but it appears that the array-based merging methods of Bilardi and Nicolau [6], Shiloach and Vishkin [25], or Hagerup and Rub [12] and the tree-contraction techniques of Abrahamson et al. <ref> [1] </ref>, Miller and Reif [21, 22], or Kosaraju and Delcher [19] will lead to an O (log 2 n) time solution with a time-processor product of O (n log n). We achieve O (log n) time while still maintaining the optimal time-processor product of O (n log n). <p> We can solve the set-expression evaluation problem by cascade merging in a dag with a bounded-width tree partition. Specifically, we convert the tree T into such an expression dag G using the rake-and-compress paradigm of Abrahamson et al. <ref> [1] </ref>, Miller and Reif [21, 22], and Kosaraju and Delcher [19]. This dag will have a tree partition with O (log n) height and have width equal to 3, with each node being labeled by an intersection or union operation. <p> A rake operation applied at a node v amounts to removing any children of v that are leaves. A compress operation applied at a node v amounts to contracting v with its child, provided v has only one child. We refer the reader to Abrahamson et al. <ref> [1] </ref>, Miller and Reif [21, 22], and Kosaraju and Delcher [19] for the details on how to use these two operations to reduce a constant fraction of the nodes in T in each round.
Reference: [2] <author> Ajtai, M., Koml os, J., and Szemer edi, E. </author> <title> Sorting in c log n parallel steps. </title> <booktitle> Combinatorica 3 (1983), </booktitle> <pages> 1-19. </pages>
Reference-contexts: Since then there has been a considerable amount of work done for this important problem (e.g., see Bitton et al. [7], JaJa [15], Karp and Ramachandran [16], and Reif [14]). Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi <ref> [2] </ref>, that one can sort in O (log n) time with an O (n log n) sized sorting network (see also Paterson [23]). <p> Our method for this problem also runs in O (log n) time using an optimal O (n) number of processors. We leave open the following questions: * Can one solve the set-expression evaluation problem optimally as a circuit (say, by extending the sorting network of Ajtai, Komlos, and Szemeredi <ref> [2] </ref>)? * What is the complexity of sorting on a CRCW PPM?
Reference: [3] <author> Atallah, M. J., Cole, R., and Goodrich, M. T. </author> <title> Cascading divide-and-conquer: A technique for designing parallel algorithms. </title> <journal> SIAM J. Comput. </journal> <volume> 18 (1989), </volume> <pages> 499-532. </pages>
Reference-contexts: Interestingly, although Cole's procedure did not improve the asymptotic complexity of sorting on the PRAM model, it did lead to improvements in the asymptotic complexity of a number of computational geometry problems, as it was the key ingredient of the cascading divide-and-conquer technique of Atallah, Cole, and Goodrich <ref> [3] </ref>. Our interest in sorting on the PPM is motivated by a desire for a parallel mergesort procedure that is more closely akin to the linked-list implementation of the sequential mergesort procedure (e.g., see Knuth [18] and Sedgewick [24]). <p> We also store a sample, L t (v), of A t (v), where the elements in L t (v) are to be passed up to u in the next stage (t + 1). As in the algorithms of Atallah et al. <ref> [3] </ref> and Cole [10], we say that a node v is full after stage t if A t (v) contains all the elements stored in the descendents of v, and v is active if A t (v) 6= ; and u is not full. <p> Proof: The proof is based on the idea, for each node v 2 G, of combining the in-coming (resp., out-going) edges for v into a binary tree of height at most dlog degree (G)e. The interested reader is referred to the work of Atallah et al. <ref> [3] </ref> and Chazelle and Guibas [9] for examples of this type of transformation. 2 The construction of such an f and H can easily be done in O (log degree (G)) time given a processor assigned to each edge in G using a method of Atallah et al. [3]. <p> et al. <ref> [3] </ref> and Chazelle and Guibas [9] for examples of this type of transformation. 2 The construction of such an f and H can easily be done in O (log degree (G)) time given a processor assigned to each edge in G using a method of Atallah et al. [3]. Taken together, the above two lemmas, immediately imply the following corollary: Corollary 4.4: Suppose one is given a dag G = (V; E) with tree partition T ().
Reference: [4] <author> Batcher, K. E. </author> <title> Sorting networks and their applications. </title> <booktitle> In Proc. 1968 Spring Joint Computer Conf. </booktitle> <address> (Reston, VA, 1968), </address> <publisher> AFIPS Press, </publisher> <pages> pp. 307-314. </pages>
Reference-contexts: We are interested in the complexity of sorting on a PPM. Let us, then, briefly review a small sample of the voluminous work previously done on parallel sorting. In 1968 Batcher <ref> [4] </ref> gave what is considered to be the first parallel sorting scheme. Specifically, his was a sorting network that sorted in O (log 2 n) time using O (n) processors.
Reference: [5] <author> Ben-Or, M. </author> <title> Lower bounds for algebraic computation trees. </title> <booktitle> In Proc. 15th Annu. ACM Sympos. Theory Comput. </booktitle> <year> (1983), </year> <pages> pp. 80-86. </pages>
Reference-contexts: even if we do not insist on the output being sorted, this problem still has an (n log n) lower bound (in the ACT model), by a simple reduction from the set equality problem, which was shown to have and (n log n) lower bound in this model by Ben-Or <ref> [5] </ref>. We can solve the set-expression evaluation problem by cascade merging in a dag with a bounded-width tree partition. Specifically, we convert the tree T into such an expression dag G using the rake-and-compress paradigm of Abrahamson et al. [1], Miller and Reif [21, 22], and Kosaraju and Delcher [19].
Reference: [6] <author> Bilardi, G., and Nicolau, A. </author> <title> Adaptive bitonic sorting: An optimal parallel algorithm for shared-memory machines. </title> <booktitle> Information and Computation 18 (1989), </booktitle> <pages> 216-228. </pages>
Reference-contexts: This problem has applications in database querying and logic programming. We do not know of any previous parallel algorithms for this problem, but it appears that the array-based merging methods of Bilardi and Nicolau <ref> [6] </ref>, Shiloach and Vishkin [25], or Hagerup and Rub [12] and the tree-contraction techniques of Abrahamson et al. [1], Miller and Reif [21, 22], or Kosaraju and Delcher [19] will lead to an O (log 2 n) time solution with a time-processor product of O (n log n).
Reference: [7] <author> Bitton, D., DeWitt, D. J., Hsiao, D. K., and Menon, J. </author> <title> A taxonomy of parallel sorting. </title> <journal> ACM Computing Surveys 16, </journal> <volume> 3 (1984), </volume> <pages> 287-318. </pages>
Reference-contexts: Specifically, his was a sorting network that sorted in O (log 2 n) time using O (n) processors. Since then there has been a considerable amount of work done for this important problem (e.g., see Bitton et al. <ref> [7] </ref>, JaJa [15], Karp and Ramachandran [16], and Reif [14]). Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi [2], that one can sort in O (log n) time with an O (n log n) sized sorting network (see also Paterson [23]).
Reference: [8] <author> Chazelle, B. </author> <title> A functional approach to data structures and its use in multidimensional searching. </title> <journal> SIAM J. Comput. </journal> <volume> 17 (1988), </volume> <pages> 427-462. </pages>
Reference-contexts: Just as the PRAM model is a generalization of the sequential RAM model, the Parallel Pointer Machine (or PPM) model is a generalization of the sequential Pointer Machine model described by Chazelle <ref> [8] </ref>, which is similar to the Linking Automaton model of Knuth [17] and Tarjan [26]. In the PPM, one has a collection of synchronized processors that may access a common memory, just as in the PRAM model.
Reference: [9] <author> Chazelle, B., and Guibas, L. J. Fractional cascading: I. </author> <title> A data structuring technique. </title> <booktitle> Algorithmica 1 (1986), </booktitle> <pages> 133-162. </pages>
Reference-contexts: The interested reader is referred to the work of Atallah et al. [3] and Chazelle and Guibas <ref> [9] </ref> for examples of this type of transformation. 2 The construction of such an f and H can easily be done in O (log degree (G)) time given a processor assigned to each edge in G using a method of Atallah et al. [3].
Reference: [10] <author> Cole, R. </author> <title> Parallel merge sort. </title> <journal> SIAM J. Comput. </journal> <volume> 17, 4 (1988), </volume> <pages> 770-785. </pages>
Reference-contexts: One drawback of these algorithms, however, is that they are strongly dependent on expander graphs. This dependence on expander graphs is not required for an optimal PRAM solution, however, for in 1988 Cole <ref> [10] </ref> gave simple methods for optimal sorting in the CREW and EREW PRAM models that do not use expander graphs, but instead are based on an elegant "cascade merging" paradigm using arrays. <p> We show that one can achieve this goal, for we give optimal O (log n)-time sorting algorithms for both the CREW PPM and EREW PPM models. Our methods are loosely based on the cascade merging paradigm introduced by Cole <ref> [10] </ref>, but the details of our methods differ considerably from those of Cole's methods. His methods crucially depend on sorted sets being represented in arrays, and it seems impossible to implement his 3 methods in a parallel pointer machine model and still maintain optimal performance. <p> A is a c-cover of B if, for any two consecutive elements e and f in A, there are at most c elements of B in the interval [e; f ). (This definition is also used by Cole <ref> [10] </ref>). Note that the first definition deals with pointer relationships between A and B, and the last three definitions deal with value relationships. Let us make two observations about these relationships before going on. <p> T can easily be built in O (log n) time using the n processors, by, say, a recursive doubling 8 procedure (see JaJa [15], Karp and Ramachandran [16], or Reif [14]). As in parallel mergesort procedure of Cole <ref> [10] </ref>, we view T as the schematic for a parallel mergesort procedure. We define a list A (v) for each v in T to be the sorted list of all elements stored in descendents of v. <p> We define a list A (v) for each v in T to be the sorted list of all elements stored in descendents of v. A high-level description of our algorithm is similar to the parallel mergesort procedure of Cole <ref> [10] </ref>, in that we construct the A (v)'s in a pipelined fashion in a series of O (log n) stages (the details of our method are quite different than those for Cole's method, however). <p> We also store a sample, L t (v), of A t (v), where the elements in L t (v) are to be passed up to u in the next stage (t + 1). As in the algorithms of Atallah et al. [3] and Cole <ref> [10] </ref>, we say that a node v is full after stage t if A t (v) contains all the elements stored in the descendents of v, and v is active if A t (v) 6= ; and u is not full. <p> crucial, for if our method does not eliminate multiple copies (in an on-line fashion), then it is easy to construct dags that give rise to exponential-sized A (v) lists. (This is in fact the main reason why it seems impossible to optimally apply the array-based cascade merging methods of Cole <ref> [10] </ref> to solve this problem.) Our method for dealing with this difficulty is actually quite simple. When a node v becomes full, we adjust A t (v) by contracting any multiple copies of an element e into a single copy. <p> Thus, we allow some elements to be cascading down the tree T while others are cascading up. This approach is similar in spirit to the EREW PRAM sorting method of Cole <ref> [10] </ref>, but is considerably different otherwise. 27 Specifically, we define A t (v), for each non-full node v, as A t (v) = L t1 (u) [ L t1 (x) [ L t1 (y): Changing the definition of A t (v) in this way has some significant impacts on our procedure. <p> In particular, we have shown that one can sort in O (log n) time using O (n) processors in the CREW and EREW PPM models. Thus, we have established the existence of simple sorting algorithms for parallel models weaker than those used by Cole <ref> [10] </ref>. Some of the interesting aspects of our methods include the use of rank-linked and predecessor-linked samples, and the use of simple token-passing schemes to implement space and processor allocation.
Reference: [11] <author> Cormen, T. H., Leiserson, C. E., and Rivest, R. L. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: Each processors may use any of the addressing schemes allowed by the sequential RAM model (see Cormen et al. <ref> [11] </ref>), such as indexed and indirect addressing, to access the memory cells, so long as such an access does not violate any concurrent-access constraints that may have been placed on the common memory.
Reference: [12] <author> Hagerup, T., and R ub, C. </author> <title> Optimal merging and sorting on the erew pram. </title> <booktitle> Information Processing Letters 33 (1989), </booktitle> <pages> 181-185. </pages>
Reference-contexts: This problem has applications in database querying and logic programming. We do not know of any previous parallel algorithms for this problem, but it appears that the array-based merging methods of Bilardi and Nicolau [6], Shiloach and Vishkin [25], or Hagerup and Rub <ref> [12] </ref> and the tree-contraction techniques of Abrahamson et al. [1], Miller and Reif [21, 22], or Kosaraju and Delcher [19] will lead to an O (log 2 n) time solution with a time-processor product of O (n log n).
Reference: [13] <author> Hong, J. W., Mehlhorn, K., and Rosenberg, A. L. </author> <title> Cost trade-offs in graph embedding with applications. </title> <editor> J. </editor> <booktitle> ACM 30 (1983), </booktitle> <pages> 709-728. </pages>
Reference-contexts: are its dilation cost which is the length of the longest path in H to which an edge in G is mapped, and its expansion cost, which is the ratio of the number of nodes in H to the number of nodes in G (e.g., see Hong, Mehlhorn, and Rosenberg <ref> [13] </ref>). The following lemmas use these measures to analyze how the above independence assumption can be made without loss of generality. Lemma 4.2: Suppose one is given a dag G = (V; E) with tree partition T ().
Reference: [14] <author> J. H. Reif, e. </author> <title> Synthesis of Parallel Algorithms. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Professor Kosaraju's research was supported by the National Science Foundation under Grants CCR-8804284, CCR-8908092, CCR-9107293, and CCR-9508545. Authors' address: Department of Computer Science, Johns Hopkins University, Baltimore, Maryland 21218; e-mail: goodrich@cs.jhu.edu, kosaraju@cs.jhu.edu. 1 Reif <ref> [14] </ref>), where one has a collection of synchronized processors that access a com-mon memory. <p> Specifically, his was a sorting network that sorted in O (log 2 n) time using O (n) processors. Since then there has been a considerable amount of work done for this important problem (e.g., see Bitton et al. [7], JaJa [15], Karp and Ramachandran [16], and Reif <ref> [14] </ref>). Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi [2], that one can sort in O (log n) time with an O (n log n) sized sorting network (see also Paterson [23]). <p> T can easily be built in O (log n) time using the n processors, by, say, a recursive doubling 8 procedure (see JaJa [15], Karp and Ramachandran [16], or Reif <ref> [14] </ref>). As in parallel mergesort procedure of Cole [10], we view T as the schematic for a parallel mergesort procedure. We define a list A (v) for each v in T to be the sorted list of all elements stored in descendents of v.
Reference: [15] <author> J aJ a, J. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1992. </year>
Reference-contexts: 1 Introduction One of the primary models of parallel computation is the Parallel Random-Access Machine (or PRAM) model (e.g., see JaJa <ref> [15] </ref>, Karp and Ramachandran [16], and A preliminary announcement of this research appeared in Proc. 30th IEEE Symp. on Foundations of Computer Science, 1989, 190-195. Professor Goodrich's research was supported by the National Science Foundation under Grants CCR-8810568, CCR-8908092, CCR-9003299, IRI-9116843, and CCR-9300079. <p> Specifically, his was a sorting network that sorted in O (log 2 n) time using O (n) processors. Since then there has been a considerable amount of work done for this important problem (e.g., see Bitton et al. [7], JaJa <ref> [15] </ref>, Karp and Ramachandran [16], and Reif [14]). Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi [2], that one can sort in O (log n) time with an O (n log n) sized sorting network (see also Paterson [23]). <p> T can easily be built in O (log n) time using the n processors, by, say, a recursive doubling 8 procedure (see JaJa <ref> [15] </ref>, Karp and Ramachandran [16], or Reif [14]). As in parallel mergesort procedure of Cole [10], we view T as the schematic for a parallel mergesort procedure.
Reference: [16] <author> Karp, R. M., and Ramachandran, V. </author> <title> Parallel algorithms for shared memory machines. </title> <booktitle> In Handbook of Theoretical Computer Science, </booktitle> <editor> J. van Leeuwen, Ed. </editor> <publisher> Elsevier/The MIT Press, </publisher> <address> Amsterdam, </address> <year> 1990, </year> <pages> pp. 869-941. 41 </pages>
Reference-contexts: 1 Introduction One of the primary models of parallel computation is the Parallel Random-Access Machine (or PRAM) model (e.g., see JaJa [15], Karp and Ramachandran <ref> [16] </ref>, and A preliminary announcement of this research appeared in Proc. 30th IEEE Symp. on Foundations of Computer Science, 1989, 190-195. Professor Goodrich's research was supported by the National Science Foundation under Grants CCR-8810568, CCR-8908092, CCR-9003299, IRI-9116843, and CCR-9300079. <p> Specifically, his was a sorting network that sorted in O (log 2 n) time using O (n) processors. Since then there has been a considerable amount of work done for this important problem (e.g., see Bitton et al. [7], JaJa [15], Karp and Ramachandran <ref> [16] </ref>, and Reif [14]). Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi [2], that one can sort in O (log n) time with an O (n log n) sized sorting network (see also Paterson [23]). <p> T can easily be built in O (log n) time using the n processors, by, say, a recursive doubling 8 procedure (see JaJa [15], Karp and Ramachandran <ref> [16] </ref>, or Reif [14]). As in parallel mergesort procedure of Cole [10], we view T as the schematic for a parallel mergesort procedure. We define a list A (v) for each v in T to be the sorted list of all elements stored in descendents of v.
Reference: [17] <author> Knuth, D. E. </author> <title> Fundamental Algorithms, </title> <booktitle> vol. 1 of The Art of Computer Pro--gramming. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1968. </year>
Reference-contexts: Just as the PRAM model is a generalization of the sequential RAM model, the Parallel Pointer Machine (or PPM) model is a generalization of the sequential Pointer Machine model described by Chazelle [8], which is similar to the Linking Automaton model of Knuth <ref> [17] </ref> and Tarjan [26]. In the PPM, one has a collection of synchronized processors that may access a common memory, just as in the PRAM model. In this model, however, the types of memory accesses are limited to those allowed by the Pointer Machine.
Reference: [18] <author> Knuth, D. E. </author> <title> Sorting and Searching, </title> <booktitle> vol. 3 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: Our interest in sorting on the PPM is motivated by a desire for a parallel mergesort procedure that is more closely akin to the linked-list implementation of the sequential mergesort procedure (e.g., see Knuth <ref> [18] </ref> and Sedgewick [24]). We show that one can achieve this goal, for we give optimal O (log n)-time sorting algorithms for both the CREW PPM and EREW PPM models.
Reference: [19] <author> Kosaraju, S. R., and Delcher, A. L. </author> <title> Optimal parallel evaluation of tree-structured computations by raking. </title> <booktitle> In Proc. </booktitle> <volume> AWOC 88, vol. </volume> <booktitle> 319 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1988, </year> <pages> pp. 101-110. </pages>
Reference-contexts: not know of any previous parallel algorithms for this problem, but it appears that the array-based merging methods of Bilardi and Nicolau [6], Shiloach and Vishkin [25], or Hagerup and Rub [12] and the tree-contraction techniques of Abrahamson et al. [1], Miller and Reif [21, 22], or Kosaraju and Delcher <ref> [19] </ref> will lead to an O (log 2 n) time solution with a time-processor product of O (n log n). We achieve O (log n) time while still maintaining the optimal time-processor product of O (n log n). <p> We can solve the set-expression evaluation problem by cascade merging in a dag with a bounded-width tree partition. Specifically, we convert the tree T into such an expression dag G using the rake-and-compress paradigm of Abrahamson et al. [1], Miller and Reif [21, 22], and Kosaraju and Delcher <ref> [19] </ref>. This dag will have a tree partition with O (log n) height and have width equal to 3, with each node being labeled by an intersection or union operation. <p> A compress operation applied at a node v amounts to contracting v with its child, provided v has only one child. We refer the reader to Abrahamson et al. [1], Miller and Reif [21, 22], and Kosaraju and Delcher <ref> [19] </ref> for the details on how to use these two operations to reduce a constant fraction of the nodes in T in each round.
Reference: [20] <author> Leighton, F. T. </author> <title> Tight bounds on the complexity of parallel sorting. </title> <journal> IEEE Transactions on Computers C-34, </journal> <volume> 4 (1985), </volume> <pages> 344-354. </pages>
Reference-contexts: Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi [2], that one can sort in O (log n) time with an O (n log n) sized sorting network (see also Paterson [23]). In 1985 Leighton <ref> [20] </ref> extended this result to show that one can produce an O (n)-node bounded-degree network capable of sorting n numbers in O (log n) steps. One drawback of these algorithms, however, is that they are strongly dependent on expander graphs.
Reference: [21] <author> Miller, G. L., and Reif, J. H. </author> <title> Parallel tree contraction part 1: Fundamentals. </title> <journal> SIAM J. Comput. </journal> <volume> 5 (1989), </volume> <pages> 47-72. </pages>
Reference-contexts: We do not know of any previous parallel algorithms for this problem, but it appears that the array-based merging methods of Bilardi and Nicolau [6], Shiloach and Vishkin [25], or Hagerup and Rub [12] and the tree-contraction techniques of Abrahamson et al. [1], Miller and Reif <ref> [21, 22] </ref>, or Kosaraju and Delcher [19] will lead to an O (log 2 n) time solution with a time-processor product of O (n log n). We achieve O (log n) time while still maintaining the optimal time-processor product of O (n log n). <p> We can solve the set-expression evaluation problem by cascade merging in a dag with a bounded-width tree partition. Specifically, we convert the tree T into such an expression dag G using the rake-and-compress paradigm of Abrahamson et al. [1], Miller and Reif <ref> [21, 22] </ref>, and Kosaraju and Delcher [19]. This dag will have a tree partition with O (log n) height and have width equal to 3, with each node being labeled by an intersection or union operation. <p> A compress operation applied at a node v amounts to contracting v with its child, provided v has only one child. We refer the reader to Abrahamson et al. [1], Miller and Reif <ref> [21, 22] </ref>, and Kosaraju and Delcher [19] for the details on how to use these two operations to reduce a constant fraction of the nodes in T in each round.
Reference: [22] <author> Miller, G. L., and Reif, J. H. </author> <title> Parallel tree contraction II: Further applications. </title> <journal> SIAM J. Computing 20 (1991), </journal> <pages> 1128-1147. </pages>
Reference-contexts: We do not know of any previous parallel algorithms for this problem, but it appears that the array-based merging methods of Bilardi and Nicolau [6], Shiloach and Vishkin [25], or Hagerup and Rub [12] and the tree-contraction techniques of Abrahamson et al. [1], Miller and Reif <ref> [21, 22] </ref>, or Kosaraju and Delcher [19] will lead to an O (log 2 n) time solution with a time-processor product of O (n log n). We achieve O (log n) time while still maintaining the optimal time-processor product of O (n log n). <p> We can solve the set-expression evaluation problem by cascade merging in a dag with a bounded-width tree partition. Specifically, we convert the tree T into such an expression dag G using the rake-and-compress paradigm of Abrahamson et al. [1], Miller and Reif <ref> [21, 22] </ref>, and Kosaraju and Delcher [19]. This dag will have a tree partition with O (log n) height and have width equal to 3, with each node being labeled by an intersection or union operation. <p> A compress operation applied at a node v amounts to contracting v with its child, provided v has only one child. We refer the reader to Abrahamson et al. [1], Miller and Reif <ref> [21, 22] </ref>, and Kosaraju and Delcher [19] for the details on how to use these two operations to reduce a constant fraction of the nodes in T in each round.
Reference: [23] <author> Paterson, M. </author> <title> Improved sorting networks with o(log n) depth. </title> <journal> Algorithmica 5, </journal> <volume> 1 (1990), </volume> <pages> 75-92. </pages>
Reference-contexts: Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi [2], that one can sort in O (log n) time with an O (n log n) sized sorting network (see also Paterson <ref> [23] </ref>). In 1985 Leighton [20] extended this result to show that one can produce an O (n)-node bounded-degree network capable of sorting n numbers in O (log n) steps. One drawback of these algorithms, however, is that they are strongly dependent on expander graphs.
Reference: [24] <author> Sedgewick, R. </author> <title> Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: Our interest in sorting on the PPM is motivated by a desire for a parallel mergesort procedure that is more closely akin to the linked-list implementation of the sequential mergesort procedure (e.g., see Knuth [18] and Sedgewick <ref> [24] </ref>). We show that one can achieve this goal, for we give optimal O (log n)-time sorting algorithms for both the CREW PPM and EREW PPM models.
Reference: [25] <author> Shiloach, Y., and Vishkin, U. </author> <title> Finding the maximum, merging, and sorting in a parallel computation model. </title> <booktitle> Journal of Algorithms 2 (1981), </booktitle> <pages> 88-102. </pages>
Reference-contexts: This problem has applications in database querying and logic programming. We do not know of any previous parallel algorithms for this problem, but it appears that the array-based merging methods of Bilardi and Nicolau [6], Shiloach and Vishkin <ref> [25] </ref>, or Hagerup and Rub [12] and the tree-contraction techniques of Abrahamson et al. [1], Miller and Reif [21, 22], or Kosaraju and Delcher [19] will lead to an O (log 2 n) time solution with a time-processor product of O (n log n).
Reference: [26] <author> Tarjan, R. E. </author> <title> A class of algorithms which require nonlinear time to maintain disjoint sets. </title> <journal> J. Comput. System Sci. </journal> <volume> 18 (1979), </volume> <pages> 110-127. 42 </pages>
Reference-contexts: Just as the PRAM model is a generalization of the sequential RAM model, the Parallel Pointer Machine (or PPM) model is a generalization of the sequential Pointer Machine model described by Chazelle [8], which is similar to the Linking Automaton model of Knuth [17] and Tarjan <ref> [26] </ref>. In the PPM, one has a collection of synchronized processors that may access a common memory, just as in the PRAM model. In this model, however, the types of memory accesses are limited to those allowed by the Pointer Machine.
References-found: 26

