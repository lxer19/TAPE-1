URL: http://www-cs-students.stanford.edu/~csilvers/papers/chi2-sigmod.ps
Refering-URL: http://www-cs-students.stanford.edu/~csilvers/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: brin@cs.stanford.edu  motwani@cs.stanford.edu  csilvers@cs.stanford.edu  
Title: Beyond Market Baskets: Generalizing Association Rules to Correlations  
Author: Sergey Brin Rajeev Motwani Craig Silverstein 
Address: Stanford, CA 94305  Stanford, CA 94305  Stanford, CA 94305  
Affiliation: Department of Computer Science Stanford University  Department of Computer Science Stanford University  Department of Computer Science Stanford University  
Abstract: One of the most well-studied problems in data mining is mining for association rules in market basket data. Association rules, whose significance is measured via support and confidence, are intended to identify rules of the type, "A customer purchasing item A often also purchases item B." Motivated by the goal of generalizing beyond market baskets and the association rules used with them, we develop the notion of mining rules that identify correlations (generalizing associations), and we consider both the absence and presence of items as a basis for generating rules. We propose measuring significance of associations via the chi-squared test for correlation from classical statistics. This leads to a measure that is upward closed in the itemset lattice, enabling us to reduce the mining problem to the search for a border between correlated and uncorrelated itemsets in the lattice. We develop pruning strategies and devise an efficient algorithm for the resulting problem. We demonstrate its effectiveness by testing it on census data and finding term dependence in a corpus of text documents, as well as on synthetic data. 
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 207-216, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: An association rule <ref> [2] </ref> is intended to capture a certain type of dependence among items represented in the database B. <p> Researchers have taken advantage of this closure property in devising algorithms. Level-wise algorithms <ref> [2] </ref> find all items with a given property among itemsets of size i (i-itemsets), and use this knowledge to explore itemsets of size i+1 ((i+1)-itemsets). Another class of algorithms, random walk algorithms [14], generated a series of random walks, each of which explores the local structure of the border.
Reference: [3] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: a performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5 </volume> <pages> 914-925, </pages> <year> 1993. </year>
Reference: [4] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A.I. Verkamo. </author> <title> Fast Discovery of Association Rules. </title> <booktitle> In Fayyad et al [9], </booktitle> <pages> pages 307-328, </pages> <year> 1996. </year>
Reference: [5] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules in large databases. </title> <booktitle> In Proceedings of the 20th International Conference on Very Large Data Bases, </booktitle> <pages> pages 487-499, </pages> <month> September </month> <year> 1994. </year>
Reference: [6] <author> A. </author> <title> Agresti. A survey of exact inference for contingency tables. </title> <journal> Statistical Science, </journal> <volume> 7 </volume> <pages> 131-177, </pages> <year> 1992. </year>
Reference: [7] <author> M. Dietzfelbinger, A. Karlin, K. Mehlhorn, F. Meyer auf der Heide, H. Rohnert, and R. Tarjan. </author> <title> Dynamic perfect hashing: Upper and lower bounds. </title> <booktitle> In Proceedings of the 18th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 524-531, </pages> <year> 1988. </year>
Reference-contexts: If so, we add it to either sig or notsig for level i + 1, depending on its 2 value. The most expensive part of the algorithm is Step 8. We propose an implementation based on perfect hash tables (see <ref> [10, 7] </ref> for a description of the perfect hash function we used). In these hash tables, there are no collisions, and insertion, deletion, and lookup all take constant time. The space used is linear in the size of the data. Both notsig and cand are stored in hash tables.
Reference: [8] <author> R. Ewald. </author> <title> Keynote address. </title> <booktitle> The 3rd International Conference on Information and Knowledge Management, </booktitle> <year> 1994. </year>
Reference-contexts: We also demonstrate the effectiveness of our algorithms by experiments on census data and finding term dependency in a corpus of text documents. 1 A classic example is the rule that people who buy diapers in the afternoon are particularly likely to buy beer at the same time <ref> [8] </ref>. 1.1 Association Rules In order to place our work in the context of earlier work, it will be helpful to first review some of the details of the past work on association rules in the market basket application. For this purpose, we define basket data in general terms.
Reference: [9] <author> U.M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthrusamy. </author> <title> Advances in Knowledge Discovery and Data Mining. </title> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference: [10] <author> M. Fredman, J. Komlos, and E. Szemeredi. </author> <title> Storing a sparse table with O(1) worst case access time. </title> <journal> Journal of the ACM, </journal> <volume> 31(3) </volume> <pages> 538-544, </pages> <year> 1984. </year>
Reference-contexts: If so, we add it to either sig or notsig for level i + 1, depending on its 2 value. The most expensive part of the algorithm is Step 8. We propose an implementation based on perfect hash tables (see <ref> [10, 7] </ref> for a description of the perfect hash function we used). In these hash tables, there are no collisions, and insertion, deletion, and lookup all take constant time. The space used is linear in the size of the data. Both notsig and cand are stored in hash tables.
Reference: [11] <author> T. Fukuda, Y. Morimoto, S. Morishita, and T. Tokuyama. </author> <title> Mining Optimized Association Rules for Numeric Attributes. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Principles of Database Systems, </booktitle> <year> 1996. </year>
Reference-contexts: The past research has emphasized techniques for improving the performance of algorithms for discovering association rules in large databases of sales information. There has also been some work on extending this paradigm to numeric and geometric data <ref> [11, 12] </ref>. While Piatetsky-Shapiro and Frawley [26] define an "association problem" as finding recurring patterns in data, much of the recent work on mining of large-scale databases has concerned the important special case of finding association rules.
Reference: [12] <author> T. Fukuda, Y. Morimoto, S. Morishita, and T. Tokuyama. </author> <title> Mining optimized association rules for numeric data. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 13-24, </pages> <year> 1996. </year>
Reference-contexts: The past research has emphasized techniques for improving the performance of algorithms for discovering association rules in large databases of sales information. There has also been some work on extending this paradigm to numeric and geometric data <ref> [11, 12] </ref>. While Piatetsky-Shapiro and Frawley [26] define an "association problem" as finding recurring patterns in data, much of the recent work on mining of large-scale databases has concerned the important special case of finding association rules.
Reference: [13] <author> J. Gray, A. Bosworth, A. Layman, and H. Pirahesh. </author> <title> Data cube: a relational aggregation operator generalizing group-by, cross-tab, and sub-totals. </title> <type> Microsoft Technical Report MSR-TR-95-22, </type> <year> 1995. </year>
Reference-contexts: Typically, the applications involve large-scale information banks such as data warehouses [30] or datacubes <ref> [13] </ref>. One of the more well-studied problems in data mining is the search for association rules in market basket data [2, 3, 17, 20, 5, 15, 16, 24, 28, 27, 4, 29]. In this fl Supported by an NSF Fellowship. y Supported by an Alfred P. <p> It is also possible to walk down the itemset lattice by deleting items from an initial, full itemset. (It turns out that the random walk algorithm has a natural implementation in terms of a datacube <ref> [13] </ref>; a connection we intend to explore in a later paper.) Both level-wise and random walk algorithms use knowledge of a set and its closure properties to make inferences about its supersets. Downward closure is a pruning property.
Reference: [14] <author> D. Gunopulos, H. Mannila, and S. Saluja. </author> <title> Discovering all most specific sentences by randomized algorithms. </title> <booktitle> In Proceedings of the 6th International Conference on Database Theory, to appear, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: Researchers have taken advantage of this closure property in devising algorithms. Level-wise algorithms [2] find all items with a given property among itemsets of size i (i-itemsets), and use this knowledge to explore itemsets of size i+1 ((i+1)-itemsets). Another class of algorithms, random walk algorithms <ref> [14] </ref>, generated a series of random walks, each of which explores the local structure of the border. A random walk is a walk up the itemset lattice. It starts with the empty itemset and adds items one at a time to form a larger itemset. <p> Since this property is not downward closed, it would not be effective at pruning in a level-wise al gorithm. A random walk algorithm, for instance <ref> [14] </ref>, might be appropriate for this kind of pruning. Combining the chi-squared correlation rule with pruning via support, we obtain the algorithm in Figure 1. We say that an itemset is significant if it is supported and minimally correlated.
Reference: [15] <author> J. Han and Y. Fu. </author> <title> Discovery of multiple-level association rules from large databases. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases, </booktitle> <pages> pages 420-431, </pages> <month> September </month> <year> 1995. </year>
Reference: [16] <author> M. Houtsma and A. Swami. </author> <title> Set-oriented mining of association rules. </title> <booktitle> In Proceedings of the International Conference on Data Engineering, </booktitle> <pages> pages 25-34, </pages> <year> 1995. </year>
Reference: [17] <author> M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivonen, and A.I. Verkamo. </author> <title> Finding interesting rules from large sets of discovered association rules. </title> <booktitle> In Proceedings of the 3rd International Conference on Information and Knowledge Management, </booktitle> <pages> pages 401-407, </pages> <year> 1994. </year>
Reference: [18] <author> H.O. Lancaster. </author> <title> The Chi-squared Distribution. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1969. </year>
Reference-contexts: In the coffee and tea example, we calculated a correlation value but could not tell whether it was statistically significant. Testing for significant correlation is a problem statisticians have been studying for over a century; refer to Lancaster <ref> [18] </ref> for the theory and a history of this problem. The preferred test for correlation involves the chi-squared statistic, which is both easy to calculate and reliable under a fairly permissive set of assumptions.
Reference: [19] <editor> P.S. de Laplace. Oeuvres completes de Laplace publiees sous les auspices de l'Academie des Sciences par M.M. les secretaires perpetuels. Gauthier-Villar, </editor> <address> Paris, </address> <year> 1878/1912. </year>
Reference: [20] <author> H. Mannila, H. Toivonen, and A. Inkeri Verkamo. </author> <title> Efficient algorithms for discovering association rules. </title> <booktitle> In Proceedings of the AAAI Workshop on Knowledge Discovery in Databases, </booktitle> <pages> pages 144-155, </pages> <month> July </month> <year> 1994. </year>
Reference: [21] <editor> A. de Moivre. Approximatio ad summam terminorum bi-nomii (a + b) n in seriem expansi. </editor> <title> Supplement to Miscellanea Analytica, </title> <journal> London, </journal> <volume> 1733. </volume>
Reference: [22] <author> D.S. Moore. </author> <title> Tests of chi-squared type. </title> <editor> In: R.B. D'Agostino and M.A. Stephens (eds), </editor> <title> Goodness-of-Fit Techniques, </title> <publisher> Mar-cel Dekker, </publisher> <address> New York, </address> <year> 1986, </year> <pages> pp. </pages> <month> 63-95.. </month>
Reference-contexts: In this case, the chi-squared test rests on the normal approximation to the binomial distribution (more precisely, to the hypergeometric distribution). This approximation breaks down when the expected values are small. As a rule of thumb, statistics texts (such as Moore <ref> [22] </ref>) recommend the use of chi-squared test only if * all cells in the contingency table have expected value greater than 1; * and, at least 80% of the cells in the contingency table have expected value greater than 5. For association rules, these conditions will frequently be broken.
Reference: [23] <author> F. Mosteller and D. Wallace. </author> <title> Inference and Disputed Authorship: The Federalists. </title> <publisher> Addison-Wesley, </publisher> <year> 1964. </year>
Reference: [24] <author> J. S. Park, M. S. Chen, and P. S. Yu. </author> <title> An effective hash based algorithm for mining association rules. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 175-186, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Step 8 can be implemented efficiently using hashing. Overall, the running time for level i is O (n jcandj minfn; 2 i g + i jnotsigj 2 ). It is instructive to compare the algorithm in Figure 1 to the hash-based algorithm of Park, Chen, and Yu <ref> [24] </ref> for the support-confidence framework. Their algorithm also uses hashing to construct a candidate set cand, which they then iterate over to verify the results. One difference is that verification is easier in their case, since they only need to test support. <p> One difference is that verification is easier in their case, since they only need to test support. We also need to test chi-squared values, a more expensive operation that makes careful construction of cand more important. Another difference is we use perfect hashing while Park, Chen, and Yu <ref> [24] </ref> allow collisions. While collisions reduce the effectiveness of pruning, they do not affect the final result. The advantage of allowing collisions is that the hash table may be smaller. Hashing with collisions is necessary when the database is much larger than main memory.
Reference: [25] <author> K. Pearson. </author> <title> On a criterion that a given system of deviations from the probable in the case of correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. </title> <journal> Philos. Mag., </journal> <volume> 5 </volume> <pages> 157-175, </pages> <year> 1900. </year>
Reference: [26] <author> G. Piatetsky and W. Frawley. </author> <title> Knowledge Discovery in Databases. </title> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: The past research has emphasized techniques for improving the performance of algorithms for discovering association rules in large databases of sales information. There has also been some work on extending this paradigm to numeric and geometric data [11, 12]. While Piatetsky-Shapiro and Frawley <ref> [26] </ref> define an "association problem" as finding recurring patterns in data, much of the recent work on mining of large-scale databases has concerned the important special case of finding association rules.
Reference: [27] <author> A. Savasere, E. Omiecinski, and S. Navathe. </author> <title> An efficient algorithm for mining association rules in large databases. </title> <booktitle> In Proceedings of the International Conference on Very Large Data Bases, </booktitle> <pages> pages 432-444, </pages> <year> 1995. </year>
Reference: [28] <author> R. Srikant and R. Agrawal. </author> <title> Mining generalized association rules. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases, </booktitle> <pages> pages 407-419, </pages> <month> September </month> <year> 1995. </year>
Reference: [29] <author> H. Toivonen. </author> <title> Sampling large databases for finding association rules. </title> <booktitle> In Proceedings of the 22nd International Conference on Very Large Data Bases, </booktitle> <pages> pages 134-145, </pages> <month> September </month> <year> 1996. </year>
Reference: [30] <author> J. Widom. </author> <title> Research problems in data warehousing. </title> <booktitle> In Proceedings of the 4th Conference on Information and Knowledge Management, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: Typically, the applications involve large-scale information banks such as data warehouses <ref> [30] </ref> or datacubes [13]. One of the more well-studied problems in data mining is the search for association rules in market basket data [2, 3, 17, 20, 5, 15, 16, 24, 28, 27, 4, 29]. In this fl Supported by an NSF Fellowship. y Supported by an Alfred P.
References-found: 29

