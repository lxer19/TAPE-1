URL: http://www.swi.psy.uva.nl/usr/maarten/ida.ps
Refering-URL: http://www.swi.psy.uva.nl/usr/maarten/papers.html
Root-URL: 
Email: email: maarten@swi.psy.uva.nl  email: F.Verdenius@ato.dlo.nl  
Phone: 2  
Title: A systematic description of greedy optimisation algorithms for cost sensitive generalisation  
Author: Maarten van Someren Cristina Torres ; and Floor Verdenius 
Address: Roetersstraat 15, 1018 WB Amsterdam, The Netherlands  Postbox 17, 6700 AA Wageningen, The Netherlands  
Affiliation: 1 Department of Social Science Informatics (SWI) Faculty of Psychology, University of Amsterdam  AgroTechnological Research Institute (ATO-DLO)  
Abstract: This paper defines a class of problems involving combinations of induction and (cost) optimisation. A framework is presented that systematically describes problems that involve construction of decision trees or rules, optimising accuracy as well as measurement- and misclassification costs. It does not present any new algorithms but shows how this framework can be used to configure greedy algorithms for constructing such trees or rules. The framework covers a number of existing algorithms. Moreover, the framework can also be used to define algorithm configurations with new functionalities, as expressed in their evaluation functions.
Abstract-found: 1
Intro-found: 1
Reference: [Langley, 1996] <author> Langley, P. </author> <year> (1996). </year> <title> Elements of machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco. </address>
Reference-contexts: Then, in section 4 the learning goals associated with the different problem types are formulated in one criterion, and in section 5 we show how these descriptions can be used to configure algorithms that can be used to solve inductive optimisation problems in the style of <ref> [Langley, 1996] </ref>. Section 6 shows how a number of existing techniques are covered by the framework, and the paper ends with some concluding remarks. 2 Inductive optimisation tasks Inductive optimisation problems concern the construction of a model from a sample of data. The data consist of a set of cases.
Reference: [Nunez, 1991] <author> Nunez, M. </author> <year> (1991). </year> <title> The use of background knowledge in decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 231-250. </pages>
Reference-contexts: On the other hand they are more expensive. Greedy algorithms have a good track record as offering a good trade-off between computation costs and quality of the result. 6 Related systems Our framework gives a systematic description that covers a number of existing systems. EG2 <ref> [Nunez, 1991] </ref> is a TDIDT algorithm that uses attribute measuring costs in generating a decision tree.
Reference: [Quinlan, 1986] <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: The model can take the form of a decision tree, a rule set, a rule or a vector. These languages are equally expressive but there is a difference in how they are acquired (see section 5) which leads to different models. A decision tree <ref> [Quinlan, 1986] </ref> consists of nodes and branches. Each node corresponds to a test on the value of an attribute. The starting node is called the root of the tree. The branches represent the possible values of the attribute. The leaves of the tree are associated with classes.
Reference: [Quinlan, 1992] <author> Quinlan, J. </author> <year> (1992). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco. </address>
Reference-contexts: The results will therefore be higher than the actual accuracy. The confidence interval approach is to construct a confidence interval for the proportion correct predictions at a node, based on the subset of the original data that is associated with this node (e.g. see <ref> [Quinlan, 1992] </ref>). It has two problems.
Reference: [Riddle et al., 1994] <author> Riddle, P., Segal, R., and Etzioni, O. </author> <year> (1994). </year> <title> Representation de sign and brute-force induction in a boeing manufacturing domain. </title> <journal> Applied Artificial Intelligence, </journal> <volume> 8 </volume> <pages> 125-147. </pages>
Reference-contexts: This description does not need to distinguish between the class it described and other classes but only to give a description that covers it well. Riddle et. al. <ref> [Riddle et al., 1994] </ref> point out that there are situations in which cases belonging to a special class, the target class, must be identified, but a characterisation of the whole input space is not required. <p> We refer to the method as AFMC, that stands for Accounting for Misclassification Costs. This system uses misclassification costs in evaluating possible decision trees. Similar notions are also worked out in [Schiffers, 1997]. Gold-digger <ref> [Riddle et al., 1994] </ref> is an algorithm that generates predictive rules, called nuggets, by looking for test/outcome combinations that are characteristic for the target class. Several nuggets are obtained by applying the algorithm iteratively. The selection function is maximal classCoverage.
Reference: [Roberts et al., 1995] <author> Roberts, H., Denby, M., and Totton, K. </author> <year> (1995). </year> <title> Accounting for misclassification costs in decision tree classifiers. </title> <editor> In Lasker, G. and Liu, X., editors, </editor> <booktitle> Proceedings of the International Symposium on Intelligent Data Analysis, IDA-95, </booktitle> <pages> pages 149-156. </pages> <note> [Schiffers, 1997] Schiffers, J. </note> <year> (1997). </year> <title> A classification approach incorporating misclassification cost. </title> <journal> Intelligent Data Analysis, </journal> <volume> Vol. 1, No. </volume> <pages> 1, </pages> <address> http://www.elsevier.com/locate/ida </address>
Reference-contexts: P probability (Leaf) fl pathM easuringCost (Leaf) N (Leaves) (5) where N (Leaves) is the number of leaves. misclassificationCost (Leaf): all cases at this leaf will be classified as the most frequent class and therefore all cases with different classes will be misclassified (as defined by <ref> [Roberts et al., 1995] </ref>). <p> EG2 [Nunez, 1991] is a TDIDT algorithm that uses attribute measuring costs in generating a decision tree. EG2 generates a decision tree with the following selection function: measurementCost (Attribute) + (w fi informationGain) (9) Roberts et al. <ref> [Roberts et al., 1995] </ref> propose an approach for integrating mis-classification costs in a tree induction algorithm. We refer to the method as AFMC, that stands for Accounting for Misclassification Costs. This system uses misclassification costs in evaluating possible decision trees. Similar notions are also worked out in [Schiffers, 1997].
Reference: [Torres and Verdenius, 1996] <author> Torres, C. and Verdenius, F. </author> <year> (1996). </year> <title> Selecting decision tree based learning algorithms. </title> <editor> In van der Herik, H., van den Bosch, A., and Weijters, T., editors, </editor> <booktitle> Proceedings Benelearn-1996, </booktitle> <address> Maastricht. </address> <publisher> RU Maastricht. </publisher>
Reference: [Turney, 1995] <author> Turney, P. D. </author> <year> (1995). </year> <title> Cost sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm. </title> <journal> Journal of AI Research, </journal> <volume> 2 </volume> <pages> 369-409. </pages>
Reference-contexts: Wether other costs become important depends on the problem at hand. Achievement costs behave as dynamical misclassification costs. In evaluation criteria achievement costs are treated similarly to misclassification costs <ref> [Turney, 1995] </ref> . We present a language that systematically describes these inductive optimi-sation problems. To see why a systematic description of inductive techniques is useful, consider the following problem, taken from our experience.
Reference: [Verdenius, 1991] <author> Verdenius, F. </author> <year> (1991). </year> <title> A method for inductive cost optimization. </title> <editor> In Kodratoff, Y., editor, </editor> <booktitle> Proceedings of the Fifth European Working Session on Learning EWSL-91, </booktitle> <pages> pages 179-191, </pages> <publisher> Berlin. </publisher> <editor> Springer Verlag. </editor> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Several nuggets are obtained by applying the algorithm iteratively. The selection function is maximal classCoverage. It was successfully applied to find the cause of faults in a production process. The MICO system <ref> [Verdenius, 1991] </ref> is based on the idea of finding a rule that maximises probability of a class while minimising attribute-value costs. This system was used to find optimal values for parameters of a production process from experiments with different parameter settings.
References-found: 9

