URL: ftp://ftp.cs.dartmouth.edu/TR/TR97-304.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR97-304/
Root-URL: http://www.cs.dartmouth.edu
Title: Automated Parallelization of Discrete State-space Generation  
Author: David M. Nicol Gianfranco Ciardo 
Date: January 22, 1997  
Address: Hanover, NH 03755 Williamsburg, VA 23185  
Affiliation: Dartmouth College Department of Computer Science  Dartmouth College College of William and Mary  
Pubnum: Technical Report PCS-TR97-304  
Abstract: We consider the problem of generating a large state-space in a distributed fashion. Unlike previously proposed solutions that partition the set of reachable states according to a hashing function provided by the user, we explore heuristic methods that completely automate the process. The first step is an initial random walk through the state space to initialize a search tree, duplicated in each processor. Then, the reachability graph is built in a distributed way, using the search tree to assign each newly found state to classes assigned to the available processors. Furthermore, we explore two remapping criteria that attempt to balance memory usage or future workload, respectively. We show how the cost of computing the global snapshot required for remapping will scale up for system sizes in the foreseeable future. An extensive set of results is presented to support our conclusions that remapping is extremely beneficial. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G.E. Blelloch. </author> <title> Scans as primitive parallel operations. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 38(11) </volume> <pages> 1526-1538, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Each processor has a picture of the global load distribution, and when implementing a remapping, a piece of workload or data goes directly from its source processor to its target processor. Direct methods have been considered in the context of large scale parallel processing systems <ref> [7, 1, 15] </ref>, but these do not take a global view. <p> Using synchronized random number generator seeds, each processor generates the same permutation vector idx [] to randomize the ordering of state components. Thus, idx [0] holds the index of the first state component examined in the lexicographical comparison, idx <ref> [1] </ref> holds the second index examined, and so on. We found this a valuable tool to protect us from the potential problems arising from correlation of states' components (consider|hashing on a vector of components whose values are highly correlated effectively diminishes the "spreading" of the hashing function.
Reference: [2] <author> G. Ciardo, J. Gluckman, and D. Nicol. </author> <title> Distributed state-space generation of discrete-state stochastic models. </title> <journal> INFORMS Journal on Computing, </journal> <note> 1997. To appear. </note>
Reference-contexts: grants CCR-9201195 and NCR-9527163, in part by a subcontract on the grant 96-SC-NSF-1011 to the Center for Advanced Computing and Communication, and in part by NASA Contract S1-19480 to the Institute for Computer Applications in Science and Engineering. 1 using a user-defined hash-function that statically partitions the graph among processors <ref> [2] </ref>. Given a generated state (typically a vector of integers), the hash-function identified the processor to which the state was statically assigned. While we showed that a well-tuned hash-function can effectively balance the workload partition and achieve reasonable execution time efficiencies as well, the method suffers from some obvious drawbacks. <p> We stress that the order in which states are found is in no way related to the lexicographic order used in the search tree. The application-specific part of state-space generation can be separated from the common aspects through a well-defined interface, such as that we describe in <ref> [2] </ref>. One benefit is the immunization of the system modeler from details concerning parallelization. The techniques we describe in this paper follow in a similar vein and have been implemented in a manner that supports integration 3 with any modeling front-end that correctly uses the interface. <p> Livelock is also possible and is detected by identifying transient states (ones that may become unreachable once left). Resource requirements can be extracted from an understanding of the relationship between state expression and resources. Parallel algorithms for performing these types of analyses are discussed in our earlier paper <ref> [2] </ref>. When the state-space represents an ergodic continuous-time Markov chain one typically solves a set of linear equations (the global flow-balance equations [22]) that associate a stationary probability with each node in the graph.
Reference: [3] <author> G. Ciardo and K. Trivedi. </author> <title> A decomposition approach for stochastic reward net models. Performance Evaluation, </title> <booktitle> 18(1) </booktitle> <pages> 37-59, </pages> <year> 1993. </year>
Reference-contexts: For the purposes of comparison with our earlier approach, we will study a model of a Flexible Manufacturing System, illustrated in Figure 6, originally discussed in <ref> [3] </ref>. A key parameter to this model is the number of tokens k initially placed in places P1, P2, and P3; increasingly larger state-spaces are generated by increasing k. This Petri net has "timed" transitions (white boxes) and "immediate" transitions (black boxes).
Reference: [4] <author> G. Ciardo, K. Trivedi, and J. Muppala. SPNP: </author> <title> Stochastic Petri net package. </title> <booktitle> In Proceedings of the 3 rd Int. Workshop on Petri Nets and Performance Models, </booktitle> <pages> pages 142-151, </pages> <address> Kyoto, Japan, </address> <month> December </month> <year> 1989. </year> <note> IEEE Press. </note>
Reference-contexts: Our methods include innovations in the mapping of states to processors, and policies for remapping workload. Our methods have been embedded in a general distributed state-space generation tool which itself has been integrated as the back-end of a stochastic Petri net modeling tool, spnp <ref> [4] </ref>. We examine the utility and costs of dynamic remapping for the tool executing on an IBM SP-2, considering effectiveness in both memory balancing and run-time parallelism. We find that our automated methods are much better than optimally hand-tuned static methods with respect to both memory balance and speedup.
Reference: [5] <author> G. Cybenko. </author> <title> Dynamic load balancing for distributed memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 7(2) </volume> <pages> 279-301, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: A synchronous view of remapping was taken in work on decision policies that focus on when to remap [18, 19, 20]; balancing the delay cost of remapping against the anticipated performance gain is the essence of these policies. Globally synchronous remapping techniques are developed in <ref> [5, 25] </ref>. These methods iterate; at each iteration, pairs of processors balance workload between them, and ultimately some global sense of balance is achieved without any of the processors ever having had a global view of the system.
Reference: [6] <author> D.L. Eager, E.D. Lazowska, and J. Zahorjan. </author> <title> Adaptive load sharing in homogeneous distributed systems. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-12(5):662-675, </volume> <year> 1986. </year>
Reference-contexts: These issues are developed in the following sections, after we discuss related work. Early work on load-sharing had a world view of independent jobs in a distributed system. 4 The main interest was in job migration policies <ref> [23, 6, 13] </ref> that best utilized system resources. The algorithms were asynchronous, workload moved without global synchronization and without a global view of the system; key issues were how transfers are initiated, and what information is used to govern those initiations.
Reference: [7] <author> D. Gerogiannis and S. Orphanoudakis. </author> <title> Load balancing requirements in parallel implementations of image feature extraction tasks. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(9) </volume> <pages> 994-1013, </pages> <year> 1993. </year>
Reference-contexts: Each processor has a picture of the global load distribution, and when implementing a remapping, a piece of workload or data goes directly from its source processor to its target processor. Direct methods have been considered in the context of large scale parallel processing systems <ref> [7, 1, 15] </ref>, but these do not take a global view.
Reference: [8] <author> Gropp, Lusk, and Skjellum. </author> <title> Using MPI. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass, </address> <year> 1994. </year>
Reference-contexts: The mechanisms we use to accomplish the global dissemination are standard vector-valued reduction primitives that have long been found in message-passing libraries such as the Intel nx library, and MPI <ref> [8] </ref>. Only experimentation on very large parallel computers will definitively answer whether such methods are appropriate on such machines. We formerly investigated two styles of remapping decision policies.
Reference: [9] <author> E. Horowitz and S. Sahni. </author> <title> Fundamentals of Data Structures. </title> <publisher> Computer Science Press, </publisher> <address> Po-tomac, Maryland, </address> <year> 1978. </year> <month> 23 </month>
Reference-contexts: As our implementation uses AVL trees <ref> [9] </ref>, our description applies specifically to these, as well as certain other commonly used search trees. One uses an application-specific comparison operator such that for any state vectors A and B, either A &lt; B, A &gt; B, or A = B.
Reference: [10] <author> L. Kale. </author> <title> Comparing the performance of two dynamic load distribution methods. </title> <booktitle> In Proceedings of the 1988 Int. Conference on Parallel Processing, </booktitle> <pages> pages 8-12, </pages> <year> 1988. </year>
Reference-contexts: Our application and parallelized branch-and-bound computations share the problem that at a point when one remaps, future workload is unknown. Various methods for approaching parallel branch-and-bound have been studied <ref> [11, 10, 14, 21] </ref>. A critical difference between our application and branch-and-bound is that we must store the state-space we explore, that is not typically done in branch-and-bound. A branch-and-bound algorithm may generate a given subproblem (and then all of its descendents) multiple times; we do not.
Reference: [11] <author> R. Karp and Y. Zhang. </author> <title> A randomized parallel branch-and-bound procedure. </title> <booktitle> In Proceedings of the 20 th Annual ACM Symp. on Theory of Computing, </booktitle> <pages> pages 290-300, </pages> <year> 1988. </year>
Reference-contexts: Our application and parallelized branch-and-bound computations share the problem that at a point when one remaps, future workload is unknown. Various methods for approaching parallel branch-and-bound have been studied <ref> [11, 10, 14, 21] </ref>. A critical difference between our application and branch-and-bound is that we must store the state-space we explore, that is not typically done in branch-and-bound. A branch-and-bound algorithm may generate a given subproblem (and then all of its descendents) multiple times; we do not.
Reference: [12] <author> V. Kumar, A. Grama, and N. Vempaty. </author> <title> Scalable load balancing techniques for parallel computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22 </volume> <pages> 60-79, </pages> <year> 1994. </year>
Reference-contexts: The algorithms were asynchronous, workload moved without global synchronization and without a global view of the system; key issues were how transfers are initiated, and what information is used to govern those initiations. More recent work has a different view of workload, but has continued in the asynchronous vein <ref> [24, 12] </ref>. A synchronous view of remapping was taken in work on decision policies that focus on when to remap [18, 19, 20]; balancing the delay cost of remapping against the anticipated performance gain is the essence of these policies. Globally synchronous remapping techniques are developed in [5, 25].
Reference: [13] <author> F. Lin and R. Keller. </author> <title> The gradient model load balancing method. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-13(1), </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: These issues are developed in the following sections, after we discuss related work. Early work on load-sharing had a world view of independent jobs in a distributed system. 4 The main interest was in job migration policies <ref> [23, 6, 13] </ref> that best utilized system resources. The algorithms were asynchronous, workload moved without global synchronization and without a global view of the system; key issues were how transfers are initiated, and what information is used to govern those initiations.
Reference: [14] <author> R. Luling and B. Monien. </author> <title> Load balancing for distributed branch-and-bound. </title> <booktitle> In Proceedings of 6 th Int. Parallel Processing Symposium, </booktitle> <pages> pages 543-548, </pages> <year> 1992. </year>
Reference-contexts: Our application and parallelized branch-and-bound computations share the problem that at a point when one remaps, future workload is unknown. Various methods for approaching parallel branch-and-bound have been studied <ref> [11, 10, 14, 21] </ref>. A critical difference between our application and branch-and-bound is that we must store the state-space we explore, that is not typically done in branch-and-bound. A branch-and-bound algorithm may generate a given subproblem (and then all of its descendents) multiple times; we do not.
Reference: [15] <author> D. Nicol. </author> <title> Communication efficient global load balancing. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference. </booktitle> <publisher> IEEE Press, </publisher> <month> April </month> <year> 1992. </year>
Reference-contexts: Each processor has a picture of the global load distribution, and when implementing a remapping, a piece of workload or data goes directly from its source processor to its target processor. Direct methods have been considered in the context of large scale parallel processing systems <ref> [7, 1, 15] </ref>, but these do not take a global view.
Reference: [16] <author> D. Nicol. </author> <title> Non-committal barrier synchronization. </title> <journal> Parallel Computing, </journal> <volume> 21 </volume> <pages> 529-549, </pages> <year> 1995. </year>
Reference-contexts: There are a variety of methods one might use to detect termination; as long as its cost is not intrusive, the choice matters little. We use the non-committal barrier synchronization <ref> [16] </ref> as that was easy to integrate into the distributed 8 generation logic. The state-classification method may be extended to allow the control set to grow dynamically, an extension we hope to investigate in the near future.
Reference: [17] <author> D. Nicol, R. Simha, and D. Towsley. </author> <title> Static assignment of stochastic tasks using majorization. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 45(6) </volume> <pages> 730-740, </pages> <year> 1996. </year>
Reference-contexts: When we do balance, we attempt to balance nearly perfectly. There is compelling theoretical evidence that in a computation of this sort we should attempt to redress any rebalance; the model studied in <ref> [17] </ref> shows that under workload growth not dissimilar to ours, the maximum memory use by any processor is stochastically smaller the closer to perfect balance we can achieve with a remapping.
Reference: [18] <author> D.M. Nicol and P.F Reynolds, Jr. </author> <title> Optimal dynamic remapping of data parallel computations. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39(2) </volume> <pages> 206-219, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: More recent work has a different view of workload, but has continued in the asynchronous vein [24, 12]. A synchronous view of remapping was taken in work on decision policies that focus on when to remap <ref> [18, 19, 20] </ref>; balancing the delay cost of remapping against the anticipated performance gain is the essence of these policies. Globally synchronous remapping techniques are developed in [5, 25]. <p> This policy focuses entirely on minimizing execution time, and we saw no clean way of extending it to encompass memory balance. Policies that weigh the cost of remapping against the anticipated benefit of remapping over the remaining lifetime of the computation are investigated in <ref> [18] </ref>. These policies principally account for the error that is natural in statistical measurements. On reflection we realized that the decision of whether to remap is fundamentally a choice of remapping frequency. Graph generation can be a long-lived computation with load constantly drifting out of balance.
Reference: [19] <author> D.M. Nicol and J.H. Saltz. </author> <title> Dynamic remapping of parallel computations with varying resource demands. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 37(9) </volume> <pages> 1073-1087, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: More recent work has a different view of workload, but has continued in the asynchronous vein [24, 12]. A synchronous view of remapping was taken in work on decision policies that focus on when to remap <ref> [18, 19, 20] </ref>; balancing the delay cost of remapping against the anticipated performance gain is the essence of these policies. Globally synchronous remapping techniques are developed in [5, 25]. <p> Only experimentation on very large parallel computers will definitively answer whether such methods are appropriate on such machines. We formerly investigated two styles of remapping decision policies. Papers <ref> [19] </ref> and [20] balance 9 the delay cost of remapping against the performance degradation due to load imbalance and global blocking at the problem's natural synchronization points. This policy focuses entirely on minimizing execution time, and we saw no clean way of extending it to encompass memory balance.
Reference: [20] <author> D.M. Nicol, J.H. Saltz, and J. Townsend. </author> <title> Delay point schedules for irregular parallel computations. </title> <journal> Int'l Journal of Parallel Programming, </journal> <volume> 18(1) </volume> <pages> 69-90, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: More recent work has a different view of workload, but has continued in the asynchronous vein [24, 12]. A synchronous view of remapping was taken in work on decision policies that focus on when to remap <ref> [18, 19, 20] </ref>; balancing the delay cost of remapping against the anticipated performance gain is the essence of these policies. Globally synchronous remapping techniques are developed in [5, 25]. <p> Only experimentation on very large parallel computers will definitively answer whether such methods are appropriate on such machines. We formerly investigated two styles of remapping decision policies. Papers [19] and <ref> [20] </ref> balance 9 the delay cost of remapping against the performance degradation due to load imbalance and global blocking at the problem's natural synchronization points. This policy focuses entirely on minimizing execution time, and we saw no clean way of extending it to encompass memory balance.
Reference: [21] <author> N. Rao and V. Kumar. </author> <title> Parallel depth-first-search, part I: Implementation. </title> <journal> Int. Journal of Parallel Programming, </journal> <volume> 16(6) </volume> <pages> 479-499, </pages> <year> 1987. </year>
Reference-contexts: Our application and parallelized branch-and-bound computations share the problem that at a point when one remaps, future workload is unknown. Various methods for approaching parallel branch-and-bound have been studied <ref> [11, 10, 14, 21] </ref>. A critical difference between our application and branch-and-bound is that we must store the state-space we explore, that is not typically done in branch-and-bound. A branch-and-bound algorithm may generate a given subproblem (and then all of its descendents) multiple times; we do not.
Reference: [22] <author> H.S. Ross. </author> <title> Stochastic Processes. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Parallel algorithms for performing these types of analyses are discussed in our earlier paper [2]. When the state-space represents an ergodic continuous-time Markov chain one typically solves a set of linear equations (the global flow-balance equations <ref> [22] </ref>) that associate a stationary probability with each node in the graph. Methods for solving linear equations in parallel are standard and will not be discussed here except to say that good performance can be expected if the graph clusters well and the states are evenly balanced among processors. <p> We may write p s &gt; E n C D = d=1 PrfD = dgE " n # Jensen's Inequality <ref> [22] </ref> states that if g (x) is convex, then for any random variable where E [X] exists, E [g (x)] g (E [X]); thus for every d, E n n .
Reference: [23] <author> J. A. Stankovic. </author> <title> An application of bayesian decision theory to decentralized control of job scheduling. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-34(2):117-130, </volume> <month> February </month> <year> 1985. </year>
Reference-contexts: These issues are developed in the following sections, after we discuss related work. Early work on load-sharing had a world view of independent jobs in a distributed system. 4 The main interest was in job migration policies <ref> [23, 6, 13] </ref> that best utilized system resources. The algorithms were asynchronous, workload moved without global synchronization and without a global view of the system; key issues were how transfers are initiated, and what information is used to govern those initiations.
Reference: [24] <author> M. Willebeek-LeMair and A. Reeves. </author> <title> Strategies for dynamic load balancing on highly parallel computers. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(9) </volume> <pages> 979-993, </pages> <year> 1993. </year>
Reference-contexts: The algorithms were asynchronous, workload moved without global synchronization and without a global view of the system; key issues were how transfers are initiated, and what information is used to govern those initiations. More recent work has a different view of workload, but has continued in the asynchronous vein <ref> [24, 12] </ref>. A synchronous view of remapping was taken in work on decision policies that focus on when to remap [18, 19, 20]; balancing the delay cost of remapping against the anticipated performance gain is the essence of these policies. Globally synchronous remapping techniques are developed in [5, 25].
Reference: [25] <author> C.-Z. Xu and F. Lau. </author> <title> Analysis of the generalized dimension exchange method for dynamic load balancing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16(4) </volume> <pages> 385-393, </pages> <year> 1992. </year> <month> 24 </month>
Reference-contexts: A synchronous view of remapping was taken in work on decision policies that focus on when to remap [18, 19, 20]; balancing the delay cost of remapping against the anticipated performance gain is the essence of these policies. Globally synchronous remapping techniques are developed in <ref> [5, 25] </ref>. These methods iterate; at each iteration, pairs of processors balance workload between them, and ultimately some global sense of balance is achieved without any of the processors ever having had a global view of the system.
Reference: [26] <author> C.-Z. Xu and F. Lau. </author> <title> Iterative dynamic load balancing in multicomputers. </title> <journal> Journal of Opera--tional Research Society, </journal> <volume> 45(7) </volume> <pages> 786-796, </pages> <month> July </month> <year> 1994. </year> <month> 25 </month>
Reference-contexts: A variety of remapping problems have been explored in the literature, see <ref> [26] </ref> for a survey. One important characteristic is whether the method is synchronous or asynchronous. We are driven towards a synchronous approach because of the overwhelming problem of keeping state location information up-to-date. At any time, any processor might generate any state, and need to send it anywhere.
References-found: 26

