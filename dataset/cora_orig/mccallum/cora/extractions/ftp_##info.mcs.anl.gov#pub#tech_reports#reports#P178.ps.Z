URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P178.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/preprints.htm
Root-URL: http://www.mcs.anl.gov
Title: STABLE PARALLEL ALGORITHMS FOR TWO-POINT BOUNDARY VALUE PROBLEMS  
Author: STEPHEN J. WRIGHT 
Keyword: Key words. parallel algorithms, two-point boundary value problems, error analysis and stability  
Date: 1992 012  
Note: SIAM J. SCI. STATIST. COMPUT. c 1991 Society for Industrial and Applied Mathematics Vol. 0, No. 0, pp. xxx-xxx, Month  AMS(MOS) subject classifications. 65F05, 65G05, 65L10, 65L20, 65W05  
Abstract: Some of the most widely used algorithms for two-point boundary value ODEs, namely finite difference and collocation methods and standard multiple shooting, proceed by setting up and solving a structured system of linear equations. It is well known that the linear system can be set up efficiently in parallel; we show here that a structured orthogonal factorization technique can be used to solve this system, and hence the overall problem, in an efficient, parallel, and stable way. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. M. Ascher and P. S. Y. Chan, </author> <title> On parallel methods for boundary value odes, </title> <journal> Computing, </journal> <volume> 46 (1991), </volume> <pages> pp. 1-17. </pages>
Reference-contexts: Ascher and Mattheij [2] develop a "theoretical multiple shooting" framework in which they show how boundary values for the sub-BVPs should be chosen to ensure well-conditioning. Ascher and Chan <ref> [1] </ref> suggest how to implement this in a parallel environment. 8 STEPHEN J. WRIGHT Another possibility, which leads to near-perfect speedup on two processors (but cannot be generalized to a higher order of parallelism) is the "burn at both ends" or "twisted" factorization. <p> When the factorizations meet in the center, a small reduced system is formed and factored. This is analogous to the approach of Lentini [11]. Finally, we mention the approach of Ascher and Chan <ref> [1] </ref>, who form the normal equations for (5) and (8), and factorize the resulting symmetric, positive definite, block-tridiagonal system using cyclic reduction. <p> The DECOMP routine uses alternate row and column pivoting (as does the algorithm described in Varah [19] and in x2) but always eliminates by rows. Approximate operation counts and storage requirements for these algorithms and the normal equations method of Ascher and Chan <ref> [1] </ref> are given in Tables 1 and 2, for separated and non-separated boundary conditions, respectively. In tabulating storage requirements, it has been assumed that intermediate information generated during the factorization | namely, the multipliers and Householder vectors | is stored for possible later use with a different right-hand side. <p> In accord with the theoretical results of x2 and x4, virtually no difference was noted between the stability properties of structured QR and row-pivoted LU. As evidence of this, we quote results for the following test problem: Problem 1 (Ascher and Chan <ref> [1] </ref>) a = 0, b = 1, n = 2, y 0 (t) = cos 2!t ! + sin 2!t B a = 1 0 1 0 ; with f (t) and d chosen so that y (t) = e t (1; 1) T .
Reference: [2] <author> U. M. Ascher and R. M. M. Mattheij, </author> <title> General framework, stability and error analysis for numerical stiff boundary value problems, </title> <journal> Numerische Mathematik, </journal> <volume> 54 (1988), </volume> <pages> pp. 355-372. </pages>
Reference-contexts: Although the number of initial and final conditions is correct, this alone is not enough to ensure well-conditioning of the sub-BVPs. In a linear algebra sense, well-conditioning of the whole matrix A DP does not guarantee well-conditioning of each of the P submatrices. Ascher and Mattheij <ref> [2] </ref> develop a "theoretical multiple shooting" framework in which they show how boundary values for the sub-BVPs should be chosen to ensure well-conditioning. Ascher and Chan [1] suggest how to implement this in a parallel environment. 8 STEPHEN J.
Reference: [3] <author> U. M. Ascher, R. M. M. Mattheij, and R. D. Russell, </author> <title> Numerical Solution of Boundary Value Problems for Ordinary Differential Equations, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1988. </year>
Reference-contexts: Hence, the conditioning of the matrices in (4) and (7) is quite similar when the h i are small. If we quantify the conditioning of the problem (1),(2) by a bound on its Green's function (see Ascher, Mattheij, and Russell <ref> [3, x3.2] </ref>), it has been shown by Osborne [16] (and also by Mattheij [14] and Lentini, Osborne, and Russell [12]) that the inverse of A S from (4) satisfies the bound kA 1 Hence if we define fl by fl = 1 + max kY i (t i+1 )k 1 4 <p> A k C k 3 7 7 5 (where the P i are permutation matrices) is produced. It is easy to show that for h sufficiently small, this factorization exists. It is equivalent to compactification <ref> [3, page 153] </ref> which, because of its similarity to single shooting, is known to be potentially unstable. However, this instability can usually be recognized by the presence of large elements in the Z i blocks.
Reference: [4] <author> Z. Bohte, </author> <title> Bounds for rounding errors in the Gaussian elimination for band systems, </title> <journal> Journal of the Institute of Mathematics and its Applications, </journal> <volume> 16 (1975), </volume> <pages> pp. 133-142. </pages>
Reference-contexts: In the partially separated cases (5),(8), similar results can be proved, without referring to the dichotomy at all, by using the results of Bohte <ref> [4] </ref>. Bohte shows that for banded linear systems, the bound on element growth in partial pivoting algorithms is exponential only in the bandwidth. <p> Proof. For the first part of the theorem, the proof follows from Theorem 2.7.2 in Golub and Van Loan [8], and x4 of Bohte <ref> [4] </ref>. The latter assumes that u :009 and (k + 1)nu 0:1; which are clearly implied by assumption (ii), since 1, k 1 and n 1.
Reference: [5] <author> D. L. Brown and J. Lorenz, </author> <title> A high order method for stiff boundary value problems with turning points, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 8 (1987), </volume> <pages> pp. 790-805. </pages>
Reference-contexts: When this is not the case, as in problem 3 below, DECOMP is known to be unstable. To test the relative speed of the linear solvers, two further problems from the literature were used in addition to problem 1. These were Problem 2 (Brown and Lorenz <ref> [5] </ref>) a = 1, b = 1, n = 4, *y 00 2 t z 0 + z = * 2 cos t + 2 *z 00 = z p z (1) = 1 z (1) = e 2= * : PARALLEL ALGORITHMS FOR BVP 15 Table 3 Box method, error
Reference: [6] <author> A. K. Cline, C. B. Moler, G. W. Stewart, and J. H. Wilkinson, </author> <title> An estimate for the condition number of a matrix, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16 (1979), </volume> <pages> pp. 368-375. </pages>
Reference-contexts: Condition estimation. For purposes of assessing the reliability of the computed solution, it is useful to have an estimate of the conditioning of the discrete system. Such an estimate can be obtained, simultaneously with the factorization and solution process, by adapting the procedure described in Cline et al. <ref> [6] </ref> to our situation. We aim to compute an estimate of the quantity ^ = kAk 1 kR 1 k 1 ; where A is one of the coefficient matrices from (4),(7),(5),(8), and R is the upper triangular factor produced by the procedure just described. <p> It is easy to show that 1 (k + 1)n ^ cond 1 (A) p kAk 1 can of course be calculated directly, and so computation of the estimate of kR 1 k 1 is the major part the task of finding ^. Following <ref> [6] </ref>, we do this by first finding vectors z and v such that R T v = z; where the components of z are all 1 and are chosen by a heuristic that attempts to maximize kvk 1 .
Reference: [7] <author> J. C. Diaz, A. Fairweather, and P. Keast, </author> <title> FORTRAN packages for solving certain almost block diagonal linear systems by modified alternate row and column elimination, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 9 (1983), </volume> <pages> pp. 358-375. </pages>
Reference-contexts: Efficient factorization schemes, based on structured Gaussian elimination, have been implemented and are widely available (see x2, and Varah [19], Diaz, Fairweather and Keast <ref> [7] </ref>, Lentini and Pereyra [13], and Keller [9].) During the last 10 years, the question of stability of algorithms for (1),(2) has received a great deal of attention (see, for example, Mattheij [15].) It has been recognized that in a well-conditioned problem (that is, one whose solution is not too sensitive <p> A more economical scheme for solving (5) and (8), described by Varah [19] and implemented by Diaz, Fairweather, and Keast <ref> [7] </ref>, is the method of alternate row and column elimination. For the first p stages of the process, we use column pivoting and elimination (involving columns 1 through n); this produces no fill-in. For stages p + 1 through n we use row pivoting and elimination for the same reason.
Reference: [8] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD, </address> <note> second ed., </note> <year> 1989. </year>
Reference-contexts: Proof. For the first part of the theorem, the proof follows from Theorem 2.7.2 in Golub and Van Loan <ref> [8] </ref>, and x4 of Bohte [4]. The latter assumes that u :009 and (k + 1)nu 0:1; which are clearly implied by assumption (ii), since 1, k 1 and n 1. <p> By simplifying this expression, and noting from the discussion above that fl 2 2n1 max j (A DP ) ij j 2 2n1 kA DP k 1 ; we have that kEk 1 (c 1 =k)ukA DP k 1 : The result now follows by setting r = 1=2 in <ref> [8, Theorem 2.7.2] </ref>. <p> For the second part, note first that k A SP k 1 (1 + o )kA SP k 1 k A 1 SP ( A SP A SP )] 1 k 1 kA 1 Provided kA 1 we have from <ref> [8, Lemma 2.3.3] </ref> that k A 1 kA 1 1 kA 1 1 o k The inequality (14) is implied by (iii). <p> Since k, fl and are all at least 1, assumption (iii) implies that o 1=2, and so cond 1 ( A SP ) 2 (1 + o )kfl 3kfl:(15) Now let s be the exact solution of A SP s = f SP . Direct application of <ref> [8, Theorem 2.7.2] </ref> with r = 1=2 shows, again using (iii), that max i=1;:::;k+1 ks i y (t i )k 1 4cond 1 (A SP )o 4kflo:(16) 6 STEPHEN J. <p> Proof. The assumptions of Lemma A.1 are consequences of (ii) and (iv) above. The first part of the result follows directly from Lemma A.2 and <ref> [8, Theorem 2.7.2] </ref>. The second part is analogous to the second part of Theorem 2.1, with c 4 replacing c 1 and c 5 replacing c 2 . 3. Parallel elimination algorithms. Other parallel and vectorizable algorithms, based on Gaussian elimination, have recently been proposed for (4),(5),(7),(8). <p> A. Appendix A. We start with a result which is similar to <ref> [8, Theorem 3.3.1] </ref>: Lemma A.1. Suppose that alternate row and column elimination, without pivoting, is applied to an N fi N matrix A with bandwidth b w to produced computed factors ^ L, ^ B, ^ U .
Reference: [9] <author> H. B. Keller, </author> <title> Accurate difference methods for two-point boundary value problems, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 11 (1974), </volume> <pages> pp. 305-320. </pages>
Reference-contexts: Efficient factorization schemes, based on structured Gaussian elimination, have been implemented and are widely available (see x2, and Varah [19], Diaz, Fairweather and Keast [7], Lentini and Pereyra [13], and Keller <ref> [9] </ref>.) During the last 10 years, the question of stability of algorithms for (1),(2) has received a great deal of attention (see, for example, Mattheij [15].) It has been recognized that in a well-conditioned problem (that is, one whose solution is not too sensitive to perturbations in M , q or
Reference: [10] <author> C. L. Lawson and R. J. Hanson, </author> <title> Solving Least Squares Problems, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1974. </year>
Reference-contexts: It is important to note that the scheme proposed above is simply standard Householder QR factorization applied to an initial row- and column-permuted form of the original matrix. Thus, we can apply the standard stability analysis for (k + 1)n-dimensional matrices from Lawson and Hanson <ref> [10] </ref> to obtain error bounds for the computed solutions. We have refined these bounds to take into account the structure of the matrix. <p> B. Appendix B. We start by stating two results on the rounding error due to Householder reduction. These results are similar to those in Lawson and Hanson <ref> [10, pp. 85-89] </ref> and Wilkinson [20, pp. 157-162]. They differ from Lawson and Hanson's results in that the O (u 2 ) term is explicitly accounted for at every stage, and from Wilkinson's in that we do not assume double-precision accumulation of inner products.
Reference: [11] <author> M. Lentini, </author> <title> Parallel solution of special large block tridiagonal systems: Tpbvp. </title> <type> Manuscript, </type> <year> 1989. </year>
Reference-contexts: Here, some form of pivoted Gaussian elimination is applied simultaneously from both ends of the matrix (either A DP or A SP ). When the factorizations meet in the center, a small reduced system is formed and factored. This is analogous to the approach of Lentini <ref> [11] </ref>. Finally, we mention the approach of Ascher and Chan [1], who form the normal equations for (5) and (8), and factorize the resulting symmetric, positive definite, block-tridiagonal system using cyclic reduction.
Reference: [12] <author> M. Lentini, M. R. Osborne, and R. D. Russell, </author> <title> The close relationships between methods for solving two-point boundary value problems, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 22 (1985), </volume> <pages> pp. 280-309. </pages>
Reference-contexts: If we quantify the conditioning of the problem (1),(2) by a bound on its Green's function (see Ascher, Mattheij, and Russell [3, x3.2]), it has been shown by Osborne [16] (and also by Mattheij [14] and Lentini, Osborne, and Russell <ref> [12] </ref>) that the inverse of A S from (4) satisfies the bound kA 1 Hence if we define fl by fl = 1 + max kY i (t i+1 )k 1 4 STEPHEN J.
Reference: [13] <author> M. Lentini and V. Pereyra, </author> <title> An adaptive finite difference solver for nonlinear two-point boundary value problems with mild boundary layers, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 14 (1977), </volume> <pages> pp. 91-111. </pages>
Reference-contexts: Efficient factorization schemes, based on structured Gaussian elimination, have been implemented and are widely available (see x2, and Varah [19], Diaz, Fairweather and Keast [7], Lentini and Pereyra <ref> [13] </ref>, and Keller [9].) During the last 10 years, the question of stability of algorithms for (1),(2) has received a great deal of attention (see, for example, Mattheij [15].) It has been recognized that in a well-conditioned problem (that is, one whose solution is not too sensitive to perturbations in M <p> Performance comparisons were made with * a row partial pivoting code (two versions, for separated and non-separated boundary conditions), and * the DECOMP and SOLVE routines from the PASVA codes <ref> [13] </ref>. The DECOMP routine uses alternate row and column pivoting (as does the algorithm described in Varah [19] and in x2) but always eliminates by rows.
Reference: [14] <author> R. M. M. Mattheij, </author> <title> The conditioning of linear boundary value problems, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 19 (1982), </volume> <pages> pp. </pages> <month> 963-978. </month> <title> [15] , Decoupling and stability of algorithms for boundary value problems, </title> <journal> SIAM Review, </journal> <volume> 27 (1985), </volume> <pages> pp. 1-44. </pages>
Reference-contexts: If we quantify the conditioning of the problem (1),(2) by a bound on its Green's function (see Ascher, Mattheij, and Russell [3, x3.2]), it has been shown by Osborne [16] (and also by Mattheij <ref> [14] </ref> and Lentini, Osborne, and Russell [12]) that the inverse of A S from (4) satisfies the bound kA 1 Hence if we define fl by fl = 1 + max kY i (t i+1 )k 1 4 STEPHEN J.
Reference: [16] <author> M. R. Osborne, </author> <title> Aspects of the numerical solution of boundary value problems with separated boundary conditions. </title> <type> Manuscript, </type> <year> 1978. </year>
Reference-contexts: Hence, the conditioning of the matrices in (4) and (7) is quite similar when the h i are small. If we quantify the conditioning of the problem (1),(2) by a bound on its Green's function (see Ascher, Mattheij, and Russell [3, x3.2]), it has been shown by Osborne <ref> [16] </ref> (and also by Mattheij [14] and Lentini, Osborne, and Russell [12]) that the inverse of A S from (4) satisfies the bound kA 1 Hence if we define fl by fl = 1 + max kY i (t i+1 )k 1 4 STEPHEN J.
Reference: [17] <author> M. Paprzycki and I. Gladwell, </author> <title> Solving almost block diagonal systems on parallel computers, </title> <booktitle> Parallel Computing, 17 (1991), </booktitle> <pages> pp. 133-153. </pages>
Reference-contexts: In many applications (such as the one described in [21]) the lack of stability is not a problem. Paprzycki and Gladwell <ref> [17] </ref> describe a partitioned elimination algorithm in which (8) is torn into P submatrices, each of which has the same form as the original A DP , and alternate row and column elimination is applied to each piece.
Reference: [18] <author> V. Pereyra, </author> <title> Iterated deferred corrections for nonlinear boundary value problems, </title> <journal> Numerische Mathematik, </journal> <volume> 8 (1968), </volume> <pages> pp. 111-125. </pages>
Reference-contexts: 6 6 s 1 s 3 s k 3 7 7 7 5 2 6 6 6 4 f 1 . . . d b 7 7 7 7 :(8) The accuracy of finite difference schemes is often enhanced by the use of deferred correction techniques (see, for example, Pereyra <ref> [18] </ref>). As has been observed, the two algorithms are closely related, in the sense that for a reasonable choice of the approximation (6), C 1 i A i should be close to Y i (t i+1 ).
Reference: [19] <author> J. M. Varah, </author> <title> Alternate row and column elimination for solving certain linear systems, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 13 (1976), </volume> <pages> pp. 71-75. </pages>
Reference-contexts: Efficient factorization schemes, based on structured Gaussian elimination, have been implemented and are widely available (see x2, and Varah <ref> [19] </ref>, Diaz, Fairweather and Keast [7], Lentini and Pereyra [13], and Keller [9].) During the last 10 years, the question of stability of algorithms for (1),(2) has received a great deal of attention (see, for example, Mattheij [15].) It has been recognized that in a well-conditioned problem (that is, one whose <p> A more economical scheme for solving (5) and (8), described by Varah <ref> [19] </ref> and implemented by Diaz, Fairweather, and Keast [7], is the method of alternate row and column elimination. For the first p stages of the process, we use column pivoting and elimination (involving columns 1 through n); this produces no fill-in. <p> Performance comparisons were made with * a row partial pivoting code (two versions, for separated and non-separated boundary conditions), and * the DECOMP and SOLVE routines from the PASVA codes [13]. The DECOMP routine uses alternate row and column pivoting (as does the algorithm described in Varah <ref> [19] </ref> and in x2) but always eliminates by rows. Approximate operation counts and storage requirements for these algorithms and the normal equations method of Ascher and Chan [1] are given in Tables 1 and 2, for separated and non-separated boundary conditions, respectively.
Reference: [20] <author> J. H. Wilkinson, </author> <title> The Algebraic Eigenvalue Problem, </title> <publisher> Oxford University Press, </publisher> <address> London, </address> <year> 1965. </year> <note> 24 STEPHEN J. WRIGHT </note>
Reference-contexts: We have refined these bounds to take into account the structure of the matrix. In addition, to allay any possible concerns about instability at the level of the O (u 2 ) terms, we have removed these terms using the style of analysis in Wilkinson <ref> [20] </ref>. The relevant results are stated and proved in Appendix B. Here, we summarize the analysis in the following Lemma and Theorem: Lemma 4.1. <p> B. Appendix B. We start by stating two results on the rounding error due to Householder reduction. These results are similar to those in Lawson and Hanson [10, pp. 85-89] and Wilkinson <ref> [20, pp. 157-162] </ref>. They differ from Lawson and Hanson's results in that the O (u 2 ) term is explicitly accounted for at every stage, and from Wilkinson's in that we do not assume double-precision accumulation of inner products.
Reference: [21] <author> S. J. Wright and V. Pereyra, </author> <title> Adaptation of a two-point boundary value problem solver to a vector-multiprocessor environment, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 11 (1990), </volume> <pages> pp. 425-449. </pages>
Reference-contexts: Parallel elimination algorithms. Other parallel and vectorizable algorithms, based on Gaussian elimination, have recently been proposed for (4),(5),(7),(8). In general, they suffer either from poor stability properties or from limitations in the amount of parallelism which is possible. Wright and Pereyra <ref> [21] </ref> describe variants of a block factorization algorithm applied to (7). In the most highly vectorized variant, a factorization of the form 2 6 6 4 ~ A 2 ~ A k 3 7 7 5 6 6 6 I W 1 . . . <p> It is equivalent to compactification [3, page 153] which, because of its similarity to single shooting, is known to be potentially unstable. However, this instability can usually be recognized by the presence of large elements in the Z i blocks. The strategy described in <ref> [21, x5] </ref> is to use this factorization where possible, and discard it in favor of a more stable method if the kZ i k are too large. In many applications (such as the one described in [21]) the lack of stability is not a problem. <p> The strategy described in [21, x5] is to use this factorization where possible, and discard it in favor of a more stable method if the kZ i k are too large. In many applications (such as the one described in <ref> [21] </ref>) the lack of stability is not a problem. Paprzycki and Gladwell [17] describe a partitioned elimination algorithm in which (8) is torn into P submatrices, each of which has the same form as the original A DP , and alternate row and column elimination is applied to each piece. <p> Four different algorithms were used to solve the linear system, namely, * SQR-1 | one-level structured QR; * ROWPP | LU factorization with row partial pivoting; * DECOMP | the DECOMP and SOLVE routines from PASVA; and * COMPACT | compactification, as implemented in the codes D4/S4 described in <ref> [21] </ref>. Tables 3 and 4 show the maximum error in the first component of the computed result. Because of its failure to decouple the fundamental solution modes, compactification performs poorly.
References-found: 20

