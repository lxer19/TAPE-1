URL: http://www.cs.toronto.edu/~greiner/PAPERS/fast-disp-spec.ps
Refering-URL: http://www.cs.toronto.edu/~greiner/PAPERS/
Root-URL: 
Email: dale@cs.toronto.edu  greiner@scr.siemens.com  
Title: Fast (Distribution Specific) Learning  
Author: Dale Schuurmans Russell Greiner 
Address: Toronto, ON M5S 1A4  Princeton, NJ 08540  
Affiliation: Department of Computer Science University of Toronto  Siemens Corporate Research  
Abstract: pac-learning results are often criticized for demanding impractically large training samples. The common wisdom is that these large samples follow from the worst case nature of the analysis, and therefore pac-learning, though desirable, must not be a practical goal. We however consider an alternative view: perhaps these large sample sizes are due to the presumed learning strategies which make inefficient use of the available training data. To demonstrate this, we consider sequential learning strategies that autonomously decide when to stop training based on observing training examples as they arrive. We show that for distribution specific learning these algorithms require far fewer training examples (on average) than existing fixed sample size approaches, and are able to learn with certainty not just high probability. In fact, a simple sequential strategy is optimally efficient in many cases.
Abstract-found: 1
Intro-found: 1
Reference: <author> Benedek, G. and Itai, A. </author> <year> (1988). </year> <title> Learnability by fixed distributions. </title> <booktitle> In Proceedings COLT-88, </booktitle> <pages> pages 80-90. </pages>
Reference-contexts: The number of training examples used by BI is given by T BI = 32 N *=2 ffi <ref> (Benedek and Itai, 1988) </ref>: (1) Below we compare the sample efficiency of this fixed sample size learning technique with the sequential learning approach. 3 A simple example We use the following simple case study to introduce a basic sequential learning strategy, and then to compare the relative sample efficiencies of the
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. </author> <year> (1989). </year> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-965. </pages>
Reference: <author> Ehrenfeucht, A., Haussler, D., Kearns, M., and Valiant, L. </author> <year> (1989). </year> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 247-261. </pages>
Reference: <author> Kulkarni, S. </author> <year> (1991). </year> <title> Problems of computational and information complexity in machine vision and learning. </title> <type> PhD thesis, </type> <institution> MIT, Department of Electrical Engineering and Computer Science. </institution>
Reference: <author> Linial, N., Mansour, Y., and Rivest, R. </author> <year> (1988). </year> <title> Some results on learnability and the Vapnik-Chervonenkis dimension. </title> <booktitle> In Proceedings COLT-88. </booktitle>
Reference-contexts: Our results also extend <ref> (Linial, Mansour and Rivest, 1988) </ref>, as our goal is to improve learning efficiency uniformly over all possible target concepts c in the class C, not just gain an advantage for certain concepts by sacrificing performance on others.
Reference: <author> Schuurmans, D. </author> <year> (1994). </year> <title> Efficient, Accurate, and Reliable Classification Learning. </title> <type> PhD thesis, </type> <institution> University of Toronto, Department of Computer Science. </institution> <note> (Forthcoming). 9 Valiant, </note> <author> L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, 27(11):1134--1142. </journal>
Reference-contexts: concept space to be cac-learnable: Theorem 1 A space (C; P) is cac-learnable iff it is uniformly reducible for all * &gt; 0 iff S cac-learns it. 2 2 These results assume the concept space satisfies a benign regularity condition, namely that it is "separable" in the sense defined in <ref> (Schuurmans, 1994) </ref>. 6 Therefore if a concept space is not uniformly reducible then it is not cac-learnable by any learner, but if it is uniformly reducible then S cac-learns it. So S can be viewed a "universal" cac-learner in this sense. <p> So S can be viewed a "universal" cac-learner in this sense. Note 1 As there are finitely coverable spaces that are not uniformly reducible <ref> (Schuurmans, 1994) </ref>, cac-learnability implies pac-learnability, but not vice versa. (Hence, not every pac-learnable concept space is cac-learnable.) However, most of the concept spaces considered by researchers are in fact uniformly reducible | only pathological examples appear not to be.
Reference: <author> Vapnik, V. N. and Chervonenkis, A. Y. </author> <year> (1971). </year> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280. </pages>
Reference-contexts: research (Blumer et al., 1989; Ehrenfeucht et al., 1989) has determined (up to constants and log factors) the rate at which the necessary and sufficient training sample sizes scale up in terms of accuracy *, reliability ffi, and the complexity of the concept class C (as measured by its VCdimension <ref> (Vapnik and Chervonenkis, 1971) </ref>).
Reference: <author> Wald, A. </author> <year> (1947). </year> <title> Sequential Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address> <month> 10 </month>
Reference-contexts: We focus on the second alternative: Can worst case pac-learning be efficiently achieved by other learning strategies that go beyond the simple "collect, filter, select" approach? Following <ref> (Wald, 1947) </ref>, we consider sequential learning strategies that autonomously decide when to stop their own training, based on observing the labeled training examples one at a time in succession.
References-found: 8

