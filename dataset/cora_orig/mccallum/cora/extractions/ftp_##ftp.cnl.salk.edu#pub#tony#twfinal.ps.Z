URL: ftp://ftp.cnl.salk.edu/pub/tony/twfinal.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00243.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: tewon@cs.cmu.edu  tony@salk.edu  rlambert@sipi.usc.edu  
Title: Blind separation of delayed and convolved sources.  
Author: Te-Won Lee Anthony J. Bell Russell H. Lambert 
Address: Pittsburgh, PA 15213, USA  10010 N. Torrey Pines Road La Jolla, California 92037, USA  USA  
Affiliation: Max-Planck-Society, GERMANY, AND Interactive Systems Group Carnegie Mellon University  Computational Neurobiology, The Salk Institute  Dept of Electrical Engineering University of South California,  
Abstract: We address the difficult problem of separating multiple speakers with multiple microphones in a real room. We combine the work of Torkkola and Amari, Cichocki and Yang, to give Natural Gradient information maximisation rules for recurrent (IIR) networks, blindly adjusting delays, separating and deconvolving mixed signals. While they work well on simulated data, these rules fail in real rooms which usually involve non-minimum phase transfer functions, not-invertible using stable IIR filters. An approach that sidesteps this problem is to perform infomax on a feedforward architecture in the frequency domain (Lambert 1996). We demonstrate real-room separation of two natural signals using this approach.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Amari S-I. Cichocki A. & Yang H.H. </author> <year> 1996. </year> <title> A new learning algorithm for blind signal separation, </title> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT press. </publisher>
Reference-contexts: differentiates as: W 0 = ( ^ W 1 ) ^ W ( ^ W 1 ) (6) The best way to maximise entropy in the feedforward system is not to follow the entropy gradient, as in [2], but to follow its `natural' gradient, as reported by Amari et al <ref> [1] </ref>: @H (y) ^ W T ^ W (7) This is an optimal rescaling of the entropy gradient [1, 3]. It simplifies the learning rule and speeds convergence considerably. <p> The best way to maximise entropy in the feedforward system is not to follow the entropy gradient, as in [2], but to follow its `natural' gradient, as reported by Amari et al [1]: @H (y) ^ W T ^ W (7) This is an optimal rescaling of the entropy gradient <ref> [1, 3] </ref>. It simplifies the learning rule and speeds convergence considerably. <p> This allows for the non-causal expansion of the non-minimum phase roots, causing the roughly symmetrical "flanged" appearance of the filters in Fig.2B. For convenience, we formulate the infomax and natural gradient infomax rules <ref> [2, 1] </ref> in the frequency domain: W / W H + fft (^y)X H (16) where the H superscript denotes the Hermitian transpose (complex conjugate).
Reference: [2] <author> Bell A.J. & Sejnowski T.J. </author> <year> 1995. </year> <title> An information maximisation approach to blind separation and blind deconvolution, </title> <journal> Neural Computation, </journal> <volume> 7, </volume> <pages> 1129-1159 </pages>
Reference-contexts: This was the architecture implicitly assumed in <ref> [2] </ref>. However, it cannot solve the delay-compensation problem, since in eq.1 each delay, D ij , delays a single source, while in eq.2 each delay, d ij is associated with a mixture, x j . Torkkola [8], has addressed the problem of solving the delay-compensation problem with a feedback architecture. <p> Our algorithm will whiten: it will remove dependencies across time which already existed in the original source signals, s i . However, it is possible to restore the characteristic autocorrelations (amplitude spectra) of the sources by post-processing. For the reasoning behind Assumption (3) see <ref> [2] </ref>. We will discuss Assumption 4 in section 5. <p> W 1 0 I, which, due to the quotient rule for matrix differentiation, differentiates as: W 0 = ( ^ W 1 ) ^ W ( ^ W 1 ) (6) The best way to maximise entropy in the feedforward system is not to follow the entropy gradient, as in <ref> [2] </ref>, but to follow its `natural' gradient, as reported by Amari et al [1]: @H (y) ^ W T ^ W (7) This is an optimal rescaling of the entropy gradient [1, 3]. It simplifies the learning rule and speeds convergence considerably. Evaluated, it gives [2]: ^ W 0 / (I <p> the entropy gradient, as in <ref> [2] </ref>, but to follow its `natural' gradient, as reported by Amari et al [1]: @H (y) ^ W T ^ W (7) This is an optimal rescaling of the entropy gradient [1, 3]. It simplifies the learning rule and speeds convergence considerably. Evaluated, it gives [2]: ^ W 0 / (I + ^yu T ) ^ W 0 ; ^y i = @y i @u i Substituting into eq.7 gives the natural gradient rule for static feedback weights: W 0 / (I + W 0 )(I + ^yu T ); (9) This reasoning may be extended <p> Performing the same coordi nate transforms as for W 0 above, gives the rule: W k / (I + W k )^yu T (We note that learning rules similar to these have been independently derived by Cichocki et al [4]). Finally, for the delays in eq.5, we derive <ref> [2, 8] </ref>: d ij / @d ij M1 X @ w ijk u (t d ij k) (12) This rule is different from that in [8] because it uses the collected temporal gradient information from all the taps. <p> This allows for the non-causal expansion of the non-minimum phase roots, causing the roughly symmetrical "flanged" appearance of the filters in Fig.2B. For convenience, we formulate the infomax and natural gradient infomax rules <ref> [2, 1] </ref> in the frequency domain: W / W H + fft (^y)X H (16) where the H superscript denotes the Hermitian transpose (complex conjugate).
Reference: [3] <author> Cardoso J-F. & Laheld B. </author> <year> 1996. </year> <title> Equivariant adaptive source separation, </title> <journal> IEEE Trans. on Signal Proc., </journal> <month> Dec. </month> <year> 1996 </year>
Reference-contexts: The best way to maximise entropy in the feedforward system is not to follow the entropy gradient, as in [2], but to follow its `natural' gradient, as reported by Amari et al [1]: @H (y) ^ W T ^ W (7) This is an optimal rescaling of the entropy gradient <ref> [1, 3] </ref>. It simplifies the learning rule and speeds convergence considerably.
Reference: [4] <author> Cichocki A., Amari S-I & Cao J. </author> <year> 1996. </year> <title> Blind separation of delayed and convolved signals with self-adaptive learning rate, </title> <booktitle> in Proc. Intern. Symp. on Nonlinear Theory and Applications (NOLTA*96), </booktitle> <address> Kochi, Japan. </address>
Reference-contexts: Performing the same coordi nate transforms as for W 0 above, gives the rule: W k / (I + W k )^yu T (We note that learning rules similar to these have been independently derived by Cichocki et al <ref> [4] </ref>). Finally, for the delays in eq.5, we derive [2, 8]: d ij / @d ij M1 X @ w ijk u (t d ij k) (12) This rule is different from that in [8] because it uses the collected temporal gradient information from all the taps.
Reference: [5] <author> Lambert R. </author> <title> 1996.Multichannel blind deconvolution: FIR matrix algebra and separation of multipath mixtures, </title> <type> PhD Thesis, </type> <institution> University of Southern Califor-nia, Department of Electrical Engineering, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: Unfortunately, real room acoustics usually involves non-minimum phase mixing. There does exist, however, a stable non-causal feedforward (FIR) inverse for non-minimum phase mixing systems. The learning rules for such a system can be formulated using the FIR polynomial matrix algebra as described by Lambert <ref> [5] </ref>. This may be performed in the time or frequency domain, the only requirements being that the inverting filters are long enough and their main energy occurs more-or-less in their centre. <p> This led us to an FIR frequency domain infomax approach suggested by Lambert <ref> [5] </ref>. The latter approach shows much better separation of speech and music mixed in a real-room. Based on these techniques, it should now be possible to develop real-world applications. Acknowledgments T.W.L. is supported by the Daimler-Benz-Fellowship, and A.J.B. by a grant from the Office of Naval Research.
Reference: [6] <author> Lee T-W. & Orglmeister R. </author> <title> Blind source separation of real-world signals. </title> <note> submitted to Proc. ICNN, </note> <institution> Houston, USA, </institution> <year> 1997. </year>
Reference-contexts: To test this application, we fed into a speech recognizer, ten sentences recorded with loud music in the background and ten sentences recorded with a simultaneous speaker interference. After separation, the recognition rate increased considerably for both cases. These results are reported in detail in <ref> [6] </ref>. 8 Conclusions Starting with `Natural gradient infomax' IIR learning rules for blind time delay adjustment, separation and deconvolution, we showed how these worked well on minimum-phase mixing, but not on non-minimum-phase mixing, as usually occurs in rooms.
Reference: [7] <author> Platt J.C. & Faggin F. </author> <year> 1992. </year> <title> Networks for the separation of sources that are superimposed and delayed, </title> <booktitle> in Moody J.E et al (eds) Advances in Neural Information Processing Systems 4, </booktitle> <publisher> Morgan-Kaufmann </publisher>
Reference-contexts: Torkkola [8], has addressed the problem of solving the delay-compensation problem with a feedback architecture. Such an architecture can, in principle, solve this problem, as shown earlier by Platt & Faggin <ref> [7] </ref>. Torkkola [9] also generalised the feedback architecture to remove dependencies across time, to achieve the deconvo-lution of mixtures which have been filtered, as in eq.1. Here we propose a slightly different architecture than Torkkola's ([9], eq.15).
Reference: [8] <author> Torkkola K. </author> <year> 1996. </year> <title> Blind separation of delayed sources based on information maximisation, </title> <booktitle> Proc IEEE ICASSP, </booktitle> <address> Atlanta, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: This was the architecture implicitly assumed in [2]. However, it cannot solve the delay-compensation problem, since in eq.1 each delay, D ij , delays a single source, while in eq.2 each delay, d ij is associated with a mixture, x j . Torkkola <ref> [8] </ref>, has addressed the problem of solving the delay-compensation problem with a feedback architecture. Such an architecture can, in principle, solve this problem, as shown earlier by Platt & Faggin [7]. <p> Performing the same coordi nate transforms as for W 0 above, gives the rule: W k / (I + W k )^yu T (We note that learning rules similar to these have been independently derived by Cichocki et al [4]). Finally, for the delays in eq.5, we derive <ref> [2, 8] </ref>: d ij / @d ij M1 X @ w ijk u (t d ij k) (12) This rule is different from that in [8] because it uses the collected temporal gradient information from all the taps. <p> Finally, for the delays in eq.5, we derive [2, 8]: d ij / @d ij M1 X @ w ijk u (t d ij k) (12) This rule is different from that in <ref> [8] </ref> because it uses the collected temporal gradient information from all the taps. <p> The bottom row shows the inverting system convolved with the mixing system, proving that W fl A is approximately the identity mapping. Delay learning is not demonstrated here, though for periodic signals like speech we observed that it is subject to local minima problems <ref> [8, 9] </ref>. Bottom row: the convolved mixing and unmixing systems. The delta-like response indicates successful blind unmixing. In (B) this occurs acausally with a time-shift. 5 Back to the feedforward architecture. The feedback architecture is elegant but limited.
Reference: [9] <author> Torkkola K. </author> <year> 1996. </year> <title> Blind separation of convolved sources based on information maximisation, </title> <booktitle> Proc. IEEE Workshop on Neural Networks and Signal Processing, </booktitle> <address> Kyota, Japan, </address> <month> Sept. </month> <year> 1996 </year>
Reference-contexts: Torkkola [8], has addressed the problem of solving the delay-compensation problem with a feedback architecture. Such an architecture can, in principle, solve this problem, as shown earlier by Platt & Faggin [7]. Torkkola <ref> [9] </ref> also generalised the feedback architecture to remove dependencies across time, to achieve the deconvo-lution of mixtures which have been filtered, as in eq.1. Here we propose a slightly different architecture than Torkkola's ([9], eq.15). <p> The bottom row shows the inverting system convolved with the mixing system, proving that W fl A is approximately the identity mapping. Delay learning is not demonstrated here, though for periodic signals like speech we observed that it is subject to local minima problems <ref> [8, 9] </ref>. Bottom row: the convolved mixing and unmixing systems. The delta-like response indicates successful blind unmixing. In (B) this occurs acausally with a time-shift. 5 Back to the feedforward architecture. The feedback architecture is elegant but limited.
References-found: 9

