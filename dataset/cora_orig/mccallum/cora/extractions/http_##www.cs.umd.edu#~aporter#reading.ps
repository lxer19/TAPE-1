URL: http://www.cs.umd.edu/~aporter/reading.ps
Refering-URL: http://www.cs.umd.edu/~aporter/html/references.html
Root-URL: 
Title: Comparing Detection Methods For Software Requirements Inspections: A Replicated Experiment  
Author: Adam A. Porter Lawrence G. Votta, Jr. Victor R. Basili 
Keyword: Controlled Experiments, Technique and Methodology Evaluation, Inspections, Reading Techniques  
Abstract: Software requirements specifications (SRS) are often validated manually. One such process is inspection, in which several reviewers independently analyze all or part of the specification and search for faults. These faults are then collected at a meeting of the reviewers and author(s). Usually, reviewers use Ad Hoc or Checklist methods to uncover faults. These methods force all reviewers to rely on nonsystematic techniques to search for a wide variety of faults. We hypothesize that a Scenario-based method, in which each reviewer uses different, systematic techniques to search for different, specific classes of faults, will have a significantly higher success rate. We evaluated this hypothesis using a 3 fi 2 4 partial factorial, randomized experimental design. Forty eight graduate students in computer science participated in the experiment. They were assembled into sixteen, three-person teams. Each team inspected two SRS using some combination of Ad Hoc, Checklist or Scenario methods. For each inspection we performed four measurements: (1) individual fault detection rate, (2) team fault detection rate, (3) percentage of faults first identified at the collection meeting (meeting gain rate), and (4) percentage of faults first identified by an individual, but never reported at the collection meeting (meeting loss rate). The experimental results are that (1) the Scenario method had a higher fault detection rate than either Ad Hoc or Checklist methods, (2) Scenario reviewers were more effective at detecting the faults their scenarios are designed to uncover, and were no less effective at detecting other faults than both Ad Hoc or Checklist reviewers, (3) Checklist reviewers were no more effective than Ad Hoc reviewers, and (4) Collection meetings produced no net improvement in the fault detection rate meeting gains were offset by meeting losses. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Watts S. Humphery, </author> <title> Managing the Software Process, </title> <publisher> AddisonWesley Publishing Co., </publisher> <address> 1989, Reading, Massachusetts. </address>
Reference-contexts: 1 Depending on the exact form of the inspection, they are sometimes called reviews or walkthroughs. For a more thorough description of the taxonomy see <ref> [1] </ref> pp. 171ff and [2]. We are focusing on the methods used to perform the first step in this process, fault detection. For this article, we define a fault detection method to be a set of fault detection techniques coupled with an assignment of responsibilities to individual reviewers. <p> Below we describe the relevant literature, several alternative fault detection methods which motivated our study, our research hypothesis, and our experimental observations, analysis and conclusions. A. Inspection Literature A summary of the origins and the current practice of inspections may be found in Humphrey <ref> [1] </ref>. Consequently, we will discuss only work directly related to our current efforts. Fagan [5] defined the basic software inspection process. While most writers have endorsed his approach [6], [1], Par- nas and Weiss are more critical [4]. <p> A. Inspection Literature A summary of the origins and the current practice of inspections may be found in Humphrey <ref> [1] </ref>. Consequently, we will discuss only work directly related to our current efforts. Fagan [5] defined the basic software inspection process. While most writers have endorsed his approach [6], [1], Par- nas and Weiss are more critical [4]. In part, they argue that effectiveness suffers because individual reviewers are 2 Fig. 1. Systematic Inspection Research Hypothesis.
Reference: [2] <institution> IEEE Standard for software reviews and audits, Soft. Eng. Tech. Comm. of the IEEE Computer Society, </institution> <year> 1989, </year> <note> IEEE Std 10281988. </note>
Reference-contexts: 1 Depending on the exact form of the inspection, they are sometimes called reviews or walkthroughs. For a more thorough description of the taxonomy see [1] pp. 171ff and <ref> [2] </ref>. We are focusing on the methods used to perform the first step in this process, fault detection. For this article, we define a fault detection method to be a set of fault detection techniques coupled with an assignment of responsibilities to individual reviewers. <p> For this experiment, all three documents were adapted to adhere to the IEEE suggested format <ref> [2] </ref>. All faults present in these SRS appear in the original documents or were generated during the adaptation process; no faults were intentionally seeded into the document. The authors discovered 42 faults in the WLMS SRS; and 26 in the CRUISE SRS.
Reference: [3] <author> Lawrence G. Votta, </author> <title> "Does every inspection need a meeting?", </title> <booktitle> in Proceedings of ACM SIGSOFT '93 Symposium on Foundations of Software Engineering. Association for Computing Machinery, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: From a study of over 50 inspections, Votta <ref> [3] </ref> collected data that strongly contradicts this assertion. In this Sec- tion, we measure the benefits of collection meetings by comparing the team and individual fault summaries to determine the meeting gain and meeting loss rates. (See Figure 4 and Figure 5).
Reference: [4] <author> Dave L. Parnas and David M. Weiss, </author> <title> "Active design reviews: </title> <booktitle> principles and practices", in Proceedings of the 8th International Conference on Software Engineering, </booktitle> <month> Aug. </month> <year> 1985, </year> <pages> pp. 215-222. </pages>
Reference-contexts: When they are not coordinated, all reviewers have identical responsibilities. In contrast, the reviewers in coordinated teams may have separate and distinct responsibilities. In practice, reviewers often use Ad Hoc or Checklist detection techniques to discharge identical, general responsibilities. Some authors, notably Parnas and Weiss <ref> [4] </ref>, have argued that inspections would be more effective if each reviewer used a different set of systematic detection techniques to discharge different, specific responsibilities. Until now, however, there have been no reproducible, quantitative studies comparing alternative detection methods for software inspections. <p> Consequently, we will discuss only work directly related to our current efforts. Fagan [5] defined the basic software inspection process. While most writers have endorsed his approach [6], [1], Par- nas and Weiss are more critical <ref> [4] </ref>. In part, they argue that effectiveness suffers because individual reviewers are 2 Fig. 1. Systematic Inspection Research Hypothesis. This figure represents a software requirements specification before and after a nonsystematic technique, general and identical responsibility inspection and a systematic technique, specific and distinct responsibility inspection. <p> Experiment Preparation The participants were given two, 75 minute lectures on software requirements specifications, the SCR tabular requirements notation, inspection procedures, the fault classification scheme, and the filling out of data collection forms. The references for these lectures were Fagan [5], Parnas <ref> [4] </ref>, and the IEEE Guide to Software Requirements Specifications [19]. The participants were then assembled into three-person teams see Section II-A.3 for details. Within each team, members were randomly assigned to act as the moderator, the recorder, or the reader during the collection meeting. D.
Reference: [5] <author> M. E. Fagan, </author> <title> "Design and code inspections to reduce errors in program development", </title> <journal> IBM Systems Journal, </journal> <volume> vol. 15, no. 3, </volume> <pages> pp. 182-211, </pages> <year> 1976. </year>
Reference-contexts: A. Inspection Literature A summary of the origins and the current practice of inspections may be found in Humphrey [1]. Consequently, we will discuss only work directly related to our current efforts. Fagan <ref> [5] </ref> defined the basic software inspection process. While most writers have endorsed his approach [6], [1], Par- nas and Weiss are more critical [4]. In part, they argue that effectiveness suffers because individual reviewers are 2 Fig. 1. Systematic Inspection Research Hypothesis. <p> Experiment Preparation The participants were given two, 75 minute lectures on software requirements specifications, the SCR tabular requirements notation, inspection procedures, the fault classification scheme, and the filling out of data collection forms. The references for these lectures were Fagan <ref> [5] </ref>, Parnas [4], and the IEEE Guide to Software Requirements Specifications [19]. The participants were then assembled into three-person teams see Section II-A.3 for details. Within each team, members were randomly assigned to act as the moderator, the recorder, or the reader during the collection meeting. D. <p> From Tables V and VI we see that even though the Checklist targets a large number of faults, it does not actually improve a reviewer's ability to find those faults. E. Analysis of Collection Meetings In his original paper on software inspections Fagan <ref> [5] </ref> asserts that Sometimes flagrant errors are found during : : : [fault detection], but in general, the number of errors found is not nearly as high as in the : : : [collection meeting] operation.
Reference: [6] <author> Barry W. Boehm, </author> <title> Software Engineering Economics, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: A. Inspection Literature A summary of the origins and the current practice of inspections may be found in Humphrey [1]. Consequently, we will discuss only work directly related to our current efforts. Fagan [5] defined the basic software inspection process. While most writers have endorsed his approach <ref> [6] </ref>, [1], Par- nas and Weiss are more critical [4]. In part, they argue that effectiveness suffers because individual reviewers are 2 Fig. 1. Systematic Inspection Research Hypothesis.
Reference: [7] <author> Charles M. Judd, Eliot R. Smith, and Louise H. Kidder, </author> <title> Research Methods in Social Relations, </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <address> Fort Worth, TX, sixth edition, </address> <year> 1991. </year>
Reference-contexts: A. Experimental Design The design of the experiment is somewhat unusual. To avoid misinterpreting the data it is important to understand the experiment and the reasons for certain elements of its design 2 . 2 See Judd, et al. <ref> [7] </ref>, chapter 4 for an excellent discussion of randomized social experimental designs. 3 Round/Specification Round 1 Round 2 WLMS CRUISE WLMS CRUISE ad hoc 1B, 1D, 1G 1A, 1C, 1E 1A 1D, 2B Detection 1H, 2A 1F, 2D Method checklist 2B 2E, 2G 1E, 2D, 2G 1B, 1H scenarios 2C, 2F
Reference: [8] <author> Mark A. Ardis, </author> <title> "Lessons from using basic lotos", </title> <booktitle> in Proceedings of the Sixteenth International Conference on Software Engineering, </booktitle> <address> Sorrento, Italy, </address> <month> May </month> <year> 1994, </year> <pages> pp. 5-14. </pages>
Reference-contexts: Our experimental specifications are atypical of industrial SRS in two ways. First, most of the experimental specification is written in a formal requirements notation. (See Sec- tion II-B.) Although several groups at AT&T and else <br>- where are experimenting with formal notations <ref> [8] </ref>, [9], it is not the industry's standard practice. Secondly, the specifications are considerably smaller than indus <br>- trial ones. 3. The inspection process in our experimental design may not be representative of software development practice.
Reference: [9] <author> S. Gerhart, D. Craigen, and T. Ralston, </author> <title> "Experience with formal methods in critical systems", </title> <journal> IEEE Software, </journal> <volume> vol. 11, no. 1, </volume> <pages> pp. 21-28, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Our experimental specifications are atypical of industrial SRS in two ways. First, most of the experimental specification is written in a formal requirements notation. (See Sec- tion II-B.) Although several groups at AT&T and else <br>- where are experimenting with formal notations [8], <ref> [9] </ref>, it is not the industry's standard practice. Secondly, the specifications are considerably smaller than indus <br>- trial ones. 3. The inspection process in our experimental design may not be representative of software development practice.
Reference: [10] <author> Stephen G. Eick, Clive R. Loader, M. David Long, Scott A. Vander Wiel, and Lawrence G. Votta, </author> <title> "Estimating software fault content before coding", </title> <booktitle> in Proceedings of the 14th International Conference on Software Engineering, </booktitle> <month> May </month> <year> 1992, </year> <pages> pp. 59-65. </pages>
Reference-contexts: Secondly, the specifications are considerably smaller than indus <br>- trial ones. 3. The inspection process in our experimental design may not be representative of software development practice. We have modeled our experiment's inspection process after the one used in several development organizations within AT&T <ref> [10] </ref>. Although this process is similar to a Fagan-style inspection, there are some differences. One difference is that reviewers use the fault detection activity to to find faults, not just to prepare for the inspection meeting.
Reference: [11] <author> G. E. P. Box, W. G. Hunter, and J. S. Hunter, </author> <title> Statistics for Experimenters, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: The second step was to evaluate the combined effect of the variables shown to be significant in the initial analysis. Both analyses use standard analysis of variance methods (see <ref> [11] </ref>, pp. 165ff and 210ff or [12]). Once these relationships were discovered and their magnitude estimated, we examined other data, such as correlations between the categories of faults detected and the detection methods used that would confirm or reject (if possible) a causal relationship between detection methods and inspection performance.
Reference: [12] <author> R. M. </author> <title> Heiberger, Computation for the Analysis of Designed Experiments, </title> <publisher> Wiley & Sons, </publisher> <address> New York, New York, </address> <year> 1989. </year>
Reference-contexts: The second step was to evaluate the combined effect of the variables shown to be significant in the initial analysis. Both analyses use standard analysis of variance methods (see [11], pp. 165ff and 210ff or <ref> [12] </ref>). Once these relationships were discovered and their magnitude estimated, we examined other data, such as correlations between the categories of faults detected and the detection methods used that would confirm or reject (if possible) a causal relationship between detection methods and inspection performance. B.
Reference: [13] <author> Kathryn L. Heninger, </author> <title> "Specifying Software Requirements for Complex Systems: New Techniques and their Application", </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. SE-6, no. 1, </volume> <pages> pp. 2-13, </pages> <month> January </month> <year> 1980. </year>
Reference-contexts: The gaps in the Scenario and Checklist columns indicate that the Checklist is a subset of the Ad Hoc and the Scenarios are a subset of the Checklist. while the other three sections are specified using the SCR tabular requirements notation <ref> [13] </ref>. For this experiment, all three documents were adapted to adhere to the IEEE suggested format [2]. All faults present in these SRS appear in the original documents or were generated during the adaptation process; no faults were intentionally seeded into the document.
Reference: [14] <author> William G. Wood, </author> <title> "Temporal logic case study", </title> <type> Tech. Rep. </type> <institution> CMU/SEI-89-TR-24, Software Engineering Institute, </institution> <address> Pittsburgh, PA, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: The authors discovered 42 faults in the WLMS SRS; and 26 in the CRUISE SRS. The authors did not inspect the ELEVATOR SRS since it was used only for training exercises. B.1.a Elevator Control System (ELEVATOR). <ref> [14] </ref> describes the functional and performance requirements of a system for monitoring the operation of a bank of elevators (16 pages). B.1.b Water Level Monitoring System (WLMS). [15] describes the functional and performance requirements of a system for monitoring the operation of a steam generating system (24 pages).
Reference: [15] <author> J. vanSchouwen, </author> <title> "The A-7 requirements model: Re-examination for real-time systems and an application to monitoring systems", </title> <type> Tech. Rep. </type> <institution> TR-90-276, Queen's University, Kingston, </institution> <address> Ontario, Canada, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: The authors did not inspect the ELEVATOR SRS since it was used only for training exercises. B.1.a Elevator Control System (ELEVATOR). [14] describes the functional and performance requirements of a system for monitoring the operation of a bank of elevators (16 pages). B.1.b Water Level Monitoring System (WLMS). <ref> [15] </ref> describes the functional and performance requirements of a system for monitoring the operation of a steam generating system (24 pages). B.1.c Automobile Cruise Control System (CRUISE). [16] describes the functional and performance requirements for an automobile cruise control system (31 pages).
Reference: [16] <author> J. Kirby, </author> <title> "Example NRL/SCR software requirements for an automobile cruise control and monitoring system", </title> <type> Tech. Rep. </type> <institution> TR-87-07, Wang Institute of Graduate Studies, </institution> <month> July </month> <year> 1984. </year>
Reference-contexts: B.1.b Water Level Monitoring System (WLMS). [15] describes the functional and performance requirements of a system for monitoring the operation of a steam generating system (24 pages). B.1.c Automobile Cruise Control System (CRUISE). <ref> [16] </ref> describes the functional and performance requirements for an automobile cruise control system (31 pages). B.2 Fault Detection Methods To make a fair assessment of the three detection methods (Ad Hoc, Checklist, and Scenario) each method should search for a well-defined population of faults.
Reference: [17] <author> G. Michael Schneider, Johnny Martin, and W. T. Tsai, </author> <title> "An ex-perimental study of fault detection in user requirements", </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 188-204, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: As a result, Scenario responsibilities are distinct subsets of Checklist and Ad Hoc responsibilities. The relationship between the three methods is depicted in The taxonomy is a composite of two schemes developed by Schneider, et al. <ref> [17] </ref> and Basili and Weiss [18]. Faults are divided into two broad types: omission in which important information is left unstated and commission in which incorrect, redundant, or ambiguous information is put into the SRS by the author.
Reference: [18] <author> V. R. Basili and D. M. Weiss, </author> <title> "Evaluation of a software require-ments document by analysis of change data", </title> <booktitle> in Proceedings of the Fifth International Conference on Software Engineering, </booktitle> <address> San Diego, CA, </address> <month> March </month> <year> 1981, </year> <pages> pp. 314-323. </pages>
Reference-contexts: As a result, Scenario responsibilities are distinct subsets of Checklist and Ad Hoc responsibilities. The relationship between the three methods is depicted in The taxonomy is a composite of two schemes developed by Schneider, et al. [17] and Basili and Weiss <ref> [18] </ref>. Faults are divided into two broad types: omission in which important information is left unstated and commission in which incorrect, redundant, or ambiguous information is put into the SRS by the author. Omission faults were further subdivided into four categories: Missing Functionality, Missing Performance, Missing Environment, and Missing Interface.
Reference: [19] <institution> IEEE Guide to Software Requirements Specifications, Soft. Eng. Tech. Comm. of the IEEE Computer Society, </institution> <year> 1984, </year> <note> IEEE Std 830-1984. </note>
Reference-contexts: The references for these lectures were Fagan [5], Parnas [4], and the IEEE Guide to Software Requirements Specifications <ref> [19] </ref>. The participants were then assembled into three-person teams see Section II-A.3 for details. Within each team, members were randomly assigned to act as the moderator, the recorder, or the reader during the collection meeting. D.
Reference: [20] <author> S. Siegel and N.J. Castellan, Jr., </author> <title> Nonparametric Statistics For the Behavioral Sciences, </title> <publisher> McGraw-Hill Book Company, </publisher> <address> New York, NY, </address> <note> second edition, </note> <year> 1988. </year>
Reference-contexts: To examine this we analyzed the individual fault summaries to see how Scenario reviewers differed from other reviewers. The detection rates of Scenario reviewers are compared with those of all other reviewers in Tables V, VI, VII and VIII. Using the one and two-sided Wilcoxon-Mann- Whitney tests <ref> [20] </ref>, we found that in most cases Scenario reviewers were more effective than Checklist or Ad Hoc reviewers at finding the faults the scenario was designed to uncover.
References-found: 20

