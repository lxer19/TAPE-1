URL: http://www.cse.psu.edu/~zha/papers/lsi_per.ps
Refering-URL: http://www.cse.psu.edu/~zha/papers.html
Root-URL: http://www.cse.psu.edu
Title: ON MATRICES WITH LOW-RANK-PLUS-SHIFT STRUCTURE: PARTIAL SVD AND LATENT SEMANTIC INDEXING  
Author: HONGYUAN ZHA AND ZHENYUE ZHANG 
Abstract: We present a detailed analysis of matrices satisfying the so-called low-rank-plus-shift property in connection with the computation of their partial singular value decomposition. The application we have in mind is Latent Semantic Indexing for information retrieval where the term-document matrices generated from a text corpus approximately satisfy this property. The analysis is motivated by developing more efficient methods for computing and updating partial SVD of large term-document matrices and gaining deeper understanding of the behavior of the methods in the presence of noise. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Berry. </author> <title> Large Scale Singular Value Computations. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 6 </volume> <pages> 13-49, </pages> <year> 1992. </year>
Reference: [2] <author> M.W. Berry, S.T. Dumais and G.W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37 </volume> <pages> 573-595, </pages> <year> 1995. </year>
Reference-contexts: In Section 6 we conclude the paper with some remarks on future research. 2. Latent Semantic Indexing. Latent semantic indexing is a concept-based automatic indexing method that aims at overcoming the two fundamental problems which plague traditional lexical-matching indexing schemes: synonymy and polysemy <ref> [2, 5] </ref>. <p> Corresponding Partial SVD and Latent Semantic Indexing 3 to each of the k reduced dimensions is associated a latent concept which may not have any explicit semantic content yet helps to discriminate documents <ref> [2, 5] </ref>. Large text corpora such as those generated from World Wide Web give rise to very large term-document matrices, and the computation of their partial SVD poses a very challenging problem. Fortunately the term-document matrices possess certain useful properties besides sparsity that can be exploited for this matter.
Reference: [3] <institution> Cornell SMART System, ftp://ftp.cs.cornell.edu/pub/smart. </institution>
Reference-contexts: Examples. In the following we will apply the above theorem to two example text collections and see how close the associated term-document matrices are to the set of matrices with low-rank-plus-shift structure. Our first example is the MEDLINE collection from the Cornell SMART system <ref> [3] </ref>. The term-document matrix is of size 3681 fi 1033. The singular value distribution is plotted on the left of Figure 1. Our second example is from a collection consisting of news articles from 20 newsgroups [4]. The term-document matrix is of size 33583 fi 1997.
Reference: [4] <institution> CMU Data, </institution> <note> http://www.cs.cmu.edu/afs/cs/project/theo-11/www/naive-bayes.html. </note>
Reference-contexts: Our first example is the MEDLINE collection from the Cornell SMART system [3]. The term-document matrix is of size 3681 fi 1033. The singular value distribution is plotted on the left of Figure 1. Our second example is from a collection consisting of news articles from 20 newsgroups <ref> [4] </ref>. The term-document matrix is of size 33583 fi 1997. Its singular values are plotted on the right of Figure 1.
Reference: [5] <author> S. Deerwester, S.T. Dumais, T.K. Landauer, G.W. Furnas and R.A. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the Society for Information Science, </journal> <volume> 41 </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: In Section 6 we conclude the paper with some remarks on future research. 2. Latent Semantic Indexing. Latent semantic indexing is a concept-based automatic indexing method that aims at overcoming the two fundamental problems which plague traditional lexical-matching indexing schemes: synonymy and polysemy <ref> [2, 5] </ref>. <p> Corresponding Partial SVD and Latent Semantic Indexing 3 to each of the k reduced dimensions is associated a latent concept which may not have any explicit semantic content yet helps to discriminate documents <ref> [2, 5] </ref>. Large text corpora such as those generated from World Wide Web give rise to very large term-document matrices, and the computation of their partial SVD poses a very challenging problem. Fortunately the term-document matrices possess certain useful properties besides sparsity that can be exploited for this matter.
Reference: [6] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: The theory of singular value decomposition (SVD) provides the following characterization of the best low rank approximation of A in terms of Frobenius norm k k F <ref> [6] </ref>. Theorem 1.1. Let the singular value decomposition of A 2 R mfin be A = P Q T with = diag (oe 1 ; : : : ; oe min (m;n) ), oe 1 : : : oe min (m;n) , and P and Q orthogonal. <p> Therefore all we need to prove is kA Jk p min p for p = 2; F and for any J 2 J k . To this end we use standard perturbation analysis of singular values which states that <ref> [6] </ref> joe i (A) oe i (J )j kA J k 2 ; i = 1; : : : ; minfm; ng; and minfm;ng X i=1 F : It follows that max k+1iminfm;ng joe i (A) oe i (J )j max 1iminfm;ng joe i (A) oe i (J )j kA J
Reference: [7] <author> G. Kowalski. </author> <title> Information Retrieval System: Theory and Implementation. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1997. </year>
Reference-contexts: LSI is an extension of the vector space model for information retrieval <ref> [7, 10] </ref>.
Reference: [8] <author> R. Krovetz and W.B. Croft. </author> <title> Lexical ambiguity and information retrieval. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 10 </volume> <pages> 115-141, </pages> <year> 1992. </year>
Reference-contexts: Synonymy refers to the problem that several different words can be used to express a concept and the keywords in a user's query may not match those in the relevant documents while polysemy means that words can have multiple meanings and user's words may match those in irrelevant documents <ref> [8] </ref>. LSI is an extension of the vector space model for information retrieval [7, 10].
Reference: [9] <author> B.N. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice Hall Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1980. </year>
Reference-contexts: The state-of-the-art methods for computing the partial SVD of large and/or sparse matrices are based on variants of Lanczos algorithms and the core computation at each iterative steps involves matrix-vector multiplications <ref> [9] </ref>. In order to effectively deal with large-scale problems, one is required to exploit various structures of the matrices.
Reference: [10] <author> G. Salton. </author> <title> Automatic Text Processing. </title> <publisher> Addison-Wesley, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: LSI is an extension of the vector space model for information retrieval <ref> [7, 10] </ref>.
Reference: [11] <author> H. Simon and H. Zha. </author> <title> Low Rank Matrix Approximation Using the Lanczos Bidiagonalization Process. </title> <type> CSE Tech. Report CSE-97-008, </type> <year> 1997. </year> <note> (Also LBNL Tech. Report LBNL-40767-UC-405.) </note>
Reference-contexts: 1. Introduction. In many applications such as compression of multiple-spectral image cubes, regularization methods for ill-posed problems, latent semantic indexing in information retrieval for large document collections, it is necessary to find a low rank approximation of a given large and/or sparse matrix A 2 R mfin <ref> [11] </ref>. The theory of singular value decomposition (SVD) provides the following characterization of the best low rank approximation of A in terms of Frobenius norm k k F [6]. Theorem 1.1.
Reference: [12] <author> G. Xu and T. Kailath. </author> <title> Fast subspace decompsotion. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 42 </volume> <pages> 539-551, </pages> <year> 1994. </year>
Reference-contexts: In the following we consider the cases where distance is defined either by Frobenius norm k k F or spectral norm k k 2 . 1 The low-rank-plus-shift structure was first discussed in the context of array signal processing <ref> [12, 13, 17] </ref>. Partial SVD and Latent Semantic Indexing 4 Theorem 3.1. Let the SVD of A be A = U V T , = diag (oe 1 ; : : : ; oe minfm;ng ), and U and V orthogonal.
Reference: [13] <author> G. Xu, H. Zha, G. Golub, and T. Kailath. </author> <title> Fast algorithms for updating signal subspaces. </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <volume> 41 </volume> <pages> 537-549, </pages> <year> 1994. </year> <title> Partial SVD and Latent Semantic Indexing 15 </title>
Reference-contexts: In the following we consider the cases where distance is defined either by Frobenius norm k k F or spectral norm k k 2 . 1 The low-rank-plus-shift structure was first discussed in the context of array signal processing <ref> [12, 13, 17] </ref>. Partial SVD and Latent Semantic Indexing 4 Theorem 3.1. Let the SVD of A be A = U V T , = diag (oe 1 ; : : : ; oe minfm;ng ), and U and V orthogonal.
Reference: [14] <author> H. Zha and H.D. Simon. </author> <title> On Updating Problems in Latent Semantic Indexing. </title> <type> Technical Report CSE-97-008, </type> <institution> Department of Computer Science and Engineering, The Pennsylvania State University, </institution> <year> 1997. </year> <note> To appear in SIAM Journal on Scientific Computing. </note>
Reference-contexts: The rest of the paper is organized as follows: In Section 2, we provide a brief background on LSI and review some of the results in <ref> [14, 15, 16] </ref> related to computing the partial SVD of term-document matrices. In Section 3, we discuss some matrix approximation problems associated with the low-rank-plus-shift structure, and show by way of examples that the term-document matrices generated from text corpora approximately satisfy Equation (1.1). <p> In LSI applications k o minfm; ng, justifying the use of the terminology low-rank-plus-shift structure. In <ref> [14] </ref>, we considered the updating problems for LSI: Let A be the term-document matrix for the original text collection and D represents a collection of new documents. The goal is to compute the partial SVD of [A; D]. <p> In Section 4, we show, however, that this is not the case since [A; D] has the low-rank-plus-shift structure <ref> [14] </ref>. We will show that no retrieval accuracy degradation will occur if updating is done with a proper implementation. <p> With the above preparation we present our first result. The proof is similar to that of a slightly special case presented in <ref> [14] </ref> and therefore is omitted. Theorem 4.1. Let A 2 R mfin and write A = [A 1 ; A 2 ]. <p> That is to say, we will obtain the same best low-rank approximation even though A is replaced by best k (A). Numerical results conducted on several text collections show that no retrieval accuracy degradation occurs when updating is computed using a proper implementation <ref> [14] </ref>. On the other hand, Theorem 4.2 also leads to some novel approaches for computing a low-rank approximation of a large matrix. There are at least two general approaches to pursue ideas based on Theorem 4.2: * An Incremental Method. <p> This incremental process can be very useful when the data collection is very large and the whole term-document matrix can not reside completely in main memory. Some computational results of this approach can be found in <ref> [14] </ref>. * A Divide-and-conquer Method. Another approach is what we call a divide-and-conquer approach, we can again divide the whole collection of documents into several groups, and compute the rank-k approximation for each group and then combine the results together into a rank-k approximation for the whole data collection. <p> Our perturbation analysis demonstrates that the results we have derived are still valid even the low-rank-plus-shift structure is approximately satisfied. The results we have proved provide theoretical justifications for the novel LSI updating algorithms and the incremental and divide-and-conquer approaches proposed in <ref> [14, 16] </ref>. Our future research will concentrate on further developing the numerical algorithms and their parallel implementations.
Reference: [15] <author> H. Zha and H. Simon. </author> <title> A Subspace-based model for Latent Semantic Indexing in information retrieval. </title> <note> To appear in Proceedings of Interface '98. </note>
Reference-contexts: In large-scale LSI applications such as the World Wide Web, the term-document matrix generated is usually very large and can not be kept in RAM or disk. In <ref> [15, 16] </ref> we have shown that the low-rank-plus-shift structure of the term-document matrix A allows us to compute its partial SVD in a block-wise fashion whereby partial SVD of sub-matrices of A are computed separately and then merged to obtain a partial SVD of A. <p> The rest of the paper is organized as follows: In Section 2, we provide a brief background on LSI and review some of the results in <ref> [14, 15, 16] </ref> related to computing the partial SVD of term-document matrices. In Section 3, we discuss some matrix approximation problems associated with the low-rank-plus-shift structure, and show by way of examples that the term-document matrices generated from text corpora approximately satisfy Equation (1.1). <p> Large text corpora such as those generated from World Wide Web give rise to very large term-document matrices, and the computation of their partial SVD poses a very challenging problem. Fortunately the term-document matrices possess certain useful properties besides sparsity that can be exploited for this matter. In <ref> [15, 16] </ref>, we developed a theoretical foundation for LSI using the concept of subspaces, and we showed that the model we proposed imposes a so-called low-rank-plus-shift structure that is approximately satisfied by the cross-product of the term-document matrices. 1 Specifically, we showed that the term-document matrix A 2 R mfin satisfies <p> In Section 4, we show, however, that this is not the case since [A; D] has the low-rank-plus-shift structure [14]. We will show that no retrieval accuracy degradation will occur if updating is done with a proper implementation. In <ref> [15, 16] </ref>, we also discussed how to compute the partial SVD of a term-document matrix in a block-column partitioned form A = [A 1 ; A 2 ] using a divide-and-conquer approach whereby the partial SVDs of A 1 and A 2 are first computed and the results are then merged
Reference: [16] <author> H. Zha, O. Marques and H. Simon. </author> <title> Large-scale SVD and Subspace-based Methods for Information Retrieval. </title> <booktitle> To appear in Proceedings of Irregular '98, Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In large-scale LSI applications such as the World Wide Web, the term-document matrix generated is usually very large and can not be kept in RAM or disk. In <ref> [15, 16] </ref> we have shown that the low-rank-plus-shift structure of the term-document matrix A allows us to compute its partial SVD in a block-wise fashion whereby partial SVD of sub-matrices of A are computed separately and then merged to obtain a partial SVD of A. <p> The rest of the paper is organized as follows: In Section 2, we provide a brief background on LSI and review some of the results in <ref> [14, 15, 16] </ref> related to computing the partial SVD of term-document matrices. In Section 3, we discuss some matrix approximation problems associated with the low-rank-plus-shift structure, and show by way of examples that the term-document matrices generated from text corpora approximately satisfy Equation (1.1). <p> Large text corpora such as those generated from World Wide Web give rise to very large term-document matrices, and the computation of their partial SVD poses a very challenging problem. Fortunately the term-document matrices possess certain useful properties besides sparsity that can be exploited for this matter. In <ref> [15, 16] </ref>, we developed a theoretical foundation for LSI using the concept of subspaces, and we showed that the model we proposed imposes a so-called low-rank-plus-shift structure that is approximately satisfied by the cross-product of the term-document matrices. 1 Specifically, we showed that the term-document matrix A 2 R mfin satisfies <p> In Section 4, we show, however, that this is not the case since [A; D] has the low-rank-plus-shift structure [14]. We will show that no retrieval accuracy degradation will occur if updating is done with a proper implementation. In <ref> [15, 16] </ref>, we also discussed how to compute the partial SVD of a term-document matrix in a block-column partitioned form A = [A 1 ; A 2 ] using a divide-and-conquer approach whereby the partial SVDs of A 1 and A 2 are first computed and the results are then merged <p> Our perturbation analysis demonstrates that the results we have derived are still valid even the low-rank-plus-shift structure is approximately satisfied. The results we have proved provide theoretical justifications for the novel LSI updating algorithms and the incremental and divide-and-conquer approaches proposed in <ref> [14, 16] </ref>. Our future research will concentrate on further developing the numerical algorithms and their parallel implementations.
Reference: [17] <author> H. Zha and Z. Zhang. </author> <title> A sorted partial Jacobi method and its convergence analysis. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 270 </volume> <pages> 79-108, </pages> <year> 1997. </year>
Reference-contexts: In the following we consider the cases where distance is defined either by Frobenius norm k k F or spectral norm k k 2 . 1 The low-rank-plus-shift structure was first discussed in the context of array signal processing <ref> [12, 13, 17] </ref>. Partial SVD and Latent Semantic Indexing 4 Theorem 3.1. Let the SVD of A be A = U V T , = diag (oe 1 ; : : : ; oe minfm;ng ), and U and V orthogonal.
References-found: 17

