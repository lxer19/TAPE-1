URL: http://polaris.cs.uiuc.edu/~pengwu/research/container-analysis.ps
Refering-URL: http://polaris.cs.uiuc.edu/~pengwu/research/research.html
Root-URL: http://www.cs.uiuc.edu
Email: padua@cs.uiuc.edu  
Title: Beyond Arrays A Container-centric Approach For Parallelization Of Real-world Symbolic Applications  
Author: Peng Wu and David Padua 
Date: August 3, 1998  
Address: pengwu,  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign  
Abstract: Parallelization of symbolic applications is difficult and a systematic approach has yet to be developed. In this paper, we introduce the concept container, which refers to any general-purpose aggregate data type, such as matrices, lists, tables, graphs and I/O streams. We propose the container-centric approach, in which containers are treated by the compiler as built-in data types. They are the target of data-parallelism and the focus of program analysis and transformations. We apply the container-centric approach to address the parallelization of symbolic applications. By hand-parallelizing a few real-world symbolic applications with the proposed techniques, we demonstrate that not only is there enough parallelism in symbolic applications, but that such applications exhibit as much regularity as we have observed in array-based applications and are highly amiable for automatic parallelization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> William Blume, Rudolf Eigenmann, Jay Hoeflinger, David Padua, Paul Petersen and Lawrence Rauchwerger. </author> <title> Automatic Detection of Parallelism: A Grand Challenge for High-Performance Computing.IEEE Parallel and Distributed Techniology, </title> <booktitle> 2(3) </booktitle> <pages> 37-47, </pages> <month> fall </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The last decade has witnessed extensive researches on automatic parallelization of numerical array-based applications. With aggressive analysis and transformation techniques <ref> [1] </ref> [2], state-of-the-art parallelizing compilers now can parallelize some large and real scientific benchmarks [3][5][4]. However, there is a large set of non-numerical applications, some of which are fairly simple, that still cannot be parallelized by current techniques. <p> However, containers observe patterns different from that of arrays in terms of privatization. For arrays and scalars, a variable is privatizable if every use of the variable must be dominated by a definition of the same variable in the same loop iteration <ref> [1] </ref>. By contrast, manipulations of temporary linear containers observe the define-use-reset cycle, in which states of the container are reset to the original ones after the use. Define-use-reset is common not only in manipulations of linear containers but also in those of common objects. <p> Two statement instances may execute in any order or even in parallel when there is no chain of dependence relations among them <ref> [1] </ref>. Several factors can lead to data dependences in container manipulations, such as structural dependences, overlapped accesses of container elements, and aliasing between container elements. Detection of structural dependences is trivial.
Reference: [2] <author> U.Banerjee, R.Eigenmann, A.Nicolau, D.A.Padua. </author> <title> Automatic Program Parallelization. </title> <booktitle> Proceedings of of IEEE, </booktitle> <address> Vol.81.No.2, Feb.1993. </address>
Reference-contexts: 1 Introduction The last decade has witnessed extensive researches on automatic parallelization of numerical array-based applications. With aggressive analysis and transformation techniques [1] <ref> [2] </ref>, state-of-the-art parallelizing compilers now can parallelize some large and real scientific benchmarks [3][5][4]. However, there is a large set of non-numerical applications, some of which are fairly simple, that still cannot be parallelized by current techniques.
Reference: [3] <author> W.Blume, R.Eigenmann, K.Faigin, J.Grout, T.Lawrence, J.Hoeflinger, D.Padua, Y.Paek, P.Petersen, W.Pottenger, L.Rauchwerger, P.Tu, and S.Weatherford. </author> <title> Restructuring Programs for High-Speed Computers with Polaris. </title> <booktitle> Proceedings of the 1996 ICPP Workshop on Challendes for Parallel Processing. </booktitle> <month> August, </month> <year> 1996. pp.149-161.1996 </year>
Reference: [4] <author> M.W.Hallm J.M.Anderson, S.P.Amarasinghe, B.R.Murphy, S.-W.Liao, E.Bugnion and M.S.Lam. </author> <title> Maximizing Multiprocessor Performance with the SUIF Compiler. </title> <booktitle> IEEE Computer, </booktitle> <month> December </month> <year> 1996. </year>
Reference: [5] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic parallelization of four Perfect-Benchmark programs. </title> <booktitle> Proceedings of 4th Workshop on Programming Languages and Compilers for Parallel Computing. </booktitle> <publisher> Pitman/MIT Press, </publisher> <month> August </month> <year> 1991. </year>
Reference: [6] <author> E.Gamma, R.Helm, R. Johnson, and J.Vlissides. </author> <title> Design Patterns. </title> <publisher> Addison-Wesley, </publisher> <address> MA, </address> <year> 1995. </year>
Reference-contexts: Experimental results are given in section 6. Section 7 compares our work with others. And section 8 summarizes and presents a conclusion. 2 The Container-centric Approach 2.1 Concept of container We define any general-purpose aggregate data type as a container <ref> [6] </ref>. Examples of containers are matrices, lists, stacks, trees, graphs, I/O streams and hash tables. In the paper we focus on two types of containers: linear containers and content-addressable containers. Containers of other types will be studied in the future.
Reference: [7] <author> David R. Musser and Atul Saini. </author> <title> STL tutorial & reference guide : C++ programming with the standard template library. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: A linear container is a container whose elements are accessed by positions within the container in an ordered manner. Commonly seen linear containers are lists, stacks and queues. Linear containers can be addressed through iterators <ref> [7] </ref>, which allows container elements to be accessed in a way similar to how arrays are accessed through indexes. In real applications, iterators together with language constructs, such as for-loops and while-loops, compose common access patterns of linear containers. In a content-addressable container, elements are accessed by keys.
Reference: [8] <author> William Blume, Ramon Doallo, Rudolf Eigenmann, John Grout, Jay Hoeflinger, Thomas Lawrence, Jaejin Lee, David Padua, Yunheung Paek, Bill Pottenger, Lawrence Rauchwerger, and Peng Tu. </author> <title> Parallel Programming with Polaris. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 78-82, </pages> <month> December </month> <year> 1996. </year>
Reference: [9] <author> W.Ludwell Harrison III.. </author> <title> The interprocedural analysis and automatic parallelization of scheme programs. </title> <journal> Lisp and Symbolic Computation, </journal> 2(3/4):179-396, 1989. 
Reference-contexts: For example, implementing linear containers with arrays can provide very flexible and efficient accessing; or, for linked-lists, access can be speeded up by providing auxiliary access pointers that directly reach elements with a long distance in between <ref> [9] </ref>. 4.3 Container privatization Container privatization plays the same role in the parallelization of container-based applications as array privatization does for array-based applications. Both efficiently eliminate dependences not due to true data-flow. However, containers observe patterns different from that of arrays in terms of privatization.
Reference: [10] <author> W.Ludwell Harrison III.. </author> <title> Generalized iteration space and the parallelization of symbolic programs. </title> <editor> In Ian Foster and Evan Tick, editors, </editor> <booktitle> Proceedings of the Workshop on Computation of Symbolic Languages for Parallel Computers. </booktitle> <institution> Argonne National Laboratory, </institution> <month> October </month> <year> 1991. </year>
Reference: [11] <author> Evelyn Duesterwald, Rajiv Gupta, Mary Lou Soffa. </author> <title> A Practical Data Flow Framework for Array Reference Analysis and its Use in Optimizations. </title> <booktitle> the proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 68-76, </pages> <month> June. </month> <year> 1993. </year>
Reference: [12] <author> James R. Larus and Paul N. Hilfinger. </author> <title> Detecting conflicts between structure accesses. </title> <booktitle> Proceedings of the SIGPLAN '88 COnference on Programming Language Design and Implementation, </booktitle> <pages> pp. 21-34, </pages> <month> June </month> <year> 1988. </year>
Reference: [13] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data dependence and data flow analysis of arrays. </title> <booktitle> Proceedings of 5th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference: [14] <author> William Blume and Rudolf Eigenmann. </author> <title> The Range Test: A Dependence Test for Symboli, Nonlinear Expressions. </title> <type> TR 1345, </type> <institution> Univ. of Illinois Urbana-Champaign, Center for Supercomputing Research & Development, </institution> <month> April </month> <year> 1994. </year> <month> 19 </month>
Reference-contexts: For example, the range associated with the loop in Figure 8 (i) is [abs begin iterator, abs end iterator]. And, for fix-position accesses, such as abs push back (), the associated range is [abs end iterator, abs end iterator]. Similar to the range test <ref> [14] </ref> for arrays, dependence tests for linear containers are based on the detection of possible overlapping between the ranges of any two iterations. Two relations on iterators are defined to compare iterators.
Reference: [15] <author> Peng Tu and David Padua. </author> <title> Automatic Array Privatization. </title> <booktitle> Proceedings of Sixth Workshop on Lan--guages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR. </address> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> vol. 768, </volume> <pages> pp. 500-521, </pages> <month> August 12-14, </month> <year> 1993. </year>
Reference: [16] <author> Martin C. Rinard, Pedro C.Diniz. </author> <title> Commutativity Analysis: A New Analysis Technique for Paral-lelizing Compilers. </title> <journal> ACM Transactions on Programming Lanugages and Systems, </journal> <volume> Volume 19, </volume> <pages> pages 942-991, </pages> <month> November </month> <year> 1997. </year>
Reference-contexts: Traditional parallelizing compilers detect parallelizable loops by proving independent computation between iterations. Though sufficient, such a constraint is not necessary. Since it is the execution order 14 that parallel loops have changed from the original ones, a loop is parallelizable if computations across iterations are commutable <ref> [16] </ref>. Thus, operations on a container are commutable if reordering the operations leaves the container in a state equivalent to that in the original ordering, according to the rest of the operations on the container.
Reference: [17] <author> J. Hummel and L. Hendren and A. Nicolau. </author> <title> A General Data Dependence Test for Dynamic, Pointer-Based Data Structures, </title> <booktitle> Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 218-229, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Based on the work of ASAP, which uses regular expressions to convey the alias property of the data structure, the authors later proposed a dependence test algorithm using theorem proving <ref> [17] </ref>.
Reference: [18] <author> L. Hendren and J. Hummel and A. Nicolau. </author> <title> Abstractions for Recursive Pointer Data Structures: Improving the Analysis and Transformation of Imperative programs, </title> <booktitle> Proceedings of the ACM SIG-PLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 249-260, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: in which the compiler discovers the shape [23][12][21] or alias properties [22] of the data structure; and (2) a data annotation approach, in which information of the data structure is provided by the user based on a pre-defined description language [24]<ref> [18] </ref>[19]. Hummel, Hendren, and Nicolau proposed two description languages, ADDS [18] and ASAP [19], by which users can describe the shape and traversal properties of the data structures. Based on the work of ASAP, which uses regular expressions to convey the alias property of the data structure, the authors later proposed a dependence test algorithm using theorem proving [17].
Reference: [19] <author> J.Hummel, L.Hendren and A.Nicolau. </author> <title> A Language for Conveying the Alising Properties of Dynamic, Poiinter-Based Data Structures, </title> <booktitle> Proceedings of the 8th Inthernational Parallel Processing Symposium, </booktitle> <address> pp.208-216, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: compiler discovers the shape [23][12][21] or alias properties [22] of the data structure; and (2) a data annotation approach, in which information of the data structure is provided by the user based on a pre-defined description language [24][18]<ref> [19] </ref>. Hummel, Hendren, and Nicolau proposed two description languages, ADDS [18] and ASAP [19], by which users can describe the shape and traversal properties of the data structures. Based on the work of ASAP, which uses regular expressions to convey the alias property of the data structure, the authors later proposed a dependence test algorithm using theorem proving [17].
Reference: [20] <author> Laurie J.Hendren, Guang R. Gao. </author> <title> Designing Programming Languages for Analyzability: a Fresh Look at Pointer Data Structures. </title> <booktitle> Proceedings of the International Conference on Computer Lanu-gaes, </booktitle> <pages> pp. 242-251, </pages> <address> Oakland, CA, </address> <month> April </month> <year> 1992. </year>
Reference: [21] <author> D.R.Chase, M.Wegman, and F.K.Zadek. </author> <title> Analysis of pointers and structures. </title> <booktitle> Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 296-310, </pages> <year> 1990. </year>
Reference: [22] <author> Laurie J.Hendren and Alexandru Nicolau. </author> <title> Parallelizing programs with recursive data structures. </title> <booktitle> IEEE Thans. on Parallel and Distributed Computing, </booktitle> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: Basic approaches of pointer analysis fall into two categories:(1) a fully automatic approach, in which the compiler discovers the shape [23][12][21] or alias properties <ref> [22] </ref> of the data structure; and (2) a data annotation approach, in which information of the data structure is provided by the user based on a pre-defined description language [24][18][19].
Reference: [23] <author> James R. Larus and Paul N. Hilfinger. </author> <title> Restructuring LISP programs for concurrent execution. </title> <booktitle> Proceedings of the ACM/SIGPLAN PPEALS 1988 Parallel Programming: Experience with applications, Languages and Systems, </booktitle> <pages> pp. 100-110, </pages> <month> July </month> <year> 1988. </year>
Reference: [24] <author> James R. Larus, </author> <title> Restructuring Symbolic Programs for Concurrent Execution on Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1989. </year> <month> 20 </month>
References-found: 24

