URL: http://www.cs.berkeley.edu/~aiken/ftp/ieeetpds95.ps
Refering-URL: http://www.cs.berkeley.edu/~aiken/pubs.html
Root-URL: 
Email: email: aiken@cs.berkeley.edu  email: nicolau@ics.uci.edu  email: snovack@ics.uci.edu  
Title: Resource-Constrained Software Pipelining  
Author: Alexander Aiken Alexandru Nicolau Steven Novack 
Address: Berkeley, CA 94720-1776  Irvine, CA 92717  Irvine, CA 92717  
Affiliation: Computer Science Division University of California, Berkeley  Department of Information and Computer Science University of California, Irvine  Department of Information and Computer Science University of California, Irvine  
Abstract: This paper presents a software pipelining algorithm for the automatic extraction of fine-grain parallelism in general loops. The algorithm accounts for machine resource constraints in a way that smoothly integrates the management of resource constraints with software pipelining. Furthermore, generality in the software pipelining algorithm is not sacrificed to handle resource constraints, and scheduling choices are made with truly global information. Proofs of correctness and the results of experiments with an implementation are also presented.
Abstract-found: 1
Intro-found: 1
Reference: [AAG + 86] <author> M. Annaratone, E. Arnould, T. Gross, H. T. Kung, M. Lam, O. Menzilcioglu, K. Sarocky, and J. A. Webb. </author> <title> Warp architecture and implementation. </title> <booktitle> In Proceedings of the 13th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 346-356, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: The next state to be executed is the leaf that terminates the (unique) path from the root where every branch is labeled by the value of that test in the store. There are other possible implementations of multi-way jumps; many mechanisms have been proposed and implemented <ref> [Fis80, KN85, AAG + 86, Ebc87] </ref>. The software pipelining algorithm we present applies to any of these control-flow mechanisms. We use the following abstraction of control-flow throughout this paper.
Reference: [Aik88] <author> A. Aiken. </author> <title> Compaction-Based Parallelization. </title> <type> PhD thesis, </type> <institution> Cornell, </institution> <year> 1988. </year> <note> Department of Computer Science Technical Report No. 88-922. </note>
Reference-contexts: Second, the algorithm represents a summary of many of the most interesting aspects of our investigation of software pipelining over the last several years <ref> [Nic85, AN88a, AN88b, Aik88, Aik90, AN91] </ref>.
Reference: [Aik90] <author> A. Aiken. </author> <title> A theory of compaction-based parallelization. </title> <journal> Theoretical Computer Science, </journal> <volume> 73 </volume> <pages> 121-154, </pages> <year> 1990. </year>
Reference-contexts: Second, the algorithm represents a summary of many of the most interesting aspects of our investigation of software pipelining over the last several years <ref> [Nic85, AN88a, AN88b, Aik88, Aik90, AN91] </ref>.
Reference: [AJLS92] <author> V. H. Allan, J. Janardhan, R.M. Lee, and M. Srinivas. </author> <title> Enhanced Region Scheduling on a Program Dependence Graph. </title> <booktitle> In Proceedings of the 25th International Symposium and Workshop on Microarchitecture (MICRO-25), </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: Another approach to improving the efficiency of the techniques presented here is to use a representation other than the control-flow graph for computing available operations. The obvious alternative is to use some form of the program dependence graph, which admits more efficient algorithms for some purposes (see <ref> [LA92, AJLS92] </ref> for uses of program dependence graphs in the context of software pipelining).
Reference: [AKPW83] <author> J. R. Allen, K. Kennedy, C. Porterfield, and J. Warren. </author> <title> Conversion of control dependence to data dependence. </title> <booktitle> In Proceedings of the 1983 Symposium on Principles of Programming Languages, </booktitle> <pages> pages 177-189, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: A second proposal for integrating modulo scheduling with conditional tests is to use if-conversion <ref> [AKPW83] </ref> before modulo scheduling and reverse if-conversion [WHB92, WMHR93] after modulo scheduling. When a loop is if-converted, the expression of control flow is changed from explicit jumps to guarded operations, where each operation of the original loop is guarded by the predicates of the conditionals that control its execution.
Reference: [AN88a] <author> A. Aiken and A. Nicolau. </author> <title> Optimal loop parallelization. </title> <booktitle> In Proceedings of the 1988 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 308-317, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Results in this line of development include a software pipelining algorithm that generates optimal code for loops without conditional tests <ref> [AN88a] </ref> and a proof that optimal software pipelining is impossible in general [SGE91]. However, this work has largely ignored resource constraints. Existing software pipelining algorithms handle resource constraints in a variety of ways. <p> Second, the algorithm represents a summary of many of the most interesting aspects of our investigation of software pipelining over the last several years <ref> [Nic85, AN88a, AN88b, Aik88, Aik90, AN91] </ref>. <p> Thus, straightforward modulo scheduling of if-converted loops overestimates resource requirements. For the case of loops without control flow and unlimited resources, there is considerable commonality between our algorithm and modulo scheduling. For example, in <ref> [AN88a] </ref>, it was shown that a simplified version of our algorithm produces optimal code for loops without conditionals in the body and for machines with sufficient resources.
Reference: [AN88b] <author> A. Aiken and A. Nicolau. </author> <title> Perfect Pipelining: A new loop parallelization technique. </title> <booktitle> In Proceedings of the 1988 European Symposium on Programming, </booktitle> <pages> pages 221-235. </pages> <publisher> Springer Verlag Lecture Notes in Computer Science no. </publisher> <address> 300, </address> <month> March </month> <year> 1988. </year> <month> 44 </month>
Reference-contexts: Second, the algorithm represents a summary of many of the most interesting aspects of our investigation of software pipelining over the last several years <ref> [Nic85, AN88a, AN88b, Aik88, Aik90, AN91] </ref>.
Reference: [AN91] <author> A. Aiken and A. Nicolau. </author> <title> A realistic resource-constrained software pipelining algorithm. </title> <booktitle> In Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 274-290. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Second, the algorithm represents a summary of many of the most interesting aspects of our investigation of software pipelining over the last several years <ref> [Nic85, AN88a, AN88b, Aik88, Aik90, AN91] </ref>.
Reference: [Bae80] <author> J. L. Baer. </author> <title> Computer Systems Architecture. </title> <publisher> Computer Press, </publisher> <year> 1980. </year>
Reference-contexts: In this section, we show how the allocation of functional units can be smoothly integrated into our software pipelining algorithm. Our approach to incorporating functional resources is similar to the reservation table methods used in dynamic scheduling algorithms <ref> [Bae80] </ref>. To describe the modifications to the algorithm that accommodate functional resources we require some additional definitions. Let ff 1 ; : : : ; f n g be the set of functional units for a machine.
Reference: [Cha81] <author> A. E. Charlesworth. </author> <title> An approach to scientific array processing: The architectural design of the AP-120b/FPS-164 family. </title> <journal> IEEE Computer, </journal> <volume> 14(3) </volume> <pages> 18-27, </pages> <year> 1981. </year>
Reference-contexts: Programmers in the microcode community software pipelined code by hand for decades [Kog77]. The first semi-automatic technique for software pipelining was proposed by Charlesworth <ref> [Cha81] </ref>. For an overview of the history of instruction level parallelism, see [RF93]. Today there are variety of algorithms and frameworks for software pipelining. We describe each and discuss its relationship to our own work.
Reference: [Cyd87] <institution> Cydrome Inc., Palo Alto, Ca. Technical Summary, </institution> <year> 1987. </year>
Reference-contexts: Modulo scheduling has been used in compilers for the FPS series [Tou84], the polycyclic machine [RG81], and Cydrome's Cydra <ref> [Cyd87] </ref>. A basic modulo scheduling algorithm works as follows. Consider a loop L that requires a resource k times per iteration of the loop body. If the target machine has t of the resource, then an upper bound on the throughput is one iteration of L every k=t cycles.
Reference: [Ebc87] <author> K. Ebcioglu. </author> <title> A compilation technique for software pipelining of loops with conditional jumps. </title> <booktitle> In Proceedings of the 20th Annual Workshop on Microprogramming, </booktitle> <pages> pages 69-79, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: The next state to be executed is the leaf that terminates the (unique) path from the root where every branch is labeled by the value of that test in the store. There are other possible implementations of multi-way jumps; many mechanisms have been proposed and implemented <ref> [Fis80, KN85, AAG + 86, Ebc87] </ref>. The software pipelining algorithm we present applies to any of these control-flow mechanisms. We use the following abstraction of control-flow throughout this paper. <p> Our approach is to modify an initial register allocation "on the fly" during software pipelining. The basic technique is easy to describe; it is based on a similar technique of Ebcioglu <ref> [Ebc87] </ref>. Consider 29 a: r1 r2 op r3 b: r2 r4 op r5 (a) Instruction b is unavailable. b 0 : r6 r4 op r5 a: r1 r2 op r3 c: r2 r6 (b) After renaming registers. the program fragment in Figure 17 (a). <p> In this situation the lack of dependence structure in the program combined with greedy scheduling heuristics tends to lead to an explosion in the set of states, slowing convergence. Variations on a device of Ebcioglu's may show how to modify the scheduler to avoid this problem <ref> [Ebc87] </ref>. The basic idea is to introduce artificial dependencies that don't harm parallelism extraction but dramatically reduce the number of potential states the scheduler may explore. <p> In our opinion, the significant practical advantage of modulo scheduling at this time is that in cases where both techniques produce equally fast schedules, the schedules produced by modulo scheduling are generally more concise [Jon91]. 9.2 Pipeline Scheduling The work most closely related to our own is that of Ebcioglu <ref> [Ebc87] </ref> and Ebcioglu and Nakatani [NE89, EN90, NE90] and later Moon and Ebcioglu [ME92]. Pipeline scheduling differs from our approach in that the loop body is not constructed by scheduling and testing for repeating states. Instead, the original loop is incrementally transformed to create a parallel schedule.
Reference: [EN89] <author> K. Ebcioglu and A. Nicolau. </author> <title> A global resource-constrained parallelization technique. </title> <booktitle> In Proceedings of the ACM SIGARCH International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Section 4 formalizes the software pipelining algorithm and provides constraints on the scheduler and available operation information that guarantee the correctness and termination of the software pipelining algorithm. The second problem is computing the sets of available operations. An algorithm for maintaining these sets incrementally was first presented in <ref> [EN89] </ref> for programs without loops (i.e. with acyclic control-flow graphs). In Section 5, we present a detailed description of the computation and maintenance of available operations for use in software pipelining of loops. Our presentation is simpler and easier to understand and implement than the algorithm in [EN89]. <p> first presented in <ref> [EN89] </ref> for programs without loops (i.e. with acyclic control-flow graphs). In Section 5, we present a detailed description of the computation and maintenance of available operations for use in software pipelining of loops. Our presentation is simpler and easier to understand and implement than the algorithm in [EN89]. The third significant problem is managing finite resources. While resource allocation does not bear directly on the correctness of our software pipelining algorithm, good resource usage is obviously important if the algorithm is to be useful in practice. <p> An algorithm for computing available operations was first given in <ref> [EN89] </ref> (for historical reasons, available operations were termed "unifiable-ops" in [EN89]). In this section we give a new presentation of available operations. While functionally equivalent to the algorithms of [EN89], our presentation is both simpler and more direct, and the final algorithms are easier to implement. <p> An algorithm for computing available operations was first given in <ref> [EN89] </ref> (for historical reasons, available operations were termed "unifiable-ops" in [EN89]). In this section we give a new presentation of available operations. While functionally equivalent to the algorithms of [EN89], our presentation is both simpler and more direct, and the final algorithms are easier to implement. The development is divided into two parts. <p> An algorithm for computing available operations was first given in <ref> [EN89] </ref> (for historical reasons, available operations were termed "unifiable-ops" in [EN89]). In this section we give a new presentation of available operations. While functionally equivalent to the algorithms of [EN89], our presentation is both simpler and more direct, and the final algorithms are easier to implement. The development is divided into two parts. First, we show how to compute the initial set of available operations.
Reference: [EN90] <author> K. Ebcioglu and T. Nakatani. </author> <title> A new compilation technique for parallelizing loops with unpredictable branches on a VLIW architecture. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 213-229. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: practical advantage of modulo scheduling at this time is that in cases where both techniques produce equally fast schedules, the schedules produced by modulo scheduling are generally more concise [Jon91]. 9.2 Pipeline Scheduling The work most closely related to our own is that of Ebcioglu [Ebc87] and Ebcioglu and Nakatani <ref> [NE89, EN90, NE90] </ref> and later Moon and Ebcioglu [ME92]. Pipeline scheduling differs from our approach in that the loop body is not constructed by scheduling and testing for repeating states. Instead, the original loop is incrementally transformed to create a parallel schedule.
Reference: [Fis80] <author> J. Fisher. </author> <title> 2 n -way jump microinstruction hardware and an effective instruction binding method. </title> <booktitle> In Proceedings of the 13th Annual Workshop on Microprogramming, </booktitle> <pages> pages 64-75, </pages> <month> December </month> <year> 1980. </year>
Reference-contexts: The next state to be executed is the leaf that terminates the (unique) path from the root where every branch is labeled by the value of that test in the store. There are other possible implementations of multi-way jumps; many mechanisms have been proposed and implemented <ref> [Fis80, KN85, AAG + 86, Ebc87] </ref>. The software pipelining algorithm we present applies to any of these control-flow mechanisms. We use the following abstraction of control-flow throughout this paper.
Reference: [Fis81] <author> J. A. Fisher. </author> <title> Trace Scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(7):478-90, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: The classical approach to scheduling the loop in Figure 1a is to unroll the loop body some number of times and then apply scheduling heuristics within the unrolled loop body <ref> [Fis81] </ref> as illustrated in Figure 1b. While this approach allows parallelism to be exploited between some iterations of the original loop, there is still sequentiality imposed between iterations of the unrolled loop body.
Reference: [FOW87] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Computing available operations requires the use of dependence analysis between operations. There are many variations on dependence analysis in the literature that satisfy our requirements (Constraint 4.3) and it is beyond the scope of this paper to include them here <ref> [KKP + 81, FOW87, PBJ + 91] </ref>. The algorithms in this section are presented using an abstract mechanism for dependence. By using a particular dependence analysis representation the algorithms can be made more efficient. We use the following definitions to model dependence analysis.
Reference: [GWN91] <author> G. Gao, Y. Wong, and Q. Ning. </author> <title> A timed Petri-Net model for fine grain loop scheduling. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 204-218, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Our approach and the approach Ebcioglu and Nakatani is more uniform, overlapping iterations on all paths, rather than a subset of paths. 9.4 Petri Net Techniques Recently there has been interest in using Petri Nets to formalize the software pipelining problem <ref> [GWN91, RA93] </ref>. There is a natural mapping from operations, dependences, and resource constraints into Petri Nets, thus combining all of these features in a single, well understood formalism. This approach has been shown to be competitive with modulo scheduling with hierarchical reduction [RA93] and appears promising.
Reference: [Huf93] <author> R. A. Huff. </author> <title> Lifetime-sensitive modulo scheduling. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 258-267, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Because of the large amount of work in the area, our discussion of each proposal is necessarily brief. 9.1 Modulo Scheduling Modulo scheduling is an important software pipelining technique introduced by Rau and Glaeser [RG81] and subsequently used as the basis for numerous other algorithms <ref> [Lam87, Jon91, RTS92, Huf93, WMHR93] </ref>. Modulo scheduling has been used in compilers for the FPS series [Tou84], the polycyclic machine [RG81], and Cydrome's Cydra [Cyd87]. A basic modulo scheduling algorithm works as follows. Consider a loop L that requires a resource k times per iteration of the loop body.
Reference: [Jon91] <author> R.B. Jones. </author> <title> Constrained Software Pipelining. </title> <type> Master's thesis, </type> <institution> Department of Computer Science, Utah State University, Logan, </institution> <address> UT, </address> <month> September </month> <year> 1991. </year> <month> 45 </month>
Reference-contexts: Because of the large amount of work in the area, our discussion of each proposal is necessarily brief. 9.1 Modulo Scheduling Modulo scheduling is an important software pipelining technique introduced by Rau and Glaeser [RG81] and subsequently used as the basis for numerous other algorithms <ref> [Lam87, Jon91, RTS92, Huf93, WMHR93] </ref>. Modulo scheduling has been used in compilers for the FPS series [Tou84], the polycyclic machine [RG81], and Cydrome's Cydra [Cyd87]. A basic modulo scheduling algorithm works as follows. Consider a loop L that requires a resource k times per iteration of the loop body. <p> Despite the differences in conception between the two algorithms, this result was later shown to hold for a small modification of modulo scheduling as well <ref> [Jon91] </ref>. In short, our algorithm combines software pipelining, resource constraints, and handling of control flow with a flexibility not matched by current modulo scheduling techniques. <p> In our opinion, the significant practical advantage of modulo scheduling at this time is that in cases where both techniques produce equally fast schedules, the schedules produced by modulo scheduling are generally more concise <ref> [Jon91] </ref>. 9.2 Pipeline Scheduling The work most closely related to our own is that of Ebcioglu [Ebc87] and Ebcioglu and Nakatani [NE89, EN90, NE90] and later Moon and Ebcioglu [ME92].
Reference: [KKP + 81] <author> D. J. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the 1981 SIGACT-SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Not all operations can be scheduled in the first state; for example, c must be scheduled after b, since c references a value that b writes. In standard compiler terminology, there is a data dependence from b to c <ref> [KKP + 81] </ref>. For this example, we assume a machine model in which all reads take place before any writes during execution of a state, and write conflicts are not permitted. <p> Computing available operations requires the use of dependence analysis between operations. There are many variations on dependence analysis in the literature that satisfy our requirements (Constraint 4.3) and it is beyond the scope of this paper to include them here <ref> [KKP + 81, FOW87, PBJ + 91] </ref>. The algorithms in this section are presented using an abstract mechanism for dependence. By using a particular dependence analysis representation the algorithms can be made more efficient. We use the following definitions to model dependence analysis.
Reference: [KN85] <author> K. Karplus and A. Nicolau. </author> <title> Efficient hardware for multi-way jumps and pre-fetches. </title> <booktitle> In Proceedings of the 18th Annual Workshop on Microprogramming, </booktitle> <pages> pages 11-18, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: The next state to be executed is the leaf that terminates the (unique) path from the root where every branch is labeled by the value of that test in the store. There are other possible implementations of multi-way jumps; many mechanisms have been proposed and implemented <ref> [Fis80, KN85, AAG + 86, Ebc87] </ref>. The software pipelining algorithm we present applies to any of these control-flow mechanisms. We use the following abstraction of control-flow throughout this paper.
Reference: [Kog77] <author> P. M. Kogge. </author> <title> The microprogramming of pipelined processors. </title> <booktitle> In Proceedings of the 4th Annual International Symposium on Computer Architecture, </booktitle> <year> 1977. </year>
Reference-contexts: Programmers in the microcode community software pipelined code by hand for decades <ref> [Kog77] </ref>. The first semi-automatic technique for software pipelining was proposed by Charlesworth [Cha81]. For an overview of the history of instruction level parallelism, see [RF93]. Today there are variety of algorithms and frameworks for software pipelining. We describe each and discuss its relationship to our own work.
Reference: [LA92] <author> R.M. Lee and V.H. Allan. </author> <title> Advanced Software Pipelining and the Program Dependence Graph. </title> <booktitle> In Fourth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: Another approach to improving the efficiency of the techniques presented here is to use a representation other than the control-flow graph for computing available operations. The obvious alternative is to use some form of the program dependence graph, which admits more efficient algorithms for some purposes (see <ref> [LA92, AJLS92] </ref> for uses of program dependence graphs in the context of software pipelining).
Reference: [Lam87] <author> M. Lam. </author> <title> A Systolic Array Optimizing Compiler. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1987. </year>
Reference-contexts: Others assume resource constraints are handled in a separate "fix-up phase" after software pipelining [NPA88]. Several software pipelining algorithms account for resource constraints directly as part of the software pipelining algorithm, e.g. <ref> [RG81, Lam87] </ref>. However, in most such algorithms the treatment of resource constraints is intimately connected to software pipelining|that is, the software pipelining is not separable from the handling of resource constraints. One of our interests is to separate what is really intrinsic to software pipelining from other, orthogonal concerns. <p> Because of the large amount of work in the area, our discussion of each proposal is necessarily brief. 9.1 Modulo Scheduling Modulo scheduling is an important software pipelining technique introduced by Rau and Glaeser [RG81] and subsequently used as the basis for numerous other algorithms <ref> [Lam87, Jon91, RTS92, Huf93, WMHR93] </ref>. Modulo scheduling has been used in compilers for the FPS series [Tou84], the polycyclic machine [RG81], and Cydrome's Cydra [Cyd87]. A basic modulo scheduling algorithm works as follows. Consider a loop L that requires a resource k times per iteration of the loop body. <p> The primary disadvantage of modulo scheduling is that it does not apply directly to loops with conditional tests in the loop body. Two extensions have been proposed to overcome this limitation. Lam introduced hierarchical reduction to combine modulo scheduling with complex control flow <ref> [Lam87] </ref> In hierarchical reduction, the "then" and "else" branches of a conditional test are first scheduled 41 independently. The shorter branch is padded with noops to make it the same length as the longer branch, and the scheduler encapsulates the entire if-then-else construct as a single statement.
Reference: [ME92] <author> S. M. Moon and K. Ebcioglu. </author> <title> An efficient resource-constrained global scheduling technique for superscalar and VLIW processors. </title> <booktitle> In Proceedings of the 25th International Symposium and Workshop on Microarchitecture (MICRO-25), </booktitle> <pages> pages 55-71, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: A partial solution is to move x as far as possible on the paths where it is blocked by a data dependence, thus allowing some sharing of common paths. Some scheduling systems have this property <ref> [Nic85, ME92] </ref>. However, this optimization may be of marginal value in our algorithm, because of the duplicated states of the sequential program are soon eliminated by subsequent scheduling anyway. <p> is that in cases where both techniques produce equally fast schedules, the schedules produced by modulo scheduling are generally more concise [Jon91]. 9.2 Pipeline Scheduling The work most closely related to our own is that of Ebcioglu [Ebc87] and Ebcioglu and Nakatani [NE89, EN90, NE90] and later Moon and Ebcioglu <ref> [ME92] </ref>. Pipeline scheduling differs from our approach in that the loop body is not constructed by scheduling and testing for repeating states. Instead, the original loop is incrementally transformed to create a parallel schedule.
Reference: [NE89] <author> T. Nakatani and K. Ebcioglu. </author> <title> "Combining" as a compilation technique for VLIW architectures. </title> <booktitle> In Proceedings of the 22nd Annual Workshop on Microprogramming, </booktitle> <pages> pages 43-55, </pages> <year> 1989. </year>
Reference-contexts: practical advantage of modulo scheduling at this time is that in cases where both techniques produce equally fast schedules, the schedules produced by modulo scheduling are generally more concise [Jon91]. 9.2 Pipeline Scheduling The work most closely related to our own is that of Ebcioglu [Ebc87] and Ebcioglu and Nakatani <ref> [NE89, EN90, NE90] </ref> and later Moon and Ebcioglu [ME92]. Pipeline scheduling differs from our approach in that the loop body is not constructed by scheduling and testing for repeating states. Instead, the original loop is incrementally transformed to create a parallel schedule.
Reference: [NE90] <author> T. Nakatani and K. Ebcioglu. </author> <title> Using a lookahead window in a compaction-based parallelizing compiler. </title> <booktitle> In Proceedings of the 23rd Annual Workshop on Microprogramming, </booktitle> <year> 1990. </year>
Reference-contexts: practical advantage of modulo scheduling at this time is that in cases where both techniques produce equally fast schedules, the schedules produced by modulo scheduling are generally more concise [Jon91]. 9.2 Pipeline Scheduling The work most closely related to our own is that of Ebcioglu [Ebc87] and Ebcioglu and Nakatani <ref> [NE89, EN90, NE90] </ref> and later Moon and Ebcioglu [ME92]. Pipeline scheduling differs from our approach in that the loop body is not constructed by scheduling and testing for repeating states. Instead, the original loop is incrementally transformed to create a parallel schedule. <p> The handling of control flow is based on the same principles as our own approach and is 7 This limitation is noted in [WMHR93]; no indication is given about how it can be overcome. 42 equally general. A scheduling window of operations is also used <ref> [NE90] </ref>, although the purpose is to reduce code explosion rather than to guarantee termination.
Reference: [Nic85] <author> A. Nicolau. </author> <title> Uniform parallelism exploitation in ordinary programs. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 614-618, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Second, the algorithm represents a summary of many of the most interesting aspects of our investigation of software pipelining over the last several years <ref> [Nic85, AN88a, AN88b, Aik88, Aik90, AN91] </ref>. <p> = succ-on-branch (r; c i ) in succ-on-branch (r; c i ) r i succ-on-branch (r i ; hi) n nodeps (r i ) nodeps (n) live (r i ; x) live (n; x) for all operations x return fhr i ; available (r i )ijr i as defined aboveg <ref> [Nic85] </ref>, but for completeness we describe a direct implementation that is closer to the way it should be done in practice. Let r be a frontier state of P , let x = schedule (ops (r); available (r)), and assume that x 6= none. <p> Proof: For brevity we only sketch the proof. The transformation can be implemented by a sequence of semantics-preserving Percolation Scheduling transformations between adjacent nodes <ref> [Nic85] </ref>. Since each individual Percolation Scheduling transformation preserves program semantics, the entire sequence preserves program semantics. 2 Of course, Lemma 5.3 only applies if P is delete consistent. We next show how to make an arbitrary loopless program delete consistent for (r; x). <p> A partial solution is to move x as far as possible on the paths where it is blocked by a data dependence, thus allowing some sharing of common paths. Some scheduling systems have this property <ref> [Nic85, ME92] </ref>. However, this optimization may be of marginal value in our algorithm, because of the duplicated states of the sequential program are soon eliminated by subsequent scheduling anyway.
Reference: [NPA88] <author> A. Nicolau, K. Pingali, and A. Aiken. </author> <title> Fine-grain compilation for pipelined machines. </title> <journal> Journal of Supercomputing, </journal> <volume> 1, </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: Existing software pipelining algorithms handle resource constraints in a variety of ways. Some algorithms deal with only weak forms of resource constraints|e.g., the number of operations that can be executed in parallel. Others assume resource constraints are handled in a separate "fix-up phase" after software pipelining <ref> [NPA88] </ref>. Several software pipelining algorithms account for resource constraints directly as part of the software pipelining algorithm, e.g. [RG81, Lam87]. However, in most such algorithms the treatment of resource constraints is intimately connected to software pipelining|that is, the software pipelining is not separable from the handling of resource constraints.
Reference: [PBJ + 91] <author> K. Pingali, M. Beck, R. Johnson, M. Moudgill, and P. Stodghill. </author> <title> Dependence flow graphs: An algebraic approach to program dependences. </title> <booktitle> In Proceedings of the 1991 Symposium on Principles of Programming Languages, </booktitle> <pages> pages 67-78, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Whether Constraint 4.3 is satisfied or not depends on the form of the data dependence analysis used to maintain operation availability information. Standard data dependence graphs satisfy Constraint 4.3, as do extensions to dependence graphs, such as labeling edges with constant distance vectors <ref> [PBJ + 91] </ref>. In fact, as far as we know, every proposed representation of dependence information satisfies this constraint. <p> Computing available operations requires the use of dependence analysis between operations. There are many variations on dependence analysis in the literature that satisfy our requirements (Constraint 4.3) and it is beyond the scope of this paper to include them here <ref> [KKP + 81, FOW87, PBJ + 91] </ref>. The algorithms in this section are presented using an abstract mechanism for dependence. By using a particular dependence analysis representation the algorithms can be made more efficient. We use the following definitions to model dependence analysis.
Reference: [PNW92] <author> R. Potasman, A. Nicolau, and H. G. Wang. </author> <title> Register allocation, renaming and their impact on fine-grain parallelism. </title> <booktitle> In Proceedings of the 1991 Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 218-235. </pages> <publisher> Springer Verlag Lecture Notes in Computer Science no. </publisher> <address> 589, </address> <month> April </month> <year> 1992. </year> <month> 46 </month>
Reference-contexts: This transformation is a heuristic|it assumes that the advantage gained in eliminating the dependence outweighs the cost of the extra copy. This is usually true, and almost always the copy operation can be removed by a later global pass of generalized copy propagation <ref> [PNW92] </ref>. There is an additional problem with the register allocation scheme described above. Including register allocation in the software pipelining algorithm requires that registers be taken into account when determining when two states are "the same".
Reference: [RA93] <author> M. Rajagopalan and V. H. Allan. </author> <title> Efficient scheduling of fine grain parallelism in loops. </title> <booktitle> In Proceedings of the 26th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 2-11, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Our approach and the approach Ebcioglu and Nakatani is more uniform, overlapping iterations on all paths, rather than a subset of paths. 9.4 Petri Net Techniques Recently there has been interest in using Petri Nets to formalize the software pipelining problem <ref> [GWN91, RA93] </ref>. There is a natural mapping from operations, dependences, and resource constraints into Petri Nets, thus combining all of these features in a single, well understood formalism. This approach has been shown to be competitive with modulo scheduling with hierarchical reduction [RA93] and appears promising. <p> There is a natural mapping from operations, dependences, and resource constraints into Petri Nets, thus combining all of these features in a single, well understood formalism. This approach has been shown to be competitive with modulo scheduling with hierarchical reduction <ref> [RA93] </ref> and appears promising. The weakness of current algorithms based on Petri Net techniques is that control flow is handled in a way very similar to if-conversion.
Reference: [RF93] <author> B. R. Rau and J. Fisher. </author> <title> Instruction-level parallel processing: History, overview, and perspective. </title> <journal> Journal of SuperComputing, </journal> <volume> 7(1/2), </volume> <month> January </month> <year> 1993. </year>
Reference-contexts: Programmers in the microcode community software pipelined code by hand for decades [Kog77]. The first semi-automatic technique for software pipelining was proposed by Charlesworth [Cha81]. For an overview of the history of instruction level parallelism, see <ref> [RF93] </ref>. Today there are variety of algorithms and frameworks for software pipelining. We describe each and discuss its relationship to our own work.
Reference: [RG81] <author> B. R. Rau and C. D. Glaeser. </author> <title> Some scheduling techniques and an easily schedulable horizontal architecture for high performance scientific computing. </title> <booktitle> In Proceedings of the 14th Annual Workshop on Microprogramming, </booktitle> <pages> pages 183-198, </pages> <month> October </month> <year> 1981. </year>
Reference-contexts: Others assume resource constraints are handled in a separate "fix-up phase" after software pipelining [NPA88]. Several software pipelining algorithms account for resource constraints directly as part of the software pipelining algorithm, e.g. <ref> [RG81, Lam87] </ref>. However, in most such algorithms the treatment of resource constraints is intimately connected to software pipelining|that is, the software pipelining is not separable from the handling of resource constraints. One of our interests is to separate what is really intrinsic to software pipelining from other, orthogonal concerns. <p> We describe each and discuss its relationship to our own work. Because of the large amount of work in the area, our discussion of each proposal is necessarily brief. 9.1 Modulo Scheduling Modulo scheduling is an important software pipelining technique introduced by Rau and Glaeser <ref> [RG81] </ref> and subsequently used as the basis for numerous other algorithms [Lam87, Jon91, RTS92, Huf93, WMHR93]. Modulo scheduling has been used in compilers for the FPS series [Tou84], the polycyclic machine [RG81], and Cydrome's Cydra [Cyd87]. A basic modulo scheduling algorithm works as follows. <p> is necessarily brief. 9.1 Modulo Scheduling Modulo scheduling is an important software pipelining technique introduced by Rau and Glaeser <ref> [RG81] </ref> and subsequently used as the basis for numerous other algorithms [Lam87, Jon91, RTS92, Huf93, WMHR93]. Modulo scheduling has been used in compilers for the FPS series [Tou84], the polycyclic machine [RG81], and Cydrome's Cydra [Cyd87]. A basic modulo scheduling algorithm works as follows. Consider a loop L that requires a resource k times per iteration of the loop body.
Reference: [RTS92] <author> B. R. Rau, P. P. Tirumalai, and M. S. Schlansker. </author> <title> Register allocation for software pipelined loops. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 283-299, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Because of the large amount of work in the area, our discussion of each proposal is necessarily brief. 9.1 Modulo Scheduling Modulo scheduling is an important software pipelining technique introduced by Rau and Glaeser [RG81] and subsequently used as the basis for numerous other algorithms <ref> [Lam87, Jon91, RTS92, Huf93, WMHR93] </ref>. Modulo scheduling has been used in compilers for the FPS series [Tou84], the polycyclic machine [RG81], and Cydrome's Cydra [Cyd87]. A basic modulo scheduling algorithm works as follows. Consider a loop L that requires a resource k times per iteration of the loop body.
Reference: [SDX86] <author> B. Su, S. Ding, and J. Xia. </author> <title> Urpr|an extension of urcr for software pipelining. </title> <booktitle> In Proceedings of the 19th Annual Workshop on Microprogramming, </booktitle> <pages> pages 104-108, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: Our algorithm treats resource constraints in a more direct and uniform way. 9.3 GURPR GURPR, for Global Unrolling, Pipelining, and Rerolling, is a software pipelining technique proposed by Su, Ding, and Xia [SDX87]. The technique is based on URPR, an algorithm for pipelining loops without tests <ref> [SDX86] </ref>. Given a loop L, the first step of GURPR is to apply URPR to each path through the original loop body. The separate pipelined paths are then put together to form the pipelined loop, with compensation code added at points where execution could jump from one path to another.
Reference: [SDX87] <author> B. Su, S. Ding, and J. Xia. </author> <title> GURPR|a method for global software pipelining. </title> <booktitle> In Proceedings of the 20th Annual Workshop on Microprogramming, </booktitle> <pages> pages 88-96, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: Our algorithm treats resource constraints in a more direct and uniform way. 9.3 GURPR GURPR, for Global Unrolling, Pipelining, and Rerolling, is a software pipelining technique proposed by Su, Ding, and Xia <ref> [SDX87] </ref>. The technique is based on URPR, an algorithm for pipelining loops without tests [SDX86]. Given a loop L, the first step of GURPR is to apply URPR to each path through the original loop body.
Reference: [SGE91] <author> U. Schwiegelshohn, F. Gasperoni, and K. Ebcioglu. </author> <title> On optimal parallelization of arbitrary loops. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11(2) </volume> <pages> 130-134, </pages> <year> 1991. </year>
Reference-contexts: Results in this line of development include a software pipelining algorithm that generates optimal code for loops without conditional tests [AN88a] and a proof that optimal software pipelining is impossible in general <ref> [SGE91] </ref>. However, this work has largely ignored resource constraints. Existing software pipelining algorithms handle resource constraints in a variety of ways. Some algorithms deal with only weak forms of resource constraints|e.g., the number of operations that can be executed in parallel. <p> that precede it, it is necessary to duplicate those operations onto each branch of the conditional after it has been scheduled. 39 8 On Optimal Software Pipelining In this section we briefly review research on the limitations of software pipelining, especially a result showing that optimal software pipelining is unachievable <ref> [SGE91] </ref>. Given this result, we show that our algorithm is "as good as possible" in the sense that it can produce arbitrarily good schedules. Research in software pipelining has naturally focused on discovering algorithms for computing pipelined schedules, both in general and for specific machines. <p> The conflict between d and e in this problem, we can rephrase the question again: given unbounded resources and a loop L without output dependences, is there an algorithm which computes a time optimal schedule for L? This question has been resolved negatively <ref> [SGE91] </ref>. Again the problem is that for some loops an optimal closed-form parallel version does not exist. While Definition 8.1 is natural, it appears that so many qualifications are required to apply it in the analysis of general software pipelining algorithms that it ceases to be useful.
Reference: [Tou84] <author> R. F. Touzeau. </author> <title> A Fortran compiler for the FPS-164 scientific computer. </title> <booktitle> In Proceedings of the 1984 ACM SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pages 48-57, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Modulo scheduling has been used in compilers for the FPS series <ref> [Tou84] </ref>, the polycyclic machine [RG81], and Cydrome's Cydra [Cyd87]. A basic modulo scheduling algorithm works as follows. Consider a loop L that requires a resource k times per iteration of the loop body.
Reference: [WHB92] <author> N. J. Warter, G. E. Haab, and J. W. Bockhaus. </author> <title> Enhanced Modulo Scheduling for Loops with Conditional Branches. </title> <booktitle> In Proceedings of the 25th International Symposium and Workshop on Microarchitecture (MICRO-25), </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: A second proposal for integrating modulo scheduling with conditional tests is to use if-conversion [AKPW83] before modulo scheduling and reverse if-conversion <ref> [WHB92, WMHR93] </ref> after modulo scheduling. When a loop is if-converted, the expression of control flow is changed from explicit jumps to guarded operations, where each operation of the original loop is guarded by the predicates of the conditionals that control its execution.
Reference: [WMHR93] <author> N. J. Warter, S. A. Mahlke, W. W. Hwu, and B. R. Rau. </author> <title> Reverse if-conversion. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 290-299, </pages> <month> June </month> <year> 1993. </year> <month> 47 </month>
Reference-contexts: Because of the large amount of work in the area, our discussion of each proposal is necessarily brief. 9.1 Modulo Scheduling Modulo scheduling is an important software pipelining technique introduced by Rau and Glaeser [RG81] and subsequently used as the basis for numerous other algorithms <ref> [Lam87, Jon91, RTS92, Huf93, WMHR93] </ref>. Modulo scheduling has been used in compilers for the FPS series [Tou84], the polycyclic machine [RG81], and Cydrome's Cydra [Cyd87]. A basic modulo scheduling algorithm works as follows. Consider a loop L that requires a resource k times per iteration of the loop body. <p> A second proposal for integrating modulo scheduling with conditional tests is to use if-conversion [AKPW83] before modulo scheduling and reverse if-conversion <ref> [WHB92, WMHR93] </ref> after modulo scheduling. When a loop is if-converted, the expression of control flow is changed from explicit jumps to guarded operations, where each operation of the original loop is guarded by the predicates of the conditionals that control its execution. <p> In this way, all non-trivial control flow in the loop is replaced by data dependences. Modulo scheduling with if-conversion appears to improve upon modulo scheduling with hierarchical reduction <ref> [WMHR93] </ref>. However, if-conversion retains the undesirable features of hierarchical reduction to a considerable degree. First, because control flow is expressed as data dependence, speculative execution of operations (i.e., moving operations above conditionals) is not possible, nor is it possible to reorder conditionals for the same reason. <p> Software pipelining is achieved by moving operations across the backedge of the loop; this has the effect of moving an operation between loop iterations. The handling of control flow is based on the same principles as our own approach and is 7 This limitation is noted in <ref> [WMHR93] </ref>; no indication is given about how it can be overcome. 42 equally general. A scheduling window of operations is also used [NE90], although the purpose is to reduce code explosion rather than to guarantee termination.
References-found: 42

