URL: ftp://prospero.isi.edu/pub/papers/prm/prm-cpe.ps.Z
Refering-URL: http://nii.isi.edu/gost-group/products/prm/papers.html
Root-URL: http://www.isi.edu
Title: The Prospero Resource Manager: A Scalable Framework for Processor Allocation in Distributed Systems  
Author: B. Clifford Neuman Santosh Rao 
Affiliation: Information Sciences Institute University of Southern California  
Date: 6(4),339-355 June 1994.  
Note: Appears in Concurrency: Practice and Experience, Vol  
Abstract: Existing techniques for allocating processors in parallel and distributed systems are not suitable for use in large distributed systems. In such systems, dedicated multiprocessors should exist as an integral component of the distributed system, and idle processors should be available to applications that need them. The Prospero Resource Manager (PRM) is a scalable resource allocation system that supports the allocation of processing resources in large networks and on multiprocessor systems. PRM employs three types of managers, the job manager, the system manager, and the node manager, to manage resources in a distributed system. Multiple independent instances of each type of manager exist, reducing bottlenecks. When making scheduling decisions each manager utilizes information most closely associated with the entities for which it is responsible. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal and A. K. Ezzat. </author> <title> Location independent remote execution in NEST. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(8):905-912, </volume> <month> August </month> <year> 1987. </year>
Reference-contexts: The execution of the program (5) depends on run-time communication libraries (also at 4) which in turn use information about the mapping of tasks to nodes (3). Locus [16], NEST <ref> [1] </ref>, Sprite [5], and V [20] support processor allocation, and remote program loading and execution (2,4,5) to harness the computing power of lightly loaded nodes. They primarily support sequential applications where task-to-task communication is not required. <p> In NEST <ref> [1] </ref>, idle machines advertise their availability, providing a dynamically changing set of available nodes; each user's workstation maintains the list of servers available for remote execution.
Reference: [2] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: As such, the job manager is better able than the system manager to allocate 5 Appears in Concurrency: Practice and Experience, Vol 6 (4),339-355 June 1994. resources to the individual tasks within a job. This is the same argument used in favor of user-level thread management on shared-memory multiprocessors <ref> [2] </ref>. In fact, we allow the job manager to be written by the application programmer if specific functionality is required, though we do not expect this to be a common practice. We plan to eventually provide alternative job managers to support fault-tolerant and real-time applications.
Reference: [3] <author> Steven Augart, B. Clifford Neuman, and Santosh Rao. </author> <title> The Prospero Data Access Protocol. </title> <note> In preparation. </note>
Reference-contexts: Users can customize this task for job initialization functions, such as prompting the user for interactive input and distributing this input to the appropriate tasks. The I/O functions included in the communications library use the Prospero Data Access Protocol (PDAP) <ref> [3] </ref> for data transfer. 4.2 Communication libraries To support parallel applications, several communications libraries are available with the PRM package. One library provides routines for sending, receiving, and broadcasting tagged messages. Its application programming interface (API) is similar to that provided on multicomputers such as the Touchstone Delta [9]. <p> The file I/O task plays an important role, supporting read and write operations to files on computers that do not export their file system. To increase the efficiency of file acccesses, we are extending the Prospero Data Access Protocol <ref> [3] </ref> to support file caching. Finally, with the ability to run applications on multiprocessor systems across wide-area networks, security will become a critical problem. It is unlikely that sites would make their resources available to others if there are no methods for protection.
Reference: [4] <author> David L. Black, David B. Golub, Daniel P. Julin, Richard F. Rashid, Richard P. Draves, Randall W. Dean, Alessandro Forin, Joseph Barrera, Hideyuki Tokuda, Gerald Malan, and David Bohman. </author> <title> Micro-kernel operating system architecture and Mach. </title> <booktitle> In Proceedings of the USENIX Workshop on Microker-nels and Other Kernel Architectures, </booktitle> <pages> pages 11-30, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: In the present implementation, the libraries use an Asynchronous Reliable Delivery Protocol (ARDP) to transmit and receive sequenced packets over the Internet using UDP. We are also implementing a version of the communication library layered on top of the Mach port mechanism <ref> [4] </ref>. This will enable the three types of managers to communicate via Mach IPC, which will in turn pave the way for server-based implementation of PRM on multicomputers running 8 Appears in Concurrency: Practice and Experience, Vol 6 (4),339-355 June 1994. Mach.
Reference: [5] <author> F. Douglis and J. Ousterhout. </author> <title> Transparent process migration for personal workstations. </title> <type> Technical Report UCB/CSD 89/540, </type> <institution> Computer Science Division, University of California, </institution> <address> Berkeley CA 94720, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: The execution of the program (5) depends on run-time communication libraries (also at 4) which in turn use information about the mapping of tasks to nodes (3). Locus [16], NEST [1], Sprite <ref> [5] </ref>, and V [20] support processor allocation, and remote program loading and execution (2,4,5) to harness the computing power of lightly loaded nodes. They primarily support sequential applications where task-to-task communication is not required. <p> Additionally, resource allocation decisions in these systems are made locally by the application without the benefit of a high-level view across jobs. This causes problems when applications run simultaneously. Sprite <ref> [5] </ref> uses a shared file as a centralized database to track available nodes. Clients select idle nodes from this file, marking the entry to flag its use. While this approach appears simple, it requires a solution to problems related to shared writable files, including locking, synchronization and consistency.
Reference: [6] <author> R. E. Felderman, E. M. Schooler, and L. Kleinrock. </author> <title> The Benevolent Bandit Laboratory: A testbed for distributed algorithms. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 7(2) </volume> <pages> 303-311, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: In the distributed approach a client multicasts a query to a group of candidate machines selecting the responder with the lowest load. This approach suffers from excessive network traffic and was found to perform worse than the central server approach. The UCLA Benevolent Bandit Laboratory (BBL) <ref> [6] </ref> provides an environment for running parallel applications on a network of personal computers. Like the other systems discussed, BBL provides processor allocation, and remote program loading and execution (2,3,4,5), incorporating the notion of a user-process manager separate from a systemwide resource manager.
Reference: [7] <author> A. Giest, A. Beguelin, J. J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM 3 user's guide and reference manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: We have ported a CM-5 version of Ocean to PRM and are using it as a test application to tune our communication libraries. A third library exports an interface identical to that of PVM version 3.2.x <ref> [7] </ref>. PVM's routines for message passing, buffer manipulation, process control and data packing and unpacking are available, enabling most PVM applications to run in the PRM environment without modification, giving them the added benefit of PRM's automatic resource allocation mechanisms. <p> The first experiment was intended to determine the communication latencies observed by application programs and compare the performance of our implementation of the PVM library over ARDP with that of PVM version 3.2.6 available from Oak Ridge National Laboratory <ref> [7] </ref>. The second experiment was designed to measure the the resource allocation performance of PRM. These experiments were conducted on a set of SPARC-10s connected by an ethernet.
Reference: [8] <author> T. P. Green and J. Synder. DQS, </author> <title> a distributed queueing system. In Workshop on Heterogeneous Network-Based Concurrent Computing. </title> <institution> Supercomputer Computations Research Inst., Florida State Univ., </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: While this is an important step towards scalable resource management techniques, a single resource manager will be unable to handle all allocation requests for a large system. DQS <ref> [8] </ref> and Lsbatch [22] are load sharing systems which emphasize processor allocation (2) and are used mainly for distributing batch jobs across machines. Parallel jobs are supported but facilities for intertask communication (4) are minimal.
Reference: [9] <institution> Intel Supercomputer Systems Division, Beaverton, </institution> <month> OR. </month> <title> Touchstone Delta C System Calls Reference Manual, </title> <month> April </month> <year> 1991. </year>
Reference-contexts: One library provides routines for sending, receiving, and broadcasting tagged messages. Its application programming interface (API) is similar to that provided on multicomputers such as the Touchstone Delta <ref> [9] </ref>. Another API provides most of the commonly used routines available to Connection Machine (CM-5) programmers. This interface implements CMMD library routines [21] through a set of macros and functions that make equivalent calls to our own library.
Reference: [10] <author> M. Litzkow and M. Solomon. </author> <title> Supporting checkpointing and process migration outside the Unix kernel. </title> <booktitle> In Usenix Conference Proceedings, </booktitle> <pages> pages 283-290, </pages> <month> Winter </month> <year> 1992. </year>
Reference-contexts: Extending this a step further, task migration mechanisms enable preempted tasks to acquire new resources and continue execution on a different node. We plan to implement task migration using the checkpointing mechanisms provided by Condor <ref> [10] </ref>. Checkpointing will also be useful for playback debugging described in [17]. The Prospero Resource Manager provides a basic framework for acquiring and managing resources in distributed environments.
Reference: [11] <author> B. Clifford Neuman. </author> <title> The Prospero File System: A global file system based on the Virtual System Model. </title> <journal> Computing Systems, </journal> <volume> 5(4), </volume> <month> Fall </month> <year> 1992. </year> <note> 14 Appears in Concurrency: Practice and Experience, Vol 6(4),339-355 June 1994. </note>
Reference-contexts: At the time a job is initiated, the job manager identifies the job's resource requirements. Using the Prospero Directory Service <ref> [11] </ref>, if available, or a configuration file otherwise, it locates system managers with jurisdiction over suitable resources and sends allocation requests. <p> This information is stored either in a configuration file or as attributes of the program in the Prospero Directory Service <ref> [11] </ref>. When a program is invoked, a new job manager is created and the job manager finds a suitable processor or set of processors by contacting system managers identified by the virtual system associated with the program. programs as if they were local to a workstation. <p> Node managers continually monitor node status and notify their system manager of any changes in node availability. The PRM package may be used standalone or configured to use the Prospero Directory Service (PDS) <ref> [11] </ref>. If configured standalone, the user specifies job configuration information in a job-description file.
Reference: [12] <author> B. Clifford Neuman. </author> <title> The Virtual System Model: A Scalable Approach to Organizing Large Systems. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <month> June </month> <year> 1992. </year> <note> Available as Department of Computer Science and Engineering Technical Report 92-06-04. </note>
Reference-contexts: The organization of such systems should be based on the conceptual relationship between resources and the mapping to physical locations should be hidden from the user. These concepts form the basis of the Virtual System Model, a new model for organizing large distributed systems <ref> [12] </ref>. 4 Appears in Concurrency: Practice and Experience, Vol 6 (4),339-355 June 1994.
Reference: [13] <author> B. Clifford Neuman. </author> <title> Scale in distributed systems. </title> <booktitle> In Readings in Distributed Computing Systems. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: Tasks execute the program, communicate with each other and perform terminal and file I/O. adversely affect performance. As a distributed system scales geographically and administratively, additional problems arise <ref> [13] </ref>. PRM addresses these problems by using multiple resource managers, each controlling a subset of the resources in the system, independent of other managers of the same type. The functions of resource management are distributed across three types of managers: system managers, job managers, and node managers.
Reference: [14] <author> B. Clifford Neuman and Santosh Rao. </author> <title> Resource management for distributed parallel systems. </title> <booktitle> In Proc. 2nd International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 316-323, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Elliot Yan implemented the improved timer and the IP delay facility in the SunOS kernel. Celeste Anderson, Steven Augart, Gennady Medvinsky, Rafael Saavedra, and Stuart Stubblebine commented on drafts of this paper. This paper is a revised version of a paper <ref> [14] </ref> that originally appeared in the 2nd IEEE Symposium on High Performance Distributed Computing, Spokane WA, July 1993.
Reference: [15] <institution> ParaSoft Corporation, Pasadena, </institution> <address> CA 91107. NetExpress 3.2 Introductory Guide, </address> <year> 1992. </year>
Reference-contexts: Parallel jobs are supported but facilities for intertask communication (4) are minimal. Both packages are based on a central server that maintains load information and dispatches queued jobs. As with the V approach, their scalability is limited. Parallel Virtual Machine (PVM) [19] and Net-Express <ref> [15] </ref> provide environments for parallel computing on a network of workstations. In the initial configuration phase, users specify a list of nodes on which they have started daemon processes.
Reference: [16] <author> G. Popek and B. Walker, </author> <title> editors. The Locus Distributed System Architecture. </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1985. </year>
Reference-contexts: The execution of the program (5) depends on run-time communication libraries (also at 4) which in turn use information about the mapping of tasks to nodes (3). Locus <ref> [16] </ref>, NEST [1], Sprite [5], and V [20] support processor allocation, and remote program loading and execution (2,4,5) to harness the computing power of lightly loaded nodes. They primarily support sequential applications where task-to-task communication is not required.
Reference: [17] <author> Santosh S. Rao and B. Clifford Neuman. </author> <title> A replay based debugger for distributed parallel environments. </title> <note> Submitted for publication. </note>
Reference-contexts: One task monitor exists for each task and is co-located with the task. Figure 4 shows the interaction of the various entities involved in debugging a PRM application. Checkpoints, consistency checks, and replay in the context of PRM are discussed in greater detail in <ref> [17] </ref>. 9 Appears in Concurrency: Practice and Experience, Vol 6 (4),339-355 June 1994. pvm_advise (PvmRouteDirect); if (tids [0] == my_tid) - /* First task: Send first */ pvm_send (tids [other], INIT_TAG); /* Initial data exchange to */ pvm_recv (tids [other], INIT_TAG + 1); /* establish direct */ /* connections */ gettimeofday <p> Extending this a step further, task migration mechanisms enable preempted tasks to acquire new resources and continue execution on a different node. We plan to implement task migration using the checkpointing mechanisms provided by Condor [10]. Checkpointing will also be useful for playback debugging described in <ref> [17] </ref>. The Prospero Resource Manager provides a basic framework for acquiring and managing resources in distributed environments. We plan to extend this framework to provide an integrated environment consisting of a set of tools that facilitate the development and execution of parallel and distributed applications.
Reference: [18] <author> J. P. Singh and J. L. Hennessey. </author> <title> Finding and exploiting parallelism in an ocean simulation program: Experience, results and implications. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(1), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: By linking with this library, programs written for the CM-5 run virtually unmodified in PRM's environment. Thus, PRM can serve as an application testbed for CM-5 programs. The Ocean program from the SPLASH benchmark suite from Stanford University <ref> [18] </ref> studies the role of eddies and boundary currents in influencing large-scale ocean movements by solving a set of partial-differential equations. We have ported a CM-5 version of Ocean to PRM and are using it as a test application to tune our communication libraries.
Reference: [19] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Parallel jobs are supported but facilities for intertask communication (4) are minimal. Both packages are based on a central server that maintains load information and dispatches queued jobs. As with the V approach, their scalability is limited. Parallel Virtual Machine (PVM) <ref> [19] </ref> and Net-Express [15] provide environments for parallel computing on a network of workstations. In the initial configuration phase, users specify a list of nodes on which they have started daemon processes.
Reference: [20] <author> M. A. Theimer and K. A. Lantz. </author> <title> Finding idle machines in a workstation-based distributed system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(11) </volume> <pages> 1444-1458, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The execution of the program (5) depends on run-time communication libraries (also at 4) which in turn use information about the mapping of tasks to nodes (3). Locus [16], NEST [1], Sprite [5], and V <ref> [20] </ref> support processor allocation, and remote program loading and execution (2,4,5) to harness the computing power of lightly loaded nodes. They primarily support sequential applications where task-to-task communication is not required. A critical issue for processor allocation in these systems is the maintenance of the database of available nodes. <p> Fault-tolerance is also poor, since failure of the file server on which the shared file resides disables the allocation mechanism completely. This approach does not scale beyond a few tens of nodes. Theimer and Lantz experimented with two approaches for processor allocation in V <ref> [20] </ref>. In a centralized approach a central server selects the least loaded node from a pool of free nodes and allocates it. Nodes proclaim their availability based on the relationship of the local load to a cutoff broadcast periodically by the server.
Reference: [21] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CMMD Reference Manual, </note> <month> October </month> <year> 1991. </year>
Reference-contexts: Its application programming interface (API) is similar to that provided on multicomputers such as the Touchstone Delta [9]. Another API provides most of the commonly used routines available to Connection Machine (CM-5) programmers. This interface implements CMMD library routines <ref> [21] </ref> through a set of macros and functions that make equivalent calls to our own library. The terminal I/O task takes on the functions of host program on the CM-5. By linking with this library, programs written for the CM-5 run virtually unmodified in PRM's environment.

References-found: 21

