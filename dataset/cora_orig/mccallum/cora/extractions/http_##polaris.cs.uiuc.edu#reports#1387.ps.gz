URL: http://polaris.cs.uiuc.edu/reports/1387.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Optimizing Instruction Cache Performance for Operating System Intensive Workloads 1  
Author: Josep Torrellas, Chun Xia, and Russell Daigle 
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development and Computer Science Department University of Illinois at Urbana-Champaign,  
Abstract: High instruction cache hit rates are key to high performance. One known technique to improve the hit rate of caches is to use an optimizing compiler to minimize cache interference via an improved layout of the code. This technique, however, has been applied to application code only, even though there is evidence that the operating system often uses the cache heavily and with less uniform patterns than applications. Therefore, it is unknown how well existing optimizations perform for systems code and whether better optimizations can be found. We address this problem in this paper. This paper characterizes in detail the locality patterns of the operating system code and shows that there is substantial locality. Unfortunately, caches are not able to extract much of it: rarely-executed special-case code disrupts spatial locality, loops with few iterations that call routines make loop locality hard to exploit, and plenty of loop-less code hampers temporal locality. As a result, interference within popular execution paths dominates instruction cache misses. Based on our observations, we propose an algorithm to expose these localities and reduce interference. For a range of cache sizes, associativities, lines sizes, and other organizations we show that we reduce total instruction miss rates by 31-86% (up to 2.9 absolute points). Using a simple model this corresponds to execution time reductions in the order of 10-25%. In addition, our optimized operating system combines well with optimized or unoptimized applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, J. Hennessy, and M. Horowitz. </author> <title> Cache Performance of Operating System and Multiprogramming Workloads. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(4) </volume> <pages> 393-431, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Indeed, there is some evidence that backs this claim. Clark [10] reported a lower performance of the VAX-11/780 cache when operating system activity was taken into account. Similarly, Agarwal et al <ref> [1] </ref> pointed out the many cache misses caused by the operating system. Torrellas et al [17] reported that the operating system code causes a large fraction of the cache misses and, in addition, suffers considerable self-interference in the cache.
Reference: [2] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: Consequently, exposing spatial locality can potentially remove many misses. 3.2.2 Loop Locality The second type of locality that we examine is the locality provided by loops. To identify the loops, we use dataflow analysis <ref> [2] </ref>. For our analysis, we divide the loops into those that do not call procedures and those that do. We now consider each category in turn. Loops Without Procedure Calls Our measurements show that these loops do not dominate the execution time of the operating system. <p> We use dataflow analysis as discussed by Aho et al <ref> [2] </ref>. Then, we select the loops with at least a minimum number of iterations per invocation (currently set to 6). We pull the basic blocks of these loops out of the sequences and put them, in the same order, in a contiguous area at the end of the sequences.
Reference: [3] <author> T. Anderson, H. Levy, B. Bershad, and E. Lazowska. </author> <title> The Interaction of Architecture and Operating System Design. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: NAG 1 613; and grant 1-1-28028 from the Univ. of Illinois Research Board. 2 Currently with Tandem Computers Inc., Cupertino, California. 1 in the cache. Other researchers like Ousterhout [16] and Anderson et al <ref> [3] </ref> also indicate the different nature of the operating system activity. Finally, Nagle et al [15] point out that instruction cache performance is becoming increasingly important in new-generation operating systems.
Reference: [4] <editor> J. B. </editor> <address> Andrews. </address>
Reference-contexts: We use a hardware performance monitor that gathers uninterrupted reference traces of application and operating system in real time without introducing perturbation. The performance monitor <ref> [4] </ref> has one probe connected to each of the four processors. The probes collect all instruction and data references 2 issued by the processors except those that hit in the per-processor 16-Kbyte first level instruction cache. Each probe has a trace buffer that stores over one million references.
References-found: 4

