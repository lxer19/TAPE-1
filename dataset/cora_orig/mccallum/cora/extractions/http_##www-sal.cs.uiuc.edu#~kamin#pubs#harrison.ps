URL: http://www-sal.cs.uiuc.edu/~kamin/pubs/harrison.ps
Refering-URL: http://www-sal.cs.uiuc.edu/~kamin/pubs/index.html
Root-URL: http://www.cs.uiuc.edu
Email: fharrison,kaming@cs.uiuc.edu  
Title: Compilation as Metacomputation: Binding Time Separation in Modular Compilers (Extended Abstract)  
Author: William L. Harrison Samuel N. Kamin 
Keyword: Compilers, Partial Evaluation, Semantics-Based Compilation, Programming Language Semantics, Monads, Monad Transformers, Pass Separation.  
Address: Urbana, Illinois 61801-2987  
Affiliation: Department of Computer Science University of Illinois, Urbana-Champaign  
Abstract: This paper presents a modular and extensible style of language specification based on meta-computations. This style uses two monads to factor the static and dynamic parts of the specification, thereby staging the specification and achieving strong binding-time separation. Because metacomputations are defined in terms of monads, they can be constructed modularly and ex-tensibly using monad transformers. A number of language constructs are specified: expressions, control-flow, imperative features, block structure, and higher-order functions and recursive bindings. Metacomputation-style specification lends itself to semantics-directed compilation, which we demonstrate by creating a modular compiler for a higher-order, imperative, Algol-like language. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. </author> <title> Ullman Compilers: Principles, Techniques, and Tools, </title> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Block structure [ Booleans Equations = Eq Expressions [ Eq Imperative [ Eq Control-flow [ Eq Block structure [ Eq Booleans Source Code: new x in x := 5; y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := <ref> [1] </ref>; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := [2] * [3]; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; For the control flow language: Comp = T St Label Id; and Exec = T CPS void
Reference: [2] <author> A. Appel, </author> <title> Modern Compiler Implementation in ML, </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1998. </year>
Reference-contexts: Consider the three definitions of the conditional if-then statement in Figure 6. The first is a dual continuation "control-flow" semantics, found commonly in compilers <ref> [2] </ref>. If B is true, then the first continuation, [[c]] ? E , is executed, otherwise c is skipped and just is executed. <p> Control-flow [ Eq Block structure [ Eq Booleans Source Code: new x in x := 5; y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := [1]; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := <ref> [2] </ref> * [3]; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; For the control flow language: Comp = T St Label Id; and Exec = T CPS void (T St Code (T St Sto Id)) To combine the compiler building blocks <p> x in x := 5; y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := [1]; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := <ref> [2] </ref> * [3]; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; For the control flow language: Comp = T St Label Id; and Exec = T CPS void (T St Code (T St Sto Id)) To combine the compiler building blocks for these languages, one simply combines the respective monad transformers: Comp <p> 5; y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := [1]; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := <ref> [2] </ref> * [3]; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; For the control flow language: Comp = T St Label Id; and Exec = T CPS void (T St Code (T St Sto Id)) To combine the compiler building blocks for these languages, one simply combines the respective monad transformers: Comp = T Env Env
Reference: [3] <author> A. Appel, </author> <title> Compiling with Continuations, </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Eq Block structure [ Eq Booleans Source Code: new x in x := 5; y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := [1]; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := [2] * <ref> [3] </ref>; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; For the control flow language: Comp = T St Label Id; and Exec = T CPS void (T St Code (T St Sto Id)) To combine the compiler building blocks for these <p> x := 5; y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := [1]; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := [2] * <ref> [3] </ref>; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; For the control flow language: Comp = T St Label Id; and Exec = T CPS void (T St Code (T St Sto Id)) To combine the compiler building blocks for these languages, one simply combines the respective monad transformers: Comp = T <p> y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := [1]; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := [2] * <ref> [3] </ref>; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; For the control flow language: Comp = T St Label Id; and Exec = T CPS void (T St Code (T St Sto Id)) To combine the compiler building blocks for these languages, one simply combines the respective monad transformers: Comp = T Env Env (T
Reference: [4] <author> O. Danvy, </author> <title> "Type-Directed Partial Evaluation," </title> <booktitle> Proceedings of the ACM Conference on the Principles of Programming Languages, </booktitle> <year> 1996. </year>
Reference: [5] <author> O. Danvy and R. Vestergaard, </author> <title> "Semantics-Based Compiling: A Case Study in Type-Directed Partial Evaluation," </title> <booktitle> Eighth International Symposium on Programming Language Implementation and Logic Programming, </booktitle> <year> 1996, </year> <pages> pages 182-497. </pages>
Reference-contexts: While our binding time analysis is not automatic as in [19, 15], we consider a far wider range of programming language features than they do. Danvy and Vestergaard <ref> [5] </ref> show how to produce code that "looks like" machine language, by expressing the source language semantics in terms of machine language-like combinators (e.g., "popblock", "push").
Reference: [6] <author> R. Davies and F. Pfenning, </author> <title> "A Modal Analysis of Staged Computation," </title> <booktitle> Proceedings of the ACM Conference on the Principles of Programming Languages, </booktitle> <year> 1996. </year>
Reference: [7] <author> D. Espinosa, </author> <title> "Semantic Lego," </title> <type> Doctoral Dissertation, </type> <institution> Columbia University, </institution> <year> 1995. </year>
Reference-contexts: The reader will recognize this distinction as the classic separation of static from dynamic. Thus, staging is an instance of metacomputation. The main contribution of this paper is a modular and extensible method of staging denotational specifications based on metacomputations, formalized via monads <ref> [7, 13, 16, 23] </ref>. A style of language specification based on metacomputation is proposed in which the static and dynamic parts of a language specification are factored into distinct monads. <p> We believe this style of language specification may have many uses, but in this paper we concentrate on one: modular compilation. Modular compilers are compilers built from building blocks that represent language features rather than compilation phases, as illustrated in Figure 2. Espinosa <ref> [7] </ref> and Liang & Hudak [13] showed how to construct modular interpreters using the notion of monads [7, 13, 16, 23] | or, more precisely, monad transformers. The current authors built on those ideas to produce modular compilers in [8]. <p> Modular compilers are compilers built from building blocks that represent language features rather than compilation phases, as illustrated in Figure 2. Espinosa [7] and Liang & Hudak [13] showed how to construct modular interpreters using the notion of monads <ref> [7, 13, 16, 23] </ref> | or, more precisely, monad transformers. The current authors built on those ideas to produce modular compilers in [8]. However, there the notion of staging, though conceptually at the heart of the approach, was not explicit in the compiler building blocks we constructed. <p> The result was awkwardness in communicating between these aspects of the domain, which meant that "gluing together" compiler blocks was sometimes delicate. Indeed, metacomputation is purposely avoided in <ref> [7, 13, 8] </ref>. A key aspect of that work is that monad transformers are used to create the single monad used to interpret or compile the language. The problem that inspired it was that monads don't compose nicely. <p> Finally, Section 6 summarizes this work and outlines future research. 2 Related work Espinosa <ref> [7] </ref> and Hudak, Liang, and Jones [13] use monad transformers to create modular, extensible interpreters. Liang [12, 14] addresses the question of whether compilers can be developed similarly, but since he does not compile to machine language, many of the issues we confront|especially staging|do not arise. <p> the full paper) improves on Reynolds's in two ways: it is monad-structured|that is, built from interchangeable parts|and it includes jumps and labels where Reynolds simply allowed code duplication and infinite programs. 3 Monads and Monad Transformers In this section, we review the theory of monads [16, 23] and monad transformers <ref> [7, 13] </ref>. Readers familiar with these topics may skip the section. <p> Unfortunately, M St (M St t ) and M 2St t are very different types. This points to a difficulty with monads: they do not compose in this simple manner. The key contribution of the work <ref> [7, 13] </ref> on monad transformers is to solve this composition problem. When applied to a monad M, a monad transformer T creates a new monad M 0 . <p> The state monad transformer also provides updateSto and getSto operations appropriate to the newly-created monad. When composing T St Sto with itself, as above, the operations on the "inner" state need to be lifted through the outer state monad; this is the main technical issue in <ref> [7, 13] </ref>. In our work in [8], we found it convenient to factor the state monad into two parts: the state proper and the address allocator. <p> In section 5, they will be combined to create a compiler. For the first two of these blocks, we also give monolithic versions, drawn from [8], to illustrate why metacomputation is helpful. 4.1 Integer Expressions Compiler Building Block Consider the standard monadic-style specification of negation <ref> [7, 13, 23] </ref> displayed in Figure 4. <p> Observe that this implementation-oriented definition calculates the same value as the standard definition, but it stores the intermediate value i as well. But where do addresses and storage come from? In [8], we added them to the Exec monad using monad transformers <ref> [7, 13] </ref> as in the "Implementation-oriented" specification in Figure 4. In that definition, rdAddr reads the current top of stack address a, inAddr increments the top of stack, and Thread stores i at a. The monad (Exec) is used to construct the domain containing both static and dynamic data.
Reference: [8] <author> W. Harrison and S. Kamin, </author> <title> "Modular Compilers Based on Monad Transformers," </title> <booktitle> Proceedings of the IEEE International Conference on Programming Languages, </booktitle> <year> 1998, </year> <pages> pages 122-131. </pages>
Reference-contexts: Espinosa [7] and Liang & Hudak [13] showed how to construct modular interpreters using the notion of monads [7, 13, 16, 23] | or, more precisely, monad transformers. The current authors built on those ideas to produce modular compilers in <ref> [8] </ref>. However, there the notion of staging, though conceptually at the heart of the approach, was not explicit in the compiler building blocks we constructed. <p> The result was awkwardness in communicating between these aspects of the domain, which meant that "gluing together" compiler blocks was sometimes delicate. Indeed, metacomputation is purposely avoided in <ref> [7, 13, 8] </ref>. A key aspect of that work is that monad transformers are used to create the single monad used to interpret or compile the language. The problem that inspired it was that monads don't compose nicely. <p> We are simply saying that having two monads | what might be called the static and dynamic monads | and composing them seems to give the "right" domain for modular compilation. This paper also includes two substantive improvements over <ref> [8] </ref> that, though less essential, also help to make the approach more practical. First, instead of writing all specifications in continuation-passing style, here we write in direct style, invoking the CPS monad transformer only when needed; this naturally simplifies many of the equations. <p> First, instead of writing all specifications in continuation-passing style, here we write in direct style, invoking the CPS monad transformer only when needed; this naturally simplifies many of the equations. Second, we include some building blocks not given in <ref> [8] </ref>, specifically optimized expressions and procedures. (These will be included in the full paper.) Together these give us the entire language of Reynolds [21] with essentially the same code generated there (but with jumps and labels, avoiding the potential for infinite programs). <p> When composing T St Sto with itself, as above, the operations on the "inner" state need to be lifted through the outer state monad; this is the main technical issue in [7, 13]. In our work in <ref> [8] </ref>, we found it convenient to factor the state monad into two parts: the state proper and the address allocator. This was really a "staging transformation," with the state monad representing dynamic computation and the address allocator static computation, but, as mentioned earlier, it led to significant complications. <p> In section 5, they will be combined to create a compiler. For the first two of these blocks, we also give monolithic versions, drawn from <ref> [8] </ref>, to illustrate why metacomputation is helpful. 4.1 Integer Expressions Compiler Building Block Consider the standard monadic-style specification of negation [7, 13, 23] displayed in Figure 4. <p> Observe that this implementation-oriented definition calculates the same value as the standard definition, but it stores the intermediate value i as well. But where do addresses and storage come from? In <ref> [8] </ref>, we added them to the Exec monad using monad transformers [7, 13] as in the "Implementation-oriented" specification in Figure 4. In that definition, rdAddr reads the current top of stack address a, inAddr increments the top of stack, and Thread stores i at a. <p> A jump to label L simply invokes the continuation stored at L. The second definition in Figure 6 presents an implementation-oriented specification of if-then in monolithic style (that is, where Code and Label are both added to Exec). Again, this represents our approach in <ref> [8] </ref>. One very subtle problem remains: what is "newSegment"? One's first impulse is to define it as a simple update to the Code store (i.e., updateCode [L 7! ]), but here is where the monolithic approach greatly complicates matters. <p> Observe that Exec does not include the Label store, and so the continuation now includes only dynamic computations. Therefore, there is no need to pass in the label count to , and so, may simply be stored in Code. 1 A full description of newSegment is found in <ref> [8] </ref>. 11 Exec = Id Comp = T Env Env (T Env Addr Id) set a = v:updateSto (a 7! v) get a = getSto ? E :unitE ( a) C [[new x in c]] : Comp (Exec void) = rdAddr ? C a: inAddr (a + 1) rdEnv ? C
Reference: [9] <author> N. D. Jones, C. K. Gomard, and P. Sestoft, </author> <title> Partial Evaluation and Automatic Program Generation, </title> <publisher> Prentice-Hall 1993. </publisher>
Reference: [10] <author> U. Jorring and W. Scherlis, </author> <title> "Compilers and Staging Transformations," </title> <booktitle> Proceedings of the ACM Conference on the Principles of Programming Languages, </booktitle> <year> 1986. </year>
Reference: [11] <author> P. Lee, </author> <title> Realistic Compiler Generation, </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: The beauty of the monadic form is that the meaning of [[]] can be reinterpreted in a variety of monads. Monadic semantics separate the description of a language from its denotation. In this sense, it is similar to action semantics [17] and high-level semantics <ref> [11] </ref>. The simplest monad is the identity monad, shown in Figure 3. Given the identity monad, we can define add as ordinary addition. [[]] would have type Expression ! int.
Reference: [12] <author> S. Liang, </author> <title> "A Modular Semantics for Compiler Generation," </title> <institution> Yale University Department of Computer Science Technical Report TR-1067, </institution> <month> February </month> <year> 1995. </year> <month> 15 </month>
Reference-contexts: Finally, Section 6 summarizes this work and outlines future research. 2 Related work Espinosa [7] and Hudak, Liang, and Jones [13] use monad transformers to create modular, extensible interpreters. Liang <ref> [12, 14] </ref> addresses the question of whether compilers can be developed similarly, but since he does not compile to machine language, many of the issues we confront|especially staging|do not arise. A syntactic form of metacomputation can be found in the two-level -calculus of Nielson [19].
Reference: [13] <author> S. Liang, P. Hudak, and M. Jones, </author> <title> Monad Transformers and Modular Interpreters. </title> <booktitle> Proceed--ings of the ACM Conference on the Principles of Programming Languages, </booktitle> <year> 1995. </year>
Reference-contexts: The reader will recognize this distinction as the classic separation of static from dynamic. Thus, staging is an instance of metacomputation. The main contribution of this paper is a modular and extensible method of staging denotational specifications based on metacomputations, formalized via monads <ref> [7, 13, 16, 23] </ref>. A style of language specification based on metacomputation is proposed in which the static and dynamic parts of a language specification are factored into distinct monads. <p> We believe this style of language specification may have many uses, but in this paper we concentrate on one: modular compilation. Modular compilers are compilers built from building blocks that represent language features rather than compilation phases, as illustrated in Figure 2. Espinosa [7] and Liang & Hudak <ref> [13] </ref> showed how to construct modular interpreters using the notion of monads [7, 13, 16, 23] | or, more precisely, monad transformers. The current authors built on those ideas to produce modular compilers in [8]. <p> Modular compilers are compilers built from building blocks that represent language features rather than compilation phases, as illustrated in Figure 2. Espinosa [7] and Liang & Hudak [13] showed how to construct modular interpreters using the notion of monads <ref> [7, 13, 16, 23] </ref> | or, more precisely, monad transformers. The current authors built on those ideas to produce modular compilers in [8]. However, there the notion of staging, though conceptually at the heart of the approach, was not explicit in the compiler building blocks we constructed. <p> The result was awkwardness in communicating between these aspects of the domain, which meant that "gluing together" compiler blocks was sometimes delicate. Indeed, metacomputation is purposely avoided in <ref> [7, 13, 8] </ref>. A key aspect of that work is that monad transformers are used to create the single monad used to interpret or compile the language. The problem that inspired it was that monads don't compose nicely. <p> Finally, Section 6 summarizes this work and outlines future research. 2 Related work Espinosa [7] and Hudak, Liang, and Jones <ref> [13] </ref> use monad transformers to create modular, extensible interpreters. Liang [12, 14] addresses the question of whether compilers can be developed similarly, but since he does not compile to machine language, many of the issues we confront|especially staging|do not arise. <p> the full paper) improves on Reynolds's in two ways: it is monad-structured|that is, built from interchangeable parts|and it includes jumps and labels where Reynolds simply allowed code duplication and infinite programs. 3 Monads and Monad Transformers In this section, we review the theory of monads [16, 23] and monad transformers <ref> [7, 13] </ref>. Readers familiar with these topics may skip the section. <p> Unfortunately, M St (M St t ) and M 2St t are very different types. This points to a difficulty with monads: they do not compose in this simple manner. The key contribution of the work <ref> [7, 13] </ref> on monad transformers is to solve this composition problem. When applied to a monad M, a monad transformer T creates a new monad M 0 . <p> The state monad transformer also provides updateSto and getSto operations appropriate to the newly-created monad. When composing T St Sto with itself, as above, the operations on the "inner" state need to be lifted through the outer state monad; this is the main technical issue in <ref> [7, 13] </ref>. In our work in [8], we found it convenient to factor the state monad into two parts: the state proper and the address allocator. <p> In section 5, they will be combined to create a compiler. For the first two of these blocks, we also give monolithic versions, drawn from [8], to illustrate why metacomputation is helpful. 4.1 Integer Expressions Compiler Building Block Consider the standard monadic-style specification of negation <ref> [7, 13, 23] </ref> displayed in Figure 4. <p> Observe that this implementation-oriented definition calculates the same value as the standard definition, but it stores the intermediate value i as well. But where do addresses and storage come from? In [8], we added them to the Exec monad using monad transformers <ref> [7, 13] </ref> as in the "Implementation-oriented" specification in Figure 4. In that definition, rdAddr reads the current top of stack address a, inAddr increments the top of stack, and Thread stores i at a. The monad (Exec) is used to construct the domain containing both static and dynamic data.
Reference: [14] <author> S. Liang, </author> <title> "Modular Monadic Semantics and Compilation," </title> <type> Doctoral Thesis, </type> <institution> Yale University, </institution> <year> 1997. </year>
Reference-contexts: Finally, Section 6 summarizes this work and outlines future research. 2 Related work Espinosa [7] and Hudak, Liang, and Jones [13] use monad transformers to create modular, extensible interpreters. Liang <ref> [12, 14] </ref> addresses the question of whether compilers can be developed similarly, but since he does not compile to machine language, many of the issues we confront|especially staging|do not arise. A syntactic form of metacomputation can be found in the two-level -calculus of Nielson [19].
Reference: [15] <author> T. Mogensen. </author> <title> "Separating Binding Times in Language Specifications," </title> <booktitle> Proceedings of the ACM Conference on Functional Programming and Computer Architecture, </booktitle> <pages> pp 12-25, </pages> <year> 1989. </year>
Reference-contexts: Expressions of mixed level, then, have strongly separated binding times by definition. Nielson [18] applies two-level -calculus to code generation for a typed -calculus, and Nielson [19] presents an algorithm for static analysis of a typed -calculus which converts one-level specifications into two-level specifications. Mogensen <ref> [15] </ref> generalizes this algorithm to handle variables of mixed binding times. The present work offers a semantic alternative to the two-level -calculus. <p> We formalize distinct levels (in the sense of Nielson [19]) as distinct monads, and the resulting specifications have all of the traditional advantages of monadic specifications (reusability, extensibility, and modularity). While our binding time analysis is not automatic as in <ref> [19, 15] </ref>, we consider a far wider range of programming language features than they do. Danvy and Vestergaard [5] show how to produce code that "looks like" machine language, by expressing the source language semantics in terms of machine language-like combinators (e.g., "popblock", "push").
Reference: [16] <author> E. Moggi, </author> <title> "Notions of Computation and Monads," </title> <booktitle> Information and Computation 93(1), </booktitle> <pages> pp. 55-92, </pages> <year> 1991. </year>
Reference-contexts: The reader will recognize this distinction as the classic separation of static from dynamic. Thus, staging is an instance of metacomputation. The main contribution of this paper is a modular and extensible method of staging denotational specifications based on metacomputations, formalized via monads <ref> [7, 13, 16, 23] </ref>. A style of language specification based on metacomputation is proposed in which the static and dynamic parts of a language specification are factored into distinct monads. <p> Modular compilers are compilers built from building blocks that represent language features rather than compilation phases, as illustrated in Figure 2. Espinosa [7] and Liang & Hudak [13] showed how to construct modular interpreters using the notion of monads <ref> [7, 13, 16, 23] </ref> | or, more precisely, monad transformers. The current authors built on those ideas to produce modular compilers in [8]. However, there the notion of staging, though conceptually at the heart of the approach, was not explicit in the compiler building blocks we constructed. <p> for that language (presented in the full paper) improves on Reynolds's in two ways: it is monad-structured|that is, built from interchangeable parts|and it includes jumps and labels where Reynolds simply allowed code duplication and infinite programs. 3 Monads and Monad Transformers In this section, we review the theory of monads <ref> [16, 23] </ref> and monad transformers [7, 13]. Readers familiar with these topics may skip the section.
Reference: [17] <author> P. Mosses, </author> <title> Action Semantics, </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: The beauty of the monadic form is that the meaning of [[]] can be reinterpreted in a variety of monads. Monadic semantics separate the description of a language from its denotation. In this sense, it is similar to action semantics <ref> [17] </ref> and high-level semantics [11]. The simplest monad is the identity monad, shown in Figure 3. Given the identity monad, we can define add as ordinary addition. [[]] would have type Expression ! int.
Reference: [18] <author> H. Nielson and F. Nielson, </author> <title> "Code Generation from two-level denotational metalanguages," in Programs as Data Objects, </title> <booktitle> Lecture Notes in Computer Science 217 (Springer, </booktitle> <address> Berlin, </address> <year> 1986). </year>
Reference-contexts: A syntactic form of metacomputation can be found in the two-level -calculus of Nielson [19]. Two-level -calculus contains two distinct -calculi|representing the static and dynamic levels. Expressions of mixed level, then, have strongly separated binding times by definition. Nielson <ref> [18] </ref> applies two-level -calculus to code generation for a typed -calculus, and Nielson [19] presents an algorithm for static analysis of a typed -calculus which converts one-level specifications into two-level specifications. Mogensen [15] generalizes this algorithm to handle variables of mixed binding times.
Reference: [19] <author> H. Nielson and F. Nielson, </author> <title> "Automatic Binding Time Analysis for a Typed -calculus," </title> <booktitle> Science of Computer Programming 10, </booktitle> <month> 2 (April </month> <year> 1988), </year> <pages> pp 139-176. </pages>
Reference-contexts: Liang [12, 14] addresses the question of whether compilers can be developed similarly, but since he does not compile to machine language, many of the issues we confront|especially staging|do not arise. A syntactic form of metacomputation can be found in the two-level -calculus of Nielson <ref> [19] </ref>. Two-level -calculus contains two distinct -calculi|representing the static and dynamic levels. Expressions of mixed level, then, have strongly separated binding times by definition. Nielson [18] applies two-level -calculus to code generation for a typed -calculus, and Nielson [19] presents an algorithm for static analysis of a typed -calculus which converts <p> form of metacomputation can be found in the two-level -calculus of Nielson <ref> [19] </ref>. Two-level -calculus contains two distinct -calculi|representing the static and dynamic levels. Expressions of mixed level, then, have strongly separated binding times by definition. Nielson [18] applies two-level -calculus to code generation for a typed -calculus, and Nielson [19] presents an algorithm for static analysis of a typed -calculus which converts one-level specifications into two-level specifications. Mogensen [15] generalizes this algorithm to handle variables of mixed binding times. The present work offers a semantic alternative to the two-level -calculus. We formalize distinct levels (in the sense of Nielson [19]) <p> <ref> [19] </ref> presents an algorithm for static analysis of a typed -calculus which converts one-level specifications into two-level specifications. Mogensen [15] generalizes this algorithm to handle variables of mixed binding times. The present work offers a semantic alternative to the two-level -calculus. We formalize distinct levels (in the sense of Nielson [19]) as distinct monads, and the resulting specifications have all of the traditional advantages of monadic specifications (reusability, extensibility, and modularity). While our binding time analysis is not automatic as in [19, 15], we consider a far wider range of programming language features than they do. <p> We formalize distinct levels (in the sense of Nielson [19]) as distinct monads, and the resulting specifications have all of the traditional advantages of monadic specifications (reusability, extensibility, and modularity). While our binding time analysis is not automatic as in <ref> [19, 15] </ref>, we consider a far wider range of programming language features than they do. Danvy and Vestergaard [5] show how to produce code that "looks like" machine language, by expressing the source language semantics in terms of machine language-like combinators (e.g., "popblock", "push").
Reference: [20] <author> J. Reynolds. </author> <title> "The Essence of Algol," </title> <booktitle> Algorithmic Languages, Proceedings of the International Symposium on Algorithmic Languages, </booktitle> <pages> pp. 345-372, </pages> <year> 1981. </year>
Reference-contexts: The compiler building block for this language appears in Figure 9. The static part of this specification allocates a free stack location a, and the program variable x is bound to an accepter-expresser pair <ref> [20] </ref> in the current environment . <p> The compiler building block for this language appears in Figure 9. For sequencing, the static part of the specification compiles c 1 and c 2 in succession, while the dynamic (boxed) part runs them in succession. For assignment, the static part of the specification retrieves the accepter <ref> [20] </ref> acc for program variable x from the current environment and compiles t, while the dynamic part calculates the value of t and passes it to acc. 12 4.5 Optimized Expression Compiler Building Block Reynolds [21] gives a more efficient translation of arithmetic expressions than we have given in our expression
Reference: [21] <author> J. Reynolds, </author> <title> "Using Functor Categories to Generate Intermediate Code," </title> <booktitle> Proceedings of the ACM Conference on the Principles of Programming Languages, </booktitle> <pages> pages 25-36, </pages> <year> 1995. </year>
Reference-contexts: Second, we include some building blocks not given in [8], specifically optimized expressions and procedures. (These will be included in the full paper.) Together these give us the entire language of Reynolds <ref> [21] </ref> with essentially the same code generated there (but with jumps and labels, avoiding the potential for infinite programs). The next section reviews the most relevant related work. In Section 3, we review the theory of monads and monad transformers and their use in language specification. <p> When the interpreter is closed over these combinators, partial evaluation of this closed term with respect to a program produces a completely dynamic term, composed of a sequence of combinators, looking very much like machine language. This approach is key to making the monadic structure useful for compilation. Reynolds' <ref> [21] </ref> demonstration of how to produce efficient code in a compiler derived from the functor category semantics of an Algol-like language was an original inspiration for this study. <p> For assignment, the static part of the specification retrieves the accepter [20] acc for program variable x from the current environment and compiles t, while the dynamic part calculates the value of t and passes it to acc. 12 4.5 Optimized Expression Compiler Building Block Reynolds <ref> [21] </ref> gives a more efficient translation of arithmetic expressions than we have given in our expression building block.
Reference: [22] <author> J. E. Stoy, </author> <title> Denotational Semantics: the Scott-Strachey Approach to Programming Language Theory, </title> <publisher> MIT Press, </publisher> <year> 1977. </year>
Reference: [23] <author> P. Wadler, </author> <title> "The essence of functional programming," </title> <booktitle> Proceedings of the ACM Conference on the Principles of Programming Languages, </booktitle> <pages> pages 1-14, </pages> <year> 1992. </year>
Reference-contexts: The reader will recognize this distinction as the classic separation of static from dynamic. Thus, staging is an instance of metacomputation. The main contribution of this paper is a modular and extensible method of staging denotational specifications based on metacomputations, formalized via monads <ref> [7, 13, 16, 23] </ref>. A style of language specification based on metacomputation is proposed in which the static and dynamic parts of a language specification are factored into distinct monads. <p> Modular compilers are compilers built from building blocks that represent language features rather than compilation phases, as illustrated in Figure 2. Espinosa [7] and Liang & Hudak [13] showed how to construct modular interpreters using the notion of monads <ref> [7, 13, 16, 23] </ref> | or, more precisely, monad transformers. The current authors built on those ideas to produce modular compilers in [8]. However, there the notion of staging, though conceptually at the heart of the approach, was not explicit in the compiler building blocks we constructed. <p> for that language (presented in the full paper) improves on Reynolds's in two ways: it is monad-structured|that is, built from interchangeable parts|and it includes jumps and labels where Reynolds simply allowed code duplication and infinite programs. 3 Monads and Monad Transformers In this section, we review the theory of monads <ref> [16, 23] </ref> and monad transformers [7, 13]. Readers familiar with these topics may skip the section. <p> In section 5, they will be combined to create a compiler. For the first two of these blocks, we also give monolithic versions, drawn from [8], to illustrate why metacomputation is helpful. 4.1 Integer Expressions Compiler Building Block Consider the standard monadic-style specification of negation <ref> [7, 13, 23] </ref> displayed in Figure 4.
Reference: [24] <author> M. Wand, </author> <title> "Deriving Target Code as a Representation of Continuation Semantics," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 4, No. 3, </volume> <pages> pp. 496-517, </pages> <year> 1982. </year> <month> 16 </month>
References-found: 24

