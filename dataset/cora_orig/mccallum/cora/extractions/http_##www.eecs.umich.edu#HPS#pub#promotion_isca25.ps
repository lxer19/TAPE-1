URL: http://www.eecs.umich.edu/HPS/pub/promotion_isca25.ps
Refering-URL: http://www.eecs.umich.edu/HPS/hps_tracecache.html
Root-URL: http://www.cs.umich.edu
Email: fsanjayp, olaf, pattg@eecs.umich.edu  
Title: Improving Trace Cache Effectiveness with Branch Promotion and Trace Packing a machine where the execution
Author: Sanjay Jeram Patel Marius Evers Yale N. Patt 
Note: On  
Address: Ann Arbor, Michigan 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory Department of Electrical Engineering and Computer Science The University of Michigan  
Abstract: The increasing widths of superscalar processors are placing greater demands upon the fetch mechanism. The trace cache meets these demands by placing logically contiguous instructions in physically contiguous storage. As a result, the trace cache delivers instructions at a high rate by supplying multiple fetch blocks each cycle. In this paper, we examine two techniques to improve the number of instructions delivered each cycle by the trace cache. The first technique, branch promotion, dynamically converts strongly biased branches into branches with static predictions. Because these promoted branches require no dynamic prediction, the branch predictor suffers less from the negative effects of interference. Branch promotion unlocks the potential of the second technique: trace packing. With trace packing, trace segments are packed with as many instructions as will fit, without regard to naturally occurring fetch block boundaries. With both techniques, the effective fetch rate of the trace cache jumps up 17% over a trace cache which implements neither. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Burger, T. Austin, and S. Bennett. </author> <title> Evaluating future microprocessors: The simplescalar tool set. </title> <type> Technical Report 1308, </type> <institution> University of Wisconsin - Madison Technical Report, </institution> <month> July </month> <year> 1996. </year>
Reference-contexts: Experimental model An executable-driven simulator which allows the modeling of wrong path effects was used for this study. The simulator was implemented using the simplescalar 2.0 tool suite <ref> [1] </ref>. All instructions undergo four stages of processing before retirement: fetch, issue, schedule, execute. All stages take at least one cycle. Figure 2 shows a block diagram of the pipeline model.
Reference: [2] <author> P.-Y. Chang, M. Evers, and Y. N. Patt. </author> <title> Improving branch prediction accuracy by reducing pattern history table interference. </title> <booktitle> In Proceedings of the 1996 ACM/IEEE Conference on Parallel Architectures and Compilation Techniques, </booktitle> <year> 1996. </year>
Reference-contexts: When such a branch is detected, the branch will be converted by the fill unit into a branch with a built-in static prediction. We call this process branch promotion. The concept is similar to branch filtering proposed by Chang et al <ref> [2] </ref>. A promoted branch requires no dynamic prediction and therefore need not consume branch predictor bandwidth when it is fetched. Its likely target instruction is either included within the trace segment or will be fetched in the subsequent fetch cycle. <p> With two bits, the fill unit can encode a branch as promoted and designate its likely outcome. Candidate branches can be detected via a hardware mechanism similar to the mechanism used for branch filtering <ref> [2] </ref>. A table, indexed by branch address, called the branch bias table, is shown in figure 5. It contains the previous outcome of the branch and the number of consecutive times the branch has had that same outcome. The bias table is updated whenever a branch is retired.
Reference: [3] <author> P.-Y. Chang, E. Hao, T.-Y. Yeh, and Y. N. Patt. </author> <title> Branch classification: A new mechanism for improving branch predictor performance. </title> <booktitle> In Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pages 22 31, </pages> <year> 1994. </year>
Reference-contexts: In order to address this limitation in an effective manner, we draw upon a frequently reported <ref> [3] </ref> characteristic of conditional branches: during execution, over 50% of conditional branches are strongly biased. When such a branch is detected, the branch will be converted by the fill unit into a branch with a built-in static prediction. We call this process branch promotion.
Reference: [4] <author> M. Franklin and M. Smotherman. </author> <title> A fill-unit approach to multiple instruction issue. </title> <booktitle> In Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pages 162171, </pages> <year> 1994. </year>
Reference-contexts: They proposed the fill unit to compact a fetch block's worth of instructions into an entry in a decoded instruction cache. Two other extensions of the original schemes were presented by Franklin and Smotherman. In <ref> [4] </ref>, they applied the original fill unit idea to dynamically create VLIW instructions out of RISC-type operations. In [13], they demonstrated how a fill unit could help overcome the decoder bottleneck of a Pentium Pro type processor.
Reference: [5] <author> D. H. Friendly, S. J. Patel, and Y. N. Patt. </author> <title> Alternative fetch and issue techniques from the trace cache fetch mechanism. </title> <booktitle> In Proceedings of the 30th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1997. </year>
Reference-contexts: In [12], Rotenberg et al presented the design and performance implications of a processor which treats traces as the atomic unit of execution. Friendly et al <ref> [5] </ref> investigated two techniques, partial matching and inactive issue, which boost performance by 15% over a trace cache not implementing either. 3. Experimental model An executable-driven simulator which allows the modeling of wrong path effects was used for this study. <p> ABC and ABD cannot be concurrently resident). See [9] for a discussion and performance results on path associativity. Also, the fill unit collects blocks after they retire, building trace segments from the retired instruction stream. Furthermore, the trace cache uses inactive issue <ref> [5] </ref>. With inactive issue, all blocks within a trace cache line are issued into the processor whether or not they match the predicted path. The blocks that do not match the prediction are said to be issued inactively.
Reference: [6] <author> W. W. Hwu and Y. N. Patt. </author> <title> Checkpoint repair for out-of-order execution machines. </title> <booktitle> In Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 1826, </pages> <year> 1987. </year>
Reference-contexts: Instructions are dispatched for execution from a 64-entry reservation station (or node table) associated with each functional unit. A 64KB L1 data cache is used for data supply. The model uses checkpoint repair <ref> [6] </ref> to recover from branch mispredictions and exceptions. The execution engine is capable of creating up to three checkpoints each cycle, one for each fetch block supplied. The memory scheduler waits for addresses to be generated before scheduling memory operations.
Reference: [7] <author> S. W. Melvin, M. C. Shebanow, and Y. N. Patt. </author> <title> Hardware support for large atomic units in dynamically scheduled machines. </title> <booktitle> In Proceedings of the 21st Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pages 6063, </pages> <year> 1988. </year>
Reference-contexts: If the execution core performs perfect memory disambiguation, then branch promotion and trace packing boosts performance to 11% over a baseline with the same execution core. 2. Related work A precursor to the trace cache was first introduced by Melvin, Shebanow and Patt <ref> [7] </ref>. They proposed the fill unit to compact a fetch block's worth of instructions into an entry in a decoded instruction cache. Two other extensions of the original schemes were presented by Franklin and Smotherman.
Reference: [8] <author> A. Moshovos, S. E. Breach, T. N. Vijaykumar, and G. S. Sohi. </author> <title> Dynamic speculation and synchronization of data dependences. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <year> 1997. </year>
Reference-contexts: Techniques which expedite memory operations via aggressive memory disambiguation, for example memory dependence speculation <ref> [8] </ref> or memory renaming [15], are more critical. In order to evaluate these effects, we performed experiments with a more aggressive, less restrictive execution engine. Figure 16 shows the performance of the icache, the baseline trace cache and the trace cache with promotion and packing. ideal, aggressive execution engine.
Reference: [9] <author> S. J. Patel, D. H. Friendly, and Y. N. Patt. </author> <title> Critical issues regarding the trace cache fetch mechanism. </title> <type> Technical Report CSE-TR-335-97, </type> <institution> University of Michigan Technical Report, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: Here, a fetch block roughly corresponds to a compiler basic block: it is a dynamic sequence of instructions starting at the current fetch address and ending at the next control instruction. Recently, the trace cache <ref> [10, 11, 9] </ref> has been proposed as a technique that is able to overcome this fetch bandwidth limitation. By placing logically contiguous instructions in physically contiguous storage, the trace cache is able to supply multiple fetch blocks each cycle. <p> Misses in the second level cache take a minimum of 50 cycles to be fetched from memory. The branch predictor modeled is the gshare predictor designed for use with the trace cache in <ref> [9] </ref> and provides up to three individual conditional branch predictions each cycle. The size of the pattern history table was fixed at 16K entries, each entry consisting of 7 2-bit counters (32KB of storage), as shown in figure 3. An ideal return address stack was modeled. <p> An ideal return address stack was modeled. The trace cache modeled does not utilize path associativity, meaning only one segment starting at a particular fetch block can be stored in the cache at any time (ie. ABC and ABD cannot be concurrently resident). See <ref> [9] </ref> for a discussion and performance results on path associativity. Also, the fill unit collects blocks after they retire, building trace segments from the retired instruction stream. Furthermore, the trace cache uses inactive issue [5].
Reference: [10] <author> A. Peleg and U. Weiser. </author> <title> Dynamic flow instruction cache memory organized around trace segments independant of virtual address line. </title> <type> U.S. Patent Number 5,381,533, </type> <year> 1994. </year>
Reference-contexts: Here, a fetch block roughly corresponds to a compiler basic block: it is a dynamic sequence of instructions starting at the current fetch address and ending at the next control instruction. Recently, the trace cache <ref> [10, 11, 9] </ref> has been proposed as a technique that is able to overcome this fetch bandwidth limitation. By placing logically contiguous instructions in physically contiguous storage, the trace cache is able to supply multiple fetch blocks each cycle. <p> In [13], they demonstrated how a fill unit could help overcome the decoder bottleneck of a Pentium Pro type processor. In 1994, Peleg and Weiser filed a patent on the trace cache concept <ref> [10] </ref>. The concept was further investigated by Rotenberg et al [11]. They presented a thorough comparison between the trace cache scheme and several hardware-based high-bandwidth fetch schemes and showed the advantage of using a trace cache, both in performance and latency.
Reference: [11] <author> E. Rotenberg, S. Bennett, and J. E. Smith. </author> <title> Trace cache: a low latency approach to high bandwidth instruction fetching. </title> <booktitle> In Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1996. </year>
Reference-contexts: Here, a fetch block roughly corresponds to a compiler basic block: it is a dynamic sequence of instructions starting at the current fetch address and ending at the next control instruction. Recently, the trace cache <ref> [10, 11, 9] </ref> has been proposed as a technique that is able to overcome this fetch bandwidth limitation. By placing logically contiguous instructions in physically contiguous storage, the trace cache is able to supply multiple fetch blocks each cycle. <p> In [13], they demonstrated how a fill unit could help overcome the decoder bottleneck of a Pentium Pro type processor. In 1994, Peleg and Weiser filed a patent on the trace cache concept [10]. The concept was further investigated by Rotenberg et al <ref> [11] </ref>. They presented a thorough comparison between the trace cache scheme and several hardware-based high-bandwidth fetch schemes and showed the advantage of using a trace cache, both in performance and latency.
Reference: [12] <author> E. Rotenberg, Q. Jacobsen, Y. Sazeides, and J. E. Smith. </author> <title> Trace processors. </title> <booktitle> In Proceedings of the 30th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1997. </year>
Reference-contexts: The concept was further investigated by Rotenberg et al [11]. They presented a thorough comparison between the trace cache scheme and several hardware-based high-bandwidth fetch schemes and showed the advantage of using a trace cache, both in performance and latency. In <ref> [12] </ref>, Rotenberg et al presented the design and performance implications of a processor which treats traces as the atomic unit of execution. Friendly et al [5] investigated two techniques, partial matching and inactive issue, which boost performance by 15% over a trace cache not implementing either. 3.
Reference: [13] <author> M. Smotherman and M. Franklin. </author> <title> Improving cisc instruction decoding performance using a fill unit. </title> <booktitle> In Proceedings of the 28th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pages 219229, </pages> <year> 1995. </year>
Reference-contexts: Two other extensions of the original schemes were presented by Franklin and Smotherman. In [4], they applied the original fill unit idea to dynamically create VLIW instructions out of RISC-type operations. In <ref> [13] </ref>, they demonstrated how a fill unit could help overcome the decoder bottleneck of a Pentium Pro type processor. In 1994, Peleg and Weiser filed a patent on the trace cache concept [10]. The concept was further investigated by Rotenberg et al [11].
Reference: [14] <author> J. Stark, P. Racunas, and Y. N. Patt. </author> <title> Reducing the performance impact of instruction cache misses by writing instructions into the reservation stations out-of-order. </title> <booktitle> In Proceedings of the 30th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pages 34 43, </pages> <year> 1997. </year>
Reference-contexts: The selector is accessed using the same 15-bit index as the gshare component. The total size data size of this predictor is approximately 32KB. All experiments were performed on the SPECint95 benchmarks and on several common UNIX applications <ref> [14] </ref>. Table 1 lists the number of instructions simulated and the input set, if the input was derived from a standard input set 1 . All simulations were run until completion (except li and ijpeg).
Reference: [15] <author> G. S. Tyson and T. M. Austin. </author> <title> Improving the accuracy and performance of memory communication through renaming. </title> <booktitle> In Proceedings of the 30th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1994. </year>
Reference-contexts: Techniques which expedite memory operations via aggressive memory disambiguation, for example memory dependence speculation [8] or memory renaming <ref> [15] </ref>, are more critical. In order to evaluate these effects, we performed experiments with a more aggressive, less restrictive execution engine. Figure 16 shows the performance of the icache, the baseline trace cache and the trace cache with promotion and packing. ideal, aggressive execution engine.
Reference: [16] <author> C. Young, N. Gloy, and M. D. Smith. </author> <title> A comparative analysis of schemes for correlated branch prediction. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 276286, </pages> <year> 1995. </year>
Reference-contexts: In addition to increasing the effective fetch rate of the trace cache, branch promotion removes easily predictable branches from the domain of the dynamic predictor. Since these branches do not update the predictor's pattern history table, interference <ref> [16] </ref> is reduced in a manner similar to branch filtering. Their outcomes, however, are added to the global branch history to maintain the integrity of the predictor's information. Because interference is reduced, prediction accuracy improves overall.
References-found: 16

