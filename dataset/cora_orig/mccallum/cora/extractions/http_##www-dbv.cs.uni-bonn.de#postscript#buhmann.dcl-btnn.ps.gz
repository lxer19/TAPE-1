URL: http://www-dbv.cs.uni-bonn.de/postscript/buhmann.dcl-btnn.ps.gz
Refering-URL: http://www-dbv.cs.uni-bonn.de/abstracts/buhmann.dcl-btnn.html
Root-URL: http://cs.uni-bonn.de
Email: email: jb@cs.bonn.edu, jb@informatik.uni-bonn.de  
Phone: Phone: +49 228 550 380 Fax: +49 228 550 382  
Title: Learning and Data Clustering  Data Clustering and Learning Correspondence:  
Author: Joachim M. Buhmann Joachim Buhmann 
Note: RUNNING HEAD:  
Address: D-53117 Bonn, Germany  omerstrae 164, D-53117 Bonn, Germany  
Affiliation: Rheinische Friedrich-Wilhelms-Universit at Institut f ur Informatik III, R omerstrae 164  Universit at Bonn, Institut f ur Informatik III, R  
Abstract-found: 0
Intro-found: 1
Reference: <author> Buhmann, J., and K uhnel, H. </author> <year> (1993). </year> <title> Vector Quantization with Complexity Costs. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(4), </volume> <pages> 1133-1145. </pages>
Reference-contexts: The cost function for K-means clustering is E cc N X K X M i (x i y ) 2 : (6) The size K of the cluster set, i.e., the complexity of the clustering solution, has to be determined by a problem-dependent complexity measure <ref> (Buhmann and K uhnel, 1993) </ref> which monotonically J. Buhmann: Data Clustering and Learning 7 grows with the number of clusters. Simultaneous minimization of the distortion costs and the complexity costs yields an optimal number K fl of clusters.
Reference: <author> Dempster, A. P., Laird, N. M, and Rubin, D. B. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statist. Soc. Ser. B, </journal> <volume> 39, </volume> <pages> 1-38. </pages>
Reference-contexts: An efficient bootstrap solution to overcome the computational problem, how to estimate pa rameters of mixture models with the maximum likelihood method, is provided by the expectation maximization (EM) algorithm <ref> (Dempster et al., 1977) </ref>. The EM algorithm estimates the unobserv able assignment variables fM i g in a first step. M i = 1 denotes that x i has been generated by component , M i = 0 otherwise.
Reference: <author> Duda, R. O., and Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley. Sect. 3.2. </publisher>
Reference-contexts: Adopting this framework of parametric statistics, the detection of data clusters reduces mathematically to the problem how to estimate the parameters Q of the probability density for a given mixture model. A powerful statistical tool for finding mixture parameters is the maximum likelihood (ML) method <ref> (Duda and Hart, 1973) </ref>, i.e., one maximizes the probability of the independently, identically distributed data set fx i ji = 1 : : : ; N g given a particular mixture model. For analytical J.
Reference: <author> Hertz, J., Krogh, A., and Palmer, R. G. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation. </title> <address> New York: </address> <publisher> Addison Wesley. </publisher>
Reference-contexts: The expected assignments hM i i, however, can be approximated by calculating the average influence E i exerted by all M k ; k 6= i on the assignment M i (pp. 29, <ref> (Hertz et al., 1991) </ref>), thereby neglecting pair-correlations (hM i M k i = hM i ihM k i). A maximum entropy estimate of E i J.
Reference: <author> Jacobs, R.A., Jordan, M.I., Nowlan, S.J., and Hinton, G.E. </author> <year> (1991). </year> <title> Adaptive Mixtures of Local Experts. </title> <journal> Neural Computation, </journal> <volume> 3(1), </volume> <pages> 79-87. </pages>
Reference-contexts: neural networks are generated by a Gaussian mixture model has considerably reduced the generalization error of layered neural networks in time series prediction tasks (Nowlan and Hinton, 1992), e.g., predicting sun spots and stock market indicators. (III) A neural network architecture based on Gaussian mixtures, the HIERARCHICAL MIXTURE OF EXPERTS <ref> (Jacobs et al., 1991) </ref>, is able to efficiently solve a real-world classification or regression task in a divide-and-conquer fashion. J.
Reference: <author> Jain, A. K., and Dubes, R. C. </author> <year> (1988). </year> <title> Algorithms for Clustering Data. </title> <address> Englewood Cliffs, NJ 07632: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: 1 INTRODUCTION Data clustering <ref> (Jain and Dubes, 1988) </ref> aims at discovering and emphasizing structure which is hidden in a data set. The structural relationships between individual data points, e.g., pronounced similarity of groups of data vectors, have to be detected in an unsupervised fashion.
Reference: <author> Kohonen, T. </author> <year> (1984). </year> <title> Self-organization and Associative Memory. </title> <publisher> Berlin: Springer. </publisher>
Reference-contexts: Biologically inspired data clustering systems which have gained a lot of attention in the neural network community are SELF-ORGANIZING FEATURE MAPS <ref> (Kohonen, 1984) </ref>. In addition to the distance between data vector x i and the nearest cluster y ff , the cost function also measures J. Buhmann: Data Clustering and Learning 9 the distances to cluster centers y ff1 which are neighbors in index space (j j = 1).
Reference: <author> MacQueen, J. </author> <year> 1967. </year> <title> Some methods for classification and analysis of multivariate observations. pp. </title> <booktitle> 281-297 Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability. </booktitle>
Reference: <author> McLachlan, G. J., and Basford, K. E. </author> <year> (1988). </year> <title> Mixture Models. </title> <address> New York, Basel: </address> <publisher> Marcel Dekker, INC. </publisher>
Reference-contexts: Each individual process is characterized by a univariate probability density which is sometimes called a natural cluster of the data set. The density of the full data set is described by a parametrized mixture model, e.g., frequently one uses Gaussian mixtures <ref> (McLachlan and Basford, 1988) </ref>. This model-based approach to data clustering requires to estimate the mixture parameters, e.g., the mean and variance of four Gaussians for the data set depicted in Fig. 1a. Bayesian statistics provides a conceptual framework to compare and validate different mixture models.
Reference: <author> Nowlan, S.J., and Hinton, G.E. </author> <year> (1992). </year> <title> Simplifying Neural Networks by Soft Weight-Sharing. </title> <journal> Neural Computation, </journal> <volume> 4(4), </volume> <pages> 473-493. </pages>
Reference-contexts: so-called radial basis functions compute function approximations in a robust and efficient way (Poggio and Girosi, 1990). (II) The a priori assumption that synaptic weights in neural networks are generated by a Gaussian mixture model has considerably reduced the generalization error of layered neural networks in time series prediction tasks <ref> (Nowlan and Hinton, 1992) </ref>, e.g., predicting sun spots and stock market indicators. (III) A neural network architecture based on Gaussian mixtures, the HIERARCHICAL MIXTURE OF EXPERTS (Jacobs et al., 1991), is able to efficiently solve a real-world classification or regression task in a divide-and-conquer fashion. J.
Reference: <author> J. Buhmann: </author> <title> Data Clustering and Learning 12 Poggio, </title> <editor> T., and Girosi, F. </editor> <year> (1990). </year> <title> Networks for approximation and learning. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 79(9), </volume> <pages> 1481-1497. </pages>
Reference: <author> Rissanen, J. </author> <year> (1989). </year> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> Singapore: World Scientific. </publisher>
Reference-contexts: Such a preference for simple models with few components is mathematically implemented for example by the MINIMUM DESCRIPTION LENGTH principle <ref> (Rissanen, 1989) </ref>. The reader should note that ML estimation without constraints yields the singular solution of one component with zero variance for each data vector ((Duda and Hart, 1973), pp. 198).
Reference: <author> Rose, K., Gurewitz, E., and Fox, G. </author> <year> (1990). </year> <title> A deterministic annealing approach to clustering. </title> <journal> Pattern Recognition Letters, </journal> <volume> 11(11), </volume> <pages> 589-594. </pages>
Reference: <author> Tikochinsky, Y., Tishby, N.Z., and Levine, R. D. </author> <year> (1984). </year> <title> Alternative Approach to Maximum-Entropy Inference. </title> <journal> Physical Review A, </journal> <volume> 30, </volume> <pages> 2638-2644. </pages>
Reference-contexts: The deterministic optimization strategy for data clustering can be related to the stochastic mixture model estimation by using a Monte Carlo search method with a controlled noise level J. Buhmann: Data Clustering and Learning 8 to minimize the clustering costs in (6). The maximum entropy framework <ref> (Tikochinsky et al., 1984) </ref> allows us to estimate the most stable probability distribution for cluster assignments if the average clustering costs are fixed at a particular noise level. A careful decrease of the noise level, called SIMULATED ANNEALING, yields optimized assignments with a high probability.
Reference: <author> J. Buhmann: </author> <title> Data Clustering and Learning 13 Gaussian mixture model is depicted in (b), the plus signs (+) are the centers of the Gaussian sources and stars (?) are cluster centers. The circles denote the covariance estimates. Figure (c) shows a data partitioning using a logarithmic complexity measure. Data clustering by a self-organizing chain is shown in (d), neighboring clusters being connected. </title>
References-found: 15

