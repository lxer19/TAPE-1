URL: http://www.isi.edu/natural-language/mt/transliterate.ps
Refering-URL: http://www.isi.edu/natural-language/GAZELLE.html
Root-URL: 
Email: knight@isi.edu, graehl@isi.edu  
Title: Machine Transliteration  
Author: Kevin Knight and Jonathan Graehl 
Address: Marina del Rey, CA 90292  
Affiliation: Information Sciences Institute University of Southern California  
Abstract: It is challenging to translate names and technical terms across languages with different alphabets and sound inventories. These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents. For example, computer in English comes out as S4Ett (konpyuutaa) in Japanese. Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries. We describe and evaluate a method for performing backwards transliterations by machine. This method uses a generative model, incorporating several distinct stages in the transliteration process.
Abstract-found: 1
Intro-found: 1
Reference: <author> M. Arbabi, S. M. Fischthal, and V. C. Cheng andd E. Bart. </author> <year> 1994. </year> <title> Algorithms for Arabic name transliteration. </title> <journal> IBM J. Res. Develop., </journal> <volume> 38(2). </volume>
Reference-contexts: Katakana phrases are the largest source of text phrases that do not appear in bilingual dictionaries or training corpora (a.k.a. "not-found words"). However, very little computational work has been done in this area; (Yamron et al., 1994) briefly mentions a pattern-matching approach, while <ref> (Arbabi et al., 1994) </ref> discuss a hybrid neural-net/expert-system approach to (forward) transliteration. The information-losing aspect of transliteration makes it hard to invert.
Reference: <author> L. E. Baum. </author> <year> 1972. </year> <title> An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process. Inequalities, </title> <type> 3. </type>
Reference-contexts: We were able to produce these pairs by manipulating a small English-katakana glossary. For each glossary entry, we converted English words into English sounds using the previous section's model, and we converted katakana words into Japanese sounds using the next section's model. We then applied the estimation-maximization (EM) algorithm <ref> (Baum, 1972) </ref> to generate symbol-mapping probabilities, shown in Figure 1. Our EM training goes like this: 1. For each English/Japanese sequence pair, compute all possible alignments between their elements.
Reference: <author> E. W. Dijkstra. </author> <year> 1959. </year> <title> A note on two problems in connexion with graphs. </title> <journal> Numerische Mathematik, </journal> <volume> 1. </volume>
Reference-contexts: We use this to combine an observed katakana string with each of the models in turn. The result is a large WFSA containing all possible English translations. We use Dijkstra's shortest-path algorithm <ref> (Dijkstra, 1959) </ref> to extract the most probable one. The approach is modular. We can test each engine independently and be confident that their results are combined correctly. We do no pruning, so the final WFSA contains every solution, however unlikely.
Reference: <author> David Eppstein. </author> <year> 1994. </year> <title> Finding the k shortest paths. </title> <booktitle> In Proc. 35th Symp. Foundations of Computer Science. IEEE. </booktitle>
Reference: <author> E. H. Jorden and H. I. Chaplin. </author> <year> 1976. </year> <title> Reading Japanese. </title> <publisher> Yale University Press, </publisher> <address> New Haven. </address>
Reference-contexts: We combined corpus analysis with guidelines from a Japanese textbook <ref> (Jorden and Chaplin, 1976) </ref> to turn up many spelling variations and unusual katakana symbols: * the sound sequence (j i) is usually written ~, but occasionally ". * (g u a) is usually fi, but occasionally . * (w o o) is variously t, t, or with a special, old-style katakana
Reference: <author> F. Pereira and M. Riley. </author> <year> 1996. </year> <title> Speech recognition by composition of weighted finite automata. </title> <note> In preprint, cmp-lg/9603001. </note>
Reference-contexts: A WFST is a WFSA with a pair of symbols on each transition, one input and one output. Inputs and outputs may include the empty symbol *. Also following <ref> (Pereira and Riley, 1996) </ref>, we have implemented a general composition algorithm for constructing an integrated model P (xjz) from models P (xjy) and P (yjz), treating WFSAs as WFSTs with identical inputs and outputs. We use this to combine an observed katakana string with each of the models in turn.
Reference: <author> F. Pereira, M. Riley, and R. Sproat. </author> <year> 1994. </year> <title> Weighted rational transductions and their application to human language processing. </title> <booktitle> In Proc. ARPA Human Language Technology Workshop. </booktitle>
Reference: <author> J. Yamron, J. Cant, A. Demedts, T. Dietzel, and Y. Ito. </author> <year> 1994. </year> <title> The automatic component of the LINGSTAT machine-aided translation system. </title> <booktitle> In Proc. ARPA Workshop on Human Language Technology. </booktitle>
Reference-contexts: Automating back-transliteration has great practical importance in Japanese/English machine translation. Katakana phrases are the largest source of text phrases that do not appear in bilingual dictionaries or training corpora (a.k.a. "not-found words"). However, very little computational work has been done in this area; <ref> (Yamron et al., 1994) </ref> briefly mentions a pattern-matching approach, while (Arbabi et al., 1994) discuss a hybrid neural-net/expert-system approach to (forward) transliteration. The information-losing aspect of transliteration makes it hard to invert.
References-found: 8

