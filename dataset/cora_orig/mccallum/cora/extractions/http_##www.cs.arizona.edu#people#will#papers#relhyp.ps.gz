URL: http://www.cs.arizona.edu/people/will/papers/relhyp.ps.gz
Refering-URL: http://www.cs.arizona.edu/people/will/papers.html
Root-URL: http://www.cs.arizona.edu
Title: Choosing a Reliable Hypothesis (Extended Abstract)  
Author: William Evans Sridhar Rajagopalan Umesh Vazirani 
Note: Supported in part by NSF grant CCR 92-01092. Supported by NSF PYI Award CCR 88-96202 and NSF grant IRI 91-20074.  
Address: Berkeley, CA 94720  
Affiliation: Department of Computer Science, University of California at  
Abstract: We study the problem of inferring an accurate model for a stochastic process from its output. We identify two desirable properties | resoluteness and reliability | of any identification algorithm. We prove that for any countable class of stochastic processes, there is an identification algorithm that has these properties. This result also formulates an optimization problem whose solution is sufficent to solve the identification problem. In this sense, our result provides an analogue to the Occam principle in a probabilistic setting. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Blumer, A. Ehrenfeucht, D. Haussler and M. Warmuth, </author> <title> Occam's Razor, </title> <journal> Inform. Processing Letters 24, </journal> <pages> 377-380. </pages>
Reference-contexts: Thus, any rigorous justification of the methodology of science must include an understanding of the identification problem. Such an interest in predictive models was also proposed in [4]. In the case that machines in M are all deterministic, there is a well known principle, Occam's Razor <ref> [1, 2] </ref>, that states that any hypothesis that is consistent with the output, and whose description is "short" is a good approximation to the true machine thus the criterion is to minimize jM j among all consistent machines. In the stochastic setting, consistency is an extremely weak restriction.
Reference: [2] <author> R. Board and L. Pitt, </author> <title> On the necessity of Occam Algorithms, </title> <booktitle> Proceedings, 22nd ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 54-63. </pages>
Reference-contexts: Thus, any rigorous justification of the methodology of science must include an understanding of the identification problem. Such an interest in predictive models was also proposed in [4]. In the case that machines in M are all deterministic, there is a well known principle, Occam's Razor <ref> [1, 2] </ref>, that states that any hypothesis that is consistent with the output, and whose description is "short" is a good approximation to the true machine thus the criterion is to minimize jM j among all consistent machines. In the stochastic setting, consistency is an extremely weak restriction.
Reference: [3] <author> J. Crutchfield and K. Young, </author> <title> Inferring Statistical Complexity, </title> <journal> Phys. Rev. Let., </journal> <volume> vol. 63, p 105, </volume> <year> 1989 </year>
Reference-contexts: An obvious reason to prefer such an hypothesis ^ M over the Bayesian predictor U M is that U M is typically much more complex than any stochastic machine M 2 M. Furthermore, returning to the example of inferring a dynamical system <ref> [3] </ref>, our goal is not simply to predict the output of the system, but also to formulate an accurate model of the underlying mechanism.
Reference: [4] <author> A. DeSantis, G. Markowsky, M. Weg-man, </author> <title> Learning Probabilistic Prediction Functions, </title> <booktitle> Proc. 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 312-328, </pages> <year> 1988. </year>
Reference-contexts: Thus, any rigorous justification of the methodology of science must include an understanding of the identification problem. Such an interest in predictive models was also proposed in <ref> [4] </ref>.
Reference: [5] <author> M. Feder, N. Merhav, and M. Gutman, </author> <title> Universal Prediction of Individual Sequences IEEE Trans. </title> <journal> Inform. Theory, </journal> <volume> vol. 38(4), </volume> <pages> pages 1258-1269, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: For this problem, it is well known that for every class M of stochastic machines, there is a universal prediction mechanism U M usually called the Bayes' predictor for M that predicts almost optimally <ref> [5, 6, 10] </ref>. By contrast, we wish to find an ^ M 2 M such that ^ M is a good predictor for the sequence x. <p> However, there do not appear to be explicit bounds on the number of mispredictions made by the working hypotheses of the algorithm (in a suitable on line model). On the other hand, [10] and <ref> [5] </ref> show how to predict as well as any Markov source in the limit using a very efficient algorithm based on Ziv-Lempel data compression [11].
Reference: [6] <author> D. Haussler, M. Kearns, and R. Schapire, </author> <title> Bounds on the Sample Complexity of Bayesian Learning Using Information Theory and the VC Dimension, </title> <booktitle> Proc. 4th Workshop on Computational Learning Theory, </booktitle> <pages> pages 61-74, </pages> <year> 1991. </year>
Reference-contexts: For this problem, it is well known that for every class M of stochastic machines, there is a universal prediction mechanism U M usually called the Bayes' predictor for M that predicts almost optimally <ref> [5, 6, 10] </ref>. By contrast, we wish to find an ^ M 2 M such that ^ M is a good predictor for the sequence x.
Reference: [7] <author> M. Li and P. </author> <title> Vitanyi, </title> <journal> Inductive Reasoning and Kolmogorov Complexity Journal of Computer and System Sciences, </journal> <volume> vol. 44, </volume> <pages> pages 343-384, </pages> <year> 1992. </year>
Reference-contexts: Notice that the min operation in the computation of H (x) can be reduced to a finite one assuming c &gt; 0. Also, the quantity (M ) = 2 cjMj can be viewed as a Bayesian prior in concert with existing literature (see <ref> [7] </ref>). The choice of constant c is critical in obtaining our results. The Stubborn algorithm guesses the source G (x 1 x i ) after seeing the ith output bit. It then predicts bit x i+1 according to its guess.
Reference: [8] <author> J. Rissanen, </author> <title> A universal prior for integers and estimation by minimum description length, </title> <journal> Ann. Statist., </journal> <volume> vol. 11, </volume> <pages> pages 416-431, </pages> <year> 1982. </year>
Reference-contexts: The algorithm is defined by a function G : f0; 1g fl 7! M, where G (x) is the ma chine that is chosen by the algorithm after seeing x. Let, def M [x] One way to view is as the length of an encoding of x (MDL principle <ref> [8] </ref>). We define G inductively. * G (*) = M 1 . * Let H (x) be the smallest such that (H (x); x) = minf (M; x) : M 2 Mg * G (xa) = ( H (xa) otherwise.
Reference: [9] <author> S. Rudich, </author> <title> Inferring the structure of a Markov Chain from its output, </title> <booktitle> Proc. 26th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 321-326, </pages> <year> 1985. </year>
Reference-contexts: For example the machine that outputs the flips of a coin is consistent with every output. A closely related setting in which the identification problem has been studied is the inference of finite state Markov sources with arbitrary (real) transition probabilities <ref> [9] </ref>. Rudich exhibited an algorithm that (in the limit) converges to the structure of the minimum state Markov chain that is equivalent to the target Markov chain.
Reference: [10] <author> J. Vitter and P. Krishnan, </author> <title> Optimal Prefetch-ing via Data Compression, </title> <booktitle> Proc. 32nd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 121-130, </pages> <year> 1991. </year>
Reference-contexts: For this problem, it is well known that for every class M of stochastic machines, there is a universal prediction mechanism U M usually called the Bayes' predictor for M that predicts almost optimally <ref> [5, 6, 10] </ref>. By contrast, we wish to find an ^ M 2 M such that ^ M is a good predictor for the sequence x. <p> However, there do not appear to be explicit bounds on the number of mispredictions made by the working hypotheses of the algorithm (in a suitable on line model). On the other hand, <ref> [10] </ref> and [5] show how to predict as well as any Markov source in the limit using a very efficient algorithm based on Ziv-Lempel data compression [11].
Reference: [11] <author> J. Ziv and A. Lempel, </author> <title> Compression of Individual Sequences via Variable-Rate Coding IEEE Transactions on Information Theory, </title> <journal> vol. </journal> <volume> 24, </volume> <pages> pages 530-536, </pages> <month> September </month> <year> 1978. </year>
Reference-contexts: On the other hand, [10] and [5] show how to predict as well as any Markov source in the limit using a very efficient algorithm based on Ziv-Lempel data compression <ref> [11] </ref>.
References-found: 11

