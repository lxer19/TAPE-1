URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P509.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts95.htm
Root-URL: http://www.mcs.anl.gov
Title: Performance of Massively Parallel Computers for Spectral Atmospheric Models  
Author: Ian T. Foster, Brian Toonen Patrick H. Worley 
Address: Argonne, IL 60439, U.S.A.  Oak Ridge, TN 37831-6367, U.S.A.  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  Mathematical Sciences Section Oak Ridge National Laboratory  
Abstract: Massively parallel processing (MPP) computer systems use high-speed interconnection networks to link hundreds or thousands of RISC microprocessors. With each microprocessor having a peak performance of 100 or more Mflops/sec, there is at least the possibility of achieving very high performance. However, the question of exactly how to achieve this performance remains unanswered. MPP systems and vector multiprocessors require very different coding styles. Different MPP systems have widely varying architectures and performance characteristics. For most problems, a range of different parallel algorithms is possible, again with varying performance characteristics. In this paper, we provide a detailed, fair evaluation of MPP performance for a weather and climate modeling application. Using a specially designed spectral transform code, we study performance on three different MPP systems: Intel Paragon, IBM SP2, and Cray T3D. We take great care to control for performance differences due to varying algorithmic characteristics. The results yield insights into MPP performance characteristics, parallel spectral transform algorithms, and coding style for MPP systems. We conclude that it is possible to construct parallel models that achieve multi-Gflops/sec performance on a range of MPPs, if the models are constructed to allow runtime selection of appropriate algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barros, S. </author> <year> 1994. </year> <type> Personal communication. </type>
Reference-contexts: This expectation has been verified empirically in the parallel version of the Integrated Forecast System developed at the European Centre for Medium-Range Weather Forecasting <ref> (Barros 1994) </ref>. Second, PSTSWM does not incorporate realistic physics or the semi-Lagrangian transport (SLT) mechanisms that are used in many modern weather and climate models.
Reference: <author> Bourke, W. </author> <year> 1972. </year> <title> An efficient, one-level, primitive-equation spectral model, </title> <journal> Mon. Wea. Rev., </journal> <volume> 102, </volume> <pages> 687-701. </pages>
Reference-contexts: Of these, finite difference methods are the easiest to parallelize because of their high degree of locality in data reference. Semi-Lagrangian methods introduce additional complexity because of their nonlocal and time-varying access patterns (Williamson and Rasch 1989). Spectral transform methods have important computational advantages <ref> (Bourke 1972) </ref>, but are in many respects the most difficult to parallelize efficiently, because of their highly nonlocal communication patterns. In this study, we used the spectral transform method to solve the nonlinear shallow water equations on the sphere.
Reference: <author> Browning, G. L., J. J. Hack, and P. N. Swarztrauber, </author> <year> 1989. </year> <title> A comparison of three numerical methods for solving differential equations on the sphere, </title> <journal> Mon. Wea. Rev., </journal> <volume> 117, </volume> <pages> 1058-1075. </pages>
Reference-contexts: above, it should not be surprising that there are often multiple viable parallel algorithms for a particular problem, with different performance characteristics in different situations. 2.2 Spectral Transform Method A variety of numerical methods|e.g., finite difference, semi-Lagrangian, and spectral transform| have been used in computer simulations of the atmospheric circulation <ref> (Browning, Hack, and Swarztrauber 1989) </ref>. Of these, finite difference methods are the easiest to parallelize because of their high degree of locality in data reference. Semi-Lagrangian methods introduce additional complexity because of their nonlocal and time-varying access patterns (Williamson and Rasch 1989).
Reference: <author> Dent, D. </author> <year> 1990. </year> <title> The ECMWF model on the Cray Y-MP8, in The Dawn of Massively Parallel Processing in Meteorology, </title> <editor> G.-R. Hoffman and D. K. Maretis, eds., </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Drake, J., I. T. Foster, J. Hack, J. Michalakes, B. Semeraro, B. Toonen, D. Williamson, and P. Worley, </author> <year> 1994. </year> <title> PCCM2: A GCM adapted for scalable parallel computers, </title> <booktitle> in Proc. 5th Symp. on Global Change Studies, </booktitle> <publisher> American Meteorological Society, </publisher> <pages> pp. 91-98. </pages>
Reference-contexts: However, we have not addressed the related issue of coding style. PSTSWM was designed deliberately to emulate the coding style of PCCM2 <ref> (Drake et al. 1994) </ref>, the message-passing parallel implementation of CCM2, and we believe that this design goal has been achieved. An advantage of this structure is that our results are directly applicable to PCCM2 and parallel implementations of similar models. A disadvantage is that achieved performance is not optimal.
Reference: <author> Foster, I. T., W. Gropp, and R. Stevens, </author> <year> 1992. </year> <title> The parallel scalability of the spectral transform method, </title> <journal> Mon. Wea. Rev., </journal> <volume> 120, </volume> <pages> 835-850. </pages>
Reference: <author> Foster, I. T., and B. </author> <title> Toonen 1994. Load-balancing algorithms for climate models, </title> <booktitle> in Proc. Scalable High Performance Computing Conf., IEEE Computer Society, </booktitle> <pages> pp. 674-681. </pages>
Reference-contexts: In general, these are difficult questions to answer because of the high cost associated with implementing and evaluating a range of different parallel algorithms on each MPP platform. In a recent study, we developed a testbed code called PSTSWM <ref> (Worley and Foster 1994) </ref> that incorporated a wide range of parallel spectral transform algorithms. Studies with this 1 testbed confirm that the performance of different algorithms can vary significantly from com-puter to computer and that no single algorithm is optimal on all platforms (Foster and Worley 1994). <p> Studies with this 1 testbed confirm that the performance of different algorithms can vary significantly from com-puter to computer and that no single algorithm is optimal on all platforms <ref> (Foster and Worley 1994) </ref>. Availability of this testbed makes it feasible to perform a comprehensive and fair bench-marking exercise of MPP platforms for spectral transform codes. We report here the results of this exercise, presenting benchmark results for the Intel Paragon, the IBM SP2, and the Cray T3D. <p> All parallel algorithms begin with the vertical dimension undecomposed in the 5 physical domain, since, in full atmospheric models, the columnar physics are unlikely to be efficiently parallelizable <ref> (Foster and Toonen 1994) </ref>. Two basic types of parallel algorithm are examined: transpose and distributed. In a transpose algorithm, the decomposition is "rotated" before a transform begins, to ensure that all data needed to compute a particular transform is local to a single processor. <p> See <ref> (Worley and Foster 1994) </ref> for more details on the performance improvement possible from tuning within algorithm classes.
Reference: <author> Foster, I. T., and P. H. Worley, </author> <year> 1993. </year> <title> Parallelizing the spectral transform method: A comparison of alternative parallel algorithms, in Parallel Processing for Scientific Computing, </title> <editor> R. </editor> <publisher> F. </publisher>
Reference: <author> Sincovec, D. E. Keyes, M. R. Leuze, L. R. Petzold, and D. A. Reed, </author> <title> eds., </title> <booktitle> Society for Industrial and Applied Mathematics, Philadelphia, </booktitle> <pages> pp. 100-107. </pages>
Reference: <author> Foster, I. T., and P. H. Worley, </author> <year> 1994. </year> <title> Parallel algorithms for the spectral transform method, </title> <type> Tech. Report ORNL/TM-12507, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, Tenn., </institution> <note> April (also available as a preprint from Argonne National Laboratory). </note>
Reference-contexts: In general, these are difficult questions to answer because of the high cost associated with implementing and evaluating a range of different parallel algorithms on each MPP platform. In a recent study, we developed a testbed code called PSTSWM <ref> (Worley and Foster 1994) </ref> that incorporated a wide range of parallel spectral transform algorithms. Studies with this 1 testbed confirm that the performance of different algorithms can vary significantly from com-puter to computer and that no single algorithm is optimal on all platforms (Foster and Worley 1994). <p> Studies with this 1 testbed confirm that the performance of different algorithms can vary significantly from com-puter to computer and that no single algorithm is optimal on all platforms <ref> (Foster and Worley 1994) </ref>. Availability of this testbed makes it feasible to perform a comprehensive and fair bench-marking exercise of MPP platforms for spectral transform codes. We report here the results of this exercise, presenting benchmark results for the Intel Paragon, the IBM SP2, and the Cray T3D. <p> All parallel algorithms begin with the vertical dimension undecomposed in the 5 physical domain, since, in full atmospheric models, the columnar physics are unlikely to be efficiently parallelizable <ref> (Foster and Toonen 1994) </ref>. Two basic types of parallel algorithm are examined: transpose and distributed. In a transpose algorithm, the decomposition is "rotated" before a transform begins, to ensure that all data needed to compute a particular transform is local to a single processor. <p> See <ref> (Worley and Foster 1994) </ref> for more details on the performance improvement possible from tuning within algorithm classes.
Reference: <author> Fox, G., R. Williams, and P. Messina, </author> <year> 1994. </year> <title> Parallel Computing Works!, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: 1 Introduction In recent years, a number of computer vendors have produced supercomputers based on a massively parallel processing (MPP) architecture. These computers have been shown to be competitive in performance with conventional vector supercomputers for many applications <ref> (Fox, Williams, and Messina 1994) </ref>. Since spectral weather and climate models are heavy users of vector supercomputers, it is interesting to determine how these models perform on MPPs and which MPPs are best suited to the execution of spectral models.
Reference: <author> Franke, H., P. Hochschild, P. Pattnaik, J.-P. Prost, and M. Snir, </author> <year> 1994. </year> <title> MPI-F: Current status and future directions, </title> <booktitle> Proc. Scalable Parallel Libraries Conference, </booktitle> <address> Mississippi State, October, </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Interprocessor communication on the SP2 was performed by using MPI-F version 1.3.8, an experimental implementation of the MPI message-passing standard (Foster, Gropp, and Skjellum 1995) developed and made available to us by Hubertus Franke of IBM Yorktown <ref> (Franke et al. 1994) </ref>. Interprocessor communication routines for the T3D were implemented by using the Shared Memory Access Library, which supports reading and writing remote (nonlocal) memory locations.
Reference: <author> Gartel, U., W. Joppich, and A. Schuller, </author> <year> 1993. </year> <title> Parallelizing the ECMWF's weather forecast program: The 2D case, </title> <journal> Parallel Computing, </journal> <volume> 19, </volume> <pages> 1413-1426. </pages>
Reference: <author> Gropp, W., E. Lusk, and A. Skjellum, </author> <year> 1995. </year> <title> Using MPI: Portable Parallel Programming with the Message Passing Interface, </title> <publisher> The MIT Press. </publisher>
Reference-contexts: Interprocessor communication on the SP2 was performed by using MPI-F version 1.3.8, an experimental implementation of the MPI message-passing standard <ref> (Foster, Gropp, and Skjellum 1995) </ref> developed and made available to us by Hubertus Franke of IBM Yorktown (Franke et al. 1994). Interprocessor communication routines for the T3D were implemented by using the Shared Memory Access Library, which supports reading and writing remote (nonlocal) memory locations.
Reference: <author> Hack, J. J., B. A. Boville, B. P. Briegleb, J. T. Kiehl, P. J. Rasch, and D. L. Williamson, </author> <year> 1992. </year> <title> Description of the NCAR Community Climate Model (CCM2), </title> <type> NCAR Tech. </type> <institution> Note NCAR/ TN-382+STR, National Center for Atmospheric Research, Boulder, Colo. </institution>
Reference-contexts: In this study, we used the spectral transform method to solve the nonlinear shallow water equations on the sphere. The resulting numerical algorithm is very similar to that used in the NCAR Community Climate Model to handle the horizontal component of the primitive equations <ref> (Hack et al. 1992) </ref>. For concreteness, we first describe the shallow water equations in the form that we solve using the spectral transform method. We then describe the spectral transform method for these equations. We finish with a brief description of the parallel algorithms being examined. Shallow Water Equations. <p> The Legendre transform then operates on each column of the intermediate array independently to produce the spectral coefficients. (The inverse spectral transform operates in the reverse sequence.) In our shallow water equation code <ref> (Hack and Jakob 1992) </ref>, each timestep begins by calculating the nonlinear terms U j, V j, U , V , and + (U 2 + V 2 )=(2 (1 2 )) on the physical grid. Next, the nonlinear terms and the state variables j, ffi, and are Fourier transformed. <p> PSTSWM is a message-passing parallel implementation of the sequential Fortran code STSWM <ref> (Hack and Jakob 1992) </ref>. <p> PSTSWM is a message-passing parallel implementation of the sequential Fortran code STSWM (Hack and Jakob 1992). STSWM uses the spectral transform method to solve the nonlinear shallow water equations on a rotating sphere; its data structures and implementation are based directly on equivalent structures and algorithms in CCM2 <ref> (Hack et al. 1992) </ref>, the Community Climate Model developed at the National Center for Atmospheric Research. PSTSWM differs from STSWM in one major respect: vertical levels have been added to permit a fair evaluation of transpose-based parallel algorithms.
Reference: <author> Hack, J. J., and R. Jakob, </author> <year> 1992. </year> <title> Description of a global shallow water model based on the spectral transform method, </title> <type> NCAR Tech. </type> <institution> Note NCAR/TN-343+STR, National Center for 29 Atmospheric Research, Boulder, Colo., </institution> <month> February. </month>
Reference-contexts: In this study, we used the spectral transform method to solve the nonlinear shallow water equations on the sphere. The resulting numerical algorithm is very similar to that used in the NCAR Community Climate Model to handle the horizontal component of the primitive equations <ref> (Hack et al. 1992) </ref>. For concreteness, we first describe the shallow water equations in the form that we solve using the spectral transform method. We then describe the spectral transform method for these equations. We finish with a brief description of the parallel algorithms being examined. Shallow Water Equations. <p> The Legendre transform then operates on each column of the intermediate array independently to produce the spectral coefficients. (The inverse spectral transform operates in the reverse sequence.) In our shallow water equation code <ref> (Hack and Jakob 1992) </ref>, each timestep begins by calculating the nonlinear terms U j, V j, U , V , and + (U 2 + V 2 )=(2 (1 2 )) on the physical grid. Next, the nonlinear terms and the state variables j, ffi, and are Fourier transformed. <p> PSTSWM is a message-passing parallel implementation of the sequential Fortran code STSWM <ref> (Hack and Jakob 1992) </ref>. <p> PSTSWM is a message-passing parallel implementation of the sequential Fortran code STSWM (Hack and Jakob 1992). STSWM uses the spectral transform method to solve the nonlinear shallow water equations on a rotating sphere; its data structures and implementation are based directly on equivalent structures and algorithms in CCM2 <ref> (Hack et al. 1992) </ref>, the Community Climate Model developed at the National Center for Atmospheric Research. PSTSWM differs from STSWM in one major respect: vertical levels have been added to permit a fair evaluation of transpose-based parallel algorithms.
Reference: <author> Kauranne, T., and S. Barros, </author> <year> 1993. </year> <title> Scalability estimates of parallel spectral atmospheric models, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science: Proceedings of the Fifth ECMWF Workshop on Use of Parallel Processors in Meteorology, </booktitle> <editor> G.-R. Hoffman and T. Kauranne, eds., </editor> <publisher> World Scientific Publishing Co. Pte. Ltd., Singapore, </publisher> <pages> pp. 312-328. </pages>
Reference: <author> Loft, R. D., and R. K. Sato, </author> <year> 1993. </year> <title> Implementation of the NCAR CCM2 on the Connection Machine, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science: Proceedings of the Fifth ECMWF Workshop on Use of Parallel Processors in Meteorology, </booktitle> <editor> G.-R. Hoffman and T. Kauranne, eds., </editor> <publisher> World Scientific Publishing Co. Pte. Ltd., Singapore, </publisher> <pages> pp. 371-393. </pages>
Reference: <author> Pelz, R. B., and W. F. Stern, </author> <year> 1993. </year> <title> A balanced parallel algorithm for spectral global climate models, in Parallel Processing for Scientific Computing, </title> <editor> R. F. Sincovec, D. E. Keyes, M. </editor> <publisher> R. </publisher>
Reference: <author> Leuze, L. R. Petzold, and D. A. Reed, </author> <title> eds., </title> <booktitle> Society for Industrial and Applied Mathematics, Philadelphia, </booktitle> <pages> pp. 126-128. </pages>
Reference: <author> Walker, D. W., P. H. Worley, and J. B. Drake, </author> <year> 1992. </year> <title> Parallelizing the spectral transform method. Part II, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 4, </volume> <pages> 509-531. </pages>
Reference: <author> Washington, W., and C. Parkinson, </author> <year> 1986. </year> <title> An Introduction to Three-Dimensional Climate Modeling, </title> <publisher> University Science Books, </publisher> <address> Mill Valley, Calif. </address>
Reference-contexts: Let i, j, and k denote unit vectors in spherical geometry; V denote the horizontal velocity, V = iu + jv; denote the geopotential; and f denote the Coriolis term. Then the horizontal momentum and mass continuity equations can be written as <ref> (Washington and Parkinson 1986) </ref> DV = f k fi V r (1) Dt where the substantial derivative is given by D ( ) j @t The spectral transform method does not solve these equations directly; rather, it uses a streamfunction-vorticity formulation in order to work with scalar fields.
Reference: <author> Williamson, D. L., J. B. Drake, J. J. Hack, R. Jakob, and P. N. Swarztrauber, </author> <year> 1992. </year> <title> A standard test set for numerical approximations to the shallow water equations on the sphere, </title> <journal> J. Computational Physics, </journal> <volume> 102, </volume> <pages> 211-224. </pages>
Reference: <author> Williamson, D. L., and P. J. Rasch, </author> <year> 1989. </year> <title> Two-dimensional semi-Lagrangian transport with shape-preserving interpolation, </title> <journal> Mon. Wea. Rev., </journal> <volume> 117, </volume> <pages> 102-129. </pages>
Reference-contexts: Of these, finite difference methods are the easiest to parallelize because of their high degree of locality in data reference. Semi-Lagrangian methods introduce additional complexity because of their nonlocal and time-varying access patterns <ref> (Williamson and Rasch 1989) </ref>. Spectral transform methods have important computational advantages (Bourke 1972), but are in many respects the most difficult to parallelize efficiently, because of their highly nonlocal communication patterns. In this study, we used the spectral transform method to solve the nonlinear shallow water equations on the sphere.
Reference: <author> Worley, P. H., and J. B. Drake, </author> <year> 1992. </year> <title> Parallelizing the spectral transform method, </title> <journal> Concur-rency: Practice and Experience, </journal> <volume> 4, </volume> <pages> 269-291. </pages>
Reference: <author> Worley, P. H., and I. T. Foster, </author> <year> 1994. </year> <title> Parallel spectral transform shallow water model: A runtime-tunable parallel benchmark code, </title> <booktitle> in Proc. Scalable High Performance Computing Conf., </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, Calif., </publisher> <pages> pp. 207-214. 30 </pages>
Reference-contexts: In general, these are difficult questions to answer because of the high cost associated with implementing and evaluating a range of different parallel algorithms on each MPP platform. In a recent study, we developed a testbed code called PSTSWM <ref> (Worley and Foster 1994) </ref> that incorporated a wide range of parallel spectral transform algorithms. Studies with this 1 testbed confirm that the performance of different algorithms can vary significantly from com-puter to computer and that no single algorithm is optimal on all platforms (Foster and Worley 1994). <p> Studies with this 1 testbed confirm that the performance of different algorithms can vary significantly from com-puter to computer and that no single algorithm is optimal on all platforms <ref> (Foster and Worley 1994) </ref>. Availability of this testbed makes it feasible to perform a comprehensive and fair bench-marking exercise of MPP platforms for spectral transform codes. We report here the results of this exercise, presenting benchmark results for the Intel Paragon, the IBM SP2, and the Cray T3D. <p> See <ref> (Worley and Foster 1994) </ref> for more details on the performance improvement possible from tuning within algorithm classes.
References-found: 26

