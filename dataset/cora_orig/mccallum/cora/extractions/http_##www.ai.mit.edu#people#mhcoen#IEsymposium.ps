URL: http://www.ai.mit.edu/people/mhcoen/IEsymposium.ps
Refering-URL: http://www.ai.mit.edu/people/mhcoen/
Root-URL: 
Email: mhcoen@ai.mit.edu  
Title: Design Principles for Intelligent Environments  
Author: Michael H. Coen 
Keyword: Content Areas: AI systems, user interfaces  
Address: 545 Technology Square Cambridge, MA 02139  
Affiliation: MIT Artificial Intelligence Lab  
Abstract: This paper describes design criteria for creating highly embedded, interactive spaces that we call Intelligent Environments. The motivation for building these systems is to bring computation into the real, physical world to support what is traditionally considered noncomputational activity. We describe an existing prototype space, known as the Intelligent Room, which was created to experiment with different forms of natural, multimodal human-computer interaction. We discuss design decisions encountered while creating the Intelligent Room and how the experiences gained during its use have shaped the creation of its successor. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Abowd, G., Atkeson, C., Feinstein, A., Hmelo, C., Kooper, R., Long, S., Sawhney, N., and Tani, M. </author> <title> Teach and Learning a Multimedia Authoring: The Classroom 2000 project. </title> <booktitle> Proceedings of the ACM Multimedia96 Conference . 1996. </booktitle> <address> http://www.cc.gatech.edu/fce/c2000.pubs/mm96/c2000.html </address>
Reference-contexts: However their modal processing is extraordinarily specific to their applications, and the applicability of such carefully tuned systems to other domains is unclear. The Classroom 2000 project ( Abowd et al. <ref> [ 1] </ref>) is an educational environment that aut omatically creates records linking simultaneous streams of information, e.g. what the teacher is saying while a student is writing down her notes on a digital pad.
Reference: 2. <author> Black, M. and Yacoob, Y. </author> <title> Recognizing Facial Expressions under Rigid and Rigid-Facial Models. </title> <booktitle> International Workshop on Automatic Face and Gesture Recognition . Zurich. </booktitle> <address> pp.12-17. </address> <year> 1995. </year>
Reference: 3. <author> Black, A. and Taylor, P. </author> <title> Festival Speech Synthesis System: system documentation (1.1.1) Human Communication Research Centre Technical Report HCRC/TR-83. </title> <institution> University of Edinburgh. </institution> <year> 1997. </year>
Reference-contexts: We are currently working on incorporating recent work in remote sound localization and capture ( Renomeron et al. [ 23]) to eliminate the need for every occupant to wear her own microphone. The Intelligent Room is capable of addressing users via the Festival Speech Synthesis System (Black et al. <ref> [ 3] </ref>). Utterances spoken by the room are also displayed on a scrollable LCD sign in case a user was unable to understand what was said.
Reference: 4. <author> Bobick, A.; Intille, S.; Davis, J.; Baird, F.; Pinhanez, C.; Campbell, L.; Ivanov, Y.; Schtte, A.; and Wilson, A. </author> <title> Design Decisions for Interactive Environments: Evaluating the KidsRoom. </title> <note> To appear AAAI 1998 Spring Symposium on Intelligent Environments . 1998. </note>
Reference-contexts: We discuss below the design of our laboratorys Intelligent Room and how experiences gained during its use have shaped the creation of its successor. Given the increasingly widespread interest in highly interactive, computational environments (Bobick et al. <ref> [ 4] </ref>), (Coen [6,7], Coen et al. [ 8]), (Cooperstock et al. [ 10]), ( Lucente et al. [ 19]), we hope these experiences will prove useful to other IE designers and implementers in the AI community. <p> Other substantial efforts towards highly interactive environments include an automated teleconferencing office (Cooperstock et al. [10]) and an immersive fictional theater (Bobick et al. <ref> [ 4] </ref>). Each of these projects makes use of embedded computation to enable unusual human-computer interactions, e.g., vision-based person trac king. However their modal processing is extraordinarily specific to their applications, and the applicability of such carefully tuned systems to other domains is unclear. <p> This approach differs from the overhead tracking system described in Bobick et al. <ref> [ 4] </ref>. Their domain had 27 high ceilings, for which it is quite reasonable to look for people from a single camera birds eye perspective. Rooms with ordinary height ceilings do not make this possible, so a stereo vision system seems necessary for performing background segmentation.
Reference: 5. <author> Cohen, P.R., Chen, L., Clow, J., Johnston, M., McGee, D., Pittman, K., and Smith, I. Quickset: </author> <title> A multimodal interface for distributed interactive simulation, </title> <booktitle> Proceedings of the UIST96 Demonstration Session , Seattle. </booktitle> <year> 1996. </year>
Reference-contexts: Having complex interactive environments learn via observation is likely to be essential to making them generally useful. Given that VCR programming is still a subject of general mirth, the prospect of programming homes is likely to cause widespread consternation. Related user-interface work such as Cohen et al. <ref> [ 5] </ref> uses multimodal interface technology to facilitate human interaction with a preexisting distributed simulator. In doing so, it provides a novel user-interface to a complex software system, but it is one that requires tying down the user to a particular computer and a specific application.
Reference: 6. <author> Coen, M. </author> <title> Building Brains for Rooms: Designing Distributed Software Agents. </title> <booktitle> Proceedings of the Ninth Conference on Innovative Applications of Artificial Intelligence. </booktitle> <address> (IAAI97). Providence, R.I. </address> <year> 1997. </year> <note> http://www.ai.mit.edu/people/mhcoen/brain.ps </note>
Reference-contexts: Do you want to see it? User : Yes, show me the satellite image. The Intelligent Room is capable of running several other applications, including a tour guide system as described in Coen <ref> [6] </ref>. We are currently developing a next generation of the Intelligent Room, called Hal , after the computer in the movie, 2001: A space odyssey . 1 Hal supports a wider range of activities than the current Intelligent Room. <p> The room can also generate infrared remote control signals to access consumer electronics items (namely, objects that dont have serial ports). Room Control The Intelligent Room is controlled by a modular system of software agents known collectively as the Scatterbrain (described in detail in Coen <ref> [ 6] </ref>). The Scatterbrain consists of approximately 30 distinct, intercommunicating software agents that run on ten different networked workstations.
Reference: 7. <author> Coen, M. </author> <title> Towards Interactive Environments: The Intelligent Room (a short paper). </title> <booktitle> Proceedings of the 1997 Conference on Human Computer Interaction (HCI97). </booktitle> <address> Bristol, U.K. </address> <year> 1997. </year>
Reference: 8. <author> Coen, M.; Abowd, G.; Bobick, A.; Cooperstock, J.; and Horvitz, E. </author> <title> Call for Particip ation: </title> <booktitle> AAAI 1998 Spring Symposium on Intelligent Environments. </booktitle> <address> http://www.aaai.org/Symposia/Spring/1998/sssregistration-98.html#3. 1998. </address>
Reference-contexts: We discuss below the design of our laboratorys Intelligent Room and how experiences gained during its use have shaped the creation of its successor. Given the increasingly widespread interest in highly interactive, computational environments (Bobick et al. [ 4]), (Coen [6,7], Coen et al. <ref> [ 8] </ref>), (Cooperstock et al. [ 10]), ( Lucente et al. [ 19]), we hope these experiences will prove useful to other IE designers and implementers in the AI community. Some of the earliest work in this area has been done wholly outside the AI community. <p> A sample interaction would be a person pointing at a location on a displayed map and asking, Whats the weather here? (This scenario and its multimodal resolution are described in detail in Coen <ref> [ 8] </ref>.) Alternatively, the person can use a laser pointer to interact with the display from a distance. (See Figure 1.) Both of these pointing systems also allow displayed screen objects to be moved (i.e. dragged) or selected (i.e. clicked).
Reference: 9. <author> Coen, M and Thomas, K. </author> <title> Intelligent control of a speech recognition system. </title> <publisher> Forthcoming. </publisher>
Reference-contexts: We tuned DragonDictates performance by creating sets of specialized grammars for each room context and having the rooms software agent controller dynamically activate different subsets of grammars depending on the context of the activity in the Intelligent Room (Coen and Thomas <ref> [ 9] </ref>). This allows us to overcome the combinatorial increase in parsing time due to incorporating natural syntactic variability in the recognition grammars. For example, we can take advantage of the fact that someone is extremely unlikely to say, Pause the video clip, if there is no video playing.
Reference: 10. <author> Cooperstock, J; Fels, S.; Buxton, W. and Smith, K. </author> <title> Environments: Throwing Away Your Ke yboard and Mouse. </title> <note> To appear. Commmunications of the ACM . 1997. </note>
Reference-contexts: We discuss below the design of our laboratorys Intelligent Room and how experiences gained during its use have shaped the creation of its successor. Given the increasingly widespread interest in highly interactive, computational environments (Bobick et al. [ 4]), (Coen [6,7], Coen et al. [ 8]), (Cooperstock et al. <ref> [ 10] </ref>), ( Lucente et al. [ 19]), we hope these experiences will prove useful to other IE designers and implementers in the AI community. Some of the earliest work in this area has been done wholly outside the AI community. <p> Saund [ 24]. Interactions with objects more complex than paper are described in theory by Fitzmaurice et al. [15] and in practice by, for example, Rauterberg et al. [22]. Other substantial efforts towards highly interactive environments include an automated teleconferencing office (Cooperstock et al. <ref> [10] </ref>) and an immersive fictional theater (Bobick et al. [ 4]). Each of these projects makes use of embedded computation to enable unusual human-computer interactions, e.g., vision-based person trac king.
Reference: 11. <author> Dang, D. </author> <title> Template Based Gesture Recognition. </title> <type> SM Thesis. </type> <institution> Massachusetts Institute of Technology. </institution> <year> 1996. </year>
Reference-contexts: The primary contribution of this work was the idea of using the physical world as its own representation in a user-interface. The Intelligent Room has a desktop environment directly motivated by the DigitalDesk, which recognizes a wider range of complex hand gestures (Dang <ref> [ 11] </ref>). Much other work has built upon the DigitalDesk framework, e.g. Saund [ 24]. Interactions with objects more complex than paper are described in theory by Fitzmaurice et al. [15] and in practice by, for example, Rauterberg et al. [22]. <p> Interactive Table Through a ceiling mounted camera, the room can detect hand-pointing gestures and newly placed documents on the surface of the conference table. The gesture recognition system has been used to support a wide variety of functions (described in Dang <ref> [ 11] </ref>). The most interesting of these allows people to place Post-It notes on the surface of the table and via verbal interaction with the room, assign them particular functions, such as dimming the lights or announcing the current time.
Reference: 12. <author> Davis, J. and Bobick, A. </author> <title> The representation and recognition of action using temporal templates. </title> <booktitle> Proceedings Computer Vision and Pattern Recognition (CVPR97). </booktitle> <address> pp.928-934. </address> <year> 1997. </year>
Reference-contexts: Visual data from a single camera can provide (in theory and increasingly often in practice) far more information than simple sensing technologies. This includes the persons identity, position [ 13], gaze direction [ 26], facial expression [ 2,18], gesture [ 31], and activity <ref> [ 12] </ref>. While there has yet to be a coherent system that unifies all of these capabilities, several prototypes are currently under development. Furthermore, enhancing the capabilities of a computer vision system often requires modifying only the software algorithms that process incoming images, not adding new hardware.
Reference: 13. <author> DeBonet, J. </author> <title> Multiple Room Occupant Location and Identification.1996. </title> <address> http://www.ai.mit.edu/people/jsd/jsd.doit/Research/HCI/Tra cking_public </address>
Reference-contexts: Visual data from a single camera can provide (in theory and increasingly often in practice) far more information than simple sensing technologies. This includes the persons identity, position <ref> [ 13] </ref>, gaze direction [ 26], facial expression [ 2,18], gesture [ 31], and activity [ 12]. While there has yet to be a coherent system that unifies all of these capabilities, several prototypes are currently under development. <p> Room Vision Systems Person Tracking The Intelligent Room can track up to four people moving in the conference area of the room at up to 15Hz. The rooms person tracking system (described in DeBonet <ref> [13] </ref>) uses two wall-mounted cameras, each approximately 8 from the ground. (A debugging window from the system showing the view from one of the cameras is shown in We initially decided that incorporating a tracking system in the Intelligent Room was essential for a number of reasons.
Reference: 14. <author> Druin, A.; and Perlin, K. </author> <title> Immersive Environments: a physical approach to the computer interface. </title> <booktitle> Proceedings of the Conference on Human Factors in Computer Systems (CHI94), </booktitle> <pages> pages 325-326, </pages> <year> 1994. </year>
Reference: 15. <author> Fitzmaurice, G., Ishii, H., Buxton, W. Bricks: </author> <title> Laying the Foundations for Graspable User Interfaces. </title> <booktitle> Proceedings of the Conference on Human Factors in Computer Systems (CHI95). </booktitle> <address> p442-449. </address> <year> 1995. </year>
Reference-contexts: Much other work has built upon the DigitalDesk framework, e.g. Saund [ 24]. Interactions with objects more complex than paper are described in theory by Fitzmaurice et al. <ref> [15] </ref> and in practice by, for example, Rauterberg et al. [22]. Other substantial efforts towards highly interactive environments include an automated teleconferencing office (Cooperstock et al. [10]) and an immersive fictional theater (Bobick et al. [ 4]).
Reference: 16. <author> Katz, B. </author> <title> Using English for Indexing and Retrieving. </title> <booktitle> In Artificial Intelligence at MIT: Expanding Frontiers . Winston, </booktitle> <editor> P.; and Shellard, S. (editors). </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA. Volume 1. </address> <year> 1990. </year>
Reference-contexts: Bigram models, however, make it quite difficult to exclude particular statements from being erroneously recognized by the system and require that we heavily post process Summits output. This is performed primarily by the START nat ural-language information retrieval system (Katz <ref> [16] </ref>). DragonDictate is a commercially available system primarily used for discrete speech dictation, meaning that users must pause after each word. This, when coupled with its relatively low word accuracy, would be an intolerable speech interface to the room.
Reference: 17. <author> Kramer, J. </author> <title> Agent Based Personalized Information Retrieval. </title> <type> SM Thesis. </type> <institution> Massachusetts Institute of Technology. </institution> <year> 1997. </year>
Reference: 18. <author> Lien, J., Zlochower, A., Cohn, J., Li, C., and Kanade, T. </author> <title> Automatically Recognizing Facial Expressions in the Spatio-Temporal Domain. </title> <booktitle> Proceedings of the Workshop on Perceptual User Interfaces (PUI97). </booktitle> <address> Alberta, Canada. pp.94-97. </address> <year> 1997. </year>
Reference: 19. <author> Lucente, M.; Zwart, G.; George, A. </author> <title> Visualization Space: A Testbed for Deviceless Multimodal User Interface. </title> <note> To appear AAAI 1998 Spring Symposium on Intelligent Environments . 1998. </note>
Reference-contexts: Given the increasingly widespread interest in highly interactive, computational environments (Bobick et al. [ 4]), (Coen [6,7], Coen et al. [ 8]), (Cooperstock et al. [ 10]), ( Lucente et al. <ref> [ 19] </ref>), we hope these experiences will prove useful to other IE designers and implementers in the AI community. Some of the earliest work in this area has been done wholly outside the AI community. <p> The p otential for automated annotation in an intelligent environment is a promising area that deserves more exploration. Mozer <ref> [19] </ref> describes a house that automatically controls basic residential comfort systems, such as heating and ventilation, by observing patterns in its occupants behaviors. Having complex interactive environments learn via observation is likely to be essential to making them generally useful.
Reference: 20. <author> Mozer, M. </author> <title> The Neural Network House: An Environment that Adapts to its Inhabitants. </title> <note> To appear AAAI 1998 Spring Symposium on Intelligent Environments . 1998. </note>
Reference: 21. <author> Newman, W. and Wellner, P. </author> <title> A Desk Supporting Computer-based interaction with paper. </title> <booktitle> Proceedings of the Conference on Human Factors in Computing Systems (CHI92). </booktitle> <address> p587-592. </address> <year> 1992. </year>
Reference-contexts: Together, both approaches can go a long way towards making computation a natural part of everyday existence. Other related work The DigitalDesk project ( Wellner [ 30], Newman et al. <ref> [21] </ref>) was an early and influe ntial system that had a birds eye view of a desktop through an overhead video camera. It recognized and responded to predetermined hand gestures made by users while interacting with real paper documents on the surface of a desk.
Reference: 22. <author> Rauterberg, M., Fjeld, M., Krueger, H., Bichsel, M., Leonhardt, U., and Meier, M. BUILD-IT: </author> <title> a computer vision-based interaction technique for a planning tool. </title> <booktitle> Proceedings of the 1997 Conference on Human Computer Interaction (HCI97) . Bristol, </booktitle> <address> U.K. </address> <year> 1997. </year>
Reference-contexts: Much other work has built upon the DigitalDesk framework, e.g. Saund [ 24]. Interactions with objects more complex than paper are described in theory by Fitzmaurice et al. [15] and in practice by, for example, Rauterberg et al. <ref> [22] </ref>. Other substantial efforts towards highly interactive environments include an automated teleconferencing office (Cooperstock et al. [10]) and an immersive fictional theater (Bobick et al. [ 4]). Each of these projects makes use of embedded computation to enable unusual human-computer interactions, e.g., vision-based person trac king.
Reference: 23. <author> Renomeron, R., Rabinkin, D., French, J., and Flanagan, J. </author> <title> Smallscale matched filter array processing for spatially selective sound capture. </title> <booktitle> Proceedings of the 134 th Meeting of the Acoustical Society of America. </booktitle> <address> San Diego, CA. </address> <year> 1997. </year>
Reference-contexts: We are currently working on incorporating recent work in remote sound localization and capture ( Renomeron et al. <ref> [ 23] </ref>) to eliminate the need for every occupant to wear her own microphone. The Intelligent Room is capable of addressing users via the Festival Speech Synthesis System (Black et al. [ 3]).
Reference: 24. <author> Saund, E. </author> <title> Example Line Drawing Analysis for the ZombieBoard Diagrammatic User Interface. </title> <note> http://www.parc.xerox.com/spl/members/saund/lda-example/lda-example.html. 1996. </note>
Reference-contexts: The Intelligent Room has a desktop environment directly motivated by the DigitalDesk, which recognizes a wider range of complex hand gestures (Dang [ 11]). Much other work has built upon the DigitalDesk framework, e.g. Saund <ref> [ 24] </ref>. Interactions with objects more complex than paper are described in theory by Fitzmaurice et al. [15] and in practice by, for example, Rauterberg et al. [22].
Reference: 25. <author> Stauffer, C. </author> <title> Adaptive Manifolds for Object Classification. </title> <note> 1996. http://www.ai.mit.edu/people/stauffer/Projects/Manifold/ </note>
Reference-contexts: Our assumption is that occlusion of a chair provides evidence that someone is sitting in it, and this person can be located using prior knowledge of the chairs position. This system will use low dimension eigenspaces for approximating object manifolds under varying pose and lighting conditions (Stauffer <ref> [25] </ref>). The advantage to this approach is that the system need not be given in advance an explicit model of the chairs it will be locating. The system can construct object manifolds itself by having a user rotate any new types of chairs she brings inside. 6.
Reference: 26. <author> Stiefelhagen, R., Yang, J., and Waibel, A. </author> <title> Tracking Eyes and Monitoring Eye Gaze. </title> <booktitle> Proceedings of the Workshop on Perceptual User Interfaces (PUI97). </booktitle> <address> Alberta, Canada. pp.98-100. </address> <year> 1997. </year>
Reference-contexts: Visual data from a single camera can provide (in theory and increasingly often in practice) far more information than simple sensing technologies. This includes the persons identity, position [ 13], gaze direction <ref> [ 26] </ref>, facial expression [ 2,18], gesture [ 31], and activity [ 12]. While there has yet to be a coherent system that unifies all of these capabilities, several prototypes are currently under development.
Reference: 27. <author> Torrance, M. </author> <title> Advances in Human-Computer Interaction: The Intelligent Room. </title> <booktitle> Working Notes of the CHI 95 Research Symposium , May 6-7, </booktitle> <address> Denver, Colorado. </address> <year> 1995. </year>
Reference: 28. <author> Want, R.; Schilit, B.; Adams, N.; Gold, R.; Petersen, K.; Goldberg, D.; Ellis, J.; and Weiser, M. </author> <title> The ParcTab Ubiquitous Computing Experiment. </title> <note> Xerox Parc technical report. </note>
Reference-contexts: them or people wear infrared-emitting badges so they can be located in a building we want to enable unencumbered interaction with non-augmented, non-computational objects (like chairs) and to do so without requiring that people attach high-tech gad getry to their bodies (as opposed to the approach in Want et al. <ref> [28] </ref>). There are several motivations behind ou r approach. The first is that unlike Weiser [29], we make unabashed use of research from the AI community. Many AI research areas have achieved sufficient maturity to offer useful, standalone subsystems that can be incorporated into larger, general-purpose projects.
Reference: 29. <author> Weiser, M. </author> <booktitle> The Computer for the 21 st Century. </booktitle> <publisher> Scientific American . pp.94-100, </publisher> <month> September, </month> <year> 1991. </year>
Reference-contexts: There are several motivations behind ou r approach. The first is that unlike Weiser <ref> [29] </ref>, we make unabashed use of research from the AI community. Many AI research areas have achieved sufficient maturity to offer useful, standalone subsystems that can be incorporated into larger, general-purpose projects.
Reference: 30. <author> Wellner, P. </author> <title> The DigitalDesk Calculator: Tangible Manipulation on a Desk Top Display, </title> <booktitle> Proceedings of UIST91. </booktitle> <address> pp.27-33. </address> <year> 1991. </year>
Reference-contexts: Together, both approaches can go a long way towards making computation a natural part of everyday existence. Other related work The DigitalDesk project ( Wellner <ref> [ 30] </ref>, Newman et al. [21]) was an early and influe ntial system that had a birds eye view of a desktop through an overhead video camera. It recognized and responded to predetermined hand gestures made by users while interacting with real paper documents on the surface of a desk.
Reference: 31. <author> Wilson, A. and Bobick, A. </author> <title> Recognition and Interpretation of Parametric Gesture. </title> <booktitle> Su bmitted to International Conference on Computer Vision (ICCV98). </booktitle> <year> 1998. </year>
Reference-contexts: Visual data from a single camera can provide (in theory and increasingly often in practice) far more information than simple sensing technologies. This includes the persons identity, position [ 13], gaze direction [ 26], facial expression [ 2,18], gesture <ref> [ 31] </ref>, and activity [ 12]. While there has yet to be a coherent system that unifies all of these capabilities, several prototypes are currently under development.
Reference: 32. <author> Zue, V. </author> <title> Human Computer Interactions Using Language Based Technology. </title> <booktitle> IEEE Intern ational Symposium on Speech, Image Processing & Neural Networks. </booktitle> <address> Hong Kong. </address> <year> 1994 </year>
Reference-contexts: For processing spoken utterances, we use both the Summit ( Zue et al. <ref> [ 32] </ref>) and DragonDictate speech recognition systems in parallel. Each of these has different strengths and used together they have fairly robust performance. The Summit system recognizes continuous speech and is particularly adept at handling syntactic variability during recognition.
References-found: 32

