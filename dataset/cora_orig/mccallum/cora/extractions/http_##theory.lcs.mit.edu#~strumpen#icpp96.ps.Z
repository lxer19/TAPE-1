URL: http://theory.lcs.mit.edu/~strumpen/icpp96.ps.Z
Refering-URL: http://theory.lcs.mit.edu/~strumpen/
Root-URL: 
Email: strumpen@eng.uiowa.edu  
Title: Software-Based Communication Latency Hiding for Commodity Workstation Networks  
Author: Volker Strumpen 
Address: Iowa, Iowa City, IA 52242  
Affiliation: Department of Electrical and Computer Engineering, University of  
Abstract: A variety of latency hiding techniques has been investigated at the hardware level. However, except multithreading, which may require substantial program structuring effort, other software-based latency hiding methods have not been investigated. In this paper, we consider design alternatives for latency hiding other than multithreading. Furthermore, we present experimental evidence for the validity of a new technique for software-based communication latency hiding for commodity workstation networks: Up to 80 percent of useful computational power can be squeezed out of a workstation CPU while communicating with TCP/IP via an Ethernet and almost 90 percent while communicating across the Internet. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. La-zowska, and Henry M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1):53 79, </volume> <month> February </month> <year> 1992. </year>
Reference-contexts: Consequently, the kernel thread scheduler can switch to another kernel thread, if there is no network activity. For such-the-like purposes scheduler activations <ref> [1] </ref> have been proposed. Compared to the two-process setting, kernel threads avoid the crossing of the process boundary and thus the overhead of message copying. However, similar to the two-process setting the user-kernel boundary needs to be crossed for both message copying and thread scheduling.
Reference: [2] <author> D. Banks and M. Prudence. </author> <title> A high-performance network architecture for a PA-RISC workstation. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2):191202, </volume> <month> Febru-ary </month> <year> 1993. </year>
Reference-contexts: Current research is emerging to level off the differences between multiprocessors and workstation networks. Research prototypes such as *T [21] or HPAM [18] based on the Medusa host-network interface <ref> [2] </ref> provide hardware support for efficient latency hiding implementations. However, since current commodity workstations and commodity networks do not offer architectural support for communication latency hiding, we consider software-based latency hiding techniques.
Reference: [3] <author> R. D. Blumofe and D. S. Park. </author> <title> Scheduling large-scale par-allel computations on networks of workstations. </title> <booktitle> In 3rd International Symposium on High-Performance Distributed Computing, </booktitle> <pages> pages 96105, </pages> <address> San Francisco, California, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: However, the UDP based communication protocol of the networking implementation does not overlap communication with useful computation. Cilk [4] focuses on efficient thread scheduling. Its main contribution is a work-stealing scheduler fulfilling theoretical efficiency criteria that are also matched in practice. The network implementation is based on Phish <ref> [3] </ref>, a predecessor of Cilk. Phish implements split-phase operations based on UDP. This scheme allows for useful computation between a communication request and the corresponding re ply, but not for overlapping the communication itself.
Reference: [4] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kusz-maul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages 207216, </booktitle> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Among these are generic models such as active messages [27], and those aiming at better performance and/or easing parallel programming by means of message-driven execution models. More closely related to our approach of communication latency hiding are Charm [14] and Cilk <ref> [4] </ref>. Charm [14] introduced the message-driven programming style. It provides automatic mapping and scheduling. So-called chares are programmed by providing entry points for messages. Messages are received by the Chare kernel and buffered in a message queue. <p> Message reception and sending use explicit buffering to provide non-blocking communication. However, the UDP based communication protocol of the networking implementation does not overlap communication with useful computation. Cilk <ref> [4] </ref> focuses on efficient thread scheduling. Its main contribution is a work-stealing scheduler fulfilling theoretical efficiency criteria that are also matched in practice. The network implementation is based on Phish [3], a predecessor of Cilk. Phish implements split-phase operations based on UDP.
Reference: [5] <author> B. Boothe and A. Ranade. </author> <title> Improved multithreading techniques for hiding communication latency in multiprocessors. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 214223, </pages> <address> Queensland, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Multithreading is now widely accepted as a means for hiding latencies, not only for memory load operations but also for hiding interprocessor communication latencies <ref> [5, 7, 12, 13, 15] </ref>. For multicomputers, multithreading and asynchronous message passing, such as offered by Intel's NX interface [22], are widely used latency hiding mechanisms. Nevertheless, they require careful structuring of parallel programs. With multithreading the programmer must identify independent threads of control that can be executed concurrently.
Reference: [6] <author> A. W. Burks, H. H. Goldstine, and J. von Neumann. </author> <title> Preliminary discussion of the logical design of an electronic computing instrument. </title> <editor> In C. G. Bell and A. Newell, editors, </editor> <booktitle> Computer Structures: Readings and Examples, chapter 4, </booktitle> <pages> pages 92119. </pages> <publisher> McGraw-Hill, </publisher> <year> 1971. </year> <note> This article is published originally as a report prepared for the U.S. </note> <institution> Army Ordnance Department, </institution> <year> 1946, </year> <editor> and in A. H. Traub, editor, Collected Works of John von Neumann, </editor> <volume> volume 5, </volume> <pages> pages 3479, </pages> <publisher> The Macmillan Company, </publisher> <address> New York, </address> <year> 1963. </year>
Reference-contexts: As far back as 1946, Arthur Burks, Herman Goldstine and John von Neumann wrote <ref> [6] </ref>: Using Ac [Accumulator] for this purpose eliminates the possibility of computing and reading from or writing on the wires simultaneously. However, simultaneous operation of the computer and the input-output organ requires temporary storage and introduces a synchronizing problem, and hence it is not being considered for the first model.
Reference: [7] <author> Edward W. Felten and Dylan McNamee. </author> <title> Improving the performance of message-passing applications by multithread-ing. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <pages> pages 8489, </pages> <address> Williamsburg, Virginia, </address> <month> April </month> <year> 1992. </year> <note> IEEE. </note>
Reference-contexts: Multithreading is now widely accepted as a means for hiding latencies, not only for memory load operations but also for hiding interprocessor communication latencies <ref> [5, 7, 12, 13, 15] </ref>. For multicomputers, multithreading and asynchronous message passing, such as offered by Intel's NX interface [22], are widely used latency hiding mechanisms. Nevertheless, they require careful structuring of parallel programs. With multithreading the programmer must identify independent threads of control that can be executed concurrently.
Reference: [8] <author> V. W. Freeh, D. K. Lowenthal, and G. R. Andrews. </author> <title> Distributed Filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 201 213, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: The network implementation is based on Phish [3], a predecessor of Cilk. Phish implements split-phase operations based on UDP. This scheme allows for useful computation between a communication request and the corresponding re ply, but not for overlapping the communication itself. The Distributed Filaments <ref> [8] </ref> approach applies multi-threading for overlapping communication of pages and calculation in a user-level distributed shared memory system. This implementation is based on a split-phase operation to handle a remote page fault. <p> This implementation is based on a split-phase operation to handle a remote page fault. After a request is sent off the local scheduler switches to another thread while waiting for the requested page. Like Charm and Phish, distributed filaments implements a reliable transport protocol on top of UDP. In <ref> [8] </ref>, results of overlapping communication and computation are reported. The gain of 21 % is relatively low compared to the numbers presented in the paper at hand.
Reference: [9] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM: Parallel Virtual Machine A Users' Guide and Tutorial for Networked Parallel Computing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1994. </year>
Reference-contexts: In fact, multithreading is used as an implementation technique for the designs shown in Figure 1 (b) and 1 (c). Two-process setting: Besides the application process, a separate process handles communication across the network. This two-process setting is implemented in PVM <ref> [9] </ref>, where the PVM daemon pvmd is responsible for communication. Scheduling of the application process and communication process is done by the op erating system's scheduler. Kernel threads: The application process consists of one (or more) calculation thread (s) (calc) and a communication thread (com).
Reference: [10] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 245257, </pages> <address> Santa Clara, California, </address> <month> April </month> <year> 1991. </year> <note> ACM. </note>
Reference-contexts: Among the trend-setting hardware designs are the following developments: The HEP architecture [23] introduced multithreading to increase processor utilization. Later, memory latency hiding techniques gained popularity when investigated for distributed shared memory architectures: Prefetching [11], coherent caches [11] and relaxed memory consistency <ref> [10] </ref> are known mainly from Stanford's DASH project [17]. The third approach hides communication latencies by means of additional communication hardware (processors) in distributed memory systems such as *T [21] or the Intel Paragon.
Reference: [11] <author> Anoop Gupta, John L. Hennessy, Kourosh Gharachorloo, T. Mowry, and W. D. Weber. </author> <title> Computative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254263, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Among the trend-setting hardware designs are the following developments: The HEP architecture [23] introduced multithreading to increase processor utilization. Later, memory latency hiding techniques gained popularity when investigated for distributed shared memory architectures: Prefetching <ref> [11] </ref>, coherent caches [11] and relaxed memory consistency [10] are known mainly from Stanford's DASH project [17]. The third approach hides communication latencies by means of additional communication hardware (processors) in distributed memory systems such as *T [21] or the Intel Paragon. <p> Among the trend-setting hardware designs are the following developments: The HEP architecture [23] introduced multithreading to increase processor utilization. Later, memory latency hiding techniques gained popularity when investigated for distributed shared memory architectures: Prefetching <ref> [11] </ref>, coherent caches [11] and relaxed memory consistency [10] are known mainly from Stanford's DASH project [17]. The third approach hides communication latencies by means of additional communication hardware (processors) in distributed memory systems such as *T [21] or the Intel Paragon.
Reference: [12] <author> Matthew Haines and Wim Bohm. </author> <title> An evaluation of software multithreading in a conventional distributed memory multiprocessor. </title> <booktitle> In 5th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 106113, </pages> <address> Dallas, Texas, </address> <month> De-cember </month> <year> 1993. </year>
Reference-contexts: Multithreading is now widely accepted as a means for hiding latencies, not only for memory load operations but also for hiding interprocessor communication latencies <ref> [5, 7, 12, 13, 15] </ref>. For multicomputers, multithreading and asynchronous message passing, such as offered by Intel's NX interface [22], are widely used latency hiding mechanisms. Nevertheless, they require careful structuring of parallel programs. With multithreading the programmer must identify independent threads of control that can be executed concurrently.
Reference: [13] <author> John Holm, Antonio Lain, and Prithviraj Banerjee. </author> <title> Compilation of scientific programs into multithreaded and message driven computation. </title> <booktitle> In Scalable High-Performance Computing Conference, </booktitle> <pages> pages 518525, </pages> <address> Knoxville, Ten-nessee, </address> <month> May </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: Multithreading is now widely accepted as a means for hiding latencies, not only for memory load operations but also for hiding interprocessor communication latencies <ref> [5, 7, 12, 13, 15] </ref>. For multicomputers, multithreading and asynchronous message passing, such as offered by Intel's NX interface [22], are widely used latency hiding mechanisms. Nevertheless, they require careful structuring of parallel programs. With multithreading the programmer must identify independent threads of control that can be executed concurrently.
Reference: [14] <author> Laxmikant V. Kale. </author> <title> The Chare kernel parallel programming language and system. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 1725. </pages> <publisher> IEEE, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Among these are generic models such as active messages [27], and those aiming at better performance and/or easing parallel programming by means of message-driven execution models. More closely related to our approach of communication latency hiding are Charm <ref> [14] </ref> and Cilk [4]. Charm [14] introduced the message-driven programming style. It provides automatic mapping and scheduling. So-called chares are programmed by providing entry points for messages. Messages are received by the Chare kernel and buffered in a message queue. <p> Among these are generic models such as active messages [27], and those aiming at better performance and/or easing parallel programming by means of message-driven execution models. More closely related to our approach of communication latency hiding are Charm <ref> [14] </ref> and Cilk [4]. Charm [14] introduced the message-driven programming style. It provides automatic mapping and scheduling. So-called chares are programmed by providing entry points for messages. Messages are received by the Chare kernel and buffered in a message queue.
Reference: [15] <author> K. Kurihara, D. Chaiken, and A. Agarwal. </author> <title> Latency tolerance through multithreading in large-scale multiprocessors. </title> <booktitle> In International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pages 91101, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Multithreading is now widely accepted as a means for hiding latencies, not only for memory load operations but also for hiding interprocessor communication latencies <ref> [5, 7, 12, 13, 15] </ref>. For multicomputers, multithreading and asynchronous message passing, such as offered by Intel's NX interface [22], are widely used latency hiding mechanisms. Nevertheless, they require careful structuring of parallel programs. With multithreading the programmer must identify independent threads of control that can be executed concurrently.
Reference: [16] <author> C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh. </author> <title> Basic linear algebra subprograms for Fortran usage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5(3):308 323, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Application performance is reported in [24, 25]. code and simultaneously exchange two messages of equal size. The communication thread's protocol ensures that this is possible without deadlock. We measure the amount of useful computation that is done within the overlap region. The BLAS daxpy (~y = a~x + ~y) <ref> [16] </ref> has been chosen as a well known computational kernel to benchmark the CPU power available for useful computation. It is applied to double precision vectors of length 1000. The daxpy computation is scheduled within a while-loop that is terminated by means of a message continuation [24].
Reference: [17] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica S. Lam. </author> <title> The Stanford Dash multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3):6379, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: Later, memory latency hiding techniques gained popularity when investigated for distributed shared memory architectures: Prefetching [11], coherent caches [11] and relaxed memory consistency [10] are known mainly from Stanford's DASH project <ref> [17] </ref>. The third approach hides communication latencies by means of additional communication hardware (processors) in distributed memory systems such as *T [21] or the Intel Paragon.
Reference: [18] <author> Richard P. Martin. HPAM: </author> <title> An active message layer for a network of HP workstations. In Hot Interconnects II, </title> <year> 1994. </year>
Reference-contexts: Furthermore, clusters are usually shared by users, and the unpredictable changes of CPU load and network load complicate their efficient utilization for distributed parallel computing. Current research is emerging to level off the differences between multiprocessors and workstation networks. Research prototypes such as *T [21] or HPAM <ref> [18] </ref> based on the Medusa host-network interface [2] provide hardware support for efficient latency hiding implementations. However, since current commodity workstations and commodity networks do not offer architectural support for communication latency hiding, we consider software-based latency hiding techniques.
Reference: [19] <author> Ron Minnich, Dan Burns, and Frank Hady. </author> <title> The memory-integrated network interface. </title> <journal> IEEE Micro, </journal> <volume> 15(1):1120, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: However, for fu-ture high-performance networks, lifting the communication protocol into user-space is the natural step towards delivering the bandwidth of such networks <ref> [19, 26] </ref>. 3 Implementation Issues This section describes the design of a user-level implementation for communication latency hiding. Since a single-threaded application is a special case of a multithreaded application with respect to our scheduling policy, multi-threaded applications are treated as the general case.
Reference: [20] <institution> MIPS Technologies, Inc. </institution> <note> R10000 Microprocessor User's Manual, 2nd edition, </note> <month> June </month> <year> 1995. </year>
Reference-contexts: The third approach hides communication latencies by means of additional communication hardware (processors) in distributed memory systems such as *T [21] or the Intel Paragon. Now, processor designers are introducing non--blocking caches together with dynamic instruction scheduling in next-generation microprocessors such as the MIPS R10000 <ref> [20] </ref> to overcome the penalty of cache misses. Multithreading has not only been investigated at the hardware but also at the software level, in particular for distributed memory systems.
Reference: [21] <author> G. M. Papadopoulos, R. Greiner, and M. J. Beckerle. </author> <title> *T: Integrated building blocks for parallel computing. </title> <booktitle> In Supercomputing'93, </booktitle> <pages> pages 624635, </pages> <address> Portland, Oregon, </address> <month> Novem-ber </month> <year> 1993. </year>
Reference-contexts: Furthermore, clusters are usually shared by users, and the unpredictable changes of CPU load and network load complicate their efficient utilization for distributed parallel computing. Current research is emerging to level off the differences between multiprocessors and workstation networks. Research prototypes such as *T <ref> [21] </ref> or HPAM [18] based on the Medusa host-network interface [2] provide hardware support for efficient latency hiding implementations. However, since current commodity workstations and commodity networks do not offer architectural support for communication latency hiding, we consider software-based latency hiding techniques. <p> The third approach hides communication latencies by means of additional communication hardware (processors) in distributed memory systems such as *T <ref> [21] </ref> or the Intel Paragon. Now, processor designers are introducing non--blocking caches together with dynamic instruction scheduling in next-generation microprocessors such as the MIPS R10000 [20] to overcome the penalty of cache misses.
Reference: [22] <author> P. Pierce. </author> <title> The NX Message Passing Interface. </title> <booktitle> Parallel Computing, </booktitle> <address> 20(4):463480, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Multithreading is now widely accepted as a means for hiding latencies, not only for memory load operations but also for hiding interprocessor communication latencies [5, 7, 12, 13, 15]. For multicomputers, multithreading and asynchronous message passing, such as offered by Intel's NX interface <ref> [22] </ref>, are widely used latency hiding mechanisms. Nevertheless, they require careful structuring of parallel programs. With multithreading the programmer must identify independent threads of control that can be executed concurrently.
Reference: [23] <author> Burton J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> In Proceedings of SPIE: Real-Time Signal Processing IV, </booktitle> <pages> pages 241248, </pages> <address> San Diego, California, </address> <month> August </month> <year> 1981. </year> <note> Volume 298. </note>
Reference-contexts: Recently, when the goal of scalable parallel computing arose, latency hiding became a topic of wide-spread interest. 2.1 Latency Hiding Techniques Different designs for latency hiding have been considered at the hardware and software level. Among the trend-setting hardware designs are the following developments: The HEP architecture <ref> [23] </ref> introduced multithreading to increase processor utilization. Later, memory latency hiding techniques gained popularity when investigated for distributed shared memory architectures: Prefetching [11], coherent caches [11] and relaxed memory consistency [10] are known mainly from Stanford's DASH project [17].
Reference: [24] <author> Volker Strumpen. </author> <title> The Network Machine. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Swiss Federal Institute of Technoloy (ETH) Zurich, </institution> <year> 1995. </year>
Reference-contexts: Our benchmark avoids the problems of intermixing the application specific and system specific effects and discloses the latency hiding potential of the underlying system. Application performance is reported in <ref> [24, 25] </ref>. code and simultaneously exchange two messages of equal size. The communication thread's protocol ensures that this is possible without deadlock. We measure the amount of useful computation that is done within the overlap region. <p> It is applied to double precision vectors of length 1000. The daxpy computation is scheduled within a while-loop that is terminated by means of a message continuation <ref> [24] </ref>. A message continuation is a computation that is executed as soon as the corresponding message transfer is completed. The resulting count of daxpy executions is used as the measure of useful CPU utilization overlapping the communication. <p> In the faster Ethernet, 0.2 to 0.3 percent of the multiplication can be completed. These numbers can be used to tune the partitioning of an application and optimize the number of processors used <ref> [24] </ref>. 5 Related Work The results presented here outperform those of a previous implementation of a latency hiding protocol of the author described in [25]. That work is based on a two-process setting, one acting as a communication process, the other as calculation process.
Reference: [25] <author> Volker Strumpen and Thomas L. Casavant. </author> <title> Implementing Communication Latency Hiding in High-Latency Computer Networks. </title> <booktitle> In High-Performance Computing and Networking, </booktitle> <volume> LNCS 919, </volume> <pages> pages 8693. </pages> <publisher> Springer-Verlag, </publisher> <address> Mi-lano, Italy, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: The original PVM implementation, based on UDP, exhibits one advantage: The operating system's scheduler schedules application process and pvmd. Apparently, this avoids the need for a scheduler that handles the interleaving of these two processes. The author implemented a communication module <ref> [25] </ref> that exploits the kernel scheduler. To avoid the overhead of copying messages between the application process and communication process (pvmd in Figure 1 (a)), messages are exchanged via shared memory segments. This implementation delivered reasonable CPU utilization when communicating via the Internet [25]. <p> The author implemented a communication module <ref> [25] </ref> that exploits the kernel scheduler. To avoid the overhead of copying messages between the application process and communication process (pvmd in Figure 1 (a)), messages are exchanged via shared memory segments. This implementation delivered reasonable CPU utilization when communicating via the Internet [25]. However, for faster networks such as an Ethernet, the time slices of the kernel scheduler are too long to allow for effective interleaving of application process and communication process. <p> Our benchmark avoids the problems of intermixing the application specific and system specific effects and discloses the latency hiding potential of the underlying system. Application performance is reported in <ref> [24, 25] </ref>. code and simultaneously exchange two messages of equal size. The communication thread's protocol ensures that this is possible without deadlock. We measure the amount of useful computation that is done within the overlap region. <p> These numbers can be used to tune the partitioning of an application and optimize the number of processors used [24]. 5 Related Work The results presented here outperform those of a previous implementation of a latency hiding protocol of the author described in <ref> [25] </ref>. That work is based on a two-process setting, one acting as a communication process, the other as calculation process. Messages are exchanged between these processes via shared memory and transferred across processors by means of the communication processes. This scheme is relatively simple to implement.
Reference: [26] <author> T. von Eicken, V. Avula, A. Basu, and V. </author> <title> Buch. Low-latency communication over ATM networks using active messages. </title> <journal> IEEE Micro, </journal> <volume> 15(1):4653, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: However, for fu-ture high-performance networks, lifting the communication protocol into user-space is the natural step towards delivering the bandwidth of such networks <ref> [19, 26] </ref>. 3 Implementation Issues This section describes the design of a user-level implementation for communication latency hiding. Since a single-threaded application is a special case of a multithreaded application with respect to our scheduling policy, multi-threaded applications are treated as the general case.
Reference: [27] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256266, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Several projects have been emerging recently to ameliorate the problems of traditional message passing style programming. Among these are generic models such as active messages <ref> [27] </ref>, and those aiming at better performance and/or easing parallel programming by means of message-driven execution models. More closely related to our approach of communication latency hiding are Charm [14] and Cilk [4]. Charm [14] introduced the message-driven programming style. It provides automatic mapping and scheduling.
References-found: 27

