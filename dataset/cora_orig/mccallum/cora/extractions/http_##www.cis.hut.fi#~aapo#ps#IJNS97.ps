URL: http://www.cis.hut.fi/~aapo/ps/IJNS97.ps
Refering-URL: http://www.cis.hut.fi/~oja/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: aapo.hyvarinen@hut.fi, erkki.oja@hut.fi  
Title: Simple Neuron Models for Independent Component Analysis  
Author: Aapo Hyvrinen and Erkki Oja 
Date: February 19, 1997  
Note: To appear in Int. J. of Neural Systems  
Web: http://nucleus.hut.fi/flaapo, floja  
Address: Rakentajanaukio 2C, FIN-02150 Espoo, Finland  
Affiliation: Helsinki University of Technology Laboratory of Computer and Information Science  
Abstract: Recently, several neural algorithms have been introduced for Independent Component Analysis. Here we approach the problem from the point of view of a single neuron. First, simple Hebbian-like learning rules are introduced for estimating one of the independent components from sphered data. Some of the learning rules can be used to estimate an independent component which has a negative kurtosis, and the others estimate a component of positive kurtosis. Next, a two-unit system is introduced to estimate an independent component of any kurtosis. The results are then generalized to estimate independent components from non-sphered (raw) mixtures. To separate several independent components, a system of several neurons with linear negative feedback is used. The convergence of the learning rules is rigorously proven without any unnecessary hypotheses on the distributions of the independent components.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bell, A. and Sejnowski, T. </author> <year> (1995). </year> <title> An information-maximization approach to blind separation and blind deconvolution. </title> <booktitle> Neural Computation, </booktitle> <address> 7:1129 1159. </address>
Reference: <author> Bell, A. and Sejnowski, T. </author> <year> (1997). </year> <title> The 'independent components' of natural scenes are edge tlters. </title> <journal> Vision Research. </journal> <note> To appear. </note>
Reference: <author> Cardoso, J.-F. and Laheld, B. H. </author> <year> (1996). </year> <title> Equivariant adaptive source separation. </title> <journal> IEEE Trans. on Signal Processing, 44(12):30173030. </journal>
Reference: <author> Comon, P. </author> <year> (1994). </year> <title> Independent component analysis a new concept? Signal Processing, </title> <publisher> 36:287314. </publisher>
Reference-contexts: The main applications are in blind source separation, feature extraction, and in a slightly modited form, in blind deconvolution. In the simplest form of ICA <ref> (Comon, 1994) </ref>, we observe m scalar random variables v 1 ; v 2 ; :::; v m which are linear combinations of n unknown independent components s 1 ; s 2 ; :::; s n , assumed mutually statistically independent, and zero-mean. In addition, we 1 must assume n m.
Reference: <author> Delfosse, N. and Loubaton, P. </author> <year> (1995). </year> <title> Adaptive blind separation of independent sources: a deation approach. Signal Processing, </title> <publisher> 45:5983. </publisher>
Reference-contexts: Then we can use a second unit to simultaneously estimate the sign of the kurtosis of the output of the trst unit <ref> (Delfosse and Loubaton, 1995) </ref>. Thus, we get a two-unit system that separates an independent component of any (non-zero) kurtosis.
Reference: <author> Donoho, D. </author> <year> (1981). </year> <title> On minimum entropy deconvolution. </title> <booktitle> In Applied Time Series Analysis II, </booktitle> <pages> pages 565608. </pages> <publisher> Academic Press. </publisher>
Reference: <author> Hurri, J., Hyvrinen, A., Karhunen, J., and Oja, E. </author> <year> (1996). </year> <title> Image feature extraction using independent component analysis. </title> <booktitle> In Proc. NORSIG'96, </booktitle> <pages> pages 475478, </pages> <address> Espoo, Finland. </address>
Reference: <author> Hyvrinen, A. </author> <year> (1996a). </year> <title> Purely local neural principal component and independent component learning. </title> <booktitle> In Proc. Int. Conf. on Artitcial Neural Networks, </booktitle> <pages> pages 139144, </pages> <address> Bochum, Germany. </address>
Reference-contexts: The basic methods presented above admit several extensions. First, the issue of locality may be important in some applications and in the case of biologically inspired modelling. The rather non-local, but computationally simple feedback used above can be replaced by a more local kind of feedback <ref> (Hyvrinen, 1996a) </ref>. Second, instead of the simple linear mixtures as in eq. (1), we can also consider more general, non-linear mixtures. In (Pajunen et al., 1996), the Self-Organizing Map was used for non-linear blind source separation for sources of negative kurtosis.
Reference: <author> Hyvrinen, A. </author> <year> (1996b). </year> <title> Simple one-unit neural algorithms for blind source separation and blind deconvolution. </title> <booktitle> In Proc. Int. Conf. on Neural Information Processing, </booktitle> <pages> pages 12011206, </pages> <address> Hong Kong. 18 Hyvrinen, </address> <publisher> A. </publisher> <year> (1997a). </year> <title> A family of txed-point algorithms for independent component analysis. </title> <booktitle> In Proc. ICASSP'97, </booktitle> <pages> pages 39173920, </pages> <address> Munich, Ger-many. </address>
Reference: <author> Hyvrinen, A. </author> <year> (1997b). </year> <title> One-unit contrast functions for independent component analysis: A statistical analysis. </title> <note> Submitted. </note>
Reference-contexts: Fourth, a large family of contrast functions was introduced in (Hyvrinen, 1997a). These contrast functions may be used instead of kurtosis in the learning rules (Hyvrinen, 1997a; Hyvrinen and Oja, 1996a), thus obtaining estimators that often have better statistical properties than those using kurtosis <ref> (Hyvrinen, 1997b) </ref>.
Reference: <author> Hyvrinen, A. and Oja, E. </author> <year> (1996a). </year> <title> Independent component analysis by general non-linear hebbian-like learning rules. </title> <type> Technical Report A41, </type> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science. </institution> <note> Submitted to a journal. </note>
Reference-contexts: The basic methods presented above admit several extensions. First, the issue of locality may be important in some applications and in the case of biologically inspired modelling. The rather non-local, but computationally simple feedback used above can be replaced by a more local kind of feedback <ref> (Hyvrinen, 1996a) </ref>. Second, instead of the simple linear mixtures as in eq. (1), we can also consider more general, non-linear mixtures. In (Pajunen et al., 1996), the Self-Organizing Map was used for non-linear blind source separation for sources of negative kurtosis.
Reference: <author> Hyvrinen, A. and Oja, E. </author> <year> (1996b). </year> <title> A neuron that learns to separate one independent component from linear mixtures. </title> <booktitle> In Proc. IEEE Int. Conf. on Neural Networks, </booktitle> <pages> pages 6267, </pages> <address> Washington, D.C. </address>
Reference: <author> Hyvrinen, A. and Oja, E. </author> <year> (1996c). </year> <title> One-unit learning rules for independent component analysis. </title> <booktitle> In NIPS*96, </booktitle> <address> Denver, Colorado. </address>
Reference: <author> Hyvrinen, A. and Oja, E. </author> <year> (1997). </year> <title> A fast txed-point algorithm for independent component analysis. Neural Computation. </title> <note> To appear. </note>
Reference-contexts: In (Pajunen et al., 1996), the Self-Organizing Map was used for non-linear blind source separation for sources of negative kurtosis. Third, the computations needed in these neural learning rules can also be performed in a computationally attractive way using a txed-point algorithm, developed in <ref> (Hyvrinen and Oja, 1997) </ref>. Fourth, a large family of contrast functions was introduced in (Hyvrinen, 1997a). These contrast functions may be used instead of kurtosis in the learning rules (Hyvrinen, 1997a; Hyvrinen and Oja, 1996a), thus obtaining estimators that often have better statistical properties than those using kurtosis (Hyvrinen, 1997b).
Reference: <author> Jutten, C. and Herault, J. </author> <year> (1991). </year> <title> Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture. Signal Processing, </title> <publisher> 24:110. </publisher>
Reference-contexts: For mathematical convenience, we detne here that the independent components s i have unit variance. This makes the (non-Gaussian) independent components unique, up to their signs. Note that no order is detned between the independent components. In blind source separation <ref> (Jutten and Herault, 1991) </ref>, the observed values of v correspond to a realization of an m-dimensional discrete-time signal v (t), t = 1; 2; :::. Then the components s i (t) are called source signals, which are usually original, uncorrupted signals or noise sources.
Reference: <author> Karhunen, J., Hyvrinen, A., Vigario, R., Hurri, J., and Oja, E. </author> <year> (1997a). </year> <title> Applications of neural blind separation to signal and image processing. </title> <booktitle> In Proc. ICASSP'97, </booktitle> <pages> pages 131134, </pages> <address> Munich, Germany. </address>
Reference: <author> Karhunen, J., Oja, E., Wang, L., Vigario, R., and Joutsensalo, J. </author> <year> (1997b). </year> <title> A class of neural networks for independent component analysis. </title> <journal> IEEE Trans. on Neural Networks. </journal> <note> To appear. </note>
Reference-contexts: allows us to calculate one (original) ICA basis vector, and to separate one independent component: s i = b T a i = M 1 b i Methods for sphering the data and for calculating the ICA basis vectors from the transformed ones by a neural network are presented in <ref> (Karhunen et al., 1997b) </ref>. It is also worthwhile to reect why sphering alone does not solve the separation problem. <p> Of course, if N = 0, this is the general one-unit algorithm of Section 3. The algorithm can be used for tnd-ing all the ICA basis vectors in a hierarchal network <ref> (Karhunen et al., 1997b) </ref>, where the sum on the right-most term is over the neurons that are higher in the hierarchy.
Reference: <author> Kendall, M. and Stuart, A. </author> <year> (1958). </year> <title> The Advanced Theory of Statistics. </title> <publisher> Charles Grin & Company. </publisher>
Reference: <author> Kushner, H. and Clark, D. </author> <year> (1978). </year> <title> Stochastic approximation methods for constrained and unconstrained systems. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Malouche, Z. and Macchi, O. </author> <year> (1996). </year> <title> Extended anti-Hebbian adaptation for unsupervised source extraction. </title> <booktitle> In Proc. ICASSP'97, </booktitle> <pages> pages 16641667, </pages> <address> Atlanta, Georgia. </address>
Reference: <author> Oja, E. </author> <year> (1992). </year> <title> Principal components, minor components, and linear neural networks. Neural Networks, </title> <publisher> 5:927935. </publisher>
Reference-contexts: To achieve this, it is simply enough to take the sum in the last term over all the neurons in the network. Then we obtain a system that resembles a non-linear version of the symmetric PCA subspace rule <ref> (Oja, 1992) </ref>. Such an algorithm is interesting because the computations can be considered more parallel than the computations in a hierarchical network.
Reference: <author> Oja, E. and Karhunen, J. </author> <year> (1985). </year> <title> On stochastic approximation of the eigen-vectors and eigenvalues of the expectation of a random matrix. </title> <journal> Journal of Math. Analysis and Applications, 106:6984. </journal>
Reference-contexts: Then we will be able to extract all the independent components. For non-sphered data, this idea is generalized by introducing a feedback that decorrelates the outputs of the neurons. 13 5.1 Parallel Networks for Sphered Data In some on-line PCA learning rules like the SGA algorithm <ref> (Oja and Karhunen, 1985) </ref> and the GHA algorithm (Sanger, 1989), a special hierarchical orthogonal-izing feedback term was used, similar to deation-type numerical algorithms for solving the eigenvectors of the covariance matrix. The same idea carries over to the more complicated ICA learning rules.
Reference: <author> Pajunen, P., Hyvrinen, A., and Karhunen, J. </author> <year> (1996). </year> <title> Nonlinear blind source separation by self-organizing maps. </title> <booktitle> In Proc. Int. Conf. on Neural Information Processing, </booktitle> <pages> pages 12071210, </pages> <institution> Hong Kong. 19 Sanger, T. </institution> <year> (1989). </year> <title> Optimal unsupervised learning in a single-layered linear feedforward network. Neural Networks, </title> <publisher> 2:459473. </publisher>
Reference-contexts: The rather non-local, but computationally simple feedback used above can be replaced by a more local kind of feedback (Hyvrinen, 1996a). Second, instead of the simple linear mixtures as in eq. (1), we can also consider more general, non-linear mixtures. In <ref> (Pajunen et al., 1996) </ref>, the Self-Organizing Map was used for non-linear blind source separation for sources of negative kurtosis. Third, the computations needed in these neural learning rules can also be performed in a computationally attractive way using a txed-point algorithm, developed in (Hyvrinen and Oja, 1997).
Reference: <author> Shalvi, O. and Weinstein, E. </author> <year> (1990). </year> <title> New criteria for blind deconvolution of nonminimum phase systems (channels). </title> <journal> IEEE Trans. on Information Theory, 36(2):312321. </journal>
Reference: <author> Wang, L., Karhunen, J., and Oja, E. </author> <year> (1995). </year> <title> A bigradient optimization ap proach for robust PCA, MCA, and source separation. </title> <booktitle> In Proc. IEEE Int. Conf on Neural Networks '95, </booktitle> <pages> pages 16841689, </pages> <address> Perth, Australia. </address>
References-found: 25

