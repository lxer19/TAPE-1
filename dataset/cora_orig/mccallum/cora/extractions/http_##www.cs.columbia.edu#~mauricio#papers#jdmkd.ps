URL: http://www.cs.columbia.edu/~mauricio/papers/jdmkd.ps
Refering-URL: http://www.cs.columbia.edu/~mauricio/
Root-URL: http://www.cs.columbia.edu
Title: Data Mining and Knowledge Discovery,  Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem  
Author: MAURICIO A. HERN ANDEZ AND SALVATORE J. STOLFO 
Keyword: data cleaning, data cleansing, duplicate elimination, semantic integration  
Address: New York, NY 10027  
Affiliation: Department of Computer Science, Columbia University,  
Note: c 1998 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  Editor: Usama Fayyad  
Pubnum: 2,  
Email: fmauricio,salg@cs.columbia.edu  
Date: 1-31 (1998)  Received November 25, 1995; Revised March 30, 1996  
Abstract: The problem of merging multiple databases of information about common entities is frequently encountered in KDD and decision support applications in large commercial and government organizations. The problem we study is often called the Merge/Purge problem and is difficult to solve both in scale and accuracy. Large repositories of data typically have numerous duplicate information entries about the same entities that are difficult to cull together without an intelligent "equational theory" that identifies equivalent items by a complex, domain-dependent matching process. We have developed a system for accomplishing this Data Cleansing task and demonstrate its use for cleansing lists of names of potential customers in a direct marketing-type application. Our results for statistically generated data are shown to be accurate and effective when processing the data multiple times using different keys for sorting on each successive pass. Combing results of individual passes using transitive closure over the independent results, produces far more accurate results at lower cost. The system provides a rule programming module that is easy to program and quite good at finding duplicates especially in an environment with massive amounts of data. This paper details improvements in our system, and reports on the successful implementation for a real-world database that conclusively validates our results previously achieved for statistically generated data. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> 10 ACM. SIGMOD record, </institution> <month> December </month> <year> 1991. </year>
Reference: <author> R. Agrawal and H. V. Jagadish. </author> <title> Multiprocessor Transitive Closure Algorithms. </title> <booktitle> In Proc. Int'l Symp. on Databases in Parallel and Distributed Systems, </booktitle> <pages> pages 56-66, </pages> <month> December </month> <year> 1988. </year>
Reference: <author> C. Batini, M. Lenzerini, and S. Navathe. </author> <title> A Comparative Analysis of Methodologies for Database Schema Integration. </title> <journal> ACM Computing Surverys, </journal> <volume> 18(4) </volume> <pages> 323-364, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: The first issue, where databases have different schema, has been addressed extensively in the literature and is known as the schema integration problem <ref> (Batini86) </ref>. We are primarily interested in the second problem: heterogeneous representations of data and its implication when merging or joining multiple datasets.
Reference: <author> D. Bitton and D. J. DeWitt. </author> <title> Duplicate Record Elimination in Large Data Files. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(2) </volume> <pages> 255-265, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: The problem of merging two or more databases has been tackled in a straightforward fashion by a simple sort of the concatenated data sets followed by a duplicate elimination phase over the sorted list <ref> (Bitton83) </ref>. However, when the databases involved are heterogeneous, meaning they do not share the same schema, or that the same real-world entities are represented differently in the datasets, the problem of merging becomes more difficult.
Reference: <author> B. P. Buckles and F. E. Petry. </author> <title> A fuzzy representation of data for relational databases. </title> <journal> Fuzzy Sets and Systems, </journal> <volume> 7 </volume> <pages> 213-226, </pages> <year> 1982. </year> <note> Generally regarded as the paper that originated Fuzzy Databases. </note>
Reference-contexts: We use a rule-based knowledge base to implement an equational theory. The problem of identifying similar instances of the same real-world entity by means of an inexact match has been studied by the Fuzzy Database <ref> (Buckles82) </ref> community. Much of the work has concentrated on the problem of executing a query Q over a fuzzy relational database. The answer for Q is the set of all tuples satisfying MERGE/PURGE 3 Q in a non-fuzzy relational database and all tuples that satisfy Q within a threshold value.
Reference: <author> J. P. Buckley. </author> <title> A Hierarchical Clustering Strategy for Very Large Fuzzy Databases. </title> <booktitle> In Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, </booktitle> <pages> pages 3573-3578, </pages> <year> 1995. </year>
Reference-contexts: Elsewhere we have treated the case of clustering in which sorting is replaced by a single-scan process (sigmod95). This clustering resembles the hierarchical clustering strategy proposed in <ref> (Buckley95) </ref> to efficiently perform queries over large fuzzy relational databases. However, we demonstrate that, as one may expect, none of these basic approaches alone can guarantee high accuracy. Here, accuracy means how many of the actual duplicates appearing in the data have been matched and merged correctly.
Reference: <author> K. W. Church and W. A. Gale. </author> <title> Probability Scoring for Spelling Correction. </title> <journal> Statistics and Computing, </journal> <volume> 1 </volume> <pages> 93-103, </pages> <year> 1991. </year>
Reference: <author> T. K. Clark. </author> <title> Analyzing Foster Childrens' Foster Home Payments Database. </title> <editor> In KDD Nuggets 95:7 (http://info.gte.com/~kdd/nuggets/95/), Piatetsky-Shapiro, ed., </editor> <year> 1995. </year>
Reference: <author> T. Dietterich and R. Michalski. </author> <title> A Comparative Review of Selected Methods for Learning from Examples. </title> <editor> In R. Michalski, J. Carbonell, and T. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> volume 1, </volume> <pages> pages 41-81. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1983. </year>
Reference: <author> R. Dubes and A. Jain. </author> <title> Clustering Techniques: The User's Dilema. </title> <journal> Pattern Recognition, </journal> <volume> 8 </volume> <pages> 247-260, </pages> <year> 1976. </year>
Reference-contexts: Here prime-representatives are a set of records extracted from each cluster of records used to represent the information in its cluster. From the pattern recognition community, we can think of these prime-representatives as analogous to the "cluster centroids" <ref> (Dubes76) </ref> generally used to represent clusters of information, or as the base element of an equivalence class. Initially, no previous set of prime-representatives exists and the first increment is just the first input relation. The concatenation step has, therefore, no effect.
Reference: <author> U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> From Data Mining to Knowledge Discovery in Databases. </title> <journal> AI Magazine, </journal> <volume> 17(3), </volume> <month> Fall </month> <year> 1996. </year>
Reference: <author> I. Fellegi and A. Sunter. </author> <title> A Theory for Record Linkage. </title> <journal> American Statistical Association Journal, </journal> <pages> pages 1183-1210, </pages> <month> December </month> <year> 1969. </year>
Reference-contexts: 1. Introduction Merging large databases acquired from different sources with heterogeneous rep resentations of information has become an increasingly important and difficult problem for many organizations. Instances of this problem appearing in the lit erature have been called record linkage <ref> (Fellegi69) </ref>, the semantic integration prob lem (SIGMODRec91) or the instance identification problem (Wang89), and more recently the data cleansing problem regarded as a crucial first step in a Knowledge Discovery in Databases (KDD) (FPSS96). Business organizations call this problem the merge/purge problem.
Reference: <author> C. L. Forgy. </author> <title> OPS5 User's Manual. </title> <type> Technical Report CMU-CS-81-135, </type> <institution> Carnegie Mellon University, </institution> <month> July </month> <year> 1981. </year>
Reference-contexts: Func 8 MAURICIO HERN ANDEZ AND SALVATORE STOLFO tions to compare these complex data types (e.g., sets, images, sound, etc.) could also be used within rules to perform the matching of complex tuples. For the purpose of experimental study, we wrote an OPS5 <ref> (Forgy81) </ref> rule program consisting of 26 rules for this particular domain of employee records and was tested repeatedly over relatively small databases of records.
Reference: <author> R. George, F. E. Petry, B. P. Buckles, and R. Srikanth. </author> <title> Fuzzy Database Systems Challenges and Opportunities of a New Era. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 11 </volume> <pages> 649-659, </pages> <year> 1996. </year>
Reference: <author> S. Ghandeharizadeh. </author> <title> Physical Database Design in Multiprocessor Database Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Wisconsin - Madison, </institution> <year> 1990. </year>
Reference: <author> M. Hernandez and S. Stolfo. </author> <title> The Merge/Purge Problem for Large Databases. </title> <booktitle> In Proceedings of the 1995 ACM-SIGMOD Conference, </booktitle> <month> May </month> <year> 1995. </year>
Reference: <author> K. Kukich. </author> <title> Techniques for Automatically Correcting Words in Text. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-439, </pages> <year> 1992. </year>
Reference: <author> M. Lebowitz. </author> <title> Not the Path to Perdition: The Utility of Similarity-Based Learning. </title> <booktitle> In Proceedings of 5th National Conference on Artificial Intelligence, </booktitle> <pages> pages 533-537, </pages> <year> 1986. </year>
Reference: <author> A. Monge and C. Elkan. </author> <title> An Efficient Domain-independent Algorithm for Detecting Approximate Duplicate Database Records. </title> <booktitle> In Proceedings of the 1997 SIGMOD Workshop on Research Issues on DMKD, </booktitle> <pages> pages 23-29, </pages> <year> 1997. </year>
Reference-contexts: The moral is simply that several distinct "cheap" passes over the data produce more accurate results than one "expensive" pass over the data. This result was verified independently by Monge and Elkan <ref> (Monge97) </ref> who recently studied the same problem using a domain-independent matching algorithm as an equational theory. In section 4 we provide a detailed treatment of a real-world data set, provided by the Child Welfare Department of the State of Washington, which was used to establish the validity of these results.
Reference: <author> C. Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. AlphaSort: </author> <title> A RISC Machine Sort. </title> <booktitle> In Proceedings of the 1994 ACM-SIGMOD Conference, </booktitle> <pages> pages 233-242, </pages> <year> 1994. </year>
Reference: <author> J. J. Pollock and A. Zamora. </author> <title> Automatic spelling correction in scientific and scholarly text. </title> <journal> ACM Computing Surveys, </journal> <volume> 27(4) </volume> <pages> 358-368, </pages> <year> 1987. </year>
Reference: <author> T. Senator, H. Goldberg, J. Wooton, A. Cottini, A. Umar, C. Klinger, W. Llamas, M. Marrone, and R. Wong. </author> <title> The FinCEN Artificial Intelligence System: Identifying Potential Money Laundering MERGE/PURGE 31 from Reports of Large Cash Transactions. </title> <booktitle> In Proceedings of the 7th Conference on Innovative Applications of AI, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: The Justice Department and other law enforcement agencies seek to discover crucial links in complex webs of financial transactions to uncover sophisticated money laundering activities <ref> (Senator95b) </ref>. Errors due to data entry mistakes, faulty sensor readings or more malicious activities, provide scores of erroneous datasets that propagate errors in each successive generation of data.
Reference: <author> Y. R. Wang and S. E. Madnick. </author> <title> The Inter-Database Instance Identification Problem in Integrating Autonomous Systems. </title> <booktitle> In Proceedings of the Sixth International Conference on Data Engineering, </booktitle> <month> February </month> <year> 1989. </year> <title> 32 MAURICIO HERN ANDEZ AND SALVATORE STOLFO Contributing Authors Mauricio. A. Hernandez is an Assistant Professor of Computer Science at the University of Illinois - Springfield. He received his Ph.D. </title> <booktitle> in Computer Science from Columbia University in 1996. His current research interests include Data Cleansing, Data Mining, and Expert Database Systems. </booktitle>
Reference-contexts: Instances of this problem appearing in the lit erature have been called record linkage (Fellegi69), the semantic integration prob lem (SIGMODRec91) or the instance identification problem <ref> (Wang89) </ref>, and more recently the data cleansing problem regarded as a crucial first step in a Knowledge Discovery in Databases (KDD) (FPSS96). Business organizations call this problem the merge/purge problem.

References-found: 23

