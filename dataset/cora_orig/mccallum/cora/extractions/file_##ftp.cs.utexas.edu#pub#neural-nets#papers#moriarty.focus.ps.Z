URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/moriarty.focus.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: 
Email: moriarty,risto@cs.utexas.edu  
Title: Evolving Neural Networks to Focus Minimax Search  
Author: David E. Moriarty and Risto Miikkulainen 
Address: Austin, Austin, TX 78712  
Affiliation: Department of Computer Sciences The University of Texas at  
Abstract: Neural networks were evolved through genetic algorithms to focus minimax search in the game of Othello. At each level of the search tree, the focus networks decide which moves are promising enough to be explored further. The networks effectively hide problem states from minimax based on the knowledge they have evolved about the limitations of minimax and the evaluation function. Focus networks were encoded in marker-based chromosomes and were evolved against a full-width minimax opponent that used the same evaluation function. The networks were able to guide the search away from poor information, resulting in stronger play while examining fewer states. When evolved with a highly sophisticated evaluation function of the Bill program, the system was able to match Bill's performance while only searching a subset of the moves. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Edwards, D., and Hart, T. </author> <year> (1963). </year> <title> The alpha-beta heuristic. </title> <type> Technical Report 30, </type> <institution> MIT. </institution>
Reference: <author> Fullmer, B., and Miikkulainen, R. </author> <year> (1992). </year> <title> Evolving finite state behavior using marker-based genetic encoding of neural networks. </title> <booktitle> In Proceedings of the First European Conference on Artificial Life. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: For each state to be explored in a search tree an activation was propagated through the network. The legal moves with activation greater than or equal to 0 were included in the search window. Evolution Each focus network's genetic representation was based on a marker-based encoding <ref> (Fullmer and Miikkulai-nen 1992) </ref> of the architecture and weights. The encoding is inspired by markers in DNA that separate protein definitions. Artificial markers in the chromosome are used to separate neural network node definitions.
Reference: <author> Goldberg, D. E. </author> <year> (1988). </year> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Hansson, O., and Mayer, A. </author> <year> (1989). </year> <title> Heuristic search as evidential reasoning. </title> <booktitle> In Proceedings of the Fifth Workshop on Uncertainty in AI. </booktitle>
Reference-contexts: An alternative to deeper searches is to decrease the errors in the evaluation function. Bayesian learning has been implemented to combine several heuristic estimates (Lee and Mahajan 1990) and to adjust the heuristic values based on values of other nodes in the tree <ref> (Hansson and Mayer 1989) </ref>. The new estimates represent a measure of belief in the heuristic value. These methods have provided stronger play, although they do not address problems inherent in minimax such as no risk taking.
Reference: <author> Hansson, O., and Mayer, A. </author> <year> (1990). </year> <title> Probabilistic heuristic estimates. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 209-220. </pages>
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, </title> <booktitle> Control and Artificial Intelligence. </booktitle> <address> Ann Arbor, MI: </address> <publisher> University of Michigan Press. </publisher>
Reference: <author> Knuth, D. E., and Moore, R. W. </author> <year> (1975). </year> <title> An analysis of alpha-beta pruning. </title> <journal> Artificial Intelligence, </journal> <volume> 6 </volume> <pages> 293-326. </pages>
Reference: <author> Korf, R. E. </author> <year> (1988). </year> <title> Search: A survey of recent results. </title> <editor> In Shrobe, H. E., editor, </editor> <booktitle> Exploring Artificial Intelligence. </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Heuristic evaluation functions, therefore, are used to approximate the payoff of a state. Unfortunately, heuristics create errors that propagate up the search tree, and can greatly diminish the effectiveness of minimax <ref> (Korf 1988) </ref>. Minimax also assumes that the opponent will always make the best move. It does not promote risk taking. Often in losing situations the best move may not be towards the highest min/max value, especially if it will still result in a loss.
Reference: <author> Korf, R. E., and Chickering, D. M. </author> <year> (1994). </year> <title> Best-first minimax search: Othello results. </title> <booktitle> In AAAI-94. </booktitle>
Reference-contexts: If the opponent gets stronger as the networks evolve, the networks would have to compensate by improving their defensive strategy, and superior overall play should result. In our implementation, focus networks searched only through uniform-depth trees. Focus networks could also be implemented with algorithms such as best-first minimax <ref> (Korf and Chickering 1994) </ref>, where the tree is grown in non-uniform depths allowing more promising moves to be searched deeper.
Reference: <author> Lee, K.-F., and Mahajan, S. </author> <year> (1990). </year> <title> The development of a world class Othello program. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 21-36. </pages>
Reference-contexts: A more directed search, therefore, seems necessary. An alternative to deeper searches is to decrease the errors in the evaluation function. Bayesian learning has been implemented to combine several heuristic estimates <ref> (Lee and Mahajan 1990) </ref> and to adjust the heuristic values based on values of other nodes in the tree (Hansson and Mayer 1989). The new estimates represent a measure of belief in the heuristic value. <p> A population of focus networks was evolved in the game of Othello. The results show that the focus networks are capable of stronger play than full-width minimax with the same evaluation function, while examining fewer positions. Also, when evolved with the highly sophisticated evaluation function of the Bill program <ref> (Lee and Mahajan 1990) </ref>, the focus networks were able to maintain Bill's level of play while searching through fewer states. The next section describes the basic idea and implementation of the focus networks. Section 3 describes marker-based encoding and the specifics of the evolution simulations. <p> The goal was to see how well the focus networks could evolve to make use of weak heuristic information, and also to provide enough errors so that the effect of focus networks would be easily seen. A separate population was evolved using the evaluation function from the Bill program <ref> (Lee and Mahajan 1990) </ref>. Bill's evaluation has been optimized through Bayesian learning and is believed to be one of the best in the world. The goal was to see if the focus networks could achieve any improvement over such an already strong heuristic.
Reference: <author> McAllester, D. A. </author> <year> (1988). </year> <title> Conspiracy numbers for min-max search. </title> <journal> Artificial Intelligence, </journal> <volume> 35 </volume> <pages> 287-310. </pages>
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1993). </year> <title> Evolving complex Othello strategies using marker-based genetic encoding of neural networks. </title> <type> Technical Report AI93-206, </type> <institution> Department of Computer Sciences, The University of Texas at Austin. </institution>
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1994). </year> <title> Improving game tree search with evolutionary neural networks. </title> <booktitle> In Proceedings of the First IEEE Conference on Evolutionary Computation. </booktitle>
Reference-contexts: A comparison of these techniques and a study of how they perhaps could be combined would be most interesting. In an earlier implementation of focus networks, a fixed-size focus window that always included the three best moves was used <ref> (Moriarty and Miikkulai-nen 1994) </ref>. This strategy achieved performance comparable to the threshold-based window with an even more dramatic reduction in the number of states evaluated. However, the fixed window system was not able to generalize well to better opponents such as Bill.
Reference: <author> Pearl, J. </author> <year> (1984). </year> <title> Heuristics: Intelligent Search Strategies for Computer Problem Solving. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: To determine a network's fitness, it was inserted into an ff-fi search program and played against a full-width, fixed-depth minimax-ff-fi search. Both players were allowed to search through the second level. To optimize ff-fi pruning, node ordering was implemented based on the values of the evaluation function <ref> (Pearl 1984) </ref>. Both players always used the same evaluation func tion. One population was evolved based on the posi-tional strategy of Iago (Rosenbloom 1982), one of the first championship-level Othello programs.
Reference: <author> Rivest, R. L. </author> <year> (1987). </year> <title> Game tree searching by min/max approximation. </title> <journal> Artificial Intelligence, </journal> <volume> 34 </volume> <pages> 77-96. </pages>
Reference: <author> Rosenbloom, P. </author> <year> (1982). </year> <title> A world championship-level Othello program. </title> <journal> Artificial Intelligence, </journal> <volume> 19 </volume> <pages> 279-320. </pages>
Reference-contexts: Both players were allowed to search through the second level. To optimize ff-fi pruning, node ordering was implemented based on the values of the evaluation function (Pearl 1984). Both players always used the same evaluation func tion. One population was evolved based on the posi-tional strategy of Iago <ref> (Rosenbloom 1982) </ref>, one of the first championship-level Othello programs. Such an evaluation function is relatively weak as it only considers the merits of single spaces without taking mobility into account 1 .
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., and McClel-land, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, </booktitle> <pages> 318-362. </pages> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The question is, how can we reliably form such a search window? The evolutionary approach is attractive because no previous knowledge of minimax or the evaluation function is needed. The usual neural network learning algorithms such as backpropagation <ref> (Rumelhart et al. 1986) </ref> would require exact target values to be specified for each training example. Such information is very difficult to establish in the search focus task. In the neuro-evolution approach, however, evolutionary pressures will guide the networks toward providing good windows for the search.
Reference: <author> Shannon, C. E. </author> <year> (1950). </year> <title> Programming a computer for playing chess. </title> <journal> Philisophical Magazine, </journal> <volume> 41 </volume> <pages> 256-275. </pages>
Reference-contexts: Introduction Almost all current game programs rely on the minimax search algorithm <ref> (Shannon 1950) </ref> to return the best move. Because of time and space constraints, searching to the end of the game is not feasible for most games. Heuristic evaluation functions, therefore, are used to approximate the payoff of a state.
References-found: 18

