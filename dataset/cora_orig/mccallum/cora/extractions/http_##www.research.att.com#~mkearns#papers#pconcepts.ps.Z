URL: http://www.research.att.com/~mkearns/papers/pconcepts.ps.Z
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/avrim/www/ML98/presentations.html
Root-URL: 
Title: Efficient Distribution-free Learning of Probabilistic Concepts  
Author: Michael J. Kearns Robert E. Schapire 
Date: August 25, 1993  
Address: 600 Mountain Avenue P.O. Box 636 Murray Hill, New Jersey 07974-0636  
Affiliation: AT&T Bell Laboratories  
Abstract: In this paper we investigate a new formal model of machine learning in which the concept (boolean function) to be learned may exhibit uncertain or probabilistic behavior|thus, the same input may sometimes be classified as a positive example and sometimes as a negative example. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. We adopt from the Valiant model of learning [27] the demands that learning algorithms be efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. In addition to giving many efficient algorithms for learning natural classes of p-concepts, we study and develop in detail an underlying theory of learning p-concepts. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Naoki Abe, Jun-ichi Takeuchi, and Manfred K. Warmuth. </author> <title> Polynomial learnability of probabilistic concepts with respect to the Kullback-Liebler divergence. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 277-289, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: In this paper we wish to study a model of learning in such uncertain environments. We formalize these settings by introducing the notion of a probabilistic concept (or p-concept). A p-concept c over a domain set X is simply a mapping c : X ! <ref> [0; 1] </ref>. For each x 2 X , we interpret c (x) as the probability that x is a positive example of the p-concept c. <p> We will see that the more difficult and more interesting goal is that of finding a good model of probability. Here the algorithm wishes to output a hypothesis p-concept h : X ! <ref> [0; 1] </ref> that is a good real-valued approximation to the target c; thus, we want jc (x) h (x)j to be small for most inputs x. Following the motivation given above, we are mainly concerned with this latter notion of learning. <p> A probabilistic concept (or p-concept) is a real-valued function c : X ! <ref> [0; 1] </ref>. When learning the p-concept c, the value c (x) is interpreted as the probability that x exemplifies the concept being learned (i.e., the probability that x is a 5 positive example). A p-concept class C is a family of p-concepts. <p> We assume implicitly that all of the p-concepts considered are measurable functions with respect to this -algrebra on X , and the Borel -algebra on <ref> [0; 1] </ref>.) The learning algorithm is given access to an oracle EX (short for "examples") that behaves as follows: EX first draws a point x 2 X randomly according to the distribution D. <p> formulations given by Theorem 2.3, Yamanishi [31] shows that an equivalent problem is to find, in polynomial time with high probability and for given *, a hypothesis h such that E x2D h p p i *. (This quantity is known as the Hellinger distance.) Finally, Abe, Takeuchi and Warmuth <ref> [1] </ref> have shown that all of these problems are equivalent (modulo polynomial-time computation) to the problem of finding, with high probability and for given *, a hypothesis with small Kullback-Liebler divergence, i.e., a hypothesis h for which E x2D c (x) lg c (x) 1 h (x) *: 2.2 Chernoff bounds <p> The additive form (also known as Hoeffding's inequality) holds also if X 1 ; : : : ; X m are independent identically distributed random variables with range in <ref> [0; 1] </ref>. 3 Efficient algorithms: The direct approach In this section, we describe efficient algorithms for learning good models of probability based on first principles and proved correct by direct arguments. Later arguments will rely on an underlying theory of p-concept learning that is developed in subsequent sections. <p> We begin with a p-concept class motivated by the problem of modeling "tallness" discussed in the introduction. 3.1 Increasing functions Theorem 3.1 The p-concept class of all nondecreasing functions c : R ! <ref> [0; 1] </ref> is polynomially learnable with a model of probability. <p> Then a probabilistic decision list c over basis F n is given by a list (f 1 ; r 1 ); : : : ; (f s ; r s ), where each f i 2 F n , and each r i 2 <ref> [0; 1] </ref>. We also assume that f s is the constant function 1. For any assignment x in the domain, c (x) is defined to be r j , where j is the least index for which f j (x) = 1. <p> Let c be a probabilistic decision list over basis F n , given by the list (f 1 ; r 1 ); : : :; (f s ; r s ). For ! 2 <ref> [0; 1] </ref>, we say that c is a probabilistic decision list with !-converging probabilities if jr i !j jr i+1 !j for 1 i &lt; s. Below, we describe an algorithm for inferring such lists when ! is known. <p> This specifically answers an open question proposed by Rivest [24] concerning the learnability of decision lists in such a noisy setting. (This problem of learning noisy decision lists was solved independently by Sakakibara [25].) Theorem 3.2 Let ! 2 <ref> [0; 1] </ref> be fixed, and let F n be a basis of functions. Then the p-concept class of probabilistic decision lists over basis F n with !-converging probabilities is learnable with a model of probability (assuming both ! and F n are known). <p> that the functions in F n are indexed so that the target p-concept c is given by the list (f 1 ; r 1 ); : : : ; (f s ; r s ). (Of course, the learning algorithm is not aware of this.) We 12 Input: ! 2 <ref> [0; 1] </ref> basis F n = ff 1 ; : : : ; f s g *, ffi, fl &gt; 0 access to random examples of a probabilistic decision list over basis F n with !-converging probabilities Output: with probability at least 1 ffi, an (*; fl)-good model of probability Procedure: <p> A visible monomial p-concept is defined over f0; 1g n by a pair (M; ff), where M is a monomial over the visible Boolean variables x 1 ; : : : ; x n and ff 2 <ref> [0; 1] </ref>. The associated p-concept c is defined for x 2 f0; 1g n to be c (x) = ff M (x). We conceptually regard the true deterministic concept as having the form M ^ I , where I is a deterministic concept over the hidden variables. <p> if the drink lacks any one of these qualities, then it certainly cannot be "the real thing." We note that the algorithm described in the proof below can be easily extended to learn any p-concept c of the form c = ffc 0 where ff is an unknown constant in <ref> [0; 1] </ref> and c 0 is a deterministic concept from some known concept class for which there exists an efficient algorithm that, like the algorithm V described in the proof, requires positive examples only, and outputs hypotheses with one-sided error on the positive-examples distribution only. <p> It is often convenient not to restrict the range of h to the set Y 0 ; for instance, if Y 0 = f0; 1g, then we may want to allow h to map into <ref> [0; 1] </ref>. In general, then, we assume that h is a function which maps X into some set Y Y 0 . In Haussler's model, the learner must choose a hypothesis from some given hypothesis space H of functions (each mapping X into Y ). <p> This discrepancy between y and h (x) is measured by a real-valued "loss" function. Formally, a loss function L is a function mapping Y fi Y 0 into <ref> [0; 1] </ref>. (The extension of such results to general bounded functions is straightforward.) Thus, the formal goal of the learner in this framework is to find a function h 2 H that minimizes the average loss E [L (h (x); y)], where the expectation is over points (x; y) drawn randomly <p> As mentioned above, in our setting Y 0 = f0; 1g since an algorithm only sees f0; 1g-labels. For decision-rule learning, the algorithm outputs f0; 1g-valued hypotheses, and thus Y = Y 0 = f0; 1g in this case. Similarly, for model-of-probability learning, we assume that hypotheses have range <ref> [0; 1] </ref>, and so Y = [0; 1]. The distribution on X fi Y 0 is naturally determined by the joint behavior of the target distribution D on X and the conditional probabilities c (x) given by the target p-concept. <p> For decision-rule learning, the algorithm outputs f0; 1g-valued hypotheses, and thus Y = Y 0 = f0; 1g in this case. Similarly, for model-of-probability learning, we assume that hypotheses have range <ref> [0; 1] </ref>, and so Y = [0; 1]. The distribution on X fi Y 0 is naturally determined by the joint behavior of the target distribution D on X and the conditional probabilities c (x) given by the target p-concept. <p> Theorem 5.1 Let H be a hypothesis space of functions mapping X into Y which satisfies certain "permissibility" assumptions (see Haussler's paper). Let D be a probability distribution on X fi Y 0 , let L : Y fi Y 0 ! <ref> [0; 1] </ref> be a loss function, let d &lt; 1 be the pseudo dimension of L H , and let S be a 22 sample of m points from X fi Y 0 chosen randomly according to D. <p> 1 ; : : : ; f d ) denote the class of all p-concepts of the form c (x) = P d i=1 a i f i (x) for a i 2 R, where we assume that the f i and a i are such that c (x) 2 <ref> [0; 1] </ref> for all x 2 X. We describe below an algorithm that learns a model of probability for p-concepts in the class C (f 1 ; : : :; f d ). <p> Thus, the target p-concept might have the form c (x) = abd (x; z) where d (x; z) denotes the Hamming distance and a and b are positive real-valued coefficients such that c is maximized at z and is always in the range <ref> [0; 1] </ref>. Here the p-concept class C is obtained by ranging over the 24 choices of the prototype z and the coefficients a and b, and the "decay function," which specifies the rate at which vectors further away from the prototype fail to exemplify the concept, is linear. <p> Let H be a class of p-concepts over domain X. Let T = f (x 1 ; r 1 ); : : : ; (x d ; r d )g be a set of d pairs, where each x i 2 X and each r i 2 <ref> [0; 1] </ref>. <p> Note that if H has pseudo dimension at least d then there always exists some w &gt; 0 such that some set of d pairs over X fi <ref> [0; 1] </ref> is w-shattered. Based on this stronger notion of shattering, we can now prove the following lower bound on sample complexity in our model. <p> An Occam algorithm for hypothesis class H over parametrized domain X, with respect to a loss function L : Y fi Y 0 ! <ref> [0; 1] </ref> is a polynomial-time algorithm A that takes as input a labeled sample S 2 (X n fi Y 0 ) m , and outputs a hypothesis h with the properties that: 1. ^ E [L h ] inf h 0 2H ^ E [L h 0 ] t = <p> More realistic is a so-called agnostic learning model in which the target p-concept is any function from X into <ref> [0; 1] </ref>, and the learner's goal is to find the best hypothesis from some fixed space of hypotheses. This is actually the framework assumed by Haussler [?] in deriving his sample-size bounds.
Reference: [2] <author> William Aiello and Milena Mihail. </author> <title> Learning the Fourier spectrum of probabilistic lists and trees. </title> <booktitle> In Proceedings of the Second Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <month> January </month> <year> 1991. </year>
Reference-contexts: The class of probabilistic decision lists has also been considered by Yamanishi [31]. He describes an algorithm, based on the principle of minimum description length, for learning a model of probability for p-concepts in this class; however, his algorithm is not computationally efficient. Also, Aiello and Mihail <ref> [2] </ref> have recently described an efficient algorithm for learning arbitrary probabilistic decision lists over the basis consisting of all literals in the special case that D is the uniform distribution. 15 3.3 Hidden-variable problems We next consider p-concept classes motivated by hidden-variable problems, in which there is an underlying deterministic concept,
Reference: [3] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: This structured behavior strongly distinguishes these learning scenarios from a "noisy" setting, such as the one considered by Angluin and Laird <ref> [3] </ref>, Kearns and Li [16], and Sloan [26]. <p> Suppose further that the label of each example is flipped (i.e., reversed) randomly with probability &lt; 1=2. This random misclassifi-cation noise model is considered, for instance, by Angluin and Laird <ref> [3] </ref>. Note that the observed behavior in such a situation can be modeled naturally by the probabilistic decision list c 0 given by (f 1 ; jb 1 j); : : : ; (f s ; jb s j). <p> However, if no such bound is known, Angluin and Laird <ref> [3] </ref> give a technique for finding a good bound using a kind of "binary search.") Thus, a corollary of Theorem 3.2 is a proof that deterministic decision lists are efficiently learnable even when the supplied examples are randomly misclassified with probability .
Reference: [4] <author> Dana Angluin and Leslie G. Valiant. </author> <title> Fast probabilistic algorithms for Hamiltonian circuits and matchings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18(2) </volume> <pages> 155-193, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: *, a hypothesis with small Kullback-Liebler divergence, i.e., a hypothesis h for which E x2D c (x) lg c (x) 1 h (x) *: 2.2 Chernoff bounds Several times, in later sections of this paper, we will make use of the following bounds on the tails of a binomial distribution <ref> [4, 13] </ref>. Lemma 2.4 (Chernoff Bounds) Let X 1 ; : : : ; X m be a sequence of m independent Bernoulli trials, each succeeding with probability p so that E [X i ] = p.
Reference: [5] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24(6) </volume> <pages> 377-380, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: In the Valiant model, Blumer et al. <ref> [5] </ref> show that it suffices for learning to find a consistent hypothesis that is slightly shorter than the sample data. <p> An interesting open problem is to give improved general upper bounds on sample size that incorporate the width of shattering. 7 Occam's Razor for general loss functions In this section, we present a generalized form of Occam's Razor <ref> [5] </ref> applicable to the minimization of bounded loss functions, and in particular to learning p-concepts with a model of probability or a decision rule. <p> Then Pr [E [L h ] inf E [L h 0 ] &gt; *] ffi: In particular, this will be the case if m max &lt; * ; 16 (ln 2)n b ! 1=(1fi) 16 ln (4=ffi) 9 ; Proof: The proof is analogous to that of Blumer et al. <ref> [5] </ref>. Let H A be the set of (at most) 2 ` hypotheses which might potentially be output by A. Let h fl 2 H be such that E [L h fl ] inf h 0 2H E [L h 0 ] + *=4.
Reference: [6] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learn-ability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: size for learning any p-concept class with a model of probability; thus the quadratic loss dimension, when finite, characterizes the sample complexity of p-concept learning with a model of probability in the same way that the Vapnik-Chervonenkis (VC) dimension characterizes sample complexity in Valiant's model. (See Blumer et al.'s paper <ref> [6] </ref> for a full discussion of the VC-dimension.) However, we show that p-concept classes of infinite quadratic loss dimension may sometimes be learned efficiently, in contrast to classes of infinite VC-dimension in the Valiant model, which are not learnable in any amount of time. (Technically, this is not always true if <p> Thus, the number of examples needed for decision-rule learning is bounded by the VC-dimension of the space of hypotheses used by the learning algorithm. (That the VC-dimension can be used in this manner was also observed by Blumer et al. <ref> [6] </ref>.) For example, consider the problem of learning a decision rule for an increasing function over R. Note that the best decision rule for such a p-concept is always of the form h a (x) = 1 for x &gt; a, and 0 otherwise, for some a. <p> Then for fl w and * + ffi 1=8, any algorithm for learning C with a model of probability requires at least bd (lg e)=8c = (d) examples. Proof: Our proof is based on the analogous lower bound proof given by Blumer et al. <ref> [6] </ref> for learning deterministic concepts. However, the analysis is more involved in the probabilistic case. Let T = f (x 1 ; r 1 ); : : : ; (x d ; r d )g be w-shattered by the p-concept class C.
Reference: [7] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: This yields a system of d linear equations in the d variables a i that is of a special form and that can be solved using standard techniques. Cormen, Leiserson and Rivest <ref> [7, Chapter 31] </ref> describe in detail how this can be done efficiently; see also Duda and Hart [9]. Let ^a 1 ; : : : ; ^a d be the resulting solution, and let h 0 = P d i=1 ^a i f i .
Reference: [8] <author> Annette J. Dobson. </author> <title> An Introduction to Generalized Linear Models. </title> <publisher> Chapman and Hall, </publisher> <year> 1990. </year>
Reference-contexts: This problem, commonly known to statisticians as regression, has received much attention in the statistics literature. (See, for instance, Dobson's book <ref> [8] </ref>.) The problem of learning a model of probability is also equivalent (with slight restrictions) to that of learning a stochastic rule as defined in the parallel work of Yamanishi [31].
Reference: [9] <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: This yields a system of d linear equations in the d variables a i that is of a special form and that can be solved using standard techniques. Cormen, Leiserson and Rivest [7, Chapter 31] describe in detail how this can be done efficiently; see also Duda and Hart <ref> [9] </ref>. Let ^a 1 ; : : : ; ^a d be the resulting solution, and let h 0 = P d i=1 ^a i f i .
Reference: [10] <author> R. M. Dudley. </author> <title> Central limit theorems for empirical measures. </title> <journal> The Annals of Probability, </journal> <volume> 6(6) </volume> <pages> 899-929, </pages> <year> 1978. </year>
Reference-contexts: We begin with a description of the learning framework that was proposed by Haussler [?], and that extends the work of Pollard [23], Dudley <ref> [10] </ref>, Vapnik [28] and others. In this framework, the learner observes pairs (x; y) drawn randomly from some product space X fi Y 0 according to some fixed distribution. <p> Let H = clamp P d o . Our algorithm outputs the hypothesis h = clamp (h 0 ). Clearly h is in H, as is the target c. Dudley <ref> [10] </ref> shows that a d-dimensional linear function space has pseudo dimension d. (This is reproved by Haussler [?], Theorem 4.) Combined with Haussler's Theorem 5 (which concerns the pseudo dimension of families of functions constructed in the same way as H), this immediately implies PD (H) d.
Reference: [11] <author> David Haussler. </author> <title> Generalizing the PAC model: Sample size bounds from metric dimension-based uniform convergence results. </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 40-45, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: We then consider the problem of hypothesis testing in the p-concept model. Working in the same framework as Haussler <ref> [11, ?] </ref>, we define a loss function that assigns a measure of goodness to any hypothesis p-concept on a f0; 1g-labeled sample. <p> Another contribution of this research is in demonstrating the feasibility and practicality of the approach suggested by Haussler <ref> [11] </ref>. His work addressed the issue of sample complexity upper bounds in great generality, even encompassing the case where the input-output relation to be learned has no prescribed functional form.
Reference: [12] <author> David Haussler, Michael Kearns, Nick Littlestone, and Manfred K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Information and Computation, </journal> <volume> 95(2) </volume> <pages> 129-161, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: With high probability, one of these hypotheses is "good," and we can find the best one by hypothesis testing each h i and outputting the one with the lowest discrete or quadratic loss. (This technique is due to Haussler et al. <ref> [12] </ref> who prove the analogous result for the deterministic PAC model.) 20 The remainder of this section describes another example of an efficient learning algorithm that employs the approach outlined above. 4.1 Probabilistic concepts of k relevant variables For a p-concept c on n Boolean variables, we say that variable x
Reference: [13] <author> Wassily Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301) </volume> <pages> 13-30, </pages> <month> March </month> <year> 1963. </year> <month> 31 </month>
Reference-contexts: *, a hypothesis with small Kullback-Liebler divergence, i.e., a hypothesis h for which E x2D c (x) lg c (x) 1 h (x) *: 2.2 Chernoff bounds Several times, in later sections of this paper, we will make use of the following bounds on the tails of a binomial distribution <ref> [4, 13] </ref>. Lemma 2.4 (Chernoff Bounds) Let X 1 ; : : : ; X m be a sequence of m independent Bernoulli trials, each succeeding with probability p so that E [X i ] = p.
Reference: [14] <author> Abraham Kandel. </author> <title> Fuzzy Techniques in Pattern Recognition. </title> <publisher> Wiley, </publisher> <year> 1982. </year>
Reference-contexts: An axiomatic theory of fuzzy sets was introduced by Zadeh [32], and they have since received much treatment by researchers in the field of pattern recognition. (See Kandel's book <ref> [14] </ref> for a good introduction.) We distinguish two possible goals for a learning algorithm in the p-concept model.
Reference: [15] <author> Michael Kearns. </author> <title> The Computational Complexity of Machine Learning. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Various techniques in this regard have been developed in the Valiant model, such as those of Pitt and Valiant [22], and Kearns and Valiant <ref> [17, 15] </ref>. Can such techniques be extended to the p-concept model? Both of these results seem to depend crucially on the deterministic nature of the Valiant model.
Reference: [16] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 267-280, </pages> <month> May </month> <year> 1988. </year> <note> To appear, SIAM Journal on Computing. </note>
Reference-contexts: This structured behavior strongly distinguishes these learning scenarios from a "noisy" setting, such as the one considered by Angluin and Laird [3], Kearns and Li <ref> [16] </ref>, and Sloan [26].
Reference: [17] <author> Michael Kearns and Leslie G. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 433-444, </pages> <month> May </month> <year> 1989. </year> <note> To appear, Journal of the Association for Computing Machinery. </note>
Reference-contexts: Various techniques in this regard have been developed in the Valiant model, such as those of Pitt and Valiant [22], and Kearns and Valiant <ref> [17, 15] </ref>. Can such techniques be extended to the p-concept model? Both of these results seem to depend crucially on the deterministic nature of the Valiant model.
Reference: [18] <author> Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. </author> <title> Toward efficient agnostic learning. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 341-352, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: x2D [jh (x) c (x)j] Pr x2D ^ M (x) = 0 ^ M (x) = 1 + jff ^ffj Pr x2D ^ M (x) = 1 Pr x2D ^ M (x) = 0 j M (x) = 1 + jff ^ffj Finally, we remark that Kearns, Schapire and Sellie <ref> [18] </ref> have recently extended this result beyond the class of partially visible monomials to the class of partially visible k-term DNF formulas.
Reference: [19] <author> Nathan Linial, Yishay Mansour, and Ronald L. Rivest. </author> <title> Results on learnability and the Vapnik-Chervonenkis dimension. </title> <booktitle> In 29th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 120-129, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: that p-concept classes of infinite quadratic loss dimension may sometimes be learned efficiently, in contrast to classes of infinite VC-dimension in the Valiant model, which are not learnable in any amount of time. (Technically, this is not always true if "dynamic" sampling is allowed; see Linial, Mansour and Rivest's paper <ref> [19] </ref> for further details.) We conclude with an investigation of Occam's Razor in the p-concept model. In the Valiant model, Blumer et al. [5] show that it suffices for learning to find a consistent hypothesis that is slightly shorter than the sample data.
Reference: [20] <author> Nick Littlestone and Manfred Warmuth. </author> <title> Relating data compression and learnability. </title> <type> Unpublished manuscript, </type> <month> November </month> <year> 1987. </year>
Reference-contexts: Note that Theorem 7.1 is only applicable to algorithms which output hypotheses over a finite alphabet. However, the theorem can be extended to apply to other algorithms in a manner similar to the approach taken by Littlestone and Warmuth <ref> [20] </ref> in the Valiant model. The basic idea is to allow the learning algorithm to output hypotheses that can be represented over the alphabet S [ f0; 1g, where S is the given sample. That is, the representation of the hypothesis may include individual examples from the sample itself.
Reference: [21] <author> Yishay Mansour. </author> <title> Learning via Fourier transform. </title> <type> Unpublished manuscript, </type> <month> April </month> <year> 1990. </year>
Reference-contexts: Finally, we remark that Theorem 5.3 can be applied to learn so-called "t-transform functions" considered by Mansour <ref> [21] </ref>. Theorem 5.3 For any set of d known computable functions f i : X ! R; 1 i d, the class C (f 1 ; : : : ; f d ) is learnable with a model of probability.
Reference: [22] <author> Leonard Pitt and Leslie G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Various techniques in this regard have been developed in the Valiant model, such as those of Pitt and Valiant <ref> [22] </ref>, and Kearns and Valiant [17, 15]. Can such techniques be extended to the p-concept model? Both of these results seem to depend crucially on the deterministic nature of the Valiant model.
Reference: [23] <author> David Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: We show that the sufficient sample size for uniform convergence is bounded above by the pseudo dimension of the p-concept class, a combinatorial measure discussed by Pollard <ref> [23] </ref>, Haussler [?] and other authors. <p> We begin with a description of the learning framework that was proposed by Haussler [?], and that extends the work of Pollard <ref> [23] </ref>, Dudley [10], Vapnik [28] and others. In this framework, the learner observes pairs (x; y) drawn randomly from some product space X fi Y 0 according to some fixed distribution. <p> We now turn to a discussion of such uniform convergence techniques applicable to p-concept classes. Haussler [?], Pollard <ref> [23] </ref> and others have described the pseudo dimension of a class of real-valued functions F on domain X, and have shown that the pseudo dimension is a powerful tool for obtaining uniform convergence results.
Reference: [24] <author> Ronald L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(3) </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: These include algorithms for arbitrary non-decreasing functions motivated above, a probabilistic analog of Rivest's decision lists <ref> [24] </ref>, and a class of "hidden-variable" p-concepts, motivated by settings such as weather prediction where the apparently probabilistic behavior may in part be due to the fact that some relevant quantities remain undiscovered. We then consider the problem of hypothesis testing in the p-concept model. <p> However, a much simpler and more efficient algorithm exists that we give in Section 5. 3.2 Probabilistic decision lists We turn next to the problem of learning a probabilistic analog of Rivest's decision lists <ref> [24] </ref>. We define such lists with respect to a basis F n of Boolean-valued functions on the domain f0; 1g n . We assume always that F n contains the constant function 1. <p> The running time is then polynomial in 1=(1 2), in addition to the usual other parameters. This specifically answers an open question proposed by Rivest <ref> [24] </ref> concerning the learnability of decision lists in such a noisy setting. (This problem of learning noisy decision lists was solved independently by Sakakibara [25].) Theorem 3.2 Let ! 2 [0; 1] be fixed, and let F n be a basis of functions.
Reference: [25] <author> Yasubumi Sakakibara. </author> <title> Algorithmic Learning of Formal Languages and Decision Trees. </title> <type> PhD thesis, </type> <institution> Tokyo Institute of Technology, </institution> <month> October </month> <year> 1991. </year> <note> Research Report IIAS-RR-91-22E, </note> <institution> International Institute for Advanced Study of Social Information Science, Fujitsu Laboratories, Ltd. </institution>
Reference-contexts: The running time is then polynomial in 1=(1 2), in addition to the usual other parameters. This specifically answers an open question proposed by Rivest [24] concerning the learnability of decision lists in such a noisy setting. (This problem of learning noisy decision lists was solved independently by Sakakibara <ref> [25] </ref>.) Theorem 3.2 Let ! 2 [0; 1] be fixed, and let F n be a basis of functions. Then the p-concept class of probabilistic decision lists over basis F n with !-converging probabilities is learnable with a model of probability (assuming both ! and F n are known).
Reference: [26] <author> Robert H. Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 91-96, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: This structured behavior strongly distinguishes these learning scenarios from a "noisy" setting, such as the one considered by Angluin and Laird [3], Kearns and Li [16], and Sloan <ref> [26] </ref>.
Reference: [27] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: In general, we wish to study the learnability of p-concept classes that are restricted in such a way as to plausibly capture some realistic situation, but are not so restricted as to make the learning problem trivial or uninteresting. We adopt from the Valiant model for learning deterministic concepts <ref> [27] </ref> the emphasis on learning algorithms that are both efficient (in the sense of polynomial time) and general (in the sense of working for the largest possible p-concept classes and against any probability distribution over the domain). <p> For instance, Valiant <ref> [27] </ref> describes such an algorithm for learning k-CNF (the class of Boolean formulas consisting of a conjunction of clauses, each a disjunction of at most k literals). Theorem 3.3 The class of visible monomial p-concepts is polynomially learnable with a model of probability. <p> Otherwise, ^p &gt; 2*=3, and we can assume henceforth that p *=3 (as is the case with probability at least 1 ffi=3). Next our algorithm attempts to learn a good approximation of M . This is done using Valiant's algorithm <ref> [27] </ref>, here denoted V , for learning monomials from positive examples only in the distribu 16 tion-free deterministic model.
Reference: [28] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: We begin with a description of the learning framework that was proposed by Haussler [?], and that extends the work of Pollard [23], Dudley [10], Vapnik <ref> [28] </ref> and others. In this framework, the learner observes pairs (x; y) drawn randomly from some product space X fi Y 0 according to some fixed distribution.
Reference: [29] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its applications, </title> <address> XVI(2):264-280, </address> <year> 1971. </year>
Reference-contexts: Specifically, we can apply the uniform convergence results of Vapnik and Chervonenkis <ref> [29] </ref> to show that, with high probability, each interval I j has probability at most *fl=2. Let S be the set of all intervals on X .
Reference: [30] <author> Halbert White. </author> <title> Learning in artificial neural networks: A statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1(4) </volume> <pages> 425-464, </pages> <year> 1989. </year>
Reference-contexts: These properties, which follow from the following theorem, are well known to statisticians. (See, for instance, White's review article <ref> [30] </ref>.) Also, note that the empirical loss ^ E [Q h ] is the average squared-error statistic commonly used by researchers in pattern recognition and statistical decision theory. 19 Theorem 4.1 For any target p-concept c, target distribution D, and p-concept h, E [Q h ] E [Q c ] =
Reference: [31] <author> Kenji Yamanishi. </author> <title> A learning criterion for stochastic rules. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 67-81, </pages> <month> August </month> <year> 1990. </year> <note> To appear, Machine Learning. 32 </note>
Reference-contexts: problem, commonly known to statisticians as regression, has received much attention in the statistics literature. (See, for instance, Dobson's book [8].) The problem of learning a model of probability is also equivalent (with slight restrictions) to that of learning a stochastic rule as defined in the parallel work of Yamanishi <ref> [31] </ref>. As noted above, we will typically assume that the probabilistic behavior exhibited by the target p-concept is, to some degree, structured. To model this structure, we study the learnability of classes of p-concepts that obey natural mathematical properties intended to model some realistic environments. <p> In addition to the formulations given by Theorem 2.3, Yamanishi <ref> [31] </ref> shows that an equivalent problem is to find, in polynomial time with high probability and for given *, a hypothesis h such that E x2D h p p i *. (This quantity is known as the Hellinger distance.) Finally, Abe, Takeuchi and Warmuth [1] have shown that all of these <p> The proof of (4) is symmetric when ^p t !. The algorithm of Figure 1 clearly runs in polynomial time. It is an open question whether this class is learnable when ! is unknown. The class of probabilistic decision lists has also been considered by Yamanishi <ref> [31] </ref>. He describes an algorithm, based on the principle of minimum description length, for learning a model of probability for p-concepts in this class; however, his algorithm is not computationally efficient.
Reference: [32] <author> L. A. Zadeh. </author> <title> Fuzzy sets. </title> <journal> Information and Control, </journal> <volume> 8(3) </volume> <pages> 338-353, </pages> <month> June </month> <year> 1965. </year> <month> 33 </month>
Reference-contexts: In this sense, p-concepts are quite similar to the related notion of a fuzzy set, a kind of "set" whose boundaries are fuzzy or unclear, and whose formal definition is nearly identical to that of a p-concept. An axiomatic theory of fuzzy sets was introduced by Zadeh <ref> [32] </ref>, and they have since received much treatment by researchers in the field of pattern recognition. (See Kandel's book [14] for a good introduction.) We distinguish two possible goals for a learning algorithm in the p-concept model.
References-found: 32

