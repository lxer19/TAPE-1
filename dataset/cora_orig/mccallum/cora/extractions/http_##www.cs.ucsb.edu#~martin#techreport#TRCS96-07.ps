URL: http://www.cs.ucsb.edu/~martin/techreport/TRCS96-07.ps
Refering-URL: http://www.cs.ucsb.edu/~martin/techreport/index.html
Root-URL: http://www.cs.ucsb.edu
Email: fpedro,marting@cs.ucsb.edu  
Phone: Phone: (805) 893-2777 Fax: (805) 893-8553  
Title: Lock Coarsening: Eliminating Lock Overhead in Automatically Parallelized Object-Based Programs  
Author: Pedro Diniz and Martin Rinard 
Address: I Bldg, Room 2106  Santa Barbara, CA 93106-5110  
Affiliation: Department of Computer Science, Engr  University of California, Santa Barbara  
Abstract: Atomic operations are a key primitive in parallel computing systems. The standard implementation mechanism for atomic operations uses mutual exclusion locks. In an object-based programming system the natural granularity is to give each object its own lock. Each operation can then make its execution atomic by acquiring and releasing the lock for the object that it accesses. But this fine lock granularity may have high synchronization overhead because it maximizes the number of executed acquire and release constructs. To achieve good performance it may be necessary to reduce the overhead by coarsening the granularity at which the computation locks objects. In this paper we describe a static analysis technique | lock coarsening | designed to automatically increase the lock granularity in object-based programs with atomic operations. We have implemented this technique in the context of a parallelizing compiler for irregular, object-based programs and used it to improve the generated parallel code. Experiments with two automatically parallelized applications show these algorithms to be effective in reducing the lock overhead to negligible levels.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(NlogN) force-calculation algorithm. </title> <booktitle> Nature, </booktitle> <pages> pages 446-449, </pages> <month> De-cember </month> <year> 1976. </year>
Reference-contexts: In this section we present experimental results that characterize the performance impact of using the lock coarsening algorithms in a parallelizing compiler. We report performance results for two automatically parallelized scientific applications: the Barnes-Hut hierarchical N-body solver <ref> [1] </ref> and the Water [13] code. 7.1 The Compilation System Although it is possible to apply the lock coarsening algorithms to any parallel program that conforms to the model of computation in Section 4, they were developed in the context of a parallelizing compiler for object-based languages [12]. <p> We compiled the parallel programs using the IRIX 5.3 CC compiler at the -O2 optimization level. 7.3 Barnes-Hut The Barnes-Hut application simulates the trajectories of a set of interacting bodies under Newto-nian forces <ref> [1] </ref>. It uses a sophisticated pointer-based data structure: a space subdivision tree that dramatically improves the efficiency of a key phase in the algorithm. The application consists of approximately 1500 lines of serial C++ code.
Reference: [2] <author> P. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: In fact, database researchers have identified lock granularity as a key issue in the implementation of atomic transactions, and found that excessive lock overhead can be a significant problem if the lock granularity is too fine <ref> [2, 6] </ref>. The proposed solution to the problem of excessive lock overhead in the context of database concur-rency control is to dynamically coarsen the lock granularity using a technique called lock escalation.
Reference: [3] <author> F. Bodin, P. Beckman, D. Gannon, J. Gotwals, S. Narayana, S. Srinivas, and Beata Winnicka. Sage++: </author> <title> An object-oriented toolkit and class library for building Fortran and C++ structuring tools. </title> <booktitle> In Proceedings of the Object-Oriented Numerics Conference, </booktitle> <year> 1984. </year>
Reference-contexts: The compiler itself is structured as a source-to-source translator that takes an unannotated serial program written in a subset of C++ and generates an explicitly parallel C++ program that performs the same computation. We use Sage++ <ref> [3] </ref> as a front end. The analysis phase consists of approximately 14,000 lines of C++ code, with approximately 1,800 devoted to interfacing with Sage++. The code generation phase of the compiler automatically generates lock constructs to ensure that operations on objects execute atomically.
Reference: [4] <author> R. Cytron. </author> <title> Doacross: Beyond vectorization for multiprocessors. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: The majority of synchronization optimization research has concentrated on removing barriers or converting barrier synchronization constructs to more efficient synchronization constructs such as counters [14]. Several researchers have also explored optimizations geared towards exploiting more fine grained concurrency available within loops <ref> [4] </ref>. These optimizations automatically insert one-way synchronization constructs such as post and wait to implement loop-carried data dependences. The research presented in this paper investigates synchronization optimizations for a compiler designed to parallelize object-based programs, not loop nests that manipulate dense arrays using affine access functions.
Reference: [5] <author> J. Goodman, M. Vernon, and P. Woest. </author> <title> Efficient synchronization primitives for large-scale cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 64-75, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: This work has concentrated on the development of more efficient implementations of synchronization primitives using various protocols and waiting mechanisms <ref> [5, 9] </ref>. The research presented in this paper is orthogonal to and synergistic with this work.
Reference: [6] <author> U. Herrmann, P. Dadam, K. Kuspert, E. Roman, and G Schlageter. </author> <title> A lock technique for disjoint and non-disjoint complex objects. </title> <booktitle> In Proceedings of the International Conference on Extending Database Technology (EDBT'90), </booktitle> <pages> pages 219-235, </pages> <address> Venice, Italy, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: In fact, database researchers have identified lock granularity as a key issue in the implementation of atomic transactions, and found that excessive lock overhead can be a significant problem if the lock granularity is too fine <ref> [2, 6] </ref>. The proposed solution to the problem of excessive lock overhead in the context of database concur-rency control is to dynamically coarsen the lock granularity using a technique called lock escalation.
Reference: [7] <author> G. Kane and J. Heinrich. </author> <title> MIPS Risc Architecture. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference-contexts: This property makes it possible for the implementation to use an extremely efficient lock implementation. Modern processors have synchronization instructions that make it possible to implement the required lock acquire and release constructs with an overhead of only several machine instructions <ref> [7] </ref>. 9.3 Efficient Synchronization Algorithms Other researchers have addressed the issue of synchronization overhead reduction. This work has concentrated on the development of more efficient implementations of synchronization primitives using various protocols and waiting mechanisms [5, 9].
Reference: [8] <author> D. Lenoski. </author> <title> The Design and Analysis of DASH: A Scalable Directory-Based Multiprocessor. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: The generated code for each application differs only in the lock coarsening policy used to reduce the lock overhead. 2 We evaluated the performance of each version by running it on a 32-processor Stanford DASH machine <ref> [8] </ref> running a modified version of the IRIX 5.2 operating system. Because the prototype compiler is a source-to-source translator, we use a standard C++ compiler to generate object code for the automatically generated parallel programs.
Reference: [9] <author> B-H. Lim and A. Agarwal. </author> <title> Reactive synchronization algorithms for multiprocessors. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: This work has concentrated on the development of more efficient implementations of synchronization primitives using various protocols and waiting mechanisms <ref> [5, 9] </ref>. The research presented in this paper is orthogonal to and synergistic with this work.
Reference: [10] <author> S. P. Midkiff and D. A. Padua. </author> <title> Compiler generated synchronization for DO loops. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 544-551, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Related Work In this section we survey related work in the area of synchronization optimizations for parallel computing and database concurrency control. 9.1 Automatically Parallelized Scientific Computations Previous parallelizing compiler research in the area of synchronization optimization has focused almost exclusively on synchronization optimizations for parallel loops in scientific computations <ref> [10] </ref>. The natural implementation of a parallel loop requires two synchronization constructs: an initiation construct to start all processors executing loop iterations, and a barrier construct at the end of the loop.
Reference: [11] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy task creation: a technique for increasing the granularity of parallel programs. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 185-197, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: It then inserts acquire and release constructs into operations that access the objects. These constructs make operations execute atomically in the context of the parallel computation. At each operation invocation site the compiler generates code that executes the operation in parallel. A straightforward application of lazy task creation techniques <ref> [11] </ref> can increase the granularity of the resulting parallel computation.
Reference: [12] <author> M. Rinard and P. Diniz. </author> <title> Commutativity analysis: A new analysis framework for parallelizing compilers. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Program Language Design and Implementation, </booktitle> <address> Philadel-phia, PA, </address> <month> May </month> <year> 1996. </year> <note> (An extended version of this document can be found at the authors' web address: http://www.cs.ucsb.edu/~fmartin,pedrog). </note>
Reference-contexts: An atomic operation first acquires the lock for the data that it manipulates, accesses the data, then releases the lock. We have implemented a compiler designed to automatically parallelize object-based computations that manipulate irregular, pointer-based data structures. This compiler uses commutativity analysis <ref> [12] </ref> as its primary analysis paradigm. For the generated program to execute correctly, each operation in the generated parallel code must execute atomically. The automatically generated code therefore contains mutual exclusion locks and constructs that acquire and release these locks. <p> This paper presents the analysis algorithms and program transformations that the compiler uses to automatically coarsen the lock granularity. We have implemented these algorithms in the context of a parallelizing compiler for object-based programs <ref> [12] </ref>; they are used to reduce lock overhead in the generated parallel code. This paper also presents experimental results that characterize the performance impact of using the lock coarsening algorithms in the compiler. The results show the algorithms to be effective in reducing the lock overhead to negligible levels. <p> If the original version does not deadlock, the transformed version can not deadlock. 7 Experimental Results We have implemented the lock coarsening algorithms described in Sections 5 and 6 and integrated them into a prototype compiler that uses commutativity analysis <ref> [12] </ref> as its primary analysis paradigm. In this section we present experimental results that characterize the performance impact of using the lock coarsening algorithms in a parallelizing compiler. <p> hierarchical N-body solver [1] and the Water [13] code. 7.1 The Compilation System Although it is possible to apply the lock coarsening algorithms to any parallel program that conforms to the model of computation in Section 4, they were developed in the context of a parallelizing compiler for object-based languages <ref> [12] </ref>. This compiler uses a new analysis framework called commutativity analysis to automatically generate parallel code. Commutativity analysis exploits the structure present in the object-based programming paradigm to view the computation as composed of a sequence of operations on objects.
Reference: [13] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In this section we present experimental results that characterize the performance impact of using the lock coarsening algorithms in a parallelizing compiler. We report performance results for two automatically parallelized scientific applications: the Barnes-Hut hierarchical N-body solver [1] and the Water <ref> [13] </ref> code. 7.1 The Compilation System Although it is possible to apply the lock coarsening algorithms to any parallel program that conforms to the model of computation in Section 4, they were developed in the context of a parallelizing compiler for object-based languages [12].
Reference: [14] <author> C. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 144-155, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year> <month> 22 </month>
Reference-contexts: The majority of synchronization optimization research has concentrated on removing barriers or converting barrier synchronization constructs to more efficient synchronization constructs such as counters <ref> [14] </ref>. Several researchers have also explored optimizations geared towards exploiting more fine grained concurrency available within loops [4]. These optimizations automatically insert one-way synchronization constructs such as post and wait to implement loop-carried data dependences.
References-found: 14

