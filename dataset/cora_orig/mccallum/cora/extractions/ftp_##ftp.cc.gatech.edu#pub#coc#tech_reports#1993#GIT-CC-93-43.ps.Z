URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-43.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Email: bodhi@watson.ibm.com schwan@cc.gatech.edu  
Title: Implementation of Scalable Blocking Locks using an Adaptative Thread Scheduler  
Author: Bodhisattwa Mukherjee Karsten Schwan T. J. 
Web: GIT-CC-93-43  
Address: Yorktown Heights, NY 10598 Atlanta, GA 30332  Atlanta, Georgia 30332-0280  
Affiliation: Watson Research Center College of Computing International Business Machines Georgia Institute of Technology  College of Computing Georgia Institute of Technology  
Abstract: Blocking locks are commonly used in parallel programs to improve application performance and system throughput. However, most implementations of such locks suffer from two major problems latency and scalability. In this paper, we propose an implementation of blocking locks using scheduler adaptation which exploits the interaction between thread schedulers and locks. By experimentation using well-known multiprocessor applications on a KSR2 multiprocessor, we demonstrate how such an implementation considerably reduces the latency and improves the scalability of blocking locks. 
Abstract-found: 1
Intro-found: 1
Reference: [ABLL92] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <journal> Transactions on Computer Systems, ACM, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Several research projects address the first problem by providing support for either (1) to recover from preemptions inside critical sections or (2) to prevent or avoid thread preemption within critical sections. For example, the scheduler activation scheme <ref> [ABLL92] </ref> sends a software interrupt to a processor of an application when the processor scheduler takes a processor away from it. The application may then choose to continue the preempted process if it was executing code inside a critical section.
Reference: [ALL89] <author> Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Specifically, we demonstrate that an implementation of scalable locks with reduced latency: 1. requires elimination of any low-level critical sections which may cause contention thus creating bottlenecks. 2. requires understanding and exploiting the cache characteristics of the underlying hardware. This was also demonstrated by Anderson <ref> [ALL89] </ref> for bus based machines such as Sequent Symmetry. Additionally, we show that adaptation of thread schedulers can be used to improve cache locality for implementation of blocking locks thus improving application performance.
Reference: [BBB + 91] <author> D. Bailey, E. Barszcz, J. Barton, D. Browning, R. Carter, L. Dagum, R. Fatoohi, P. Freder-ickson, T. Lasinski, R. Schreiber, H. Simon, V. Venkatakrishnan, and S. Weeratunga. </author> <title> The NAS parallel benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <month> Fall </month> <year> 1991. </year>
Reference-contexts: Integer Sort. Integer Sort (IS) is part of the Numerical Aerodynamic Simulation (NAS) parallel benchmark suite <ref> [BBB + 91] </ref>. It is used in "particle-in-cell method" applications to implement a bucket sort. In addition to an array of locks, used to allow portions of data to be locked at a time, it uses four barriers to synchronize its computation phases.
Reference: [Bla90] <author> D. Black. </author> <title> Scheduling support for concurrency and parallelism in the Mach operating system. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year> <month> 14 </month>
Reference-contexts: As a part of our research on adaptive kernels for multiprocessors, we have constructed a framework for reconfigurable schedulers which can detect application behavior and adapts itself to suit the changing requirements of dynamic applications. Earlier operating system kernels such as Synthesis [MP89, PM91] and Mach <ref> [Bla90] </ref> have already demonstrated the usefulness of such dynamic adaptation of scheduler attributes (such as priority and time quanta) to improve system throughput. In this paper, we investigate the possibility of using scheduler adaptation to improve performance of other kernel components.
Reference: [CMS93] <author> Christian Clemencon, Bodhisattwa Mukherjee, and Karsten Schwan. </author> <title> Distributed shared abstractions (dsa) on large-scale multiprocessors. </title> <booktitle> In Proc. of the Fourth USENIX Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pages 227-246, </pages> <month> September </month> <year> 1993. </year>
Reference: [EGSM94] <author> Greg Eisenhauer, Weiming Gu, Karsten Schwan, and Niru Mallavarupu. </author> <title> Falcon toward interactive parallel programs: The on-line steering of a molecular dynamics application. </title> <booktitle> In Proceedings of The Third International Symposium on High-Performance Distributed Computing (HPDC-3), </booktitle> <pages> pages 26-34, </pages> <month> August </month> <year> 1994. </year> <note> Also available as Georgia Institute of Technology Tech Report GIT-CC-94-08. </note>
Reference-contexts: Such adaptation can be either synchronous (where monitoring and adaptation are performed in-line and synchronously with the execution of certain operations on the software component [MS93b, MS93c]), or asynchronous (where monitoring and adaptation are performed by additional execution threads dedicated to these tasks <ref> [EGSM94, Muk94] </ref>). The adaptive scheduler component presented in this paper uses an asynchronous scheme. In our past research, we have used a similar scheme to implement adaptive multiprocessor locks which dynamically detect an application's locking patterns and adapt themselves to suit its changing requirements [MS93c, Muk94], thus improving application performance.
Reference: [ELS88] <author> J. Edler, J. Lipkis, and E. Schonberg. </author> <title> Process management for highly parallel unix systems. </title> <booktitle> In Proceedings of the USENIX Workshop on UNIX and Supercomputers, </booktitle> <pages> pages 1-17, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: The application may then choose to continue the preempted process if it was executing code inside a critical section. The Psyche [MSLM91] and Symunix <ref> [ELS88] </ref> operating systems take the other approach by providing prevention techniques based on improved communication between applications and the scheduler.
Reference: [Gop88] <author> Prabha Gopinath. </author> <title> Programming and Execution of Object-Based, Parallel, Hard Real-Time Applications. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Sciences, The Ohio State University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: Finally, Section 6 concludes the paper by summarizing the contributions of this paper. 2 Summary of Past Research Dynamic Software Adaptation. A dynamically adaptable software system is a feedback control system driven by data generated by a monitoring module. Such systems may change software components in many ways <ref> [Gop88] </ref>, including: (1) substitution of software module by others with different characteristics ("module based approach") [MS94], (2) changes to the interaction structure of an application ("building block based approach") [Muk94], and (3) changes to selected parameters determining the implementation and internal structure of a software module ("parameter based approach") [MS94].
Reference: [IFKR92] <author> H. Burkhardt III, S. Frank, B. Knobe, and J. Rothnie. </author> <title> Overview of the ksr1 computer system. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendall Square Research, </institution> <address> Boston, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: There are two basic owner state types - non-exclusive and exclusive-ownership. Programmatically, non-exclusive state is the same as read-only state <ref> [IFKR92] </ref>.
Reference: [JLK63] <author> D. Sweeney J.D. Little, K. Murty and C. Karel. </author> <title> An algorithm for the traveling salesman problem. </title> <journal> Operations Research, </journal> <volume> 11, </volume> <year> 1963. </year>
Reference-contexts: In addition to an array of locks, used to allow portions of data to be locked at a time, it uses four barriers to synchronize its computation phases. The details of the parallel algorithm appear in [RSRM93]. TSP. The Traveling Sales Person application is implemented using the LMSK algorithm <ref> [JLK63] </ref>, a branch-and-bound algorithm that proceeds by dynamic construction of a search tree, at the root of which is a description of the initial problem. Independent subproblems are generated by selection of specific edges from the graph and creation of children of the root.
Reference: [LA94] <author> Beng-Hong Lim and Anant Agarwal. </author> <title> Reactive Synchronization Algorithms for Multiprocessors. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: In our past research, we have used a similar scheme to implement adaptive multiprocessor locks which dynamically detect an application's locking patterns and adapt themselves to suit its changing requirements [MS93c, Muk94], thus improving application performance. A similar lock imple 2 mentation, called reactive locks, was proposed in <ref> [LA94] </ref> by Lim et. al at MIT. The specific results of their research are complementary to our efforts, because `reactive' locks alter the implementation of locks depending on the contention level, whereas adaptive locks implement a hybrid of waiting mechanisms.
Reference: [LS93] <author> E. Lazowska and M. Squillante. </author> <title> Using processor-cache affinity in shared-memory multiprocessor scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-43, </pages> <month> February </month> <year> 1993. </year>
Reference: [MP89] <author> Henry Massalin and Calton Pu. </author> <title> Threads and input/output in the Synthesis kernel. </title> <booktitle> In Proceedings of the 12th Symposium on Operating Systems Principles, </booktitle> <pages> pages 191-201, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: As a part of our research on adaptive kernels for multiprocessors, we have constructed a framework for reconfigurable schedulers which can detect application behavior and adapts itself to suit the changing requirements of dynamic applications. Earlier operating system kernels such as Synthesis <ref> [MP89, PM91] </ref> and Mach [Bla90] have already demonstrated the usefulness of such dynamic adaptation of scheduler attributes (such as priority and time quanta) to improve system throughput. In this paper, we investigate the possibility of using scheduler adaptation to improve performance of other kernel components.
Reference: [MS93a] <author> Bodhisattwa Mukherjee and Karsten Schwan. </author> <title> Application dependent heterogeneous thread schedulers. </title> <type> Technical Report GIT-CC-93/43, </type> <institution> College of Computing, Georgia Tech, </institution> <month> August </month> <year> 1993. </year> <note> To be completed. </note>
Reference: [MS93b] <author> Bodhisattwa Mukherjee and Karsten Schwan. </author> <title> Experiments with a configurable lock for multiprocessors. </title> <booktitle> In Proc. of the twenty second International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 205-208, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: By setting values to these attributes (like tuning an AM radio tuner), an implementation is tuned for an application. Such adaptation can be either synchronous (where monitoring and adaptation are performed in-line and synchronously with the execution of certain operations on the software component <ref> [MS93b, MS93c] </ref>), or asynchronous (where monitoring and adaptation are performed by additional execution threads dedicated to these tasks [EGSM94, Muk94]). The adaptive scheduler component presented in this paper uses an asynchronous scheme.
Reference: [MS93c] <author> Bodhisattwa Mukherjee and Karsten Schwan. </author> <title> Improving performance by use of adaptive objects: Experimentation with a configurable multiprocessor thread package. </title> <booktitle> In Proc. of the second International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 59-66, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: By setting values to these attributes (like tuning an AM radio tuner), an implementation is tuned for an application. Such adaptation can be either synchronous (where monitoring and adaptation are performed in-line and synchronously with the execution of certain operations on the software component <ref> [MS93b, MS93c] </ref>), or asynchronous (where monitoring and adaptation are performed by additional execution threads dedicated to these tasks [EGSM94, Muk94]). The adaptive scheduler component presented in this paper uses an asynchronous scheme. <p> The adaptive scheduler component presented in this paper uses an asynchronous scheme. In our past research, we have used a similar scheme to implement adaptive multiprocessor locks which dynamically detect an application's locking patterns and adapt themselves to suit its changing requirements <ref> [MS93c, Muk94] </ref>, thus improving application performance. A similar lock imple 2 mentation, called reactive locks, was proposed in [LA94] by Lim et. al at MIT. <p> The poor scalability of the above implementation of blocking locks is attributed to the following factors: 1. The critical section protected by the sub lock generates considerable contention (especially for the centralized queue implementation of the TSP application <ref> [MS93c] </ref>) resulting in significant subpage movements. 2. Every queue and dequeue operation, since performed in different processors, causes in creased cache invalidation signals and subpage movements. 3. The lock state variable also increases subpage movements since different remote processors write to this variable.
Reference: [MS94] <author> Bodhisattwa Mukherjee and Karsten Schwan. </author> <title> Adaptive operating system abstractions: A case study of multiprocessor locks. </title> <type> Technical Report GIT-CC-94/39, </type> <institution> College of Computing, Georgia Tech, </institution> <month> June </month> <year> 1994. </year> <note> Submitted to ACM TOCS. 15 </note>
Reference-contexts: A dynamically adaptable software system is a feedback control system driven by data generated by a monitoring module. Such systems may change software components in many ways [Gop88], including: (1) substitution of software module by others with different characteristics ("module based approach") <ref> [MS94] </ref>, (2) changes to the interaction structure of an application ("building block based approach") [Muk94], and (3) changes to selected parameters determining the implementation and internal structure of a software module ("parameter based approach") [MS94]. <p> ways [Gop88], including: (1) substitution of software module by others with different characteristics ("module based approach") <ref> [MS94] </ref>, (2) changes to the interaction structure of an application ("building block based approach") [Muk94], and (3) changes to selected parameters determining the implementation and internal structure of a software module ("parameter based approach") [MS94]. The adaptation of kernel components presented in this paper (as used in C-kernel, an adaptive operating system kernel for multiprocessors being developed at Georgia Tech [Muk94]) uses the "parameter based approach", where individual attributes of a component's implementation are changed in order to improve a program's performance. <p> Poorly distributed workloads: For dynamic applications, it may be infeasible or difficult to distribute computational loads evenly across processors and/or threads resulting in processor sharing by multiple application threads. The branch and bound parallel algorithm for the Traveling Sales Person program <ref> [MS94, Muk94] </ref> is an example of this class. 4. Multiprogramming: If a system supports multiprogramming, for increased throughput it is necessary (1) to schedule multiple threads to timeshare each processor, and (2) to block a thread instead of spinning indefinitely for shared resources. <p> Blocking locks can improve performance, because they permit preemption of a thread waiting for a critical section, thereby allowing other threads in the processor to perform useful work. A detail discussion of this topic with results of experimentation performed on a KSR2 multiprocessor appears in <ref> [MS94] </ref> and [Muk94]. In this paper, we present an algorithm for scalable blocking locks on a KSR2 multiprocessor to demonstrate how the interaction of thread schedulers and thread synchronization can be utilized to improve cache-locality of applications, thus improving its performance. <p> To summarize, the poststore operation is useful when there are multiple readers but one writer, whereas the prefetch operation is useful when there are multiple writers. 4 Classical Implementation of Blocking Locks As demonstrated by experimentation on BBN Butterfly and KSR2 multiprocessors in our earlier work <ref> [MS94, Muk94] </ref>, applications using blocking locks will outperform those using spin locks if (1) the length of the critical section is large and (2) there are available threads to perform useful work when the contender thread blocks. <p> The performance of these three parallel applications, Cholesky, Integer Sort, and TSP (the synchronization pattern for the locks of these applications appear in <ref> [MS94, Muk94] </ref>) using the above implementation of blocking locks on a KSR2 multiprocessor (run on 1-30 processors) is shown in Figures 2, 3, and 4.
Reference: [MSLM91] <author> B. Marsh, M. Scott, T. Leblanc, and E. Markatos. </author> <title> First-class user-level threads. </title> <booktitle> In Pro--ceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 110-21, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: For example, the scheduler activation scheme [ABLL92] sends a software interrupt to a processor of an application when the processor scheduler takes a processor away from it. The application may then choose to continue the preempted process if it was executing code inside a critical section. The Psyche <ref> [MSLM91] </ref> and Symunix [ELS88] operating systems take the other approach by providing prevention techniques based on improved communication between applications and the scheduler.
Reference: [Muk91] <author> Bodhisattwa Mukherjee. </author> <title> A portable and reconfigurable threads package. </title> <booktitle> In Proceedings of the Sun User Group Technical Conference, </booktitle> <pages> pages 101-112, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Once a tour is found for a subproblem, the search space may be pruned by deletion of the unnecessary branches and leaf nodes. The algorithm is implemented as a collection of asynchronous cooperating threads <ref> [Muk91] </ref> synchronizing using four locks [Muk94], most of which address the shared queue in which subproblems are stored.
Reference: [Muk94] <author> Bodhisattwa Mukherjee. </author> <title> Reconfigurable Multiprocessor Operating System Kernel for High Performance Computing. </title> <type> PhD thesis, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Such systems may change software components in many ways [Gop88], including: (1) substitution of software module by others with different characteristics ("module based approach") [MS94], (2) changes to the interaction structure of an application ("building block based approach") <ref> [Muk94] </ref>, and (3) changes to selected parameters determining the implementation and internal structure of a software module ("parameter based approach") [MS94]. The adaptation of kernel components presented in this paper (as used in C-kernel, an adaptive operating system kernel for multiprocessors being developed at Georgia Tech [Muk94]) uses the "parameter based <p> ("building block based approach") <ref> [Muk94] </ref>, and (3) changes to selected parameters determining the implementation and internal structure of a software module ("parameter based approach") [MS94]. The adaptation of kernel components presented in this paper (as used in C-kernel, an adaptive operating system kernel for multiprocessors being developed at Georgia Tech [Muk94]) uses the "parameter based approach", where individual attributes of a component's implementation are changed in order to improve a program's performance. These attributes implement a set of "tuning knobs". By setting values to these attributes (like tuning an AM radio tuner), an implementation is tuned for an application. <p> Such adaptation can be either synchronous (where monitoring and adaptation are performed in-line and synchronously with the execution of certain operations on the software component [MS93b, MS93c]), or asynchronous (where monitoring and adaptation are performed by additional execution threads dedicated to these tasks <ref> [EGSM94, Muk94] </ref>). The adaptive scheduler component presented in this paper uses an asynchronous scheme. In our past research, we have used a similar scheme to implement adaptive multiprocessor locks which dynamically detect an application's locking patterns and adapt themselves to suit its changing requirements [MS93c, Muk94], thus improving application performance. <p> The adaptive scheduler component presented in this paper uses an asynchronous scheme. In our past research, we have used a similar scheme to implement adaptive multiprocessor locks which dynamically detect an application's locking patterns and adapt themselves to suit its changing requirements <ref> [MS93c, Muk94] </ref>, thus improving application performance. A similar lock imple 2 mentation, called reactive locks, was proposed in [LA94] by Lim et. al at MIT. <p> In this paper, we apply this adaptation scheme to implement adaptive thread schedulers which are used to implement scalable blocking locks for multiprocessors. A number of schemes for synchronous and asynchronous adaptation used to implement configurable kernel components are presented and compared in <ref> [Muk94] </ref> which are not repeated here for brevity. 3 Blocking Locks Parallel applications commonly use multiple threads per processor. Such applications belong to various classes including: 1. Communication-Computation overlap: Multiple threads are used to overlap thread communication with computational activities in "communication intensive" programs. <p> Poorly distributed workloads: For dynamic applications, it may be infeasible or difficult to distribute computational loads evenly across processors and/or threads resulting in processor sharing by multiple application threads. The branch and bound parallel algorithm for the Traveling Sales Person program <ref> [MS94, Muk94] </ref> is an example of this class. 4. Multiprogramming: If a system supports multiprogramming, for increased throughput it is necessary (1) to schedule multiple threads to timeshare each processor, and (2) to block a thread instead of spinning indefinitely for shared resources. <p> Blocking locks can improve performance, because they permit preemption of a thread waiting for a critical section, thereby allowing other threads in the processor to perform useful work. A detail discussion of this topic with results of experimentation performed on a KSR2 multiprocessor appears in [MS94] and <ref> [Muk94] </ref>. In this paper, we present an algorithm for scalable blocking locks on a KSR2 multiprocessor to demonstrate how the interaction of thread schedulers and thread synchronization can be utilized to improve cache-locality of applications, thus improving its performance. <p> To summarize, the poststore operation is useful when there are multiple readers but one writer, whereas the prefetch operation is useful when there are multiple writers. 4 Classical Implementation of Blocking Locks As demonstrated by experimentation on BBN Butterfly and KSR2 multiprocessors in our earlier work <ref> [MS94, Muk94] </ref>, applications using blocking locks will outperform those using spin locks if (1) the length of the critical section is large and (2) there are available threads to perform useful work when the contender thread blocks. <p> This section presents a few selected results of experiments performed with the above implementation of blocking locks using three multiprocessor programs derived from commonly used 7 8 benchmark suites, the algorithms and implementations of which are briefly presented next (see <ref> [Muk94] </ref> for details). Cholesky. The Cholesky application belongs to the SPLASH [SWG92] parallel benchmark suite. It performs parallel Cholesky factorization of a sparse positive definite matrix. <p> Once a tour is found for a subproblem, the search space may be pruned by deletion of the unnecessary branches and leaf nodes. The algorithm is implemented as a collection of asynchronous cooperating threads [Muk91] synchronizing using four locks <ref> [Muk94] </ref>, most of which address the shared queue in which subproblems are stored. <p> The performance of these three parallel applications, Cholesky, Integer Sort, and TSP (the synchronization pattern for the locks of these applications appear in <ref> [MS94, Muk94] </ref>) using the above implementation of blocking locks on a KSR2 multiprocessor (run on 1-30 processors) is shown in Figures 2, 3, and 4.
Reference: [Ous82] <author> J. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proceedings of Distributed Computing Systems Conference, </booktitle> <pages> pages 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference: [PM91] <author> Calton Pu and Henry Massalin. </author> <title> Quaject composition in the Synthesis kernel. </title> <booktitle> In Proceedings of the International Workshop on Object Orientation in Operating Systems, </booktitle> <pages> pages 17-18, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: As a part of our research on adaptive kernels for multiprocessors, we have constructed a framework for reconfigurable schedulers which can detect application behavior and adapts itself to suit the changing requirements of dynamic applications. Earlier operating system kernels such as Synthesis <ref> [MP89, PM91] </ref> and Mach [Bla90] have already demonstrated the usefulness of such dynamic adaptation of scheduler attributes (such as priority and time quanta) to improve system throughput. In this paper, we investigate the possibility of using scheduler adaptation to improve performance of other kernel components.
Reference: [RSRM93] <author> U. Ramachandran, G. Shah, S. Ravikumar, and J. Muthukumarasamy. </author> <title> Scalability study of the KSR-1. </title> <booktitle> In Proceedings of the twenty second International Conference on Parallel Processing, </booktitle> <pages> pages I-237-240, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: It is used in "particle-in-cell method" applications to implement a bucket sort. In addition to an array of locks, used to allow portions of data to be locked at a time, it uses four barriers to synchronize its computation phases. The details of the parallel algorithm appear in <ref> [RSRM93] </ref>. TSP. The Traveling Sales Person application is implemented using the LMSK algorithm [JLK63], a branch-and-bound algorithm that proceeds by dynamic construction of a search tree, at the root of which is a description of the initial problem.
Reference: [SWG92] <author> J. Pal Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Cholesky. The Cholesky application belongs to the SPLASH <ref> [SWG92] </ref> parallel benchmark suite. It performs parallel Cholesky factorization of a sparse positive definite matrix.
Reference: [SZG91] <author> Karsten Schwan, Hongyi Zhou, and Ahmed Gheith. </author> <title> Multiprocessor real-time threads. </title> <journal> Operating Systems Review, </journal> <volume> 25(4) </volume> <pages> 35-46, </pages> <month> Oct. </month> <year> 1991. </year> <note> Also appears in the Jan. 1992 issue of Operating Systems Review. </note>
Reference: [WKS94] <author> R. Wisniewski, L. Kontothanassis, and M. Scott. </author> <title> Scalable spin locks for multiprogrammed systems. </title> <booktitle> In Proceedings of the Eighth International Parallel Processing Symposium, </booktitle> <pages> pages 583-589, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Recent research in operating systems for parallel and distributed machines has demonstrated that program and operating system performance can be improved by considering the interactions among the different system components used by a program. Ongoing work on synchronization dependent scheduling <ref> [WKS94] </ref> explores a few such problems caused by the interaction of thread schedulers with synchronization primitives. These problems can take the following two forms: 1. Problems due to involuntary preemption. <p> For example, the Psyche scheduler provides a warning allowing processes to estimate whether the remaining time quanta is sufficient to execute a critical section, whereas the Symunix scheduler allows processes to issue requests not to be preempted. A similar approach is taken by Scott et. al. <ref> [WKS94] </ref> in their design of scalable spin locks. A state variable associated with each process's context is used by applications and operating systems to set the state of processes to either preemptible or non-preemptible at different phases of execution. The research presented in this paper is complementary to ongoing research. <p> Although this paper does not address the problem of untimely thread preemption as listed in Section 1, such a 13 framework can be easily used to provide the support necessary (as listed in <ref> [WKS94] </ref>) to prevent such thread preemption. 6 Concluding Remarks In this paper, we compared the performance of different implementations of blocking locks on a KSR2 multiprocessor.
References-found: 26

