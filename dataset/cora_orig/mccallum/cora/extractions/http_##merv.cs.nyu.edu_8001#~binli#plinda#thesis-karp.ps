URL: http://merv.cs.nyu.edu:8001/~binli/plinda/thesis-karp.ps
Refering-URL: http://merv.cs.nyu.edu:8001/~binli/plinda/
Root-URL: http://www.cs.nyu.edu
Title: Fault-tolerant Parallel Processing Combining Linda, Checkpointing, and Transactions  
Author: Karpjoo Jeong 
Degree: A dissertation in the Department of Computer Science submitted to the faculty of the Graduate School of Arts and Sciences in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Approved: Professor Dennis Shasha  
Note: Research Advisor  
Address: New York University.  
Affiliation: at  
Date: January, 1996  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 207-216, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Tuple space is checkpointed every 100 seconds. 75 5.4.3 Biological Pattern Discovery In this subsection, we present the results of our experiment to apply PLinda to a data mining application. We parallelized a sequential biological pattern discovery program [63] using PLinda. These types of data mining applications <ref> [1, 23, 35] </ref> are interesting because they are compute-intensive and they are usually coarse grain parallel problems.; First, we explain the problem and the sequential approach. We then describe our parallel approach and show the performance results.
Reference: [2] <author> Yeshayahu Artsy and Raphael Finkel. </author> <title> Designing a process migration facility: </title> <booktitle> The Charlotte experience. IEEE Computer, </booktitle> <pages> pages 47-56, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Workstations are only intermittently idle as a rule. A system which can utilize intermittently idle workstations can make computing on networked workstations very cost effective. In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref> to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. <p> In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems <ref> [2, 11, 25, 44] </ref>. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. Popular systems include Linda [13], PVM [60, 24], MPI [29] and Express. Unfortunately, few support fault tolerance or utilization of idle workstations. <p> Unfortunately, few support fault tolerance or utilization of idle workstations. Also, there has been a considerable amount of work [37, 51] on fault tolerance in distributed systems, but most of the work has not addressed the problem of utilizing idle workstations for parallel computation. There have been research efforts <ref> [2, 14, 18, 11, 25, 48, 53] </ref> to develop software systems to utilize idle workstations and some of them [2, 14, 11, 25] are designed to address fault tolerance. <p> There have been research efforts [2, 14, 18, 11, 25, 48, 53] to develop software systems to utilize idle workstations and some of them <ref> [2, 14, 11, 25] </ref> are designed to address fault tolerance. However, to our knowledge, there is no system to aim at supporting both parallel processing and fault tolerance together with the utilization of idle workstations. <p> Therefore, it is crucial to guarantee that workstations will be used only while they are idle. To address this issue, various work-stealing systems have been developed <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. During execution, these systems monitor the idleness status of 2 workstations and migrate processes from busy or overloaded machines to idle or under--utilized ones. <p> There has been a considerable amount of research done on how to utilize idle or under-utilized workstations connected by a network <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems [2, 11, 25, 44] which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems <ref> [2, 11, 25, 44] </ref> which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> For idleness detection and scheduling we do not intend to re-invent techniques for PLinda. Instead, we take advantage of various techniques already developed by other work-stealing systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. The process migration issue is 48 addressed by treating "busy" machines (i.e., those which are being used by their owners) as failed; that is, owner activity is handled as failure. <p> Also, because the programmer can customize fault tolerance based on application characteristics, PLinda is an efficient and convenient way to execute semi-parallel applications. The parallel systems which intend to utilize idle workstations for parallel applications <ref> [2, 11, 21, 25, 44] </ref> either depend on support from the underlying operating systems or assume restricted programming models or characteristics about the applications they are intended for. Few of them can tolerate failure during parallel computation.
Reference: [3] <author> D. E. Bakken and R. D. Schlichting. </author> <title> Tolerating failures in the bag-of-tasks programming paradigm. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 248-255, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Transactions are especially effective in the Linda model where the tuple space (shared memory) is the only mechanism for communication between processes and storage of shared data. For the same reason, most other fault tolerant work on Linda also supports some similar mechanism, though with less functionality <ref> [3, 4] </ref>. In this section, we discuss the issues that are raised when transactions are used for robust parallel computation. First, we explain transactions in the context of databases 1 . Transactions are an abstraction with the following ACID properties: * Atomicity. <p> The backup process takes over the task of the failed process. Unlike transparent approaches where the backup process continues execution transparently, PLinda requires backup processes to take over tasks of their failed processes explicitly. 2.8 Related work 2.8.1 Fault Tolerance Work on Linda In <ref> [3, 4] </ref>, Bakken and Schlichting present FT-Linda, a variant of Linda that addresses fault tolerance. For tuple space reliability, FT-Linda assumes a set of replicated tuple spaces connected together by an ordered atomic broadcast network. Thus, FT-Linda is better than PLinda for fault tolerant applications where availability is important. <p> In the PLinda project, our major research focus has been on how to incorporate transactions into the Linda model without entailing high runtime overhead or complicating the model. Unlike other fault tolerance work <ref> [3, 4] </ref> on Linda which attempts to use only some limited form of transactions and therefore complicates the Linda programming paradigm, we have sought to integrate the full functionality of transactions into Linda.
Reference: [4] <author> D. E. Bakken and R. D. Schlichting. </author> <title> Supporting fault tolerant parallel programming in Linda. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <year> 1994. </year>
Reference-contexts: In fact, such a scheme has already been demonstrated in fault-tolerant parallel computing systems such as FT-Linda <ref> [4] </ref> and PLinda [38]. * Using workstations connected by LANs or even WANs, large scale high performance parallel processing is possible for these problems because computation basically consists of a large number of mostly independent tasks. 1 However, it is difficult for the end-user to find workstations which are idle for <p> In this way, every process has the same state in a failure-free execution. Upon disagreement, the minority is ignored. 1.3.3 Fault-tolerant Programming Languages Various fault-tolerant programming languages have been developed to ease the task of constructing fault-tolerant programs. Examples are Argus [47], Avalon [26], Fault-tolerant Concurrent C [19], FT-Linda <ref> [4] </ref>, Orca [41] and FT-SR [56], In general, these fault-tolerant programming languages are distinguished by what program structuring paradigms they support since they all assume the fail-stop processor failure model. Argus and Avalon support the object/action model. <p> Argus and Avalon support the object/action model. Reliability and concurrency control are supported by saving local state to disk and running every object invocation as a transaction or a nested transaction. Fault-tolerant Concurrent C [19] and FT-Linda <ref> [4] </ref> support the replicated state machine paradigm. The primary extension of fault-tolerant Concurrent C which extends Concurrent C [33] is a set of primitives for replicating processes. The runtime system guarantees that all the replicas of a process behave as if they were a single process. <p> The mechanisms allow a programmer to take advantage of the characteristics of a given application when making it failure-resilient. However, the drawback is additional programming work. For coarse grain parallel applications which have simple control structures, the additional programming work is usually negligible <ref> [4, 38] </ref> and therefore the customization approach is more appropriate. In PLinda, the transaction commit mechanism stores data in the volatile tuple space 6 and is therefore extremely lightweight 1 . Such a commit mechanism does not guaran-tee the durability of committed transactions, since the tuple space might fail. <p> Transactions are especially effective in the Linda model where the tuple space (shared memory) is the only mechanism for communication between processes and storage of shared data. For the same reason, most other fault tolerant work on Linda also supports some similar mechanism, though with less functionality <ref> [3, 4] </ref>. In this section, we discuss the issues that are raised when transactions are used for robust parallel computation. First, we explain transactions in the context of databases 1 . Transactions are an abstraction with the following ACID properties: * Atomicity. <p> The backup process takes over the task of the failed process. Unlike transparent approaches where the backup process continues execution transparently, PLinda requires backup processes to take over tasks of their failed processes explicitly. 2.8 Related work 2.8.1 Fault Tolerance Work on Linda In <ref> [3, 4] </ref>, Bakken and Schlichting present FT-Linda, a variant of Linda that addresses fault tolerance. For tuple space reliability, FT-Linda assumes a set of replicated tuple spaces connected together by an ordered atomic broadcast network. Thus, FT-Linda is better than PLinda for fault tolerant applications where availability is important. <p> In the PLinda project, our major research focus has been on how to incorporate transactions into the Linda model without entailing high runtime overhead or complicating the model. Unlike other fault tolerance work <ref> [3, 4] </ref> on Linda which attempts to use only some limited form of transactions and therefore complicates the Linda programming paradigm, we have sought to integrate the full functionality of transactions into Linda.
Reference: [5] <author> Arash Baratloo, Partha Dasgupta, Zvi M. Kedem, and Dmitri Krakovsky. </author> <title> Calypso goes to wall street: A case study. </title> <booktitle> In Proceedings of the Third International Conference on Artificial Intelligence Applications on Wall Street, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The program consists of three phases: 1. Input the observed and analyzed market data. 2. Calculate the value of the OAS for each bond in the index. 3. Compute the market-weighted average of the resulting OAS and option values. See <ref> [10, 5] </ref> for details. We parallelized the second phase because the computation in the second phase is most compute-intensive and each bond can be analyzed independently of other bonds. Since this problem is easily parallelizable, parallelizing the sequential code using PLinda was straightforward and took only a few hours.
Reference: [6] <author> P.A. Bernstein, Vassos Hadzilacos, and Nathan Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1987. </year>
Reference-contexts: In fact a stable tuple space and knowledge of the critical portions of the state of the failing process are all that is needed. 1.4.3 Linda and Transactions Transactions are in particular effective for Linda's tuple space abstraction because tuple space is a shared data resource much like a database <ref> [34, 6] </ref>. Tuple space can be optimized for transactions independently because it is a separate data storage from process address spaces. Finally, in tuple space, there is a logical unit of data (which is a tuple) for manipulation which is independent of a language type system. <p> Section 2.8 compares PLinda with other fault-tolerant Linda-variant systems and programming languages/systems that support transactions. Section 2.9 concludes this chapter. 2.2 Transactions and Robust Parallel Computation in the Linda Model There are several fault tolerance abstractions for building failure-resilient applications.Among the popular ones are transactions <ref> [6, 34] </ref> and ordered atomic broadcast [7, 8, 40, 61]. Transactions have been mostly used for database applications such as bank or airline reservation applications where reliable management of persistent data is crucial. <p> The execution of transactions appears as if they were executed in a serial order. Transactions require concurrency control for this property. The most common implementation technique for concurrency control is two phase locking <ref> [6, 34] </ref>. For a transaction, two phase locking holds locks on accessed data items until the transaction commits, and therefore prevents transactions from accessing uncommitted updates. * Durability. Committed updates in databases survive failure. <p> In our version of two phase locking, write locks are held until commit, but read locks are held only while the read occurs (known as degree two serializability <ref> [6, 34] </ref> in the database world). The write locks prevent transactions from accessing updates made by a transaction until the transaction commits. So, a transaction abort affects no others | it's as if the transaction had never started. <p> However, we plan to add the former approach to the implementation in the future. PLinda is designed to support two kinds of tuple space: * Checkpoint-protected tuple space. Unlike most transaction processing systems <ref> [6, 34] </ref>, PLinda replicates the "transaction-consistent" state of tuple space on disk only periodically called tuple space checkpoint. Transaction commit operations do not require updates to be written to disk before they are finished. <p> For example, the approach can make arbitrary distributed programs fault-tolerant in a programmer-transparent manner. The technique used in backward error recovery is checkpointing and rollback. Checkpointing techniques have been widely used for database systems to make recovery fast <ref> [6, 34] </ref> Since database applications do not require process resiliency, those techniques don't recover process state. However, they ensure that committed transactions are serialized and never lost. They must do so because the user can see results of committed transactions immediately and expect them to survive failure. <p> The prototype is built on UNIX and TCP/IP using C++. The implementation consists of four major components: the server program, the daemon program, the administration program, and the client library. There is a vast amount of literature about the implementation of tuple space and transaction processing systems <ref> [9, 6, 12, 34, 46] </ref>. In the design of the PLinda runtime system, we have focused on how to incorporate existing implementation techniques for transactions into those for tuple space. Throughout this chapter, we therefore concentrate on presenting implementation strategies rather than describing implementation details.
Reference: [7] <author> K. P. Birman. </author> <title> The process group approach to reliable distributed computing. </title> <type> Technical report, </type> <institution> Cornell University, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Section 2.9 concludes this chapter. 2.2 Transactions and Robust Parallel Computation in the Linda Model There are several fault tolerance abstractions for building failure-resilient applications.Among the popular ones are transactions [6, 34] and ordered atomic broadcast <ref> [7, 8, 40, 61] </ref>. Transactions have been mostly used for database applications such as bank or airline reservation applications where reliable management of persistent data is crucial. PLinda is a research effort to apply the transaction processing technology to a different class of applications, robust parallel computation.
Reference: [8] <author> K. P. Birman and T. A. Joseph. </author> <title> Exploiting virtual synchrony in distributed systems. </title> <journal> ACM Operating Systems Review, </journal> <volume> 21(5) </volume> <pages> 123-138, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Section 2.9 concludes this chapter. 2.2 Transactions and Robust Parallel Computation in the Linda Model There are several fault tolerance abstractions for building failure-resilient applications.Among the popular ones are transactions [6, 34] and ordered atomic broadcast <ref> [7, 8, 40, 61] </ref>. Transactions have been mostly used for database applications such as bank or airline reservation applications where reliable management of persistent data is crucial. PLinda is a research effort to apply the transaction processing technology to a different class of applications, robust parallel computation.
Reference: [9] <author> R. Bjornson. </author> <title> Linda on Distributed Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: If no matching tuple is found, then in and rd block until a matching tuple is inserted. The difference is that in is destructive (i.e. removes the tuple) while rd is not. 5 A more detailed description of the Linda model is found in <ref> [9, 12, 13, 44, 46] </ref>. Linda has several characteristics which make it popular. First, the model is simple. Accessing tuple space is intuitive and requires only four operations. Second, the model is flexible. <p> The prototype is built on UNIX and TCP/IP using C++. The implementation consists of four major components: the server program, the daemon program, the administration program, and the client library. There is a vast amount of literature about the implementation of tuple space and transaction processing systems <ref> [9, 6, 12, 34, 46] </ref>. In the design of the PLinda runtime system, we have focused on how to incorporate existing implementation techniques for transactions into those for tuple space. Throughout this chapter, we therefore concentrate on presenting implementation strategies rather than describing implementation details.
Reference: [10] <author> F. Black, E. Derman, and W. </author> <title> Toy. One-factor model of interest rates and its application to treasury bond options. </title> <journal> Financial Analysts Journal, </journal> <pages> pages 33-39, </pages> <address> January-February 90. </address>
Reference-contexts: The problem is to compute characteristics of the market-weighted averages of the collection of securities grouped by criteria (i.e., indices). Specifically, the Option-Adjusted-Spread (OAS) and the embedded option value of a bond are calculated and the market-weighted average of the resulting OAS and option values are computed <ref> [10] </ref>. We parallelized a sequential program implemented by Dmitri Krakovsky 3 at New York University. The program uses a binomial tree option pricing model [10] for the OAS and the option values of bonds. The program consists of three phases: 1. Input the observed and analyzed market data. 2. <p> the Option-Adjusted-Spread (OAS) and the embedded option value of a bond are calculated and the market-weighted average of the resulting OAS and option values are computed <ref> [10] </ref>. We parallelized a sequential program implemented by Dmitri Krakovsky 3 at New York University. The program uses a binomial tree option pricing model [10] for the OAS and the option values of bonds. The program consists of three phases: 1. Input the observed and analyzed market data. 2. Calculate the value of the OAS for each bond in the index. 3. Compute the market-weighted average of the resulting OAS and option values. <p> The program consists of three phases: 1. Input the observed and analyzed market data. 2. Calculate the value of the OAS for each bond in the index. 3. Compute the market-weighted average of the resulting OAS and option values. See <ref> [10, 5] </ref> for details. We parallelized the second phase because the computation in the second phase is most compute-intensive and each bond can be analyzed independently of other bonds. Since this problem is easily parallelizable, parallelizing the sequential code using PLinda was straightforward and took only a few hours.
Reference: [11] <author> Clemens H. Cap and Volker Strumpen. </author> <title> Efficient parallel computing in distributed workstation environments. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 1221-1234, </pages> <year> 1993. </year>
Reference-contexts: Workstations are only intermittently idle as a rule. A system which can utilize intermittently idle workstations can make computing on networked workstations very cost effective. In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref> to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. <p> In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems <ref> [2, 11, 25, 44] </ref>. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. Popular systems include Linda [13], PVM [60, 24], MPI [29] and Express. Unfortunately, few support fault tolerance or utilization of idle workstations. <p> Unfortunately, few support fault tolerance or utilization of idle workstations. Also, there has been a considerable amount of work [37, 51] on fault tolerance in distributed systems, but most of the work has not addressed the problem of utilizing idle workstations for parallel computation. There have been research efforts <ref> [2, 14, 18, 11, 25, 48, 53] </ref> to develop software systems to utilize idle workstations and some of them [2, 14, 11, 25] are designed to address fault tolerance. <p> There have been research efforts [2, 14, 18, 11, 25, 48, 53] to develop software systems to utilize idle workstations and some of them <ref> [2, 14, 11, 25] </ref> are designed to address fault tolerance. However, to our knowledge, there is no system to aim at supporting both parallel processing and fault tolerance together with the utilization of idle workstations. <p> Therefore, it is crucial to guarantee that workstations will be used only while they are idle. To address this issue, various work-stealing systems have been developed <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. During execution, these systems monitor the idleness status of 2 workstations and migrate processes from busy or overloaded machines to idle or under--utilized ones. <p> There has been a considerable amount of research done on how to utilize idle or under-utilized workstations connected by a network <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems [2, 11, 25, 44] which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems <ref> [2, 11, 25, 44] </ref> which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> For idleness detection and scheduling we do not intend to re-invent techniques for PLinda. Instead, we take advantage of various techniques already developed by other work-stealing systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. The process migration issue is 48 addressed by treating "busy" machines (i.e., those which are being used by their owners) as failed; that is, owner activity is handled as failure. <p> Also, because the programmer can customize fault tolerance based on application characteristics, PLinda is an efficient and convenient way to execute semi-parallel applications. The parallel systems which intend to utilize idle workstations for parallel applications <ref> [2, 11, 21, 25, 44] </ref> either depend on support from the underlying operating systems or assume restricted programming models or characteristics about the applications they are intended for. Few of them can tolerate failure during parallel computation.
Reference: [12] <author> N. Carriero. </author> <title> Implementing Tuple Space Machines. </title> <type> PhD thesis, </type> <institution> Yale University, Department of Computer Science, </institution> <year> 1987. </year>
Reference-contexts: If no matching tuple is found, then in and rd block until a matching tuple is inserted. The difference is that in is destructive (i.e. removes the tuple) while rd is not. 5 A more detailed description of the Linda model is found in <ref> [9, 12, 13, 44, 46] </ref>. Linda has several characteristics which make it popular. First, the model is simple. Accessing tuple space is intuitive and requires only four operations. Second, the model is flexible. <p> The prototype is built on UNIX and TCP/IP using C++. The implementation consists of four major components: the server program, the daemon program, the administration program, and the client library. There is a vast amount of literature about the implementation of tuple space and transaction processing systems <ref> [9, 6, 12, 34, 46] </ref>. In the design of the PLinda runtime system, we have focused on how to incorporate existing implementation techniques for transactions into those for tuple space. Throughout this chapter, we therefore concentrate on presenting implementation strategies rather than describing implementation details.
Reference: [13] <author> N. Carriero and D. Gelernter. </author> <title> How to Write Parallel Programs: A First Course. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> 88 </month>
Reference-contexts: For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. Popular systems include Linda <ref> [13] </ref>, PVM [60, 24], MPI [29] and Express. Unfortunately, few support fault tolerance or utilization of idle workstations. <p> If no matching tuple is found, then in and rd block until a matching tuple is inserted. The difference is that in is destructive (i.e. removes the tuple) while rd is not. 5 A more detailed description of the Linda model is found in <ref> [9, 12, 13, 44, 46] </ref>. Linda has several characteristics which make it popular. First, the model is simple. Accessing tuple space is intuitive and requires only four operations. Second, the model is flexible.
Reference: [14] <author> J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy, and R.J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Workstations are only intermittently idle as a rule. A system which can utilize intermittently idle workstations can make computing on networked workstations very cost effective. In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref> to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. <p> Unfortunately, few support fault tolerance or utilization of idle workstations. Also, there has been a considerable amount of work [37, 51] on fault tolerance in distributed systems, but most of the work has not addressed the problem of utilizing idle workstations for parallel computation. There have been research efforts <ref> [2, 14, 18, 11, 25, 48, 53] </ref> to develop software systems to utilize idle workstations and some of them [2, 14, 11, 25] are designed to address fault tolerance. <p> There have been research efforts [2, 14, 18, 11, 25, 48, 53] to develop software systems to utilize idle workstations and some of them <ref> [2, 14, 11, 25] </ref> are designed to address fault tolerance. However, to our knowledge, there is no system to aim at supporting both parallel processing and fault tolerance together with the utilization of idle workstations. <p> Therefore, it is crucial to guarantee that workstations will be used only while they are idle. To address this issue, various work-stealing systems have been developed <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. During execution, these systems monitor the idleness status of 2 workstations and migrate processes from busy or overloaded machines to idle or under--utilized ones. <p> There has been a considerable amount of research done on how to utilize idle or under-utilized workstations connected by a network <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems [2, 11, 25, 44] which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> For idleness detection and scheduling we do not intend to re-invent techniques for PLinda. Instead, we take advantage of various techniques already developed by other work-stealing systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. The process migration issue is 48 addressed by treating "busy" machines (i.e., those which are being used by their owners) as failed; that is, owner activity is handled as failure.
Reference: [15] <author> D. Cheng. </author> <title> A survey of parallel programming languages and tools. </title> <type> Technical Report RND-93-005, </type> <institution> NASA Ames Research Center, </institution> <year> 1993. </year>
Reference-contexts: Among those problems, many problems can be considered as coarse grain parallel or embarrassingly parallelizable; that is, they are easy to parallelize in large mutually independent chunks of computation. As a result of the recent proliferation of parallel software systems <ref> [15] </ref>, we should therefore see those scientists benefit from parallel processing in solving coarse grain parallel problems. Unfortunately, the reality is not the case.
Reference: [16] <author> David Cheriton. </author> <title> The V distributed system. </title> <booktitle> Communication of the ACM, </booktitle> <pages> pages 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference: [17] <author> S. Chiba, K. Kato, and T. Masuda. </author> <title> Exploiting a weak consistency to implement distributed tuple space. </title> <booktitle> In Proceedings of the 12th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 416-423, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Other fault tolerance work has also produced useful ideas. Xu and Liskov proposed a protocol to replicate tuples and to maintain consistency of replicas, despite processor failures [66]. [43] discussed the performance and availability issues concerning replication techniques for tuple space. <ref> [17] </ref> also presented a protocol to relax the consistency of tuple space replicas to improve performance. [42] proposed a scheme based on checkpointing the processes and logging all the tuple space accesses. 2.8.2 Programming Languages and Systems Supporting Transactions There have been research efforts to develop programming languages and systems to
Reference: [18] <author> Henry Clark and Bruce McMillin. </author> <title> DAWGS|a distributed computer server utilizing idle workstations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 175-186, </pages> <year> 1992. </year>
Reference-contexts: Workstations are only intermittently idle as a rule. A system which can utilize intermittently idle workstations can make computing on networked workstations very cost effective. In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref> to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. <p> Unfortunately, few support fault tolerance or utilization of idle workstations. Also, there has been a considerable amount of work [37, 51] on fault tolerance in distributed systems, but most of the work has not addressed the problem of utilizing idle workstations for parallel computation. There have been research efforts <ref> [2, 14, 18, 11, 25, 48, 53] </ref> to develop software systems to utilize idle workstations and some of them [2, 14, 11, 25] are designed to address fault tolerance. <p> Therefore, it is crucial to guarantee that workstations will be used only while they are idle. To address this issue, various work-stealing systems have been developed <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. During execution, these systems monitor the idleness status of 2 workstations and migrate processes from busy or overloaded machines to idle or under--utilized ones. <p> There has been a considerable amount of research done on how to utilize idle or under-utilized workstations connected by a network <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems [2, 11, 25, 44] which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> For idleness detection and scheduling we do not intend to re-invent techniques for PLinda. Instead, we take advantage of various techniques already developed by other work-stealing systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. The process migration issue is 48 addressed by treating "busy" machines (i.e., those which are being used by their owners) as failed; that is, owner activity is handled as failure. <p> But the scheduler processes must achieve agreement on resource allocation. Such agreement is both difficult to implement and expensive at runtime due to required communication and synchronization costs. There are also hybrid approaches to scheduling. Market systems <ref> [18, 44, 49, 62] </ref> are an example. These systems apply the idea of economic bidding to the scheduling problem. Each machine bids autonomously and a single broker process collects the bids. The broker assigns tasks to machines based on their bids. <p> These systems can be classified by their target applications into two categories: 54 sequential/semi-parallel and parallel. The sequential systems <ref> [48, 18] </ref> o*oad sequential jobs (i.e., jobs that do not have internal parallelism) from overloaded machines to under-utilized or idle ones. Since these systems are only intended for sequential jobs, they are not applicable to parallel application users.
Reference: [19] <author> R. F. Cmelik, N. H. Gehani, and W. D. Roome. </author> <title> Fault tolerant concurrent C: A tool for writing fault tolerant distributed programs. </title> <booktitle> In Proceedings of the Ninteenth International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 55-61, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: In this way, every process has the same state in a failure-free execution. Upon disagreement, the minority is ignored. 1.3.3 Fault-tolerant Programming Languages Various fault-tolerant programming languages have been developed to ease the task of constructing fault-tolerant programs. Examples are Argus [47], Avalon [26], Fault-tolerant Concurrent C <ref> [19] </ref>, FT-Linda [4], Orca [41] and FT-SR [56], In general, these fault-tolerant programming languages are distinguished by what program structuring paradigms they support since they all assume the fail-stop processor failure model. Argus and Avalon support the object/action model. <p> Argus and Avalon support the object/action model. Reliability and concurrency control are supported by saving local state to disk and running every object invocation as a transaction or a nested transaction. Fault-tolerant Concurrent C <ref> [19] </ref> and FT-Linda [4] support the replicated state machine paradigm. The primary extension of fault-tolerant Concurrent C which extends Concurrent C [33] is a set of primitives for replicating processes. The runtime system guarantees that all the replicas of a process behave as if they were a single process.
Reference: [20] <author> F. Cristian. </author> <title> Understanding fault-tolerant distributed systems. </title> <journal> Communications of the ACM, </journal> <volume> 34(2) </volume> <pages> 56-78, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Failure models about processors <ref> [20] </ref> which are commonly used are (from least permissive to most permissive): * Fail-stop. The processor fails by stopping without making any inconsistent state transitions [55]. * Omission and timing. The processor fails by not responding to an input or by giving an untimely response, respectively. * Byzantine.
Reference: [21] <author> P. Dasgupta, R.J. LeBlanc, M. Ahamad, and U. Ramachandran. </author> <title> The Clouds distributed operating system. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 34-44, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: They are Argus [47], Avalon [26], Camelot [26], Clouds <ref> [21] </ref> and TABS [27]. Argus is a programming language and system to support the implementation and execution of distributed applications such as mail systems and inventory control systems [47]. The principal mechanism of Argus is guardians which are a special kind of abstract objects. <p> Thus, processes can be easily designed to communicate with each other. Implementation of the runtime system is also relatively simpler because of the flat transaction model. Checkpoint-protected tuple space makes transaction commits efficient. Camelot [26], Clouds <ref> [21] </ref> and TABS [27] provide distributed transaction facilities as a set of user libraries or operating system features. <p> Also, because the programmer can customize fault tolerance based on application characteristics, PLinda is an efficient and convenient way to execute semi-parallel applications. The parallel systems which intend to utilize idle workstations for parallel applications <ref> [2, 11, 21, 25, 44] </ref> either depend on support from the underlying operating systems or assume restricted programming models or characteristics about the applications they are intended for. Few of them can tolerate failure during parallel computation.
Reference: [22] <author> Geert Deconinck, Johan Vounckx, Rudi Cuyvers, and Rudy Lauwereins. </author> <title> Survey of checkpointing and rollback techniques. </title> <institution> Technical Report O3.1.8 and O3.1.12, ESAT-ACCA Laboratory, Katholieke Universiteit Leuven, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: In the restartable action paradigm, the runtime system periodically saves the local states of processes to stable storage. On processor failure, it restarts failed processes on another processor by recovering their states from stable storage. The checkpointing and rollback scheme <ref> [22] </ref> is the most commonly used technique to implement this paradigm. In the replicated state machine paradigm [57], an application is structured as a set of services, and each service is implemented as multiple deterministic processes which are identical. <p> Therefore, E L is equivalent to E. 3.7 Related Work There has been a considerable amount of work done on checkpointing and rollback-recover techniques for distributed systems whose execution mechanisms are similar to the PLinda mechanism. In this section, we will only review the most closely related work. See <ref> [22] </ref> for a comprehensive survey. Among various approaches to making parallel/distributed systems failure-resilient, backward error recovery is the most general and commonly used [37]. Backward error recovery is the only known mechanism that can tolerate faults which were unexpected at system design time [54, 37]. <p> First, a process usually has to lose a lot of work on failure recovery (or, migration). Most techniques for process resiliency entail periodic checkpoints of process images to disk. On failure recovery, processes roll back to the last checkpoint on disk <ref> [22] </ref>. In general, checkpoints are taken infrequently for runtime efficiency and therefore, failure recovery causes a lot of work to be lost. Process migration for load balancing reasons may occur frequently and therefore a lot of work should not be lost on migration. <p> Thus, this programming model allows the programmer to design lightweight custom fault tolerance for an application by taking advantage of characteristics of the application | copying only critical information about local state to tuple space in volatile memory at each commit. (In contrast, systems <ref> [22] </ref> that support transparent fault tolerance save entire process images to disk.) Experimental results from real coarse grain parallel applications show that the performance differences between executions with fault tolerance mechanisms activated and those with the mechanisms disabled are not significant (the difference is in fact negligible for the corporate bond <p> In addition to low runtime overhead, another important advantage of this scheme over transparent approaches <ref> [22] </ref> to fault tolerance is to support robust parallel computation on heterogeneous machines. Continuation committing and failure recovery are based only on process variables which are architecture-independent. In this dissertation, we have further extended this PLinda fault tolerance scheme to support process migration for the utilization of idle workstations.
Reference: [23] <author> V. Dhar and A. Tuzhilin. </author> <title> Abstract-driven pattern discovery in databases. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 926-938, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Tuple space is checkpointed every 100 seconds. 75 5.4.3 Biological Pattern Discovery In this subsection, we present the results of our experiment to apply PLinda to a data mining application. We parallelized a sequential biological pattern discovery program [63] using PLinda. These types of data mining applications <ref> [1, 23, 35] </ref> are interesting because they are compute-intensive and they are usually coarse grain parallel problems.; First, we explain the problem and the sequential approach. We then describe our parallel approach and show the performance results.
Reference: [24] <author> J. Dongarra, G. A. Geist, R. Mancheck, and V. S. Sunderam. </author> <title> Integrated PVM framework supports heterogeneous network computing. </title> <journal> Computers in Physics, </journal> <volume> 7(2) </volume> <pages> 166-175, </pages> <year> 1993. </year>
Reference-contexts: For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. Popular systems include Linda [13], PVM <ref> [60, 24] </ref>, MPI [29] and Express. Unfortunately, few support fault tolerance or utilization of idle workstations. Also, there has been a considerable amount of work [37, 51] on fault tolerance in distributed systems, but most of the work has not addressed the problem of utilizing idle workstations for parallel computation.
Reference: [25] <author> Fred Douglis and John Ousterhout. </author> <title> Transparent process migration: Design alternatives and the Sprite implementation. </title> <journal> Software-Practice and Experience, </journal> <volume> 21(8) </volume> <pages> 757-785, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Workstations are only intermittently idle as a rule. A system which can utilize intermittently idle workstations can make computing on networked workstations very cost effective. In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref> to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. <p> In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems <ref> [2, 11, 25, 44] </ref>. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. Popular systems include Linda [13], PVM [60, 24], MPI [29] and Express. Unfortunately, few support fault tolerance or utilization of idle workstations. <p> Unfortunately, few support fault tolerance or utilization of idle workstations. Also, there has been a considerable amount of work [37, 51] on fault tolerance in distributed systems, but most of the work has not addressed the problem of utilizing idle workstations for parallel computation. There have been research efforts <ref> [2, 14, 18, 11, 25, 48, 53] </ref> to develop software systems to utilize idle workstations and some of them [2, 14, 11, 25] are designed to address fault tolerance. <p> There have been research efforts [2, 14, 18, 11, 25, 48, 53] to develop software systems to utilize idle workstations and some of them <ref> [2, 14, 11, 25] </ref> are designed to address fault tolerance. However, to our knowledge, there is no system to aim at supporting both parallel processing and fault tolerance together with the utilization of idle workstations. <p> Their advantages over massively parallel computers are wide availability and cost-effectiveness. First, unlike supercomputers installed in a few institutions, these machines are widely available; many institutions have hundreds of high performance workstations which are unused most of the time <ref> [25, 52] </ref>. Second, they are already paid for and are connected via communication networks; no additional cost is required for parallel processing. Finally, they can rival supercomputers with their aggregate computing power and main memory. <p> Therefore, it is crucial to guarantee that workstations will be used only while they are idle. To address this issue, various work-stealing systems have been developed <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. During execution, these systems monitor the idleness status of 2 workstations and migrate processes from busy or overloaded machines to idle or under--utilized ones. <p> There has been a considerable amount of research done on how to utilize idle or under-utilized workstations connected by a network <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems [2, 11, 25, 44] which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems <ref> [2, 11, 25, 44] </ref> which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> For idleness detection and scheduling we do not intend to re-invent techniques for PLinda. Instead, we take advantage of various techniques already developed by other work-stealing systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. The process migration issue is 48 addressed by treating "busy" machines (i.e., those which are being used by their owners) as failed; that is, owner activity is handled as failure. <p> Also, because the programmer can customize fault tolerance based on application characteristics, PLinda is an efficient and convenient way to execute semi-parallel applications. The parallel systems which intend to utilize idle workstations for parallel applications <ref> [2, 11, 21, 25, 44] </ref> either depend on support from the underlying operating systems or assume restricted programming models or characteristics about the applications they are intended for. Few of them can tolerate failure during parallel computation.
Reference: [26] <author> Jeffrey L. Eppinger, Lily B. Mummert, and Alfred Z. Spector, </author> <title> editors. Camelot and Avalon: A Distributed Transaction Facility. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1991. </year>
Reference-contexts: In this way, every process has the same state in a failure-free execution. Upon disagreement, the minority is ignored. 1.3.3 Fault-tolerant Programming Languages Various fault-tolerant programming languages have been developed to ease the task of constructing fault-tolerant programs. Examples are Argus [47], Avalon <ref> [26] </ref>, Fault-tolerant Concurrent C [19], FT-Linda [4], Orca [41] and FT-SR [56], In general, these fault-tolerant programming languages are distinguished by what program structuring paradigms they support since they all assume the fail-stop processor failure model. Argus and Avalon support the object/action model. <p> They are Argus [47], Avalon <ref> [26] </ref>, Camelot [26], Clouds [21] and TABS [27]. Argus is a programming language and system to support the implementation and execution of distributed applications such as mail systems and inventory control systems [47]. The principal mechanism of Argus is guardians which are a special kind of abstract objects. <p> They are Argus [47], Avalon <ref> [26] </ref>, Camelot [26], Clouds [21] and TABS [27]. Argus is a programming language and system to support the implementation and execution of distributed applications such as mail systems and inventory control systems [47]. The principal mechanism of Argus is guardians which are a special kind of abstract objects. <p> Reliability and concurrency control are supported by saving local state to disk and running every handler call as a transaction or a nested transaction. Avalon <ref> [26] </ref> is a set of linguistic constructs which can be implemented as extensions to familiar programming languages such as C++, Common Lisp and Ada. The computation model of Avalon resembles that of Argus. However, Avalon gives the programmer explicit control over commit and abort operations but Argus does not. <p> Thus, processes can be easily designed to communicate with each other. Implementation of the runtime system is also relatively simpler because of the flat transaction model. Checkpoint-protected tuple space makes transaction commits efficient. Camelot <ref> [26] </ref>, Clouds [21] and TABS [27] provide distributed transaction facilities as a set of user libraries or operating system features.
Reference: [27] <author> A. Z. Spector et al. </author> <title> Support for distributed transactions in the TABS prototype. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(6):520-530, </volume> <year> 1985. </year>
Reference-contexts: They are Argus [47], Avalon [26], Camelot [26], Clouds [21] and TABS <ref> [27] </ref>. Argus is a programming language and system to support the implementation and execution of distributed applications such as mail systems and inventory control systems [47]. The principal mechanism of Argus is guardians which are a special kind of abstract objects. <p> Thus, processes can be easily designed to communicate with each other. Implementation of the runtime system is also relatively simpler because of the flat transaction model. Checkpoint-protected tuple space makes transaction commits efficient. Camelot [26], Clouds [21] and TABS <ref> [27] </ref> provide distributed transaction facilities as a set of user libraries or operating system features.
Reference: [28] <author> R. Felderman, E. Schooler, and L. Klienrock. </author> <title> The Benevolent Bandit Laboratory: A testbed for distributed algorithms. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 7(2), </volume> <month> February </month> <year> 1989. </year>
Reference-contexts: Workstations are only intermittently idle as a rule. A system which can utilize intermittently idle workstations can make computing on networked workstations very cost effective. In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref> to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. <p> Therefore, it is crucial to guarantee that workstations will be used only while they are idle. To address this issue, various work-stealing systems have been developed <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. During execution, these systems monitor the idleness status of 2 workstations and migrate processes from busy or overloaded machines to idle or under--utilized ones. <p> There has been a considerable amount of research done on how to utilize idle or under-utilized workstations connected by a network <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems [2, 11, 25, 44] which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> For idleness detection and scheduling we do not intend to re-invent techniques for PLinda. Instead, we take advantage of various techniques already developed by other work-stealing systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. The process migration issue is 48 addressed by treating "busy" machines (i.e., those which are being used by their owners) as failed; that is, owner activity is handled as failure.
Reference: [29] <author> The MPI Forum. </author> <title> MPI: A message passing interface. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 878-883. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year> <month> 89 </month>
Reference-contexts: For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. Popular systems include Linda [13], PVM [60, 24], MPI <ref> [29] </ref> and Express. Unfortunately, few support fault tolerance or utilization of idle workstations. Also, there has been a considerable amount of work [37, 51] on fault tolerance in distributed systems, but most of the work has not addressed the problem of utilizing idle workstations for parallel computation.
Reference: [30] <author> Geoffrey C. Fox, Roy D. Williams, and Paul C. </author> <title> Messina. </title> <publisher> Parallel Computing Works ! Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Introduction 1.1 Motivation Many scientific and engineering problems from Biology, Physics and other areas require computing a large number of mostly independent tasks <ref> [30, 32, 50, 63, 64] </ref>. A typical example is the analysis of a large (high energy) physics data set [30, 32]. The data set consists of 10 5 10 8 event records which can be analyzed independently. The analysis of each event record is usually compute-intensive. <p> Introduction 1.1 Motivation Many scientific and engineering problems from Biology, Physics and other areas require computing a large number of mostly independent tasks [30, 32, 50, 63, 64]. A typical example is the analysis of a large (high energy) physics data set <ref> [30, 32] </ref>. The data set consists of 10 5 10 8 event records which can be analyzed independently. The analysis of each event record is usually compute-intensive. Regarding these problems, we observe: * Parallel processing is indispensable due to the enormous amount of computation.
Reference: [31] <author> K.A. Frenkel. </author> <title> The human genome project and informatics. </title> <journal> Communications of the ACM, </journal> <volume> 34(11) </volume> <pages> 41-51, </pages> <month> November </month> <year> 1991. </year>
Reference: [32] <author> I. Gaines and T. Nash. </author> <title> Use of new computer technologies in elementary particle physics. </title> <journal> Ann. Rev. of Nucl. Part. Sci., </journal> <volume> 37, </volume> <year> 1987. </year>
Reference-contexts: Introduction 1.1 Motivation Many scientific and engineering problems from Biology, Physics and other areas require computing a large number of mostly independent tasks <ref> [30, 32, 50, 63, 64] </ref>. A typical example is the analysis of a large (high energy) physics data set [30, 32]. The data set consists of 10 5 10 8 event records which can be analyzed independently. The analysis of each event record is usually compute-intensive. <p> Introduction 1.1 Motivation Many scientific and engineering problems from Biology, Physics and other areas require computing a large number of mostly independent tasks [30, 32, 50, 63, 64]. A typical example is the analysis of a large (high energy) physics data set <ref> [30, 32] </ref>. The data set consists of 10 5 10 8 event records which can be analyzed independently. The analysis of each event record is usually compute-intensive. Regarding these problems, we observe: * Parallel processing is indispensable due to the enormous amount of computation.
Reference: [33] <author> N. H. Gehani and W. D. Roome. </author> <title> Concurrent C. </title> <journal> Software-Practice and Experience, </journal> <volume> 16(9) </volume> <pages> 821-844, 86. </pages>
Reference-contexts: Reliability and concurrency control are supported by saving local state to disk and running every object invocation as a transaction or a nested transaction. Fault-tolerant Concurrent C [19] and FT-Linda [4] support the replicated state machine paradigm. The primary extension of fault-tolerant Concurrent C which extends Concurrent C <ref> [33] </ref> is a set of primitives for replicating processes. The runtime system guarantees that all the replicas of a process behave as if they were a single process. FT-Linda is a fault-tolerant variant of Linda.
Reference: [34] <author> J. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: In fact a stable tuple space and knowledge of the critical portions of the state of the failing process are all that is needed. 1.4.3 Linda and Transactions Transactions are in particular effective for Linda's tuple space abstraction because tuple space is a shared data resource much like a database <ref> [34, 6] </ref>. Tuple space can be optimized for transactions independently because it is a separate data storage from process address spaces. Finally, in tuple space, there is a logical unit of data (which is a tuple) for manipulation which is independent of a language type system. <p> Section 2.8 compares PLinda with other fault-tolerant Linda-variant systems and programming languages/systems that support transactions. Section 2.9 concludes this chapter. 2.2 Transactions and Robust Parallel Computation in the Linda Model There are several fault tolerance abstractions for building failure-resilient applications.Among the popular ones are transactions <ref> [6, 34] </ref> and ordered atomic broadcast [7, 8, 40, 61]. Transactions have been mostly used for database applications such as bank or airline reservation applications where reliable management of persistent data is crucial. <p> In the design of PLinda we chose transactions as the primary fault tolerance mechanism for the following reason. Transactions are a simple but effective abstraction for controlling concurrent access to shared data and maintaining a consistent state of shared data in the presence of failure <ref> [34] </ref>. Transactions are especially effective in the Linda model where the tuple space (shared memory) is the only mechanism for communication between processes and storage of shared data. For the same reason, most other fault tolerant work on Linda also supports some similar mechanism, though with less functionality [3, 4]. <p> The execution of transactions appears as if they were executed in a serial order. Transactions require concurrency control for this property. The most common implementation technique for concurrency control is two phase locking <ref> [6, 34] </ref>. For a transaction, two phase locking holds locks on accessed data items until the transaction commits, and therefore prevents transactions from accessing uncommitted updates. * Durability. Committed updates in databases survive failure. <p> Instead, the thread must leave a persistent indication of where it is in the set of transactions it has executed. In databases, such persistent indications allowing a multi-transaction thread to resume from failure is known as mini-batching <ref> [34] </ref>. Unfortunately, any such technique to make continuations persistent requires additional programming effort or new higher level abstractions. (PLinda currently offers only low level abstractions as we will explain later on.) The serializability property reduces parallelism. <p> In our version of two phase locking, write locks are held until commit, but read locks are held only while the read occurs (known as degree two serializability <ref> [6, 34] </ref> in the database world). The write locks prevent transactions from accessing updates made by a transaction until the transaction commits. So, a transaction abort affects no others | it's as if the transaction had never started. <p> However, we plan to add the former approach to the implementation in the future. PLinda is designed to support two kinds of tuple space: * Checkpoint-protected tuple space. Unlike most transaction processing systems <ref> [6, 34] </ref>, PLinda replicates the "transaction-consistent" state of tuple space on disk only periodically called tuple space checkpoint. Transaction commit operations do not require updates to be written to disk before they are finished. <p> Since failure is in fact rare, checkpoint-protected tuple space is better suited to robust parallel computation in which only the final answer is important. If by contrast, intermediate transaction commits might be important, as in a transaction processing monitor application <ref> [34] </ref>, stable tuple space would be more appropriate. 2.5 Continuation Committing In this section, we discuss how to make processes resilient to failure. There are two approaches to resilient processes: process replication with atomic broadcast and process checkpointing. <p> For example, the approach can make arbitrary distributed programs fault-tolerant in a programmer-transparent manner. The technique used in backward error recovery is checkpointing and rollback. Checkpointing techniques have been widely used for database systems to make recovery fast <ref> [6, 34] </ref> Since database applications do not require process resiliency, those techniques don't recover process state. However, they ensure that committed transactions are serialized and never lost. They must do so because the user can see results of committed transactions immediately and expect them to survive failure. <p> The prototype is built on UNIX and TCP/IP using C++. The implementation consists of four major components: the server program, the daemon program, the administration program, and the client library. There is a vast amount of literature about the implementation of tuple space and transaction processing systems <ref> [9, 6, 12, 34, 46] </ref>. In the design of the PLinda runtime system, we have focused on how to incorporate existing implementation techniques for transactions into those for tuple space. Throughout this chapter, we therefore concentrate on presenting implementation strategies rather than describing implementation details.
Reference: [35] <author> J. Han, Y. Cai, and N. Cercone. </author> <title> Knowledge discovery in databases: An attribute-oriented approach. </title> <booktitle> In Proceedings of the 18th International Conference on Very Large Data Bases, </booktitle> <pages> pages 547-559, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Tuple space is checkpointed every 100 seconds. 75 5.4.3 Biological Pattern Discovery In this subsection, we present the results of our experiment to apply PLinda to a data mining application. We parallelized a sequential biological pattern discovery program [63] using PLinda. These types of data mining applications <ref> [1, 23, 35] </ref> are interesting because they are compute-intensive and they are usually coarse grain parallel problems.; First, we explain the problem and the sequential approach. We then describe our parallel approach and show the performance results.
Reference: [36] <author> L. C. K. Hui. </author> <title> Combinatorial Pattern Matching, Lecture Notes in Computer Science, volume 644, chapter Color Set Size Problem with Applications to String Matching, </title> <address> pages 230-243. </address> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Candidate patterns are formed by finding all promising segments (i.e., those which have a chance to appear in patterns of interest) and combining them. To facilitate finding segments, this algorithm uses a generalized suffix tree <ref> [36] </ref> (GST) constructed from a small sample of sequences (the size of the sample depends on the requirement for result accuracy).
Reference: [37] <author> Pankaj Jalote. </author> <title> Fault Tolerance in Distributed Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: Popular systems include Linda [13], PVM [60, 24], MPI [29] and Express. Unfortunately, few support fault tolerance or utilization of idle workstations. Also, there has been a considerable amount of work <ref> [37, 51] </ref> on fault tolerance in distributed systems, but most of the work has not addressed the problem of utilizing idle workstations for parallel computation. <p> In this section, we will only review the most closely related work. See [22] for a comprehensive survey. Among various approaches to making parallel/distributed systems failure-resilient, backward error recovery is the most general and commonly used <ref> [37] </ref>. Backward error recovery is the only known mechanism that can tolerate faults which were unexpected at system design time [54, 37]. For example, the approach can make arbitrary distributed programs fault-tolerant in a programmer-transparent manner. The technique used in backward error recovery is checkpointing and rollback. <p> See [22] for a comprehensive survey. Among various approaches to making parallel/distributed systems failure-resilient, backward error recovery is the most general and commonly used [37]. Backward error recovery is the only known mechanism that can tolerate faults which were unexpected at system design time <ref> [54, 37] </ref>. For example, the approach can make arbitrary distributed programs fault-tolerant in a programmer-transparent manner. The technique used in backward error recovery is checkpointing and rollback.
Reference: [38] <author> K. Jeong and D. Shasha. PLinda 2.0: </author> <title> A transactional/checkpointing approach to fault tolerant Linda. </title> <booktitle> In Proceedings of the 13th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 96-105, </pages> <year> 1994. </year>
Reference-contexts: In fact, such a scheme has already been demonstrated in fault-tolerant parallel computing systems such as FT-Linda [4] and PLinda <ref> [38] </ref>. * Using workstations connected by LANs or even WANs, large scale high performance parallel processing is possible for these problems because computation basically consists of a large number of mostly independent tasks. 1 However, it is difficult for the end-user to find workstations which are idle for a long time. <p> The mechanisms allow a programmer to take advantage of the characteristics of a given application when making it failure-resilient. However, the drawback is additional programming work. For coarse grain parallel applications which have simple control structures, the additional programming work is usually negligible <ref> [4, 38] </ref> and therefore the customization approach is more appropriate. In PLinda, the transaction commit mechanism stores data in the volatile tuple space 6 and is therefore extremely lightweight 1 . Such a commit mechanism does not guaran-tee the durability of committed transactions, since the tuple space might fail.
Reference: [39] <author> David B. Johnson. </author> <title> Distributed System Fault Tolerance Using Message Logging and Checkpointing. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1989. </year>
Reference-contexts: If a process fails, then all the processes usually roll back to the last checkpoint. In message logging/replay <ref> [59, 39] </ref>, also called independent checkpointing, processes save local state to disk independently of one another. They also record all the inter-process messages in message logs and periodically save the log to disk.
Reference: [40] <author> T. A. Joseph and K. P. Birman. </author> <title> Reliable Broadcast Protocols. In Sape Mullender, editor, </title> <journal> Distributed Systems, </journal> <volume> chapter 14., </volume> <pages> pages 293-318. </pages> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: Section 2.9 concludes this chapter. 2.2 Transactions and Robust Parallel Computation in the Linda Model There are several fault tolerance abstractions for building failure-resilient applications.Among the popular ones are transactions [6, 34] and ordered atomic broadcast <ref> [7, 8, 40, 61] </ref>. Transactions have been mostly used for database applications such as bank or airline reservation applications where reliable management of persistent data is crucial. PLinda is a research effort to apply the transaction processing technology to a different class of applications, robust parallel computation.
Reference: [41] <author> M. Frans Kaashoek, Raymond Michiels, Henri E. Bal, and Andrew S. Tanenbaum. </author> <title> Transparent fault-tolerant in parallel Orca programs. </title> <booktitle> In Proceedings of the USENIX Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: Upon disagreement, the minority is ignored. 1.3.3 Fault-tolerant Programming Languages Various fault-tolerant programming languages have been developed to ease the task of constructing fault-tolerant programs. Examples are Argus [47], Avalon [26], Fault-tolerant Concurrent C [19], FT-Linda [4], Orca <ref> [41] </ref> and FT-SR [56], In general, these fault-tolerant programming languages are distinguished by what program structuring paradigms they support since they all assume the fail-stop processor failure model. Argus and Avalon support the object/action model. <p> In coordinated checkpointing, (also called synchronous or distributed checkpointing), all processes stop at checkpoint time, synchronize to agree on a consistent global state and 45 write their states to disks <ref> [45, 41] </ref> together. If a process fails, then all the processes usually roll back to the last checkpoint. In message logging/replay [59, 39], also called independent checkpointing, processes save local state to disk independently of one another.
Reference: [42] <author> S. Kambhatla. </author> <title> Recovery with limited replay: Fault-tolerant processes in Linda. </title> <type> Technical Report CS/E 90-019, </type> <institution> Oregon Graduate Institute, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: Xu and Liskov proposed a protocol to replicate tuples and to maintain consistency of replicas, despite processor failures [66]. [43] discussed the performance and availability issues concerning replication techniques for tuple space. [17] also presented a protocol to relax the consistency of tuple space replicas to improve performance. <ref> [42] </ref> proposed a scheme based on checkpointing the processes and logging all the tuple space accesses. 2.8.2 Programming Languages and Systems Supporting Transactions There have been research efforts to develop programming languages and systems to use transactions as the foundation for constructing distributed applications.
Reference: [43] <author> S. Kambhatla. </author> <title> Replication issues for a distributed and highly available Linda tuple space. </title> <type> Master's thesis, </type> <institution> Department of Computer Science, Oregon Graduate Institute, </institution> <year> 1991. </year>
Reference-contexts: In addition, designing stateful Piranha processes would also be simpler with mechanisms like continuation committing in PLinda. Other fault tolerance work has also produced useful ideas. Xu and Liskov proposed a protocol to replicate tuples and to maintain consistency of replicas, despite processor failures [66]. <ref> [43] </ref> discussed the performance and availability issues concerning replication techniques for tuple space. [17] also presented a protocol to relax the consistency of tuple space replicas to improve performance. [42] proposed a scheme based on checkpointing the processes and logging all the tuple space accesses. 2.8.2 Programming Languages and Systems Supporting
Reference: [44] <author> D. Kaminsky. </author> <title> Adaptive Parallelism with Piranha. </title> <type> PhD thesis, </type> <institution> Yale University, Department of Computer Science, </institution> <year> 1994. </year>
Reference-contexts: Workstations are only intermittently idle as a rule. A system which can utilize intermittently idle workstations can make computing on networked workstations very cost effective. In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref> to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. <p> In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems <ref> [2, 11, 25, 44] </ref>. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. Popular systems include Linda [13], PVM [60, 24], MPI [29] and Express. Unfortunately, few support fault tolerance or utilization of idle workstations. <p> Therefore, it is crucial to guarantee that workstations will be used only while they are idle. To address this issue, various work-stealing systems have been developed <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. During execution, these systems monitor the idleness status of 2 workstations and migrate processes from busy or overloaded machines to idle or under--utilized ones. <p> If no matching tuple is found, then in and rd block until a matching tuple is inserted. The difference is that in is destructive (i.e. removes the tuple) while rd is not. 5 A more detailed description of the Linda model is found in <ref> [9, 12, 13, 44, 46] </ref>. Linda has several characteristics which make it popular. First, the model is simple. Accessing tuple space is intuitive and requires only four operations. Second, the model is flexible. <p> In institutes where all the machines are rebooted periodically, fault-tolerant long-running parallel applications also need to rely on disks. The Piranha system is a Linda variant designed to utilize idle workstations effectively for parallel computation <ref> [44] </ref>. In spite of its different objective and no direct concern for 28 fault tolerance, Piranha deals with fault tolerance-related issues, and shows another use of fault tolerance. In the Piranha system, worker processes, called Piranha, execute tasks on idle workstations. <p> There has been a considerable amount of research done on how to utilize idle or under-utilized workstations connected by a network <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems [2, 11, 25, 44] which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems <ref> [2, 11, 25, 44] </ref> which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> For idleness detection and scheduling we do not intend to re-invent techniques for PLinda. Instead, we take advantage of various techniques already developed by other work-stealing systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. The process migration issue is 48 addressed by treating "busy" machines (i.e., those which are being used by their owners) as failed; that is, owner activity is handled as failure. <p> Common idleness criteria are: * Keyboard, mouse and console idle times. * Load average. * The number of users logged on. * Remote-login-session idle time. Besides these, there are other criteria such as time of day which are used relatively less often. See <ref> [44] </ref> for a comprehensive description about idleness criteria. There have been several studies to find effective settings for idleness criteria [44, 53]. <p> Besides these, there are other criteria such as time of day which are used relatively less often. See [44] for a comprehensive description about idleness criteria. There have been several studies to find effective settings for idleness criteria <ref> [44, 53] </ref>. Kaminsky reported that the Piranha project team had run large production applications on a volunteered pool of over 60 workstations and only one machine was withdrawn from the pool. <p> Other experimental results have also shown the effectiveness of similar idleness criteria [53]. Also, there have been various studies to measure how often workstations are idle <ref> [44, 52] </ref>. They found that machines are idle most of the time. For example, Mutka and Livny 49 found that 70%-80% of machines are idle weekends and that 50% were idle even at peak times [52]. <p> But the scheduler processes must achieve agreement on resource allocation. Such agreement is both difficult to implement and expensive at runtime due to required communication and synchronization costs. There are also hybrid approaches to scheduling. Market systems <ref> [18, 44, 49, 62] </ref> are an example. These systems apply the idea of economic bidding to the scheduling problem. Each machine bids autonomously and a single broker process collects the bids. The broker assigns tasks to machines based on their bids. <p> Therefore, scheduling multiple parallel applications is crucial to effectively utilizing idle workstations. Also, both throughput and turnaround time can benefit because most parallel applications do not exibit linear speedup; that is, they are more efficient on a smaller number of processors <ref> [44] </ref>. A commonly used scheme is to partition processors and to assign partitions to applications. For example, processors can be evenly divided or each processor can select an application randomly. <p> Also, because the programmer can customize fault tolerance based on application characteristics, PLinda is an efficient and convenient way to execute semi-parallel applications. The parallel systems which intend to utilize idle workstations for parallel applications <ref> [2, 11, 21, 25, 44] </ref> either depend on support from the underlying operating systems or assume restricted programming models or characteristics about the applications they are intended for. Few of them can tolerate failure during parallel computation. <p> Few of them can tolerate failure during parallel computation. Among those systems, Piranha <ref> [44] </ref> is a Linda-variant system aimed at utilizing idle workstations. The current design of the PLinda idleness detection mechanisms is inspired by ideas proposed by Piranha. 4.6 Summary In this chapter, we discussed how to utilize idle workstations for parallel computation in PLinda. <p> This implies that the PLinda failure recovery mechanism is viable for process migration for idle workstation utilization unless the idleness status of workstations changes too frequently. In our experience, workstations usually stay idle for a sufficiently long time (e.g, at least, a few minutes) once they become idle <ref> [44] </ref>. For the message logging/replay method, we observed a significant increase in execution time when we increased the number of worker process failures.
Reference: [45] <author> R. Koo and S. Toueg. </author> <title> Checkpointing and rollback recovery for distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):23-31, </volume> <year> 1987. </year>
Reference-contexts: Instead, the runtime system takes a global snapshot and saves it to disk periodically. This method is reminiscent of work by Koo and Tueg <ref> [45] </ref>. Specifically, this execution method is as follows: 1. As in message logging/replay, a process performs continuation committing only when the runtime system explicitly requests it. Instead, the process makes a local copy of continuation at each commit and keeps it until the next commit. <p> In coordinated checkpointing, (also called synchronous or distributed checkpointing), all processes stop at checkpoint time, synchronize to agree on a consistent global state and 45 write their states to disks <ref> [45, 41] </ref> together. If a process fails, then all the processes usually roll back to the last checkpoint. In message logging/replay [59, 39], also called independent checkpointing, processes save local state to disk independently of one another. <p> We gave the motivation for our tunable approach to execution: an execution model needs to be flexible in order to to deal with different application characteristics. The message logging/replay and coordinated checkpointing methods are similar to work by work by Koo and Tueg <ref> [45] </ref> and Strom and Yemini [59], respectively. Their techniques are aimed either at supporting efficient normal execution or at avoiding massive rollback on failure, not both. In contrast, PLinda allows the end-user to choose one of three execution methods for an application at runtime, depending on application characteristics. <p> In addition to losing a lot of work on failure, failure recovery usually requires not only failed processes but also live processes to roll back to previous checkpoints. For example, techniques for coordinated checkpointing <ref> [45] </ref> require massive rollback on failure, and those for message logging/replay may also require cascading rollbacks on failure.
Reference: [46] <author> J. Leichter. </author> <title> Shared Tuple Memories: Shared Memories, Buses and LAN's-Linda Implementation Across the Spectrum of Connectivity. </title> <type> PhD thesis, </type> <institution> Yale University, Department of Computer Science, </institution> <year> 1989. </year> <month> 90 </month>
Reference-contexts: If no matching tuple is found, then in and rd block until a matching tuple is inserted. The difference is that in is destructive (i.e. removes the tuple) while rd is not. 5 A more detailed description of the Linda model is found in <ref> [9, 12, 13, 44, 46] </ref>. Linda has several characteristics which make it popular. First, the model is simple. Accessing tuple space is intuitive and requires only four operations. Second, the model is flexible. <p> The prototype is built on UNIX and TCP/IP using C++. The implementation consists of four major components: the server program, the daemon program, the administration program, and the client library. There is a vast amount of literature about the implementation of tuple space and transaction processing systems <ref> [9, 6, 12, 34, 46] </ref>. In the design of the PLinda runtime system, we have focused on how to incorporate existing implementation techniques for transactions into those for tuple space. Throughout this chapter, we therefore concentrate on presenting implementation strategies rather than describing implementation details.
Reference: [47] <author> B. Liskov. </author> <title> Distributed programming in Argus. </title> <journal> Communication of the ACM, </journal> <volume> 31(3) </volume> <pages> 300-312, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: In this way, every process has the same state in a failure-free execution. Upon disagreement, the minority is ignored. 1.3.3 Fault-tolerant Programming Languages Various fault-tolerant programming languages have been developed to ease the task of constructing fault-tolerant programs. Examples are Argus <ref> [47] </ref>, Avalon [26], Fault-tolerant Concurrent C [19], FT-Linda [4], Orca [41] and FT-SR [56], In general, these fault-tolerant programming languages are distinguished by what program structuring paradigms they support since they all assume the fail-stop processor failure model. Argus and Avalon support the object/action model. <p> They are Argus <ref> [47] </ref>, Avalon [26], Camelot [26], Clouds [21] and TABS [27]. Argus is a programming language and system to support the implementation and execution of distributed applications such as mail systems and inventory control systems [47]. The principal mechanism of Argus is guardians which are a special kind of abstract objects. <p> They are Argus <ref> [47] </ref>, Avalon [26], Camelot [26], Clouds [21] and TABS [27]. Argus is a programming language and system to support the implementation and execution of distributed applications such as mail systems and inventory control systems [47]. The principal mechanism of Argus is guardians which are a special kind of abstract objects. Guardians encapsulate information within local state and permit it to be accessed by means of special procedures, called handlers, that can be called from other guardians.
Reference: [48] <author> M. Litzkow, M. Livny, and M.W. </author> <title> Mutka. Condor|a hunter of idle workstations. </title> <booktitle> In Proceedings of the 8th International Conference on Distributed Computing Systems, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: Workstations are only intermittently idle as a rule. A system which can utilize intermittently idle workstations can make computing on networked workstations very cost effective. In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref> to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. <p> Unfortunately, few support fault tolerance or utilization of idle workstations. Also, there has been a considerable amount of work [37, 51] on fault tolerance in distributed systems, but most of the work has not addressed the problem of utilizing idle workstations for parallel computation. There have been research efforts <ref> [2, 14, 18, 11, 25, 48, 53] </ref> to develop software systems to utilize idle workstations and some of them [2, 14, 11, 25] are designed to address fault tolerance. <p> Therefore, it is crucial to guarantee that workstations will be used only while they are idle. To address this issue, various work-stealing systems have been developed <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. During execution, these systems monitor the idleness status of 2 workstations and migrate processes from busy or overloaded machines to idle or under--utilized ones. <p> There has been a considerable amount of research done on how to utilize idle or under-utilized workstations connected by a network <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems [2, 11, 25, 44] which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> For idleness detection and scheduling we do not intend to re-invent techniques for PLinda. Instead, we take advantage of various techniques already developed by other work-stealing systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. The process migration issue is 48 addressed by treating "busy" machines (i.e., those which are being used by their owners) as failed; that is, owner activity is handled as failure. <p> These systems can be classified by their target applications into two categories: 54 sequential/semi-parallel and parallel. The sequential systems <ref> [48, 18] </ref> o*oad sequential jobs (i.e., jobs that do not have internal parallelism) from overloaded machines to under-utilized or idle ones. Since these systems are only intended for sequential jobs, they are not applicable to parallel application users.
Reference: [49] <author> T.W. Malone, R.E. Fikes, K.R. Grant, and M.T. Howard. </author> <title> Enterprise: A Market-like Task Scheduler for Distributed Computing Environments. </title> <publisher> Elsevier Science Publishers, </publisher> <year> 1988. </year>
Reference-contexts: But the scheduler processes must achieve agreement on resource allocation. Such agreement is both difficult to implement and expensive at runtime due to required communication and synchronization costs. There are also hybrid approaches to scheduling. Market systems <ref> [18, 44, 49, 62] </ref> are an example. These systems apply the idea of economic bidding to the scheduling problem. Each machine bids autonomously and a single broker process collects the bids. The broker assigns tasks to machines based on their bids.
Reference: [50] <author> Timothy G. Mattson, </author> <title> editor. </title> <booktitle> Parallel Computing in Computational Chemistry. ACS Symposium Series 592. </booktitle> <publisher> American Chemical Society, </publisher> <year> 1995. </year>
Reference-contexts: Introduction 1.1 Motivation Many scientific and engineering problems from Biology, Physics and other areas require computing a large number of mostly independent tasks <ref> [30, 32, 50, 63, 64] </ref>. A typical example is the analysis of a large (high energy) physics data set [30, 32]. The data set consists of 10 5 10 8 event records which can be analyzed independently. The analysis of each event record is usually compute-intensive.
Reference: [51] <author> Shivakant Mishra and Richard D. Schlichting. </author> <title> Abstractions for constructing dependable distributed systems. </title> <type> Technical Report TR 92-19, </type> <institution> University of Arizona, Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Popular systems include Linda [13], PVM [60, 24], MPI [29] and Express. Unfortunately, few support fault tolerance or utilization of idle workstations. Also, there has been a considerable amount of work <ref> [37, 51] </ref> on fault tolerance in distributed systems, but most of the work has not addressed the problem of utilizing idle workstations for parallel computation. <p> There are three common program structuring paradigms for fault-tolerant software <ref> [51] </ref>: the object/action model, the restartable action paradigm, and the replicated state machine paradigm. In the object/action model, an application program consists of objects and actions. Objects encapsulate critical data in local state and export certain operations to modify data.
Reference: [52] <author> M.W. Mutka and M. Livny. </author> <title> Profiling workstations' available capacity for remote execution. </title> <booktitle> In Performance '87, </booktitle> <pages> pages 529-544. </pages> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1988. </year>
Reference-contexts: Workstations are only intermittently idle as a rule. A system which can utilize intermittently idle workstations can make computing on networked workstations very cost effective. In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref> to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. <p> Their advantages over massively parallel computers are wide availability and cost-effectiveness. First, unlike supercomputers installed in a few institutions, these machines are widely available; many institutions have hundreds of high performance workstations which are unused most of the time <ref> [25, 52] </ref>. Second, they are already paid for and are connected via communication networks; no additional cost is required for parallel processing. Finally, they can rival supercomputers with their aggregate computing power and main memory. <p> Therefore, it is crucial to guarantee that workstations will be used only while they are idle. To address this issue, various work-stealing systems have been developed <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. During execution, these systems monitor the idleness status of 2 workstations and migrate processes from busy or overloaded machines to idle or under--utilized ones. <p> There has been a considerable amount of research done on how to utilize idle or under-utilized workstations connected by a network <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems [2, 11, 25, 44] which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> For idleness detection and scheduling we do not intend to re-invent techniques for PLinda. Instead, we take advantage of various techniques already developed by other work-stealing systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. The process migration issue is 48 addressed by treating "busy" machines (i.e., those which are being used by their owners) as failed; that is, owner activity is handled as failure. <p> Other experimental results have also shown the effectiveness of similar idleness criteria [53]. Also, there have been various studies to measure how often workstations are idle <ref> [44, 52] </ref>. They found that machines are idle most of the time. For example, Mutka and Livny 49 found that 70%-80% of machines are idle weekends and that 50% were idle even at peak times [52]. <p> Also, there have been various studies to measure how often workstations are idle [44, 52]. They found that machines are idle most of the time. For example, Mutka and Livny 49 found that 70%-80% of machines are idle weekends and that 50% were idle even at peak times <ref> [52] </ref>. Kaminsky reported that 86%, 94%, and 75% were idle on average, at night, and during the daytime, respectively. Also, we have conducted a one week experiment with 19 machines at the NYU computer science department.
Reference: [53] <author> D.A. Nichols. </author> <title> Using idle workstations in a shared computing environment. </title> <journal> ACM Operating Systems Review, </journal> <volume> 21(5), </volume> <year> 1987. </year>
Reference-contexts: Workstations are only intermittently idle as a rule. A system which can utilize intermittently idle workstations can make computing on networked workstations very cost effective. In fact, for sequential or semi-parallel (i.e., multiple tasks with no inter-dependency) jobs, there are systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref> to utilize idle or under-utilize workstations effectively. For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. <p> Unfortunately, few support fault tolerance or utilization of idle workstations. Also, there has been a considerable amount of work [37, 51] on fault tolerance in distributed systems, but most of the work has not addressed the problem of utilizing idle workstations for parallel computation. There have been research efforts <ref> [2, 14, 18, 11, 25, 48, 53] </ref> to develop software systems to utilize idle workstations and some of them [2, 14, 11, 25] are designed to address fault tolerance. <p> Therefore, it is crucial to guarantee that workstations will be used only while they are idle. To address this issue, various work-stealing systems have been developed <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. During execution, these systems monitor the idleness status of 2 workstations and migrate processes from busy or overloaded machines to idle or under--utilized ones. <p> There has been a considerable amount of research done on how to utilize idle or under-utilized workstations connected by a network <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. In that work various systems were developed to address the above issues. We call them "work-stealing" systems. Among them, there are systems [2, 11, 25, 44] which support parallel computation, but few support both fault tolerance and idle workstation utilization. <p> For idleness detection and scheduling we do not intend to re-invent techniques for PLinda. Instead, we take advantage of various techniques already developed by other work-stealing systems <ref> [2, 14, 18, 11, 25, 28, 44, 48, 52, 53] </ref>. The process migration issue is 48 addressed by treating "busy" machines (i.e., those which are being used by their owners) as failed; that is, owner activity is handled as failure. <p> Besides these, there are other criteria such as time of day which are used relatively less often. See [44] for a comprehensive description about idleness criteria. There have been several studies to find effective settings for idleness criteria <ref> [44, 53] </ref>. Kaminsky reported that the Piranha project team had run large production applications on a volunteered pool of over 60 workstations and only one machine was withdrawn from the pool. <p> Other experimental results have also shown the effectiveness of similar idleness criteria <ref> [53] </ref>. Also, there have been various studies to measure how often workstations are idle [44, 52]. They found that machines are idle most of the time. For example, Mutka and Livny 49 found that 70%-80% of machines are idle weekends and that 50% were idle even at peak times [52].
Reference: [54] <author> James Steven Plank. </author> <title> Efficient Checkpointing on MIMD Architectures. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: See [22] for a comprehensive survey. Among various approaches to making parallel/distributed systems failure-resilient, backward error recovery is the most general and commonly used [37]. Backward error recovery is the only known mechanism that can tolerate faults which were unexpected at system design time <ref> [54, 37] </ref>. For example, the approach can make arbitrary distributed programs fault-tolerant in a programmer-transparent manner. The technique used in backward error recovery is checkpointing and rollback.
Reference: [55] <author> R. D. Schlichting and F. B. Schneider. </author> <title> Fail-stop processors: An approach to designing fault tolerant computing systems. </title> <journal> ACM Transactions on Computing Systems, </journal> <volume> 1(3) </volume> <pages> 222-238, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: Failure models about processors [20] which are commonly used are (from least permissive to most permissive): * Fail-stop. The processor fails by stopping without making any inconsistent state transitions <ref> [55] </ref>. * Omission and timing. The processor fails by not responding to an input or by giving an untimely response, respectively. * Byzantine. The processor fails in an arbitrary manner. In general, fault-tolerant software is designed to assure correct behavior in the face of failures characterized by a failure model.
Reference: [56] <author> Richard D. Schlichting and Vicraj T. Thomas. FT-SR: </author> <title> A programming language for constructing fault-tolerant distributed systems. </title> <type> Technical Report TR 92-31, </type> <institution> University of Arizona, Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Upon disagreement, the minority is ignored. 1.3.3 Fault-tolerant Programming Languages Various fault-tolerant programming languages have been developed to ease the task of constructing fault-tolerant programs. Examples are Argus [47], Avalon [26], Fault-tolerant Concurrent C [19], FT-Linda [4], Orca [41] and FT-SR <ref> [56] </ref>, In general, these fault-tolerant programming languages are distinguished by what program structuring paradigms they support since they all assume the fail-stop processor failure model. Argus and Avalon support the object/action model.
Reference: [57] <author> Fred B. Schneider. </author> <title> Implementing fault-tolerant services using the state machine approach: A tutorial. </title> <journal> ACM Computing Surveys, </journal> <volume> 22(4) </volume> <pages> 299-319, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: On processor failure, it restarts failed processes on another processor by recovering their states from stable storage. The checkpointing and rollback scheme [22] is the most commonly used technique to implement this paradigm. In the replicated state machine paradigm <ref> [57] </ref>, an application is structured as a set of services, and each service is implemented as multiple deterministic processes which are identical. Each request for a service is broadcast to every process implementing the service. <p> If we want to use our idle machine strategy for servers, one possible method is to replicate the server over idle machines using the replicated state machine paradigm <ref> [57] </ref>. In this method, client processes communicate with the server replicas via atomic ordered broadcast. When a machine where a server replica is running becomes busy or fails, the replica is destroyed. Then, a new server replica is spawned on an idle machine and recovers state from another replica.
Reference: [58] <author> Kenneth Slonneger and Barry L. Kurtz. </author> <title> Formal Syntax and Semantics of Programming Languages. </title> <publisher> Addison Wesley, </publisher> <year> 1995. </year>
Reference: [59] <author> R. Strom and S. Yemini. </author> <title> Optimistic recovery in distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Reconstruction of continuation is based on message logging and replay techniques which are similar to work by Strom and Yemini <ref> [59] </ref>. Specifically, the method is as follows: * Periodic continuation committing and continuous message logging. 1. <p> The process resumes normal execution from C 3 . The PLinda message logging/replay method is similar to work by Strom and Yemini <ref> [59] </ref>. However, their and our techniques differ in several ways. First, the PLinda message logging/replay method is based on the transaction mechanism. Therefore, transaction processing overhead (mainly lock management in PLinda) is implicit in PLinda. Second, disk access is not required in PLinda. <p> If a process fails, then all the processes usually roll back to the last checkpoint. In message logging/replay <ref> [59, 39] </ref>, also called independent checkpointing, processes save local state to disk independently of one another. They also record all the inter-process messages in message logs and periodically save the log to disk. <p> We gave the motivation for our tunable approach to execution: an execution model needs to be flexible in order to to deal with different application characteristics. The message logging/replay and coordinated checkpointing methods are similar to work by work by Koo and Tueg [45] and Strom and Yemini <ref> [59] </ref>, respectively. Their techniques are aimed either at supporting efficient normal execution or at avoiding massive rollback on failure, not both. In contrast, PLinda allows the end-user to choose one of three execution methods for an application at runtime, depending on application characteristics.
Reference: [60] <author> V.S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concur-rency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: For parallel computation, there are also systems [2, 11, 25, 44]. In the last several years, there has been a proliferation of commercial and research prototype parallel software systems on networks of workstations. Popular systems include Linda [13], PVM <ref> [60, 24] </ref>, MPI [29] and Express. Unfortunately, few support fault tolerance or utilization of idle workstations. Also, there has been a considerable amount of work [37, 51] on fault tolerance in distributed systems, but most of the work has not addressed the problem of utilizing idle workstations for parallel computation.
Reference: [61] <author> Robbert van Renesse, K. P. Birman, R. Cooper, B. Glade, and P. Stephenson. </author> <title> Reliable Multicast between Microkernels. </title> <institution> Cornell University, </institution> <year> 1992. </year>
Reference-contexts: Section 2.9 concludes this chapter. 2.2 Transactions and Robust Parallel Computation in the Linda Model There are several fault tolerance abstractions for building failure-resilient applications.Among the popular ones are transactions [6, 34] and ordered atomic broadcast <ref> [7, 8, 40, 61] </ref>. Transactions have been mostly used for database applications such as bank or airline reservation applications where reliable management of persistent data is crucial. PLinda is a research effort to apply the transaction processing technology to a different class of applications, robust parallel computation.
Reference: [62] <author> C.A. Waldspurger, T. Hogg, B.A. Huberman, J.O. Kephart, and W.S. Stornetta. Spawn: </author> <title> A distributed computational economy. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(2) </volume> <pages> 103-117, </pages> <month> February </month> <year> 1992. </year> <month> 91 </month>
Reference-contexts: But the scheduler processes must achieve agreement on resource allocation. Such agreement is both difficult to implement and expensive at runtime due to required communication and synchronization costs. There are also hybrid approaches to scheduling. Market systems <ref> [18, 44, 49, 62] </ref> are an example. These systems apply the idea of economic bidding to the scheduling problem. Each machine bids autonomously and a single broker process collects the bids. The broker assigns tasks to machines based on their bids.
Reference: [63] <author> J. T. L. Wang, G.-W. Chirn, T. G. Marr, B. A. Shapiro, D. Shasha, and K. Zhang. </author> <title> Combinatorial pattern discovery for scientific data: Some preliminary results. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 115-125, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Introduction 1.1 Motivation Many scientific and engineering problems from Biology, Physics and other areas require computing a large number of mostly independent tasks <ref> [30, 32, 50, 63, 64] </ref>. A typical example is the analysis of a large (high energy) physics data set [30, 32]. The data set consists of 10 5 10 8 event records which can be analyzed independently. The analysis of each event record is usually compute-intensive. <p> Tuple space is checkpointed every 100 seconds. 75 5.4.3 Biological Pattern Discovery In this subsection, we present the results of our experiment to apply PLinda to a data mining application. We parallelized a sequential biological pattern discovery program <ref> [63] </ref> using PLinda. These types of data mining applications [1, 23, 35] are interesting because they are compute-intensive and they are usually coarse grain parallel problems.; First, we explain the problem and the sequential approach. We then describe our parallel approach and show the performance results. <p> Interesting patterns are usually those which appear frequently in sequences in the database. Among the various algorithms in the field, we have parallelized the work presented in <ref> [63] </ref> where the problem is defined as follows: * Database D. A set of sequences. * Patterns. The pattern has the form *X1*X2* where X1 and X2 denote subsequences (called segments in [63]) and * represents a variable length "dont-care" (VLDC). A VLDC can match zero or more letters. <p> Among the various algorithms in the field, we have parallelized the work presented in <ref> [63] </ref> where the problem is defined as follows: * Database D. A set of sequences. * Patterns. The pattern has the form *X1*X2* where X1 and X2 denote subsequences (called segments in [63]) and * represents a variable length "dont-care" (VLDC). A VLDC can match zero or more letters. The length of a pattern is defined to be the number of the non-VLDC letters. * Distance metric.
Reference: [64] <author> J. T. L. Wang, T. G. Marr, D. Shasha, B. A. Shapiro, and G.-W. Chirn. </author> <title> Discovering active motifs in sets of related protein sequences and using them for classification. </title> <journal> Nucleic Acids Research, </journal> <volume> 22(14) </volume> <pages> 2769-2775, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Introduction 1.1 Motivation Many scientific and engineering problems from Biology, Physics and other areas require computing a large number of mostly independent tasks <ref> [30, 32, 50, 63, 64] </ref>. A typical example is the analysis of a large (high energy) physics data set [30, 32]. The data set consists of 10 5 10 8 event records which can be analyzed independently. The analysis of each event record is usually compute-intensive.
Reference: [65] <author> Jingwen Wang, Songnian Zhou, Khalid Ahmed, and Weihong Long. LSBATCH: </author> <title> A distributed load sharing batch system. </title> <type> Technical Report CSRI-286, </type> <institution> University of Toronto, </institution> <month> April </month> <year> 1993. </year>
Reference: [66] <author> A. Xu and B. Liskov. </author> <title> A design for a fault-tolerant, distributed implemenation of Linda. </title> <booktitle> In Proceedings of the Ninteenth International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 199-206, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: In addition, designing stateful Piranha processes would also be simpler with mechanisms like continuation committing in PLinda. Other fault tolerance work has also produced useful ideas. Xu and Liskov proposed a protocol to replicate tuples and to maintain consistency of replicas, despite processor failures <ref> [66] </ref>. [43] discussed the performance and availability issues concerning replication techniques for tuple space. [17] also presented a protocol to relax the consistency of tuple space replicas to improve performance. [42] proposed a scheme based on checkpointing the processes and logging all the tuple space accesses. 2.8.2 Programming Languages and Systems
Reference: [67] <author> Songnian Zhou, Jingwen Wang, Xiaohu Zheng, and Pierre Delisle. </author> <title> Utopia: A load sharing facility for large, heterogeneous distributed computer systems. </title> <type> Technical Report CSRI-257, </type> <institution> University of Toronto, </institution> <month> April </month> <year> 1992. </year> <month> 92 </month>
References-found: 67

