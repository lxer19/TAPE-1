URL: http://www.cs.umn.edu/Users/dept/users/padma/optimization/process94.ps.Z
Refering-URL: http://www.cs.umn.edu/Users/dept/users/padma/optimization/
Root-URL: http://www.cs.umn.edu
Title: Tradeoffs in Parallel Query Processing and its Implications for Query Optimization  
Author: Bhaskar Himatsingka Jaideep Srivastava Thomas M. Niccum 
Keyword: Parallel Databases, Query Processing, Query Optimization  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science University of Minnesota  
Abstract: Selecting the best plan for executing a given query is the problem of query optimization. The focus of query optimization for sequential machines has been on finding query plans which involve the least amount of work, since response time is equivalent to work done in a uniprocessor environment. With the advent of parallel computers and their application to data management, this is no longer true. It may not be the case that the best sequential plan will result in the best parallel plan, since sequential dependencies in certain plans make them inherently less parallelizable. It is thus possible to reduce the response time of a query by selecting a plan which may do more work but is also more parallelizable. I/O has traditionally been a bottleneck for query processing, and the difference in I/O and CPU speeds is increasing with modern technology. Since join processing is very CPU intensive, maximizing overlap between I/O, and CPU resource utlization is desirable. Researchers have looked at Asynchronous I/O as a tool to achieve this effect. In this paper we study the problem of parallel join execution on a shared nothing architecture with support for asynchronous I/O, and asynchronous message passing. We examine the tradeoffs among different query execution models and propose a viable alternative. An analytical cost model for the proposed query execution model is developed. Tradeoffs among different query plan structures are explored, and query domains where one structure does better than the other are categorized. Based on this we propose a heuristic approach which tries to achieve the best of all worlds, and gives a stable, and good average performance across most domains. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M.S. Chen, M.L. Lo, P.S. Yu, and H.C. Young. </author> <title> Using segmented right-deep trees for the execution of pipelined hash joins. </title> <booktitle> Proceedings of the 18th International Conference on Very Large Data Bases, </booktitle> <pages> pages 15-26, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: section 5 theoretically compares two execution structures, namely left-deep and right-deep, section 6 describes the heuristic approach, section 7 discusses the evaluation parameters, and the experimental design, section 8 has results and discussions, and section 9 has conclusions and future work. 2 Related Work Work in parallel query optimization [18] <ref> [1] </ref> [2] [8] [7] [10] [15] [20] [23] [6] [26] [22] has focussed on minimizing the total execution time (elapsed time) for a query. It is difficult, if not impossible, to directly compare the different techniques proposed in literature. <p> There still seems to be no consensus on the right model for any of the above domains. There is almost a consensus on one issue though, namely the representation of an execution plan. Most of the proposed methods [6] [23] <ref> [1] </ref> [15] [8] [18] represent an execution plan using an annotated tree. The nodes of the tree correspond to joins in [23] [15], while they correspond to finer granularity operations like hash, probe, sort, merge, etc. in [6] [18] [8] [1]. <p> Most of the proposed methods [6] [23] <ref> [1] </ref> [15] [8] [18] represent an execution plan using an annotated tree. The nodes of the tree correspond to joins in [23] [15], while they correspond to finer granularity operations like hash, probe, sort, merge, etc. in [6] [18] [8] [1]. This is mainly dependent on the granularity at which the different methods have tried to exploit parallelism. The annotations on the nodes of the tree represent the algorithm used to process the operation, and the resources allocated to the operation. <p> This is predominantly because researchers have looked at various different architectures, namely shared-everything [8] [7] [15] [22] [23] [26], shared-disk [18] <ref> [1] </ref> [2], and shared-nothing architectures [20] [10]. <p> Both these models assume asynchronous I/O, and perfect overlap between CPU, I/O, and communication. Sequential vs. random I/O is not considered in their cost models, while the execution model of [15] ignores pipelined parallelism. Chen et al <ref> [1] </ref> consider a variant of the above models where they incorporate pipelined operations. However, they assume the operations to be CPU limited and thus ignore the effect due to seek and latency times of the disk. Resource contention, and periodic bursty behaviour of pipelines are also not considered. <p> An initial attempt has been made to model resource contention. The execution models of [23] [15] [6] [18] [26] assume that a processor will be involved in only one join operation during any stage in the execution. <ref> [1] </ref> assumes a hybrid approach where scan operators to access disk for multiple joins are allowed to be executed on the same processor, but the cpu operations corresponding to table building and probing from multiple joins are not allowed to be executed on the same processor. <p> Since it is very costly, even for small query trees, to generate the optimal plan, all parallel optimization techniques propose heuristic solutions. The heuristic solutions differ in the search space considered for the optimal plan. Heuristics have been proposed to generate right-deep execution trees <ref> [1] </ref>, a subset of bushy trees [1] constructed by joining right-deep trees, cutting the search space of bushy trees using some memory constraints [23], dynamic programming approaches [6], parallelizing sequential plans [8] [18], etc. Resource allocation adds another degree of freedom to the plans which can be generated. <p> The heuristic solutions differ in the search space considered for the optimal plan. Heuristics have been proposed to generate right-deep execution trees <ref> [1] </ref>, a subset of bushy trees [1] constructed by joining right-deep trees, cutting the search space of bushy trees using some memory constraints [23], dynamic programming approaches [6], parallelizing sequential plans [8] [18], etc. Resource allocation adds another degree of freedom to the plans which can be generated. Thus, most techniques assume memory proportional processor allocation [1] <p> <ref> [1] </ref> constructed by joining right-deep trees, cutting the search space of bushy trees using some memory constraints [23], dynamic programming approaches [6], parallelizing sequential plans [8] [18], etc. Resource allocation adds another degree of freedom to the plans which can be generated. Thus, most techniques assume memory proportional processor allocation [1] [18], while some use resource idling minimization techniques [15] [23]. Thus, we can see that there is no consensus on the ideal search space or the search strategies to use. <p> Considering these factors, it is desirable that I/O be asynchronous, enabling CPU and I/O overlap and thus enhanced resource utilization. Hence, we assume an asynchronous I/O model. A similar I/O model has been assumed in [15] [9] <ref> [1] </ref> [2]. It should be noted that this I/O model differs from that considered in [20], where tradeoffs between left-deep and right-deep trees were first explored. <p> The heuristic at each stage selects the next join such that the intermediate relation size at that stage is minimized. * Heuristic Right Deep (HRD): This is a mirror image of the left-deep tree constructed using heuristic HLD. This heuristic is the same as the one studied in <ref> [1] </ref> * Segmented Right Deep (SRD): This heuristic [1] generates bushy trees. However, since left-deep structures are not considered, the execution consists of a sequence of right-deep structures. We use the BC heuristic proposed, as it was shown to have better performance. <p> This heuristic is the same as the one studied in <ref> [1] </ref> * Segmented Right Deep (SRD): This heuristic [1] generates bushy trees. However, since left-deep structures are not considered, the execution consists of a sequence of right-deep structures. We use the BC heuristic proposed, as it was shown to have better performance.
Reference: [2] <author> M.S. Chen, P.S. Yu, and K.L. Wu. </author> <title> Scheduling and processor allocation for parallel execution of multi-join queries. </title> <booktitle> Proceedings of the 8th International Conference on Data Engineering, </booktitle> <pages> pages 58-67, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: 5 theoretically compares two execution structures, namely left-deep and right-deep, section 6 describes the heuristic approach, section 7 discusses the evaluation parameters, and the experimental design, section 8 has results and discussions, and section 9 has conclusions and future work. 2 Related Work Work in parallel query optimization [18] [1] <ref> [2] </ref> [8] [7] [10] [15] [20] [23] [6] [26] [22] has focussed on minimizing the total execution time (elapsed time) for a query. It is difficult, if not impossible, to directly compare the different techniques proposed in literature. <p> This is predominantly because researchers have looked at various different architectures, namely shared-everything [8] [7] [15] [22] [23] [26], shared-disk [18] [1] <ref> [2] </ref>, and shared-nothing architectures [20] [10]. <p> Considering these factors, it is desirable that I/O be asynchronous, enabling CPU and I/O overlap and thus enhanced resource utilization. Hence, we assume an asynchronous I/O model. A similar I/O model has been assumed in [15] [9] [1] <ref> [2] </ref>. It should be noted that this I/O model differs from that considered in [20], where tradeoffs between left-deep and right-deep trees were first explored.
Reference: [3] <author> D. DeWitt. </author> <title> Parallel database systems: The future of high performance database systems. </title> <journal> Communications of the ACM, </journal> <pages> pages 85-98, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The above (conflicting) requirements have led to a number of new query processing algorithms and query optimization techniques for parallel databases <ref> [3] </ref>. Specifically, it is important to consider intra-operator parallelism, i.e. an operation executes on more than one processor, inter-operator parallelism, i.e. two or more operators are evaluated independently in parallel, and pipelining, i.e. a sequence of operators are executed in a producer-consumer fashion. <p> Parallel database architectures can be broadly classified into shared nothing, shared-disk and shared everything [24] <ref> [3] </ref>. Each of these architectures poses a number of interesting tradeoffs for query processing, which must be examined carefully to develop query optimization and scheduling strategies. Recent years have seen an increasing number of studies [24] [3] [19] which examine these tradeoffs. <p> database architectures can be broadly classified into shared nothing, shared-disk and shared everything [24] <ref> [3] </ref>. Each of these architectures poses a number of interesting tradeoffs for query processing, which must be examined carefully to develop query optimization and scheduling strategies. Recent years have seen an increasing number of studies [24] [3] [19] which examine these tradeoffs. Since the shared nothing architecture seems to be the most promising one in terms of scalability and speedup [5], it is the focus of our present study 1 .
Reference: [4] <author> D. DeWitt and R. Gerber. </author> <title> Multiprocessor hashed-based join algorithms. </title> <booktitle> Proceedings of the 11th International Conference on Very Large Data Bases, </booktitle> <pages> pages 151-164, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Parallel Asynchronous I/O is generally being accepted as the architectural solution to this problem. 4 join methods, the hash join has been found to have superior performance in terms of scalability <ref> [4] </ref> [16] [26]. Moreover, for exploiting inter-operator parallelism hash joins provide the opportunity of using pipelining.
Reference: [5] <author> D.J. DeWitt, S. Ghandeharizadeh, D.A. Schneider, A. Bricker, H-I Hsiao, and R. Rasmussen. </author> <title> The gamma database machine project. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <pages> pages 44-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Recent years have seen an increasing number of studies [24] [3] [19] which examine these tradeoffs. Since the shared nothing architecture seems to be the most promising one in terms of scalability and speedup <ref> [5] </ref>, it is the focus of our present study 1 . In this paper, we study the impact of asynchronous I/O on query processing and optimization techniques for shared nothing architectures. We examine the tradeoffs among different query execution models and propose a viable alternative. <p> Data Declustering: Data (relations) can be distributed across all the processing units or across some of the processing units. This decision is mainly driven by the access pattern on individual relations, I/O subsystem characteristics, and the need to minimize response time for individual/batches of queries <ref> [5] </ref>. Since one of the major applications of parallel databases is in decision support systems, where the relations are relatively large and the queries are complex, and since most of the relational operations have been shown to scale linearly [5], full declustering is desirable in such systems. <p> and the need to minimize response time for individual/batches of queries <ref> [5] </ref>. Since one of the major applications of parallel databases is in decision support systems, where the relations are relatively large and the queries are complex, and since most of the relational operations have been shown to scale linearly [5], full declustering is desirable in such systems. Thus we assume full declustering across all the processing units (disks). <p> Processors: Since data distribution is assumed to be uniform, and the network provides a constant per PU bandwidth independent of the number of processors, performance of relational operations will scale linearly with increase in number of processors <ref> [5] </ref>. Thus, this parameter has been kept a constant at 32 processors in our experiments. 7.4 Workload/Queries Number of Relations: The number of relations involved in the joins were kept a constant at 8 relations.
Reference: [6] <author> Sumit Ganguly, W. Hasan, and R. Krishnamurthy. </author> <title> Query optimization for parallel execution. </title> <booktitle> Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data, </booktitle> <year> 1992. </year>
Reference-contexts: left-deep and right-deep, section 6 describes the heuristic approach, section 7 discusses the evaluation parameters, and the experimental design, section 8 has results and discussions, and section 9 has conclusions and future work. 2 Related Work Work in parallel query optimization [18] [1] [2] [8] [7] [10] [15] [20] [23] <ref> [6] </ref> [26] [22] has focussed on minimizing the total execution time (elapsed time) for a query. It is difficult, if not impossible, to directly compare the different techniques proposed in literature. <p> There still seems to be no consensus on the right model for any of the above domains. There is almost a consensus on one issue though, namely the representation of an execution plan. Most of the proposed methods <ref> [6] </ref> [23] [1] [15] [8] [18] represent an execution plan using an annotated tree. The nodes of the tree correspond to joins in [23] [15], while they correspond to finer granularity operations like hash, probe, sort, merge, etc. in [6] [18] [8] [1]. <p> Most of the proposed methods <ref> [6] </ref> [23] [1] [15] [8] [18] represent an execution plan using an annotated tree. The nodes of the tree correspond to joins in [23] [15], while they correspond to finer granularity operations like hash, probe, sort, merge, etc. in [6] [18] [8] [1]. This is mainly dependent on the granularity at which the different methods have tried to exploit parallelism. The annotations on the nodes of the tree represent the algorithm used to process the operation, and the resources allocated to the operation. <p> This model assumes synchronous I/O, and thus no overlap between I/O and CPU. Sequential and random I/O requests are not differentiated in this cost model. Bursty effects of pipelining are ignored in this model too. Ganguly et al <ref> [6] </ref> have proposed a similar approach at the operator-tree level. An initial attempt has been made to model resource contention. The execution models of [23] [15] [6] [18] [26] assume that a processor will be involved in only one join operation during any stage in the execution. [1] assumes a hybrid <p> Bursty effects of pipelining are ignored in this model too. Ganguly et al <ref> [6] </ref> have proposed a similar approach at the operator-tree level. An initial attempt has been made to model resource contention. The execution models of [23] [15] [6] [18] [26] assume that a processor will be involved in only one join operation during any stage in the execution. [1] assumes a hybrid approach where scan operators to access disk for multiple joins are allowed to be executed on the same processor, but the cpu operations corresponding to table <p> The heuristic solutions differ in the search space considered for the optimal plan. Heuristics have been proposed to generate right-deep execution trees [1], a subset of bushy trees [1] constructed by joining right-deep trees, cutting the search space of bushy trees using some memory constraints [23], dynamic programming approaches <ref> [6] </ref>, parallelizing sequential plans [8] [18], etc. Resource allocation adds another degree of freedom to the plans which can be generated. Thus, most techniques assume memory proportional processor allocation [1] [18], while some use resource idling minimization techniques [15] [23]. <p> An operator-tree is a more detailed representation of the execution plan. It is generated by expanding each join operation in the join tree into a hash operation and a probe operation. We illustrate this in figure 2. The final execution plan is represented as an annotated tree <ref> [6] </ref>. Annotations at the node are used to represent the algorithm being used to implement the operation, and the resources assigned to the operation [6]. Annotations at the edges represent whether the operation feeds a subsequent operation in a pipelined execution or is the last operation in the pipeline. <p> We illustrate this in figure 2. The final execution plan is represented as an annotated tree <ref> [6] </ref>. Annotations at the node are used to represent the algorithm being used to implement the operation, and the resources assigned to the operation [6]. Annotations at the edges represent whether the operation feeds a subsequent operation in a pipelined execution or is the last operation in the pipeline. The objective of a query optimizer can vary across applications.
Reference: [7] <author> W. Hong. </author> <title> Exploiting inter-operator parallelism in xprs. </title> <booktitle> Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 19-28, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: compares two execution structures, namely left-deep and right-deep, section 6 describes the heuristic approach, section 7 discusses the evaluation parameters, and the experimental design, section 8 has results and discussions, and section 9 has conclusions and future work. 2 Related Work Work in parallel query optimization [18] [1] [2] [8] <ref> [7] </ref> [10] [15] [20] [23] [6] [26] [22] has focussed on minimizing the total execution time (elapsed time) for a query. It is difficult, if not impossible, to directly compare the different techniques proposed in literature. <p> This is predominantly because researchers have looked at various different architectures, namely shared-everything [8] <ref> [7] </ref> [15] [22] [23] [26], shared-disk [18] [1] [2], and shared-nothing architectures [20] [10]. <p> The costs for these are discussed in section 4. Also, when there are multiple processes trying to access data from the same disk simultaneously, I/O is assumed to turn random <ref> [7] </ref>. 2. Data Declustering: Data (relations) can be distributed across all the processing units or across some of the processing units. This decision is mainly driven by the access pattern on individual relations, I/O subsystem characteristics, and the need to minimize response time for individual/batches of queries [5]. <p> 125000 tuples M Medium 250000 - 750000 tuples L Large 750000 - 1250000 tuples VL Very Large 2000000 - 5000000 tuples VVL Very Very Large 6000000 - 10000000 tuples VVVL Very Very Very Large 11000000 - 15000000 tuples Join Edges: We generate join edges using the following technique described in <ref> [7] </ref>. The relations are divided into two sets, namely selected and left. Initially all the relations are in the set left. Each time a relation is randomly picked from set left and joined to a relation selected randomly from set selected.
Reference: [8] <author> W. Hong and M. Stonebraker. </author> <title> Optimization of parallel query execution plans in xprs. </title> <booktitle> Proceedings of the 1st International conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 218-225, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: theoretically compares two execution structures, namely left-deep and right-deep, section 6 describes the heuristic approach, section 7 discusses the evaluation parameters, and the experimental design, section 8 has results and discussions, and section 9 has conclusions and future work. 2 Related Work Work in parallel query optimization [18] [1] [2] <ref> [8] </ref> [7] [10] [15] [20] [23] [6] [26] [22] has focussed on minimizing the total execution time (elapsed time) for a query. It is difficult, if not impossible, to directly compare the different techniques proposed in literature. <p> There still seems to be no consensus on the right model for any of the above domains. There is almost a consensus on one issue though, namely the representation of an execution plan. Most of the proposed methods [6] [23] [1] [15] <ref> [8] </ref> [18] represent an execution plan using an annotated tree. The nodes of the tree correspond to joins in [23] [15], while they correspond to finer granularity operations like hash, probe, sort, merge, etc. in [6] [18] [8] [1]. <p> Most of the proposed methods [6] [23] [1] [15] <ref> [8] </ref> [18] represent an execution plan using an annotated tree. The nodes of the tree correspond to joins in [23] [15], while they correspond to finer granularity operations like hash, probe, sort, merge, etc. in [6] [18] [8] [1]. This is mainly dependent on the granularity at which the different methods have tried to exploit parallelism. The annotations on the nodes of the tree represent the algorithm used to process the operation, and the resources allocated to the operation. <p> This is predominantly because researchers have looked at various different architectures, namely shared-everything <ref> [8] </ref> [7] [15] [22] [23] [26], shared-disk [18] [1] [2], and shared-nothing architectures [20] [10]. <p> Schneider et al [20], and Hong et al <ref> [8] </ref> assume an integrated task which works on multiple joins in a pipelined fashion on a set of processors. They, however, do not develop analytical cost models which can be used for query optimization. <p> Heuristics have been proposed to generate right-deep execution trees [1], a subset of bushy trees [1] constructed by joining right-deep trees, cutting the search space of bushy trees using some memory constraints [23], dynamic programming approaches [6], parallelizing sequential plans <ref> [8] </ref> [18], etc. Resource allocation adds another degree of freedom to the plans which can be generated. Thus, most techniques assume memory proportional processor allocation [1] [18], while some use resource idling minimization techniques [15] [23]. <p> A two phase approach to parallel query optimization was first proposed as a heuristic by Hong and Stonebraker <ref> [8] </ref>. They generate a work minimized query plan and then parallelize it. This is important in a multiuser environment where the exact resources available at run time are not known to the optimizer.
Reference: [9] <author> K.A. Hua and C. Lee. </author> <title> Handling data skew in multiprocessor database computers using partition tuning. </title> <booktitle> Proceedings of the 17th International Conference on Very Large Data Bases, </booktitle> <pages> pages 525-535, </pages> <month> September </month> <year> 1991. </year> <month> 30 </month>
Reference-contexts: All of these have impact on the execution model, leading to different cost models. Lu et al [15] proposed a cost model in which the elapsed time of a query plan is considered. Hua et al <ref> [9] </ref> propose a similar cost model for single join execution. Both these models assume asynchronous I/O, and perfect overlap between CPU, I/O, and communication. Sequential vs. random I/O is not considered in their cost models, while the execution model of [15] ignores pipelined parallelism. <p> Considering these factors, it is desirable that I/O be asynchronous, enabling CPU and I/O overlap and thus enhanced resource utilization. Hence, we assume an asynchronous I/O model. A similar I/O model has been assumed in [15] <ref> [9] </ref> [1] [2]. It should be noted that this I/O model differs from that considered in [20], where tradeoffs between left-deep and right-deep trees were first explored. <p> Since we are using asynchronous I/O as well as asynchronous message passing, there will be overlap in processing time across these components of the parallel machine. We assume complete overlap within phases of an execution, and no overlap across phases as in, [15] <ref> [9] </ref>. <p> During the execution of a task there will be overlap between cpu, io, and communication. Since I/O and communication are asynchronous, the overlap will be very high and we make the assumption that there will be complete overlap between the resources. This assumption is consistent with that made in <ref> [9] </ref> [15]. As a consequence, the elapsed time for a task is given by max (t cpu ; t io ; t comm ) where t cpu ; t io ; t comm are the CPU, I/O and communication costs for processing the task. <p> Data Distribution: Data distribution also plays an important part in join selectivity estimation, and on the performance of the join. Various sampling strategies attempt to handle non-uniformity in data [14]. Skew-insensitive join algorithms have also been proposed in literature <ref> [9] </ref>. However, this aspect of data sets is outside the scope of our present study. 7.3 Architecture CPU speed: The CPU MIPS rating has a substantial effect on the computation to I/O ratio for an algorithm. This becomes especially important in an environment with asynchronous I/O.
Reference: [10] <author> K.A. Hua, Y.L. Lo, and H.C. Young. </author> <title> Including the load balancing issue in the optimization of multi-way join queries for shared-nothing databse computers. </title> <booktitle> Proceedings of the 2nd International conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 74-83, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: two execution structures, namely left-deep and right-deep, section 6 describes the heuristic approach, section 7 discusses the evaluation parameters, and the experimental design, section 8 has results and discussions, and section 9 has conclusions and future work. 2 Related Work Work in parallel query optimization [18] [1] [2] [8] [7] <ref> [10] </ref> [15] [20] [23] [6] [26] [22] has focussed on minimizing the total execution time (elapsed time) for a query. It is difficult, if not impossible, to directly compare the different techniques proposed in literature. <p> This is predominantly because researchers have looked at various different architectures, namely shared-everything [8] [7] [15] [22] [23] [26], shared-disk [18] [1] [2], and shared-nothing architectures [20] <ref> [10] </ref>. Even when considering the same architecture, methods differ in whether they differentiate between sequential and random I/O, if overlap in I/O and cpu utilization is considered, if inter-operator and pipelined parallelism is considered, and if multiple joins are allowed to be processed simultaneously on the same processor. <p> Thus, we can see that there is no consensus on the ideal search space or the search strategies to use. Of the little work on query optimization for shared-nothing systems, [20] does not give detailed cost models, while <ref> [10] </ref> does not address pipelined parallelism. Schneider [20] proposed the use of pipelined right-deep plans for multi-way join execution, and compared its performance with left-deep plans. However, I/O is assumed to be synchronous in his simulation model.
Reference: [11] <author> Y.E. Ioannidis and Y. Kang. </author> <title> Randomized algorithms for optimizing large join queries. </title> <booktitle> Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Since it is an NP-Hard problem, an efficient algorithm for optimal solutions is only feasible for small problem sizes. This has led to substantial research in improved heuristics over the years [25] <ref> [11] </ref> [12]. The focus of query optimization for sequential machines has been on finding query plans which involve the least amount of work, since response time is equivalent to work done in a uniprocessor environment.
Reference: [12] <author> Y.E. Ioannidis and Y. Kang. </author> <title> Left-deep vs bushy trees: An analysis of strategy spaces and its implications for query optimization. </title> <booktitle> Proceedings of the 1991 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 168-177, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Since it is an NP-Hard problem, an efficient algorithm for optimal solutions is only feasible for small problem sizes. This has led to substantial research in improved heuristics over the years [25] [11] <ref> [12] </ref>. The focus of query optimization for sequential machines has been on finding query plans which involve the least amount of work, since response time is equivalent to work done in a uniprocessor environment. <p> Each join in the join-tree has its building relation to the left and its probing relation to the right. If each internal node of a join-tree has at least one leaf (i.e. base relation) as a child, then the tree is called deep <ref> [12] </ref>, otherwise, it is called bushy. A left deep tree is a deep tree whose probing relations are restricted to base relations. Conversely, a right-deep tree is a deep tree whose building relations are restricted to base relations. Figure 1 shows different types of join-trees.
Reference: [13] <author> M. Jarke and J. Koch. </author> <title> Query optimization in databse systems. </title> <journal> ACM Computing Surveys, </journal> <pages> pages 111-152, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: For a declarative query there are a large number of ways (usually exponential) of executing it, each having a different cost. Selecting the best ( usually least cost) plan for executing a given query is the problem of query optimization. The success in building good query optimizers [21] <ref> [13] </ref> for relational databases has been an important reason for their popularity. Since it is an NP-Hard problem, an efficient algorithm for optimal solutions is only feasible for small problem sizes. This has led to substantial research in improved heuristics over the years [25] [11] [12].
Reference: [14] <author> R.J. Lipton, J.F. Naughton, and D.A. Schneider. </author> <title> Practical selectivity estimation through adaptive sampling. </title> <booktitle> Proceedings of International Conference on Management of Data, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: We use a similar representation in our study. While a substantial amount of research has been done to develop accurate cost models for query optimization in the sequential environment [21] [17], and continues to be done <ref> [14] </ref>, cost models for the parallel environment are still in their infancy. This is predominantly because researchers have looked at various different architectures, namely shared-everything [8] [7] [15] [22] [23] [26], shared-disk [18] [1] [2], and shared-nothing architectures [20] [10]. <p> This was varied indirectly by varying join 20 selectivities (next section). Data Distribution: Data distribution also plays an important part in join selectivity estimation, and on the performance of the join. Various sampling strategies attempt to handle non-uniformity in data <ref> [14] </ref>. Skew-insensitive join algorithms have also been proposed in literature [9]. However, this aspect of data sets is outside the scope of our present study. 7.3 Architecture CPU speed: The CPU MIPS rating has a substantial effect on the computation to I/O ratio for an algorithm.
Reference: [15] <author> H. Lu, M.C. Shan, and K.L. Tan. </author> <title> Optimization of multi-way join queries for parallel execution. </title> <booktitle> Proceedings of the 17th International Conference on Very Large Data Bases, </booktitle> <month> September </month> <year> 1991. </year>
Reference-contexts: execution structures, namely left-deep and right-deep, section 6 describes the heuristic approach, section 7 discusses the evaluation parameters, and the experimental design, section 8 has results and discussions, and section 9 has conclusions and future work. 2 Related Work Work in parallel query optimization [18] [1] [2] [8] [7] [10] <ref> [15] </ref> [20] [23] [6] [26] [22] has focussed on minimizing the total execution time (elapsed time) for a query. It is difficult, if not impossible, to directly compare the different techniques proposed in literature. <p> There still seems to be no consensus on the right model for any of the above domains. There is almost a consensus on one issue though, namely the representation of an execution plan. Most of the proposed methods [6] [23] [1] <ref> [15] </ref> [8] [18] represent an execution plan using an annotated tree. The nodes of the tree correspond to joins in [23] [15], while they correspond to finer granularity operations like hash, probe, sort, merge, etc. in [6] [18] [8] [1]. <p> There is almost a consensus on one issue though, namely the representation of an execution plan. Most of the proposed methods [6] [23] [1] <ref> [15] </ref> [8] [18] represent an execution plan using an annotated tree. The nodes of the tree correspond to joins in [23] [15], while they correspond to finer granularity operations like hash, probe, sort, merge, etc. in [6] [18] [8] [1]. This is mainly dependent on the granularity at which the different methods have tried to exploit parallelism. <p> This is predominantly because researchers have looked at various different architectures, namely shared-everything [8] [7] <ref> [15] </ref> [22] [23] [26], shared-disk [18] [1] [2], and shared-nothing architectures [20] [10]. <p> All of these have impact on the execution model, leading to different cost models. Lu et al <ref> [15] </ref> proposed a cost model in which the elapsed time of a query plan is considered. Hua et al [9] propose a similar cost model for single join execution. Both these models assume asynchronous I/O, and perfect overlap between CPU, I/O, and communication. <p> Hua et al [9] propose a similar cost model for single join execution. Both these models assume asynchronous I/O, and perfect overlap between CPU, I/O, and communication. Sequential vs. random I/O is not considered in their cost models, while the execution model of <ref> [15] </ref> ignores pipelined parallelism. Chen et al [1] consider a variant of the above models where they incorporate pipelined operations. However, they assume the operations to be CPU limited and thus ignore the effect due to seek and latency times of the disk. <p> Bursty effects of pipelining are ignored in this model too. Ganguly et al [6] have proposed a similar approach at the operator-tree level. An initial attempt has been made to model resource contention. The execution models of [23] <ref> [15] </ref> [6] [18] [26] assume that a processor will be involved in only one join operation during any stage in the execution. [1] assumes a hybrid approach where scan operators to access disk for multiple joins are allowed to be executed on the same processor, but the cpu operations corresponding to <p> Resource allocation adds another degree of freedom to the plans which can be generated. Thus, most techniques assume memory proportional processor allocation [1] [18], while some use resource idling minimization techniques <ref> [15] </ref> [23]. Thus, we can see that there is no consensus on the ideal search space or the search strategies to use. Of the little work on query optimization for shared-nothing systems, [20] does not give detailed cost models, while [10] does not address pipelined parallelism. <p> Resource overlap, i.e. of CPU and I/O, is also minimized when I/O is synchronous. Considering these factors, it is desirable that I/O be asynchronous, enabling CPU and I/O overlap and thus enhanced resource utilization. Hence, we assume an asynchronous I/O model. A similar I/O model has been assumed in <ref> [15] </ref> [9] [1] [2]. It should be noted that this I/O model differs from that considered in [20], where tradeoffs between left-deep and right-deep trees were first explored. <p> Since we are using asynchronous I/O as well as asynchronous message passing, there will be overlap in processing time across these components of the parallel machine. We assume complete overlap within phases of an execution, and no overlap across phases as in, <ref> [15] </ref> [9]. <p> Since I/O and communication are asynchronous, the overlap will be very high and we make the assumption that there will be complete overlap between the resources. This assumption is consistent with that made in [9] <ref> [15] </ref>. As a consequence, the elapsed time for a task is given by max (t cpu ; t io ; t comm ) where t cpu ; t io ; t comm are the CPU, I/O and communication costs for processing the task.
Reference: [16] <author> H. Lu, K.L Tan, and M.C. Shan. </author> <title> Hash-based join algorithms for multiprocessor computers with shared memory. </title> <booktitle> Proceedings of the 16th International Conference on Very Large Data Bases, </booktitle> <pages> pages 198-209, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Parallel Asynchronous I/O is generally being accepted as the architectural solution to this problem. 4 join methods, the hash join has been found to have superior performance in terms of scalability [4] <ref> [16] </ref> [26]. Moreover, for exploiting inter-operator parallelism hash joins provide the opportunity of using pipelining.
Reference: [17] <author> L.F. Mackert and G.M. Lohman. </author> <title> R* optimizer validation and performance evaluation for local queries. </title> <booktitle> Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1986. </year>
Reference-contexts: Annotations on edges have been used to separate phases of execution in most cases. We use a similar representation in our study. While a substantial amount of research has been done to develop accurate cost models for query optimization in the sequential environment [21] <ref> [17] </ref>, and continues to be done [14], cost models for the parallel environment are still in their infancy. This is predominantly because researchers have looked at various different architectures, namely shared-everything [8] [7] [15] [22] [23] [26], shared-disk [18] [1] [2], and shared-nothing architectures [20] [10].
Reference: [18] <author> Thomas M. Niccum, J. Srivastava, B. Himatsingka, and Jianzhong Li. </author> <title> A tree decomposition approach to the parallel execution of relational query plans. </title> <type> Technical Report TR 93-16, </type> <institution> University of Minnesota, Minneapolis, Department of Computer Science, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: model, section 5 theoretically compares two execution structures, namely left-deep and right-deep, section 6 describes the heuristic approach, section 7 discusses the evaluation parameters, and the experimental design, section 8 has results and discussions, and section 9 has conclusions and future work. 2 Related Work Work in parallel query optimization <ref> [18] </ref> [1] [2] [8] [7] [10] [15] [20] [23] [6] [26] [22] has focussed on minimizing the total execution time (elapsed time) for a query. It is difficult, if not impossible, to directly compare the different techniques proposed in literature. <p> There still seems to be no consensus on the right model for any of the above domains. There is almost a consensus on one issue though, namely the representation of an execution plan. Most of the proposed methods [6] [23] [1] [15] [8] <ref> [18] </ref> represent an execution plan using an annotated tree. The nodes of the tree correspond to joins in [23] [15], while they correspond to finer granularity operations like hash, probe, sort, merge, etc. in [6] [18] [8] [1]. <p> Most of the proposed methods [6] [23] [1] [15] [8] <ref> [18] </ref> represent an execution plan using an annotated tree. The nodes of the tree correspond to joins in [23] [15], while they correspond to finer granularity operations like hash, probe, sort, merge, etc. in [6] [18] [8] [1]. This is mainly dependent on the granularity at which the different methods have tried to exploit parallelism. The annotations on the nodes of the tree represent the algorithm used to process the operation, and the resources allocated to the operation. <p> This is predominantly because researchers have looked at various different architectures, namely shared-everything [8] [7] [15] [22] [23] [26], shared-disk <ref> [18] </ref> [1] [2], and shared-nothing architectures [20] [10]. <p> Bursty effects of pipelining are ignored in this model too. Ganguly et al [6] have proposed a similar approach at the operator-tree level. An initial attempt has been made to model resource contention. The execution models of [23] [15] [6] <ref> [18] </ref> [26] assume that a processor will be involved in only one join operation during any stage in the execution. [1] assumes a hybrid approach where scan operators to access disk for multiple joins are allowed to be executed on the same processor, but the cpu operations corresponding to table building <p> Heuristics have been proposed to generate right-deep execution trees [1], a subset of bushy trees [1] constructed by joining right-deep trees, cutting the search space of bushy trees using some memory constraints [23], dynamic programming approaches [6], parallelizing sequential plans [8] <ref> [18] </ref>, etc. Resource allocation adds another degree of freedom to the plans which can be generated. Thus, most techniques assume memory proportional processor allocation [1] [18], while some use resource idling minimization techniques [15] [23]. <p> constructed by joining right-deep trees, cutting the search space of bushy trees using some memory constraints [23], dynamic programming approaches [6], parallelizing sequential plans [8] <ref> [18] </ref>, etc. Resource allocation adds another degree of freedom to the plans which can be generated. Thus, most techniques assume memory proportional processor allocation [1] [18], while some use resource idling minimization techniques [15] [23]. Thus, we can see that there is no consensus on the ideal search space or the search strategies to use.
Reference: [19] <author> Erhard Rahm. </author> <title> Parallel query processing in shared disk database systems. </title> <booktitle> SIGMOD record, </booktitle> <pages> pages 32-37, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Each of these architectures poses a number of interesting tradeoffs for query processing, which must be examined carefully to develop query optimization and scheduling strategies. Recent years have seen an increasing number of studies [24] [3] <ref> [19] </ref> which examine these tradeoffs. Since the shared nothing architecture seems to be the most promising one in terms of scalability and speedup [5], it is the focus of our present study 1 . <p> Based on this we propose a heuristic approach which gives a stable and good performance across most domains. 1 Recently it has been pointed out that tradeoffs in shared disk architectures need to be re-examined <ref> [19] </ref>.
Reference: [20] <author> D.A. Schneider and D.J. DeWitt. </author> <title> Tradeoffs in processing complex join queries via hashing in multiprocessor database machines. </title> <booktitle> Proceedings of the 16th International Conference on Very Large Data Bases, </booktitle> <month> September </month> <year> 1990. </year>
Reference-contexts: structures, namely left-deep and right-deep, section 6 describes the heuristic approach, section 7 discusses the evaluation parameters, and the experimental design, section 8 has results and discussions, and section 9 has conclusions and future work. 2 Related Work Work in parallel query optimization [18] [1] [2] [8] [7] [10] [15] <ref> [20] </ref> [23] [6] [26] [22] has focussed on minimizing the total execution time (elapsed time) for a query. It is difficult, if not impossible, to directly compare the different techniques proposed in literature. <p> This is predominantly because researchers have looked at various different architectures, namely shared-everything [8] [7] [15] [22] [23] [26], shared-disk [18] [1] [2], and shared-nothing architectures <ref> [20] </ref> [10]. Even when considering the same architecture, methods differ in whether they differentiate between sequential and random I/O, if overlap in I/O and cpu utilization is considered, if inter-operator and pipelined parallelism is considered, and if multiple joins are allowed to be processed simultaneously on the same processor. <p> Schneider et al <ref> [20] </ref>, and Hong et al [8] assume an integrated task which works on multiple joins in a pipelined fashion on a set of processors. They, however, do not develop analytical cost models which can be used for query optimization. <p> Thus, most techniques assume memory proportional processor allocation [1] [18], while some use resource idling minimization techniques [15] [23]. Thus, we can see that there is no consensus on the ideal search space or the search strategies to use. Of the little work on query optimization for shared-nothing systems, <ref> [20] </ref> does not give detailed cost models, while [10] does not address pipelined parallelism. Schneider [20] proposed the use of pipelined right-deep plans for multi-way join execution, and compared its performance with left-deep plans. However, I/O is assumed to be synchronous in his simulation model. <p> Thus, we can see that there is no consensus on the ideal search space or the search strategies to use. Of the little work on query optimization for shared-nothing systems, <ref> [20] </ref> does not give detailed cost models, while [10] does not address pipelined parallelism. Schneider [20] proposed the use of pipelined right-deep plans for multi-way join execution, and compared its performance with left-deep plans. However, I/O is assumed to be synchronous in his simulation model. <p> Hence, we assume an asynchronous I/O model. A similar I/O model has been assumed in [15] [9] [1] [2]. It should be noted that this I/O model differs from that considered in <ref> [20] </ref>, where tradeoffs between left-deep and right-deep trees were first explored. Due to high seek times for disks, it is also desirable that data be placed sequentially on a disk, and access be sequential to minimize data retrieval time. <p> These have been explained in detail in <ref> [20] </ref>, and we briefly go over these to facilitate the understanding of the task execution model and the cost model. Due to the nature of the hash operation, a probe cannot proceed unless the hash table which it is probing is completely built. <p> Also, since data is assumed to be uniformly distributed across all PUs, per PU costs are the same and correspond to the elapsed time for the task. We present the parameters and their values in the following table: The algorithm parameters are taken from <ref> [20] </ref>, the CPU speed is the mips rating of a SPARC II, the effective bandwidth per PU is the bandwidth of the network interface card at each node of the CM5, and the disk parameters are derived from an IBM 00662 model S12, 3-1/2 inch 1.0 GByte disk drive, rotating at <p> The above condition is expected to hold for many queries, and thus the theorem shows a very large subclass of queries for which one needs to look only at left deep orderings to find the optimal plan. In his comparison between right-deep and left-deep trees Schneider <ref> [20] </ref> showed the elapsed time of right-deep trees to be less than left-deep trees, though all the above conditions were true for the sample queries. The predominant reason was the fact that I/O was assumed to be synchronous. <p> The combination of the above mentioned execution strategies results in increased resource utilization, and increased CPU, I/O utilization overlap for right-deep trees. Hence, right-deep trees performed somewhat better than left deep trees as reported in <ref> [20] </ref>. This however will not occur if I/O is asynchronous, and as shown above it is important to consider left-deep trees for such an architecture. Exceptions: Exceptions to the conditions in the corollary can arise when the join selectivities are large. <p> This equation is correct if there is always enough memory to hold the hash table corresponding to each join node in the left-deep join tree. However, if there isn't enough memory to hold even one hash table at a given stage, then the parallel hybrid-hash join algorithm <ref> [20] </ref> is used, for which the extra cost of executing the join is different from that shown above. In this case the extra cost for join at level i is approximately 3 fl (jI i j + jI i+1 j).
Reference: [21] <editor> P.P. Selinger and et al. </editor> <title> Access path selection in a relational database management system. </title> <booktitle> Proceedings of the 1979 ACM SIGMOD International Conference on Management of Data, </booktitle> <year> 1979. </year>
Reference-contexts: For a declarative query there are a large number of ways (usually exponential) of executing it, each having a different cost. Selecting the best ( usually least cost) plan for executing a given query is the problem of query optimization. The success in building good query optimizers <ref> [21] </ref> [13] for relational databases has been an important reason for their popularity. Since it is an NP-Hard problem, an efficient algorithm for optimal solutions is only feasible for small problem sizes. This has led to substantial research in improved heuristics over the years [25] [11] [12]. <p> Annotations on edges have been used to separate phases of execution in most cases. We use a similar representation in our study. While a substantial amount of research has been done to develop accurate cost models for query optimization in the sequential environment <ref> [21] </ref> [17], and continues to be done [14], cost models for the parallel environment are still in their infancy. This is predominantly because researchers have looked at various different architectures, namely shared-everything [8] [7] [15] [22] [23] [26], shared-disk [18] [1] [2], and shared-nothing architectures [20] [10].
Reference: [22] <author> E.J. Shekita, H.C. Young, and K.L Tan. </author> <title> Multi-join query optimization for symmetric multi-processors. </title> <booktitle> Proceedings of the 19th International Conference on Very Large Data Bases, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: right-deep, section 6 describes the heuristic approach, section 7 discusses the evaluation parameters, and the experimental design, section 8 has results and discussions, and section 9 has conclusions and future work. 2 Related Work Work in parallel query optimization [18] [1] [2] [8] [7] [10] [15] [20] [23] [6] [26] <ref> [22] </ref> has focussed on minimizing the total execution time (elapsed time) for a query. It is difficult, if not impossible, to directly compare the different techniques proposed in literature. <p> This is predominantly because researchers have looked at various different architectures, namely shared-everything [8] [7] [15] <ref> [22] </ref> [23] [26], shared-disk [18] [1] [2], and shared-nothing architectures [20] [10].
Reference: [23] <author> J. Srivastava and G. Elsesser. </author> <title> Optimizing multi-join queries in parallel relational databases. </title> <booktitle> Proceedings of the 2nd International conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 84-92, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: namely left-deep and right-deep, section 6 describes the heuristic approach, section 7 discusses the evaluation parameters, and the experimental design, section 8 has results and discussions, and section 9 has conclusions and future work. 2 Related Work Work in parallel query optimization [18] [1] [2] [8] [7] [10] [15] [20] <ref> [23] </ref> [6] [26] [22] has focussed on minimizing the total execution time (elapsed time) for a query. It is difficult, if not impossible, to directly compare the different techniques proposed in literature. <p> There still seems to be no consensus on the right model for any of the above domains. There is almost a consensus on one issue though, namely the representation of an execution plan. Most of the proposed methods [6] <ref> [23] </ref> [1] [15] [8] [18] represent an execution plan using an annotated tree. The nodes of the tree correspond to joins in [23] [15], while they correspond to finer granularity operations like hash, probe, sort, merge, etc. in [6] [18] [8] [1]. <p> There is almost a consensus on one issue though, namely the representation of an execution plan. Most of the proposed methods [6] <ref> [23] </ref> [1] [15] [8] [18] represent an execution plan using an annotated tree. The nodes of the tree correspond to joins in [23] [15], while they correspond to finer granularity operations like hash, probe, sort, merge, etc. in [6] [18] [8] [1]. This is mainly dependent on the granularity at which the different methods have tried to exploit parallelism. <p> This is predominantly because researchers have looked at various different architectures, namely shared-everything [8] [7] [15] [22] <ref> [23] </ref> [26], shared-disk [18] [1] [2], and shared-nothing architectures [20] [10]. <p> However, they assume the operations to be CPU limited and thus ignore the effect due to seek and latency times of the disk. Resource contention, and periodic bursty behaviour of pipelines are also not considered. Srivastava et al <ref> [23] </ref> use a bottom-up approach in which the response time of a tree rooted at an operator is estimated as the sum of the time taken 3 by the operator, and the maximum of the response times of its children. <p> Sequential and random I/O requests are not differentiated in this cost model. Bursty effects of pipelining are ignored in this model too. Ganguly et al [6] have proposed a similar approach at the operator-tree level. An initial attempt has been made to model resource contention. The execution models of <ref> [23] </ref> [15] [6] [18] [26] assume that a processor will be involved in only one join operation during any stage in the execution. [1] assumes a hybrid approach where scan operators to access disk for multiple joins are allowed to be executed on the same processor, but the cpu operations corresponding <p> The heuristic solutions differ in the search space considered for the optimal plan. Heuristics have been proposed to generate right-deep execution trees [1], a subset of bushy trees [1] constructed by joining right-deep trees, cutting the search space of bushy trees using some memory constraints <ref> [23] </ref>, dynamic programming approaches [6], parallelizing sequential plans [8] [18], etc. Resource allocation adds another degree of freedom to the plans which can be generated. Thus, most techniques assume memory proportional processor allocation [1] [18], while some use resource idling minimization techniques [15] [23]. <p> of bushy trees using some memory constraints <ref> [23] </ref>, dynamic programming approaches [6], parallelizing sequential plans [8] [18], etc. Resource allocation adds another degree of freedom to the plans which can be generated. Thus, most techniques assume memory proportional processor allocation [1] [18], while some use resource idling minimization techniques [15] [23]. Thus, we can see that there is no consensus on the ideal search space or the search strategies to use. Of the little work on query optimization for shared-nothing systems, [20] does not give detailed cost models, while [10] does not address pipelined parallelism.
Reference: [24] <author> M. Stonebraker. </author> <title> The case for shared nothing. </title> <journal> Database Engineering, </journal> <volume> 9(1) </volume> <pages> 4-9, </pages> <year> 1986. </year>
Reference-contexts: Parallel database architectures can be broadly classified into shared nothing, shared-disk and shared everything <ref> [24] </ref> [3]. Each of these architectures poses a number of interesting tradeoffs for query processing, which must be examined carefully to develop query optimization and scheduling strategies. Recent years have seen an increasing number of studies [24] [3] [19] which examine these tradeoffs. <p> Parallel database architectures can be broadly classified into shared nothing, shared-disk and shared everything <ref> [24] </ref> [3]. Each of these architectures poses a number of interesting tradeoffs for query processing, which must be examined carefully to develop query optimization and scheduling strategies. Recent years have seen an increasing number of studies [24] [3] [19] which examine these tradeoffs. Since the shared nothing architecture seems to be the most promising one in terms of scalability and speedup [5], it is the focus of our present study 1 .
Reference: [25] <author> A. Swami and A. Gupta. </author> <title> Optimizing of large join queries. </title> <booktitle> Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 8-17, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Since it is an NP-Hard problem, an efficient algorithm for optimal solutions is only feasible for small problem sizes. This has led to substantial research in improved heuristics over the years <ref> [25] </ref> [11] [12]. The focus of query optimization for sequential machines has been on finding query plans which involve the least amount of work, since response time is equivalent to work done in a uniprocessor environment. <p> Since exhaustive techniques for this would be expensive, heuristic techniques, such as simulated annealing <ref> [25] </ref>, greedy approaches, etc. can be used. In this study we use the following greedy approach: Greedy Approach: In the proposed greedy approach, at each step the sum of the sizes of the next consecutive pair of intermediate relations is minimized.
Reference: [26] <author> A.N. Wilschut and P.M.G. Apers. </author> <title> Dataflow query execution in a parallel main memory environment. </title> <booktitle> Proceedings of the 1st International conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 68-77, </pages> <month> December </month> <year> 1991. </year> <month> 32 </month>
Reference-contexts: and right-deep, section 6 describes the heuristic approach, section 7 discusses the evaluation parameters, and the experimental design, section 8 has results and discussions, and section 9 has conclusions and future work. 2 Related Work Work in parallel query optimization [18] [1] [2] [8] [7] [10] [15] [20] [23] [6] <ref> [26] </ref> [22] has focussed on minimizing the total execution time (elapsed time) for a query. It is difficult, if not impossible, to directly compare the different techniques proposed in literature. <p> This is predominantly because researchers have looked at various different architectures, namely shared-everything [8] [7] [15] [22] [23] <ref> [26] </ref>, shared-disk [18] [1] [2], and shared-nothing architectures [20] [10]. <p> Bursty effects of pipelining are ignored in this model too. Ganguly et al [6] have proposed a similar approach at the operator-tree level. An initial attempt has been made to model resource contention. The execution models of [23] [15] [6] [18] <ref> [26] </ref> assume that a processor will be involved in only one join operation during any stage in the execution. [1] assumes a hybrid approach where scan operators to access disk for multiple joins are allowed to be executed on the same processor, but the cpu operations corresponding to table building and <p> Parallel Asynchronous I/O is generally being accepted as the architectural solution to this problem. 4 join methods, the hash join has been found to have superior performance in terms of scalability [4] [16] <ref> [26] </ref>. Moreover, for exploiting inter-operator parallelism hash joins provide the opportunity of using pipelining. <p> In this case a parallel hybrid-hash join algorithm is used. Thus, the parallel hybrid-hash join algorithm also corresponds to a feasible task. Since two-way hash joins <ref> [26] </ref> are not considered in this study, there are no other feasible task structures. 3.3.2 Execution of Tasks Since the focus of this study is to examine the tradeoffs between left-deep and right-deep join-trees as execution plans, we focus our attention on plans whose join tree is right-deep or left-deep.
References-found: 26

