URL: http://www.frc.ri.cmu.edu/~bdigney/latestPubs/sab_96_hierachical.ps
Refering-URL: http://www.frc.ri.cmu.edu/~bdigney/latestPubs/
Root-URL: 
Email: bdigney@dres.dnd.ca  
Title: Emergent Hierarchical Control Structures: Learning Reactive/Hierarchical Relationships in Reinforcement Environments  
Author: Bruce L. Digney 
Note: E mail  
Address: Box 4000, Medicine Hat, Alberta, CANADA, T1A 8K6  
Affiliation: Defence Research Establishment Suffield  
Abstract: The use of externally imposed hierarchical structures to reduce the complexity of learning control is common. However, it is acknowledged that learning the hierarchical structure itself is an important step towards more general (learning of many things as required) and less bounded (learning of a single thing as specified) learning. Presented in this paper is a reinforcement learning algorithm called Nested Q-learning that generates a hierarchical control structure in reinforcement learning domains. The emergent structure combined with learned bottom-up reactive reactions results in a reactive hierarchical control system. Effectively, the learned hierarchy decomposes what would otherwise be a monolithic evaluation function into many smaller evaluation functions that can be recombined without the loss of previously learned information. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Barto, A.G., Sutton, R.S. and Watkins C.H. </author> <year> (1989), </year> <title> Learning and Sequential Decision Making, </title> <type> COINS Technical Report. </type>
Reference-contexts: 1 Introduction The use of Q-learning <ref> [1] </ref> and other related reinforcement learning techniques is common for the control of autonomous robots [2]. However, long learning times combined with the slow speed (when compared to simulations) and the frailties of robot hardware present considerable problems when scaling up to real applications.
Reference: [2] <author> Digney B. L. </author> <year> (1994), </year> <title> A Distributed Adaptive Control System for a Quadruped Mobile Robot, From animals to animats 3: </title> <address> SAB 94, Brighton, UK, </address> <month> August </month> <year> 1994, </year> <pages> pp. 344-354, </pages> <publisher> MIT Press-Bradford Books, </publisher> <month> Massachussets. </month> <title> Q 11 Q 25 Q 25 Q 11 (b) Second skill learned, Q 10 .(a) First skills learned Q 9 Q 11 and Q 25 . trol structure after the first task, fulfilled by skill Q 9 , is learned, and (b) after the second task, fulfilled by skill Q 10 , is learned. Note: Q 10 learns to use previously acquired skills. </title>
Reference-contexts: 1 Introduction The use of Q-learning [1] and other related reinforcement learning techniques is common for the control of autonomous robots <ref> [2] </ref>. However, long learning times combined with the slow speed (when compared to simulations) and the frailties of robot hardware present considerable problems when scaling up to real applications. Often, to reduce the problem to a more tractable size, the control problem is hand decomposed into a hierarchical structure [3].
Reference: [3] <author> Albus, J.S. </author> <year> (1991), </year> <title> Outline of a Theory of Intelligence. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 21, 3, </volume> <pages> pp. 473-509. </pages>
Reference-contexts: However, long learning times combined with the slow speed (when compared to simulations) and the frailties of robot hardware present considerable problems when scaling up to real applications. Often, to reduce the problem to a more tractable size, the control problem is hand decomposed into a hierarchical structure <ref> [3] </ref>. This abstracts it into many smaller, more easily learned portions. However, hand decomposition imposes the designer's preconceived notions on the robot which, from the robot's point of view, may be inefficient or incorrect.
Reference: [4] <author> Digney, B.L. </author> <year> (1996), </year> <title> Shaping Emergent Control Struc tures with Scaffolding Actions and Staged Learning, </title> <booktitle> Artificial Life V, </booktitle> <month> May, </month> <year> 1996, </year> <note> Nara-Ken, Japan. (to appear) </note>
Reference-contexts: This continual carrying forward of learned information is called life-long learning and provides the robot with a head start when learning new tasks <ref> [4] </ref>. As the robot moves from task to task and environment to environment it will have the accumulated information of its past experiences available to it as skills. These transportable skills will allow the robot to learn progressively more complex tasks. <p> This continual learning lends itself to use in pre-training or shaping, either by chance or through a regimented training program. In nested Q-Learning controlled robots the use of scaffolding actions and staged learning to exploit this online skill transfer between task/environment settings, is discussed in more detail elsewhere <ref> [4] </ref>. 2 Learning Hierarchical Structures 2.1 Introduction Consider the example of a robot perceiving its world through sensors and receiving internal and external reinforcement as pictured schematically in Figure 1 (a). This robot is capable of acting on its world using a number of primitive actuator movements.
Reference: [5] <author> Tyrrel, T., </author> <year> (1992), </year> <title> The Use of Hierarchies for Action Selection, </title> <booktitle> From animals to animats 2: </booktitle> <volume> SAB 92, </volume> <pages> pp. 138-148, </pages> <publisher> MIT Press-Bradford Books, Massachussets. </publisher>
Reference-contexts: In the field of intelligent control, the strategies connecting the sensors to the actions are either hardwired by a designer, taught to the agent or left to be autonomously learned by the agent. There are many variations <ref> [5] </ref> [6] of architectures in which such control systems may be implemented. The two main variations are flat and hierarchical as shown schematically in Figure 1 (b), (1) and (2), respectively.
Reference: [6] <author> Meyer, J.A. and Guillot, A. </author> <year> (1994), </year> <note> From SAB90 to SAB94, From animals to animats 3: SAB 94, </note> <institution> Brighton UK, </institution> <month> August </month> <year> 1994, </year> <pages> pp. 2-11, </pages> <publisher> MIT Press-Bradford Books, Massachussets. </publisher>
Reference-contexts: In the field of intelligent control, the strategies connecting the sensors to the actions are either hardwired by a designer, taught to the agent or left to be autonomously learned by the agent. There are many variations [5] <ref> [6] </ref> of architectures in which such control systems may be implemented. The two main variations are flat and hierarchical as shown schematically in Figure 1 (b), (1) and (2), respectively.
Reference: [7] <author> Maes, P. and Brooks, R.A., </author> <year> (1990), </year> <title> Learning to coordi nate behaviors, </title> <booktitle> Eighth National Conference on Artificial Intelligence pp. </booktitle> <pages> 796-802, </pages> <year> 1990. </year>
Reference-contexts: Hierarchical learning control systems are often hand-crafted with a number of low level behaviors controlled by a gating function. The low level behaviors map the current state of the agent into actuator activities while the gating function determines which low level behavior is to be active. Maes and Brooks <ref> [7] </ref> used this approach by hand-crafting the individual low level behaviors while the gating system was learned from a reinforcement signal during operation. Mahadevan and Connell [8] used the complementary approach in which a hand-crafted gating function and separate reinforcement signals which were supplied for each individual behavior.
Reference: [8] <author> Mahadevan, S. and Connell, J. </author> <year> (1991), </year> <title> Automatic programming of behavior based robots using reinforcement learning, </title> <booktitle> Nineth National Conference on Artificial Intelligence, </booktitle> <address> Anaheim, CA, </address> <year> 1991. </year>
Reference-contexts: Maes and Brooks [7] used this approach by hand-crafting the individual low level behaviors while the gating system was learned from a reinforcement signal during operation. Mahadevan and Connell <ref> [8] </ref> used the complementary approach in which a hand-crafted gating function and separate reinforcement signals which were supplied for each individual behavior. Lin [9] used a similar approach combined with staged learning to first train the individual behaviors and then train a gating function.
Reference: [9] <author> Long-Ji, L. </author> <year> (1993), </year> <title> Hierarchical learning of robot skills, </title> <booktitle> IEEE International Conference on Neural Networks, </booktitle> <address> San Francisco, </address> <year> 1993, </year> <pages> pp. 181-186. </pages>
Reference-contexts: Mahadevan and Connell [8] used the complementary approach in which a hand-crafted gating function and separate reinforcement signals which were supplied for each individual behavior. Lin <ref> [9] </ref> used a similar approach combined with staged learning to first train the individual behaviors and then train a gating function. Dayan and Hinton's Feudal Q-learning [10] used a hierarchical master/slave type architecture with a set of hand-crafted commands and corresponding reinforcement functions.
Reference: [10] <author> Dayan, P. and Hinton, G.E., </author> <title> Feudal reinforcement learn ing, </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateco, CA, </address> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year>
Reference-contexts: Lin [9] used a similar approach combined with staged learning to first train the individual behaviors and then train a gating function. Dayan and Hinton's Feudal Q-learning <ref> [10] </ref> used a hierarchical master/slave type architecture with a set of hand-crafted commands and corresponding reinforcement functions. The commands served as actions available to the high level master.
Reference: [11] <author> Singh, P.S., </author> <year> (1992), </year> <title> Transfer of learning by composing solutions of elemental sequential tasks, </title> <journal> Machine Learning, </journal> <volume> 8(3/4), </volume> <pages> pp. 323-339, </pages> <year> 1992. </year>
Reference-contexts: In this approach, learning occurs at both levels simultaneously. The lower levels learn the commands and the higher level learns when to invoke the low level commands to perform some desired task. Singh's Compositional Q-learning system <ref> [11] </ref> first learned elemental tasks and then the gating functions that switch between elemental tasks in the correct temporal sequence to achieve some high level goal. Kaelbling's Hierarchical Distance to Goal (HDG) [12] viewed the world on two levels of resolution.
Reference: [12] <author> Kaelbling, </author> <title> L.P., (1993), Hierarchical learning in stochas tic domains: Preliminary results, </title> <booktitle> Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, MA, </address> <year> 1993. </year>
Reference-contexts: Singh's Compositional Q-learning system [11] first learned elemental tasks and then the gating functions that switch between elemental tasks in the correct temporal sequence to achieve some high level goal. Kaelbling's Hierarchical Distance to Goal (HDG) <ref> [12] </ref> viewed the world on two levels of resolution. Regions, with their centres referred to as landmarks, were used to move an agent into the same region as the goal's location. Then local actions were used to move to the goal.
Reference: [13] <author> Thurn, S.B., </author> <year> (1992), </year> <title> Efficient exploration in reinforcement learning, </title> <type> Technical Report CMU-CS-92-102, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: It can be random, error or re-cency based. In this development a recency based exploration policy is used which ensures that the actions (primitive/skill) that were taken less recently are favored over the more recent actions <ref> [13] </ref>. Upon performing the selected action, u fl , be it a primitive action or a skill, the robot advances from state x v to the next state x w and incurs a total reinforcement signal, r T OT AL .
References-found: 13

