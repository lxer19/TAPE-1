URL: http://www.cs.helsinki.fi/~jkivinen/abstracts/kw-rlbmrp-97.ps.gz
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/1998/97abstracts.html
Root-URL: 
Title: Relative Loss Bounds for Multidimensional Regression Problems  
Author: Jyrki Kivinen Manfred K. Warmuth 
Address: P.O. Box 26 (Teollisuuskatu 23) FIN-00014 University of Helsinki, Finland  Santa Cruz, CA 95064, USA  
Affiliation: Department of Computer Science  Department of Computer Science University of California, Santa Cruz  
Abstract: We study on-line generalized linear regression with multidimensional outputs, i.e., neural networks with multiple output nodes but no hidden nodes. We allow at the final layer transfer functions such as the soft-max function that need to consider the linear activations to all the output neurons. We use distance functions of a certain kind in two completely independent roles in deriving and analyzing on-line learning algorithms for such tasks. We use one distance function to define a matching loss function for the (possibly multidimensional) transfer function, which allows us to generalize earlier results from one-dimensional to multidimensional outputs. We use another distance function as a tool for measuring progress made by the on-line updates. This shows how previously studied algorithms such as gradient descent and exponentiated gradient fit into a common framework. We evaluate the performance of the algorithms using relative loss bounds that compare the loss of the on-line algoritm to the best off-line predictor from the relevant model class, thus completely eliminating probabilistic assumptions about the data.
Abstract-found: 1
Intro-found: 1
Reference: [Ama85] <author> S. Amari. </author> <title> Differential Geometrical Methods in Statistics. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1985. </year>
Reference-contexts: Jagota and Warmuth [JW98] view (1) as an Euler discretization of a system of partial differential equations and investigate the performance of the algorithm as the discretization parameter approaches zero. The distance functions we use here have previously been applied in the context of exponential families by Amari <ref> [Ama85] </ref> and others. Here we only need some basic technical properties of the distance functions that can easily be derived from the definitions. For a discussion of our line of work in a statistical context see Azoury and Warmuth [AW97]. <p> loss function. (To allow y j = 0 or y j = 0, we adopt the standard convention that 0 ln 0 = 0 ln (0=0) = 0 and y ln (y=0) = 1 for y &gt; 0.) 2 In the relative loss bound proofs we use the basic property <ref> [JW98, Ama85] </ref> D ( b a ; a) = D ( b a ; b a) + D ( b a; a) + (( b a) (a)) ( b a b a) : (3) This shows that our distances do not satisfy the triangle inequality.
Reference: [AHW95] <author> P. Auer, M. Herbster, and M. K. Warmuth. </author> <title> Exponentially many local minima for single neurons. </title> <booktitle> In Proc. 1995 Neural Information Processing Conference, </booktitle> <pages> pages 316-317. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: In particular, when we apply the logistic transfer function and try to find a weight vector ! that minimizes the total square loss over ` examples (x t ; y t ), we may have up to ` n local minima <ref> [AHW95, Bud93] </ref>. Hence, some other choice of loss function might be more convenient. <p> In the one-dimensional case it can be shown that any continuous strictly increasing transfer function has a specific matching loss function L such that, among other useful properties, P t L (y t ; (! x t )) is always convex in !, so local minima are not a problem <ref> [AHW95] </ref>. For example, the matching loss function for the logistic transfer function is the relative entropy (a generalization of the logarithmic loss for continuous-valued outcomes). The square loss is the matching loss function for the identity transfer function (i.e., linear regression). <p> In the one-dimensional case, the integral is a simple definite integral, and has a convex potential (i.e., integral) function if it is strictly increasing and continuous <ref> [AHW95, HKW95] </ref>. Let now have range V R k and distance function D .
Reference: [AW97] <author> K. Azoury and M. K. Warmuth. </author> <title> Relative loss bounds and the exponential family of distributions. </title> <type> Unpublished manuscript, </type> <year> 1997. </year>
Reference-contexts: Here we only need some basic technical properties of the distance functions that can easily be derived from the definitions. For a discussion of our line of work in a statistical context see Azoury and Warmuth <ref> [AW97] </ref>. In Section 2 we review the definition of a matching loss function and give examples. Section 3 discusses the general additive algorithm in more detail.
Reference: [Bud93] <author> M. Budinich. </author> <title> Some notes on perceptron learning. </title> <journal> J. Phys. A.: Math. Gen., </journal> <volume> 26 </volume> <pages> 4237-4247, </pages> <year> 1993. </year>
Reference-contexts: In particular, when we apply the logistic transfer function and try to find a weight vector ! that minimizes the total square loss over ` examples (x t ; y t ), we may have up to ` n local minima <ref> [AHW95, Bud93] </ref>. Hence, some other choice of loss function might be more convenient.
Reference: [CB97] <author> N. Cesa-Bianchi. </author> <title> Analysis of two gradient-based algorithms for on-line regres sion. </title> <booktitle> In Proc. 10th Annu. Conf. on Comput. Learning Theory, </booktitle> <pages> pages 163-170. </pages> <publisher> ACM, </publisher> <year> 1997. </year>
Reference-contexts: Cesa-Bianchi <ref> [CB97] </ref> has used somewhat different methods to obtain bounds also in cases in which the loss function does not match the transfer function.
Reference: [Fos91] <author> D. P. Foster. </author> <title> Prediction in the worst case. </title> <journal> The Annals of Statistics, </journal> <volume> 19(2):1084 1090, </volume> <year> 1991. </year>
Reference-contexts: For more discussion and examples in the special case of linear regression, see [KW97]. An interesting related idea is using all the previous examples in the update instead of just the last one. For work along these lines in the linear case see Vovk [Vov97] and Foster <ref> [Fos91] </ref>. 4 RELATIVE LOSS BOUNDS Consider a sequence S = ((x 1 ; y 1 ); : : : ; (x ` ; y ` )) of training examples, and let Loss (GA; S) = P ` t=1 L (y t ; b y t ) be the loss incurred by
Reference: [GLS97] <author> A. J. Grove, N. Littlestone, and D. Schuurmans. </author> <title> General convergence results for linear discriminant updates. </title> <booktitle> In Proc. 10th Annu. Conf. on Comput. Learning Theory, </booktitle> <pages> pages 171-183. </pages> <publisher> ACM, </publisher> <year> 1997. </year>
Reference-contexts: the input data is typically added to realize positive and negative weights with arbitrary norm.) In work parallel to this, the analogue of the general additive update (1) in the context of linear classification, i.e., with a threshold transfer function, has recently been developed and analyzed by Grove et al. <ref> [GLS97] </ref> with methods and results very similar to ours. Cesa-Bianchi [CB97] has used somewhat different methods to obtain bounds also in cases in which the loss function does not match the transfer function.
Reference: [HKW95] <author> D. P. Helmbold, J. Kivinen, and M. K. Warmuth. </author> <title> Worst-case loss bounds for sigmoided linear neurons. </title> <booktitle> In Proc. Neural Information Processing Systems 1995, </booktitle> <pages> pages 309-315. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: For the identity transfer function, the matching loss function is the squared Euclidean distance. The first result we get from this observation connecting matching losses to a general notion of distance is that certain previous results on generalized linear regression with matching loss on one-dimensional outputs <ref> [HKW95] </ref> directly generalize to multidimensional outputs. From a more general point of view, a much more interesting feature of these distance functions is how they allow us to view certain previously known learning algorithms, and introduce new ones, in a simple unified framework. <p> Notice that here is updated by the gradient with respect to !, so this is not just a gradient descent with reparameterization [JW98]. However, we obtain the usual on-line gradient descent when is the identity function. When is the softmax function, we get the so-called exponentiated gradient (EG) algorithm <ref> [KW97, HKW95] </ref>. The connection of the distance function D to the update (1) is two-fold. <p> Second, the distance function D can be used as a potential function in analyzing the performance of the resulting algorithm. The same distance functions have been used previously for exactly the same purposes <ref> [KW97, HKW95] </ref> in important special cases (the gradient descent and EG algorithms) but without realizing the full generality of the method. <p> contrast, the choice of the transfer function depends on what kind of a regression problem we wish to solve.) Earlier work suggests that the softmax parameterization function (i.e., the EG algorithm) is particularly suited for situations in which some sparse weight vector ! gives a good match to the data <ref> [HKW95, KW97] </ref>. (Because softmax normalizes the weight vector and makes the components positive, a simple transformation of the input data is typically added to realize positive and negative weights with arbitrary norm.) In work parallel to this, the analogue of the general additive update (1) in the context of linear classification, <p> In the one-dimensional case, the integral is a simple definite integral, and has a convex potential (i.e., integral) function if it is strictly increasing and continuous <ref> [AHW95, HKW95] </ref>. Let now have range V R k and distance function D .
Reference: [JW98] <author> A. K. Jagota and M. K. Warmuth. </author> <title> Continuous versus discrete-time nonlinear gradient descent: Relative loss bounds and convergence. </title> <booktitle> Presented at Fifth Symposium on Artificial Intelligence and Mathematics, </booktitle> <address> Ft. Lauderdale, FL, </address> <year> 1998. </year>
Reference-contexts: We call this algorithm the general additive algorithm with parameterization function . Notice that here is updated by the gradient with respect to !, so this is not just a gradient descent with reparameterization <ref> [JW98] </ref>. However, we obtain the usual on-line gradient descent when is the identity function. When is the softmax function, we get the so-called exponentiated gradient (EG) algorithm [KW97, HKW95]. The connection of the distance function D to the update (1) is two-fold. <p> Cesa-Bianchi [CB97] has used somewhat different methods to obtain bounds also in cases in which the loss function does not match the transfer function. Jagota and Warmuth <ref> [JW98] </ref> view (1) as an Euler discretization of a system of partial differential equations and investigate the performance of the algorithm as the discretization parameter approaches zero. The distance functions we use here have previously been applied in the context of exponential families by Amari [Ama85] and others. <p> loss function. (To allow y j = 0 or y j = 0, we adopt the standard convention that 0 ln 0 = 0 ln (0=0) = 0 and y ln (y=0) = 1 for y &gt; 0.) 2 In the relative loss bound proofs we use the basic property <ref> [JW98, Ama85] </ref> D ( b a ; a) = D ( b a ; b a) + D ( b a; a) + (( b a) (a)) ( b a b a) : (3) This shows that our distances do not satisfy the triangle inequality.
Reference: [KW97] <author> J. Kivinen and M. K. Warmuth. </author> <title> Additive versus exponentiated gradient updates for linear prediction. </title> <journal> Information and Computation, </journal> <volume> 132(1) </volume> <pages> 1-64, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: Notice that here is updated by the gradient with respect to !, so this is not just a gradient descent with reparameterization [JW98]. However, we obtain the usual on-line gradient descent when is the identity function. When is the softmax function, we get the so-called exponentiated gradient (EG) algorithm <ref> [KW97, HKW95] </ref>. The connection of the distance function D to the update (1) is two-fold. <p> Second, the distance function D can be used as a potential function in analyzing the performance of the resulting algorithm. The same distance functions have been used previously for exactly the same purposes <ref> [KW97, HKW95] </ref> in important special cases (the gradient descent and EG algorithms) but without realizing the full generality of the method. <p> contrast, the choice of the transfer function depends on what kind of a regression problem we wish to solve.) Earlier work suggests that the softmax parameterization function (i.e., the EG algorithm) is particularly suited for situations in which some sparse weight vector ! gives a good match to the data <ref> [HKW95, KW97] </ref>. (Because softmax normalizes the weight vector and makes the components positive, a simple transformation of the input data is typically added to realize positive and negative weights with arbitrary norm.) In work parallel to this, the analogue of the general additive update (1) in the context of linear classification, <p> Under certain regularity assumptions, the update rule of the GA algorithm can be shown to approximately solve this minimization problem. For more discussion and examples in the special case of linear regression, see <ref> [KW97] </ref>. An interesting related idea is using all the previous examples in the update instead of just the last one. <p> Basically, our goal is to show that if some W achieves a small loss, then the algorithm is not doing much worse, regardless of how the sequence S was generated. Making additional probabilistic assumptions allows such on-line loss bounds to be converted into more traditional results about generalization errors <ref> [KW97] </ref>. To give the bounds for Loss (GA; S) in terms of Loss (W; S) we need some additional parameters.
Reference: [MN89] <author> P. McCullagh and J. A. Nelder. </author> <title> Generalized Linear Models. </title> <publisher> Chapman & Hall, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: The goodness of the fit is quantitatively measured in terms of a loss function; the square loss given by P t;j (y t;j y t;j ) 2 =2 is a popular choice. In generalized linear regression <ref> [MN89] </ref> we fix a transfer function and apply it on top of a linear model. Thus, in the one-dimensional case we would have f (!; x t ) = (! x t ).
Reference: [Vov97] <author> V. Vovk. </author> <title> Competitive on-line linear regression. </title> <booktitle> In Proc. Neural Information Processing Systems 1997. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1998. </year>
Reference-contexts: For more discussion and examples in the special case of linear regression, see [KW97]. An interesting related idea is using all the previous examples in the update instead of just the last one. For work along these lines in the linear case see Vovk <ref> [Vov97] </ref> and Foster [Fos91]. 4 RELATIVE LOSS BOUNDS Consider a sequence S = ((x 1 ; y 1 ); : : : ; (x ` ; y ` )) of training examples, and let Loss (GA; S) = P ` t=1 L (y t ; b y t ) be the
References-found: 12

