URL: http://www.coli.uni-sb.de/~christer/acl97.ps
Refering-URL: http://www.coli.uni-sb.de/~christer/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: christer@research.bell-labs.com  Atro.Voutilainen@Helsinki.FI  
Title: Comparing a Linguistic and a Stochastic Tagger  
Author: Christer Samuelsson Atro Voutilainen 
Address: 600 Mountain Ave, Room 2D-339 Murray Hill, NJ 07974, USA  P.O. Box 4  Finland  
Affiliation: Lucent Technologies Bell Laboratories  Research Unit for Multilingual Language Technology  University of Helsinki  
Pubnum: FIN-00014  
Abstract: Concerning different approaches to automatic PoS tagging: EngCG-2, a constraint-based morphological tagger, is compared in a double-blind test with a state-of-the-art statistical tagger on a common disambiguation task using a common tag set. The experiments show that for the same amount of remaining ambiguity, the error rate of the statistical tagger is one order of magnitude greater than that of the rule-based one. The two related issues of priming effects compromising the results and disagreement between human annotators are also addressed.
Abstract-found: 1
Intro-found: 1
Reference: <author> J-P Chanod and P. Tapanainen. </author> <year> 1995. </year> <title> Tagging French: comparing a statistical and a constraint-based method. </title> <booktitle> In Procs. 7th Conference of the European Chapter of the Association for Computational Linguistics, </booktitle> <pages> pp. 149-157, </pages> <booktitle> ACL, </booktitle> <year> 1995. </year>
Reference: <author> K. W. Church. </author> <year> 1988. </year> <title> "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text". </title> <booktitle> In Procs. 2nd Conference on Applied Natural Language Processing , pp. </booktitle> <pages> 136-143, </pages> <booktitle> ACL, </booktitle> <year> 1988. </year>
Reference-contexts: Also EngCG's output was converted into this format to enable direct comparison with the statistical tagger. 3 The Statistical Tagger The statistical tagger used in the experiments is a classical trigram-based HMM decoder of the kind described in e.g. <ref> (Church 1988) </ref>, (DeRose 1988) and numerous other articles.
Reference: <author> K. Church. </author> <year> 1992. </year> <title> Current Practice in Part of Speech Tagging and Suggestions for the Future. </title> <editor> In Simmons (ed.), Sbornik praci: </editor> <booktitle> In Honor of Henry Kucera. </booktitle> <institution> Michigan Slavic Studies, </institution> <year> 1992. </year>
Reference: <author> D. Cutting, J. Kupiec, J. Pedersen and P. Sibun. </author> <year> 1992. </year> <title> A Practical Part-of-Speech Tagger. </title> <booktitle> In Procs. 3rd Conference on Applied Natural Language Processing , pp. </booktitle> <pages> 133-140, </pages> <booktitle> ACL, </booktitle> <year> 1992. </year>
Reference-contexts: 1 Introduction There are currently two main methods for automatic part-of-speech tagging. The prevailing one uses essentially statistical language models automatically derived from usually hand-annotated corpora. These corpus-based models can be represented e.g. as collocational matrices (Garside et al. (eds.) 1987; Church 1988), Hidden Markov models <ref> (cf. Cutting et al. 1992) </ref>, local rules (e.g. Hindle 1989) and neural networks (e.g. Schmid 1994). Taggers using these statistical language models are generally reported to assign the correct and unique tag to 95-97% of words in running text, using tag sets ranging from some dozens to about 130 tags.
Reference: <author> S. J. DeRose. </author> <year> 1988. </year> <title> "Grammatical Category Disambiguation by Statistical Optimization". </title> <booktitle> In Computational Linguistics 14(1), </booktitle> <pages> pp. 31-39, </pages> <booktitle> ACL, </booktitle> <year> 1988. </year>
Reference-contexts: Also EngCG's output was converted into this format to enable direct comparison with the statistical tagger. 3 The Statistical Tagger The statistical tagger used in the experiments is a classical trigram-based HMM decoder of the kind described in e.g. (Church 1988), <ref> (DeRose 1988) </ref> and numerous other articles.
Reference: <author> N. W. Francis and H. Kucera. </author> <year> 1982. </year> <title> Frequency Analysis of English Usage, </title> <publisher> Houghton Mif-flin, </publisher> <address> Boston, </address> <year> 1982. </year>
Reference-contexts: the last two contain rather rough heuristic constraints. 3 However, for an interesting experiment suggesting otherwise, see (Chanod and Tapanainen 1995). 2 Preparation of Corpus Resources 2.1 Annotation of training corpus The stochastic tagger was trained on a sample of 357,000 words from the Brown University Corpus of Present-Day English <ref> (Francis and Kucera 1982) </ref> that was annotated using the EngCG tags. The corpus was first analysed with the EngCG lexical anal-yser, and then it was fully disambiguated and, when necessary, corrected by a human expert. This annotation took place a few years ago. <p> By varying the threshold, we can perform a recall-precision, or error-rate-ambiguity, tradeoff. A similar strategy is adopted in (de Mar-cken 1990). 4 Experiments The statistical tagger was trained on 357,000 words from the Brown corpus <ref> (Francis and Kucera 1982) </ref>, reannotated using the EngCG annotation scheme (see above). In a first set of experiments, a 35,000 word subset of this corpus was set aside and used to evaluate the tagger's performance when trained on successively larger portions of the remaining 322,000 words.
Reference: <editor> R. Garside, G. Leech and G. Sampson (eds.). </editor> <year> 1987. </year> <title> The Computational Analysis of English. </title> <address> London and New York: </address> <publisher> Longman, </publisher> <year> 1987. </year>
Reference: <author> B. Greene and G. Rubin. </author> <year> 1971. </year> <title> Automatic grammatical tagging of English. </title> <institution> Brown University, Providence, </institution> <year> 1971. </year>
Reference-contexts: The less popular approach is based on hand-coded linguistic rules. Pioneering work was done in the 1960's <ref> (e.g. Greene and Rubin 1971) </ref>. Recently, new interest in the linguistic approach has been shown e.g. in the work of (Karlsson 1990; Voutilainen et al. 1992; Oflazer and Kuruoz 1994; Chanod and Tapanainen 1995; Karlsson et al. (eds.) 1995; Vouti-lainen 1995).
Reference: <author> D. Hindle. </author> <year> 1989. </year> <title> Acquiring disambiguation rules from text. </title> <booktitle> In Procs. 27th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 118-125, </pages> <booktitle> ACL, </booktitle> <year> 1989. </year>
Reference-contexts: The prevailing one uses essentially statistical language models automatically derived from usually hand-annotated corpora. These corpus-based models can be represented e.g. as collocational matrices (Garside et al. (eds.) 1987; Church 1988), Hidden Markov models (cf. Cutting et al. 1992), local rules <ref> (e.g. Hindle 1989) </ref> and neural networks (e.g. Schmid 1994). Taggers using these statistical language models are generally reported to assign the correct and unique tag to 95-97% of words in running text, using tag sets ranging from some dozens to about 130 tags.
Reference: <author> F. Jelinek and R. L. Mercer. </author> <year> 1980. </year> <title> "Interpolated Estimation of Markov Source Paramenters from Sparse Data". </title> <booktitle> Pattern Recognition in Practice: </booktitle> <pages> 381-397. </pages> <publisher> North Holland, </publisher> <year> 1980. </year>
Reference-contexts: for blending the distributions applies equally well to smoothing the transition probabilities p ij , i.e., the tag N-gram probabilities, and both the scheme and its application to these two tasks are described in detail in (Samuelsson 1996), where it was also shown to compare favourably to (deleted) interpolation, see <ref> (Jelinek and Mercer 1980) </ref>, even when the back-off weights of the latter were optimal. The ffi variables enable finding the most probable state sequence under the HMM, from which the most likely assignment of tags to words can be directly established.
Reference: <author> F. Karlsson. </author> <year> 1990. </year> <title> Constraint Grammar as a Framework for Parsing Running Text. </title> <booktitle> In Procs. CoLing'90. In Procs. 14th International Conference on Computational Linguistics, ICCL, </booktitle> <year> 1990. </year>
Reference: <editor> F. Karlsson, A. Voutilainen, J. Heikkila and A. Anttila (eds.). </editor> <year> 1995. </year> <title> Constraint Grammar. A Language-Independent System for Parsing Unrestricted Text. </title> <address> Berlin and New York: </address> <publisher> Mouton de Gruyter, </publisher> <year> 1995. </year>
Reference: <author> B. Krenn and C. Samuelsson. </author> <title> The Linguist's Guide to Statistics. </title> <note> Version of April 23, 1996. http://coli.uni-sb.de/~christer. </note>
Reference-contexts: Following conventional notation, e.g. (Rabiner 1989, pp. 272-274) and <ref> (Krenn and Samuelsson 1996, pp. 42-46) </ref>, the tagger recursively calculates the ff, fi, fl and ffi variables for each word string position t = 1; : : :; T and each possible state 4 s i : i = 1; : : : ; n: ff t (i) = P (W
Reference: <author> C. G. de Marcken. </author> <year> 1990. </year> <title> "Parsing the LOB Corpus". </title> <booktitle> In Procs. 28th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 243-251, </pages> <booktitle> ACL, </booktitle> <year> 1990. </year>
Reference-contexts: Of course, the most probable tag is never discarded, even if its probability happens to be less than the threshold value. By varying the threshold, we can perform a recall-precision, or error-rate-ambiguity, tradeoff. A similar strategy is adopted in <ref> (de Mar-cken 1990) </ref>. 4 Experiments The statistical tagger was trained on 357,000 words from the Brown corpus (Francis and Kucera 1982), reannotated using the EngCG annotation scheme (see above).
Reference: <author> K. Oflazer and I. Kuruoz. </author> <year> 1994. </year> <title> Tagging and morphological disambiguation of Turkish text. </title> <booktitle> In Procs. 4th Conference on Applied Natural Language Processing , ACL, </booktitle> <year> 1994. </year>
Reference: <author> L. R. Rabiner. </author> <year> 1989. </year> <title> "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition". </title> <booktitle> In Readings in Speech Recognition, </booktitle> <pages> pp. 267-296. </pages> <editor> Alex Waibel and Kai-Fu Lee (eds), </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Following conventional notation, e.g. <ref> (Rabiner 1989, pp. 272-274) </ref> and (Krenn and Samuelsson 1996, pp. 42-46), the tagger recursively calculates the ff, fi, fl and ffi variables for each word string position t = 1; : : :; T and each possible state 4 s i : i = 1; : : : ; n: ff
Reference: <author> G. Sampson. </author> <year> 1995. </year> <title> English for the Computer, </title> <publisher> Oxford University Press, </publisher> <year> 1995. </year>
Reference: <author> C. Samuelsson. </author> <year> 1996. </year> <title> "Handling Sparse Data by Successive Abstraction". </title> <booktitle> In Procs. 16th International Conference on Computational Linguistics, </booktitle> <pages> pp. 895-900, ICCL, </pages> <year> 1996. </year>
Reference-contexts: Following conventional notation, e.g. (Rabiner 1989, pp. 272-274) and <ref> (Krenn and Samuelsson 1996, pp. 42-46) </ref>, the tagger recursively calculates the ff, fi, fl and ffi variables for each word string position t = 1; : : :; T and each possible state 4 s i : i = 1; : : : ; n: ff t (i) = P (W <p> The method for blending the distributions applies equally well to smoothing the transition probabilities p ij , i.e., the tag N-gram probabilities, and both the scheme and its application to these two tasks are described in detail in <ref> (Samuelsson 1996) </ref>, where it was also shown to compare favourably to (deleted) interpolation, see (Jelinek and Mercer 1980), even when the back-off weights of the latter were optimal.
Reference: <author> H. Schmid. </author> <year> 1994. </year> <title> Part-of-speech tagging with neural networks. </title> <booktitle> In Procs. 15th International Conference on Computational Linguistics, </booktitle> <pages> pp. 172-176, ICCL, </pages> <year> 1994. </year>
Reference-contexts: The prevailing one uses essentially statistical language models automatically derived from usually hand-annotated corpora. These corpus-based models can be represented e.g. as collocational matrices (Garside et al. (eds.) 1987; Church 1988), Hidden Markov models (cf. Cutting et al. 1992), local rules (e.g. Hindle 1989) and neural networks <ref> (e.g. Schmid 1994) </ref>. Taggers using these statistical language models are generally reported to assign the correct and unique tag to 95-97% of words in running text, using tag sets ranging from some dozens to about 130 tags. The less popular approach is based on hand-coded linguistic rules.
Reference: <author> P. Tapanainen. </author> <year> 1996. </year> <title> The Constraint Grammar Parser CG-2. </title> <type> Publ. 27, </type> <institution> Dept. General Linguistics, University of Helsinki, </institution> <year> 1996. </year>
Reference-contexts: Fred Karlsson proposed the Constraint Grammar framework in the late 1980s. Juha Heikkila and Timo Jarvinen contributed with their work on English morphology and lexicon. Kimmo Koskenniemi wrote the software for morphological analysis. Pasi Tapanainen has written various implementations of the CG parser, including the recent CG-2 parser <ref> (Tapanainen 1996) </ref>. The quality of the investigation and presentation was boosted by a number of suggestions to improvements and (often sceptical) comments from numerous ACL reviewers and UPenn associates, in particular from Mark Liberman.
Reference: <author> P. Tapanainen and A. Voutilainen. </author> <year> 1994. </year> <title> Tagging accurately don't guess if you know. </title> <booktitle> In Procs. 4th Conference on Applied Natural Language Processing, ACL, </booktitle> <year> 1994. </year>
Reference: <author> A. Voutilainen. </author> <year> 1995. </year> <title> "A syntax-based part of speech analyser". </title> <booktitle> In Procs. 7th Conference of the European Chapter of the Association for Computational Linguistics, </booktitle> <pages> pp. 157-164, </pages> <booktitle> ACL, </booktitle> <year> 1995. </year>
Reference-contexts: Secondly, the adopted analysis of most of the constructions where humans tend to be uncertain is documented as a collection of tag application principles in the form of a grammarian's manual <ref> (for further details, cf. Voutilainen and Jarvinen 1995) </ref>. The corpus-annotation procedure allows us to perform a text-book statistical hypothesis test. Let the null hypothesis be that any two human evaluators will necessarily disagree in at least 3% of the cases.
Reference: <author> A. Voutilainen and J. Heikkila. </author> <year> 1994. </year> <title> An English constraint grammar (EngCG): a surface-syntactic parser of English. </title> <editor> In Fries, Tottie and Schneider (eds.), </editor> <title> Creating and using English language corpora, </title> <address> Rodopi, </address> <year> 1994. </year>
Reference: <author> A. Voutilainen, J. Heikkila and A. Anttila. </author> <year> 1992. </year> <title> Constraint Grammar of English. A Performance-Oriented Introduction. </title> <type> Publ. 21, </type> <institution> Dept. General Linguistics, University of Helsinki, </institution> <year> 1992. </year>
Reference: <author> A. Voutilainen and T. Jarvinen. </author> <title> "Specifying a shallow grammatical representation for parsing purposes". </title> <booktitle> In Procs. 7th Conference of the Euro-pean Chapter of the Association for Computational Linguistics, </booktitle> <pages> pp. 210-214, </pages> <booktitle> ACL, </booktitle> <year> 1995. </year>
Reference-contexts: Secondly, the adopted analysis of most of the constructions where humans tend to be uncertain is documented as a collection of tag application principles in the form of a grammarian's manual <ref> (for further details, cf. Voutilainen and Jarvinen 1995) </ref>. The corpus-annotation procedure allows us to perform a text-book statistical hypothesis test. Let the null hypothesis be that any two human evaluators will necessarily disagree in at least 3% of the cases.
References-found: 25

