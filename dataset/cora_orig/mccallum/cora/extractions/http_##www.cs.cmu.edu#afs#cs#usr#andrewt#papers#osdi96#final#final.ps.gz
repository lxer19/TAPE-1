URL: http://www.cs.cmu.edu/afs/cs/usr/andrewt/papers/osdi96/final/final.ps.gz
Refering-URL: http://www.cs.cmu.edu/~andrewt/papers.html
Root-URL: 
Title: A Trace-Driven Comparison of Algorithms for Parallel Prefetching and Caching  
Author: Tracy Kimbrel Andrew Tomkins R. Hugo Patterson Brian Bershad Pei Cao Edward W. Felten Garth A. Gibson Anna R. Karlin Kai Li 
Abstract: High-performance I/O systems depend on prefetching and caching in order to deliver good performance to applications. These two techniques have generally been considered in isolation, even though there are significant interactions between them; a block prefetched too early reduces the effectiveness of the cache, while a block cached too long reduces the effectiveness of prefetching. In this paper we study the effects of several combined prefetching and caching strategies for systems with multiple disks. Using disk-accurate trace-driven simulation, we explore the performance characteristics of each of the algorithms in cases in which applications provide full advance knowledge of accesses using hints. Some of the strategies have been published with theoretical performance bounds, and some are components of systems that have been built. One is a new algorithm that combines the desirable characteristics of the others. We find that when performance is limited by I/O stalls, aggressive prefetching helps to alleviate the problem; that more conservative prefetching is appropriate when significant I/O stalls are not present; and that a single, simple strategy is capable of doing both. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acharya, M. Uysal, R. Bennett, A. Mendelson, M. Beynon, J. Hollingsworth, J. Saltz and A. Sussman. </author> <title> Tuning the Performance of I/O-Intensive Parallel Applications. </title> <booktitle> Proceedings of the Fourth Annual Workshop on I/O in Parallel and Distributed Systems, </booktitle> <pages> pages 15-27, </pages> <month> May, </month> <year> 1996. </year>
Reference-contexts: Write performance is less important as write behind strategies can mask update latency. Read-intensive applications that stall for I/O a significant fraction of their running time include text search, 3D scientific visualization, relational database queries, multimedia servers and object code linkers. Many of these applications have predictable access patterns <ref> [25, 1, 18] </ref>. The ability to provide the file system with hints about future references has motivated research into the design of policies that use this information to reduce I/O overhead [25, 26, 7, 6].
Reference: [2] <author> L.A. Belady. </author> <title> A Study of Replacement Algorithms for Virtual Storage Computers. </title> <journal> IBM Systems Journal, </journal> <volume> 5(2) </volume> <pages> 78-101, </pages> <year> 1966. </year>
Reference-contexts: Fixed horizon, aggressive and forestall can all be adapted to deal with these more general situations [5, 26]. 1.5 Related work Caching and prefetching have been known techniques to improve storage hierarchies for many years <ref> [2, 12] </ref>. In architectures, the work on caching and prefetching has focused on bridging the performance gap between CPU and main memory [29]. Research using caching and prefetching in database systems [9, 23, 10] showed that it is important to use applications' knowledge to perform caching and prefetching.
Reference: [3] <author> Jim Gray. </author> <title> The Benchmark Handbook. </title> <address> Morgan-Kaufman, San Mateo, CA. </address> <year> 1991. </year>
Reference-contexts: The relations are those used in the Wisconsin Benchmark <ref> [3] </ref>. Since the result relation is small, most of the file accesses are reads. <p> The selection query is part of the Wisconsin Benchmark suite <ref> [3] </ref> and uses indexed search. ld: the Ultrix link-editor, building the Ultrix 4.3 kernel from about 25MB of object files. xds: a 3-D data visualization program, XDataSlice, generating 25 planar slice images at random orientations from a 64MB data file.
Reference: [4] <author> Pei Cao, Edward Felten, and Kai Li. </author> <title> Application-Controlled File Caching Policies. </title> <booktitle> In USENIX Summer 1994 Technical Conference, </booktitle> <pages> pages 171-182, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Research using caching and prefetching in database systems [9, 23, 10] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [12, 20, 14, 22, 30, 4, 13, 7, 26] </ref>. The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files.
Reference: [5] <author> Pei Cao, Edward W. Felten, Anna Karlin, and Kai Li. </author> <title> Implementation and Performance of Integrated Application-Controlled Caching, Prefetching and Disk Scheduling. </title> <type> Technical Report TR-CS95-493, </type> <institution> Princeton University, </institution> <year> 1995. </year>
Reference-contexts: Clearly, prefetch-ing and caching algorithms must deal effectively with missing or incorrect hints, as well as multiple simultaneously executing processes. Fixed horizon, aggressive and forestall can all be adapted to deal with these more general situations <ref> [5, 26] </ref>. 1.5 Related work Caching and prefetching have been known techniques to improve storage hierarchies for many years [2, 12]. In architectures, the work on caching and prefetching has focused on bridging the performance gap between CPU and main memory [29]. <p> Recently, caching and prefetching have also been studied for parallel file systems [11, 18, 25]. Although much work has been done in file caching and prefetching, most of it has considered one or the other in isolation. Recent studies for the single disk case showed <ref> [6, 5] </ref> that it is important to integrate prefetching, caching and disk scheduling together and that a properly integrated strategy can perform much better than a naive strategy. For the multi-disk case, a theoretical study [16] presented and analyzed aggressive and reverse aggressive.
Reference: [6] <author> Pei Cao, Edward W. Felten, Anna Karlin, and Kai Li. </author> <title> A study of Integrated Prefetching and Caching Strategies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> pages , May </month> <year> 1995. </year>
Reference-contexts: Many of these applications have predictable access patterns [25, 1, 18]. The ability to provide the file system with hints about future references has motivated research into the design of policies that use this information to reduce I/O overhead <ref> [25, 26, 7, 6] </ref>. The two key techniques that are enabled by detailed information about future accesses are deep prefetching and better-than-LRU cache replacement. This paper explores the tradeoff between aggressive prefetching and optimal cache replacement. <p> Recently, caching and prefetching have also been studied for parallel file systems [11, 18, 25]. Although much work has been done in file caching and prefetching, most of it has considered one or the other in isolation. Recent studies for the single disk case showed <ref> [6, 5] </ref> that it is important to integrate prefetching, caching and disk scheduling together and that a properly integrated strategy can perform much better than a naive strategy. For the multi-disk case, a theoretical study [16] presented and analyzed aggressive and reverse aggressive. <p> Our simulations use actual CPU times collected in our traces and an accurate simulation model of modern disk drives, and charge a driver overhead for each request made to a disk. 2.2 Optimal prefetching rules The following simple rules can be assumed of any optimal strategy in the single-disk case <ref> [6] </ref>. 1 * Optimal fetching: when fetching, always fetch the miss ing block that will be referenced soonest; * Optimal replacement: when fetching, always evict the block in the cache whose next reference is furthest in the future; * Do no harm: never evict block A to fetch block B when <p> sequence tend to be separated by many intervening references to blocks that are present in the cache, we'd expect fixed horizon to have performance much closer to optimal than its worst case. 2.4 The aggressive algorithm The (multi-disk) aggressive algorithm is based on the Cao et al. (single-disk) aggressive algorithm <ref> [6] </ref>, which is provably near-optimal in the single-disk case. (Multi-disk) aggressive: Whenever a disk is free, prefetch the first missing block on that disk, replacing the block whose next reference is furthest in the future, under the condition that the next access to the evicted block is after the next access
Reference: [7] <author> Pei Cao, Edward W. Felten, and Kai Li. </author> <title> Implementation and Performance of Application-Controlled File Caching. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 165-178, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Many of these applications have predictable access patterns [25, 1, 18]. The ability to provide the file system with hints about future references has motivated research into the design of policies that use this information to reduce I/O overhead <ref> [25, 26, 7, 6] </ref>. The two key techniques that are enabled by detailed information about future accesses are deep prefetching and better-than-LRU cache replacement. This paper explores the tradeoff between aggressive prefetching and optimal cache replacement. <p> Research using caching and prefetching in database systems [9, 23, 10] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [12, 20, 14, 22, 30, 4, 13, 7, 26] </ref>. The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files.
Reference: [8] <author> P.M. Chen and D.A. Patterson. </author> <title> Maximizing Performance in a Striped Disk Array. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 322-331, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This simulation models fine architectural details to provide a very accurate simulation of the HP 97560 disk drive. Table 1 lists several characteristics of the HP 97560 (taken from [27]). The CMU simulator uses the Berkeley RaidSim <ref> [8] </ref> simulator, as modified at CMU, to simulate 0661 IBM Lightning disk drives. The simulators were cross-validated on a common set of traces. The CMU simulator does not implement reverse aggressive. We obtained good agreement between the simulators on the results for aggressive and fixed horizon for several traces.
Reference: [9] <author> H.T. Chou and D.J. DeWitt. </author> <title> An Evaluation of Buffer Management Strategies for Relational Database Systems. </title> <booktitle> In Proceedings of the 19th International Conference on Very Large Data Bases, </booktitle> <pages> pages 127-141, </pages> <address> Dublin, Ireland, </address> <year> 1993. </year>
Reference-contexts: In architectures, the work on caching and prefetching has focused on bridging the performance gap between CPU and main memory [29]. Research using caching and prefetching in database systems <ref> [9, 23, 10] </ref> showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems [12, 20, 14, 22, 30, 4, 13, 7, 26].
Reference: [10] <author> Kenneth M. Curewitz, P. Krishnan, and Jeffrey S. Vitter. </author> <title> Practical Prefetching via Data Compression. </title> <booktitle> In Proceedings of the 1993 ACM Conference on Management of Data (SIG-MOD), </booktitle> <pages> pages 257-266, </pages> <address> Washington, DC, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In architectures, the work on caching and prefetching has focused on bridging the performance gap between CPU and main memory [29]. Research using caching and prefetching in database systems <ref> [9, 23, 10] </ref> showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems [12, 20, 14, 22, 30, 4, 13, 7, 26]. <p> The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files. Another large body of work has been on predicting future access patterns <ref> [11, 30, 23, 10, 13] </ref>. Recently, caching and prefetching have also been studied for parallel file systems [11, 18, 25]. Although much work has been done in file caching and prefetching, most of it has considered one or the other in isolation.
Reference: [11] <author> Carla Schlatter Ellis and David Kotz. </author> <title> Prefetching in File System for MIMD Multiprocessors. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages 306-314, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files. Another large body of work has been on predicting future access patterns <ref> [11, 30, 23, 10, 13] </ref>. Recently, caching and prefetching have also been studied for parallel file systems [11, 18, 25]. Although much work has been done in file caching and prefetching, most of it has considered one or the other in isolation. <p> The limitation of this approach is that it only benefits applications that make sequential references to large files. Another large body of work has been on predicting future access patterns [11, 30, 23, 10, 13]. Recently, caching and prefetching have also been studied for parallel file systems <ref> [11, 18, 25] </ref>. Although much work has been done in file caching and prefetching, most of it has considered one or the other in isolation.
Reference: [12] <editor> R.J. Feiertag and E.I. Organisk. </editor> <booktitle> The Multics Input/Ouput System. In Proceedings of the 3rd Symposium on Operating Systems Principles, </booktitle> <pages> pages 35-41, </pages> <year> 1971. </year>
Reference-contexts: Fixed horizon, aggressive and forestall can all be adapted to deal with these more general situations [5, 26]. 1.5 Related work Caching and prefetching have been known techniques to improve storage hierarchies for many years <ref> [2, 12] </ref>. In architectures, the work on caching and prefetching has focused on bridging the performance gap between CPU and main memory [29]. Research using caching and prefetching in database systems [9, 23, 10] showed that it is important to use applications' knowledge to perform caching and prefetching. <p> Research using caching and prefetching in database systems [9, 23, 10] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [12, 20, 14, 22, 30, 4, 13, 7, 26] </ref>. The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files. <p> File caching and prefetching have become standard techniques for sequential file systems [12, 20, 14, 22, 30, 4, 13, 7, 26]. The most common prefetching approach is to perform sequential readahead <ref> [12, 20, 21] </ref>. The limitation of this approach is that it only benefits applications that make sequential references to large files. Another large body of work has been on predicting future access patterns [11, 30, 23, 10, 13].
Reference: [13] <author> Jim Griffioen and Randy Appleton. </author> <title> Reducing File System Latency using a Predictive Approach. </title> <booktitle> In USENIX Summer 1994 Technical Conference, </booktitle> <pages> pages 197-208, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Research using caching and prefetching in database systems [9, 23, 10] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [12, 20, 14, 22, 30, 4, 13, 7, 26] </ref>. The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files. <p> The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files. Another large body of work has been on predicting future access patterns <ref> [11, 30, 23, 10, 13] </ref>. Recently, caching and prefetching have also been studied for parallel file systems [11, 18, 25]. Although much work has been done in file caching and prefetching, most of it has considered one or the other in isolation.
Reference: [14] <author> John H. Howard, Michael Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Research using caching and prefetching in database systems [9, 23, 10] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [12, 20, 14, 22, 30, 4, 13, 7, 26] </ref>. The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files.
Reference: [15] <author> M. Kim. </author> <title> Synchronized Disk Interleaving. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 35(11) </volume> <pages> 978-988, </pages> <year> 1986. </year>
Reference-contexts: As a result, parallel disk arrays have become an attractive means for achieving high performance from storage devices at low cost <ref> [15, 28, 24] </ref>. Independently accessible multiple disks offer the advantage of both increased bandwidth and reduced contention on individual disk arms. However, many applications do not benefit from this parallelism because their I/O accesses are serial. This problem is particularly severe for read-intensive applications.
Reference: [16] <author> Tracy Kimbrel and Anna R. Karlin. </author> <title> Near-optimal Parallel Prefetching and Caching. </title> <booktitle> In Proceedings of the 1996 IEEE Symposium on Foundations of Computer Science, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Reverse aggressive goes beyond aggressive's use of future knowledge by attempting to balance disk workload through carefully selected replacement decisions. Previously, it was shown theoretically that on any access pattern known in advance, reverse aggressive's elapsed time is close to optimal <ref> [16] </ref>. It is the only one of these algorithms with this theoretical performance guarantee. However, it is not a practical algorithm. First, it is much more complex than the other algorithms, and second, its decisions depend on information farther in the future than the other algorithms. <p> Recent studies for the single disk case showed [6, 5] that it is important to integrate prefetching, caching and disk scheduling together and that a properly integrated strategy can perform much better than a naive strategy. For the multi-disk case, a theoretical study <ref> [16] </ref> presented and analyzed aggressive and reverse aggressive. Other parallel prefetching strategies include one stripe lookahead prefetching on RAID arrays, and Patterson et al. [26]'s TIP2 system. The one stripe lookahead benefits only applications that use large files, and would perform little prefetching for other applications. <p> Aggressive is the most aggressive prefetching strategy that is consistent with the four optimal prefetching rules described in section 2.2. As mentioned, some of these rules are no longer valid in the multiple disk case. This provides some of the intuition for the following theorem. Theorem 1 <ref> [16] </ref> For any access pattern, and any layout of data on disks, the elapsed time of aggressive is at most d (1+* 1 ) times that of the optimal elapsed time (the minimum possible), where d is the number of disks, and * 1 is a small constant that depends on <p> For a proof of correctness, more details on how and why this algorithm works well, and a proof of the following theorem, see <ref> [16] </ref>. Theorem 2 [16] For any request sequence, and for any layout of the data on the disks, the elapsed time of reverse aggressive is at most 1 + * 2 times the optimal elapsed time. 3 There are two key properties of reverse aggressive that result in this theorem. <p> For a proof of correctness, more details on how and why this algorithm works well, and a proof of the following theorem, see <ref> [16] </ref>. Theorem 2 [16] For any request sequence, and for any layout of the data on the disks, the elapsed time of reverse aggressive is at most 1 + * 2 times the optimal elapsed time. 3 There are two key properties of reverse aggressive that result in this theorem.
Reference: [17] <author> Tracy Kimbrel, Andrew Tomkins, R. Hugo Patterson, Brian Bershad, Pei Cao, Edward W. Felten, Garth A. Gibson, Anna R. Karlin, and Kai Li. </author> <title> A Trace-Driven Comparison of Algorithms for Parallel Prefetching and Caching. </title> <type> Technical Report UW-CSE-96-09-01, </type> <institution> University of Washington, </institution> <year> 1996. </year>
Reference-contexts: This effect is noticable, but less pronounced, on the synth trace as well. On the remaining traces, reverse aggressive's elapsed time varies from 3.6% worse to 10.7% better than the superior of fixed horizon and aggressive in any given configuration. For the full data, see <ref> [17] </ref>. disks demand fixed aggressive reverse fetching horizon aggressive 1 .82 .98 .99 .98 3 .27 .82 .87 .85 5 .16 .66 .70 .69 7 .12 .50 .62 .50 10 .08 .36 .43 .35 16 .05 .22 .26 .21 Table 4: Disk utilization on the postgres-select trace. <p> In this section, we explore the behavior of the algorithms when some of these parameters are varied. For lack of space, we present general observations and only a small portion of the data. For the full data, see <ref> [17] </ref>. We have already described most of the primary effects that explain what we see. These are: * scheduling: an increase in the number of outstanding fetches issued by a prefetching algorithm results in increased latitude to reorder fetches and thus reduced disk response times. <p> Once again, forestall has the best performance of the three practical algorithms. On all remaining traces, over all configurations, forestall's performance was between 2% worse and 5.8% better than the best of aggressive and fixed horizon in that configuration. For the full data, see <ref> [17] </ref>. Table 8 shows the utilization of the disks by forestall on the postgres-select trace. Its utilization falls between those of aggressive and fixed horizon, as expected.
Reference: [18] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Practical Prefetching Techniques for Multiprocessor File Systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1) </volume> <pages> 33-51, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Write performance is less important as write behind strategies can mask update latency. Read-intensive applications that stall for I/O a significant fraction of their running time include text search, 3D scientific visualization, relational database queries, multimedia servers and object code linkers. Many of these applications have predictable access patterns <ref> [25, 1, 18] </ref>. The ability to provide the file system with hints about future references has motivated research into the design of policies that use this information to reduce I/O overhead [25, 26, 7, 6]. <p> The limitation of this approach is that it only benefits applications that make sequential references to large files. Another large body of work has been on predicting future access patterns [11, 30, 23, 10, 13]. Recently, caching and prefetching have also been studied for parallel file systems <ref> [11, 18, 25] </ref>. Although much work has been done in file caching and prefetching, most of it has considered one or the other in isolation.
Reference: [19] <author> David Kotz, Song Bac Toh, and Sriram Radhakrishnan. </author> <title> A Detailed Simulation Model of the HP 97560 Disk Drive. </title> <type> Technical Report PCS-TR94-220, </type> <institution> Department of Computer Science, Datmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: However, our simulators do not model serialization of DMA transactions. 6 Two separate simulators were developed, one at Wash-ington (UW) and one at Carnegie Mellon (CMU). The UW simulator uses the disk drive simulation of Kotz et al. <ref> [19] </ref> (which is based on that of Ruemmler and Wilkes [27]) to accurately model I/O costs. This simulation models fine architectural details to provide a very accurate simulation of the HP 97560 disk drive. Table 1 lists several characteristics of the HP 97560 (taken from [27]).
Reference: [20] <author> Marshall K. McKusick, William N. Joy, Samuel J. Le*er, and Robert S. Fabry. </author> <title> A Fast File System for UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 181-197, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: Research using caching and prefetching in database systems [9, 23, 10] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [12, 20, 14, 22, 30, 4, 13, 7, 26] </ref>. The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files. <p> File caching and prefetching have become standard techniques for sequential file systems [12, 20, 14, 22, 30, 4, 13, 7, 26]. The most common prefetching approach is to perform sequential readahead <ref> [12, 20, 21] </ref>. The limitation of this approach is that it only benefits applications that make sequential references to large files. Another large body of work has been on predicting future access patterns [11, 30, 23, 10, 13].
Reference: [21] <author> L. W. McVoy and S. R. Kleiman. </author> <title> Extent-like Performance from a UNIX File System. </title> <booktitle> In Proceedings of the 1991 Winter USENIX Conference, </booktitle> <pages> pages 33-43, </pages> <year> 1991. </year>
Reference-contexts: File caching and prefetching have become standard techniques for sequential file systems [12, 20, 14, 22, 30, 4, 13, 7, 26]. The most common prefetching approach is to perform sequential readahead <ref> [12, 20, 21] </ref>. The limitation of this approach is that it only benefits applications that make sequential references to large files. Another large body of work has been on predicting future access patterns [11, 30, 23, 10, 13].
Reference: [22] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout. </author> <title> Caching in the Sprite File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Research using caching and prefetching in database systems [9, 23, 10] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [12, 20, 14, 22, 30, 4, 13, 7, 26] </ref>. The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files.
Reference: [23] <author> Mark Palmer and Stanley B. Zdonik. </author> <title> Fido: A Cache That Learns to Fetch. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Data Bases, </booktitle> <pages> pages 255-264, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: In architectures, the work on caching and prefetching has focused on bridging the performance gap between CPU and main memory [29]. Research using caching and prefetching in database systems <ref> [9, 23, 10] </ref> showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems [12, 20, 14, 22, 30, 4, 13, 7, 26]. <p> The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files. Another large body of work has been on predicting future access patterns <ref> [11, 30, 23, 10, 13] </ref>. Recently, caching and prefetching have also been studied for parallel file systems [11, 18, 25]. Although much work has been done in file caching and prefetching, most of it has considered one or the other in isolation.
Reference: [24] <author> D.A. Patterson, G. Gibson, and R.H. Katz. </author> <title> A Case for Redundant Arrays for Inexpensive Disks (RAID). </title> <booktitle> In Proceedings of ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: As a result, parallel disk arrays have become an attractive means for achieving high performance from storage devices at low cost <ref> [15, 28, 24] </ref>. Independently accessible multiple disks offer the advantage of both increased bandwidth and reduced contention on individual disk arms. However, many applications do not benefit from this parallelism because their I/O accesses are serial. This problem is particularly severe for read-intensive applications.
Reference: [25] <author> R. Hugo Patterson and Garth A. Gibson. </author> <title> Exposing I/O Con-currency with Informed Prefetching. </title> <booktitle> In Proceedings of the Third International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 7-16, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Write performance is less important as write behind strategies can mask update latency. Read-intensive applications that stall for I/O a significant fraction of their running time include text search, 3D scientific visualization, relational database queries, multimedia servers and object code linkers. Many of these applications have predictable access patterns <ref> [25, 1, 18] </ref>. The ability to provide the file system with hints about future references has motivated research into the design of policies that use this information to reduce I/O overhead [25, 26, 7, 6]. <p> Many of these applications have predictable access patterns [25, 1, 18]. The ability to provide the file system with hints about future references has motivated research into the design of policies that use this information to reduce I/O overhead <ref> [25, 26, 7, 6] </ref>. The two key techniques that are enabled by detailed information about future accesses are deep prefetching and better-than-LRU cache replacement. This paper explores the tradeoff between aggressive prefetching and optimal cache replacement. <p> The limitation of this approach is that it only benefits applications that make sequential references to large files. Another large body of work has been on predicting future access patterns [11, 30, 23, 10, 13]. Recently, caching and prefetching have also been studied for parallel file systems <ref> [11, 18, 25] </ref>. Although much work has been done in file caching and prefetching, most of it has considered one or the other in isolation.
Reference: [26] <author> R.H. Patterson, G.A. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. </author> <title> Informed Prefetching and Caching. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-95, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Many of these applications have predictable access patterns [25, 1, 18]. The ability to provide the file system with hints about future references has motivated research into the design of policies that use this information to reduce I/O overhead <ref> [25, 26, 7, 6] </ref>. The two key techniques that are enabled by detailed information about future accesses are deep prefetching and better-than-LRU cache replacement. This paper explores the tradeoff between aggressive prefetching and optimal cache replacement. <p> These are the factors that are addressed by the reverse aggressive algorithm. 1.3 Comparing approaches The fixed horizon algorithm is based on the second Informed Prefetching (TIP2) system of Patterson, Gibson et al. <ref> [26] </ref>, which manages allocation of cache space and I/O bandwidth between multiple processes, only some of which are disclosing some or all of their future accesses. TIP2 is designed for the case in which sufficient I/O bandwidth exists to service the request stream without stalling on I/O. <p> Clearly, prefetch-ing and caching algorithms must deal effectively with missing or incorrect hints, as well as multiple simultaneously executing processes. Fixed horizon, aggressive and forestall can all be adapted to deal with these more general situations <ref> [5, 26] </ref>. 1.5 Related work Caching and prefetching have been known techniques to improve storage hierarchies for many years [2, 12]. In architectures, the work on caching and prefetching has focused on bridging the performance gap between CPU and main memory [29]. <p> Research using caching and prefetching in database systems [9, 23, 10] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [12, 20, 14, 22, 30, 4, 13, 7, 26] </ref>. The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files. <p> parallel prefetching strategies include one stripe lookahead prefetching on RAID arrays, and Patterson et al. <ref> [26] </ref>'s TIP2 system. The one stripe lookahead benefits only applications that use large files, and would perform little prefetching for other applications. TIP2 uses the fixed horizon algorithm we have studied here. Patterson et al. [26] also present a cost-benefit technique for controlling buffer allocations for both hinting and non-hinting applications in a multi-process environment. Previous studies of the algorithms considered here have been incomparable. <p> It may be necessary to violate all of the rules except first opportunity to produce an optimal schedule. 2.3 The fixed horizon algorithm As described earlier, the fixed horizon algorithm is based on the TIP2 system running a single hinting process <ref> [26] </ref>.
Reference: [27] <author> Chris Ruemmler and John Wilkes. </author> <title> An Introduction to Disk Drive Modelling. </title> <journal> In IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: However, our simulators do not model serialization of DMA transactions. 6 Two separate simulators were developed, one at Wash-ington (UW) and one at Carnegie Mellon (CMU). The UW simulator uses the disk drive simulation of Kotz et al. [19] (which is based on that of Ruemmler and Wilkes <ref> [27] </ref>) to accurately model I/O costs. This simulation models fine architectural details to provide a very accurate simulation of the HP 97560 disk drive. Table 1 lists several characteristics of the HP 97560 (taken from [27]). <p> simulation of Kotz et al. [19] (which is based on that of Ruemmler and Wilkes <ref> [27] </ref>) to accurately model I/O costs. This simulation models fine architectural details to provide a very accurate simulation of the HP 97560 disk drive. Table 1 lists several characteristics of the HP 97560 (taken from [27]). The CMU simulator uses the Berkeley RaidSim [8] simulator, as modified at CMU, to simulate 0661 IBM Lightning disk drives. The simulators were cross-validated on a common set of traces. The CMU simulator does not implement reverse aggressive.
Reference: [28] <author> K. Salem and H. Garcia-Molina. </author> <title> Disk Striping. </title> <booktitle> In the 2nd IEEE Conference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <month> Feb. </month> <year> 1986. </year>
Reference-contexts: As a result, parallel disk arrays have become an attractive means for achieving high performance from storage devices at low cost <ref> [15, 28, 24] </ref>. Independently accessible multiple disks offer the advantage of both increased bandwidth and reduced contention on individual disk arms. However, many applications do not benefit from this parallelism because their I/O accesses are serial. This problem is particularly severe for read-intensive applications.
Reference: [29] <author> Alan J. Smith. </author> <title> Second Bibliography on Cache Memories. </title> <journal> Computer Architecture News, </journal> <volume> 19(4) </volume> <pages> 154-182, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In architectures, the work on caching and prefetching has focused on bridging the performance gap between CPU and main memory <ref> [29] </ref>. Research using caching and prefetching in database systems [9, 23, 10] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems [12, 20, 14, 22, 30, 4, 13, 7, 26].
Reference: [30] <author> C. Tait and D. Duchamp. </author> <title> Service Interface and Replica Management Algorithm for Mobile File System Clients. </title> <booktitle> In Proceedings of Parallel and Distributed Information Systems, </booktitle> <pages> pages 190-196. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: Research using caching and prefetching in database systems [9, 23, 10] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [12, 20, 14, 22, 30, 4, 13, 7, 26] </ref>. The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files. <p> The most common prefetching approach is to perform sequential readahead [12, 20, 21]. The limitation of this approach is that it only benefits applications that make sequential references to large files. Another large body of work has been on predicting future access patterns <ref> [11, 30, 23, 10, 13] </ref>. Recently, caching and prefetching have also been studied for parallel file systems [11, 18, 25]. Although much work has been done in file caching and prefetching, most of it has considered one or the other in isolation.
References-found: 30

