URL: ftp://cag.lcs.mit.edu/pub/dm/papers/ousterhout:bsd.ps.gz
Refering-URL: http://www.pdos.lcs.mit.edu/~dm/
Root-URL: 
Title: A Trace-Driven Analysis of the UNIX 4.2 BSD File System  
Author: John K. Ousterhout, Herve Da Costa, David Harrison, John A. Kunze, Mike Kupfer, and James G. Thompson 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division Electrical Engineering and Computer Sciences University of California  
Abstract: We analyzed the UNIX 4.2 BSD file system by recording user-level activity in trace files and writing programs to analyze the traces. The tracer did not record individual read and write operations, yet still provided tight bounds on what information was accessed and when. The trace analysis shows that the average file system bandwidth needed per user is low (a few hundred bytes per second). Most of the files accessed are open only a short time and are accessed sequentially. Most new information is deleted or overwritten within a few minutes of its creation. We also wrote a simulator that uses the traces to predict the performance of caches for disk blocks. The moderate-sized caches used in UNIX reduce disk traffic for file blocks by about 50%, but larger caches (several megabytes) can eliminate 90% or more of all disk traffic. With those large caches, large block sizes (16 kbytes or more) result in the fewest disk accesses. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Feder, J. </author> <title> ``The Evolution of UNIX System Performance.'' </title> <journal> Bell Laboratories Technical Journal, </journal> <volume> Vol. 63, No. 8, </volume> <month> October </month> <year> 1984, </year> <pages> pp. 1791-1814. </pages>
Reference-contexts: Block Size We also evaluated the effectiveness of different block sizes. The original UNIX system used 512-byte blocks, but the block size has grown since then to 1024 bytes in AT&T's System V <ref> [1] </ref> and 4096 bytes in most 4.2 BSD systems. Figure 6 and Table VII show the results of varying the block size and cache size.
Reference: [2] <author> Lazowska, E.D. et al. </author> <title> File Access Performance of Disk-less Workstations. </title> <type> Technical Report 84-06-01, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> June </month> <year> 1984. </year>
Reference-contexts: Two other recent studies contain UNIX measurements that partially overlap ours: Lazowska et al. analyzed block size tradeoffs and reported on the disk I/O required per user <ref> [2] </ref>, and Leffler et al. reported on the effectiveness of current UNIX disk caches [4]. Sections 5 and 6 of this paper compare their results and ours. 3. Gathering the Data Our main concern in gathering file system trace information was the volume of data. <p> The first ``other factor'' is paging activity, which consists primarily of loading programs on demand from disk files into main memory. Paging to and from swapping store can also result in I/O activity but is rare in 4.2 BSD systems (see <ref> [2] </ref> and [6]). We estimated the effects of paging by logging execve system calls and recording the sizes of the files that were executed. <p> If only ten second intervals are considered, users active in these intervals tend to have much higher transfer rates (a few kilobytes per second per user) but there are fewer active users. In <ref> [2] </ref> Lazowska et al. reported about 4 kbytes of I/O per second per active user. <p> Satyanarayanan's file-size measurements are roughly comparable to ours (about 50% of all his files were less than 2500 bytes), even though his measurements were made statically and his system did not support hierarchical directories [10]. The measurements of Lazowska et. al. are also very similar to ours <ref> [2] </ref>. Our last measurement of access patterns is displayed in programs tend to open files, read or write their contents, then close the files again very quickly. <p> Even for a cache size of 400 kbytes, an 8-kbyte block size results in about 10% fewer disk I/Os than a 4-kbyte block size and 60% fewer I/Os than a 1-kbyte block size. This conclusion is similar to the one reached by Lazowska et. al. in <ref> [2] </ref>. <p> The trace data include information about which files were executed as programs; we simulated paging activity by forcing a whole-file read to each program file at the time the program was executed. We did not attempt to simulate page-out activity, since <ref> [2] </ref> and [6] indicate that it rarely happens. As Figure 7 shows, the simulated paging resulted in degraded performance for small cache sizes (the large program files increased the total working set of file information), but improved miss ratios for large cache sizes.
Reference: [3] <author> Lukac, T. </author> <title> ``A UNIX File System Logical I/O Trace Package.'' M.S. </title> <type> Report, </type> <institution> U.C. Berkeley, </institution> <year> 1984. </year>
Reference-contexts: The kernel modifications were based on a Master's project by Tibor Lukac <ref> [3] </ref>. Luis Felipe Cabrera, Alan Smith, and the SOSP program committee provided helpful comments on early drafts of the paper. This work was supported in part by the Defense Advanced Research Projects Agency under Contract No. N00039-85-R-0269 and in part by the National Science Foundation under grant ECS-8351961.
Reference: [4] <author> McKusick, M.K., Karels, M., and Leffler, S. </author> <title> ``Performance Improvements and Functional Enhancements in 4.3 BSD.'' </title> <booktitle> Proceedings of the 1985 Usenix Summer Conference, </booktitle> <address> Portland, Oregon, </address> <month> June </month> <year> 1985, </year> <pages> pp. 519-531. </pages>
Reference-contexts: Two other recent studies contain UNIX measurements that partially overlap ours: Lazowska et al. analyzed block size tradeoffs and reported on the disk I/O required per user [2], and Leffler et al. reported on the effectiveness of current UNIX disk caches <ref> [4] </ref>. Sections 5 and 6 of this paper compare their results and ours. 3. Gathering the Data Our main concern in gathering file system trace information was the volume of data. We wished to gather data over several days to prevent temporary unusual activity from biasing the results. <p> However, 4.2 BSD contains a directory cache to hold recently-used entries. Leffler et al. report that the directory cache achieves an 85% hit ratio <ref> [4] </ref>. 4. The Traced Systems We collected trace data on three different systems, all timeshared VAX-11/780s in the Department of Electrical Engineering and Computer Sciences at U.C. Berkeley. The machines' names are ``Ucbarpa'', ``Ucbernie'', and ``Ucbcad'', and the traces we used for analysis are called ``A5'', ``E3'', and ``C4'', respectively. <p> According to our simulations, this combination of cache size and write policy should reduce disk accesses by about a factor of two. - 19 - Trace-Driven Analysis of 4.2 BSD File System January 2, 1993 However, Leffler et al. report a measured cache miss ratio of only about 15% <ref> [4] </ref>. There are two explanations for the discrepancy. First, there are many programs that make I/O requests in units smaller than the cache block size; this inflates the number of logical I/Os and reduces the miss ratio. Second, the measurements in [4] include block accesses for paging, directories, and file descriptors, <p> a measured cache miss ratio of only about 15% <ref> [4] </ref>. There are two explanations for the discrepancy. First, there are many programs that make I/O requests in units smaller than the cache block size; this inflates the number of logical I/Os and reduces the miss ratio. Second, the measurements in [4] include block accesses for paging, directories, and file descriptors, which we did not consider. We ran a crude test to verify the hypothesis that paging accesses also exhibit high locality.
Reference: [5] <author> McKusick, M.K., Joy, W.N., Leffler, S.J., and Fabry, </author> <title> R.S. ``A Fast File System for UNIX.'' </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 2, No. 3, </volume> <month> August </month> <year> 1984, </year> <pages> pp. 181-197. </pages>
Reference: [6] <author> Nelson, </author> <title> M.N. and Duffy, J.A. Feasibility of Network Paging and a Page Server Design. Term project, </title> <type> CS 262, </type> <institution> Department of EECS, University of California, Berkeley, </institution> - <note> 22 - Trace-Driven Analysis of 4.2 BSD File System January 2, </note> <year> 1993 </year> <month> May </month> <year> 1984. </year>
Reference-contexts: The first ``other factor'' is paging activity, which consists primarily of loading programs on demand from disk files into main memory. Paging to and from swapping store can also result in I/O activity but is rare in 4.2 BSD systems (see [2] and <ref> [6] </ref>). We estimated the effects of paging by logging execve system calls and recording the sizes of the files that were executed. The total number of bytes in such files ranged from 1.2 to 2 times the total number of bytes of logical file I/O, depending on the system measured. <p> The trace data include information about which files were executed as programs; we simulated paging activity by forcing a whole-file read to each program file at the time the program was executed. We did not attempt to simulate page-out activity, since [2] and <ref> [6] </ref> indicate that it rarely happens. As Figure 7 shows, the simulated paging resulted in degraded performance for small cache sizes (the large program files increased the total working set of file information), but improved miss ratios for large cache sizes.
Reference: [7] <author> Porcar, J.M. </author> <title> File Migration in Distributed Computer Systems. </title> <type> Ph.D. Dissertation, </type> <institution> University of California, Berke-ley, </institution> <month> July </month> <year> 1982. </year>
Reference-contexts: In another study, Porcar analyzed dynamic trace data for files in an - 2 - Trace-Driven Analysis of 4.2 BSD File System January 2, 1993 IBM batch environment <ref> [7] </ref>. He considered only shared files, which accounted for less than 10% of all the files accessed in his system. Satyanarayanan analyzed file sizes and lifetimes on a PDP-10 system [10], but the study was made statically by scanning the contents of disk storage at a fixed point in time. <p> They are typically accessed by positioning within the file and then reading or writing a small amount of data. The file sizes shown in Figure 2 are much smaller than those measured for IBM systems in <ref> [7] </ref> and [11]. We believe that this difference is due to the better support provided in UNIX for short files, including hierarchical directories and block-based disk allocation instead of track-based allocation.
Reference: [8] <author> Ritchie, D.M. and Thompson, K. </author> <title> ``The UNIX TimeSharing System.'' </title> <journal> Communications of the ACM, </journal> <volume> Vol. 17, No. 7, </volume> <month> July </month> <year> 1974, </year> <pages> pp. 365-375. </pages>
Reference: [9] <author> Rodriguez-Rosell, J. </author> <title> ``Empirical Data Reference Behavior in Data Base Systems.'' </title> <booktitle> IEEE Computer , November 1976, </booktitle> <pages> pp. 9-13. </pages>
Reference-contexts: We also think that the results will apply to operating systems other than UNIX. For example, Smith's disk cache study reaches conclusions similar to ours [12], even though his study used IBM mainframes and was based on physical disk blocks rather than logical file accesses. Rodriguez-Rosell determined in <ref> [9] </ref> that database systems also exhibit sequential access patterns. The generality of our conclusions is also supported by the similarity of the results for the three different traces.
Reference: [10] <author> Satyanarayanan, M. </author> <title> ``A Study of File Sizes and Functional Lifetimes.'' </title> <booktitle> Proc 8th Symposium on Operating Systems Principles, </booktitle> <year> 1981, </year> <pages> pp. 96-108. </pages>
Reference-contexts: He considered only shared files, which accounted for less than 10% of all the files accessed in his system. Satyanarayanan analyzed file sizes and lifetimes on a PDP-10 system <ref> [10] </ref>, but the study was made statically by scanning the contents of disk storage at a fixed point in time. <p> Satyanarayanan's file-size measurements are roughly comparable to ours (about 50% of all his files were less than 2500 bytes), even though his measurements were made statically and his system did not support hierarchical directories <ref> [10] </ref>. The measurements of Lazowska et. al. are also very similar to ours [2]. Our last measurement of access patterns is displayed in programs tend to open files, read or write their contents, then close the files again very quickly. <p> On the other hand, there are a few files that stay open for long periods of time, such as temporary files used by the text editor. 5.3. File Lifetimes Both Satyanarayanan <ref> [10] </ref> and Smith [11] have published measurements of file lifetimes (the intervals between when files - 11 - Trace-Driven Analysis of 4.2 BSD File System January 2, 1993 A5 C4 (a) File Size (kbytes) 100 60 20 200150100500 Percent Files A5 C4 of File Size (kbytes) 100 60 20 200150100500 Percent
Reference: [11] <author> Smith, A.J. </author> <title> ``Analysis of Long Term File Reference Patterns for Application to File Migration Algorithms.'' </title> <journal> IEEE Transactions on Software Engineering. </journal> <volume> Vol. SE-7, No. 4, </volume> <month> July, </month> <year> 1981, </year> <pages> pp. 403-417. </pages>
Reference-contexts: The published studies are limited in scope, and most deal with older operating systems. As a consequence, the results may not be applicable in planning future systems. For example, Smith studied the file access behavior of IBM mainframes in order to predict the effects of automatic file migration <ref> [11] </ref>. He only considered files used by a particular interactive editor, which were mostly program source files. The data were gathered as a series of daily scans of the disk, so they do not include files whose lifetimes were less than a day. <p> They are typically accessed by positioning within the file and then reading or writing a small amount of data. The file sizes shown in Figure 2 are much smaller than those measured for IBM systems in [7] and <ref> [11] </ref>. We believe that this difference is due to the better support provided in UNIX for short files, including hierarchical directories and block-based disk allocation instead of track-based allocation. <p> On the other hand, there are a few files that stay open for long periods of time, such as temporary files used by the text editor. 5.3. File Lifetimes Both Satyanarayanan [10] and Smith <ref> [11] </ref> have published measurements of file lifetimes (the intervals between when files - 11 - Trace-Driven Analysis of 4.2 BSD File System January 2, 1993 A5 C4 (a) File Size (kbytes) 100 60 20 200150100500 Percent Files A5 C4 of File Size (kbytes) 100 60 20 200150100500 Percent Bytes Transferred files
Reference: [12] <author> Smith, A.J. </author> <title> ``Disk Cache Miss Ratio Analysis and Design Considerations.'' </title> <journal> ACM Transactions on Computer Systems, </journal> <month> August </month> <year> 1985, </year> <pages> pp. 161-203. </pages>
Reference-contexts: Satyanarayanan analyzed file sizes and lifetimes on a PDP-10 system [10], but the study was made statically by scanning the contents of disk storage at a fixed point in time. More recently, Smith used trace data from IBM mainframes to predict the performance of disk caches <ref> [12] </ref>; his conclusions are similar to ours although he used different trace information (physical disk addresses, no information about files, transfer sizes or reading versus writing). <p> If we had attempted to record all file system activity, an enormous amount of data would have been produced. For example, the traces for Smith's cache study contained 1.5 gigabytes or more per day <ref> [12] </ref>. We feared that the work involved in writing such a trace file would have consumed a substantial fraction of the CPU. It might have perturbed our results, and it certainly would have made us unpopular with the systems' users. <p> However, we think that most of the conclusions will apply across a wide range of personal workstations and timesharing systems. We also think that the results will apply to operating systems other than UNIX. For example, Smith's disk cache study reaches conclusions similar to ours <ref> [12] </ref>, even though his study used IBM mainframes and was based on physical disk blocks rather than logical file accesses. Rodriguez-Rosell determined in [9] that database systems also exhibit sequential access patterns.
Reference: [13] <author> Thompson, J. </author> <title> ``File Deletion in The UNIX System: Its Impact of File System Design and Analysis.'' </title> <type> CS 266 term project, </type> <institution> Department of EECS, University of Califor-nia, Berkeley, </institution> <month> April </month> <year> 1985. </year> <month> - 23 </month> - 
Reference-contexts: The measurements in Sections 5 and 6 were averaged over intervals of at least 10 seconds and often longer, so we do not believe that the time imprecision biased our results very much. A later study <ref> [13] </ref> suggests that no-read-write approach exaggerates slightly the burstiness of the system. This makes our performance numbers slightly pessimistic. For example, [13] concludes that actual cache miss ratios will be 2-3% lower than predicted by Section 6. 3.2. <p> A later study <ref> [13] </ref> suggests that no-read-write approach exaggerates slightly the burstiness of the system. This makes our performance numbers slightly pessimistic. For example, [13] concludes that actual cache miss ratios will be 2-3% lower than predicted by Section 6. 3.2. Missing Data Our trace analyses consider both user- and system-initiated file access, but they examine only the actual bytes contained in files.
References-found: 13

