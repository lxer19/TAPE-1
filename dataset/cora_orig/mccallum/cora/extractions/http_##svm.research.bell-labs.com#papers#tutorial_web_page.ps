URL: http://svm.research.bell-labs.com/papers/tutorial_web_page.ps
Refering-URL: http://svm.research.bell-labs.com/SVMdoc.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: burges@lucent.com  
Title: A Tutorial on Support Vector Machines for Pattern Recognition  
Author: CHRISTOPHER J.C. BURGES Editor: Usama Fayyad 
Keyword: Support Vector Machines, Statistical Learning Theory, VC Dimension, Pattern Recognition  
Affiliation: Bell Laboratories, Lucent Technologies  
Note: 1-43 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light. 
Abstract-found: 1
Intro-found: 1
Reference: <author> 10 M.A. Aizerman, E.M. Braverman, and L.I. Rozoner. </author> <title> Theoretical foundations of the potential function method in pattern recognition learning. </title> <journal> Automation and Remote Control, </journal> <volume> 25 </volume> <pages> 821-837, </pages> <year> 1964. </year> <note> 42 M. </note> <author> Anthony and N. Biggs. </author> <title> Pac learning and neural networks. </title> <booktitle> In The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 694-697, </pages> <year> 1995. </year>
Reference: <author> K.P. Bennett and E. Bredensteiner. </author> <note> Geometry in learning. In Geometry at Work, page to appear, </note> <institution> Washington, </institution> <address> D.C., </address> <year> 1998. </year> <institution> Mathematical Association of America. </institution>
Reference-contexts: There is a nice geometric interpretation for the dual problem: it is basically finding the two closest points of convex hulls of the two sets. See <ref> (Bennett and Bredensteiner, 1998) </ref>. 9.
Reference: <author> C.M. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1995. </year>
Reference: <author> V. Blanz, B. Scholkopf, H. Bulthoff, C. Burges, V. Vapnik, and T. Vetter. </author> <title> Comparison of view-based object recognition algorithms using realistic 3d models. </title> <editor> In C. von der Malsburg, W. von Seelen, J. </editor> <address> C. </address>
Reference: <editor> Vorbruggen, and B. Sendhoff, editors, </editor> <booktitle> Artificial Neural Networks | ICANN'96, </booktitle> <pages> pages 251 - 256, </pages> <address> Berlin, </address> <year> 1996. </year> <booktitle> Springer Lecture Notes in Computer Science, </booktitle> <volume> Vol. </volume> <pages> 1112. </pages>
Reference: <author> B. E. Boser, I. M. Guyon, and V .Vapnik. </author> <title> A training algorithm for optimal margin classifiers. </title> <booktitle> In Fifth Annual Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, 1992. </address> <publisher> ACM. </publisher>
Reference-contexts: Nonlinear Support Vector Machines How can the above methods be generalized to the case where the decision function 11 is not a linear function of the data? <ref> (Boser, Guyon and Vapnik, 1992) </ref>, showed that a rather old trick (Aizerman, 1964) can be used to accomplish this in an astonishingly straightforward way. <p> A simple, effective combination trains 22 N one-versus-rest classifiers (say, "one" positive, "rest" negative) for the N -class case and takes the class for a test point to be that corresponding to the largest positive distance <ref> (Boser, Guyon and Vapnik, 1992) </ref>. 4.4. Global Solutions and Uniqueness When is the solution to the support vector training problem global, and when is it unique? By "global", we mean that there exists no other point in the feasible region at which the objective function takes a lower value. <p> Finally, if most SVs are at the upper bound, and N S =l 1, we have C of O (D L l 2 ). For larger problems, two decomposition algorithms have been proposed to date. In the "chunking" method <ref> (Boser, Guyon and Vapnik, 1992) </ref>, one starts with a small, arbitrary 26 subset of the data and trains on that.
Reference: <author> James R. Bunch and Linda Kaufman. </author> <title> Some stable methods for calculating inertia and solving symmetric linear systems. </title> <journal> Mathematics of computation, </journal> <volume> 31(137) </volume> <pages> 163-179, </pages> <year> 1977. </year>
Reference: <author> James R. Bunch and Linda Kaufman. </author> <title> A computational method for the indefinite quadratic programming problem. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 34 </volume> <pages> 341-370, </pages> <year> 1980. </year>
Reference: <author> C. J. C. Burges and B. Scholkopf. </author> <title> Improving the accuracy and speed of support vector learning machines. </title>
Reference: <editor> In M. Mozer, M. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9, </booktitle> <pages> pages 375-381, </pages> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. </publisher>
Reference: <author> C.J.C. Burges. </author> <title> Simplified support vector decision rules. </title> <editor> In Lorenza Saitta, editor, </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 71-77, </pages> <address> Bari, Italy, 1996. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Examples by Pictures one not. The two classes are denoted by circles and disks respectively. Support vectors are identified with an extra circle. The error in the non-separable case is identified with a cross. The reader is invited to use Lucent's SVM Applet <ref> (Burges, Knirsch and Haratsch, 1996) </ref> to experiment and create pictures like these (if possible, try using 16 or 24 bit color). decision surface. 4. <p> Despite this, ideas along these lines can be used to significantly speed up the test phase of SVMs <ref> (Burges, 1996) </ref>. Note also that it is easy to find kernels (for example, kernels which are functions of the dot products of the x i in L) such that the training algorithm and solution found are independent of the dimension of both L and H. <p> A second limitation is speed and size, both in training and testing. While the speed problem in test phase is largely solved in <ref> (Burges, 1996) </ref>, this still requires two training passes. Training for very large datasets (millions of support vectors) is an unsolved problem. Discrete data presents another problem, although with suitable rescaling excellent results have nevertheless been obtained (Joachims, 1997).
Reference: <author> C.J.C. Burges. </author> <title> Building locally invariant kernels. </title> <booktitle> In Proceedings of the 1997 NIPS Workshop on Support Vector Machines (to appear), </booktitle> <year> 1998. </year>
Reference-contexts: The same technique could be used for SVM regression to find much more efficient function representations (which could be used, for example, in data compression). Combining these two methods gave a factor of 50 speedup (while the error rate increased from 1.0% to 1.1%) on the NIST digits <ref> (Burges and Scholkopf, 1997) </ref>. 10. Conclusions SVMs provide a new approach to the problem of pattern recognition (together with regression estimation and linear operator inversion) with clear connections to the underlying statistical learning theory.
Reference: <author> C.J.C. Burges, P. Knirsch, and R. Haratsch. </author> <title> Support vector web page: </title> <type> http://svm.research.bell-labs.com. Technical report, </type> <institution> Lucent Technologies, </institution> <year> 1996. </year>
Reference-contexts: Examples by Pictures one not. The two classes are denoted by circles and disks respectively. Support vectors are identified with an extra circle. The error in the non-separable case is identified with a cross. The reader is invited to use Lucent's SVM Applet <ref> (Burges, Knirsch and Haratsch, 1996) </ref> to experiment and create pictures like these (if possible, try using 16 or 24 bit color). decision surface. 4. <p> Despite this, ideas along these lines can be used to significantly speed up the test phase of SVMs <ref> (Burges, 1996) </ref>. Note also that it is easy to find kernels (for example, kernels which are functions of the dot products of the x i in L) such that the training algorithm and solution found are independent of the dimension of both L and H. <p> A second limitation is speed and size, both in training and testing. While the speed problem in test phase is largely solved in <ref> (Burges, 1996) </ref>, this still requires two training passes. Training for very large datasets (millions of support vectors) is an unsolved problem. Discrete data presents another problem, although with suitable rescaling excellent results have nevertheless been obtained (Joachims, 1997).
Reference: <author> C. Cortes and V. Vapnik. </author> <title> Support vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-297, </pages> <year> 1995. </year>
Reference-contexts: This can be done by introducing positive slack variables ~ i ; i = 1; ; l in the constraints <ref> (Cortes and Vapnik, 1995) </ref>, which then become: 14 x i w + b 1 + ~ i for y i = 1 (41) Thus, for an error to occur, the corresponding ~ i must exceed unity, so P i ~ i is an upper bound on the number of training errors.
Reference: <author> R. Courant and D. </author> <title> Hilbert. Methods of Mathematical Physics. </title> <publisher> Interscience, </publisher> <year> 1953. </year>
Reference: <author> Luc Devroye, Laszlo Gyorfi, and Gabor Lugosi. </author> <title> A Probabilistic Theory of Pattern Recognition. Springer Verlag, </title> <journal> Applications of Mathematics Vol. </journal> <volume> 31, </volume> <year> 1996. </year>
Reference: <author> H. Drucker, C.J.C. Burges, L. Kaufman, A. Smola, and V. Vapnik. </author> <title> Support vector regression machines. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 9 </volume> <pages> 155-161, </pages> <year> 1997. </year>
Reference-contexts: For the regression estimation case, SVMs have been compared on benchmark time series prediction tests (Muller et al., 1997; Mukherjee, Osuna and Girosi, 1997), the Boston housing problem <ref> (Drucker et al., 1997) </ref>, and (on artificial data) on the PET operator inversion problem (Vapnik, Golowich 2 and Smola, 1996). In most of these cases, SVM generalization performance (i.e. error rates on test sets) either matches or is significantly better than that of competing methods.
Reference: <author> R. Fletcher. </author> <title> Practical Methods of Optimization. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> 2nd edition, </address> <year> 1987. </year>
Reference-contexts: This particular dual formulation of the problem is called the Wolfe dual <ref> (Fletcher, 1987) </ref>. <p> The Karush-Kuhn-Tucker Conditions The Karush-Kuhn-Tucker (KKT) conditions play a central role in both the theory and practice of constrained optimization. For the primal problem above, the KKT conditions may be stated <ref> (Fletcher, 1987) </ref>: @ L P = w - i @ L P = i y i (x i w + b) 1 0 i = 1; ; l (19) ff i (y i (w x i + b) 1) = 0 8i (21) The KKT conditions are satisfied at the solution <p> Furthermore, the problem for SVMs is convex (a convex objective function, with constraints which give a convex feasible region), and for convex problems (if the regularity condition holds), the KKT conditions are necessary and sufficient for w; b; ff to be a solution <ref> (Fletcher, 1987) </ref>. Thus solving the SVM problem is equivalent to finding a solution to the KKT conditions. This fact results in several approaches to finding the solution (for example, the primal-dual path following method mentioned in Section 5). <p> It turns out that every local solution is also global. This is a property of any convex programming problem <ref> (Fletcher, 1987) </ref>. <p> (note that for quadratic objective functions F , the Hessian is positive definite if and only if F is strictly convex; this is not true for non-quadratic F : there, a positive definite Hessian implies a strictly convex objective function, but not vice versa (consider F = x 4 ) <ref> (Fletcher, 1987) </ref>). However, even if the Hessian is positive semidefinite, the solution can still be unique: consider two points along the real line with coordinates x 1 = 1 and x 2 = 2, and with polarities + and .
Reference: <author> Stuart Geman and Elie Bienenstock. </author> <title> Neural networks and the bias / variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference: <author> F. Girosi. </author> <title> An equivalence between sparse approximation and support vector machines. </title> <note> Neural Computation (to appear); CBCL AI Memo 1606, </note> <institution> MIT, </institution> <year> 1998. </year>
Reference: <author> I. Guyon, V. Vapnik, B. Boser, L. Bottou, and S.A. Solla. </author> <title> Structural risk minimization for character recognition. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 4 </volume> <pages> 471-479, </pages> <year> 1992. </year>
Reference-contexts: The reader may also find the thesis of (Scholkopf, 1997) helpful. The problem which drove the initial development of SVMs occurs in several guises the bias variance tradeoff (Geman and Bienenstock, 1992), capacity control <ref> (Guyon et al., 1992) </ref>, overfitting (Montgomery and Peck, 1992) but the basic idea is the same. <p> In the following we will call the right hand side of Eq. (3) the "risk bound." We depart here from some previous nomenclature: the authors of <ref> (Guyon et al., 1992) </ref> call it the "guaranteed risk", but this is something of a misnomer, since it is really a bound on a risk, not a risk, and it holds only with a certain probability, and so is not guaranteed. <p> Nonlinear Support Vector Machines How can the above methods be generalized to the case where the decision function 11 is not a linear function of the data? <ref> (Boser, Guyon and Vapnik, 1992) </ref>, showed that a rather old trick (Aizerman, 1964) can be used to accomplish this in an astonishingly straightforward way. <p> A simple, effective combination trains 22 N one-versus-rest classifiers (say, "one" positive, "rest" negative) for the N -class case and takes the class for a test point to be that corresponding to the largest positive distance <ref> (Boser, Guyon and Vapnik, 1992) </ref>. 4.4. Global Solutions and Uniqueness When is the solution to the support vector training problem global, and when is it unique? By "global", we mean that there exists no other point in the feasible region at which the objective function takes a lower value. <p> Finally, if most SVs are at the upper bound, and N S =l 1, we have C of O (D L l 2 ). For larger problems, two decomposition algorithms have been proposed to date. In the "chunking" method <ref> (Boser, Guyon and Vapnik, 1992) </ref>, one starts with a small, arbitrary 26 subset of the data and trains on that.
Reference: <author> P.R. Halmos. </author> <title> A Hilbert Space Problem Book. </title> <address> D. </address> <publisher> Van Nostrand Company, Inc., </publisher> <year> 1967. </year>
Reference-contexts: Some authors (e.g. (Kolmogorov, 1970)) also require that it be separable (that is, it must have a countable subset whose closure is the space itself), and some <ref> (e.g. Halmos, 1967) </ref> don't. It's a generalization mainly because its inner product can be any inner product, not just the scalar ("dot") product used here (and in Euclidean spaces in general). It's interesting that the older mathematical literature (e.g.
Reference: <author> Roger A. Horn and Charles R. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference: <author> T. Joachims. </author> <title> Text categorization with support vector machines. </title> <type> Technical report, LS VIII Number 23, </type> <institution> University of Dortmund, </institution> <year> 1997. </year> <month> ftp://ftp-ai.informatik.uni-dortmund.de/pub/Reports/report23.ps.Z. </month>
Reference-contexts: for isolated handwritten digit recognition (Cortes and Vapnik, 1995; Scholkopf, Burges and Vapnik, 1995; Scholkopf, Burges and Vapnik, 1996; Burges and Scholkopf, 1997), object recognition (Blanz et al., 1996), speaker identification (Schmidt, 1996), charmed quark detection 1 , face detection in images (Osuna, Freund and Girosi, 1997a), and text categorization <ref> (Joachims, 1997) </ref>. For the regression estimation case, SVMs have been compared on benchmark time series prediction tests (Muller et al., 1997; Mukherjee, Osuna and Girosi, 1997), the Boston housing problem (Drucker et al., 1997), and (on artificial data) on the PET operator inversion problem (Vapnik, Golowich 2 and Smola, 1996). <p> While the speed problem in test phase is largely solved in (Burges, 1996), this still requires two training passes. Training for very large datasets (millions of support vectors) is an unsolved problem. Discrete data presents another problem, although with suitable rescaling excellent results have nevertheless been obtained <ref> (Joachims, 1997) </ref>. Finally, although some work has been done on training a multiclass SVM in one step 24 , the optimal design for multiclass SVM classifiers is a further area for research. 9.
Reference: <author> L. Kaufman. </author> <title> Solving the qp problem for support vector training. </title> <booktitle> In Proceedings of the 1997 NIPS Workshop on Support Vector Machines (to appear), </booktitle> <year> 1998. </year>
Reference: <author> A.N. Kolmogorov and S.V.Fomin. </author> <title> Introductory Real Analysis. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1970. </year>
Reference-contexts: Specifically, it is any linear space, with an inner product defined, which is also complete with respect to the corresponding norm (that is, any Cauchy sequence of points converges to a point in the space). Some authors (e.g. <ref> (Kolmogorov, 1970) </ref>) also require that it be separable (that is, it must have a countable subset whose closure is the space itself), and some (e.g. Halmos, 1967) don't. <p> Halmos, 1967) don't. It's a generalization mainly because its inner product can be any inner product, not just the scalar ("dot") product used here (and in Euclidean spaces in general). It's interesting that the older mathematical literature <ref> (e.g. Kolmogorov, 1970) </ref> also required that Hilbert spaces be infinite dimensional, and that mathematicians are quite happy defining infinite dimensional Euclidean spaces. Research on Hilbert spaces centers on operators in those spaces, since the basic properties have long since been worked out. <p> Thus fH; ng and fH; ng are different oriented hyperplanes. 6. Such a set of m points (which span an m 1 dimensional subspace of a linear space) are said to be "in general position" <ref> (Kolmogorov, 1970) </ref>. The convex hull of a set of m points in general position defines an m 1 dimensional simplex, the vertices of which are the points themselves. 7.
Reference: <author> O.L. Mangarasian. </author> <title> Nonlinear Programming. </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1969. </year>
Reference: <author> Garth P. McCormick. </author> <title> Non Linear Programming: Theory, Algorithms and Applications. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1983. </year>
Reference: <author> D.C. Montgomery and E.A. Peck. </author> <title> Introduction to Linear Regression Analysis. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: The reader may also find the thesis of (Scholkopf, 1997) helpful. The problem which drove the initial development of SVMs occurs in several guises the bias variance tradeoff (Geman and Bienenstock, 1992), capacity control (Guyon et al., 1992), overfitting <ref> (Montgomery and Peck, 1992) </ref> but the basic idea is the same.
Reference: <author> More and Wright. </author> <title> Optimization Guide. </title> <publisher> SIAM, </publisher> <year> 1993. </year>
Reference-contexts: For small problems, any general purpose optimization package that solves linearly constrained convex quadratic programs will do. A good survey of the available solvers, and where to get them, can be found 16 in <ref> (More and Wright, 1993) </ref>. For larger problems, a range of existing techniques can be brought to bear. A full exploration of the relative merits of these methods would fill another tutorial.
Reference: <author> Jorge J. More and Gerardo Toraldo. </author> <title> On the solution of large quadratic programming problems with bound constraints. </title> <journal> SIAM J. Optimization, </journal> <volume> 1(1) </volume> <pages> 93-113, </pages> <year> 1991. </year>
Reference: <author> S. Mukherjee, E. Osuna, and F. Girosi. </author> <title> Nonlinear prediction of chaotic time series using a support vector machine. </title> <booktitle> In Proceedings of the IEEE Workshop on Neural Networks for Signal Processing 7, </booktitle> <pages> pages 511-519, </pages> <address> Amelia Island, FL, </address> <year> 1997. </year>
Reference: <author> K.-R. Muller, A. Smola, G. Ratsch, B. Scholkopf, J. Kohlmorgen, and V. Vapnik. </author> <title> Predicting time series with support vector machines. </title> <booktitle> In Proceedings, International Conference on Artificial Neural Networks, page 999. Springer Lecture Notes in Computer Science, </booktitle> <year> 1997. </year>
Reference: <author> Edgar Osuna, Robert Freund, and Federico Girosi. </author> <title> An improved training algorithm for support vector machines. </title> <booktitle> In Proceedings of the 1997 IEEE Workshop on Neural Networks for Signal Processing, Eds. </booktitle>
Reference: <author> J. Principe, L. Giles, N. Morgan, E. </author> <booktitle> Wilson, </booktitle> <pages> pages 276 - 285, </pages> <address> Amelia Island, FL, </address> <year> 1997. </year>
Reference: <author> Edgar Osuna, Robert Freund, and Federico Girosi. </author> <title> Training support vector machines: an application to face detection. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 130 - 136, </pages> <year> 1997. </year> <title> 43 Edgar Osuna and Federico Girosi. Reducing the run-time complexity of support vector machines. </title> <booktitle> In International Conference on Pattern Recognition (submitted), </booktitle> <year> 1998. </year>
Reference: <author> William H. Press, Brain P. Flannery, Saul A. Teukolsky, and William T. Vettering. </author> <title> Numerical recipes in C: </title> <booktitle> the art of scientific computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference: <author> M. Schmidt. </author> <title> Identifying speaker with support vector networks. </title> <booktitle> In Interface '96 Proceedings, </booktitle> <address> Sydney, </address> <year> 1996. </year>
Reference-contexts: For the pattern recognition case, SVMs have been used for isolated handwritten digit recognition (Cortes and Vapnik, 1995; Scholkopf, Burges and Vapnik, 1995; Scholkopf, Burges and Vapnik, 1996; Burges and Scholkopf, 1997), object recognition (Blanz et al., 1996), speaker identification <ref> (Schmidt, 1996) </ref>, charmed quark detection 1 , face detection in images (Osuna, Freund and Girosi, 1997a), and text categorization (Joachims, 1997).
Reference: <author> B. Scholkopf. </author> <title> Support Vector Learning. </title> <editor> R. </editor> <publisher> Oldenbourg Verlag, </publisher> <address> Munich, </address> <year> 1997. </year>
Reference-contexts: The reader may also find the thesis of <ref> (Scholkopf, 1997) </ref> helpful. The problem which drove the initial development of SVMs occurs in several guises the bias variance tradeoff (Geman and Bienenstock, 1992), capacity control (Guyon et al., 1992), overfitting (Montgomery and Peck, 1992) but the basic idea is the same. <p> the RBF case, the number of centers (N S in Eq. (61)), the centers themselves (the s i ), the weights (ff i ), and the threshold (b) are all produced automatically by the SVM training and give excellent results compared to classical RBFs, for the case of Gaussian RBFs <ref> (Scholkopf et al, 1997) </ref>. <p> Even though their VC dimension is infinite (if the data is allowed to take all values in R d L ), SVM RBFs can have excellent performance <ref> (Scholkopf et al, 1997) </ref>. A similar story holds for polynomial SVMs. How come? 7. The Generalization Performance of SVMs In this Section we collect various arguments and bounds relating to the generalization performance of SVMs. <p> The same technique could be used for SVM regression to find much more efficient function representations (which could be used, for example, in data compression). Combining these two methods gave a factor of 50 speedup (while the error rate increased from 1.0% to 1.1%) on the NIST digits <ref> (Burges and Scholkopf, 1997) </ref>. 10. Conclusions SVMs provide a new approach to the problem of pattern recognition (together with regression estimation and linear operator inversion) with clear connections to the underlying statistical learning theory.
Reference: <author> B. Scholkopf, C. Burges, and V. Vapnik. </author> <title> Extracting support data for a given task. </title> <editor> In U. M. Fayyad and R. Uthurusamy, editors, </editor> <booktitle> Proceedings, First International Conference on Knowledge Discovery & Data Mining. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1995. </year>
Reference: <author> B. Scholkopf, C. Burges, and V. Vapnik. </author> <title> Incorporating invariances in support vector learning machines. </title> <editor> In C. von der Malsburg, W. von Seelen, J. C. Vorbruggen, and B. Sendhoff, editors, </editor> <booktitle> Artificial Neural Networks | ICANN'96, </booktitle> <pages> pages 47 - 52, </pages> <address> Berlin, </address> <year> 1996. </year> <booktitle> Springer Lecture Notes in Computer Science, </booktitle> <volume> Vol. </volume> <pages> 1112. </pages>
Reference: <author> B. Scholkopf, P. Simard, A. Smola, and V. Vapnik. </author> <title> Prior knowledge in support vector kernels. </title> <editor> In M. Jordan, M. Kearns, and S. Solla, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 10, </booktitle> <address> Cambridge, MA, 1998. </address> <publisher> MIT Press. In press. </publisher>
Reference: <author> B. Scholkopf, A. Smola, and K.-R. Muller. </author> <title> Nonlinear component analysis as a kernel eigenvalue problem. </title> <booktitle> Neural Computation, </booktitle> <year> 1998. </year> <note> In press. </note>
Reference: <author> B. Scholkopf, A. Smola, K.-R. Muller, C.J.C. Burges, and V. Vapnik. </author> <title> Support vector methods in learning and feature extraction. </title> <booktitle> In Ninth Australian Congress on Neural Networks (to appear), </booktitle> <year> 1998. </year>
Reference: <author> B. Scholkopf, K. Sung, C. Burges, F. Girosi, P. Niyogi, T. Poggio, and V. Vapnik. </author> <title> Comparing support vector machines with gaussian kernels to radial basis function classifiers. </title> <journal> IEEE Trans. Sign. Processing, </journal> <volume> 45:2758 - 2765, </volume> <year> 1997. </year>
Reference-contexts: The reader may also find the thesis of <ref> (Scholkopf, 1997) </ref> helpful. The problem which drove the initial development of SVMs occurs in several guises the bias variance tradeoff (Geman and Bienenstock, 1992), capacity control (Guyon et al., 1992), overfitting (Montgomery and Peck, 1992) but the basic idea is the same. <p> the RBF case, the number of centers (N S in Eq. (61)), the centers themselves (the s i ), the weights (ff i ), and the threshold (b) are all produced automatically by the SVM training and give excellent results compared to classical RBFs, for the case of Gaussian RBFs <ref> (Scholkopf et al, 1997) </ref>. <p> Even though their VC dimension is infinite (if the data is allowed to take all values in R d L ), SVM RBFs can have excellent performance <ref> (Scholkopf et al, 1997) </ref>. A similar story holds for polynomial SVMs. How come? 7. The Generalization Performance of SVMs In this Section we collect various arguments and bounds relating to the generalization performance of SVMs. <p> The same technique could be used for SVM regression to find much more efficient function representations (which could be used, for example, in data compression). Combining these two methods gave a factor of 50 speedup (while the error rate increased from 1.0% to 1.1%) on the NIST digits <ref> (Burges and Scholkopf, 1997) </ref>. 10. Conclusions SVMs provide a new approach to the problem of pattern recognition (together with regression estimation and linear operator inversion) with clear connections to the underlying statistical learning theory.
Reference: <author> John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. </author> <title> A framework for structural risk minimization. </title> <booktitle> In Proceedings, 9th Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 68-76, </pages> <year> 1996. </year>
Reference: <author> John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. </author> <title> Structural risk minimization over data-dependent hierarchies. </title> <type> Technical report, NeuroCOLT Technical Report NC-TR-96-053, </type> <year> 1996. </year>
Reference: <author> A. Smola and B. Scholkopf. </author> <title> On a kernel-based method for pattern recognition, regression, approximation and operator inversion. </title> <note> Algorithmica (to appear), </note> <year> 1998. </year>
Reference: <author> A. Smola, B. Scholkopf, and K.-R. Muller. </author> <title> General cost functions for support vector regression. </title> <booktitle> In Ninth Australian Congress on Neural Networks (to appear), </booktitle> <year> 1998. </year>
Reference: <author> Alex J. Smola, Bernhard Scholkopf, and Klaus-Robert Muller. </author> <title> The connection between regularization operators and support vector kernels. Neural Networks (to appear), </title> <year> 1998. </year>
Reference: <author> M. O. Stitson, A. Gammerman, V. Vapnik, V.Vovk, C. Watkins, and J. Weston. </author> <title> Support vector anova decomposition. </title> <type> Technical report, </type> <institution> Royal Holloway College, Report number CSD-TR-97-22, </institution> <year> 1997. </year>
Reference: <author> Gilbert Strang. </author> <title> Introduction to Applied Mathematics. </title> <publisher> Wellesley-Cambridge Press, </publisher> <year> 1986. </year>
Reference: <author> R. J. Vanderbei. </author> <title> Interior point methods : Algorithms and formulations. </title> <journal> ORSA J. Computing, </journal> <volume> 6(1) </volume> <pages> 32-34, </pages> <year> 1994. </year>
Reference: <author> R.J. Vanderbei. </author> <title> LOQO: An interior point code for quadratic programming. </title> <type> Technical report, </type> <institution> Program in Statistics & Operations Research, Princeton University, </institution> <year> 1994. </year>
Reference: <author> V. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data [in Russian]. </title> <publisher> Nauka, </publisher> <address> Moscow, </address> <year> 1979. </year> <title> (English translation: </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1982). </year>
Reference-contexts: The books (Vapnik, 1995; Vapnik, 1998) contain excellent descriptions of SVMs, but they leave room for an account whose purpose from the start is to teach. Although the subject can be said to have started in the late seventies <ref> (Vapnik, 1979) </ref>, it is only now receiving increasing attention, and so the time appears suitable for an introductory review. The tutorial dwells entirely on the pattern recognition problem. <p> Neither can generalize well. The exploration and formalization of these concepts has resulted in one of the shining peaks of the theory of statistical learning <ref> (Vapnik, 1979) </ref>. In the following, bold typeface will indicate vector or matrix quantities; normal typeface will be used for vector and matrix components and for scalars. We will label components of vectors and matrices with Greek indices, and label vectors and matrices themselves with Roman indices. <p> The theory grew out of considerations of under what circumstances, and how quickly, the mean of some empirical quantity converges uniformly, as the number of data points increases, to the true mean (that which would be calculated from an infinite amount of data) <ref> (Vapnik, 1979) </ref>. Let us start with one of these bounds. The notation here will largely follow that of (Vapnik, 1995). Suppose we are given l observations. <p> Structural Risk Minimization We can now summarize the principle of structural risk minimization (SRM) <ref> (Vapnik, 1979) </ref>. Note that the VC confidence term in Eq. (3) depends on the chosen class of functions, whereas the empirical risk and actual risk depend on the one particular function chosen by the training procedure. <p> For the proof we assume the following lemma, which in <ref> (Vapnik, 1979) </ref> is held to follow from symmetry arguments 20 : Lemma: Consider n d + 1 points lying in a ball B 2 R d . Let the points be shatterable by gap tolerant classifiers with margin M . <p> The derivation of the bound assumes that the empirical risk converges uniformly to the actual risk as the number of training observations increases <ref> (Vapnik, 1979) </ref>. A necessary and sufficient condition for this is that lim l!1 H (l)=l = 0, where l is the number of training samples and H (l) is the VC entropy of the set of decision functions (Vapnik, 1979; Vapnik, 1995). <p> The sum of torques on the decision sheet is then: X * 1 ::: n s i n1 F i n = i 10. In the original formulation <ref> (Vapnik, 1979) </ref> they were called "extreme vectors." 11. By "decision function" we mean a function f (x) whose sign represents the class assigned to data point x. 12. By "intrinsic dimension" we mean the number of parameters required to specify a point on the manifold. 13.
Reference: <author> V. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: Let us start with one of these bounds. The notation here will largely follow that of <ref> (Vapnik, 1995) </ref>. Suppose we are given l observations. Each observation consists of a pair: a vector x i 2 R n ; i = 1; : : : ; l and the associated "truth" y i , given to us by a trusted source. <p> For the case described here, it can only take the values 0 and 1. Now choose some such that 0 1. Then for losses taking these values, with probability 1 , the following bound holds <ref> (Vapnik, 1995) </ref>: R (ff) R emp (ff) + h (log (2l=h) + 1) log (=4) where h is a non-negative integer called the Vapnik Chervonenkis (VC) dimension, and is a measure of the notion of capacity mentioned above. <p> Intuitively, one might be led to expect that learning machines with many parameters would have high VC dimension, while learning machines with few parameters would have low VC dimension. There is a striking counterexample to this, due to E. Levin and J.S. Denker <ref> (Vapnik, 1995) </ref>: A learning machine with just one parameter, but with infinite VC dimension (a family of classifiers is said to have infinite VC dimension if it can shatter l points, no matter how large l). <p> This can be done by introducing positive slack variables ~ i ; i = 1; ; l in the constraints <ref> (Cortes and Vapnik, 1995) </ref>, which then become: 14 x i w + b 1 + ~ i for y i = 1 (41) Thus, for an error to occur, the corresponding ~ i must exceed unity, so P i ~ i is an upper bound on the number of training errors. <p> Thus for the neural network case, the architecture (number of weights) is determined by SVM training. Note, however, that the hyperbolic tangent kernel only satisfies Mercer's condition for certain values of the parameters and ffi (and of the data kxk 2 ). This was first noticed experimentally <ref> (Vapnik, 1995) </ref>; however some necessary conditions on these parameters for positivity are now known 14 . 7, but where the kernel was chosen to be a cubic polynomial. <p> These ideas can be used to show how gap tolerant classifiers implement structural risk minimization. The extension of the above example to spaces of arbitrary dimension is encapsulated in a (modified) theorem of <ref> (Vapnik, 1995) </ref>: Theorem 6 For data in R d , the VC dimension h of gap tolerant classifiers of minimum margin M min and maximum diameter D max is bounded above 19 by minfdD 2 max =M 2 1. <p> However, a gap tolerant classifier is not an SVM, and so the argument does not constitute a rigorous demonstration of structural risk minimization for SVMs. The original argument for structural risk minimization for SVMs is known to be flawed, since the structure there is determined by the data (see <ref> (Vapnik, 1995) </ref>, Section 5.11). I believe that there is a further subtle problem with the original argument. The structure is defined so that no training points are members of the margin set. However, one must still specify how test points that fall into the margin are to be labeled. <p> Consider the smallest area equilateral triangle containing two given points in R 2 . If the points' position vectors are linearly dependent, the center of the triangle cannot be expressed in terms of them.) 7.4. A Bound from Leave-One-Out <ref> (Vapnik, 1995) </ref> gives an alternative bound on the actual risk of support vector machines: E [P (error)] E [Number of support vectors] Number of training samples ; (93) where P (error) is the actual risk for a machine trained on l 1 examples, E [P (error)] is the expectation of the <p> Note however that the proof only holds for the second definition of shattering given above. Finally, note that the usual form of the VC bounds is easily derived from Eq. (A.14) by using s (A; n) (en=h) h (where h is the VC dimension) <ref> (Vapnik, 1995) </ref>, setting = 8s (A; n) exp n* 2 =32 , and solving for *. Clearly these results apply to our gap tolerant classifiers of Section 7.1. <p> Thanks to L. Kaufman for providing me with these results. 19. Recall that the "ceiling" sign de means "smallest integer greater than or equal to." Also, there is a typo in the actual formula given in <ref> (Vapnik, 1995) </ref>, which I have corrected here. 20. Note, for example, that the distance between every pair of vertices of the symmetric simplex is the same: see Eq. (26). However, a rigorous proof is needed, and as far as I know is lacking. 21. Thanks to J. <p> Thanks to J. Shawe-Taylor for pointing this out. 22. V. Vapnik, Private Communication. 23. There is an alternative bound one might use, namely that corresponding to the set of totally bounded non-negative functions (Equation (3.28) in <ref> (Vapnik, 1995) </ref>). However, for loss functions taking the value zero or one, and if the empirical risk is zero, this bound is looser than that in Eq. (3) whenever h (log (2l=h)+1)log (=4) l &gt; 1=16, which is the case here. 24. V. Blanz, Private Communication
Reference: <author> V. Vapnik. </author> <title> Statistical Learning Theory. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, </address> <note> in preparation. </note>
Reference: <author> V. Vapnik, S. Golowich, and A. Smola. </author> <title> Support vector method for function approximation, regression estimation, </title> <booktitle> and signal processing. Advances in Neural Information Processing Systems, </booktitle> <volume> 9 </volume> <pages> 281-287, </pages> <year> 1996. </year>
Reference-contexts: For the regression estimation case, SVMs have been compared on benchmark time series prediction tests (Muller et al., 1997; Mukherjee, Osuna and Girosi, 1997), the Boston housing problem (Drucker et al., 1997), and (on artificial data) on the PET operator inversion problem <ref> (Vapnik, Golowich 2 and Smola, 1996) </ref>. In most of these cases, SVM generalization performance (i.e. error rates on test sets) either matches or is significantly better than that of competing methods. <p> So far we have considered cases where is done implicitly. One can equally well turn things around and start with , and then construct the corresponding kernel. For example <ref> (Vapnik, 1996) </ref>, if L = R 1 , then a Fourier expansion in the data x, cut off after N terms, has the form f (x) = 2 N X (a 1r cos (rx) + a 2r sin (rx)) (72) and this can be viewed as a dot product between two
Reference: <author> Grace Wahba. </author> <title> Support vector machines, reproducing kernel hilbert spaces and the gacv. </title> <booktitle> In Proceedings of the 1997 NIPS Workshop on Support Vector Machines (to appear), </booktitle> <year> 1998. </year>
Reference: <author> J. Weston, A. Gammerman, M. O. Stitson, V. Vapnik, V.Vovk, and C. Watkins. </author> <title> Density estimation using support vector machines. </title> <type> Technical report, </type> <institution> Royal Holloway College, Report number CSD-TR-97-23, </institution> <year> 1997. </year>
Reference-contexts: In most of these cases, SVM generalization performance (i.e. error rates on test sets) either matches or is significantly better than that of competing methods. The use of SVMs for density estimation <ref> (Weston et al., 1997) </ref> and ANOVA decomposition (Stit-son et al., 1997) has also been studied.
References-found: 60

