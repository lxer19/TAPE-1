URL: http://polaris.cs.uiuc.edu/reports/1439.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Examination of a memory access classification scheme for pointer-intensive and numeric programs  
Author: Sharad Mehrotra Luddy Harrison 
Keyword: CPU architecture, data cache, memory access pattern classification, instruction profiling, memory latency tolerance  
Note: Some aspects of this research are covered by a patent application filed by the University of Illinois. Corresponding author.  
Address: 1308 West Main Street Urbana, IL 61801-2307  One Kendall Square, Building 200 Cambridge, MA 02139  
Affiliation: CSRD and UI Department of Computer Science  UI Dept. of Computer Science and Connected Components Corporation  
Email: (mehrotra@csrd.uiuc.edu)  (harrison@csrd.uiuc.edu)  
Phone: (217) 244-4657  (617) 577-1024  
Date: 1 December 1995  
Abstract: In recent work, we described a data prefetch mechanism for pointer-intensive and numeric computations, and presented some aggregate measurements on a suite of benchmarks to quantify its performance potential [MH95]. The basis for this device is a simple classification of memory access patterns in programs that we introduced earlier [HM94]. In this paper we take a close look at two codes from our suite, an English parser called Link-Gram, and the circuit simulation program spice2g6, and present a detailed analysis of them in the context of our model. Focusing on just two programs allows us to display a wider range of data, and discuss relevant code fragments extracted from their source distributions. Results from this study provide a deeper understanding of our memory access classification scheme, and suggest additional optimizations for future data prefetch mechanisms. 
Abstract-found: 1
Intro-found: 1
Reference: [APS95] <author> Todd M. Austin, Dionisios N. Pnevmatikatos, and Gurindar S. Sohi. </author> <title> Streamlining Data Cache Access with Fast Address Calculation. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 369-380, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Selvidge had similar goals in the experiments he reported in his thesis [Sel92]. McNiven and Davidson made an early study of the data reference patterns in programs [MD88]. Austin et al <ref> [APS95] </ref> profiled load instructions while developing software support for their fast address calculation mechanism. They reported aggregate data from their experiments, not individual instruction profiles. Lebeck and Wood used their CProf cache profiling system to analyze cache bottlenecks on a subset of the Spec92 codes [LW94].
Reference: [AR94] <author> Santosh G. Abraham and B. Ramakrishna Rau. </author> <title> Predicting Load Latencies Using Cache Profiling. </title> <type> Technical Report HPL-94-110, </type> <institution> Hewlett-Packard Laboratories, </institution> <address> Palo Alto, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Section 5 discusses our experimental methodology, and presents detailed analysis and results for programs Link-Gram and spice2g6. Section 6 offers some conclusions from this research. 2 Related work Related work is drawn from several topics of research. Closely related is the work by Abraham and Rau <ref> [AR94] </ref>. They reported results from the profiling of load instructions in the Spec89 benchmarks. They were interested in using the data to construct more effective instruction scheduling algorithms, and to improve compile-time cache management. Selvidge had similar goals in the experiments he reported in his thesis [Sel92]. <p> This design decision allows us to allows us to avoid the complexities introduced by conflict misses <ref> [AR94, LW94] </ref>. We now examine the twenty most heavily missed loads in Link-Gram and spice2g6. This data is presented in Tables 3 and 4 respectively. Each entry in these two tables has eleven fields.
Reference: [BCJ + 94] <author> David F. Bacon, Jyh-Herng Chow, Dz-ching R. Ju, Kalyan Muthukumar, and Vivek Sarkar. </author> <title> A Compiler Framework for Restructuring Data Declarations to Enhance Cache and TLB Effectiveness. </title> <booktitle> In Proceedings of CAS-CON '94, </booktitle> <pages> pages 270-282, </pages> <address> Toronto, Canada, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: For some programs, particularly scientific codes operating on dense arrays or matrices, data reference pattern prediction is easy. Consequently, several hardware prefetch mechanisms have been proposed for such codes, and effective compiler techniques developed for them <ref> [LRW91, Mow94, BCJ + 94, CMT94] </ref>. Predicting the memory access patterns in pointer-intensive and sparse numerical computations is a much harder problem, and has received far less attention in the literature [TJ92, Sel92]. <p> Other analytic studies of cache behavior include [FTJ95, LRW91]. The branch prediction and classification problem has received extensive attention in the literature; [YGS95, CHYP94] are two recent studies. Finally, several compiler techniques for memory hierarchy management have been reported, for example <ref> [CMT94, BCJ + 94, Mow94, Sel92] </ref>. 3 Classifying load instructions This section describes our load classification model. We will illustrate it using code fragments written in C. First, consider the code in Figure 1 that performs a reduction on array a.
Reference: [BZ93] <author> David A. Barrett and Benjamin G. Zorn. </author> <title> Using Lifetime Predictors to Improve Memory Allocation Performance. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <pages> pages 187-196, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: None of these studies has proposed a model to explain load behavior across a broad range of programs, as our work does. In other studies, Vajapeyam and Hsu [VH92] reported basic block characteristics for scalar Cray Y-MP code. Barrett and Zorn <ref> [BZ93] </ref> measured the lifetimes of dynamically allocated objects to predict lifetimes of short-lived objects, so that they could build better storage allocators. <p> Zorn working in collaboration with Grunwald, has published several papers on other aspects of dynamic memory management in pro 5 int i, m, a [100]; m = m + a [i]; grams; see <ref> [BZ93] </ref> for references. Cvetanovic and Bhandarkar [CB94] used performance monitoring hardware on a DEC Alpha system to produce a thorough instruction-level characterization of that machine running transaction processing and Spec92 workloads. Temam and Jalby have provided a mathematical characterization of the cache behavior of sparse matrix-vector multiplication [TJ92].
Reference: [CB94] <author> Zarka Cvetanovic and Dileep Bhandarkar. </author> <title> Characterization of Alpha AXP Performance Using TP and SPEC Workloads. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 60-70, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Modern processors therefore achieve excellent performance on programs whose working sets fit in their primary caches. However, when executing programs with large working sets or complex memory access patterns, CPU performance degenerates, becoming roughly inversely proportional to the number of data cache misses that cannot be masked <ref> [CB94] </ref>. <p> Zorn working in collaboration with Grunwald, has published several papers on other aspects of dynamic memory management in pro 5 int i, m, a [100]; m = m + a [i]; grams; see [BZ93] for references. Cvetanovic and Bhandarkar <ref> [CB94] </ref> used performance monitoring hardware on a DEC Alpha system to produce a thorough instruction-level characterization of that machine running transaction processing and Spec92 workloads. Temam and Jalby have provided a mathematical characterization of the cache behavior of sparse matrix-vector multiplication [TJ92].
Reference: [CB95] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> Effective Hardware-Based Data Prefetching for High-Performance Processors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(5) </volume> <pages> 609-623, </pages> <month> May </month> <year> 1995. </year>
Reference: [CHYP94] <author> Po-Yung Chang, Eric Hao, Tse-Yu Yeh, and Yale Patt. </author> <title> Branch Classification: a New Mechanism for Improving Branch Predictor Performance. </title> <booktitle> In Proceedings of the 27th Annual International Symposium on Microarchitec-ture, </booktitle> <pages> pages 22-31, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Temam and Jalby have provided a mathematical characterization of the cache behavior of sparse matrix-vector multiplication [TJ92]. Other analytic studies of cache behavior include [FTJ95, LRW91]. The branch prediction and classification problem has received extensive attention in the literature; <ref> [YGS95, CHYP94] </ref> are two recent studies. Finally, several compiler techniques for memory hierarchy management have been reported, for example [CMT94, BCJ + 94, Mow94, Sel92]. 3 Classifying load instructions This section describes our load classification model. We will illustrate it using code fragments written in C.
Reference: [CKP91] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <booktitle> Software prefetch-ing. In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year> <month> 29 </month>
Reference: [CMT94] <author> Steve Carr, Kathryn S. McKinley, and Chau-Wen Tseng. </author> <title> Compiler Opti--mizations for Improving Data Locality. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 252-262, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: For some programs, particularly scientific codes operating on dense arrays or matrices, data reference pattern prediction is easy. Consequently, several hardware prefetch mechanisms have been proposed for such codes, and effective compiler techniques developed for them <ref> [LRW91, Mow94, BCJ + 94, CMT94] </ref>. Predicting the memory access patterns in pointer-intensive and sparse numerical computations is a much harder problem, and has received far less attention in the literature [TJ92, Sel92]. <p> Other analytic studies of cache behavior include [FTJ95, LRW91]. The branch prediction and classification problem has received extensive attention in the literature; [YGS95, CHYP94] are two recent studies. Finally, several compiler techniques for memory hierarchy management have been reported, for example <ref> [CMT94, BCJ + 94, Mow94, Sel92] </ref>. 3 Classifying load instructions This section describes our load classification model. We will illustrate it using code fragments written in C. First, consider the code in Figure 1 that performs a reduction on array a.
Reference: [Coh76] <author> Ellis Cohen. </author> <title> Program Reference for SPICE2. </title> <type> Technical Report ERL-M592, </type> <institution> University of California, Electronics Research Laboratory, Berkeley, </institution> <address> CA 94720, </address> <month> 14 June </month> <year> 1976. </year>
Reference-contexts: For this reason, the implementors chose a linked-list representation for the Y-matrix. For additional details of the spice2g6 implementation, consult Chapter 8 of [MA93], and <ref> [Coh76] </ref>. This version of spice is the basis of several commercial re-implementations, and is one of the key tools used by a large portion of the semiconductor industry.
Reference: [CR94] <author> Mark J. Charney and Anthony P. Reeves. </author> <title> Correlation-Based Hardware Prefetching. </title> <note> Submitted to IEEE Transactions on Computers, </note> <month> September </month> <year> 1994. </year>
Reference: [DER86] <author> I. S. Duff, A.M. Erisman, and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford University Press, </publisher> <address> New York, NY, </address> <year> 1986. </year> <note> Printed in paperback (with corrections) 1989. </note>
Reference-contexts: In the linked-list traversal, the base address of each object retrieved from memory varies (as we step through the heap randomly); however, the offset within each object where the pointer to the next object is to be found is fixed. Numerous other sparse data structure representations exist <ref> [DER86] </ref>. Some use linked structures to index the sparse array, as in Figure 3, while others use indirection vectors for storing the indices of nonzero elements. An example of the latter representation is shown in Figure 4.
Reference: [DS95] <author> Fredrik Dahlgren and Per Stenstrom. </author> <title> Effectiveness of Hardware-Based Stride and Sequential Prefetching in Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the first IEEE Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 68-77, </pages> <month> January </month> <year> 1995. </year>
Reference: [EV93] <author> Richard J. Eickemeyer and S. Vassiliadis. </author> <title> A load-instruction unit for pipelined processors. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 37(4) </volume> <pages> 547-564, </pages> <month> July </month> <year> 1993. </year>
Reference: [FPJ92] <author> John W. C. Fu, Janak H. Patel, and Bob L. Janssens. </author> <title> Stride Directed Prefetching in Scalar Processors. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 102-110, </pages> <month> December </month> <year> 1992. </year>
Reference: [FTJ95] <author> Christine Fricker, Olivier Temam, and William Jalby. </author> <title> Influence of Cross-Interferences on Blocked Loops: A Case Study with Matrix-Vector Multiply. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(4) </volume> <pages> 562-575, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Temam and Jalby have provided a mathematical characterization of the cache behavior of sparse matrix-vector multiplication [TJ92]. Other analytic studies of cache behavior include <ref> [FTJ95, LRW91] </ref>. The branch prediction and classification problem has received extensive attention in the literature; [YGS95, CHYP94] are two recent studies.
Reference: [GGV90] <author> Edward H. Gornish, Elana D. Granston, and Alexander V. Veidenbaum. </author> <title> Compiler-directed data prefetching in multiprocessors with memory hierarchies. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <pages> pages 354-368, </pages> <institution> Department of Computer Science, Urbana, </institution> <address> IL 61801, </address> <month> June </month> <year> 1990. </year>
Reference: [Gor95] <author> Edward H. Gornish. </author> <title> Adaptive and integrated data cache prefetching for shared-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, Urbana, </institution> <address> IL 61801, </address> <month> January </month> <year> 1995. </year>
Reference: [Gwe92] <author> Linley Gwennap. </author> <title> Microprocessor Developments Beyond 1995: Experts See Limits to Superscalar Benefits | Memory Becomes Paramount. </title> <type> Microprocessor Report, </type> <month> 9 December </month> <year> 1992. </year>
Reference-contexts: 1 Introduction The ever-increasing gap between microprocessor and memory speeds has been well documented <ref> [Gwe92, HP90] </ref>. Instruction and data caches have become the principal means of bridging this speed discrepancy. Typically, first-level caches are small (8K to 32K bytes in size), direct mapped or modestly associative, and integrated on CPU chips 1 .
Reference: [HM94] <author> Luddy Harrison and Sharad Mehrotra. </author> <title> A data prefetch mechanism for accelerating general-purpose computation. </title> <type> Technical Report 1351, </type> <institution> CSRD, University of Illinois at Urbana-Champaign, Urbana, </institution> <address> IL 61801, </address> <month> 8 May </month> <year> 1994. </year> <note> 30 Last revised 9 March 1995. This report is the basis for Patent Applica--tion No. </note> <month> 08508290, </month> <title> Prefetch System Applicable to Complex Memory Access Schemes, </title> <note> filed by the University of Illinois on 27 July 1995. </note>
Reference-contexts: We have recently described a hardware data prefetch mechanism that is applicable to both pointer-intensive and numeric programs [MH95]. The basis for this device is a simple classification of memory access patterns in programs that we introduced earlier <ref> [HM94] </ref>. In this paper we take a close look at two codes from our suite, a grammar parser called Link-Gram, and the circuit simulation program spice2g6, and present a detailed analysis of them in the context of our model.
Reference: [HP90] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA 94403, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction The ever-increasing gap between microprocessor and memory speeds has been well documented <ref> [Gwe92, HP90] </ref>. Instruction and data caches have become the principal means of bridging this speed discrepancy. Typically, first-level caches are small (8K to 32K bytes in size), direct mapped or modestly associative, and integrated on CPU chips 1 . <p> Today's advanced processor organizations have achieved a canonical form | a decou-pled superscalar implementation. It is well known that the major obstacles to achieving higher performance in processor designs remain control and data hazards <ref> [HP90] </ref>. Consequently, numerous compiler and architecture research efforts continue to focus on reducing their detrimental effects. Current micro-architectural and compiler practices are most effective at enhancing pipeline core performance. In contrast, most well known data hazard resolution schemes are able to mask latencies of a few cycles at best. <p> This problem is particularly severe for load misses, since loads are involved in true data dependences (also known as flow or read-after-write dependences <ref> [HP90] </ref>), while write misses can often be masked with write buffering. 1 These design choices are made because the on-chip cache array access has to be completed within a single CPU clock cycle; the TLB lookup usually takes another cycle. 2 The DEC Alpha 21164 is an exception, since it integrates
Reference: [Jou90] <author> Norman P. Jouppi. </author> <title> Improving Direct-mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference: [JT93] <author> Ivan Jegou and Olivier Temam. </author> <title> Speculative Prefetching. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <pages> pages 57 - 66, </pages> <month> July </month> <year> 1993. </year>
Reference: [KL91] <author> Alexander C. Klaiber and Henry M. Levy. </author> <title> An architecture for software-controlled data prefetching. </title> <booktitle> In Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 43-53, </pages> <month> May </month> <year> 1991. </year>
Reference: [Lar93] <author> James R. Larus. </author> <title> Efficient Program Tracing. </title> <journal> IEEE Computer, </journal> <volume> 26(5) </volume> <pages> 52-61, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Further, the sparse data structures used in spice2g6 make it highly resistant to compile-time transformations. For these reasons, it is an interesting code to study. 5.2 Simulation methodology Both programs are compiled with standard optimization, and the resulting executables instrumented using Qpt <ref> [Lar93] </ref>. We have modified Qpt so that in addition to generating instruction and data traces, it also generates the contents of all memory locations that are read, a unique identifier (an integer) for each load when it executes, and the load op-code type (byte, half, or word load).
Reference: [LRW91] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: For some programs, particularly scientific codes operating on dense arrays or matrices, data reference pattern prediction is easy. Consequently, several hardware prefetch mechanisms have been proposed for such codes, and effective compiler techniques developed for them <ref> [LRW91, Mow94, BCJ + 94, CMT94] </ref>. Predicting the memory access patterns in pointer-intensive and sparse numerical computations is a much harder problem, and has received far less attention in the literature [TJ92, Sel92]. <p> Temam and Jalby have provided a mathematical characterization of the cache behavior of sparse matrix-vector multiplication [TJ92]. Other analytic studies of cache behavior include <ref> [FTJ95, LRW91] </ref>. The branch prediction and classification problem has received extensive attention in the literature; [YGS95, CHYP94] are two recent studies.
Reference: [LW94] <author> Alvin R. Lebeck and David A. Wood. </author> <title> Cache Profiling and the SPEC Benchmarks: A Case Study. </title> <journal> IEEE Computer, </journal> <volume> 27(10) </volume> <pages> 15-26, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Austin et al [APS95] profiled load instructions while developing software support for their fast address calculation mechanism. They reported aggregate data from their experiments, not individual instruction profiles. Lebeck and Wood used their CProf cache profiling system to analyze cache bottlenecks on a subset of the Spec92 codes <ref> [LW94] </ref>. They used the results to manually tune the codes using data structure and loop transformations. As mentioned earlier, many data prefetching schemes have been proposed in the literature, using hardware, software, or hybrid techniques. <p> This design decision allows us to allows us to avoid the complexities introduced by conflict misses <ref> [AR94, LW94] </ref>. We now examine the twenty most heavily missed loads in Link-Gram and spice2g6. This data is presented in Tables 3 and 4 respectively. Each entry in these two tables has eleven fields.
Reference: [MA93] <author> G. Massobrio and P. Antognetti. </author> <title> Semiconductor device modeling with SPICE. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <note> second edition, </note> <year> 1993. </year>
Reference-contexts: For this reason, the implementors chose a linked-list representation for the Y-matrix. For additional details of the spice2g6 implementation, consult Chapter 8 of <ref> [MA93] </ref>, and [Coh76]. This version of spice is the basis of several commercial re-implementations, and is one of the key tools used by a large portion of the semiconductor industry.
Reference: [MD88] <author> Geoffrey D. McNiven and Edward S. Davidson. </author> <title> Analysis of Memory Referencing Behavior For Design of Local Memories. </title> <booktitle> In Proceedings of the 15th International Symposium on Computer Architecture, </booktitle> <pages> pages 56-63, </pages> <address> Honolulu, HI, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: They were interested in using the data to construct more effective instruction scheduling algorithms, and to improve compile-time cache management. Selvidge had similar goals in the experiments he reported in his thesis [Sel92]. McNiven and Davidson made an early study of the data reference patterns in programs <ref> [MD88] </ref>. Austin et al [APS95] profiled load instructions while developing software support for their fast address calculation mechanism. They reported aggregate data from their experiments, not individual instruction profiles. Lebeck and Wood used their CProf cache profiling system to analyze cache bottlenecks on a subset of the Spec92 codes [LW94].
Reference: [MDO94] <author> Ann Marie Grizzaffi Maynard, Colette M. Donnelly, and Bret R. Olszewski. </author> <title> Contrasting Characteristics and Cache Performance of Technical and MultiUser Commercial Workloads. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 145-156, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Predicting the memory access patterns in pointer-intensive and sparse numerical computations is a much harder problem, and has received far less attention in the literature [TJ92, Sel92]. This is a significant omission, since both these types of codes generate memory access patterns that lead to poor data cache behavior <ref> [MDO94] </ref>. Examples of such computations include transaction processing, graphical user interfaces, text and language processing tools, operating systems, and finite element methods for computational fluid dynamics, structural engineering, etc.
Reference: [MH95] <author> Sharad Mehrotra and Luddy Harrison. </author> <title> Classifying the performance potential of a data-prefetch mechanism for pointer-intensive and numeric programs. </title> <type> Technical Report 1458, </type> <institution> CSRD, University of Illinois at Urbana-Champaign, Urbana, </institution> <address> IL 61801, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: This is because compiler transformations to improve CPU memory hierarchy performance are typically based upon dependence testing of linear array index expressions in Fortran loop nests. We have recently described a hardware data prefetch mechanism that is applicable to both pointer-intensive and numeric programs <ref> [MH95] </ref>. The basis for this device is a simple classification of memory access patterns in programs that we introduced earlier [HM94]. <p> In this section we briefly describe the IRB; see <ref> [MH95] </ref> for more details. The IRB is organized as two mutually cooperating sub-units: a recurrence recognition unit (RRU) and a prefetch unit (PU). <p> However, unlike Link-Gram, no load shows a dramatic reduction in miss count for spice2g6. There is additional experimental evidence to show that this miss covering property of the well predicted loads can be consistently exploited during prefetching, for several important codes <ref> [MH95] </ref>. 5.3.3 Analysis of code fragments To get a good understanding of the nature of the load misses in Link-Gram and spice2g6, and to understand why load prediction accuracy is highly variable, we will examine source code and dis-assembled MIPS assembly code for routines containing several of the loads from Tables
Reference: [Mow94] <author> Todd C. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Electrical Engineering, Stanford, </institution> <address> CA 94305, </address> <month> March </month> <year> 1994. </year> <month> 31 </month>
Reference-contexts: For some programs, particularly scientific codes operating on dense arrays or matrices, data reference pattern prediction is easy. Consequently, several hardware prefetch mechanisms have been proposed for such codes, and effective compiler techniques developed for them <ref> [LRW91, Mow94, BCJ + 94, CMT94] </ref>. Predicting the memory access patterns in pointer-intensive and sparse numerical computations is a much harder problem, and has received far less attention in the literature [TJ92, Sel92]. <p> Other analytic studies of cache behavior include [FTJ95, LRW91]. The branch prediction and classification problem has received extensive attention in the literature; [YGS95, CHYP94] are two recent studies. Finally, several compiler techniques for memory hierarchy management have been reported, for example <ref> [CMT94, BCJ + 94, Mow94, Sel92] </ref>. 3 Classifying load instructions This section describes our load classification model. We will illustrate it using code fragments written in C. First, consider the code in Figure 1 that performs a reduction on array a.
Reference: [PK94] <author> Subbarao Palacharla and Richard E. Kessler. </author> <title> Evaluating Stream Buffers as a Secondary Cache Replacement. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 24-33, </pages> <month> April </month> <year> 1994. </year>
Reference: [Sel92] <author> Charles William Selvidge. </author> <title> Compilation-Based Prefetching for Memory Latency Tolerance. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <address> Cambridge, MA 02139, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Consequently, several hardware prefetch mechanisms have been proposed for such codes, and effective compiler techniques developed for them [LRW91, Mow94, BCJ + 94, CMT94]. Predicting the memory access patterns in pointer-intensive and sparse numerical computations is a much harder problem, and has received far less attention in the literature <ref> [TJ92, Sel92] </ref>. This is a significant omission, since both these types of codes generate memory access patterns that lead to poor data cache behavior [MDO94]. <p> They reported results from the profiling of load instructions in the Spec89 benchmarks. They were interested in using the data to construct more effective instruction scheduling algorithms, and to improve compile-time cache management. Selvidge had similar goals in the experiments he reported in his thesis <ref> [Sel92] </ref>. McNiven and Davidson made an early study of the data reference patterns in programs [MD88]. Austin et al [APS95] profiled load instructions while developing software support for their fast address calculation mechanism. They reported aggregate data from their experiments, not individual instruction profiles. <p> Other analytic studies of cache behavior include [FTJ95, LRW91]. The branch prediction and classification problem has received extensive attention in the literature; [YGS95, CHYP94] are two recent studies. Finally, several compiler techniques for memory hierarchy management have been reported, for example <ref> [CMT94, BCJ + 94, Mow94, Sel92] </ref>. 3 Classifying load instructions This section describes our load classification model. We will illustrate it using code fragments written in C. First, consider the code in Figure 1 that performs a reduction on array a.
Reference: [SH92] <author> James E. Smith and Wei-Chung Hsu. </author> <title> Prefetching in Supercomputer Instruction Caches. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 588-597, </pages> <month> November </month> <year> 1992. </year>
Reference: [Smi78] <author> Alan Jay Smith. </author> <title> Sequential Program Prefetching in Memory Hierarchies. </title> <journal> IEEE Computer, </journal> <volume> 11(12) </volume> <pages> 7-21, </pages> <month> December </month> <year> 1978. </year>
Reference: [ST91] <author> Daniel D. Sleator and Davy Temperley. </author> <title> Parsing English with a Link Grammar. </title> <type> Technical Report CMU-CS-91-196, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA 15213, </address> <month> Octo-ber </month> <year> 1991. </year> <note> Version 2 of this code has just been released. Check http://bobo.link.cs.cmu.edu/grammar/html/intro.html. </note>
Reference-contexts: or indirect address pattern. 5 Experimental evaluation In this section we describe the target codes for this study, our experimental framework, and analysis and simulation results. 5.1 Code descriptions Link-Gram Our first program is a system for parsing the English language that is based upon the concept of link grammars <ref> [ST91] </ref>. A link grammar consists of a set of words, each of which has a linking requirement. <p> First, it is a prototypical symbolic code, but one that is not very long (approximately 12,800 lines C code). Second, it uses complex and interesting algorithms. Third, it appears to be quite effective at parsing complex English sentences and capturing several phenomena in the language <ref> [ST91] </ref>. Finally, it is the basis of several handwriting and voice-recognition tools currently being implemented in industry. We anticipate that such codes will consume ever-increasing numbers of processor cycles in future generation CPUs. spice2g6 This is the well known circuit simulation program developed at UC Berkeley.
Reference: [TJ92] <author> Olivier Temam and William Jalby. </author> <title> Characterizing the Behavior of Sparse Algorithms on Caches. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 578-587, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Consequently, several hardware prefetch mechanisms have been proposed for such codes, and effective compiler techniques developed for them [LRW91, Mow94, BCJ + 94, CMT94]. Predicting the memory access patterns in pointer-intensive and sparse numerical computations is a much harder problem, and has received far less attention in the literature <ref> [TJ92, Sel92] </ref>. This is a significant omission, since both these types of codes generate memory access patterns that lead to poor data cache behavior [MDO94]. <p> Cvetanovic and Bhandarkar [CB94] used performance monitoring hardware on a DEC Alpha system to produce a thorough instruction-level characterization of that machine running transaction processing and Spec92 workloads. Temam and Jalby have provided a mathematical characterization of the cache behavior of sparse matrix-vector multiplication <ref> [TJ92] </ref>. Other analytic studies of cache behavior include [FTJ95, LRW91]. The branch prediction and classification problem has received extensive attention in the literature; [YGS95, CHYP94] are two recent studies.
Reference: [VH92] <author> Sriram Vajapeyam and Wei-Chung Hsu. </author> <title> On the Instruction-Level Characteristics of Scalar Code in Highly-Vectorized Scientific Applications. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitec-ture, </booktitle> <pages> pages 20-28, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Some of these have provided classifications of load instructions, but almost always focused on scientific codes. None of these studies has proposed a model to explain load behavior across a broad range of programs, as our work does. In other studies, Vajapeyam and Hsu <ref> [VH92] </ref> reported basic block characteristics for scalar Cray Y-MP code. Barrett and Zorn [BZ93] measured the lifetimes of dynamically allocated objects to predict lifetimes of short-lived objects, so that they could build better storage allocators.
Reference: [YGHH94] <author> Yoji Yamada, John Gyllenhall, Grant Haab, and Wen-mei W. Hwu. </author> <title> Data Relocation and Prefetching for Programs with Large Data Sets. </title> <booktitle> In Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <month> November </month> <year> 1994. </year>
Reference: [YGS95] <author> Cliff Young, Nicolas Gloy, and Michael D. Smith. </author> <title> A Comparative Analysis of Schemes for Correlated Branch Prediction. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 276-286, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Temam and Jalby have provided a mathematical characterization of the cache behavior of sparse matrix-vector multiplication [TJ92]. Other analytic studies of cache behavior include [FTJ95, LRW91]. The branch prediction and classification problem has received extensive attention in the literature; <ref> [YGS95, CHYP94] </ref> are two recent studies. Finally, several compiler techniques for memory hierarchy management have been reported, for example [CMT94, BCJ + 94, Mow94, Sel92]. 3 Classifying load instructions This section describes our load classification model. We will illustrate it using code fragments written in C.
References-found: 41

