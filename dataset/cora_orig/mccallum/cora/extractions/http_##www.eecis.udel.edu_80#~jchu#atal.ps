URL: http://www.eecis.udel.edu:80/~jchu/atal.ps
Refering-URL: http://www.eecis.udel.edu:80/~jchu/
Root-URL: http://www.cis.udel.edu
Email: fjchu,carberryg@cis.udel.edu  
Title: Intelligent Agents Agent Theories, Architectures, and Languages, II, pp. 111-126 Conflict Detection and Resolution in
Author: Jennifer Chu-Carroll and Sandra Carberry 
Address: 19716, USA  
Affiliation: Department of Computer and Information Sciences University of Delaware Newark, DE  
Abstract: In multi-agent collaborative planning, since each agent is autonomous and heterogeneous, it is inevitable that conflicts arise among the agents during the planning process. A collaborative agent, however, must be capable of detecting and resolving these conflicts. This paper describes a computational model that captures the collaborative planning process in a Propose-Evaluate-Modify cycle of actions. Our model is capable of evaluating a given proposal to detect potential conflicts regarding both proposed actions and proposed beliefs, and of initiating collaborative negotiation subdialogues to resolve the detected conflicts. In situations where multiple conflicts arise, our model identifies the focus of the modification process and selects appropriate evidence to justify the necessity for such modification. Finally, our model handles the negotiation of proposed domain actions, proposed problem-solving actions, and proposed beliefs in a unified manner. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> James Allen. </author> <title> Discourse structure in the TRAINS project. </title> <booktitle> In Darpa Speech and Natural Language Workshop, </booktitle> <year> 1991. </year>
Reference-contexts: to resolve the detected conflicts, 3) select the most effective aspect to address in its pursuit of conflict resolution when multiple conflicts arise, and 4) select appropriate evidence to justify its beliefs during conflict resolution. 2 Modeling Collaborative Activities In multi-agent collaborative planning, the expected outcome is a shared plan <ref> [ 14, 1 ] </ref> that all the agents 1) believe will achieve their common goal, and 2) agree to adopt as the means to achieve this goal. <p> In utterance (1), S proposes a plan consisting of taking two courses this semester and three next semester. A evaluates this proposal based on her private beliefs <ref> [ 1 ] </ref> , decides that taking three courses this semester and two next semester is a better alternative than S's proposal, and in utterance (2) points out the disadvantage of S's proposal as a means of implicitly conveying her desire to modify S's proposal. <p> In addition, Rosenschein and Zlotkin [ 30 ] proposed a general theory char-acterizing the relationship between domains and appropriate negotiation mechanisms. These research efforts have focused on different aspects of conflict resolution from ours. Allen <ref> [ 1 ] </ref> proposed different plan modalities that capture the shared and individual beliefs during collaboration, but did not address strategies for conflict resolution should discrepancies in the agents' beliefs arise.
Reference: 2. <author> Alison Cawsey, Julia Galliers, Brian Logan, Steven Reece, and Karen Sparck Jones. </author> <title> Revising beliefs and intentions: A unified framework for agent interaction. </title> <booktitle> In The Ninth Biennial Conference of the Society for the Study of Artificial Intelligence and Simulation of Behaviour, </booktitle> <pages> pages 130-139, </pages> <year> 1993. </year>
Reference-contexts: However, these agents employ a belief revision mechanism to determine whether or not a new belief will be adopted, but are not capable of engaging in negotiation dialogues to square away the discrepancies between the agents' beliefs should conflicts arise. Cawsey et al., in developing their automated librarian <ref> [ 2, 21 ] </ref> , introduced the idea of utilizing a belief revision mechanism [ 10 ] to predict whether a given set of evidence is sufficient to change a user's existing belief.
Reference: 3. <author> Jennifer Chu-Carroll and Sandra Carberry. </author> <title> A plan-based model for response generation in collaborative task-oriented dialogues. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 799-805, </pages> <year> 1994. </year>
Reference-contexts: Based on our analysis of these dialogues, we have developed a model that captures collaborative planning activities in a Propose-Evaluate-Modify cycle of actions <ref> [ 3 ] </ref> , based on Sidner's model of proposal/acceptance and proposal/rejection sequences for collaborative discourse [ 31, 32 ] . <p> Agents also collaborate on their beliefs, forming a set of mutual beliefs that are relevant to the task at hand. Thus we use an enhanced version of the dialogue model described in [ 19 ] to capture the current intentions of the participants. The enhanced dialogue model <ref> [ 3 ] </ref> has four levels: domain, problem-solving, belief, and discourse. <p> The following sections briefly describe the processes for detecting conflicts in proposed actions and beliefs. For further details on the conflict detection processes, see <ref> [ 3, 6 ] </ref> . 3.1 Evaluating Proposed Actions Pollack [ 27 ] argued that a plan can be invalid because one of its actions is infeasible, meaning that the action cannot be performed by its agent, or because the plan itself is ill-formed, which occurs when a child action in
Reference: 4. <author> Jennifer Chu-Carroll and Sandra Carberry. </author> <title> Communication for conflict resolution in multi-agent collaborative planning. </title> <booktitle> In Proceedings of the First International Conference on Mul-tiagent Systems, </booktitle> <pages> pages 49-56, </pages> <year> 1995. </year>
Reference-contexts: They include the university course advisement domain where an advisor and a student collaborate on developing a plan to achieve the student's domain goal (for examples in this domain, see <ref> [ 4 ] </ref> ), the battleship domain in which multiple strategists collaborate on developing the best attack/defense tactics, and so on.
Reference: 5. <author> Jennifer Chu-Carroll and Sandra Carberry. </author> <title> Generating information-sharing subdialogues in expert-user consultation. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: In situations where an agent does not have sufficient information to determine whether to accept a proposal, she will initiate an information-sharing subdialogue to exchange information with the other agent (s) in order to re-evaluate the proposal. The generation of these information-sharing subdialogues is discussed in <ref> [ 5 ] </ref> . better destination for re-routing RA220.
Reference: 6. <author> Jennifer Chu-Carroll and Sandra Carberry. </author> <title> Response generation in collaborative negotiation. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 136-143, </pages> <year> 1995. </year>
Reference-contexts: The following sections briefly describe the processes for detecting conflicts in proposed actions and beliefs. For further details on the conflict detection processes, see <ref> [ 3, 6 ] </ref> . 3.1 Evaluating Proposed Actions Pollack [ 27 ] argued that a plan can be invalid because one of its actions is infeasible, meaning that the action cannot be performed by its agent, or because the plan itself is ill-formed, which occurs when a child action in <p> Grice's maxim of quantity [ 11 ] specifies that one should not contribute more information than is required. 7 Thus, it is important that a collaborative agent select sufficient and effective, but not excessive, evidence to justify an intended mutual belief. 6 For a full account of this algorithm, see <ref> [ 6 ] </ref> . 7 Walker [ 37 ] has shown the importance of IRU's (Informationally Redundant Utterances) in efficient discourse, but we leave including appropriate IRU's for future work. Select-Focus-Modification ( bel): 1. If bel is a leaf node, return bel along with the agent's evidence against bel. 2.
Reference: 7. <institution> Columbia University Transcripts. Transcripts derived from audiotape conversations made at Columbia University, </institution> <address> New York, NY. </address> <note> Provided by Kathleen McKeown, </note> <year> 1985. </year>
Reference-contexts: In order to model collaborative planning processes, we have analyzed collaborative planning discourse in a university course advisement domain <ref> [ 7 ] </ref> , a travel planning domain [ 33 ] , and a simulated cargo shipping domain [ 12 ] . <p> To illustrate how the Propose-Evaluate-Modify cycle of actions captures planning processes between collaborative agents, consider the following dialogue segment based on a transcript of naturally occurring course-advisement dialogues <ref> [ 7 ] </ref> in which an advisor (A) and a student (S) are collaborating on planning the student's schedule: (1) S: I was going to say two [courses] this time and then three next time. (2) A: And if you take two and then don't pass one, you also would be
Reference: 8. <author> Susan E. Conry, Robert A. Meyer, and Victor R. Lesser. </author> <title> Multistage negotiation in distributed planning. </title> <editor> In Alan H. Bond and Les Gasser, editors, </editor> <booktitle> Readings in Distributed Artificial Intelligence, </booktitle> <pages> pages 367-383. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1988. </year>
Reference-contexts: will be incorporated into the existing model and the planning process will continue. 5 Related Work Researchers in distributed AI have developed conflict resolution strategies for various types of conflicts, including resolving conflicting goals between non-fully cooperative agents [ 34 ] , resolving conflicts in resource allocation among cooperative agents <ref> [ 8 ] </ref> , resolving conflicts between coordinating subproblems in distributed problem-solving [ 20 ] , etc. In addition, Rosenschein and Zlotkin [ 30 ] proposed a general theory char-acterizing the relationship between domains and appropriate negotiation mechanisms.
Reference: 9. <author> Stephanie Elzer, Jennifer Chu-Carroll, and Sandra Carberry. </author> <title> Constructing and utilizing a model of user preferences in collaborative consultation dialogues. </title> <journal> Computational Intelligence. </journal> <note> To appear. </note>
Reference-contexts: Thus we evaluate the optimality of a plan with respect to the preferences of the agent (s) performing the actions <ref> [ 9 ] </ref> . 2 The evaluation of domain and problem-solving actions is therefore a top-down process which terminates as soon as an infeasible action, an invalid parent-child relationship, or a suboptimal action is found, since conflicts about child actions are irrelevant if the parent action is not accepted.
Reference: 10. <author> Julia R. Galliers. </author> <title> Autonomous belief revision and communication. In Gardenfors, editor, Belief Revision. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: The evaluation of belief trees is a bottom-up process, because acceptance of a proposed belief, bel, may be influenced by acceptance of the beliefs represented by its children in the belief tree, which are intended to provide support for bel. A simplified version of Galliers' belief revision mechanism <ref> [ 10, 21 ] </ref> is used to determine whether or not a belief or evidential relationship should be accepted. <p> Notice that step 3 of the algorithm invokes a function, Predict, which predicts whether or not the other agent's belief in a proposition will change after a set of evidence is presented to him. Predict makes use of the belief revision mechanism <ref> [ 10 ] </ref> discussed in Section 3.2 to predict whether the other agent will adopt bel based on the agent's knowledge of the other agent's beliefs and the evidence to be presented to him [ 21 ] . <p> Cawsey et al., in developing their automated librarian [ 2, 21 ] , introduced the idea of utilizing a belief revision mechanism <ref> [ 10 ] </ref> to predict whether a given set of evidence is sufficient to change a user's existing belief.
Reference: 11. <author> H. Paul Grice. </author> <title> Logic and conversation. </title> <editor> In Peter Cole and Jerry L. Morgan, editors, </editor> <title> Syntax and Semantics 3: </title> <booktitle> Speech Acts, </booktitle> <pages> pages 41-58. </pages> <publisher> Academic Press, Inc., </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects [ 28 ] . On the other hand, Grice's maxim of quantity <ref> [ 11 ] </ref> specifies that one should not contribute more information than is required. 7 Thus, it is important that a collaborative agent select sufficient and effective, but not excessive, evidence to justify an intended mutual belief. 6 For a full account of this algorithm, see [ 6 ] . 7
Reference: 12. <author> Derek Gross, James F. Allen, and David R. Traum. </author> <title> The TRAINS 91 dialogues. </title> <type> Technical Report TN92-1, </type> <institution> Department of Computer Science, University of Rochester, </institution> <year> 1993. </year>
Reference-contexts: In order to model collaborative planning processes, we have analyzed collaborative planning discourse in a university course advisement domain [ 7 ] , a travel planning domain [ 33 ] , and a simulated cargo shipping domain <ref> [ 12 ] </ref> . An important common feature of these planning domains is that the participating agents collaborate on developing the best plan for achieving their shared goal in terms of the interest of the agents as a group, instead of attempting to maximize each agent's own benefits.
Reference: 13. <author> Barbara Grosz and Sarit Kraus. </author> <title> Collaborative plans for group activities. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1993. </year>
Reference-contexts: Grosz, Sidner and Kraus developed a SharedPlan model for collaborative activities which captures the agents' intentions in developing a plan to be carried out jointly by the agents <ref> [ 14, 13 ] </ref> . However, in their model the agents will avoid adopting conflicting intentions, instead of trying to resolve them.
Reference: 14. <author> Barbara J. Grosz and Candace L. Sidner. </author> <title> Plans for discourse. </title> <editor> In Cohen, Morgan, and Pollack, editors, </editor> <booktitle> Intentions in Communication, chapter 20, </booktitle> <pages> pages 417-444. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: to resolve the detected conflicts, 3) select the most effective aspect to address in its pursuit of conflict resolution when multiple conflicts arise, and 4) select appropriate evidence to justify its beliefs during conflict resolution. 2 Modeling Collaborative Activities In multi-agent collaborative planning, the expected outcome is a shared plan <ref> [ 14, 1 ] </ref> that all the agents 1) believe will achieve their common goal, and 2) agree to adopt as the means to achieve this goal. <p> Grosz, Sidner and Kraus developed a SharedPlan model for collaborative activities which captures the agents' intentions in developing a plan to be carried out jointly by the agents <ref> [ 14, 13 ] </ref> . However, in their model the agents will avoid adopting conflicting intentions, instead of trying to resolve them.
Reference: 15. <author> Dale Hample. </author> <title> Refinements on the cognitive model of argument: Concreteness, involvement and group scores. </title> <journal> The Western Journal of Speech Communication, </journal> <volume> 49 </volume> <pages> 267-285, </pages> <year> 1985. </year>
Reference-contexts: Selecting Justification for a Claim Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message <ref> [ 22, 29, 26, 15 ] </ref> . Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects [ 28 ] .
Reference: 16. <author> Michael N. Huhns and David M. Bridgeland. </author> <title> Multiagent truth maintenance. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(6) </volume> <pages> 1437-1445, </pages> <year> 1991. </year>
Reference-contexts: Sidner [ 31, 32 ] formulated an artificial language for modeling collaborative discourse; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Huhns and Bridgeland <ref> [ 16 ] </ref> as well as Malheiro et al. [ 24, 23 ] utilized Truth Maintenance Systems for maintaining the coherence of an agent's private beliefs [ 24 ] and, in addition, coherence among agents' beliefs [ 16, 23 ] . <p> Huhns and Bridgeland [ 16 ] as well as Malheiro et al. [ 24, 23 ] utilized Truth Maintenance Systems for maintaining the coherence of an agent's private beliefs [ 24 ] and, in addition, coherence among agents' beliefs <ref> [ 16, 23 ] </ref> . However, these agents employ a belief revision mechanism to determine whether or not a new belief will be adopted, but are not capable of engaging in negotiation dialogues to square away the discrepancies between the agents' beliefs should conflicts arise.
Reference: 17. <author> Aravind Joshi, Bonnie Webber, and Ralph M. Weischedel. </author> <title> Living up to expectations: Computing expert responses. </title> <booktitle> In Proceedings of the Fourth National Conference on Artificial Intelligence, </booktitle> <pages> pages 169-175, </pages> <year> 1984. </year>
Reference-contexts: Furthermore, Joshi et al. <ref> [ 17 ] </ref> and van Beek [ 35 ] contended that a system cannot be considered cooperative if it knows of a better alternative to the user's proposal but does not suggest it to the user.
Reference: 18. <author> Aravind K. Joshi. </author> <title> Mutual beliefs in question-answer systems. In N.V. Smith, editor, Mutual Knowledge, </title> <booktitle> chapter 4, </booktitle> <pages> pages 181-197. </pages> <publisher> Academic Press, </publisher> <year> 1982. </year>
Reference-contexts: It is the attempt to satisfy this precondition that leads to the generation of natural language utterances to square away <ref> [ 18 ] </ref> the conflict in the agents' beliefs. action can only be performed when s1 believes that node is not acceptable while s2 believes that it is (when s1 and s2 disagree about the feasibility of node when it is instantiated as an action, or about the truth of node
Reference: 19. <author> Lynn Lambert and Sandra Carberry. </author> <title> A tripartite plan-based model of dialogue. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 47-54, </pages> <year> 1991. </year>
Reference-contexts: For example, the agents might negotiate the strategies they will use to construct the domain plan. Agents also collaborate on their beliefs, forming a set of mutual beliefs that are relevant to the task at hand. Thus we use an enhanced version of the dialogue model described in <ref> [ 19 ] </ref> to capture the current intentions of the participants. The enhanced dialogue model [ 3 ] has four levels: domain, problem-solving, belief, and discourse.
Reference: 20. <author> Susan E. Lander and Victor R. Lesser. </author> <title> Negotiated search: Cooperative search among heterogeneous expert agents. </title> <booktitle> In AAAI-92 Workshop: Cooperation Among Heterogeneous Intelligent Systems, </booktitle> <pages> pages 74-83, </pages> <year> 1992. </year>
Reference-contexts: continue. 5 Related Work Researchers in distributed AI have developed conflict resolution strategies for various types of conflicts, including resolving conflicting goals between non-fully cooperative agents [ 34 ] , resolving conflicts in resource allocation among cooperative agents [ 8 ] , resolving conflicts between coordinating subproblems in distributed problem-solving <ref> [ 20 ] </ref> , etc. In addition, Rosenschein and Zlotkin [ 30 ] proposed a general theory char-acterizing the relationship between domains and appropriate negotiation mechanisms. These research efforts have focused on different aspects of conflict resolution from ours.
Reference: 21. <author> Brian Logan, Steven Reece, Alison Cawsey, Julia Galliers, and Karen Sparck Jones. </author> <title> Belief revision and dialogue management in information retrieval. </title> <type> Technical Report 339, </type> <institution> University of Cambridge, Computer Laboratory, </institution> <year> 1994. </year>
Reference-contexts: The evaluation of belief trees is a bottom-up process, because acceptance of a proposed belief, bel, may be influenced by acceptance of the beliefs represented by its children in the belief tree, which are intended to provide support for bel. A simplified version of Galliers' belief revision mechanism <ref> [ 10, 21 ] </ref> is used to determine whether or not a belief or evidential relationship should be accepted. <p> Predict makes use of the belief revision mechanism [ 10 ] discussed in Section 3.2 to predict whether the other agent will adopt bel based on the agent's knowledge of the other agent's beliefs and the evidence to be presented to him <ref> [ 21 ] </ref> . Selecting Justification for a Claim Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message [ 22, 29, 26, 15 ] . <p> However, these agents employ a belief revision mechanism to determine whether or not a new belief will be adopted, but are not capable of engaging in negotiation dialogues to square away the discrepancies between the agents' beliefs should conflicts arise. Cawsey et al., in developing their automated librarian <ref> [ 2, 21 ] </ref> , introduced the idea of utilizing a belief revision mechanism [ 10 ] to predict whether a given set of evidence is sufficient to change a user's existing belief. <p> They argued that in the information retrieval dialogues they analyzed, in no cases does negotiation extend beyond the initial belief conflict and its immediate resolution. ( <ref> [ 21 ] </ref> , page 141); thus they do not provide a mechanism for collaborative negotiation.
Reference: 22. <author> Joseph A. Luchok and James C. McCroskey. </author> <title> The effect of quality of evidence on attitude change and source credibility. </title> <journal> The Southern Speech Communication Journal, </journal> <volume> 43 </volume> <pages> 371-383, </pages> <year> 1978. </year>
Reference-contexts: Selecting Justification for a Claim Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message <ref> [ 22, 29, 26, 15 ] </ref> . Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects [ 28 ] . <p> This produces a set of possible candidate justifications, and heuristics are then applied to select from among the possible justifications. The first heuristic prefers evidence in which agent A is most confident since high-quality evidence produces more attitude change than any other evidence form <ref> [ 22 ] </ref> . Furthermore, agent A can better justify a belief in which she has high confidence should agent B not accept it.
Reference: 23. <author> B. Malheiro and E. Oliveira. </author> <title> Consistency and context management in a multi-agent belief revision testbed. </title> <editor> In M. Wooldridge, J. P. M uller, and M. Tambe, editors, </editor> <booktitle> Intelligent Agents Volume II Proceedings of the 1995 Workshop on Agent Theories, Architectures, and Languages (ATAL-95), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> (In this volume). </note>
Reference-contexts: Sidner [ 31, 32 ] formulated an artificial language for modeling collaborative discourse; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Huhns and Bridgeland [ 16 ] as well as Malheiro et al. <ref> [ 24, 23 ] </ref> utilized Truth Maintenance Systems for maintaining the coherence of an agent's private beliefs [ 24 ] and, in addition, coherence among agents' beliefs [ 16, 23 ] . <p> Huhns and Bridgeland [ 16 ] as well as Malheiro et al. [ 24, 23 ] utilized Truth Maintenance Systems for maintaining the coherence of an agent's private beliefs [ 24 ] and, in addition, coherence among agents' beliefs <ref> [ 16, 23 ] </ref> . However, these agents employ a belief revision mechanism to determine whether or not a new belief will be adopted, but are not capable of engaging in negotiation dialogues to square away the discrepancies between the agents' beliefs should conflicts arise.
Reference: 24. <author> Benedita Malheiro, Nicholas R. Jennings, and Eugenio Oliveira. </author> <title> Belief revision in multi-agent systems. </title> <booktitle> In Proceedings of the 11th European Conference on Artificial Intelligence, </booktitle> <pages> pages 294-298, </pages> <year> 1994. </year>
Reference-contexts: Sidner [ 31, 32 ] formulated an artificial language for modeling collaborative discourse; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Huhns and Bridgeland [ 16 ] as well as Malheiro et al. <ref> [ 24, 23 ] </ref> utilized Truth Maintenance Systems for maintaining the coherence of an agent's private beliefs [ 24 ] and, in addition, coherence among agents' beliefs [ 16, 23 ] . <p> Huhns and Bridgeland [ 16 ] as well as Malheiro et al. [ 24, 23 ] utilized Truth Maintenance Systems for maintaining the coherence of an agent's private beliefs <ref> [ 24 ] </ref> and, in addition, coherence among agents' beliefs [ 16, 23 ] .
Reference: 25. <author> Donald D. Morley. </author> <title> Subjective message constructs: A theory of persuasion. </title> <journal> Communication Monographs, </journal> <volume> 54 </volume> <pages> 183-203, </pages> <year> 1987. </year>
Reference-contexts: Furthermore, agent A can better justify a belief in which she has high confidence should agent B not accept it. Wyer [ 39 ] and Morley <ref> [ 25 ] </ref> argued that evidence is most persuasive if it is previously unknown to the hearer; thus, the second heuristic prefers evidence that is novel to agent B.
Reference: 26. <author> Richard E. Petty and John T. Cacioppo. </author> <title> The effects of involvement on responses to argument quantity and quality: Central and peripheral routes to persuasion. </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> 46(1) </volume> <pages> 69-81, </pages> <year> 1984. </year>
Reference-contexts: Selecting Justification for a Claim Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message <ref> [ 22, 29, 26, 15 ] </ref> . Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects [ 28 ] .
Reference: 27. <author> Martha E. Pollack. </author> <title> A model of plan inference that distinguishes between the beliefs of actors and observers. </title> <booktitle> In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 207-214, </pages> <year> 1986. </year>
Reference-contexts: The following sections briefly describe the processes for detecting conflicts in proposed actions and beliefs. For further details on the conflict detection processes, see [ 3, 6 ] . 3.1 Evaluating Proposed Actions Pollack <ref> [ 27 ] </ref> argued that a plan can be invalid because one of its actions is infeasible, meaning that the action cannot be performed by its agent, or because the plan itself is ill-formed, which occurs when a child action in the plan does not contribute to its parent action as <p> proposed belief. 4 The process starts at the leaf nodes of the proposed 2 In the air-traffic control domain, these preferences can be viewed as qualities of a resultant plan that are generally preferred by most agents, such as preferring re-routing flights to the nearest possible destination. 3 A recipe <ref> [ 27 ] </ref> is a template for performing an action.
Reference: 28. <author> John C. Reinard. </author> <title> The empirical study of the persuasive effects of evidence, the status after fifty years of research. </title> <journal> Human Communication Research, </journal> <volume> 15(1) </volume> <pages> 3-59, </pages> <year> 1988. </year>
Reference-contexts: Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects <ref> [ 28 ] </ref> .
Reference: 29. <author> Rodney A. Reynolds and Michael Burgoon. </author> <title> Belief processing, reasoning, and evidence. In Bostrom, editor, </title> <type> Communication Yearbook 7, chapter 4, </type> <pages> pages 83-104. </pages> <publisher> Sage Publications, </publisher> <year> 1983. </year>
Reference-contexts: Selecting Justification for a Claim Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message <ref> [ 22, 29, 26, 15 ] </ref> . Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects [ 28 ] .
Reference: 30. <author> Jeffery S. Rosenschein and Gilad Zlotkin. </author> <title> Rules of Encounter Designing Conventions for Automated Negotiation among Computers. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: In addition, Rosenschein and Zlotkin <ref> [ 30 ] </ref> proposed a general theory char-acterizing the relationship between domains and appropriate negotiation mechanisms. These research efforts have focused on different aspects of conflict resolution from ours.
Reference: 31. <author> Candace L. Sidner. </author> <title> Using discourse to negotiate in collaborative activity: </title> <booktitle> An artificial language. In AAAI-92 Workshop: Cooperation Among Heterogeneous Intelligent Systems, </booktitle> <pages> pages 121-128, </pages> <year> 1992. </year>
Reference-contexts: Based on our analysis of these dialogues, we have developed a model that captures collaborative planning activities in a Propose-Evaluate-Modify cycle of actions [ 3 ] , based on Sidner's model of proposal/acceptance and proposal/rejection sequences for collaborative discourse <ref> [ 31, 32 ] </ref> . <p> However, in their model the agents will avoid adopting conflicting intentions, instead of trying to resolve them. Sidner <ref> [ 31, 32 ] </ref> formulated an artificial language for modeling collaborative discourse; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions.
Reference: 32. <author> Candace L. Sidner. </author> <title> An artificial discourse language for collaborative negotiation. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 814-819, </pages> <year> 1994. </year>
Reference-contexts: Based on our analysis of these dialogues, we have developed a model that captures collaborative planning activities in a Propose-Evaluate-Modify cycle of actions [ 3 ] , based on Sidner's model of proposal/acceptance and proposal/rejection sequences for collaborative discourse <ref> [ 31, 32 ] </ref> . <p> When presented with a proposal, a collaborative agent needs to decide whether or not she will accept the proposal and make it part of the shared plan being developed by the agents <ref> [ 32 ] </ref> . In this evaluation process, the agent must decide whether or not the proposed actions will contribute to a valid and reasonably efficient way to achieve their shared domain goal, as well as whether or not the proposed beliefs conflict with her existing beliefs. <p> However, in their model the agents will avoid adopting conflicting intentions, instead of trying to resolve them. Sidner <ref> [ 31, 32 ] </ref> formulated an artificial language for modeling collaborative discourse; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions.
Reference: 33. <author> SRI Transcripts. </author> <title> Transcripts derived from audiotape conversations made at SRI International, </title> <address> Menlo Park, CA, </address> <year> 1992. </year> <title> Prepared by Jacqueline Kowtko under the direction of Patti Price. </title>
Reference-contexts: In order to model collaborative planning processes, we have analyzed collaborative planning discourse in a university course advisement domain [ 7 ] , a travel planning domain <ref> [ 33 ] </ref> , and a simulated cargo shipping domain [ 12 ] .
Reference: 34. <author> Katia Sycara. </author> <title> Argumentation: Planning other agents' plans. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 517-523, </pages> <year> 1989. </year>
Reference-contexts: This results in collaborative negotiation. Such negotiation differs from other kinds of negotiation, such as labor negotiation <ref> [ 34 ] </ref> , in that the agents are not trying to enforce their views on one another or to maximize their own benefits, but rather are trying to share their individual knowledge and beliefs in order to determine what really is best. <p> Since the proposed additions have been agreed upon by both agents, they will be incorporated into the existing model and the planning process will continue. 5 Related Work Researchers in distributed AI have developed conflict resolution strategies for various types of conflicts, including resolving conflicting goals between non-fully cooperative agents <ref> [ 34 ] </ref> , resolving conflicts in resource allocation among cooperative agents [ 8 ] , resolving conflicts between coordinating subproblems in distributed problem-solving [ 20 ] , etc. In addition, Rosenschein and Zlotkin [ 30 ] proposed a general theory char-acterizing the relationship between domains and appropriate negotiation mechanisms. <p> On the other hand, in non-collaborative environments, such as labor negotiation <ref> [ 34 ] </ref> , each agent's goal is to maximize his own interests while attempting to reach an agreement with the other agents.
Reference: 35. <author> Peter G. van Beek. </author> <title> A model for generating better explanations. </title> <booktitle> In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 215-220, </pages> <address> Stanford, CA, </address> <year> 1987. </year>
Reference-contexts: Furthermore, Joshi et al. [ 17 ] and van Beek <ref> [ 35 ] </ref> contended that a system cannot be considered cooperative if it knows of a better alternative to the user's proposal but does not suggest it to the user.
Reference: 36. <author> Marilyn A. Walker. </author> <title> Redundancy in collaborative dialogue. </title> <booktitle> In Proceedings of the 15th International Conference on Computational Linguistics, </booktitle> <pages> pages 345-351, </pages> <year> 1992. </year>
Reference-contexts: It is essential that the agents be able to detect and resolve such conflicts as soon as they arise <ref> [ 36 ] </ref> in order to prevent them from working on developing different plans. This paper presents a model for a computational agent that is capable of detecting and resolving conflicts during a collaborative planning process, such as in the scenario described above. <p> proposed belief even though the proposed evidence is accepted. 4 Collaborative Negotiation for Conflict Resolution Once a collaborative agent detects a relevant conflict, she must notify the other agent of the conflict and attempt to resolve it to do otherwise is to fail in her responsibilities as a collaborative participant <ref> [ 36 ] </ref> . This results in collaborative negotiation.
Reference: 37. <author> Marilyn A. Walker. </author> <title> Discourse and deliberation: Testing a collaborative strategy. </title> <booktitle> In Proceedings of the 15th International Conference on Computational Linguistics, </booktitle> <year> 1994. </year>
Reference-contexts: ] specifies that one should not contribute more information than is required. 7 Thus, it is important that a collaborative agent select sufficient and effective, but not excessive, evidence to justify an intended mutual belief. 6 For a full account of this algorithm, see [ 6 ] . 7 Walker <ref> [ 37 ] </ref> has shown the importance of IRU's (Informationally Redundant Utterances) in efficient discourse, but we leave including appropriate IRU's for future work. Select-Focus-Modification ( bel): 1. If bel is a leaf node, return bel along with the agent's evidence against bel. 2.
Reference: 38. <author> Michael Wooldridge and Nicholas R. Jennings. </author> <title> Intelligent agents: </title> <journal> Theory and practice. Knowledge Engineering Review, </journal> <volume> 10(2), </volume> <year> 1995. </year>
Reference-contexts: 1 Introduction With the development of intelligent agents, Wooldridge and Jennings <ref> [ 38 ] </ref> envision the following scenario occurring sometime in the future: The key air-traffic control systems in the country of Ruritania suddenly fail, due to freak weather conditions.
Reference: 39. <author> Robert S. Wyer, Jr. </author> <title> Information redundancy, inconsistency, and novelty and their role in impression formation. </title> <journal> Journal of Experimental Social Psychology, </journal> <volume> 6 </volume> <pages> 111-127, </pages> <year> 1970. </year>
Reference-contexts: The first heuristic prefers evidence in which agent A is most confident since high-quality evidence produces more attitude change than any other evidence form [ 22 ] . Furthermore, agent A can better justify a belief in which she has high confidence should agent B not accept it. Wyer <ref> [ 39 ] </ref> and Morley [ 25 ] argued that evidence is most persuasive if it is previously unknown to the hearer; thus, the second heuristic prefers evidence that is novel to agent B.
Reference: 40. <author> R. Michael Young, Johanna D. Moore, and Martha E. Pollack. </author> <title> Towards a principled representation of discourse plans. </title> <booktitle> In Proceedings of the Sixteenth Annual Meeting of the Cognitive Science Society, </booktitle> <pages> pages 946-951, </pages> <year> 1994. </year>
Reference-contexts: Conflict resolution and negotiation are necessary only if the top-level proposed beliefs are not accepted since if the agents agree on a particular belief relevant to the domain plan being constructed, it is irrelevant whether they both agree on all the evidence for that belief <ref> [ 40 ] </ref> .
References-found: 40

