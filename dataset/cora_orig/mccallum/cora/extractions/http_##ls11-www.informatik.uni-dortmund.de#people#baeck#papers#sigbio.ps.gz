URL: http://ls11-www.informatik.uni-dortmund.de/people/baeck/papers/sigbio.ps.gz
Refering-URL: http://ls11-www.informatik.uni-dortmund.de/people/baeck/ea_general.html
Root-URL: http://ls11-www.informatik.uni-dortmund.de/people/baeck/ea_general.html
Title: Evolutionary Algorithms  
Author: Thomas Back 
Address: P.O. Box 50 05 00 4600 Dortmund 50 Germany  
Affiliation: University of Dortmund Department of Computer Science  
Abstract: Genetic Algorithms and Evolution Strategies, the main representatives of a class of algorithms based on the model of natural evolution, are discussed w.r.t. their basic working mechanisms, differences, and application possibilities. The mechanism of self-adaptation of strategy parameters within Evolution Strategies is emphasized and turns out to be the major difference to Genetic Algorithms, since it allows for an on-line adaptation of strategy parameters without exogenous control. Furthermore, the importance of selection as a mechanism to control the character of the search to be either more volume oriented or more path oriented is pointed out. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Back and F. Hoffmeister. </author> <title> Extended selection mechanisms in genetic algorithms. </title> <booktitle> In Belew and Booker [5], </booktitle> <pages> pages 92-99. </pages>
Reference-contexts: A first comparison of the "recombined" selection mechanisms gives evidence for the statement, that the extinctiveness is the main characteristic feature for determining the search properties of Evolutionary Algorithms <ref> [1] </ref>, and that the degree of extinctiveness (i.e. the ration of =) serves as an instrument to lay emphasis on convergence velocity or convergence reliability, i.e. a high probability to locate the global optimum.
Reference: [2] <author> T. Back, F. Hoffmeister, and H.-P. Schwefel. </author> <title> A survey of evolution strategies. </title> <booktitle> In Belew and Booker [5], </booktitle> <pages> pages 2-9. </pages>
Reference-contexts: This can be achieved by introducing correlated mutations via incorporating the covariances of the standard deviations into the individuals. Then, up to n (n1)=2 additional strategy parameters are added to the genotype, and mutation and recombination are extended w.r.t. the covariances <ref> [2] </ref>. Recombination in ESs with &gt; 1 is always done on the whole parent population. The standard mechanisms are discrete and intermediate recombination.
Reference: [3] <author> T. Back, F. Hoffmeister, and H.-P. Schwefel. </author> <title> Applications of evolutionary algorithms. Report of the Systems Analysis Research Group (LS XI) SYS-2/92, </title> <institution> University of Dortmund, Department of Computer Science, </institution> <year> 1992. </year> <note> In print. </note>
Reference-contexts: variety of real world applications as well as research problems, including such different tasks as the optimization of neural networks (topology and weights), job shop scheduling and sequencing problems, dynamic control, game theory, data analysis, and computer aided optimal design of many technical products (for a list of EA-applications see <ref> [3] </ref>). The importance of these algorithms will increase further with the dissemination of massively parallel hardware, since besides performing objective function evaluations in parallel by using a master-slave approach, the population can also easily be split up coarse-grained into parallel subpopulations or fine-grained into parallel individuals.
Reference: [4] <author> J. E. Baker. </author> <title> Adaptive selection methods for genetic algorithms. </title> <booktitle> In Grefenstette [9], </booktitle> <pages> pages 101-111. </pages>
Reference-contexts: This way, selection in EAs provides the active driving force for improvement. Just a look at proportional selection in GAs gives a first impression of the importance of selection, since there is a still ongoing discussion of advantages and disadvantages of this selection mechanism <ref> [4, 19, 8] </ref>. One of the disadvantages is the problem of superindi-viduals, i.e. individuals having so much better fitness values than the remainder of the population, that their offspring quickly dominate the population, leading to a loss of genetic diversity. This loss of diversity is often called premature convergence. <p> Deterministically selecting the best out of individuals eliminates the need for a scaling mechanism. For GAs, a rank-based selection mechanism has also been proposed by Baker <ref> [4] </ref>, who uses a statically fixed linear function to assign decreasing selection probabilities to individuals sorted by fitness, such that the best (worst) individual receives the highest (lowest) selection probability (linear ranking).
Reference: [5] <editor> R. K. Belew and L. B. Booker, editors. </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms and their Applications, </booktitle> <institution> University of California, </institution> <address> San Diego, USA, 1991. </address> <publisher> Mor-gan Kaufmann Publishers. </publisher>
Reference: [6] <author> M. L. Cramer. </author> <title> A representation for the adaptive generation of simple sequential programs. </title> <booktitle> In Grefenstette [9], </booktitle> <pages> pages 183-187. </pages>
Reference-contexts: Several of these Evolutionary Algorithms are nowadays broadly accepted, the most important ones being the Genetic Algorithm (GA) and Classifier System (CFS) by Hol-land [12, 13], the Evolution Strategy (ES) by Rechen-berg and Schwefel [16, 18], and the Genetic Programming Paradigm (GPP) <ref> [6, 15] </ref>. Each of these approaches is based upon the collective learning process within a population of individuals, each of which represents a search point in the space of potential solutions to a given problem. The start population is usually initialized at random.
Reference: [7] <author> D. E. Goldberg. </author> <title> Genetic algorithms in search, optimization and machine learning. </title> <publisher> Addison Wes-ley, </publisher> <year> 1989. </year>
Reference-contexts: (a i ) = minff 0 (a j ) j a j 2 P (t)g + f 0 (a i ) in case of a maximization task (for minimization: f (a i ) = maxff 0 (a j ) j a j 2 P (t)g f 0 (a i )) <ref> [7] </ref>. After selection, the frequencies of better individuals have increased at the expense of worse individuals, but no new search points are created so far. Recombination and mutation are used to achieve this. <p> Recombination and mutation are used to achieve this. For recombination ! c : I fi I ! I, which exchanges information between individuals, a large amount of crossover operators is described in the literature (e.g. see <ref> [7] </ref>). <p> Building blocks and their combination to form longer and longer useful substrings are the most important working mechanism of a GA. Therefore, consideration of the schema theorem and the building block hypothesis are the main design criteria for applying a GA to a certain problem <ref> [7] </ref>. 3 Evolution Strategies According to Rechenberg [16], the first efforts towards an Evolution Strategy took place in 1964 at the Technical University of Berlin. The experimental applications to parameter optimization at that time dealt with hydrodynamical problems like shape optimization of a bended pipe and of a flashing nozzle.
Reference: [8] <author> D. E. Goldberg and K. Deb. </author> <title> A comparative analysis of selection schemes used in genetic algorithms. </title> <editor> In G. J. E. Rawlins, editor, </editor> <booktitle> Foundations of Genetic Algorithms, </booktitle> <pages> pages 69-93. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: This way, selection in EAs provides the active driving force for improvement. Just a look at proportional selection in GAs gives a first impression of the importance of selection, since there is a still ongoing discussion of advantages and disadvantages of this selection mechanism <ref> [4, 19, 8] </ref>. One of the disadvantages is the problem of superindi-viduals, i.e. individuals having so much better fitness values than the remainder of the population, that their offspring quickly dominate the population, leading to a loss of genetic diversity. This loss of diversity is often called premature convergence.
Reference: [9] <editor> J. J. Grefenstette, editor. </editor> <booktitle> Proceedings of the First International Conference on Genetic Algorithms and Their Applications, </booktitle> <address> Hillsdale, New Jersey, 1985. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: [10] <author> J. J. Grefenstette and J. E. Baker. </author> <title> How genetic algorithms work: A critical look at implicit parallelism. </title> <booktitle> In Schaffer [17], </booktitle> <pages> pages 20-27. </pages>
Reference-contexts: Compared to scaling mechanisms for proportional selection, which have never been investigated systematically (see <ref> [10] </ref> for an overview of scaling techniques), the advantages of rank-based methods are actually seen in their striking elegance and the avoidance of superindividuals and loss of selective pressure in later generations.
Reference: [11] <author> F. Hoffmeister. </author> <title> Scalable parallelism by evolutionary algorithms. </title> <editor> In M. Grauer and D. B. Press-mar, editors, </editor> <booktitle> Parallel Computing and Mathematical Optimization, volume 367 of Lecture Notes in Economics and Mathematical Systems, </booktitle> <pages> pages 177-198, </pages> <address> Berlin, 1991. </address> <publisher> Springer. </publisher>
Reference-contexts: Such parallel implementations are scalable concerning their total population size to arbitrary numbers of parallel processing elements and provide an elegant way to exploit the parallel hardware to solve increasingly harder problems <ref> [11] </ref>.
Reference: [12] <author> J. H. Holland. </author> <title> Adaptation in natural and artificial systems. </title> <publisher> The University of Michigan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference-contexts: Several of these Evolutionary Algorithms are nowadays broadly accepted, the most important ones being the Genetic Algorithm (GA) and Classifier System (CFS) by Hol-land <ref> [12, 13] </ref>, the Evolution Strategy (ES) by Rechen-berg and Schwefel [16, 18], and the Genetic Programming Paradigm (GPP) [6, 15]. <p> This paper focuses on GAs and ESs, which are the most frequently used and best understood instances of Evolutionary Algorithms. 2 Genetic Algorithms Genetic Algorithms were mainly developed by John Holland at Ann Arbor, Michigan, during the 60ies and early 70ies. The basic theoretical investigations are given by Holland <ref> [12] </ref>, and an implementation and application to parameter optimization was done in the same year by De Jong [14]. GAs work on binary strings of a fixed length l, i.e. I = f0; 1g l . The fitness function f : I ! IR evaluates the quality of individuals.
Reference: [13] <author> J. H. Holland and J. S. Reitman. </author> <title> Cognitive systems based on adaptive algorithms. </title> <editor> In D. A. Waterman and F. Hayes-Roth, editors, </editor> <title> Pattern-directed inference systems. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Several of these Evolutionary Algorithms are nowadays broadly accepted, the most important ones being the Genetic Algorithm (GA) and Classifier System (CFS) by Hol-land <ref> [12, 13] </ref>, the Evolution Strategy (ES) by Rechen-berg and Schwefel [16, 18], and the Genetic Programming Paradigm (GPP) [6, 15].
Reference: [14] <author> K. D. Jong. </author> <title> An analysis of the behaviour of a class of genetic adaptive systems. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1975. </year> <note> Diss. Abstr. Int. 36(10), 5140B, University Microfilms No. 76-9381. </note>
Reference-contexts: The basic theoretical investigations are given by Holland [12], and an implementation and application to parameter optimization was done in the same year by De Jong <ref> [14] </ref>. GAs work on binary strings of a fixed length l, i.e. I = f0; 1g l . The fitness function f : I ! IR evaluates the quality of individuals.
Reference: [15] <author> J. R. Koza. </author> <title> Hierarchical genetic algorithms operating on populations of computer programs. </title> <editor> In N. S. Sridharan, editor, </editor> <booktitle> Eleventh international joint conference on artificial intelligence, </booktitle> <pages> pages 768-774. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> August </month> <year> 1989. </year>
Reference-contexts: Several of these Evolutionary Algorithms are nowadays broadly accepted, the most important ones being the Genetic Algorithm (GA) and Classifier System (CFS) by Hol-land [12, 13], the Evolution Strategy (ES) by Rechen-berg and Schwefel [16, 18], and the Genetic Programming Paradigm (GPP) <ref> [6, 15] </ref>. Each of these approaches is based upon the collective learning process within a population of individuals, each of which represents a search point in the space of potential solutions to a given problem. The start population is usually initialized at random.
Reference: [16] <author> I. Rechenberg. </author> <title> Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biolo-gischen Evolution. </title> <publisher> Frommann-Holzboog Verlag, Stuttgart, </publisher> <year> 1973. </year>
Reference-contexts: Several of these Evolutionary Algorithms are nowadays broadly accepted, the most important ones being the Genetic Algorithm (GA) and Classifier System (CFS) by Hol-land [12, 13], the Evolution Strategy (ES) by Rechen-berg and Schwefel <ref> [16, 18] </ref>, and the Genetic Programming Paradigm (GPP) [6, 15]. Each of these approaches is based upon the collective learning process within a population of individuals, each of which represents a search point in the space of potential solutions to a given problem. <p> Therefore, consideration of the schema theorem and the building block hypothesis are the main design criteria for applying a GA to a certain problem [7]. 3 Evolution Strategies According to Rechenberg <ref> [16] </ref>, the first efforts towards an Evolution Strategy took place in 1964 at the Technical University of Berlin. The experimental applications to parameter optimization at that time dealt with hydrodynamical problems like shape optimization of a bended pipe and of a flashing nozzle. <p> For this algorithm, which can be seen as a probabilistic gradient search technique, Rechenberg calculated the convergence rates for two different objective function topologies, leading to a theoretically supported rule for step size control <ref> [16] </ref>. These objective functions are the linear corridor of width b, i.e. f 1 (~x) = c 0 +c 1 x 1 where 8i 2 f2; : : : ; ng : b=2 x i b=2, and the sphere model f 2 (~x) = P n i . <p> Calculation of the corresponding optimal success probabilities p opt yields p opt 0:184 for f 1 and p opt 0:270 for f 2 , which form the basis of Rechenberg's 1=5 success rule <ref> [16] </ref>: The ratio of successful mutations to all mutations should be 1=5. If it is greater, increase; if it is less, decrease the standard deviation oe. Schwefel [18] gives reasons to use the multiplicative factors 0:82 and 1=0:82 for the adjustment of oe, which should take place every n mutations.
Reference: [17] <editor> J. D. Schaffer, editor. </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms and Their Applications, </booktitle> <address> San Mateo, California, June 1989. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [18] <author> H.-P. Schwefel. </author> <title> Numerical Optimization of Computer Models. </title> <publisher> Wiley, </publisher> <address> Chichester, </address> <year> 1981. </year>
Reference-contexts: Several of these Evolutionary Algorithms are nowadays broadly accepted, the most important ones being the Genetic Algorithm (GA) and Classifier System (CFS) by Hol-land [12, 13], the Evolution Strategy (ES) by Rechen-berg and Schwefel <ref> [16, 18] </ref>, and the Genetic Programming Paradigm (GPP) [6, 15]. Each of these approaches is based upon the collective learning process within a population of individuals, each of which represents a search point in the space of potential solutions to a given problem. <p> If it is greater, increase; if it is less, decrease the standard deviation oe. Schwefel <ref> [18] </ref> gives reasons to use the multiplicative factors 0:82 and 1=0:82 for the adjustment of oe, which should take place every n mutations. The frequency of successful mutations should be measured e.g. over intervals of 10 n trials. <p> The frequency of successful mutations should be measured e.g. over intervals of 10 n trials. So far, the algorithm achieves linear convergence for the sphere model, and the 1=5 success rule increases efficiency at the cost of robustness <ref> [18] </ref>. The rule may lead the (1+1)-ES to premature termination even in the case of unimodal functions if there are sharp ridges or active restrictions. <p> Concerning the convergence rate and without recombination, the (1,)-ES-strategy was investigated theoretically for the corridor and sphere model, resulting in an optimal ratio of = 1=5 to achieve maximum convergence rates <ref> [18] </ref>. An individual is now represented as a vector a = (x 1 ; : : : ; x n ; oe 1 ; : : : ; oe n ) 2 IR n , consisting of n object variables and their corresponding n standard deviations for individual mutations.
Reference: [19] <author> D. Whitley. </author> <title> The GENITOR algorithm and selection pressure: Why rank-based allocation of reproductive trials is best. </title> <booktitle> In Schaffer [17], </booktitle> <pages> pages 116-121. </pages>
Reference-contexts: This way, selection in EAs provides the active driving force for improvement. Just a look at proportional selection in GAs gives a first impression of the importance of selection, since there is a still ongoing discussion of advantages and disadvantages of this selection mechanism <ref> [4, 19, 8] </ref>. One of the disadvantages is the problem of superindi-viduals, i.e. individuals having so much better fitness values than the remainder of the population, that their offspring quickly dominate the population, leading to a loss of genetic diversity. This loss of diversity is often called premature convergence.
References-found: 19

