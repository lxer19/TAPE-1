URL: http://www.fzi.de/prost/people/weisbrod/fynesse97.ps.gz
Refering-URL: http://www.fzi.de/prost/people/weisbrod.html
Root-URL: http://www.fzi.de
Email: riedml@ira.uka.de  spott@ipd.info.uni-karlsruhe.de  weisbrod@fzi.de  
Author: Martin Spott Joachim Weisbrod 
Note: Table of Contents  
Address: Am Fasanengarten 5 76128 Karlsruhe, Germany  Vincenz-Priessnitz-Str. 3 76128 Karlsruhe, Germany  Haid-und-Neu-Str. 10-14 76131 Karlsruhe, Germany  
Affiliation: Martin Riedmiller University of Karlsruhe Computer Science Dept.  University of Karlsruhe Computer Science Dept.  Forschungszentrum Informatik  
Abstract: First Results on the Application of Abstract |A system is presented that learns to control a priori unknown nonlinear dynamical systems and enables the user to interpret, examine and correct the control strategy in every stage of learning. The application of dynamic programming methods to train a neural network for such control purposes was already quite successful. In order to shorten the long training process the system should allow the incorporation of a priori knowledge about the control strategy in terms of classical linear controllers, fuzzy control rules etc. As a solution, we propose the hybrid control architecture Fy-nesse: The control strategy is represented by a fuzzy relation that can be interpreted and contain a priori knowledge, whereas the more complex part of learning is solved by a neural network as before. This article reports results on the application of the Fy-nesse controller to a chemical plant. the Fynesse Control Architecture
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <type> Technical report, </type> <year> 1993. </year>
Reference-contexts: 1 Introduction Self-learning of appropriate control behavior assuming only a minimum of external training information is a field of growing research interest. One of the most promising techniques in this area of self learning control is the application of dynamic programming methods as proposed by several authors [8], [10], <ref> [1] </ref>. In recent work, this technique was successfully applied for self-learning control of dynamical systems. The approach uses multilayer perceptrons that iteratively approximate the optimal solution which then defines the optimal control strategy [6], [4], [5]. <p> One of the most promising approaches for learning high quality control of technical processes with dynamic behavior is to use the theory of dynamic programming as the foundation of the learning process, because it offers a solid mathematical foundation [8], [10], <ref> [1] </ref>. The ideas of dynamic programming are described in the following section. 2.2 Dynamic Programming Dynamic programming was first introduced by Bellman [2] to solve a special sort of optimization tasks, where temporal relations play a central role. <p> This assumes, that the state space is finite. If we do not want to assume a finite number of states for example because the current state information is a vector of contin uous sensor values we have to apply an-other technique. The Real Time Dynamic Approach proposed in <ref> [1] </ref> suggests to update only those states, that occur during a control trial. The idea is to start the plant in one of a finite set of possible start states, X 0 .
Reference: [2] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Prince-ton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: The ideas of dynamic programming are described in the following section. 2.2 Dynamic Programming Dynamic programming was first introduced by Bellman <ref> [2] </ref> to solve a special sort of optimization tasks, where temporal relations play a central role.
Reference: [3] <author> O. Follinger. </author> <note> Nichtlineare Regelungen. Olden-bourg, 7. edition, </note> <year> 1993. </year>
Reference-contexts: The task can be shortly described as follows (for a detailed description see <ref> [3] </ref>): In a reactor there is a chemical substance with concentration x 1 . The substance chemically reacts with the fluid in the reactor in an energy-emitting decay process. This leads to a raise of the temperature x 2 in the reactor. <p> One of the goals of the Fynesse approach is to show a good final control quality that is comparable to the quality of conventional controller design. The following nonlinear control law is taken from <ref> [3] </ref>. It is derived using the complete knowledge of the nonlinear plant behavior. <p> Then, this initial policy is used to pretrain the neural value function. To demonstrate the benefits of incorporating a priori knowledge, we start from a crisp linear control law described in <ref> [3] </ref>. The linear control law is given by: pretrain the Fynesse controller.
Reference: [4] <author> M. Riedmiller. </author> <title> Learning to control dynamic systems. </title> <editor> In Robert Trappl, editor, </editor> <booktitle> Proceedings of the 13th. European Meeting on Cybernetics and Systems Research - 1996 (EMCSR '96), </booktitle> <address> Vi-enna, </address> <year> 1996. </year>
Reference-contexts: In recent work, this technique was successfully applied for self-learning control of dynamical systems. The approach uses multilayer perceptrons that iteratively approximate the optimal solution which then defines the optimal control strategy [6], <ref> [4] </ref>, [5]. The use of self-learning methods instead of costly analytical modelling allows to control dynamical systems without worrying about their probably very complex internal structure. However, the advantage of completely self-learning from scratch is somewhat affected by the large amount of training experiences until the control knowledge is complete.
Reference: [5] <author> M. Riedmiller. </author> <title> Self learning neural controllers. </title> <type> PhD thesis, </type> <institution> University of Karlsruhe, </institution> <year> 1996. </year> <note> in german. </note>
Reference-contexts: In recent work, this technique was successfully applied for self-learning control of dynamical systems. The approach uses multilayer perceptrons that iteratively approximate the optimal solution which then defines the optimal control strategy [6], [4], <ref> [5] </ref>. The use of self-learning methods instead of costly analytical modelling allows to control dynamical systems without worrying about their probably very complex internal structure. However, the advantage of completely self-learning from scratch is somewhat affected by the large amount of training experiences until the control knowledge is complete. <p> Unfortunately, closed loop control of dynamic systems is an ongoing process, and typically no such terminal state does exist. Instead, we formulate other conditions, under which convergence can be proven (for an exact specification of the assumptions and the proof the reader is referred to <ref> [5] </ref>): 1. there exists a set of states X + with zero direct costs: 8x 2 X + 8u : r (x; u) = 0 2. for all other states direct costs are pos itive: 8x =2 X + 8u : r (x; u) &gt; 0 3. with the available control
Reference: [6] <author> M. Riedmiller and B. Janusz. </author> <title> Self learning control of a mobile robot. </title> <booktitle> In Proceedings of the IEEE ICNN '95, </booktitle> <address> Perth, Australia, </address> <year> 1995. </year>
Reference-contexts: In recent work, this technique was successfully applied for self-learning control of dynamical systems. The approach uses multilayer perceptrons that iteratively approximate the optimal solution which then defines the optimal control strategy <ref> [6] </ref>, [4], [5]. The use of self-learning methods instead of costly analytical modelling allows to control dynamical systems without worrying about their probably very complex internal structure.
Reference: [7] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> (3):9-44, 1988. 
Reference-contexts: Note however, that this target value is not given externally. It also depends on the neural net, namely on the value of the successor state, V (x t+1 ). This special type of learning is therefore called Temporal Difference (TD)-learning <ref> [7] </ref>. Finally, one word should be said concerning the representation of the cost function. The pure value iteration idea assumes the evaluation of the states. However, this implies the need for a model f of the process in order to choose the appropriate action (equation 6).
Reference: [8] <author> C. J. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> Phd thesis, </type> <institution> Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: 1 Introduction Self-learning of appropriate control behavior assuming only a minimum of external training information is a field of growing research interest. One of the most promising techniques in this area of self learning control is the application of dynamic programming methods as proposed by several authors <ref> [8] </ref>, [10], [1]. In recent work, this technique was successfully applied for self-learning control of dynamical systems. The approach uses multilayer perceptrons that iteratively approximate the optimal solution which then defines the optimal control strategy [6], [4], [5]. <p> One of the most promising approaches for learning high quality control of technical processes with dynamic behavior is to use the theory of dynamic programming as the foundation of the learning process, because it offers a solid mathematical foundation <ref> [8] </ref>, [10], [1]. The ideas of dynamic programming are described in the following section. 2.2 Dynamic Programming Dynamic programming was first introduced by Bellman [2] to solve a special sort of optimization tasks, where temporal relations play a central role. <p> However, this implies the need for a model f of the process in order to choose the appropriate action (equation 6). An alternative approach is to represent the value of the costs for state/actions pairs x; u directly. This is the idea of the Q-Learning approach presented in <ref> [8] </ref>.
Reference: [9] <author> J. Weisbrod. </author> <title> Fuzzy control revisited | why is it working? In P. </title> <editor> P. Wang, editor, </editor> <booktitle> Advances in Fuzzy Theory and Technology, </booktitle> <volume> Vol. III, </volume> <pages> pages 219-244. </pages> <address> Bookwrights, Durham (NC), </address> <year> 1995. </year>
Reference-contexts: This figure shows the approximation of the fuzzy relation qual using four fuzzy rules. want to use. We concentrate on Mamdani-type and Goedel-type rules, as they play an important role in the theory of possibility and evidence <ref> [9] </ref>. The conclusions should fulfill the following requirement: The aggregation of all rule relations must be a good approximation of the fuzzy relation qual 2 . The solving is an optimization problem.

References-found: 9

