URL: http://www.cs.unm.edu/~immsec/publications/oakland-with-submit-info.ps.gz
Refering-URL: http://www.cs.unm.edu/~immsec/papers.htm
Root-URL: http://www.cs.unm.edu
Email: fchristy,forrest,bapg@cs.unm.edu  
Title: Detecting Intrusions Using System Calls: Alternative Data Models  
Author: Christina Warrender Stephanie Forrest Barak Pearlmutter 
Date: October 25, 1998  
Address: Albuquerque, NM 87131-1386  
Affiliation: Dept. of Computer Science University of New Mexico  
Abstract: Intrusion detection systems rely on a wide variety of observable data to distinguish between legitimate and illegitimate activities. In this paper we study one such observable|sequences of system calls into the kernel of an operating system. Using system-call data sets generated by several different programs, each consisting both of normal and intrusive behavior, we compare the ability of different data-modeling methods to represent normal behavior accurately and to recognize intrusions. We compare the following methods: simple enumeration of observed sequences, methods based on relative frequencies of different sequences, a data mining technique, and Hidden Markov Models (HMMs). All of the methods perform adequately, with HMMs giving the best overall results. We discuss the factors affecting the performance of each method, and conclude that for this particular problem, weaker methods than HMMs are likely sufficient. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. C. Carrasco and J. Oncina. </author> <title> Learning stochastic regular grammars by means of a state merging method. </title> <booktitle> In Proceedings of the Second International ICGI Colloquium on Grammatical Inteference and Applications, </booktitle> <pages> pages 139-152, </pages> <address> Ali-cante, Spain, </address> <year> 1994. </year>
Reference-contexts: There are many techniques for building either deterministic or probabilistic automata for this sort of task, for 4 example, <ref> [1, 16, 10] </ref>. These methods generally determine the frequencies with which individual symbols (system calls in our case) occur, conditioned on some number of previous symbols.
Reference: [2] <author> W. W. Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Machine Learning: the 12th International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: These mismatches are aggregated into locality frame counts as described earlier. Again, the threshold on locality frame counts is the primary sensitivity parameter. 5.3 RIPPER RIPPER|Repeated Incremental Pruning to Produce Error Reduction|is a rule learning system developed by William Cohen <ref> [2] </ref>. It, like other rule learning systems, is more typically used for classification problems. Training samples consist of a set of attributes describing the object to be classified, and a target class to which the object belongs.
Reference: [3] <author> M. Damashek. </author> <title> Gauging similarity with n-grams: Language-independent categorization of text. </title> <journal> Science, </journal> <volume> 267 </volume> <pages> 843-848, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: For the system-call application, the events are occurrences of each pattern of system calls in a sequence. One example of a frequency-based method is the n-gram vector used to classify text documents <ref> [3] </ref>. Each document is represented by a vector that is a histogram of sequence frequencies. Each element corresponds to one sequence of length n (called an n-gram), and the value of the element is the normalized frequency with which the n-gram occurs in the document. <p> It is also difficult to determine what size vector to use; the space of all possible sequences is much too large, and we cannot guarantee that the subset of sequences observed in traces of normal behavior is complete. Finally, the coarse clustering of documents in <ref> [3] </ref> does not suggest sufficient precision to discriminate between normal and intrusive traces of the same program. Other frequency-based methods examine sequences individually, making them suitable for on-line use.
Reference: [4] <author> S. Forrest, S. A. Hofmeyr, A. Somayaji, and T. A. Longstaff. </author> <title> A sense of self for unix processes. </title> <booktitle> In Proceedings of the 1996 IEEE Symposium on Security and Privacy, </booktitle> <pages> pages 120-128, </pages> <address> Los Alamitos, CA, 1996. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: 1 Introduction In 1996, Forrest and others introduced a simple intrusion detection method based on monitoring the system calls used by active, privileged processes <ref> [4] </ref>. Each process is represented by its trace|the ordered list of system calls used by that process from the beginning of its execution to the end. <p> There are two important characteristics of the approach introduced in <ref> [4] </ref>. First, it identifies a simple observable (short sequences of system calls) that distinguishes between normal and intrusive behavior. This observable is much simpler than earlier proposals, especially those based on standard audit packages, such as SunOS's BSM. <p> The list of methods discussed here is by no means exhaustive, but it does cover those we believe are most applicable to the problem at hand. 2.1 Enumerating Sequences The methods described in <ref> [4, 7] </ref> depend only on enumerating sequences that occur empirically in traces of normal behavior and subsequently monitoring for unknown patterns. There are two different methods of enumeration, each of which defines a different model, or generalization, of the data. <p> There are two different methods of enumeration, each of which defines a different model, or generalization, of the data. There was no statistical analysis of these patterns in the earlier work. The original paper used lookahead pairs <ref> [4] </ref>. The database of normal patterns consisted of a list for each system call of the system calls that follow it at a separation of 0, 1, 2, up to k system calls. <p> For these reasons, we decided to use HMMs as our finite state machine representative for these experiments. 3 Data Sets The original studies of the system-call approach were conducted primarily on synthetic data sets 2 <ref> [4, 13, 7, 6] </ref>. Although the earlier studies on synthetic data sets were suggestive, they are not necessarily good predictors of how the methods will perform in fielded systems. Consequently, we have used a wider variety of data sets for our current study.
Reference: [5] <author> P. Helman and J. Bhangoo. </author> <title> A statistically based system for prioritizing information exploration under uncertainty. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans, </journal> <volume> 27(4) </volume> <pages> 449-466, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: Helman and Bhangoo propose ranking each sequence by comparing how often the sequence is known to occur in normal traces with how often it is expected to occur in intrusions <ref> [5] </ref>. Sequences occurring frequently in intrusions and/or infrequently in normal traces are considered to be more suspicious. Unfortunately, frequencies of each sequence in all possible intrusions are not known a priori. We must, therefore, choose a frequency distribution for abnormal sequences by assumption. <p> Unfortunately, frequencies of each sequence in all possible intrusions are not known a priori. We must, therefore, choose a frequency distribution for abnormal sequences by assumption. Several 3 possibilities for choosing this distribution are mentioned in <ref> [5] </ref>, the simplest of which is to assume that the abnormal distribution is uniform. The Helman and Bhangoo method makes several assumptions that are problematic for the system-call application. First, it assumes that the data are independent and stationary.
Reference: [6] <author> G. G. Helmer, J. S. K. Wong, V. Honavar, and L. Miller. </author> <title> Intelligent agents for intrusion detection. </title> <booktitle> In Proceedings, IEEE Information Technology Conference, </booktitle> <pages> pages 121-124, </pages> <address> Syracuse, NY, </address> <month> September </month> <year> 1998. </year>
Reference-contexts: Over the past several years, many statistically-based learning techniques have been developed. Several such methods have the potential for generating more accurate and/or more compact models of the system-call data, and at least two groups have published results of their own experiments on alternative models applied to system calls <ref> [13, 6] </ref>. Most of the available methods were designed for specific applications, and each has its own idiosyncrasies. <p> For these reasons, we decided to use HMMs as our finite state machine representative for these experiments. 3 Data Sets The original studies of the system-call approach were conducted primarily on synthetic data sets 2 <ref> [4, 13, 7, 6] </ref>. Although the earlier studies on synthetic data sets were suggestive, they are not necessarily good predictors of how the methods will perform in fielded systems. Consequently, we have used a wider variety of data sets for our current study.
Reference: [7] <author> S. A. Hofmeyr, S. Forrest, and A. Somayaji. </author> <title> Intrusion detection using sequences of system calls. </title> <note> To appear in Journal of Computer Security. </note>
Reference-contexts: The list of methods discussed here is by no means exhaustive, but it does cover those we believe are most applicable to the problem at hand. 2.1 Enumerating Sequences The methods described in <ref> [4, 7] </ref> depend only on enumerating sequences that occur empirically in traces of normal behavior and subsequently monitoring for unknown patterns. There are two different methods of enumeration, each of which defines a different model, or generalization, of the data. <p> gave good results on the original (synthetic) data sets. 1 The empirical approach taken here ignores the family of methods based on formal specification of a program's legal activities, such as [9]. 2 The later paper reported that contiguous sequences of some fixed length gave better discrimination than lookahead pairs <ref> [7] </ref>. The database of normal behavior remained compact, and computational efficiency was still reasonable. As the earlier method was known as time-delay embedding (tide), this method was called sequence time-delay embedding (stide). <p> The Helman and Bhangoo method makes several assumptions that are problematic for the system-call application. First, it assumes that the data are independent and stationary. Although a series of complete program traces might well be stationary (no ordered correlations among separate traces) <ref> [7] </ref>, the sequences within the trace are not. Programs often have different distributions of sequences at the beginning of their execution than they do at the end, and there might be many such distinct regions within the trace [10]. <p> For these reasons, we decided to use HMMs as our finite state machine representative for these experiments. 3 Data Sets The original studies of the system-call approach were conducted primarily on synthetic data sets 2 <ref> [4, 13, 7, 6] </ref>. Although the earlier studies on synthetic data sets were suggestive, they are not necessarily good predictors of how the methods will perform in fielded systems. Consequently, we have used a wider variety of data sets for our current study. <p> At each point in our test trace, we check whether the current sequence is a mismatch, and keep track of how many of the last 20 sequences were mismatches. This Locality Frame Count (LFC) gives us our anomaly signal. (A somewhat different approach was taken in <ref> [7] </ref>, where the measure of anomalous behavior was based on Hamming distances between unknown sequences and their closest match in the normal database.) We then set a threshold on the LFC, below which traces are still considered to be normal.
Reference: [8] <author> H. S. Javitz and A. Valdes. </author> <title> "the nides statistical component: Description and justification". </title> <type> Technical report, </type> <institution> Computer Science Laboratory, SRI International, </institution> <address> Menlo Park, CA, </address> <month> March </month> <year> 1993. </year> <month> 19 </month>
Reference-contexts: Also, sequences of system calls are clearly not independent, especially when the sequences overlap as ours do. A second problem is that of characterizing the frequencies of abnormal sequences accurately. SRI takes a different approach in its Emerald system <ref> [8] </ref>. Emerald compares short-term frequency distributions from new, unknown traces with a longer-term historical distribution rather than with static normal and abnormal distributions. Knowledge (or estimation) of the abnormal frequencies is not required.
Reference: [9] <author> C. Ko, G. Fink, and K. Levitt. </author> <title> Automated detection of vulnerabilities in priv--iledged programs by execution monitoring. </title> <booktitle> In Proceedings of the 10th annual Computer Security Applications Conference, </booktitle> <pages> pages 134-144, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: This method can be implemented efficiently, and it gave good results on the original (synthetic) data sets. 1 The empirical approach taken here ignores the family of methods based on formal specification of a program's legal activities, such as <ref> [9] </ref>. 2 The later paper reported that contiguous sequences of some fixed length gave better discrimination than lookahead pairs [7]. The database of normal behavior remained compact, and computational efficiency was still reasonable.
Reference: [10] <author> A. P. Kosoresow and S. A. Hofmeyr. </author> <title> A shape of self for unix processes. </title> <journal> IEEE Software, </journal> <volume> 14(5) </volume> <pages> 35-42, </pages> <year> 1997. </year>
Reference-contexts: Programs often have different distributions of sequences at the beginning of their execution than they do at the end, and there might be many such distinct regions within the trace <ref> [10] </ref>. Also, sequences of system calls are clearly not independent, especially when the sequences overlap as ours do. A second problem is that of characterizing the frequencies of abnormal sequences accurately. SRI takes a different approach in its Emerald system [8]. <p> There are many techniques for building either deterministic or probabilistic automata for this sort of task, for 4 example, <ref> [1, 16, 10] </ref>. These methods generally determine the frequencies with which individual symbols (system calls in our case) occur, conditioned on some number of previous symbols.
Reference: [11] <author> W. Lee, </author> <year> 1998. </year> <type> personal communication. </type>
Reference-contexts: RIPPER has a difficult time learning rules for classes about which there is not enough information, such as a system call that only occurs at the end of a sequence 11 once <ref> [11] </ref>. Because the frequencies of each sequence are not being recorded, simple duplication of each sequence y times is effective. We replicated each training sample twelve times to create the training file, as did Lee and Stolfo in [12].
Reference: [12] <author> W.Lee and S. J. Stolfo. </author> <title> Data mining approaches for intrusion detection. </title> <booktitle> To appear in Proceedings of the 7th USENIX Security Symposium. </booktitle>
Reference-contexts: Also, by identifying just the main features of such patterns, the method should be able to generalize to include normal patterns that were missed in the training data. Lee and others took this approach to the system call data <ref> [13, 12] </ref>. They used a program called "RIPPER" to characterize sequences occurring in normal data by a smaller set of rules that capture the common elements in those sequences. During monitoring, sequences violating those rules are treated as anomalies. <p> We are primarily interested in the application to anomaly detection, where we do not have both positive and negative instances. Lee and others <ref> [13, 12] </ref> adapted RIPPER to anomaly detection by using it to learn rules to predict system calls within short sequences of program traces. For each program, we used a list of all unique sequences of length 10 occurring in that program to create the RIPPER training samples. <p> Because the frequencies of each sequence are not being recorded, simple duplication of each sequence y times is effective. We replicated each training sample twelve times to create the training file, as did Lee and Stolfo in <ref> [12] </ref>. RIPPER training times shown in Table 2 do not include the time to extract the training samples from the traces. RIPPER takes these training samples and forms a hypothesis|a list of rules to describe normal sequences.
Reference: [13] <author> W. Lee, S. J. Stolfo, and P. K. Chan. </author> <title> Learning patterns from unix process execution traces for intrusion detection. </title> <booktitle> In AAAI Workshop on AI Approaches to Fraud Detection and Risk Management, </booktitle> <pages> pages 50-56. </pages> <publisher> AAAI Press, </publisher> <month> July </month> <year> 1997. </year>
Reference-contexts: Over the past several years, many statistically-based learning techniques have been developed. Several such methods have the potential for generating more accurate and/or more compact models of the system-call data, and at least two groups have published results of their own experiments on alternative models applied to system calls <ref> [13, 6] </ref>. Most of the available methods were designed for specific applications, and each has its own idiosyncrasies. <p> Also, by identifying just the main features of such patterns, the method should be able to generalize to include normal patterns that were missed in the training data. Lee and others took this approach to the system call data <ref> [13, 12] </ref>. They used a program called "RIPPER" to characterize sequences occurring in normal data by a smaller set of rules that capture the common elements in those sequences. During monitoring, sequences violating those rules are treated as anomalies. <p> For these reasons, we decided to use HMMs as our finite state machine representative for these experiments. 3 Data Sets The original studies of the system-call approach were conducted primarily on synthetic data sets 2 <ref> [4, 13, 7, 6] </ref>. Although the earlier studies on synthetic data sets were suggestive, they are not necessarily good predictors of how the methods will perform in fielded systems. Consequently, we have used a wider variety of data sets for our current study. <p> We are primarily interested in the application to anomaly detection, where we do not have both positive and negative instances. Lee and others <ref> [13, 12] </ref> adapted RIPPER to anomaly detection by using it to learn rules to predict system calls within short sequences of program traces. For each program, we used a list of all unique sequences of length 10 occurring in that program to create the RIPPER training samples.
Reference: [14] <author> L. R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-286, </pages> <year> 1989. </year>
Reference-contexts: Many, but not all, of the algorithms for building these automata are based on the assumption that the data are stationary. A particularly powerful finite state machine is the hidden Markov model, used widely in speech recognition and also in DNA sequence modeling <ref> [15, 14] </ref>. A hidden Markov model (HMM) describes a doubly stochastic process. An HMM's states represent some unobservable condition of the system being modeled. In each state, there is a certain probability of producing any of the observable system outputs and a separate probability indicating the likely next states. <p> For a program using S system calls, and hence a model of S states, this means roughly 2S 2 values. In most cases, transition and symbol probabilities were initialized randomly, and then trained using the Baum-Welch algorithm as described in <ref> [14] </ref>. Occasionally, however, prior knowledge is useful in performing the initialization. This was the case with the lpr data sets. A primary difference between lpr traces is in the length of the document being printed. This is reflected in the traces as the number of read-write pairs.
Reference: [15] <author> L. R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Many, but not all, of the algorithms for building these automata are based on the assumption that the data are stationary. A particularly powerful finite state machine is the hidden Markov model, used widely in speech recognition and also in DNA sequence modeling <ref> [15, 14] </ref>. A hidden Markov model (HMM) describes a doubly stochastic process. An HMM's states represent some unobservable condition of the system being modeled. In each state, there is a certain probability of producing any of the observable system outputs and a separate probability indicating the likely next states.
Reference: [16] <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> The power of amnesia: Learning probabilistic automata with variable memory length. </title> <journal> Machine Learning, </journal> <volume> 25, </volume> <year> 1996. </year> <month> 20 </month>
Reference-contexts: There are many techniques for building either deterministic or probabilistic automata for this sort of task, for 4 example, <ref> [1, 16, 10] </ref>. These methods generally determine the frequencies with which individual symbols (system calls in our case) occur, conditioned on some number of previous symbols.
References-found: 16

