URL: http://cobar.cs.umass.edu/pubfiles/jmccarthy_ijcai95.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/
Root-URL: 
Email: Email: fjmccarthy,lehnertg@cs.umass.edu  
Title: Using Decision Trees for Coreference Resolution  
Author: Joseph F. McCarthy and Wendy G. Lehnert 
Address: Amherst, MA 01003-4610  
Affiliation: Department of Computer Science University of Massachusetts  
Note: To appear in Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI '95)  
Abstract: This paper describes resolve, a system that uses decision trees to learn how to classify coref-erent phrases in the domain of business joint ventures. An experiment is presented in which the performance of resolve is compared to the performance of a manually engineered set of rules for the same task. The results show that decision trees achieve higher performance than the rules in two of three evaluation metrics developed for the coreference task. In addition to achieving better performance than the rules, resolve provides a framework that facilitates the exploration of the types of knowledge that are useful for solving the coreference problem.
Abstract-found: 1
Intro-found: 1
Reference: [ Aberdeen et al., 1992 ] <author> J. Aberdeen, J. Burger, D. Connolly, S. Roberts, and M. Vilain. MITRE-Bedford ALEMBIC: </author> <title> MUC-4 test results and analysis. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 116-123, </pages> <year> 1992. </year>
Reference: [ Appelt et al., 1992 ] <author> D. E. Appelt, J. Bear, J. R. Hobbs, D. Israel, and M. Tyson. </author> <title> SRI International FASTUS system: MUC-4 test results and analysis. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 143-147, </pages> <year> 1992. </year>
Reference-contexts: This rather conservative approach to coreference was shared by a number of MUC system developers <ref> [ Appelt et al., 1992; Ayuso et al., 1992 ] </ref> , though not all [ Iwanska et al., 1992 ] . Another factor influencing the coreference module was the short time allotted to developing and testing this system component.
Reference: [ Ayuso et al., 1992 ] <author> D. Ayuso, S. Boisen, H. Fox, H. Gish, R. Ingria, and R. Weischedel. </author> <title> BBN: Description of the PLUM system as used in MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 169-176, </pages> <year> 1992. </year>
Reference-contexts: This rather conservative approach to coreference was shared by a number of MUC system developers <ref> [ Appelt et al., 1992; Ayuso et al., 1992 ] </ref> , though not all [ Iwanska et al., 1992 ] . Another factor influencing the coreference module was the short time allotted to developing and testing this system component.
Reference: [ Burger et al., 1994 ] <author> J. Burger, M. Vilain, J. Aberdeen, D. Connolly, and L. Hirschman. </author> <title> A model-theoretic coref-erence scoring scheme. </title> <type> Technical report, </type> <institution> The MITRE Corporation, Bedford, </institution> <address> MA, </address> <year> 1994. </year>
Reference-contexts: An evaluation methodology for the coreference task is being developed for the upcoming Sixth Message Understanding Evaluation and Conference (MUC-6). The met-rics used for evaluating overall IE system performance are being adapted for use on this subtask (cf. <ref> [ Burger et al., 1994 ] </ref> ), where the answer key for each text contains a set of phrases and the coreference links among them. However, evaluation of coreference performance is complicated by the need to take into account the implicit coreference links among phrases.
Reference: [ Chinchor and Sundheim, 1993 ] <author> N. Chinchor and B. Sund-heim. </author> <title> MUC-5 evaluation metrics. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference (MUC-5), </booktitle> <pages> pages 22-29, </pages> <year> 1993. </year>
Reference-contexts: Thus, simply measuring the accuracy of a coreference classifier is inadequate for evaluating how well the classifier performs its task. Two metrics that have been used to evaluate the performance of IE systems are recall and precision <ref> [ Chin-chor, 1991; 1992; Chinchor and Sundheim, 1993 ] </ref> . Recall is the percentage of information in a text that is correctly extracted by a system; precision is the percentage of information extracted by a system that is correct.
Reference: [ Chinchor, 1991 ] <author> N. Chinchor. </author> <title> MUC-3 evaluation metrics. </title> <booktitle> In Proceedings of the Third Message Understanding Conference (MUC-3), </booktitle> <pages> pages 17-24, </pages> <year> 1991. </year>
Reference: [ Chinchor, 1992 ] <author> N. Chinchor. </author> <title> MUC-4 evaluation metrics. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 22-29, </pages> <year> 1992. </year>
Reference-contexts: A function to combine recall and precision into a single measure of performance was incorporated into the Fourth Message Understanding Evaluation and Conference <ref> [ Chinchor, 1992 ] </ref> .
Reference: [ Grosz et al., 1983 ] <author> B. J. Grosz, A. K. Joshi, and S. Wein-stein. </author> <title> Providing a unified account of definite noun phrases in discourse. </title> <booktitle> In Proceedings of the 21st Annual Meeting of the ACL, </booktitle> <pages> pages 44-50, </pages> <year> 1983. </year>
Reference-contexts: We don't have any features that identify the various syntactic constituents of a sentence, e.g., subject or direct object, nor do we have any features that identify clause boundaries (only sentence boundaries). These features will be incorporated in future experiments. Features based on focus of attention <ref> [ Sidner, 1979; Grosz et al., 1983 ] </ref> , which presuppose knowledge about syntactic constituents may also prove useful.
Reference: [ Iwanska et al., 1992 ] <author> L. Iwanska, D. Appelt, D. Ayuso, K. Dahlgren, B. Glover Stalls, R. Grishman, G. Krupka, C. Montgomery, and E. Riloff. </author> <title> Computational aspects of discourse in the context of MUC-3. </title> <booktitle> In Proceedings of the Third Message Understanding Conference (MUC-3), </booktitle> <pages> pages 256-282, </pages> <year> 1992. </year>
Reference-contexts: This rather conservative approach to coreference was shared by a number of MUC system developers [ Appelt et al., 1992; Ayuso et al., 1992 ] , though not all <ref> [ Iwanska et al., 1992 ] </ref> . Another factor influencing the coreference module was the short time allotted to developing and testing this system component. Since coreference resolution was a late stage in processing, upstream components had to be stabilized before serious development could take place on coreference.
Reference: [ Lehnert et al., 1991 ] <author> W. Lehnert, C. Cardie, D. Fisher, E. Riloff, and R. Williams. </author> <title> University of Massachusetts: Description of the CIRCUS system as used for MUC-3. </title> <booktitle> In Proceedings of the Third Message Understanding Conference (MUC-3), </booktitle> <pages> pages 223-233, </pages> <year> 1991. </year>
Reference-contexts: However, the discourse processing capabilities of these systems, particularly their coreference resolution components, have often been cited as weak areas [ Weir and Fritzson, 1993; Moldovan et al., 1992; Aberdeen et al., 1992 ] . The IE systems developed at UMass <ref> [ Lehnert et al., 1991; 1992; 1993 ] </ref> also displayed weak coreference resolution capabilities.
Reference: [ Lehnert et al., 1992 ] <author> W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, E. Riloff, and S. Soderland. </author> <title> University of Massachusetts: Description of the CIRCUS system as used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 282-288, </pages> <year> 1992. </year>
Reference: [ Lehnert et al., 1993 ] <author> W. Lehnert, J. McCarthy, S. Soder-land, E. Riloff, C. Cardie, J. Peterson, F. Feng, C. Dolan, and S. Goldman. </author> <title> University of Massachusetts/Hughes: Description of the CIRCUS system as used for MUC-5. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference (MUC-5), </booktitle> <pages> pages 277-290, </pages> <year> 1993. </year>
Reference: [ Lehnert, 1991 ] <author> W. Lehnert. </author> <title> Symbolic/subsymbolic sentence analysis: Exploiting the best of two worlds. </title> <editor> In J. Barn-den and J. Pollack, editors, </editor> <booktitle> Advances in Connectionist and Neural Computation Theory, </booktitle> <volume> Vol. 1, </volume> <pages> pages 135-164. </pages> <publisher> Ablex Publishers, </publisher> <address> Norwood, NJ, </address> <year> 1991. </year>
Reference-contexts: Resolve was then iteratively trained and tested on different partitions of this set of feature vectors. The data structure used in discourse processing by the UMass/Hughes MUC-5 IE system was the memory token, which converted the case frame output from the circus sentence analyzer <ref> [ Lehnert, 1991 ] </ref> into a more system-independent representation. Prior to corefer-ence processing, each memory token contained one noun phrase, one or more lexical patterns encompassing that phrase, part-of-speech tags, semantic features, and information that was inferred from either the phrase or the context in which the phrase was found.
Reference: [ Merchant, 1993 ] <author> R. H. Merchant. </author> <title> Tipster program overview. </title> <booktitle> In Proceedings of the TIPSTER Text Program (Phase I), </booktitle> <pages> pages 1-2, </pages> <year> 1993. </year>
Reference-contexts: This problem can be recast as a classification problem: given two references, do they refer to the same object or different objects. The Message Understanding Conferences (MUCs) [ Sundheim, 1991; 1992; 1993 ] and the Tipster Project <ref> [ Merchant, 1993 ] </ref> helped both to define the information extraction task and to push the technology of IE systems.
Reference: [ Moldovan et al., 1992 ] <author> D. Moldovan, S. Cha, M. Chung, K. Hendrickson, J. Kim, and S. Kowalski. </author> <title> USC: MUC-4 test results and analysis. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 164-166, </pages> <year> 1992. </year>
Reference: [ Quinlan, 1993 ] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Resolve used the C4.5 decision tree system <ref> [ Quinlan, 1993 ] </ref> to learn how to classify coreferent phrases for the experiments reported in this paper. <p> row shows the results for pruned decision trees. 9 The third row in Table 3 shows the results from a second experiment, in which the rule set from the coref-erence module of the UMass/Hughes MUC-5 IE system 9 Default settings for all C4.5 parameters were used throughout this experiment (see <ref> [ Quinlan, 1993 ] </ref> , Chapter 9, for more information about C4.5 parameters). System Recall Precision F-measure Resolve (unpruned) 85.4% 87.6% 86.5% Resolve (pruned) 80.1% 92.4% 85.8% MUC-5 rule set 67.7% 94.4% 78.9% Table 3: Results for EJV entity coreference resolution.
Reference: [ Sidner, 1979 ] <author> C. L. Sidner. </author> <title> Towards a computational theory of definite anaphora comprehension in English discourse. </title> <type> TR 537, </type> <institution> M.I.T. Artificial Intelligence Laboratory, </institution> <year> 1979. </year>
Reference-contexts: We don't have any features that identify the various syntactic constituents of a sentence, e.g., subject or direct object, nor do we have any features that identify clause boundaries (only sentence boundaries). These features will be incorporated in future experiments. Features based on focus of attention <ref> [ Sidner, 1979; Grosz et al., 1983 ] </ref> , which presuppose knowledge about syntactic constituents may also prove useful.
Reference: [ Sundheim, 1991 ] <author> B. M. Sundheim. </author> <title> Overview of the third message understanding evaluation and conference. </title> <booktitle> In Proceedings of the Third Message Understanding Conference (MUC-3), </booktitle> <pages> pages 3-16, </pages> <year> 1991. </year>
Reference-contexts: One of the many challenges facing an IE system is to determine which references refer to which objects. This problem can be recast as a classification problem: given two references, do they refer to the same object or different objects. The Message Understanding Conferences (MUCs) <ref> [ Sundheim, 1991; 1992; 1993 ] </ref> and the Tipster Project [ Merchant, 1993 ] helped both to define the information extraction task and to push the technology of IE systems.
Reference: [ Sundheim, 1992 ] <author> B. M. Sundheim. </author> <title> Overview of the fourth message understanding evaluation and conference. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 3-21, </pages> <year> 1992. </year>
Reference: [ Sundheim, 1993 ] <author> B. M. Sundheim. </author> <title> TIPSTER/MUC-5 information extraction system evaluation. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference (MUC-5), </booktitle> <pages> pages 27-44, </pages> <year> 1993. </year>
Reference: [ van Rijsbergen, 1979 ] <author> C. J. van Rijsbergen. </author> <title> Information Retrieval. </title> <publisher> Butterworths, </publisher> <address> London, </address> <year> 1979. </year>
Reference-contexts: A function to combine recall and precision into a single measure of performance was incorporated into the Fourth Message Understanding Evaluation and Conference [ Chinchor, 1992 ] . The F-measure, a metric used to evaluate Information Retrieval (IR) system performance <ref> [ van Rijsbergen, 1979 ] </ref> , combines recall and precision scores into a single number using the formula F = fi 2 fi P + R where P is the precision score, R is the recall score and fi is the relative weight given to recall over precision.
Reference: [ Weir and Fritzson, 1993 ] <author> C. Weir and R. Fritzson. </author> <title> UNISYS: Description of the CBAS system used for MUC-5. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference (MUC-5), </booktitle> <pages> pages 249-261, </pages> <year> 1993. </year>
References-found: 22

