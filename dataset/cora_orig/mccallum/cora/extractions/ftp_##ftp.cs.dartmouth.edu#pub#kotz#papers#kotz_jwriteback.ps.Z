URL: ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/kotz:jwriteback.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~dfk/papers/jwriteback.html
Root-URL: http://www.cs.dartmouth.edu
Email: David.Kotz@Dartmouth.edu  carla@cs.duke.edu  
Title: Caching and Writeback Policies in Parallel File Systems  
Author: David Kotz Carla Schlatter Ellis 
Date: July 30, 1992  
Address: Hanover, NH 03755-3551  Durham, NC 27706  
Affiliation: Dept. of Math and Computer Science Dartmouth College  Dept. of Computer Science Duke University  
Web: URL ftp://ftp.cs.dartmouth.edu/pub/CS-papers/Kotz/kotz:jwriteback.ps.Z  
Note: Copyright 1993 by Academic Press, Inc. Appeared in J. Parallel and Distributed Computing 17(1-2), pages 140-145. Available at  This research was supported in part by NSF grants CCR-8721781 and CCR-8821809 and DARPA/NASA subcontract of NCC2-560.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. G. Baker, J. H. Hartman, M. D. Kupfer, K. W. Shirriff, and J. K. Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212, </pages> <year> 1991. </year>
Reference-contexts: While caching has not been studied for parallel file systems, Alan Smith has extensively Kotz and Ellis: Caching and Writeback Policies in Parallel File Systems 5 studied caching in uniprocessors with general-purpose workloads [20]. Uniprocessor and distributed-system file access patterns have been measured many times <ref> [7, 13, 1] </ref>. Sequential access is most common. Supercomputer file access patterns (a scientific workload) involve huge files (tens to thousands of megabytes) accessed primarily sequentially, sometimes repeatedly [11].
Reference: [2] <institution> BBN Advanced Computers. Butterfly Products Overview, </institution> <year> 1987. </year>
Reference-contexts: WriteFree is a compromise between WriteThru and WriteBack. Kotz and Ellis: Caching and Writeback Policies in Parallel File Systems 9 4 Experiments We implemented a file system testbed as heavily parameterized parallel program running on a BBN GP1000 parallel processor <ref> [2] </ref>, an MIMD machine. Since the multiprocessor does not have parallel disks, however, they are simulated. The testbed includes the synthetic workload, the file system, and the set of simulated disks.
Reference: [3] <author> T. W. Crockett. </author> <title> File concepts for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference-contexts: This is only a small sample, however, and the programs are parallelized sequential programs, not parallel programs per se. Crockett's discussion of parallel file access also influences our workload model <ref> [3] </ref>. In summary, little is known about parallel file access patterns, but it appears that some type of sequentiality will dominate. 3 Models and Policies 3.1 Architectural Models Our architectural model is a multiple instruction stream, multiple data stream (MIMD) shared-memory multiprocessor with parallel, independent disks.
Reference: [4] <author> E. DeBenedictus and J. M. del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Eleventh Annual IEEE International Phoenix Conference on Computers and Communications (IPCCC), </booktitle> <month> Apr. </month> <year> 1992. </year> <note> To appear. </note>
Reference-contexts: We call the latter structure Parallel Independent Disks (PID). Examples of a PID architecture include Intel's Concurrent File System [15, 8, 17], the Bridge simulated file system [6, 5] for the BBN Butterfly, and the file system for the nCUBE/2 <ref> [12, 4, 17] </ref>. While caching has not been studied for parallel file systems, Alan Smith has extensively Kotz and Ellis: Caching and Writeback Policies in Parallel File Systems 5 studied caching in uniprocessors with general-purpose workloads [20].
Reference: [5] <author> P. Dibble, M. Scott, and C. Ellis. </author> <title> Bridge: A high-performance file system for parallel processors. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computer Systems, </booktitle> <pages> pages 154-161, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: We call the latter structure Parallel Independent Disks (PID). Examples of a PID architecture include Intel's Concurrent File System [15, 8, 17], the Bridge simulated file system <ref> [6, 5] </ref> for the BBN Butterfly, and the file system for the nCUBE/2 [12, 4, 17]. While caching has not been studied for parallel file systems, Alan Smith has extensively Kotz and Ellis: Caching and Writeback Policies in Parallel File Systems 5 studied caching in uniprocessors with general-purpose workloads [20].
Reference: [6] <author> P. C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: We call the latter structure Parallel Independent Disks (PID). Examples of a PID architecture include Intel's Concurrent File System [15, 8, 17], the Bridge simulated file system <ref> [6, 5] </ref> for the BBN Butterfly, and the file system for the nCUBE/2 [12, 4, 17]. While caching has not been studied for parallel file systems, Alan Smith has extensively Kotz and Ellis: Caching and Writeback Policies in Parallel File Systems 5 studied caching in uniprocessors with general-purpose workloads [20].
Reference: [7] <author> R. Floyd. </author> <title> Short-term file reference patterns in a UNIX environment. </title> <type> Technical Report 177, </type> <institution> Dept. of Computer Science, Univ. of Rochester, </institution> <month> Mar. </month> <year> 1986. </year>
Reference-contexts: While caching has not been studied for parallel file systems, Alan Smith has extensively Kotz and Ellis: Caching and Writeback Policies in Parallel File Systems 5 studied caching in uniprocessors with general-purpose workloads [20]. Uniprocessor and distributed-system file access patterns have been measured many times <ref> [7, 13, 1] </ref>. Sequential access is most common. Supercomputer file access patterns (a scientific workload) involve huge files (tens to thousands of megabytes) accessed primarily sequentially, sometimes repeatedly [11]. <p> We assume that the file system internal buffer size is the same as the block size. Thus, one buffer holds one block. Most files are opened for either reading or writing, with few files updated <ref> [7, 13] </ref>. In this paper we focus on write-only patterns, and investigate delayed-write policies. We use three representative write-only parallel file access patterns. lw1 Local Whole file, one process: a single process writes the entire file from start to finish.
Reference: [8] <author> J. C. French, T. W. Pratt, and M. Das. </author> <title> Performance measurement of a parallel input/output system for the Intel iPSC/2 hypercube. </title> <booktitle> Proceedings of the 1991 ACM Sig-metrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 178-187, </pages> <year> 1991. </year>
Reference-contexts: The second, which also declusters data over many disks, is to attach independent controllers and disks to separate processors or ports on the interconnection network. We call the latter structure Parallel Independent Disks (PID). Examples of a PID architecture include Intel's Concurrent File System <ref> [15, 8, 17] </ref>, the Bridge simulated file system [6, 5] for the BBN Butterfly, and the file system for the nCUBE/2 [12, 4, 17].
Reference: [9] <author> M. Y. Kim. </author> <title> Synchronized disk interleaving. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(11):978-988, </volume> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: In Section 4 we present the experiments, performance measures, and results. Section 5 concludes. 2 Background There are two ways to attach multiple disks to a multiprocessor. The first is to attach a striped disk array to a processor or to the interconnection network. Disk striping <ref> [9, 14, 19] </ref> declusters the data of a file across numerous disks, accessing them in parallel through a single controller. The second, which also declusters data over many disks, is to attach independent controllers and disks to separate processors or ports on the interconnection network.
Reference: [10] <author> D. Kotz. </author> <title> Prefetching and Caching Techniques in File Systems for MIMD Multiprocessors. </title> <type> PhD thesis, </type> <institution> Duke University, </institution> <month> Apr. </month> <year> 1991. </year> <note> Available as technical report CS-1991-016. </note> <author> Kotz and Ellis: </author> <title> Caching and Writeback Policies in Parallel File Systems 15 </title>
Reference-contexts: The use of a real parallel processor, combined with real-time execution and measurement, allows us to directly include the effects of memory contention, synchronization overhead, inter-process dependencies, and other overhead, as they are caused by our workload under various management policies. See <ref> [10] </ref> for more details. 4.1 Experimental Parameters The parameters described here are the base from which we make other variations. There were 20 processes running on 20 processors. The patterns all wrote 4 MBytes of data, or about 200 KBytes per process. <p> Disk requests were queued in the appropriate disk queue. The disk service time was simulated using a constant artificial delay of 30 msec, a reasonable approximation of the average 3 access time for the small, inexpensive disk drives that might be used. See <ref> [10] </ref> for more experimental details, and results of variations of many of these parameters. 2 Actually, we used an exponential distribution truncated at 150 msec. The exponential nature of the distribution is not important.
Reference: [11] <author> E. L. Miller and R. H. Katz. </author> <title> Input/output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 567-576, </pages> <month> Nov. </month> <year> 1991. </year> <title> [12] nCUBE Corporation. nCUBE 2 supercomputers: </title> <type> Technical overview. </type> <institution> Brochure, </institution> <year> 1990. </year>
Reference-contexts: Uniprocessor and distributed-system file access patterns have been measured many times [7, 13, 1]. Sequential access is most common. Supercomputer file access patterns (a scientific workload) involve huge files (tens to thousands of megabytes) accessed primarily sequentially, sometimes repeatedly <ref> [11] </ref>. Five parallel scientific applications, chosen from the PERFECT benchmarks [16] and parallelized for an eight-processor Alliant, have only sequential access patterns. [18]. This is only a small sample, however, and the programs are parallelized sequential programs, not parallel programs per se.
Reference: [13] <author> J. Ousterhout, H. D. Costa, D. Harrison, J. Kunze, M. Kupfer, and J. Thompson. </author> <title> A trace driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <month> Dec. </month> <year> 1985. </year>
Reference-contexts: While caching has not been studied for parallel file systems, Alan Smith has extensively Kotz and Ellis: Caching and Writeback Policies in Parallel File Systems 5 studied caching in uniprocessors with general-purpose workloads [20]. Uniprocessor and distributed-system file access patterns have been measured many times <ref> [7, 13, 1] </ref>. Sequential access is most common. Supercomputer file access patterns (a scientific workload) involve huge files (tens to thousands of megabytes) accessed primarily sequentially, sometimes repeatedly [11]. <p> We assume that the file system internal buffer size is the same as the block size. Thus, one buffer holds one block. Most files are opened for either reading or writing, with few files updated <ref> [7, 13] </ref>. In this paper we focus on write-only patterns, and investigate delayed-write policies. We use three representative write-only parallel file access patterns. lw1 Local Whole file, one process: a single process writes the entire file from start to finish.
Reference: [14] <author> D. Patterson, G. Gibson, and R. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: In Section 4 we present the experiments, performance measures, and results. Section 5 concludes. 2 Background There are two ways to attach multiple disks to a multiprocessor. The first is to attach a striped disk array to a processor or to the interconnection network. Disk striping <ref> [9, 14, 19] </ref> declusters the data of a file across numerous disks, accessing them in parallel through a single controller. The second, which also declusters data over many disks, is to attach independent controllers and disks to separate processors or ports on the interconnection network.
Reference: [15] <author> P. Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: The second, which also declusters data over many disks, is to attach independent controllers and disks to separate processors or ports on the interconnection network. We call the latter structure Parallel Independent Disks (PID). Examples of a PID architecture include Intel's Concurrent File System <ref> [15, 8, 17] </ref>, the Bridge simulated file system [6, 5] for the BBN Butterfly, and the file system for the nCUBE/2 [12, 4, 17].
Reference: [16] <author> L. </author> <title> Pointer. PERFECT: Performance evaluation for cost-effective transformations: </title> <type> Report 2. Technical Report 964, </type> <institution> CSRD | Univ. of Illinois, </institution> <month> Nov. </month> <year> 1991. </year> <note> With Addenda 1 and 2. </note>
Reference-contexts: Uniprocessor and distributed-system file access patterns have been measured many times [7, 13, 1]. Sequential access is most common. Supercomputer file access patterns (a scientific workload) involve huge files (tens to thousands of megabytes) accessed primarily sequentially, sometimes repeatedly [11]. Five parallel scientific applications, chosen from the PERFECT benchmarks <ref> [16] </ref> and parallelized for an eight-processor Alliant, have only sequential access patterns. [18]. This is only a small sample, however, and the programs are parallelized sequential programs, not parallel programs per se. Crockett's discussion of parallel file access also influences our workload model [3].
Reference: [17] <author> T. W. Pratt, J. C. French, P. M. Dickens, and S. A. Janet, Jr. </author> <title> A comparison of the architecture and performance of two parallel file systems. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 161-166, </pages> <year> 1989. </year>
Reference-contexts: The second, which also declusters data over many disks, is to attach independent controllers and disks to separate processors or ports on the interconnection network. We call the latter structure Parallel Independent Disks (PID). Examples of a PID architecture include Intel's Concurrent File System <ref> [15, 8, 17] </ref>, the Bridge simulated file system [6, 5] for the BBN Butterfly, and the file system for the nCUBE/2 [12, 4, 17]. <p> We call the latter structure Parallel Independent Disks (PID). Examples of a PID architecture include Intel's Concurrent File System [15, 8, 17], the Bridge simulated file system [6, 5] for the BBN Butterfly, and the file system for the nCUBE/2 <ref> [12, 4, 17] </ref>. While caching has not been studied for parallel file systems, Alan Smith has extensively Kotz and Ellis: Caching and Writeback Policies in Parallel File Systems 5 studied caching in uniprocessors with general-purpose workloads [20].
Reference: [18] <author> A. L. N. Reddy and P. Banerjee. </author> <title> A study of I/O behavior of Perfect benchmarks on a multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 312-321, </pages> <year> 1990. </year>
Reference-contexts: Sequential access is most common. Supercomputer file access patterns (a scientific workload) involve huge files (tens to thousands of megabytes) accessed primarily sequentially, sometimes repeatedly [11]. Five parallel scientific applications, chosen from the PERFECT benchmarks [16] and parallelized for an eight-processor Alliant, have only sequential access patterns. <ref> [18] </ref>. This is only a small sample, however, and the programs are parallelized sequential programs, not parallel programs per se. Crockett's discussion of parallel file access also influences our workload model [3].
Reference: [19] <author> K. Salem and H. Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In IEEE 1986 Conference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <year> 1986. </year>
Reference-contexts: Recent trends have shown that improvements in the speed of disk hardware are not keeping up with the increasing raw speed of processors. Parallel I/O mechanisms, such as disk striping <ref> [19] </ref>, could provide a significant boost in performance. The challenge is to make this extensive disk hardware bandwidth easily available to parallel programs. <p> In Section 4 we present the experiments, performance measures, and results. Section 5 concludes. 2 Background There are two ways to attach multiple disks to a multiprocessor. The first is to attach a striped disk array to a processor or to the interconnection network. Disk striping <ref> [9, 14, 19] </ref> declusters the data of a file across numerous disks, accessing them in parallel through a single controller. The second, which also declusters data over many disks, is to attach independent controllers and disks to separate processors or ports on the interconnection network.
Reference: [20] <author> A. J. Smith. </author> <title> Disk cache-miss ratio analysis and design considerations. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 161-203, </pages> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: While caching has not been studied for parallel file systems, Alan Smith has extensively Kotz and Ellis: Caching and Writeback Policies in Parallel File Systems 5 studied caching in uniprocessors with general-purpose workloads <ref> [20] </ref>. Uniprocessor and distributed-system file access patterns have been measured many times [7, 13, 1]. Sequential access is most common. Supercomputer file access patterns (a scientific workload) involve huge files (tens to thousands of megabytes) accessed primarily sequentially, sometimes repeatedly [11].

References-found: 19

