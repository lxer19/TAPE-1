URL: http://omega.albany.edu:8008/npde.ps
Refering-URL: http://omega.albany.edu:8008/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-Mail: carlos@omega.albany.edu  
Title: On a New Class of Density Estimators  
Author: Carlos C. Rodrguez 
Note: Contents  
Date: December 21, 1992  
Affiliation: Department of Mathematics and Statistics State University of New York at Albany  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> L. Devroye. </author> <title> A course in density estimation. </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction Let X 1 ; X 2 ; : : :; X n be i.i.d. random p-vectors with unknown bounded continuous density f with respect to the Lebesgue measure on IR p . Consider the classical k-nearest neighbor density estimator of f (z) (see Devroye's book <ref> [1] </ref> or the papers [3], [6]) given by h n (z) = (S (R (k))) where S (r) = fx 2 IR p : kx zk rg is the p-sphere about z of radius r. We consider k k to be an arbitrary norm on IR p . <p> We consider the class of density estimators that can be written as: 2 f n (z) = K z x c 1 0 (5) where ! is a density on <ref> [0; 1] </ref> with mean c i.e. 8u 2 [0; 1] !(u) 0; 0 Z 1 u!(u)du = c: (6) In (5) and h (u) may be stochastic but they are assumed to be functions of the sample size n, satisfying 8u 2 (0; 1) and as n ! 1 the following <p> We consider the class of density estimators that can be written as: 2 f n (z) = K z x c 1 0 (5) where ! is a density on <ref> [0; 1] </ref> with mean c i.e. 8u 2 [0; 1] !(u) 0; 0 Z 1 u!(u)du = c: (6) In (5) and h (u) may be stochastic but they are assumed to be functions of the sample size n, satisfying 8u 2 (0; 1) and as n ! 1 the following four conditions: i. h p (u) = p <p> Let H n (r) = n (r)=n i.e. the empirical distribution of kX zk when the vector X has density f . Notice that n (r) is binomial (n; H (r)) where H is the distribution function of kX zk. Define for u 2 <ref> [0; 1] </ref> h (u) = inffr : H n (r) &gt; uH n ()g. <p> Expanding again f (x) about z we can write f (x) = f (z) + (x z) 0 rf (z) + 2 + 2 where, as before, H 2 (x) denotes the Hessian of f at x and x fl = z + t (x z) for some t 2 <ref> [0; 1] </ref>. The assumed continuity of H 2 at z implies that H 2 (x fl ) = H 2 (z) + o (1) as kx zk ! 0.
Reference: [2] <author> L. Devroye and L. Gyorfi. </author> <title> Nonparametric Density Estimation: The L 1 View. </title> <publisher> John Wiley and Sons, </publisher> <year> 1985. </year>
Reference-contexts: The little o and the limits above should be understood as almost sure when necessary. The following class of density estimators includes, as special cases, several well known estimates (see <ref> [2] </ref> pp. 192-194 and references therein) and it is similar to (5): Z 1 !(u)K z X [nu]+1 Z 1 !(u)h p (u)du The class defined above shows a nicer symmetry between the numerator and the denominator than the class (5). <p> PROOF. Let g n (z) be as in (4). Then, we can write f n (z) = g n (z)= 0 c p !(u)du g n (z)= ~ h n (z) (13) The numerator g n (z) goes almost surely to f (z) (see <ref> [2] </ref> p.149 theorem 2B). By (7) part (i) and the dominated or monotone convergence theorem (depending 5 on what holds in (7)(iv)) the denominator goes (a.s.) to c 1 R 1 0 u!(u)du = 1.
Reference: [3] <author> E. Fix and J.L. Hodges. </author> <title> Discrimination analysis. nonparametric discrimination: Consistency properties. </title> <type> Report 4, </type> <institution> Project Number 21-49-004, USAF school of Aviation medicine, Randolf Field, Texas, </institution> <year> 1951. </year>
Reference-contexts: Consider the classical k-nearest neighbor density estimator of f (z) (see Devroye's book [1] or the papers <ref> [3] </ref>, [6]) given by h n (z) = (S (R (k))) where S (r) = fx 2 IR p : kx zk rg is the p-sphere about z of radius r. We consider k k to be an arbitrary norm on IR p .
Reference: [4] <author> J. Kiefer. </author> <title> Iterated logarithm analogues for sample quantiles when p n # 0. </title> <booktitle> In Proc. sixth Berkeley Symp. Math. Stat. Prob., </booktitle> <volume> volume 1, </volume> <pages> pages 227-244, </pages> <year> 1972. </year>
Reference-contexts: Define, H (r) = P [kX zk r] = S (r) then R (1) &lt; R (2) &lt; : : : &lt; R (n) are the order statistics of n iid random variables with parent distribution H (r). We shall use Lemma 1 from [8] which is proved in <ref> [4] </ref>. Lemma 2.1 (Kiefer) Let z n be a sample ff n -tile from n iid random variables uniformly distributed on (0; 1). If ff n ! 0 and nff n = log log n ! 1 as n ! 1, then z n =ff n ! 1 a.s.
Reference: [5] <author> M. Loeve. </author> <title> Probability theory. </title> <publisher> Van Nostrand, Princeton, </publisher> <year> 1960. </year>
Reference-contexts: Thus, for all * &gt; 0 p bk X P c fl fl p bk X e *=c fl p which goes to zero as n ! 1 which implies that fl n is asymptotically normal (see <ref> [5] </ref> p.316).
Reference: [6] <author> D.O. Loftsgaarden and C.P. Quesenberry. </author> <title> A nonparametric estimate of a multivariate density function. </title> <journal> Ann. Math. Statist., </journal> <volume> 36 </volume> <pages> 1049-1051, </pages> <year> 1965. </year>
Reference-contexts: Consider the classical k-nearest neighbor density estimator of f (z) (see Devroye's book [1] or the papers [3], <ref> [6] </ref>) given by h n (z) = (S (R (k))) where S (r) = fx 2 IR p : kx zk rg is the p-sphere about z of radius r. We consider k k to be an arbitrary norm on IR p .
Reference: [7] <author> Y.P. Mack and M. Rosenblatt. </author> <title> Multivariate k-nearest neighbor density estimates. </title> <journal> Journ. Mult. Anal., </journal> <volume> 9 </volume> <pages> 1-15, </pages> <year> 1979. </year>
Reference-contexts: Equation (8) has a similar interpretation but with a higher degree of symmetry between numerator and denominator. Several authors have tried to combine the k-nn and kernel methods by replacing in the kernel (4) with the stochastic radius R (k) appearing in the k-nn (1) (e.g. see [8], <ref> [7] </ref>). However, the only attempt at smoothing empirical volumes appears to be the Maximum Entropy Histograms (see [9], [10]) and only for p = 1 and uniform smoothing functions K and !. <p> It is also shown there that the estimators (10) strongly dominate the classical kernel and the generalized k-nn (see <ref> [7] </ref>). We find that no matter what smoothness parameter is used in the generalized k-nn (or classical kernel) estimator with kernel K, there is always an estimator from the class (10) with the same kernel with a strictly smaller asymptotic mean squared error (AMSE). <p> (79) Replacing (79) into the expression for ff we find the optimum ff = ff fl given by ff fl = lim k (cbk 1) 2 Asymptotic mean square error comparisons Theorem 3.1 can be used to show that the class of estimators (10) dominates the generalized k-nn class (see <ref> [7] </ref>) and thus, the classical kernel as well. To see this choose k = an 4 p+4 . <p> 2 (x)(dx) (1 ff)f 2 (x) + o k 1 We also have from theorem 3.1 that as n ! 1 E [f n (z)] = f (z) + k 1=2 B (z) + o k 1=2 : (82) Replacing the expression for B (z) and using the notation in <ref> [7] </ref>, Q (f )(z) = (z) + fi 1 fl (z) we have E [f n (z)] = f (z) + 2 [fif (z)] 2=p k 2=p fl (z) n + o k 2=p (83) with q = 1 b 2=p ffi 20 Therefore, when b = c = ffi = <p> E [f n (z)] = f (z) + 2 [fif (z)] 2=p k 2=p fl (z) n + o k 2=p (83) with q = 1 b 2=p ffi 20 Therefore, when b = c = ffi = 1 (i.e. no weighting in the denominators) we recover the results in <ref> [7] </ref> as a special case of equations (81) and (83). Moreover, if we choose b = ffi (85) (which is a proper choice since b 1) we have from (84) that q = 0 and the asymptotic expression for the bias of the generalized k-nn coincides with (83).
Reference: [8] <author> D.S. Moore and J.W. Yackel. </author> <title> Large sample properties of nearest neighbor density function estimators. </title> <editor> In S.S. Gupta and D.S. Moore, editors, </editor> <title> Statistical decision theory and related topics. </title> <publisher> Academic Press, </publisher> <year> 1976. </year>
Reference-contexts: Equation (8) has a similar interpretation but with a higher degree of symmetry between numerator and denominator. Several authors have tried to combine the k-nn and kernel methods by replacing in the kernel (4) with the stochastic radius R (k) appearing in the k-nn (1) (e.g. see <ref> [8] </ref>, [7]). However, the only attempt at smoothing empirical volumes appears to be the Maximum Entropy Histograms (see [9], [10]) and only for p = 1 and uniform smoothing functions K and !. <p> Define, H (r) = P [kX zk r] = S (r) then R (1) &lt; R (2) &lt; : : : &lt; R (n) are the order statistics of n iid random variables with parent distribution H (r). We shall use Lemma 1 from <ref> [8] </ref> which is proved in [4]. Lemma 2.1 (Kiefer) Let z n be a sample ff n -tile from n iid random variables uniformly distributed on (0; 1). <p> PROOF. (of theorem 3.1) The schema of the proof follows <ref> [8] </ref> theorem 3, i.e. we show that we can write p where A n ! N 0; fif 2 (z) K 2 (y)(dy) f 2 (z) (40) B n ! N B (z); fff 2 (z) (41) with lim cov (A n ; B n ) = 0: (42) Hence, the <p> A n and B n are given by: A n = fig fl B n = k fig fl o 11 2 PROOF. (of equation (40)) By corollary 2.1 we have (since f n = g fl n when K is constant) as- lim g fl By lemma 2 in <ref> [8] </ref> we have as n ! 1 Z n ! N (0; 1) (48) P Z provided that K is not constant on S 0 (1). Notice that if K is constant on S 0 (1) the asymptotic normality still holds.
Reference: [9] <author> C. Rodrguez and J. Van Ryzin. </author> <title> Maximum entropy histograms. </title> <journal> Stat. and Prob. letters, </journal> <volume> 3 </volume> <pages> 117-120, </pages> <year> 1985. </year>
Reference-contexts: Several authors have tried to combine the k-nn and kernel methods by replacing in the kernel (4) with the stochastic radius R (k) appearing in the k-nn (1) (e.g. see [8], [7]). However, the only attempt at smoothing empirical volumes appears to be the Maximum Entropy Histograms (see <ref> [9] </ref>, [10]) and only for p = 1 and uniform smoothing functions K and !. This simple uniform smoothing of the denominators of (1) reduces the asymptotic variance and motivated the definition of (5) and (8).
Reference: [10] <author> C. Rodrguez and J. Van Ryzin. </author> <title> Large sample properties of maximum entropy histograms. </title> <journal> IEEE Trans. Inf. thr., </journal> <volume> IT-32 No.6:751-759, </volume> <year> 1986. </year> <month> 22 </month>
Reference-contexts: Several authors have tried to combine the k-nn and kernel methods by replacing in the kernel (4) with the stochastic radius R (k) appearing in the k-nn (1) (e.g. see [8], [7]). However, the only attempt at smoothing empirical volumes appears to be the Maximum Entropy Histograms (see [9], <ref> [10] </ref>) and only for p = 1 and uniform smoothing functions K and !. This simple uniform smoothing of the denominators of (1) reduces the asymptotic variance and motivated the definition of (5) and (8). <p> A global measure of the asymptotic relative efficiency of our estimators with respect to the generalized k-nn can be obtained by computing the ratio of the asymptotic mean square error expression (see <ref> [10] </ref>) (AMSE) N ; (AMSE) ! for the generalized k-nn and ours respectively.
References-found: 10

