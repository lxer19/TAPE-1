URL: http://www.cs.cmu.edu/afs/cs/project/cmcl/archive/Nectar-papers/89asplos.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs/usr/prs/WWW/papers.html
Root-URL: 
Title: The Design of Nectar: A Network Backplane for Heterogeneous Multicomputers architecture provides a flexible way
Author: Emmanuel A. Arnould H. T. Kung Francois J. Bitz Robert D. Sansom Eric C. Cooper Peter A. Steenkiste 
Note: The Nectar  This research was supported in part by Defense Advanced Research Projects Agency (DOD) monitored by the Space and Naval Warfare Systems Command under Contract N00039-87-C-0251, and in part by the Office of Naval Research under Contracts N00014-87-K-0385 and N00014-87-K-0533. Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-III), Boston, Massachusetts, April 3-6, 1989.  
Address: Pittsburgh, Pennsylvania 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: We have designed and built a prototype Nectar system that has been operational since November 1988. This paper presents the motivation and goals for Nectar and describes its hardware and software. The presentation emphasizes how the goals influenced the design decisions and led to the novel aspects of Nectar. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Marco Annaratone, Emmanuel Arnould, Thomas Gross, H. T. Kung, Monica Lam, Onat Menzilcioglu, and Jon A. Webb. </author> <title> The Warp computer: architecture, implementation and performance. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1523-1538, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: The solution embodied in the Nectar architecture is a two-level structure, with fine-grained parallelism 1 within tasks at individual nodes, and coarse-grained par-allelism among tasks on different nodes. This system-level approach is influenced by our experience with previous projects such as the Warp 1 systolic array machine <ref> [1] </ref> and the Mach multiprocessor operating system [12]. The Nectar architecture provides a general and systematic way to handle heterogeneity and task-level parallelism. A variety of existing systems can be plugged into a flexible, extensible network backplane. <p> One of the first Nectar applications is in the area of vision. The application uses a Warp machine <ref> [1] </ref> for low-level vision analysis and Sun workstations for manipulating image features that are stored in a distributed spatial database. It requires both high bandwidth for image transfer and low latency for communication between nodes in the database.
Reference: [2] <author> William C. Athas and Charles L. Seitz. </author> <title> Multicomputers: message-passing concurrent computers. </title> <journal> Computer, </journal> <volume> 21(8) </volume> <pages> 9-24, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: It is similar to the communication interface 10 available on other distributed-memory machines such as hypercubes <ref> [2] </ref>. An important difference is that Nectarine must accommodate heterogeneous nodes, operating systems, memories, attached processors, and other devices. Nectarine presents the programmer with a simple communication abstraction: applications consist of tasks that communicate by transferring messages between user-specified buffers. Tasks are processes on any CAB or node.
Reference: [3] <author> Luis-Felipe Cabrera, Edward Hunter, Michael J. Karels, and David A. </author> <title> Mosher. User-process communication performance in networks of computers. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-14(1):38-53, </volume> <month> January </month> <year> 1988. </year>
Reference: [4] <author> David R. Cheriton. VMTP: </author> <title> Versatile Message Transaction Protocol. </title> <type> RFC 1045, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1988. </year>
Reference-contexts: The current transport protocols are simple and Nectar-specific. We plan to experiment with the corresponding Internet protocols (IP, TCP, and VMTP <ref> [4] </ref>) over Nectar in the coming year. Each transport protocol can be invoked through a procedure call or by placing a command in a special mailbox. In both cases, the message to be sent is specified as a list of areas located in CAB memory or in CAB-accessible VME memory.
Reference: [5] <author> Greg Chesson. </author> <title> Protocol engine design. </title> <booktitle> In Proceedings of the Summer 1987 USENIX Conference, </booktitle> <pages> pages 209-215, </pages> <month> June </month> <year> 1987. </year>
Reference: [6] <author> David D. Clark. </author> <title> The structuring of systems using upcalls. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 171-180, </pages> <publisher> ACM, </publisher> <month> De-cember </month> <year> 1985. </year>
Reference-contexts: During a send, the datalink gathers the packet when it transfers the data to the fiber output queue using DMA. During a receive, the datalink interrupt handler, invoked by the start of packet signal, executes an upcall <ref> [6] </ref> to a transport layer routine. This routine uses the transport header to determine the destination mailbox for the packet. The datalink layer then sets up the DMA to transfer the incoming data to the destination mailbox.
Reference: [7] <author> E. Clementi, J. Detrich, S. Chin, G. Corongiu, D. Folsom, D. Logan, R. Caltabiano, A. Carnevali, J. Helin, M. Russo, A. Gnuda, and P. Palamidese. </author> <title> Large-scale computations on a scalar, vector and parallel Supercomputer. </title> <editor> In E. Clementi and S. Chin, editors, </editor> <booktitle> Structure and Dynamics of Nucleic Acids, Proteins and Membranes, </booktitle> <pages> pages 403-450, </pages> <publisher> Plenum Press, </publisher> <year> 1986. </year>
Reference-contexts: Several large applications are being ported to Nectar using this approach, including simulated annealing and a solid modeling system based on the octree data structure. Large-scale scientific applications that execute well on loosely-coupled arrays of processors <ref> [7] </ref> are also easily ported to Nectar. Powerful, general-purpose Nectar nodes can provide sufficient processing power and memory to meet the computational demands of these applications and the Nectar-net has the bandwidth to meet their communication needs.
Reference: [8] <author> Eric C. Cooper and Richard P. Draves. </author> <title> C Threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: To provide the required efficiency and flexibility, we built the CAB kernel around lightweight processes similar to Mach threads <ref> [8] </ref>. Threads support multitasking so the CAB can execute multiple activities concurrently in a time-shared fashion, but, since threads have little state associated with them, the cost of context switching is low.
Reference: [9] <author> Allesandro Forin, Joseph Barrera, and Richard Sanzi. </author> <title> The shared memory server. </title> <booktitle> In Winter USENIX Conference, Usenix, </booktitle> <address> San Diego, </address> <month> Jan-uary </month> <year> 1989. </year> <title> [10] iPSC/2 C Programmer's Reference Manual. </title> <publisher> Intel Corporation, </publisher> <month> March </month> <year> 1988. </year>
Reference-contexts: The high bandwidth and low latency provided by Nectar also make it an attractive architecture for communication-intensive distributed applications. Examples of such applications include distributed transaction systems, such as Camelot [13], and the simulation of shared virtual memory over a distributed system using Mach <ref> [9] </ref>. In these applications, the CAB will play a critical role as an operating system co-processor. 8 Conclusions Architectures to exploit the high-level, irregular parallelism found in large applications should support heterogeneity, low-latency communication, and scalability.
Reference: [11] <author> Hemant Kanakia and David R. Cheriton. </author> <title> The VMP network adaptor board (NAB): high-performance network communication for multiprocessors. </title> <booktitle> In Proceedings of the SIGCOMM '88 Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 175-187, </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1988. </year> <note> Also published as Computer Communications Review, 18(4). </note>
Reference: [12] <author> Richard F. Rashid, Avadis Tevanian, Jr., Michael W. Young, David B. Golub, Robert V. Baron, David L. Black, William Bolosky, and Jonathan J. Chew. </author> <title> Machine-independent virtual memory management for paged uniprocessor and multiprocessor architectures. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-37(8):896-908, </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: This system-level approach is influenced by our experience with previous projects such as the Warp 1 systolic array machine [1] and the Mach multiprocessor operating system <ref> [12] </ref>. The Nectar architecture provides a general and systematic way to handle heterogeneity and task-level parallelism. A variety of existing systems can be plugged into a flexible, extensible network backplane. <p> As of early 1989 the prototype consists of 2 HUBs and 4 CABs. The system will be expanded to about 30 CABs in Spring 1989. In the prototype, a node can be any system running UNIX or Mach <ref> [12] </ref> with a VME interface. The initial Nectar system at Carnegie Mellon will have Sun-3s, Sun-4s and Warp systems as nodes. To speed up hardware construction, the prototype uses only off-the-shelf parts and 16 fi 16 crossbars.
Reference: [13] <author> Alfred Z. Spector, Joshua J. Bloch, Dean S. Daniels, Richard P. Draves, Daniel J. Duchamp, Jeffrey L. Eppinger, Sherri G. Menees, and Dean S. Thompson. </author> <title> The Camelot project. </title> <journal> Database Engineering, </journal> <volume> 9(4), </volume> <month> December </month> <year> 1986. </year> <note> Also published as Technical Report CMU-CS-86-166, </note> <institution> Computer Science Department, Carnegie Mellon University, </institution> <month> November </month> <year> 1986. </year>
Reference-contexts: The high bandwidth and low latency provided by Nectar also make it an attractive architecture for communication-intensive distributed applications. Examples of such applications include distributed transaction systems, such as Camelot <ref> [13] </ref>, and the simulation of shared virtual memory over a distributed system using Mach [9]. In these applications, the CAB will play a critical role as an operating system co-processor. 8 Conclusions Architectures to exploit the high-level, irregular parallelism found in large applications should support heterogeneity, low-latency communication, and scalability.
Reference: [14] <author> Milind Tambe, Dirk Kalp, Anoop Gupta, Charles Forgy, Brian Milnes, and Allen Newell. Soar/PSM-E: </author> <title> investigating match parallelism in a learning production system. </title> <booktitle> In Proceedings of the ACM/SIGPLAN PPEALS 1988: Parallel Programming: Experience with Applications, Languages, and Systems, </booktitle> <pages> pages 146-161, </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1988. </year> <note> Also published as SIGPLAN Notices, 23(9). 12 </note>
Reference-contexts: We are implementing a parallel production system as an example of an application that requires run-time load balancing. Matching is performed in parallel using a distributed RETE network, and tokens that propagate through the network are stored in a distributed task queue <ref> [14] </ref>. The low latency communication of Nectar provides good support for the fine-grained parallelism required by this application. The flexibility of Nectar allows it to run applications originally written for other parallel systems.
References-found: 13

