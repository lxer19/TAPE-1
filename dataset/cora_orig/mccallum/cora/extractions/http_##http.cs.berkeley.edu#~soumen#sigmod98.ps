URL: http://http.cs.berkeley.edu/~soumen/sigmod98.ps
Refering-URL: http://http.cs.berkeley.edu/~soumen/pub.html
Root-URL: http://www.cs.berkeley.edu
Email: soumen@almaden.ibm.com  dom@almaden.ibm.com  indyk@cs.stanford.edu  
Title: Enhanced hypertext categorization using hyperlinks  
Author: Soumen Chakrabarti Piotr Indyk 
Address: Byron Dom  
Affiliation: IBM Almaden  IBM Almaden  Stanford University  
Abstract: A major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured search using topic taxonomies, circumvents keyword ambiguity, and improves the quality of search and profile-based routing and filtering. Therefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain high-quality semantic clues that are lost upon a purely term-based classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics. We experimented with pre-classified samples from Yahoo! 1 and the US Patent Database 2 . In previous work, we developed a text classifier that misclassified only 13% of the documents in the well-known Reuters benchmark; this was comparable to the best results ever obtained. This classifier misclassified 36% of the patents, indicating that classifying hypertext can be more difficult than classifying text. Naively using terms in neighboring documents increased error to 38%; our hypertext classifier reduced it to 21%. Results with the Yahoo! sample were more dramatic: the text classifier showed 68% error, whereas our hypertext classifier reduced this to only 21%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Ali. </author> <title> Learning relational rules from relational data. Data Mining Seminar, </title> <institution> IBM Almaden, </institution> <address> San Jose, CA, </address> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: Rather than using raw data from the related records, one can pre-process those raw attribute values into a synthesized feature. One example is whether the parent was classified as high or low risk. This is called feature engineering <ref> [1] </ref>. 3.3 Using class information from pre-classified neighbors Noting that text from neighbors is too noisy to help classification, we reflect on the process and rationale for citation in the first place. <p> This is a very difficult problem in general; a few specific situations have been handled in the inductive logic programming literature [29, 26, 27, 23]. However, these techniques have not yet been applied to large-scale data mining scenarios <ref> [1] </ref>. Another difference is the difference in dimensionality [4]. Decision-tree classifiers handle up to hundreds of features or attributes, whereas text corpora often have a lexicon in the hundred thousands. 5.3 Computer vision and pattern recognition Some of our methods are inspired by image-analysis literature.
Reference: [2] <author> C. Apte, F. Damerau, and S. M. Weiss. </author> <title> Automated learning of decision rules for text categorization. </title> <journal> ACM Transactions on Information Systems, </journal> <note> 1994. IBM Research Report RC18879. </note>
Reference-contexts: The vocabulary is coherent and the quality of authorship is high. For the Reuters dataset, classifiers based on rule induction or feature selection classify 80% to 87% of the documents correctly <ref> [2, 4, 38] </ref>. The problem: Hypertext in general and the Web in particular encourage diverse authorship, navigational and citation links, and short, fragmented documents whose topics can be determined only in the broader context of the local region of the link graph.
Reference: [3] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Clas sification and Regression Trees. </title> <publisher> Wadsworth & Brooks/Cole, </publisher> <year> 1984. </year> <note> ISBN: 0-534-98054-6. </note>
Reference-contexts: Also note that our problem required further sophistication compared to non-local term absorption. 5.2 Machine learning and data mining Decision tree classifiers such as CART <ref> [3] </ref> are widely used for classifying numerical and categorical data. These have been adapted to scale to large relational databases for data mining purposes [25, 37]. These typically work on a single relational table giving attributes of each entity (e.g., a customer).
Reference: [4] <author> S. Chakrabarti, B. Dom, R. Agrawal, and P. Raghavan. </author> <title> Using taxonomy, discriminants, and signatures for navigating in text databases. </title> <booktitle> In VLDB, </booktitle> <address> Athens, Greece, </address> <month> Aug. </month> <year> 1997. </year> <note> Invited submission to VLDB Journal, </note> <year> 1998. </year>
Reference-contexts: Topic identifiers constitute structured meta-data that can be used to index a text database. Such structuring helps circumvent keyword ambiguity and improves the quality of ad-hoc searching and browsing <ref> [4, 5, 12, 16] </ref> as well as profile-based routing of documents as in the so-called "push" or filtering services. Topic identification is one example of extracting structured information from a semi-structured or unstructured source. <p> The vocabulary is coherent and the quality of authorship is high. For the Reuters dataset, classifiers based on rule induction or feature selection classify 80% to 87% of the documents correctly <ref> [2, 4, 38] </ref>. The problem: Hypertext in general and the Web in particular encourage diverse authorship, navigational and citation links, and short, fragmented documents whose topics can be determined only in the broader context of the local region of the link graph. <p> Hypertext poses new challenges to automatic classifiers. This was evident from our early experiments (described later in detail). The same classifier that was 87% accurate for Reuters, tested on a sample of patents, correctly classified only 64% <ref> [4] </ref>. On a sample from Yahoo! the performance was even poorer: only 32% of the documents were classified correctly. This is comparable to a recent study by Sahami on a similar sample from Yahoo using a more detailed learning program that used Bayesian nets [18, 30]. <p> This and later phases of TAPER are heavily optimized to handle up to 2 32 tokens and documents and 2 16 topic nodes. For lack of space, such details of system design and implementation are omitted; they can be found in our earlier work <ref> [4] </ref>. The next step after scanning is called feature selection. Fix an internal node c 0 and its children c; c 1 ; c 2 , etc., and consider the classifier at c 0 , whose goal is to route documents into those subtrees that best match a test document. <p> The details of the estimation of have been described elsewhere <ref> [4] </ref>. An alternative binary model truncates n (d; t), the number of times term t occurs in document d, to a f0; 1g value [18]. The estimation of changes slightly, but much of the above framework remains unchanged. <p> This is a very difficult problem in general; a few specific situations have been handled in the inductive logic programming literature [29, 26, 27, 23]. However, these techniques have not yet been applied to large-scale data mining scenarios [1]. Another difference is the difference in dimensionality <ref> [4] </ref>. Decision-tree classifiers handle up to hundreds of features or attributes, whereas text corpora often have a lexicon in the hundred thousands. 5.3 Computer vision and pattern recognition Some of our methods are inspired by image-analysis literature.
Reference: [5] <author> C. Chekuri, M. Goldwasser, P. Raghavan, and E. Upfal. </author> <title> Web search using automatic classification. </title> <booktitle> In Sixth World Wide Web Conference, </booktitle> <address> San Jose, CA, </address> <year> 1996. </year>
Reference-contexts: Topic identifiers constitute structured meta-data that can be used to index a text database. Such structuring helps circumvent keyword ambiguity and improves the quality of ad-hoc searching and browsing <ref> [4, 5, 12, 16] </ref> as well as profile-based routing of documents as in the so-called "push" or filtering services. Topic identification is one example of extracting structured information from a semi-structured or unstructured source.
Reference: [6] <author> R. Chellappa and A. Jain. </author> <title> Markov Random Fields: Theory and Applications. </title> <publisher> Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: Limited range of influence: Pr (c i jN i ; K ) = Pr (c i jN i ; N i ); where N K i = N i " K . This is equivalent to assuming a first-order Markov Random Field <ref> [6] </ref>. 2.
Reference: [7] <author> W. B. Croft and H. R. </author> <title> Turtle. A retrieval model for incorporating hypertext links. </title> <booktitle> In ACM Hypertext, </booktitle> <pages> pages 213-224. </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: We discuss the latter approach in some detail. Lucarella and Zanzi [24] propose a rule-based inference framework based on a set of predicates. Similar relations between queries, terms, and documents are captured by a four-layer directed acyclic Bayesian network used by Croft and Turtle <ref> [7, 8] </ref>, shown in Figure 11.
Reference: [8] <author> W. B. Croft and H. R. </author> <title> Turtle. Retrieval strategies for hypertext. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 29(3) </volume> <pages> 313-324, </pages> <year> 1993. </year>
Reference-contexts: This is the first topic classification system known to us that combines textual and linkage features into a general statistical model to infer the topic of interlinked documents. (In x5, we will comment on the superficial similarity with a large body of work on hypertext retrieval <ref> [8, 14, 22, 36] </ref> and discuss how both our problem and technique are essentially different.) Our exploration starts with an obvious idea for exploiting links commonly found in the aforesaid retrieval literature: including the text of a document's neighbors (documents that cite or are cited by it) into it as if <p> Prefixes like OIO@ are tags to separate the original terms from these engineered features. This seems preferable to merging both local and non-local terms into the same space, as done in earlier retrieval research <ref> [8, 14, 22, 36] </ref>. In qualitative terms, it is conceivable that a classifier loses no necessary information when presented with ffi 0 i (which is now gigantic) rather than ffi i and its associated corpus graph. <p> Since then, many authors have used variants of this approach on different corpora, with varying success. Croft and Turtle had mild success with the CACM and CISI corpora <ref> [8] </ref>. With the same corpora, Savoy proposed a scheme in which artificial links are added at query time through relevance feedback; this showed a more significant improvement [34, 35, 36]. <p> We discuss the latter approach in some detail. Lucarella and Zanzi [24] propose a rule-based inference framework based on a set of predicates. Similar relations between queries, terms, and documents are captured by a four-layer directed acyclic Bayesian network used by Croft and Turtle <ref> [7, 8] </ref>, shown in Figure 11.
Reference: [9] <author> W. Deng and S. S. Iyengar. </author> <title> A new probabilistic relaxation scheme and its application to edge detection. </title> <journal> IEEE Transactions of Pattern Analysis and Machine Intelligence, </journal> <volume> 18(4) </volume> <pages> 432-437, </pages> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: The more likely situation is that only a fraction of the documents in the test document's neighborhood are pre-classified. Our model above leads to a circularity in that case. Fortunately, a wealth of literature in Markov Random Field Theory <ref> [17, 28, 9] </ref> enables us to design an iterative algorithm that initially guesses the classes based on text alone, then updates them iteratively. The class assignment is guaranteed to converge if initiated sufficiently close to a "consistent" configuration. <p> In pattern recognition, pixel classification is studied in a more general context of relaxation labeling and the convergence properties of relaxation-labeling algorithms are studied using Markov random fields <ref> [17, 28, 9] </ref>. 6 Conclusion We have developed new methods for automatically classifying hypertext into a given topic hierarchy, using an iterative relaxation algorithm.
Reference: [10] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: In our system, rather than search for arbitrary subsets, we first order the terms by decreasing ability to separate the classes; one notion of such ability that we have derived from the Pattern Recognition literature <ref> [10] </ref> and found effective, is the following score: score (t) = P 2 c jcj d2c f (t; d; c) (c; t) where c; c 0 ; c 1 ; c 2 are children of internal node c 0 , f (t; d; c) is the number of times term t <p> Assuming that there is a probability distribution for such collections, we want to choose C such that Pr (CjG; T ) is maximum. This constitutes a Bayes Classifier, which gives the minimum expected error rate over all classifiers <ref> [10] </ref>.
Reference: [11] <author> D. Eppstein. </author> <title> Finding the k shortest paths. </title> <booktitle> In Symposium on the Foundations of Computer Science. IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: It also turns out that we can extract the shortest P paths in time O (kjj + k log k + P log P ) <ref> [11] </ref>. We have not observed any adverse effects of such truncation of (10) on the accuracy of classification. Typically, after the top two or three class choices, the remaining classes have probabilities as low as 10 30 and can be ignored.
Reference: [12] <author> D. Florescu, D. Koller, and A. Levy. </author> <title> Using probabilistic information in data integration. </title> <booktitle> In VLDB, </booktitle> <address> Athens, Greece, </address> <year> 1997. </year>
Reference-contexts: Topic identifiers constitute structured meta-data that can be used to index a text database. Such structuring helps circumvent keyword ambiguity and improves the quality of ad-hoc searching and browsing <ref> [4, 5, 12, 16] </ref> as well as profile-based routing of documents as in the so-called "push" or filtering services. Topic identification is one example of extracting structured information from a semi-structured or unstructured source.
Reference: [13] <author> H. P. Frei and D. Steiger. </author> <title> Making use of hypertext links when retrieving information. </title> <booktitle> In ACM European Conference on Hypertext (ECHT), </booktitle> <pages> pages 102-111. </pages> <publisher> ACM, </publisher> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: With the same corpora, Savoy proposed a scheme in which artificial links are added at query time through relevance feedback; this showed a more significant improvement [34, 35, 36]. Frei and Steiger annotated links with frequent terms from the source and target documents to enhance retrieval in UNIX manuals <ref> [13, 14] </ref>. Smith and Chang used captions from HTML pages to construct a text index for searching for embedded images on the Web [39]. Index terms collected from documents and their linked neighborhoods have been used in vector-space retrieval systems [31] and systems based on inference networks.
Reference: [14] <author> H. P. Frei and D. Steiger. </author> <title> The use of semantic links in hypertext information retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 31(1) </volume> <pages> 1-13, </pages> <year> 1995. </year>
Reference-contexts: This is the first topic classification system known to us that combines textual and linkage features into a general statistical model to infer the topic of interlinked documents. (In x5, we will comment on the superficial similarity with a large body of work on hypertext retrieval <ref> [8, 14, 22, 36] </ref> and discuss how both our problem and technique are essentially different.) Our exploration starts with an obvious idea for exploiting links commonly found in the aforesaid retrieval literature: including the text of a document's neighbors (documents that cite or are cited by it) into it as if <p> Prefixes like OIO@ are tags to separate the original terms from these engineered features. This seems preferable to merging both local and non-local terms into the same space, as done in earlier retrieval research <ref> [8, 14, 22, 36] </ref>. In qualitative terms, it is conceivable that a classifier loses no necessary information when presented with ffi 0 i (which is now gigantic) rather than ffi i and its associated corpus graph. <p> With the same corpora, Savoy proposed a scheme in which artificial links are added at query time through relevance feedback; this showed a more significant improvement [34, 35, 36]. Frei and Steiger annotated links with frequent terms from the source and target documents to enhance retrieval in UNIX manuals <ref> [13, 14] </ref>. Smith and Chang used captions from HTML pages to construct a text index for searching for embedded images on the Web [39]. Index terms collected from documents and their linked neighborhoods have been used in vector-space retrieval systems [31] and systems based on inference networks.
Reference: [15] <author> Y. Freund and D. Ron. </author> <title> Learning to model sequences generated by switching distributions. </title> <booktitle> In Proceedings of the Eighth Annual ACM Conference on Computational Learning Theory (COLT), </booktitle> <pages> pages 41-50, </pages> <year> 1995. </year>
Reference-contexts: Learning, algorithms have been discovered for solving related but simpler problems, such as segmenting a sequence of coin toss outcomes (using a number of hidden coins) such that each segment is likely to be generated by a single coin <ref> [15] </ref>. These algorithms already have high complexity which makes them inapplicable in our context. Therefore, we resort to following approximate approach.
Reference: [16] <author> M. Hearst and C. Karadi. Cat-a-Cone: </author> <title> An interactive interface for specifying searches and viewing retrieval results using a large category hierarchy. </title> <booktitle> In Proceedings of ACM SIGIR 97, </booktitle> <pages> pages 246-255. </pages> <publisher> ACM Press, </publisher> <year> 1997. </year>
Reference-contexts: Topic identifiers constitute structured meta-data that can be used to index a text database. Such structuring helps circumvent keyword ambiguity and improves the quality of ad-hoc searching and browsing <ref> [4, 5, 12, 16] </ref> as well as profile-based routing of documents as in the so-called "push" or filtering services. Topic identification is one example of extracting structured information from a semi-structured or unstructured source.
Reference: [17] <author> R. A. Hummel and S. W. Zucker. </author> <title> On the foundations of relaxation labeling processes. </title> <journal> IEEE Transactions of Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5(3):267-287, </volume> <month> May </month> <year> 1983. </year>
Reference-contexts: The more likely situation is that only a fraction of the documents in the test document's neighborhood are pre-classified. Our model above leads to a circularity in that case. Fortunately, a wealth of literature in Markov Random Field Theory <ref> [17, 28, 9] </ref> enables us to design an iterative algorithm that initially guesses the classes based on text alone, then updates them iteratively. The class assignment is guaranteed to converge if initiated sufficiently close to a "consistent" configuration. <p> This procedure can be performed iteratively, to increase the quality of the classification. This iteration constitutes a kind a relaxation labeling, a technique that has been applied in the computer vision and image processing fields <ref> [17, 28] </ref>. Let K symbolize everything that is known about . Using our previous notation, K = fG; T ; C K g. Using this we want to estimate the unknown classes C U . Let U represent the corresponding documents (i.e. those whose classes are unknown). <p> In pattern recognition, pixel classification is studied in a more general context of relaxation labeling and the convergence properties of relaxation-labeling algorithms are studied using Markov random fields <ref> [17, 28, 9] </ref>. 6 Conclusion We have developed new methods for automatically classifying hypertext into a given topic hierarchy, using an iterative relaxation algorithm.
Reference: [18] <author> D. Koller and M. Sahami. </author> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In International Conference on Machine Learning, </booktitle> <volume> volume 14. </volume> <publisher> Morgan-Kaufmann, </publisher> <month> July </month> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: On a sample from Yahoo! the performance was even poorer: only 32% of the documents were classified correctly. This is comparable to a recent study by Sahami on a similar sample from Yahoo using a more detailed learning program that used Bayesian nets <ref> [18, 30] </ref>. The text classifier performed poorly because web documents are extremely diverse, featuring home pages, topical resource lists, hierarchical HTML documents generated automatically from L A T E X, active pages with scripts and links, etc. Even within a broad area there is little consistency in vocabulary. <p> The details of the estimation of have been described elsewhere [4]. An alternative binary model truncates n (d; t), the number of times term t occurs in document d, to a f0; 1g value <ref> [18] </ref>. The estimation of changes slightly, but much of the above framework remains unchanged. Classification over the entire taxonomy can then be posed as a shortest path problem on the taxonomy.
Reference: [19] <author> K. L. Kwok. </author> <title> The use of titles and cited titles as document representations for automatic classification. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 11 </volume> <pages> 201-206, </pages> <year> 1975. </year>
Reference-contexts: Starting in 1975, Kwok, in a series of papers <ref> [19, 20, 21] </ref>, proposed a measure of document similarity based on the degree of term intersection with titles of cited documents. Using a small set of 37 medical abstracts, Kwok showed that using citation titles leads to good cluster separation.
Reference: [20] <author> K. L. Kwok. </author> <title> A document-document similarity measure based on cited titles and probability theory and its application to relevance feedback retrieval. </title> <editor> In C. J. van Rijsbergen, editor, </editor> <booktitle> Research and Development in Information Retrieval, </booktitle> <pages> pages 221-232. </pages> <publisher> Cambridge University Press, </publisher> <year> 1984. </year>
Reference-contexts: Starting in 1975, Kwok, in a series of papers <ref> [19, 20, 21] </ref>, proposed a measure of document similarity based on the degree of term intersection with titles of cited documents. Using a small set of 37 medical abstracts, Kwok showed that using citation titles leads to good cluster separation.
Reference: [21] <author> K. L. Kwok. </author> <title> A probabilistic theory of indexing and similarity measure based on cited and citing documents. </title> <journal> Journal of the American Society for Information Science, </journal> <pages> 36(342-351), </pages> <year> 1985. </year>
Reference-contexts: Starting in 1975, Kwok, in a series of papers <ref> [19, 20, 21] </ref>, proposed a measure of document similarity based on the degree of term intersection with titles of cited documents. Using a small set of 37 medical abstracts, Kwok showed that using citation titles leads to good cluster separation.
Reference: [22] <author> K. L. Kwok. </author> <title> On the use of bibliographically related titles for the enhancements of document representations. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(2) </volume> <pages> 123-131, </pages> <year> 1988. </year>
Reference-contexts: This is the first topic classification system known to us that combines textual and linkage features into a general statistical model to infer the topic of interlinked documents. (In x5, we will comment on the superficial similarity with a large body of work on hypertext retrieval <ref> [8, 14, 22, 36] </ref> and discuss how both our problem and technique are essentially different.) Our exploration starts with an obvious idea for exploiting links commonly found in the aforesaid retrieval literature: including the text of a document's neighbors (documents that cite or are cited by it) into it as if <p> Prefixes like OIO@ are tags to separate the original terms from these engineered features. This seems preferable to merging both local and non-local terms into the same space, as done in earlier retrieval research <ref> [8, 14, 22, 36] </ref>. In qualitative terms, it is conceivable that a classifier loses no necessary information when presented with ffi 0 i (which is now gigantic) rather than ffi i and its associated corpus graph. <p> The idea of using nonlocal terms from linked documents has also been used in a large number of papers on retrieval. In 1963, Salton [31] proposed using terms associated by citation for retrieval, but later experiments by Salton and others <ref> [22, 33] </ref> raised the following issue that we address in this paper: Important terms may be supplied in some instances, producing substantial performance improvements; in other cases, the process adds indifferent or poor terms to the content description : : : No theory exists which would help in distinguishing valuable term
Reference: [23] <author> N. Lavrac and S. Dzeroski. </author> <title> Inductive Logic Programming - Techniques and Applications. </title> <address> Chichester, </address> <year> 1994. </year>
Reference-contexts: How can we do this while favoring our classifier with a good signal-to-noise ratio? A similar problem appears in inductive logic programming <ref> [23] </ref>. Suppose we are given a relational table, or several tables, containing information about patients with hypertension. Apart from local attributes of a person such as age and weight, there can be other attributes such as father and mother, whose records we shall assume are also in the database. <p> However, little appears to have been done with relationships between entities. In the case where entities are patients, this could be useful in, say, diagnosing diseases. This is a very difficult problem in general; a few specific situations have been handled in the inductive logic programming literature <ref> [29, 26, 27, 23] </ref>. However, these techniques have not yet been applied to large-scale data mining scenarios [1]. Another difference is the difference in dimensionality [4].
Reference: [24] <author> D. Lucarella and A. Zanzi. </author> <title> Information retrieval from hypertext: an approach using plausible inference. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 29(3) </volume> <pages> 299-312, </pages> <year> 1993. </year>
Reference-contexts: Index terms collected from documents and their linked neighborhoods have been used in vector-space retrieval systems [31] and systems based on inference networks. We discuss the latter approach in some detail. Lucarella and Zanzi <ref> [24] </ref> propose a rule-based inference framework based on a set of predicates. Similar relations between queries, terms, and documents are captured by a four-layer directed acyclic Bayesian network used by Croft and Turtle [7, 8], shown in Figure 11.
Reference: [25] <author> M. Mehta, R. Agrawal, and J. Rissanen. SLIQ: </author> <title> A fast scalable classifier for data mining. </title> <booktitle> In Proc. of the Fifth Int'l Conference on Extending Database Technology, </booktitle> <address> Avignon, France, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: Also note that our problem required further sophistication compared to non-local term absorption. 5.2 Machine learning and data mining Decision tree classifiers such as CART [3] are widely used for classifying numerical and categorical data. These have been adapted to scale to large relational databases for data mining purposes <ref> [25, 37] </ref>. These typically work on a single relational table giving attributes of each entity (e.g., a customer). However, little appears to have been done with relationships between entities. In the case where entities are patients, this could be useful in, say, diagnosing diseases.
Reference: [26] <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the Workshop on Algorithmic Learning Theory. Japanese Society for Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: However, little appears to have been done with relationships between entities. In the case where entities are patients, this could be useful in, say, diagnosing diseases. This is a very difficult problem in general; a few specific situations have been handled in the inductive logic programming literature <ref> [29, 26, 27, 23] </ref>. However, these techniques have not yet been applied to large-scale data mining scenarios [1]. Another difference is the difference in dimensionality [4].
Reference: [27] <author> M. Pazzani, C. Brunk, and G. Silverstein. </author> <title> A knowledge-intensive approach to learning relational concepts. </title> <booktitle> In Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <address> Ithaca, NY, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, little appears to have been done with relationships between entities. In the case where entities are patients, this could be useful in, say, diagnosing diseases. This is a very difficult problem in general; a few specific situations have been handled in the inductive logic programming literature <ref> [29, 26, 27, 23] </ref>. However, these techniques have not yet been applied to large-scale data mining scenarios [1]. Another difference is the difference in dimensionality [4].
Reference: [28] <author> L. Pelkowitz. </author> <title> A continuous relaxation labeling algorithm for markov random fields. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 20(3) </volume> <pages> 709-715, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The more likely situation is that only a fraction of the documents in the test document's neighborhood are pre-classified. Our model above leads to a circularity in that case. Fortunately, a wealth of literature in Markov Random Field Theory <ref> [17, 28, 9] </ref> enables us to design an iterative algorithm that initially guesses the classes based on text alone, then updates them iteratively. The class assignment is guaranteed to converge if initiated sufficiently close to a "consistent" configuration. <p> This procedure can be performed iteratively, to increase the quality of the classification. This iteration constitutes a kind a relaxation labeling, a technique that has been applied in the computer vision and image processing fields <ref> [17, 28] </ref>. Let K symbolize everything that is known about . Using our previous notation, K = fG; T ; C K g. Using this we want to estimate the unknown classes C U . Let U represent the corresponding documents (i.e. those whose classes are unknown). <p> This relaxation is guaranteed to converge to a locally consistent assignment provided it is initiated "close enough" to such a consistent state <ref> [28] </ref>. Note that K includes all the text T , the link structure G and the known classes C K and that N K i includes the corresponding subsets of these. <p> In pattern recognition, pixel classification is studied in a more general context of relaxation labeling and the convergence properties of relaxation-labeling algorithms are studied using Markov random fields <ref> [17, 28, 9] </ref>. 6 Conclusion We have developed new methods for automatically classifying hypertext into a given topic hierarchy, using an iterative relaxation algorithm.
Reference: [29] <author> R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5, 3 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: However, little appears to have been done with relationships between entities. In the case where entities are patients, this could be useful in, say, diagnosing diseases. This is a very difficult problem in general; a few specific situations have been handled in the inductive logic programming literature <ref> [29, 26, 27, 23] </ref>. However, these techniques have not yet been applied to large-scale data mining scenarios [1]. Another difference is the difference in dimensionality [4].
Reference: [30] <author> M. Sahami. </author> <title> Web classification using Bayesian nets. </title> <type> Personal communication, </type> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: On a sample from Yahoo! the performance was even poorer: only 32% of the documents were classified correctly. This is comparable to a recent study by Sahami on a similar sample from Yahoo using a more detailed learning program that used Bayesian nets <ref> [18, 30] </ref>. The text classifier performed poorly because web documents are extremely diverse, featuring home pages, topical resource lists, hierarchical HTML documents generated automatically from L A T E X, active pages with scripts and links, etc. Even within a broad area there is little consistency in vocabulary. <p> Furthermore, this error rate was achieved at a large feature set size of over 14,000 terms. It appears that this may be a basic limitation of text-based classification; a more detailed learning program that used Bayesian nets showed comparable performance with a comparable sample from Yahoo! <ref> [30] </ref>. In contrast, the IO-bridge based classifier has excellent performance: it uses only 14 features (the class labels) and has only 25% error! 4.2 Locality in bridges Sometimes an IO-bridge may not be "pure"; it may include a sequence of topics, with many out-links for each topic.
Reference: [31] <author> G. Salton. </author> <title> Associative document retrieval techniques using bibli ographic information. </title> <journal> Journal of the ACM, </journal> <volume> 10(4) </volume> <pages> 440-457, </pages> <month> Oct. </month> <year> 1963. </year>
Reference-contexts: The idea of using nonlocal terms from linked documents has also been used in a large number of papers on retrieval. In 1963, Salton <ref> [31] </ref> proposed using terms associated by citation for retrieval, but later experiments by Salton and others [22, 33] raised the following issue that we address in this paper: Important terms may be supplied in some instances, producing substantial performance improvements; in other cases, the process adds indifferent or poor terms to <p> Smith and Chang used captions from HTML pages to construct a text index for searching for embedded images on the Web [39]. Index terms collected from documents and their linked neighborhoods have been used in vector-space retrieval systems <ref> [31] </ref> and systems based on inference networks. We discuss the latter approach in some detail. Lucarella and Zanzi [24] propose a rule-based inference framework based on a set of predicates.
Reference: [32] <author> G. Salton and M. J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: This may not be fixed by a static list of "stopword" sites, since different sites may assume this role for different topics. 4.1 Bridges Co-citation is a wellstudied phenomenon in linked corpora, such as academic papers <ref> [32] </ref>. Documents that cite or are above is the percentage of cases for which outlinks at a fixed offset from a given outlink point to some document of the same topic as the given document at offset 0.
Reference: [33] <author> G. Salton and Y. Zhang. </author> <title> Enhancement of text representations using related document titles. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 22(5) </volume> <pages> 385-394, </pages> <year> 1986. </year>
Reference-contexts: Although these observations are for our particular corpus, the problems may arise in other hy-perlinked corpora as well, e.g., Salton and Zhang found that inclusion of cited titles can add spurious words that degrade retrieval quality <ref> [33] </ref> (also see x5). <p> The idea of using nonlocal terms from linked documents has also been used in a large number of papers on retrieval. In 1963, Salton [31] proposed using terms associated by citation for retrieval, but later experiments by Salton and others <ref> [22, 33] </ref> raised the following issue that we address in this paper: Important terms may be supplied in some instances, producing substantial performance improvements; in other cases, the process adds indifferent or poor terms to the content description : : : No theory exists which would help in distinguishing valuable term
Reference: [34] <author> J. Savoy. </author> <title> A learning scheme for information retrieval in hyper text. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 30(4) </volume> <pages> 515-533, </pages> <year> 1994. </year>
Reference-contexts: Croft and Turtle had mild success with the CACM and CISI corpora [8]. With the same corpora, Savoy proposed a scheme in which artificial links are added at query time through relevance feedback; this showed a more significant improvement <ref> [34, 35, 36] </ref>. Frei and Steiger annotated links with frequent terms from the source and target documents to enhance retrieval in UNIX manuals [13, 14]. Smith and Chang used captions from HTML pages to construct a text index for searching for embedded images on the Web [39].
Reference: [35] <author> J. Savoy. </author> <title> A new probabilistic scheme for information retrieval in hypertext. </title> <journal> The New Review of Hypermedia and Multimedia, </journal> <pages> pages 107-134, </pages> <year> 1995. </year>
Reference-contexts: Croft and Turtle had mild success with the CACM and CISI corpora [8]. With the same corpora, Savoy proposed a scheme in which artificial links are added at query time through relevance feedback; this showed a more significant improvement <ref> [34, 35, 36] </ref>. Frei and Steiger annotated links with frequent terms from the source and target documents to enhance retrieval in UNIX manuals [13, 14]. Smith and Chang used captions from HTML pages to construct a text index for searching for embedded images on the Web [39].
Reference: [36] <author> J. Savoy. </author> <title> An extended vector processing scheme for searching information in hypertext. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 32(2) </volume> <pages> 155-170, </pages> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: This is the first topic classification system known to us that combines textual and linkage features into a general statistical model to infer the topic of interlinked documents. (In x5, we will comment on the superficial similarity with a large body of work on hypertext retrieval <ref> [8, 14, 22, 36] </ref> and discuss how both our problem and technique are essentially different.) Our exploration starts with an obvious idea for exploiting links commonly found in the aforesaid retrieval literature: including the text of a document's neighbors (documents that cite or are cited by it) into it as if <p> Prefixes like OIO@ are tags to separate the original terms from these engineered features. This seems preferable to merging both local and non-local terms into the same space, as done in earlier retrieval research <ref> [8, 14, 22, 36] </ref>. In qualitative terms, it is conceivable that a classifier loses no necessary information when presented with ffi 0 i (which is now gigantic) rather than ffi i and its associated corpus graph. <p> Croft and Turtle had mild success with the CACM and CISI corpora [8]. With the same corpora, Savoy proposed a scheme in which artificial links are added at query time through relevance feedback; this showed a more significant improvement <ref> [34, 35, 36] </ref>. Frei and Steiger annotated links with frequent terms from the source and target documents to enhance retrieval in UNIX manuals [13, 14]. Smith and Chang used captions from HTML pages to construct a text index for searching for embedded images on the Web [39].
Reference: [37] <author> J. Shafer, R. Agrawal, and M. Mehta. SPRINT: </author> <title> A scalable parallel classifier for data mining. </title> <booktitle> In Proc. of the 22th Int'l Conference on Very Large Databases, </booktitle> <address> Bombay, India, </address> <month> Sept </month> <year> 1996. </year>
Reference-contexts: Also note that our problem required further sophistication compared to non-local term absorption. 5.2 Machine learning and data mining Decision tree classifiers such as CART [3] are widely used for classifying numerical and categorical data. These have been adapted to scale to large relational databases for data mining purposes <ref> [25, 37] </ref>. These typically work on a single relational table giving attributes of each entity (e.g., a customer). However, little appears to have been done with relationships between entities. In the case where entities are patients, this could be useful in, say, diagnosing diseases.
Reference: [38] <author> Y. Singer and W. W. Cohen. </author> <title> Context-sensitive learning methods for text categorization. </title> <booktitle> In SIGIR. ACM, </booktitle> <year> 1996. </year>
Reference-contexts: The vocabulary is coherent and the quality of authorship is high. For the Reuters dataset, classifiers based on rule induction or feature selection classify 80% to 87% of the documents correctly <ref> [2, 4, 38] </ref>. The problem: Hypertext in general and the Web in particular encourage diverse authorship, navigational and citation links, and short, fragmented documents whose topics can be determined only in the broader context of the local region of the link graph.
Reference: [39] <author> J. R. Smith and S.-F. Chang. </author> <title> Visually searching the web for content. </title> <journal> IEEE Multimedia, </journal> <volume> 4(3) </volume> <pages> 12-20, </pages> <year> 1997. </year>
Reference-contexts: Frei and Steiger annotated links with frequent terms from the source and target documents to enhance retrieval in UNIX manuals [13, 14]. Smith and Chang used captions from HTML pages to construct a text index for searching for embedded images on the Web <ref> [39] </ref>. Index terms collected from documents and their linked neighborhoods have been used in vector-space retrieval systems [31] and systems based on inference networks. We discuss the latter approach in some detail. Lucarella and Zanzi [24] propose a rule-based inference framework based on a set of predicates.
Reference: [40] <author> R. Weiss, B. Velez, M. A. Sheldon, C. Nemprempre, P. Szilagyi, A. Duda, and D. K. Gifford. HyPursuit: </author> <title> A hierarchical network search engine that exploits content-link hypertext clustering. </title> <booktitle> In ACM Hypertext, </booktitle> <address> Washington, DC, </address> <month> Mar. </month> <year> 1996. </year> <month> 12 </month>
Reference-contexts: Citation-based similarity, or a combination of citation and term-based similarity, can then be used to perform unsupervised clustering of documents <ref> [40] </ref>. These common documents hint that two or more pages have the same class, while not committing what that class could be. We call these documents bridges. Figure 8 (a) illustrates the scenario.
References-found: 40

