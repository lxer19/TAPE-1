URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1997/GIT-CC-97-34.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.97.html
Root-URL: 
Email: reid@cc.gatech.edu  
Title: Instructions Scheduling for Highly Super-scalar Processors an instruction reorder buffer awaiting execution. Instructions are executed
Author: Bill Appelbe, Raja Das, and Reid Harmon bill, raja, 
Keyword: Run-time Instruction Scheduling After instructions  
Note: have been fetched and decoded, they are placed in  must be executed in order. However, LOADs and STOREs can speculatively execute out of order, provided that such instructions  This work supported in part by NSF grant ASC-9624987.  
Address: Atlanta, GA 30332-0280  
Affiliation: College of Computing, Georgia Tech  
Abstract: This article analyzes the impact of super-scalar processor architecture upon instruction scheduling. The compile-time schedule is shown to significantly impact performance, despite out-of-order execution. The problem of determining an optimal schedule at compile-time is shown to be NP-complete. A variety of heuristics for instructions scheduling are applied to benchmarks, and it is shown that traditional depth-first instruction scheduling performs badly compared to a variety of breadth-first instruction scheduling heuristics. Modern high-performance microprocessors, such as the HP PA-8000[12] or the PowerPC 620[4], are all super-scalar, meaning that they can simultaneously fetch, decode, and execute several instructions at once. Current processors are often at least 4-way super-scalar. Together with the ability to execute multiple instructions in a single cycle, such highly super-scalar processors usually have several other characteristics that significantly impact compiler optimization and instruction scheduling, including: 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: The size of the instruction buffer was set at 16 times the super-scalarity 4 The standard Aho, Sethi & Uhlmann <ref> [1] </ref> algorithm for code generation for trees is depth-first to minimize register usage 7 large basic blocks. Thus applu, with an inner block with over 100 instructions, shows a speedup of almost 200% from any depth-first schedule over gcc's schedule.
Reference: [2] <author> David Bernstein and Michael Rodeh. </author> <title> Global instruction scheduling for superscalar machines. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 241-255. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1991. </year>
Reference: [3] <author> David G. Bradlee, Robert R. Henry, and Susan J. Eggers. </author> <title> The marion system for retargetable instruction scheduling. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 229-240. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1991. </year>
Reference: [4] <author> Trung A. Diep, Christopher Nelson, and John Paul Shen. </author> <title> Performance evaluation of the powerpc 620 microarchitecture. </title> <booktitle> In Proceedings of The 22nd Annual Symposium on Computer Architecture, </booktitle> <pages> pages 163-187, </pages> <year> 1995. </year>
Reference: [5] <author> David Dunn and Wei-Chung Hsu. </author> <title> Instruction scheduling for the hp pa-8000. </title> <booktitle> In Micro-29, </booktitle> <pages> pages 298-307, </pages> <year> 1996. </year>
Reference: [6] <author> John R. Ellis. Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1985. </year>
Reference: [7] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability, A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> San Francisco, California, </address> <year> 1979. </year>
Reference-contexts: 1 A runtime dependence graph is more precise then a compile-time dependence graph because represents actual dependences between instructions at run-time 3 Determining if an optimal schedule exists is NP-complete if the number of processors (super-scalarity) is greater than 2, even if all instructions execute in unit time <ref> [7, page 239] </ref>.
Reference: [8] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: This allows us to precisely determine the cycles required to execute standard computational benchmarks on different super-scalar architectures, using various instruction scheduling strategies. The environment we used consists of Simulator SuperDLX [13], which simulates the DLX <ref> [8] </ref> instruction set. Our simulator includes support for a reconfigurable, multi-level cache, specification of the numbers and latencies of all functional units and various branch prediction strategies.
Reference: [9] <author> Dorit S Hochbaum(ed.). </author> <title> Approximation Algorithms for NP-Hard Problems. </title> <publisher> PWS Publishing Company, </publisher> <address> San Francisco, California, </address> <year> 1997. </year>
Reference-contexts: algorithms have been developed, such as Graham's List Scheduling (LS), which gets within a factor 2 of the optimal schedule, but it has been shown that there is a lower bound on approximation algorithms (no better than 4/3 of the optimal solution in the worst case, unless P = NP <ref> [9, page 17] </ref>). Instruction Buffering Our model assumes that a k-way super-scalar processor has one of each type of functional unit, such as an adder or multiplier or load/store unit. However, one other functional unit which is not explicitly taken into account in the above analysis is the reorder buffer.
Reference: [10] <author> Anne Holler. </author> <title> Optimization for a superscalar out-of-order machine. </title> <booktitle> In Micro-29, </booktitle> <pages> pages 336-348, </pages> <year> 1996. </year>
Reference: [11] <author> Richard A. Huff. </author> <title> Lifetime-sensitive modulo scheduling. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 258-267. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1993. </year>
Reference: [12] <author> Ashok Kumar. </author> <title> The HP PA-8000 RISC CPU. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 27-32, </pages> <month> MArch/April </month> <year> 1997. </year>
Reference: [13] <author> Cecile Moura. SuperDLX: </author> <title> A Generic Superscalar Simulator. </title> <type> ACAPS Technical Memo 64, </type> <institution> McGill University, School of Computer Science, Montreal, Canada, </institution> <year> 1993. </year>
Reference-contexts: This allows us to precisely determine the cycles required to execute standard computational benchmarks on different super-scalar architectures, using various instruction scheduling strategies. The environment we used consists of Simulator SuperDLX <ref> [13] </ref>, which simulates the DLX [8] instruction set. Our simulator includes support for a reconfigurable, multi-level cache, specification of the numbers and latencies of all functional units and various branch prediction strategies.
Reference: [14] <author> Shlomit S. Pinter. </author> <title> Register allocation with instruction scheduling: A new approach. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 248-257. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1993. </year> <month> 10 </month>
References-found: 14

