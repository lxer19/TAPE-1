URL: http://www.cs.umn.edu/Users/dept/users/kumar/matrix.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: agupta@cs.umn.edu and kumar@cs.umn.edu  
Title: Scalability of Parallel Algorithms for Matrix Multiplication  
Author: Anshul Gupta and Vipin Kumar 
Note: This work was supported by IST/SDIO through the Army Research Office grant 28408-MA-SDI to the University of Minnesota and by the University of Minnesota Army High Performance Computing Research Center under contract DAAL03-89-C-0038. CM-5 is a trademark of the  
Date: TR 91-54, November 1991 (Revised April 1994)  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science, University of Minnesota  Thinking Machines Corporation.  
Abstract: A number of parallel formulations of dense matrix multiplication algorithm have been developed. For arbitrarily large number of processors, any of these algorithms or their variants can provide near linear speedup for sufficiently large matrix sizes and none of the algorithms can be clearly claimed to be superior than the others. In this paper we analyze the performance and scalability of a number of parallel formulations of the matrix multiplication algorithm and predict the conditions under which each formulation is better than the others. We present a parallel formulation for hypercube and related architectures that performs better than any of the schemes described in the literature so far for a wide range of matrix sizes and number of processors. The superior performance and the analytical scalability expressions for this algorithm are verified through experiments on the Thinking Machines Corporation's CM-5 TM y parallel computer for up to 512 processors. We show that special hardware permitting simultaneous communication on all the ports of the processors does not improve the overall scalability of the matrix multiplication algorithms on a hypercube. We also discuss the dependence of scalability on technology dependent factors such as communication and computation speeds and show that under certain conditions, it may be better to have a parallel computer with k-fold as many processors rather than one with the same number of processors, each k-fold as fast. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Since the matrix multiplication algorithm is highly computation intensive, there has been a great deal of interest in developing parallel formulations of this algorithm and testing its performance on various parallel architectures <ref> [1, 3, 5, 6, 7, 9, 11, 17, 18, 37, 8] </ref>. Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in [3, 18].
Reference: [2] <author> M. L. Barton and G. R. Withers. </author> <title> Computing performance as a function of the speed, quantity, and the cost of processors. </title> <booktitle> In Supercomputing '89 Proceedings, </booktitle> <pages> pages 759-764, </pages> <year> 1989. </year>
Reference-contexts: This should be contrasted with the conventional wisdom that suggests that better performance is always obtained using fewer faster processors <ref> [2] </ref>. 9 Experimental Results We verified a part of the analysis of this paper through experiments of the CM-5 parallel computer. On this machine, the fat-tree [30] like communication network on the CM-5 provides simultaneous paths for communication between all pairs of processors.
Reference: [3] <author> Jarle Berntsen. </author> <title> Communication efficient matrix multiplication on hypercubes. </title> <journal> Parallel Computing, </journal> <volume> 12 </volume> <pages> 335-342, </pages> <year> 1989. </year>
Reference-contexts: Since the matrix multiplication algorithm is highly computation intensive, there has been a great deal of interest in developing parallel formulations of this algorithm and testing its performance on various parallel architectures <ref> [1, 3, 5, 6, 7, 9, 11, 17, 18, 37, 8] </ref>. Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in [3, 18]. <p> Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in <ref> [3, 18] </ref>. In particular, Berntsen [3] presents an algorithm which has a strictly smaller communication overhead than Cannon's algorithm, but has a smaller degree of concurrency. Ho et al. [18] present another variant of Cannon's algorithm for a hypercube which permits communication on all channels simultaneously. <p> Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in [3, 18]. In particular, Berntsen <ref> [3] </ref> presents an algorithm which has a strictly smaller communication overhead than Cannon's algorithm, but has a smaller degree of concurrency. Ho et al. [18] present another variant of Cannon's algorithm for a hypercube which permits communication on all channels simultaneously. <p> In <ref> [3] </ref>, Berntsen describes an algorithm which exploits greater connectivity provided by a hypercube. The algorithm uses p = 2 3q processors with the restriction that p n 3=2 for multiplying two n fi n matrices A and B. <p> The hypercube is split into 2 q subcubes, each performing a submatrix multiplication between submatrices of A of size n 2 q fi n 2 2q and submatrices of B of size n 2 2q fi n 2 q using Cannon's algorithm. It is shown in <ref> [3] </ref> that the time spent in data communication by this algorithm on a hypercube is 2t s p 1=3 + 1 3 t s log p + 3t w p 2=3 , and hence the total parallel execution time is given by the following equation: T p = p 1 t <p> the difference in the efficiencies is quite significant. 0.1 0.3 0.5 0.7 0.9 100 150 200 250 300 350 400 450 E GK Cannon's (p = 512). 10 Concluding Remarks In this paper we have presented the scalability analysis of a number of matrix multiplication algorithms described in the literature <ref> [5, 9, 11, 3, 18] </ref>. Besides analyzing these classical algorithms, we show that the GK algorithm that we present in this paper outperforms all the well known algorithms for a significant range of number of processors and matrix sizes.
Reference: [4] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year> <month> 23 </month>
Reference-contexts: It can easily be shown that an O (p log p) scalability is the best any parallel formulation of the conventional O (n 3 ) algorithm can achieve on any parallel architecture <ref> [4] </ref> and the DNS algorithm achieves this lower bound on a hypercube. 5.4 Isoefficiency Analysis of the GK Algorithm The total overhead T o for this algorithm is equal to 5 3 t s p log p+ 5 3 t w n 2 p 1=3 log p and the following equations
Reference: [5] <author> L. E. Cannon. </author> <title> A cellular computer to implement the Kalman Filter Algorithm. </title> <type> PhD thesis, </type> <institution> Montana State University, Bozman, MT, </institution> <year> 1969. </year>
Reference-contexts: Since the matrix multiplication algorithm is highly computation intensive, there has been a great deal of interest in developing parallel formulations of this algorithm and testing its performance on various parallel architectures <ref> [1, 3, 5, 6, 7, 9, 11, 17, 18, 37, 8] </ref>. Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in [3, 18]. <p> Some of the early parallel formulations of matrix multiplication were developed by Cannon <ref> [5] </ref>, Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in [3, 18]. In particular, Berntsen [3] presents an algorithm which has a strictly smaller communication overhead than Cannon's algorithm, but has a smaller degree of concurrency. <p> processor is O ( n 2 p p ) and thus the total memory requirement is O (n 2 p p) words as against O (n 2 ) for the sequential algorithm. 4.2 Cannon's Algorithm A parallel algorithm that is memory efficient and is frequently used is due to Cannon <ref> [5] </ref>. Again the two n fi n matrices A and B are divided into square submatrices of size n p p fi n p p among the p processors of a wrap-around mesh (which can be embedded in a hypercube if the algorithm was to be implemented on it). <p> the difference in the efficiencies is quite significant. 0.1 0.3 0.5 0.7 0.9 100 150 200 250 300 350 400 450 E GK Cannon's (p = 512). 10 Concluding Remarks In this paper we have presented the scalability analysis of a number of matrix multiplication algorithms described in the literature <ref> [5, 9, 11, 3, 18] </ref>. Besides analyzing these classical algorithms, we show that the GK algorithm that we present in this paper outperforms all the well known algorithms for a significant range of number of processors and matrix sizes.
Reference: [6] <author> V. Cherkassky and R. Smith. </author> <title> Efficient mapping and implementations of matrix algorithms on a hypercube. </title> <journal> The Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 7-27, </pages> <year> 1988. </year>
Reference-contexts: Since the matrix multiplication algorithm is highly computation intensive, there has been a great deal of interest in developing parallel formulations of this algorithm and testing its performance on various parallel architectures <ref> [1, 3, 5, 6, 7, 9, 11, 17, 18, 37, 8] </ref>. Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in [3, 18].
Reference: [7] <author> N. P. Chrisopchoides, M. Aboelaze, E. N. Houstis, and C. E. Houstis. </author> <title> The parallelization of some level 2 and 3 BLAS operations on distributed-memory machines. </title> <booktitle> In Proceedings of the First International Conference of the Austrian Center of Parallel Computation. Springer-Verlag Series Lecture Notes in Computer Science, </booktitle> <year> 1991. </year>
Reference-contexts: Since the matrix multiplication algorithm is highly computation intensive, there has been a great deal of interest in developing parallel formulations of this algorithm and testing its performance on various parallel architectures <ref> [1, 3, 5, 6, 7, 9, 11, 17, 18, 37, 8] </ref>. Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in [3, 18].
Reference: [8] <author> Eric F. Van de Velde. </author> <title> Multicomputer matrix computations: Theory and practice. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pages 1303-1308, </pages> <year> 1989. </year>
Reference-contexts: Since the matrix multiplication algorithm is highly computation intensive, there has been a great deal of interest in developing parallel formulations of this algorithm and testing its performance on various parallel architectures <ref> [1, 3, 5, 6, 7, 9, 11, 17, 18, 37, 8] </ref>. Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in [3, 18].
Reference: [9] <author> Eliezer Dekel, David Nassimi, and Sartaj Sahni. </author> <title> Parallel matrix and graph algorithms. </title> <journal> SIAM Journal on Computing, </journal> <volume> 10 </volume> <pages> 657-673, </pages> <year> 1981. </year>
Reference-contexts: Since the matrix multiplication algorithm is highly computation intensive, there has been a great deal of interest in developing parallel formulations of this algorithm and testing its performance on various parallel architectures <ref> [1, 3, 5, 6, 7, 9, 11, 17, 18, 37, 8] </ref>. Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in [3, 18]. <p> Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni <ref> [9] </ref>, and Fox et al. [11]. Variants and improvements of these algorithms have been presented in [3, 18]. In particular, Berntsen [3] presents an algorithm which has a strictly smaller communication overhead than Cannon's algorithm, but has a smaller degree of concurrency. <p> p + n 2 p 2=3 matrix elements per processor. 4.5 The DNS Algorithm 4.5.1 One Element Per Processor Version An algorithm that uses a hypercube with p = n 3 = 2 3q processors to multiply two n fi n matrices was proposed by Dekel, Nassimi and Sahni in <ref> [9, 35] </ref>. The p processors can be visualized as being arranged in an 2 q fi 2 q fi 2 q array. <p> There are more than one ways to adapt this 8 algorithm to use fewer than n 3 processors. The method proposed by Dekel, Nassimi and Sahni in <ref> [9, 35] </ref> is as follows. 4.5.2 Variant With More Than One Element Per Processor This variant of the DNS algorithm can work with n 2 r processors, where 1 &lt; r &lt; n, thus using one processor for more than one element of each of the two n fi n matrices. <p> the difference in the efficiencies is quite significant. 0.1 0.3 0.5 0.7 0.9 100 150 200 250 300 350 400 450 E GK Cannon's (p = 512). 10 Concluding Remarks In this paper we have presented the scalability analysis of a number of matrix multiplication algorithms described in the literature <ref> [5, 9, 11, 3, 18] </ref>. Besides analyzing these classical algorithms, we show that the GK algorithm that we present in this paper outperforms all the well known algorithms for a significant range of number of processors and matrix sizes.
Reference: [10] <author> G. C. Fox, M. Johnson, G. Lyzenga, S. W. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors: Volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: For a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances (e.g., <ref> [21, 12, 29, 34, 16, 10, 31, 33, 32, 40] </ref>). The ease with which a parallel algorithm can achieve speedups proportional to p on a parallel architecture can serve as a measure of the scalability of the parallel system. <p> time, the total parallel execution time for all the movements of the sub-blocks of both the matrices is given by the following equation: T p = p p n 2 p 4.3 Fox's Algorithm This algorithm is due to Fox et al and is described in detail in [11] and <ref> [10] </ref>. The input matrices are initially distributed among the processors in the same manner as in the algorithm in Section 4.1. The algorithm works in p p iterations, where p is the number of processors being used.
Reference: [11] <author> G. C. Fox, S. W. Otto, and A. J. G. Hey. </author> <title> Matrix algorithms on a hypercube I: Matrix multiplication. </title> <journal> Parallel Computing, </journal> <volume> 4 </volume> <pages> 17-31, </pages> <year> 1987. </year>
Reference-contexts: Since the matrix multiplication algorithm is highly computation intensive, there has been a great deal of interest in developing parallel formulations of this algorithm and testing its performance on various parallel architectures <ref> [1, 3, 5, 6, 7, 9, 11, 17, 18, 37, 8] </ref>. Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in [3, 18]. <p> Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. <ref> [11] </ref>. Variants and improvements of these algorithms have been presented in [3, 18]. In particular, Berntsen [3] presents an algorithm which has a strictly smaller communication overhead than Cannon's algorithm, but has a smaller degree of concurrency. <p> n 2 time, the total parallel execution time for all the movements of the sub-blocks of both the matrices is given by the following equation: T p = p p n 2 p 4.3 Fox's Algorithm This algorithm is due to Fox et al and is described in detail in <ref> [11] </ref> and [10]. The input matrices are initially distributed among the processors in the same manner as in the algorithm in Section 4.1. The algorithm works in p p iterations, where p is the number of processors being used. <p> the difference in the efficiencies is quite significant. 0.1 0.3 0.5 0.7 0.9 100 150 200 250 300 350 400 450 E GK Cannon's (p = 512). 10 Concluding Remarks In this paper we have presented the scalability analysis of a number of matrix multiplication algorithms described in the literature <ref> [5, 9, 11, 3, 18] </ref>. Besides analyzing these classical algorithms, we show that the GK algorithm that we present in this paper outperforms all the well known algorithms for a significant range of number of processors and matrix sizes.
Reference: [12] <author> Ananth Grama, Anshul Gupta, and Vipin Kumar. Isoefficiency: </author> <title> Measuring the scalability of parallel algorithms and architectures. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 1(3) </volume> <pages> 12-21, </pages> <month> August, </month> <year> 1993. </year> <note> Also available as Technical Report TR 93-24, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: For a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances (e.g., <ref> [21, 12, 29, 34, 16, 10, 31, 33, 32, 40] </ref>). The ease with which a parallel algorithm can achieve speedups proportional to p on a parallel architecture can serve as a measure of the scalability of the parallel system.
Reference: [13] <author> Ananth Grama, Vipin Kumar, and V. Nageshwara Rao. </author> <title> Experimental evaluation of load balancing techniques for the hypercube. </title> <booktitle> In Proceedings of the Parallel Computing '91 Conference, </booktitle> <pages> pages 497-514, </pages> <year> 1991. </year>
Reference: [14] <author> Anshul Gupta and Vipin Kumar. </author> <title> The scalability of FFT on parallel computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(8) </volume> <pages> 922-932, </pages> <month> August </month> <year> 1993. </year> <note> A detailed version available as Technical Report TR 90-53, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: In many algorithms, these constants contribute a multiplicative term to the isoefficiency function, but in some others they effect the asymptotic isoefficiency of a parallel system (e.g., parallel FFT <ref> [14] </ref>). For instance, a multiplicative term of (t w ) 3 appears in most isoefficiency functions of matrix multiplication algorithms described in this paper.
Reference: [15] <author> Anshul Gupta, Vipin Kumar, and A. H. Sameh. </author> <title> Performance and scalability of preconditioned conjugate gradient methods on parallel computers. </title> <type> Technical Report TR 92-64, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1992. </year> <booktitle> A short version appears in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 664-674, </pages> <year> 1993. </year>
Reference: [16] <author> John L. Gustafson, Gary R. Montry, and Robert E. Benner. </author> <title> Development of parallel methods for a 1024-processor hypercube. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 609-638, </pages> <year> 1988. </year>
Reference-contexts: For a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances (e.g., <ref> [21, 12, 29, 34, 16, 10, 31, 33, 32, 40] </ref>). The ease with which a parallel algorithm can achieve speedups proportional to p on a parallel architecture can serve as a measure of the scalability of the parallel system.
Reference: [17] <author> Paul G. Hipes. </author> <title> Matrix multiplication on the JPL/Caltech Mark IIIfp hypercube. </title> <type> Technical Report C3P 746, </type> <institution> Concurrent Computation Program, California Institute of Technology, Pasadena, </institution> <address> CA, </address> <year> 1989. </year>
Reference-contexts: Since the matrix multiplication algorithm is highly computation intensive, there has been a great deal of interest in developing parallel formulations of this algorithm and testing its performance on various parallel architectures <ref> [1, 3, 5, 6, 7, 9, 11, 17, 18, 37, 8] </ref>. Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in [3, 18].
Reference: [18] <author> C.-T. Ho, S. L. Johnsson, and Alan Edelman. </author> <title> Matrix multiplication on hypercubes using full bandwidth and constant storage. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 447-451, </pages> <year> 1991. </year>
Reference-contexts: Since the matrix multiplication algorithm is highly computation intensive, there has been a great deal of interest in developing parallel formulations of this algorithm and testing its performance on various parallel architectures <ref> [1, 3, 5, 6, 7, 9, 11, 17, 18, 37, 8] </ref>. Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in [3, 18]. <p> Some of the early parallel formulations of matrix multiplication were developed by Cannon [5], Dekel, Nassimi and Sahni [9], and Fox et al. [11]. Variants and improvements of these algorithms have been presented in <ref> [3, 18] </ref>. In particular, Berntsen [3] presents an algorithm which has a strictly smaller communication overhead than Cannon's algorithm, but has a smaller degree of concurrency. Ho et al. [18] present another variant of Cannon's algorithm for a hypercube which permits communication on all channels simultaneously. <p> Variants and improvements of these algorithms have been presented in [3, 18]. In particular, Berntsen [3] presents an algorithm which has a strictly smaller communication overhead than Cannon's algorithm, but has a smaller degree of concurrency. Ho et al. <ref> [18] </ref> present another variant of Cannon's algorithm for a hypercube which permits communication on all channels simultaneously. This algorithm too, while reducing communication, also reduces the degree of concurrency. <p> Hence, among the algorithms discussed in this paper, the ones that can potentially benefit from simultaneous communications on all the ports are the simple algorithm (or its variations <ref> [18] </ref>) and the GK algorithm. 7.1 The Simple Algorithm With All Port Communication This algorithm requires an all-to-all broadcast of the sub-blocks of the matrices A and B among groups of p p processors. <p> Ho et al. <ref> [18] </ref> give a memory efficient version of this algorithm which has somewhat higher execution time than that given by Equation (16). <p> However, as mentioned in <ref> [18] </ref>, the lower limit on the message size imposes the condition that n 1 2 p log p. This requires that n 3 = W 1 8 p 1:5 (log p) 3 . <p> the difference in the efficiencies is quite significant. 0.1 0.3 0.5 0.7 0.9 100 150 200 250 300 350 400 450 E GK Cannon's (p = 512). 10 Concluding Remarks In this paper we have presented the scalability analysis of a number of matrix multiplication algorithms described in the literature <ref> [5, 9, 11, 3, 18] </ref>. Besides analyzing these classical algorithms, we show that the GK algorithm that we present in this paper outperforms all the well known algorithms for a significant range of number of processors and matrix sizes.
Reference: [19] <author> Kai Hwang. </author> <title> Advanced Computer Architecture: Parallelism, Scalability, Programmability. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference: [20] <author> S. L. Johnsson and C.-T. Ho. </author> <title> Optimum broadcasting and personalized communication in hypercubes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(9) </volume> <pages> 1249-1268, </pages> <month> September </month> <year> 1989. </year> <month> 24 </month>
Reference-contexts: On a hypercube, it is possible to employ a more sophisticated scheme for one-to-all broadcast <ref> [20] </ref> of sub-blocks of matrix A among the rows. Using this scheme, the parallel execution time can be improved to n 3 p +2t w p p +t s p log p+2n t s t w log p, which is still worse than Cannon's algorithm. <p> The total parallel execution time is therefore given by the following equation: T p = p 5 t s log p + 3 n 2 This execution time can be further reduced by using a more sophisticated scheme for one-to-all broadcast on a hypercube <ref> [20] </ref>. This is discussed in detail in Section 5.4. 5 Scalability Analysis If W is the size of the problem to be solved and T o (W; P ) is the total overhead, then the efficiency E is given by W W +T o (W;p) . <p> w respectively for this algorithm: 12 n 3 = W / 3 n 3 = W / 27 w p (log p) 3 (14) The communication overheads and the isoefficiency function of the GK algorithm can be improved by using a more sophisticated scheme for one-to-all broadcast on a hypercube <ref> [20] </ref>. Due to the complexity of this scheme, we have used the simple one-to-all broadcast scheme in our implementation of this algorithm on the CM-5. We therefore use Equation (7) in Sections 6 for comparing the GK algorithm with the other algorithms discussed in this paper. <p> In <ref> [20] </ref>, Johnsson and Ho present a more sophisticated one-to-all broadcast scheme that will reduce this time to t s log p + t w m + 2t w log pd q t w log p e. <p> It can be shown that the asymptotically highest isoefficiency term now is 5 3 t s p log p which is an improvement over O (p (log p) 3 ) isoefficiency function for the naive broadcasting scheme. The broadcasting scheme of <ref> [20] </ref> requires that the message be broken up into packets and an optimal packet size to obtain the broadcast time given above is q t w log p for a message of size m. <p> This feature of the hardware can be utilized to significantly reduce the communication cost of certain operations involving broadcasting and personalized communication <ref> [20] </ref>. In this section we investigate as to what extent can the performance of the algorithms described in Section 4 can be improved by utilizing simultaneous communication on all the log p ports of the hypercube processors. <p> the number of processors in order to utilize all the communication channels of the hypercube is higher than the isoefficiency function of the algorithm implemented on a simple hypercube with one port communication at a time. 7.2 The GK Algorithm With All Port Communication Using the one-to-all broadcast scheme of <ref> [20] </ref> for a hypercube with simultaneous all-port com munication, the parallel execution time of the GK algorithm can be reduced to the following: T p = p n 2 + 6 p 1=3 t s t w (17) The communication terms now yield an isoefficiency function of O (p log p),
Reference: [21] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: For a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances (e.g., <ref> [21, 12, 29, 34, 16, 10, 31, 33, 32, 40] </ref>). The ease with which a parallel algorithm can achieve speedups proportional to p on a parallel architecture can serve as a measure of the scalability of the parallel system.
Reference: [22] <author> Vipin Kumar, Ananth Grama, and V. Nageshwara Rao. </author> <title> Scalable load balancing techniques for parallel computers. </title> <type> Technical Report 91-55, </type> <institution> Computer Science Department, University of Minnesota, </institution> <year> 1991. </year> <note> To appear in Journal of Distributed and Parallel Computing, </note> <year> 1994. </year>
Reference: [23] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <type> Technical Report TR 91-18, </type> <institution> Department of Computer Science Department, University of Minnesota, Minneapolis, MN, </institution> <year> 1991. </year> <note> To appear in Journal of Parallel and Distributed Computing, </note> <year> 1994. </year> <booktitle> A shorter version appears in Proceedings of the 1991 International Conference on Supercomputing, </booktitle> <pages> pages 396-405, </pages> <year> 1991. </year>
Reference-contexts: Scalability analysis is a an effective tool for predicting the performance of various algorithm-architecture combinations. Hence a great deal of research has been done to develop methods for scalability analysis <ref> [23] </ref>. The isoefficiency function [24, 26] is one such metric of scalability which is a measure of an algorithm's capability to effectively utilize an increasing number of processors on a parallel architecture.
Reference: [24] <author> Vipin Kumar and V. N. Rao. </author> <title> Parallel depth-first search, part II: Analysis. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(6) </volume> <pages> 501-519, </pages> <year> 1987. </year>
Reference-contexts: Scalability analysis is a an effective tool for predicting the performance of various algorithm-architecture combinations. Hence a great deal of research has been done to develop methods for scalability analysis [23]. The isoefficiency function <ref> [24, 26] </ref> is one such metric of scalability which is a measure of an algorithm's capability to effectively utilize an increasing number of processors on a parallel architecture. <p> An important feature of the isoefficiency function is that it succinctly captures the impact of communication overheads, concurrency, serial bottlenecks, load imbalance, etc. in a single expression. In this paper, we use the isoefficiency metric <ref> [24] </ref> to analyze the scalability of a number of parallel formulations of the matrix multiplication algorithm for hypercube and related architectures. <p> The ease with which a parallel algorithm can achieve speedups proportional to p on a parallel architecture can serve as a measure of the scalability of the parallel system. The isoefficiency function <ref> [24, 26] </ref> is one such metric of scalability which is a measure of an algorithm's capability to effectively utilize an increasing number of processors on a parallel architecture.
Reference: [25] <author> Vipin Kumar and V. N. Rao. </author> <title> Load balancing on the hypercube architecture. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pages 603-608, </pages> <year> 1989. </year>
Reference: [26] <author> Vipin Kumar and V. N. Rao. </author> <title> Scalable parallel formulations of depth-first search. </title> <editor> In Vipin Kumar, P. S. Gopalakrishnan, and Laveen N. Kanal, editors, </editor> <booktitle> Parallel Algorithms for Machine Intelligence and Vision. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: Scalability analysis is a an effective tool for predicting the performance of various algorithm-architecture combinations. Hence a great deal of research has been done to develop methods for scalability analysis [23]. The isoefficiency function <ref> [24, 26] </ref> is one such metric of scalability which is a measure of an algorithm's capability to effectively utilize an increasing number of processors on a parallel architecture. <p> The ease with which a parallel algorithm can achieve speedups proportional to p on a parallel architecture can serve as a measure of the scalability of the parallel system. The isoefficiency function <ref> [24, 26] </ref> is one such metric of scalability which is a measure of an algorithm's capability to effectively utilize an increasing number of processors on a parallel architecture.
Reference: [27] <author> Vipin Kumar and Vineet Singh. </author> <title> Scalability of Parallel Algorithms for the All-Pairs Shortest Path Problem: </title>
References-found: 27

