URL: http://www.cse.ogi.edu/~berkling/paper.ps.Z
Refering-URL: http://www.cse.ogi.edu/~berkling/kay.html
Root-URL: http://www.cse.ogi.edu
Title: Automatic Language Identification with Sequences of Language-Independent Phoneme Clusters  
Author: Kay Margarethe Berkling 
Degree: A dissertation submitted to the faculty of the Oregon Graduate Institute of Science Technology in partial fulfillment of the requirements for the degree Doctor of Philosophy in Computer Science and Engineering  
Date: 1991  October 1996  
Note: 1991 B.A. French, Syracuse University 1991 B.A. German, Syracuse University  
Address: 1991  
Affiliation: B.S. Computer Engineering, Syracuse University  B.S. Mathematics, Syracuse University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Andersen, O., Dalsgaard, P., and Barry, W. </author> <title> On the use of data-driven clustering technique for identification of poly- and mono-phonemes for four european languages. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Adelaide, </booktitle> <address> Australia, </address> <month> April </month> <year> 1994), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 121 - 124. </pages>
Reference-contexts: Dalsgaard uses this knowledge to build a language identification system. Dalsgaard distinguishes between language dependent phonemes (mono-phones) and language independent phonemes (poly-phones) [24]. By using the similarity of acoustic phonetic features or data-driven clustering (employing a similarity measure based on a global confusion matrix <ref> [2, 1] </ref>), he either clusters phonemes across languages or identifies them as mono-phonemes. This establishes a super set of labels valid across all languages to label a multilingual database used for training phonemic recognizers [6]. <p> Assuming independence of features, we next derive a vector ~ff which will be used in the weighted scalar product with the feature vector to produce a linear mapping onto a single dimensional space as depicted in Figure 3.3. = ff 1 u <ref> [1] </ref> + ff 2 u [2] 1 s [1] 2 + ff 2 (3.12) The goal is to derive the vector ~ff which minimizes the error. In order to find the optimal ff, the error will be differentiated with respect to ~ff. <p> Assuming independence of features, we next derive a vector ~ff which will be used in the weighted scalar product with the feature vector to produce a linear mapping onto a single dimensional space as depicted in Figure 3.3. = ff 1 u <ref> [1] </ref> + ff 2 u [2] 1 s [1] 2 + ff 2 (3.12) The goal is to derive the vector ~ff which minimizes the error. In order to find the optimal ff, the error will be differentiated with respect to ~ff. <p> 2 ( @ 2 + @ 1 ) 4 ( 2 2 ) 2 ( 1 @ 2 @ff 2 ) = 0 2 ( @ff 2 @ff 2 ) = 1 2 1 1 + 2 ( 1 @ 2 @ff 2 ) Since 2 i = s i <ref> [1] </ref> 2 + ff 2 and i = u i [1] + ff 2 u i [2] 2 2 2 ) u 2 [2] u 1 [2] = 2 1 1 + 2 ff 2 (s 1 [2] 2 + s 2 [2] 2 ) Let S [i] 2 = s <p> 2 2 ) 2 ( 1 @ 2 @ff 2 ) = 0 2 ( @ff 2 @ff 2 ) = 1 2 1 1 + 2 ( 1 @ 2 @ff 2 ) Since 2 i = s i <ref> [1] </ref> 2 + ff 2 and i = u i [1] + ff 2 u i [2] 2 2 2 ) u 2 [2] u 1 [2] = 2 1 1 + 2 ff 2 (s 1 [2] 2 + s 2 [2] 2 ) Let S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 <p> i [2] 2 2 2 ) u 2 [2] u 1 [2] = 2 1 1 + 2 ff 2 (s 1 [2] 2 + s 2 [2] 2 ) Let S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 <ref> [1] </ref> + ff 2 u 2 [2]) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u <p> u 1 [2] = 2 1 1 + 2 ff 2 (s 1 [2] 2 + s 2 [2] 2 ) Let S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 <ref> [1] </ref> + ff 2 u 2 [2]) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( <p> 2 (s 1 [2] 2 + s 2 [2] 2 ) Let S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 <ref> [1] </ref> + ff 2 u 2 [2]) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( u 2 [1]u 1 [1] )( s 2 [2] 2 <p> [2] 2 + s 2 [2] 2 ) Let S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 <ref> [1] </ref> + ff 2 u 2 [2]) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( u 2 [1]u 1 [1] )( s 2 [2] 2 +s 1 [2] <p> [i] 2 + s 2 [i] 2 then 2 1 = (u 2 <ref> [1] </ref> + ff 2 u 2 [2]) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( u 2 [1]u 1 [1] )( s 2 [2] 2 +s 1 [2] 2 ) 42 The weighting of the new word is proportional to the difference in <p> then 2 1 = (u 2 <ref> [1] </ref> + ff 2 u 2 [2]) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( u 2 [1]u 1 [1] )( s 2 [2] 2 +s 1 [2] 2 ) 42 The weighting of the new word is proportional to the difference in mean occurrence frequency between the two languages. <p> [2]) (u 1 <ref> [1] </ref> + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( u 2 [1]u 1 [1] )( s 2 [2] 2 +s 1 [2] 2 ) 42 The weighting of the new word is proportional to the difference in mean occurrence frequency between the two languages. <p> 1 [2]) = (u 2 <ref> [1] </ref> u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( u 2 [1]u 1 [1] )( s 2 [2] 2 +s 1 [2] 2 ) 42 The weighting of the new word is proportional to the difference in mean occurrence frequency between the two languages. <p> Given the optimal ~ff the estimated error between the two languages is guaranteed to decrease after adding the second word. 1 e 4 [ ( 2 1) 2 1 + 2 ] 1 e 4 [ (u 2 <ref> [1] </ref>u 1 [1]) 2 (3.14) In order to estimate the error based on a list of N sequences the top N sequences in the aligned strings from both languages, sorted by their estimated error according to Equation 3.13 1 , are chosen. <p> Adding a word to a given list results in the following distributions for the language pair i = 1,2 (assuming independence of sequence occurrences): List ' N (u i <ref> [1] </ref> + ffu i [2]; s i [1] 2 + ff 2 s i [2] 2 ) = N ( i ; i ) And their combined error is estimated as: 1 e 4 [ ( 2 1 ) 2 1 + 2 ] In theory there is no limit to <p> Adding a word to a given list results in the following distributions for the language pair i = 1,2 (assuming independence of sequence occurrences): List ' N (u i <ref> [1] </ref> + ffu i [2]; s i [1] 2 + ff 2 s i [2] 2 ) = N ( i ; i ) And their combined error is estimated as: 1 e 4 [ ( 2 1 ) 2 1 + 2 ] In theory there is no limit to the number of sequences used for discriminating
Reference: [2] <author> Andersen, O., Dalsgaard, P., and Barry, W. </author> <title> Data-driven identification of poly- and mono-phonemes for four european languages. </title> <booktitle> In Proceedings Eurospeech 93 (Berlin, </booktitle> <address> Germany, </address> <month> September </month> <year> 1993), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 759-762. </pages>
Reference-contexts: Dalsgaard uses this knowledge to build a language identification system. Dalsgaard distinguishes between language dependent phonemes (mono-phones) and language independent phonemes (poly-phones) [24]. By using the similarity of acoustic phonetic features or data-driven clustering (employing a similarity measure based on a global confusion matrix <ref> [2, 1] </ref>), he either clusters phonemes across languages or identifies them as mono-phonemes. This establishes a super set of labels valid across all languages to label a multilingual database used for training phonemic recognizers [6]. <p> Assuming independence of features, we next derive a vector ~ff which will be used in the weighted scalar product with the feature vector to produce a linear mapping onto a single dimensional space as depicted in Figure 3.3. = ff 1 u [1] + ff 2 u <ref> [2] </ref> 1 s [1] 2 + ff 2 (3.12) The goal is to derive the vector ~ff which minimizes the error. In order to find the optimal ff, the error will be differentiated with respect to ~ff. <p> @ 2 @ff 2 ) = 0 2 ( @ff 2 @ff 2 ) = 1 2 1 1 + 2 ( 1 @ 2 @ff 2 ) Since 2 i = s i [1] 2 + ff 2 and i = u i [1] + ff 2 u i <ref> [2] </ref> 2 2 2 ) u 2 [2] u 1 [2] = 2 1 1 + 2 ff 2 (s 1 [2] 2 + s 2 [2] 2 ) Let S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 [1] + <p> 2 ( @ff 2 @ff 2 ) = 1 2 1 1 + 2 ( 1 @ 2 @ff 2 ) Since 2 i = s i [1] 2 + ff 2 and i = u i [1] + ff 2 u i <ref> [2] </ref> 2 2 2 ) u 2 [2] u 1 [2] = 2 1 1 + 2 ff 2 (s 1 [2] 2 + s 2 [2] 2 ) Let S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 [1] + ff 2 u 2 [2]) (u 1 <p> 2 @ff 2 ) = 1 2 1 1 + 2 ( 1 @ 2 @ff 2 ) Since 2 i = s i [1] 2 + ff 2 and i = u i [1] + ff 2 u i <ref> [2] </ref> 2 2 2 ) u 2 [2] u 1 [2] = 2 1 1 + 2 ff 2 (s 1 [2] 2 + s 2 [2] 2 ) Let S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 [1] + ff 2 u 2 [2]) (u 1 [1] + ff <p> ( 1 @ 2 @ff 2 ) Since 2 i = s i [1] 2 + ff 2 and i = u i [1] + ff 2 u i <ref> [2] </ref> 2 2 2 ) u 2 [2] u 1 [2] = 2 1 1 + 2 ff 2 (s 1 [2] 2 + s 2 [2] 2 ) Let S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 [1] + ff 2 u 2 [2]) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) <p> 2 ) Since 2 i = s i [1] 2 + ff 2 and i = u i [1] + ff 2 u i <ref> [2] </ref> 2 2 2 ) u 2 [2] u 1 [2] = 2 1 1 + 2 ff 2 (s 1 [2] 2 + s 2 [2] 2 ) Let S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 [1] + ff 2 u 2 [2]) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 <p> u 2 <ref> [2] </ref> u 1 [2] = 2 1 1 + 2 ff 2 (s 1 [2] 2 + s 2 [2] 2 ) Let S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 [1] + ff 2 u 2 [2]) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] <p> 1 + 2 ff 2 (s 1 <ref> [2] </ref> 2 + s 2 [2] 2 ) Let S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 [1] + ff 2 u 2 [2]) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( u 2 [1]u 1 [1] )( <p> 2 ) Let S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 [1] + ff 2 u 2 <ref> [2] </ref>) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( u 2 [1]u 1 [1] )( s 2 [2] 2 +s 1 [2] 2 ) 42 The weighting of <p> S [i] 2 = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 [1] + ff 2 u 2 <ref> [2] </ref>) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( u 2 [1]u 1 [1] )( s 2 [2] 2 +s 1 [2] 2 ) 42 The weighting of the new word <p> = s 1 [i] 2 + s 2 [i] 2 then 2 1 = (u 2 [1] + ff 2 u 2 <ref> [2] </ref>) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( u 2 [1]u 1 [1] )( s 2 [2] 2 +s 1 [2] 2 ) 42 The weighting of the new word is proportional to <p> ff 2 u 2 <ref> [2] </ref>) (u 1 [1] + ff 2 u 1 [2]) = (u 2 [1] u 1 [1]) + ff 2 (u 2 [2] u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( u 2 [1]u 1 [1] )( s 2 [2] 2 +s 1 [2] 2 ) 42 The weighting of the new word is proportional to the difference in mean occurrence frequency between the two languages. <p> 2 [1] u 1 [1]) + ff 2 (u 2 <ref> [2] </ref> u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( u 2 [1]u 1 [1] )( s 2 [2] 2 +s 1 [2] 2 ) 42 The weighting of the new word is proportional to the difference in mean occurrence frequency between the two languages. <p> [1]) + ff 2 (u 2 <ref> [2] </ref> u 1 [2]) ) 4u [2] = S [1] 2 +ff 2 ) 4u [2]S [1] 2 + ff 2 2 4u [2]S [2] 2 4u [2]S [1] 2 = ( u 2 [1]u 1 [1] )( s 2 [2] 2 +s 1 [2] 2 ) 42 The weighting of the new word is proportional to the difference in mean occurrence frequency between the two languages. <p> Adding a word to a given list results in the following distributions for the language pair i = 1,2 (assuming independence of sequence occurrences): List ' N (u i [1] + ffu i <ref> [2] </ref>; s i [1] 2 + ff 2 s i [2] 2 ) = N ( i ; i ) And their combined error is estimated as: 1 e 4 [ ( 2 1 ) 2 1 + 2 ] In theory there is no limit to the number of sequences <p> Adding a word to a given list results in the following distributions for the language pair i = 1,2 (assuming independence of sequence occurrences): List ' N (u i [1] + ffu i <ref> [2] </ref>; s i [1] 2 + ff 2 s i [2] 2 ) = N ( i ; i ) And their combined error is estimated as: 1 e 4 [ ( 2 1 ) 2 1 + 2 ] In theory there is no limit to the number of sequences used for discriminating between two languages.
Reference: [3] <author> Andersen, O., Dalsgaard, P., and Hansen, A. </author> <title> Multi-lingual testing of a self-learning approach to phonemic transcription of orthography. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 1117-1120. </pages>
Reference: [4] <author> Barnard, E., and Cole, R. A. </author> <title> A neural-net training program based on conjugate-gradient optimization. </title> <type> Tech. Rep. CSE 89-014, </type> <institution> Oregon Graduate Institute, </institution> <year> 1989. </year>
Reference-contexts: The neural network classifiers used here are fully-connected, feed-forward networks trained using back-propagation with conjugate gradient optimization <ref> [4] </ref> using a mean squared error criterion. Such a neural network has three layers. Each node in the first layer corresponds to an input feature derived from the waveform. An equal number of frames from each of the phoneme classes are sampled according to the labeled database.
Reference: [5] <author> Barnett, J., Bamberg, P., Demedts, A., Even, S. V., and Manganaro, L. </author> <title> Comparative performance in large-vocabulary isolated word recognition in five european languages. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 189-192. </pages>
Reference: [6] <author> Barry, W., and Dalsgaard, P. </author> <title> Speech database annotation. the importance of a multi-lingual approach. </title> <booktitle> In Proceedings Eurospeech 93 (Berlin, </booktitle> <address> Germany, </address> <month> September </month> <year> 1993), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 759-762. </pages>
Reference-contexts: This establishes a super set of labels valid across all languages to label a multilingual database used for training phonemic recognizers <ref> [6] </ref>. He thus is able to increase the training data for phoneme recognizers by merging poly-phones across 22 returns a likelihood score. The maximum score identifies the language of the speech. languages and improving his phoneme recognition in each of the languages.
Reference: [7] <author> Beattie, V., and Rohlicek, J. R. </author> <title> An integrated multi-dialect speech recognition system with optional speaker adaptation. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 1123-1126. </pages>
Reference: [8] <author> Berkling, K. M., Arai, T., Barnard, E., and R.A.Cole. </author> <title> Analysis of phoneme-based features for language identification. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Adelaide, </booktitle> <address> Australia, </address> <month> April </month> <year> 1994), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 289 - 292. </pages>
Reference-contexts: For this purpose we distinguish between two types of phonemes: Mono-phonemes Phonemes occurring in one language Poly-phonemes Phonemes similar across languages As was shown in <ref> [8] </ref>, most of the language dependent information is concentrated in the mono-phonemes and not in the poly-phonemes 1 . Therefore, there is a high degree of redundancy in recognizing speech at the phoneme level with respect to the set of poly-phonemes. <p> In Dalsgaard [24], Berkling <ref> [8] </ref> and Zissman [112] language dependent phonemes, called mono-phonemes or key-phones, have been shown to contain most of the language discriminating information. Dalsgaard uses this knowledge to build a language identification system. Dalsgaard distinguishes between language dependent phonemes (mono-phones) and language independent phonemes (poly-phones) [24]. <p> The above approaches are all related to one another and were developed during the same time as the sequence of research which forms the core of this thesis <ref> [78, 108, 8, 11, 9, 10] </ref>. We also base language recognition on keyword selection, detection and classification. The language classification in this thesis is non-linear and the weighted keyword selection is based on the Bhattacharyya distance.
Reference: [9] <author> Berkling, K. M., and Barnard, E. </author> <title> Theoretical error prediction for a language identification system using optimal phoneme clustering. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 351-354. </pages>
Reference-contexts: We believe that this amount of detailed modeling may not be necessary <ref> [11, 9] </ref>. The issue is to show that it is feasible to replace the parallel approach, which models all the phonemes in all the languages, with a more conservative 12 speech representation. <p> The above approaches are all related to one another and were developed during the same time as the sequence of research which forms the core of this thesis <ref> [78, 108, 8, 11, 9, 10] </ref>. We also base language recognition on keyword selection, detection and classification. The language classification in this thesis is non-linear and the weighted keyword selection is based on the Bhattacharyya distance.
Reference: [10] <author> Berkling, K. M., and Barnard, E. </author> <title> Language identification with inexact sequence matching. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Philadelphia, </booktitle> <address> USA, </address> <month> October </month> <year> 1996), </year> <pages> pp. 1796-1799. </pages>
Reference-contexts: The above approaches are all related to one another and were developed during the same time as the sequence of research which forms the core of this thesis <ref> [78, 108, 8, 11, 9, 10] </ref>. We also base language recognition on keyword selection, detection and classification. The language classification in this thesis is non-linear and the weighted keyword selection is based on the Bhattacharyya distance.
Reference: [11] <author> Berkling, K. M., and Barnard, E. </author> <title> Language identification with multilingual phoneme clusters. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Yokohama, </booktitle> <address> Japan, </address> <month> September </month> <year> 1994), </year> <journal> vol. </journal> <volume> 4, </volume> <pages> pp. 1891-1894. 115 116 </pages>
Reference-contexts: We believe that this amount of detailed modeling may not be necessary <ref> [11, 9] </ref>. The issue is to show that it is feasible to replace the parallel approach, which models all the phonemes in all the languages, with a more conservative 12 speech representation. <p> The above approaches are all related to one another and were developed during the same time as the sequence of research which forms the core of this thesis <ref> [78, 108, 8, 11, 9, 10] </ref>. We also base language recognition on keyword selection, detection and classification. The language classification in this thesis is non-linear and the weighted keyword selection is based on the Bhattacharyya distance.
Reference: [12] <author> Bourlard, H. </author> <title> Towards increasing speech recognition error rates. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 883-894. </pages>
Reference-contexts: It has recently been pointed out that the path to improved speech-recognition systems does not necessarily imply improved performance at every step along the way <ref> [12, 13] </ref>. Instead, some steps need to be taken even if they increase error rates; other criteria (such as robustness, elegance, or extendability) should be used to evaluate such steps. We believe the same to be true in language identification.
Reference: [13] <author> Bourlard, H., Hermansky, H., and Morgan, N. </author> <title> Towards increasing speech recognition error rates. </title> <journal> Speech Communications 18, </journal> <month> 3 (May </month> <year> 1996), </year> <pages> 205-231. </pages>
Reference-contexts: It has recently been pointed out that the path to improved speech-recognition systems does not necessarily imply improved performance at every step along the way <ref> [12, 13] </ref>. Instead, some steps need to be taken even if they increase error rates; other criteria (such as robustness, elegance, or extendability) should be used to evaluate such steps. We believe the same to be true in language identification.
Reference: [14] <author> Calvert, D. R. </author> <title> Descriptive Phonetics, 2nd ed. </title> <publisher> Georg Thieme Verlag Stuttgart, </publisher> <year> 1986. </year>
Reference-contexts: The algorithm introduced in this thesis automatically selects features without restriction to solve the particular problems of any given language pair. 3. Discrimination between Dialects Dialects refer to the words, language forms, pronunciations, and speech habits peculiar to people of specific geographic regions <ref> [14] </ref>. The reason our system is flexible enough for this type of application is similar to item 2 above because there is a fine line between similar languages and dialects.
Reference: [15] <author> Carey, M., and Parris, E. </author> <title> Topic spotting with task independent models. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 3, </volume> <pages> pp. 2133-2136. </pages>
Reference: [16] <author> Carey, M. J., and Parris, E. S. </author> <title> Topic spotting with task independent models. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 3, </volume> <pages> pp. 2133-2136. </pages>
Reference-contexts: This algorithm can be applied directly to language identification, even though, rather than looking for rare, long words one now selects frequent and mostly shorter pseudo-words. Other work closely related to this approach is recent work done at Ensigma relating to topic identification and language identification <ref> [16, 85, 101] </ref>. The above approaches are all related to one another and were developed during the same time as the sequence of research which forms the core of this thesis [78, 108, 8, 11, 9, 10]. We also base language recognition on keyword selection, detection and classification.
Reference: [17] <author> Cole, R. A., Inouye, J. W. T., Muthusamy, Y. K., and Gopalakrishnan, M. </author> <title> Language identification with neural networks: a feasibility study. </title> <booktitle> In Proceedings IEEE Pacific Rim Conference on Communications, Computers, and Signal Processing (June 1989), </booktitle> <pages> pp. 525-529. </pages>
Reference: [18] <author> Comrie, B. </author> <title> The World's Major Languages, 1st ed. </title> <publisher> Oxford University Press, </publisher> <year> 1990. </year>
Reference-contexts: Some languages may differ based on broad categories and their sequences [76]. For example, German, which has sequences of consonants is distinguished from Japanese due to its inherent structure where each consonant is followed by a vowel (consonant-vowel structure) <ref> [18] </ref>. While broad categories are a linguistically sound representation of multilingual speech (broad categories are valid across languages) they generally discard information which may be useful in distinguish languages. A review of these approaches is presented in Section 2.1.
Reference: [19] <author> Cook, G., and Robinson, T. </author> <title> Utterance clustering for large vocabulary continuous speech recognition. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 141-144. </pages>
Reference: [20] <author> Cover, T. M., and Thomas, J. A. </author> <title> Elements of Information Theory, 2nd ed. </title> <publisher> John Wiley, </publisher> <year> 1991. </year>
Reference: [21] <author> Crystal, D. </author> <title> The Cambridge Encyclopedia of Language. </title> <publisher> Cambridge University Press, </publisher> <year> 1987, </year> <pages> pp. </pages> <month> 160-169,280-339. </month>
Reference: [22] <author> Cutler, A., Davis, S. M., and Otake, T. </author> <title> Listeners' representations of within-word structure: a cross-linguistic and cross-dialectral investigation. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 3, </volume> <pages> pp. 1703-1706. </pages>
Reference: [23] <author> Dalsgaard, P. </author> <title> Phoneme label alignment using acoustic-phonetic features and gaussian probability density functions. </title> <booktitle> Computer Speech and Language 6 (1992), </booktitle> <pages> 303-329. </pages>
Reference: [24] <author> Dalsgaard, P., and Andersen, O. </author> <title> Identification of mono- and poly-phonemes using acoustic-phonetic features derived by a self-organizing neural network. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Banff, </booktitle> <month> October </month> <year> 1992), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 547-550. </pages>
Reference-contexts: In Dalsgaard <ref> [24] </ref>, Berkling [8] and Zissman [112] language dependent phonemes, called mono-phonemes or key-phones, have been shown to contain most of the language discriminating information. Dalsgaard uses this knowledge to build a language identification system. Dalsgaard distinguishes between language dependent phonemes (mono-phones) and language independent phonemes (poly-phones) [24]. <p> In Dalsgaard <ref> [24] </ref>, Berkling [8] and Zissman [112] language dependent phonemes, called mono-phonemes or key-phones, have been shown to contain most of the language discriminating information. Dalsgaard uses this knowledge to build a language identification system. Dalsgaard distinguishes between language dependent phonemes (mono-phones) and language independent phonemes (poly-phones) [24]. By using the similarity of acoustic phonetic features or data-driven clustering (employing a similarity measure based on a global confusion matrix [2, 1]), he either clusters phonemes across languages or identifies them as mono-phonemes.
Reference: [25] <author> Dalsgaard, P., and Andersen, O. </author> <title> Application of inter-language phoneme similarities for language-identification. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Yokohama, </booktitle> <address> Japan, </address> <month> September </month> <year> 1994), </year> <journal> vol. </journal> <volume> 4, </volume> <pages> pp. 1903-1906. </pages>
Reference-contexts: The main result is the ability to substitute poly-phonemes for their language dependent counterparts without losing recognition accuracy on the average. Dalsgaard employs the mono-phones for language identification by modeling each language in terms of the corresponding mono-phones and the union of all poly-phones across all languages <ref> [25] </ref>. A grammarless Viterbi search returns an automatically derived alignment of an incoming utterance by using each of the language dependent models. A post-phoneme weighting algorithm emphasizes mono-phoneme occurrences and returns a probability score associated with the alignment. The language model returning the highest score is the language identified.
Reference: [26] <author> Dalsgaard, P., and et al. </author> <title> On the use of acoustic-phonetic features in ineractive labelling of multi-lingual speech corpora. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (San Francisco, USA, </booktitle> <volume> March 92), vol. 1, </volume> <pages> pp. 549 - 552. 117 </pages>
Reference: [27] <author> Eady, S. J. </author> <title> Differences in the F 0 patterns of speech: Tone language versus stress language. Language and Speech 25, </title> <booktitle> 1 (1982), </booktitle> <pages> 29-42. </pages>
Reference: [28] <author> Fanty, M., Pochmara, J., and Cole, R. </author> <title> An interactive environment for speech recognition research. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Banff,Alberta,Canada, </booktitle> <month> October </month> <year> 1992), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 1543-1547. </pages>
Reference: [29] <author> Foil, J. T. </author> <title> Language identification using noisy speech. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (April 1986), </booktitle> <volume> vol. 2, </volume> <pages> pp. 861-864. </pages>
Reference: [30] <author> Fukunaga, K. </author> <title> Introduction to Statistical Pattern Recognition, 2nd ed. </title> <publisher> Academic Press, Inc., </publisher> <year> 1990. </year>
Reference-contexts: Given the distribution of sequence occurrences as derived in the previous section, the discrimination error for a pair of languages based on a one-dimensional feature space of one sequence is estimated with the Bhattacharyya distance <ref> [30] </ref> as given here. 39 error = 1 e 4 [ (u 2 [L]u 1 [L]) 2 2 log 1 s 1 [L] 2 +s 2 [L] 2 (3.11) Suppose now, that we want to estimate the joint error due to any two sequences resulting in a two dimensional feature space.
Reference: [31] <author> Gish, H., Ng, K., and Rohlicek, R. </author> <title> Secondary processing using speech segments for an hmm word spotting system. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Banff,Alberta,Canada, </booktitle> <month> October </month> <year> 1992), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 17-20. </pages>
Reference-contexts: As pointed out earlier, one of the problems with using word spotting is that the process becomes topic dependent. 2.2.2 Topic Identification with Word/Sequence Spotting Gish et al. approached language identification by applying algorithms developed originally for topic identification <ref> [31, 93, 70, 65] </ref> The solution is decomposed into three subtasks: (1) Keyword selection, (2) topic modeling, and (3) event detection. Keywords are selected based on a score which calculates how much each word contributes to the discrimination between any two given topics.
Reference: [32] <author> Goeschel, J. </author> <title> Artikulation und Distribution der sogenannten Liquida r in den Europaeischen Sprachen. </title> <booktitle> Indogermanische Forschungen 76 (1971), </booktitle> <pages> 84-126. </pages>
Reference: [33] <author> Goodman, F. J., Martin, A. F., and Wohlford, R. E. </author> <title> Improved automatic language identification in noisy speech. </title> <booktitle> In International Conference of the American Society for Signal Processing (1989), </booktitle> <pages> pp. 528-531. </pages>
Reference: [34] <author> Grover, C., Jamieson, D. G., and Dobrovolsky, M. B. </author> <title> Intonation in English, French and German: perception and production. Language and Speech 30, </title> <booktitle> 3 (1987), </booktitle> <pages> 277-295. </pages>
Reference: [35] <author> Hanley, T. D., Snidecor, J. C., and Ringel, R. L. </author> <title> Some acoustic differences among languages. </title> <type> Phonetica 14, </type> <month> 2 </month> <year> (1966), </year> <pages> 97-107. </pages>
Reference: [36] <author> Hazen, T. J. </author> <title> Automatic Language Identification Using a Segment-based Approach. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Frequency Modeling of Sequences. 2. Sequence Selection and Error Estimation. 3. Phoneme Merging. The flowchart shown in Figure 3.7 depicts the iterative process of merging phonemes and estimating language classification error. At the extremes of this algorithm we obtain either a phoneme based system <ref> [36] </ref> or a broad category based system [76]. This algorithm will return the lowest number of phoneme clusters without losing language discriminability, together with an error estimate as a function of time due to the chosen features.
Reference: [37] <author> Hazen, T. J., and Zue, V. W. </author> <title> Recent improvements in an approach to segment-based automatic language identification. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Yokohama, </booktitle> <address> Japan, </address> <month> September </month> <year> 1994), </year> <journal> vol. </journal> <volume> 4, </volume> <pages> pp. 1883-1886. </pages>
Reference-contexts: The most interesting result to note from this work is that he is able to distinguish between three Asian languages (Chinese, Korean, and Japanese) and three European languages (German, French, and English) by using parameters derived from ratios of occurrence frequencies of the pitch slopes. Hazen <ref> [37] </ref> on the other hand was less successful when using pitch, prosody, and duration as a standalone system, or when adding pitch to his existing language identification systems. Hazen integrates the prosodic model with acoustic modeling, a phonological language model, and a-priori language probability.
Reference: [38] <author> Hermansky, H. </author> <title> Perceptual linear predictive (plp) analysis of speech. </title> <journal> Journal of the Acoustic Society of America 4 (April 1990), </journal> <pages> 1738-1752. </pages>
Reference-contexts: In order to recognize this type of noisy speech we rely on previous research in speech recognition. We choose to use a signal representation called PLP (perceptual linear prediction) which has been shown to work well on telephone speech <ref> [38] </ref>. Once the signal is thus represented, a neural network is used to classify the signal in terms of a linguistic speech unit such as a phoneme for example [89]. An important part of this chapter is the selection of the speech units which the neural network will discriminate. <p> An equal number of frames from each of the phoneme classes are sampled according to the labeled database. In this thesis, a frame is defined to be a 6ms window of the digitized speech. The acoustic input is represented with a seventh order Perceptual Linear Predictive (PLP) model <ref> [38] </ref>, yielding 8 coefficients (including one for energy). This representation is a modification of linear predictive coding taking into account knowledge about the human hearing mechanism.
Reference: [39] <author> Hieronymus, J. L. </author> <title> ASCII phonetic symbols for the world's languages: Worldbet. </title> <type> Tech. rep., </type> <institution> AT&T Bell Laboratories, </institution> <address> Murray Hill, NJ 07974 USA, </address> <year> 1994. </year> <month> ftp://speech.cse.ogi.edu/pub/docs/worldbet.ps. </month>
Reference-contexts: All story files were labeled with worldbet <ref> [39] </ref>, a new ASCII encoding of the International Phonetic Alphabet (IPA) including non-European languages (see Appendix A). In most cases the symbols consist of a concatenation of an IPA symbol with diacritics.
Reference: [40] <author> House, A. S., and Neuberg, E. P. </author> <title> Toward automatic identification of the language of an utterance: Priliminary methodological considerations. </title> <journal> Journal of the Acoustical Society of America 62, </journal> <volume> 3 (1977), </volume> <pages> 708-713. </pages>
Reference: [41] <author> Irii, H., Itoh, K., and Kitawaki, N. </author> <title> Multilingual speech database for evaluating quality of digitized speech. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Kobe, Japan, 1990), </booktitle> <volume> vol. 2, </volume> <pages> pp. 1025-1028. 118 </pages>
Reference: [42] <author> Itahashi, S., and Du, L. </author> <title> Language identification based on speech fundamental frequency. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 1359-1362. </pages>
Reference-contexts: Pitch. Pitch has been shown by Itahashi to be useful when basing the identification exclusively on this feature <ref> [42, 43] </ref>. Itahashi uses prosody as the sole feature in his language identification system. He argues that fundamental frequency is more robust than segmental parameters in noisy environments.
Reference: [43] <author> Itahashi, S., Zhou, J., and Tanaka, K. </author> <title> Spoken language discrimination using speech fundamental frequency. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Yokohama, </booktitle> <address> Japan, </address> <month> September </month> <year> 1994), </year> <journal> vol. </journal> <volume> 4, </volume> <pages> pp. 1899-1902. </pages>
Reference-contexts: Pitch. Pitch has been shown by Itahashi to be useful when basing the identification exclusively on this feature <ref> [42, 43] </ref>. Itahashi uses prosody as the sole feature in his language identification system. He argues that fundamental frequency is more robust than segmental parameters in noisy environments.
Reference: [44] <author> Jakobson, R., Fant, C. M., and Halle, M. </author> <title> Preliminaries to Speech Analysis. The Distinctive Features and their Correlates, 7th ed. </title> <publisher> The MIT Press, </publisher> <year> 1967. </year>
Reference: [45] <author> Jelinek, F. </author> <title> Readings in Speech Recognition. </title> <publisher> Morgan Kaufman Publishers Inc., </publisher> <address> San Mateo, California, USA, </address> <year> 1990, </year> <title> Ch. </title> <booktitle> Language Processing for Speech Recognition, </booktitle> <pages> pp. 450-507. </pages>
Reference-contexts: Since this procedure decreases the amount of training data available per parameter, emphasis is put on increasing the available training data via usage of phoneme sequences derived from textual sources. Triphone probability is traditionally <ref> [45] </ref> estimated as follows: P r (s 3 js 1 ; s 2 ) = 3 f (s 3 js 1 ; s 2 ) + 2 f (s 3 js 2 ) + 1 f (s 3 ); where s i denotes phoneme symbol i, f () denotes the frequency
Reference: [46] <author> Jongenburger, W., and Heuven, V. V. </author> <title> The role of linguistic stress in the time course of word recognition in stress-acent languages. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 3, </volume> <pages> pp. 1695-1698. </pages>
Reference: [47] <author> Kadambe, S., and Hieronymus, J. L. </author> <title> Spontaneous speech language identification with a knowledge of linguistics. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Yokohama, </booktitle> <address> Japan, </address> <month> September </month> <year> 1994), </year> <journal> vol. </journal> <volume> 4, </volume> <pages> pp. 1879-1882. </pages>
Reference-contexts: Virtually all the best language-identification systems today use fine phoneme recognition and language modeling ( the exception is the system by K.P.Li based on the spectral representation of speech [64], discussed in Section 2.3). These systems <ref> [104, 47, 113, 57] </ref> usually have a considerable degree of complexity. Complexity and language identification accuracy can be increased by adding models for additional variabilities such as gender and phoneme duration [56, 57, 112]. <p> The use of sub-word modeling maintains task independence. It is clear that incorporating word level recognition directly into the complex systems described above would be a formidable task. This section outlines some approaches taken where word or sub-word modeling is used. Triphone Modeling In Kadambe <ref> [47] </ref> the sub-word models consist of triphones. The assumption here is that triphones are optimal sub-word models regardless of the languages in the system. In going from context independent to context dependent phoneme modeling, more linguistic knowledge can be applied and therefore an additional source of discriminative features is captured.
Reference: [48] <author> Kotani, M., and Matsumoto, H. </author> <title> Sound perception between two languages based on analyses of onomatopoeic expression. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 3, </volume> <pages> pp. 2263-2266. </pages>
Reference: [49] <author> Kwan, H., and Hirose, K. </author> <title> Recognized phoneme-based n-gram modeling in automatic language identification. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 1367-1370. </pages>
Reference-contexts: The goal in this thesis was to find a set of phoneme-like tokens to represent speech in a linguistically meaningful way while preserving performance (see also <ref> [49] </ref>). A token set of common phoneme-like speech units across languages was created by taking the union of phonemes from all of the languages that the system is trained on. The resulting complexity was systematically reduced.
Reference: [50] <author> Ladefoged, P. </author> <title> Discussion note. the revised international phonetic alphabet. </title> <booktitle> Language 66, </booktitle> <volume> 3, </volume> <pages> 550-552. </pages>
Reference: [51] <author> Ladefoged, P. </author> <title> The revised international phonetic alphabet. </title> <journal> Journal of the International Phonetic Association 19, </journal> <volume> 2 (1990), </volume> <pages> 67-80. </pages>
Reference: [52] <author> Ladefoged, P. </author> <title> A Course in Phonetics, </title> <publisher> 3rd ed. Harcourt Brace Jovanovich, </publisher> <year> 1993. </year>
Reference: [53] <author> Lamel, L., and Gauvain, J.-L. </author> <title> A phone-based approach to non-linguistic speech feature identification. </title> <booktitle> Computer Speech and Language 9, </booktitle> <month> 1 (January </month> <year> 1995), </year> <pages> 87-105. </pages>
Reference: [54] <author> Lamel, L., Gauvain, J.-L., and Adda-Decker, M. </author> <title> Issues in large vocabulary. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 185-189. </pages>
Reference: [55] <author> Lamel, L. F., and Gauvain, J.-L. S. </author> <title> Language identification using phone-based acoustic likelihoods. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Adelaide, </booktitle> <address> Australia, </address> <month> April </month> <year> 1994), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 293 - 296. </pages>
Reference: [56] <author> Lamel, L. F., and Gauvain, J.-L. S. </author> <title> Identifying non-linguistic speech features. </title> <booktitle> In Proceedings Eurospeech 93 (Berlin, </booktitle> <address> Germany, </address> <month> September </month> <year> 1993), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 23-30. 119 </pages>
Reference-contexts: These systems [104, 47, 113, 57] usually have a considerable degree of complexity. Complexity and language identification accuracy can be increased by adding models for additional variabilities such as gender and phoneme duration <ref> [56, 57, 112] </ref>. I briefly outline some of the important systems that fall into this category. 19 Lincoln Lab System (1994) [111, 113, 112] HMM-based phoneme recognizers for each of the languages in the system are trained using the phonetically labeled training set from the OGI-TS database.
Reference: [57] <author> Lames, L. F., and Gauvain, J. L. </author> <title> Language identification using phone-based acoustic likelihoods. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Adelaide, </booktitle> <address> Australia, </address> <month> April </month> <year> 1994), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 293 - 296. </pages>
Reference-contexts: In Muthusamy [78], going from acoustic features to broad categories to phonemes shows large improvements in the system with each step. Work by Zissman et al. and Lamel et al. <ref> [111, 113, 57] </ref> confirms this issue by moving from ergodic Hidden Markov Models (HMMs) trained on acoustic input to broad-category trained language models to phonemically trained language models. <p> Virtually all the best language-identification systems today use fine phoneme recognition and language modeling ( the exception is the system by K.P.Li based on the spectral representation of speech [64], discussed in Section 2.3). These systems <ref> [104, 47, 113, 57] </ref> usually have a considerable degree of complexity. Complexity and language identification accuracy can be increased by adding models for additional variabilities such as gender and phoneme duration [56, 57, 112]. <p> These systems [104, 47, 113, 57] usually have a considerable degree of complexity. Complexity and language identification accuracy can be increased by adding models for additional variabilities such as gender and phoneme duration <ref> [56, 57, 112] </ref>. I briefly outline some of the important systems that fall into this category. 19 Lincoln Lab System (1994) [111, 113, 112] HMM-based phoneme recognizers for each of the languages in the system are trained using the phonetically labeled training set from the OGI-TS database. <p> Other Systems in this Category Lamel et al. <ref> [57] </ref> identify a set of non-linguistic speech features in order to model them separately in a recognition system. These include language, gender, speaker, dialect, speech disfluencies, etc. Ergodic HMMs are trained for each of the features and the models are run in parallel during the classification phase.
Reference: [58] <author> Lander, T., and Metzler, T. </author> <title> The cslu labeling guide. </title> <type> Tech. Rep. </type> <institution> CSLU 003, Oregon Graduate Institute, </institution> <year> 1994. </year>
Reference: [59] <author> Leonard, R. G. </author> <title> Language recognition test and evaluation. </title> <type> Tech. Rep. </type> <institution> RADC-TR-80-83, Air Force Rome Air Development Center, </institution> <month> March </month> <year> 1980. </year>
Reference: [60] <author> Leonard, R. G., and Doddington, G. R. </author> <title> Automatic language identification. </title> <type> Tech. Rep. </type> <institution> RADC-TR-74-200, Air Force Rome Air Development Center, </institution> <month> August </month> <year> 1974. </year>
Reference: [61] <author> Leonard, R. G., and Doddington, G. R. </author> <title> Automatic language discrimination. </title> <type> Tech. Rep. </type> <institution> RADC-TR-78-5, Air Force Rome Air Development Center, </institution> <month> January </month> <year> 1978. </year>
Reference: [62] <author> Leonard, R. G., and Doddington, G. R. </author> <title> Automatic language identification. </title> <type> Tech. Rep. </type> <institution> RADC-TR-75-264, Air Force Rome Air Development Center, </institution> <month> October </month> <year> 1975. </year>
Reference: [63] <author> Li, K., and Edwards, T. </author> <title> Statistical models for automatic language identification. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (April 1980), </booktitle> <pages> pp. 884-887. </pages>
Reference: [64] <author> Li, K. P. </author> <title> Automatic language identification using syllabic spectral features. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Adelaide, </booktitle> <address> Australia, </address> <month> April </month> <year> 1994), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 297 - 300. </pages>
Reference-contexts: Virtually all the best language-identification systems today use fine phoneme recognition and language modeling ( the exception is the system by K.P.Li based on the spectral representation of speech <ref> [64] </ref>, discussed in Section 2.3). These systems [104, 47, 113, 57] usually have a considerable degree of complexity. Complexity and language identification accuracy can be increased by adding models for additional variabilities such as gender and phoneme duration [56, 57, 112]. <p> Optimal scaling factors are required to integrate these features appropriately. However, the effect of the prosodic model in terms of performance is not remarkable. Spectral Features. Since speaker-dependent differences can be greater than language-dependent differences, K.P. Li <ref> [64] </ref> models speech in speaker- and language-dependent dimensions. It can be shown that the variation in the spectral features between any pair of speakers is much larger than the language differences obtained from a single bilingual speaker.
Reference: [65] <author> Lund, M., and Gish, H. </author> <title> Two novel lanuage model estimation techniques for statistical language identification. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 1363-1366. </pages>
Reference-contexts: As pointed out earlier, one of the problems with using word spotting is that the process becomes topic dependent. 2.2.2 Topic Identification with Word/Sequence Spotting Gish et al. approached language identification by applying algorithms developed originally for topic identification <ref> [31, 93, 70, 65] </ref> The solution is decomposed into three subtasks: (1) Keyword selection, (2) topic modeling, and (3) event detection. Keywords are selected based on a score which calculates how much each word contributes to the discrimination between any two given topics. <p> In an incoming utterance the number of keyword events is estimated by summing the probability of a putative hit over time. The probability at time t of having seen the keyword is calculated using the forward-backward scoring algorithm used in the Baum-Welch algorithm. Gish et al. <ref> [65] </ref> apply this algorithm to language identification by generating pseudo-word clusters. Each cluster is represented by the cluster centroid in the form of a single sequence of English 26 phonemes. Pseudo-word spotting is performed by finding inaccurate matches. <p> We also base language recognition on keyword selection, detection and classification. The language classification in this thesis is non-linear and the weighted keyword selection is based on the Bhattacharyya distance. Furthermore, sequences in <ref> [65] </ref> can only belong to one cluster, whereas for our system they can be associated with several clusters.
Reference: [66] <author> Lund, M., Ma, K., and Gish, H. </author> <title> Statistical language identification based on untranscribed training. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Atlanta, </booktitle> <address> USA, </address> <month> May </month> <year> 1996), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 793-796. </pages>
Reference-contexts: Even though the availability of larger databases has increased, data are not usually labeled at the phonemic level or even at the word level. The system must compensate for this. Recent advances in LID have addressed issues such as automatically labeling data with phoneme-like units <ref> [66] </ref>. Databases have grown from including four languages to ten, twenty, and more. Systems will most likely generalize to a large number of languages in the future.
Reference: [67] <author> Martin, S., Liermann, J., and Ney, H. </author> <title> Algorithms for bigram and trigram word clustering. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 1253-1256. </pages>
Reference: [68] <author> Marzal, A., and Vidal, E. </author> <title> Computation of normalized edit distance and applications. </title> <journal> In IEEE Transactions on Pattern Analysis and Machine Intelligence (September 1993), </journal> <volume> vol. 15, </volume> <pages> pp. 926-932. </pages>
Reference: [69] <author> Matsunaga, S., Singer, H., and Matsumura, T. </author> <title> Continuous speech recognition using non-uniform unit based acoustic and language models. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 3, </volume> <pages> pp. 1619-1622. </pages>
Reference: [70] <author> McDonough, J., and Gish, H. </author> <title> Issues in topic identification on the switchboard corpus. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Yokohama, </booktitle> <address> Japan, </address> <month> October </month> <year> 1994), </year> <journal> vol. </journal> <volume> 4, </volume> <pages> pp. 2163-2166. 120 </pages>
Reference-contexts: As pointed out earlier, one of the problems with using word spotting is that the process becomes topic dependent. 2.2.2 Topic Identification with Word/Sequence Spotting Gish et al. approached language identification by applying algorithms developed originally for topic identification <ref> [31, 93, 70, 65] </ref> The solution is decomposed into three subtasks: (1) Keyword selection, (2) topic modeling, and (3) event detection. Keywords are selected based on a score which calculates how much each word contributes to the discrimination between any two given topics.
Reference: [71] <author> McDonough, J., and Ng, K. </author> <title> Approaches to topic identification on the switchboard corpus. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Adelaide, Australia, </booktitle> <volume> April 94), vol. 1, </volume> <pages> pp. 385-388. </pages>
Reference: [72] <author> Mendoza, S., Gilick, L., Ito, Y., Lowe, S., and Newman, M. </author> <title> Automatic language identification using large vocabulary coninuous speech recognition. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Atlanta, </booktitle> <address> USA, </address> <month> May </month> <year> 1996), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 785-788. </pages>
Reference: [73] <author> Moisa, L., and Giachin, E. </author> <title> Automatic clustering of words for probabilistic language models. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 1249-1252. </pages>
Reference: [74] <author> Muthusamy, Y., Cole, R., and Oshika, B. </author> <title> The OGI multi-language telephone speech corpus. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Banff,Alberta,Canada, </booktitle> <month> October </month> <year> 1992), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 895-898. </pages>
Reference-contexts: The speech representation is developed in Section 5.3. Section 5.4 describes the implementation of the final language identification system and results are given in Section 5.5. 5.1 The Data The database used in this study is the OGI T S database <ref> [74] </ref>, which is a multi-language telephone speech database including speech from 21 languages.
Reference: [75] <author> Muthusamy, Y. K. </author> <title> A review of research in automatic language identification. </title> <type> Tech. Rep. CS/E 92-009, </type> <institution> Oregon Graduate Institute, </institution> <month> May </month> <year> 1992. </year>
Reference: [76] <author> Muthusamy, Y. K. </author> <title> A Segmental Approach to Automatic Language Identification. </title> <type> PhD thesis, </type> <institution> Oregon Graduate Institute, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Alternate methods express multi-lingual speech by creating a supra-lingual speech unit. Creating broad-categories, for example, one can express nasals such as /ng/, /n/, or /m/ in both German and English within a single category. Some languages may differ based on broad categories and their sequences <ref> [76] </ref>. For example, German, which has sequences of consonants is distinguished from Japanese due to its inherent structure where each consonant is followed by a vowel (consonant-vowel structure) [18]. <p> Chapter 2 Previous Work A recent review [77] of language identification contains a detailed literature review of historical and present approaches to language identification. In addition, Muthusamy <ref> [76] </ref> presents thorough discussions of the subject in his thesis. We will therefore limit our literature review to detailing the modern approaches, and evaluating them with respect to the approach taken in this thesis. Related work with respect to speech representation will be addressed in Section 2.1. <p> Sequence Selection and Error Estimation. 3. Phoneme Merging. The flowchart shown in Figure 3.7 depicts the iterative process of merging phonemes and estimating language classification error. At the extremes of this algorithm we obtain either a phoneme based system [36] or a broad category based system <ref> [76] </ref>. This algorithm will return the lowest number of phoneme clusters without losing language discriminability, together with an error estimate as a function of time due to the chosen features.
Reference: [77] <author> Muthusamy, Y. K., Barnard, E., and Cole, R. A. </author> <title> Reviewing automatic language identification. </title> <journal> IEEE Signal Processing Magazine 11, </journal> <month> 4 (October </month> <year> 1994), </year> <pages> 33-41. </pages>
Reference-contexts: The implementation and application to multilingual telephone speech verifies the theoretical results. 3. Implementation: * Implementation of the developed theory and verification on automatically gen erated data. * Implementation of a language identification module tested on multilingual tele phone speech. Chapter 2 Previous Work A recent review <ref> [77] </ref> of language identification contains a detailed literature review of historical and present approaches to language identification. In addition, Muthusamy [76] presents thorough discussions of the subject in his thesis.
Reference: [78] <author> Muthusamy, Y. K., Berkling, K. M., Arai, T., Cole, R. A., and Barnard, E. </author> <title> A comparison of approaches to automatic language identification. </title> <booktitle> In Proceedings Eurospeech 93 (Berlin, </booktitle> <address> Germany, </address> <month> September </month> <year> 1993), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 1307-1310. </pages>
Reference-contexts: In Muthusamy <ref> [78] </ref>, going from acoustic features to broad categories to phonemes shows large improvements in the system with each step. <p> The above approaches are all related to one another and were developed during the same time as the sequence of research which forms the core of this thesis <ref> [78, 108, 8, 11, 9, 10] </ref>. We also base language recognition on keyword selection, detection and classification. The language classification in this thesis is non-linear and the weighted keyword selection is based on the Bhattacharyya distance.
Reference: [79] <author> Muthusamy, Y. K., and Cole, R. A. </author> <title> A segment-based automatic language identification system. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <editor> J.E.Moody, S.J.Hanson, and R.P.Lippmann, Eds., </editor> <volume> vol. 4. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1992, </year> <pages> pp. 241-250. </pages>
Reference: [80] <author> Muthusamy, Y. K., and Cole, R. A. </author> <title> Automatic segmentation and identification of ten languages using telephone speech. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing 92 (Banff, </booktitle> <address> Alberta, Canada, </address> <year> 1992 </year> <month> October), </month> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 1007-1010. </pages>
Reference: [81] <author> Muthusamy, Y. K., Cole, R. A., and Gopalakrishnan, M. </author> <title> A segment-based approach to automatic language identification. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Toronto, </booktitle> <address> Canada, </address> <month> May </month> <year> 1991), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 353-356. </pages>
Reference: [82] <author> Muthusamy, Y. K., Jain, N., and Cole, R. A. </author> <title> Perceptual benchmarks for automatic language identification. </title> <booktitle> In International Conference on Speech and Signal Processing (Adelaide, </booktitle> <address> Australia, </address> <month> April </month> <year> 1994), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 333-336. </pages>
Reference-contexts: It is therefore not necessary to decode the segmented speech into a string of words and subsequently their meaning as is common in speech-recognition systems. However, we believe that understanding the utterance may ultimately be the key to robustness in language 8 representation. identification <ref> [82] </ref>. For this reason it is important to reduce the complexity of language identification systems in each of its parts to facilitate growth in the number information sources used by the system. <p> Such a feature set has been the subject of Section 2.2 and is also addressed in text-based speech recognition. 27 2.3.1 Language Identification without Phonemes Pitch can be an important feature when captured correctly. This has repeatedly been reported by human listeners during perceptual experiments <ref> [82] </ref>. While this is an important feature, we consider pitch to be an orthogonal issue; pitch (like other information sources of information such as grammar and speech understanding) can be added to existing systems. Pitch. <p> We limit our discussion to perceptual studies and text-based systems. Perceptual Experiments Taking part in the perceptual experiment <ref> [82] </ref> had one of the greatest effects on the way we designed our system. Our own experience, and those of others voiced during interviews after the test, were that language identification was rarely based on long words but mostly on short syllables and phoneme occurrences.
Reference: [83] <author> Nakagawa, S., Ueda, Y., and Seino, T. </author> <title> Speaker-independent, text-independent language identification by HMM. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Banff, </booktitle> <address> Alberta, Canada, </address> <month> October </month> <year> 1992), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 1011-1014. 121 </pages>
Reference: [84] <author> Nowell, P., and Moore, R. </author> <title> The application of dynamic programming techniques to non-word based topic spotting. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 1355-1358. </pages>
Reference-contexts: Pseudo-word spotting is performed by finding inaccurate matches. The identified language corresponds to the maximal log likelihood of matching the incoming string to two language models in the pairwise system. A similar approach to topic identification developed by Moore and Novell <ref> [84] </ref>, is based on spotting for unusual and topic dependent words. These words tend to be long, so a word spotting algorithm needs to take into account errors due to both the errorful phoneme transcription and variations in ponunciations.
Reference: [85] <author> Parris, E., and Carey, M. </author> <title> Language identification using multiple knowledge sources. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Detroit,USA, </booktitle> <month> May </month> <year> 1995), </year> <journal> vol. </journal> <volume> 5, </volume> <pages> pp. 3519-3523. </pages>
Reference-contexts: This algorithm can be applied directly to language identification, even though, rather than looking for rare, long words one now selects frequent and mostly shorter pseudo-words. Other work closely related to this approach is recent work done at Ensigma relating to topic identification and language identification <ref> [16, 85, 101] </ref>. The above approaches are all related to one another and were developed during the same time as the sequence of research which forms the core of this thesis [78, 108, 8, 11, 9, 10]. We also base language recognition on keyword selection, detection and classification.
Reference: [86] <author> Pye, D., Young, S., and Woodland, P. </author> <title> Large vocabulary multilinual speech recognition using htk. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 181-184. </pages>
Reference: [87] <author> Ramesh, P., and Roe, E. B. </author> <title> Language identification with embeddeed word models. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Yokohama, </booktitle> <address> Japan, </address> <month> September </month> <year> 1994), </year> <journal> vol. </journal> <volume> 4, </volume> <pages> pp. 1887-1890. </pages>
Reference-contexts: The obtained results confirm this assumption. However the high complexity required of such a system may possibly be avoided, by selectively choosing sub-words according to their discriminating power between languages. Embedded Word Models of Frequent Words and Phrases The system by Ramesh and Roe <ref> [87] </ref> is based on the general design used in all of the above systems. Their word-spotting algorithm depends on the existence of specific words in the utterance to be classified since words are specified at the phoneme level.
Reference: [88] <author> Reyes, A. A., Seino, T., and Nakagawa, S. </author> <title> Two language identification methods based on hmms. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing (Yokohama, </booktitle> <address> Japan, </address> <month> September </month> <year> 1994), </year> <journal> vol. </journal> <volume> 4, </volume> <pages> pp. 1895-1898. </pages>
Reference: [89] <author> Roginski, K. R. </author> <title> A Neural Network Phonetic Classifier for Telephone Speech. </title> <type> Master's thesis, </type> <institution> Oregon Graduate Institute, </institution> <year> 1991. </year>
Reference-contexts: Once the signal is thus represented, a neural network is used to classify the signal in terms of a linguistic speech unit such as a phoneme for example <ref> [89] </ref>. An important part of this chapter is the selection of the speech units which the neural network will discriminate. By starting with the union of all phonemes across six languages in the database we can approximate a universal representation. <p> The sampling intervals are shown in Figure 5.1 and have been derived from work done by Roginski <ref> [89] </ref> who experimented with different window sizes to find the optimal static representation of a frame to be classified. The objective is to provide substantial contextual information about the chosen frame to the network. The number of hidden nodes was derived experimentally for best performance.
Reference: [90] <author> Salavedra, J., Zeljkovic, I., J.Wilpon, Rahmin, M., and Jacobsen, C. </author> <title> Multi-lingual connected digits recognition. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 3, </volume> <pages> pp. 2119-2123. </pages>
Reference: [91] <author> Savic, M., Acosta, E., and Gupta, S. K. </author> <title> An automatic language identification system. </title> <booktitle> In International Conference of the American Society of Signal Processing (Toronto,Canada, 1991), </booktitle> <volume> vol. 2, </volume> <pages> pp. 817-820. </pages>
Reference: [92] <author> Schultz, T., Rogina, I., and Waibel, A. </author> <title> Lvcsr-based language identification. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Atlanta, </booktitle> <address> USA, </address> <month> May </month> <year> 1996), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 781-784. </pages>
Reference: [93] <author> Siu, M., Gish, H., and Rohlicek, R. </author> <title> Predicting word spotting performance. </title> <booktitle> In Proceedings International Conference on Spoken Language Understanding (Yoko-hama,Japan, </booktitle> <month> October </month> <year> 1994), </year> <journal> vol. </journal> <volume> 4, </volume> <pages> pp. 2195-2198. </pages>
Reference-contexts: As pointed out earlier, one of the problems with using word spotting is that the process becomes topic dependent. 2.2.2 Topic Identification with Word/Sequence Spotting Gish et al. approached language identification by applying algorithms developed originally for topic identification <ref> [31, 93, 70, 65] </ref> The solution is decomposed into three subtasks: (1) Keyword selection, (2) topic modeling, and (3) event detection. Keywords are selected based on a score which calculates how much each word contributes to the discrimination between any two given topics.
Reference: [94] <author> Sole, M.-J., and Estebas, E. </author> <title> Connected speech processes: a cross-linguistic study. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 3, </volume> <pages> pp. 2239-2242. </pages>
Reference: [95] <author> Strom, V. </author> <title> Detection of accents. </title> <booktitle> In Proceedings Eurospeech (Madrid, Spain, Septem-ber 1995), </booktitle> <volume> vol. 3, </volume> <pages> pp. 2039-2041. </pages>
Reference: [96] <author> Sugiyama, M. </author> <title> Automatic language recognition using acoustic features. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (May 1991), </booktitle> <volume> vol. 2, </volume> <pages> pp. 813-816. </pages>
Reference: [97] <author> Sugiyama, M. </author> <title> Automatic language recognition using acoustic features. </title> <type> Tech. Rep. </type> <institution> TR-I-0167, ATR Interpreting Telephony Research Laboratories, </institution> <year> 1991. </year> <month> 122 </month>
Reference: [98] <author> Tucker, R. C. F., Carey, M. J., and Parris, E. S. </author> <title> Automatic language identification using sub-word models. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Adelaide, </booktitle> <address> Australia, </address> <month> April </month> <year> 1994), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 301 - 304. </pages>
Reference: [99] <author> Ueberla, J. P. </author> <title> More efficient clustering of n-grams for statistical language modeling. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 1257-1260. </pages>
Reference: [100] <author> Vilar, J. M., Vidal, E., and Marzal, A. </author> <title> Learning language translation in limited domains using finite-state models: some extensions and improvements. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 1231-1230. </pages>
Reference: [101] <author> Wright, J., Carey, M., and Parris, E. </author> <title> Statistical models for topic identification using phoneme substrings. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Atlanta, </booktitle> <address> USA, </address> <month> May </month> <year> 1996), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 307-310. </pages>
Reference-contexts: This algorithm can be applied directly to language identification, even though, rather than looking for rare, long words one now selects frequent and mostly shorter pseudo-words. Other work closely related to this approach is recent work done at Ensigma relating to topic identification and language identification <ref> [16, 85, 101] </ref>. The above approaches are all related to one another and were developed during the same time as the sequence of research which forms the core of this thesis [78, 108, 8, 11, 9, 10]. We also base language recognition on keyword selection, detection and classification.
Reference: [102] <author> Yan, Y. </author> <title> Development of An Approach to Language Identification based on Language-dependent Phone Recognition. </title> <type> PhD thesis, </type> <institution> Oregon Graduate Institute, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: This results in the implementation of a "word"-spotting algorithm (Chapter 5). It has been shown in the past that good phoneme recognition directly affects the success of language identification <ref> [102] </ref>. In order to obtain good phoneme recognition, a meaningful phoneme representation is often sacrificed; for example choosing to decode speech with phoneme set from language A to discriminate between languages B and C because this results in the best performance. <p> When applying our algorithm to this setup, the 114 only change in the block diagram of Figure 6.2 is the use of language-dependent speech unit alignment. Although the resulting system does not currently perform as well as the very best systems for language identification <ref> [102, 112] </ref>, we believe that it is interesting in its own right. As the accuracy of speech recognition improves, it may also be that a system such as ours improves more substantially than those based on, say, phonotactic statistics.
Reference: [103] <author> Yan, Y., and Barnard, E. </author> <title> An approach to automatic language identification based on language-dependent phone recognition. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Detroit,USA, </booktitle> <month> May </month> <year> 1995), </year> <journal> vol. </journal> <volume> 5, </volume> <pages> pp. 3511-3514. </pages>
Reference-contexts: To improve this system, it is extended to model duration and gender dependent variabilities. 20 OGI System (1995) <ref> [108, 103, 104, 107, 106] </ref> It is believed that increasing the number of parameters increases the variability of the system to the extent that it does not generalize well to previously unseen data.
Reference: [104] <author> Yan, Y., and Barnard, E. </author> <title> An approach to language identification with enhanced language model. </title> <booktitle> In Proceedings Eurospeech (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 1351-1354. </pages>
Reference-contexts: Virtually all the best language-identification systems today use fine phoneme recognition and language modeling ( the exception is the system by K.P.Li based on the spectral representation of speech [64], discussed in Section 2.3). These systems <ref> [104, 47, 113, 57] </ref> usually have a considerable degree of complexity. Complexity and language identification accuracy can be increased by adding models for additional variabilities such as gender and phoneme duration [56, 57, 112]. <p> To improve this system, it is extended to model duration and gender dependent variabilities. 20 OGI System (1995) <ref> [108, 103, 104, 107, 106] </ref> It is believed that increasing the number of parameters increases the variability of the system to the extent that it does not generalize well to previously unseen data. <p> In order to reduce the complexity, several groups have tried to map phonemes across 21 languages by representing larger sets of languages in terms of phonemes taken from only a subset of the languages <ref> [113, 104] </ref>.
Reference: [105] <author> Yan, Y., and Barnard, E. </author> <title> Neural networks and linear classifiers. automatic language identification. </title> <booktitle> In In Proceedings IEEE International Conference on Neural Networks and Signal Processing (December 1995), </booktitle> <pages> pp. 812-815. </pages>
Reference: [106] <author> Yan, Y., and Barnard, E. </author> <title> Experiments for an approach to langugage iden-tivication with conversational telephone speech. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Atlanta, </booktitle> <address> USA, </address> <month> May </month> <year> 1996), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 789-792. </pages>
Reference-contexts: To improve this system, it is extended to model duration and gender dependent variabilities. 20 OGI System (1995) <ref> [108, 103, 104, 107, 106] </ref> It is believed that increasing the number of parameters increases the variability of the system to the extent that it does not generalize well to previously unseen data.
Reference: [107] <author> Yan, Y., Barnard, E., and Cole, R. </author> <title> Development of an approach to automatic language identification based on phone recognition. </title> <booktitle> Computer Speech and Language 10, </booktitle> <month> 1 (January </month> <year> 1996), </year> <pages> 37-53. </pages>
Reference-contexts: To improve this system, it is extended to model duration and gender dependent variabilities. 20 OGI System (1995) <ref> [108, 103, 104, 107, 106] </ref> It is believed that increasing the number of parameters increases the variability of the system to the extent that it does not generalize well to previously unseen data.
Reference: [108] <author> Yan, Y., Berkling, K. M., and Barnard, E. </author> <title> Bigram models and phoneme clusters for language identification. </title> <booktitle> In Proceedings Speech Research Symposium XIV SRS (Baltimore, </booktitle> <address> Maryland, </address> <month> June </month> <year> 1994), </year> <pages> pp. 22-30. </pages>
Reference-contexts: To improve this system, it is extended to model duration and gender dependent variabilities. 20 OGI System (1995) <ref> [108, 103, 104, 107, 106] </ref> It is believed that increasing the number of parameters increases the variability of the system to the extent that it does not generalize well to previously unseen data. <p> The above approaches are all related to one another and were developed during the same time as the sequence of research which forms the core of this thesis <ref> [78, 108, 8, 11, 9, 10] </ref>. We also base language recognition on keyword selection, detection and classification. The language classification in this thesis is non-linear and the weighted keyword selection is based on the Bhattacharyya distance.
Reference: [109] <author> Ziegler, D.-V. </author> <title> The Automatic Identification of Languages Using Linguistic Recognition Signals. </title> <type> PhD thesis, </type> <institution> State University of New York at Buffalo, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: Our clustering approach is based on the fact that there is only a small set of phonemes that are language dependent, and most other phones do not contribute significantly to the listener's language identification process unless they occur in specific context. Text-based Systems Ziegler <ref> [109] </ref> built a text-based language-identification system. He employs occurrence frequencies of signals ("subwords") much as we propose. Ideal properties of signals are high frequency and significant inter-language variability. Ziegler therefore uses a weighting for scoring which is based on signal detection according to linguistic significance.
Reference: [110] <author> Zissman, M., Gleason, T., Rekart, D., and Losievicz, B. </author> <title> Automatic dialect identification of extemporaneous, conversational Latin American Spanish speech. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Atlanta, </booktitle> <address> USA, </address> <month> May </month> <year> 1996), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 777-780. 123 </pages>
Reference-contexts: In the future, systems will be used in a variety of different tasks, including applications to improve robustness of speech recognition systems for dialect and accent detection. Preliminary systems are already in place <ref> [110] </ref>.
Reference: [111] <author> Zissman, M. A. </author> <title> Automatic language identification using gaussian mixtures and hidden markov models. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (April 1993), </booktitle> <volume> vol. 2, </volume> <pages> pp. 399-402. </pages>
Reference-contexts: In Muthusamy [78], going from acoustic features to broad categories to phonemes shows large improvements in the system with each step. Work by Zissman et al. and Lamel et al. <ref> [111, 113, 57] </ref> confirms this issue by moving from ergodic Hidden Markov Models (HMMs) trained on acoustic input to broad-category trained language models to phonemically trained language models. <p> Complexity and language identification accuracy can be increased by adding models for additional variabilities such as gender and phoneme duration [56, 57, 112]. I briefly outline some of the important systems that fall into this category. 19 Lincoln Lab System (1994) <ref> [111, 113, 112] </ref> HMM-based phoneme recognizers for each of the languages in the system are trained using the phonetically labeled training set from the OGI-TS database. The training speech for each of the N languages is passed through each of the L front end phoneme recognizers, where N L.
Reference: [112] <author> Zissman, M. A. </author> <title> Language identification using phoneme recognition and phonotactic language modeling. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (May 1995), </booktitle> <volume> vol. 5, </volume> <pages> pp. 3503-3506. </pages>
Reference-contexts: These systems [104, 47, 113, 57] usually have a considerable degree of complexity. Complexity and language identification accuracy can be increased by adding models for additional variabilities such as gender and phoneme duration <ref> [56, 57, 112] </ref>. I briefly outline some of the important systems that fall into this category. 19 Lincoln Lab System (1994) [111, 113, 112] HMM-based phoneme recognizers for each of the languages in the system are trained using the phonetically labeled training set from the OGI-TS database. <p> Complexity and language identification accuracy can be increased by adding models for additional variabilities such as gender and phoneme duration [56, 57, 112]. I briefly outline some of the important systems that fall into this category. 19 Lincoln Lab System (1994) <ref> [111, 113, 112] </ref> HMM-based phoneme recognizers for each of the languages in the system are trained using the phonetically labeled training set from the OGI-TS database. The training speech for each of the N languages is passed through each of the L front end phoneme recognizers, where N L. <p> In Dalsgaard [24], Berkling [8] and Zissman <ref> [112] </ref> language dependent phonemes, called mono-phonemes or key-phones, have been shown to contain most of the language discriminating information. Dalsgaard uses this knowledge to build a language identification system. Dalsgaard distinguishes between language dependent phonemes (mono-phones) and language independent phonemes (poly-phones) [24]. <p> When applying our algorithm to this setup, the 114 only change in the block diagram of Figure 6.2 is the use of language-dependent speech unit alignment. Although the resulting system does not currently perform as well as the very best systems for language identification <ref> [102, 112] </ref>, we believe that it is interesting in its own right. As the accuracy of speech recognition improves, it may also be that a system such as ours improves more substantially than those based on, say, phonotactic statistics.
Reference: [113] <author> Zissman, M. A., and Singer, E. </author> <title> Automatic language identification of telephone speech messages using phoneme recognition and n-gram modelling. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (Ade-laide, </booktitle> <address> Australia, </address> <month> April </month> <year> 1994), </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 305-308. </pages>
Reference-contexts: In Muthusamy [78], going from acoustic features to broad categories to phonemes shows large improvements in the system with each step. Work by Zissman et al. and Lamel et al. <ref> [111, 113, 57] </ref> confirms this issue by moving from ergodic Hidden Markov Models (HMMs) trained on acoustic input to broad-category trained language models to phonemically trained language models. <p> Virtually all the best language-identification systems today use fine phoneme recognition and language modeling ( the exception is the system by K.P.Li based on the spectral representation of speech [64], discussed in Section 2.3). These systems <ref> [104, 47, 113, 57] </ref> usually have a considerable degree of complexity. Complexity and language identification accuracy can be increased by adding models for additional variabilities such as gender and phoneme duration [56, 57, 112]. <p> Complexity and language identification accuracy can be increased by adding models for additional variabilities such as gender and phoneme duration [56, 57, 112]. I briefly outline some of the important systems that fall into this category. 19 Lincoln Lab System (1994) <ref> [111, 113, 112] </ref> HMM-based phoneme recognizers for each of the languages in the system are trained using the phonetically labeled training set from the OGI-TS database. The training speech for each of the N languages is passed through each of the L front end phoneme recognizers, where N L. <p> In order to reduce the complexity, several groups have tried to map phonemes across 21 languages by representing larger sets of languages in terms of phonemes taken from only a subset of the languages <ref> [113, 104] </ref>.
References-found: 113

