URL: http://www.cs.iastate.edu/~honavar/Papers/icgi96.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/publist.html
Root-URL: 
Email: parekh@cs.iastate.edu honavar@cs.iastate.edu  
Title: An Incremental Interactive Algorithm for Regular Grammar Inference  
Author: Rajesh Parekh Vasant Honavar 
Address: 226 Atanasoff Hall,  Ames, IA 50011. U.S.A.  
Affiliation: Artificial Intelligence Research Group Department of Computer Science  Iowa State University,  
Abstract: We present interactive algorithms for learning regular grammars from positive examples and membership queries. A structurally complete set of strings from a language L(G) corresponding to an unknown regular grammar G implicitly specifies a lattice (or version space) which represents a space of candidate grammars containing the unknown grammar G. This lattice can be searched efficiently using membership queries to identify the unknown grammar G using an implicit representation of the version space in the form of two sets S and G that correspond (respectively) to the set of most specific and most general grammars consistent with the set of positive examples provided and the queries answered by the teacher at any given time. We present a provably correct incremental version of the algorithm in which a structurally complete set of positive samples is not necessarily available to the learner at the beginning of learning. The learner constructs a lattice of grammars based on the strings provided at the start and performs candidate elimination by posing safe membership queries. When additional examples become available, the learner incrementally updates the lattice and continues with candidate elimination. Eventually, when the set of positive samples provided by the teacher encompasses a structurally complete set for the unknown grammar, the algorithm terminates by identifying the unknown grammar G.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Angluin, D. </author> <title> A Note on the Number of Queries Needed to Identify Regular Languages. </title> <journal> Infor mation and control, </journal> <volume> 51. </volume> <year> 1981. </year> <pages> pp 76-87. 11 </pages>
Reference-contexts: When the structurally complete sample set has been acquired, an example that is not accepted by all FSA in G k can be classified as negative. The idea of incremental lattice update was inspired by Hirsh's work on Incremental Version-Space Merging [8]. Angluin <ref> [1] </ref> has proposed an algorithm ID to infer the target grammar from a live complete set of examples using a polynomial number of membership queries. The live complete set can be constructed given a structurally complete sample.
Reference: [2] <author> Angluin, D. </author> <title> Learning Regular Sets from Queries and Counterexamples. </title> <journal> Information and Com putation, </journal> <volume> 75. </volume> <year> 1987. </year> <pages> pp 87-106. </pages>
Reference-contexts: Our algorithm does not store the previous examples and is guaranteed to converge to the desired target solution instead of a set of candidate solutions as is the case for VanLehn and Ball's method. Several other versions of grammar inference problem have been studied extensively in the literature. Angluin <ref> [2] </ref> has proposed a polynomial time algorithm (L fl ), which allows the learner to infer the target grammar by posing both membership and equivalence queries. Natarajan [14] has described an adaptation of Angluin's L fl procedure to the PAC learning framework.
Reference: [3] <author> Biermann, A.W. and Feldman, J.A. </author> <title> A Survey of Results in Grammatical Inference. </title> <editor> In Watan abe S. (editor), </editor> <booktitle> Frontiers of Pattern Recognition. </booktitle> <publisher> Academic Press. </publisher> <year> 1972. </year> <pages> pp. 31-54. </pages>
Reference-contexts: 1 Introduction Regular Grammar Inference <ref> [3] </ref> is the process of learning an unknown regular grammar (G) given a finite set of positive examples S + , possibly a finite, non-empty set of negative examples S , and possibly a knowledgeable teacher who can answer queries posed by the learner.
Reference: [4] <author> Dupont, P., Miclet, L., and Vidal, E. </author> <title> What is the Search Space of the Regular Inference?. </title> <booktitle> In Proceedings of the Second International Colloquium on Grammatical Inference, </booktitle> <address> ICGI-94, Alicante, Spain, </address> <month> September </month> <year> 1994. </year> <pages> pp. 25-37. </pages>
Reference-contexts: and unknown (target) FSA interchangeably. 2 Inference given a Structurally Complete S + The teacher provides a structurally complete set of positive samples S + which implicitly defines a lattice (or version space) of candidate grammars or the initial hypothesis space that is guaranteed to contain the unknown grammar (G) <ref> [15, 16, 4] </ref>. The learner generates strings and queries the teacher about their membership in G.
Reference: [5] <author> Fu. </author> <title> K.S. Syntactic Pattern Recognition and Applications. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> N.J. </address> <year> 1982. </year>
Reference-contexts: The authors would like to thank Professor Giora Slutzki for several helpful discussions and suggestions related to this work 1 practical significance in pattern recognition and language acquisition <ref> [5, 9, 12] </ref>. We present an algorithm for regular grammar inference in an active learning framework wherein the learner's task is to accurately infer the unknown grammar using the teacher supplied positive examples and answers to membership queries.
Reference: [6] <author> Giles, C., Chen, D., Miller, H., Sun, G., and Lee, Y. </author> <title> Second-order Recurrent Neural Networks for Grammatical Inference. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks 91, </booktitle> <volume> vol. 2, </volume> <pages> pp. 273-281, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Natarajan [14] has described an adaptation of Angluin's L fl procedure to the PAC learning framework. Rivest and Schapire [18] have suggested a diversity based mechanism dealing with homing sequences. Giles et al <ref> [6] </ref> use recurrent neural networks to learn FSA using positive and negative samples. Lankhorst [11] has presented a genetic algorithms based approach for learning context free grammars. Experimental analysis is currently underway to study the expected case time and space complexity bounds for our method.
Reference: [7] <author> Harrison, M. </author> <title> Introduction to Switching and Automata Theory. </title> <publisher> McGraw-Hill, </publisher> <address> NY 1965. </address>
Reference-contexts: 0 ; Cg where, W = S fi T, w 0 = (s 0 ; t 0 ) , ffi w ((s,t); ) = (ffi s (s; ); ffi t (t; )) for all 2 and C = f (s,t) j s 2 A and t 2 T - Bg <ref> [7] </ref> forms the query "y 2 L (G)"? that is posed to the teacher. Based on the teacher's response fi is pruned and elements of S and G become progressively more general and more specific respectively.
Reference: [8] <author> Hirsh, H. </author> <title> Incremental Version-Space Merging: A General Framework for Concept Learning. </title> <publisher> Kluwer Academic Publishers. </publisher> <year> 1990. </year>
Reference-contexts: When the structurally complete sample set has been acquired, an example that is not accepted by all FSA in G k can be classified as negative. The idea of incremental lattice update was inspired by Hirsh's work on Incremental Version-Space Merging <ref> [8] </ref>. Angluin [1] has proposed an algorithm ID to infer the target grammar from a live complete set of examples using a polynomial number of membership queries. The live complete set can be constructed given a structurally complete sample.
Reference: [9] <author> Honavar, V.G. </author> <title> Toward Learning Systems That Integrate Different Strategies and Represen tations. In: Artificial Intelligence and Neural Networks: Steps toward Principled Integration. Honavar, </title> <editor> V. & Uhr, L. </editor> <address> (editors) New York: </address> <publisher> Academic Press, </publisher> <year> 1994. </year>
Reference-contexts: The authors would like to thank Professor Giora Slutzki for several helpful discussions and suggestions related to this work 1 practical significance in pattern recognition and language acquisition <ref> [5, 9, 12] </ref>. We present an algorithm for regular grammar inference in an active learning framework wherein the learner's task is to accurately infer the unknown grammar using the teacher supplied positive examples and answers to membership queries.
Reference: [10] <author> Hopcroft, J., and Ullman, J. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley Publishing Company, Inc. </publisher> <year> 1979. </year>
Reference-contexts: Clearly, L (A s ) L (A). For a detailed treatment of this subject see <ref> [10] </ref>. 2 1.2 The Grammar Inference Problem We present a mechanism for inference of a FSA (not necessarily deterministic) that is equivalent to the FSA corresponding to the unknown regular grammar (G).
Reference: [11] <author> Lankhorst, M. </author> <title> A Genetic Algorithm for Induction of Nondeterministic Pushdown Automata. </title> <institution> University of Groningen, Computer Science Report CS-R 9502, </institution> <address> The Netherlands. </address> <year> 1995. </year>
Reference-contexts: Natarajan [14] has described an adaptation of Angluin's L fl procedure to the PAC learning framework. Rivest and Schapire [18] have suggested a diversity based mechanism dealing with homing sequences. Giles et al [6] use recurrent neural networks to learn FSA using positive and negative samples. Lankhorst <ref> [11] </ref> has presented a genetic algorithms based approach for learning context free grammars. Experimental analysis is currently underway to study the expected case time and space complexity bounds for our method.
Reference: [12] <author> Miclet, L. and Quinqueton J. </author> <title> Learning from Examples in Sequences and Grammatical Infer ence. </title> <note> In Ferrate, </note> <author> G., Pavlidis, T., Sanfeliu, A., and Bunke H. </author> <title> (editors) Syntactic and Structural Pattern Recognition. </title> <booktitle> NATO ASI Series Vol. </booktitle> <address> F45, </address> <year> 1986. </year> <pages> pp. 153-171. </pages>
Reference-contexts: The authors would like to thank Professor Giora Slutzki for several helpful discussions and suggestions related to this work 1 practical significance in pattern recognition and language acquisition <ref> [5, 9, 12] </ref>. We present an algorithm for regular grammar inference in an active learning framework wherein the learner's task is to accurately infer the unknown grammar using the teacher supplied positive examples and answers to membership queries.
Reference: [13] <author> Mitchell, T. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18. </volume> <year> 1982. </year> <pages> pp 203-226. </pages>
Reference-contexts: Based on the teacher's response fi is pruned and elements of S and G become progressively more general and more specific respectively. Since the lattice defines a partial order and the MSE (MGE) test can be performed efficiently on the elements of the lattice, the version-space algorithm <ref> [13] </ref> can be adapted for candidate elimination. Algorithm: 1. Set S = fP 0 g and G = fP E m 1 g. 2. <p> The hypothesis space defined by the set S + is too large to be represented explicitly or to be searched exhaustively. Our algorithm uses a compact representation of the hypothesis space in terms of S and G. The proposed algorithm uses an efficient bidirectional search strategy inspired by Mitchell's <ref> [13] </ref> version space algorithm. This enables the learner to eliminate large parts of the hypothesis space based on a single informative query and also to make unambiguous inferences even though the algorithm has not converged to a single solution.
Reference: [14] <author> Natarajan, B. K., </author> <title> Machine Learning: A Theoretical Approach. </title> <publisher> Morgan Kaufmann San Mateo, </publisher> <address> California. </address> <year> 1992. </year>
Reference-contexts: Several other versions of grammar inference problem have been studied extensively in the literature. Angluin [2] has proposed a polynomial time algorithm (L fl ), which allows the learner to infer the target grammar by posing both membership and equivalence queries. Natarajan <ref> [14] </ref> has described an adaptation of Angluin's L fl procedure to the PAC learning framework. Rivest and Schapire [18] have suggested a diversity based mechanism dealing with homing sequences. Giles et al [6] use recurrent neural networks to learn FSA using positive and negative samples.
Reference: [15] <author> Pao, T.W.L., and Carr, J.W.III. </author> <title> A solution of the Syntactic Induction-Inference Problem for Regular Languages. </title> <journal> Computer Languages, </journal> <volume> Vol. 3 1978, </volume> <pages> pp. 53-64. </pages>
Reference-contexts: and unknown (target) FSA interchangeably. 2 Inference given a Structurally Complete S + The teacher provides a structurally complete set of positive samples S + which implicitly defines a lattice (or version space) of candidate grammars or the initial hypothesis space that is guaranteed to contain the unknown grammar (G) <ref> [15, 16, 4] </ref>. The learner generates strings and queries the teacher about their membership in G.
Reference: [16] <author> Parekh R.G., and Honavar V.G. </author> <title> An Efficient Interactive Algorithm for Regular Language Learning. </title> <institution> Computer Science TR95-02, Iowa State University, Ames. </institution> <year> 1995. </year> <note> (Preliminary version appeared in Proceedings of the 5th UNB AI Symposium, Fredericton, </note> <institution> Canada, </institution> <year> 1993). </year>
Reference-contexts: and unknown (target) FSA interchangeably. 2 Inference given a Structurally Complete S + The teacher provides a structurally complete set of positive samples S + which implicitly defines a lattice (or version space) of candidate grammars or the initial hypothesis space that is guaranteed to contain the unknown grammar (G) <ref> [15, 16, 4] </ref>. The learner generates strings and queries the teacher about their membership in G.
Reference: [17] <author> Porat S., and Feldman J. A., </author> <title> Learning Automata from Ordered Examples. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <pages> pp 109-138. </pages> <year> 1991. </year>
Reference-contexts: Our approach offers an alternative to the ID procedure when 10 a structurally complete set of samples is available. A direct extension of the ID procedure to the incremental version has not been studied. Porat and Feldman <ref> [17] </ref> have proposed an algorithm that uses a complete ordered sample and membership queries and is guaranteed to converge in the limit. They maintain a single working hypothesis that gets modified after the presentation of each sample.
Reference: [18] <author> Schapire, R., </author> <title> The Design and Analysis of Efficient Learning Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Angluin [2] has proposed a polynomial time algorithm (L fl ), which allows the learner to infer the target grammar by posing both membership and equivalence queries. Natarajan [14] has described an adaptation of Angluin's L fl procedure to the PAC learning framework. Rivest and Schapire <ref> [18] </ref> have suggested a diversity based mechanism dealing with homing sequences. Giles et al [6] use recurrent neural networks to learn FSA using positive and negative samples. Lankhorst [11] has presented a genetic algorithms based approach for learning context free grammars.
Reference: [19] <author> VanLehn, K. and Ball, W. </author> <title> A Version Space Approach to Learning Context-Free Grammars. </title> <booktitle> Machine Learning 2, </booktitle> <year> 1987. </year> <pages> pp 39-74. 12 </pages>
Reference-contexts: One potential advantage we see in maintaining a space of hypotheses (as is the case in our method) is the ability to make unambiguous inferences even when our algorithm has not converged to the desired solution. VanLehn and Ball <ref> [19] </ref> have proposed a version-space approach to learning context-free grammars from a set of positive and negative examples that returns a set of grammars consistent with the given sample set.
References-found: 19

