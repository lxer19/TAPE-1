URL: http://polaris.cs.uiuc.edu/reports/1352.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: c  
Author: flCopyright by Sirpa Helena Saarinen 
Date: 1994  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. W. Aha, D. Kibler, and M. K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: In most high dimensional data bases the number of data points is prohibitively low: if the dimension is d and if the data set is in a cube <ref> [0; 1] </ref> d and if we partition the cube at 0.5 in every dimension and position one point in each subcube, we would need 2 d points in total for a uniform distribution. <p> The 2-3-2 network in Figure 1 has 19 parameters or weights and so the Jacobian has 19 singular values. Our 2-dimensional input space is sampled on a uniform mesh in <ref> [0; 1] </ref> 2 with 400 training data points in each case presented and a maximum of 2000 function evaluations is allowed in the Levenberg-Marquardt algorithm unless indicated otherwise. The implementation of the Levenberg-Marquardt algorithm used here is that written by J. More and is available in MINPACK. <p> As an example of a larger network, Figure 2.8 shows the singular values of the initial Jacobian for four cases for a 5-7-2 network with weights chosen randomly in the region (1; 1) and t i sampled randomly in the cube <ref> [0; 1] </ref> 5 . From the graphs we can conclude that the Jacobian becomes more ill-conditioned as the norm of the weight vector increases. <p> Many recent studies on large data sets show that conventional parametric algorithms perform comparably to simple m nearest neighbor methods [39, 46, 30]. Learning systems which save examples or choose relevant examples from the given data are also popular in artificial intelligence applications <ref> [1, 63] </ref>. Considering the current availability of dense storage media, such memory-based algorithms need a general framework since many of them have similarities in the way they use memory and training data. <p> The storage based algorithms do not use an abstract parametric model, but use instead the raw data for some sort of local interpolation. Storage based algorithms often use decision trees or nonparametric classification rules. Related to these algorithms are case-based reasoning [63] and instance based learning <ref> [1] </ref>. We do not wish to make a strong distinction between these different methods, but rather discuss a data oriented approach to learning in general. <p> This has appeared to be one of the major objections to the use of the m-nearest neighbor method in practice. Many of the storage based algorithms have been aimed specifically at reducing the size of the stored design set <ref> [1, 35, 15] </ref>. These methods are often called "edited" m-nearest neighbor methods, since they edit the original database by removing points and obtain a design set that is smaller than the original data set. <p> In this sense, discarding points which appear to evaluate incorrectly actually results in a net loss of information from the system. There are algorithms, for instance by Aha <ref> [1] </ref>, where only those instances which cannot be predicted by the current model are saved. However, very few attempts have been made to simultaneously limit the size of the database to within a given size and also store "relevant" points of the data. <p> We have chosen to use the following types of data sets: the training set will be a uniform distribution in the cube <ref> [0; 1] </ref> k and the test set used for queries will be another uniform distribution in the same volume.
Reference: [2] <author> M. Arbib. </author> <title> Brains, Machines and Mathematics. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1987. </year>
Reference-contexts: It can be shown that this eventually leads to a w with the desired property in a finite number of steps, if such a w exists. Proofs can be found in <ref> [54, 36, 44, 2] </ref>. The Perception Convergence Theorem seems quite satisfactory until one begins to question the class of problems that can be effectively solved by linear separability as given by (3.4).
Reference: [3] <author> J. L. Bentley. </author> <title> Multidimensional binary search trees used for associative searching. </title> <journal> Comm. of the ACM, </journal> <volume> 18 </volume> <pages> 509-517, </pages> <year> 1975. </year>
Reference-contexts: This is also the reason why many search 103 methods are time consuming in very high dimensions. In 2 and 3 dimensions this issue is solved using e.g. Voronoi diagrams. A good description for the k-d-tree can be found in <ref> [3, 22] </ref>. The k-d-tree is a k-dimensional binary search tree, with one or more levels for each dimension that the point data set is partitioned into.
Reference: [4] <author> J. L. Bentley. </author> <title> K-d-trees for semidynamic point sets. </title> <booktitle> In Sixth Annual ACM Symposium on Computational Geometry, </booktitle> <pages> pages 187-197, </pages> <year> 1990. </year>
Reference-contexts: That method will eventually find all nearest neighbor points by using an expanding hypercube. It can be implemented most efficiently in (parallel) hardware. We next concentrate on the kd-tree only <ref> [22, 4] </ref>. Despite this apparent limitation our search algorithm can be generalized also for other methods which are based on trees, grid structures and the branch and bound method. <p> This test is incorporated as a modification to the k-d-tree as the procedure check in the algorithm in figure 6.12 (the variables used are in figure 6.11). We add this test-routine to the basic k-d-tree algorithm and follow the notation provided in <ref> [4] </ref>. The procedure find_nnb in figure 6.12 is a modified version of the well-known procedure for sequential m nearest neighbor search with k-d-trees.
Reference: [5] <author> A. Bjorck and G. Golub. </author> <title> Numerical methods for computing angles between linear sub-spaces. </title> <journal> Mathematics of Computation, </journal> <volume> 27 </volume> <pages> 579-594, </pages> <year> 1973. </year>
Reference-contexts: The cosines of the canonical angles between range spaces are used to measure how closely two blocks J 1 and J 2 of columns of J come to spanning the same space (as is needed for Cases 2 and 3). The method for computing such cosines is from <ref> [5] </ref>.
Reference: [6] <author> L. Breiman, W. Meisel, and E. Purcell. </author> <title> Variable kernel estimates of multivariate densities. </title> <journal> Technometrics, </journal> <volume> 19 </volume> <pages> 135-144, </pages> <year> 1977. </year>
Reference-contexts: Details of the properties of a kernel function can be found in [49]. Often the kernel width h is chosen as a constant derived from the sample data but also variable kernel widths have been suggested. For instance, Breiman et al. <ref> [6] </ref> suggested the use of the kth nearest neighbor distance to a point as the kernel width. In the k-NN method, given a point x, one finds the k nearest points from the design set to this point using some metric.
Reference: [7] <author> M.D. Buhmann. </author> <title> Multivariate interpolation in odd dimensional euclidean spaces using multiquadratics. </title> <type> Technical Report DAMTP 1988/NA6, </type> <institution> University of Cambridge, Dept. of Appl. Math. and Theor. Physics, </institution> <year> 1988. </year> <month> 142 </month>
Reference-contexts: u+l (P j ) 0 (Q l ) second layer weights J i;u+l (Q l ) last layer weights In this chapter we will use 1 (x), but the method in this chapter can be used to derive similar results for other excitation functions as well (including radial basis functions <ref> [7, 52] </ref>). 2.3.1 Explicit form of the Jacobian for a two-layer network The Jacobian for the multilayer feedforward network problem can be written explicitly using the notation from the previous section.
Reference: [8] <author> G. A. Carpenter and S. Grossberg. </author> <title> ART 2: self-organization of stable category recognition codes for analog input pattern recognition machine. </title> <journal> Applied Optics, </journal> <volume> 26 </volume> <pages> 4919-4930, </pages> <year> 1987. </year>
Reference-contexts: to approximate the 69 input/output behavior of a certain data set but new data cannot be readily incorporated into the network function without the danger of unlearning old training data. (This is a variant of elasticity-plasticity problem identified by Grossberg and Carpenter that motivated their work on Adaptive Resonance Theory <ref> [8] </ref>.) Simple methods for storing data and using it for supervised learning or classification purposes has been found to be comparable to model based approaches [30, 46]. The storage based algorithms do not use an abstract parametric model, but use instead the raw data for some sort of local interpolation.
Reference: [9] <author> T. M. Cover and P. E. Hart. </author> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 13 </volume> <pages> 21-27, </pages> <year> 1967. </year>
Reference-contexts: The m-nearest neighbor method is simple to use and it appears to have in the large sample limit a risk less than twice that of the Bayes risk <ref> [9] </ref>. This result is not dependent on the underlying joint probability distribution functions of the points and therefore the m-nearest neighbor method is widely used. However, in order to obtain the large sample limit, one would have to use large amounts of storage. <p> k i = maxfk 1 ; ::k m g; m X k j = k ) ^!(x) = ! i : (5:3) The nearest neighbor classification methods have been shown to have good asymptotic error rates: the 1-NN and k-NN classifier errors are bounded by twice the Bayes error rate <ref> [9] </ref>. However, in practice storing a large number of sample points and finding the neighbors in an efficient way have been problematic. <p> The background for this suggestion lies in the inherent errors of the nearest neighbor classifier. These can originate from the paradigm itself <ref> [9] </ref>, from the sample size limitation [25] and from various sampling and noise problems rising in practical applications, all of which can increase the classification error significantly when compared to an ideal asymptotic situation.
Reference: [10] <author> Cray Research, Inc. </author> <title> UNICOS Performance Utilities Reference Manual, </title> <address> SR-2040 6.0. </address> <year> 1991. </year>
Reference-contexts: Figure 26 10; 9; ::; 0; ::9; 10 and three are indicated. the curves for x = 5 with + (on the left-hand side of the figure) and x = 2 with fl (on the right-hand side). 27 indicated. 2.3 shows the graph for B (x; y) for x 2 <ref> [10; 10] </ref> and y 2 [20; 20]. Note that B (x; y) varies slowly with x. It is easy to see that sup x;y A (x; y) = 1 and sup x;y B (x; y) = 0:25 from the properties of and 0 . <p> Table 3.1 compares the performance on a single processor of the Cray Y-MP of a neural network evaluator, written initially as a naive implementation of Griewank's formulation, and then modified to the version shown in Figure 3.3. The numbers are from the Perftrace utility <ref> [10] </ref>, and the function evaluators were called 10000 times each for a 2-128-64-1 network. The number of flops shown was adjusted to reflect the implementation of division on the Cray, which counts as one reciprocal approximation and three multiplications in Perftrace.
Reference: [11] <author> G. Cybenko. </author> <title> Approximations by superpositions of a single function. </title> <journal> Mathematics of Control, Signals and Systems, </journal> <volume> 2 </volume> <pages> 303-314, </pages> <year> 1989. </year>
Reference-contexts: These choices were made because multilayer feedforward networks are commonly used by researchers and the classification problem is one for which neural nets are potentially suitable; see <ref> [11, 55, 36] </ref>. Furthermore, the training problems examined here are primarily overdetermined, that is, the number of training data points is greater than or equal to the number of network parameters. <p> It has been shown that under very general conditions on the activation functions, such classes of neural networks as in (2.7) are universal approximators <ref> [11] </ref>. <p> These networks have been successfully used in many neural network applications [59] and they can be used also as universal function approximators <ref> [11] </ref>. The form of the one hidden layer Jacobian is shown below with a numbering similar to that in Figure 2.1.
Reference: [12] <author> Belur V. Dasarathy. </author> <title> Nearest Neighbor(NN) Norms: NN pattern classification techniques. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: However, in high dimensions even these methods become cumbersome [22]. Various schemes have been developed to address the storage issue, some of which are discussed in <ref> [12, 67, 29, 35, 33, 15] </ref>. Finding an optimal representative subset of the available data is a difficult task [35, 50, 14, 27], and in the proposed method we will therefore specifically avoid removing points from the design set.
Reference: [13] <author> J. Dennis and R. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <year> 1983. </year>
Reference-contexts: Quasi-Newton methods have a superlinear rate of convergence if, roughly speaking, H (x fl ) is nonsingular and the matrices B k are chosen so that J T J + B k approximates H (x fl ) along the search directions; see <ref> [13] </ref> for a more detailed description of both quasi-Newton methods and their convergence properties. Newton's method has a quadratic rate of convergence, provided that the Hessian is nonsingular at x fl .
Reference: [14] <author> P. A. Devijver and J. Kittler. </author> <title> On the edited nearest neighbor rule. </title> <booktitle> Proc. Fifth Internat. Conf. on Pattern Recognition, </booktitle> <address> Miami Beach, FL, </address> <pages> pages 72-80, </pages> <year> 1980. </year>
Reference-contexts: The resulting error rates of some of the editing methods have been discussed in detail in <ref> [23, 50, 14, 15] </ref>. It is not always known whether the editing methods will be able always to achieve the Bayesian error rate [23]. However, the removal of points from the design set does alleviate the storage problem which is often a more serious problem than a non-Bayesian error rate. <p> However, in high dimensions even these methods become cumbersome [22]. Various schemes have been developed to address the storage issue, some of which are discussed in [12, 67, 29, 35, 33, 15]. Finding an optimal representative subset of the available data is a difficult task <ref> [35, 50, 14, 27] </ref>, and in the proposed method we will therefore specifically avoid removing points from the design set. An important result pertaining to these classification methods is the bias of nearest neighbor errors for finite samples.
Reference: [15] <author> P. A. Devijver and J. Kittler. </author> <title> Pattern Recognition: A Statistical Approach. </title> <publisher> Prentice Hall International, </publisher> <year> 1982. </year>
Reference-contexts: This has appeared to be one of the major objections to the use of the m-nearest neighbor method in practice. Many of the storage based algorithms have been aimed specifically at reducing the size of the stored design set <ref> [1, 35, 15] </ref>. These methods are often called "edited" m-nearest neighbor methods, since they edit the original database by removing points and obtain a design set that is smaller than the original data set. <p> In order to reduce the error and the amount of storage when using the k-nearest neighbor method (k-NN method) for classification, two basic types of data reduction or editing methods have been introduced: those which remove points near the Bayesian decision region <ref> [67, 33, 15] </ref> and those which remove points far away from the decision region [35, 29], both resulting in a smaller 82 design set. The resulting error rates of some of the editing methods have been discussed in detail in [23, 50, 14, 15]. <p> The resulting error rates of some of the editing methods have been discussed in detail in <ref> [23, 50, 14, 15] </ref>. It is not always known whether the editing methods will be able always to achieve the Bayesian error rate [23]. However, the removal of points from the design set does alleviate the storage problem which is often a more serious problem than a non-Bayesian error rate. <p> The marginal density function is denoted by p (x) and the conditional density, p (xj! i ), is denoted by p i (x). We will now review briefly the kernel function and the k-NN method. A more detailed discussion can be found elsewhere <ref> [23, 15] </ref>. The Parzen or kernel estimate of the p.d.f. at point x for class ! i can be written as ^p i (x) = h d N i j=1 x x j ); (5:1) where h is the kernel width and K (:) is a kernel function. <p> However, in high dimensions even these methods become cumbersome [22]. Various schemes have been developed to address the storage issue, some of which are discussed in <ref> [12, 67, 29, 35, 33, 15] </ref>. Finding an optimal representative subset of the available data is a difficult task [35, 50, 14, 27], and in the proposed method we will therefore specifically avoid removing points from the design set.
Reference: [16] <author> R. Duffin. </author> <title> On Fourier's analysis of linear inequality systems. </title> <journal> Mathematical Programming Study, </journal> <volume> 1 </volume> <pages> 71-95, </pages> <year> 1974. </year>
Reference-contexts: The Perception Convergence Theorem is based on an algorithm for solving a finite system of linear, homogeneous inequalities. Viewed as such, Rosenblatt's procedure can be regarded as a novel method for solving a restricted class of linear programs which had earlier been studied by Fourier, Motzkin and others <ref> [16] </ref>. To see the connection, note that the replacement of x j by (x j ; 1) for all j makes the threshold one of the weights, so we can assume that the inequalities are homogeneous.
Reference: [17] <author> Thomas A. Elkins. </author> <title> Cubical and spherical estimation of multivariate probability density. </title> <journal> J. Amer. Stat. Assoc., </journal> <volume> 63 </volume> <pages> 1495-1513, </pages> <year> 1968. </year> <month> 143 </month>
Reference-contexts: Approximating the probability density function with hyperspheres is not a novel idea <ref> [17] </ref>. A method that uses an information theory based criterion function for the reduction of the data set size was presented in [27]. However, in this chapter we merge the common ideas of kernel functions and k-NN methods and use them to limit the number of points the design set. <p> We further consider only those x's for which all k neighbors belong to the same class as x. We will use hyperspheres because they appear to be good and simple approximators <ref> [17] </ref>. There will be k or k + 1 points in the sphere B (x; r x ) depending on whether the point x itself is or is not counted to be among its nearest neighbors (let us use k + 1 in 86 the following). <p> Basically (x) = p (x)N 0 , where N 0 is the total number of points in the sample. A similar approximation is used when a hyperspherical kernel is chosen in (5.1) <ref> [49, 17] </ref>.
Reference: [18] <author> E. Fix and J. L. Hodges. </author> <title> Discriminatory Analysis: Nonparametric Discrimination: Con--sistency Properties. </title> <editor> In Belur V. Dasarathy, editor, </editor> <title> Nearest Neighbor (NN) norms: </title> <booktitle> NN Pattern Classification Techniques, </booktitle> <pages> pages 31-39. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: networks on simpler problems, the training times for these AM's appear shorter than what could be expected for neural networks. 81 Chapter 5 An approximate k-nearest neighbor method Kernel functions and nearest neighbor methods are often used for nonparametric estimation of probability density functions (p.d.f.) in statistical pattern recognition problems <ref> [49, 18] </ref>. These nonparametric methods are based on the notion that the value of the density function at one point can be approximated by using the frequency of data samples in a nearby region. <p> The size of the region where the density function is estimated is therefore variable. The k-NN probability density estimator for class i at point x is often expressed <ref> [18, 40] </ref> as ^p i (x) = N i v i (x) where v i (x) is the volume of the region around x containing its k-nearest neighbors from class ! i , i.e. v i (x) = fy 2 R d jd (x; y) d (x; x i )g, where
Reference: [19] <author> C. Fraley. </author> <title> Solution of Nonlinear Least-Squares Problems. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1987. </year> <note> also available as Technical Report # STAN-CS-87-1165, </note> <institution> Department of Computer Science. </institution>
Reference-contexts: Algorithms for minimizing (x) usually take advantage of the special structure of r (x) and H (x); the reader is referred to <ref> [19] </ref> for a survey of such algorithms. A given point x fl is a critical point of if r (x fl ) = 0, and x fl is a local minimum only if all the eigenvalues of H (x fl ) are nonnegative.
Reference: [20] <author> P. W. Frey and D. J. </author> <title> Slate. Letter recognition using Holland-style Adaptive Classifiers. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 161-182, </pages> <year> 1991. </year>
Reference-contexts: To explain the near linear dependence between columns of J we are therefore interested in the quantities A (x; y) = (x + y) (x); B (x; y) = (x + y) (x); where x 2 [; ] and y 2 [ffi; ffi]. of y 2 <ref> [20; 20] </ref>. Note that the differences in the graphs for large values of jxj are small. <p> 0; ::9; 10 and three are indicated. the curves for x = 5 with + (on the left-hand side of the figure) and x = 2 with fl (on the right-hand side). 27 indicated. 2.3 shows the graph for B (x; y) for x 2 [10; 10] and y 2 <ref> [20; 20] </ref>. Note that B (x; y) varies slowly with x. It is easy to see that sup x;y A (x; y) = 1 and sup x;y B (x; y) = 0:25 from the properties of and 0 . <p> problems A, B and C, respectively: A. a subset of a large speech database [53] with nearly 30,000 phonemes, where each sample data point is represented by 14 cepstral coefficients; B. a small speech database (with 4000 elements) [38] which has 20 cepstral coeffi cients; C. a letter recognition database <ref> [20] </ref> with 20,000 characters each with 16 attributes. These databases are rather small for continuous learning but large enough to test the finite learning system paradigm as discussed in section 4.3.
Reference: [21] <author> J. H. Friedman, F. Bashett, and L. J. Shustek. </author> <title> An algorithm for finding nearest neighbors. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 24 </volume> <pages> 1000-1006, </pages> <year> 1975. </year>
Reference-contexts: Several algorithms have been developed for avoiding this expensive calculation. Often a multidimensional binary tree structure is used for finding the nearest neighbors efficiently [28, 22] but other types of data structures and techniques, such as the branch and bound method, have also been presented <ref> [21, 68, 37, 60] </ref>. These data structures are very efficient in low dimensional spaces but their time performance appear slow or they 96 become cumbersome to implement in high dimensional spaces.
Reference: [22] <author> J.H. Friedman, J.L. Bentley, and R.A. Finkel. </author> <title> An algorithm for finding best matches in logarithmic expected time. </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 3 </volume> <pages> 209-226, </pages> <year> 1977. </year>
Reference-contexts: However, for the system to be efficient we have to introduce a much faster data structure since we need to perform several nearest neighbor queries during the lifetime of the system. The k-d-tree, developed by Friedman et al. <ref> [22] </ref>, is specifically suited for efficient neighbor searching in k-dimensional data. <p> The build-up time for such a modified k-d-tree is of the same order, O (N log N ), as the generic k-d-tree presented by Friedman et al <ref> [22] </ref> since it only needs a few extra quantities in each terminal and non-terminal node as compared to the generic k-d-tree. 4.4.1 The databases used for testing We will use three different databases for in training and testing our algorithm, which we will call problems A, B and C, respectively: A. <p> However, in practice storing a large number of sample points and finding the neighbors in an efficient way have been problematic. Neighbor finding has been addressed through intricate data structure schemes which have lead to O (N log N ) time complexity algorithms <ref> [28, 22] </ref> for samples of size N . However, in high dimensions even these methods become cumbersome [22]. Various schemes have been developed to address the storage issue, some of which are discussed in [12, 67, 29, 35, 33, 15]. <p> Neighbor finding has been addressed through intricate data structure schemes which have lead to O (N log N ) time complexity algorithms [28, 22] for samples of size N . However, in high dimensions even these methods become cumbersome <ref> [22] </ref>. Various schemes have been developed to address the storage issue, some of which are discussed in [12, 67, 29, 35, 33, 15]. <p> In this chapter we have no space to discuss the implementation details, which will be shown in a forthcoming paper, where the idea of finding the nearest spheres and points efficiently is similar in nature to the method discussed in <ref> [22] </ref>. We will show how the proposed method works for two different data sets containing speech data. The Bayesian minimum risk is difficult to find for these data sets, since in general the underlying distributions are hard to find for high dimensional data. <p> Several algorithms have been developed for avoiding this expensive calculation. Often a multidimensional binary tree structure is used for finding the nearest neighbors efficiently <ref> [28, 22] </ref> but other types of data structures and techniques, such as the branch and bound method, have also been presented [21, 68, 37, 60]. <p> Furthermore, only a few of the recently published nearest neighbor searching methods have addressed the time performance issue in high dimensional spaces: the performance has been rigorously analysed as a function of dimension in <ref> [22] </ref> and a simplified version of a well-known search algorithm is investigated in [43]. In theoretical computer science the finding of the exact nearest neighbors to a query point is a fundamental problem. <p> That method will eventually find all nearest neighbor points by using an expanding hypercube. It can be implemented most efficiently in (parallel) hardware. We next concentrate on the kd-tree only <ref> [22, 4] </ref>. Despite this apparent limitation our search algorithm can be generalized also for other methods which are based on trees, grid structures and the branch and bound method. <p> This is also the reason why many search 103 methods are time consuming in very high dimensions. In 2 and 3 dimensions this issue is solved using e.g. Voronoi diagrams. A good description for the k-d-tree can be found in <ref> [3, 22] </ref>. The k-d-tree is a k-dimensional binary search tree, with one or more levels for each dimension that the point data set is partitioned into. <p> In high dimensions (k 10) often all the buckets of a small k-d-tree are searched 104 in a single query. An estimate for the average number of records, q, examined is derived for a uniform distribution in <ref> [22] </ref>. The average number of records examined for the L 1 -metric is q (m; k; b) = b (( b where b is the number of elements in each bucket and m refers to the number of nearest neighbors being searched. The result for other metrics is similar. <p> Another method, which we will spend the rest of this section of, is to decide based on geometry and expectation, whether to continue the search for neighbors further. The k-d-tree is well-known for limiting the search of nearby buckets by the "bounds-overlap-ball" calculation <ref> [22, 62] </ref>: a nearby bucket is not searched for neighbors if the current search radius (the distance to the mth nearest neighbor) does not overlap with the bucket. In low density high dimensional spaces this heuristics appears not to be too efficient.
Reference: [23] <author> K. Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> second edition, </address> <year> 1990. </year>
Reference-contexts: The resulting error rates of some of the editing methods have been discussed in detail in <ref> [23, 50, 14, 15] </ref>. It is not always known whether the editing methods will be able always to achieve the Bayesian error rate [23]. However, the removal of points from the design set does alleviate the storage problem which is often a more serious problem than a non-Bayesian error rate. <p> The resulting error rates of some of the editing methods have been discussed in detail in [23, 50, 14, 15]. It is not always known whether the editing methods will be able always to achieve the Bayesian error rate <ref> [23] </ref>. However, the removal of points from the design set does alleviate the storage problem which is often a more serious problem than a non-Bayesian error rate. Note that even with modern memory devices, a storage problem can still arise for large data sets. <p> The marginal density function is denoted by p (x) and the conditional density, p (xj! i ), is denoted by p i (x). We will now review briefly the kernel function and the k-NN method. A more detailed discussion can be found elsewhere <ref> [23, 15] </ref>. The Parzen or kernel estimate of the p.d.f. at point x for class ! i can be written as ^p i (x) = h d N i j=1 x x j ); (5:1) where h is the kernel width and K (:) is a kernel function. <p> In classification problems the kernel method can be used easily if the a priori probabilities of the classes are known (details can be found in <ref> [23] </ref>). The simplest k-NN method used in classification problems is the voting approach [23]. Given a point x, one finds its k nearest neighbors and counts the number of points k i from class ! i among the k points. <p> In classification problems the kernel method can be used easily if the a priori probabilities of the classes are known (details can be found in <ref> [23] </ref>). The simplest k-NN method used in classification problems is the voting approach [23]. Given a point x, one finds its k nearest neighbors and counts the number of points k i from class ! i among the k points. <p> In high dimensional spaces this effect will be even worse, as is shown in <ref> [25, 23] </ref> and the accurate estimation of the p.d.f. in the local region becomes more important. <p> However, especially in the voting k-nearest neighbor method <ref> [23] </ref> it might not be necessary to obtain the correct nearest neighbors in all cases. Since only a maximum-measure is used (as in (5.3)), the crude neighborhood approximation might be sufficient for many cases. <p> At the same time, some of the concerns raised about some of the other data reduction methods have been addressed by not physically removing points from the decision regions <ref> [23] </ref>. 5.3 Computational results In the algorithm in section 5.2 we need to find the neighboring points and hyperspheres to a given point. For the algorithm to be useful in practice, we have to find both types of neighbors efficiently. <p> The region where the nearest neighbors are located is then spherical. The expected value of the jth moment of the distance to the mth nearest neighbor can be estimated using a linear approximation for the density coverage <ref> [24, 23] </ref> for a uniform distribution: E [d j 99 where -m = 2 + 1) (m + j=k) (N + 1) ; and p is the probability density at x.
Reference: [24] <author> K. Fukunaga and T. E. Flick. </author> <title> The 2-nn rule for more accurate nn risk estimation. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7 </volume> <pages> 107-112, </pages> <year> 1985. </year>
Reference-contexts: The region where the nearest neighbors are located is then spherical. The expected value of the jth moment of the distance to the mth nearest neighbor can be estimated using a linear approximation for the density coverage <ref> [24, 23] </ref> for a uniform distribution: E [d j 99 where -m = 2 + 1) (m + j=k) (N + 1) ; and p is the probability density at x.
Reference: [25] <author> K. Fukunaga and D. M. Hummels. </author> <title> Bias of nearest neighbor error estimates. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 9 </volume> <pages> 103-112, </pages> <year> 1987. </year>
Reference-contexts: An important result pertaining to these classification methods is the bias of nearest neighbor errors for finite samples. In <ref> [25] </ref> it is shown that an increase of the sample size will not result in a significant decrease of the asymptotic errors for the 1- and 2-NN methods. <p> In high dimensional spaces this effect will be even worse, as is shown in <ref> [25, 23] </ref> and the accurate estimation of the p.d.f. in the local region becomes more important. <p> Even if this method will be able to reduce the number of data points it will not be able to improve the convergence of the error rate. The number of points that should be used theoretically <ref> [25] </ref> is so large that the number of hyperspheres would probably saturate before enough points have been seen. However, the approximate method might be able to appear to have more points in a region (by being piece-wise constant) and therefore simulate a larger data set. <p> The background for this suggestion lies in the inherent errors of the nearest neighbor classifier. These can originate from the paradigm itself [9], from the sample size limitation <ref> [25] </ref> and from various sampling and noise problems rising in practical applications, all of which can increase the classification error significantly when compared to an ideal asymptotic situation. Considering this one could argue that any approximately correct subset of nearest neighbors is as good as the exact set.
Reference: [26] <author> K. Fukunaga and D. L. Kessell. </author> <title> Estimation of classification error. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 20 </volume> <pages> 1521-1527, </pages> <year> 1971. </year> <month> 144 </month>
Reference-contexts: The probability of error can be considered to be a function of two arguments, *(L; T ), where L is the set used during the training or design process of the system, and T is the set used in the testing of the system <ref> [26] </ref>. The initial set for such a learning system will be I even if it does not appear explicitly in the notation for *. <p> The value of m refers to the value used during training and testing using a m-nearest neighbor rule. The problem column refers to the examples in the previous Table. simple error estimate which can be used is the re-substitution error which limits the Bayes error from below <ref> [26] </ref>. Let use denote the error when using the m-nearest neighbor method by e (L; T ) = jT j x2T where e (L; T ) is the error when the test set is T and the design set is L with I as the initial state for the memory.
Reference: [27] <author> K. Fukunaga and J. M. Mantock. </author> <title> Nonparametric data reduction. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 115-118, </pages> <year> 1984. </year>
Reference-contexts: Approximating the probability density function with hyperspheres is not a novel idea [17]. A method that uses an information theory based criterion function for the reduction of the data set size was presented in <ref> [27] </ref>. However, in this chapter we merge the common ideas of kernel functions and k-NN methods and use them to limit the number of points the design set. We do not use an explicit criterion function and we approximate the data set instead with nonparametric methods. <p> However, in high dimensions even these methods become cumbersome [22]. Various schemes have been developed to address the storage issue, some of which are discussed in [12, 67, 29, 35, 33, 15]. Finding an optimal representative subset of the available data is a difficult task <ref> [35, 50, 14, 27] </ref>, and in the proposed method we will therefore specifically avoid removing points from the design set. An important result pertaining to these classification methods is the bias of nearest neighbor errors for finite samples.
Reference: [28] <author> K. Fukunaga and P.M. Narendra. </author> <title> A branch and bound algorithm for computing k-nearest neighbors. </title> <journal> IEEE Trans. on Computers., </journal> <volume> 24 </volume> <pages> 750-753, </pages> <year> 1975. </year>
Reference-contexts: However, in practice storing a large number of sample points and finding the neighbors in an efficient way have been problematic. Neighbor finding has been addressed through intricate data structure schemes which have lead to O (N log N ) time complexity algorithms <ref> [28, 22] </ref> for samples of size N . However, in high dimensions even these methods become cumbersome [22]. Various schemes have been developed to address the storage issue, some of which are discussed in [12, 67, 29, 35, 33, 15]. <p> Several algorithms have been developed for avoiding this expensive calculation. Often a multidimensional binary tree structure is used for finding the nearest neighbors efficiently <ref> [28, 22] </ref> but other types of data structures and techniques, such as the branch and bound method, have also been presented [21, 68, 37, 60].
Reference: [29] <author> G. W. Gates. </author> <title> The reduced nearest neighbor rule. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 18 </volume> <pages> 431-433, </pages> <year> 1972. </year>
Reference-contexts: and the amount of storage when using the k-nearest neighbor method (k-NN method) for classification, two basic types of data reduction or editing methods have been introduced: those which remove points near the Bayesian decision region [67, 33, 15] and those which remove points far away from the decision region <ref> [35, 29] </ref>, both resulting in a smaller 82 design set. The resulting error rates of some of the editing methods have been discussed in detail in [23, 50, 14, 15]. It is not always known whether the editing methods will be able always to achieve the Bayesian error rate [23]. <p> However, in high dimensions even these methods become cumbersome [22]. Various schemes have been developed to address the storage issue, some of which are discussed in <ref> [12, 67, 29, 35, 33, 15] </ref>. Finding an optimal representative subset of the available data is a difficult task [35, 50, 14, 27], and in the proposed method we will therefore specifically avoid removing points from the design set.
Reference: [30] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Compuation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: Many recent studies on large data sets show that conventional parametric algorithms perform comparably to simple m nearest neighbor methods <ref> [39, 46, 30] </ref>. Learning systems which save examples or choose relevant examples from the given data are also popular in artificial intelligence applications [1, 63]. <p> Such problems are closely related to classical regression and a survey of approaches in the context of learning is given by Geman et al. in <ref> [30] </ref>. Generally speaking, it is possible to divide current learning algorithms into two classes: those that use a parametric model which is fitted to the data and those that basically store raw training data for use in lookup-tables. (See [30] for a discussion of the differences in the context of learning.) <p> the context of learning is given by Geman et al. in <ref> [30] </ref>. Generally speaking, it is possible to divide current learning algorithms into two classes: those that use a parametric model which is fitted to the data and those that basically store raw training data for use in lookup-tables. (See [30] for a discussion of the differences in the context of learning.) The first class of methods includes all parametric model based methods, such as parametric classifiers, neural networks and other basis function approaches. <p> of unlearning old training data. (This is a variant of elasticity-plasticity problem identified by Grossberg and Carpenter that motivated their work on Adaptive Resonance Theory [8].) Simple methods for storing data and using it for supervised learning or classification purposes has been found to be comparable to model based approaches <ref> [30, 46] </ref>. The storage based algorithms do not use an abstract parametric model, but use instead the raw data for some sort of local interpolation. Storage based algorithms often use decision trees or nonparametric classification rules. Related to these algorithms are case-based reasoning [63] and instance based learning [1].
Reference: [31] <author> P. Gill, W. Murray, and M. Wright. </author> <title> Practical Optimization. </title> <publisher> Academic Press, </publisher> <address> London and New York, </address> <year> 1981. </year>
Reference-contexts: However, the computational determination of r is a difficult problem, and can have a dramatic effect on the resulting search direction (see the example in <ref> [31, page 136] </ref>). Furthermore the resulting search direction has nonzero components only in a subspace of dimension r. When r t n and the subspace changes slowly from iteration to iteration, the method can fail to make sufficient progress to a minimum. <p> When r t n and the subspace changes slowly from iteration to iteration, the method can fail to make sufficient progress to a minimum. Methods for circumventing this difficulty generally add a component in the orthogonal complement of the subspace of dimension r to the search direction, as in <ref> [31] </ref>. A second regularization approach adds a small multiple fl k x k of 20 the norm of the weight vector to the objective function, possibly allowing fl to change on each iteration.
Reference: [32] <author> G. Golub and C. Van Loan. </author> <title> Matrix Computations. </title> <publisher> John Hopkins University Press, </publisher> <address> Balti-more, 2 edition, </address> <year> 1989. </year>
Reference-contexts: The eigenvalues of B are p (with multiplicity 1) and 0 (with multiplicity p 1), while the eigenvalues of E are no larger than p* by the Gersgorin Disk Theorem. An application of the Wielandt-Hoffman Theorem <ref> [32] </ref> shows that A T A has one eigenvalue p satisfying j p pj p*, and the other eigenvalues i satisfy j i j p*.
Reference: [33] <author> K. Chidananda Gowda and G. Krishna. </author> <title> The condensed nearest neighbor rule using the concept of mutual nearest neighbors. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 25 </volume> <pages> 488-490, </pages> <year> 1979. </year>
Reference-contexts: In order to reduce the error and the amount of storage when using the k-nearest neighbor method (k-NN method) for classification, two basic types of data reduction or editing methods have been introduced: those which remove points near the Bayesian decision region <ref> [67, 33, 15] </ref> and those which remove points far away from the decision region [35, 29], both resulting in a smaller 82 design set. The resulting error rates of some of the editing methods have been discussed in detail in [23, 50, 14, 15]. <p> However, in high dimensions even these methods become cumbersome [22]. Various schemes have been developed to address the storage issue, some of which are discussed in <ref> [12, 67, 29, 35, 33, 15] </ref>. Finding an optimal representative subset of the available data is a difficult task [35, 50, 14, 27], and in the proposed method we will therefore specifically avoid removing points from the design set.
Reference: [34] <author> A. Griewank. </author> <title> On automatic differentiation. </title> <type> Technical report, </type> <institution> Argonne National lab, </institution> <year> 1988. </year>
Reference-contexts: The computational feasibility of such a descent algorithm lies in the efficient computation of rN (W; Inputs) for a given value of Inputs. The efficient gradient evaluation method that is typically used is essentially the reverse differentiation technique as described in <ref> [34] </ref>. The first documented description of such efficient gradient evaluation is due to Paul Werbos in 1974 [64] where it was applied to general network models with neural networks arising only as a special case. <p> Griewank <ref> [34] </ref> has shown that by using automatic differentiation with reverse accumulation, a function decomposable into basic operations such as addition and multiplication can have its gradient computed for at most five times the number of operations needed to evaluate the function itself.
Reference: [35] <author> P.E. Hart. </author> <title> The condensed nearest neighbor rule. </title> <journal> IEEE Trans. Inform.Theory, </journal> <volume> 14 </volume> <pages> 515-516, </pages> <year> 1968. </year>
Reference-contexts: This has appeared to be one of the major objections to the use of the m-nearest neighbor method in practice. Many of the storage based algorithms have been aimed specifically at reducing the size of the stored design set <ref> [1, 35, 15] </ref>. These methods are often called "edited" m-nearest neighbor methods, since they edit the original database by removing points and obtain a design set that is smaller than the original data set. <p> In such an approach, the Bayesian boundary is assumed to be adequately estimated by the initial design set and the nonparametric rule. The purged dataset is also called a consistent subset of the original set <ref> [35] </ref>. Arguments supporting such editing methods have been largely based on limited storage capabilities in the past. <p> However, very few attempts have been made to simultaneously limit the size of the database to within a given size and also store "relevant" points of the data. We do not aim to find a minimal consistent subset as in <ref> [35] </ref> but rather the best one with a given size. Therefore it would be natural to start with an initial set of examples and modify them with subsequent instances from the same distribution. <p> and the amount of storage when using the k-nearest neighbor method (k-NN method) for classification, two basic types of data reduction or editing methods have been introduced: those which remove points near the Bayesian decision region [67, 33, 15] and those which remove points far away from the decision region <ref> [35, 29] </ref>, both resulting in a smaller 82 design set. The resulting error rates of some of the editing methods have been discussed in detail in [23, 50, 14, 15]. It is not always known whether the editing methods will be able always to achieve the Bayesian error rate [23]. <p> However, in high dimensions even these methods become cumbersome [22]. Various schemes have been developed to address the storage issue, some of which are discussed in <ref> [12, 67, 29, 35, 33, 15] </ref>. Finding an optimal representative subset of the available data is a difficult task [35, 50, 14, 27], and in the proposed method we will therefore specifically avoid removing points from the design set. <p> However, in high dimensions even these methods become cumbersome [22]. Various schemes have been developed to address the storage issue, some of which are discussed in [12, 67, 29, 35, 33, 15]. Finding an optimal representative subset of the available data is a difficult task <ref> [35, 50, 14, 27] </ref>, and in the proposed method we will therefore specifically avoid removing points from the design set. An important result pertaining to these classification methods is the bias of nearest neighbor errors for finite samples.
Reference: [36] <author> J. Hertz, A. Krough, and R. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: These choices were made because multilayer feedforward networks are commonly used by researchers and the classification problem is one for which neural nets are potentially suitable; see <ref> [11, 55, 36] </ref>. Furthermore, the training problems examined here are primarily overdetermined, that is, the number of training data points is greater than or equal to the number of network parameters. <p> It can be shown that this eventually leads to a w with the desired property in a finite number of steps, if such a w exists. Proofs can be found in <ref> [54, 36, 44, 2] </ref>. The Perception Convergence Theorem seems quite satisfactory until one begins to question the class of problems that can be effectively solved by linear separability as given by (3.4).
Reference: [37] <author> B. S. Kim and S. B. Park. </author> <title> A fast k nearest neighbor finding algorithm based on the ordered partition. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 8 </volume> <pages> 761-766, </pages> <year> 1986. </year> <month> 145 </month>
Reference-contexts: Several algorithms have been developed for avoiding this expensive calculation. Often a multidimensional binary tree structure is used for finding the nearest neighbors efficiently [28, 22] but other types of data structures and techniques, such as the branch and bound method, have also been presented <ref> [21, 68, 37, 60] </ref>. These data structures are very efficient in low dimensional spaces but their time performance appear slow or they 96 become cumbersome to implement in high dimensional spaces. <p> Other methods use ordered lists based on various characteristics and concentrate the search methodically to only parts of the ordered lists <ref> [37] </ref>. A special mention should be given to the fast neighbor method presented in [68] that uses a non-arithmetic cube based algorithm that needs about mC (p; k) distance calculations to identify m nearest neigbors in a p-metric (C is a constant that depends on p and k).
Reference: [38] <author> Teuvo Kohonen, Jari Kangas, Jorma Laaksonen, and Kari Torkkola. </author> <title> LVQ PAK: A pro-gram package for the correct application of Learning Vector Quantization algorithms. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Baltimore, </address> <month> June </month> <year> 1992. </year> <note> IEEE. (to be published). </note>
Reference-contexts: databases for in training and testing our algorithm, which we will call problems A, B and C, respectively: A. a subset of a large speech database [53] with nearly 30,000 phonemes, where each sample data point is represented by 14 cepstral coefficients; B. a small speech database (with 4000 elements) <ref> [38] </ref> which has 20 cepstral coeffi cients; C. a letter recognition database [20] with 20,000 characters each with 16 attributes. These databases are rather small for continuous learning but large enough to test the finite learning system paradigm as discussed in section 4.3. <p> In Figure 4.2 we show the effect of continuous training on database B (from <ref> [38] </ref>). We chose the initial set I to be of size 600 and the system was "trained" by showing points from the set L with about 1,400 points and using a 5-nearest neighbor rule. The error rate was obtained by testing with the test set of size 1962. <p> The original set with 800 points had an error rate of over 10 %. The second data set consists of cepstral coefficient data in 20 dimensions with 19 classes. This data set is explained in detail in <ref> [38] </ref>. We use the first 1982 samples for training (with n = 981 and N n = 981) and the remaining 1982 as the test set. The classes do not contain equally many points. <p> Each vector has the dimension 134 20 and the data set has 20 classes with various numbers of examples in each class. The data set is described in more detail in <ref> [38] </ref>. The error estimate for a point will be measured as e (x; c actual ) = 1; if c est 6= c actual = 0; otherwise; (6.18) i.e. if the actual class, c actual , of the point agrees with the estimated class, c est .
Reference: [39] <author> Y. Lee. </author> <title> Handwritten digit recognition using k-nearest neighbor, radial-basis function, and backpropagation neural networks. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 440-449, </pages> <year> 1991. </year>
Reference-contexts: Many recent studies on large data sets show that conventional parametric algorithms perform comparably to simple m nearest neighbor methods <ref> [39, 46, 30] </ref>. Learning systems which save examples or choose relevant examples from the given data are also popular in artificial intelligence applications [1, 63].
Reference: [40] <author> D. O. Loftsgaarden and C. P. Quesenberry. </author> <title> A nonparametric estimate of a multivariate density function. </title> <journal> Ann. Math. Stat., </journal> <volume> 36 </volume> <pages> 1049-1051, </pages> <year> 1965. </year>
Reference-contexts: The size of the region where the density function is estimated is therefore variable. The k-NN probability density estimator for class i at point x is often expressed <ref> [18, 40] </ref> as ^p i (x) = N i v i (x) where v i (x) is the volume of the region around x containing its k-nearest neighbors from class ! i , i.e. v i (x) = fy 2 R d jd (x; y) d (x; x i )g, where
Reference: [41] <author> David Luenberger. </author> <title> Introduction to Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Except for pattern search methods, the convergence properties of optimization algorithms for differentiable functions depend on properties of the first and/or second derivatives of the objective function <ref> [41] </ref>. For example, steepest descent explicitly requires the first derivative to define its search direction, and implicitly relies on the second derivative whose properties govern the rate of convergence. <p> Steepest descent has a q-linear rate of convergence (see [48] for a definition of q-linear) with an asymptotic error constant proportional to ( 1)=( + 1), where is the condition number of H (x fl ) <ref> [41] </ref>. Conjugate gradient methods generally have a linear rate of convergence, but their behaviour depends on the definition of the conjugacy scalar fi as well as the frequency of restart, i.e., reini-tialization of the algorithm [51].
Reference: [42] <author> W.S. McCulloch and W. Pitts. </author> <title> A logical calculus of ideas imminent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5 </volume> <pages> 115-133, </pages> <year> 1943. </year>
Reference-contexts: formal description of the neural network training problem while Section 4 contains a concrete implementation example. 3.2 Historical background The basic mathematical formalism behind neural networks can be traced to the work of McCul-loch and Pitts who, in 1943, presented a simple model of the high-level functionality of a neuron <ref> [42] </ref>. A McCulloch-Pitts neural node has n inputs, x i , 1 i n, n weights, w i , 1 i n and a threshold, . <p> Then y = ( P i x i w i ) describes the McCulloch-Pitts neuron. In a seminal paper <ref> [42] </ref>, McCulloch and Pitts studied the capabilities of networks of such neurons, especially with respect to their ability to simulate arbitrary finite automata.
Reference: [43] <author> L. Miclet and M. Dabouz. </author> <title> Approximate fast nearest-neighbour recognition. </title> <journal> Pattern Recognition Letters, </journal> <volume> 1 </volume> <pages> 277-285, </pages> <year> 1983. </year>
Reference-contexts: Furthermore, only a few of the recently published nearest neighbor searching methods have addressed the time performance issue in high dimensional spaces: the performance has been rigorously analysed as a function of dimension in [22] and a simplified version of a well-known search algorithm is investigated in <ref> [43] </ref>. In theoretical computer science the finding of the exact nearest neighbors to a query point is a fundamental problem. With the "exact" nearest neighbors we refer to those points which are closest to the query point for a given metric.
Reference: [44] <author> M. Minsky and S. Papert. </author> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: It can be shown that this eventually leads to a w with the desired property in a finite number of steps, if such a w exists. Proofs can be found in <ref> [54, 36, 44, 2] </ref>. The Perception Convergence Theorem seems quite satisfactory until one begins to question the class of problems that can be effectively solved by linear separability as given by (3.4). <p> The Perception Convergence Theorem seems quite satisfactory until one begins to question the class of problems that can be effectively solved by linear separability as given by (3.4). Such questions and much deeper ones were raised by Minsky and Papert in their influential book, Perceptrons, published in 1968 <ref> [44] </ref>. They demonstrated mathematically, that not only were many interesting classes of problems not linearly separable, but that simple, seemingly reasonable restrictions on McCulloch-Pitts networks, such as neurons having bounded in-degrees, significantly limited the class of problems solvable by such techniques.
Reference: [45] <author> J. J. </author> <title> More. The Levenberg-Marquardt algorithm: Implementation and theory. </title> <editor> In G. A. Watson, editor, </editor> <title> Numerical Analysis, </title> <booktitle> Lecture Notes in Mathematics, </booktitle> <volume> volume 630, </volume> <pages> pages 105-116, </pages> <year> 1977. </year>
Reference-contexts: So if fl is prevented from decreasing to zero the quality of solution can suffer, while if fl is decreased to zero eventually the same ill-conditioning problems are encountered. Nevertheless the Levenberg-Marquardt algorithm used in Section 2.3.4 implicitly uses this second form of regularization (see <ref> [45] </ref>), and can provide adequate solutions in many cases. For most overdetermined nonlinear least squares problems, these considerations are minor. Generally the Jacobian is full rank (but ill-conditioning can occur) and it is only at exceptional points that J is rank-deficient, but even then the rank-deficiency is small.
Reference: [46] <author> K. Ng and R. P. Lippmann. </author> <title> A comparison study of the practical characteristics of neural network and conventional pattern classifiers. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 970-976. </pages> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year> <month> 146 </month>
Reference-contexts: Many recent studies on large data sets show that conventional parametric algorithms perform comparably to simple m nearest neighbor methods <ref> [39, 46, 30] </ref>. Learning systems which save examples or choose relevant examples from the given data are also popular in artificial intelligence applications [1, 63]. <p> of unlearning old training data. (This is a variant of elasticity-plasticity problem identified by Grossberg and Carpenter that motivated their work on Adaptive Resonance Theory [8].) Simple methods for storing data and using it for supervised learning or classification purposes has been found to be comparable to model based approaches <ref> [30, 46] </ref>. The storage based algorithms do not use an abstract parametric model, but use instead the raw data for some sort of local interpolation. Storage based algorithms often use decision trees or nonparametric classification rules. Related to these algorithms are case-based reasoning [63] and instance based learning [1].
Reference: [47] <author> S. M. Omohundro. </author> <title> Efficient algorithms with neural network behavior. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 273-347, </pages> <year> 1987. </year>
Reference-contexts: It has been suggested elsewhere, that finding some neighbor, not necessarily the correct one, is sufficient in some classification experiments <ref> [47] </ref>.
Reference: [48] <author> J. M. Ortega and W. C. Rheinboldt. </author> <title> Iterative Solution of Nonlinear Equations in Several Variables. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Steepest descent has a q-linear rate of convergence (see <ref> [48] </ref> for a definition of q-linear) with an asymptotic error constant proportional to ( 1)=( + 1), where is the condition number of H (x fl ) [41].
Reference: [49] <author> E. Parzen. </author> <title> On the estimation of a probability density function and the mode. </title> <journal> Ann. Math. Stat., </journal> <volume> 33 </volume> <pages> 1065-1076, </pages> <year> 1962. </year>
Reference-contexts: networks on simpler problems, the training times for these AM's appear shorter than what could be expected for neural networks. 81 Chapter 5 An approximate k-nearest neighbor method Kernel functions and nearest neighbor methods are often used for nonparametric estimation of probability density functions (p.d.f.) in statistical pattern recognition problems <ref> [49, 18] </ref>. These nonparametric methods are based on the notion that the value of the density function at one point can be approximated by using the frequency of data samples in a nearby region. <p> Details of the properties of a kernel function can be found in <ref> [49] </ref>. Often the kernel width h is chosen as a constant derived from the sample data but also variable kernel widths have been suggested. For instance, Breiman et al. [6] suggested the use of the kth nearest neighbor distance to a point as the kernel width. <p> Basically (x) = p (x)N 0 , where N 0 is the total number of points in the sample. A similar approximation is used when a hyperspherical kernel is chosen in (5.1) <ref> [49, 17] </ref>.
Reference: [50] <author> C. S. Penrod and T. J. Wagner. </author> <title> Another look at the edited nearest neighbor rule. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> 7 </volume> <pages> 92-94, </pages> <year> 1977. </year>
Reference-contexts: The resulting error rates of some of the editing methods have been discussed in detail in <ref> [23, 50, 14, 15] </ref>. It is not always known whether the editing methods will be able always to achieve the Bayesian error rate [23]. However, the removal of points from the design set does alleviate the storage problem which is often a more serious problem than a non-Bayesian error rate. <p> However, in high dimensions even these methods become cumbersome [22]. Various schemes have been developed to address the storage issue, some of which are discussed in [12, 67, 29, 35, 33, 15]. Finding an optimal representative subset of the available data is a difficult task <ref> [35, 50, 14, 27] </ref>, and in the proposed method we will therefore specifically avoid removing points from the design set. An important result pertaining to these classification methods is the bias of nearest neighbor errors for finite samples.
Reference: [51] <author> M. J. D. Powell. </author> <title> Restart procedures for the conjugate gradient method. </title> <journal> Mathematical Programming, </journal> <volume> 12 </volume> <pages> 241-254, </pages> <year> 1977. </year>
Reference-contexts: Conjugate gradient methods generally have a linear rate of convergence, but their behaviour depends on the definition of the conjugacy scalar fi as well as the frequency of restart, i.e., reini-tialization of the algorithm <ref> [51] </ref>.
Reference: [52] <author> M. J. D. Powell. </author> <title> Approximation theory and methods. </title> <publisher> Cambridge University Press, </publisher> <year> 1981. </year>
Reference-contexts: As with most approximation methods, they require the estimation of certain (possibly nonunique) parameters which are defined by the problem to be solved <ref> [52] </ref>. In neural network terminology, finding those parameters is called the training problem, and algorithms for finding them are called training algorithms. <p> u+l (P j ) 0 (Q l ) second layer weights J i;u+l (Q l ) last layer weights In this chapter we will use 1 (x), but the method in this chapter can be used to derive similar results for other excitation functions as well (including radial basis functions <ref> [7, 52] </ref>). 2.3.1 Explicit form of the Jacobian for a two-layer network The Jacobian for the multilayer feedforward network problem can be written explicitly using the notation from the previous section.
Reference: [53] <author> P. Price, W. M. Fisher, J. Bernstein, and D. S. Pallett. </author> <title> The DARPA 1000-word resource management database for continuous speech recognition. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 651-654, </pages> <year> 1988. </year>
Reference-contexts: quantities in each terminal and non-terminal node as compared to the generic k-d-tree. 4.4.1 The databases used for testing We will use three different databases for in training and testing our algorithm, which we will call problems A, B and C, respectively: A. a subset of a large speech database <ref> [53] </ref> with nearly 30,000 phonemes, where each sample data point is represented by 14 cepstral coefficients; B. a small speech database (with 4000 elements) [38] which has 20 cepstral coeffi cients; C. a letter recognition database [20] with 20,000 characters each with 16 attributes. <p> The two sample sets consist of cepstral coefficient data used for phoneme recognition. The first data set is a subset of the data in <ref> [53] </ref> and has 14 dimensions. We have chosen 2,000 points for training and 1,200 points for testing purposes. There are 4 classes in these data sets, with equal number of points of each class in the sets (the phonemes represented are: red, nun, kick and sis).
Reference: [54] <author> F. Rosenblatt. </author> <booktitle> Principles of Neurodynamics. </booktitle> <address> New York, </address> <publisher> Spartan, </publisher> <year> 1963. </year>
Reference-contexts: The crowning achievement of Rosenblatt's work was the so-called Perception Convergence Theorem which stated that if such weights and a threshold did exist, then they could indeed be effectively computed in a finite number of steps although there is no a priori bound on the number of steps required <ref> [54] </ref>. The Perception Convergence Theorem is based on an algorithm for solving a finite system of linear, homogeneous inequalities. Viewed as such, Rosenblatt's procedure can be regarded as a novel method for solving a restricted class of linear programs which had earlier been studied by Fourier, Motzkin and others [16]. <p> It can be shown that this eventually leads to a w with the desired property in a finite number of steps, if such a w exists. Proofs can be found in <ref> [54, 36, 44, 2] </ref>. The Perception Convergence Theorem seems quite satisfactory until one begins to question the class of problems that can be effectively solved by linear separability as given by (3.4).
Reference: [55] <author> D. E. Rumelhart and J. L. McClelland. </author> <title> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: These choices were made because multilayer feedforward networks are commonly used by researchers and the classification problem is one for which neural nets are potentially suitable; see <ref> [11, 55, 36] </ref>. Furthermore, the training problems examined here are primarily overdetermined, that is, the number of training data points is greater than or equal to the number of network parameters. <p> Therefore the inputs in Figure 2.1 are the same as the network inputs (shown as v 1 and v 2 ). A more elaborate description of neural networks in general can be found in <ref> [55] </ref>. We will next derive the functional form of the network function. Let the number of inputs to the network be h, the number of first layer nodes be p, and the number of second layer nodes be s. <p> Paul Werbos, to our knowledge, introduced ideas of efficient reverse- mode gradient computation in his 1974 Harvard Ph.D. thesis [64]. In the early and mid-1980's, those ideas were rediscovered by various members of the PDP group <ref> [55] </ref> and called the Generalized Delta Rule. The usage of backpropagation to denote gradient computation using reverse-mode differentiation seems to have evolved after 1985 since reverse-mode differentiation involves propagating errors back through the computation graph. <p> In the next section of the chapter, we will clarify the close connection between reverse-mode differentiation and backpropagation in neural networks. 3.3 Neural networks and backpropagation In this section we will describe the feedforward neural network training problem and review backpropagation. General reviews of backpropagation can be found in <ref> [55] </ref> and [65].
Reference: [56] <author> S. Saarinen, R. Bramley, and G. Cybenko. </author> <title> Neural networks, backpropagation, and automatic differentiation. </title> <editor> In A. Griewank and G. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 31-42. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: Singular values and eigenvalues were computed using Matlab 3.5 running on a Sun Sparcstation-1. Jacobian and Hessian matrices were computed with 48-bit mantissa arithmetic, using a form of automatic differentiation (see <ref> [56] </ref> for implementation details). The 2-3-2 network in Figure 1 has 19 parameters or weights and so the Jacobian has 19 singular values.
Reference: [57] <author> S. Saarinen, R. Bramley, and G. Cybenko. </author> <title> Ill-conditioning in neural network training problems. </title> <journal> SIAM J. Sci. Comp., </journal> <volume> 14 </volume> <pages> 693-714, </pages> <year> 1993. </year>
Reference-contexts: Once the arguments of these functions move into these flat regions, the possiblity for collinearity among the columns of a Jacobian increases significantly. Moreover, the internal structure of these networks is such that there are many common intermediate values. These observations are analyzed in detail in <ref> [57] </ref> leading to the conclusion that many neural network training problems will be highly ill-conditioned. The analysis is complemented with experiments demonstrating that such ill-conditioning actually occurs quite often, even in small problems with small residuals at the global minimum. <p> It is important to note, since this fact is often misunderstood, that problems 50 with no redundancy at the global minimum can still have redundant parameterizations (and hence ill-conditioning) around a local minimum. We refer the reader to <ref> [57] </ref> for details. In Section 2 of this chapter, we present a short history of neural network computations with special attention paid to the role that efficient gradient computation has played in the recent resurgence of interest in the field.
Reference: [58] <author> S. Saarinen and G. Cybenko. </author> <title> Reverse neighbor finding in k-d-trees. </title> <type> Technical Report 1204, </type> <institution> CSRD, </institution> <year> 1992. </year>
Reference-contexts: We have developed a local update rule which also uses the k-d-tree and which is discussed in detail in <ref> [58] </ref>. Given a point x the algorithm in [58] finds those points which have x among their m nearest neighbors in O (log N ) time. <p> We have developed a local update rule which also uses the k-d-tree and which is discussed in detail in <ref> [58] </ref>. Given a point x the algorithm in [58] finds those points which have x among their m nearest neighbors in O (log N ) time.
Reference: [59] <author> T. Sejnowski and C. Rosenberg. </author> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 1134-1142, </pages> <year> 1987. </year>
Reference-contexts: This minimization is called training. The current interest in using feedforward neural networks for pattern recognition problems has revealed that the training algorithms are computationally time consuming for a large class of algorithms <ref> [59] </ref>. Examples of these algorithms are backpropagation (which can be classified 14 as a steepest descent algorithm), conjugate gradient algorithms, and other non-linear optimiza-tion algorithms for parameter estimation. <p> These networks have been successfully used in many neural network applications <ref> [59] </ref> and they can be used also as universal function approximators [11]. The form of the one hidden layer Jacobian is shown below with a numbering similar to that in Figure 2.1.
Reference: [60] <author> I. K. Sethi. </author> <title> A fast algorithm for recognizing nearest neighbors. </title> <journal> IEEE Trans. Systems, Man and Cybernetics, </journal> <volume> 11 </volume> <pages> 245-248, </pages> <year> 1981. </year>
Reference-contexts: Several algorithms have been developed for avoiding this expensive calculation. Often a multidimensional binary tree structure is used for finding the nearest neighbors efficiently [28, 22] but other types of data structures and techniques, such as the branch and bound method, have also been presented <ref> [21, 68, 37, 60] </ref>. These data structures are very efficient in low dimensional spaces but their time performance appear slow or they 96 become cumbersome to implement in high dimensional spaces.
Reference: [61] <author> D. M. Y. Sommerville. </author> <title> An introduction to the geometry of N dimensions. </title> <publisher> Dover, </publisher> <year> 1958. </year>
Reference-contexts: By induction, or by arguing like in the building-phase of the rectangle, we get for the number of r-dimensional boundaries incident on a k-dimensional rectangle (see also <ref> [61] </ref>): N r = 2 0 B @ r C C : (6:7) Let use introduce the variable v r to denote the region incident to a boundary of dimension r into which a bucket is partitioned by the surface regions of edge length d m (we show v 0 and
Reference: [62] <author> R. F. Sproull. </author> <title> Refinements to nearest-neighbor searching in k-dimensional trees. </title> <journal> Algorith-mica, </journal> <volume> 6 </volume> <pages> 579-589, </pages> <year> 1991. </year>
Reference-contexts: The leaf nodes are also called buckets and we will primarily use this word in the rest of the chapter. The resulting partitions can be hyper-rectangles or consist of arbitrarily placed hyperplanes <ref> [62] </ref>. We will limit ourselves to the case when the partitions are hyper-rectangles. The algorithm will not lose any generality due to this. <p> The result for other metrics is similar. For small k's the number of records searched is logarithmic but in high dimensions the number of data points must also increase above q in order to obtain a logarithmic behavior <ref> [62] </ref>. It is the behavior in the high dimensional end that we will study further and specifically for data sets in high dimensions which are not exponentially large. <p> Let us refer to l calculated this way as the "effective dimension" of the data set. The analyzes for the time performance results for a k-d-tree often use a uniform distribution for the data points. Based on experiments the performance of a k-d-tree improves for a nonuniform distribution <ref> [62] </ref>. We will derive our results based on a uniform distribution and show results for non-uniform cases in sections 6.4.2 and 6.4.3. 105 6.2.3 Review of geometrical issues In this section we derive the sub-problems that arise in our algorithm (section 6.3). First we introduce some notation. <p> Another method, which we will spend the rest of this section of, is to decide based on geometry and expectation, whether to continue the search for neighbors further. The k-d-tree is well-known for limiting the search of nearby buckets by the "bounds-overlap-ball" calculation <ref> [22, 62] </ref>: a nearby bucket is not searched for neighbors if the current search radius (the distance to the mth nearest neighbor) does not overlap with the bucket. In low density high dimensional spaces this heuristics appears not to be too efficient.
Reference: [63] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1213-1228, </pages> <year> 1986. </year>
Reference-contexts: Many recent studies on large data sets show that conventional parametric algorithms perform comparably to simple m nearest neighbor methods [39, 46, 30]. Learning systems which save examples or choose relevant examples from the given data are also popular in artificial intelligence applications <ref> [1, 63] </ref>. Considering the current availability of dense storage media, such memory-based algorithms need a general framework since many of them have similarities in the way they use memory and training data. <p> The storage based algorithms do not use an abstract parametric model, but use instead the raw data for some sort of local interpolation. Storage based algorithms often use decision trees or nonparametric classification rules. Related to these algorithms are case-based reasoning <ref> [63] </ref> and instance based learning [1]. We do not wish to make a strong distinction between these different methods, but rather discuss a data oriented approach to learning in general.
Reference: [64] <author> Paul J. Werbos. </author> <title> Beyond Regression: New tools for prediction and analysis in the behavioral sciences. </title> <type> PhD thesis, </type> <institution> Harvard Univ., </institution> <address> Cambridge, MA, </address> <year> 1974. </year>
Reference-contexts: The efficient gradient evaluation method that is typically used is essentially the reverse differentiation technique as described in [34]. The first documented description of such efficient gradient evaluation is due to Paul Werbos in 1974 <ref> [64] </ref> where it was applied to general network models with neural networks arising only as a special case. <p> This set the stage for the backpropogation 53 algorithm, which does not have any convergence properties in general, but which does effectively change network weights using a gradient descent method. Paul Werbos, to our knowledge, introduced ideas of efficient reverse- mode gradient computation in his 1974 Harvard Ph.D. thesis <ref> [64] </ref>. In the early and mid-1980's, those ideas were rediscovered by various members of the PDP group [55] and called the Generalized Delta Rule.
Reference: [65] <author> Paul J. </author> <title> Werbos. </title> <booktitle> Proceedings of the IEEE. </booktitle> <pages> pages 1550-1560. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: The inexpensive computation of gradients that backpropagation allows makes many neural network simulations feasible but it is important to note that the discovery and use of backpropagation was independent of and arguably followed by the development of general automatic differentiation theory <ref> [65] </ref>. <p> General reviews of backpropagation can be found in [55] and <ref> [65] </ref>. A feedforward neural network can be described using the notion of layers of nodes, where all the nodes of one layer are connected to all the nodes in the two immediate neighboring layers (this is a special case of feedforward networks where all nodes are connected with each others). <p> In <ref> [65] </ref> Werbos presents a computation model of backpropagation for a fully connected general network. We will introduce the main idea of backpropagation with an example using a small, layered network. In Figure 3.1 the neural network has four layers, one input and one 55 output layer and two "hidden" layers. <p> Backpropagation uses the ideas of partial derivatives and ordered derivatives in calculating a derivative for a function. In this chapter we will present the equations for a layered network but for a more general network one should consult [66] and <ref> [65] </ref>. <p> These two observations have contributed to the name "backpropagation". For clarity reasons we have omitted the notation of "ordered derivatives" which can be found in <ref> [65] </ref>. However, the resulting equations are the same.
Reference: [66] <author> Paul J. </author> <title> Werbos. </title> <booktitle> Proceedings of the Eigth International Conference on Mathematical and Computer Modeling. </booktitle> <publisher> Pergamon Press., </publisher> <year> 1991. </year>
Reference-contexts: We will show next how to calculate the derivatives in (3.1). Backpropagation uses the ideas of partial derivatives and ordered derivatives in calculating a derivative for a function. In this chapter we will present the equations for a layered network but for a more general network one should consult <ref> [66] </ref> and [65].
Reference: [67] <author> D. L. Wilson. </author> <title> Asymptotic properties of nearest neighbor rules using edited data. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> 2 </volume> <pages> 408-421, </pages> <year> 1972. </year> <month> 148 </month>
Reference-contexts: In order to reduce the error and the amount of storage when using the k-nearest neighbor method (k-NN method) for classification, two basic types of data reduction or editing methods have been introduced: those which remove points near the Bayesian decision region <ref> [67, 33, 15] </ref> and those which remove points far away from the decision region [35, 29], both resulting in a smaller 82 design set. The resulting error rates of some of the editing methods have been discussed in detail in [23, 50, 14, 15]. <p> However, in high dimensions even these methods become cumbersome [22]. Various schemes have been developed to address the storage issue, some of which are discussed in <ref> [12, 67, 29, 35, 33, 15] </ref>. Finding an optimal representative subset of the available data is a difficult task [35, 50, 14, 27], and in the proposed method we will therefore specifically avoid removing points from the design set.
Reference: [68] <author> T. P. Yunck. </author> <title> A technique to identify nearest neighbors. </title> <journal> IEEE Trans. Systems, Man and Cybernetics, </journal> <volume> 6 </volume> <pages> 678-683, </pages> <year> 1979. </year> <month> 149 </month>
Reference-contexts: Several algorithms have been developed for avoiding this expensive calculation. Often a multidimensional binary tree structure is used for finding the nearest neighbors efficiently [28, 22] but other types of data structures and techniques, such as the branch and bound method, have also been presented <ref> [21, 68, 37, 60] </ref>. These data structures are very efficient in low dimensional spaces but their time performance appear slow or they 96 become cumbersome to implement in high dimensional spaces. <p> Other methods use ordered lists based on various characteristics and concentrate the search methodically to only parts of the ordered lists [37]. A special mention should be given to the fast neighbor method presented in <ref> [68] </ref> that uses a non-arithmetic cube based algorithm that needs about mC (p; k) distance calculations to identify m nearest neigbors in a p-metric (C is a constant that depends on p and k). That method will eventually find all nearest neighbor points by using an expanding hypercube.
References-found: 68

