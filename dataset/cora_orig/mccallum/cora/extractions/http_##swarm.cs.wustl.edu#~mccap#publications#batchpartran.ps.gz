URL: http://swarm.cs.wustl.edu/~mccap/publications/batchpartran.ps.gz
Refering-URL: http://swarm.cs.wustl.edu/~mccap/publications/
Root-URL: 
Email: pjm3@cs.wustl.edu barry@cs.wustl.edu  
Title: Batch Parallel Training of Simple Recurrent Neural Networks  
Author: Peter J. McCann and Barry L. Kalman 
Address: Campus Box 1045, St. Louis, Missouri 63130-4899  
Affiliation: Department of Computer Science, Washington University,  
Abstract: A concurrent implementation of the method of conjugate gradients for training Elman networks is discussed. The parallelism is obtained in the computation of the error gradient and the method is therefore applicable to any gradient descent training technique for this form of network. The experimental results were obtained on a Sun Sparc Center 2000 multiprocessor. The Sparc 2000 is a shared memory machine well suited to coarse-grained distributed computations, but the concurrency could be extended to other architectures as well.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Elman, J.L. </author> <year> (1990). </year> <title> Finding Structure in Time. </title> <booktitle> Cognitive Science 14, </booktitle> <pages> 179-211. </pages>
Reference-contexts: 1 Introduction It takes an exceptionally large amount of computer time to train recurrent networks because of the added complexity of the derivative calculations. In this work, we focus on one type of recurrent network, Elman's Simple Recurrent Network <ref> [1] </ref>, and we present a way to distribute the gradient computation. sequence information. The context units hold copies of the hidden unit activations from the previous pattern presentation, and therefore the output of the network can depend not only on the current input but also on the entire input history.
Reference: [2] <author> Kalman, B.L., and Kwasny, Stan C. </author> <year> (1993). </year> <title> TRAINREC: A System for Training Feedforward and Simple Recurrent Networks Efficiently and Correctly. </title> <type> Technical Report WUCS-93-26, </type> <institution> St. Louis: Department of Computer Science, Washington University. </institution>
Reference-contexts: Our network architecture includes "skip connections" that bypass the hidden layer. It has been determined experimentally that these connections allow for faster network training. They provide an alternate set of parameters for the linearly separable, or perceptron, portion of the problem. See <ref> [2] </ref> for a more complete discussion of the rationale for these connections. Each input sequence is an ordered set of patterns because of the recurrent connections in the network. These allow the network to learn sequence information and base its output on the history of the inputs presented to it. <p> At the beginning of a sequence, we can set the feedback activations to zero, so that they have no impact on the output during the first pattern presentation. We have found empirically that this is the best choice for the initial conditions. See <ref> [2] </ref> for a more detailed discussion. During subsequent presentations, the feedback units are copied back from the hidden layer and provide the context needed fl This material is based upon work supported by the National Science Foundation under Grant No. IRI-9201987. Thanks to Dr. <p> The derivative calculation for recurrent networks is quite computationally intense. It scales as O (kF k 4 ) for calculating the derivatives with respect to weights that connect input units to hidden units. See <ref> [2] </ref> for the details of these calculations. Note that certain implementations of second order methods may require a line search along the descent direction indicated by the gradient in order to find a minimum in that direction.
Reference: [3] <author> Kalman, B.L., and Kwasny, Stan C. </author> <year> (1992). </year> <title> Why Tanh: Choosing a Sigmoidal Function. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks (Baltimore 1992), </booktitle> <volume> vol. IV, </volume> <pages> 578-581. </pages> <address> New York: </address> <publisher> IEEE. </publisher>
Reference: [4] <author> Kalman, B.L., and Kwasny, Stan C. </author> <year> (1991). </year> <title> A Superior Error Function for Training Neural Nets. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks (Seattle 1991), </booktitle> <volume> vol. II, </volume> <pages> 49-52. </pages> <address> New York: </address> <publisher> IEEE. </publisher>
Reference: [5] <author> Kramer, A., and Sangiovanni-Vincentelli, A. </author> <year> (1989). </year> <title> Efficient Parallel Learning Algorithms for Neural Networks. </title> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <editor> ed. D.S. Touretzky, </editor> <address> 40-48. San Mateo: </address> <publisher> Morgan Kaufman. </publisher>
References-found: 5

