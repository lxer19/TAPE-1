URL: http://www-csag.cs.uiuc.edu/papers/ic-cedar-full.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Title: Optimizing COOP Languages: Study of a Protein Dynamics Program  
Author: Xingbin Zhang Vijay Karamcheti Tony Ng Andrew A. Chien 
Address: 1304 W. Springfield Avenue, Urbana, IL 61801  
Affiliation: Department of Computer Science, University of Illinois at Urbana-Champaign  
Abstract: fzhang,vijayk,achieng@cs.uiuc.edu ng@lcs.mit.edu A shortened version of this paper appears in the Proceedings of IPPS'96. Abstract Fine-grained concurrent object-oriented programming (COOP) models which provide a shared names-pace, object-level concurrency and implicit dynamic thread creation can simplify the programming of irregular parallel applications on distributed memory machines. Unfortunately, COOP models are often perceived as inefficient and thus few complete applications have been implemented in COOP languages. In this paper, we study the implementation techniques required to obtain efficient parallel execution of fine-grained COOP languages using a complete, medium-sized protein molecular dynamics program, IC-CEDAR. We found that even given high data locality and achieving good sequential efficiency, an implementation that relies only on thread-oriented compiler and runtime optimizations and software multithreading fails to achieve parallel efficiency. We show that two major sources of overhead | lack of processor-level data reuse and fine-grained threads for remote object accesses | contribute to this inefficiency. Using code transformations that can be automated in a compiler, we demonstrate two processor-oriented optimizations for fine-grained COOP languages that overcome the inefficiency and enable IC-CEDAR to achieve parallel performance comparable to a highly-tuned SPMD program. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gul Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Concurrent Object-Oriented Pro gramming We outline salient features of our programming model that facilitate irregular applications and briefly describe its implementation in the Concert System. 2.1. Programming Model Concert supports a fine-grained concurrent object-oriented programming model based on Actors <ref> [1, 7, 18] </ref>. Computation is expressed as method invocations on objects and collections of objects. Method invocations on objects conceptually operate within dynamically created fine-grained threads that are inherently concurrent, with execution order constrained only by data or programmer imposed control dependency.
Reference: [2] <author> B. N. Bershad, M. J. Zekauskas, and W. A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Proceedings of COMPCON 1993, </booktitle> <month> March </month> <year> 1993. </year>
Reference-contexts: For more dynamic applications where the data access pattern is coupled with computation, such as the radiosity code in the SPLASH-2 [50, 45] benchmark suite, we believe that runtime-based caching techniques for node-level reuse are required. Demand-driven object-caching approaches are used in several modern distributed shared memory (DSM) systems <ref> [2, 42, 21, 25] </ref>. However, effectively exploiting node-level reuse is challenging for fine-grained COOP languages. Because object access granularity is often small, runtime caching techniques must have low overhead and avoid large synchronization overheads and cache performance degradation. 7.
Reference: [3] <author> D. A. </author> <title> Case. Computer simulations of protein dynamics and thermodynamics. </title> <journal> IEEE Computer, </journal> <volume> 26:47, </volume> <year> 1993. </year>
Reference-contexts: For example, the simulation of a single medium-sized protein in water for a nanosecond requires many CRAY Y-MP CPU-days <ref> [3] </ref>. Although having ample concurrency, the application is challenging to parallelize on distributed machines because of the irregular computation and communication patterns. IC-CEDAR is based on CEDAR [17], a sequential MD program written partly in C and partly in FORTRAN, and the algorithms shown here all preserve CEDAR's simulation model.
Reference: [4] <author> J. S. Chase, F. G. Amador, E. Lazowska, H. Levy, and R. J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of Twelfth Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-58. </pages> <publisher> ACM SIGOPS, ACM Press, </publisher> <month> December </month> <year> 1989. </year>
Reference-contexts: Our work is related to several dynamic programming language systems [51, 37, 8] whose objective is to achieve high performance on distributed memory systems. Implementations of these languages have traditionally focused on communication overhead reductions [48] and thread-oriented optimizations [47, 12, 37], relying on user-specified data distribution <ref> [37, 4] </ref> to reduce communication volume. As our results show, these techniques are insufficient to achieve good parallel performance for applications similar to IC-CEDAR, when the code is expressed in a style natural to the computation.
Reference: [5] <author> Andrew A. Chien and Julian Dolby. </author> <title> The Illinois Concert system: A problem-solving environment for irregular applications. </title> <booktitle> In Proceedings of DAGS'94, The Symposium on Parallel Computation and Problem Solving Environments., </booktitle> <year> 1994. </year>
Reference-contexts: Object concurrency control also frees the programmer from managing explicit locking and unlocking. Examples of using this programming model are shown in the next section. 2.2. Programming System The Concert System <ref> [5] </ref> represents a state-of-the-art implementation of the above programming model on scalable parallel machines. On distributed memory machines, the execution model synthesizes a global namespace by detecting remote object invocations and translating them into communication operations.
Reference: [6] <author> Andrew A. Chien, Vijay Karamcheti, John Plevyak, and Xingbin Zhang. </author> <title> Concurrent Aggregates language report 2.0. </title> <note> Available via anonymous ftp from cs.uiuc.edu in /pub/csag or from http://www-csag.cs.uiuc.edu/, September 1993. </note>
Reference-contexts: Although the application is computationally intensive and has ample concurrency, the major phases exhibit irregular computation and data access patterns. Although the computation in IC-CEDAR is naturally expressed in Concert's high-level programming model, 1 IC-CEDAR is 5,000 lines in Concurrent Aggregates <ref> [6] </ref>, a dynamically-typed concurrent object-oriented language. 2 Version 3.0 of the Concert System, which implements the Concurrent Aggregates language [6], is publicly available at http://www-csag.cs.uiuc.edu/. <p> Although the computation in IC-CEDAR is naturally expressed in Concert's high-level programming model, 1 IC-CEDAR is 5,000 lines in Concurrent Aggregates <ref> [6] </ref>, a dynamically-typed concurrent object-oriented language. 2 Version 3.0 of the Concert System, which implements the Concurrent Aggregates language [6], is publicly available at http://www-csag.cs.uiuc.edu/. A new release which also supports the ICC++ language [14], a parallel C++ dialect, is in preparation. 1 our goal is to achieve performance comparable to other highly-tuned implementations, such as FORTRAN or C with message passing, while preserving the natural expression of computation.
Reference: [7] <author> William D. Clinger. </author> <title> Foundations of actor semantics. </title> <type> Technical Report AI-TR-633, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <year> 1981. </year>
Reference-contexts: Concurrent Object-Oriented Pro gramming We outline salient features of our programming model that facilitate irregular applications and briefly describe its implementation in the Concert System. 2.1. Programming Model Concert supports a fine-grained concurrent object-oriented programming model based on Actors <ref> [1, 7, 18] </ref>. Computation is expressed as method invocations on objects and collections of objects. Method invocations on objects conceptually operate within dynamically created fine-grained threads that are inherently concurrent, with execution order constrained only by data or programmer imposed control dependency.
Reference: [8] <author> David Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages an Operating Systems, </booktitle> <pages> pages 164-75, </pages> <year> 1991. </year>
Reference-contexts: Second, detrimental cache interference between communication and computation routines can be avoided, and finally, more specialized optimizations, such as bulk communication, can be applied separately to the different phases. Our work is related to several dynamic programming language systems <ref> [51, 37, 8] </ref> whose objective is to achieve high performance on distributed memory systems. Implementations of these languages have traditionally focused on communication overhead reductions [48] and thread-oriented optimizations [47, 12, 37], relying on user-specified data distribution [37, 4] to reduce communication volume.
Reference: [9] <author> S. E. DeBolt and P. A. Kollman. AMBERCUBE MD, </author> <title> parallelization of AMBER's molecular dynamics module for distributed-memory hypercube computers. </title> <journal> J. Comp. Chem., </journal> <volume> 14(3) </volume> <pages> 312-329, </pages> <year> 1993. </year>
Reference-contexts: The original SHAKE algorithm is intrinsically sequential. While we experimented with several graph-based paralleliza-tions of SHAKE from MD literature, the algorithm giving the best convergence and concurrency results, the Reciprocal Exchange algorithm <ref> [9] </ref>, still has limited concurrency and produces speedup that saturates around 16 nodes for our data set. 3.4. Summary The major phases of IC-CEDAR all exhibit highly irregular work distribution and data access patterns.
Reference: [10] <author> High Performance Fortran Forum. </author> <title> High performance Fortran language specification version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: The inspector/executor execution model has been extensively studied for SPMD programs [38, 40, 46, 43, 20] and has been shown to give good performance for irregular applications whose data access pattern is independent of the ongoing computation. Ongoing efforts to compile such applications in parallel FORTRAN (for example HPF <ref> [10] </ref> and FORTRAN-D [19]) to the inspector/executor model share many issues with the required transformations here.
Reference: [11] <author> Geoffrey C. Fox. </author> <title> Numerical Algorithms for Modern Parallel Computer Architectures. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: To exploit spatial locality in IC-CEDAR, we use a heuristic that distributes and groups objects based on their spatial coordinates. A common approach to decompose objects into clusters is Orthogonal Recursive Bisection <ref> [11] </ref>, which recursively bisects the simulation space. However, unlike simulations involving only point objects in space, a large number of objects in IC-CEDAR (such as spatial cells and charge groups) span a volume in space.
Reference: [12] <author> Seth Copen Goldstein, Klaus Eric Schauser, and David Culler. </author> <title> Lazy threads, stacklets, and synchronizers: Enabling primitives for parallel languages. </title> <booktitle> In Proceedings of POOMA'94, </booktitle> <year> 1994. </year>
Reference-contexts: Our work is related to several dynamic programming language systems [51, 37, 8] whose objective is to achieve high performance on distributed memory systems. Implementations of these languages have traditionally focused on communication overhead reductions [48] and thread-oriented optimizations <ref> [47, 12, 37] </ref>, relying on user-specified data distribution [37, 4] to reduce communication volume. As our results show, these techniques are insufficient to achieve good parallel performance for applications similar to IC-CEDAR, when the code is expressed in a style natural to the computation.
Reference: [13] <author> Ananth Y. Grama, Vipin Kumar, and Ahmed Sameh. </author> <title> Scalable parallel formulations of the Barnes-Hut method for n-body simulations. </title> <booktitle> In Proceedings of Supercomputing Conference, </booktitle> <pages> pages 439-448, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: We instead apply a spatial decomposition method based on a Morton Encoding [41] traversal of spatial cells, a technique borrowed from astrophysics applications <ref> [13, 49] </ref>. 6 The spatial cells used during the multicell neighbor list phase are convenient units for both the Mor-ton traversal and load-balancing. <p> Speculative inlining and 6 We are currently also adapting Peano-Hilbert encoding <ref> [41, 44, 13] </ref> which does not have spatial discontinuity as in Morton Encoding to dimensions that are not power-of-two. 5 hybrid execution, in particular, optimize for the cases when the target of a method invocation is local, by exploiting the underlying spatial locality.
Reference: [14] <institution> Concurrent Systems Architecture Group. </institution> <note> The ICC++ reference manual. </note> <institution> Concurrent Systems Architecture Group Memo, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: A new release which also supports the ICC++ language <ref> [14] </ref>, a parallel C++ dialect, is in preparation. 1 our goal is to achieve performance comparable to other highly-tuned implementations, such as FORTRAN or C with message passing, while preserving the natural expression of computation. <p> Sample code is shown in Figure 3. 4 For example 10 A cutoff in a 40 A 3 simulation space. 5 All sample codes are shown using ICC++ <ref> [14] </ref> syntax.
Reference: [15] <author> Robert H. Halstead Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: The Concert runtime implements a flexible hybrid execution model [35], which utilizes stack-based sequential execution and creates threads lazily only when required. The runtime system exposes specialized versions [22] of important runtime primitives, such as remote method invocation and synchronization via futures <ref> [15] </ref>, to the compiler to exploit compile-time information. In addition, communication is realized via low overhead messaging layers: Fast Messages [23, 24] on the CRAY T3D and Active Messages [48] on the TMC CM-5.
Reference: [16] <author> L. Hendren, A. Nicolau, and J. Hummel. </author> <title> Abstractions for recursive pointer data structures: Improving the analysis and transformation of imperative programs. </title> <booktitle> In Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 249-260. </pages> <booktitle> ACM SIGPLAN, </booktitle> <publisher> ACM Press, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: In addition, the pervasive use of heap-based data structures also complicates dataflow analysis of object access pattern. However, the non-recursive data structures, used in IC-CEDAR and similar applications, are amenable to high-level data structure descriptions <ref> [34, 16] </ref>, based on which precise data access patterns can be extracted. We are currently developing a compiler framework that uses high-level data structure annotations in conjunction with concurrency analysis to compile this class of applications.
Reference: [17] <author> J. Hermans and M. Carson. </author> <title> Cedar documentation. Unpublished manual for CEDAR, </title> <year> 1985. </year>
Reference-contexts: For example, the simulation of a single medium-sized protein in water for a nanosecond requires many CRAY Y-MP CPU-days [3]. Although having ample concurrency, the application is challenging to parallelize on distributed machines because of the irregular computation and communication patterns. IC-CEDAR is based on CEDAR <ref> [17] </ref>, a sequential MD program written partly in C and partly in FORTRAN, and the algorithms shown here all preserve CEDAR's simulation model. Simulations are carried out in discrete time steps with four computation phases, as shown in Figure 1.
Reference: [18] <author> C. Hewitt and H. Baker. </author> <title> Actors and continuous func-tionals. </title> <booktitle> In Proceedings of the IFIP Working Conference on Formal Description of Programming Concepts, </booktitle> <pages> pages 367-87, </pages> <month> August </month> <year> 1977. </year>
Reference-contexts: Concurrent Object-Oriented Pro gramming We outline salient features of our programming model that facilitate irregular applications and briefly describe its implementation in the Concert System. 2.1. Programming Model Concert supports a fine-grained concurrent object-oriented programming model based on Actors <ref> [1, 7, 18] </ref>. Computation is expressed as method invocations on objects and collections of objects. Method invocations on objects conceptually operate within dynamically created fine-grained threads that are inherently concurrent, with execution order constrained only by data or programmer imposed control dependency.
Reference: [19] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for FORTRAN D on MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <month> August </month> <year> 1992. </year>
Reference-contexts: Ongoing efforts to compile such applications in parallel FORTRAN (for example HPF [10] and FORTRAN-D <ref> [19] </ref>) to the inspector/executor model share many issues with the required transformations here.
Reference: [20] <author> Yuan-Shin Hwang, Raja Das, Joel Saltz, Bernard Brooks, and Milan Hodoscek. </author> <title> Parallelizing molecular dynamics programs for distributed memory machines. </title> <journal> IEEE Computational Science and Engineering, </journal> <pages> pages 18-29, </pages> <month> Summer </month> <year> 1995. </year>
Reference-contexts: Comparison to Other Implementations Finally, to place our parallel performance results in context, Table 4 compares the overall execution time on 64 nodes of the CRAY T3D between IC-CEDAR and a parallel implementation of CHARMM, a similar protein MD package <ref> [20] </ref>. CHARMM is a highly-tuned SPMD implementation using sequential FORTRAN with manually inserted calls to the CHAOS [39] runtime library for communication and loadbalancing. <p> We believe for applications exhibiting similar properties as IC-CEDAR, compile-time global program transformations is the best approach to exploit node-level reuse and group communication. The inspector/executor execution model has been extensively studied for SPMD programs <ref> [38, 40, 46, 43, 20] </ref> and has been shown to give good performance for irregular applications whose data access pattern is independent of the ongoing computation.
Reference: [21] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-performance all-software distributed shared memory. </title> <booktitle> In Proceedings of the Symposium on Operating Systems Principles, </booktitle> <year> 1995. </year>
Reference-contexts: For more dynamic applications where the data access pattern is coupled with computation, such as the radiosity code in the SPLASH-2 [50, 45] benchmark suite, we believe that runtime-based caching techniques for node-level reuse are required. Demand-driven object-caching approaches are used in several modern distributed shared memory (DSM) systems <ref> [2, 42, 21, 25] </ref>. However, effectively exploiting node-level reuse is challenging for fine-grained COOP languages. Because object access granularity is often small, runtime caching techniques must have low overhead and avoid large synchronization overheads and cache performance degradation. 7.
Reference: [22] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concert efficient runtime support for concurrent object-oriented programming languages on stock hardware. </title> <booktitle> In Proceedings of Supercomputing'93, </booktitle> <year> 1993. </year>
Reference-contexts: implementation overhead in fine-grained COOP languages [36, 32] (A previous study [36] discusses some of the optimizations in detail and shows that sequential performance equivalent to C is achieved for the Livermore kernels [30], a de manding numerical benchmark.) and a runtime system with high performance implementation of COOP primitives <ref> [22] </ref> and a hybrid execution model [35] that creates parallel threads lazily to optimize sequential ex ecution. We found that even given high data locality and achieving good sequential efficiency, an implementation that relies only on thread-oriented compiler and run-time optimizations and software multithreading fails to achieve parallel efficiency. <p> The Concert runtime implements a flexible hybrid execution model [35], which utilizes stack-based sequential execution and creates threads lazily only when required. The runtime system exposes specialized versions <ref> [22] </ref> of important runtime primitives, such as remote method invocation and synchronization via futures [15], to the compiler to exploit compile-time information. In addition, communication is realized via low overhead messaging layers: Fast Messages [23, 24] on the CRAY T3D and Active Messages [48] on the TMC CM-5.
Reference: [23] <author> Vijay Karamcheti and Andrew A. Chien. </author> <title> A comparison of architectural support for messaging on the TMC CM-5 and the Cray T3D. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/cm5-t3d-messaging.ps. </note>
Reference-contexts: The runtime system exposes specialized versions [22] of important runtime primitives, such as remote method invocation and synchronization via futures [15], to the compiler to exploit compile-time information. In addition, communication is realized via low overhead messaging layers: Fast Messages <ref> [23, 24] </ref> on the CRAY T3D and Active Messages [48] on the TMC CM-5. In addition to the standard features of the programming model, IC-CEDAR also utilizes general placement directives of collection of objects (similar to map arrays in HPF) for spatial-based object distribution and grouping. 3. <p> Although multithreading in the execution model is effective to hide the communication and remote invocation latency, the processor overhead of communication and synchronization dominates. Even with a low overhead communication layer <ref> [23] </ref>, an access to a remote object involves sending and receiving two messages (request and reply) and the execution of a remote accessor handler, costing nearly 12 microseconds of total processor overhead on the T3D.
Reference: [24] <author> Vijay Karamcheti and Andrew A. Chien. </author> <title> FM| fast messaging on the Cray T3D. </title> <note> Available from http://www-csag.cs.uiuc.edu/papers/t3dfm-manual.ps, February 1995. </note>
Reference-contexts: The runtime system exposes specialized versions [22] of important runtime primitives, such as remote method invocation and synchronization via futures [15], to the compiler to exploit compile-time information. In addition, communication is realized via low overhead messaging layers: Fast Messages <ref> [23, 24] </ref> on the CRAY T3D and Active Messages [48] on the TMC CM-5. In addition to the standard features of the programming model, IC-CEDAR also utilizes general placement directives of collection of objects (similar to map arrays in HPF) for spatial-based object distribution and grouping. 3.
Reference: [25] <author> P. Keleher, A. L. Cox, S. Dwarkadas, and W. Zwaenopol. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: For more dynamic applications where the data access pattern is coupled with computation, such as the radiosity code in the SPLASH-2 [50, 45] benchmark suite, we believe that runtime-based caching techniques for node-level reuse are required. Demand-driven object-caching approaches are used in several modern distributed shared memory (DSM) systems <ref> [2, 42, 21, 25] </ref>. However, effectively exploiting node-level reuse is challenging for fine-grained COOP languages. Because object access granularity is often small, runtime caching techniques must have low overhead and avoid large synchronization overheads and cache performance degradation. 7.
Reference: [26] <author> Arvind Krishnamurthy and Katherine Yelick. </author> <title> Optimizing parallel programs with explicit synchronization. </title> <booktitle> In Proceedings of the 1995 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 196-204, </pages> <month> June </month> <year> 1995. </year> <month> 11 </month>
Reference-contexts: General compiler-directed node-level reuse and communication grouping for COOP programs is still an open problem, because the programming model supports a shared namespace in the presence of inherently concurrent threads, requiring additional concurrency [28] and access cycle <ref> [26] </ref> information. In addition, the pervasive use of heap-based data structures also complicates dataflow analysis of object access pattern. However, the non-recursive data structures, used in IC-CEDAR and similar applications, are amenable to high-level data structure descriptions [34, 16], based on which precise data access patterns can be extracted.
Reference: [27] <editor> Lomdahl, et. al. </editor> <booktitle> 50 GFlops molecular dynamics on the Connection Machine 5. In Proceedings of Supercomputing '93, </booktitle> <pages> page 520. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year>
Reference-contexts: Each atom belongs to a unique charge group (groups of 1 to 10 atoms whose total charge is neutral) and if any pair of atoms from two charge groups are within the cutoff, all atoms from the two groups must be paired. We use a multicell algorithm <ref> [27] </ref> which divides the simulation space into fixed-sized spatial cells to reduce the computational complexity in the neighbor list construction. Each cell maintains a list of neighboring cells that could potentially contain atoms within the cutoff radius.
Reference: [28] <author> Stephen P. Masticola and Barbara G. Ryder. </author> <title> Non-concurrency analysis. </title> <booktitle> In Proceedings of Fourth Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 129-138, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: General compiler-directed node-level reuse and communication grouping for COOP programs is still an open problem, because the programming model supports a shared namespace in the presence of inherently concurrent threads, requiring additional concurrency <ref> [28] </ref> and access cycle [26] information. In addition, the pervasive use of heap-based data structures also complicates dataflow analysis of object access pattern.
Reference: [29] <author> J. A. McCammon and Stephen C. Harvey. </author> <title> Dynamics of Proteins and Nucleic Acids. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: The force calculation on each group of 2 to 4 atoms requires both reading the position fields and updating the force fields of each atom, causing irregular data access pattern, which also forms spherical spatial locality regions as in the neighbor list phase. 3.3. SHAKE SHAKE <ref> [29] </ref>, an iterative coordinate correction algorithm, is applied after each position update.
Reference: [30] <author> F. H. McMahon. </author> <title> The Livermore Fortran kernels: a computer test of the numerical performance range. </title> <type> Technical report UCRL-53745, </type> <institution> Lawerence Livermore National Laboratory, Livermore, California, </institution> <year> 1986. </year>
Reference-contexts: The Concert system in cludes: a global optimizing compiler capable of eliminating the majority of implementation overhead in fine-grained COOP languages [36, 32] (A previous study [36] discusses some of the optimizations in detail and shows that sequential performance equivalent to C is achieved for the Livermore kernels <ref> [30] </ref>, a de manding numerical benchmark.) and a runtime system with high performance implementation of COOP primitives [22] and a hybrid execution model [35] that creates parallel threads lazily to optimize sequential ex ecution. <p> The optimizations achieve sequential performance equal to that of C for a large number of programs [32, 36], including the Liver-more kernels <ref> [30] </ref>, a demanding numerical benchmark. The Concert runtime implements a flexible hybrid execution model [35], which utilizes stack-based sequential execution and creates threads lazily only when required.
Reference: [31] <author> J. Plevyak and A. Chien. </author> <title> Type directed cloning for object-oriented programs. </title> <booktitle> In Proceedings of the Workshop for Languages and Compilers for Parallel Computing, </booktitle> <year> 1995. </year>
Reference-contexts: On distributed memory machines, the execution model synthesizes a global namespace by detecting remote object invocations and translating them into communication operations. To hide latency of remote invocations, the execution model also implements software multi-threading on commercial single-threaded microprocessors. The Concert compiler <ref> [32, 31, 36, 33, 34] </ref> implements a suite of interprocedural transformations, including global concrete type inference, inlining, and interprocedural cloning and constant propagation, and conventional scalar optimizations.
Reference: [32] <author> John Plevyak. </author> <title> Optimization of Object-Oriented and Concurrent Programs. </title> <type> PhD thesis, </type> <institution> University of Illi-nois at Urbana-Champaign, Urbana, Illinois. </institution> <note> In Preparation. </note>
Reference-contexts: The Concert system in cludes: a global optimizing compiler capable of eliminating the majority of implementation overhead in fine-grained COOP languages <ref> [36, 32] </ref> (A previous study [36] discusses some of the optimizations in detail and shows that sequential performance equivalent to C is achieved for the Livermore kernels [30], a de manding numerical benchmark.) and a runtime system with high performance implementation of COOP primitives [22] and a hybrid execution model [35] <p> On distributed memory machines, the execution model synthesizes a global namespace by detecting remote object invocations and translating them into communication operations. To hide latency of remote invocations, the execution model also implements software multi-threading on commercial single-threaded microprocessors. The Concert compiler <ref> [32, 31, 36, 33, 34] </ref> implements a suite of interprocedural transformations, including global concrete type inference, inlining, and interprocedural cloning and constant propagation, and conventional scalar optimizations. <p> The Concert compiler [32, 31, 36, 33, 34] implements a suite of interprocedural transformations, including global concrete type inference, inlining, and interprocedural cloning and constant propagation, and conventional scalar optimizations. The optimizations achieve sequential performance equal to that of C for a large number of programs <ref> [32, 36] </ref>, including the Liver-more kernels [30], a demanding numerical benchmark. The Concert runtime implements a flexible hybrid execution model [35], which utilizes stack-based sequential execution and creates threads lazily only when required.
Reference: [33] <author> John Plevyak and Andrew Chien. </author> <title> Precise object-oriented type inference and its use in program optimization. </title> <note> Submitted for publication, TOPLAS, </note> <year> 1994. </year>
Reference-contexts: On distributed memory machines, the execution model synthesizes a global namespace by detecting remote object invocations and translating them into communication operations. To hide latency of remote invocations, the execution model also implements software multi-threading on commercial single-threaded microprocessors. The Concert compiler <ref> [32, 31, 36, 33, 34] </ref> implements a suite of interprocedural transformations, including global concrete type inference, inlining, and interprocedural cloning and constant propagation, and conventional scalar optimizations.
Reference: [34] <author> John Plevyak, Vijay Karamcheti, and Andrew Chien. </author> <title> Analysis of dynamic structures for efficient parallel execution. </title> <booktitle> In Proceedings of the Sixth Workshop for Languages and Compilers for Parallel Machines, </booktitle> <pages> pages 37-56, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: On distributed memory machines, the execution model synthesizes a global namespace by detecting remote object invocations and translating them into communication operations. To hide latency of remote invocations, the execution model also implements software multi-threading on commercial single-threaded microprocessors. The Concert compiler <ref> [32, 31, 36, 33, 34] </ref> implements a suite of interprocedural transformations, including global concrete type inference, inlining, and interprocedural cloning and constant propagation, and conventional scalar optimizations. <p> In addition, the pervasive use of heap-based data structures also complicates dataflow analysis of object access pattern. However, the non-recursive data structures, used in IC-CEDAR and similar applications, are amenable to high-level data structure descriptions <ref> [34, 16] </ref>, based on which precise data access patterns can be extracted. We are currently developing a compiler framework that uses high-level data structure annotations in conjunction with concurrency analysis to compile this class of applications.
Reference: [35] <author> John Plevyak, Vijay Karamcheti, Xingbin Zhang, and Andrew Chien. </author> <title> A hybrid execution model for fine-grained languages on distributed memory multicom-puters. </title> <booktitle> In Proceedings of Supercomputing'95, </booktitle> <year> 1995. </year>
Reference-contexts: [36, 32] (A previous study [36] discusses some of the optimizations in detail and shows that sequential performance equivalent to C is achieved for the Livermore kernels [30], a de manding numerical benchmark.) and a runtime system with high performance implementation of COOP primitives [22] and a hybrid execution model <ref> [35] </ref> that creates parallel threads lazily to optimize sequential ex ecution. We found that even given high data locality and achieving good sequential efficiency, an implementation that relies only on thread-oriented compiler and run-time optimizations and software multithreading fails to achieve parallel efficiency. <p> The optimizations achieve sequential performance equal to that of C for a large number of programs [32, 36], including the Liver-more kernels [30], a demanding numerical benchmark. The Concert runtime implements a flexible hybrid execution model <ref> [35] </ref>, which utilizes stack-based sequential execution and creates threads lazily only when required. The runtime system exposes specialized versions [22] of important runtime primitives, such as remote method invocation and synchronization via futures [15], to the compiler to exploit compile-time information.
Reference: [36] <author> John Plevyak, Xingbin Zhang, and Andrew A. Chien. </author> <title> Obtaining sequential efficiency in concurrent object-oriented programs. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 311-321, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: The Concert system in cludes: a global optimizing compiler capable of eliminating the majority of implementation overhead in fine-grained COOP languages <ref> [36, 32] </ref> (A previous study [36] discusses some of the optimizations in detail and shows that sequential performance equivalent to C is achieved for the Livermore kernels [30], a de manding numerical benchmark.) and a runtime system with high performance implementation of COOP primitives [22] and a hybrid execution model [35] <p> The Concert system in cludes: a global optimizing compiler capable of eliminating the majority of implementation overhead in fine-grained COOP languages [36, 32] (A previous study <ref> [36] </ref> discusses some of the optimizations in detail and shows that sequential performance equivalent to C is achieved for the Livermore kernels [30], a de manding numerical benchmark.) and a runtime system with high performance implementation of COOP primitives [22] and a hybrid execution model [35] that creates parallel threads lazily <p> On distributed memory machines, the execution model synthesizes a global namespace by detecting remote object invocations and translating them into communication operations. To hide latency of remote invocations, the execution model also implements software multi-threading on commercial single-threaded microprocessors. The Concert compiler <ref> [32, 31, 36, 33, 34] </ref> implements a suite of interprocedural transformations, including global concrete type inference, inlining, and interprocedural cloning and constant propagation, and conventional scalar optimizations. <p> The Concert compiler [32, 31, 36, 33, 34] implements a suite of interprocedural transformations, including global concrete type inference, inlining, and interprocedural cloning and constant propagation, and conventional scalar optimizations. The optimizations achieve sequential performance equal to that of C for a large number of programs <ref> [32, 36] </ref>, including the Liver-more kernels [30], a demanding numerical benchmark. The Concert runtime implements a flexible hybrid execution model [35], which utilizes stack-based sequential execution and creates threads lazily only when required.
Reference: [37] <author> A. Rogers, M. Carlisle, J. Reppy, and L. Hendren. </author> <title> Supporting dynamic data structures on distributed memory machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <year> 1995. </year>
Reference-contexts: Second, detrimental cache interference between communication and computation routines can be avoided, and finally, more specialized optimizations, such as bulk communication, can be applied separately to the different phases. Our work is related to several dynamic programming language systems <ref> [51, 37, 8] </ref> whose objective is to achieve high performance on distributed memory systems. Implementations of these languages have traditionally focused on communication overhead reductions [48] and thread-oriented optimizations [47, 12, 37], relying on user-specified data distribution [37, 4] to reduce communication volume. <p> Our work is related to several dynamic programming language systems [51, 37, 8] whose objective is to achieve high performance on distributed memory systems. Implementations of these languages have traditionally focused on communication overhead reductions [48] and thread-oriented optimizations <ref> [47, 12, 37] </ref>, relying on user-specified data distribution [37, 4] to reduce communication volume. As our results show, these techniques are insufficient to achieve good parallel performance for applications similar to IC-CEDAR, when the code is expressed in a style natural to the computation. <p> Our work is related to several dynamic programming language systems [51, 37, 8] whose objective is to achieve high performance on distributed memory systems. Implementations of these languages have traditionally focused on communication overhead reductions [48] and thread-oriented optimizations [47, 12, 37], relying on user-specified data distribution <ref> [37, 4] </ref> to reduce communication volume. As our results show, these techniques are insufficient to achieve good parallel performance for applications similar to IC-CEDAR, when the code is expressed in a style natural to the computation.
Reference: [38] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berry-man. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference-contexts: To study the performance impact of fine-grained communication threads, in the final version of the force kernel we completely separated communication from computation using the inspector/executor execution model <ref> [38] </ref>. In the neighbor list and force computation phases, an inspector determines all remote data accesses and performs the communication before the actual computation. <p> We believe for applications exhibiting similar properties as IC-CEDAR, compile-time global program transformations is the best approach to exploit node-level reuse and group communication. The inspector/executor execution model has been extensively studied for SPMD programs <ref> [38, 40, 46, 43, 20] </ref> and has been shown to give good performance for irregular applications whose data access pattern is independent of the ongoing computation.
Reference: [39] <editor> J.H. Saltz et al. </editor> <title> A manual for the CHAOS runtime library. </title> <type> Technical Report CS-TK-3437, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1995. </year>
Reference-contexts: CHARMM is a highly-tuned SPMD implementation using sequential FORTRAN with manually inserted calls to the CHAOS <ref> [39] </ref> runtime library for communication and loadbalancing. The CHARMM execution time 9 is for a larger myo-globin simulation containing 14,026 atoms (3830 water molecules), with a 14 A cutoff resulting in 6.7 million nonbonded pairs.
Reference: [40] <editor> J. Saltz et al. </editor> <title> Parti procedures for realistic loops. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computers Conference, </booktitle> <pages> pages 67-74, </pages> <year> 1991. </year>
Reference-contexts: We believe for applications exhibiting similar properties as IC-CEDAR, compile-time global program transformations is the best approach to exploit node-level reuse and group communication. The inspector/executor execution model has been extensively studied for SPMD programs <ref> [38, 40, 46, 43, 20] </ref> and has been shown to give good performance for irregular applications whose data access pattern is independent of the ongoing computation.
Reference: [41] <author> Hanan Samet. </author> <title> The Design and Analysis of Spatial Data Structures. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: Therefore, any recursive bisector of space is likely to intersect many objects, requiring careful partitioning around the bisector to obtain good results, even for cubic spatial cells [44]. We instead apply a spatial decomposition method based on a Morton Encoding <ref> [41] </ref> traversal of spatial cells, a technique borrowed from astrophysics applications [13, 49]. 6 The spatial cells used during the multicell neighbor list phase are convenient units for both the Mor-ton traversal and load-balancing. <p> Speculative inlining and 6 We are currently also adapting Peano-Hilbert encoding <ref> [41, 44, 13] </ref> which does not have spatial discontinuity as in Morton Encoding to dimensions that are not power-of-two. 5 hybrid execution, in particular, optimize for the cases when the target of a method invocation is local, by exploiting the underlying spatial locality.
Reference: [42] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The design and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In First Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1994. </year>
Reference-contexts: For more dynamic applications where the data access pattern is coupled with computation, such as the radiosity code in the SPLASH-2 [50, 45] benchmark suite, we believe that runtime-based caching techniques for node-level reuse are required. Demand-driven object-caching approaches are used in several modern distributed shared memory (DSM) systems <ref> [2, 42, 21, 25] </ref>. However, effectively exploiting node-level reuse is challenging for fine-grained COOP languages. Because object access granularity is often small, runtime caching techniques must have low overhead and avoid large synchronization overheads and cache performance degradation. 7.
Reference: [43] <author> Shamik D. Sharma, Ravi Ponnusamy, Bongki Moon, Yuan shin Hwan, Raja Das, and Joel Saltz. </author> <title> Run-time and compile-time support for adaptive irregular problems. </title> <booktitle> In Supercomputing '94, </booktitle> <year> 1994. </year>
Reference-contexts: We believe for applications exhibiting similar properties as IC-CEDAR, compile-time global program transformations is the best approach to exploit node-level reuse and group communication. The inspector/executor execution model has been extensively studied for SPMD programs <ref> [38, 40, 46, 43, 20] </ref> and has been shown to give good performance for irregular applications whose data access pattern is independent of the ongoing computation.
Reference: [44] <author> Jaswinder Pal Singh. </author> <title> Parallel Hierachical N-Body Methods and Their Implications For Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University Department of Computer Science, Stanford, </institution> <address> CA, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: Therefore, any recursive bisector of space is likely to intersect many objects, requiring careful partitioning around the bisector to obtain good results, even for cubic spatial cells <ref> [44] </ref>. We instead apply a spatial decomposition method based on a Morton Encoding [41] traversal of spatial cells, a technique borrowed from astrophysics applications [13, 49]. 6 The spatial cells used during the multicell neighbor list phase are convenient units for both the Mor-ton traversal and load-balancing. <p> Speculative inlining and 6 We are currently also adapting Peano-Hilbert encoding <ref> [41, 44, 13] </ref> which does not have spatial discontinuity as in Morton Encoding to dimensions that are not power-of-two. 5 hybrid execution, in particular, optimize for the cases when the target of a method invocation is local, by exploiting the underlying spatial locality.
Reference: [45] <author> Jaswinder Pal Singh, Chris Hold, Takashi Totsuka, Anoop Gupta, and John L. Hennessy. </author> <title> Load balancing and data locality in hierarchical n-body methods. </title> <type> Technical Report CSL-TR-92-505, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: We are currently developing a compiler framework that uses high-level data structure annotations in conjunction with concurrency analysis to compile this class of applications. For more dynamic applications where the data access pattern is coupled with computation, such as the radiosity code in the SPLASH-2 <ref> [50, 45] </ref> benchmark suite, we believe that runtime-based caching techniques for node-level reuse are required. Demand-driven object-caching approaches are used in several modern distributed shared memory (DSM) systems [2, 42, 21, 25]. However, effectively exploiting node-level reuse is challenging for fine-grained COOP languages. <p> We believe the optimizations will yield significant performance improvement for IC-CEDAR and similar applications centered around non-recursive data structures. Finally, we are studying runtime object caching and function migration techniques for more dynamic applications, such as the radiosity code <ref> [50, 45] </ref>, where the data access pattern is intrinsically coupled with ongoing computation.
Reference: [46] <author> Shun tak Leung and John Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In Proceedings of Fourth Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We believe for applications exhibiting similar properties as IC-CEDAR, compile-time global program transformations is the best approach to exploit node-level reuse and group communication. The inspector/executor execution model has been extensively studied for SPMD programs <ref> [38, 40, 46, 43, 20] </ref> and has been shown to give good performance for irregular applications whose data access pattern is independent of the ongoing computation.
Reference: [47] <author> Kenjiro Taura, Satoshi Matsuoka, and Akinori Yonezawa. StackThreads: </author> <title> An abstract machine for scheduling fine-grain threads on stock CPUs. </title> <booktitle> In Joint Symposium on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: Our work is related to several dynamic programming language systems [51, 37, 8] whose objective is to achieve high performance on distributed memory systems. Implementations of these languages have traditionally focused on communication overhead reductions [48] and thread-oriented optimizations <ref> [47, 12, 37] </ref>, relying on user-specified data distribution [37, 4] to reduce communication volume. As our results show, these techniques are insufficient to achieve good parallel performance for applications similar to IC-CEDAR, when the code is expressed in a style natural to the computation.
Reference: [48] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1992. </year> <note> Available from http://www.cs.cornell.edu/Info/People/tve/ucb papers/isca92.ps. </note>
Reference-contexts: The runtime system exposes specialized versions [22] of important runtime primitives, such as remote method invocation and synchronization via futures [15], to the compiler to exploit compile-time information. In addition, communication is realized via low overhead messaging layers: Fast Messages [23, 24] on the CRAY T3D and Active Messages <ref> [48] </ref> on the TMC CM-5. In addition to the standard features of the programming model, IC-CEDAR also utilizes general placement directives of collection of objects (similar to map arrays in HPF) for spatial-based object distribution and grouping. 3. <p> Our work is related to several dynamic programming language systems [51, 37, 8] whose objective is to achieve high performance on distributed memory systems. Implementations of these languages have traditionally focused on communication overhead reductions <ref> [48] </ref> and thread-oriented optimizations [47, 12, 37], relying on user-specified data distribution [37, 4] to reduce communication volume. As our results show, these techniques are insufficient to achieve good parallel performance for applications similar to IC-CEDAR, when the code is expressed in a style natural to the computation.
Reference: [49] <author> M. Warren and J. Salmon. </author> <title> A parallel hashed oct-tree n-body algorithm. </title> <booktitle> In Proceedings of Supercomputing Conference, </booktitle> <pages> pages 12-21, </pages> <year> 1993. </year>
Reference-contexts: We instead apply a spatial decomposition method based on a Morton Encoding [41] traversal of spatial cells, a technique borrowed from astrophysics applications <ref> [13, 49] </ref>. 6 The spatial cells used during the multicell neighbor list phase are convenient units for both the Mor-ton traversal and load-balancing.
Reference: [50] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Tor-rie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <year> 1995. </year>
Reference-contexts: We are currently developing a compiler framework that uses high-level data structure annotations in conjunction with concurrency analysis to compile this class of applications. For more dynamic applications where the data access pattern is coupled with computation, such as the radiosity code in the SPLASH-2 <ref> [50, 45] </ref> benchmark suite, we believe that runtime-based caching techniques for node-level reuse are required. Demand-driven object-caching approaches are used in several modern distributed shared memory (DSM) systems [2, 42, 21, 25]. However, effectively exploiting node-level reuse is challenging for fine-grained COOP languages. <p> We believe the optimizations will yield significant performance improvement for IC-CEDAR and similar applications centered around non-recursive data structures. Finally, we are studying runtime object caching and function migration techniques for more dynamic applications, such as the radiosity code <ref> [50, 45] </ref>, where the data access pattern is intrinsically coupled with ongoing computation.
Reference: [51] <editor> Akinori Yonezawa, editor. </editor> <title> ABCL: An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> ISBN 0-262-24029-7. </note>
Reference-contexts: Second, detrimental cache interference between communication and computation routines can be avoided, and finally, more specialized optimizations, such as bulk communication, can be applied separately to the different phases. Our work is related to several dynamic programming language systems <ref> [51, 37, 8] </ref> whose objective is to achieve high performance on distributed memory systems. Implementations of these languages have traditionally focused on communication overhead reductions [48] and thread-oriented optimizations [47, 12, 37], relying on user-specified data distribution [37, 4] to reduce communication volume.
References-found: 51

