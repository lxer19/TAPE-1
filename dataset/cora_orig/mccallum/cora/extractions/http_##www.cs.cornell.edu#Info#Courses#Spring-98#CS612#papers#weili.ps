URL: http://www.cs.cornell.edu/Info/Courses/Spring-98/CS612/papers/weili.ps
Refering-URL: http://www.cs.cornell.edu/Info/Courses/Spring-98/CS612/
Root-URL: 
Email: wei@cs.rochester.edu  
Title: Compiler Cache Optimizations for Banded Matrix Problems  
Author: Wei Li 
Note: To appear in the Proceedings of the 1995 International Conference on Supercomputing.  
Address: Rochester, NY 14627 U.S.A.  
Affiliation: Department of Computer Science University of Rochester  
Abstract: Almost every modern processor is designed with a memory hierarchy organized into several levels, each of which is smaller, faster, and more expensive than the level below. High performance requires the effective use of the cached data, i.e. cache locality. Smart compiler transformations can relieve the programmer from hand-optimizing for the specific machine architectures. Most of the existing compiler optimizations are developed for dense matrix programs. Irregular problems, on the other hand, have to rely on runtime optimizations, since the data access patterns are unknown at the compile-time. However, many scientific computing problems result in solving linear systems where the matrix of coefficients is banded, a structure known at the compile-time, but more complicated than the dense matrices. Banded matrix problems are interesting since substantial savings can be made by exploiting the mathematical properties of the bandedness. The complicated memory access patterns in the banded matrix programs make the existing compile-time optimizations impossible to use. In this paper, we present a new compile-time technique for optimizing banded-matrix programs. We first develop a new data reuse model and an algorithm called height reduction to improve cache locality. Then with the height reduction algorithm, we extend loop tiling to exploit not only intra-tile data locality but also inter-tile data locality. We call the new tiling affinity tiling. We show that the algorithms also helps to eliminate or reduce false sharing in multiprocessor systems. With the height reduction algorithm and affinity tiling, significant performance improvement (speedups from 2.5 to 10) has been observed on HP workstations (over the original sequential code) and KSR1 multiprocessors (over the original parallel code). fl This work was supported in part by an NSF Research Initiation Award (CCR-9409120) and ARPA contract F19628-94-C-0057.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E.L. Allgower. </author> <title> Exact inverse of certain band matrices. </title> <journal> Numer. Math., </journal> <volume> 21 </volume> <pages> 279-84, </pages> <year> 1973. </year>
Reference-contexts: Banded matrix problems are interesting since substantial savings can be made by exploiting the mathematical properties of the bandedness <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. Savings can be made when solving banded triangular systems. Furthermore, many important numerical operations will preserve the bandedness. When solving banded systems, the triangular factor of LU decomposition, Cholesky decomposition and SVD decomposition will all be banded. <p> In this paper, we present a new approach for optimizing banded matrix problems to improve data locality, where the banded matrices are stored with the memory-saving data structures as in 2.3 Related Work There is an immense amount of literature in the scientific computing community on banded matrix computations <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. A considerable amount of research deals with the reduction of the band width, and taking advantage of the mathematical properties of the banded matrices. For example, a nice property is that the triangular factors in LU are also banded when solving banded systems.
Reference: [2] <author> E. Ayguade and J. Torres. </author> <title> Partitioning the statement per it-eration space using non-singular matrices. </title> <booktitle> In Proceedings of The 1993 ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: This transformation theory is based on the use of integer lattices as the model of loop nests and the use of non-singular matrices as the model of loop transformations. The framework has been extended by Ayguade and Torres to allow the transformations of statements in the loop nest <ref> [2] </ref>. Linear loop transformation has been a very active area with a lot of interesting results, e.g. the work by Wolf and Lam [31], and Benerjee [3] on unimodular transformations; and other general frameworks by Sarkar and Thekkath [28], Kelly and Pugh [21], and Kulkarni, Stumm, Unrau and Li [22].
Reference: [3] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of the Workshop on Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 192-219, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: The framework has been extended by Ayguade and Torres to allow the transformations of statements in the loop nest [2]. Linear loop transformation has been a very active area with a lot of interesting results, e.g. the work by Wolf and Lam [31], and Benerjee <ref> [3] </ref> on unimodular transformations; and other general frameworks by Sarkar and Thekkath [28], Kelly and Pugh [21], and Kulkarni, Stumm, Unrau and Li [22]. In this section, we give a brief overview of the framework from [25].
Reference: [4] <author> R. Bianchini and T. J. LeBlanc. </author> <title> Software caching on cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the Fourth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 521-526, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: This problem is called false sharing, which has been identified by many researchers as a major obstacle to high performance <ref> [12, 4, 5, 10] </ref>. Therefore, in a multiprocessor system, we need to exploit cache locality and avoid false sharing. In this paper, we make the following contributions: * We develop a new data reuse model and a compiler algorithm called height reduction to improve cache locality. <p> As a by-product, the technique can also improve cache locality by generating stride one accesses. A recent work by Cierniak and Li [8] unifies both data and control transformations for improving data locality. The work by Eggers and Jeremiassen [12] and by Bianchini and LeBlanc <ref> [4] </ref> shows that for some programs, program restructuring and data restructuring can eliminate or reduce false sharing so that the performance can be improved. However, these transformation techniques are all performed by hand on specific application programs. No systematic compiler transformation techniques are available.
Reference: [5] <author> William J. Bolosky and Michael L. Scott. </author> <title> False sharing and its effect on shared memory performance. </title> <booktitle> In Proceedings of the Fourth Usenix Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pages 57-71, </pages> <address> San Diego, CA, </address> <month> September </month> <year> 1993. </year> <note> Also available as MSR-TR-93-1, </note> <institution> Microsoft Research Laboratory, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: This problem is called false sharing, which has been identified by many researchers as a major obstacle to high performance <ref> [12, 4, 5, 10] </ref>. Therefore, in a multiprocessor system, we need to exploit cache locality and avoid false sharing. In this paper, we make the following contributions: * We develop a new data reuse model and a compiler algorithm called height reduction to improve cache locality. <p> However, these transformation techniques are all performed by hand on specific application programs. No systematic compiler transformation techniques are available. Bolosky and Scott examine several candidate formalizations of false sharing, and provide additional evidence of its importance <ref> [5] </ref>. Dubois et al. describe a run-time mechanism to eliminate misses due to false sharing [10].
Reference: [6] <author> Z. Bolte. </author> <title> Bounds for rounding errors in the Gaussian elimination for band systems. </title> <journal> J. Inst. Math. Applic., </journal> <volume> 16 </volume> <pages> 133-42, </pages> <year> 1975. </year>
Reference-contexts: Banded matrix problems are interesting since substantial savings can be made by exploiting the mathematical properties of the bandedness <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. Savings can be made when solving banded triangular systems. Furthermore, many important numerical operations will preserve the bandedness. When solving banded systems, the triangular factor of LU decomposition, Cholesky decomposition and SVD decomposition will all be banded. <p> In this paper, we present a new approach for optimizing banded matrix problems to improve data locality, where the banded matrices are stored with the memory-saving data structures as in 2.3 Related Work There is an immense amount of literature in the scientific computing community on banded matrix computations <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. A considerable amount of research deals with the reduction of the band width, and taking advantage of the mathematical properties of the banded matrices. For example, a nice property is that the triangular factors in LU are also banded when solving banded systems.
Reference: [7] <author> S. Carr, K. McKinley, and C.-W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 252-262, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The techniques by Ferrante, Sarkar, and Thrash [14] estimate the number of distinct cache lines used by a given loop in a loop nest. Given this estimate, they compute the number of cache misses for a loop nest. Carr, McKinley and Tseng <ref> [7] </ref> developed a simple memory model that can be used for both perfectly and imperfectly nested loops. They used loop permutations, fusion and distribution for transformations. Gannon, Jalby and Gallivan [15] introduced the notion of uniformly generated data reuse, and proposed the notion of windows to capture the data reuse.
Reference: [8] <author> M. Cierniak and W. Li. </author> <title> Unifying data and control transformations for distributed shared-memory machines. </title> <booktitle> In Proceedings of SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <address> La Jolla, California, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Li and Pingali [24] showed that loop transformations can reduce non-local memory accesses on NUMA parallel machines. As a by-product, the technique can also improve cache locality by generating stride one accesses. A recent work by Cierniak and Li <ref> [8] </ref> unifies both data and control transformations for improving data locality. The work by Eggers and Jeremiassen [12] and by Bianchini and LeBlanc [4] shows that for some programs, program restructuring and data restructuring can eliminate or reduce false sharing so that the performance can be improved.
Reference: [9] <author> E. Cuthill. </author> <title> Several strategies for reducing the bandwidth of matrices. In D.J. </title> <editor> Rose and R.A. Willoughby, editors, </editor> <title> Sparse Matrices and Their Applications. </title> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: Banded matrix problems are interesting since substantial savings can be made by exploiting the mathematical properties of the bandedness <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. Savings can be made when solving banded triangular systems. Furthermore, many important numerical operations will preserve the bandedness. When solving banded systems, the triangular factor of LU decomposition, Cholesky decomposition and SVD decomposition will all be banded. <p> In this paper, we present a new approach for optimizing banded matrix problems to improve data locality, where the banded matrices are stored with the memory-saving data structures as in 2.3 Related Work There is an immense amount of literature in the scientific computing community on banded matrix computations <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. A considerable amount of research deals with the reduction of the band width, and taking advantage of the mathematical properties of the banded matrices. For example, a nice property is that the triangular factors in LU are also banded when solving banded systems.
Reference: [10] <author> M. Dubois, J. Skeppstedt, L. Ricciulli, K. Ramamurthy, and P. Stenstrom. </author> <title> The detection and elimination of useless misses in multiprocessors. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 88-97, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This problem is called false sharing, which has been identified by many researchers as a major obstacle to high performance <ref> [12, 4, 5, 10] </ref>. Therefore, in a multiprocessor system, we need to exploit cache locality and avoid false sharing. In this paper, we make the following contributions: * We develop a new data reuse model and a compiler algorithm called height reduction to improve cache locality. <p> No systematic compiler transformation techniques are available. Bolosky and Scott examine several candidate formalizations of false sharing, and provide additional evidence of its importance [5]. Dubois et al. describe a run-time mechanism to eliminate misses due to false sharing <ref> [10] </ref>. The recent work by Granston and Wijshoff proposes using loop blocking and scheduling of a single loop to avoid false sharing [19]. 3 Linear Loop Transformations In this paper, we use loop transformations to optimize the program for data locality.
Reference: [11] <author> I.S. Duff. </author> <title> A survey of sparse matrix research. </title> <booktitle> In Proc. IEEE, </booktitle> <volume> volume 65, </volume> <pages> pages 500-535, </pages> <year> 1977. </year>
Reference-contexts: Banded matrix problems are interesting since substantial savings can be made by exploiting the mathematical properties of the bandedness <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. Savings can be made when solving banded triangular systems. Furthermore, many important numerical operations will preserve the bandedness. When solving banded systems, the triangular factor of LU decomposition, Cholesky decomposition and SVD decomposition will all be banded. <p> In this paper, we present a new approach for optimizing banded matrix problems to improve data locality, where the banded matrices are stored with the memory-saving data structures as in 2.3 Related Work There is an immense amount of literature in the scientific computing community on banded matrix computations <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. A considerable amount of research deals with the reduction of the band width, and taking advantage of the mathematical properties of the banded matrices. For example, a nice property is that the triangular factors in LU are also banded when solving banded systems.
Reference: [12] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In Proc. 1991 International Conference on Parallel Processing, </booktitle> <year> 1991. </year>
Reference-contexts: This problem is called false sharing, which has been identified by many researchers as a major obstacle to high performance <ref> [12, 4, 5, 10] </ref>. Therefore, in a multiprocessor system, we need to exploit cache locality and avoid false sharing. In this paper, we make the following contributions: * We develop a new data reuse model and a compiler algorithm called height reduction to improve cache locality. <p> As a by-product, the technique can also improve cache locality by generating stride one accesses. A recent work by Cierniak and Li [8] unifies both data and control transformations for improving data locality. The work by Eggers and Jeremiassen <ref> [12] </ref> and by Bianchini and LeBlanc [4] shows that for some programs, program restructuring and data restructuring can eliminate or reduce false sharing so that the performance can be improved. However, these transformation techniques are all performed by hand on specific application programs. No systematic compiler transformation techniques are available.
Reference: [13] <author> C. Eisenbeis, W. Jalby, D. Windheiser, and F. Bodin. </author> <title> A strategy for array management in local memory. </title> <booktitle> In Proc. 3th Annual Workshop on Languages and Compilers, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: They used loop permutations, fusion and distribution for transformations. Gannon, Jalby and Gallivan [15] introduced the notion of uniformly generated data reuse, and proposed the notion of windows to capture the data reuse. Eisenbeis, Jalby, Windheiser and Bodin <ref> [13] </ref> used the windows to develop a strategy to explicitly manage the data transfers to local memory.
Reference: [14] <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On estimating and exchange cache effectiveness. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: The subset that can be brought into the innermost position, and has the best objective function from the reuse vector space model is chosen to be tiled. The techniques by Ferrante, Sarkar, and Thrash <ref> [14] </ref> estimate the number of distinct cache lines used by a given loop in a loop nest. Given this estimate, they compute the number of cache misses for a loop nest.
Reference: [15] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: Given this estimate, they compute the number of cache misses for a loop nest. Carr, McKinley and Tseng [7] developed a simple memory model that can be used for both perfectly and imperfectly nested loops. They used loop permutations, fusion and distribution for transformations. Gannon, Jalby and Gallivan <ref> [15] </ref> introduced the notion of uniformly generated data reuse, and proposed the notion of windows to capture the data reuse. Eisenbeis, Jalby, Windheiser and Bodin [13] used the windows to develop a strategy to explicitly manage the data transfers to local memory. <p> For example, the reuse vector above is carried by the second loop. If a reuse vector is carried by the loop i, then the height of the reuse vector is n i + 1 . The data reuse vector can be computed by solving a system of linear equations <ref> [15, 30] </ref>.
Reference: [16] <author> N.E. Gibbs, W.G. Poole, and P.K. Stockmeyer. </author> <title> An algorithm for reducing the bandwidth and profile of a sparse matrix. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 13 </volume> <pages> 236-50, </pages> <year> 1976. </year>
Reference-contexts: Banded matrix problems are interesting since substantial savings can be made by exploiting the mathematical properties of the bandedness <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. Savings can be made when solving banded triangular systems. Furthermore, many important numerical operations will preserve the bandedness. When solving banded systems, the triangular factor of LU decomposition, Cholesky decomposition and SVD decomposition will all be banded. <p> In this paper, we present a new approach for optimizing banded matrix problems to improve data locality, where the banded matrices are stored with the memory-saving data structures as in 2.3 Related Work There is an immense amount of literature in the scientific computing community on banded matrix computations <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. A considerable amount of research deals with the reduction of the band width, and taking advantage of the mathematical properties of the banded matrices. For example, a nice property is that the triangular factors in LU are also banded when solving banded systems.
Reference: [17] <author> N.E. Gibbs, W.G. Poole, and P.K. Stockmeyer. </author> <title> A comparison of several bandwidth and profile reduction algorithms. </title> <journal> In ACM Trans. Math Soft., </journal> <volume> volume 2, </volume> <pages> pages 322-30, </pages> <year> 1976. </year>
Reference-contexts: Banded matrix problems are interesting since substantial savings can be made by exploiting the mathematical properties of the bandedness <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. Savings can be made when solving banded triangular systems. Furthermore, many important numerical operations will preserve the bandedness. When solving banded systems, the triangular factor of LU decomposition, Cholesky decomposition and SVD decomposition will all be banded. <p> In this paper, we present a new approach for optimizing banded matrix problems to improve data locality, where the banded matrices are stored with the memory-saving data structures as in 2.3 Related Work There is an immense amount of literature in the scientific computing community on banded matrix computations <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. A considerable amount of research deals with the reduction of the band width, and taking advantage of the mathematical properties of the banded matrices. For example, a nice property is that the triangular factors in LU are also banded when solving banded systems.
Reference: [18] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: are reused, since the next array element accessed by the program is the array element next to it in the same column and most likely in the same cache line. 2.2 Banded Matrix Computations Many scientific computing problems result in solving linear systems where the matrix of coefficients is banded <ref> [18] </ref>. For example, when every variable x i appears in a few adjacent equations of the ith equation, the coefficient matrix is a matrix, where the non-zeros exist in the neighborhood of the diagonal. <p> The disadvantage is that the program to access such a data structure has more complicated data access patterns that prevent existing compiler optimizations. An example of banded matrix assignment is shown in Figure 1 (d). Following the convention of storing the banded matrix in the scientific computing community <ref> [18] </ref>, we assume that all banded matrices are stored like Figure 1 (c), i.e. only the non-zero elements are stored. This has increased the difficulty of exploiting data locality. In order to exploit cache locality, it is preferable that data accesses are within a small working set.
Reference: [19] <author> E. D. Granston and H. A. G. Wijshoff. </author> <title> Managing pages in shared virtual memory systems: Getting the compiler into the game. </title> <booktitle> In Proceedings of the 1993 International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: Dubois et al. describe a run-time mechanism to eliminate misses due to false sharing [10]. The recent work by Granston and Wijshoff proposes using loop blocking and scheduling of a single loop to avoid false sharing <ref> [19] </ref>. 3 Linear Loop Transformations In this paper, we use loop transformations to optimize the program for data locality. We use the linear transformation framework by Li and Pingali [25] to develop our new algorithms.
Reference: [20] <author> J. Hennessy and D. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: This is called spatial locality. Typically, a cache hit takes only one cycle, while a cache miss takes 8-32 cycles <ref> [20] </ref>. It is ideal for all memory references to hit in the cache, however, since cache size is much smaller than main memory, once new data needs to be brought in from main memory, some data in the cache has to be replaced.
Reference: [21] <author> W. Kelly and W. Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> Technical Report CS-TR-3193, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Linear loop transformation has been a very active area with a lot of interesting results, e.g. the work by Wolf and Lam [31], and Benerjee [3] on unimodular transformations; and other general frameworks by Sarkar and Thekkath [28], Kelly and Pugh <ref> [21] </ref>, and Kulkarni, Stumm, Unrau and Li [22]. In this section, we give a brief overview of the framework from [25].
Reference: [22] <author> D. Kulkarni, M. Stumm, R. Unrau, and W. Li. </author> <title> A generalized theory of loop transformations. </title> <type> Technical Report CSRI-317, </type> <institution> Computer Systems Research Institute, Department of Computer Science, Department of Electrical and Computer Engineering, University of Toronto, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: Linear loop transformation has been a very active area with a lot of interesting results, e.g. the work by Wolf and Lam [31], and Benerjee [3] on unimodular transformations; and other general frameworks by Sarkar and Thekkath [28], Kelly and Pugh [21], and Kulkarni, Stumm, Unrau and Li <ref> [22] </ref>. In this section, we give a brief overview of the framework from [25]. We reduce the problem of transforming a loop nest to transformation of its iteration space, where an iteration space is an integer lattice [29], and every dimension corresponds to one loop in the loop nest.
Reference: [23] <author> W. Li. </author> <title> Compiler optimizations for cache locality and coherence. </title> <type> Technical Report 504, </type> <institution> Department of Computer Science, University of Rochester, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Once a solution, which consists of a set of all valid vectors, for the null space of Q is found, we need to choose a vector that satisfies the data dependences as discussed above. More details can be found in <ref> [23] </ref>. 6 Affinity Tiling The goal of loop tiling [32, 30] is to exploit the data locality within the tile. We propose a new tiling called affinity tiling, which makes use of height reduction, to exploit the data locality not only within a tile but also among tiles.
Reference: [24] <author> W. Li and K. Pingali. </author> <title> Access Normalization: Loop restructuring for NUMA compilers. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4), </volume> <month> November </month> <year> 1993. </year> <title> An earlier version appeared in Proc. </title> <booktitle> 5th International Conference on Architectural Support for Programming Languagesand Operating Systems, </booktitle> <month> October, </month> <year> 1992. </year>
Reference-contexts: Li and Pingali <ref> [24] </ref> showed that loop transformations can reduce non-local memory accesses on NUMA parallel machines. As a by-product, the technique can also improve cache locality by generating stride one accesses. A recent work by Cierniak and Li [8] unifies both data and control transformations for improving data locality.
Reference: [25] <author> W. Li and K. Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(2), </volume> <month> April </month> <year> 1994. </year> <title> An earlier version appeared in the Proc. </title> <booktitle> 5th Annual Workshop on Languages and Compilers for Parallelism, </booktitle> <month> August, </month> <year> 1992. </year>
Reference-contexts: The rest of the paper is organized as follows: In Section 2, we discuss the technical problems we are solving. This paper uses the linear loop transformation framework based on non-singular matrices and integer lattice theory <ref> [25] </ref> to optimize loop nests for better data locality and coherence. We review the loop transformation framework in Section 3. In Section 4, we define a new, more accurate cache reuse model. The optimization algorithm is described in Sections 5, and affinity tiling is presented in Section 6. <p> We use the linear transformation framework by Li and Pingali <ref> [25] </ref> to develop our new algorithms. This framework makes it possible to employ complex program transformations to improve data locality and eliminate false sharing for banded matrix programs. <p> In this section, we give a brief overview of the framework from <ref> [25] </ref>. We reduce the problem of transforming a loop nest to transformation of its iteration space, where an iteration space is an integer lattice [29], and every dimension corresponds to one loop in the loop nest. <p> Some of these problems are non-trivial. More details can be found in <ref> [25] </ref>. To see how this framework is useful for optimizing banded matrix programs, we consider the program in Figure 1 (d) again. The loop nest (j; i) is represented by the iteration space (j; i). Let (u; v) be the iteration space of the transformed loop nest. <p> Let (u; v) be the iteration space of the transformed loop nest. We can use the following non-singular matrix as the transformation matrix. v = 1 1 i From this matrix, we can generate the new loop nest using the algorithms in <ref> [25] </ref>. The new loop nest is shown below. <p> Obviously, the transformation matrix has to be non-singular, and does not violate data dependences <ref> [25] </ref>. In the next section, we propose a cache reuse model, and in Sections 5 and 6 we present the algorithms for constructing transformations to improve data locality. 4 A Cache Reuse Model In this section, we present a new data reuse model. <p> In addition to the non-singularity condition, any row added must not violate data dependences. Any dependence carried by the existing rows can be deleted from the dependence matrix, since any added row will not produce illegal dependences <ref> [25] </ref>. Since v = a 1 x 1 + ::: + a t x t , we need to choose a set of values for fa 1 ; :::; a t g such that v T D 0. <p> The first column is the basis vector of the intersection of all reuses, i.e. the most important reuse dimension. We then call the height reduction algorithm to produce the transformation matrix, shown in Figure 3 (d), from that reuse matrix. Given the transformation matrix, we use the algorithms in <ref> [25] </ref> to generate the new loop nest, shown in Figure 3 (b). The generation of the new loop nest requires handling general loop bounds with max, min, and variables, which is provided by our loop transformation framework. <p> In the final stage, the loop nest is tiled to improve the temporal locality. We would like to emphasize that the multiplications and divisions generated in the transformed code can always be strength-reduced to additions and subtractions <ref> [25] </ref>. In fact, we will present the results with and without strength reduction. The execution times for the different versions on the HP workstation are shown in Figure 3 (e). The problem size n is 500. The time for the original version is 101.3 seconds.
Reference: [26] <author> R.S. Martin and J.H. Wilkinson. </author> <title> Solution of symmetric and unsymmetric band equations and the calculation of eigenval-ues of band matrices. </title> <journal> Numer. Math., </journal> <volume> 9 </volume> <pages> 279-301, </pages> <year> 1967. </year>
Reference-contexts: Banded matrix problems are interesting since substantial savings can be made by exploiting the mathematical properties of the bandedness <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. Savings can be made when solving banded triangular systems. Furthermore, many important numerical operations will preserve the bandedness. When solving banded systems, the triangular factor of LU decomposition, Cholesky decomposition and SVD decomposition will all be banded. <p> In this paper, we present a new approach for optimizing banded matrix problems to improve data locality, where the banded matrices are stored with the memory-saving data structures as in 2.3 Related Work There is an immense amount of literature in the scientific computing community on banded matrix computations <ref> [1, 6, 9, 11, 16, 17, 26] </ref>. A considerable amount of research deals with the reduction of the band width, and taking advantage of the mathematical properties of the banded matrices. For example, a nice property is that the triangular factors in LU are also banded when solving banded systems.
Reference: [27] <author> A. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Gannon, Jalby and Gallivan [15] introduced the notion of uniformly generated data reuse, and proposed the notion of windows to capture the data reuse. Eisenbeis, Jalby, Windheiser and Bodin [13] used the windows to develop a strategy to explicitly manage the data transfers to local memory. Porterfield <ref> [27] </ref> studied the problem of estimating the number of cache lines for uniprocessor for j = 1, n A [i, j] = end (a) Dense Data Structures (b) Banded Matrix Algorithm for j = 1, n A [i, j-i+p+1] = end (c) Banded Data Structures (d) Banded Matrix Algorithm machines, when
Reference: [28] <author> V. Sarkar and R. Thekkath. </author> <title> A general framework for iteration-reordering loop transformations. </title> <booktitle> In SIGPLAN'92 Programming Language and Implementation Conference, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: Linear loop transformation has been a very active area with a lot of interesting results, e.g. the work by Wolf and Lam [31], and Benerjee [3] on unimodular transformations; and other general frameworks by Sarkar and Thekkath <ref> [28] </ref>, Kelly and Pugh [21], and Kulkarni, Stumm, Unrau and Li [22]. In this section, we give a brief overview of the framework from [25].
Reference: [29] <author> A. Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> John Wiley & Sons, </publisher> <year> 1986. </year>
Reference-contexts: In this section, we give a brief overview of the framework from [25]. We reduce the problem of transforming a loop nest to transformation of its iteration space, where an iteration space is an integer lattice <ref> [29] </ref>, and every dimension corresponds to one loop in the loop nest. For example, a loop nest with loop i and loop j (i the outer loop, and j the inner loop) is represented by a two dimensional iteration space.
Reference: [30] <author> M. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proc. ACM SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: For example, a nice property is that the triangular factors in LU are also banded when solving banded systems. Previous work on compile-time algorithms for cache locality has been on dense matrix computations. Wolf and Lam <ref> [30] </ref> focus on loop tiling of the innermost loops as a means of achieving cache locality. They try all possible subset of the loops in the loop nest, and then try to bring that subset into the innermost position. <p> For example, the reuse vector above is carried by the second loop. If a reuse vector is carried by the loop i, then the height of the reuse vector is n i + 1 . The data reuse vector can be computed by solving a system of linear equations <ref> [15, 30] </ref>. <p> In other words, the reuse matrix for the reuse is as follows: R = (~r 1 ; :::; ~r k ; ~r 0 ). Note that all reuses, self, group, temporal and spatial, defined in <ref> [30] </ref> are included in the reuse matrix. In general, there are many individual reuse matrices (for exam ple, every pair of array references that have reuse generate a reuse matrix.). The global reuse matrix is the union of all these individual The outermost loop is loop 1. reuse matrices. <p> Once a solution, which consists of a set of all valid vectors, for the null space of Q is found, we need to choose a vector that satisfies the data dependences as discussed above. More details can be found in [23]. 6 Affinity Tiling The goal of loop tiling <ref> [32, 30] </ref> is to exploit the data locality within the tile. We propose a new tiling called affinity tiling, which makes use of height reduction, to exploit the data locality not only within a tile but also among tiles. <p> Therefore loop tiling of m loops will result in both the tile loop nest (the outer m loops) and the in-tile loop nest (the inner m loops) in the right order to exploit both intra-tile and inter-tile locality. Loop tiling is not always legal <ref> [32, 30] </ref>. If the dependence vector is (1; 1) T , it will prevent tiling. The reader can easily check that tiling would violate data dependences. However, loop transformations may be applied to make a loop nest tilable.
Reference: [31] <author> M. Wolf and M. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> October </month> <year> 1991. </year>
Reference-contexts: The framework has been extended by Ayguade and Torres to allow the transformations of statements in the loop nest [2]. Linear loop transformation has been a very active area with a lot of interesting results, e.g. the work by Wolf and Lam <ref> [31] </ref>, and Benerjee [3] on unimodular transformations; and other general frameworks by Sarkar and Thekkath [28], Kelly and Pugh [21], and Kulkarni, Stumm, Unrau and Li [22]. In this section, we give a brief overview of the framework from [25].
Reference: [32] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman Publishing, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: By the time the program comes back to visit the next element in the same column after a (possible) long row, that cache line may have been evicted due to small cache size. However, a simple loop interchange <ref> [32] </ref> of loops i and j, if legal, will make the loop nest traverse the columns of the array, which matches the physical layout of the array. <p> Once a solution, which consists of a set of all valid vectors, for the null space of Q is found, we need to choose a vector that satisfies the data dependences as discussed above. More details can be found in [23]. 6 Affinity Tiling The goal of loop tiling <ref> [32, 30] </ref> is to exploit the data locality within the tile. We propose a new tiling called affinity tiling, which makes use of height reduction, to exploit the data locality not only within a tile but also among tiles. <p> Therefore loop tiling of m loops will result in both the tile loop nest (the outer m loops) and the in-tile loop nest (the inner m loops) in the right order to exploit both intra-tile and inter-tile locality. Loop tiling is not always legal <ref> [32, 30] </ref>. If the dependence vector is (1; 1) T , it will prevent tiling. The reader can easily check that tiling would violate data dependences. However, loop transformations may be applied to make a loop nest tilable.
References-found: 32

