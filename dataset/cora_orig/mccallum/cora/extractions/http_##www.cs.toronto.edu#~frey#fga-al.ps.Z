URL: http://www.cs.toronto.edu/~frey/fga-al.ps.Z
Refering-URL: http://www.cs.toronto.edu/~frey/fga-al.abs.html
Root-URL: 
Email: frey@cs.utoronto.ca  frank@comm.utoronto.ca  haloeliger@access.ch  nicwi@isy.liu.se  
Title: Factor Graphs and Algorithms  
Author: Brendan J. Frey Frank R. Kschischang Hans-Andrea Loeliger Niclas Wiberg 
Address: 405 North Mathews Avenue Urbana, IL 61801, USA  Toronto, Ontario M5S 3G4, CANADA  Gartenstrae 120 CH-4052 Basel, SWITZERLAND  S-581 83 Linkoping, SWEDEN  
Affiliation: The Beckman Institute  ECE Department University of Toronto  Endora Tech AG  Dept. of Electrical Engineering Linkoping University  
Date: 1997  
Note: 1998. In Proceedings of the 35th Allerton Conference on Communication, Control and Computing  
Abstract: A factor graph is a bipartite graph that expresses how a global function of several variables factors into a product of local functions. Factor graphs subsume many other graphical models, including Bayesian networks, Markov random fields, and Tanner graphs. We describe a general algorithm for computing "marginals" of the global function by distributed message-passing in the corresponding factor graph. A wide variety of algorithms developed in the artificial intelligence, statistics, signal processing, and digital communications communities can be derived as specific instances of this general algorithm, including Pearl's "belief propagation" and "belief revision" algorithms, the fast Fourier transform, the Viterbi algorithm, the forward/backward algorithm, and the iterative "turbo" decoding algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Verdu and H. V. </author> <title> Poor, "Abstract dynamic programming models under commu-tativity conditions," </title> <journal> SIAM J. on Control and Optimization, </journal> <volume> vol. 25, </volume> <pages> pp. 990-1006, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Obviously, this can be extended to an arbitrary ring. Indeed, as pointed out by Verdu and Poor <ref> [1] </ref> and other authors (notably McEliece [2, 4]; see also [3]), the appropriate algebraic structure is that of a semiring, equipped with associative and commutative `+' and `fi' operations, and a distributive law that permits distribution of `fi' over `+'.
Reference: [2] <author> R. J. </author> <title> McEliece, "On the BJCR trellis for linear block codes," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 1072-1092, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: Obviously, this can be extended to an arbitrary ring. Indeed, as pointed out by Verdu and Poor [1] and other authors (notably McEliece <ref> [2, 4] </ref>; see also [3]), the appropriate algebraic structure is that of a semiring, equipped with associative and commutative `+' and `fi' operations, and a distributive law that permits distribution of `fi' over `+'.
Reference: [3] <author> N. Wiberg, </author> <title> Codes and Decoding on General Graphs. </title> <type> PhD thesis, </type> <institution> Linkoping University, Sweden, </institution> <year> 1996. </year>
Reference-contexts: More generally, one could impose more complicated constraints, for example requiring that x E be an element of some more complicated linear code. Such codes were considered by Tanner [5]. Even more generally, following Wiberg, et al. <ref> [3, 6] </ref>, one could introduce (unobserved) state variables, not considered part of the "codeword," but which participate in defining locally valid behavior. These "local check" descriptions are naturally described using a bipartite factor graph (called a Tanner graph in [3, 6] or, to emphasize that unobserved state variables are permitted, a <p> Even more generally, following Wiberg, et al. <ref> [3, 6] </ref>, one could introduce (unobserved) state variables, not considered part of the "codeword," but which participate in defining locally valid behavior. These "local check" descriptions are naturally described using a bipartite factor graph (called a Tanner graph in [3, 6] or, to emphasize that unobserved state variables are permitted, a Tanner-Wiberg-Loeliger graph [7]). 2.2 Indicator Functions and a Posteriori Probabilities Continuing with Example 2, let us select, with uniform probability, a codeword (x 1 ; : : : ; x n ) to transmit over a memoryless channel, and <p> Obviously, this can be extended to an arbitrary ring. Indeed, as pointed out by Verdu and Poor [1] and other authors (notably McEliece [2, 4]; see also <ref> [3] </ref>), the appropriate algebraic structure is that of a semiring, equipped with associative and commutative `+' and `fi' operations, and a distributive law that permits distribution of `fi' over `+'. <p> Examples of factor graphs of this type include the graphs that describe low-density parity-check codes introduced by Gallager [18], and the turbo codes introduced by Berrou, et al. [20]. (See <ref> [3, 23] </ref> for a Tanner graph description of turbo codes, and [12,19] for a Bayesian network description.) Nevertheless, by proceeding as if the graph contained no cycles (i.e., by applying the "carry-one-variable" sum-product algorithm), excellent results may be obtained.
Reference: [4] <author> S. M. Aji and R. J. </author> <title> McEliece, "A general algorithm for distributing information on a graph," </title> <booktitle> in Proc. 1997 IEEE Int. Symp. on Inform. Theory, </booktitle> <address> (Ulm, Germany), p. 6, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Obviously, this can be extended to an arbitrary ring. Indeed, as pointed out by Verdu and Poor [1] and other authors (notably McEliece <ref> [2, 4] </ref>; see also [3]), the appropriate algebraic structure is that of a semiring, equipped with associative and commutative `+' and `fi' operations, and a distributive law that permits distribution of `fi' over `+'. <p> Thus, we recover the well-known trellis-processing algorithms as special cases of the sum-product algorithm. 4.2 The Fast Fourier Transform Following Aji and McEliece <ref> [4] </ref>, who develop a fast Hadamard transform using a graph-based approach, we now develop the fast Fourier transform (FFT) using factor graphs.
Reference: [5] <author> R. M. Tanner, </author> <title> "A recursive approach to low complexity codes," </title> <journal> IEEE Trans. on Inform. Theory, </journal> <volume> vol. IT-27, </volume> <pages> pp. 533-547, </pages> <month> Sept. </month> <year> 1981. </year>
Reference-contexts: More generally, one could impose more complicated constraints, for example requiring that x E be an element of some more complicated linear code. Such codes were considered by Tanner <ref> [5] </ref>. Even more generally, following Wiberg, et al. [3, 6], one could introduce (unobserved) state variables, not considered part of the "codeword," but which participate in defining locally valid behavior.
Reference: [6] <author> N. Wiberg, H.-A. Loeliger, and R. Kotter, </author> <title> "Codes and iterative decoding on general graphs," </title> <journal> European Trans. on Telecommun., </journal> <volume> vol. 6, </volume> <pages> pp. 513-525, </pages> <address> Sep./Oct. </address> <year> 1995. 1998. </year> <booktitle> In Proceedings of the 35th Allerton Conference on Communication, Control and Computing 1997 </booktitle>
Reference-contexts: More generally, one could impose more complicated constraints, for example requiring that x E be an element of some more complicated linear code. Such codes were considered by Tanner [5]. Even more generally, following Wiberg, et al. <ref> [3, 6] </ref>, one could introduce (unobserved) state variables, not considered part of the "codeword," but which participate in defining locally valid behavior. These "local check" descriptions are naturally described using a bipartite factor graph (called a Tanner graph in [3, 6] or, to emphasize that unobserved state variables are permitted, a <p> Even more generally, following Wiberg, et al. <ref> [3, 6] </ref>, one could introduce (unobserved) state variables, not considered part of the "codeword," but which participate in defining locally valid behavior. These "local check" descriptions are naturally described using a bipartite factor graph (called a Tanner graph in [3, 6] or, to emphasize that unobserved state variables are permitted, a Tanner-Wiberg-Loeliger graph [7]). 2.2 Indicator Functions and a Posteriori Probabilities Continuing with Example 2, let us select, with uniform probability, a codeword (x 1 ; : : : ; x n ) to transmit over a memoryless channel, and <p> The "min-sum" formulation is also the framework in which so-called "nonserial" dynamic programming is posed; see [21] for a text book treatment. See <ref> [6] </ref> for a discussion of the min-sum algorithm. 4 Examples We now give a few examples of how the sum-product algorithm may be applied in practice. 1998.
Reference: [7] <author> G. D. Forney, Jr., </author> <title> "The forward-backward algorithm," </title> <booktitle> in Proc. 34th Annual Allerton Conf. on Communication, Control, and Computing, (Allerton House, </booktitle> <address> Monticello, </address> <publisher> Illinois), </publisher> <pages> pp. 432-446, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: These "local check" descriptions are naturally described using a bipartite factor graph (called a Tanner graph in [3, 6] or, to emphasize that unobserved state variables are permitted, a Tanner-Wiberg-Loeliger graph <ref> [7] </ref>). 2.2 Indicator Functions and a Posteriori Probabilities Continuing with Example 2, let us select, with uniform probability, a codeword (x 1 ; : : : ; x n ) to transmit over a memoryless channel, and suppose that y = (y 1 ; : : : ; y n )
Reference: [8] <author> R. Kindermann and J. L. Snell, </author> <title> Markov Random Fields and their Applications. </title> <address> Providence, Rhode Island: </address> <publisher> American Mathematical Society, </publisher> <year> 1980. </year>
Reference-contexts: "singleton" local function nodes can be absorbed into the corresponding variable node, so that effectively, the factor graph describing the a posteriori joint probability distribution, given the observed channel output, is equivalent to the factor graph of the code itself. 2.3 Markov Random Fields A Markov random field (see, e.g., <ref> [8] </ref>) is a graphical model based on an undirected graph G = (V; E) in which each vertex corresponds to a random variable. Denote by n (v) the neighbors of v 2 V , i.e., the set of vertices of V connected to v by a single edge of E.
Reference: [9] <author> C. J. Preston, </author> <title> Gibbs States on Countable Sets. </title> <publisher> Cambridge University Press, </publisher> <year> 1974. </year>
Reference: [10] <author> V. Isham, </author> <title> "An introduction to spatial point processes and Markov random fields," </title> <journal> Int. Stat. Rev., </journal> <volume> vol. 49, </volume> <pages> pp. 21-43, </pages> <year> 1981. </year>
Reference: [11] <author> G. E. Hinton and T. J. Sejnowski, </author> <title> "Learning and relearning in Boltzmann machines," in Parallel Distributed Processing: Explorations in the Microstructure of Cognition (D. </title> <editor> E. Rumelhart and J. L. McClelland, eds.), </editor> <volume> vol. I, </volume> <pages> pp. 282-317, </pages> <address> Cambridge MA.: </address> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference: [12] <author> F. R. Kschischang and B. J. Frey, </author> <title> "Iterative decoding of compound codes by probability propagation in graphical models," </title> <journal> IEEE J. Selected Areas in Commun., </journal> <volume> vol. 16, </volume> <month> Jan. </month> <year> 1998. </year>
Reference-contexts: In other words, G is an MRF if every variable v is independent of non-neighboring variables in the graph, given the values of its immediate neighbors. MRFs are well developed in statistics, and have been used in a variety of applications (see, e.g., [8-11]). See <ref> [12] </ref> for a brief discussion of the use of MRFs to describe codes. <p> may be preferable to an MRF in expressing such a factorization, since distinct factorizations, i.e., factorizations with different Qs in (3), may yield precisely the same underlying MRF graph, whereas they will always yield distinct factor graphs. (An example in a coding context of this MRF ambiguity is given in <ref> [12] </ref>.) 1998. In Proceedings of the 35th Allerton Conference on Communication, Control and Computing 1997 2.4 Bayesian Networks Bayesian networks (see, e.g., [13-16]) are graphical models for a collection of random variables that are based on directed acyclic graphs (DAGs). <p> The first to connect Bayesian networks and belief propagation with applications in coding theory were MacKay and Neal [17], who independently re-discovered Gallager's earlier work on low-density parity-check codes [18] (including Gallager's decoding algorithm.) More recently, at least two papers <ref> [12, 19] </ref> develop a view of the "turbo decoding" algorithm [20] as an instance of probability propagation in a Bayesian network code model. Each vertex v in a Bayesian network is associated with a random variable. <p> In some implementations, it may be possible that message passing can occur concurrently, while in other implementations, the messages will of necessity be passed serially. In general a variety of message passing schedules are possible. In <ref> [12] </ref>, two such schedules|the two-way schedule, and the flooding schedule| are described. <p> We refer the reader to <ref> [12] </ref> for a description of these message-passing schedules. 3.5 Generalization to other Semirings Up to now, we have assumed that all products and sums are computed in the field of real numbers. Obviously, this can be extended to an arbitrary ring. <p> It is easy to show that the sum-product algorithm, when applied to cycle-free Bayesian networks, yields Pearl's belief propagation algorithm [13]. See <ref> [12] </ref> for a discussion of this correspondence. 5 Approximate "Marginalization" in Graphs with Cycles In a great many applications, one is forced to consider factor graphs that contain cycles.
Reference: [13] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year> <note> Revised second printing. </note>
Reference-contexts: In Proceedings of the 35th Allerton Conference on Communication, Control and Computing 1997 2.4 Bayesian Networks Bayesian networks (see, e.g., [13-16]) are graphical models for a collection of random variables that are based on directed acyclic graphs (DAGs). Bayesian networks, combined with Pearl's "belief propagation algorithm" <ref> [13] </ref> have become an important tool in expert systems over the past decade. <p> This allows the flow of "causality" to be inferred equally well from the factor graph as from the Bayesian network itself. Representing such causality using directed edges is often cited as one of the basic advantages of modeling a probability distribution using a Bayesian network <ref> [13] </ref>; by including arrows in the corresponding factor graph, this advantage is retained. It is easy to show that the sum-product algorithm, when applied to cycle-free Bayesian networks, yields Pearl's belief propagation algorithm [13]. <p> often cited as one of the basic advantages of modeling a probability distribution using a Bayesian network <ref> [13] </ref>; by including arrows in the corresponding factor graph, this advantage is retained. It is easy to show that the sum-product algorithm, when applied to cycle-free Bayesian networks, yields Pearl's belief propagation algorithm [13]. See [12] for a discussion of this correspondence. 5 Approximate "Marginalization" in Graphs with Cycles In a great many applications, one is forced to consider factor graphs that contain cycles.
Reference: [14] <author> F. V. Jensen, </author> <title> An Introduction to Bayesian Networks. </title> <address> New York: </address> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference: [15] <author> R. E. </author> <title> Neapolitan, Probabilistic Reasoning in Expert Systems: Theory and Algorithms. </title> <publisher> Toronto: John Wiley & Sons, </publisher> <year> 1990. </year>
Reference: [16] <author> B. J. Frey, </author> <title> Bayesian Networks for Pattern Classification, Data Compression and Channel Coding. </title> <institution> Toronto, Canada: Department of Electrical and Computer Engineering, University of Toronto, </institution> <year> 1997. </year> <note> Doctoral dissertation available at http://www.cs.utoronto.ca/~frey. </note>
Reference: [17] <author> D. J. C. MacKay and R. M. Neal, </author> <title> "Good codes based on very sparse matrices," in Cryptography and Coding. </title> <booktitle> 5th IMA Conference (C. </booktitle> <editor> Boyd, ed.), </editor> <volume> no. </volume> <booktitle> 1025 in Lecture Notes in Computer Science, </booktitle> <pages> pp. 100-111, </pages> <address> Berlin Germany: </address> <publisher> Springer, </publisher> <year> 1995. </year>
Reference-contexts: Bayesian networks, combined with Pearl's "belief propagation algorithm" [13] have become an important tool in expert systems over the past decade. The first to connect Bayesian networks and belief propagation with applications in coding theory were MacKay and Neal <ref> [17] </ref>, who independently re-discovered Gallager's earlier work on low-density parity-check codes [18] (including Gallager's decoding algorithm.) More recently, at least two papers [12, 19] develop a view of the "turbo decoding" algorithm [20] as an instance of probability propagation in a Bayesian network code model. <p> For example, simulation results of MacKay and Neal <ref> [17] </ref> indicate that low-density parity-check codes can be decoded (with reasonable complexity) using this approach so that the E b =N 0 required to achieve a bit error rate of 10 5 is within approximately 2dB of the Shannon limit [17] (using antipodal signaling on a white Gaussian noise channel). <p> For example, simulation results of MacKay and Neal <ref> [17] </ref> indicate that low-density parity-check codes can be decoded (with reasonable complexity) using this approach so that the E b =N 0 required to achieve a bit error rate of 10 5 is within approximately 2dB of the Shannon limit [17] (using antipodal signaling on a white Gaussian noise channel).
Reference: [18] <author> R. G. Gallager, </author> <title> Low-Density Parity-Check Codes. </title> <address> Cambridge, MA: </address> <publisher> M.I.T. Press, </publisher> <year> 1963. </year>
Reference-contexts: The first to connect Bayesian networks and belief propagation with applications in coding theory were MacKay and Neal [17], who independently re-discovered Gallager's earlier work on low-density parity-check codes <ref> [18] </ref> (including Gallager's decoding algorithm.) More recently, at least two papers [12, 19] develop a view of the "turbo decoding" algorithm [20] as an instance of probability propagation in a Bayesian network code model. Each vertex v in a Bayesian network is associated with a random variable. <p> In many such cases, exact marginalization in a spanning tree proves to be computationally infeasible due to the excessive "thickness" of some of the edges. Examples of factor graphs of this type include the graphs that describe low-density parity-check codes introduced by Gallager <ref> [18] </ref>, and the turbo codes introduced by Berrou, et al. [20]. (See [3, 23] for a Tanner graph description of turbo codes, and [12,19] for a Bayesian network description.) Nevertheless, by proceeding as if the graph contained no cycles (i.e., by applying the "carry-one-variable" sum-product algorithm), excellent results may be obtained.
Reference: [19] <author> R. J. McEliece, D. J. C. MacKay, and J.-F. Cheng, </author> <title> "Turbo decoding as an instance of Pearl's `belief propagation' algorithm," </title> <journal> IEEE J. on Selected Areas in Commun., </journal> <volume> vol. 16, </volume> <month> Jan. </month> <year> 1998. </year>
Reference-contexts: The first to connect Bayesian networks and belief propagation with applications in coding theory were MacKay and Neal [17], who independently re-discovered Gallager's earlier work on low-density parity-check codes [18] (including Gallager's decoding algorithm.) More recently, at least two papers <ref> [12, 19] </ref> develop a view of the "turbo decoding" algorithm [20] as an instance of probability propagation in a Bayesian network code model. Each vertex v in a Bayesian network is associated with a random variable.
Reference: [20] <author> C. Berrou, A. Glavieux, and P. Thitimajshima, </author> <title> "Near Shannon limit error-correcting coding and decoding: Turbo codes," </title> <booktitle> in Proc. IEEE Int. Conf. Commun. </booktitle> <address> (ICC), (Geneva, Switzerland), </address> <pages> pp. 1064-1070, </pages> <year> 1993. </year>
Reference-contexts: The first to connect Bayesian networks and belief propagation with applications in coding theory were MacKay and Neal [17], who independently re-discovered Gallager's earlier work on low-density parity-check codes [18] (including Gallager's decoding algorithm.) More recently, at least two papers [12, 19] develop a view of the "turbo decoding" algorithm <ref> [20] </ref> as an instance of probability propagation in a Bayesian network code model. Each vertex v in a Bayesian network is associated with a random variable. <p> Examples of factor graphs of this type include the graphs that describe low-density parity-check codes introduced by Gallager [18], and the turbo codes introduced by Berrou, et al. <ref> [20] </ref>. (See [3, 23] for a Tanner graph description of turbo codes, and [12,19] for a Bayesian network description.) Nevertheless, by proceeding as if the graph contained no cycles (i.e., by applying the "carry-one-variable" sum-product algorithm), excellent results may be obtained.
Reference: [21] <author> U. Bertele and F. Brioschi, </author> <title> Nonserial Dynamic Programming. </title> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: The minimum cost valid configuration is the "maximum-likelihood" sequence, the cost of which is computed (in a factor graph containing no cycles) by the "min-sum" generalization of the "sum-product" algorithm. The "min-sum" formulation is also the framework in which so-called "nonserial" dynamic programming is posed; see <ref> [21] </ref> for a text book treatment. See [6] for a discussion of the min-sum algorithm. 4 Examples We now give a few examples of how the sum-product algorithm may be applied in practice. 1998.
Reference: [22] <author> L. R. Bahl, J. Cocke, F. Jelinek, and J. Raviv, </author> <title> "Optimal decoding of linear codes for minimizing symbol error rate," </title> <journal> IEEE Trans. on Inform. Theory, </journal> <volume> vol. 20, </volume> <pages> pp. 284-287, </pages> <month> Mar. </month> <year> 1974. </year>
Reference-contexts: Then, the sum-product algorithm will compute the message ff i (s i ) = x i s i1 and send this to variable node s i . This is precisely the "forward" step in the forward/backward algorithm described in <ref> [22] </ref>. The "backward" (fi) step is described in like manner. In this way, the sum-product algorithm specializes, on a trellis, to the well known forward/backward algorithm.
Reference: [23] <author> N. Wiberg, H.-A. Loeliger, and R. Kotter, </author> <title> "Codes and iterative decoding on general graphs," </title> <journal> European Transactions on Telecommunications, </journal> <volume> vol. 6, </volume> <pages> pp. 513-525, </pages> <year> 1995. </year>
Reference-contexts: Examples of factor graphs of this type include the graphs that describe low-density parity-check codes introduced by Gallager [18], and the turbo codes introduced by Berrou, et al. [20]. (See <ref> [3, 23] </ref> for a Tanner graph description of turbo codes, and [12,19] for a Bayesian network description.) Nevertheless, by proceeding as if the graph contained no cycles (i.e., by applying the "carry-one-variable" sum-product algorithm), excellent results may be obtained.
References-found: 23

