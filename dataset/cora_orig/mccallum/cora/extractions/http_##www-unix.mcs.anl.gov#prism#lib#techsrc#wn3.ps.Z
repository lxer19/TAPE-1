URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn3.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Email: bischof@mcs.anl.gov  xiaobai@mcs.anl.gov  
Title: A Framework for Symmetric Band Reduction and Tridiagonalization 1  
Author: Christian H. Bischof Xiaobai Sun 
Note: Preprint MCS-P298-0392  
Address: Argonne, IL 60439-4801  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: This paper develops a framework for bandwidth reduction and tridiagonalization algorithms for symmetric banded matrices. The algorithm family includes the algorithms by Rutishauser and Schwarz, which underly the EISPACK and LAPACK implementations, and the algorithm recently proposed by Lang. Our framework leads to algorithms that require fewer floating-point operations, allow for space-time tradeoffs, enable the use of block orthogonal transformations, and increase the degree of parallelism inherent in the algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred Aho, John Hopcroft, and Jeffrey Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1974. </year>
Reference-contexts: Given a limit on sb, we can use dynamic programming <ref> [1] </ref> to determine an optimal sequence fd i g from the cost function given in (2.1).
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Ham-marling, A. McKenney, and D. Sorensen. </author> <title> LAPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1992. </year> <note> to appear. </note>
Reference-contexts: The R-S algorithm can be vectorized (along the diagonal) [12] and this variant is the basis of the band reduction algorithms in EISPACK [18, 10] and LAPACK <ref> [2] </ref>. The R-S algorithm requires storage for one extra subdiagonal, and Rutishauser's Householder approach requires storage for b 1 extra subdiagonals. To assess the storage requirements of various algorithms, we introduce the concept of working semibandwidth.
Reference: [3] <author> Christian H. Bischof. </author> <title> A Pipelined QR Factorization algorithm with Adaptive Blocking, </title> <address> pages 10-20. </address> <publisher> Ellis Horwood Publishers, </publisher> <address> Chichester, U.K., </address> <year> 1989. </year>
Reference-contexts: transformations are applied only to a diagonal subblock (as compared to whole rows or columns in the usual dense schemes), we expect a block algorithm to be superior only for matrices of relatively large bandwidth, and on parallel machines, where block transformations also result in a reduction of message transfer <ref> [3, 8] </ref>. 11 x x x x x x x 0 0 0 b d+1 6 Experimental Results While the constant- and doubling-stride algorithms are superior from a complexity point of view, they lead to algorithms with shorter vector lengths, and they require more passes over the data than Lang's algorithm.
Reference: [4] <author> Christian H. Bischof and Xiaobai Sun. </author> <title> A divide-and-conquer method for computing complementary invariant subspaces of symmetric matrices. </title> <type> Technical Report MCS-P286-0192, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: 1 Introduction Reduction to tridiagonal form is a major step in eigenvalue computations for symmetric matrices. If the matrix is full, the conventional Householder tridiagonalization approach <ref> [11, 4, p. 276] </ref> or block variants thereof [9] is the method of choice. However, for banded matrices with semibandwidth b, where b o n, this approach is not optimal since the matrix being reduced has completely filled in after log 2 (n=(b 1)) reduction steps.
Reference: [5] <author> Christian H. Bischof and Charles F. Van Loan. </author> <title> The WY representation for products of Householder matrices. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 8:s2-s13, </volume> <year> 1987. </year>
Reference-contexts: However, in doing so, we have enabled the use of block transformations. For example, we can use the "WY" <ref> [5] </ref> or "compact WY" [16] representation for the product of Householder matrices to express the Householder updates in closed form and then exploit the speed of matrix-matrix multiply in the application of those transformations.
Reference: [6] <author> H. Chang, S. Utku, M Salama, and D. Rapp. </author> <title> A parallel Householder tridiagonalization stratagem using scattered square decomposition. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 297-311, </pages> <year> 1988. </year>
Reference-contexts: We mention in particular that this extra degree of parallelism could be favorably exploited even in a tridiagonalization algorithm for dense matrices, where current approaches <ref> [6, 8] </ref> are limited to exploiting parallelism through pipelining of different reduction steps, or inside the application of a Householder or block Householder reduction.
Reference: [7] <author> Jack Dongarra and Eric Grosse. </author> <title> Distribution of mathematical software by electronic mail. </title> <journal> Communications of the ACM, </journal> <volume> 30(5) </volume> <pages> 403-407, </pages> <year> 1987. </year>
Reference-contexts: On the Sparcstation, we used the Fortran BLAS from netlib <ref> [7] </ref>, on the IBM, we used the vendor-supplied assembler routines. All computations were performed in single precision. Figures 10 and 11 show the relative behavior of the new algorithms with respect to Lang's algorithm as predicted by the complexity analysis and as actually observed.
Reference: [8] <author> Jack Dongarra and Robert van de Geijn. </author> <title> Reduction to condensed form on distributed memory architectures. </title> <type> Technical Report CS-91-130, </type> <institution> Computer Science Department, The University of Tennessee, </institution> <year> 1991. </year>
Reference-contexts: We mention in particular that this extra degree of parallelism could be favorably exploited even in a tridiagonalization algorithm for dense matrices, where current approaches <ref> [6, 8] </ref> are limited to exploiting parallelism through pipelining of different reduction steps, or inside the application of a Householder or block Householder reduction. <p> transformations are applied only to a diagonal subblock (as compared to whole rows or columns in the usual dense schemes), we expect a block algorithm to be superior only for matrices of relatively large bandwidth, and on parallel machines, where block transformations also result in a reduction of message transfer <ref> [3, 8] </ref>. 11 x x x x x x x 0 0 0 b d+1 6 Experimental Results While the constant- and doubling-stride algorithms are superior from a complexity point of view, they lead to algorithms with shorter vector lengths, and they require more passes over the data than Lang's algorithm.
Reference: [9] <author> Jack J. Dongarra, Sven J. Hammarling, and Danny C. Sorensen. </author> <title> Block reduction of matrices to condensed form for eigenvalue computations. </title> <type> Technical Report MCS-TM-99, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> September </month> <year> 1987. </year> <month> 15 </month>
Reference-contexts: 1 Introduction Reduction to tridiagonal form is a major step in eigenvalue computations for symmetric matrices. If the matrix is full, the conventional Householder tridiagonalization approach [11, 4, p. 276] or block variants thereof <ref> [9] </ref> is the method of choice. However, for banded matrices with semibandwidth b, where b o n, this approach is not optimal since the matrix being reduced has completely filled in after log 2 (n=(b 1)) reduction steps.
Reference: [10] <author> B. Garbow, J. Boyle, J. Dongarra, and C. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide Extension, </title> <booktitle> volume 51 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: The R-S algorithm can be vectorized (along the diagonal) [12] and this variant is the basis of the band reduction algorithms in EISPACK <ref> [18, 10] </ref> and LAPACK [2]. The R-S algorithm requires storage for one extra subdiagonal, and Rutishauser's Householder approach requires storage for b 1 extra subdiagonals. To assess the storage requirements of various algorithms, we introduce the concept of working semibandwidth.
Reference: [11] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1983. </year>
Reference-contexts: 1 Introduction Reduction to tridiagonal form is a major step in eigenvalue computations for symmetric matrices. If the matrix is full, the conventional Householder tridiagonalization approach <ref> [11, 4, p. 276] </ref> or block variants thereof [9] is the method of choice. However, for banded matrices with semibandwidth b, where b o n, this approach is not optimal since the matrix being reduced has completely filled in after log 2 (n=(b 1)) reduction steps.
Reference: [12] <author> Linda Kaufman. </author> <title> Banded eigenvalue solvers on vector machines. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 10(1) </volume> <pages> 73-86, </pages> <month> March </month> <year> 1984. </year>
Reference-contexts: Department of Energy, under Contract W-31-109-Eng-38. 1 x x H the significant work involved in chasing the triangular bulges, this algorithm does not perform better than the Givens-rotations based R-S algorithm. The R-S algorithm can be vectorized (along the diagonal) <ref> [12] </ref> and this variant is the basis of the band reduction algorithms in EISPACK [18, 10] and LAPACK [2]. The R-S algorithm requires storage for one extra subdiagonal, and Rutishauser's Householder approach requires storage for b 1 extra subdiagonals.
Reference: [13] <author> Bruno Lang. Parallele Reduktion symmetrischer Bandmatrizen auf Tridiagonalgestalt. </author> <type> PhD thesis, </type> <institution> Universitat Karlsruhe (TH), </institution> <year> 1991. </year>
Reference-contexts: In both algorithmic approaches, each reduction step has two parts: * Band reduction (either from b to b 1, or b to 1), and * Bulge Chasing to maintain banded form. Either way, the bulk of the computation is spent in bulge chasing. Lang's algorithm <ref> [13, 14] </ref> improves on the bulge chasing strategy. It employs Householder transformations to eliminate all b 1 subdiagonal entries in the current column, but instead of chasing out the whole triangular bulge (only to have it reappear the next step) Lang's algorithm chases only the first column of the bulges.
Reference: [14] <author> Bruno Lang. </author> <title> Reducing symmetric banded matrices to tridiagonal form a comparison of a new parallel algorithm with two serial algorithms on the iPSC/860. </title> <type> Technical report, </type> <institution> Universitat Karlsruhe, Institut fur Angewandte Mathematik, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: In both algorithmic approaches, each reduction step has two parts: * Band reduction (either from b to b 1, or b to 1), and * Bulge Chasing to maintain banded form. Either way, the bulk of the computation is spent in bulge chasing. Lang's algorithm <ref> [13, 14] </ref> improves on the bulge chasing strategy. It employs Householder transformations to eliminate all b 1 subdiagonal entries in the current column, but instead of chasing out the whole triangular bulge (only to have it reappear the next step) Lang's algorithm chases only the first column of the bulges. <p> By using Householder reductions, and an improved bulge chasing strategy (compared to Rutishauser's algorithm) Lang arrived at an algorithm that increases computational granularity to allow for the use of BLAS-2 kernels, while maintaining a degree of parallelism sufficient for a moderate number p of processors <ref> [14] </ref>. Our approach increases both the inherent parallelism in the algorithm and enables the use of BLAS 3 kernels through the use of block orthogonal transformations.
Reference: [15] <author> H. </author> <title> Rutishauser. On Jacobi rotation patterns. </title> <booktitle> In Proc. of Symposia in Applied Mathematics, </booktitle> <volume> Vol. 15, </volume> <booktitle> Experimental Arithmetic, High Speed Computing and Mathematics, </booktitle> <pages> pages 219-239, </pages> <year> 1963. </year>
Reference-contexts: However, for banded matrices with semibandwidth b, where b o n, this approach is not optimal since the matrix being reduced has completely filled in after log 2 (n=(b 1)) reduction steps. It is well known that the algorithm of Rutishauser <ref> [15] </ref> and Schwarz [17] (called the R-S algorithm in the rest of the paper) is more economical than the standard approach when b is small compared to n. In the R-S algorithm, elements in the current column to be reduced are annihilated one at a time by Givens rotations. <p> In the R-S algorithm, elements in the current column to be reduced are annihilated one at a time by Givens rotations. Each Givens rotation generates a fill-in element outside of the original band, and the fill-in is chased out by a sequence of Givens rotations. In the same paper <ref> [15] </ref>, Rutishauser also suggested a band reduction scheme based on Householder transformations that annihilates all b 1 elements in the current column instead of only one, Rutishauser used an analogous chasing scheme to drive out the triangular bulge generated by the reduction with a sequence of Householder transformations, as shown in
Reference: [16] <author> Robert Schreiber and Charles Van Loan. </author> <title> A storage efficient WY representation for products of Householder transformations. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 10(1) </volume> <pages> 53-57, </pages> <year> 1989. </year>
Reference-contexts: However, in doing so, we have enabled the use of block transformations. For example, we can use the "WY" [5] or "compact WY" <ref> [16] </ref> representation for the product of Householder matrices to express the Householder updates in closed form and then exploit the speed of matrix-matrix multiply in the application of those transformations.
Reference: [17] <author> Hans Rudolph Schwarz. </author> <title> Tridiagonalization of a symmetric band matrix. </title> <journal> Numerische Mathe-matik, </journal> <volume> 12 </volume> <pages> 231-241, </pages> <year> 1968. </year>
Reference-contexts: However, for banded matrices with semibandwidth b, where b o n, this approach is not optimal since the matrix being reduced has completely filled in after log 2 (n=(b 1)) reduction steps. It is well known that the algorithm of Rutishauser [15] and Schwarz <ref> [17] </ref> (called the R-S algorithm in the rest of the paper) is more economical than the standard approach when b is small compared to n. In the R-S algorithm, elements in the current column to be reduced are annihilated one at a time by Givens rotations.
Reference: [18] <author> B. Smith, J. Boyle, J. Dongarra, B. Garbow, Y. Ikebe, V. Klema, and C. B. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <note> second edition, 1976. 16 </note>
Reference-contexts: The R-S algorithm can be vectorized (along the diagonal) [12] and this variant is the basis of the band reduction algorithms in EISPACK <ref> [18, 10] </ref> and LAPACK [2]. The R-S algorithm requires storage for one extra subdiagonal, and Rutishauser's Householder approach requires storage for b 1 extra subdiagonals. To assess the storage requirements of various algorithms, we introduce the concept of working semibandwidth.
References-found: 18

