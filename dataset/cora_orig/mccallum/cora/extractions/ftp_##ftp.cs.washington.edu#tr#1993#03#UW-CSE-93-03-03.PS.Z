URL: ftp://ftp.cs.washington.edu/tr/1993/03/UW-CSE-93-03-03.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Latency Analysis of TCP on an ATM Network  
Author: Alec Wolman, Geoff Voelker, and Chandramohan A. Thekkath 
Date: 93-03-03  
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Pubnum: Technical Report  
Abstract: In this paper, we characterize the latency of TCP on an ATM network. Latency reduction is a difficult task, and careful analysis is the first step towards reduction. We investigate the impact of both the network controller and the protocol implementation on latency. We find that a low latency network controller has a significant impact on the overall latency even for a reliable transport protocol such as TCP, and that replacing the ULTRIX TCP implementation with the BSD 4.4 alpha implementation improves the latency up to 20%. We also characterize the impact on latency of some widely discussed improvements to TCP, such as header prediction and combining the checksum calculation with data copying. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> John B. Carter and Willy Zwaenopoel. </author> <title> "Optimistic Implementation of Bulk Data Transfer." </title> <booktitle> In Proceedings of the 1989 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1989. </year> <month> 14 </month>
Reference-contexts: The second technique involves exploiting traffic locality to predict the next incoming packet and to avoid the protocol control block (PCB) lookup cost. Others have studied using traffic locality to improve throughput for bulk data transfer protocols <ref> [1, 13] </ref>. In the BSD implementation, the TCP input processing keeps a single entry cache of the most recently used PCB. If the incoming packet is from the same connection as the previous packet, the call to the PCB lookup routine is avoided.
Reference: [2] <author> David D. Clark, Van Jacobson, John Romkey, and Howard Salwen. </author> <title> "An Analysis of TCP Pro--cessing Overhead." </title> <journal> IEEE Communications Magazine (June 1989), </journal> <pages> 23-39. </pages>
Reference-contexts: Second, our study allows us to answer the following questions: Can we provide evidence that TCP is a viable option for a transport layer for RPC? How have the changes in technology affected the results of earlier studies (e.g., <ref> [2] </ref>)? Is latency dominated by the cost of operating system services, such as buffer management? If so, can the use of such services be reduced enough to make latency acceptable for applications that require low latency? 1.1 System Overview Before we describe our experiments, we briefly describe the software and hardware <p> Sections 3 and 4 study the effect of several modifications that we felt were important based on the results in Section 2. Many of these modifications are not new and have been suggested by others as well in the literature <ref> [2, 6] </ref>, however, our focus here is on the effect of these modifications on latency rather than throughput. 2 Measurement of the Baseline System The baseline system that we are concerned with is the 4.4 BSD alpha release running on an ATM network. <p> Compared to the round trip time, this is not a significant portion of the round trip time, which is in disagreement with <ref> [2] </ref>. Latency characteristics change in a number of ways as the data transfer size grows above 1K. <p> Eliminating the checksum (discussed below) opens the possibility of alleviating these data copying costs, given a network adapter that supports DMA. With a combined copy and checksum, Clark and Jacobson <ref> [2] </ref> discuss a network adapter design that eliminates the need for a second copy. 2.2.5 Scheduling The scheduling times for switching contexts are independent of data transfer size, both in scheduling the software interrupt for IP queue processing and in scheduling the user process to return the received data from the <p> the issue of the data copy, we address the problem of reducing the remaining protocol processing time using header prediction in the next section and the problem of optimizing the checksum in a subsequent section. 3 Header Prediction Header prediction has often been suggested as a performance benefit for TCP <ref> [2] </ref>. There are two distinct kinds of optimizations that are often called header prediction. The first, involving prefilling parts of the transport header, is a known optimization for lowering latency [11, 8], and is not discussed further here. <p> 151 56 4000 807 350 1157 430 Table 7: Checksum and Copy Measurements. a only a small improvement in latency, and that the current implementation of header precomputation does not improve latency in a bidirectional RPC style of communication. 4 TCP Checksums 4.1 Optimizing the Checksum An optimization suggested in <ref> [2] </ref> is to combine the checksum calculation with one of the data copies. In ULTRIX, data is copied at least twice on both send and receive. One copy moves the data between user and kernel space. The other copy moves the data between kernel and device memory. <p> For comparison, on a Sun-3 (20MHz 68020) for 1KB of data, Van Jacobson reported 130 s for the checksum, and 140 s for the memory to memory copy <ref> [2] </ref>. The combined cost was 200 s. On the DECstation 5000/200, using the standard ULTRIX kernel routines to do the checksum takes 207 s, and the copy takes 91 s. The combined checksum and copy takes 111 s.
Reference: [3] <author> Dan Dobberpuhl, R. Witek, et al. </author> <title> "A 200 MHz 64 bit Dual Issue CMOS Microprocessor." </title> <booktitle> International Solid-State Circuits Conference 1992, </booktitle> <month> February </month> <year> 1992. </year>
Reference: [4] <institution> FORE Systems. TCA-100 TURBOchannel ATM Computer Interface, </institution> <note> User's Manual, </note> <year> 1992. </year>
Reference-contexts: 1 Introduction In this paper, we investigate the latency characteristics of the TCP transport protocol on an ATM network <ref> [4] </ref>. The characteristics of LAN technologies have changed a great deal in the last few years. With faster network hardware, the disparity between software and hardware costs is even greater. This increases the importance of efficient protocol implementations and efficient operating system interfaces. <p> The following factors in network communication make measuring TCP performance, especially latency, interesting: * The existence of a high quality TCP software implementation: the BSD 4.4 alpha TCP code. * The availability of low latency network interfaces: e.g., the FORE TCA-100 ATM interface <ref> [4] </ref>. This work was supported in part by the National Science Foundation under Grants No. CCR-8907666, CDA-9123308, and CCR-9200832, by the Washington Technology Center, Apple Computer, Boeing Computer Services, Digital Equipment Corporation, and the Hewlett-Packard Corporation. Chandramohan A.
Reference: [5] <author> Jonathan Kay and Joseph Pasquale. </author> <title> "A Performance Analysis of TCP/IP and UDP/IP Networking Software for the DECstation 5000" Tech Report, </title> <booktitle> CSL U.C. </booktitle> <address> San Diego/Sequoia, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: Earlier work by Kay and Pasquale <ref> [5] </ref> on TCP/IP performance of the ULTRIX 4.2A system on DECstations 5000/200s using an FDDI network had concluded that major latency improvements in TCP processing times would be difficult to achieve. <p> However, it is clear that latency can be further reduced by eliminating the checksum calculation altogether for local area traffic. It is already common practice to eliminate the UDP checksum for NFS traffic, although the mechanism does not distinguish between local traffic and traffic through routers. Kay and Pasquale <ref> [5] </ref> describe a mechanism to implement this change in the protocol in general. We therefore restrict ourselves to an analysis of the error characteristics of eliminating the checksum, the remaining issue left unaddressed. <p> But, the latency of the 8000 byte case is reduced by 40%. 5 Conclusions We characterized the latency costs of TCP communication on the FORE ATM network, and investigated various methods for reducing those costs. A recent study <ref> [5] </ref> concludes that the costs are well 13 Average ATM Round Trip Time Without Checksum Size (bytes) Checksum No Checksum Ratio 4 1021 1020 1.0 80 1289 1233 0.96 500 2140 1808 0.84 4000 5891 3633 0.62 Table 9: Comparison of round trip latencies over ATM with and without the TCP
Reference: [6] <author> Jonathan Kay and Joseph Pasquale. </author> <title> "Measurement, Analysis, and Improvement of UDP/IP Throughput for the DECstation 5000" Tech Report, </title> <booktitle> CSL U.C. </booktitle> <address> San Diego/Sequoia, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Sections 3 and 4 study the effect of several modifications that we felt were important based on the results in Section 2. Many of these modifications are not new and have been suggested by others as well in the literature <ref> [2, 6] </ref>, however, our focus here is on the effect of these modifications on latency rather than throughput. 2 Measurement of the Baseline System The baseline system that we are concerned with is the 4.4 BSD alpha release running on an ATM network.
Reference: [7] <author> V. Jacobson, R. Braden, and D. </author> <title> Borman. "TCP Extensions for High Performance." </title> <type> RFC 1323, </type> <institution> LBL, USC/ISI, and Cray Research, </institution> <month> May </month> <year> 1992. </year>
Reference: [8] <author> David B. Johnson and Willy Zwaenopoel. </author> <title> "The Peregrine High-Performance RPC System." </title> <note> To appear in Software Practice and Experience. </note>
Reference-contexts: There are two distinct kinds of optimizations that are often called header prediction. The first, involving prefilling parts of the transport header, is a known optimization for lowering latency <ref> [11, 8] </ref>, and is not discussed further here. The second technique involves exploiting traffic locality to predict the next incoming packet and to avoid the protocol control block (PCB) lookup cost. Others have studied using traffic locality to improve throughput for bulk data transfer protocols [1, 13].
Reference: [9] <author> Paul E. McKenney and Ken F. Dove. </author> <title> "Efficient Demultiplexing of Incoming TCP Packets." </title> <booktitle> In Proceedings of SIGCOMM '92, </booktitle> <address> Maryland, USA. </address>
Reference-contexts: The lookup algorithm for the PCBs is just a linear search through the linked list of PCBs. McKenney and Dove study alternative data structures for PCB lookup, and analyze these data structures by the expected average search length <ref> [9] </ref>. However, they do not discuss how long a search of any given length will take. While this facilitates comparisons, it is difficult to study the absolute effect of header prediction. We measured the cost of a search for a variety of lengths and show the results in Table 5.
Reference: [10] <author> John K. Ousterhout. </author> <title> "Why Aren't Operating Systems Getting Faster As Fast as Hardware?" In Proceedings of the USENIX 1990 Summer Conference, </title> <month> June </month> <year> 1990, </year> <pages> pp. 247-256. </pages>
Reference-contexts: On the DECstation 5000/200, using the standard ULTRIX kernel routines to do the checksum takes 207 s, and the copy takes 91 s. The combined checksum and copy takes 111 s. This relative performance is not very surprising and is consistent with the observations by Ousterhout <ref> [10] </ref>. 4.1.1 Kernel Implementation Issues On the transmit side, we first investigated deferring the checksum calculation until the copy from kernel to device memory. However, the design our ATM interface makes this impossible. Recall that it uses a simple memory mapped transmit FIFO.
Reference: [11] <author> M.D. Schroeder and M. Burrows. </author> <title> "Performance of Firefly RPC." </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 1-17, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: There are two distinct kinds of optimizations that are often called header prediction. The first, involving prefilling parts of the transport header, is a known optimization for lowering latency <ref> [11, 8] </ref>, and is not discussed further here. The second technique involves exploiting traffic locality to predict the next incoming packet and to avoid the protocol control block (PCB) lookup cost. Others have studied using traffic locality to improve throughput for bulk data transfer protocols [1, 13].
Reference: [12] <author> M. Schroeder, A. Birrell, M. Burrows, H. Murray, R. Needham, T. Rodeheffer, E. Satterthwaite, and C. Thacker. "Autonet: </author> <title> A High-Speed Self-Configuring Local Area Network Using Point-to-Point Links. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 9(8) </volume> <pages> 1318-1335, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Latency measurements typically involve estimates of small code paths that take on the order of microseconds. To achieve this level of granularity, we used a real time clock that ticked at 40ns. This clock is on a TurboChannel card, the AN-1 controller from DEC SRC <ref> [12] </ref>. The clock is initialized at boot time, and user-level processes can access it by issuing a system call that maps the clock address into the process's address space. Reading the clock is then just a matter of dereferencing a pointer.
Reference: [13] <author> Cheng Song and Lawrence Landweber. </author> <title> "Optimizing Bulk Data Transfer Performance: A Packet Train Approach." </title> <booktitle> In Proceedings of SIGCOMM '88, </booktitle> <month> September </month> <year> 1988. </year>
Reference-contexts: The second technique involves exploiting traffic locality to predict the next incoming packet and to avoid the protocol control block (PCB) lookup cost. Others have studied using traffic locality to improve throughput for bulk data transfer protocols <ref> [1, 13] </ref>. In the BSD implementation, the TCP input processing keeps a single entry cache of the most recently used PCB. If the incoming packet is from the same connection as the previous packet, the call to the PCB lookup routine is avoided.
Reference: [14] <author> Chandramohan Thekkath and Henry Levy. </author> <title> "Limits to Low-Latency Communication on High-Speed Networks." </title> <type> Technical Report 91-06-01, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> June </month> <year> 1991. </year> <month> 15 </month>
Reference-contexts: Simply replacing the ULTRIX TCP implementation with the latest BSD version improved the latency by as much as 20%. Others with experience designing high performance "lightweight" RPC systems have noticed that controller design has a significant impact on performance <ref> [14] </ref>. We discovered that controller design has a large impact on latency even using a relatively "heavyweight" protocol such as TCP. Operating system services such as memory allocation had less impact than we expected, yet context switching had more of an impact at small packet sizes than expected.
References-found: 14

