URL: ftp://ftp.cs.wisc.edu/wwt/pads96_optimistic.ps
Refering-URL: http://www.cs.wisc.edu/~markhill/
Root-URL: 
Email: wwt@cs.wisc.edu  
Title: Optimistic Simulation of Parallel Architectures Using Program Executables  
Author: Sashikanth Chandrasekaran and Mark. D. Hill 
Keyword: (2) on hosts with high message latencies and no synchronization hardware.  
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: A key tool of computer architects is computer simulation at the level of detail that can execute program executables. The time and memory requirements of such simulations can be enormous, especially when the machine under design|the target|is a parallel machine. Thus, it is attractive to use parallel simulation, as successfully demonstrated by the Wisconsin Wind Tunnel (WWT). WWT uses a conservative simulation algorithm and eschews network simulation to make looka-head adequate. Nevertheless, we find most of WWT's slowdown to be due to the synchronization overhead in the conservative simulation algorithm. This paper examines the use of optimistic algorithms to perform parallel simulations of parallel machines. We first show that we can make optimistic algorithms work correctly even with WWT's direct execution of program executables. We checkpoint processor registers (integer, floating-point, and condition codes) and use executable editing to log the value of memory words just before they are overwritten by stores. Second, we consider the performance of two optimistic algorithms. The first executes programs optimistically, but performs protocol events (e.g., sending messages) conservatively. The second executes everything optimistically and is similar to Time Warp with lazy message cancellation. Unfortunately, both approaches make parallel simulation performance worse for the default WWT assumptions. We conclude by speculating on the performance of optimistic simulation when simulating (1) target network details, and fl This work is supported in part by NSF Grant MIP-9225097, Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant #F33615-94-1-1525 and ARPA order no. B550, Sun Microsystems and Thinking Machines Corporation, Our Thinking Machines CM-5 was purchased through NSF Institutional Infrastructure Grant No. CDA-9024618 with matching funding from the University of Wisconsin Graduate School. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Wright Laboratory Avionics Directorate or the U.S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon, </author> <title> The NAS Parallel Benchmarks, </title> <type> Technical Report RNR-91-002, Revision 2, </type> <institution> Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: We performed experiments using four other shared-memory benchmarks (Barnes, Cholesky and Mp3d from the SPLASH benchmark suite [17] and a par-allelized version of Appbt <ref> [1] </ref>) and obtained similar results.
Reference: [2] <author> Doug Burger and David A. Wood, </author> <title> Accuracy vs. Performance in Parallel Simulation of Interconnection Networks, </title> <booktitle> International Symposium on Parallel Processing, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Depending on the desired accuracy of simulation of the target network contention and topology, the quantum length must be less than or equal to the message latency. The fixed network latency assumption results in an error of over 20% in several cases <ref> [2] </ref>. Reducing the quantum length results in a further increase in the synchronization cost. Since optimistic techniques improve the lookahead and reduce the frequency of synchronization, they may perform better if the network simulation messages are not rolled back frequently.
Reference: [3] <author> Helen Davis, Stephen R. Goldschmidt, and John Hennessy, </author> <title> Multiprocessor Simulation and Tracing Using Tango, </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II99-107, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Therefore, researchers have begun using parallel simulation|i.e, using an existing parallel machine (the host) to simulate the parallel machine under study (the target). A technique known as direct execution <ref> [3] </ref> is used to execute program executables. With direct execution, the host simulates only those features in the target machine that it does not support (e.g., cache-coherence, synchronization operations). In this paper we use the term event only to refer to those actions that require simulation by the host.
Reference: [4] <author> Babak Falsafi and David A. Wood, </author> <title> Cost=Performance of a Parallel Computer Simulator, </title> <booktitle> In Proceedings of PADS, </booktitle> <year> 1994. </year>
Reference-contexts: However, only the target cache is simulated and the technique has been shown to scale only up to 8 processors. WWT models the parallel machine more accurately and could use the above techniques when the host is a shared-memory machine. Falsafi and Wood <ref> [4] </ref> propose simulating multiple target nodes in a single host node. The advantage of this approach is that it reduces load imbalance.
Reference: [5] <author> Richard M. Fujimoto, </author> <title> Performance of Time Warp Under Synthetic Workloads, </title> <booktitle> Proceedings of the SCS Multiconfer-ence on Distributed Simulation, </booktitle> <month> January </month> <year> 1990. </year>
Reference-contexts: We found that the Time Warp like technique performs up to 2.5 times slower than the conservative simulation. Several researchers have reported successes with us-ing optimistic simulation <ref> [5, 12, 22] </ref>. The two notable reasons for the sharp contrast in our conclusions are: * Our workloads are directly executed programs.
Reference: [6] <author> Richard M. Fujimoto, </author> <title> Parallel Discrete Event Simulation, </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> October </month> <year> 1990. </year>
Reference: [7] <author> Rahmat Hyder and David A. Wood, </author> <title> Synchronization Support for Networks of Workstations, </title> <booktitle> In Proceedings of the International Conference on Supercomputing (ICS), </booktitle> <year> 1996. </year>
Reference-contexts: The CM-5, on the other hand, provides fast messages and hardware support for fast barriers and reductions (the latencies of these operations are all less than 10s on the CM-5). Hyder and Wood study the implications of latency and synchronization tradeoffs using a variety of applications <ref> [7] </ref>. How would high latency networks and no hardware synchronization affect the trade off between conservative and optimistic simulation? Both techniques must ensure that all messages sent during a quantum are received before the beginning of the next quantum. <p> In the absence of synchronization hardware, each host node must send an acknowledgment message for each message that it receives and perform a simple software barrier once all acknowledgments have been received <ref> [7] </ref>. On a host with a network latency of 100s, synchronization at the end of the quantum would be an order of magnitude more expensive than on the CM-5. This favors optimistic techniques since they need to incur the high cost of this software synchronization less often.
Reference: [8] <author> David R. Jefferson, </author> <title> Virtual Time, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Mechanisms would hence be required to undo incorrectly sent messages and incorrectly simulated protocol events. We implement the classical solution proposed by Jeffer-son <ref> [8] </ref> and send anti-messages that annihilate incor rectly sent messages. 2 All protocol events essentially perform one or more of the following actions: * Modify the state of a block (either in the target cache or directory).
Reference: [9] <author> James R. Larus and Eric Schnarr, EEL: </author> <title> Machine-Independent Executable Editing, </title> <booktitle> Programming Languages Design and Implementation (PLDI), </booktitle> <year> 1995. </year>
Reference-contexts: Instead, the target memory is saved incrementally by logging all changes to it. The target program changes the state of memory by executing store instructions. We use EEL <ref> [9] </ref>, an executable editing library, to instrument the target store instructions with a small piece of code (e.g., four instructions before a store-word) that loads the old value from memory and saves it in a log in the target address space. 1 Note that stores to both private and shared memory
Reference: [10] <author> David Nicol, </author> <title> Conservative Parallel Simulation of Priority Class Queuing Networks, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(3) </volume> <pages> 398-412, </pages> <month> May </month> <year> 1992. </year>
Reference: [11] <author> David Nicol, </author> <title> Global Synchronization for Optimistic Parallel Discrete Event Simulation, </title> <booktitle> Proceedings of the seventh workshop on Parallel and Distributed Simulation, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: We have used a simple algorithm for computing the global virtual time (GVT). This algorithm requires that all host nodes perform a barrier before the GVT can be computed. Researchers have recently proposed efficient algorithms to perform global synchronization with optimistic simulation <ref> [11] </ref>. Since a large fraction of the simulation time is spent in global synchronization, incorporating the new algorithms would improve the performance of optimistic simulation. Finally, we are in the process of porting WWT to a cluster of workstations connected by a Myrinet switch.
Reference: [12] <author> Presley, M., Ebling, M., Wieland, F., and Jefferson, D. R., </author> <title> Benchmarking the Time Warp Operating System with a computer network simulation, </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 21, </volume> <month> 2 (March </month> <year> 1989), </year> <pages> pp. 8-13. </pages>
Reference-contexts: Optimistic approaches have been shown to perform better when simulating target systems such as queuing networks and when using stochastic workloads <ref> [12] </ref>. This paper, on the other hand, compares the performance of conservative and optimistic approaches by executing shared-memory applications. We find that for all the applications that we executed on a CM-5 host, the conservative technique performs better than either optimistic technique. <p> We found that the Time Warp like technique performs up to 2.5 times slower than the conservative simulation. Several researchers have reported successes with us-ing optimistic simulation <ref> [5, 12, 22] </ref>. The two notable reasons for the sharp contrast in our conclusions are: * Our workloads are directly executed programs.
Reference: [13] <author> Steven K. Reinhardt, Babak Falsafi, and David A. Wood, </author> <title> Kernel Support for the Wisconsin Wind Tunnel, </title> <booktitle> Proceedings of the Second USENIX on Microkernels and Other Kernel Architectures, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: Whenever the target program returns control to WWT (due to a quantum expiration, or an event), the executive interface to the CM-5 kernel <ref> [13] </ref> saves the target global registers (including the program counter) and the condition codes in a buffer. We only need to copy the buffer and save the floating-point registers and the floating-point status register to partially checkpoint the target processor state.
Reference: [14] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood, </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers, </title> <booktitle> ACM SIGMETRICS, </booktitle> <year> 1993. </year>
Reference-contexts: The common features, such as program instructions, are not simulated|instead, they are directly executed by the host. The Wiscon-sin Wind Tunnel (WWT) is a simulator that executes program executables on a Thinking Machines CM-5 host to simulate cache-coherent shared-memory computers <ref> [14] </ref>. WWT directly executes all instructions and memory references that hit in the target cache. Program executables are edited so that during WWT's direct execution they also keep track of target execution time (by incrementing a counter).
Reference: [15] <author> P. Reynolds, </author> <title> A Spectrum of Options for Parallel Simulation, </title> <booktitle> Proceedings of the 1988 Winter Simulation Conference, </booktitle> <pages> pages 325-332. </pages>
Reference-contexts: Using these techniques, we study two optimistic approaches: * A hybrid approach that optimistically executes target program instructions and memory references that hit in the target cache and conservatively simulates protocol events. We use Reynolds' terminology <ref> [15] </ref> and call this a risk free optimistic approach. * An aggressive Time Warp-like optimistic approach that executes all target program instructions, memory references (including those that miss in the target cache) and protocol events op timistically.
Reference: [16] <author> Gautam Shah, Umakishore Ramachandran, and Richard Fujimoto, Timepatch: </author> <title> A novel technique for the parallel simulation of multiprocessor caches, </title> <address> TR-94-52, GIT, </address> <month> Oc-tober </month> <year> 1994. </year>
Reference-contexts: In the delta exchange method event processing is divided into two steps |The first step does the basic event processing while the second step exchanges the new state values. In the rollback queue mechanism the C++ assignment operators are overloaded to automatically save state information. Shah et al. <ref> [16] </ref> simulate a shared-memory target machine on a shared-memory host. Their technique is optimistic with respect to timing correctness and reconciliation is performed only at synchronization points. Rollback of the target program is never required because the underlying host machine keeps the memory consistent and only data-race-free programs are executed.
Reference: [17] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta, </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory, </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: An all-hardware directory-based coherence protocol is used to maintain a sequentially consistent view of shared-memory. The three applications that we chose to present are Ocean, Sparse, and Water. Ocean and Water are from the SPLASH benchmark suite <ref> [17] </ref>, and Sparse is a locally written shared-memory program. Ocean is a hydrodynamic simulation that models a two-dimensional cross-section of a cuboidal basin. The input data set used was a 98 fi 98 grid. Sparse solves AX = B in parallel for a sparse matrix A. <p> We performed experiments using four other shared-memory benchmarks (Barnes, Cholesky and Mp3d from the SPLASH benchmark suite <ref> [17] </ref> and a par-allelized version of Appbt [1]) and obtained similar results.
Reference: [18] <author> Jeff S. Steinman, SPEEDES: </author> <title> A Multiple-Synchronization Environment for Parallel Discrete-Event Simulation, </title> <journal> International Journal in Computer Simulation, </journal> <volume> Vol. 2, </volume> <pages> Pages 251-286. </pages>
Reference-contexts: Program executables are edited so that during WWT's direct execution they also keep track of target execution time (by incrementing a counter). WWT regains control on cache misses and simulates the target cache, directory, etc., by sending timestamped messages. WWT uses the conservative time bucket synchronization mechanism <ref> [18] </ref> to coordinate simulation of the processor nodes. Simulation proceeds in parallel for quanta of duration Q. Each node must synchronize with all other nodes at the end of every quantum, after which all nodes proceed in parallel for another quantum Q. <p> This strategy is similar to the Breathing Time Buckets strategy supported in SPEEDES <ref> [18] </ref>. Like the conservative technique, events are processed in quanta. However, these quanta do not have the constant length, Q. Instead, the event horizon determines the quantum length.
Reference: [19] <author> Jeff S. Steinman, </author> <title> Incremental State Saving in SPEEDES using C++, </title> <booktitle> In Proceeding of the 1993 Winter Simulation Conference, </booktitle> <pages> Pages 687-96. </pages>
Reference-contexts: A state manager exports an interface to the application and calls this interface before each change of a block of state. The backtrace of memory snapshots are saved in a buffer that is unwound on rollback. Incremental state saving is performed in SPEEDES using two techniques <ref> [19] </ref>. time. This figure compares the performance of conservative and optimistic simulations as the host network latency is increased. All synchronization operations are performed in software. The application program was Water running on 8 and 16 host nodes.
Reference: [20] <author> Thinking Machines Corporation, </author> <title> The Connection Machine CM-5 Technical Summary, </title> <year> 1991. </year>
Reference: [21] <author> Brian W. Unger, John G. Cleary, Alan Covington, and Darrin West, </author> <title> An External State Management System for Optimistic Parallel Simulation, </title> <booktitle> In Proceedings of the 1993 Winter Simulation Conference. </booktitle>
Reference-contexts: Figure 3 illustrates that for applications with a low communication overhead, optimistic techniques may be a better choice on future parallel systems. 6 Related Work This paper presented techniques to integrate direct execution with optimistic simulation and studied the performance of three simulation techniques. Unger et al. <ref> [21] </ref> present an incremental state saving scheme in the Jade simulation environment. A state manager exports an interface to the application and calls this interface before each change of a block of state. The backtrace of memory snapshots are saved in a buffer that is unwound on rollback.
Reference: [22] <author> Wieland, F. et al., </author> <title> Distributed combat simulation and Time Warp: The model and its performance, </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation 21, </booktitle> <month> 2 (March </month> <year> 1989), </year> <pages> pp. 14-20. </pages>
Reference-contexts: We found that the Time Warp like technique performs up to 2.5 times slower than the conservative simulation. Several researchers have reported successes with us-ing optimistic simulation <ref> [5, 12, 22] </ref>. The two notable reasons for the sharp contrast in our conclusions are: * Our workloads are directly executed programs.
References-found: 22

