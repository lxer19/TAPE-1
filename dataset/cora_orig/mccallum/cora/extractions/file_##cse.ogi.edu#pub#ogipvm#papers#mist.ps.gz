URL: file://cse.ogi.edu/pub/ogipvm/papers/mist.ps.gz
Refering-URL: http://www.cse.ogi.edu/~walpole/publications.html
Root-URL: http://www.cse.ogi.edu
Email: mist@cse.ogi.edu  
Title: MIST: PVM with Transparent Migration and Checkpointing task migration, in conjunction with a global scheduler,
Author: Jeremy Casas, Dan Clark, Phil Galbiati, Ravi Konuru, Steve Otto, Robert Prouty and Jonathan Walpole 
Note: Transparent  
Date: May 1995  
Address: PO Box 91000 Portland, OR 97291-1000, USA  
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute of Science Technology  
Web: http://www.cse.ogi.edu/DISC/projects/mist/  
Abstract: We are currently involved in research to enable PVM to take advantage of shared networks of workstations (NOWs) more effectively. In such a computing environment, it is important to utilize workstations unobtrusively and recover from machine failures. Towards this goal, we have enhanced PVM with transparent task migration, checkpointing, and global scheduling. These enhancements are part of the MIST project which takes an open systems approach in developing a cohesive, distributed parallel computing environment. This open systems approach promotes plug-and-play integration of independently developed modules, such as Condor, DQS, AVS, Prospero, XPVM, PIOUS, Ptools, etc. We have implemented a global scheduler as a PVM resource manager which can take advantage of task migration to perform dynamic scheduling of tasks. Some extensions to the resource manager interface were required. The task migration mechanism also serves as the basis for transparent checkpointing, which is a common method for improving a system's fault-tolerance. We have developed a PVM prototype that integrates checkpointing and migration. This paper presents an overview of the entire system, issues raised by this work, and discusses future plans. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. L. Beguelin, J. J. Dongarra, A. Geist, and R. J. Manchek V. S. Sunderam. </author> <title> Heterogeneous network computing. </title> <booktitle> In Sixth SIAM Conference on Parallel Processing. </booktitle> <publisher> SIAM, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction PVM <ref> [1, 2, 3] </ref> is a widely used, public-domain software system that allows a heterogeneous network of parallel and serial computers to be programmed as a single computational resource. This resource appears to the application programmer as a potentially large message-passing virtual computer.
Reference: [2] <author> J. J. Dongarra, A. Geist, R. J. Manchek, and V. S. Sunderam. </author> <title> Integrated PVM framework supports heterogeneous network computing. </title> <booktitle> Computers in Physics, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: 1 Introduction PVM <ref> [1, 2, 3] </ref> is a widely used, public-domain software system that allows a heterogeneous network of parallel and serial computers to be programmed as a single computational resource. This resource appears to the application programmer as a potentially large message-passing virtual computer.
Reference: [3] <author> A. L. Beguelin, J. J. Dongarra, A. Geist, R. J. Manchek, S. W. Otto, and J. Walpole. </author> <title> PVM: Experiences, current status and future direction. </title> <booktitle> In Supercomputing '93 Proceedings, </booktitle> <pages> pages 765-6, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction PVM <ref> [1, 2, 3] </ref> is a widely used, public-domain software system that allows a heterogeneous network of parallel and serial computers to be programmed as a single computational resource. This resource appears to the application programmer as a potentially large message-passing virtual computer.
Reference: [4] <author> Jeremy Casas, Dan Clark, Ravi Konuru, Steve Otto, Robert Prouty, and Jonathan Walpole. MPVM: </author> <title> A migration transparent version of PVM. </title> <type> Technical Report CSE-95-002, </type> <institution> Dept. of Computer Science and Engineering, Oregon 11 Graduate Institute of Science & Technology, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: In the context of this vision, this paper presents the mechanisms we have for supporting intelligent scheduling and resource management. We present the prototypes we have for the MIST kernel, the sched-uler, and the system load monitor components. For the MIST kernel, we use MMPVM <ref> [4] </ref> (Multi-user, Mi-gratable PVM), an enhanced version of PVM that supports transparent task migration, application check-pointing, and multi-user application execution. For the scheduler, we make use of an enhanced version of resource manager interface provided by PVM. <p> Within this time, we've seen tasks move around as workstation owners allow/disallow usage of their workstations. While we have yet to quantify the performance of the MIST system and its effect on the applications, it is certainly up and running. 3.1 MMPVM MMPVM <ref> [4] </ref> is an enhanced version of PVM that is capable of transparent process migration, application checkpointing/restart, and running applications of different users. It is implemented entirely at user-level, using facilities available through standard Unix system calls and libraries.
Reference: [5] <author> Al Geist, Adam Beguelin, Jack Dongarra, We-icheng Jiang, Ro bert Manchek, and Vaidy Sun-deram. </author> <title> PVM 3 user's guide and reference manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Task ID virtualization allows tasks to communicate with each other regard 3 less of their real location. In PVM, the host ID of the machine a task is on is encoded in its task ID <ref> [5] </ref>. In MMPVM, the host number encoded in a task ID may no longer be that of the host the task is really on. Task ID virtualization is achieved by maintaining task-to-host mappings on the pvmds. These task-to-host mappings are updated whenever tasks migrate.
Reference: [6] <author> Khaled Al-Saqabi, Steve W. Otto, and Jonathan Walpole. </author> <title> Gang scheduling in heterogenous distributed systems. </title> <type> Technical Report CSE-94-023, </type> <institution> Dept. of Computer Science and Engineering, Ore-gon Graduate Institute of Science & Technology, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: We then extended this interface to allow the GS to make use of the new capabilities of MMPVM. The extensions are shown in Table 2. Using this extended interface, we have an implementation of the scheduling algorithm of Al-Saqabi <ref> [6] </ref>. This scheduling algorithm takes into account processor heterogeneity in terms of architecture and speed and the availability/unavailability of workstations to schedule tasks in the virtual machine.
Reference: [7] <author> Kipp E. B. Hickman. </author> <title> The SSL protocol. RFC draft specification, </title> <note> available at http://home.netscape.com/newsref/std/SSL.html. </note>
Reference-contexts: This implementation will remove the requirement that the daemons be run as root. This will also isolate MIST users from each other, and remove the security problem presented by the highly privileged daemons. The next step would be to add encryption, perhaps via SSL <ref> [7] </ref>, to all communications. Encryption of all MIST network traffic would significantly complicate, if not entirely prevent, malicious eavesdropping on MIST data transmissions. This service could then be extended to add mutual authentication among all MIST tasks, possibly using a system like Kerberos [8].
Reference: [8] <author> J. Steiner, C. Neuman, and J. Schiller. </author> <title> Kerberos: An authentication service for open network systems. </title> <booktitle> In Proceedings of the Usenix Winter Conference, </booktitle> <pages> pages 191-202, </pages> <address> Berkeley, CA, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: Encryption of all MIST network traffic would significantly complicate, if not entirely prevent, malicious eavesdropping on MIST data transmissions. This service could then be extended to add mutual authentication among all MIST tasks, possibly using a system like Kerberos <ref> [8] </ref>. Authentication would prevent unauthorized utilization of the MIST system and the resources to which it has access.
Reference: [9] <author> Robert Wahbe, Steven Lucco, Thomas E. Ander-son, and Susan L. Graham. </author> <title> Software fault isolation. </title> <booktitle> In Fourteenth ACM Symposium on Operating System Principles, </booktitle> <volume> volume 27, </volume> <pages> pages 203-216, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: This service could then be extended to add mutual authentication among all MIST tasks, possibly using a system like Kerberos [8]. Authentication would prevent unauthorized utilization of the MIST system and the resources to which it has access. The final step would incorporate technology like Software Fault Isolation <ref> [9] </ref> to force strict adherence to the MIST API, thus preventing access to any unsecure, unauthenticated services. 5 Related Work Condor [10, 11, 12], PRM [13], UPVM [14], DQS [15], Lsbatch [16], Fail-Save PVM [17], and DOME [18, 19, 20] are other software systems that support adaptive parallel application execution on
Reference: [10] <author> Michael J. Litzkow, Miron Livny, and Matt W. </author> <title> Mutka. Condor Ahunter of idle workstations. </title> <booktitle> In Proceedings of the 8th IEEE International Conference on Distributed Computing Systems, </booktitle> <pages> pages 104-111, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Authentication would prevent unauthorized utilization of the MIST system and the resources to which it has access. The final step would incorporate technology like Software Fault Isolation [9] to force strict adherence to the MIST API, thus preventing access to any unsecure, unauthenticated services. 5 Related Work Condor <ref> [10, 11, 12] </ref>, PRM [13], UPVM [14], DQS [15], Lsbatch [16], Fail-Save PVM [17], and DOME [18, 19, 20] are other software systems that support adaptive parallel application execution on a shared network of workstations. <p> These systems either employ process migration, checkpointing, suspension, resumption, or a any combination of these to support adaptive execution. A common trait among these systems, MMPVM included, is that all these systems are implemented at user-level, requiring no special support from the hardware or the operating system. Condor <ref> [10] </ref> is a software package that allows user applications unobtrusive access to machines on a shared network. Condor achieves load balance by executing user applications on idle or lightly loaded machines, at the same time remains unobtrusive to machine owners by migrating applications away from reclaimed machines.
Reference: [11] <author> Jim Pruyne and Miron Livny. </author> <title> Providing resource management services to parallel applications. </title> <editor> In J. Dongarra and B. Tourancheau, editors, </editor> <booktitle> 2nd workshop on Environments and Tools for Parallel Scientific Computing, </booktitle> <pages> pages 152-161, </pages> <year> 1995. </year>
Reference-contexts: Authentication would prevent unauthorized utilization of the MIST system and the resources to which it has access. The final step would incorporate technology like Software Fault Isolation [9] to force strict adherence to the MIST API, thus preventing access to any unsecure, unauthenticated services. 5 Related Work Condor <ref> [10, 11, 12] </ref>, PRM [13], UPVM [14], DQS [15], Lsbatch [16], Fail-Save PVM [17], and DOME [18, 19, 20] are other software systems that support adaptive parallel application execution on a shared network of workstations. <p> Condor was initially designed for use with sequential applications. Migration of sequential applications is achieved by taking a core dump of the application and combining it with the application's executable file to create a checkpoint file. Extensions were then made to support parallel applications, PVM applications in particular <ref> [11, 12] </ref>. Resource management support for PVM applications include task scheduling, deletion, suspension and resumption. A notification service is 9 also provided to inform other tasks of the application that one of its tasks was deleted, suspended or resumed.
Reference: [12] <author> Jim Pruyne and Miron Livny. </author> <title> Parallel processing on dynamic resources with CARMI. </title> <booktitle> In Proceedings of the Workshop on Job Scheduling for Parallel Processing, International Parallel Processing Symposium '95, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Authentication would prevent unauthorized utilization of the MIST system and the resources to which it has access. The final step would incorporate technology like Software Fault Isolation [9] to force strict adherence to the MIST API, thus preventing access to any unsecure, unauthenticated services. 5 Related Work Condor <ref> [10, 11, 12] </ref>, PRM [13], UPVM [14], DQS [15], Lsbatch [16], Fail-Save PVM [17], and DOME [18, 19, 20] are other software systems that support adaptive parallel application execution on a shared network of workstations. <p> Condor was initially designed for use with sequential applications. Migration of sequential applications is achieved by taking a core dump of the application and combining it with the application's executable file to create a checkpoint file. Extensions were then made to support parallel applications, PVM applications in particular <ref> [11, 12] </ref>. Resource management support for PVM applications include task scheduling, deletion, suspension and resumption. A notification service is 9 also provided to inform other tasks of the application that one of its tasks was deleted, suspended or resumed.
Reference: [13] <author> B. Clifford Neuman and Santosh Rao. </author> <title> The Pros-pero Resource Manager: A scalable framework for processor allocation in distributed systems. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 6(4) </volume> <pages> 339-355, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The final step would incorporate technology like Software Fault Isolation [9] to force strict adherence to the MIST API, thus preventing access to any unsecure, unauthenticated services. 5 Related Work Condor [10, 11, 12], PRM <ref> [13] </ref>, UPVM [14], DQS [15], Lsbatch [16], Fail-Save PVM [17], and DOME [18, 19, 20] are other software systems that support adaptive parallel application execution on a shared network of workstations. These systems either employ process migration, checkpointing, suspension, resumption, or a any combination of these to support adaptive execution. <p> We hope to be able to integrate MMPVM with the Condor system in the future. The Prospero Resource Manager (PRM) <ref> [13] </ref> is a software environment which provides a scalable and flexible resource management structure for execution of both sequential and parallel applications. PRM's and MMPVM's support for unobtrusive execution of parallel applications are functionally the same. They differ, however, in terms of implementation.
Reference: [14] <author> Ravi Konuru, Jeremy Casas, Steve Otto, Robert Prouty, and Jona than Walpole. </author> <title> A user-level process package for PVM. </title> <booktitle> In 1994 Scalable High-Performance Computing Conference, </booktitle> <pages> pages 48-55. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: The final step would incorporate technology like Software Fault Isolation [9] to force strict adherence to the MIST API, thus preventing access to any unsecure, unauthenticated services. 5 Related Work Condor [10, 11, 12], PRM [13], UPVM <ref> [14] </ref>, DQS [15], Lsbatch [16], Fail-Save PVM [17], and DOME [18, 19, 20] are other software systems that support adaptive parallel application execution on a shared network of workstations. These systems either employ process migration, checkpointing, suspension, resumption, or a any combination of these to support adaptive execution. <p> MMPVM transfers process state directly through the network. PRM, on the other hand, uses Condor style checkpointing which involves the creation of core dumps and checkpoint files. From our experience with both types of migration schemes, our method is approximately 10x faster than the core dump method. UPVM <ref> [14] </ref> is another software system, very similar to MMPVM, that enables transparent migration of PVM tasks. Unlike MMPVM where each task is a Unix process, however, tasks in UPVM are implemented as User-Level Processes (ULPs).
Reference: [15] <author> T. Green and J. Snyder. DQS, </author> <title> a distributed queuing system. </title> <type> Technical report, </type> <institution> Supercomputer Computations Research Institute, Florida State Uni versity, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: The final step would incorporate technology like Software Fault Isolation [9] to force strict adherence to the MIST API, thus preventing access to any unsecure, unauthenticated services. 5 Related Work Condor [10, 11, 12], PRM [13], UPVM [14], DQS <ref> [15] </ref>, Lsbatch [16], Fail-Save PVM [17], and DOME [18, 19, 20] are other software systems that support adaptive parallel application execution on a shared network of workstations. These systems either employ process migration, checkpointing, suspension, resumption, or a any combination of these to support adaptive execution. <p> Second, since multiple ULPs share the address space of a single Unix process, there is a limit on the number of ULPs the application can have. This limit depends on the size of the virtual address space of the Unix process and the memory requirements of each ULP. DQS <ref> [15] </ref> and LSF (successor of Lsbatch [16]) are two other software systems that mainly support load balancing of batch applications on a network of workstations. These systems support execution of parallel applications to a limited extent.
Reference: [16] <author> J. Wang, S. Zhou, K. Ahmed, and W. Long. Lsbatch: </author> <title> A distributed load sharing batch system. </title> <type> Technical Report CSRI-286, </type> <institution> Computer Sys-terms Research Institute, University of Toronto, Toronto, Canada, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: The final step would incorporate technology like Software Fault Isolation [9] to force strict adherence to the MIST API, thus preventing access to any unsecure, unauthenticated services. 5 Related Work Condor [10, 11, 12], PRM [13], UPVM [14], DQS [15], Lsbatch <ref> [16] </ref>, Fail-Save PVM [17], and DOME [18, 19, 20] are other software systems that support adaptive parallel application execution on a shared network of workstations. These systems either employ process migration, checkpointing, suspension, resumption, or a any combination of these to support adaptive execution. <p> This limit depends on the size of the virtual address space of the Unix process and the memory requirements of each ULP. DQS [15] and LSF (successor of Lsbatch <ref> [16] </ref>) are two other software systems that mainly support load balancing of batch applications on a network of workstations. These systems support execution of parallel applications to a limited extent.
Reference: [17] <author> Juan Leon, Allan Fisher, and Peter Steenkiste. </author> <title> Fail-safe PVM: A portable package for dis-trubuted programming with transparent recovery. </title> <type> Technical Report CMU-CS-93-124, </type> <institution> Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: The final step would incorporate technology like Software Fault Isolation [9] to force strict adherence to the MIST API, thus preventing access to any unsecure, unauthenticated services. 5 Related Work Condor [10, 11, 12], PRM [13], UPVM [14], DQS [15], Lsbatch [16], Fail-Save PVM <ref> [17] </ref>, and DOME [18, 19, 20] are other software systems that support adaptive parallel application execution on a shared network of workstations. These systems either employ process migration, checkpointing, suspension, resumption, or a any combination of these to support adaptive execution. <p> These systems support execution of parallel applications to a limited extent. Recent releases of DQS 3.1.2 and LSF 2.1 are said to have better support for parallel applications, but the extent of this support unknown to us at this time. Fail-Safe PVM <ref> [17] </ref> is an extension to PVM that implements transparent application checkpointing and restart. Just like the current implementation of MM-PVM's checkpoint/restart facility, Fail-Safe PVM uses synchronous checkpointing and messages are explicitly flushed at checkpoint time.
Reference: [18] <author> Adam Beguelin, Erik Seligman, and Michael Starkey. Dome: </author> <title> Distributed object migration environment. </title> <type> Technical Report CMU-CS-94-153, </type> <institution> Carnegie Mellon University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: The final step would incorporate technology like Software Fault Isolation [9] to force strict adherence to the MIST API, thus preventing access to any unsecure, unauthenticated services. 5 Related Work Condor [10, 11, 12], PRM [13], UPVM [14], DQS [15], Lsbatch [16], Fail-Save PVM [17], and DOME <ref> [18, 19, 20] </ref> are other software systems that support adaptive parallel application execution on a shared network of workstations. These systems either employ process migration, checkpointing, suspension, resumption, or a any combination of these to support adaptive execution. <p> It has to be stated, how-ever, that this difference between Fail-Safe PVM and MMPVM was essentially brought about by our goal of making PVM multi-application and multi-user, a goal Fail-Safe PVM wasn't designed for. DOME <ref> [18, 19] </ref> is a computing environment that supports heterogeneous checkpointing through the use of C++ class abstractions. DOME also supports dynamic load balancing through transparent data redistribution. Checkpoints are generated at a "high-level" based on data structures and variables that the application developer defines as part of the DOME environment.
Reference: [19] <author> Erik Seligman and Adam Beguelin. </author> <title> High-level fault tolerance in distributed programs. </title> <type> Technical Report CMU-CS-94-223, </type> <institution> Carnegie Mellon University, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: The final step would incorporate technology like Software Fault Isolation [9] to force strict adherence to the MIST API, thus preventing access to any unsecure, unauthenticated services. 5 Related Work Condor [10, 11, 12], PRM [13], UPVM [14], DQS [15], Lsbatch [16], Fail-Save PVM [17], and DOME <ref> [18, 19, 20] </ref> are other software systems that support adaptive parallel application execution on a shared network of workstations. These systems either employ process migration, checkpointing, suspension, resumption, or a any combination of these to support adaptive execution. <p> It has to be stated, how-ever, that this difference between Fail-Safe PVM and MMPVM was essentially brought about by our goal of making PVM multi-application and multi-user, a goal Fail-Safe PVM wasn't designed for. DOME <ref> [18, 19] </ref> is a computing environment that supports heterogeneous checkpointing through the use of C++ class abstractions. DOME also supports dynamic load balancing through transparent data redistribution. Checkpoints are generated at a "high-level" based on data structures and variables that the application developer defines as part of the DOME environment.
Reference: [20] <author> Jose Nagib Cotrim Arabe, Adam Begueline, Bruce Lowekamp, Erik Seligman, Mike Starkey, and Peter Stephan. Dome: </author> <title> Parallel programming in a heterogeneous multi-user environment. </title> <note> Submitted as a Technical Paper for Supercomputing '95. </note>
Reference-contexts: The final step would incorporate technology like Software Fault Isolation [9] to force strict adherence to the MIST API, thus preventing access to any unsecure, unauthenticated services. 5 Related Work Condor [10, 11, 12], PRM [13], UPVM [14], DQS [15], Lsbatch [16], Fail-Save PVM [17], and DOME <ref> [18, 19, 20] </ref> are other software systems that support adaptive parallel application execution on a shared network of workstations. These systems either employ process migration, checkpointing, suspension, resumption, or a any combination of these to support adaptive execution. <p> Instead, it relies on a very structured programming model where the point at which the application was checkpointed should be accessible through a "goto" statement from main (). A way of getting around this restriction is proposed in <ref> [20] </ref> by the use of a preprocessor that annotates the application with the necessary statement labels and goto calls.
Reference: [21] <author> Fred Douglis and John Ousterhout. </author> <title> Process migration in the Sprite operating system. </title> <booktitle> In Proceedings of the 7th IEEE International Conference on Distributed Computing Systems, </booktitle> <pages> pages 18-25, </pages> <address> Berlin, West Germany, </address> <month> September21-25 </month> <year> 1987. </year>
Reference-contexts: Aside from these software systems, support for adaptive execution on a shared environment is also available from systems such as Sprite <ref> [21, 22] </ref>, Mosix [23, 24], V [25], Mach [26] and Chorus [27]. The difference between these systems from MMPVM and those mentioned previously is that these systems are implemented at the operating system level.
Reference: [22] <author> Fred Douglis and John Ousterhout. </author> <title> Transparent process migration: Design alternatives and the Sprite implementation. </title> <journal> Software | Practice & Experience, </journal> <volume> 21(8) </volume> <pages> 757-785, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Aside from these software systems, support for adaptive execution on a shared environment is also available from systems such as Sprite <ref> [21, 22] </ref>, Mosix [23, 24], V [25], Mach [26] and Chorus [27]. The difference between these systems from MMPVM and those mentioned previously is that these systems are implemented at the operating system level.
Reference: [23] <author> Amnon Barak and Ami Litman. </author> <title> MOS | A multi-computer distributed operating system. </title> <journal> Software | Practice & Experience, </journal> <volume> 15(8) </volume> <pages> 725-737, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Aside from these software systems, support for adaptive execution on a shared environment is also available from systems such as Sprite [21, 22], Mosix <ref> [23, 24] </ref>, V [25], Mach [26] and Chorus [27]. The difference between these systems from MMPVM and those mentioned previously is that these systems are implemented at the operating system level.
Reference: [24] <author> Amnon Barak, Shai Guday, and Richard G. Wheeler. </author> <title> The MOSIX Distributed Operating System Load Balancing for Unix. </title> <booktitle> Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Aside from these software systems, support for adaptive execution on a shared environment is also available from systems such as Sprite [21, 22], Mosix <ref> [23, 24] </ref>, V [25], Mach [26] and Chorus [27]. The difference between these systems from MMPVM and those mentioned previously is that these systems are implemented at the operating system level.
Reference: [25] <author> Marvin M. Theimer, Keith A. Lantz, and David R. Cheriton. </author> <title> Preemptable remote execution facilities for the V-System. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 2-12, </pages> <address> Orcas Islands, Washing-ton, </address> <month> December1-4 </month> <year> 1985. </year>
Reference-contexts: Aside from these software systems, support for adaptive execution on a shared environment is also available from systems such as Sprite [21, 22], Mosix [23, 24], V <ref> [25] </ref>, Mach [26] and Chorus [27]. The difference between these systems from MMPVM and those mentioned previously is that these systems are implemented at the operating system level.
Reference: [26] <author> Dejan S. Milojicic, Wolfgang Zint, Andreas Dan-gel, and Peter Giese. </author> <title> Task migration on the top of the Mach microkernel. </title> <booktitle> In MACH III Symposium Proceedings, </booktitle> <pages> pages 273-289, </pages> <address> Santa Fe, New Mexico, </address> <month> April19-21 </month> <year> 1993. </year>
Reference-contexts: Aside from these software systems, support for adaptive execution on a shared environment is also available from systems such as Sprite [21, 22], Mosix [23, 24], V [25], Mach <ref> [26] </ref> and Chorus [27]. The difference between these systems from MMPVM and those mentioned previously is that these systems are implemented at the operating system level.
Reference: [27] <author> M. O'Connor, B. Tangney, V. Cahill, and N. Har-ris. </author> <title> Microkernel support for migration. </title> <note> Submitted to Distributed Systems Engineering Journal, </note> <month> December </month> <year> 1993. </year>
Reference-contexts: Aside from these software systems, support for adaptive execution on a shared environment is also available from systems such as Sprite [21, 22], Mosix [23, 24], V [25], Mach [26] and Chorus <ref> [27] </ref>. The difference between these systems from MMPVM and those mentioned previously is that these systems are implemented at the operating system level.
Reference: [28] <author> G. Cheng, G. Fox, K. Mills, and Marek Pod-gorny. </author> <title> Developing interactive PVM-based parallel programs on distributed computing systems within AVS framework. </title> <booktitle> In 3rd Annual International AVS Conference, JOIN THE REVOLUTION: </booktitle> <address> AVS '94, Boston, MA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Second, we would like to be able to integrate our software with software systems already available from other research groups such as Condor, PRM, DQS, AVS <ref> [28] </ref>, XPVM [29], PIOUS [30], Ptools [31], etc. and, of course, with the official PVM release. 7 Summary This paper presented our prototype for an intelligent scheduling and resource management system for running PVM applications.
Reference: [29] <author> James A. Kohl and G. A. Geist. XPVM: </author> <title> A graphical console and monitor for PVM. In 2nd PVM User's Group Meeting, </title> <institution> Oak Ridge, TN, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Second, we would like to be able to integrate our software with software systems already available from other research groups such as Condor, PRM, DQS, AVS [28], XPVM <ref> [29] </ref>, PIOUS [30], Ptools [31], etc. and, of course, with the official PVM release. 7 Summary This paper presented our prototype for an intelligent scheduling and resource management system for running PVM applications.
Reference: [30] <author> S. Moyer and V. S. Sunderam. </author> <title> PIOUS: A scalable parallel i/o system for distributed computing environments. </title> <booktitle> In 1994 Scalable High-Performance Computing Conference, </booktitle> <pages> pages 71-78. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: Second, we would like to be able to integrate our software with software systems already available from other research groups such as Condor, PRM, DQS, AVS [28], XPVM [29], PIOUS <ref> [30] </ref>, Ptools [31], etc. and, of course, with the official PVM release. 7 Summary This paper presented our prototype for an intelligent scheduling and resource management system for running PVM applications.
Reference: [31] <author> William Gropp and Ewing Lusk. </author> <title> Scalable Unix tools on parallel processors. </title> <booktitle> In 1994 Scalable High-Performance Computing Conference, </booktitle> <pages> pages 56-62. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year> <month> 13 </month>
Reference-contexts: Second, we would like to be able to integrate our software with software systems already available from other research groups such as Condor, PRM, DQS, AVS [28], XPVM [29], PIOUS [30], Ptools <ref> [31] </ref>, etc. and, of course, with the official PVM release. 7 Summary This paper presented our prototype for an intelligent scheduling and resource management system for running PVM applications.
References-found: 31

