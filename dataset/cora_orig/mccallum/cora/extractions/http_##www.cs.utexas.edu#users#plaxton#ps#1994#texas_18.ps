URL: http://www.cs.utexas.edu/users/plaxton/ps/1994/texas_18.ps
Refering-URL: http://www.cs.utexas.edu/users/plaxton/html/abc.html
Root-URL: 
Email: Email: ftl@math.mit.edu.  
Title: Hypercubic Sorting Networks the n! possible input permutations, (ii) an O(lg n)-depth hypercubic comparator network
Author: Tom Leighton C. Greg Plaxton 
Address: Cambridge, MA 02139.  
Affiliation: Department of Mathematics and Laboratory for Computer Science, Massachusetts Institute of Technology,  
Note: (i) a comparator network of depth c lg n, c 7:44, that sorts the vast majority of  such algorithms have been previously discovered, but this algorithm has a signif  AMS subject classifications. 68P10, 68Q22, 68Q25, 68R05  Supported by AFOSR Contract F49620-92-J-0125, DARPA Contract N00014-91-J-1698, and DARPA Contract N00014-92-J-1799.  
Date: May 17, 1994  
Abstract: This paper provides an analysis of a natural d-round tournament over n = 2 d players, and demonstrates that the tournament possesses a surprisingly strong ranking property. The ranking property of this tournament is used to design efficient sorting algorithms for a variety of different models of parallel computation: y Department of Computer Science, University of Texas at Austin, Austin, TX 78712. Email: plaxton@cs.utexas.edu. Supported by NSF Research Initiation Award CCR-9111591, and the Texas Advanced Research Program under Grant No. 003658-480. Part of this work was done while the author was visiting the MIT Laboratory for Computer Science. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Aiello, F. T. Leighton, B. Maggs, and M. Newman. </author> <title> Fast algorithms for bit-serial routing on a hypercube. </title> <booktitle> In Proceedings of the 2nd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 55-64, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The only previous result of this kind that does not rely on the AKS sorting network is the recent work of Aiello, Leighton, Maggs, and Newman <ref> [1] </ref>, which provides a randomized bit-serial routing algorithm that runs in optimal time with very high probability on the hypercube. <p> Notational Conventions Our type conventions and defined constants are summarized in Tables 1 and 2, respec 5 Symbol Type Symbol Type a; b; d; i; j; k; m; n integer ff; fi binary string c real constant * empty string f; g; h function permutation p; q real number in <ref> [0; 1] </ref> set of permutations u; v; w; z real number 0-1 vector x; y various set of 0-1 vectors A; B; C set o; !; fi; asymptotic symbol E probabilistic event summation symbol X; Y random variable fl c ; c ; c defined constant D probability distribution other Greek <p> Let R (d) and R (d; a) denote the uniform distributions over (d) and (d; a), respec tively. For all p in <ref> [0; 1] </ref>, let R (d; p) denote the distribution that assigns probability p k (1 p) pow (d)k to each 0-1 d-vector in (d; k). Let 0 R (d; k) denote the uniform distribution over (d; k). <p> D (resp., D 0 ) is the probability distribution over (d) that assigns probability p i (resp., p 0 i ) to the d-vector bin (i; d), 0 i &lt; pow (d) = n, then define D D 0 if and only if there exist real numbers x ij in <ref> [0; 1] </ref>, 0 i &lt; n, 0 j &lt; n, such that: 9 (i) 0i&lt;n x ij = 1, 0 j &lt; n, i = 0j&lt;n x ij p j , 0 i &lt; n, and (iii) x ij &gt; 0 only if bin (j; d) bin (i; d). <p> One may easily verify that for each binary string ff, the following conditions hold: (i) ff (0) = 0, (ii) ff (1) = 1, (iii) ff (p) is a monotonically increasing for p in <ref> [0; 1] </ref>, (iv) ff (p) is a degree-pow (jffj) polynomial in p. Conditions (i), (ii), and (iii) imply that ff (p) is in [0; 1] for all p in [0; 1]. <p> binary string ff, the following conditions hold: (i) ff (0) = 0, (ii) ff (1) = 1, (iii) ff (p) is a monotonically increasing for p in <ref> [0; 1] </ref>, (iv) ff (p) is a degree-pow (jffj) polynomial in p. Conditions (i), (ii), and (iii) imply that ff (p) is in [0; 1] for all p in [0; 1]. Lemma 5.2 Output wire j of a 0-1 no-elimination (d; p)-tournament receives a 0 with probability ff (p), where ff = bin (j; d). <p> hold: (i) ff (0) = 0, (ii) ff (1) = 1, (iii) ff (p) is a monotonically increasing for p in <ref> [0; 1] </ref>, (iv) ff (p) is a degree-pow (jffj) polynomial in p. Conditions (i), (ii), and (iii) imply that ff (p) is in [0; 1] for all p in [0; 1]. Lemma 5.2 Output wire j of a 0-1 no-elimination (d; p)-tournament receives a 0 with probability ff (p), where ff = bin (j; d). <p> We can easily calculate that ff (1=2) 0:796 and fi (1=2) 0:882, suggesting that the player with record ff should be rated above the player with record fi. 16 Lemma 5.3 For all binary strings ff and fi, and all p in <ref> [0; 1] </ref>, fiff (p) = ff ( fi (p)): Proof: For ff = *, the result is immediate since * (p) = p. For jffj &gt; 0, we prove the result by induction on jffj. <p> In particular, for any binary string ff, we define ff (z) to be the function such that ff ( ff (p)) = p for all p in <ref> [0; 1] </ref>. Unlike ff , ff is not a polynomial for jffj 1. However, like ff , there is a simple inductive scheme for computing ff . This is demonstrated by the following lemma. Lemma 5.4 For all binary strings ff, and all z in [0; 1], * (z) = z; <p> p for all p in <ref> [0; 1] </ref>. Unlike ff , ff is not a polynomial for jffj 1. However, like ff , there is a simple inductive scheme for computing ff . This is demonstrated by the following lemma. Lemma 5.4 For all binary strings ff, and all z in [0; 1], * (z) = z; q 1ff (z) = ff (z): Proof: Since * (p) = p for all p in [0; 1], * is the identity function, and thus * is also the identity function. Hence * (z) = z for all z in [0; 1]. <p> This is demonstrated by the following lemma. Lemma 5.4 For all binary strings ff, and all z in <ref> [0; 1] </ref>, * (z) = z; q 1ff (z) = ff (z): Proof: Since * (p) = p for all p in [0; 1], * is the identity function, and thus * is also the identity function. Hence * (z) = z for all z in [0; 1]. By Lemma 5.3, we have 0ff (p) = ff ( 0 (p)) 17 for all p in [0; 1]. <p> and all z in <ref> [0; 1] </ref>, * (z) = z; q 1ff (z) = ff (z): Proof: Since * (p) = p for all p in [0; 1], * is the identity function, and thus * is also the identity function. Hence * (z) = z for all z in [0; 1]. By Lemma 5.3, we have 0ff (p) = ff ( 0 (p)) 17 for all p in [0; 1]. <p> (p) = p for all p in <ref> [0; 1] </ref>, * is the identity function, and thus * is also the identity function. Hence * (z) = z for all z in [0; 1]. By Lemma 5.3, we have 0ff (p) = ff ( 0 (p)) 17 for all p in [0; 1]. <p> The proof that 1ff (z) = q ff (z) proceeds in a similar fashion. By Lemma 5.3, we have 1ff (p) = ff ( 1 (p)) for all p in <ref> [0; 1] </ref>. Setting p = 1ff (z), we find that ff ( 1ff (z) 2 ) = 1ff ( 1ff (z)) = ff ( ff (z)): Since ff is a monotonically increasing function, we have 1ff (z) 2 = ff (z) and thus 1ff (z) = ff (z); as desired. <p> Note that ff (0) = 0 and ff (1) = 1 for all binary strings ff. The following lemma is analogous to Lemma 5.3. Lemma 5.5 For all binary strings ff and fi, and all z in <ref> [0; 1] </ref>, fiff (z) = fi ( ff (z)): 18 Proof: Since * is the identity function, the result is immedate for fi = *. For jfij &gt; 0, we prove the result by induction on jfij. <p> These definitions are used primarily in Sections 5.4 and 5.5, but also appear in subsequent sections. For all x &lt; y in <ref> [0; 1] </ref>, 1, and d 0, let y (1 x) ; (3) ( ff (x); ff (y)) ; (4) X ff:jffj=d h ff (x; y) ; (5) 0&lt;x<y&lt;1 h 0 (x; y) + h 1 (x; y) ; (6) 1 + 1 c = sup 1 lg () = lg (4 <p> h 0 (x; y) + h 1 (x; y) ; (6) 1 + 1 c = sup 1 lg () = lg (4 2 2) 0:228; and (8) p Informally, we think of (x; y) as a measure of the "distance" between x and y for x &lt; y in <ref> [0; 1] </ref>. The function h ff (x; y) may then be viewed as the fractional decrease in the distance between x and y that results from applying ff to both x and y. <p> Lemma 5.13 shows that for certain small values of ", the difference ff (1 ") ff (") is small for most binary strings ff. Lemma 5.8 For all x &lt; y in <ref> [0; 1] </ref>, y x (x; y)=2: Proof: Define (z) = 4 z z for z in [0; 1]. Since d (z) = 4 z 1 for z in [0; 1], we know that (z) is a non-decreasing function of z. <p> Lemma 5.13 shows that for certain small values of ", the difference ff (1 ") ff (") is small for most binary strings ff. Lemma 5.8 For all x &lt; y in <ref> [0; 1] </ref>, y x (x; y)=2: Proof: Define (z) = 4 z z for z in [0; 1]. Since d (z) = 4 z 1 for z in [0; 1], we know that (z) is a non-decreasing function of z. <p> Lemma 5.8 For all x &lt; y in <ref> [0; 1] </ref>, y x (x; y)=2: Proof: Define (z) = 4 z z for z in [0; 1]. Since d (z) = 4 z 1 for z in [0; 1], we know that (z) is a non-decreasing function of z. Hence, (x; y) (y x) = y (1x) 4 lg e = (y) (x) and thus y x 4 lg e Lemma 5.9 For all x &lt; y in [0; 1], we have: (i) h * (x; y) = <p> d (z) = 4 z 1 for z in <ref> [0; 1] </ref>, we know that (z) is a non-decreasing function of z. Hence, (x; y) (y x) = y (1x) 4 lg e = (y) (x) and thus y x 4 lg e Lemma 5.9 For all x &lt; y in [0; 1], we have: (i) h * (x; y) = 1, and (ii) for all binary strings ff and fi, h fiff (x; y) = h fi ( ff (x); ff (y)) h ff (x; y): Proof: Since * is the identity function, h * (x; y) = 1 for all <p> h * (x; y) = 1, and (ii) for all binary strings ff and fi, h fiff (x; y) = h fi ( ff (x); ff (y)) h ff (x; y): Proof: Since * is the identity function, h * (x; y) = 1 for all x &lt; y in <ref> [0; 1] </ref>. <p> Lemma 5.12 For all x &lt; y in <ref> [0; 1] </ref>, 1, and d 0, we have H (x; y; d) () d : Proof: For d = 0, the result is immediate since H (x; y; 0) = 1. <p> = (1 2 ") pow ((fl 1) d)=4. (Note that 0 &lt; ffi &lt; 1=4 since (fl; "; d) is an admissible triple.) We remark that, using Lemma 5.4, ff (z) can be computed in O (jffj) arithmetic operations (counting square root as a single operation) for any z in <ref> [0; 1] </ref>. <p> This occurs with probability at least 1 (n + 1) "(d) = 1 pow ( pow (g (d))), as required. Lemma 6.1 For all a, b, and d such that 0 b a d, and all " and " 0 in <ref> [0; 1] </ref>, we have Sort D (d; a; " + 2 " 0 ) Sort D (d; a; b; ") + Sort D (d; b; " 0 ) + 2 Merge D (d; b; 1): Proof: We may assume that a &gt; b, since the claim is trivial otherwise. <p> Lemmas 4.3, 4.6, and 6.1 together imply that Sort D (d; a; ") Sort D (d; a; b; ") + O (b): (10) for all a, b, and d such that 0 b a d, and all " in <ref> [0; 1] </ref>. Lemma 6.2 For all a, b, and d such that 0 b a d, and all " in [0; 1], we have Sort D (d; a; b + 1; ") Most D (d; a; b; ") + Insert D (d; a b): Proof: We argue that a (d; a)-network <p> D (d; a; ") Sort D (d; a; b; ") + O (b): (10) for all a, b, and d such that 0 b a d, and all " in <ref> [0; 1] </ref>. Lemma 6.2 For all a, b, and d such that 0 b a d, and all " in [0; 1], we have Sort D (d; a; b + 1; ") Most D (d; a; b; ") + Insert D (d; a b): Proof: We argue that a (d; a)-network in Sort N (d; a; b + 1; ") can be constructed by composing: (a) any (d; a)-network in Most <p> Hence A has a dirty region of size at most 2 pow (b) = pow (b + 1) after stage (d), as desired. Lemma 6.3 For all a, b, and d such that 0 b a d, and all fl, ", and " 0 in <ref> [0; 1] </ref>, we have Sort D (d; a; b 0 + 3; " + 2 " 0 ) Sort D (d; a; b; ")+Sort D (d; b + 2; b 0 + 1; " 0 )+Merge D (d; b b 0 ; 1); where b 0 = bfl (b + 2)c. <p> 6.3, we find that Sort D (d; a; b (fl c + o (1)) (b + 2)c + 3; pow ( pow (o (b))) + " 0 ) Sort D (d; a; b; " 0 ) + (3 2 fl c + o (1)) b; for all " 0 in <ref> [0; 1] </ref>. <p> Proof: Similar to the proof of Corollary 2.1. Lemma 7.1 For all a, b and d such that 0 b a d, and all " and " 0 in <ref> [0; 1] </ref>, we have Sort h D (d; a; " + 2 " 0 ) Sort h D (d; a; b; ") + Sort h D (d; b; " 0 ) + 2 Merge h D (d; b; 1) + O (a): Proof: Similar to the proof of Lemma 6.1. <p> This accounts for the additive O (a) term on the RHS of the inequality. Lemma 7.2 For all a, b and d such that 0 b a d, and all " in <ref> [0; 1] </ref>, we have Sort h D (d; a; b + 1; ") Most h D (d; a; b; ") + Insert h D (d; a b) + O (a): Proof: Similar to the proof of Lemma 6.2. <p> Lemmas 4.3 and 7.1 (with b = bfl ac + 1) now give Sort h D (d; a; pow ( pow (fi (a))) + 2 " 0 ) Sort h D (d; bfl ac + 1; " 0 ) + O (a): for all " 0 in <ref> [0; 1] </ref>. <p> 1; O (pow (3 a=2) "(a))) = O (a): Lemmas 4.3 and 7.1 now give Sort h D (d; a; O (pow (3 a=2) "(a)) + 2 " 0 ) Sort h D (d; bfl (a) ac + 1; " 0 ) + O (a); for all " 0 in <ref> [0; 1] </ref>. Iteratively applying the preceding inequality, we find that Sort h D (d; a; O (pow (3 b=2) "(b))) Sort h D (d; b; 0) + O (a -(a)) for all a, b, and d such that 0 b a d.
Reference: [2] <author> M. Ajtai, J. Komlos, and E. Szemeredi. </author> <title> Sorting in c log n parallel steps. </title> <journal> Combinatorica, </journal> <volume> 3 </volume> <pages> 1-19, </pages> <year> 1983. </year>
Reference-contexts: The source of the difficulty of this particular exercise was subsequently revealed by Ajtai, Komlos, and Szemeredi <ref> [2] </ref>, who provided an optimal O (lg n)-depth construction known as the AKS sorting network. While the AKS sorting network represents a major theoretical breakthrough, it suffers from two significant shortcomings. First, the multiplicative constant hidden within the O-notation is sufficiently large that the result remains impractical. <p> A hypercubic version of the construction is discussed in the next paragraph.) At the expense of allowing the network to fail on a small fraction of the n! possible input permutations, this construction improves upon the asymptotic depth of the best previously known sorting networks by several orders of magnitude <ref> [2, 14] </ref>. We make use of the AKS construction as part of our network. <p> Lemma 4.6 For all a and d such that 0 a d, we have Sort D (d; a; 0) = O (a): Proof: This bound is due to Ajtai, Komlos and Szemeredi <ref> [2] </ref>. The constant factor associated with the AKS sorting network is impractically large.
Reference: [3] <author> K. E. Batcher. </author> <title> Sorting networks and their applications. </title> <booktitle> In Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <volume> vol. 32, </volume> <pages> pages 307-314, </pages> <year> 1968. </year>
Reference-contexts: Hence the well-known (n lg n) sequential lower bound for comparison-based sorting implies an (lg n) lower bound on the depth of any n-input sorting network. An elegant O (lg 2 n)-depth upper bound is given by Batcher's bitonic sorting network <ref> [3] </ref>. For small values of n, the depth of bitonic sort either matches or is very close to matching that of the best constructions known (a very limited number of which are known to be optimal) [10, Section 5.3.4]. <p> Lemma 4.3 For all a and d such that 0 &lt; a d, we have Merge D (d; a 1; 1) a; Merge h D (d; a 1; 1) = O (a): Proof: These bounds are established by Batcher's bitonic merge network <ref> [3] </ref>. <p> Lemma 4.4 For all a and d such that 0 a d, we have Insert D (d; a) a; Insert h D (d; a) = O (a): Proof: This bound is also established by Batcher's bitonic merge network <ref> [3] </ref>. <p> The basic idea is to use a tree-like network.) Lemma 4.5 For all a and d such that 0 a d, we have Sort h D (d; a; 0) = O (a 2 ): Proof: This bound is due to Batcher <ref> [3] </ref>, and follows from repeated application of Lemma 4.3. Lemma 4.6 For all a and d such that 0 a d, we have Sort D (d; a; 0) = O (a): Proof: This bound is due to Ajtai, Komlos and Szemeredi [2].
Reference: [4] <author> V. E. </author> <title> Benes. Optimal rearrangeable multistage connecting networks. </title> <journal> Bell System Technical Journal, </journal> <volume> 43 </volume> <pages> 1641-1656, </pages> <year> 1964. </year>
Reference-contexts: Furthermore, the gate assignments of N can be computed in time polynomial in pow (a). 13 Proof: This is a straightforward consequence of the work of Benes <ref> [4] </ref>. In particular, for a = d, the Benes permutation network corresponds to a hypercubic d-network satisfying properties (i), (iii), and (v).
Reference: [5] <author> H. Chernoff. </author> <title> A measure of the asymptotic efficiency for tests of a hypothesis based on the sum of observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23 </volume> <pages> 493-509, </pages> <year> 1952. </year>
Reference-contexts: Note that, unlike the p fi 's, each q ff is a random variable. Furthermore, the random variable X ff is easily seen to be the sum of pow (b) independent Bernoulli trials, where trial fi has success probability p fi . Thus, a standard Chernoff-type argument <ref> [5] </ref> implies PrfjX ff p pow (b)j # pow (b)g 2 e 2# 2 pow (b) (15) for all # 0. Define a random execution to be ffi-balanced if p ffi q ff p + ffi for all ff.
Reference: [6] <author> R. E. Cypher. </author> <title> Theoretical aspects of VLSI pin limitations. </title> <journal> SIAM J. Comput., </journal> <volume> 22 </volume> <pages> 58-63, </pages> <year> 1993. </year>
Reference-contexts: Second, the structure of the network is sufficiently "irregular" that it does not seem to map efficiently to common interconnection schemes. In fact, Cypher has proven that any emulation of the AKS network on the cube-connected cycles requires (lg 2 n) time <ref> [6] </ref>. The latter issue is of significant interest, since a primary motivation for considering the problem of constructing small-depth sorting networks is to obtain a fast parallel sorting algorithm for a general-purpose parallel computer.
Reference: [7] <author> R. E. Cypher and C. G. Plaxton. </author> <title> Techniques for shared key sorting. </title> <type> Technical report, </type> <institution> IBM Almaden Research Center, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: A small improvement to the Sharesort bound is known when polynomial-time "preprocessing" (to compute certain look-up tables) is allowed. In particular, the running time of Sharesort can be improved to O (d (lg d) lg fl d) in that case <ref> [7] </ref>. This improvement has been incorporated into the "(d) bound of Theorem 6. If exponential pre-processing is allowed, the running time of Sharesort can be improved further to O (d lg d) [7]. <p> running time of Sharesort can be improved to O (d (lg d) lg fl d) in that case <ref> [7] </ref>. This improvement has been incorporated into the "(d) bound of Theorem 6. If exponential pre-processing is allowed, the running time of Sharesort can be improved further to O (d lg d) [7]. However, it is not clear whether the latter result could be used to improve the "(d) bound of Theorem 6. (The lack of uniformity can be eliminated through randomization.
Reference: [8] <author> R. E. Cypher and C. G. Plaxton. </author> <title> Deterministic sorting in nearly logarithmic time on the hypercube and related computers. </title> <journal> JCSS, </journal> <volume> 47 </volume> <pages> 501-548, </pages> <year> 1993. </year>
Reference-contexts: of our sorting algorithm is non-adaptive in the sense that it can be described solely in terms of oblivious routing and compare-interchange operations; there is no queueing. (The very 4 high probability version is adaptive because it makes use of the Sharesort algorithm of Cypher and Plaxton as a subroutine <ref> [8] </ref>.) Note that the permutation routing problem, in which each processor has a packet of information to send to another processor, and no two packets are destined to the same processor, is trivially reducible to the sorting problem. (The idea is to sort the packets based on their destination addresses.) Hence, <p> The scheme of Section 7 can also be used to prove Theorem 5 below with the function " as defined in Theorem 6. In this case, we can dramatically decrease the failure probability probability by making use of the Sharesort algorithm of Cypher and Plaxton <ref> [8] </ref>. Sharesort is a polynomial-time uniform hypercubic sorting algorithm with worst-case running time O (d lg 2 d) [8]. Note that Sharesort runs in O (d) time on O (d= lg 2 d)-cubes. <p> In this case, we can dramatically decrease the failure probability probability by making use of the Sharesort algorithm of Cypher and Plaxton <ref> [8] </ref>. Sharesort is a polynomial-time uniform hypercubic sorting algorithm with worst-case running time O (d lg 2 d) [8]. Note that Sharesort runs in O (d) time on O (d= lg 2 d)-cubes. Hence, we can modify the scheme of Section 7 by cutting off the sorting recurrence at fi (d= lg 2 d)-cubes instead of fi ( p d)-cubes (as allowed by bitonic sort).
Reference: [9] <author> W. Hoeffding. </author> <title> On the distribution of the number of successes in independent trials. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 27 </volume> <pages> 713-721, </pages> <year> 1956. </year>
Reference: [10] <author> D. E. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> volume 3. </volume> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: In fact, any n-input comparator network that sorts the 2 n possible 0-1 vectors of length n is a sorting network. The latter result is known as the 0-1 principle for sorting networks <ref> [10, Section 5.3.4] </ref>. It is natural to consider the problem of constructing sorting networks of optimal depth. Note that at most bn=2c comparisons can be performed at any given level of a comparator network. <p> For small values of n, the depth of bitonic sort either matches or is very close to matching that of the best constructions known (a very limited number of which are known to be optimal) <ref> [10, Section 5.3.4] </ref>. Thus, one might suspect the depth of Batcher's bitonic sorting network to be optimal to within a constant factor, or perhaps even to within a lower-order additive term. <p> Lemma 4.1 For any d-network N , we have Z (d) Sort (N ) () (d) Sort (N ): Proof: This lemma is known as the 0-1 principle for sorting networks, and is proven in <ref> [10, Section 5.3.4] </ref>. (The proof is given in the context of deterministic networks.
Reference: [11] <author> F. T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, and Hypercubes. </title> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: This correspondence allows any shu*e-only comparator network to be efficiently emulated (i.e., with constant slowdown) on any hypercubic machine. (We remark that "hypercubic machines" are more commonly referred to as "hy-percubic networks" <ref> [11, Chapter 3] </ref>. We prefer the term "hypercubic machines" in the present context only because we use the term "networks" to refer to comparator networks.) However, the class of hypercubic machines is most often defined in terms of efficient emulation of so-called "normal" algorithms [11, Chapter 3], which effectively allow the <p> commonly referred to as "hy-percubic networks" <ref> [11, Chapter 3] </ref>. We prefer the term "hypercubic machines" in the present context only because we use the term "networks" to refer to comparator networks.) However, the class of hypercubic machines is most often defined in terms of efficient emulation of so-called "normal" algorithms [11, Chapter 3], which effectively allow the data to either be shu*ed or unshu*ed at each step. Thus, hypercubic comparator networks, as defined above, would seem to represent the most natural class of comparator networks corresponding to hypercubic machines. <p> In fact, standard reductions <ref> [11, Section 3.4.3] </ref> allow us to apply our sorting algorithm to efficiently solve a variety of other routing problems as well (e.g., many-to-one routing with combining). <p> the input is already in bitonic form. (We remark that in the classic sorting network model, where a given level may contain fewer than pow (d 1) gates, it is possible to match this depth bound while achieving size pow (d) 1 instead of d pow (d) [13] (see also <ref> [11, Section 3.5.4] </ref>). <p> We then use this coin-tossing network to develop a polynomial-time uniform hypercubic "algorithm" that sorts every d-vector in O (d) time with high probability. We define a hypercubic algorithm as any normal hypercube algorithm. (See <ref> [11, Section 3.1.3] </ref>, for example, for a definition of the class of normal hypercube algorithms). Every depth-a hypercubic sorting d-network corresponds to a (possibly non-uniform) hypercubic sorting algorithm that runs in O (a) time on any pow (d)-processors hypercubuc machine. <p> Omega machines belong to the class of butterfly-like machines discussed in <ref> [11, Section 3.8.1] </ref>. Observe that there is a close correspondence between an order-d omega machine M and a depth-d shu*e d-network N .
Reference: [12] <author> F. T. Leighton, B. M. Maggs, A. G. Ranade, and S. B. Rao. </author> <title> Randomized routing and sorting on fixed-connection networks. </title> <journal> Journal of Algorithms, </journal> <note> to appear. </note>
Reference-contexts: Probability of failure aside, Flashsort requires more storage than our algorithm, since it makes use of a fi (lg n)-sized priority queue at each processor. On the other hand, a very high probability sorting algorithm with constant size queues has previously been given by Leighton, Maggs, Ranade, and Rao <ref> [12] </ref>. <p> In fact, standard reductions [11, Section 3.4.3] allow us to apply our sorting algorithm to efficiently solve a variety of other routing problems as well (e.g., many-to-one routing with combining). Interestingly, all previously known optimal-time algorithms for permutation routing on hypercubic machines <ref> [12, 17, 19] </ref> are randomized, and do not achieve a success probability better than "very high". Thus, the results of Section 10 provide a permutation routing algorithm for hypercubic machines with a much smaller probability of failure than any previously known O (lg n)-time algorithm.
Reference: [13] <author> F. T. Leighton and C. G. Plaxton. </author> <title> A (fairly) simple circuit that (usually) sorts. </title> <booktitle> In Proceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 264-274, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Note that Section 5.3 contains a number of important technical definitions related to the no-elimination tournament. Sections 6 through 11 present the applications of the no-elimination tournament discussed Section 2. Section 12 offers some concluding remarks. The results of this paper first appeared in preliminary form in <ref> [13] </ref> and [15]. 2 Overview of Applications In Sections 6 through 11 of the paper, we use the strong ranking property of the no-elimination tournament to design efficient sorting algorithms for a variety of different models of parallel computation. <p> is required since the input is already in bitonic form. (We remark that in the classic sorting network model, where a given level may contain fewer than pow (d 1) gates, it is possible to match this depth bound while achieving size pow (d) 1 instead of d pow (d) <ref> [13] </ref> (see also [11, Section 3.5.4]).
Reference: [14] <author> M. S. Paterson. </author> <title> Improved sorting networks with O(log N ) depth. </title> <journal> Algorithmica, </journal> <volume> 5 </volume> <pages> 75-92, </pages> <year> 1990. </year>
Reference-contexts: A hypercubic version of the construction is discussed in the next paragraph.) At the expense of allowing the network to fail on a small fraction of the n! possible input permutations, this construction improves upon the asymptotic depth of the best previously known sorting networks by several orders of magnitude <ref> [2, 14] </ref>. We make use of the AKS construction as part of our network. <p> A similar technique has recently been used by Ajtai, Komlos, and Szemeredi as part of an improved version of their original sorting network construction. The multiplicative constant associated with the new construction is significantly lower than the constant established by Paterson <ref> [14] </ref>, though it remains impractical.
Reference: [15] <author> C. G. Plaxton. </author> <title> A hypercubic sorting network with nearly logarithmic depth. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 405-416, </pages> <month> May </month> <year> 1992. </year> <month> 45 </month>
Reference-contexts: Note that Section 5.3 contains a number of important technical definitions related to the no-elimination tournament. Sections 6 through 11 present the applications of the no-elimination tournament discussed Section 2. Section 12 offers some concluding remarks. The results of this paper first appeared in preliminary form in [13] and <ref> [15] </ref>. 2 Overview of Applications In Sections 6 through 11 of the paper, we use the strong ranking property of the no-elimination tournament to design efficient sorting algorithms for a variety of different models of parallel computation.
Reference: [16] <author> C. G. Plaxton and T. Suel. </author> <title> A lower bound for sorting networks based on the shu*e permutation. </title> <booktitle> In Proceedings of the 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 70-79, </pages> <month> June </month> <year> 1992. </year> <note> To appear in Mathematical Systems Theory. </note>
Reference-contexts: Batcher's bitonic sort provides an O (lg 2 n) upper bound for this problem, and recently, Plaxton and Suel <ref> [16] </ref> have established an (lg 2 n= lg lg n) lower bound. (The same lower bound holds for the class of unshu*e-only sorting networks.) From a practical point of view, Knuth's shu*e-only requirement would seem to be overly-restrictive. <p> exist hypercubic sorting networks of depth 2 p Note that this bound is o (lg 1+" n) for any constant " &gt; 0. (See Theorem 4 for a more precise form of the upper bound.) Given the aforementioned (lg 2 n= lg lg n) lower bound of Plaxton and Suel <ref> [16] </ref>, our upper bound establishes a surprisingly strong separation between the power of shu*e-only comparator networks and that of hypercubic comparator networks. In Section 10, an optimal O (lg n)-time randomized sorting algorithm is given for any hypercubic machine.
Reference: [17] <author> A. G. Ranade. </author> <title> How to emulate shared memory. </title> <journal> JCSS, </journal> <volume> 42 </volume> <pages> 307-326, </pages> <year> 1991. </year>
Reference-contexts: In fact, standard reductions [11, Section 3.4.3] allow us to apply our sorting algorithm to efficiently solve a variety of other routing problems as well (e.g., many-to-one routing with combining). Interestingly, all previously known optimal-time algorithms for permutation routing on hypercubic machines <ref> [12, 17, 19] </ref> are randomized, and do not achieve a success probability better than "very high". Thus, the results of Section 10 provide a permutation routing algorithm for hypercubic machines with a much smaller probability of failure than any previously known O (lg n)-time algorithm.
Reference: [18] <author> J. H. Reif and L. G. Valiant. </author> <title> A logarithmic time sort for linear size networks. </title> <journal> JACM, </journal> <volume> 34 </volume> <pages> 60-76, </pages> <year> 1987. </year>
Reference-contexts: essentially no auxiliary variables. (A global OR operation involving a single bit at each processor is used to check whether the sort has been completed.) A number of optimal time randomized sorting algorithms were previously known for certain hypercubic machines. 4 For example, the Flashsort algorithm of Reif and Valiant <ref> [18] </ref> is in this category. However, none of these algorithms has a success probability better than "very high". Probability of failure aside, Flashsort requires more storage than our algorithm, since it makes use of a fi (lg n)-sized priority queue at each processor.
Reference: [19] <author> L. G. Valiant and G. J. Brebner. </author> <title> Universal schemes for parallel communication. </title> <booktitle> In Proceedings of the 13th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 263-277, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: In fact, standard reductions [11, Section 3.4.3] allow us to apply our sorting algorithm to efficiently solve a variety of other routing problems as well (e.g., many-to-one routing with combining). Interestingly, all previously known optimal-time algorithms for permutation routing on hypercubic machines <ref> [12, 17, 19] </ref> are randomized, and do not achieve a success probability better than "very high". Thus, the results of Section 10 provide a permutation routing algorithm for hypercubic machines with a much smaller probability of failure than any previously known O (lg n)-time algorithm.
References-found: 19

