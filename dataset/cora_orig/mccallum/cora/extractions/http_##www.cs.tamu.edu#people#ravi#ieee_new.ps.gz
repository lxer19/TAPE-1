URL: http://www.cs.tamu.edu/people/ravi/ieee_new.ps.gz
Refering-URL: http://www.cs.tamu.edu/people/ravi/
Root-URL: http://www.cs.tamu.edu
Email: Email: (bhuyan,ravi)@cs.tamu.edu  
Title: Performance of Multistage Bus Networks for a Distributed Shared Memory Multiprocessor  
Author: Laxmi N. Bhuyan, Ravi R. Iyer, Tahsin Askar, Ashwini K. Nanda and Mohan Kumar L. N. Bhuyan and R. Iyer 
Keyword: interconnection network, routing, queueing model, performance analysis, packet-switching, execution driven simulation  
Address: College Station, TX 77843-3112,  Austin, TX 78741  P.O. Box 218, Yorktown Heights, NY 10598  Box U 1987 Perth, WA 6001, Australia  
Affiliation: Department of Computer Science, Texas A&M University,  Devices,  IBM TJ Watson Research Center,  Department of Computer Science, Curtin University of Technology, GPO  
Note: This research was supported by NSF grants MIP-9301959 and 9622740  are with the  T. Askar is with Advanced Micro  A. Nanda is with  M. Kumar is with the  
Abstract: A Multistage Bus Network (MBN) is proposed in this paper to overcome some of the shortcomings of the conventional multistage interconnection networks (MINs), single bus and hierarchical bus interconnection networks. The MBN consists of multiple stages of buses connected in a manner similar to the MINs and has the same bandwidth at each stage. A switch in an MBN is similar to that in a MIN switch except that there is a single bus connection instead of a crossbar. MBNs support bidirectional routing and there exists a number of paths between any source and destination pair. In this paper we develop self routing techniques for the various paths, present an algorithm to route a request along the path with minimum distance and analyze the probabilities of a packet taking different routes. Further, we derive a performance analysis of a synchronous packet-switched MBN in a distributed shared memory environment and compare the results with those of an equivalent bidirectional MIN (BMIN). Finally, we present the execution time of various applications on the MBN and the BMIN through an execution-driven simulation. We show that the MBN provides similar performance to a BMIN while offering simplicity in hardware and more fault-tolerance than a conventional MIN. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.W. Wilson, </author> <title> "Hierarchical cache/bus architecture for shared memory multiprocessors," </title> <booktitle> Proc.14th Ann. Int'l. Symp. on Comp. Arch., </booktitle> <pages> pp. 244-252, </pages> <year> 1987. </year> <type> [2] "KSR1 Technical Summary," </type> <institution> Kendall Square Research Corp., </institution> <year> 1992. </year>
Reference: [3] <author> BBN Laboratories Inc., </author> <title> "Butterfly Parallel Processor Overview, </title> <type> version 1," </type> <month> Dec. </month> <year> 1985. </year>
Reference-contexts: A reply from memory travels in the opposite direction through the same path in the MBN or BMIN. It may be noted that in case of a MIN like Butterfly <ref> [3] </ref>, a reply has to traverse in the same direction (i.e., from processor to memory side) to reach the requesting processor because the MIN has unidirectional links. <p> Figures 11 and 12 show a comparison of the performance of the MBN to that of the conventional MIN (CMIN) and the proposed bidirectional MIN (BMIN). Conventional MIN is similar to the network employed in the Butterfly machine <ref> [3] </ref>, where both the request and the response packets travel in one direction from processor to the memory side. On the other hand, BMINs allow all the four routings proposed for the MBN.
Reference: [4] <author> C. B. Stunkel, D. G. Shea, D. G. Grice, P. H. Hochschild and M. Tsao, </author> <title> "The SP1 high-performance switch," </title> <booktitle> Proc. 1994 Scalable High-Performance Computing Conference, </booktitle> <pages> pp. 150-157, </pages> <month> May </month> <year> 1994. </year>
Reference: [5] <author> C. E. Leiserson etal, </author> <title> "The Network Architecture of the Connection Machine CM-5," </title> <booktitle> Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 272-285, </pages> <year> 1992. </year>
Reference: [6] <author> L. N. Bhuyan, and D. P. Agrawal, </author> <title> "Design and performance of generalized interconnection networks," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol C-32, </volume> <pages> pp. 1081-1090, </pages> <month> Dec. </month> <year> 1983. </year>
Reference: [7] <author> L. N. Bhuyan and A. K. Nanda, </author> <title> "Multistage bus network (MBN) : An interconnection network for cache coherent multiprocessors," </title> <booktitle> Proc. of the 3rd IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> Dec. </month> <year> 1991. </year>
Reference: [8] <author> L.N. Bhuyan, A.K. Nanda, and T. Askar, </author> " <title> Performance and Reliability of the Multistage Bus Network," </title> <booktitle> Proc. of the 1994 International Conference on Parallel Processing, </booktitle> <pages> pp. 26-33. </pages>
Reference-contexts: In a number of applications, a large portion of the requests are made to the cluster processors. In <ref> [8] </ref>, we studied the performance of the MBN with varying probabilities for cluster requests. In the study forward-U and backward-U routings were allowed only at the first and last stages. All other requests were routed by forward (FW) routing. <p> This increase in performance is due to a lower contention in the crossbar switch. However, the BMIN gives this increased performance at the expense of cost. In <ref> [8] </ref>, a cost parameter based on the number of connection points in a switch is presented. The number of connections is k 2 for a k fi k switch, where as for a bus, the number of connections is 2k. <p> Thus, the total cost of BMIN and MBN are kN log k N and 2N log k N respectively. If we include these parameters along with the processor utilization and the response time, the cost-effectiveness of the MBN is higher than that of the BMIN, as shown in <ref> [8] </ref>. A 4 fi 4 switch size works out to be most cost-efficient for different network sizes and workload inputs. VI. Execution-driven Simulation and Results The execution time of an application on a multiprocessor architecture is the ultimate parameter that indicates the performance.
Reference: [9] <author> H. Xu, Y. Gui and L. M. Ni, </author> <title> "Optimal Software Multicast in Wormhole-Routed Multistage Networks," </title> <booktitle> Proc. Supercomputing 1994, </booktitle> <pages> pp. 703-712, </pages> <year> 1994. </year>
Reference-contexts: It may be noted that in case of a MIN like Butterfly [3], a reply has to traverse in the same direction (i.e., from processor to memory side) to reach the requesting processor because the MIN has unidirectional links. In <ref> [9] </ref>, bidirectional links are used between stages and hence the requesting and reply messages may travel in the forward and backward directions respectively.
Reference: [10] <author> E. A. Brewer, C. N. Dellarocas et. al., " PROTEUS: </author> <title> A High-Performance Parallel Architecture Simulator," </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <month> Sept </month> <year> 1991. </year>
Reference-contexts: In order to show that the MBN performs similar to the Bidirectional MIN (BMIN), we study their performance by using an execution-driven simulation of various applications. Our simulator is based on Proteus <ref> [10] </ref>, originally developed at MIT. However, this original simulator modeled the indirect interconnection networks based on an analytical model. We have modified the simulator extensively to exactly model the BMIN and the MBN using 2 fi 2 switches and packet-switching strategy.
Reference: [11] <author> H. Suzuki et. al., </author> <title> "Output-buffer switch architecture for asynchronous transfer mode," </title> <journal> International Journal of Digital and Analog Cabled Systems, </journal> <volume> Vol. 2, </volume> <year> 1989, </year> <pages> pp. 269-276. </pages>
Reference: [12] <author> C. P. Kruskal and M. Snir, </author> <title> "The performance of multistage interconnection networks for multiprocessors," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol C-32, </volume> <pages> pp. 1091-1098, </pages> <month> Dec. </month> <year> 1983. </year>
Reference-contexts: The transmission of request and reply packets goes through the network following the routings given earlier in the paper. We shall assume a synchronous and packet switched system for analyzing the multistage networks. Since a buffer size of four or more gives the same effect as an infinite buffer <ref> [12] </ref>, [13], for simplicity, we shall assume an infinite buffer for MBN and BMIN. The analysis can be extended to finite buffers, but the equations will be fairly complicated [13]. Since our aim here is to analyze the routing schemes, we prefer to give the basic infinite buffer analysis. <p> (considering all stages) l m : average queue length in a memory module d m : average delay in a memory module P u : processor utilization (fraction of time the processor is busy) The performance analysis of the MBNs and BMINs will be carried out under the following assumptions <ref> [12] </ref>, [13]. Packets are generated at each source node by independent and identically distributed random processes. At any point of time a processor is either busy doing some internal computations or is waiting for the response to a memory request.
Reference: [13] <author> J. Ding and L. N. Bhuyan, </author> <title> "Finite Buffer Analysis of Multistage Interconnection Networks," </title> <journal> IEEE Transaction on Computers, </journal> <pages> pp. 243-247, </pages> <month> Feb </month> <year> 1994. </year>
Reference-contexts: We shall assume a synchronous and packet switched system for analyzing the multistage networks. Since a buffer size of four or more gives the same effect as an infinite buffer [12], <ref> [13] </ref>, for simplicity, we shall assume an infinite buffer for MBN and BMIN. The analysis can be extended to finite buffers, but the equations will be fairly complicated [13]. Since our aim here is to analyze the routing schemes, we prefer to give the basic infinite buffer analysis. <p> Since a buffer size of four or more gives the same effect as an infinite buffer [12], <ref> [13] </ref>, for simplicity, we shall assume an infinite buffer for MBN and BMIN. The analysis can be extended to finite buffers, but the equations will be fairly complicated [13]. Since our aim here is to analyze the routing schemes, we prefer to give the basic infinite buffer analysis. The bus service time (for MBN) or the link service time (for BMIN) to transfer a message forms one system cycle time. <p> all stages) l m : average queue length in a memory module d m : average delay in a memory module P u : processor utilization (fraction of time the processor is busy) The performance analysis of the MBNs and BMINs will be carried out under the following assumptions [12], <ref> [13] </ref>. Packets are generated at each source node by independent and identically distributed random processes. At any point of time a processor is either busy doing some internal computations or is waiting for the response to a memory request.
Reference: [14] <author> L. Kleinrock, </author> <title> Queueing Systems volume 1: Theory, </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: The mean number of arriving requests, E = ntq and the variance, V = ntq (1 q). The average queue length Q at the queueing center can be found using the Pollaczek-Khinchine (P-K) mean value formula <ref> [14] </ref>, Q = 2 V (10) The throughput of these requests is E=t.
Reference: [15] <author> L. M. Censier and P. Feautrier, </author> <title> "A New Solution to Coherence Problems in Multicache Systems," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-27, no. 12, </volume> <pages> pp. 1112-1118, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: We have modified the simulator extensively to exactly model the BMIN and the MBN using 2 fi 2 switches and packet-switching strategy. The system considered in this paper has private cache memories that operate based on a directory-based cache-coherence protocol <ref> [15] </ref>. The node configuration and the network 31 Fig. 15. The node configuration and network interface interface in the simulator are modeled as shown in the Figure 15.
Reference: [16] <author> R. Iyer, </author> <title> "Distributed Shared Memory Multiprocessors using Multistage Bus Networks," M.S. </title> <type> Thesis, </type> <institution> Texas A&M University, Dept. of Computer Science, </institution> <year> 1996 </year>
Reference-contexts: An initial value of 0.5 for P u and an accuracy of 0.00001 were used to generate the analytical results, presented in the next section. V. Results and Discussions We performed extensive cycle-by-cycle simulations to verify that the proposed routings work and measured the routing probabilities and network delays <ref> [16] </ref>. The simulation was done using a synchronous packet-switched distributed memory environment. The simulation specifications are the same as the analysis and are detailed below with a view to making the network operation more clear. 25 Fig. 9. <p> Both the cache controller (CC) and memory controller (MC) are connected to the network interface so that they can directly communicate with the other nodes. The system parameters used for the simulation, are given in Table IV. Further details on the simulation can be found in <ref> [16] </ref>, [17]. A. The Benchmark Applications We have selected some numerical applications as the workload for evaluating the network performance in a cache-coherent shared-memory environment.
Reference: [17] <author> A. Kumar and L.N. Bhuyan, </author> <title> "Evaluating virtual channels for cache coherent shared memory multiprocessors," </title> <booktitle> ACM International Conference on Supercomuting, </booktitle> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year> <month> 37 </month>
Reference-contexts: Both the cache controller (CC) and memory controller (MC) are connected to the network interface so that they can directly communicate with the other nodes. The system parameters used for the simulation, are given in Table IV. Further details on the simulation can be found in [16], <ref> [17] </ref>. A. The Benchmark Applications We have selected some numerical applications as the workload for evaluating the network performance in a cache-coherent shared-memory environment. <p> Application BMIN Latency MBN Latency CMIN Latency Matmul 224.48 231.25 538.48 FWA 491.24 506.74 1415.73 LU 162.00 165.93 182.96 TABLE VI Average Message Latencies using different networks and the simulation environment can be found in <ref> [17] </ref>. B. Simulation Results The characteristics of the applications, as measured in the simulation, are given in Table V. The table shows the number of shared memory references, the cache misses on the shared memory references and the total number of messages generated during the execution.

References-found: 16

