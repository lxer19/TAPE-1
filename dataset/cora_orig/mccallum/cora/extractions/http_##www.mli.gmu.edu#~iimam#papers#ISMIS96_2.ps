URL: http://www.mli.gmu.edu/~iimam/papers/ISMIS96_2.ps
Refering-URL: http://www.mli.gmu.edu/~iimam/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: @aic.gmu.edu  
Title: LEARNING FOR DECISION MAKING: The FRD Approach and a Comparative Study  Machine Learning and Inference Laboratory  
Author: Ibrahim F. Imam and Ryszard S. Michalski George Mason 
Note: Also with the  
Address: Fairfax, VA. 22030  iimam, michalski-  
Affiliation: University  Institute of Computer Science, Polish Academy of Sciences  
Abstract: This paper concerns the issue of what is the best form for learning, representing and using knowledge for decision making. The proposed answer is that such knowledge should be learned and represented in a declarative form. When needed for decision making, it should be efficiently transferred to a procedural form that is tailored to the specific decision making situation. Such an approach combines advantages of the declarative representation, which facilitates learning and incremental knowledge modification, and the procedural representation, which facilitates the use of knowledge for decision making. This approach also allows one to determine decision structures that may avoid attributes that unavailable or difficult to measure in any given situation. Experimental investigations of the system, FRD-1, have demonstrated that decision structures obtained via the declarative route often have not only higher predictive accuracy but are also are simpler than those learned directly from facts.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arciszewski, T, Bloedorn, E., Michalski, R., Mustafa, M., and Wnek, J., </author> <title> "Constructive Induction in Structural Design", Reports of Machine Learning and Inference Laboratory, </title> <institution> MLI-92-7, George Mason University, </institution> <year> 1992. </year>
Reference-contexts: Both systems were set to their default parameters. The experiments were divided into two parts. The first part is concerned with designed problems, the MONKs [15], and the second part was concerned with real-world problems: Wind bracing for tall buildings <ref> [1] </ref>, Mushrooms classification, Breast cancer diagnosis. All the results reported here are the average of 100 runs. For each data set, we reported the predictive accuracy, the complexity of the learned decision trees, and the time taken for learning.
Reference: [2] <author> Bloedorn, E., Wnek, J., Michalski, R.S., and Kaufman, K., "AQ17: </author> <title> A Multistrategy Learning System: The Method and Users Guide, Reports of Machine Learning and Inference Laboratory, </title> <institution> MLI-93-12, George Mason University, </institution> <year> 1993. </year>
Reference-contexts: Such a function involves assigning a value to a decision variable based on values of attributes characterizing the decision making situation. Figure 1 shows an architecture of the system, called FRD (from Facts to Rules to Decisions). The decision rules are learned by either the AQ15 [10] or AQ17-DCI <ref> [2] </ref> learning systems. They also can be directly edited into the system. The transformation of decision rules to decision structures is accomplished using the AQDT-2 algorithm.
Reference: [3] <author> Breiman, L., Friedman, J.H., Olshen, R.A. & Stone, C.J., </author> <title> Classification and Regression Structures, </title> <type> Belmont, </type> <institution> California: Wadsworth Int. Group, </institution> <year> 1984. </year>
Reference-contexts: The MAL criterion uses conjunction and disjunction operators. The information-based criteria are based on information theory. These criteria measure the information conveyed by dividing the training examples into subsets. Examples of such criteria include the information gain, an entropy reduction measure [13], the gini index of diversity <ref> [3] </ref>, Gain-ratio measure [14], and others [4, 5]. The statistics-based criteria measure the correlation between the decision classes and the other attributes. Such criteria use statistical distributions in determining whether or not there is a correlation.
Reference: [4] <author> Clark, P. & Niblett, T., </author> <title> Induction in Noisy Domains in I. </title> <editor> Bratko and N. Lavrac, (Eds.), </editor> <booktitle> Progress in Machine Learning, </booktitle> <publisher> Sigma Press, </publisher> <address> Wilmslow, </address> <year> 1987. </year>
Reference-contexts: The information-based criteria are based on information theory. These criteria measure the information conveyed by dividing the training examples into subsets. Examples of such criteria include the information gain, an entropy reduction measure [13], the gini index of diversity [3], Gain-ratio measure [14], and others <ref> [4, 5] </ref>. The statistics-based criteria measure the correlation between the decision classes and the other attributes. Such criteria use statistical distributions in determining whether or not there is a correlation.
Reference: [5] <author> Cestnik, B. & Karalic, A., </author> <title> The Estimation of Probabilities in Attribute Selection Measures for Decision Structure Induction in Proceeding of the European Summer School on Machine Learning, </title> <address> July 22-31, , Belgium, </address> <year> 1991. </year>
Reference-contexts: The information-based criteria are based on information theory. These criteria measure the information conveyed by dividing the training examples into subsets. Examples of such criteria include the information gain, an entropy reduction measure [13], the gini index of diversity [3], Gain-ratio measure [14], and others <ref> [4, 5] </ref>. The statistics-based criteria measure the correlation between the decision classes and the other attributes. Such criteria use statistical distributions in determining whether or not there is a correlation.
Reference: [6] <author> Imam, I.F. and Michalski, </author> <title> R.S., "Learning Decision Structures from Decision Rules: A method and initial results from a comparative study", </title> <journal> in Journal of Intelligent Information Systems JIIS, </journal> <volume> Vol. 2, No. 3, </volume> <pages> pp. 279-304, </pages> <editor> Kerschberg, L., Ras, Z., & Zemankova, M. (Eds.), </editor> <publisher> Kluwer Academic Pub., </publisher> <address> MA, </address> <year> 1993. </year>
Reference-contexts: The nodes of the structure are assigned individual tests (e.g., a single attribute, a function, or a relation), the branches are assigned possible test outcomes (or ranges of outcomes), and the leaves are assigned one specific decision or a set of candidate decisions (with corresponding probabilities), or an undetermined decision <ref> [6, 7] </ref>. A decision structure reduces to a familiar decision tree, when each node is assigned a single attribute and has at most one parent, the branches from each node are assigned single values of that attribute, and leaves are assigned single, definite decisions. <p> The decision rules can be obtained be a rule learning program (we used AQ15c and AQ17-DCI) or from a domain expert. The earlier work on the above ideas was presented in several papers (e.g., <ref> [6, 11] </ref>. This paper presents a description of an extended method used in FRD-2 system and its testing on several problems. 2 RELA TED RESEARCH The method used here for learning decision structures from decision rules is similar to learning decision trees from examples.
Reference: [7] <author> Kohavi, R., </author> <title> Bottom-Up Induction of Oblivious Read-Once Decision-Graphs: Strengths and Limitations, </title> <booktitle> Proceedings of AAAI-94, </booktitle> <pages> pp. 613-18, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: The nodes of the structure are assigned individual tests (e.g., a single attribute, a function, or a relation), the branches are assigned possible test outcomes (or ranges of outcomes), and the leaves are assigned one specific decision or a set of candidate decisions (with corresponding probabilities), or an undetermined decision <ref> [6, 7] </ref>. A decision structure reduces to a familiar decision tree, when each node is assigned a single attribute and has at most one parent, the branches from each node are assigned single values of that attribute, and leaves are assigned single, definite decisions.
Reference: [8] <author> Michie, D., Muggleton, S., Page, D. and Srinivasan, A., </author> <title> International East-West Challenge, </title> <publisher> Oxford University, </publisher> <address> UK, </address> <year> 1994. </year>
Reference-contexts: The remaining examples in each case were used for testing the obtained descriptions and determining their prediction accuracy. 4.1 Learning T ask-oriented Decision Structur e s This subsection briefly illustrates the capabilities of the system for learning task-oriented decision structures. Experiments involved the East-West Challenge problem <ref> [8] </ref>. The East-Westbound problem is concerned with discriminating between two groups of train-like structures. Each train consists of several cars (two to four), each containing various loads of different shapes.
Reference: [9] <author> Michalski, </author> <title> R.S, Designing Extended Entry Decision Tables and Optimal Decision Trees Using Decision Diagrams, </title> <type> Technical Report No.898, </type> <institution> Urbana: University of Illinois, </institution> <year> 1978. </year>
Reference-contexts: These categories are logic-based, information-based, and statistics-based. The logic-based criteria for selecting attributes use logical relationships between the attributes and the decision classes to determine the best attribute to be a node in the decision tree, such as the MAL criterion Minimizing Added Leaves <ref> [9] </ref>. The MAL criterion uses conjunction and disjunction operators. The information-based criteria are based on information theory. These criteria measure the information conveyed by dividing the training examples into subsets.
Reference: [10] <author> Michalski, R.S., Mozetic, I., Hong, J. and Lavrac, N., </author> <title> The MultiPurpose Incremental Learning System AQ15 and Its Testing Application to Three Medical Domains, </title> <booktitle> Proceedings of AAAI-86, </booktitle> <pages> (pp. 1041-1045), </pages> <address> Philadelphia, PA., </address> <year> 1986. </year>
Reference-contexts: Such a function involves assigning a value to a decision variable based on values of attributes characterizing the decision making situation. Figure 1 shows an architecture of the system, called FRD (from Facts to Rules to Decisions). The decision rules are learned by either the AQ15 <ref> [10] </ref> or AQ17-DCI [2] learning systems. They also can be directly edited into the system. The transformation of decision rules to decision structures is accomplished using the AQDT-2 algorithm.
Reference: [11] <author> Michalski, </author> <title> R.S., and Imam, I.F., Learning Problem-oriented Decision Structures from Decision Rules: </title> <booktitle> The AQDT-2 System, in The Proceedings of the International Symposium on Methodology for Intelligent Systems, </booktitle> <address> ISMIS-94, Charlotte, NC, </address> <month> October, </month> <year> 1994. </year>
Reference-contexts: The decision rules can be obtained be a rule learning program (we used AQ15c and AQ17-DCI) or from a domain expert. The earlier work on the above ideas was presented in several papers (e.g., <ref> [6, 11] </ref>. This paper presents a description of an extended method used in FRD-2 system and its testing on several problems. 2 RELA TED RESEARCH The method used here for learning decision structures from decision rules is similar to learning decision trees from examples.
Reference: [12] <author> Mingers, J., </author> <title> An Empirical Comparison of selection Measures for Decision-Structure Induction, </title> <journal> Machine Learning, </journal> <volume> Vol. 3, No. 3, </volume> <pages> (pp. 319-342), </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: The statistics-based criteria measure the correlation between the decision classes and the other attributes. Such criteria use statistical distributions in determining whether or not there is a correlation. Examples of statistical criteria include the Chisquare and the G statistics <ref> [12] </ref>. 3 THE FRD-2 METHOD The proposed methodology separates the function of knowledge acquisition or discovery from the function of applying knowledge to decision making.
Reference: [13] <author> Quinlan, J.R., </author> <title> Learning efficient classification procedures and their application to chess end games in R.S. </title> <editor> Michalski, J.G. Carbonell and T.M. Mitchell, (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Los Altos: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: The MAL criterion uses conjunction and disjunction operators. The information-based criteria are based on information theory. These criteria measure the information conveyed by dividing the training examples into subsets. Examples of such criteria include the information gain, an entropy reduction measure <ref> [13] </ref>, the gini index of diversity [3], Gain-ratio measure [14], and others [4, 5]. The statistics-based criteria measure the correlation between the decision classes and the other attributes. Such criteria use statistical distributions in determining whether or not there is a correlation.
Reference: [14] <author> Quinlan, J. R., </author> <title> Probabilistic decision structures, </title> <editor> in Y. Kodratoff and R.S. Michalski (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Vol. III, </booktitle> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> (pp. 63-111), </pages> <month> June, </month> <year> 1990. </year>
Reference-contexts: The information-based criteria are based on information theory. These criteria measure the information conveyed by dividing the training examples into subsets. Examples of such criteria include the information gain, an entropy reduction measure [13], the gini index of diversity [3], Gain-ratio measure <ref> [14] </ref>, and others [4, 5]. The statistics-based criteria measure the correlation between the decision classes and the other attributes. Such criteria use statistical distributions in determining whether or not there is a correlation. <p> 4 0 1,3 c) using only attributes from Car #3 d) with lower costs for x34 and x37. 4.2 Comparing decision tr ees fr om FRD-1 and C4.5 s This subsection presents a comparison between the decision trees obtained by FRD-1 and C4.5, a well-known program for learning decision trees <ref> [14] </ref>. This The experiments were performed on 6 different data sets. Both systems were set to their default parameters. The experiments were divided into two parts.
Reference: [15] <author> Thrun, S.B., Mitchell, T. and Cheng, J., (Eds.) </author> <title> The MONKs Problems: A Performance Comparison of Different Learning Algorithms, </title> <type> Technical Report, </type> <institution> Carnegie Mellon University, </institution> <month> October, </month> <year> 1991. </year>
Reference-contexts: This The experiments were performed on 6 different data sets. Both systems were set to their default parameters. The experiments were divided into two parts. The first part is concerned with designed problems, the MONKs <ref> [15] </ref>, and the second part was concerned with real-world problems: Wind bracing for tall buildings [1], Mushrooms classification, Breast cancer diagnosis. All the results reported here are the average of 100 runs.
References-found: 15

