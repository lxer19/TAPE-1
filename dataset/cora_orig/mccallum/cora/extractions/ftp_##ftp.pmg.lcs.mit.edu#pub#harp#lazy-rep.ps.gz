URL: ftp://ftp.pmg.lcs.mit.edu/pub/harp/lazy-rep.ps.gz
Refering-URL: http://www.pmg.lcs.mit.edu/areas/repl.html
Root-URL: 
Title: Providing High Availability Using Lazy Replication  
Author: Rivka Ladin Barbara Liskov Liuba Shrira Sanjay Ghemawat 
Note: This research was supported in part by the National Science Foundation under Grant CCR-8822158 and in part by the Advanced Research Projects Agency of the Department of Defense, monitored by the Office of Naval Research under Contract N00014-89-J-1988.  
Address: One Kendall Square Cambridge, MA 02139  545 Technology Square Cambridge, MA 02139  
Affiliation: Digital Equipment Corp.  MIT Laboratory for Computer Science  
Abstract: To provide high availability for services such as mail or bulletin boards, data must be replicated. One way to guarantee consistency of replicated data is to force service operations to occur in the same order at all sites, but this approach is expensive. For some applications a weaker causal operation order can preserve consistency while providing better performance. This paper describes a new way of implementing causal operations. Our technique also supports two other kinds of operations: operations that are totally ordered with respect to one another, and operations that are totally ordered with respect to all other operations. The method performs well in terms of response time, operation processing capacity, amount of stored state, and number and size of messages; it does better than replication methods based on reliable multicast techniques. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Alsberg P. A. and Day, J. D. </author> . <title> A Principle for Resilient Sharing of Distributed Resources. </title> <booktitle> Proc. of the 2nd International Conference on Software Engineering, </booktitle> <month> October, </month> <year> 1976, </year> <pages> pp. 627-644. </pages> <note> Also available in unpublished form as CAC Document number 202 Center for Advanced Computation University of Illinois, Urbana-Champaign, Illinois 61801 by Alsberg, Benford, Day, and Grapa.. </note>
Reference-contexts: R 16 Of course, if only one replica could run forced updates, there would be an availability problem if that replica were inaccessible. To solve this problem, we allow different replicas to act as R over time. We do this by using a variant of the primary copy method <ref> [1, 28, 29] </ref> with view changes [6, 7] to mask failures. An active view always consists of a majority of replicas; one of the replicas in the view is the designated primary and the others are backups. <p> Comparison with Other Work Our work builds on numerous previous results, including general replication techniques such as voting [2, 10, 14] and primary copy <ref> [1, 28, 29] </ref>, but is most closely related to approaches that provide high availability for applications where operations need not be ordered identically at all replicas. This section discusses this closely related work. The idea of exploiting system semantics to enhance availability appears first in [9, 36].
Reference: 2. <author> Bernstein, P. A., Goodman, N. </author> <title> "An Algorithm for Concurrency Control and Recovery in Replicated Distributed Databases". </title> <journal> ACM Transactions on database Systems 9, </journal> <month> 4 (December </month> <year> 1984), </year> <pages> 596-615. </pages>
Reference-contexts: Comparison with Other Work Our work builds on numerous previous results, including general replication techniques such as voting <ref> [2, 10, 14] </ref> and primary copy [1, 28, 29], but is most closely related to approaches that provide high availability for applications where operations need not be ordered identically at all replicas. This section discusses this closely related work.
Reference: 3. <author> Birman, K. P., and Joseph, T. A. </author> <title> "Reliable Communication in the Presence of Failures". </title> <journal> ACM Trans. on Computer Systems 5, </journal> <month> 1 (February </month> <year> 1987), </year> <pages> 47-76. </pages>
Reference-contexts: Our method generalizes these other techniques because it allows queries to be performed on stale data while ensuring that the information observed respects causality. The idea of allowing an application to make use of operations with differing ordering requirements appears in the work on ISIS <ref> [3] </ref> and Psync [27], and in fact we support the same three orders as ISIS. These systems provide a reliable multicast mechanism that allows processes within a process group consisting of both clients and servers to communicate; a possible application of the mechanism is a replicated service. <p> The first clause states that all updates identified by a query result label correspond to calls made by front ends. The second clause states that the result label identifies all required updates plus possibly some additional ones. The third 2 A similar concept occurs in <ref> [3] </ref> but no practical way to implement it was described. 4 Let q be a query. <p> The idea of allowing the designer of an application to choose from a set of primitives of differing strengths appears first in the work on ISIS <ref> [3] </ref> and later in Psync [27]. Both systems provide multicast communication mechanisms that can be used to provide a replicated service. The Psync approach is limited to two kinds of operations, commutative and totally ordered, and only one operation can be of the more efficient commutative type. <p> Every client request is sent to all group members, resulting in substantial message traffic unless the entire system is located on a single local area net that supports broadcast. To ensure eventual delivery of requests, messages contain information about past history. In the earlier version of ISIS <ref> [3] </ref>, messages contained descriptions of earlier requests; this resulted in very large messages and a garbage collection problem (recognizing when old requests could be discarded). ISIS now uses a multipart timestamp scheme [4] similar to ours.
Reference: 4. <author> Birman, K. and Schiper, A. and Stephenson, P. </author> <title> "Lightweight Causal and Atomic Group Multicast". </title> <journal> ACM Transactions on Computer Systems 9, </journal> <month> 3 (August </month> <year> 1991). </year>
Reference-contexts: Furthermore, the front end's timestamp need be included in a message to the replica only if it increases because the front end received a timestamp in a message from another client. (A similar optimization is used in ISIS <ref> [4] </ref>.) Streaming does not cause responses to queries to be delayed. Client-to-client communication may be delayed, however; the front end cannot send on a message from its client to another client until it knows the timestamp for the most recent update requested by its client. <p> Since the sending and receiving of messages is expensive, the use of gossip to reduce the number of messages that servers handle decreases the load. For example, our scheme places less load on servers than schemes such as the ISIS multicast <ref> [4] </ref> in which every update causes a message to be received at every server. To get a sense of how well lazy replication would perform in practice, we implemented a prototype causal operation service and compared its performance with an unreplicated prototype. <p> In the earlier version of ISIS [3], messages contained descriptions of earlier requests; this resulted in very large messages and a garbage collection problem (recognizing when old requests could be discarded). ISIS now uses a multipart timestamp scheme <ref> [4] </ref> similar to ours. Timestamps are used to order client operations but they have fields for clients as well as servers.
Reference: 5. <author> Birrell, A., Levin, R., Needham, R., and Schroeder, M., "Grapevine: </author> <title> An Exercise in Distributed Computing". </title> <journal> Comm. of the ACM 25, </journal> <month> 4 (April </month> <year> 1982), </year> <pages> 260-274. </pages>
Reference-contexts: Having a server at every client wastes space (since the server state must be stored at every client node) and also wastes network bandwidth (since every client node must be notified about every update). 25 In the Grapevine system <ref> [5] </ref> and its successor [20] service nodes are distinct from client nodes. Every client operation is performed at a single server, and updates propagate in the background to other replicas. Thus Grapevine solves the problems of wasted space and network bandwidth but it sacrifices causality.
Reference: 6. <author> El-Abbadi, A., and Toueg, S. </author> <title> Maintaining Availability in Partitioned Replicated Databases. </title> <booktitle> Proc. of the Fifth Symposium on Principles of Database Systems, ACM, </booktitle> <year> 1986, </year> <pages> pp. 240-251. </pages>
Reference-contexts: Each replica would send gossip only to its neighbors. If there is a failure (crash or partition), the structure would be reconstituted by carrying out a view change algorithm <ref> [6, 7] </ref>. This approach causes information to propagate more slowly than having replicas gossip with all other replicas. Communication between the front end and the replicas can be made more efficient by taking advantage of the fact that a front end typically communicates with the same replica. <p> To solve this problem, we allow different replicas to act as R over time. We do this by using a variant of the primary copy method [1, 28, 29] with view changes <ref> [6, 7] </ref> to mask failures. An active view always consists of a majority of replicas; one of the replicas in the view is the designated primary and the others are backups. <p> This is analogous to what happens in other systems that support atomic operations: reading is not allowed in a minority partition, since if it were inconsistent data could be observed <ref> [6] </ref>.
Reference: 7. <author> El-Abbadi, A., Skeen, D., and Cristian, F. </author> <title> An Efficient Fault-tolerant Protocol for Replicated Data Management. </title> <booktitle> Proc. of the Fourth Symposium on Principles of Database Systems, ACM, </booktitle> <year> 1985, </year> <pages> pp. 215-229. </pages>
Reference-contexts: Each replica would send gossip only to its neighbors. If there is a failure (crash or partition), the structure would be reconstituted by carrying out a view change algorithm <ref> [6, 7] </ref>. This approach causes information to propagate more slowly than having replicas gossip with all other replicas. Communication between the front end and the replicas can be made more efficient by taking advantage of the fact that a front end typically communicates with the same replica. <p> To solve this problem, we allow different replicas to act as R over time. We do this by using a variant of the primary copy method [1, 28, 29] with view changes <ref> [6, 7] </ref> to mask failures. An active view always consists of a majority of replicas; one of the replicas in the view is the designated primary and the others are backups.
Reference: 8. <author> Farrell, K. A. </author> <title> A Deadlock Detection Scheme for Argus. </title> <type> Senior Thesis. </type> <institution> Laboratory of Computer Science, M.I.T., </institution> <address> Cambridge, MA. </address>
Reference-contexts: The replicated service continues to provide service in spite of node failures and network partitions. We have applied the method to a number of applications, including distributed garbage collection [18], deadlock detection <ref> [8] </ref>, and orphan detection [23], locating movable objects in a distributed system [15], and deletion of unused versions in a hybrid concurrency control scheme [35]. Another system that can benefit from causally-ordered operations is the familiar electronic mail system. <p> Another application that could profit from this approach is deadlock detection <ref> [8] </ref>. 4.2. Multiple Services We now consider a system containing many services. Causality can be preserved across a number of services by using joint front ends (one per client node) that manage all of them.
Reference: 9. <author> Fischer, M. J., and Michael, A. </author> <title> Sacrificing Serializability to Attain High Availability of Data in an Unreliable Network. </title> <booktitle> Proc. of the Symposium on Principles of Database Systems, ACM, </booktitle> <month> March, </month> <year> 1982, </year> <pages> pp. 70-75. </pages>
Reference-contexts: Our technique is based on the gossip approach first introduced by Fischer and Michael <ref> [9] </ref>, and later enhanced by Wuu and Bernstein [36] and our own earlier work [16]. We have extended the earlier work in two important ways: by supporting causal ordering for updates as well as queries and by incorporating forced and immediate operations to make a more generally applicable method. <p> Timestamps are partially ordered in the obvious way: t s ( t s ... t s ) 1 1 n n Two timestamps t and s are merged by taking their component-wise maximum. (Multipart timestamps were used in Locus [30] and also in <ref> [9, 13, 16, 22, 36] </ref>.) Both uids and labels are represented by multipart timestamps. Every update operation is assigned a unique multipart timestamp as its uid. A label is created by merging timestamps; a label timestamp t identifies the updates whose timestamps are less than or equal to t. <p> This section discusses this closely related work. The idea of exploiting system semantics to enhance availability appears first in <ref> [9, 36] </ref>. In these systems, servers are coresident with clients and propagate information about updates by means of gossip. Causality is easily preserved for a single client, since it always communicates with the server at its node; preservation of inter-client causality is not discussed.
Reference: 10. <author> Gifford, D. K. </author> <title> Weighted Voting for Replicated Data. </title> <booktitle> Proc. of the Seventh Symposium on Operating Systems Principles, ACM SIGOPS, </booktitle> <address> Pacific Grove, CA, </address> <month> December, </month> <year> 1979, </year> <pages> pp. 150-162. </pages>
Reference-contexts: Our system can be instantiated with only forced operations. In this case it provides the same order for all updates at all replicas, and will perform similarly to other replication techniques that guarantee this property (e.g., voting <ref> [10] </ref> or primary copy [28]). Our method generalizes these other techniques because it allows queries to be performed on stale data while ensuring that the information observed respects causality. <p> Comparison with Other Work Our work builds on numerous previous results, including general replication techniques such as voting <ref> [2, 10, 14] </ref> and primary copy [1, 28, 29], but is most closely related to approaches that provide high availability for applications where operations need not be ordered identically at all replicas. This section discusses this closely related work.
Reference: 11. <author> Gifford, D. K. </author> <title> Weighted Voting for Replicated Data. </title> <booktitle> Proc. of the Seventh Symposium on Operating Systems Principles, ACM, </booktitle> <month> December, </month> <year> 1979, </year> <pages> pp. 150-162. </pages>
Reference-contexts: Immediate operations are performed at all replicas in the same order relative to all other operations. They have the effect of being performed immediately when the operation returns, and are ordered consistently with external events <ref> [11] </ref>. They would be useful to remove an individual from a classified mailing-list "at once", so that no messages addressed to that list would be delivered to that user after the remove operation returns. It is easy to construct and use highly-available applications with our method.
Reference: 12. <author> Barbara Liskov and Sanjay Ghemawat and Robert Gruber and Paul Johnson and Liuba Shrira and Michael Williams. </author> <title> Replication in the -Harp file system. </title> <address> SOSP13, </address> , <year> 1991, </year> <pages> pp. </pages> <month> 226--238.- </month>
Reference-contexts: replicas are geographically distributed; if the replicas are close, the update can be recorded on volatile storage provided replicas have uninterruptible power supplies that allow them to copy volatile information to disk if there is a power failure. (This technique has been used to good effect in the Harp system <ref> [12] </ref>.) A replica that loses its state in a crash recovers it by reading the state of enough other replicas so that it can determine the last update it processed.
Reference: 13. <author> Heddaya, A. and Hsu, M. and Weihl, W. </author> <title> "Two Phase Gossip: Managing Distributed Event Histories". </title> <journal> Information Sciences: </journal> <note> An International Journal 49, 1-2 (Oct./Nov. 1989). Special issue on databases. </note>
Reference-contexts: Timestamps are partially ordered in the obvious way: t s ( t s ... t s ) 1 1 n n Two timestamps t and s are merged by taking their component-wise maximum. (Multipart timestamps were used in Locus [30] and also in <ref> [9, 13, 16, 22, 36] </ref>.) Both uids and labels are represented by multipart timestamps. Every update operation is assigned a unique multipart timestamp as its uid. A label is created by merging timestamps; a label timestamp t identifies the updates whose timestamps are less than or equal to t.
Reference: 14. <author> Herlihy, M. </author> <title> "A Quorum-consensus Replication Method for Abstract Data Types". </title> <journal> ACM Trans. on Computer Systems 4, </journal> <month> 1 (February </month> <year> 1986), </year> <pages> 32-53. </pages>
Reference-contexts: To determine the operation categories, the programmer can use techniques developed for determining permissible concurrency <ref> [14, 31, 32, 34] </ref>. Our method does not delay update operations (such as send_mail), and typically provides the response to a query (such as read_mail) in one message round trip. <p> Comparison with Other Work Our work builds on numerous previous results, including general replication techniques such as voting <ref> [2, 10, 14] </ref> and primary copy [1, 28, 29], but is most closely related to approaches that provide high availability for applications where operations need not be ordered identically at all replicas. This section discusses this closely related work.
Reference: 15. <author> Hwang, D. </author> <title> Constructing a Highly-Available Location Service for a Distributed Environment. </title> <type> Technical Report MIT/LCS/TR-410, </type> <institution> M.I.T. Laboratory for Computer Science, </institution> <address> Cambridge, MA, </address> <month> January, </month> <year> 1988. </year>
Reference-contexts: The replicated service continues to provide service in spite of node failures and network partitions. We have applied the method to a number of applications, including distributed garbage collection [18], deadlock detection [8], and orphan detection [23], locating movable objects in a distributed system <ref> [15] </ref>, and deletion of unused versions in a hybrid concurrency control scheme [35]. Another system that can benefit from causally-ordered operations is the familiar electronic mail system. <p> We assume there is a fixed number of replicas residing at fixed locations and that front ends and replicas know how to find replicas; a technique for reconfiguring services (i.e., adding or removing replicas) is described in <ref> [15] </ref>. We also assume that replicas eventually recover from crashes; Section 3.2 discusses how this happens. In this section we describe how the front end and service replicas together implement the three types of operations.
Reference: 16. <author> Ladin, R., Liskov, B., and Shrira, L. </author> <title> "A Technique for Constructing Highly-Available Services". </title> <booktitle> Algorithmica 3 (1988), </booktitle> <pages> 393-420. </pages>
Reference-contexts: Our technique is based on the gossip approach first introduced by Fischer and Michael [9], and later enhanced by Wuu and Bernstein [36] and our own earlier work <ref> [16] </ref>. We have extended the earlier work in two important ways: by supporting causal ordering for updates as well as queries and by incorporating forced and immediate operations to make a more generally applicable method. <p> Timestamps are partially ordered in the obvious way: t s ( t s ... t s ) 1 1 n n Two timestamps t and s are merged by taking their component-wise maximum. (Multipart timestamps were used in Locus [30] and also in <ref> [9, 13, 16, 22, 36] </ref>.) Both uids and labels are represented by multipart timestamps. Every update operation is assigned a unique multipart timestamp as its uid. A label is created by merging timestamps; a label timestamp t identifies the updates whose timestamps are less than or equal to t. <p> Also, different clients typically use different replicas, and a client may not observe the effects of an update it learned about in a message from another client. Our earlier work <ref> [16, 22] </ref> separates servers from clients and supports a limited form of causality. Client requests are executed at just one server, and information about updates is propagated in gossip.
Reference: 17. <author> Ladin, R. and Mazer, M. S. and Wolman, A. </author> <title> A General Tool for Replicating Distributed Services. </title> <booktitle> Proceedings of 1st International Conference on Parallel and Distribut ed Information Systems, </booktitle> <month> December, </month> <year> 1991. </year>
Reference-contexts: Our performance expectations are backed up by the experiments discussed in Section 3. A real implementation of a generic service that can be instantiated to provide replicated services is underway <ref> [17] </ref>. The forced and immediate operations are important because they increase the applicability of the approach, allowing it to be used for applications in which some updates require a stronger ordering than causality. Forced updates are also interesting in their own right.
Reference: 18. <author> Ladin, R., </author> <title> A Method for Constructing Highly Available Services and An Algorithm for Distributed Garbage Collection. </title> <type> Ph.D. </type> <institution> Th., M.I.T. Laboratory for Computer Science, </institution> <address> Cambridge, MA, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: The replicated service continues to provide service in spite of node failures and network partitions. We have applied the method to a number of applications, including distributed garbage collection <ref> [18] </ref>, deadlock detection [8], and orphan detection [23], locating movable objects in a distributed system [15], and deletion of unused versions in a hybrid concurrency control scheme [35]. Another system that can benefit from causally-ordered operations is the familiar electronic mail system. <p> The replica groups communicate with one another via a lower-level replica group, i.e., they are clients of the lower-level group; in fact, the scheme can be extended to an arbitrary number of levels. This scheme has been proposed for the garbage collection service <ref> [18] </ref>; in this application, a client's query is ordered only with respect to its own updates, although the speed with which inaccessible objects are discarded depends on how quickly global information propagates from one replica group to another (via the lower-level replica group).
Reference: 19. <author> Lamport, L. </author> <title> "Time, Clocks, and the Ordering of Events in a Distributed System". </title> <journal> Comm. of the ACM 21, </journal> <month> 7 (July </month> <year> 1978), </year> <pages> 558-565. </pages>
Reference-contexts: To achieve availability, the server's state must be replicated. Consistency of replicated state can be guaranteed by forcing service operations to occur in the same order at all sites. However, some applications can preserve consistency with a weaker, causal ordering <ref> [19] </ref>, leading to better performance. This paper describes a new technique that supports causal order. An operation call is executed at just one replica; updating of other replicas happens by lazy exchange of "gossip" messages hence the name "lazy replication".
Reference: 20. <author> Lampson, B. W. </author> <title> Designing a Global Name Service. </title> <booktitle> Proc. of the 5th Symposium on Principles of Distributed Computing, ACM SIGACT-SIGOPS, </booktitle> <month> August, </month> <year> 1986, </year> <pages> pp. 1-10. 29 </pages>
Reference-contexts: Having a server at every client wastes space (since the server state must be stored at every client node) and also wastes network bandwidth (since every client node must be notified about every update). 25 In the Grapevine system [5] and its successor <ref> [20] </ref> service nodes are distinct from client nodes. Every client operation is performed at a single server, and updates propagate in the background to other replicas. Thus Grapevine solves the problems of wasted space and network bandwidth but it sacrifices causality.
Reference: 21. <author> Lampson, B. W., and Sturgis, H. E. </author> <title> Crash Recovery in a Distributed Data Storage System. </title> <institution> Xerox Research Center, </institution> <address> Palo Alto, Ca., </address> <year> 1979. </year>
Reference-contexts: Typically clocks are monotonic both while nodes are up and across crashes. If they need to be adjusted this is done by slowing them down or speeding them up. Also, clocks are usually stable; if not the clock 3 value can be saved to stable storage <ref> [21] </ref> periodically and recovered after a crash. For the system to run efficiently, clocks of server and client nodes should be loosely synchronized with a skew bounded by some e. Synchronized clocks are not needed for correctness, but without them certain suboptimal situations can arise. <p> Reliability and Availability The traditional way of achieving high reliability is to record information on a sufficient number of nonvolatile devices. For example, recording information on stable storage <ref> [21] </ref> insures that it will survive failure of its node and also a single media failure. However, this solution is wasteful, since we already provide a different kind of redundancy, namely recording updates at many replicas. Also it provides poor availability.
Reference: 22. <author> Liskov, B., and Ladin, R. </author> <title> Highly-Available Distributed Services and Fault-Tolerant Distributed Garbage Collection. </title> <booktitle> Proc. of the 5th ACM Symposium on Principles of Distributed Computing, ACM, </booktitle> <address> Calgary, Alberta, Canada, </address> <month> August, </month> <year> 1986. </year>
Reference-contexts: Timestamps are partially ordered in the obvious way: t s ( t s ... t s ) 1 1 n n Two timestamps t and s are merged by taking their component-wise maximum. (Multipart timestamps were used in Locus [30] and also in <ref> [9, 13, 16, 22, 36] </ref>.) Both uids and labels are represented by multipart timestamps. Every update operation is assigned a unique multipart timestamp as its uid. A label is created by merging timestamps; a label timestamp t identifies the updates whose timestamps are less than or equal to t. <p> Also, different clients typically use different replicas, and a client may not observe the effects of an update it learned about in a message from another client. Our earlier work <ref> [16, 22] </ref> separates servers from clients and supports a limited form of causality. Client requests are executed at just one server, and information about updates is propagated in gossip.
Reference: 23. <author> Liskov, B., Scheifler, R., Walker, E., and Weihl, W. </author> <title> Orphan Detection (Extended Abstract). </title> <booktitle> Proc. of the 17th International Symposium on Fault-Tolerant Computing, IEEE, </booktitle> <address> Pittsburgh, Pa., </address> <month> July, </month> <year> 1987, </year> <pages> pp. 2-7. </pages>
Reference-contexts: The replicated service continues to provide service in spite of node failures and network partitions. We have applied the method to a number of applications, including distributed garbage collection [18], deadlock detection [8], and orphan detection <ref> [23] </ref>, locating movable objects in a distributed system [15], and deletion of unused versions in a hybrid concurrency control scheme [35]. Another system that can benefit from causally-ordered operations is the familiar electronic mail system.
Reference: 24. <author> Liskov, B., Bloom, T., Gifford, D., Scheifler, R., and Weihl, W. </author> <title> Communication in the Mercury System. </title> <booktitle> Proc. of the 21st Annual Hawaii Conference on System Sciences, IEEE, </booktitle> <month> January, </month> <year> 1988, </year> <pages> pp. 178-187. </pages>
Reference-contexts: Communication between the front end and the replicas can be made more efficient by taking advantage of the fact that a front end typically communicates with the same replica. Communication could be done over a streaming connection such as TCP or Mercury <ref> [24] </ref>. In this case, the front end need not wait to receive the uid timestamp from an update it requested before sending the next operation (query or update) to the replica. <p> A further optimization is possible when using streaming: operations from the front end can be batched if they are small. (Mercury streams <ref> [24] </ref> do this automatically.) A message would be sent from the front end when the client does a query, when the buffer is full, or when its client is sending a message to another client.
Reference: 25. <author> Liskov, B. </author> <title> "Distributed Programming in Argus". </title> <journal> Comm. of the ACM 31, </journal> <month> 3 (March </month> <year> 1988), </year> <pages> 300-312. </pages>
Reference-contexts: The prototypes implement a simple location service with insertion and lookup operations. They are implemented in Argus <ref> [25] </ref> and run on a network of VAXStation 3200's connected by a 10 megabit-per-second ethernet. An Argus program is composed of modules called guardians that may reside on different nodes of a network.
Reference: 26. <author> Mills, D. L. </author> <title> Network Time Protocol (version1) specification and implementation. DARPA-Internet Report RFC-1059, </title> <booktitle> DARPA, </booktitle> <year> 1988. </year>
Reference-contexts: The network can partition, and messages can be lost, delayed, duplicated, and delivered out of order. The configuration of the system can change; nodes can leave and join the network at any time. We assume nodes have loosely synchronized clocks. There are practical protocols, such as NTP <ref> [26] </ref> that with low cost synchronize clocks in geographically distributed networks. A replicated application is implemented by service consisting of replicas running at different nodes in a network. To hide replication from clients, the system also provides front end code that runs at client nodes.
Reference: 27. <author> Mishra, Sh., Peterson, L. L., Schlichting, R.D. </author> <title> Implementing Fault-Tolerant Replicated Objects Using Psync. </title> <booktitle> Proc. of the Eighth Symposium on Reliable Distributed Systems, </booktitle> <month> October, </month> <year> 1989. </year>
Reference-contexts: Our method generalizes these other techniques because it allows queries to be performed on stale data while ensuring that the information observed respects causality. The idea of allowing an application to make use of operations with differing ordering requirements appears in the work on ISIS [3] and Psync <ref> [27] </ref>, and in fact we support the same three orders as ISIS. These systems provide a reliable multicast mechanism that allows processes within a process group consisting of both clients and servers to communicate; a possible application of the mechanism is a replicated service. Our technique is a replication method. <p> The idea of allowing the designer of an application to choose from a set of primitives of differing strengths appears first in the work on ISIS [3] and later in Psync <ref> [27] </ref>. Both systems provide multicast communication mechanisms that can be used to provide a replicated service. The Psync approach is limited to two kinds of operations, commutative and totally ordered, and only one operation can be of the more efficient commutative type.
Reference: 28. <author> Oki, B. M., and Liskov, B. </author> <title> Viewstamped Replication: A New Primary Copy Method to Support Highly-Available Distributed Systems. </title> <booktitle> Proc. of the 7th ACM Symposium on Principles of Distributed Computing, ACM, </booktitle> <month> Aug., </month> <year> 1988. </year>
Reference-contexts: Our system can be instantiated with only forced operations. In this case it provides the same order for all updates at all replicas, and will perform similarly to other replication techniques that guarantee this property (e.g., voting [10] or primary copy <ref> [28] </ref>). Our method generalizes these other techniques because it allows queries to be performed on stale data while ensuring that the information observed respects causality. <p> R 16 Of course, if only one replica could run forced updates, there would be an availability problem if that replica were inaccessible. To solve this problem, we allow different replicas to act as R over time. We do this by using a variant of the primary copy method <ref> [1, 28, 29] </ref> with view changes [6, 7] to mask failures. An active view always consists of a majority of replicas; one of the replicas in the view is the designated primary and the others are backups. <p> Comparison with Other Work Our work builds on numerous previous results, including general replication techniques such as voting [2, 10, 14] and primary copy <ref> [1, 28, 29] </ref>, but is most closely related to approaches that provide high availability for applications where operations need not be ordered identically at all replicas. This section discusses this closely related work. The idea of exploiting system semantics to enhance availability appears first in [9, 36].
Reference: 29. <author> Oki, B. M. </author> <title> Viewstamped Replication for Highly Available Distributed Systems. </title> <type> Technical Report MIT/LCS/TR-423, </type> <institution> M.I.T. Laboratory for Computer Science, </institution> <address> Cambridge, MA, </address> <month> August, </month> <year> 1988. </year>
Reference-contexts: R 16 Of course, if only one replica could run forced updates, there would be an availability problem if that replica were inaccessible. To solve this problem, we allow different replicas to act as R over time. We do this by using a variant of the primary copy method <ref> [1, 28, 29] </ref> with view changes [6, 7] to mask failures. An active view always consists of a majority of replicas; one of the replicas in the view is the designated primary and the others are backups. <p> Comparison with Other Work Our work builds on numerous previous results, including general replication techniques such as voting [2, 10, 14] and primary copy <ref> [1, 28, 29] </ref>, but is most closely related to approaches that provide high availability for applications where operations need not be ordered identically at all replicas. This section discusses this closely related work. The idea of exploiting system semantics to enhance availability appears first in [9, 36].
Reference: 30. <author> Parker, D. S., Popek, G. J., Rudisin, G., Stoughton, A., Walker, B., Walton, E., Chow, J., Edwards, D., Kiser, S., and Kline, C. </author> <title> "Detection of Mutual Inconsistency in Distributed Systems". </title> <journal> IEEE Transactions on Software Engineering SE-9 (May 1983), </journal> <pages> 240-247. </pages>
Reference-contexts: Timestamps are partially ordered in the obvious way: t s ( t s ... t s ) 1 1 n n Two timestamps t and s are merged by taking their component-wise maximum. (Multipart timestamps were used in Locus <ref> [30] </ref> and also in [9, 13, 16, 22, 36].) Both uids and labels are represented by multipart timestamps. Every update operation is assigned a unique multipart timestamp as its uid.
Reference: 31. <author> Schmuck, F. B. </author> <title> The Use of Efficient Broadcast Protocols in Asynchronous Distributed Systems. </title> <type> Tech. </type> <institution> Rept. 88-928, Dept. of Computer Science, Cornell University, </institution> <month> Aug., </month> <year> 1988. </year>
Reference-contexts: To determine the operation categories, the programmer can use techniques developed for determining permissible concurrency <ref> [14, 31, 32, 34] </ref>. Our method does not delay update operations (such as send_mail), and typically provides the response to a query (such as read_mail) in one message round trip.
Reference: 32. <author> Schwarz, P., and Spector, A. </author> <title> "Synchronizing Shared Abstract Types". </title> <journal> ACM Trans. on Computer Systems 2, </journal> <month> 3 (August </month> <year> 1984). </year>
Reference-contexts: To determine the operation categories, the programmer can use techniques developed for determining permissible concurrency <ref> [14, 31, 32, 34] </ref>. Our method does not delay update operations (such as send_mail), and typically provides the response to a query (such as read_mail) in one message round trip.
Reference: 33. <author> Skeen, D., and Wright, D. D. </author> <title> Increasing Availability in Partitioned Database Systems. </title> <type> Technical Report 83-581, </type> <institution> Dept. of Computer Science, Cornell University, </institution> <note> Ithaca, </note> <author> N. Y., </author> <year> 1984. </year>
Reference-contexts: The primary of the active view will carry out immediate updates, but only if the view contains all replicas of the service. Timestamps for immediate updates are assigned in the same way as for forced updates, by using the R part of the timestamp. We use a three-phase algorithm <ref> [33] </ref> to perform an immediate update. Phase 1 is a "pre-prepare" phase in which the primary asks every backup to send its log and timestamp.
Reference: 34. <author> Weihl, W. and B. Liskov. </author> <title> "Implementation of Resilient, Atomic Data Types". </title> <booktitle> ACM Transaction on Programming Languages and Systems 7, </booktitle> <month> 2 (April </month> <year> 1985). </year>
Reference-contexts: To determine the operation categories, the programmer can use techniques developed for determining permissible concurrency <ref> [14, 31, 32, 34] </ref>. Our method does not delay update operations (such as send_mail), and typically provides the response to a query (such as read_mail) in one message round trip.
Reference: 35. <author> Weihl, W. </author> <title> "Distributed Version Management for Read-only Actions". </title> <journal> IEEE Trans. on Software Engineering, Special Issue on Distributed Systems 13, </journal> <volume> 1 (1987), </volume> <pages> 55-64. </pages>
Reference-contexts: We have applied the method to a number of applications, including distributed garbage collection [18], deadlock detection [8], and orphan detection [23], locating movable objects in a distributed system [15], and deletion of unused versions in a hybrid concurrency control scheme <ref> [35] </ref>. Another system that can benefit from causally-ordered operations is the familiar electronic mail system.
Reference: 36. <author> Wuu, G. T. J., and Bernstein, A. J. </author> <title> Efficient Solutions to the Replicated Log and Dictionary Problems. </title> <booktitle> Proc. of the Third Annual Symposium on Principles of Distributed Computing, ACM, </booktitle> <month> August, </month> <year> 1984, </year> <pages> pp. 233-242. </pages> <note> i Table of Contents </note>
Reference-contexts: Our technique is based on the gossip approach first introduced by Fischer and Michael [9], and later enhanced by Wuu and Bernstein <ref> [36] </ref> and our own earlier work [16]. We have extended the earlier work in two important ways: by supporting causal ordering for updates as well as queries and by incorporating forced and immediate operations to make a more generally applicable method. <p> Timestamps are partially ordered in the obvious way: t s ( t s ... t s ) 1 1 n n Two timestamps t and s are merged by taking their component-wise maximum. (Multipart timestamps were used in Locus [30] and also in <ref> [9, 13, 16, 22, 36] </ref>.) Both uids and labels are represented by multipart timestamps. Every update operation is assigned a unique multipart timestamp as its uid. A label is created by merging timestamps; a label timestamp t identifies the updates whose timestamps are less than or equal to t. <p> If no replica in A knows that r is known in B, there is nothing we can do. However, as suggested in <ref> [36] </ref>, progress can be made if replicas include their copy of ts_table in gossip messages and receivers merge this information with their own ts_tables. In this way, each replica would get a more recent view of what other nodes know. 10 2.1.3. <p> This section discusses this closely related work. The idea of exploiting system semantics to enhance availability appears first in <ref> [9, 36] </ref>. In these systems, servers are coresident with clients and propagate information about updates by means of gossip. Causality is easily preserved for a single client, since it always communicates with the server at its node; preservation of inter-client causality is not discussed.
Reference: 1. <institution> Introduction 1 </institution>
Reference-contexts: R 16 Of course, if only one replica could run forced updates, there would be an availability problem if that replica were inaccessible. To solve this problem, we allow different replicas to act as R over time. We do this by using a variant of the primary copy method <ref> [1, 28, 29] </ref> with view changes [6, 7] to mask failures. An active view always consists of a majority of replicas; one of the replicas in the view is the designated primary and the others are backups. <p> Comparison with Other Work Our work builds on numerous previous results, including general replication techniques such as voting [2, 10, 14] and primary copy <ref> [1, 28, 29] </ref>, but is most closely related to approaches that provide high availability for applications where operations need not be ordered identically at all replicas. This section discusses this closely related work. The idea of exploiting system semantics to enhance availability appears first in [9, 36].
Reference: 2. <author> The Replicated Service 2 2.1. </author> <title> Causal Operations 3 2.1.1. Implementation Overview 5 2.1.2. Processing at Each Replica 8 2.1.3. Optimizations 10 2.1.4. Analysis 11 2.2. Other Operation Types 15 2.2.1. Implementation of Forced Updates 15 2.2.2. Implementation of Immediate Updates 17 </title>
Reference-contexts: Comparison with Other Work Our work builds on numerous previous results, including general replication techniques such as voting <ref> [2, 10, 14] </ref> and primary copy [1, 28, 29], but is most closely related to approaches that provide high availability for applications where operations need not be ordered identically at all replicas. This section discusses this closely related work.
Reference: 3. <author> Performance 18 3.1. </author> <title> Normal case operation 18 3.2. Reliability and Availability 21 </title>
Reference-contexts: Our method generalizes these other techniques because it allows queries to be performed on stale data while ensuring that the information observed respects causality. The idea of allowing an application to make use of operations with differing ordering requirements appears in the work on ISIS <ref> [3] </ref> and Psync [27], and in fact we support the same three orders as ISIS. These systems provide a reliable multicast mechanism that allows processes within a process group consisting of both clients and servers to communicate; a possible application of the mechanism is a replicated service. <p> The first clause states that all updates identified by a query result label correspond to calls made by front ends. The second clause states that the result label identifies all required updates plus possibly some additional ones. The third 2 A similar concept occurs in <ref> [3] </ref> but no practical way to implement it was described. 4 Let q be a query. <p> The idea of allowing the designer of an application to choose from a set of primitives of differing strengths appears first in the work on ISIS <ref> [3] </ref> and later in Psync [27]. Both systems provide multicast communication mechanisms that can be used to provide a replicated service. The Psync approach is limited to two kinds of operations, commutative and totally ordered, and only one operation can be of the more efficient commutative type. <p> Every client request is sent to all group members, resulting in substantial message traffic unless the entire system is located on a single local area net that supports broadcast. To ensure eventual delivery of requests, messages contain information about past history. In the earlier version of ISIS <ref> [3] </ref>, messages contained descriptions of earlier requests; this resulted in very large messages and a garbage collection problem (recognizing when old requests could be discarded). ISIS now uses a multipart timestamp scheme [4] similar to ours.
Reference: 4. <author> Scalability 23 4.1. </author> <title> Large Services 23 4.2. Multiple Services 24 </title>
Reference-contexts: Furthermore, the front end's timestamp need be included in a message to the replica only if it increases because the front end received a timestamp in a message from another client. (A similar optimization is used in ISIS <ref> [4] </ref>.) Streaming does not cause responses to queries to be delayed. Client-to-client communication may be delayed, however; the front end cannot send on a message from its client to another client until it knows the timestamp for the most recent update requested by its client. <p> Since the sending and receiving of messages is expensive, the use of gossip to reduce the number of messages that servers handle decreases the load. For example, our scheme places less load on servers than schemes such as the ISIS multicast <ref> [4] </ref> in which every update causes a message to be received at every server. To get a sense of how well lazy replication would perform in practice, we implemented a prototype causal operation service and compared its performance with an unreplicated prototype. <p> In the earlier version of ISIS [3], messages contained descriptions of earlier requests; this resulted in very large messages and a garbage collection problem (recognizing when old requests could be discarded). ISIS now uses a multipart timestamp scheme <ref> [4] </ref> similar to ours. Timestamps are used to order client operations but they have fields for clients as well as servers.
Reference: 5. <institution> Comparison with Other Work 24 </institution>
Reference-contexts: Having a server at every client wastes space (since the server state must be stored at every client node) and also wastes network bandwidth (since every client node must be notified about every update). 25 In the Grapevine system <ref> [5] </ref> and its successor [20] service nodes are distinct from client nodes. Every client operation is performed at a single server, and updates propagate in the background to other replicas. Thus Grapevine solves the problems of wasted space and network bandwidth but it sacrifices causality.
Reference: 6. <institution> Conclusion 26 ii </institution>
Reference-contexts: Each replica would send gossip only to its neighbors. If there is a failure (crash or partition), the structure would be reconstituted by carrying out a view change algorithm <ref> [6, 7] </ref>. This approach causes information to propagate more slowly than having replicas gossip with all other replicas. Communication between the front end and the replicas can be made more efficient by taking advantage of the fact that a front end typically communicates with the same replica. <p> To solve this problem, we allow different replicas to act as R over time. We do this by using a variant of the primary copy method [1, 28, 29] with view changes <ref> [6, 7] </ref> to mask failures. An active view always consists of a majority of replicas; one of the replicas in the view is the designated primary and the others are backups. <p> This is analogous to what happens in other systems that support atomic operations: reading is not allowed in a minority partition, since if it were inconsistent data could be observed <ref> [6] </ref>.
References-found: 42

