URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-91-14.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: ELFS: ObjectOriented Extensible File Systems  
Author: Andrew S. Grimshaw Edmond C. Loyot, Jr. 
Abstract: Computer Science Report No. TR-91-14 July 8, 1991 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Boral and D. J. Dewitt. </author> <title> Database Machines: An Idea Whose Time has Passed. </title> <booktitle> In Proceedings of the 1983 International Workshop on Database Machines, </booktitle> <pages> pp. 166-187. </pages> <year> 1983. </year>
Reference-contexts: CPU speeds have increased over one hundred fold in the last ten years, yet I/O latencies have improved only by a factor of about four and I/O bandwidths by a factor of about ten. The result is that I/O is increasingly a bottleneck <ref> [1, 12] </ref>. Even on a single processor IBM Risc System 6000, in the approximately 22ms it takes to satisfy a RANDOM I/O request, 1.84 million oating point operations can be performed. The problem is particularly acute for the new high performance parallel architectures.
Reference: [2] <author> T. W. Crockett. </author> <title> File Concepts for Parallel I/O. </title> <booktitle> Proceedings Supercomputing 89, </booktitle> <pages> pp. 574-579, </pages> <month> November, </month> <year> 1989. </year>
Reference-contexts: Very high bandwidth is achieved by scattering pieces of the pfo to multiple devices, each of which may be accessed in parallel (see Figure 2). This idea has been studied extensively <ref> [2, 8, 9, 13, 14, 15, 16] </ref>. What distinguishes our work from others is that the decomposition may be specified using pfos, e.g., row-wise, column-wise, in blocks, etc. This is in contrast to systems such as CFS [14]. CFS stripes the file across the disks in sequential 4K chunks.
Reference: [3] <author> J. J. Dongarra. </author> <title> Performance of Various Computers Using Standard Linear Equation Software. </title> <type> Technical report no. </type> <institution> CS-89-85, Computer Science Department, University of Ten-nessee, Knoxville, TN, </institution> <month> November </month> <year> 1990. </year>
Reference: [4] <author> A. S. Grimshaw. </author> <title> The Mentat Run-Time System: Support for Medium Grain Parallel Computation. </title> <booktitle> Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pp. 1064-1073. </pages> <address> Charleston, SC., </address> <month> April, </month> <year> 1990. </year>
Reference-contexts: When using a variable_consistency_file an application has complete control of the consistency semantics of the files used, and may vary the semantics over time to achieve the desired trade-off of performance and consistency. 3. Mentat Programming Environment We have begun implementation of ELFS using the Mentat parallel programming system <ref> [4, 5, 7] </ref>. Mentat is an object-oriented, parallel computation system designed to provide easy-to-use parallelism for parallel and distributed systems. Mentat alleviates most of the burden of explicit parallelization that message passing systems typically place on the programmer.
Reference: [5] <author> A. S. Grimshaw. </author> <title> An Introduction to Parallel Object-Oriented Parallel Programming with 16 Mentat, </title> <institution> Computer Science Report No. TR-91-07, University of Virginia. </institution>
Reference-contexts: When complete, ELFS will consist of two parts: the definition of a file system class hierarchy, and compiler and run-time system services that support asynchronous objects and multiple outstanding requests without explicit programmer control. The system will be implemented using the parallel programming language MPL <ref> [5, 7] </ref>, which is an extension of C++ [19]. The class hierarchy in our system contains the base class unix_file (see Figure 1).. This class supports the standard UNIX file operations, creat, open, close, lseek, read, and write. <p> When using a variable_consistency_file an application has complete control of the consistency semantics of the files used, and may vary the semantics over time to achieve the desired trade-off of performance and consistency. 3. Mentat Programming Environment We have begun implementation of ELFS using the Mentat parallel programming system <ref> [4, 5, 7] </ref>. Mentat is an object-oriented, parallel computation system designed to provide easy-to-use parallelism for parallel and distributed systems. Mentat alleviates most of the burden of explicit parallelization that message passing systems typically place on the programmer.
Reference: [6] <author> A. S. Grimshaw, and J. Prem, </author> <title> High Performance Parallel File Objects, </title> <booktitle> To appear in 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR., </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Finally, variable_consistency_files can be used to avoid the consistency semantics of NFS files [9]. 2.2.1 Parallel File Objects - pfos Parallel file objects attack both the bandwidth and latency problems on parallel and distributed systems. A detailed description, including performance, can be found in <ref> [6] </ref>. In addition to the unix_file operators, pfos provide operators that allow the user to: 2. The distinction between independent and contained objects is not unusual, and is driven by efficiency considerations. 6 1) specify the structure of the file, e.g. 1D array, 2D array, 3D array.
Reference: [7] <author> A. S. Grimshaw and Jane W. S. Liu. </author> <title> Mentat: An Object-Oriented Macro Data Flow System, </title> <booktitle> Proceedings of OOPSLA 87. ACM, </booktitle> <month> October </month> <year> 1987. </year>
Reference-contexts: When complete, ELFS will consist of two parts: the definition of a file system class hierarchy, and compiler and run-time system services that support asynchronous objects and multiple outstanding requests without explicit programmer control. The system will be implemented using the parallel programming language MPL <ref> [5, 7] </ref>, which is an extension of C++ [19]. The class hierarchy in our system contains the base class unix_file (see Figure 1).. This class supports the standard UNIX file operations, creat, open, close, lseek, read, and write. <p> When using a variable_consistency_file an application has complete control of the consistency semantics of the files used, and may vary the semantics over time to achieve the desired trade-off of performance and consistency. 3. Mentat Programming Environment We have begun implementation of ELFS using the Mentat parallel programming system <ref> [4, 5, 7] </ref>. Mentat is an object-oriented, parallel computation system designed to provide easy-to-use parallelism for parallel and distributed systems. Mentat alleviates most of the burden of explicit parallelization that message passing systems typically place on the programmer.
Reference: [8] <author> D. F. Kotz and C. S. Ellis. </author> <title> Prefetching in File Systems for MIMD Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 218-230, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Very high bandwidth is achieved by scattering pieces of the pfo to multiple devices, each of which may be accessed in parallel (see Figure 2). This idea has been studied extensively <ref> [2, 8, 9, 13, 14, 15, 16] </ref>. What distinguishes our work from others is that the decomposition may be specified using pfos, e.g., row-wise, column-wise, in blocks, etc. This is in contrast to systems such as CFS [14]. CFS stripes the file across the disks in sequential 4K chunks. <p> Related Work Other researchers have identified the potential I/O bottleneck in high performance computers <ref> [8, 12] </ref>. Most of the proposed solutions involve caching, logging, prefetching or parallel disk hardware [8, 12, 14]. These solutions all require special hardware or operating system modifications and focus strictly on I/O performance. The solution presented in this report is broader in scope. <p> Related Work Other researchers have identified the potential I/O bottleneck in high performance computers [8, 12]. Most of the proposed solutions involve caching, logging, prefetching or parallel disk hardware <ref> [8, 12, 14] </ref>. These solutions all require special hardware or operating system modifications and focus strictly on I/O performance. The solution presented in this report is broader in scope.
Reference: [9] <author> E. Levy and A. Silbershatz. </author> <title> Distributed File Systems: Concepts and Examples. </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 22, no. 4, </volume> <pages> pp. 321-374, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Binary_tree_files illustrate how aggressive prefetching for non-sequential data structures based on object structure, in this case a tree, can be performed, providing a performance improvement over traditional file systems. Finally, variable_consistency_files can be used to avoid the consistency semantics of NFS files <ref> [9] </ref>. 2.2.1 Parallel File Objects - pfos Parallel file objects attack both the bandwidth and latency problems on parallel and distributed systems. A detailed description, including performance, can be found in [6]. In addition to the unix_file operators, pfos provide operators that allow the user to: 2. <p> Very high bandwidth is achieved by scattering pieces of the pfo to multiple devices, each of which may be accessed in parallel (see Figure 2). This idea has been studied extensively <ref> [2, 8, 9, 13, 14, 15, 16] </ref>. What distinguishes our work from others is that the decomposition may be specified using pfos, e.g., row-wise, column-wise, in blocks, etc. This is in contrast to systems such as CFS [14]. CFS stripes the file across the disks in sequential 4K chunks. <p> Recall first the consistency semantics of UNIX files, where writes to a file by a process are immediately visible by other processes. Two distributed file systems, NFS [18] and Sprite [11], take different approaches to consistency <ref> [9] </ref>. NFS does not support UNIX semantics [9]. In order to improve performance, NFS caches data blocks at the clients machine, improving performance at the expense of consistency. The designers of NFS made this trade-off because they felt performance was paramount. <p> Recall first the consistency semantics of UNIX files, where writes to a file by a process are immediately visible by other processes. Two distributed file systems, NFS [18] and Sprite [11], take different approaches to consistency <ref> [9] </ref>. NFS does not support UNIX semantics [9]. In order to improve performance, NFS caches data blocks at the clients machine, improving performance at the expense of consistency. The designers of NFS made this trade-off because they felt performance was paramount. Sprite on the other hand, preserves UNIX semantics no matter the cost.
Reference: [10] <author> P. W. Madany, R. H. Campbell, V. F. Russo, and D. E. Leyens. </author> <title> A Class Hierarchy for Building Stream-Oriented File Systems. </title> <type> TR no. </type> <institution> UIUCDCS-R-89-1507, Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: The solution presented in this report is broader in scope. By providing a framework for developing application-specific file abstractions, ELFS reduces the overall effort involved in programming high performance computers as well as addressing the I/O performance problem. The memory management and file systems of the Choices <ref> [10, 17] </ref> operating system provide some of the same functionality as the file abstraction framework presented in this report. ELFS differs from the CHOICES file system in four significant ways. First, ELFS enables the specification of file behavior at the application level, not at the operating system level.
Reference: [11] <author> M. Nelson, B. Welch, and J. K. Ousterhout. </author> <title> Caching in the Sprite Network File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 6, no. 1. </volume> <month> February, </month> <year> 1988. </year>
Reference-contexts: Recall first the consistency semantics of UNIX files, where writes to a file by a process are immediately visible by other processes. Two distributed file systems, NFS [18] and Sprite <ref> [11] </ref>, take different approaches to consistency [9]. NFS does not support UNIX semantics [9]. In order to improve performance, NFS caches data blocks at the clients machine, improving performance at the expense of consistency. The designers of NFS made this trade-off because they felt performance was paramount.
Reference: [12] <author> J. Osterhout and F. Douglas. </author> <title> Beating the I/O Bottleneck: A Case for Log-Structured File Systems. </title> <journal> Operating Systems Review, </journal> <volume> vol. 23, no. 1, </volume> <pages> pp. 11-28, </pages> <month> January, </month> <year> 1989. </year>
Reference-contexts: CPU speeds have increased over one hundred fold in the last ten years, yet I/O latencies have improved only by a factor of about four and I/O bandwidths by a factor of about ten. The result is that I/O is increasingly a bottleneck <ref> [1, 12] </ref>. Even on a single processor IBM Risc System 6000, in the approximately 22ms it takes to satisfy a RANDOM I/O request, 1.84 million oating point operations can be performed. The problem is particularly acute for the new high performance parallel architectures. <p> Related Work Other researchers have identified the potential I/O bottleneck in high performance computers <ref> [8, 12] </ref>. Most of the proposed solutions involve caching, logging, prefetching or parallel disk hardware [8, 12, 14]. These solutions all require special hardware or operating system modifications and focus strictly on I/O performance. The solution presented in this report is broader in scope. <p> Related Work Other researchers have identified the potential I/O bottleneck in high performance computers [8, 12]. Most of the proposed solutions involve caching, logging, prefetching or parallel disk hardware <ref> [8, 12, 14] </ref>. These solutions all require special hardware or operating system modifications and focus strictly on I/O performance. The solution presented in this report is broader in scope.
Reference: [13] <author> D. Patterson, G. Gibson, and R. Katz. </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID). </title> <booktitle> Proceedings of the International Conference on Management of Data, </booktitle> <pages> pp. 108-116, </pages> <month> June, </month> <year> 1988. </year>
Reference-contexts: Very high bandwidth is achieved by scattering pieces of the pfo to multiple devices, each of which may be accessed in parallel (see Figure 2). This idea has been studied extensively <ref> [2, 8, 9, 13, 14, 15, 16] </ref>. What distinguishes our work from others is that the decomposition may be specified using pfos, e.g., row-wise, column-wise, in blocks, etc. This is in contrast to systems such as CFS [14]. CFS stripes the file across the disks in sequential 4K chunks.
Reference: [14] <author> P. Pierce. </author> <title> A Concurrent File System for a Highly Parallel Mass Storage Subsystem. </title> <booktitle> Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, Monterey, </booktitle> <pages> pp. 155-160, </pages> <month> March, </month> <year> 1989. </year>
Reference-contexts: Very high bandwidth is achieved by scattering pieces of the pfo to multiple devices, each of which may be accessed in parallel (see Figure 2). This idea has been studied extensively <ref> [2, 8, 9, 13, 14, 15, 16] </ref>. What distinguishes our work from others is that the decomposition may be specified using pfos, e.g., row-wise, column-wise, in blocks, etc. This is in contrast to systems such as CFS [14]. CFS stripes the file across the disks in sequential 4K chunks. <p> This idea has been studied extensively [2, 8, 9, 13, 14, 15, 16]. What distinguishes our work from others is that the decomposition may be specified using pfos, e.g., row-wise, column-wise, in blocks, etc. This is in contrast to systems such as CFS <ref> [14] </ref>. CFS stripes the file across the disks in sequential 4K chunks. This decomposition will not be appropriate for all applications. <p> Related Work Other researchers have identified the potential I/O bottleneck in high performance computers [8, 12]. Most of the proposed solutions involve caching, logging, prefetching or parallel disk hardware <ref> [8, 12, 14] </ref>. These solutions all require special hardware or operating system modifications and focus strictly on I/O performance. The solution presented in this report is broader in scope.
Reference: [15] <author> A. L. N. Reddy and P. Banerjee. </author> <title> An Evolution of Multiple-Disk I/O Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 38, no. 12, </volume> <pages> pp. 1680-1690, </pages> <month> December, </month> <year> 1989. </year>
Reference-contexts: Very high bandwidth is achieved by scattering pieces of the pfo to multiple devices, each of which may be accessed in parallel (see Figure 2). This idea has been studied extensively <ref> [2, 8, 9, 13, 14, 15, 16] </ref>. What distinguishes our work from others is that the decomposition may be specified using pfos, e.g., row-wise, column-wise, in blocks, etc. This is in contrast to systems such as CFS [14]. CFS stripes the file across the disks in sequential 4K chunks.
Reference: [16] <author> A. L. N. Reddy and P. Banerjee. </author> <title> Design, Analysis, and Simulation of I/O Architectures for Hypercube Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 1, no. </volume> <pages> 2, </pages> <address> pp.140-151, April,1990. </address>
Reference-contexts: Very high bandwidth is achieved by scattering pieces of the pfo to multiple devices, each of which may be accessed in parallel (see Figure 2). This idea has been studied extensively <ref> [2, 8, 9, 13, 14, 15, 16] </ref>. What distinguishes our work from others is that the decomposition may be specified using pfos, e.g., row-wise, column-wise, in blocks, etc. This is in contrast to systems such as CFS [14]. CFS stripes the file across the disks in sequential 4K chunks.
Reference: [17] <author> V. F. Russo and R. H. Campbell. </author> <title> Virtual Memory and Backing Storage Management in Multiprocessor Operating Systems Using Object-Oriented Design Techniques. </title> <booktitle> Proceedings of OOPSLA 89. ACM 1989. </booktitle>
Reference-contexts: The solution presented in this report is broader in scope. By providing a framework for developing application-specific file abstractions, ELFS reduces the overall effort involved in programming high performance computers as well as addressing the I/O performance problem. The memory management and file systems of the Choices <ref> [10, 17] </ref> operating system provide some of the same functionality as the file abstraction framework presented in this report. ELFS differs from the CHOICES file system in four significant ways. First, ELFS enables the specification of file behavior at the application level, not at the operating system level.
Reference: [18] <author> R. Sandberg, D Goldberg, S. Kleiman, D. Walsh, and B. Lyone. </author> <title> Design and Implementation of the Sun Network File System. </title> <booktitle> In Proceedings of Usenix 1985 Summer Conference, </booktitle> <pages> pp. 119-130, </pages> <month> June, </month> <year> 1985. </year>
Reference-contexts: Recall first the consistency semantics of UNIX files, where writes to a file by a process are immediately visible by other processes. Two distributed file systems, NFS <ref> [18] </ref> and Sprite [11], take different approaches to consistency [9]. NFS does not support UNIX semantics [9]. In order to improve performance, NFS caches data blocks at the clients machine, improving performance at the expense of consistency. The designers of NFS made this trade-off because they felt performance was paramount.
Reference: [19] <author> B. Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachu-setts, </address> <year> 1986. </year>
Reference-contexts: The system will be implemented using the parallel programming language MPL [5, 7], which is an extension of C++ <ref> [19] </ref>. The class hierarchy in our system contains the base class unix_file (see Figure 1).. This class supports the standard UNIX file operations, creat, open, close, lseek, read, and write.
Reference: [20] <author> B. Duquet, and T. Cornwell. </author> <title> Deconvolution on the Digital Production Cray X-MP, p. 10, </title> <journal> NRAO Newsletter, </journal> <volume> no. 24, </volume> <month> July 1, </month> <year> 1985. </year>
Reference-contexts: Evidence of this problem abounds. NRAO (the National Radio Astronomy Observatory), for example, has many database bound applications. One, a deconvolution algorithm, consumes 20 minutes of Cray CPU time, yet takes over 10 hours of wall clock time. The difference is due to database waits <ref> [20] </ref>. The advent of highly parallel architectures has made the problem even worse. For example, the Intel 128 node iPSC/860 has a peak performance of 7680 double precision mega-ops, and the recently released Delta has a peak rate almost four times that of the iPSC/860.
References-found: 20

