URL: ftp://ftp.ai.mit.edu/pub/users/gideon/papers/iccv95-draft2.ps.gz
Refering-URL: http://www.ai.mit.edu/people/gideon/gideon.html
Root-URL: 
Title: Accurate Internal Camera Calibration using Rotation, with Analysis of Sources of Error  
Author: G. P. Stein 
Address: Cambridge, MA, 02139  
Affiliation: Artificial Intelligence Laboratory MIT  
Abstract: 1 This paper describes a simple and accurate method for internal camera calibration based on tracking image features through a sequence of images while the camera undergoes pure rotation. A special calibration object is not required and the method can therefore be used both for laboratory calibration and for self calibration in autonomous robots. Experimental results with real images show that focal length and aspect ratio can be found to within 0.15 percent, and lens distortion error can be reduced to a fraction of a pixel. The location of the principal point and the location of the center of radial distortion can each be found to within a few pixels. We perform a simple analysis to show to what extent the various technical details affect the accuracy of the results. We show that having pure rotation is important if the features are derived from objects close to the camera. In the basic method accurate angle measurement is important. The need to accurately measure the angles can be eliminated by rotating the camera through a complete circle while taking an overlapping sequence of images and using the constraint that the sum of the angles must equal 360 degrees. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Basu, A., </author> <title> "Active Calibration: </title> <booktitle> Alternative Strategy and Analysis" In Proceedings of IEEE Conference on Computer Vision and Pattern recognition, </booktitle> <pages> 495-500, </pages> <address> New York, NY, </address> <month> June </month> <year> (1993) </year>
Reference-contexts: Simulation results indicate that the method is very sensitive even to small errors in feature location. Methods that use camera rotation are presented in <ref> [1] </ref> [3] [5]. Basu [1] tracks the motion of contour lines in the image as the camera pans or tilts through small angles. No lens distortion model is used. The experimental results show that the focal length and scale factor can be found within 3 percent error. <p> Simulation results indicate that the method is very sensitive even to small errors in feature location. Methods that use camera rotation are presented in <ref> [1] </ref> [3] [5]. Basu [1] tracks the motion of contour lines in the image as the camera pans or tilts through small angles. No lens distortion model is used. The experimental results show that the focal length and scale factor can be found within 3 percent error. <p> In this paper all the parameters are found in one step. The method for finding the focal length presented in [3] (and in <ref> [1] </ref>) uses small angle approximations to predict the motion of points in the image. On the other hand, for signal to noise reasons, large rotations are required to make accurate measurements. In this paper the full 3D rotation equations are used which are accurate for large and small angles. <p> As we will only be using the center part of the image the accuracy of the feature detector will also be reduced. 6.5 Use of small angle approximations Both [3] and <ref> [1] </ref> use small angle approximations to reach closed form solutions for the camera parameters. These approximations, sin () ' and cos () ' 1, are only good for very small angles.
Reference: [2] <author> Brown, </author> <note> D.C.,"Close -range Camera Calibration" Photogrammetric Engineering 37 855-866 (1971) </note>
Reference-contexts: For more about this issue see [9][10]. Geometric objects whose images have some characteristic that is invariant to the actual position of the object in space, can be used to calibrate some of the internal camera parameters. Results based on the plumb line method <ref> [2] </ref>, which uses the images of straight lines for calibrating lens distortion, and a method for finding the aspect ratio using spheres [8], are given in section 7 for comparison to the rotation method. More can be found in [10].
Reference: [3] <author> Du, F. and Brady, M., </author> <title> "Self Calibration of the Intrinsic Parameters of Cameras for Active Vision Systems" In Proceedings of IEEE Conference on Computer Vision and Pattern recognition, </title> <address> 477-482, New York, NY, </address> <month> June </month> <year> (1993) </year>
Reference-contexts: Simulation results indicate that the method is very sensitive even to small errors in feature location. Methods that use camera rotation are presented in [1] <ref> [3] </ref> [5]. Basu [1] tracks the motion of contour lines in the image as the camera pans or tilts through small angles. No lens distortion model is used. The experimental results show that the focal length and scale factor can be found within 3 percent error. <p> No lens distortion model is used. The experimental results show that the focal length and scale factor can be found within 3 percent error. No experimental results were given for the principal point. In <ref> [3] </ref> first the principal point is found and then using the principal point the focal length and scale factor are found, and finally lens distortion parameters. <p> Simulation results show that given features with an accuracy of 0:2 pixels the principal point can be found to within a few pixels and the focal length can be found to within 1 percent. The accuracy of the experimental results are not shown. The work in <ref> [3] </ref> is the most closely related to this paper and a comparison is in order. [3] uses three separate stages to recover the camera parameters. In this paper all the parameters are found in one step. The method for finding the focal length presented in [3] (and in [1]) uses small <p> The accuracy of the experimental results are not shown. The work in <ref> [3] </ref> is the most closely related to this paper and a comparison is in order. [3] uses three separate stages to recover the camera parameters. In this paper all the parameters are found in one step. The method for finding the focal length presented in [3] (and in [1]) uses small angle approximations to predict the motion of points in the image. <p> The work in <ref> [3] </ref> is the most closely related to this paper and a comparison is in order. [3] uses three separate stages to recover the camera parameters. In this paper all the parameters are found in one step. The method for finding the focal length presented in [3] (and in [1]) uses small angle approximations to predict the motion of points in the image. On the other hand, for signal to noise reasons, large rotations are required to make accurate measurements. <p> On the other hand, for signal to noise reasons, large rotations are required to make accurate measurements. In this paper the full 3D rotation equations are used which are accurate for large and small angles. Finding the radial distortion in <ref> [3] </ref> requires being able to rotate the camera accurately around the XY axes of the camera. This is a rather complex mechanical setup. In the method presented here, one is only required to be able to rotate about the center of projection. <p> One could try to use points that are located only near the center of the image as in <ref> [3] </ref>. The effects of radial distortion will be much smaller but that will also limit one to small angle rotations and loss in angle measurement accuracy. <p> As we will only be using the center part of the image the accuracy of the feature detector will also be reduced. 6.5 Use of small angle approximations Both <ref> [3] </ref> and [1] use small angle approximations to reach closed form solutions for the camera parameters. These approximations, sin () ' and cos () ' 1, are only good for very small angles.
Reference: [4] <editor> Faugeras,O.D., et al., </editor> <booktitle> "Camera Self-Calibration : Theory and Experiments" In Proceedings of ECCV, </booktitle> <pages> 321-334, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> May </month> <year> (1992) </year>
Reference-contexts: Since the traditional methods of camera calibration use known world coordinates they are not suitable for self calibration of mobile robots. To calibrate a camera using only feature coordinates in the image plane one must have more than a single image. Faugeras et al. <ref> [4] </ref> develop a method where a motion sequence of a camera moving in an unconstrained manner can be used to calculate the internal camera parameters. Simulation results indicate that the method is very sensitive even to small errors in feature location.
Reference: [5] <author> Hartley, R.I., </author> <title> "Self-Calibration from Multiple Views with a Rotating Camera" in Proceedings of the European Conference on Computer Vision (1994) </title>
Reference-contexts: Simulation results indicate that the method is very sensitive even to small errors in feature location. Methods that use camera rotation are presented in [1] [3] <ref> [5] </ref>. Basu [1] tracks the motion of contour lines in the image as the camera pans or tilts through small angles. No lens distortion model is used. The experimental results show that the focal length and scale factor can be found within 3 percent error. <p> This is a rather complex mechanical setup. In the method presented here, one is only required to be able to rotate about the center of projection. Recent work by Hartley <ref> [5] </ref> has shown that given a perfect perspective projection model it is possible in theory to recover both the camera parameters and the angle of rotation. This has been shown to work nicely in simulation but no accuracy results have been shown with real images. <p> An increase in the focal length estimate can be nearly completely compensated for by decreasing the angle estimate. In practice we could not achieve better than 4 percent accuracy in focal length estimates using the iterative method presented in <ref> [5] </ref>. 1.3 Brief overview of the rotation method The basic idea of the rotation method is very simple.
Reference: [6] <author> Longuet-Higgins, H.C., </author> <title> "A computer algorithm for reconstructing a scene from two projections" Nature 293,133-135 (1981) </title>
Reference-contexts: Camera calibration is important if we wish to derive metric information from the images. Typical applications would be structure from motion and pose estimation which often use the perspective projection model and assume that the internal camera parameters are known <ref> [6] </ref>. For such work, accurately calibrating the internal camera parameters is critical but the external camera calibration is not important. 1.2 Related work More extensive reviews of calibration methods appear in [9][10][11]. Most techniques for camera calibration use a set of points with known world coordinates (control points).
Reference: [7] <institution> More, J.J., et al., "User Guide for Minpack-1" Argonne National Laboratory, Argonne, Illinois (1980) </institution>
Reference-contexts: The details of the features and feature detector are not critical and are presented fully in [10]. 4.4 Nonlinear optimization The camera parameters were found using a nonlinear optimization program based on the subroutine LMDIF from the software package MINPACK-1 <ref> [7] </ref>. This subroutine uses a modified Levenberg-Marquart algorithm. The program typically takes about 6 iterations to get the first 5 digits and 12 iterations to terminate. For M = 10 and N = 20 this took 11 seconds on a SPARCstation ELC.
Reference: [8] <author> Penna, M.A. </author> <title> "Camera Calibration: A quick and Easy Way to Determine the Scale Factor" IEEE Trans. </title> <journal> Pattern Anal. Machine Intell. </journal> <month> 13,1240-1245 </month> <year> (1991) </year>
Reference-contexts: Results based on the plumb line method [2], which uses the images of straight lines for calibrating lens distortion, and a method for finding the aspect ratio using spheres <ref> [8] </ref>, are given in section 7 for comparison to the rotation method. More can be found in [10]. Since the traditional methods of camera calibration use known world coordinates they are not suitable for self calibration of mobile robots.
Reference: [9] <editor> Slama, C.C. ed, </editor> <title> Manual of Photogramme-try,4th edition, </title> <journal> American Society of Photogram-metry (1980). </journal>
Reference-contexts: but the external camera calibration is not important. 1.2 Related work More extensive reviews of calibration methods appear in <ref> [9] </ref>[10][11]. Most techniques for camera calibration use a set of points with known world coordinates (control points). Control points can be from a calibration object [11] or a known outdoor scene [9]. The calibration process can be stated as follows: given a set of control points (X i ; Y i ; Z i ) and their image (x i ; y i ) find the external and internal parameters which will best map the control points to their image. <p> A problem arises due to the interaction between the external and the internal parameters. The Manual of Photogrammetry <ref> [9] </ref> claims that "the strong coupling that exists between interior elements of orientation [principal point and focal length] and exterior elements can be expected to result in unacceptably large variances for these particular projective parameters when recovered on a frame-by-frame basis". The main problem is the error in focal length. <p> The standard model for lens distortion <ref> [9] </ref> is a mapping from the distorted image coordinates, (x d ; y d ), that are observable, to the undistorted image plane coordinates, (x u ; y u ): x u = x d + x 0 0 2 0 4 y u = y d + y 0 0 <p> + y d = (x 2 d c yr ) 2 (3) It has been shown in [10] that allowing the center of radial distortion, (c xr ; c yr ) to be different from the principal point is equivalent to adding a term for decenter-ing distortion as given in <ref> [9] </ref>. Finally, ~ P is converted to frame buffer coordinates: x = S y = y d + c y where (c x ; c y ) is the principal point in frame buffer coordinates and S is the aspect ratio or scale factor of the image.
Reference: [10] <author> Stein, </author> <title> G.P., "Internal Camera Calibration using Rotation and Geometric Shapes" AITR-1426, </title> <type> Master's Thesis, </type> <institution> Massachussets Institute of Technology, Artificial Intelligence Laboratory (1993). </institution>
Reference-contexts: Results based on the plumb line method [2], which uses the images of straight lines for calibrating lens distortion, and a method for finding the aspect ratio using spheres [8], are given in section 7 for comparison to the rotation method. More can be found in <ref> [10] </ref>. Since the traditional methods of camera calibration use known world coordinates they are not suitable for self calibration of mobile robots. To calibrate a camera using only feature coordinates in the image plane one must have more than a single image. <p> y u ): x u = x d + x 0 0 2 0 4 y u = y d + y 0 0 2 0 4 where: r d = x d + y d = (x 2 d c yr ) 2 (3) It has been shown in <ref> [10] </ref> that allowing the center of radial distortion, (c xr ; c yr ) to be different from the principal point is equivalent to adding a term for decenter-ing distortion as given in [9]. <p> In practice we found that the repeatability of angular measurements was between 1:22 0 <ref> [10] </ref>. 4.2 How to obtain pure rotation ? To obtain pure rotation the camera is mounted on an XY stage that has a fine adjustment. This in turn, is mounted on the rotation stage. <p> The image is viewed on the TV monitor (b). detected. We found we could position the center of projection less than 0:5 mm from the axis of rotation. For more details see <ref> [10] </ref>. 4.3 Features and feature detector The rotation method does not require any special calibration object but it does require that features can be detected reliably in the image. <p> In order to simplify the feature extraction process we used the corners in a black and white checkerboard pattern printed on 8 fi 11 inch paper. The details of the features and feature detector are not critical and are presented fully in <ref> [10] </ref>. 4.4 Nonlinear optimization The camera parameters were found using a nonlinear optimization program based on the subroutine LMDIF from the software package MINPACK-1 [7]. This subroutine uses a modified Levenberg-Marquart algorithm. The program typically takes about 6 iterations to get the first 5 digits and 12 iterations to terminate.
Reference: [11] <author> Weng, J et al. </author> <title> "Camera Calibration with Distortion Models and Accuracy Evaluation" IEEE Trans. </title> <journal> Pattern Anal. Machine Intell. </journal> <month> 14,965-980 </month> <year> (1992) </year>
Reference-contexts: the internal camera parameters is critical but the external camera calibration is not important. 1.2 Related work More extensive reviews of calibration methods appear in [9][10]<ref> [11] </ref>. Most techniques for camera calibration use a set of points with known world coordinates (control points). Control points can be from a calibration object [11] or a known outdoor scene [9].
References-found: 11

