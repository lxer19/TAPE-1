URL: ftp://rtcl.eecs.umich.edu/outgoing/jdolter/diss.ps.Z
Refering-URL: http://www.eecs.umich.edu/RTCL/harts/
Root-URL: http://www.cs.umich.edu
Title: ABSTRACT A PROGRAMMABLE ROUTING CONTROLLER SUPPORTING MULTI-MODE ROUTING AND SWITCHING IN DISTRIBUTED REAL-TIME SYSTEMS.  
Author: by James William Dolter Chair: Kang G. Shin 
Abstract: Distributed systems with point-to-point interconnection networks are natural candidate architectures for supporting embedded real-time applications due mainly to their potential for high-performance and high-dependability with the multiplicity of processors and in-ternode routes. In addition, distributed systems based on point-to-point interconnection networks also hold great promise for delivering systems with some level of intermediate dependability: something not as expensive and complete as the ultra-dependable systems used in mission-critical applications but more dependable than systems constructed using standard networking techniques and commercially-available components. This dissertation investigates an area of the design space in which a homogeneous distributed computing system is constructed that has the potential for an intermediate level of dependability. Most of the characteristics of this system can be traced to either the parallel computing domain, the distributed systems domain, or the ultra-dependable systems domain. The unifying feature not explicitly present in the three "parent" domains is the flexibility of the underlying communication support hardware which allows these other features to co-exist while supporting the end goals. One of the central themes of this dissertation is that if flexibility is provided in the front-end routing hardware, the management of issues related to operating in this hybrid domain are then possible. To that end, most of the results presented here focus on the development and implementation of a programmable routing controller (PRC) that acts both as a single sample in the design space and supports further exploration of the design space. Specifically, this work proposes an architecture and an implementation for a front-end programmable routing controller that supports multiple routing and switching schemes. We then proceed to analyze the performance of one of the supported switching schemes for a hexagonal mesh interconnection network based on the capabilities provided by the PRC. We then conclude by showing how the capability of supporting multiple routing and switching schemes simultaneously offers great potential for supporting real-time communication subsystems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Am79168/Am79169-275 TAXI-275 Transmitter/Receiver Transparent Asynchronous Transmitter/Receiver Interface, Advanced Micro Devices, </institution> <address> 15765-b-0 edition. </address>
Reference-contexts: The PRC combines the functionality of the previously-designed Routing Controller [20] and Packet Controller [15] as well as exploiting the availability of the AM79168 TAXIchip Transmitters and the AM79169 TAXIchip Receivers <ref> [2, 1] </ref>. This design greatly improves performance over its predecessors while preserving the experimental flexibility and support for distributed real-time communication required for HARTS. <p> ) 01 - f 02 int src row = src = edge dimension; 03 int src col = src % edge dimension; 04 - 05 int delta row = mod (src row + offset [0] - offset [2], edge dimension); 06 int delta col = mod (src col + offset <ref> [1] </ref> - offset [3], edge dimension); 07 - 08 return edge dimension fl delta row + delta col; 09 - g class and SQMesh class it is relatively straightforward to implement as shown on lines 23-31 in Figure 5.4 and lines 21-46 in Figure 5.5. <p> = 2 fl E - t - 2; 38 - retval [2] = r + 1; 39 - g 40 else f 41 if (t 2 fl E + r - 1) f 42 - retval [0] = t - 2 fl E - r + 1; 43 - retval <ref> [1] </ref> = r + 1; 44 - g 45 else f 46 - retval [2] = 2 fl E + r - t - 1; 47 - retval [1] = t + 2 - 2 fl E; 48 - g 50 - g 52 - g 53 return retval; 54 - <p> + r - 1) f 42 - retval [0] = t - 2 fl E - r + 1; 43 - retval <ref> [1] </ref> = r + 1; 44 - g 45 else f 46 - retval [2] = 2 fl E + r - t - 1; 47 - retval [1] = t + 2 - 2 fl E; 48 - g 50 - g 52 - g 53 return retval; 54 - g 115 00 - OffsetVec SQMesh::map ( NodeId src, NodeId dest ) 01 - f 02 - OffsetVec retval (max direction,0); 03 - 04 - src %= total <p> dest col - src col ; 15 - 16 if ( delta row ) 17 - retval [2] = delta row; 18 else 19 - retval [0] = delta row; 20 - 21 if ( delta col ) 22 - retval [3] = delta col; 23 else 24 - retval <ref> [1] </ref> = delta col; 25 - 26 return retval; 27 - g 116 5.4 Event Management and Flow 5.4.1 Event queue Two of the major problems that needed to be overcome during the development of pp-mess-sim were the representation of time and the efficient management of the queue of pending events.
Reference: [2] <institution> Am79168/Am79169 TAXI tm -275 Technical Manual, Advanced Micro Devices, </institution> <address> ban-0.1m-1/93/0 17490a edition. </address>
Reference-contexts: The PRC combines the functionality of the previously-designed Routing Controller [20] and Packet Controller [15] as well as exploiting the availability of the AM79168 TAXIchip Transmitters and the AM79169 TAXIchip Receivers <ref> [2, 1] </ref>. This design greatly improves performance over its predecessors while preserving the experimental flexibility and support for distributed real-time communication required for HARTS. <p> return dest; 11 - 00 NodeId SQMesh::translate ( NodeId src , OffsetVec offset ) 01 - f 02 int src row = src = edge dimension; 03 int src col = src % edge dimension; 04 - 05 int delta row = mod (src row + offset [0] - offset <ref> [2] </ref>, edge dimension); 06 int delta col = mod (src col + offset [1] - offset [3], edge dimension); 07 - 08 return edge dimension fl delta row + delta col; 09 - g class and SQMesh class it is relatively straightforward to implement as shown on lines 23-31 in Figure <p> - 1; 31 - g 33 - g 34 else f 35 - =fl destination is in the upper part of H-mesh centered at source fl= 36 if (t 2 fl E - 2) f 37 - retval [3] = 2 fl E - t - 2; 38 - retval <ref> [2] </ref> = r + 1; 39 - g 40 else f 41 if (t 2 fl E + r - 1) f 42 - retval [0] = t - 2 fl E - r + 1; 43 - retval [1] = r + 1; 44 - g 45 else f 46 <p> + 1; 39 - g 40 else f 41 if (t 2 fl E + r - 1) f 42 - retval [0] = t - 2 fl E - r + 1; 43 - retval [1] = r + 1; 44 - g 45 else f 46 - retval <ref> [2] </ref> = 2 fl E + r - t - 1; 47 - retval [1] = t + 2 - 2 fl E; 48 - g 50 - g 52 - g 53 return retval; 54 - g 115 00 - OffsetVec SQMesh::map ( NodeId src, NodeId dest ) 01 - <p> row = dest = edge dimension; 11 int dest col = dest % edge dimension; 12 - 13 int delta row = dest row - src row ; 14 int delta col = dest col - src col ; 15 - 16 if ( delta row ) 17 - retval <ref> [2] </ref> = delta row; 18 else 19 - retval [0] = delta row; 20 - 21 if ( delta col ) 22 - retval [3] = delta col; 23 else 24 - retval [1] = delta col; 25 - 26 return retval; 27 - g 116 5.4 Event Management and Flow
Reference: [3] <author> G. Albertengo and R. Sisto, </author> <title> "Parallel crc generation," </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 63-71, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Another drawback to serial CRC generators and checkers is the time required to process a single n-bit word of data. As noted in <ref> [3] </ref>, it takes n clock pulses for a serial CRC implementation to process a single word of data. By contrast, an n-bit parallel CRC generator requires only a single cycle. <p> For this reason, and since data is not always available in serial form, parallel CRC generators have been studied by a number of researchers. Implementations have ranged from variations on the software approach using a lookup table stored in ROM [31] to fully parallel encoders <ref> [3, 37] </ref>. For the PRC, packet data is only available in parallel form and at data rates such 58 that serial computation is not feasible. This leads to the selection of a parallel CRC with the obvious drawback of the number of gates required for an implementation. <p> f 02 int src row = src = edge dimension; 03 int src col = src % edge dimension; 04 - 05 int delta row = mod (src row + offset [0] - offset [2], edge dimension); 06 int delta col = mod (src col + offset [1] - offset <ref> [3] </ref>, edge dimension); 07 - 08 return edge dimension fl delta row + delta col; 09 - g class and SQMesh class it is relatively straightforward to implement as shown on lines 23-31 in Figure 5.4 and lines 21-46 in Figure 5.5. <p> int (total nodes); 03 int E = int (edge dimension); 04 - OffsetVec retval (max direction,0); 05 int k = mod (dest - src, N); 06 - 08 - retval [0] = k; 09 - g 10 else f 11 if (k &gt; N - E ) 12 - retval <ref> [3] </ref> = N - k; 13 else f 14 int t = mod ((k - E), (3 fl E - 2)); 16 - 18 - =fl destination is in the lower part of H-mesh centered at source fl= 19 if (t r) f 20 - retval [3] = r - t <p> ) 12 - retval <ref> [3] </ref> = N - k; 13 else f 14 int t = mod ((k - E), (3 fl E - 2)); 16 - 18 - =fl destination is in the lower part of H-mesh centered at source fl= 19 if (t r) f 20 - retval [3] = r - t ; 21 - retval [4] = E - r - 1 ; 22 - g 23 else f 24 if (t E - 1) f 25 - retval [0] = t - E + 1; 26 - retval [5] = E - r - 1; 27 <p> = t - r; 30 - retval [4] = E - t - 1; 31 - g 33 - g 34 else f 35 - =fl destination is in the upper part of H-mesh centered at source fl= 36 if (t 2 fl E - 2) f 37 - retval <ref> [3] </ref> = 2 fl E - t - 2; 38 - retval [2] = r + 1; 39 - g 40 else f 41 if (t 2 fl E + r - 1) f 42 - retval [0] = t - 2 fl E - r + 1; 43 - retval <p> - src row ; 14 int delta col = dest col - src col ; 15 - 16 if ( delta row ) 17 - retval [2] = delta row; 18 else 19 - retval [0] = delta row; 20 - 21 if ( delta col ) 22 - retval <ref> [3] </ref> = delta col; 23 else 24 - retval [1] = delta col; 25 - 26 return retval; 27 - g 116 5.4 Event Management and Flow 5.4.1 Event queue Two of the major problems that needed to be overcome during the development of pp-mess-sim were the representation of time and
Reference: [4] <author> T. Anderson, H. Levy, B. Bershad, and E. Lazowska, </author> <title> "The interaction of architecture and operating system design," </title> <booktitle> in Proc. Int'l Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 108-120, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: PRC RX 3 instructs the RXBUS interface to store the receive time-stamp in the RXFIFO, thereby reading out the CRC error flag and signaling an RX EOP event. 41 3.3 PRC Implementation There are many specific issues that need to be addressed when approaching the implementation of any communication subsystem <ref> [4, 52, 51, 16, 34, 24, 29, 7] </ref> These include accessing shared resources, flow-control, error detection, and routing. <p> 13 else f 14 int t = mod ((k - E), (3 fl E - 2)); 16 - 18 - =fl destination is in the lower part of H-mesh centered at source fl= 19 if (t r) f 20 - retval [3] = r - t ; 21 - retval <ref> [4] </ref> = E - r - 1 ; 22 - g 23 else f 24 if (t E - 1) f 25 - retval [0] = t - E + 1; 26 - retval [5] = E - r - 1; 27 - g 28 else f 29 - retval [5] <p> 22 - g 23 else f 24 if (t E - 1) f 25 - retval [0] = t - E + 1; 26 - retval [5] = E - r - 1; 27 - g 28 else f 29 - retval [5] = t - r; 30 - retval <ref> [4] </ref> = E - t - 1; 31 - g 33 - g 34 else f 35 - =fl destination is in the upper part of H-mesh centered at source fl= 36 if (t 2 fl E - 2) f 37 - retval [3] = 2 fl E - t -
Reference: [5] <author> H. Carr, J. Evans, R. Kessler, L. Stoller, and M. Swanson, </author> <title> "Mayfly system software," </title> <type> Technical Report HPL-SAL-89-25, </type> <institution> Hewlett Packard Company, </institution> <month> April </month> <year> 1989. </year> <note> Available in RTCL library. </note>
Reference-contexts: Post Office (Mayfly/FAIM-1) The Post Office (PO) of the Mayfly system <ref> [5, 17, 18] </ref> (developed by Hewlett-Packard Laboratories) is a system whose architecture and functionality most closely matches that of the PRC. <p> if (t r) f 20 - retval [3] = r - t ; 21 - retval [4] = E - r - 1 ; 22 - g 23 else f 24 if (t E - 1) f 25 - retval [0] = t - E + 1; 26 - retval <ref> [5] </ref> = E - r - 1; 27 - g 28 else f 29 - retval [5] = t - r; 30 - retval [4] = E - t - 1; 31 - g 33 - g 34 else f 35 - =fl destination is in the upper part of H-mesh <p> [4] = E - r - 1 ; 22 - g 23 else f 24 if (t E - 1) f 25 - retval [0] = t - E + 1; 26 - retval <ref> [5] </ref> = E - r - 1; 27 - g 28 else f 29 - retval [5] = t - r; 30 - retval [4] = E - t - 1; 31 - g 33 - g 34 else f 35 - =fl destination is in the upper part of H-mesh centered at source fl= 36 if (t 2 fl E - 2) f 37 - retval
Reference: [6] <author> M.-S. Chen, K. G. Shin, and D. D. Kandlur, </author> <title> "Addressing, routing and broadcasting in hexagonal mesh multiprocessors," </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. 39, no. 1, </volume> <pages> pp. 10-18, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: This chapter presents a brief background on the target system (i.e., HARTS), as well as examining and classifying a number of routing and switching schemes. 2.1 C-wrapped Hexagonal Mesh One of the interconnection topologies that has received considerable attention throughout this dissertation is the C-wrapped hexagonal mesh <ref> [6, 49] </ref>. <p> Figure 2.1 shows an H 4 with the wrap links shown in gray and labeled with the nodes to which they are connected. See 8 9 <ref> [6] </ref> for a more detailed discussion of some of the properties of the C-wrapped H-Mesh and comparisons with other existing topologies. Several of the properties discussed in [6] are relevant to the development of this work. First, the C-type wrapping results in a homogeneous network. <p> See 8 9 <ref> [6] </ref> for a more detailed discussion of some of the properties of the C-wrapped H-Mesh and comparisons with other existing topologies. Several of the properties discussed in [6] are relevant to the development of this work. First, the C-type wrapping results in a homogeneous network. Consequently, any node can view itself as the center (labeled as node 0) of the mesh. This allows the physical interface to the network to be independent of the network size. <p> The six neighbors of a node in a C-wrapped H-mesh can be thought of as being in the directions d 0 , d 1 , : : : , d 5 . To send a message using the base routing algorithm presented in <ref> [6] </ref>, the source node calculates the shortest paths to the destination and encodes this routing information into three integers denoted by m 0 , m 1 and m 2 . <p> This process then continues until the PRC RX 4 has accumulated enough of the routing header to make a routing decision. In the case of the algorithm presented by Chen et al. <ref> [6] </ref>, the first 4 bytes of data are sufficient to determine the packet's type, its priority, and its destination. Based on this, the PRC RX 4 can then choose how to handle the packet. <p> A shape is a route that a packet can traverse. The above definition of a shape is motivated by the fact that all minimal routes between any pair of nodes are formed by links along one or two directions only <ref> [6] </ref>. <p> ); 08 virtual Node& operator []( NodeId ); 09 virtual NodeId neighbor ( NodeId , Direction ); 10 virtual NodeId translate ( NodeId , OffsetVec ); 11 virtual OffsetVec map ( NodeId , NodeId ); 12 virtual void error ( const charfl ); 13 private: 14 unsigned short dir v <ref> [6] </ref>; 15 - g; 17 inline CWHMesh::~CWHMesh () fg 18 - 19 inline Node& CWHMesh::node ( NodeId n ) f return fl (nodes [n]); g 20 - 21 inline Node& CWHMesh::operator []( NodeId n ) f return fl (nodes [n]);g 22 - 23 inline NodeId CWHMesh::neighbor ( NodeId n , Direction
Reference: [7] <author> D. D. Clark and D. L. Tennenhouse, </author> <title> "Architectural considerations for a new generation of communication protocols," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 200-208, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: PRC RX 3 instructs the RXBUS interface to store the receive time-stamp in the RXFIFO, thereby reading out the CRC error flag and signaling an RX EOP event. 41 3.3 PRC Implementation There are many specific issues that need to be addressed when approaching the implementation of any communication subsystem <ref> [4, 52, 51, 16, 34, 24, 29, 7] </ref> These include accessing shared resources, flow-control, error detection, and routing.
Reference: [8] <author> E. C. Cooper, P. A. Steenkiste, R. D. Ransom, and B. D. Zill, </author> <title> "Protocol implementation on the Nectar communication processor," </title> <booktitle> in Proceedings of the SIGCOMM Symposium, </booktitle> <pages> pp. 135-144. </pages> <publisher> ACM, </publisher> <month> September </month> <year> 1990. </year>
Reference-contexts: CMU's Nectar project's design and implementation of the Communication Accelerator Board (CAB) clearly identifies some of the important features that a general purpose communication system should efficiently support <ref> [8, 30, 33, 48] </ref>. Heterogeneity, scalability, low-latency, and high-bandwidth communication were the stated goals of the Nectar 72 project. To achieve these goals they designed an efficient parallel crossbar interconnection network that can be cascaded in multiple levels to support more nodes; hence satisfying the scalability, low-latency, and high-bandwidth requirements.
Reference: [9] <author> W. J. Dally and C. L. Seitz, </author> <title> "The torus routing chip," </title> <journal> Journal of Distributed Computing, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 187-196, </pages> <year> 1986. </year>
Reference-contexts: Torus Routing Chip The torus routing chip (TRC) is designed as a building block for high-throughput, low latency message-passing concurrent computers based on a byte-wide parallel k-ary n-cube interconnection network <ref> [9, 11] </ref>. The TRC implements a provable deadlock-free interconnection network by routing variable length messages using wormhole switching on two virtual channels per physical link.
Reference: [10] <author> W. J. Dally and C. L. Seitz, </author> <title> "Deadlock-free message routing in multiprocessor interconnection networks," </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. C-36, no. 5, </volume> <pages> pp. 547-553, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Since packets can hold a link while simultaneously requesting the use of other links, and circular waits are possible in the network, conditions are satisfied for the possibility of deadlock. Deadlock-free routing algorithms have been discussed previously in <ref> [10] </ref> to get around this problem and usually involve multiplexing multiple channels on the physical links and some ordering constraint on how the links are assigned. 2.4.3 Routing Methods As mentioned earlier, routing is the process that selects the path (s) that a packet will traverse when traveling from the source <p> Most notable is the efficient use of wormhole routing that appeared in the Mesh Routing Chip (MRC) used in the Amtex 2010 [42] and the theory behind constructing deadlock-free routing by defining an order in which the packets use the available virtual channels <ref> [10] </ref>. When evaluating the TRC in the intended operating domain of the PRC the fixed routing algorithm, lack of end-to-end error detection, and lack of operating system support could be considered major drawbacks. The beauty of the TRC is that it is small and simple.
Reference: [11] <author> W. Dally, </author> <title> VLSI and PARALLEL COMPUTATION, chapter Network and Processor Architecture for Message-Driven Computers, </title> <journal> pp. </journal> <pages> 140-222, </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year> <pages> 183 184 </pages>
Reference-contexts: Torus Routing Chip The torus routing chip (TRC) is designed as a building block for high-throughput, low latency message-passing concurrent computers based on a byte-wide parallel k-ary n-cube interconnection network <ref> [9, 11] </ref>. The TRC implements a provable deadlock-free interconnection network by routing variable length messages using wormhole switching on two virtual channels per physical link. <p> The beauty of the TRC is that it is small and simple. Message Driven Processor The Message-Driven Processor (MDP), like its predecessors (TRC/NDF/ADC), is designed to be part of a multicomputer capable of supporting fine grain, message passing, parallel computation <ref> [12, 11, 36, 45] </ref>. What is novel about their approach is that the processor that comprises the core of each node has been made network aware.
Reference: [12] <author> W. Dally, A. Chien, S. Fiske, W. Horwat, J. Keen, P. Nuth, J. Larivee, and B. Totty, </author> <title> "Message-driven processor architecture version 11," MIT Concurrent VLSI Architecture Memo 14, </title> <publisher> MIT, </publisher> <month> August </month> <year> 1988. </year>
Reference-contexts: The beauty of the TRC is that it is small and simple. Message Driven Processor The Message-Driven Processor (MDP), like its predecessors (TRC/NDF/ADC), is designed to be part of a multicomputer capable of supporting fine grain, message passing, parallel computation <ref> [12, 11, 36, 45] </ref>. What is novel about their approach is that the processor that comprises the core of each node has been made network aware.
Reference: [13] <author> S. W. Daniel, </author> <title> "Improving distributed system communications through hardware support," </title> <address> Prelim, </address> <year> 1992. </year>
Reference-contexts: The equations used in the generator and check units may be found in Appendix D. Information concerning the error coverage for the bit interleaved approach can be found in <ref> [13, 14] </ref>. 59 3.3.4 Operating System Support The PRC interacts with, and supports, the operating system running on the IMU in several ways. These include inbound and outbound page sequencing, interrupt management, precise time-stamping of packets, and the error detection as described previously.
Reference: [14] <author> S. W. Daniel and J. W. Dolter, </author> <title> "Error-burst detection with bit-interleaved crcs," </title> <note> In preparation, </note> <year> 1993. </year>
Reference-contexts: The equations used in the generator and check units may be found in Appendix D. Information concerning the error coverage for the bit interleaved approach can be found in <ref> [13, 14] </ref>. 59 3.3.4 Operating System Support The PRC interacts with, and supports, the operating system running on the IMU in several ways. These include inbound and outbound page sequencing, interrupt management, precise time-stamping of packets, and the error detection as described previously.
Reference: [15] <author> S. W. Daniel, </author> <title> "A packet control for HARTS: An experiemental distributed real-time system," </title> <institution> Real-Time Computing Laboratory Techinal Report RTCL-TR-1-91, Real-Time Computing Laboratory, </institution> <address> 1301 Beal Ave, Ann Arbor, MI 48109-2122, </address> <year> 1991. </year>
Reference-contexts: This device is a custom-designed application-specific IC (ASIC) that has been developed using Cascade Design Automation's EPOCH system and Cadence Design Systems' Verilog-XL. The PRC combines the functionality of the previously-designed Routing Controller [20] and Packet Controller <ref> [15] </ref> as well as exploiting the availability of the AM79168 TAXIchip Transmitters and the AM79169 TAXIchip Receivers [2, 1]. This design greatly improves performance over its predecessors while preserving the experimental flexibility and support for distributed real-time communication required for HARTS.
Reference: [16] <author> B. Davie, </author> <title> "The architecture and implementation of a high-speed host interface," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 228-239, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: PRC RX 3 instructs the RXBUS interface to store the receive time-stamp in the RXFIFO, thereby reading out the CRC error flag and signaling an RX EOP event. 41 3.3 PRC Implementation There are many specific issues that need to be addressed when approaching the implementation of any communication subsystem <ref> [4, 52, 51, 16, 34, 24, 29, 7] </ref> These include accessing shared resources, flow-control, error detection, and routing.
Reference: [17] <author> A. Davis, R. Hodgson, B. Schediwy, and K. Stevens, </author> <title> "Mayfly system hardware," </title> <type> Technical Report HPL-SAL-89-23, </type> <institution> Hewlett-Packard Company, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: Post Office (Mayfly/FAIM-1) The Post Office (PO) of the Mayfly system <ref> [5, 17, 18] </ref> (developed by Hewlett-Packard Laboratories) is a system whose architecture and functionality most closely matches that of the PRC.
Reference: [18] <author> A. L. Davis, "Mayfly: </author> <title> A general-purpose, scalable, </title> <booktitle> parallel processing architecture," Lisp and Symbolic Computation, </booktitle> <volume> vol. 5, no. 1/2, </volume> <pages> pp. 7-47, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Post Office (Mayfly/FAIM-1) The Post Office (PO) of the Mayfly system <ref> [5, 17, 18] </ref> (developed by Hewlett-Packard Laboratories) is a system whose architecture and functionality most closely matches that of the PRC. <p> x x x TAXI RX 5 Table A.4: Cut-Through Addressed Slave Device Encoding 152 Cut-Through Bus Signal Summary Pins Type Signal Names Description 6 Input CTREQ [5:0] TAXI Receiver Bus Request Lines 0 CTREQ [11:0] PRC Receiver Bus Request Lines 0 CTREQ [17:12] PRC TFU Request Lines 1 Input CTREQ <ref> [18] </ref> IMU Request Line 3 Output CTMST [2:0] Bus Master Grant Status 1 Input CTEXT Bus Cycle Extend 1 BiDir CTDSTRB Data Strobe 1 BiDir CTACK Command Ack 4 BiDir CTCTL [3:0] Command Sub-Bus 8 BiDir CTADDR [7:0] Slave Address Sub-Bus 8 BiDir CTDATA [7:0] Data Sub-Bus Table A.5: Cut-Through Bus
Reference: [19] <author> A. Davis and S. Rovison, </author> <title> "The architecture of the faim-1: Symbolic multiprocessing system," </title> <booktitle> in Proc. Int'l Joint Conf. on Artificial Intelligence, </booktitle> <pages> pp. 32-38, </pages> <address> Los Angeles, </address> <month> August </month> <year> 1985. </year>
Reference-contexts: The PO is a independent packet delivery subsystem that has evolved from its original inception as the communication support for Schlumberger's FAIM-1 symbolic multiprocessor <ref> [19, 49, 50] </ref> to its current role. The end target environment for the system has changed from supporting an ultra-concurrent symbolic multiprocessor for AI systems to the existing design goals of being a scalable general-purpose parallel processing environment for modern programming languages.
Reference: [20] <author> J. W. Dolter, P. Ramanathan, and K. G. Shin, </author> <title> "A microprogrammable VLSI routing controller for HARTS," </title> <type> Technical Report CSE-TR-12-89, </type> <institution> Dept. of Electrical Engineering and Computer Science, University of Michigan, </institution> <address> Ann Arbor, </address> <year> 1989. </year>
Reference-contexts: This device is a custom-designed application-specific IC (ASIC) that has been developed using Cascade Design Automation's EPOCH system and Cadence Design Systems' Verilog-XL. The PRC combines the functionality of the previously-designed Routing Controller <ref> [20] </ref> and Packet Controller [15] as well as exploiting the availability of the AM79168 TAXIchip Transmitters and the AM79169 TAXIchip Receivers [2, 1]. This design greatly improves performance over its predecessors while preserving the experimental flexibility and support for distributed real-time communication required for HARTS.
Reference: [21] <author> J. W. Dolter, P. Ramanathan, and K. G. Shin, </author> <title> "A microprogrammable VLSI routing controller for HARTS," </title> <booktitle> in International Conference on Computer Design: VLSI in Computers, </booktitle> <pages> pp. 160-163, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: In contrast to the analytical model, the simulator makes very few simplifying assump tions in modeling the behavior of virtual cut-through in HARTS. The simulator accurately models the delivery of each message by emulating the timing of the routing hardware <ref> [21] </ref> along the route of a packet at the microcode level. Also captured are the internal bus ac cess overheads that the packets experience if they are unable to cut-through an intermediate node.
Reference: [22] <author> J. Goldberg, M. W. Green, W. H. Kautz, K. N. Levitt, P. M. Melliar-Smith, R. L. Schwartz, and C. B. Weinstock, </author> <title> "Development and analysis of the software implemented fault-tolerance (sift) computer," </title> <type> Contractor Report 172146, </type> <institution> NASA Langley Research Center, </institution> <month> February </month> <year> 1984. </year>
Reference-contexts: The need for ultra-dependable computers that can function in real-time has been recog 1 2 nized for some time and resulted in the initial developments of the Software Implemented Fault-Tolerance (SIFT) computer <ref> [22] </ref> and the Fault-Tolerant Multiprocessor (FTMP)[44]. This commitment continued with the research and implementation of both the Fault-Tolerant Processor (FTP)[43] and its incorporation into the Advanced Information Processing Systems (AIPS).
Reference: [23] <author> J. M. Gordon, </author> <title> Efficient Schemes for Massively Fault-Tolerant Parallel Communication, </title> <type> PhD thesis, </type> <institution> The Unversity of Michigan, </institution> <year> 1990. </year>
Reference-contexts: However, it is important to ensure that the architecture can support a reasonable number of the possible alternatives. There are many properties that can be used to classify routing algorithms <ref> [39, 47, 46, 23, 41] </ref>. Figure 2.4 shows a simplified collection of properties that will be considered for the discussion of routing algorithms. Routing decisions can be either distributed, centralized, or directed (source-list) by the source node.
Reference: [24] <author> Z. Haas, </author> <title> "A communication architecture for high-speed networking," </title> <booktitle> in IEEE INFO-COM, </booktitle> <pages> pp. 433-441, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: PRC RX 3 instructs the RXBUS interface to store the receive time-stamp in the RXFIFO, thereby reading out the CRC error flag and signaling an RX EOP event. 41 3.3 PRC Implementation There are many specific issues that need to be addressed when approaching the implementation of any communication subsystem <ref> [4, 52, 51, 16, 34, 24, 29, 7] </ref> These include accessing shared resources, flow-control, error detection, and routing.
Reference: [25] <author> D. D. Kandlur and K. G. Shin, </author> <title> "Reliable broadcast algorithms for HARTS," </title> <journal> ACM Trans. Computer Systems, </journal> <volume> vol. 9, no. 4, </volume> <pages> pp. 374-398, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: The CTBUS is necessary for the implementation of multiple switching techniques such as virtual cut-through and wormhole routing. It also provides the primitives necessary for low-level flow control and broadcast algorithms <ref> [25] </ref>. Finally, since the CTBUS is a critical resource, the arbitration method emphasizes fairness and scales well under varying loads. The CTBUS interconnects 25 separate components: 6 CTBUS transmitters, 6 CTBUS receivers, 6 PRC receivers, 6 PRC transmitter fetch units, and the Network Processor's IMU. <p> Second, making the multiple IRDATA lines available to the appropriate bus masters and using an address mapping that allows the selection of multiple slave devices provides efficient hardware support for broadcast algorithms <ref> [25] </ref>. This feature is significant in that it allows a one-to-many transaction in a single bus cycle.
Reference: [26] <author> P. Kermani and L. Kleinrock, </author> <title> "Virtual cut-through: A new computer communication switching technique," </title> <journal> Computer Networks, </journal> <volume> vol. 3, no. 4, </volume> <pages> pp. 267-286, </pages> <month> September </month> <year> 1979. </year> <month> 185 </month>
Reference-contexts: Two more recently proposed variants on the above switching methods are virtual cut-through switching and wormhole routing. Virtual cut-through switching, a variant on packet-switching, was first presented and its average performance analyzed in <ref> [26] </ref>. Virtual cut-through switching reduces packet latency by immediately forwarding the packet out of an intermediate node the moment the node's outgoing link on the packet's path is available, even if the packet has not been received in its entirety. <p> However, the actual improvement it offers over a packet-switching scheme for packet deliveries has not yet been accurately evaluated. Kermani and Kleinrock carried out a mean value analysis of the performance of virtual cut-through for a general interconnection network <ref> [26] </ref>. However, a mean value analysis is 76 77 not adequate for real-time applications because worst-case communication delays often play an important role in the design of real-time systems. <p> For example, the mean value analysis cannot answer questions like what is the probability of a successful delivery given a delay or what is the delay bound such that the probability of a successful delivery is greater than a specified threshold. The authors of <ref> [26] </ref> wanted to avoid any dependence on the interconnection topology in their analysis. As a result, they assumed that the probability of a packet getting buffered at an intermediate node is a given parameter. <p> As a result, they assumed that the probability of a packet getting buffered at an intermediate node is a given parameter. Since one cannot get a reasonable estimate of the performance of virtual cut-through without an accurate estimate of the probability of buffering, the approach in <ref> [26] </ref> becomes useful only if we can accurately determine the probability of buffering for a given interconnection topology. However, determining the probability of buffering at an intermediate node for a given topology is not simple. <p> Consequently, to evaluate the probability of buffering, we have to account for the fraction of packets generated at other nodes that pass through each given node. In contrast to <ref> [26] </ref>, we first derive the probability that a packet is destined for a particular node by characterizing the hexagonal mesh topology. This probability of branching is then used as a parameter in a queueing network to determine the throughput rates at each node in the mesh. <p> A2: Exponentially-distributed packet lengths with mean `. A3: The length of a packet is regenerated at each intermediate node of its route indepen dently of its length at other intermediate nodes. A4: Nodes have no preferential direction for communication. Assumptions A1-A3 are consistent with Kermani and Kleinrock's assumptions in <ref> [26] </ref>. Although not completely accurate, it has been shown through empirical studies that these assumptions lead to a fairly accurate characterization of message arrivals. <p> The maximum packet 129 generation rate for the smaller H-meshes is higher due to the fact that the average packet distance is larger. The packet latencies for virtual cut-through switching have been analyzed in <ref> [26] </ref>, and given below: T c = 1 It is instructive to compare the packet latencies for the two forms of switching as a function of channel load. The graphs of the latencies for different dimensions of H-mesh are shown in Fig. 6.2.
Reference: [27] <author> L. Kleinrock, </author> <title> Queueing systems, volume I: Theory, </title> <publisher> John Wiley & Sons, </publisher> <year> 1975. </year>
Reference-contexts: Lemma 5 The throughput at each service center is 6 G P e1 Proof : Jackson's theorem <ref> [27] </ref> states that the total throughput T i at service center i is given by the solution to the set of traffic flow equations T i = G + k=0 By assumption A4 and the homogeneous nature of the C-wrapped H-mesh all T i are equal.
Reference: [28] <author> A. Kovaleski, S. Ratheal, and F. Lombardi, </author> <title> "An architecture and interconnection scheme for time-sliced buses in real-time processing," </title> <booktitle> Proc. Real-Time Systems Symposium, </booktitle> <pages> pp. 20-27, </pages> <year> 1986. </year>
Reference-contexts: Bus Arbitration Scheme Arbitration for the CTBUS is based on a binary priority tree in which the different levels of the tree are permuted according to the state of a counter. This scheme is derived from the work of Kovaleski et al. <ref> [28] </ref> and provides the basis for a demand slot access method. This scheme was chosen for its "fairness" and scalable performance over varying loads. this example a 3-bit counter is used to control the permutation of the tree.
Reference: [29] <author> A. Krishnakumar and K. Sabnani, </author> <title> "VLSI implementations of communication protocols a survey," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 7, no. 7, </volume> <pages> pp. 1082-1090, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: PRC RX 3 instructs the RXBUS interface to store the receive time-stamp in the RXFIFO, thereby reading out the CRC error flag and signaling an RX EOP event. 41 3.3 PRC Implementation There are many specific issues that need to be addressed when approaching the implementation of any communication subsystem <ref> [4, 52, 51, 16, 34, 24, 29, 7] </ref> These include accessing shared resources, flow-control, error detection, and routing.
Reference: [30] <author> H. Kung, R. Sansom, S. Schlick, P. Steenkiste, M. Arnould, F. J. Bitz, F. Christianson, E. C. Cooper, O. Menzilcioglu, D. Ombres, and B. Zill, </author> <title> "Network-based multicomput-ers: An emerging parallel architecture," </title> <booktitle> in Supercomputing 91, </booktitle> <pages> pp. 664-673. </pages> <publisher> IEEE, ACM, </publisher> <address> New York, NY, USA, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: CMU's Nectar project's design and implementation of the Communication Accelerator Board (CAB) clearly identifies some of the important features that a general purpose communication system should efficiently support <ref> [8, 30, 33, 48] </ref>. Heterogeneity, scalability, low-latency, and high-bandwidth communication were the stated goals of the Nectar 72 project. To achieve these goals they designed an efficient parallel crossbar interconnection network that can be cascaded in multiple levels to support more nodes; hence satisfying the scalability, low-latency, and high-bandwidth requirements.
Reference: [31] <author> R. Lee, </author> <title> "Cyclic code redundancy," </title> <booktitle> Digital Design, </booktitle> <volume> vol. 11, no. 7, </volume> <pages> pp. 77-85, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: For this reason, and since data is not always available in serial form, parallel CRC generators have been studied by a number of researchers. Implementations have ranged from variations on the software approach using a lookup table stored in ROM <ref> [31] </ref> to fully parallel encoders [3, 37]. For the PRC, packet data is only available in parallel form and at data rates such 58 that serial computation is not feasible.
Reference: [32] <author> S. J. Le*er, M. K. McKusick, M. J. Karels, and J. S. Quarterman, </author> <title> The Design and Implementation of the 4.3BSD Unix Operating System, </title> <publisher> Computer Science, Addison Wesley, </publisher> <month> May </month> <year> 1989. </year> <note> ISBN 0-201-06196-1. </note>
Reference-contexts: Packets that are transmitted and received by the PRC are stored in NP buffer memory in a linked page structure similar to the indirect MBUF cluster structures used by many UNIX 31 (a) Prior to Transmission (b) Transmitted on Physical Links (c) After Reception H &lt; 64 32 communication subsystems <ref> [32] </ref>. Furthermore, the pages referenced by these structures have the following characteristics: the page size is 256 bytes, pages are longword aligned, and pages must contain an integral number of longwords starting from an offset of 0 from the base of the page.
Reference: [33] <author> O. Menzilcioglu and S. Schlick, "Nectar CAB: </author> <title> A high-speed network processor," </title> <booktitle> in Proc. Int'l Conf. on Distributed Computing Systems, </booktitle> <pages> pp. 508-515, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: CMU's Nectar project's design and implementation of the Communication Accelerator Board (CAB) clearly identifies some of the important features that a general purpose communication system should efficiently support <ref> [8, 30, 33, 48] </ref>. Heterogeneity, scalability, low-latency, and high-bandwidth communication were the stated goals of the Nectar 72 project. To achieve these goals they designed an efficient parallel crossbar interconnection network that can be cascaded in multiple levels to support more nodes; hence satisfying the scalability, low-latency, and high-bandwidth requirements.
Reference: [34] <author> R. Metcalfe, </author> <title> "Computer/network interface design: Lessons from Arpanet and Ethernet," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 173-180, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: PRC RX 3 instructs the RXBUS interface to store the receive time-stamp in the RXFIFO, thereby reading out the CRC error flag and signaling an RX EOP event. 41 3.3 PRC Implementation There are many specific issues that need to be addressed when approaching the implementation of any communication subsystem <ref> [4, 52, 51, 16, 34, 24, 29, 7] </ref> These include accessing shared resources, flow-control, error detection, and routing.
Reference: [35] <author> E. Miller, </author> <title> "High-speed cyclic redundancy check generation and verification," </title> <journal> IBM Technical Disclosure Bulletin, </journal> <volume> vol. 21, no. 8, </volume> <pages> pp. 3065-3066, </pages> <month> January </month> <year> 1979. </year>
Reference-contexts: ctaddr1,go ctbus; 167 - # The clear all at the beginning will reset our IR to high 168 jump true,init; 169 - although designers have been known to convert parallel data to serial form to run it through a serial CRC register, and then to reconvert the data to parallel <ref> [35] </ref>. Another drawback to serial CRC generators and checkers is the time required to process a single n-bit word of data. As noted in [3], it takes n clock pulses for a serial CRC implementation to process a single word of data.
Reference: [36] <author> M. Noakes, D. Wallach, and W. Dally, </author> <title> "The J-machine multicomputer: An architectural evaluation," </title> <booktitle> in Proc. Int'l Symposium on Computer Architecture, </booktitle> <pages> pp. 224-235, </pages> <year> 1993. </year>
Reference-contexts: The beauty of the TRC is that it is small and simple. Message Driven Processor The Message-Driven Processor (MDP), like its predecessors (TRC/NDF/ADC), is designed to be part of a multicomputer capable of supporting fine grain, message passing, parallel computation <ref> [12, 11, 36, 45] </ref>. What is novel about their approach is that the processor that comprises the core of each node has been made network aware.
Reference: [37] <author> A. Pandeya and T. Cass, </author> <title> "Parallel crc lets many lines use one circuit," </title> <booktitle> Computer Design, </booktitle> <volume> vol. 14, no. 9, </volume> <pages> pp. 87-91, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: For this reason, and since data is not always available in serial form, parallel CRC generators have been studied by a number of researchers. Implementations have ranged from variations on the software approach using a lookup table stored in ROM [31] to fully parallel encoders <ref> [3, 37] </ref>. For the PRC, packet data is only available in parallel form and at data rates such 58 that serial computation is not feasible. This leads to the selection of a parallel CRC with the obvious drawback of the number of gates required for an implementation.
Reference: [38] <author> P. Ramanathan, D. D. Kandlur, and K. G. Shin, </author> <title> "Hardware assisted software clock synchronization for homogeneous distributed systems," </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. C-39, no. 4, </volume> <pages> pp. 514-524, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: First, time-stamps are added by the transmitting and receiving PRCs. These time-stamps can be accumulated to obtain the total transmission time of a message and are useful for distributed information services such as clock synchronization and reliable broadcast as described in <ref> [38] </ref>. The accumulation of time-stamps and modification of the packet size in order to accommodate the time-stamps is the responsibility of the IMU. Second, a portion of the routing header is not covered by the CRC. <p> Another feature specifically incorporated into the implementation of the PRC to support OS functions is send time-stamps and receive time-stamps. The ability to accurately time-stamp messages close to the network hardware and how this aids in synchronization of the clocks in a distributed system is discussed in <ref> [38] </ref> and motivated its inclusion in this work. 3.3.5 Design Decisions Revisited At many points during the implementation of the PRC we were faced with design decisions that shaped the final capabilities of the PRC.
Reference: [39] <author> D. A. Reed and R. M. Fujimoto, </author> <title> Multicomputer Networks: Message-Based Parallel Processing, </title> <editor> M. I. T. </editor> <publisher> Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: However, it is important to ensure that the architecture can support a reasonable number of the possible alternatives. There are many properties that can be used to classify routing algorithms <ref> [39, 47, 46, 23, 41] </ref>. Figure 2.4 shows a simplified collection of properties that will be considered for the discussion of routing algorithms. Routing decisions can be either distributed, centralized, or directed (source-list) by the source node.
Reference: [40] <author> D. A. Reed and D. C. Grunwald, </author> <title> "The performance of multicomputer interconnection networks," </title> <journal> IEEE Computer, </journal> <volume> vol. 20, no. 6, </volume> <pages> pp. 63-73, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: We will now present a set of definitions that closely follows the terminology and framework outlined by Reed and Fujimoto <ref> [40] </ref>. It is also important to recognize the location of HARTS, including the PRC, within the entire spectrum of communication architectures. This work investigates systems with a homogeneous point-to-point interconnection supporting coarse-grain parallel computations, rather than trying to operate in the domain of tightly-coupled multiprocessor networks.
Reference: [41] <author> J. Rexford and K. G. Shin, </author> <title> "Shortest-path routing in homogeneous point-to-point ne-towrks with virtual cut-through switching," </title> <institution> Computer Science and Engineering Techi-nal Report CSE-TR-146-92, University of Michigan, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: However, it is important to ensure that the architecture can support a reasonable number of the possible alternatives. There are many properties that can be used to classify routing algorithms <ref> [39, 47, 46, 23, 41] </ref>. Figure 2.4 shows a simplified collection of properties that will be considered for the discussion of routing algorithms. Routing decisions can be either distributed, centralized, or directed (source-list) by the source node.
Reference: [42] <author> C. L. Seitz, </author> <title> VLSI and PARALLEL COMPUTATION, </title> <booktitle> chapter Concurrent Architectures, </booktitle> <pages> pp. 1-84, </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year> <month> 186 </month>
Reference-contexts: Although the TRC is now quite dated, some of its contributing features can be found 70 in many of the second generation parallel machines. Most notable is the efficient use of wormhole routing that appeared in the Mesh Routing Chip (MRC) used in the Amtex 2010 <ref> [42] </ref> and the theory behind constructing deadlock-free routing by defining an order in which the packets use the available virtual channels [10].
Reference: [43] <author> T. B. Smith, </author> <title> "Fault tolerant processor concepts and operation," </title> <type> Contractor Report CSPL-P-1727, </type> <institution> Charles Stark Draper Laboratory, </institution> <month> May </month> <year> 1983. </year>
Reference: [44] <author> T. B. Smith and J. H. Lala, </author> <title> "Development and evaluation of a fault-tolerant multiprocessor (FTMP) computer volume I FTMP principles of operation," </title> <type> Contractor Report 166071, </type> <institution> NASA Langley Research Center, </institution> <month> May </month> <year> 1985. </year>
Reference: [45] <author> E. Spertus, S. Goldstein, K. Schauser, T. von Eicken, D. Culler, and W. Dally, </author> <title> "Evaluation of mechanisms for fine-grained parallel programs in the J-machine and the CM-5," </title> <booktitle> in Proc. Int'l Symposium on Computer Architecture, </booktitle> <pages> pp. 302-313, </pages> <year> 1993. </year>
Reference-contexts: The beauty of the TRC is that it is small and simple. Message Driven Processor The Message-Driven Processor (MDP), like its predecessors (TRC/NDF/ADC), is designed to be part of a multicomputer capable of supporting fine grain, message passing, parallel computation <ref> [12, 11, 36, 45] </ref>. What is novel about their approach is that the processor that comprises the core of each node has been made network aware.
Reference: [46] <author> J. D. Spragins, J. H. Hammond, and K. Pawlikowski, </author> <title> Telecommunications Protocols and Design, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1991. </year>
Reference-contexts: However, it is important to ensure that the architecture can support a reasonable number of the possible alternatives. There are many properties that can be used to classify routing algorithms <ref> [39, 47, 46, 23, 41] </ref>. Figure 2.4 shows a simplified collection of properties that will be considered for the discussion of routing algorithms. Routing decisions can be either distributed, centralized, or directed (source-list) by the source node.
Reference: [47] <author> W. Stallings, </author> <title> Data and Computer Communications, </title> <publisher> Macmillan Publishing Company, </publisher> <address> New York, </address> <note> second edition, </note> <year> 1988. </year>
Reference-contexts: However, it is important to ensure that the architecture can support a reasonable number of the possible alternatives. There are many properties that can be used to classify routing algorithms <ref> [39, 47, 46, 23, 41] </ref>. Figure 2.4 shows a simplified collection of properties that will be considered for the discussion of routing algorithms. Routing decisions can be either distributed, centralized, or directed (source-list) by the source node.
Reference: [48] <author> P. Steenkiste, </author> <title> "Analyzing communication latency using the nectar communication processor," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 199-209. </pages> <publisher> ACM, ACM, </publisher> <address> New York, NY, USA, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: CMU's Nectar project's design and implementation of the Communication Accelerator Board (CAB) clearly identifies some of the important features that a general purpose communication system should efficiently support <ref> [8, 30, 33, 48] </ref>. Heterogeneity, scalability, low-latency, and high-bandwidth communication were the stated goals of the Nectar 72 project. To achieve these goals they designed an efficient parallel crossbar interconnection network that can be cascaded in multiple levels to support more nodes; hence satisfying the scalability, low-latency, and high-bandwidth requirements.
Reference: [49] <author> K. S. Stevens, </author> <title> "The communication framework for a distributed ensemble architecture," </title> <type> AI Technical Report 47, </type> <institution> Schlumberger Research Laboratory, </institution> <month> February </month> <year> 1986. </year>
Reference-contexts: This chapter presents a brief background on the target system (i.e., HARTS), as well as examining and classifying a number of routing and switching schemes. 2.1 C-wrapped Hexagonal Mesh One of the interconnection topologies that has received considerable attention throughout this dissertation is the C-wrapped hexagonal mesh <ref> [6, 49] </ref>. <p> The PO is a independent packet delivery subsystem that has evolved from its original inception as the communication support for Schlumberger's FAIM-1 symbolic multiprocessor <ref> [19, 49, 50] </ref> to its current role. The end target environment for the system has changed from supporting an ultra-concurrent symbolic multiprocessor for AI systems to the existing design goals of being a scalable general-purpose parallel processing environment for modern programming languages.
Reference: [50] <author> K. S. Stevens, S. V. Robison, and A. L. Davis, </author> <title> "The Post Office | Communication support for distributed ensemble architectures," </title> <booktitle> in Proc. Int'l Conf. on Distributed Computing Systems, </booktitle> <pages> pp. 160-166, </pages> <year> 1986. </year>
Reference-contexts: The PO is a independent packet delivery subsystem that has evolved from its original inception as the communication support for Schlumberger's FAIM-1 symbolic multiprocessor <ref> [19, 49, 50] </ref> to its current role. The end target environment for the system has changed from supporting an ultra-concurrent symbolic multiprocessor for AI systems to the existing design goals of being a scalable general-purpose parallel processing environment for modern programming languages.
Reference: [51] <author> L. Svobodova, </author> <title> "Implementing osi systems," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 7, no. 7, </volume> <pages> pp. 1115-1129, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: PRC RX 3 instructs the RXBUS interface to store the receive time-stamp in the RXFIFO, thereby reading out the CRC error flag and signaling an RX EOP event. 41 3.3 PRC Implementation There are many specific issues that need to be addressed when approaching the implementation of any communication subsystem <ref> [4, 52, 51, 16, 34, 24, 29, 7] </ref> These include accessing shared resources, flow-control, error detection, and routing.
Reference: [52] <author> C. A. Thekkath and H. M. Levy, </author> <title> "Limits to low-latency communication on high-speed networks," </title> <journal> ACM Trans. Computer Systems, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: PRC RX 3 instructs the RXBUS interface to store the receive time-stamp in the RXFIFO, thereby reading out the CRC error flag and signaling an RX EOP event. 41 3.3 PRC Implementation There are many specific issues that need to be addressed when approaching the implementation of any communication subsystem <ref> [4, 52, 51, 16, 34, 24, 29, 7] </ref> These include accessing shared resources, flow-control, error detection, and routing.
References-found: 52

