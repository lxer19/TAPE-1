URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-282.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> C. Tappert, C. Suen, and T. Wakahara. </author> <title> The State of the Art in On-Line Handwriting Recognition IEEE T. Pat. </title> <journal> Anal. & Mach. Int., </journal> <pages> pp. 787-808, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: 1. INTRODUCTION Traditionally, the first step in handwriting recognition is the segmentation of words into component characters <ref> [1] </ref>. However, in modern continuous speech recognition efforts, phonemes are not segmented before training or recognition. Instead, segmentation occurs simultaneously with recognition. If such a system could be adapted for handwriting, the very difficult and time consuming issue of segmentation could be avoided.
Reference: [2] <author> R. Nag, K. H. Wong, F. Fallside, </author> <title> Script Recognition using Hidden Markov Models. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pp. 2071-2074, </pages> <address> Tokyo, Japan, </address> <year> 1986. </year>
Reference-contexts: For each sample point, an analysis program computed a two-element feature vector: the writing angle at that sample and the change in the writing angle <ref> [2] </ref>. These time series of feature vectors were then fed into the BYBLOS system. For this task, BYBLOS quantizes the feature vectors for a sentence into 64 different clusters.
Reference: [3] <author> K. Nathan, J. Bellegarda, D. Nahamoo, E. Bellegarda. </author> <title> On-Line Handwriting Recognition Using Continuous Parameter Hidden Markov Models. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pp. </pages> <address> V-121-124, Minneapolis, MN, </address> <year> 1993. </year>
Reference: [4] <author> F. Kubala, A. Anastasakos, J. Makhoul, L. Nguyen, R. Schwartz, G. Zavaliagkos. </author> <title> Comparative Experiments on Large Vocabulary Speech Recognition To be presented at ICASSP, </title> <address> Adelaide, Australia, </address> <year> 1994. </year>
Reference-contexts: In this study, our objective is to deal with continuous cursive handwriting and large vocabularies (thousands of words), using a speech recognition system and language models. 3. INITIAL SYSTEM In the initial system, the BBN BYBLOS Continuous Speech Recognition system <ref> [4] </ref> (see Figure 1) was used without modification on an on-line cursive handwriting corpus created from prompts from the ARPA Airline Travel Information Service (ATIS) corpus [5]. These full sentence prompts (approximately 10 words per sentence) were written by a single subject.
Reference: [5] <editor> MADCOW Multi-Site Data Collection for a Spoken Language Corpus. </editor> <booktitle> Proc. DARPA Speech and Natural Language Workshop, </booktitle> <pages> pp. 7-14, </pages> <address> Harriman, NY, </address> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: INITIAL SYSTEM In the initial system, the BBN BYBLOS Continuous Speech Recognition system [4] (see Figure 1) was used without modification on an on-line cursive handwriting corpus created from prompts from the ARPA Airline Travel Information Service (ATIS) corpus <ref> [5] </ref>. These full sentence prompts (approximately 10 words per sentence) were written by a single subject. These sentences were then reviewed (verified) to make sure that the prompts were transcribed correctly. <p> A 7-state HMM model was chosen to represent each symbol (see Figure 3). Since the penning of a script letter often differs depending on the letters written before and after it, additional HMMs are used to model these contextual effects <ref> [5] </ref>. Adjacent effects between two letters (bilets) are modeled as well as three letter (trilet) contexts. In a given set of sentences there may be many trilets, up to the number of symbols cubed. However, in English only a subset of these are allowed.
Reference: [6] <author> R. M. Schwartz, Y. L. Chow, O. A. Kimball, S. Roucos, M. Krasner, and J. </author> <title> Makhoul Context-Dependent Modeling for Acoustic-Phonetic Recognition of Continuous Speech. </title> <booktitle> Proc. ICASSP, </booktitle> <address> pp.1205-1208, Tampa, FL, </address> <month> March </month> <year> 1985. </year>
Reference-contexts: Based on these good preliminary results, we embarked on a more ambitious task with a larger vocabulary and more writers. 4. WALL STREET JOURNAL: A 25,000 WORD TASK Recently, we have collected cursive written data using text from the ARPA Wall Street Journal task (WSJ) <ref> [6] </ref>, including numerals, punctuation, and other symbols, for a total of 88 symbols (62 alphanumeric, 24 punctuation and special symbols, space, and backspace). The prompts from the Wall Street Journal consist mainly of full sentences with scattered article headings and stock listings (all are referred to as sentences for convenience).
Reference: [7] <author> D. </author> <title> Paul The Design for the Wall Street Journal-based CSR Corpus. </title> <booktitle> Proc. DARPA Speech and Natural Language Workshop, </booktitle> <pages> pp. 357-360, </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
References-found: 7

