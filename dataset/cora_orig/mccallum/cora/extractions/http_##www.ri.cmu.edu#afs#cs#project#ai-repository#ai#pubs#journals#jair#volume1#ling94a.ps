URL: http://www.ri.cmu.edu/afs/cs/project/ai-repository/ai/pubs/journals/jair/volume1/ling94a.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs/project/ai-repository/ai/pubs/journals/jair/volume1/
Root-URL: 
Email: ling@csd.uwo.ca  
Title: Learning the Past Tense of English Verbs: The Symbolic Pattern Associator vs. Connectionist Models  
Author: Charles X. Ling 
Address: Ontario London, Ontario, Canada N6A 5B7  
Affiliation: Department of Computer Science The University of Western  
Note: Journal of Artificial Intelligence Research 1 (1994) 209-229 Submitted 11/93; published 2/94  
Abstract: Learning the past tense of English verbs | a seemingly minor aspect of language acquisition | has generated heated debates since 1986, and has become a landmark task for testing the adequacy of cognitive modeling. Several artificial neural networks (ANNs) have been implemented, and a challenge for better symbolic models has been posed. In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons on the generalization ability between ANN models and the SPA under different representations. We conclude that the SPA generalizes the past tense of unseen verbs better than ANN models by a wide margin, and we offer insights as to why this should be the case. We also discuss a new default strategy for decision-tree learning algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cottrell, G., & Plunkett, K. </author> <year> (1991). </year> <title> Using a recurrent net to learn the past tense. </title> <booktitle> In Proceedings of the Cognitive Science Society Conference. </booktitle>
Reference: <author> Daugherty, K., & Seidenberg, M. </author> <year> (1993). </year> <title> Beyond rules and exceptions: A connectionist modeling approach to inflectional morphology. </title> <editor> In Lima, S. (Ed.), </editor> <title> The Reality of Linguistic Rules. </title> <publisher> John Benjamins. </publisher>
Reference: <author> Dietterich, T., & Bakiri, G. </author> <year> (1991). </year> <title> Error-correcting output codes: A general method for improving multiclass inductive learning programs. </title> <booktitle> In AAAI-91 (Proceedings of Ninth National Conference on Artificial Intelligence). </booktitle>
Reference: <author> Dietterich, T., Hild, H., & Bakiri, G. </author> <year> (1990). </year> <title> A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <booktitle> In Proceedings of the 7th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A similar approach was proposed for dealing with the distributed (binary) encoding in multiclass learning tasks such as NETtalk (English text-to-speech mapping) <ref> (Dietterich, Hild, & Bakiri, 1990) </ref>. Each tree takes as input the set of all attributes in the input patterns, and is used to determine the value of 1. The SPA programs and relevant datasets can be obtained anonymously from ftp.csd.uwo.ca under pub/SPA/ .
Reference: <author> Feng, C., King, R., Sutherland, A., & Henery, R. </author> <year> (1992). </year> <title> Comparison of symbolic, statistical and neural network classifiers. </title> <type> Manuscript. </type> <institution> Department of Computer Science, University of Ottawa. </institution>
Reference: <author> Fodor, J., & Pylyshyn, Z. </author> <year> (1988). </year> <title> Connectionism and cognitive architecture: A critical analysis. </title> <editor> In Pinker, S., & Mehler, J. (Eds.), </editor> <booktitle> Connections and Symbols, </booktitle> <pages> pp. 3 - 71. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: hand, supporters of the symbolic approach believe that symbol structures such as parse trees, propositions, etc., and the rules for their manipulations, are critical at the cognitive level, while the connectionist approach may only provide an account of the neural structures in which the traditional symbol-processing cognitive architecture is implemented <ref> (Fodor & Pylyshyn, 1988) </ref>. Pinker (1991) and Prasada and Pinker (1993) argue that a proper c fl1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. Ling accounting for regular verbs should be dependent upon production rules, while irregular past-tense inflections may be generalized by ANN-like associative memory.
Reference: <author> Geman, S., Bienenstock, E., & Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4, 1 - 58. </volume>
Reference-contexts: Another major difference between ANNs and ID3 is that ANNs have a larger variation and a weaker bias (cf. <ref> (Geman, Bienenstock, & Doursat, 1992) </ref>) than ID3. Many more Boolean functions (e.g., linearly separable functions) can fit a small network (e.g., one with no hidden units) than they can a small decision tree.
Reference: <author> Lachter, J., & Bever, T. </author> <year> (1988). </year> <title> The relation between linguistic structure and associative theories of language learning a constructive critique of some connectionist learning models. </title> <editor> In Pinker, S., & Mehler, J. (Eds.), </editor> <title> Connections and Symbols, pp. 195 - 247. Cambridge, MA: MIT Press. 228 Learning the Past Tense: Symbolic vs Connectionist Models Ling, </title> <editor> X., Cherwenka, S., & Marinov, M. </editor> <year> (1993). </year> <title> A symbolic model for learning the past tenses of English verbs. </title> <booktitle> In Proceedings of IJCAI-93 (Thirteenth International Conference on Artificial Intelligence), </booktitle> <pages> pp. 1143-1149. </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Ling, X., & Marinov, M. </author> <year> (1993). </year> <title> Answering the connectionist challenge: a symbolic model of learning the past tense of English verbs. </title> <journal> Cognition, </journal> <volume> 49 (3), </volume> <pages> 235-290. </pages>
Reference-contexts: We have shown <ref> (Ling & Marinov, 1993) </ref> that the SPA's results are much more psychologically realistic than ANN models when compared with human subjects. On the issue of the predictive accuracy, MacWhinney and Leinbach (1991) did not report important results of their model on unseen regular verbs. <p> entirely opaque, the SPA can represent the acquired knowledge in the form of production rules, and allow for further processing, resulting in higher-level categories such as the verb stem and the voiced consonants, linguistically realistic production rules using these new categories for regular verbs, and associative templates for irregular verbs <ref> (Ling & Marinov, 1993) </ref>. 213 Ling 3. <p> Third, those production rules at the phonological level can easily be further generalized into first-order rules that use more abstract, high-level symbolic categories such as morphemes and the verb stem <ref> (Ling & Marinov, 1993) </ref>. <p> Since entities of these rules are symbols with semantic meanings, the acquired knowledge often is comprehensible to the human observer. In addition, further processing and integration of these rules can yield high-level knowledge (e.g., rules using verb stems) <ref> (Ling & Marinov, 1993) </ref>. Another feature of the SPA is that the trees for different output attributes contain identical components (branches and subtrees) (Ling & Marinov, 1993). <p> In addition, further processing and integration of these rules can yield high-level knowledge (e.g., rules using verb stems) <ref> (Ling & Marinov, 1993) </ref>. Another feature of the SPA is that the trees for different output attributes contain identical components (branches and subtrees) (Ling & Marinov, 1993). These components have similar roles as hidden units in ANNs since they are shared in the decision trees of more than one output attribute. <p> Nevertheless, straightforward symbolic format requires little representation engineering compared with the distributed representation in ANNs. 4.6 Right-justified, Isolated Suffix Representation MacWhinney and Leinbach (1991) did not report important results of the predictive accuracy of their model on unseen regular verbs. In his reply (MacWhinney, 1993) to our paper <ref> (Ling & Marinov, 1993) </ref>, MacWhinney re-implemented the ANN model. In his new implementation, 1,200 verb stem and past-tense pairs were in the training set, among which 1081 were regular and 119 were irregular. Training took 4,200 epochs, and reached 100% correct on regulars and 80% on irregulars. <p> However, because SPA produces simple production rules that use these phoneme letters directly, those rules can be further generalized to first-order rules with new representations such as stems and the voiced consonants which can be used across the board in other such rule-learning modules <ref> (Ling & Marinov, 1993) </ref>. <p> Some techniques (such as adding copy connections and weight decaying) exist, but their exact effects on biasing towards classes of functions are not clear. From our analyses <ref> (Ling & Marinov, 1993) </ref>, the underlying regularities governing the inflection of the past tense of English verbs do form a small set of production rules with phoneme letters. This is especially so for regular verbs; all the rules are either identity rules or the suffix-adding rules.
Reference: <author> MacWhinney, B. </author> <year> (1990). </year> <title> The CHILDES Project: Tools for Analyzing Talk. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: The major departure from Rumelhart and McClelland's model is that the Wickelphone/Wickelfeature representational format is replaced with the UNIBET <ref> (MacWhinney, 1990) </ref> phoneme representational system which allows the assignment of a single alphabetic/numerical letter to each of the total 36 phonemes. MacWhinney and Leinbach use special templates with which to code each phoneme and its position in a word. <p> The set contains about 1400 stem/past tense pairs. Learning is based upon the phonological UNIBET representation <ref> (MacWhinney, 1990) </ref>, in which different phonemes are represented by different alphabetic/numerical letters. There is a total of 36 phonemes. The source file is transferred into the standard format of pairs of input and output patterns.
Reference: <author> MacWhinney, B. </author> <year> (1993). </year> <title> Connections and symbols: closing the gap. </title> <journal> Cognition, </journal> <volume> 49 (3), </volume> <pages> 291-296. </pages>
Reference-contexts: Without any further processing, there is only so much blood that can be squeezed out of a turnip, and each of our systems [SPA and ANN] extracted what they could. <ref> (MacWhinney, 1993, page 295) </ref> We will show that this is not the case; obviously there are reasons why one learning algorithm outperforms another (otherwise why do we study different learning algorithms?). <p> Nevertheless, straightforward symbolic format requires little representation engineering compared with the distributed representation in ANNs. 4.6 Right-justified, Isolated Suffix Representation MacWhinney and Leinbach (1991) did not report important results of the predictive accuracy of their model on unseen regular verbs. In his reply <ref> (MacWhinney, 1993) </ref> to our paper (Ling & Marinov, 1993), MacWhinney re-implemented the ANN model. In his new implementation, 1,200 verb stem and past-tense pairs were in the training set, among which 1081 were regular and 119 were irregular.
Reference: <author> MacWhinney, B., & Leinbach, J. </author> <year> (1991). </year> <title> Implementations are not conceptualizations: Revising the verb model. </title> <journal> Cognition, </journal> <volume> 40, 121 - 157. </volume>
Reference-contexts: The proper way of debating conceptualizations is by contrasting competitive implementations. To do this in the present case, we would need a symbolic implementation that could be contrasted with the current implementation. <ref> (MacWhinney & Leinbach, 1991, page 153) </ref> In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon the symbolic decision tree learning algorithm ID3 (Quinlan, 1986). <p> Altogether, 2062 regular and irregular English verbs are selected for the experiment | 1650 of them are used for training (1532 regular and 118 irregular), but only 13 low frequency irregular verbs are used for testing <ref> (MacWhinney & Leinbach, 1991, page 144) </ref>. Training the network takes 24,000 epochs. At the end of training there still are 11 errors on the irregular pasts. <p> Training the network takes 24,000 epochs. At the end of training there still are 11 errors on the irregular pasts. MacWhinney and Leinbach believe that if they allow the network to run for several additional days and give it additional hidden unit resources, it probably can reach complete convergence <ref> (MacWhinney & Leinbach, 1991, page 151) </ref>. The only testing error rate reported is based on a very small and biased test sample of 13 unseen irregular verbs; 9 out of 13 are predicted incorrectly. <p> They do not test their model on any of the unseen regular verbs: "Unfortunately, we did not test a similar set of 13 regulars." <ref> (MacWhinney & Leinbach, 1991, page 151) </ref>. 2.3 Criticism of the Connectionist Models Previous and current criticisms of the connectionist models of learning the past tenses of English verbs center mainly on several issues.
Reference: <author> Pinker, S. </author> <year> (1991). </year> <title> Rules of language. </title> <journal> Science, </journal> <volume> 253, 530 - 535. </volume>
Reference: <author> Pinker, S., & Prince, A. </author> <year> (1988). </year> <title> On language and connectionism: Analysis of a parallel distributed processing model of language acquisition. </title> <editor> In Pinker, S., & Mehler, J. (Eds.), </editor> <booktitle> Connections and Symbols, </booktitle> <pages> pp. 73 - 193. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: One of the inherent deficits of the connectionist implementations is that there is no such thing as a variable for verb stem, and hence there is no way for the model to attain the knowledge that one could add suffix to a stem to get its past tense <ref> (Pinker & Prince, 1988, page 124) </ref>.
Reference: <author> Plunkett, K., & Marchman, V. </author> <year> (1991). </year> <title> U-shaped learning and frequency effects in a mul-tilayered perceptron: Implications for child language acquisition. </title> <journal> Cognition, </journal> <volume> 38, 43 - 102. </volume>
Reference: <author> Prasada, S., & Pinker, S. </author> <year> (1993). </year> <title> Generalization of regular and irregular morphological patterns. </title> <booktitle> Language and Cognitive Processes, </booktitle> <volume> 8 (1), 1 - 56. </volume>
Reference-contexts: 59.2 83.0 29.2 77.8 60.0 16.0 55.6 58.0 8.0 53.0 58.7 16.0 54.4 80.9 20.0 74.8 Table 4: Comparisons of testing accuracy of SPA and ANN with distributed and symbolic representations. children and adults occasionally extend irregular inflection to irregular-sounding regular verbs or pseudo verbs (such as cleef | cleft) <ref> (Prasada & Pinker, 1993) </ref>. The more similar the novel verb is to the cluster of irregular verbs with similar phonological patterns, the more likely the prediction of an irregular past-tense form.
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 (1), 81 - 106. </volume>
Reference-contexts: To do this in the present case, we would need a symbolic implementation that could be contrasted with the current implementation. (MacWhinney & Leinbach, 1991, page 153) In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon the symbolic decision tree learning algorithm ID3 <ref> (Quinlan, 1986) </ref>. We have shown (Ling & Marinov, 1993) that the SPA's results are much more psychologically realistic than ANN models when compared with human subjects. On the issue of the predictive accuracy, MacWhinney and Leinbach (1991) did not report important results of their model on unseen regular verbs. <p> a large weight matrix; it is therefore hard to see how this knowledge can be further generalized into more abstract representations and categories. 3.1 The Architecture of the Symbolic Pattern Associator The SPA is based on C4.5 (Quinlan, 1993) which is an improved implementation of the ID3 learning algorithm (cf. <ref> (Quinlan, 1986) </ref>). ID3 is a program for inducing classification rules in the form of decision trees from a set of classified examples. It uses information gain ratio as a criterion for selecting attributes as roots of the subtrees.
Reference: <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5 Programs for Machine Learning. </title> <publisher> Morgan Kaufmann: </publisher> <address> San Mateo, CA. </address>
Reference-contexts: a distributed representation (phonetic feature vectors), and the acquired knowledge is embedded in a large weight matrix; it is therefore hard to see how this knowledge can be further generalized into more abstract representations and categories. 3.1 The Architecture of the Symbolic Pattern Associator The SPA is based on C4.5 <ref> (Quinlan, 1993) </ref> which is an improved implementation of the ID3 learning algorithm (cf. (Quinlan, 1986)). ID3 is a program for inducing classification rules in the form of decision trees from a set of classified examples. <p> An important feature of the SPA is explicit knowledge representation. Decision trees for output attributes can easily be transformed into propositional production rules <ref> (Quinlan, 1993) </ref>. Since entities of these rules are symbols with semantic meanings, the acquired knowledge often is comprehensible to the human observer. In addition, further processing and integration of these rules can yield high-level knowledge (e.g., rules using verb stems) (Ling & Marinov, 1993).
Reference: <author> Ripley, B. </author> <year> (1992). </year> <title> Statistical aspects of neural networks. </title> <type> Invited lectures for SemStat (Seminaire Europeen de Statistique, </type> <address> Sandbjerg, Denmark, </address> <month> 25-30 April </month> <year> 1992). </year>
Reference: <author> Rumelhart, D., & McClelland, J. </author> <year> (1986). </year> <title> On learning the past tenses of English verbs. </title>
Reference: <editor> In Rumelhart, D., McClelland, J., </editor> & <booktitle> the PDP Research Group (Eds.), Parallel Distributed Processing Vol 2, </booktitle> <pages> pp. 216 - 271. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Shavlik, J., Mooney, R., & Towell, G. </author> <year> (1991). </year> <title> Symbolic and neural learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6 (2), 111 - 144. </volume>
Reference: <author> Weiss, S., & Kulikowski, C. </author> <year> (1991). </year> <title> Computer Systems that Learn: classification and prediction methods from statistics, neural networks, machine learning, and expert systems. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <month> 229 </month>
References-found: 23

