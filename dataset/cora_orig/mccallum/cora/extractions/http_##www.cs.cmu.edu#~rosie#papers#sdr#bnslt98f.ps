URL: http://www.cs.cmu.edu/~rosie/papers/sdr/bnslt98f.ps
Refering-URL: http://www.cs.cmu.edu/~rosie/
Root-URL: http://www.cs.cmu.edu/~jr6b
Title: EXPERIMENTS IN INFORMATION RETRIEVAL FROM SPOKEN DOCUMENTS 1  
Author: A. G. Hauptmann, R. E. Jones, K. Seymore, S. T. Slattery, M. J. Witbrock*, and M. A. Siegler 
Address: Pittsburgh, PA 15213-3890  4616 Henry St. Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  *Justsystem Pittsburgh Research Center  
Note: 1 This research was supported in part by DARPA under research contract F33615-93-1-1330 and N00039-91-C-0158. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of DARPA or the U. S. Government.  
Abstract: This paper describes the experiments performed as part of the TREC-97 Spoken Document Retrieval Track. The task was to pick the correct document from 35 hours of recognized speech documents, based on a text query describing exactly one document. Among the experiments we described here are: Vocabulary size experiments to assess the effect of words missing from the speech recognition vocabulary; experiments with speech recognition using a stemmed language model; using confidence annotations that estimate of the correctness of each recognized word; using multiple hypotheses from the recognizer. And finally we also measured the effects of corpus size on the SDR task. Despite fairly high word error rates, information retrieval performance was only slightly degraded for speech recognizer transcribed documents. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M.-Y. Hwang, </author> <title> Subphonetic Acoustic Modeling for Speaker-Independent Continuous Speech Recognition. </title> <type> PhD Thesis, </type> <institution> CMU-CS-93-230, Carnegie Mellon University, </institution> <year> 1993. </year>
Reference: 2. <author> L. L. Chase, </author> <type> PhD thesis, </type> <institution> Carnegie Mellon University Robotics Tech Report, </institution> <year> 1997. </year>
Reference-contexts: We will compare the results of our annotation to this ideal, which we call Perfect Annotation. Features for Confidence Annotation The confidence annotation we performed is based on work by Lin Chase <ref> [2] </ref>, though annotation has been explored by many others including [3,4,5]. Typically confidence annotation is performed by taking information available about individual occurrences of words in the hypothesized text, from information produced within the speech recognizer, or outside the recognizer.
Reference: 3. <author> S. Cox and R. Rose, </author> <title> Confidence Measures for the Switchboard Database, </title> <booktitle> IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <year> 1996. </year>
Reference: 4. <author> L. Gillick and Y. Ito, </author> <title> Confidence Estimation and Evaluation, </title> <booktitle> LVCSR Hub-5 Workshop Presentation, </booktitle> <year> 1996. </year>
Reference: 5. <author> P. Jeanrenaud, M. Siu, H. Gish, </author> <title> Large Vocabulary Word Scoring as a Basis for Transcription Generation, </title> <booktitle> Proceedings of Eurospeech, </booktitle> <year> 1995. </year>
Reference: 6. <author> M. F. Porter, </author> <title> An algorithm for suffix stripping, </title> <booktitle> Program, </booktitle> <volume> 14(3) </volume> <pages> 130-137, </pages> <month> July </month> <year> 1980. </year>
Reference-contexts: Term mappings: A set of 4578 mappings was used to map words with irregular word endings that were not properly covered by an implementation of the Porter <ref> [6] </ref> algorithm. An online Houghton-Mifflin dictionary was used for this lookup of irregular words and their roots. An example of this mapping is APPENDICESfiAPPENDIX Term stemming: An implementation of the Porter algorithm was applied to map words to their common root.
Reference: 7. <author> J. R. Quinlan, </author> <title> Programs for Machine Learning, </title> <address> San Francisco, Calif.: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: We conducted experiments by splitting the training data into two sections, training our decision tree on one half, testing on the other half, then reversing the roles. Decision Tree Building The decision tree building algorithm we use is C4.5 <ref> [7] </ref>. It functions by taking all training data, and attempting to find rules based on features which distinguish between classes. Each item of training data is a word along with its associated features (described above), and its class of correct or incorrect.
Reference: 8. <author> K. Seymore, S. Chen, M. Eskenazi, and R. Rosenfeld. </author> <title> Language and Pronunciation Modeling in the CMU 1996 Hub 4 Evaluation, </title> <booktitle> Proc. Spoken Language Systems Technology Workshop. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1997. </year>
Reference-contexts: As a new feature, this language model incorporated cross-sentence-boundary trigrams to better model utterances containing more than one sentence. The lexicon was chosen from the most common words in the corpus, at a size that balanced the tradeoff between leaving words out-of-vocabulary and introducing acoustically confusable words <ref> [8] </ref>. For this evaluation, the vocabulary was comprised of the most frequent 51,000 words in the BN LM corpus, supplemented by some 200 multiword phrases and some 150 acronyms.
Reference: 9. <author> M. Siegler, U. Jain, B. Raj, and R. Stern. </author> <title> Automatic Segmentation, Classification, and Clustering of Broadcast News Audio, </title> <booktitle> Proc. Spoken Language Systems Technology Workshop. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1997. </year>
Reference-contexts: 1. INTRODUCTION For the first time, the 1997 Text REtrieval Conference <ref> (TREC-97) </ref> included an evaluation track for information retrieval on spoken documents. In this paper, we describe some experiments for the spoken document retrieval, with details of both the speech recognition system and the information retrieval engine. <p> The Speech Recognition Component The Sphinx-III speech recognition system was used for the CMU TREC SDR evaluation, and it was configured similarly to the that used in the 1996 DARPA CSR evaluation <ref> [9] </ref>, although several changes have been made to the recognizer since then. Sphinx-III is a large vocabulary, speaker independent, fully continuous hidden Markov model speech recognizer with separately trained acoustic, language and lexical models.
Reference: 10. <author> M. J. Witbrock, and A. G. Hauptmann, </author> <title> Speech Recognition and Information Retrieval, </title> <booktitle> Proceedings of the 1997 DARPA Speech Recognition Workshop, </booktitle> <address> Chantilly, VA, </address> <month> February 2-5, </month> <year> 1997. </year>
Reference-contexts: An example of this mapping is APPENDICESfiAPPENDIX Term stemming: An implementation of the Porter algorithm was applied to map words to their common root. A heavily stripped down core of the CMU Informedia SEIDX engine <ref> [10] </ref> was used to compare queries with documents.
Reference: 11. <author> D. Graff, Z. Wu, R. MacIntyre and M. Liberman, </author> <title> The 1996 Broadcast News Speech and Language-Model Corpus, </title> <booktitle> Proceedings of the 1997 DARPA Speech Recognition Workshop, </booktitle> <address> Chantilly, VA, </address> <month> February 2-5, </month> <year> 1997. </year>
Reference-contexts: In this paper, we describe some experiments for the spoken document retrieval, with details of both the speech recognition system and the information retrieval engine. The SDR Data The speech data were identical to the training data used in the 1997 ARPA Speech Recognition Workshop HUB-4 broadcast news evaluations <ref> [11] </ref>. The main difference lay in the split between training and testing data; here roughly half of the material was reserved for the test data and only half the total acoustic data was used for training the acoustic models. <p> The decoder used a Katz-smoothed trigram language model [12] trained on the 1992-1996 Broadcast News Language Modeling (BN LM) corpus <ref> [11] </ref>. This is a fairly standard language model, much like those that have been used by the DARPA speech recognition community for the past several years. As a space optimization, singleton trigrams and bigrams were excluded.
Reference: 12. <author> S. Katz, </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer, </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3),400-401, </volume> <month> March, </month> <year> 1987. </year>
Reference-contexts: For the current evaluation a gender-independent HMM with 6,000 senonically-tied states and 16 diagonal-covariance Gaussian mixtures was trained on a union of the CSR Wall Street Journal corpus and the 1996 TREC-6 training set. The decoder used a Katz-smoothed trigram language model <ref> [12] </ref> trained on the 1992-1996 Broadcast News Language Modeling (BN LM) corpus [11]. This is a fairly standard language model, much like those that have been used by the DARPA speech recognition community for the past several years. As a space optimization, singleton trigrams and bigrams were excluded.
Reference: 13. <editor> Salton, G., Ed, </editor> <title> The SMART Retrieval System, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1971. </year>
Reference-contexts: Stopword removal: A set of 811 stopwords from a combination of the SMART <ref> [13] </ref> IR engine and other available stopword lists were removed entirely. Term mappings: A set of 4578 mappings was used to map words with irregular word endings that were not properly covered by an implementation of the Porter [6] algorithm.
References-found: 13

