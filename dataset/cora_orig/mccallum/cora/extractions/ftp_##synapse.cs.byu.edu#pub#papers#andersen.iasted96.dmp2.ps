URL: ftp://synapse.cs.byu.edu/pub/papers/andersen.iasted96.dmp2.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: email: tim@axon.cs.byu.edu, martinez@cs.byu.edu  
Title: Using Multiple Node Types to Improve the Performance of DMP (Dynamic Multilayer Perceptron)  
Author: Tim L. Andersen and Tony R. Martinez 
Address: Provo, Utah 84602  
Affiliation: Computer Science Department, Brigham Young University,  
Date: 249-252, 1996.  
Note: Reference: Proceedings of the IASTED International Conference on Artificial Intelligence, Expert Systems and Neural Networks, pp.  
Abstract: This paper discusses a method for training multilayer perceptron networks called DMP2 (Dynamic Multilayer Perceptron 2). The method is based upon a divide and conquer approach which builds networks in the form of binary trees, dynamically allocating nodes and layers as needed. The focus of this paper is on the effects of using multiple node types within the DMP framework. Simulation results show that DMP2 performs favorably in comparison with other learning algorithms, and that using multiple node types can be beneficial to network performance. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Andersen, Tim and Tony Martinez. </author> <title> A Provably Convergent Dynamic Training Method for Multilayer Perceptron Networks. </title> <booktitle> Proceedings of the 2nd International Symposium on Neuroinformatics and Neurocomputers, </booktitle> <year> 1995. </year> <title> Azimi-Sadjadi, Mahmood (1993). Recursive Dynamic Node Creation in Multilayer Neural Networks. </title> <journal> I E E E Transactions on Neural Networks, </journal> <volume> Vol 4, No 2, </volume> <pages> pp 242 2 56 </pages> . 
Reference-contexts: Each node has as inputs the output of at most two other nodes and the training set inputs. A Genetic Algorithm (GA) is used to train the individual nodes of the network. The original DMP method, DMP1 <ref> (Andersen and Martinez 95) </ref>, generated networks which consisted entirely of perceptron nodes. However, since with the DMP method the individual nodes in the network are trained independently from all other nodes, it is possible to generate networks where each node implements a different type of decision function. <p> Both of these properties are proven as formal theorems in <ref> (Andersen and Martinez 95) </ref>. The decision function used and the way it is implemented can affect the guaranteed convergence properties of the network.
Reference: <author> Bartlett, </author> <title> Eric (1994). Dynamic Node Architecture Learning: An Information Theoretic Approach. </title> <booktitle> Neural Networks, </booktitle> <volume> Vol 7, No 1, </volume> <pages> pp 129-140. </pages>
Reference: <author> Fahlman, S. and C. </author> <month> Lebiere </month> <year> (1991). </year> <title> The Cascade Correlation Learning Architecture. </title>
Reference-contexts: Thus, several multilayer training algorithms have been proposed which do not require the user to specify the network architecture a-priori. Some of these include EGP (Romaniuk 1993), Cascade Correlation <ref> (Fahlman 1991) </ref>, Iterative Atrophy (Smotroff 1991), DCN (Romaniuk 1993), ASOCS (Martinez 1990), and DNAL (Bartlett 1993). All of these methods seek to dynamically generate an appropriate network structure for solving the given learning problem during the training phase.
Reference: <author> Martinez, Tony and Jacques Vidal (1988). </author> <title> Adaptive Parallel Logic Networks. </title> <journal> Journal of Parallel and Distributed Computing. </journal> <volume> Vol 5, </volume> <pages> pp 26-58. </pages>
Reference: <author> Martinez, T. R. and Campbell, D. M. </author> <year> (1991). </year> <title> "A Self Adjusting Dynamic Logic Module", </title> <journal> Journal of Parallel and Distributed Computing, v11, </journal> <volume> no. 4, </volume> <pages> pp. 303-13. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: The numbers in parenthesis are the standard deviations for the reported results. The three other learning methods reported on are c4.5 <ref> (Quinlan 1986) </ref>, a mutli-layer network trained with back propogation, and a single layer perceptron network.
Reference: <author> Romaniuk, Steve and Lawrence Hall (1993). </author> <title> Divide and Conquer Neural Networks. </title> <booktitle> Neural Networks, </booktitle> <volume> Vol 6, </volume> <pages> pp 1105-1116. </pages>
Reference-contexts: Thus, several multilayer training algorithms have been proposed which do not require the user to specify the network architecture a-priori. Some of these include EGP <ref> (Romaniuk 1993) </ref>, Cascade Correlation (Fahlman 1991), Iterative Atrophy (Smotroff 1991), DCN (Romaniuk 1993), ASOCS (Martinez 1990), and DNAL (Bartlett 1993). All of these methods seek to dynamically generate an appropriate network structure for solving the given learning problem during the training phase. <p> Thus, several multilayer training algorithms have been proposed which do not require the user to specify the network architecture a-priori. Some of these include EGP <ref> (Romaniuk 1993) </ref>, Cascade Correlation (Fahlman 1991), Iterative Atrophy (Smotroff 1991), DCN (Romaniuk 1993), ASOCS (Martinez 1990), and DNAL (Bartlett 1993). All of these methods seek to dynamically generate an appropriate network structure for solving the given learning problem during the training phase. <p> This paper discusses a dynamic method for training multilayer perceptron networks which we call DMP 2 (Dynamic Multilayer Perceptron 2). This method is similar in spirit to the DCN method given in <ref> (Romaniuk 1993) </ref> in that: The network begins with a single perceptron node and dynamically adds nodes as needed. As nodes are added to the network, each node is trained independently from all other nodes.
Reference: <author> Rosenblatt, F. </author> <year> (1958). </year> <title> The Perceptron: A probabilistic Model for Information Storage and Organization in the Brain. </title> <journal> Psychological Review, </journal> <volume> Vol. 65, No. </volume> <pages> 6. </pages>
Reference-contexts: 1 Introduction One of the first models used in the field of neural networking is the single layer, simple perceptron model <ref> (Rosenblatt 1958) </ref>. The well understood weakness of single layer perceptron networks is that they are able to learn only those functions which are linearly separable.
Reference: <author> Smotroff, Ira, David Friedman and Dennis Connolly (1991). </author> <title> Self Organizing Modular Neural Networks. </title> <booktitle> International Joint Conference on Neural Networks, II, </booktitle> <pages> 187-192. </pages>
Reference-contexts: Thus, several multilayer training algorithms have been proposed which do not require the user to specify the network architecture a-priori. Some of these include EGP (Romaniuk 1993), Cascade Correlation (Fahlman 1991), Iterative Atrophy <ref> (Smotroff 1991) </ref>, DCN (Romaniuk 1993), ASOCS (Martinez 1990), and DNAL (Bartlett 1993). All of these methods seek to dynamically generate an appropriate network structure for solving the given learning problem during the training phase.
Reference: <institution> Zarndt, Frederick (1995). Masters Thesis at Brigham Young University. </institution>
References-found: 10

