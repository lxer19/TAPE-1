URL: http://www.isi.edu/people/katia/replicator-ICDCS.ps.Z
Refering-URL: http://www.isi.edu/people/katia/
Root-URL: http://www.isi.edu
Email: katia@isi.edu  fdanzig, dante, erhyuantg@usc.edu  
Title: A Tool for Massively Replicating Internet Archives: Design, Implementation, and Experience  
Author: Katia Obraczka Peter Danzig, Dante DeLucia, Erh-Yuan Tsai 
Address: 4676 Admiralty Way Marina del Rey, CA 90292, USA  Los Angeles, CA 90089-0781  
Affiliation: University of Southern California Information Science Institute  University of Southern California Computer Science Department  
Abstract: This paper reports the design, implementation, and performance of a scalable and efficient tool to replicate Internet information services. Our tool targets replication degrees of tens of thousands of weakly-consistent replicas scattered throughout the Internet's thousands of autonomously administered domains. The main goal of our replication tool is to make existing replication algorithms scale in today's exponentially-growing, autonomously-managed internetworks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Berners-Lee, R. Cailliau, J-F. Groff, and B. Poller-mann. </author> <title> World-Wide Web: The information universe. </title> <journal> Electronic Networking: Research, Applications and Policy, </journal> <volume> 1(2), </volume> <month> Spring </month> <year> 1992. </year>
Reference-contexts: 1. Introduction Internet services provide large, rapidly evolving, highly accessed, autonomously managed information spaces. To achieve adequate performance, services such as WWW <ref> [1] </ref> will have to replicate their data in thousands of autonomous networks. As an example of a highly replicated service, take Internet news [5]. Although it manages a dynamic, flat, gigabyte database, it responds to queries in seconds. <p> Consequently it is written as a Unix daemon that does not fork, but instead uses non-blocking I/O for all communication. The interface to flood-d is via an interface that can be queried with either `telnet' or more conveniently with a WWW browser that speaks HTTP <ref> [1] </ref>. 3.1. Membership and Multiple Groups When a new site joins a group, it sends a join request to an existing group member.
Reference: [2] <author> C.M. Bowman, P.B. Danzig, D.R. Hardy, U. Manber, and M.F. Schwartz. </author> <title> The Harvest information discovery and access system. </title> <booktitle> Proceedings of the Second International World Wide Web Conference, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: The main contribution of the replication tool we built is to make GNS-like services scale in today's exponentially growing, autonomously managed internetworks. In this paper, we describe our experience designing, implementing, deploying and measuring our replication tool's performance. 2. Design The Harvest resource discovery system <ref> [2] </ref> has been designed and implemented to solve the scalability and efficiency problems of early resource discovery services. For availability and response time, Harvest relies on massive replication of its servers.
Reference: [3] <author> P.B. Danzig, K. Obraczka, and A. Kumar. </author> <title> An analysis of wide-area name server traffic: A study of the domain name system. </title> <booktitle> Proc. of the ACM SIGCOMM '92, </booktitle> <address> Baltimore, Maryland, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: They also use these administrative boundaries to partition their name space into several domains, which only need to be replicated in a handful of servers to meet adequate performance. In fact, according to the results we report in <ref> [3] </ref>, over 85% of second level domains in the DNS hierarchy are replicated at most three times, while 100% of these domains use at most 7 replicas. The same study also shows that more than 90% of DNS's second-level domains store less than 1,000 entries.
Reference: [4] <author> J. Howard, M. Kazar, S. Menees, D. Nichols, M. Satyanarayanan, R. Sidebotham, and M. West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1):5181, </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: Because of the limited domain sizes and small number of replicas, DNS's primary-copy replication scheme performs quite adequately. Similarly, distributed file systems organize their file space hierarchically, where intermediate nodes are directories and leaf nodes are files. Like LOCUS [11], Andrew-AFS 1 <ref> [4] </ref>, and Coda [12], distributed file systems use locality of reference to partition their file space into directory subtrees. File servers replicate a subset of files in a directory subtree. Both LOCUS and Andrew provide read-only file replication, while Coda uses distributed updates to keep its writable replicas weakly consistent.
Reference: [5] <author> B. Kantor and P. Lapsley. </author> <title> Network news transfer protocol a proposed standard for the stream-based transmission of news. Internet Request for Comments RFC 977, </title> <month> February </month> <year> 1986. </year>
Reference-contexts: 1. Introduction Internet services provide large, rapidly evolving, highly accessed, autonomously managed information spaces. To achieve adequate performance, services such as WWW [1] will have to replicate their data in thousands of autonomous networks. As an example of a highly replicated service, take Internet news <ref> [5] </ref>. Although it manages a dynamic, flat, gigabyte database, it responds to queries in seconds. In contrast, popular WWW and FTP servers are constantly too overloaded to provide reasonable response time to users.
Reference: [6] <author> B. Lampson. </author> <title> Designing a global name service. </title> <booktitle> Proceedings of the 5th. ACM Symposium on the Principles of Distributed Computing, </booktitle> <pages> pages 110, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Also, it insulates groups from topological rearrangements of their neighboring groups and from most of the network traffic associated with group membership. For each replication group, flood-d automatically builds the logical topology over which updates travel. Unlike Lampson's Global Name Service (GNS) <ref> [6] </ref>, flood-d's logical update topologies are not restricted to a Hamiltonian cycle. By automating the process of building update topologies among replicas of a service, flood-d offloads system administrators from having to make logical topology decisions. We argue that efficient replication algorithms flood data between replicas.
Reference: [7] <author> L. </author> <title> McLoughlin. </title> <journal> The FTP-mirror software. </journal> <note> Available from ftp://src.doc.ic.ac.uk/computing/archiving/mirror, </note> <month> January </month> <year> 1994. </year>
Reference-contexts: The replication tool we implemented consists of two separate services: flood-d and mirror-d, whose name expresses its reliance on the well-known FTP-mirror package <ref> [7] </ref>. Flood-d estimates the underlying physical network load by measuring available bandwidth and round-trip time (RTT) between members of a replication group. Based on these estimates, flood-d computes a fault-tolerant, low-cost, low-diameter logical update topology for the group.
Reference: [8] <author> P. Mockapetris. </author> <title> Domain names concepts and facilities. Internet Request for Comments RFC 1034, </title> <month> November </month> <year> 1987. </year>
Reference-contexts: What Current Algorithms Lack As existing naming services and distributed file systems have demonstrated, the problem of replicating data that can be partitioned into autonomously managed subspaces has well-known solutions. Naming services such as the Domain Name Service (DNS) <ref> [8] </ref> and Grapevine organize their name space hierarchically according to well-defined administrative boundaries. They also use these administrative boundaries to partition their name space into several domains, which only need to be replicated in a handful of servers to meet adequate performance.
Reference: [9] <author> K. Obraczka. </author> <title> Massively Replicating Services in Wide-Area Internetworks. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of southern California, </institution> <month> De-cember </month> <year> 1994. </year>
Reference-contexts: The algorithm first computes a minimum spanning tree connecting all the nodes, and then, for each node whose degree d is less than the required connectivity k, adds the current cheapest edge until d = k. We are currently using k = 2. The original topology generator <ref> [9] </ref> produced low-cost, low-diameter, k-connected topologies for a group using simulated annealing as its optimization technique. Our experiments demonstrated that this sophisticated algorithm only produced moderate reductions in total edge cost. Consequently, in practice we use a simpler, faster, less optimal algorithm.
Reference: [10] <author> D. Oppen and Y. Dalal. </author> <title> The Clearinghouse: A decentralized agent for locating named objects in a distributed environment. </title> <journal> ACM Transactions on Office Information Systems, </journal> <volume> 1(3):230253, </volume> <month> July </month> <year> 1983. </year>
Reference-contexts: Because layered network protocols hide the network topology from application programs, replicas themselves cannot select their flooding peers to optimize use of the network. Both Grapevine and its commercial successor, the Clearinghouse <ref> [10] </ref> ignore network and update topology. GNS assumes the existence of a single administrator who hand-configures the topology over which updates travel. The GNS administrator places replicas in a Hamiltonian cy 1 Andrew is the name of the research project at Carnegie-Mellon University.
Reference: [11] <author> G. Popek, B. Walker, J. Chow, D. Edwards, C. Kline, and G. Rudisin ang G. Thiel. </author> <title> LOCUS: A network transparent, high reliability distributed system. </title> <booktitle> Proc. of the 8th. Symposium on Operating Systems Principles, </booktitle> <pages> pages 169177, </pages> <month> December </month> <year> 1981. </year>
Reference-contexts: Because of the limited domain sizes and small number of replicas, DNS's primary-copy replication scheme performs quite adequately. Similarly, distributed file systems organize their file space hierarchically, where intermediate nodes are directories and leaf nodes are files. Like LOCUS <ref> [11] </ref>, Andrew-AFS 1 [4], and Coda [12], distributed file systems use locality of reference to partition their file space into directory subtrees. File servers replicate a subset of files in a directory subtree.
Reference: [12] <author> M. Satyanarayanan. </author> <title> Scalable, secure, and highly available distributed file access. </title> <journal> Computer Magazine, </journal> <volume> 23(5):921, </volume> <month> May </month> <year> 1990. </year> <month> 8 </month>
Reference-contexts: Because of the limited domain sizes and small number of replicas, DNS's primary-copy replication scheme performs quite adequately. Similarly, distributed file systems organize their file space hierarchically, where intermediate nodes are directories and leaf nodes are files. Like LOCUS [11], Andrew-AFS 1 [4], and Coda <ref> [12] </ref>, distributed file systems use locality of reference to partition their file space into directory subtrees. File servers replicate a subset of files in a directory subtree. Both LOCUS and Andrew provide read-only file replication, while Coda uses distributed updates to keep its writable replicas weakly consistent.
References-found: 12

