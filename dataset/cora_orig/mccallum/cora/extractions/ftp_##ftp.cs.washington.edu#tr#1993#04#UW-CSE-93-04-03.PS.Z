URL: ftp://ftp.cs.washington.edu/tr/1993/04/UW-CSE-93-04-03.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Efficient Support for Multicomputing on ATM Networks  
Author: Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Technical Report 93-04-03 April 12, 1993 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, Dan Nussbaum, Mike Parkin, and Donald Yeung. </author> <title> The MIT Alewife machine: A large-scale distributed-memory multiprocessor. </title> <type> Technical Report MIT/LCS Memo TM-454, </type> <institution> Laboratory for Computer Science, MIT, </institution> <year> 1991. </year>
Reference-contexts: compare-and-swap operation is reliable in the face of cell loss.) Beyond the cost of interrupt handling, additional performance gains are possible by redesigning the network interface to avoid using the I/O bus, using network interfaces connected to the cache bus instead, as is done in dedicated multiprocessors such as Alewife <ref> [1] </ref>. One might ask whether a design optimized for cell-size writes suffers on larger block transfers, relative to a specialized block transfer primitive. To answer this question, we implemented and measured a block transfer instruction that transfers data from an arbitrary-sized user buffer rather than a fixed set of registers.
Reference: [2] <author> Thomas E. Anderson, Henry M. Levy, Brian N. Bershad, and Edward D. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Simple processor support for demultiplexing I/O interrupts, e.g., as suggested in <ref> [2] </ref>, would reduce the costs of a software approach. Given low-cost interrupt handling, the software approach has a flexibility advantage over a tightly integrated, pure hardware approach providing a few special network instructions. Application-specific instructions can be easily added using the software approach.
Reference: [3] <author> Emmanuel A. Arnould, Francois J. Bitz, Eric C. Cooper, H.T. Kung, Robert D. Sansom, and Peter A. Steenkiste. </author> <title> The design of Nectar: A network backplane for heterogeneous multicomputers. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1989. </year>
Reference-contexts: More recently, multi-computer researchers have incorporated some of these ideas into reliable networks for distributed-memory multicomputers [25]. The Nectar project at CMU connected workstations using a high-performance custom network <ref> [3] </ref>, while DEC's VAXclusters system [17] used complex communication controllers to provide distribution services for a cluster. At a coarse granularity, systems like Ivy [19], Munin [8], and Memnet [11] provide coherent remote page access using high-level software or hardware.
Reference: [4] <author> Brian N. Bershad. </author> <title> The increasing irrelevance of IPC performance for microkernel-based operating systems. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-kernels and Other Architectures, </booktitle> <pages> pages 205-211, </pages> <month> April </month> <year> 1992. </year> <month> 11 </month>
Reference-contexts: The actual computation time for each RPC is again negligible. For comparison, the time to make a same-machine null RPC call using a well optimized system <ref> [4] </ref> is shown in the last column.
Reference: [5] <author> Brian N. Bershad, Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1), </volume> <month> February </month> <year> 1990. </year>
Reference-contexts: We now examine marshaling and transport in the context of our RPC implementation. 3.1.1 Marshaling Given the notion of protected, remote memory that can be shared between a client and server, it is natural to extend techniques used for high-performance same-machine RPC, such as LRPC <ref> [5] </ref> and URPC [6]. In our system, the server exports stacks that are then imported by clients at bind time. On an RPC call, the client stub picks an available stack for the server and builds a call frame on that stack using the remote write operation.
Reference: [6] <author> Brian N. Bershad, Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> User-level interprocess communication for shared memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(2), </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: We now examine marshaling and transport in the context of our RPC implementation. 3.1.1 Marshaling Given the notion of protected, remote memory that can be shared between a client and server, it is natural to extend techniques used for high-performance same-machine RPC, such as LRPC [5] and URPC <ref> [6] </ref>. In our system, the server exports stacks that are then imported by clients at bind time. On an RPC call, the client stub picks an available stack for the server and builds a call frame on that stack using the remote write operation.
Reference: [7] <author> Andrew D. Birrell and Bruce Jay Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: The function of the transport layer is to provide an efficient mechanism to cope with data loss. Traditional RPC transports were designed for the Ethernet, where RPC calls typically fit into a single packet. Consequently RPC protocols have been optimized for single packet exchanges <ref> [7] </ref>. With the small cell size of an ATM network, however, data for typical RPC packets may not fit into a single ATM cell. There are at least two alternative schemes to optimize for the common case of multi-packet exchange on ATM networks.
Reference: [8] <author> John B. Carter, John K. Bennet, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The Nectar project at CMU connected workstations using a high-performance custom network [3], while DEC's VAXclusters system [17] used complex communication controllers to provide distribution services for a cluster. At a coarse granularity, systems like Ivy [19], Munin <ref> [8] </ref>, and Memnet [11] provide coherent remote page access using high-level software or hardware. We expect that our model could be used as a lower-level mechanism for alternative implementations of shared, coherent, distributed virtual memory systems such as these.
Reference: [9] <author> David R. Cheriton and Carey L. Williamson. </author> <title> VMTP as the transport layer for high-performance distributed systems. </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(6) </volume> <pages> 37-44, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: In this approach the entire message must be retransmitted even if only one packet is lost. But, if packets are rarely lost, this scheme has the advantage of a simple design and implementation. Another alternative, and the one we use, is based on selective acknowledgements <ref> [9, 10] </ref>. A fixed number of cells (say N ) are grouped into a message; the cells are then transmitted without waiting for an acknowledgement.
Reference: [10] <author> David D. Clark, Mark L. Lambert, and Lixia Zhang. NETBLT: </author> <title> A high throughput transport protocol. </title> <booktitle> In Proceedings of the 1987 SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 353-359, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: In this approach the entire message must be retransmitted even if only one packet is lost. But, if packets are rarely lost, this scheme has the advantage of a simple design and implementation. Another alternative, and the one we use, is based on selective acknowledgements <ref> [9, 10] </ref>. A fixed number of cells (say N ) are grouped into a message; the cells are then transmitted without waiting for an acknowledgement.
Reference: [11] <author> Gary Delp. </author> <title> The Architecture and Implementation of Memnet : A High-Speed Shared Memory Computer Communication Network. </title> <type> Ph.D. thesis, </type> <institution> University of Delaware, </institution> <year> 1988. </year>
Reference-contexts: The Nectar project at CMU connected workstations using a high-performance custom network [3], while DEC's VAXclusters system [17] used complex communication controllers to provide distribution services for a cluster. At a coarse granularity, systems like Ivy [19], Munin [8], and Memnet <ref> [11] </ref> provide coherent remote page access using high-level software or hardware. We expect that our model could be used as a lower-level mechanism for alternative implementations of shared, coherent, distributed virtual memory systems such as these.
Reference: [12] <author> Edward W. Felten. </author> <title> The case for application-specific communication protocols. </title> <booktitle> In Proceedings of Intel Supercomputer Systems Division Technology Focus Conference, </booktitle> <pages> pages 171-181, </pages> <year> 1992. </year>
Reference-contexts: While it is feasible for applications to directly use this access model, we expect that higher layers of software will typically hide many of the details from applications. For example, a stub compiler could use our low-level instructions to provide standard, at-most-once, RPC semantics; or, a communication compiler <ref> [12] </ref> could automatically generate high-performance communication code tailored to specific application requirements. In both cases, the mechanized approach relieves the programmer from the details of descriptor exchange, remote memory offsets, message loss, synchronization, and so on.
Reference: [13] <author> FORE Systems, </author> <title> 1000 Gamma Drive, Pittsburgh PA 15238. TCA-100 TURBOchannel ATM Computer Interface, User's Manual, </title> <year> 1992. </year>
Reference-contexts: In this section we briefly summarize the characteristics of the interface and the host architecture before describing our implementation. We use FORE host-network interfaces to connect our DECstation 5000s to a 140 Mb/s ATM network <ref> [13] </ref>. The interface is located on the TurboChannel I/O bus and does not have DMA capabilities. Instead, it implements two FIFO queues, one for transmitting ATM cells to the network and the other to buffer received cells.
Reference: [14] <institution> Intel Supercomputer Systems Division. Paragon XP/S Product Overview, </institution> <year> 1991. </year>
Reference-contexts: 1 Introduction Recent advances in network performance have increased the attractiveness of parallel computing on workstation clusters. Such loosely-coupled structures built from commodity parts offer several advantages over dedicated multicomputers, such as the Intel Paragon <ref> [14] </ref> and Thinking Machines CM-5 [24]: commodity parts are lower cost, they don't require dedicated use, and they are flexibly scaled and upgraded.
Reference: [15] <author> David B. Johnson and Willy Zwaenepoel. </author> <title> The Peregrine high-performance RPC system. </title> <journal> Software Practice and Experience, </journal> <volume> 23(2) </volume> <pages> 201-221, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: There is really no marshaling or demarshaling per se; data moves directly from source memory to destination memory without unnecessary copying or buffering. (Using writable stacks to simplify demarshaling costs 7 can be done even without the remote memory model <ref> [15] </ref>, however doing so is more complex.) Once the stack is ready, the client activates the server by writing a flag word in the server, for which the server polls. Call-by-reference is straightforward to provide through the remote read and write primitives.
Reference: [16] <institution> Kendall Square Research Inc., </institution> <address> 170 Tracer Lane, Waltham, MA 02154. </address> <institution> Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: In effect, we show how shared-memory primitives can be implemented and used effectively at a coarse granularity (relative to shared-memory multicomputers such as the Dash [18] or KSR <ref> [16] </ref>), in support of parallel and distributed programming using current technology.
Reference: [17] <author> Nancy P. Kronenberg, Henry M. Levy, and William D. Strecker. Vaxclusters: </author> <title> A closely-coupled distributed system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2) </volume> <pages> 130-146, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: More recently, multi-computer researchers have incorporated some of these ideas into reliable networks for distributed-memory multicomputers [25]. The Nectar project at CMU connected workstations using a high-performance custom network [3], while DEC's VAXclusters system <ref> [17] </ref> used complex communication controllers to provide distribution services for a cluster. At a coarse granularity, systems like Ivy [19], Munin [8], and Memnet [11] provide coherent remote page access using high-level software or hardware.
Reference: [18] <author> Daniel Lenoski, James Loudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessey. </author> <title> The DASH prototype: implementation and performance. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In effect, we show how shared-memory primitives can be implemented and used effectively at a coarse granularity (relative to shared-memory multicomputers such as the Dash <ref> [18] </ref> or KSR [16]), in support of parallel and distributed programming using current technology.
Reference: [19] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The Nectar project at CMU connected workstations using a high-performance custom network [3], while DEC's VAXclusters system [17] used complex communication controllers to provide distribution services for a cluster. At a coarse granularity, systems like Ivy <ref> [19] </ref>, Munin [8], and Memnet [11] provide coherent remote page access using high-level software or hardware. We expect that our model could be used as a lower-level mechanism for alternative implementations of shared, coherent, distributed virtual memory systems such as these.
Reference: [20] <author> Richard J. Littlefield. </author> <title> Characterizing and tuning communication performance on the Touchstone DELTA and the iPSC/860. </title> <booktitle> In Proceedings of the 1992 Intel User's Group Meeting, </booktitle> <pages> pages 4-7, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The average time required to perform a remote write of a given size is shown in Table 2. We did not perform measurements on the CM-5 and the DELTA, but use values reported elsewhere <ref> [20, 25] </ref>. Table 3 compares the sustained throughput achieved by the various systems. Our implementation overlaps multiple write requests; switch overhead is included in the measurement.
Reference: [21] <author> J. M. Ortega and R. G. Voight. </author> <title> Solution of partial differential equations on vector and parallel computers. </title> <journal> SIAM Review, </journal> <volume> 27(149), </volume> <year> 1985. </year>
Reference-contexts: The problem is known to be NP hard, so our particular solution uses a heuristic. Our program has an optimizer that scans the grid and changes spins if doing so would lower the total energy. The optimizer contains two loops that are written using red/black coloring <ref> [21] </ref>, a common parallelization technique used in scientific applications. The grid is spatially distributed amongst the processors' memories such that each processor has two nearest neighbors. The boundaries of the grid contain data that is shared between neighboring processors.
Reference: [22] <author> Michael D. Schroeder and Michael Burrows. </author> <title> Performance of Firefly RPC. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 1-17, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: We then describe two parallel applications that we have implemented using the model, and show their speedups on the network. 3.1 RPC Support for Distributed Systems It is possible to bypass our interface and build RPC using a conventional network model and well-known techniques <ref> [22] </ref>. However, it is convenient to have one interface for all workloads if performance is not impacted. We demonstrate in this section that an efficient RPC based on the shared memory primitives is in fact feasible.
Reference: [23] <author> Alfred Z. Spector. </author> <title> Performing remote operations efficiently on a local computer network. </title> <journal> Communications of the ACM, </journal> <volume> 25(4) </volume> <pages> 246-260, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: The notion of network remote memory access is not new: Spector first explored the idea on the experimental Ethernet <ref> [23] </ref>, concluding that the small data packet and high bandwidth requirements ruled out Ethernet-style networks as a basis for realistic loosely-coupled multicomputing. More recently, multi-computer researchers have incorporated some of these ideas into reliable networks for distributed-memory multicomputers [25]. <p> These mechanisms include (1) descriptor maintenance, (2) export and import of segments, (3) application-based pinning/unpinning of virtual memory pages, (4) segment write inhibit for synchronization, and (5) interrupt control. Support for these facilities is an important part of our model, and differentiates our work from previous efforts (e.g., <ref> [23] </ref>); however, space prohibits a complete discussion of these operations in this paper. 3 2.2 The Testbed We have implemented our network access model using a software layer on top of an existing ATM host-network interface.
Reference: [24] <institution> Thinking Machines Corporation. </institution> <type> CM-5 Technical Summary, </type> <year> 1991. </year>
Reference-contexts: 1 Introduction Recent advances in network performance have increased the attractiveness of parallel computing on workstation clusters. Such loosely-coupled structures built from commodity parts offer several advantages over dedicated multicomputers, such as the Intel Paragon [14] and Thinking Machines CM-5 <ref> [24] </ref>: commodity parts are lower cost, they don't require dedicated use, and they are flexibly scaled and upgraded. While workstation clusters have been widely used for distributed (and sometimes parallel) applications in the past, their communication environment relies on heavyweight client/server models and message-based (RPC) communication.
Reference: [25] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year> <month> 13 </month>
Reference-contexts: More recently, multi-computer researchers have incorporated some of these ideas into reliable networks for distributed-memory multicomputers <ref> [25] </ref>. The Nectar project at CMU connected workstations using a high-performance custom network [3], while DEC's VAXclusters system [17] used complex communication controllers to provide distribution services for a cluster. <p> The average time required to perform a remote write of a given size is shown in Table 2. We did not perform measurements on the CM-5 and the DELTA, but use values reported elsewhere <ref> [20, 25] </ref>. Table 3 compares the sustained throughput achieved by the various systems. Our implementation overlaps multiple write requests; switch overhead is included in the measurement.
References-found: 25

