URL: http://www.research.att.com/~singer/papers/pos_acl.ps.gz
Refering-URL: http://www.research.att.com/~singer/pub.html
Root-URL: 
Email: Internet: schuetze@csli.stanford.edu Internet: singer@cs.huji.ac.il  
Title: PART-OF-SPEECH TAGGING USING A VARIABLE MEMORY MARKOV MODEL  
Author: Hinrich Schutze Yoram Singer 
Address: Stanford, CA 94305-4115 Hebrew University, Jerusalem 91904  
Affiliation: Center for the Study of Institute of Computer Science and Language and Information Center for Neural Computation  
Abstract: We present a new approach to disambiguating syntactically ambiguous words in context, based on Variable Memory Markov (VMM) models. In contrast to fixed-length Markov models, which predict based on fixed-length histories, variable memory Markov models dynamically adapt their history length based on the training data, and hence may use fewer parameters. In a test of a VMM based tagger on the Brown corpus, 95.81% of tokens are correctly classified. 
Abstract-found: 1
Intro-found: 1
Reference: <author> N. Abe and M. Warmuth, </author> <title> On the computational complexity of approximating distributionsby probabilistic automata, </title> <journal> Machine Learning, </journal> <volume> Vol. 9, </volume> <pages> pp. 205-260, </pages> <year> 1992. </year>
Reference-contexts: We currently investigate other stochastic models that can accommodate long distance statistical correlation (see (Singer and Tishby, 1994) for preliminary results). However, there are theoretical clues that those models are much harder to learn (Kearns et al., 1993), including HMM based models <ref> (Abe and Warmuth, 1992) </ref>. Another drawback of the current tagging scheme is the independence assumption of the underlying tags and the observed words, and the ad-hoc estimation of the static probabilities.
Reference: <author> C. Antoniak, </author> <title> Mixture of Dirichlet processes with applications to Bayesian nonparametric problems, </title> <journal> Annals of Statistics, </journal> <volume> Vol. 2, </volume> <pages> pp. 1152-174, </pages> <year> 1974. </year>
Reference-contexts: The a-posteriori probability estimation of the individual words can be estimated from the word counts and the tag class priors. Those priors can be modeled as a mixture of Dirichlet distributions <ref> (Antoniak, 1974) </ref>, where each mixture component would correspond to a different tag class. Currently we estimate the state transition probabilities from the conditional counts assuming a uniform prior. The same technique can be used to estimate those parameters as well.
Reference: <author> J. Berger, </author> <title> Statistical decision theory and Bayesian analysis, </title> <publisher> New-York: Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: We are pursuing a systematic scheme to estimate those probabilities based on Bayesian statistics, by assigning a discrete probability distribution, such as the Dirichlet distribution <ref> (Berger, 1985) </ref>, to each tag class. The a-posteriori probability estimation of the individual words can be estimated from the word counts and the tag class priors. Those priors can be modeled as a mixture of Dirichlet distributions (Antoniak, 1974), where each mixture component would correspond to a different tag class.
Reference: <author> E. Brill. </author> <title> Automatic grammar induction and parsing free text: A transformation-based approach. </title> <booktitle> In Proceedings of ACL 31, </booktitle> <pages> pp. 259-265, </pages> <year> 1993. </year>
Reference-contexts: Because of the large number of parameters higher-order fixed length models are hard to estimate. (See <ref> (Brill, 1993) </ref> for a rule-based approach to incorporating higher-order information.) In a Hidden Markov Model (HMM) (Kupiec, 1992; Jelinek, 1985), a different state is defined for each POS tag and the transition probabilities and the output probabilities are estimated using the EM (Dempster et al., 1977) algorithm, which guarantees convergence to
Reference: <author> E. Charniak, Curtis Hendrickson, Neil Jacobson, and Mike Perkowitz, </author> <title> Equations for Part-of-Speech Tagging, </title> <booktitle> Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. 784-789, </pages> <year> 1993. </year>
Reference-contexts: Two stochastic methods have been widely used for POS tagging: fixed order Markov models and Hidden Markov models. Fixed order Markov models are used in (Church, 1989) and <ref> (Charniak et al., 1993) </ref>. Since the order of the model is assumed to be fixed, a short memory (small order) is typically used, since the number of possible combinations grows exponentially. <p> word only depends on its tags, we get: P (t 1;n ; w 1;n ) = i=1 Given a variable memory Markov model M, P (t i jt 1;i1 ) is estimated by P (t i jS i1 ; M) where 1 Part of the following derivation is adapted from <ref> (Charniak et al., 1993) </ref>. S i = t (*; t 1;i ), since the dynamics of the sequence are represented by the transition probabilities of the corresponding automaton. <p> On the other hand, a past participle is virtually impossible after "MD RB" whereas adverbs that are not preceded by modals modify past participles quite often. While it is known that Markov models of order 2 give a slight improvement over order-1 models <ref> (Charniak et al., 1993) </ref>, the number of parameters in our model is much smaller than in a full order-2 Markov model (49*184 = 9016 vs. 184*184*184 = 6,229,504). <p> As a consequence, the probability that a new word is an article is zero, whereas it is high for verbs and nouns. We need a smoothing scheme that takes this fact into account. Extending an idea in <ref> (Charniak et al., 1993) </ref>, we estimate the probability of tag conversion to find an adequate smoothing scheme. Open and closed classes differ in that words often add a tag from an open class, but rarely from a closed class. <p> The second sentence is an example for one of the tagging mistakes in the Brown corpus, "elected" is clearly used as a past participle, not as a past tense form. Comparison with other Results Charniak et al.'s result of 95.97% <ref> (Charniak et al., 1993) </ref> is slightly better than ours. This difference is probably due to the omission of rare tags that permit reliable prediction of the following tag (the case of "HVZfl" for "hasn't").
Reference: <author> K.W. Church, </author> <title> A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text, </title> <booktitle> Proceedings of ICASSP, </booktitle> <year> 1989. </year>
Reference-contexts: Therefore, a correct syntactic classification of words in context is important for most syntactic and other higher-level processing of natural language text. Two stochastic methods have been widely used for POS tagging: fixed order Markov models and Hidden Markov models. Fixed order Markov models are used in <ref> (Church, 1989) </ref> and (Charniak et al., 1993). Since the order of the model is assumed to be fixed, a short memory (small order) is typically used, since the number of possible combinations grows exponentially.
Reference: <author> A. Dempster, N. Laird, and D. Rubin, </author> <title> Maximum Likelihood estimation from Incomplete Data via the EM algorithm, </title> <journal> J. Roy. Statist. Soc., </journal> <volume> Vol. 39(B), </volume> <pages> pp. 1-38, </pages> <year> 1977. </year>
Reference-contexts: fixed length models are hard to estimate. (See (Brill, 1993) for a rule-based approach to incorporating higher-order information.) In a Hidden Markov Model (HMM) (Kupiec, 1992; Jelinek, 1985), a different state is defined for each POS tag and the transition probabilities and the output probabilities are estimated using the EM <ref> (Dempster et al., 1977) </ref> algorithm, which guarantees convergence to a local minimum (Wu, 1983). The advantage of an HMM is that it can be trained using untagged text. On the other hand, the training procedure is time consuming, and a fixed model (topology) is assumed.
Reference: <author> W.N. Francis and F. Kucera, </author> <title> Frequency Analysis of English Usage, </title> <publisher> Houghton Mi*in, </publisher> <address> Boston MA, </address> <year> 1982. </year>
Reference-contexts: The estimation of the static parameters P (t i jw i ) is described in the next section. We trained the variable memory Markov model on the Brown corpus <ref> (Francis and Kucera, 1982) </ref>, with every tenth sentence removed (a total of 1,022,462 tags). The four stylistic tag modifiers "FW" (foreign word), "TL" (title), "NC" (cited word), and "HL" (headline) were ignored reducing the complete set of 471 tags to 184 different tags.
Reference: <author> F. Jelinek, </author> <title> Robust part-of-speech tagging using a hidden Markov model, </title> <type> IBM Tech. Report, </type> <year> 1985. </year>
Reference: <author> M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. Schapire, L. Sellie, </author> <title> On the Learnability of Discrete Distributions, </title> <booktitle> The 25th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1994. </year>
Reference: <author> S. Kullback, </author> <title> Information Theory and Statistics, </title> <publisher> New-York: Wiley, </publisher> <year> 1959. </year>
Reference-contexts: A node s, must be added to the tree if it statistically differs from its parent node s. A natural measure to check the statistical difference is the relative entropy (also known as the Kullback-Leibler (KL) divergence) <ref> (Kullback, 1959) </ref>, between the conditional probabilities P (js) and P (js).
Reference: <author> J. Kupiec, </author> <title> Robust part-of-speech tagging using a hidden Markov model, </title> <booktitle> Computer Speech and Language, </booktitle> <volume> Vol. 6, </volume> <pages> pp. 225-242, </pages> <year> 1992. </year>
Reference-contexts: This difference is probably due to the omission of rare tags that permit reliable prediction of the following tag (the case of "HVZfl" for "hasn't"). Kupiec achieves up to 96.36% correctness <ref> (Kupiec, 1992) </ref>, without using a tagged corpus for training as we do. But the results are not easily comparable with ours since a lexicon is used that lists only possible tags.
Reference: <author> L.R. Rabiner and B. H. Juang, </author> <title> An Introduction to Hidden Markov Models, </title> <journal> IEEE ASSP Magazine, </journal> <volume> Vol. 3, No. 1, </volume> <pages> pp. 4-16, </pages> <year> 1986. </year>
Reference: <author> J. Rissanen, </author> <title> Modeling by shortest data discription, </title> <journal> Automatica, </journal> <volume> Vol. 14, </volume> <pages> pp. 465-471, </pages> <year> 1978. </year>
Reference: <author> J. Rissanen, </author> <title> Stochastic complexity and modeling, </title> <journal> The Annals of Statistics, </journal> <volume> Vol. 14, No. 3, </volume> <pages> pp. 1080-1100, </pages> <year> 1986. </year>
Reference: <author> J. Rissanen and G. G. Langdon, </author> <title> Universal modeling and coding, </title> <journal> IEEE Trans. on Info. Theory, IT-27, </journal> <volume> No. 3, </volume> <pages> pp. 12-23, </pages> <year> 1981. </year>
Reference: <author> D. Ron, Y. Singer, and N. Tishby, </author> <title> The power of Amnesia, </title> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <year> 1993. </year>
Reference-contexts: Therefore, a VMM based tagger can be used for pruning many of the tagging alternatives using its prediction probability, but not as a complete tagging system. Furthermore, the VMM power can be better utilized in low level language processing tasks such as cleaning up corrupted text as demonstrated in <ref> (Ron et al., 1993) </ref>. We currently investigate other stochastic models that can accommodate long distance statistical correlation (see (Singer and Tishby, 1994) for preliminary results). However, there are theoretical clues that those models are much harder to learn (Kearns et al., 1993), including HMM based models (Abe and Warmuth, 1992).
Reference: <author> D. Ron, Y. Singer, and N. Tishby, </author> <title> Learning Probabilistic Automata with Variable Memory Length, </title> <booktitle> Proceedings of the 1994 Workshop on Computational Learning Theory, </booktitle> <year> 1994. </year>
Reference: <author> Y. Singer and N. Tishby, </author> <title> Inferring Probabilistic Acyclic Automata Using the Minimum Description Length Principle, </title> <booktitle> Proceedings of IEEE Intl. Symp. on Info. Theory, </booktitle> <year> 1994. </year>
Reference-contexts: Furthermore, the VMM power can be better utilized in low level language processing tasks such as cleaning up corrupted text as demonstrated in (Ron et al., 1993). We currently investigate other stochastic models that can accommodate long distance statistical correlation (see <ref> (Singer and Tishby, 1994) </ref> for preliminary results). However, there are theoretical clues that those models are much harder to learn (Kearns et al., 1993), including HMM based models (Abe and Warmuth, 1992).
Reference: <author> R. Weischedel, M. Meteer, R. Schwartz, </author> <title> L. </title>
Reference: <author> Ramshaw, and J. Palmucci. </author> <title> Coping with ambiguity and unknown words through probabilistic models. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 359-382, </pages> <year> 1993. </year>
Reference: <author> J. Wu, </author> <title> On the convergence properties of the EM algorithm, </title> <journal> Annals of Statistics, </journal> <volume> Vol. 11, </volume> <pages> pp. 95-103, </pages> <year> 1983. </year>
Reference-contexts: rule-based approach to incorporating higher-order information.) In a Hidden Markov Model (HMM) (Kupiec, 1992; Jelinek, 1985), a different state is defined for each POS tag and the transition probabilities and the output probabilities are estimated using the EM (Dempster et al., 1977) algorithm, which guarantees convergence to a local minimum <ref> (Wu, 1983) </ref>. The advantage of an HMM is that it can be trained using untagged text. On the other hand, the training procedure is time consuming, and a fixed model (topology) is assumed. Another disadvantage is due to the local convergence properties of the EM algorithm.
References-found: 22

