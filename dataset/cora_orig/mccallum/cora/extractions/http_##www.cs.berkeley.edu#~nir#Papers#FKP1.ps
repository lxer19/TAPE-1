URL: http://www.cs.berkeley.edu/~nir/Papers/FKP1.ps
Refering-URL: http://www.cs.berkeley.edu/~nir/Abstracts/FKP1.html
Root-URL: http://www.cs.berkeley.edu
Email: nir@cs.berkeley.edu  koller@cs.stanford.edu  avi@cs.stanford.edu  
Title: Structured Representation of Complex Stochastic Systems  
Author: Nir Friedman Daphne Koller Avi Pfeffer 
Address: 387 Soda Hall U.C. Berkeley Berkeley, CA 94720  Gates Building, 1A Stanford, CA 94305-9010  Gates Building, 1A Stanford, CA 94305-9010  
Affiliation: Computer Science Division  Computer Science Department Stanford University  Computer Science Department Stanford University  
Abstract: This paper considers the problem of representing complex systems that evolve stochastically over time. Dynamic Bayesian networks provide a compact representation for stochastic processes. Unfortunately, they are often unwieldy since they cannot explicitly model the complex organizational structure of many real life systems: the fact that processes are typically composed of several interacting subprocesses, each of which can, in turn, be further decomposed. We propose a hierarchically structured representation language which extends both dynamic Bayesian networks and the object-oriented Bayesian network framework of [9], and show that our language allows us to describe such systems in a natural and modular way. Our language supports a natural representation for certain system characteristics that are hard to capture using more traditional frameworks. For example, it allows us to represent systems where some processes evolve at a different rate than others, or systems where the processes interact only intermittently. We provide a simple inference mechanism for our representation via translation to Bayesian networks, and suggest ways in which the inference algorithm can exploit the additional structure encoded in our representation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller. </author> <title> Context-specific independence in Bayesian networks. </title> <booktitle> In Proc. </booktitle> <address> UAI, </address> <year> 1996. </year>
Reference-contexts: The unrolled object U N (X) is an interacting OOBN object defined as follows: * If A is a transient attribute of X, then U N (X) contains an attribute A [0]a copy of X:A in D 0 and N attributes A <ref> [1] </ref>; : : : ; A [N ], each of which is a copy of X:A. <p> The transition which updates the client's state based on the query results would be enabled only if both of these variables are true. We can extend this idea to our probabilistic framework by using the techniques of Boutilier et al. <ref> [1] </ref>. They define a notion of context specific independence (CSI), which corresponds to situations in which the variables on which some variable depends are different in different situations (or contexts).
Reference: [2] <author> X. Boyen and D. Koller. </author> <title> Tractable inference for complex stochastic processes. </title> <note> Submitted to UAI '98, </note> <year> 1998. </year>
Reference-contexts: If both are large, neither option will be feasible. This problem also occurs in traditional DBNs, and approximate algorithms for DBN inference is an important area of current research <ref> [4, 7, 2] </ref>. The work of Boyen and Koller [2] is particularly relevant to our discussion, as it explicitly utilizes a decomposition of a complex process into weakly interacting subprocesses. <p> If both are large, neither option will be feasible. This problem also occurs in traditional DBNs, and approximate algorithms for DBN inference is an important area of current research [4, 7, 2]. The work of Boyen and Koller <ref> [2] </ref> is particularly relevant to our discussion, as it explicitly utilizes a decomposition of a complex process into weakly interacting subprocesses. They utilize this decomposition to approximate the joint distribution over a Markovian separator by assuming that the states of the sub-processes are independent. <p> We believe that approximations are crucial for inference in large complex processes, and we hy pothesize that the encapsulation structure of our representation can guide approximation methods such as these of <ref> [4, 2] </ref>. We plan to examine these issues in future work. Acknowledgments Part of this work was done while Nir Friedman was at Stanford.
Reference: [3] <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persistence and causation. </title> <journal> Comp. Int., </journal> <volume> 5(3), </volume> <year> 1989. </year>
Reference-contexts: Thus, for example, a computer network is usually composed of several sub-networks, each of which is composed of servers and clients, and so on. How do we represent such a stochastic dynamic system? A standard approach, in the AI literature, is to use a dynamic Bayesian network (DBN) <ref> [3] </ref>. A DBN is a temporally-extended version of a Bayesian network (BN) [11], and shares many of the same advantages.
Reference: [4] <author> Z. Ghahramani and M. I. Jordan. </author> <title> Factorial hidden Markov models. </title> <journal> Machine Learning, </journal> <volume> 29, </volume> <year> 1997. </year>
Reference-contexts: If both are large, neither option will be feasible. This problem also occurs in traditional DBNs, and approximate algorithms for DBN inference is an important area of current research <ref> [4, 7, 2] </ref>. The work of Boyen and Koller [2] is particularly relevant to our discussion, as it explicitly utilizes a decomposition of a complex process into weakly interacting subprocesses. <p> We believe that approximations are crucial for inference in large complex processes, and we hy pothesize that the encapsulation structure of our representation can guide approximation methods such as these of <ref> [4, 2] </ref>. We plan to examine these issues in future work. Acknowledgments Part of this work was done while Nir Friedman was at Stanford.
Reference: [5] <author> E.J. Horvitz, H.J. Suermondt, and G.F. Cooper. </author> <title> Bounded conditioning: Flexible inference for decisions under scarce resources. </title> <booktitle> In Proc. </booktitle> <address> UAI, </address> <year> 1989. </year>
Reference-contexts: If we only know that runs where there are many queries are highly improbable, most of the probability mass would be on those runs where the interaction pattern is sparse. The bounded conditioning algorithm of <ref> [5] </ref> can then be used to restrict attention only to runs with sparse interaction. 7 Conclusion In this paper, we have presented a new language for representing complex dynamic systems with uncertainty.
Reference: [6] <author> T. Huang, D. Koller, J. Malik, G. Ogasawara, B. Rao, S.J. Russell, and J. Weber. </author> <title> Automatic symbolic traffic scene analysis using belief networks. </title> <booktitle> In AAAI, </booktitle> <year> 1994. </year>
Reference-contexts: However, most of the activity of a process is internal to it, and therefore encapsulated from the rest of the world. Simon [13] has observed that this type of hierarchical decomposition of complex systems in terms of weakly interacting subsystems is extremely common. For example, in a highway system <ref> [6] </ref>, most of the activity inside a car is local to it; only limited aspects, such as lane occupancy and speed, influence other cars on the highway. DBNs do not support the notion of a process, far less the ability to refer to its interactions with others. <p> In a standard DBN model, we begin by picking some fixed granularity of time, and then generating multiple instances of each state variable in our domain, one instance for each time slice. For example, in the traffic surveillance application of <ref> [6] </ref>, accurate tracking requires that the locations of vehicles be modeled at the rate of 30 time slices a second. All other variables in the system are therefore modeled at the same level of granularity. <p> However, most of these variables, e.g., the weather or the alertness of the driver, evolve much more slowly than the car's location. Clearly, had the presence of the location variables not forced them to do so, it would not have occurred to the designers of <ref> [6] </ref> to model these other variables so finely. What we want is a framework that allows us to model different parts of the system at different levels of time granularity. One could imagine trying to extend the DBN framework directly in order to achieve this goal.
Reference: [7] <author> K. Kanazawa, D. Koller, and S.J. Russell. </author> <title> Stochastic simulation algorithms for dynamic probabilistic networks. </title> <booktitle> In Proc. </booktitle> <address> UAI, </address> <year> 1995. </year>
Reference-contexts: If both are large, neither option will be feasible. This problem also occurs in traditional DBNs, and approximate algorithms for DBN inference is an important area of current research <ref> [4, 7, 2] </ref>. The work of Boyen and Koller [2] is particularly relevant to our discussion, as it explicitly utilizes a decomposition of a complex process into weakly interacting subprocesses.
Reference: [8] <author> U. Kjaerulff. </author> <title> A computational scheme for reasoning in dynamic probabilistic networks. </title> <booktitle> In Proc. </booktitle> <address> UAI, </address> <year> 1992. </year>
Reference-contexts: As we mentioned above, the key to most BN algorithms is the separation of the inference problem into two pieces using a set of variables that render the pieces conditionally independent (e.g., the interface of an object in OOBNs). DBN inference algorithms, even the most sophisticated <ref> [8] </ref>, rely on the same basic idea. Largely, DBN inference algorithms focus on Markovian separators, which separate the future of the process from its past.
Reference: [9] <author> D. Koller and A. Pfeffer. </author> <title> Object-oriented Bayesian networks. </title> <booktitle> In Proc. </booktitle> <address> UAI, </address> <year> 1997. </year>
Reference-contexts: For example, if our computer network uses many servers of the same type, we would like to utilize the same model for all of them. Our earlier work on the Object-Oriented Bayesian Network (OOBN) framework <ref> [9] </ref> addresses these problems for the case of static models. This framework supports decomposition and modularity by defining the domain in terms of a set of objects. <p> As we show, the explicit representation of such properties allows us to model the system in terms of a smaller number of variables, thereby potentially making inference more efficient. 2 Object-oriented Bayesian networks We begin with a brief overview of the OOBN framework <ref> [9] </ref>, on which our current work is based. An OOBN is a hierarchically structured probabilistic model, based on Bayesian networks (BNs). BNs provide a concise representation of a joint probability distribution over a set of variables. <p> An OOBN object X is well defined if G (X) is acyclic. Given this acyclicity requirement, the probabilistic model for an OOBN object is guaranteed to define a coherent conditional probability distribution. Theorem 2.4: <ref> [9] </ref> The probability model for a well defined OOBN object defines a conditional probability distribution over its value attributes given its input attributes. The OOBN framework also allows classes of objects to be defined. <p> Koller and Pfeffer show that the induced network is well-defined and captures the distribution defined by B: Theorem 2.6: <ref> [9] </ref> Let B be an OOBN, then P B (S (B)), the distribution B defines over simple attributes, is the same as P BN (B) (S (B)). <p> Koller and Pfeffer define a cost function Cost (X) that measures the complexity of this computation for an object X, one which is (in the worst-case) exponential in the number of simple variables in Interface (X) [ S A2V (X) Interface (A). They show that Theorem 2.8: <ref> [9] </ref> The complexity of the inference in BN (B) is O ( X Cost (X)).
Reference: [10] <author> A. Manna and A. Pnueli. </author> <title> Temporal Verification of Reactive Systems. </title> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Such situations are represented very naturally in the transition system framework of Manna and Pnueli <ref> [10] </ref>. There, each process can take one of several possible transitions, which affect its state. However, each transition is associated with a guard, which may or may not be true in a given state.
Reference: [11] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: How do we represent such a stochastic dynamic system? A standard approach, in the AI literature, is to use a dynamic Bayesian network (DBN) [3]. A DBN is a temporally-extended version of a Bayesian network (BN) <ref> [11] </ref>, and shares many of the same advantages. <p> Each variable is associated with a conditional probability table (CPT), encoding the local dependence of a variable on its parents. The complete joint distribution over the set of variables is defined as the product of the conditional probability of each node given its parents <ref> [11] </ref>. OOBNs extend BNs by allowing structured objects, which are hierarchically composed of other objects. Specifically, an object has a set of attributes. Some of these are simple, and correspond to nodes in a traditional BN. Others are complex, and take other objects as their value.
Reference: [12] <author> R. Shachter, S. Andersen, and P. Szolovits. </author> <title> Global conditioning for probabilisitic inference in belief networks. </title> <booktitle> In Proc. UAI, </booktitle> <pages> pp. 514-522, </pages> <year> 1994. </year>
Reference-contexts: If we know that there are exactly k active interactions in N time slices, there are only N interaction patterns, and a total of N 2 k` possible interactions between the server and the client. Using a process of global conditioning <ref> [12] </ref>, we can do a case analysis on the different possible interactions, thereby separating the client and server processes. The cost of this separation is N 2 k` , which (for small k) is exponentially smaller than the 2 N` required without using the sparsity.
Reference: [13] <editor> H. Simon. </editor> <booktitle> The Sciences of the Artificial. </booktitle> <publisher> MIT Press, </publisher> <year> 1981. </year>
Reference-contexts: In many real-life domains, including the ones above, the high-level processes are structured entities, each composed of lower-level processes. These processes interact, thereby probabilistically influencing each other. However, most of the activity of a process is internal to it, and therefore encapsulated from the rest of the world. Simon <ref> [13] </ref> has observed that this type of hierarchical decomposition of complex systems in terms of weakly interacting subsystems is extremely common.
References-found: 13

