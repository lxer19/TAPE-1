URL: ftp://ftp.ai.mit.edu/pub/cva/AI1532.ps.Z
Refering-URL: http://www.ai.mit.edu/people/npcarter/npcarter.html
Root-URL: 
Title: The M-Machine Multicomputer  
Author: Marco Fillo, Stephen W. Keckler, William J. Dally, Nicholas P. Carter, Andrew Chang, Yevgeny Gurevich, Whay S. Lee 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. Copyright c Massachusetts Institute of Technology, 1995  
Date: 1532 March, 1995  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY  
Pubnum: A.I. Memo No.  
Abstract: The M-Machine is an experimental multicomputer being developed to test architectural concepts motivated by the constraints of modern semiconductor technology and the demands of programming systems. The M-Machine computing nodes are connected with a 3-D mesh network; each node is a multithreaded processor incorporating 12 function units, on-chip cache, and local memory. The multiple function units are used to exploit both instruction-level and thread-level parallelism. A user accessible message passing system yields fast communication and synchronization between nodes. Rapid access to remote memory is provided transparently to the user with a combination of hardware and software mechanisms. This paper presents the architecture of the M-Machine and describes how its mechanisms maximize both single thread performance and overall system throughput. This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. The research described in this paper was supported by the Advanced Research Projects Agency and monitored by the Air Force Electronic Systems Division under contract F19628-92-C-0045 and in part by the Air Force Office of Scientific Research under contract F49620-94-1-0462. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal et al. </author> <title> The MIT Alewife machine: A large-scale distributed-memory multiprocessor. In Scalable Shared Memory Multiprocessors. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: This code is easily incorporated within the remote read and write handlers described in Section 4.2. Using local memory as a repository will allow remote data to be cached locally beyond the capacity of the local on-chip cache alone. Discussion: Directory-based, cache coherent multiprocessors such as Alewife <ref> [1] </ref> and DASH [15] implement coherence policies in hardware. This improves perfor mance at the cost of flexibility. Like the M-Machine, FLASH [14] implements remote memory access and cache coherence in software, but uses a coprocessor.
Reference: [2] <author> Shekhar Borkar et al. </author> <title> Supporting systolic and mem ory communication in iWarp. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 70-81, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Discussion: The M-Machine provides direct register-to-register communication, avoiding the overhead of memory copying at both the sender and the receiver, and eliminating the dedicated memory for message arrival, as is found on the J-Machine [6]. Register-mapped network interfaces have been used previously in the J-Machine and iWarp <ref> [2] </ref>, and have been described by *T [20] as well as Henry and Joerg [11]. However, none of these systems provide protection for user-level messages.
Reference: [3] <author> Nicholas P. Carter, Stephen W. Keckler, and William J. Dally. </author> <title> Hardware support for fast capability-based addressing. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 319-327. </pages> <publisher> Association for Computing Machinery Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: Special load and store operations may specify a precondition and a postcondition on the synchronization bit. These are the only atomic read-modify-write memory operations. The M-Machine supports a single global virtual address space. A light-weight capability system imple-2 Single error correcting, double error detecting ments protection through guarded pointers <ref> [3] </ref>, while paging is used to manage the relocation of data in physical memory within the virtual address space. The segmentation and paging mechanisms are independent so that protection may be preserved on variable-size segments of memory.
Reference: [4] <author> Robert P. Colwell, W. Eric Hall, Chandra S. Joshi, David B. Papworth, Paul K. Rodman, and James E. Tornes. </author> <title> Architecture and implementation of a VLIW supercomputer. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 910-919. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Novem-ber </month> <year> 1990. </year>
Reference-contexts: In addition, superscalars attempt to schedule instructions at runtime (much of which could be done at compile time), but they can only examine a small subsequence of the instruction stream. Very Long Instruction Word (VLIW) processors such as the Multiflow Trace series <ref> [4] </ref> use only compile time scheduling to manage instruction-level parallelism, resource usage, and communication among a partitioned register file. However, the strict lock-step execution is unable to tolerate the dynamic latencies found in multiprocessors.
Reference: [5] <institution> Cray Research, Inc., Chippewa Falls, WI. Cray T3D System Architecture Overview, </institution> <year> 1993. </year>
Reference-contexts: Most messages fit easily in this size and larger messages can be packetized and reassembled with very low overhead. Automatic translation of virtual processor numbers to physical processor identifiers is used in the Cray T3D <ref> [5] </ref>. The use of virtual addresses as message destinations in the M-Machine has two advantages.
Reference: [6] <author> William J. Dally et al. </author> <title> The J-Machine: A fine-grain con current computer. </title> <editor> In G.X. Ritter, editor, </editor> <booktitle> Proceedings of the IFIP Congress, </booktitle> <pages> pages 1147-1153. </pages> <publisher> North-Holland, </publisher> <month> August </month> <year> 1989. </year>
Reference-contexts: Discussion: The M-Machine provides direct register-to-register communication, avoiding the overhead of memory copying at both the sender and the receiver, and eliminating the dedicated memory for message arrival, as is found on the J-Machine <ref> [6] </ref>. Register-mapped network interfaces have been used previously in the J-Machine and iWarp [2], and have been described by *T [20] as well as Henry and Joerg [11]. However, none of these systems provide protection for user-level messages.
Reference: [7] <author> Steven J. Frank et al. </author> <title> Multiprocessor digital data pro cessing system. United States Patent No. </title> <address> 5,055,999, </address> <month> October 8, </month> <year> 1991. </year>
Reference-contexts: Like the M-Machine, FLASH [14] implements remote memory access and cache coherence in software, but uses a coprocessor. However, this system does not provide block status bits in the TLB to support caching remote data in DRAM. The subpage status bits of the KSR-1 <ref> [7] </ref> perform a function similar to that of the block status bits of the M-Machine. Implementing a remote memory access and coherence completely in software on a conventional processor would involve delays much greater than those shown in Table 1 as evidenced by experience with the Ivy system [16].
Reference: [8] <author> Anoop Gupta and Wolf-Dietrich Weber. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: Preliminary results. </title> <booktitle> In Proceedings of 16th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 273-280. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1989. </year>
Reference-contexts: However, fewer synchronization operations are required, and many of them can be included in data transfer between clusters. Using multiple threads to hide memory latencies and pipeline delays has been explored in several different studies and machines. Gupta and Weber explore the use of multiple hardware contexts in multiprocessors <ref> [8] </ref>, but the context switch overhead prevents the masking of pipeline latencies. MASA [9] as well as HEP [22] use fine grain multithreading to issue an instruction from a different context on every cycle in order to mask pipeline latencies.
Reference: [9] <author> Robert H. Halstead and Tetsuya Fujita. MASA: </author> <title> a mul tithreaded processor architecture for parallel symbolic computing. </title> <booktitle> In 15th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 443-451. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1988. </year>
Reference-contexts: Using multiple threads to hide memory latencies and pipeline delays has been explored in several different studies and machines. Gupta and Weber explore the use of multiple hardware contexts in multiprocessors [8], but the context switch overhead prevents the masking of pipeline latencies. MASA <ref> [9] </ref> as well as HEP [22] use fine grain multithreading to issue an instruction from a different context on every cycle in order to mask pipeline latencies. However, with the required round-robin scheduling, single thread performance is degraded by the number of pipeline stages.
Reference: [10] <author> John L. Hennessy and Norman P. Jouppi. </author> <title> Computer technology and architecture: An evolving interaction. </title> <booktitle> Computer, </booktitle> <pages> pages 18-29, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Advances in VLSI technology have resulted in computers with chip area dominated by memory and not by processing resources. The normalized area (in 2 ) of a VLSI chip 1 is increasing by 50% per year, while gate speed and communication bandwidth are increasing by 20% per year <ref> [10] </ref>. As a result, a 64-bit processor with a pipelined FPU (400M 2 ) is only 11% of a 3.6G 2 1993 0.5m chip and only 4% of a 10G 2 1996 0.35m chip.
Reference: [11] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A tightly coupled processor-network interface. </title> <booktitle> In Fifth International Conference on Architectural Support for Pro gramming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 111-122. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1992. </year>
Reference-contexts: Register-mapped network interfaces have been used previously in the J-Machine and iWarp [2], and have been described by *T [20] as well as Henry and Joerg <ref> [11] </ref>. However, none of these systems provide protection for user-level messages. Systems, like the J-Machine, that provide user ac cess to the network interface without atomicity must temporarily disable interrupts to allow the sending process to complete the message.
Reference: [12] <author> William M. Johnson. </author> <title> Superscalar Microprocessor De sign. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1991. </year>
Reference-contexts: Superscalar processors execute multiple instructions simultaneously by relying upon run-time scheduling mechanisms to determine data dependencies <ref> [23, 12] </ref>. However, they do not scale well with increasing number of function units because a greater number of register file ports and connections to the function units are required.
Reference: [13] <author> Stephen W. Keckler and William J. Dally. </author> <title> Pro cessor coupling: Integrating compile time and run-time scheduling for parallelism. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 202-213, </pages> <address> Queensland, Australia, </address> <month> May </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: To do this, nodes are designed to manage parallelism from the instruction level to the process level. The 12 function units in a single M-Machine node are controlled using a form of Processor Coupling <ref> [13] </ref> to exploit in 1 The parameter is a normalized, process independent unit of distance equivalent to one half of the gate length [18]. <p> A synchronization pipeline stage holds the next instruction to be issued from each of the six V-Threads until all of its operands are present and all of the required resources are available <ref> [13] </ref>. At every cycle this stage decides which instruction to issue from those which are ready to run. An H-Thread that is stalled waiting for data or resource availability consumes no resources other than the thread slot that holds its state. <p> However, the strict lock-step execution is unable to tolerate the dynamic latencies found in multiprocessors. Processor Coupling was originally introduced in <ref> [13] </ref> and used implicit synchronization between the clusters on every wide instruction. Relaxing the synchronization, as described in this section, has several advantages. First, it is easier to implement because control is localized completely within the clusters.
Reference: [14] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Hein lein, Richard Simoni, et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proc. 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313. </pages> <publisher> IEEE, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: The interleaving performed by the GTLB, although not as versatile as the CRAY T3D address centrifuge or the interleaving of the RP3 [21], provides a means of distributing ranges of the address space across a region of nodes. In contrast to both *T and FLASH <ref> [14] </ref> which use a separate communication coprocessor for receiving messages, the M-Machine incorporates that function on its already existing execution resources, an H-Thread in 9 the event V-Thread. This avoids idling resources associated with a dedicated processor. <p> Discussion: Directory-based, cache coherent multiprocessors such as Alewife [1] and DASH [15] implement coherence policies in hardware. This improves perfor mance at the cost of flexibility. Like the M-Machine, FLASH <ref> [14] </ref> implements remote memory access and cache coherence in software, but uses a coprocessor. However, this system does not provide block status bits in the TLB to support caching remote data in DRAM.
Reference: [15] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hen-nessy. </author> <title> The DASH prototype: Implementation and performance. </title> <booktitle> In Proceedings of 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 92-103. </pages> <publisher> IEEE, </publisher> <year> 1992. </year>
Reference-contexts: Using local memory as a repository will allow remote data to be cached locally beyond the capacity of the local on-chip cache alone. Discussion: Directory-based, cache coherent multiprocessors such as Alewife [1] and DASH <ref> [15] </ref> implement coherence policies in hardware. This improves perfor mance at the cost of flexibility. Like the M-Machine, FLASH [14] implements remote memory access and cache coherence in software, but uses a coprocessor.
Reference: [16] <author> Kai Li. Ivy: </author> <title> A shared virtual memory system for paral lel computing. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 94-101, </pages> <year> 1988. </year>
Reference-contexts: This fine grained control over data is similar to that provided in hardware based cache coherent multiprocessors, and alleviates the false sharing that exists in other software data coherence systems <ref> [16] </ref>. <p> Implementing a remote memory access and coherence completely in software on a conventional processor would involve delays much greater than those shown in Table 1 as evidenced by experience with the Ivy system <ref> [16] </ref>. The M-Machine's fast exception handling in a dedicated H-Thread avoids the delay associated with context switching and allows the user thread to execute in parallel with the exception handler. The GTLB avoids the overhead of manual translation and the cost of a system call to access the network.
Reference: [17] <author> P. G. Lowney, S. G. Freudenberger, T. J. Karzes, W. D. Lichtenstein, R. P. Nix, J. S. O'Donnell, and J. C. Rut-tenberg. </author> <title> The multiflow trace scheduling compiler. </title> <journal> The Journal of Supercomputing, </journal> <volume> 7(1-2):51-142, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: A cycle-accurate simulator of the M-Machine has been completed and is being used for software development. M-Machine software is being designed and implemented jointly with the Scalable Concurrent Programming group at Caltech. The Multiflow compiler <ref> [17] </ref> is being ported to the M-Machine to generate long instructions spanning multiple clusters. It is currently able to generate code for a single cluster. A prototype runtime system consisting of primitive message and event handlers has also been implemented.
Reference: [18] <author> Carver A. Mead and Lynn A. Conway. </author> <title> Introduction to VLSI Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass, </address> <year> 1980. </year>
Reference-contexts: The 12 function units in a single M-Machine node are controlled using a form of Processor Coupling [13] to exploit in 1 The parameter is a normalized, process independent unit of distance equivalent to one half of the gate length <ref> [18] </ref>. For a 0:5m process, is 0:25m. struction level parallelism by executing 12 operations from the same thread, or to exploit thread-level parallelism by executing operations from up to six different threads. The fast internode communication allows collaborating threads to reside on different nodes.
Reference: [19] <author> Michael D. Noakes, Deborah A. Wallach, and William J. Dally. </author> <title> The J-Machine multicomputer: An architectural evaluation. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 224-235, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: Two message priorities are provided: user messages are sent at priority zero, while priority 1 is used for system level message reply, thus avoiding deadlock. Message Address Translation: As described in <ref> [19] </ref>, the explicit management of processor identifiers by application programs is cumbersome and slow. To eliminate this overhead, the map implements a Global Translation Lookaside Buffer (GTLB), backed by a software Global Destination Table (GDT), to hold mappings of virtual address regions to node numbers.
Reference: [20] <author> G. M. Papadopoulos, G. A. Boughton, R. Grainer, and M. J. Beckerle. </author> <title> *T: Integrated building blocks for parallel computing. </title> <booktitle> In Proc. Supercomputing 1993, </booktitle> <pages> pages 624-635. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: Register-mapped network interfaces have been used previously in the J-Machine and iWarp [2], and have been described by *T <ref> [20] </ref> as well as Henry and Joerg [11]. However, none of these systems provide protection for user-level messages. Systems, like the J-Machine, that provide user ac cess to the network interface without atomicity must temporarily disable interrupts to allow the sending process to complete the message.
Reference: [21] <author> G.F. Pfister et al. </author> <title> The IBM research parallel proces sor prototype (RP3): Introduction and architecture. </title> <booktitle> In Proc. International Conference on Parallel Processing, </booktitle> <pages> pages 764-771, </pages> <year> 1985. </year>
Reference-contexts: It also facilitates the implementation of global shared memory. The interleaving performed by the GTLB, although not as versatile as the CRAY T3D address centrifuge or the interleaving of the RP3 <ref> [21] </ref>, provides a means of distributing ranges of the address space across a region of nodes.
Reference: [22] <author> Burton J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> In SPIE Vol. 298 Real-Time Signal Processing IV, </booktitle> <pages> pages 241-248. </pages> <publisher> Denelcor, Inc., Aurora, CO, </publisher> <year> 1981. </year>
Reference-contexts: Using multiple threads to hide memory latencies and pipeline delays has been explored in several different studies and machines. Gupta and Weber explore the use of multiple hardware contexts in multiprocessors [8], but the context switch overhead prevents the masking of pipeline latencies. MASA [9] as well as HEP <ref> [22] </ref> use fine grain multithreading to issue an instruction from a different context on every cycle in order to mask pipeline latencies. However, with the required round-robin scheduling, single thread performance is degraded by the number of pipeline stages.
Reference: [23] <author> R.M. Tomasulo. </author> <title> An efficient algorithm for exploiting multiple arithmetic units. </title> <journal> IBM Journal, </journal> <volume> 11 </volume> <pages> 25-33, </pages> <month> Jan-uary </month> <year> 1967. </year> <month> 13 </month>
Reference-contexts: Superscalar processors execute multiple instructions simultaneously by relying upon run-time scheduling mechanisms to determine data dependencies <ref> [23, 12] </ref>. However, they do not scale well with increasing number of function units because a greater number of register file ports and connections to the function units are required.
References-found: 23

