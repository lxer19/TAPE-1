URL: http://www-robotics.usc.edu/~maja/publications/robolearn97.ps.gz
Refering-URL: http://www-robotics.usc.edu/~maja/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fmichaudf,majag@cs.brandeis.edu,  
Phone: tel.: (617) 736-2738 (617) 736-2708 fax: (617) 736-2741  
Title: BEHAVIOR EVALUATION AND LEARNING FROM AN INTERNAL POINT OF VIEW  
Author: Fran~cois Michaud and Maja J. Mataric 
Web: http://www.cs.brandeis.edu/~agents  
Address: Waltham, MA 02254-9110  
Affiliation: Interaction Lab, Volen Center for Complex Systems Computer Science Department Brandeis University  
Abstract: This paper describes a new approach that makes a robot learn by evaluating its own performance based on the use of its resources. For a behavior-based robot, this means that learning is accomplished from the observation of behavior use over time. The acquired knowledge can then be exploited for future selection of behaviors. When applied to the multi-robot domain, this approach will make the robot find regularities in its interactions with its environment and exploit them efficiently, eventually resulting in specialization within the group. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rodney A. Brooks. </author> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Jour nal of Robotics and Automation, </journal> <volume> RA-2(1):14-23, </volume> <month> March </month> <year> 1986. </year>
Reference-contexts: Learning the optimal solution would then be defined according to the internal point of view of the robot, instead of being evaluated from an external observer. For a behavior-based system <ref> [1] </ref>, resources are the behaviors available for controlling the actions of the robot. Assuming that behaviors are programmed with an initial policy, the objective is to try to find some regularities in the interactions between the robot and the world reflected in the robot's use of behaviors over time. <p> These behaviors define the robot's skills for handling the situations encountered in its environment. In a behavior-based system, behaviors from the repertoire all run in parallel and are usually prioritized using an arbitration mechanism like subsumption <ref> [1] </ref>. However, allowing all behaviors to concurrently take part in the control of the robot might be impossible when the robot has multiple tasks to accomplish and needs to behave differently depending on its changing goals. <p> A learning algorithm for behavior-based systems <ref> [1] </ref> is proposed based on this idea. In this learning algorithm, the use of behaviors over time is represented in a graph, and performance is evaluated according to the purpose of the behaviors used.
Reference: [2] <author> Rodney A. Brooks. </author> <title> Mars: Multiple agency reactivity system. </title> <type> Technical report, </type> <note> IS Robotics, </note> <year> 1996. </year>
Reference-contexts: These robots are equipped with seven sonars and a Fast Track Vision System. They are programmed using MARS (Multiple Agency Reactivity System) <ref> [2] </ref>, a language for programming multiple concurrent processes and behaviors. In the experiments, one Pioneer is placed in a "corral" with another Pioneer and other robots, and its goal is to find blocks and to bring them home.
Reference: [3] <author> Pattie Maes and Rodney A. Brooks. </author> <title> Learning to coordinate behaviors. </title> <booktitle> In Proc. Heigth National Conf. on Artificial Intelligence AAAI, </booktitle> <volume> volume 2, </volume> <pages> pages 796-802, </pages> <year> 1990. </year>
Reference-contexts: ways to improve the ability of a robot to behave in its environment, such as by acquiring knowledge about the world (e.g., using a topological representation) [6, 9], by acquiring special skills (behaviors or control policies) [4, 10] or by learning when to use such skills according to sensed conditions <ref> [3, 7] </ref>. Learning in the mobile robot domain is challenging. First, there are difficulties intrinsic to the robot, caused by incomplete and imprecise perception of the environment and by limited processing and memory capabilities. <p> behaviors can also be pre-programmed by the designer according to what can be anticipated about the tasks the robot needs to accomplish. 1 and maybe more Behaviors can be activated according to sensed conditions from the world, and it has been shown that these can be learned through reinforcement learning <ref> [3, 7] </ref>. But observing behavior use over time can also be an interesting source of information for behavior activation [9], especially in the multi-robot domain where the dynamics with the environment are very important.
Reference: [4] <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55 </volume> <pages> 311-365, </pages> <year> 1992. </year>
Reference-contexts: 1 INTRODUCTION Learning can be used in many ways to improve the ability of a robot to behave in its environment, such as by acquiring knowledge about the world (e.g., using a topological representation) [6, 9], by acquiring special skills (behaviors or control policies) <ref> [4, 10] </ref> or by learning when to use such skills according to sensed conditions [3, 7]. Learning in the mobile robot domain is challenging. First, there are difficulties intrinsic to the robot, caused by incomplete and imprecise perception of the environment and by limited processing and memory capabilities.
Reference: [5] <author> Sridhar Mahadevan and Leslie Pack Kaelbling. </author> <title> The NSF workshop on reinforcement learning: summary and observations. </title> <journal> AI Magazine, </journal> <month> De-cember </month> <year> 1996. </year>
Reference-contexts: This project also tackles important problems of incomplete and imperfect perception, learning using prior knowledge and multi-agent learning <ref> [5] </ref>. We are currently in the process of testing this learning algorithm in a multi-robot foraging task, and we hope to get a better understanding of its effects on group dynamics.
Reference: [6] <editor> Maja J. Mataric. </editor> <title> Integration of representation into goal-driven behavior-based robots. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 8(3) </volume> <pages> 304-312, </pages> <year> 1992. </year>
Reference-contexts: 1 INTRODUCTION Learning can be used in many ways to improve the ability of a robot to behave in its environment, such as by acquiring knowledge about the world (e.g., using a topological representation) <ref> [6, 9] </ref>, by acquiring special skills (behaviors or control policies) [4, 10] or by learning when to use such skills according to sensed conditions [3, 7]. Learning in the mobile robot domain is challenging.
Reference: [7] <editor> Maja J. Mataric. </editor> <title> Reinforcement learning in the multi-robot domain. </title> <booktitle> Autonomous Robots, </booktitle> <volume> 4(1), </volume> <month> January </month> <year> 1997. </year>
Reference-contexts: ways to improve the ability of a robot to behave in its environment, such as by acquiring knowledge about the world (e.g., using a topological representation) [6, 9], by acquiring special skills (behaviors or control policies) [4, 10] or by learning when to use such skills according to sensed conditions <ref> [3, 7] </ref>. Learning in the mobile robot domain is challenging. First, there are difficulties intrinsic to the robot, caused by incomplete and imprecise perception of the environment and by limited processing and memory capabilities. <p> A possible solution is to dynamically activate the appropriate behaviors according to the robot's goal <ref> [7, 9] </ref>. In that regard, knowing when to use a behavior is as important 1 as the behavior itself. <p> behaviors can also be pre-programmed by the designer according to what can be anticipated about the tasks the robot needs to accomplish. 1 and maybe more Behaviors can be activated according to sensed conditions from the world, and it has been shown that these can be learned through reinforcement learning <ref> [3, 7] </ref>. But observing behavior use over time can also be an interesting source of information for behavior activation [9], especially in the multi-robot domain where the dynamics with the environment are very important.
Reference: [8] <author> Andrew Kachites McCallum. </author> <title> Learning to use selective attention and short-term memory in sequential tasks. </title> <editor> In Pattie Maes, Maja J. Mataric, Jean-Arcady Meyer, Jordan Pollack, and Stew-art W. Wilson, editors, </editor> <booktitle> From Animals to Ani-mats 4: International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 315-324. </pages> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: The work of McCallum <ref> [8] </ref> is most related to ours in the notion of learning from history. McCallum uses a tree representation in a reinforcement framework to learn the appropriate actions to take according to the relevant features and the memory of those features from previous states.
Reference: [9] <author> Fran~cois Michaud, Gerard Lachiver, and Chon Tam Le Dinh. </author> <title> A new control architecture combining reactivity, deliberation and motivation for situated autonomous agent. </title> <editor> In Pattie Maes, Maja J. Mataric, Jean-Arcady Meyer, Jordan Pollack, and Stewart W. Wilson, editors, </editor> <title> From Animals to Animat 4: </title> <booktitle> International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 245-254. </pages> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 INTRODUCTION Learning can be used in many ways to improve the ability of a robot to behave in its environment, such as by acquiring knowledge about the world (e.g., using a topological representation) <ref> [6, 9] </ref>, by acquiring special skills (behaviors or control policies) [4, 10] or by learning when to use such skills according to sensed conditions [3, 7]. Learning in the mobile robot domain is challenging. <p> A possible solution is to dynamically activate the appropriate behaviors according to the robot's goal <ref> [7, 9] </ref>. In that regard, knowing when to use a behavior is as important 1 as the behavior itself. <p> But observing behavior use over time can also be an interesting source of information for behavior activation <ref> [9] </ref>, especially in the multi-robot domain where the dynamics with the environment are very important. Using this information, we want the robot to learn to activate behaviors based on the history of their use.
Reference: [10] <author> Ulrich Nehmzow, Tim Smithers, and Brendan McGonigle. </author> <title> Increasing behavioural repertoire in a mobile robot. </title> <editor> In Jean-Arcady Meyer, Her-bert L. Roitblat, and Stewart W. Wilson, editors, </editor> <booktitle> From Animals to Animats 2: International Conference on Simulation of Adaptive Behaviors, </booktitle> <pages> pages 291-297. </pages> <publisher> The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: 1 INTRODUCTION Learning can be used in many ways to improve the ability of a robot to behave in its environment, such as by acquiring knowledge about the world (e.g., using a topological representation) [6, 9], by acquiring special skills (behaviors or control policies) <ref> [4, 10] </ref> or by learning when to use such skills according to sensed conditions [3, 7]. Learning in the mobile robot domain is challenging. First, there are difficulties intrinsic to the robot, caused by incomplete and imprecise perception of the environment and by limited processing and memory capabilities.
Reference: [11] <author> Ashwin Ram and Juan Carlos Santamaria. </author> <title> Mul-tistrategy learning in reactive control systems for autonomous robotic navigation. </title> <journal> Informatica, </journal> <volume> 17(4) </volume> <pages> 347-369, </pages> <year> 1993. </year>
Reference-contexts: Related work includes Ram and Santamaria <ref> [11] </ref> who consider history from inputs and from behavior activation for setting future behavior activation. A case-based algorithm is used to retrieve similar cases, adapt the activation of behaviors, learn new associations or adapt existing cases. A reward signal is used to try to maximize consistency in "useful" behaviors.
References-found: 11

