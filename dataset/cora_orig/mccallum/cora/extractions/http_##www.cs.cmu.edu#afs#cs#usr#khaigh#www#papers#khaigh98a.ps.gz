URL: http://www.cs.cmu.edu/afs/cs/usr/khaigh/www/papers/khaigh98a.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs/usr/khaigh/www/papers/khaigh98a.abstract.html
Root-URL: http://www.cs.cmu.edu
Email: khaigh@cs.cmu.edu  mmv@cs.cmu.edu  
Title: Learning Situation-Dependent Costs: Improving Planning from Probabilistic Robot Execution  
Author: Karen Zita Haigh Manuela M. Veloso 
Note: To appear in Agents '98  
Address: Pittsburgh PA 15213-3891  
Affiliation: Computer Science Department, Carnegie Mellon University,  
Web: http://www.cs.cmu.edu/~khaigh  http://www.cs.cmu.edu/~mmv  
Abstract: Real world robot tasks are so complex that it is hard to hand-tune all of the domain knowledge, especially to model the dynamics of the environment. Several research efforts focus on applying machine learning to map learning, sensor/action mapping, and vision. The work presented in this paper explores machine learning techniques for robot planning. The goal is to use real robotic navigational execution as a data source for learning. Our system collects execution traces, and extracts relevant information to improve the efficiency of generated plans. In this article, we present the representation of the path planner and the navigation modules, and describe the execution trace. We show how training data is extracted from the execution trace. We introduce the concept of situation-dependent costs, where situational features can be attached to the costs used by the path planner. In this way, the planner can generate paths that are appropriate for a given situation. We present experimental results from a simulated, controlled environment as well as from data collected from the actual robot. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. C. Baird. </author> <title> Residual algorithms: Reinforcement learning with function approximation. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference (ICML95), </booktitle> <pages> pages 30-37, </pages> <year> 1995. </year>
Reference-contexts: We would like a multi-agent planner to learn, for example, that one of its agents has a weaker arm and can't pick up the heaviest packages. Once these problems have 1 Recently, several research have been exploring techniques for allowing generalization <ref> [1, 5, 15] </ref>. Experimentally, these algorithms seem to produce reasonable policies. However, they may be extremely computationally intense. 1. Create plan 2. Execute; record execution trace including features F 3. Identify events E in execution trace 4. Learn mapping: F fi E ! C 5.
Reference: [2] <author> C. Baroglio, A. Giordana, M. Kaiser, M. Nuttin, and R. Pi--ola. </author> <title> Learning controllers for industrial robots. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 221-249, </pages> <year> 1996. </year>
Reference-contexts: It needs to learn. Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. [12, 13, 25]), or learning operational parameters for better actuator control (e.g. <ref> [2, 4, 19] </ref>). Instead of improving low-level actuator control, our work fo-cusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. [11, 14, 21, 18, 24]).
Reference: [3] <author> R. A. Becker, J. M. Chambers, and A. R. Wilks. </author> <title> The New S Language. </title> <address> (Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks/Cole), </publisher> <year> 1988. </year> <note> Code available from http://www.- mathsoft.com/splus/. </note>
Reference-contexts: Other learning mechanisms may be appropriate in different robot architectures with different data representations. We selected an off-the-shelf package, namely S-PLUS <ref> [3] </ref>, as the regression tree implementation. A regression tree is fitted for each arc using binary recursive partitioning where the data is successively split until data is too sparse or nodes are pure (below a preset deviance). Splits are selected to maximize the reduction in deviance of the node.
Reference: [4] <author> S. W. Bennett and G. F. DeJong. </author> <title> Real-world robotics: Learning to plan for robust execution. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 121-161, </pages> <year> 1996. </year>
Reference-contexts: It needs to learn. Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. [12, 13, 25]), or learning operational parameters for better actuator control (e.g. <ref> [2, 4, 19] </ref>). Instead of improving low-level actuator control, our work fo-cusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. [11, 14, 21, 18, 24]).
Reference: [5] <author> J. A. Boyan and A. W. Moore. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 369-76, </pages> <year> 1995. </year>
Reference-contexts: We would like a multi-agent planner to learn, for example, that one of its agents has a weaker arm and can't pick up the heaviest packages. Once these problems have 1 Recently, several research have been exploring techniques for allowing generalization <ref> [1, 5, 15] </ref>. Experimentally, these algorithms seem to produce reasonable policies. However, they may be extremely computationally intense. 1. Create plan 2. Execute; record execution trace including features F 3. Identify events E in execution trace 4. Learn mapping: F fi E ! C 5.
Reference: [6] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <address> (Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks/Cole), </publisher> <year> 1984. </year>
Reference-contexts: The input to the algorithm is the events matrix described in Section 3.3. The desired output is situation-dependent knowledge in a form that can be used by the planner. We selected regression trees <ref> [6] </ref> as our learning mechanism because the data often contains disjunctive descriptions, the data may contain irrelevant features, the data might be sparse, especially for certain features, and the learned costs are continuous values.
Reference: [7] <author> J. M. Chambers and T. Hastie. </author> <title> Statistical models in S. </title> <address> (Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks/Cole), </publisher> <year> 1992. </year>
Reference-contexts: Splits are selected to maximize the reduction in deviance of the node. Deviance of a node is calculated as D = P (y i ) 2 , for all examples i and predicted values y i within the node. Chambers & Hastie <ref> [7] </ref> discuss the method in more detail. We prune the tree using 10-fold random cross validation, in which a tree is built using 90% of the data, and then the remaining 10% of the data is used to test the tree.
Reference: [8] <author> R. Goodwin. </author> <title> Meta-Level Control for Decision-Theoretic Planners. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1996. </year> <note> Available as Technical Report CMU-CS-96-186. </note>
Reference-contexts: Learning opportunities are extracted from the execution traces created by by the navigation module. Figure 2 shows how our algorithm fits into the framework of the Xavier architecture. The path planner uses a decision-theoretic A* algorithm that operates on a topological map with metric information <ref> [8] </ref>. Navigation is done using Partially Observable Markov Decision Process Models (POMDPs) [23]. Our learning algorithm affects the arc costs of the topological map so that the planner will select plans with a higher efficiency.
Reference: [9] <author> K. Z. Haigh. </author> <title> Learning Situation-Dependent Planning Knowledge from Uncertain Robot Execution Data. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> February </month> <year> 1998. </year> <note> Available as technical report CMU-CS-98-108. </note>
Reference-contexts: This paper presents the continuation of the project, in which learning has been incorporated. Rogue processes uncertain navigation data to create improved domain models for its planners, successfully abstracting numeric sensor information into symbolic planner information. The complete system, in greater detail, can be found elsewhere <ref> [9] </ref>. Our approach relies on examining the execution traces of the robot to identify situations in which the planner's be-haviour needs to change. <p> In this situation, since room states have high-probability self-transitions, Viterbi's algorithm will very often never correct itself, instead claiming that the robot's most likely path was only within the room. We developed an improvement to Viterbi's algorithm, called Multi/Markov Viterbi <ref> [9] </ref>, which heuristically improves the estimate of the path by selecting using a different generating state: it generates the sequence from the most likely Markov state (). <p> This fact complicates the reconstruction of the arc path because a single Markov sequence may map to multiple arc sequences. Selecting the correct one is a challenging problem that we address with a greedy heuristic, based on expectation times <ref> [9] </ref>. a) Markov Representation b) Planner Arc Representation 3.3 Creating a Uniform Output Once arc traversal events have been identified from the execution trace, updated costs need to be calculated. The cost evaluation function, C, yields an updated arc traversal weight. <p> Additional features from the execution trace can be trivially added; this matrix was recorded for the experiments described in Section 5, while sonar readings and other features were added for experiments involving the task planner <ref> [9] </ref>. 4 Learning Algorithm In this section we present the learning mechanism we use to create the mapping from situation features (F ) and events (E) to costs (C). The input to the algorithm is the events matrix described in Section 3.3. <p> Planners can benefit from understanding the patterns of the environment that affect task achievability. This situation-dependent knowledge can be incorporated into the planning effort so that tasks can be achieved with greater reliability and efficiency. Haigh <ref> [9] </ref> describe an implementation of the approach for the robot's task planner. Situation-dependent features are an effective way to capture the changing nature of a real-world environment.
Reference: [10] <author> K. Z. Haigh and M. M. Veloso. </author> <title> High-level planning and low-level execution: Towards a complete robotic agent. </title> <booktitle> In Proceedings of the First International Conference on Autonomous Agents, </booktitle> <pages> pages 363-370, </pages> <year> 1997. </year>
Reference-contexts: We have been developing a robot architecture, Rogue, which aims at equipping a real robot with the ability to learn from its own execution experiences <ref> [10] </ref>. This paper presents the continuation of the project, in which learning has been incorporated. Rogue processes uncertain navigation data to create improved domain models for its planners, successfully abstracting numeric sensor information into symbolic planner information. The complete system, in greater detail, can be found elsewhere [9].
Reference: [11] <author> L. P. Kaelbling, M. L. Littman, and A. W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: Instead of improving low-level actuator control, our work fo-cusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. <ref> [11, 14, 21, 18, 24] </ref>). Reinforcement Learning techniques [11] learn the value of being in a particular state, which is then used to select the optimal action. This approach can be viewed as learning the integral of action costs. <p> Instead of improving low-level actuator control, our work fo-cusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. [11, 14, 21, 18, 24]). Reinforcement Learning techniques <ref> [11] </ref> learn the value of being in a particular state, which is then used to select the optimal action. This approach can be viewed as learning the integral of action costs.
Reference: [12] <author> S. Koenig and R. G. Simmons. </author> <title> Passive distance learning for robot navigation. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference (ICML96), </booktitle> <pages> pages 266-274, </pages> <year> 1996. </year>
Reference-contexts: To be truly autonomous, the robot needs to be able to use accumulated experience and feedback about its performance to improve its behaviour. It needs to learn. Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. <ref> [12, 13, 25] </ref>), or learning operational parameters for better actuator control (e.g. [2, 4, 19]). Instead of improving low-level actuator control, our work fo-cusses at the planning stages of the system.
Reference: [13] <author> D. Kortenkamp and T. Weymouth. </author> <title> Topological mapping for mobile robots using a combination of sonar and vision sensing. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> pages 979-984, </pages> <year> 1994. </year>
Reference-contexts: To be truly autonomous, the robot needs to be able to use accumulated experience and feedback about its performance to improve its behaviour. It needs to learn. Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. <ref> [12, 13, 25] </ref>), or learning operational parameters for better actuator control (e.g. [2, 4, 19]). Instead of improving low-level actuator control, our work fo-cusses at the planning stages of the system.
Reference: [14] <author> J. Lindner, R. R. Murphy, and E. Nitz. </author> <title> Learning the expected utility of sensors and algorithms. </title> <booktitle> In IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, </booktitle> <pages> pages 583-590, </pages> <year> 1994. </year>
Reference-contexts: Instead of improving low-level actuator control, our work fo-cusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. <ref> [11, 14, 21, 18, 24] </ref>). Reinforcement Learning techniques [11] learn the value of being in a particular state, which is then used to select the optimal action. This approach can be viewed as learning the integral of action costs. <p> Moreover, Reinforcement Learning techniques typically learn a universal action model for a single goal. Our situation-dependent learning approach learns knowledge that will be transferrable to other similar tasks. IMPROV [18] learns action descriptions, but its performance degrades dramatically with environmental noise. Clementine <ref> [14] </ref> and CSL [24] both learn sensor utilities, including which sensor to use for what information. LIVE [21] learns a model of the environment, as well as the costs of applying actions in that environment.
Reference: [15] <author> A. K. McCallum. </author> <title> Reinforcement Learning with Selective Perception and Hidden State. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, Rochester, </institution> <address> NY, </address> <year> 1995. </year>
Reference-contexts: We would like a multi-agent planner to learn, for example, that one of its agents has a weaker arm and can't pick up the heaviest packages. Once these problems have 1 Recently, several research have been exploring techniques for allowing generalization <ref> [1, 5, 15] </ref>. Experimentally, these algorithms seem to produce reasonable policies. However, they may be extremely computationally intense. 1. Create plan 2. Execute; record execution trace including features F 3. Identify events E in execution trace 4. Learn mapping: F fi E ! C 5.
Reference: [16] <author> T. M. Mitchell. </author> <title> Machine Learning. </title> <address> (New York, NY: McGraw Hill), </address> <year> 1997. </year>
Reference-contexts: Bayesian learning would not successfully handle disjunctive functions, k-nearest neigh-bour algorithms would not handle irrelevant features well, neural networks would not generalize well for sparse data exposition world presented in Section 5.1). and standard decision trees do not handle continuous valued output particularly well <ref> [16] </ref>. Other learning mechanisms may be appropriate in different robot architectures with different data representations. We selected an off-the-shelf package, namely S-PLUS [3], as the regression tree implementation.
Reference: [17] <author> J. O'Sullivan, K. Z. Haigh, and G. D. Armstrong. </author> <type> Xavier. </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> April </month> <year> 1997. </year> <note> Manual, Version 0.3, unpublished internal report. Available via http://www.cs.cmu.edu/~Xavier/. </note>
Reference-contexts: Most of these "problems" model the actual behaviour of the robot, allowing code developed on the simulator to run successfully on the robot with no modification <ref> [17] </ref>. The simulator allows us to tightly control the experiments to ensure that the learning algorithm is indeed learning appropriate situation-dependent costs. an exposition of the variety one might see at a conference.
Reference: [18] <author> D. J. Pearson. </author> <title> Learning Procedural Planning Knowledge in Complex Environments. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of Michi-gan, </institution> <address> Ann Arbor, MI, </address> <year> 1996. </year> <note> Available as Technical Report CSE-TR-309-96. </note>
Reference-contexts: Instead of improving low-level actuator control, our work fo-cusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. <ref> [11, 14, 21, 18, 24] </ref>). Reinforcement Learning techniques [11] learn the value of being in a particular state, which is then used to select the optimal action. This approach can be viewed as learning the integral of action costs. <p> Moreover, Reinforcement Learning techniques typically learn a universal action model for a single goal. Our situation-dependent learning approach learns knowledge that will be transferrable to other similar tasks. IMPROV <ref> [18] </ref> learns action descriptions, but its performance degrades dramatically with environmental noise. Clementine [14] and CSL [24] both learn sensor utilities, including which sensor to use for what information. LIVE [21] learns a model of the environment, as well as the costs of applying actions in that environment.
Reference: [19] <author> D. A. Pomerleau. </author> <title> Neural network perception for mobile robot guidance. </title> <address> (Dordrecht, Netherlands: </address> <publisher> Kluwer Academic), </publisher> <year> 1993. </year>
Reference-contexts: It needs to learn. Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. [12, 13, 25]), or learning operational parameters for better actuator control (e.g. <ref> [2, 4, 19] </ref>). Instead of improving low-level actuator control, our work fo-cusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. [11, 14, 21, 18, 24]).
Reference: [20] <author> L. R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: However, the action sequence stored in the trace, together with the probability distribution, can be used to calculate the most likely sequence of Markov states that the robot passed through. The algorithm to calculate this sequence is known as Viterbi's algorithm <ref> [20] </ref>. By starting at the most likely Markov state at a given time, and then using the transition probabilities between Markov states, it estimates which state the robot was in during the previous time step. Recurs-ing backwards through time, it can calculate the complete sequence.
Reference: [21] <author> W.-M. Shen. </author> <title> Autonomous Learning from the Environment. </title> <address> (New York, NY: </address> <publisher> Computer Science Press), </publisher> <year> 1994. </year>
Reference-contexts: Instead of improving low-level actuator control, our work fo-cusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. <ref> [11, 14, 21, 18, 24] </ref>). Reinforcement Learning techniques [11] learn the value of being in a particular state, which is then used to select the optimal action. This approach can be viewed as learning the integral of action costs. <p> Our situation-dependent learning approach learns knowledge that will be transferrable to other similar tasks. IMPROV [18] learns action descriptions, but its performance degrades dramatically with environmental noise. Clementine [14] and CSL [24] both learn sensor utilities, including which sensor to use for what information. LIVE <ref> [21] </ref> learns a model of the environment, as well as the costs of applying actions in that environment. In some situations, it is enough to learn that a particular action has a certain average probability or cost. However, there are times when actions may have different costs under different situations.
Reference: [22] <author> R. Simmons, R. Goodwin, K. Z. Haigh, S. Koenig, and J. O'Sullivan. </author> <title> A layered architecture for office delivery robots. </title> <booktitle> In Proceedings of the First International Conference on Autonomous Agents, </booktitle> <pages> pages 245-252, </pages> <year> 1997. </year>
Reference-contexts: These steps are summarized in Table 1. Learning occurs incrementally and off-line; each time a plan is executed, new data is collected and added to previous data, and then all data is used for creating a new set of situation-dependent rules. Rogue uses the Xavier robot <ref> [22] </ref> as its learning platform (see Figure 1). Knowledge in the path planner is represented as a topological map of the environment in which the robot navigates. The map is a graph with nodes and arcs representing office rooms, corridors, doors and lobbies. <p> In Section 5 we present some experimental results. Finally, in Section 6 we summarize the main points of the paper. 2 Architecture & Representation Xavier is a mobile robot being developed at CMU <ref> [22] </ref>. It is built on an RWI B24 base and includes bump sensors, a laser light striper, sonars, a color camera and a speech board.
Reference: [23] <author> R. Simmons and S. Koenig. </author> <title> Probabilistic robot navigation in partially observable environments. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 1080-1087, </pages> <year> 1995. </year>
Reference-contexts: Figure 2 shows how our algorithm fits into the framework of the Xavier architecture. The path planner uses a decision-theoretic A* algorithm that operates on a topological map with metric information [8]. Navigation is done using Partially Observable Markov Decision Process Models (POMDPs) <ref> [23] </ref>. Our learning algorithm affects the arc costs of the topological map so that the planner will select plans with a higher efficiency. The challenge is to create variable costs depending on high-level features and to extract this information automatically from the robot's execution data. <p> In our system, we use parallel for the navigation module. Each transition corresponds to 1 metre, and hence this corridor is represented as being 2, 3 or 4 metres long. Only forward transitions are marked. Reproduced from Simmons & Koenig <ref> [23] </ref>. algorithm. Although s2 has a greater probability than s3, Viterbi's algorithm selects s3 as the sequence-generating state. ( is the state probability, while ffi is the sequence probability.) Markov chains, where each corresponds to one of the possible lengths of the edge [23]. <p> Reproduced from Simmons & Koenig <ref> [23] </ref>. algorithm. Although s2 has a greater probability than s3, Viterbi's algorithm selects s3 as the sequence-generating state. ( is the state probability, while ffi is the sequence probability.) Markov chains, where each corresponds to one of the possible lengths of the edge [23]. Figure 3 illustrates an example for a corridor that may be two, three or four metres long.
Reference: [24] <author> M. Tan. </author> <title> Cost-sensitive robot learning. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1991. </year> <note> Available as Technical Report CMU-CS-91-134. </note>
Reference-contexts: Instead of improving low-level actuator control, our work fo-cusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. <ref> [11, 14, 21, 18, 24] </ref>). Reinforcement Learning techniques [11] learn the value of being in a particular state, which is then used to select the optimal action. This approach can be viewed as learning the integral of action costs. <p> Moreover, Reinforcement Learning techniques typically learn a universal action model for a single goal. Our situation-dependent learning approach learns knowledge that will be transferrable to other similar tasks. IMPROV [18] learns action descriptions, but its performance degrades dramatically with environmental noise. Clementine [14] and CSL <ref> [24] </ref> both learn sensor utilities, including which sensor to use for what information. LIVE [21] learns a model of the environment, as well as the costs of applying actions in that environment.
Reference: [25] <author> S. Thrun. </author> <title> A Bayesian approach to landmark discovery and active perception for mobile robot navigation. </title> <type> Technical Report CMU-CS-96-122, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1996. </year> <title> generated 31 July 1997. The histogram below the graph indicates volume of training data; most data was collected between 1:30pm and 2:45pm. volume of training data; most data was collected between 1pm and 6pm. </title>
Reference-contexts: To be truly autonomous, the robot needs to be able to use accumulated experience and feedback about its performance to improve its behaviour. It needs to learn. Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. <ref> [12, 13, 25] </ref>), or learning operational parameters for better actuator control (e.g. [2, 4, 19]). Instead of improving low-level actuator control, our work fo-cusses at the planning stages of the system.
References-found: 25

