URL: http://www.cs.berkeley.edu/~arvindk/papers/connect-dimacs95.ps
Refering-URL: http://www.cs.berkeley.edu/~arvindk/
Root-URL: 
Title: Connected Components on Distributed Memory Machines  
Author: Arvind Krishnamurthy, Steven S. Lumetta, David E. Culler, and Katherine Yelick 
Date: 00, 19xx  
Note: DIMACS Series in Discrete Mathematics and Theoretical Computer Science Volume  
Abstract: The efforts of the theory community to develop efficient PRAM algorithms often receive little attention from application programmers. Although there are PRAM algorithm implementations that perform reasonably on shared memory machines, they often perform poorly on distributed memory machines, where the cost of remote memory accesses is relatively high. We present a hybrid approach to solving the connected components problem, whereby a PRAM algorithm is merged with a sequential algorithm and then optimized to create an efficient distributed memory implementation. The sequential algorithm handles local work on each processor, and the PRAM algorithm handles interactions between processors. Our hybrid algorithm uses the Shiloach-Vishkin CRCW PRAM algorithm on a partition of the graph distributed over the processors and sequential breadth-first search within each local subgraph. The implementation uses the Split-C language developed at Berkeley, which provides a global address space and allows us to easily manipulate the distributed graph data structure. We present our first version, then provide a detailed account of the optimizations used to create the final version. For graphs from real-world problems, we obtain speedups on the order of 20 on a 32-processor CM-5 and 238 on a 512-processor CM-5. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. H. Arpaci, D. E. Culler, A. Krishnamurthy, S. Steinberg, K. Yelick, </author> <title> "Empirical Evaluation of the Cray T3D: A Compiler Perspective," </title> <booktitle> Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: Iterate over the following steps until done: 4 Implementations of the language exist on a variety of machines including the IBM SP-2, the Intel Paragon, the Cray T3D, and the Meiko CS-2 <ref> [1, 8, 10, 11] </ref>. CONNECTED COMPONENTS ON DISTRIBUTED MEMORY MACHINES 7 before entering the global phase, we reduce the amount of work required for each iteration of that phase. a. Termination Check. Move star components with no remaining remote edges to a finished component list.
Reference: [2] <author> B. Awerbuch, Y. Shiloach, </author> <title> "New connectivity and MSF algorithms for Ultracomputer and PRAM," </title> <booktitle> International Conference on Parallel Processing, </booktitle> <year> 1983, </year> <pages> pp. 175-179. </pages>
Reference-contexts: The solutions have running times linear in the number of edges and vertices in the graph and are easy to implement. Many efficient parallel solutions <ref> [2, 6, 12] </ref> have been devised, but these solutions are often complex and difficult to implement. Our implementation is a hybrid of a sequential search on the subgraph local to each processor and a variant of the Shiloach-Vishkin PRAM algorithm [12] on the global collection of subgraphs.
Reference: [3] <author> D. Bader and J. Jaja, </author> <title> "Parallel Algorithms for Image Histogramming and Connected Components with an Experimental Study," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> June </month> <year> 1996. </year> <title> CONNECTED COMPONENTS ON DISTRIBUTED MEMORY MACHINES 21 </title>
Reference-contexts: However, their implementation is optimized to labeling a 2D graph, which allows a compact representation of the edge list. As a result, they are able to greatly reduce the storage requirements for their algorithm. Bader and Jaja <ref> [3] </ref> adopt a similar strategy for identifying components of 2D images such that each component is a maximal collection of adjacent pixels with the same intensity. Our algorithm is more general-purpose since the input graph can have arbitrary connectivity, and we still obtain similar performance results.
Reference: [4] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, K. Yelick, </author> <title> "Parallel Programming in Split-C," </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <address> Portland, Oregon, </address> <month> November </month> <year> 1993, </year> <pages> pp. 262-273. </pages>
Reference-contexts: The two algorithms are merged, then the result is optimized. In this paper, we present a hybrid algorithm for finding the connected components of a graph on a distributed memory machine. We implemented and optimized the algorithm on a CM-5 using the Split-C language developed at Berkeley <ref> [4] </ref>. 1 Each node in an instance of Greiner's Tertiary graph randomly selects three other nodes as neighbors, resulting in an average degree of six and a graph that has only one connected component with high probability. <p> In the global phases, the algorithm must address the issues of efficient remote data access and synchronization between processors. The global phases are hence more difficult to program. Fortunately, we can make use of the Split-C language <ref> [4] </ref> to simplify the task. Split-C provides the abstraction of a global address space on a distributed memory 6 A. KRISHNAMURTHY, S. S. LUMETTA, D. E. CULLER, AND K.
Reference: [5] <author> M. Flanigan, P. Tamayo, </author> <title> "A Parallel Cluster Labelling Method for Monte Carlo Dynamics," </title> <journal> International Journal of Modern Physics C, </journal> <volume> Vol. 3, No. 6, </volume> <year> 1992, </year> <pages> 1235-1249. </pages>
Reference-contexts: This is highly encouraging considering that our results were obtained on a 32-processor CM-5 without vector units, which is a much cheaper machine. Another piece of related work is the implementation of the parallel clustering algorithm by Flanigan and Tamayo <ref> [5] </ref> on a CM-5 using CMMD, which is a message passing library provided by Thinking Machines Corporation. As mentioned earlier, the clustering algorithm requires a connected components labeling on a 2D mesh.
Reference: [6] <author> H. Gazit, </author> <title> "An Optimal Randomized Parallel Algorithm for Finding Connected Components in a Graph," </title> <journal> SIAM Journal of Computing 20(6), </journal> <month> December </month> <year> 1991. </year>
Reference-contexts: The solutions have running times linear in the number of edges and vertices in the graph and are easy to implement. Many efficient parallel solutions <ref> [2, 6, 12] </ref> have been devised, but these solutions are often complex and difficult to implement. Our implementation is a hybrid of a sequential search on the subgraph local to each processor and a variant of the Shiloach-Vishkin PRAM algorithm [12] on the global collection of subgraphs.
Reference: [7] <author> J. Greiner, </author> <title> "A Comparison of Parallel Algorithms for Connected Components," </title> <booktitle> to appear in the Symposium on Parallel Algorithms and Architectures 1994. </booktitle>
Reference-contexts: The graph, denoted AD3 for "average degree three," is generated by having each node pick zero to three other random nodes as neighbors. AD3 is a variant of the Tertiary graph used by Greiner <ref> [7] </ref> for bench-marking connected components algorithms. 1 Graphs corresponding to physical systems usually exhibit locality in their structure. However, the AD3 graph exhibits almost no locality, and could therefore be viewed as an extreme input to our algorithm. <p> However, the AD3 graph exhibits almost no locality, and could therefore be viewed as an extreme input to our algorithm. Previous parallel implementations of connected components algorithms have focused primarily on shared-memory machines <ref> [7] </ref>. For distributed memory machines, a straightforward implementation of a PRAM algorithm is generally of little use because of the high cost of remote accesses and the frequency of such accesses in most PRAM algorithms. <p> We then discuss our measurement methodology. 5.1. Graph construction. As our results depend fairly heavily upon the types of graphs studied, we first describe those graphs. We used five separate types of graphs; four are drawn directly from the work of Greiner <ref> [7] </ref>, and the fifth is a modified form of another graph used in that work. The first two graphs are built on a two-dimensional toroidal mesh. Each edge in the mesh is present with some fixed probability, either 40% or 60% in our measurements. <p> Comparison with Earlier Work Though a lot of research has been done in proposing theoretically optimal algorithms for finding connected components of a graph, not much work has been done in implementing these algorithms efficiently on parallel machines. Greiner <ref> [7] </ref> implemented the connected components algorithm on the Cray C-90 and on the 20 A. KRISHNAMURTHY, S. S. LUMETTA, D. E. CULLER, AND K. YELICK Connection Machine 2. However, the C-90 is a shared bus multiprocessor system, and the CM-2 is a SIMD machine.
Reference: [8] <author> L. T. Liu, D. E. Culler, </author> <title> "Evaluation of the Intel Paragon on Active Message Communication," </title> <booktitle> Proceedings of the Intel Supercomputer Users Group Conference, </booktitle> <year> 1995. </year>
Reference-contexts: Iterate over the following steps until done: 4 Implementations of the language exist on a variety of machines including the IBM SP-2, the Intel Paragon, the Cray T3D, and the Meiko CS-2 <ref> [1, 8, 10, 11] </ref>. CONNECTED COMPONENTS ON DISTRIBUTED MEMORY MACHINES 7 before entering the global phase, we reduce the amount of work required for each iteration of that phase. a. Termination Check. Move star components with no remaining remote edges to a finished component list.
Reference: [9] <author> S. S. Lumetta, A. Krishnamurthy, D. E. Culler, </author> <title> "Towards Modeling the Performance of a Fast Connected Components Algorithm on Parallel Machines," </title> <booktitle> Proceedings of Supercomputing '95, </booktitle> <address> San Diego, California, </address> <month> December </month> <year> 1995, </year> <note> available at http://www.supercomp.org.sc95/proceedings/465 SLUM/SC95.HTM. </note>
Reference-contexts: The algorithm terminates when no two trees in the forest share an edge and all trees in the forest are of height one. 2 A separate paper <ref> [9] </ref> presents more detailed results for the algorithm on several different platforms and demonstrates the best connected components performance seen to date. 4 A. KRISHNAMURTHY, S. S. LUMETTA, D. E. CULLER, AND K. YELICK is replaced with the vertex' grandparent. <p> The resulting implementation is very efficient and obtains speedups on the order of 20 on a 32-processor CM-5 and 238 on a 256-processor CM-5. In related work <ref> [9] </ref>, we demonstrate that our algorithm is the fastest in the world.
Reference: [10] <author> S. Luna, </author> <title> "Implementing an Efficient Portable Global Memory Layer on Distributed Memory Multiprocessors," </title> <editor> U. </editor> <address> C. </address> <institution> Berkeley Technical Report #CSD-94-810, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Iterate over the following steps until done: 4 Implementations of the language exist on a variety of machines including the IBM SP-2, the Intel Paragon, the Cray T3D, and the Meiko CS-2 <ref> [1, 8, 10, 11] </ref>. CONNECTED COMPONENTS ON DISTRIBUTED MEMORY MACHINES 7 before entering the global phase, we reduce the amount of work required for each iteration of that phase. a. Termination Check. Move star components with no remaining remote edges to a finished component list.
Reference: [11] <author> K. E. Schauser, C. J. Scheiman, </author> <title> "Experience with Active Messages on the Meiko CS-2," </title> <booktitle> Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: Iterate over the following steps until done: 4 Implementations of the language exist on a variety of machines including the IBM SP-2, the Intel Paragon, the Cray T3D, and the Meiko CS-2 <ref> [1, 8, 10, 11] </ref>. CONNECTED COMPONENTS ON DISTRIBUTED MEMORY MACHINES 7 before entering the global phase, we reduce the amount of work required for each iteration of that phase. a. Termination Check. Move star components with no remaining remote edges to a finished component list.
Reference: [12] <author> Y. Shiloach, U. Vishkin, </author> <title> "An O(log n) Parallel Connectivity Algorithm," </title> <journal> Journal of Algorithms, </journal> <volume> No. 3, </volume> <year> 1982, </year> <pages> pp. 57-67. </pages>
Reference-contexts: The solutions have running times linear in the number of edges and vertices in the graph and are easy to implement. Many efficient parallel solutions <ref> [2, 6, 12] </ref> have been devised, but these solutions are often complex and difficult to implement. Our implementation is a hybrid of a sequential search on the subgraph local to each processor and a variant of the Shiloach-Vishkin PRAM algorithm [12] on the global collection of subgraphs. <p> Many efficient parallel solutions [2, 6, 12] have been devised, but these solutions are often complex and difficult to implement. Our implementation is a hybrid of a sequential search on the subgraph local to each processor and a variant of the Shiloach-Vishkin PRAM algorithm <ref> [12] </ref> on the global collection of subgraphs. In this section, we briefly describe the key components of the PRAM algorithm. In the following discussion, we denote the vertex set of the input graph by V and the edge set by E. <p> Unconditional hooking is necessary to obtain log (n) bound on the running time, but is not necessary for correctness <ref> [12] </ref>. The algorithm follows: 1. For each vertex u, set Parent (u) u 2. Repeat until no change occurs in an iteration: a.
Reference: [13] <author> J. S. Wang, R. H. Swendsen, </author> <title> "Cluster Monte Carlo Algorithms," </title> <journal> Physica A, </journal> <volume> No. 167, </volume> <year> 1990, </year> <pages> pp. 565-579. </pages> <institution> Computer Science Division, University of California at Berkeley, Berkeley, Cali-fornia 94720 E-mail address: farvindk,stevel,culler,yelickg@CS.Berkeley.EDU </institution>
Reference-contexts: Connected component labeling is used in Physics to implement clustering in Monte Carlo algorithms such as that of Swendsen and Wang <ref> [13] </ref>, which simulates physical systems near critical temperatures by repeatedly grouping particles into clusters (connected components) and choosing a new state for each cluster. The graphs used for these applications have underlying grid topologies in either two or three dimensions.
References-found: 13

