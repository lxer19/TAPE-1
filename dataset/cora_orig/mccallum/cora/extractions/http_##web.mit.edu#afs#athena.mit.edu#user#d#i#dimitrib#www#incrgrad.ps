URL: http://web.mit.edu/afs/athena.mit.edu/user/d/i/dimitrib/www/incrgrad.ps
Refering-URL: http://web.mit.edu/afs/athena.mit.edu/user/d/i/dimitrib/www/publ.html
Root-URL: 
Title: A NEW CLASS OF INCREMENTAL GRADIENT METHODS 1 FOR LEAST SQUARES PROBLEMS  
Author: by Dimitri P. Bertsekas 
Date: August 1996 LIDS-P-2301  
Abstract: The LMS method for linear least squares problems difiers from the steepest descent method in that it processes data blocks one-by-one, with intermediate adjustment of the parameter vector under optimization. This mode of operation often leads to faster convergence when far from the eventual limit, and to slower (sublinear) convergence when close to the optimal solution. We embed both LMS and steepest descent, as well as other intermediate methods, within a one-parameter class of algorithms, and we propose a hybrid class of methods that combine the faster early convergence rate of LMS with the faster ultimate linear convergence rate of steepest descent. These methods are well-suited for neural network training problems with large data sets. Furthermore, these methods allow the efiective use of scaling based for example on diagonal or other approximations of the Hessian matrix. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> REFERENCES </institution>
Reference: [BeT89] <author> Bertsekas, D. P., and Tsitsiklis, J. </author> <title> N.,"Parallel and Distributed Computation: Numerical Methods," </title> <publisher> Prentice-Hall, Englewood Clifis, </publisher> <editor> N. J., </editor> <year> 1989. </year>
Reference-contexts: and the data blocks are nonquadratic, it is also possible to show a result analogous to Prop. 2, but again the proof is technically complex and will not be given. (2) Convergence results for parallel asynchronous versions of our method can be given, in the spirit of those in [TBA86], <ref> [BeT89] </ref> (Ch. 7), and [MaS94]. These results follow well established methods of analysis that rely on the stepsize being su-ciently small. (3) Variations of our method involving a quadratic momentum term are possible.
Reference: [BeT96] <author> Bertsekas, D. P., and Tsitsiklis, J. </author> <title> N.,"Neuro-Dynamic Programming," </title> <publisher> Athena Scientiflc, </publisher> <address> Belmont, MA., </address> <year> 1996. </year>
Reference-contexts: ) = 0; lim s (fi; ) = 0: From Eqs. (31), (32), and (34), we then obtain lim x (fi; ) = @ fi j=1 1 1 0 m X c j A = x : (b) We need the following well-known lemma (for a proof, see [Luo91], [Ber95a], <ref> [BeT96] </ref>). 10 2.
Reference: [Bel94] <author> Bell, B. M., </author> <title> "The Iterated Kalman Smoother as a Gauss-Newton Method," </title> <journal> SIAM J. on Optimization, </journal> <volume> Vol. 4, </volume> <year> 1994, </year> <note> p. 626. </note>
Reference: [Ber95a] <author> Bertsekas, D. P., </author> <title> "Nonlinear Programming," </title> <publisher> Athena Scientiflc, </publisher> <address> Belmont, MA, </address> <year> 1995. </year>
Reference-contexts: (fi; ) = 0; lim s (fi; ) = 0: From Eqs. (31), (32), and (34), we then obtain lim x (fi; ) = @ fi j=1 1 1 0 m X c j A = x : (b) We need the following well-known lemma (for a proof, see [Luo91], <ref> [Ber95a] </ref>, [BeT96]). 10 2. <p> These results follow well established methods of analysis that rely on the stepsize being su-ciently small. (3) Variations of our method involving a quadratic momentum term are possible. The use of such terms dates to the heavy ball method of Poljak (see [Pol64], [Pol87], <ref> [Ber95a] </ref>) in connection with the steepest descent method, and has become popular in the context of the incremental gradient method, particularly for neural network training problems (see [MaS94] for an analysis). (4) Diagonal scaling of the iterations generating i is possible by replacing the equation i = x k fi k
Reference: [Ber95b] <author> Bertsekas, D. P., </author> <title> "Incremental Least Squares Methods and the Extended Kalman Filter," </title> <institution> Lab. for Info. and Decision Systems Report LIDS-P-2237, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <note> 1995; to appear in SIAM J. on Optimization. </note>
Reference: [Dav76] <author> Davidon, W. C., </author> <title> "New Least Squares Algorithms," </title> <journal> J. of Optimization Theory and Applications, </journal> <volume> Vol. 18, </volume> <year> 1976, </year> <pages> pp. 187-197. </pages>
Reference: [Gai94] <author> Gaivoronski, A. A., </author> <title> "Convergence Analysis of Parallel Backpropagation Algorithm for Neural Networks," </title> <journal> Optimization Methods and Software, </journal> <volume> Vol. 4, </volume> <year> 1994, </year> <pages> pp. 117-134. </pages>
Reference: [Gri94] <author> Grippo, L., </author> <title> "A Class of Unconstrained Minimization Methods for Neural Network Training," </title> <journal> Optimization Methods and Software, </journal> <volume> Vol. 4, </volume> <year> 1994, </year> <pages> pp. 135-150. </pages>
Reference: [KuC78] <author> Kushner, H. J., and Clark, D. S., </author> <title> "Stochastic Approximation Methods for Constrained and Unconstrained Systems," </title> <publisher> Springer-Verlag, </publisher> <address> NY, </address> <year> 1978. </year>
Reference: [Lju77] <author> Ljung, L., </author> <title> "Analysis of Recursive Stochastic Algorithms," </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> Vol. 22, </volume> <year> 1977, </year> <pages> pp. 551-575. </pages>
Reference: [LuT94] <author> Luo, Z. Q., and Tseng, P., </author> <title> "Analysis of an Approximate Gradient Projection Method with Applications to the Backpropagation Algorithm," </title> <journal> Optimization Methods and Software, </journal> <volume> Vol. 4, </volume> <year> 1994, </year> <pages> pp. 85-101. </pages>
Reference: [Luo91] <author> Luo, Z. Q., </author> <title> "On the Convergence of the LMS Algorithm with Adaptive Learning Rate for Linear Feedforward Networks," </title> <journal> Neural Computation, </journal> <volume> Vol. 3, </volume> <year> 1991, </year> <pages> pp. 226-245. </pages>
Reference-contexts: S (fi; ) = 0; lim s (fi; ) = 0: From Eqs. (31), (32), and (34), we then obtain lim x (fi; ) = @ fi j=1 1 1 0 m X c j A = x : (b) We need the following well-known lemma (for a proof, see <ref> [Luo91] </ref>, [Ber95a], [BeT96]). 10 2.
Reference: [MaS94] <author> Mangasarian, O. L., and Solodov, M. V., </author> <year> 1994. </year> <title> "Serial and Parallel Backpropagation 18 References Convergence Via Nonmonotone Perturbed Minimization," </title> <journal> Optimization Methods and Software, </journal> <volume> Vol. 4, </volume> <year> 1994, </year> <pages> pp. 103-116. </pages>
Reference-contexts: are nonquadratic, it is also possible to show a result analogous to Prop. 2, but again the proof is technically complex and will not be given. (2) Convergence results for parallel asynchronous versions of our method can be given, in the spirit of those in [TBA86], [BeT89] (Ch. 7), and <ref> [MaS94] </ref>. These results follow well established methods of analysis that rely on the stepsize being su-ciently small. (3) Variations of our method involving a quadratic momentum term are possible. <p> The use of such terms dates to the heavy ball method of Poljak (see [Pol64], [Pol87], [Ber95a]) in connection with the steepest descent method, and has become popular in the context of the incremental gradient method, particularly for neural network training problems (see <ref> [MaS94] </ref> for an analysis). (4) Diagonal scaling of the iterations generating i is possible by replacing the equation i = x k fi k h i [cf.
Reference: [Man93] <author> Mangasarian, O. </author> <title> L, "Mathematical Programming in Neural Networks," </title> <journal> ORSA J. on Computing, </journal> <volume> Vol. 5, </volume> <year> 1993, </year> <pages> pp. 349-360. </pages>
Reference: [PoT73] <author> Poljak, B. T., and Tsypkin, Y. Z., </author> <title> "Pseudogradient Adaptation and Training Algorithms," </title> <journal> Automation and Remote Control, </journal> <volume> Vol. 12, </volume> <year> 1973, </year> <pages> pp. 83-94. </pages>
Reference: [Pol87] <author> Poljak, B. </author> <title> T, "Introduction to Optimization," Optimization Software Inc., </title> <address> N.Y., </address> <year> 1987. </year>
Reference-contexts: These results follow well established methods of analysis that rely on the stepsize being su-ciently small. (3) Variations of our method involving a quadratic momentum term are possible. The use of such terms dates to the heavy ball method of Poljak (see [Pol64], <ref> [Pol87] </ref>, [Ber95a]) in connection with the steepest descent method, and has become popular in the context of the incremental gradient method, particularly for neural network training problems (see [MaS94] for an analysis). (4) Diagonal scaling of the iterations generating i is possible by replacing the equation i = x k fi
Reference: [TBA86] <author> Tsitsiklis, J. N., Bertsekas, D. P., and Athans, M., </author> <title> "Distributed Asynchronous Deterministic and Stochastic Gradient Optimization Algorithms," </title> <journal> IEEE Trans. on Aut. Control, </journal> <volume> Vol. AC-31, </volume> <year> 1986, </year> <pages> pp. 803-812. </pages>
Reference-contexts: 1, and the data blocks are nonquadratic, it is also possible to show a result analogous to Prop. 2, but again the proof is technically complex and will not be given. (2) Convergence results for parallel asynchronous versions of our method can be given, in the spirit of those in <ref> [TBA86] </ref>, [BeT89] (Ch. 7), and [MaS94]. These results follow well established methods of analysis that rely on the stepsize being su-ciently small. (3) Variations of our method involving a quadratic momentum term are possible.
Reference: [WaT90] <author> Watanabe, K., and Tzafestas, S. G., </author> <title> "Learning Algorithms for Neural Networks with the Kalman Filters," </title> <journal> J. Intelligent and Robotic Systems, </journal> <volume> Vol. 3, </volume> <year> 1990, </year> <pages> pp. 305-319. </pages>
Reference: [Whi89] <author> White, H., </author> <title> "Some Asymptotic Results for Learning in Single Hidden-Layer Feedforward Network Models," </title> <journal> J. Am. Statistical Association, </journal> <volume> Vol. 84, </volume> <year> 1989. </year>
Reference: [WiH60] <author> Widrow, B., and Hofi, M. E., </author> <title> "Adaptive Switching Circuits," </title> <booktitle> Institute of Radio Engineers, Western Electronic Show and Convention, Convention Record, part 4, </booktitle> <year> 1960, </year> <pages> pp. 96-104. </pages>
Reference: [WiS85] <author> Widrow, B., and Stearns, S. D., </author> <title> "Adaptive Signal Processing," </title> <publisher> Prentice-Hall, Englewood Clifis, </publisher> <editor> N. J., </editor> <year> 1985. </year> <month> 19 </month>
References-found: 22

