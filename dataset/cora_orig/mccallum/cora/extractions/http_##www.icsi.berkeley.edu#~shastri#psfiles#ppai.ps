URL: http://www.icsi.berkeley.edu/~shastri/psfiles/ppai.ps
Refering-URL: http://www.icsi.berkeley.edu/~shastri/shruti/index.html
Root-URL: http://www.icsi.berkeley.edu
Title: Massively parallel knowledge representation and reasoning: Taking a cue from the brain  
Author: Lokendra Shastri afl and D.R. Mani b J. Geller, H. Kitano, and C. Suttner. 
Note: To appear in Parallel Processing for Artificial Intelligence 3, (Eds.)  Elsevier Science. This work was partially funded by NSF grant IRI 88-05465, ARO grant DAA29-84-9-0027, ONR grants N00014-93-1-1149 and N00014-95-C-0182, and NSF resource grant CCR930001N.  
Address: 1947 Center Street, Ste. 600 Berkeley, CA 94707  14 Crosby Drive Bedford, MA 01730  
Affiliation: a International Computer Science Institute  b Thinking Machines Corporation  
Abstract: Any intelligent system capable of common sense reasoning and language understanding must be capable of performing rapid inferences with reference to a large body of knowledge. The ability to perform rapid inferences with large knowledge bases is also essential for supporting flexible and effective access to the enormous body of electronically available data. Since complexity theory tells that not all inferences can be computed effectively, it is important to identify interesting classes of inference that can be performed effectively. Over the past several years we have tried to do so by working within a neurally motivated, massively parallel computational model. Our approach is motivated by the belief that situating the knowledge representation and reasoning problem within a neurally motivated computational architecture will not only enhance our understanding of the mind/brain, but it will also lead to the development of effective knowledge representation and reasoning systems implemented on existing hardware. In this chapter we substantiate this claim and review some results of pursuing this approach. These include a characterization of reflexive reasoning | reasoning that can be performed effectively by neurally plausible networks; the design of CSN | a connectionist semantic network that can perform inheritance and recognition in time proportional to the depth of the conceptual hierarchy; shruti | a connectionist knowledge representation and inference system that can encode a large number of facts, rules, and a type hierarchy, and perform a class of first-order inferences with extreme efficiency, and shruti-cm5 an implementation of shruti on the CM-5 that can encode over half a million rules, facts, and types and respond to reflexive queries within a few hundred milliseconds. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> V. Ajjanagadde and L. Shastri. </author> <title> Rules and variables in neural nets. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 121-134, </pages> <year> 1991. </year>
Reference: 2. <author> A. Baddley. </author> <title> Working Memory. </title> <publisher> Clarendon Press, </publisher> <year> 1986. </year>
Reference-contexts: facts represented in the wmrr during an episode of reflexive reasoning should not be confused with the small number of short-term facts an agent may overtly keep track of during reflective processing and problem solving. wmrr should not be confused with the short-term memory implicated in various memory span tasks <ref> [2] </ref>.
Reference: 3. <author> P. Dietz, D. Krizanc, S. Rajasekaran, and L. Shastri. </author> <title> A lower bound result for the common element problem and its implication for reflexive reasoning. </title> <type> Technical Report MS-CIS-93-73, </type> <institution> Department of Computer and Information Science, University of Pennsylvania, </institution> <year> 1993. </year>
Reference-contexts: In [23] we had conjectured that when performing reasoning via backward chaining, any network model whose size is linear in jLT KBj and which computes inferences in time independent of jLT KBj must limit itself to answering reflexive queries involving balanced rules. A recent result <ref> [3] </ref> establishes a lower bound of (log n) on the time required for inferences that violate this restriction and thus provides a proof of our conjecture. 4.3. The computational model Our computational model is a network of nodes connected via weighted links.
Reference: 4. <author> M. P. Evett, W. A. Andersen, and J. A. Hendler. </author> <title> Providing computationally effective knowledge representation via massive parallelism. </title> <editor> In L. Kanal and V. Kumar, editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <address> New York, 1994. </address> <publisher> Elsevier Science Publication. </publisher>
Reference: 5. <author> S. E. Fahlman. </author> <title> NETL: A System for Representing and Using Real World Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1979. </year>
Reference: 6. <author> J. A. Feldman and D. H. Ballard. </author> <title> Connectionist models and their properties. </title> <journal> Cognitive Science, </journal> <volume> 6(3) </volume> <pages> 205-254, </pages> <year> 1982. </year>
Reference-contexts: Many cognitive tasks, and certainly all the perceptual ones, that humans can perform in a few hundred milliseconds would require millions of instructions on a serial (von Neumann) computer, and it is apparent that a serial computer will be unable to perform these tasks within an acceptable time-frame <ref> [6] </ref>. The crux of the problem becomes apparent if one examines the architecture of a traditional von Neumann computer. <p> For example, see [4-7,11,12,32]. 7 structures. 7 The relative simplicity of a neuron's processing ability with reference to the needs of symbolic computation, and the restriction on the complexity of messages exchanged by neurons, impose strong constraints on the nature of neural representations and processes <ref> [6] </ref>. A specific limitation of neurally plausible systems is that they have difficulty representing composite structures in a dynamic fashion. Consider the representation of the fact give (John, Mary, a-Book).
Reference: 7. <author> J. Geller and C. Du. </author> <title> Parallel implementation of a class reasoner. </title> <journal> Journal of Theoretical Artificial Intelligence, </journal> <volume> 3 </volume> <pages> 109-127, </pages> <year> 1991. </year>
Reference: 8. <author> R. V. Guha and D. B. Lenat. </author> <title> CYC: A mid-term report. </title> <journal> AI Magazine, </journal> <volume> 11(3) </volume> <pages> 32-59, </pages> <year> 1990. </year>
Reference-contexts: Plausible estimates of the size of such a knowledge base range from several hundred thousand to more than a million items <ref> [8] </ref>. Nevertheless, we can understand language at the rate of several hundred words per minute. This clearly suggests that we are capable of performing a wide range of inferences with reference to a large knowledge base within a few hundred milliseconds. <p> Let us amplify: * Reflexive reasoning occurs with respect to a large body of background knowledge. A serious attempt at compiling common sense knowledge suggests that our background knowledge base may contain as many as 10 6 items <ref> [8] </ref>.
Reference: 9. <author> J. Henderson. </author> <title> Connectionist syntactic parsing using temporal variable binding. </title> <journal> Journal of Psycholinguistic Research, </journal> <volume> 23(5) </volume> <pages> 353-379, 1004. </pages>
Reference-contexts: Thus an agent would be unable to make a prediction (or answer a query)|even when the prediction (or answer) logically follows from the knowledge encoded in the LT KB|if the length of the derivation leading to the prediction (or the answer) exceeds this bound. Henderson <ref> [9] </ref> has developed an on-line parser for English using a shruti-like architecture whose speed is independent of the size of the grammar and which can recover the structure of arbitrarily long sentences as long as the dynamic state required to parse the sentence does not exceed the capacity of the parser's
Reference: 10. <author> M. A. Just and P. A. Carpenter. </author> <title> A capacity theory of comprehension: Individual differences in working memory. </title> <journal> Psychological Review, </journal> <volume> 99(1) </volume> <pages> 122-149, </pages> <year> 1993. </year>
Reference-contexts: Most proposals characterizing the capacity of the working memory underlying cognitive processing have not paid adequate attention to the structure 24 of items in the working memory and their role in processing. Even recent proposals such as <ref> [10] </ref> characterize working memory capacity in terms of "total activation".
Reference: 11. <author> H. Kitano. </author> <title> Challenges of massive parallelism. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 813-834, </pages> <year> 1993. </year>
Reference: 12. <author> H. Kitano, H. Hendler, T. Higuchi, D. Moldovan, and D. Waltz. </author> <booktitle> Massively parallel artificial intelligence. In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 557-562, </pages> <year> 1991. </year>
Reference: 13. <author> G. Lakoff and M. Johnson. </author> <title> Metaphors We Live By. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1980. </year>
Reference-contexts: The structure imposed on the knowledge base is a gross attempt to mimic a plausible structuring of real-world knowledge bases. This is motivated by the notion that knowledge about complex domains are learned and grounded in metaphorical mappings from (to) some basic perceptually and bodily grounded domains <ref> [13] </ref>. Queries for experimenting with each artificial knowledge base were generated using the facts associated with predicates, and the inference dependency graph representing rules interconnecting predicates.
Reference: 14. <author> D. R. Mani. </author> <title> The Design and Implementation of Massively Parallel Knowledge Representation and Reasoning Systems: A Connectionist Approach. </title> <type> PhD thesis, </type> <institution> Depart 37 ment of Computer and Information Science, University of Pennsylvania, </institution> <year> 1995. </year>
Reference-contexts: Shruti-cm5 In this section, we briefly describe the design and implementation of shruti-cm5, an spmd asynchronous message passing parallel reflexive reasoning system developed on the Connection Machines CM-5. A more detailed desctiption of shruti-cm5 can be found in <ref> [14] </ref>. 5.2.1. The connection machine CM-5 The Connection Machine model CM-5 [27] is an MIMD machine consisting of anywhere from 32 to 1024 powerful processors. 8 Each processing node is a general-purpose computer which can execute instructions autonomously and perform interprocessor communication. <p> In this mode, all communication, synchronization and data layout are under the programs' explicit control. 5.2.2. The design of shruti-cm5 We outline the design and functionality of shruti-cm5. A detailed discussion and justification of the design choices may be be found in <ref> [14] </ref>. The Knowledge Base. Shruti-cm5 supports all of shruti's legal rules, facts and type hierarchy relations. The type hierarchy can encode both is-a relations (which explicate the subconcept-superconcept relations between entities) and labeled relations which specify that two entities are related by a relation R. <p> When these active messages arrive at their destinations, they invoke handler functions which receive and process the incoming activation, and update the relevant frontiers. A detailed description of each component of the activation propagation loop and the structure of the active messages used can be found in <ref> [14] </ref>. Activation propagation in the type hierarchy occurs in a manner similar to that of the rule base. As entities become active, they broadcast their activations to all the processors in the partition. The processors cache this information for fast, local access during fact matching and other operations. <p> Shruti-cm5 timings are averages over ten runs. CM-5 processors for the query is-a hypernym (Entity,Sparrow)?. For a detailed description see <ref> [14] </ref>. 6. CONCLUSION In this chapter we have shown that there exist interesting and useful classes of inference that can be performed rapidly by neurally motivated and massively parallel computational models. These include a class of inheritance and recognition problems and a class of reflexive first-order inferences.
Reference: 15. <author> D. R. Mani and L. Shastri. </author> <title> Reflexive reasoning with multiple instantiation in a connectionist reasoning system with a type hierarchy. </title> <journal> Connection Science, </journal> <volume> 5(3 </volume> & 4):205-242, 1993. 
Reference-contexts: As entities become active, they broadcast their activations to all the processors in the partition. The processors cache this information for fast, local access during fact matching and other operations. Multiple instantiation of predicates and concepts <ref> [15] </ref> is handled procedurally by performing necessary checks and maintaining required bookkeeping information to ensure that (i) any predicate or concept represents at most a bounded number of instantiations and (ii) a given instantiation is represented at most once so that no two banks of a predicate or concept represent the
Reference: 16. <author> G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, K. Miller, and R. Tengi. </author> <title> Five papers on WordNet. </title> <type> Technical Report CSL-43, </type> <institution> Princeton University, </institution> <month> July </month> <year> 1990. </year> <month> Revised March </month> <year> 1993. </year>
Reference-contexts: Experimental results Shruti-cm5 has been tested using (i) artificial KBs containing over half a million rules and facts, and (ii) WordNet, a real-world lexical database <ref> [16] </ref>. In this section we present 31 different sizes. Avg. <p> This speedup in suggests that shruti-cm5 does exploit parallelism and can benefit from the availability of even greater parallelism. 35 5.3.2. Experiments with WordNet WordNet is an on-line lexical database which attempts to organize information in terms of word meanings and their inter-relationships <ref> [16] </ref>. Groups of synonymous words, termed synsets, are used to represent meanings. The meanings of synsets are then defined in terms of their relationships with other synsets.
Reference: 17. <author> L. Shastri. </author> <title> Massive parallelism in artificial intelligence. </title> <type> Technical Report MS-CIS-86-77, </type> <institution> University of Pennsylvania, </institution> <year> 1986. </year>
Reference-contexts: about ourselves, our family, friends, colleagues, history and geography; our knowledge of artifacts, sports, art, music; some basic principles of science and mathematics; and our models of social, civic, and political interactions. 2 The significance of computational effectiveness in the context of AI was first discussed in these terms in <ref> [17] </ref>. 3 Two caveats are in order. First, we are assuming that a path leading to a forced win exists, but such a path may not exist.
Reference: 18. <author> L. Shastri. </author> <title> Semantic Networks: An Evidential Formulation and its Connectionist Realization. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: These issues are discused in Section 5. 3. CSN|A CONNECTIONIST SEMANTIC NETWORK Several years ago we developed CSN, a connectionist semantic network <ref> [18] </ref> that solves a class of inheritance and recognition problems extremely fast|in time proportional to the depth of the conceptual hierarchy. <p> The network is designed automatically by a network compiler that takes as input, a high level specification of the knowledge to be encoded in the network. The syntax of the specification language parallels the representation language described above. In <ref> [18] </ref> we identify specific constraints that must be satisfied by the conceptual structure in order to achieve an efficient connectionist realization.
Reference: 19. <author> L. Shastri. </author> <title> Default reasoning in semantic networks: A formalization of recognition and inheritance. </title> <journal> Artificial Intelligence, </journal> <volume> 39(3) </volume> <pages> 283-355, </pages> <year> 1989. </year>
Reference-contexts: One could argue that neurons may be capable of communicating more complex messages by using variations in interspike delays to encode information (for e.g., see Strehler & Lestienne 1986). However, Thorpe and Imbert <ref> (1989) </ref> have argued that in the context of rapid processing, the firing rate of neurons relative to the time available to neurons to respond to their inputs implies that a presynaptic neuron can only communicate one or two spikes to a postsynaptic neuron before the latter must produce an output.
Reference: 20. <author> L. Shastri. </author> <title> The relevance of connectionism to AI: A representation and reasoning perspective. </title> <editor> In J. A. Barnden and J. B. Pollack, editors, </editor> <booktitle> Advances in Connectionist and Neural Computation Theory, </booktitle> <volume> Volume 1. </volume> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, NJ, </address> <year> 1991. </year>
Reference-contexts: These inferences are performed rapidly, spontaneously and without conscious effort|as though they were a reflex response of our cognitive apparatus. In view of this we have described such reasoning as reflexive <ref> [20] </ref>. Reflexive reasoning may be contrasted with reflective reasoning which requires reflection, conscious deliberation, and often an overt consideration of alternatives and weighing of possibilities. Reflective reasoning takes longer and often requires the use of external props such as a paper and pencil.
Reference: 21. <author> L. Shastri. </author> <title> A computational model of tractable reasoning|taking inspiration from cognition. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <year> 1993. </year>
Reference-contexts: But as argued in <ref> [21] </ref> reflexive reasoning requires a more stringent criterion of tractability. Let us amplify: * Reflexive reasoning occurs with respect to a large body of background knowledge. A serious attempt at compiling common sense knowledge suggests that our background knowledge base may contain as many as 10 6 items [8]. <p> While our language is more general than Datalog in its treatment of relations, it does impose a restriction on the form of rules (see below). 4.2. The class of reflexive inference A characterization of the class of reflexive inference is provided in <ref> [21] </ref>. This characterization is facilitated by the following definitions: 2 Any variable that occurs in multiple argument positions in the antecedent of a rule is a pivotal variable. 2 A rule is balanced if all pivotal variables occurring in the rule also appear in its consequent.
Reference: 22. <author> L. Shastri. </author> <title> Structured connectionist models. </title> <editor> In M. A. Arbib, editor, </editor> <title> The Handbook of Brain Theory and Neural Networks. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference: 23. <author> L. Shastri and V. Ajjanagadde. </author> <title> From simple associations to systematic reasoning: A connectionist representation of rules, variables and dynamic bindings using temporal synchrony. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 16(3) </volume> <pages> 417-494, </pages> <year> 1993. </year>
Reference-contexts: In <ref> [23] </ref> we had conjectured that when performing reasoning via backward chaining, any network model whose size is linear in jLT KBj and which computes inferences in time independent of jLT KBj must limit itself to answering reflexive queries involving balanced rules.
Reference: 24. <author> L. Shastri and D. J. Grannes. </author> <title> Dealing with negated knowledge and inconsistency in a neurally motivated model of memory and reflexive reasoning. </title> <type> Technical Report TR-95-041, </type> <institution> International Computer Science Institute, </institution> <year> 1995. </year>
Reference-contexts: The form of queries is similar to that of facts except that whereas repeated universally quantifed variables can occur in queries, existentially quantified variables are assumed to be distinct. Recently, we have extended shruti to allow negative literals to occur in facts, queries, and rules <ref> [24] </ref>. The representation language shares the function-free property of Datalog [30] but differs from it in a number of ways.
Reference: 25. <author> W. Singer. </author> <title> Synchronization of cortical activity and is putative role in information processing and learning. </title> <journal> Annual Review of Psysiology, </journal> <volume> 55 </volume> <pages> 349-74, </pages> <year> 1993. </year>
Reference-contexts: Emerging neurophysiological data suggests that the basic mechanisms proposed for representing and propagating dynamic variable bindings, namely, the propagation of rhythmic patterns of activity and the synchronous activation of nodes, exist in the brain and appear to play a role in the representation and processing of information <ref> [25] </ref>. By generalizing the results of our work on shruti we have identified a class of inference that can be performed by a parallel network whose size is linear in jLT KBj, in time that is proportional to the depth of inference and is otherwise independent of jLT KBj.
Reference: 26. <author> U. K. Sources, </author> <month> April </month> <year> 1994. </year> <title> National Library of Medicine. </title>
Reference-contexts: Several of these knowledge sources|in addition to the common sense knowledge base|will be very large and may contain several hundred thousand items. For example, the Unified Medical Language System's terminological component contains 190,863 entries consisting of medical, clinical and chemical concepts <ref> [26] </ref>. While database and information retrieval technology has evolved considerably over the past decade, the development of large-scale yet efficient knowledge based systems capable of supporting inference has lagged behind.
Reference: 27. <author> TMC. </author> <title> Connection machine CM-5 technical summary. </title> <type> Technical Report CMD-TS5, </type> <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Shruti-cm5 In this section, we briefly describe the design and implementation of shruti-cm5, an spmd asynchronous message passing parallel reflexive reasoning system developed on the Connection Machines CM-5. A more detailed desctiption of shruti-cm5 can be found in [14]. 5.2.1. The connection machine CM-5 The Connection Machine model CM-5 <ref> [27] </ref> is an MIMD machine consisting of anywhere from 32 to 1024 powerful processors. 8 Each processing node is a general-purpose computer which can execute instructions autonomously and perform interprocessor communication. Each processor can have up to 32 megabytes of local memory 9 and optional vector processing hardware.
Reference: 28. <institution> TMC. </institution> <note> CMMD Reference Manual. Version 3.0. </note> <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: We therefore use knowledge-level partitioning in mapping shruti onto the CM-5. This decision is also motivated by the fact that all the information pertaining to a predicate cluster can be encoded within a single CM-5 active message (see below). Active Messages and Communication. Shruti-cm5 uses CMMD library functions <ref> [28] </ref> for broadcasting and synchronization, while almost all interprocessor communication is achieved using CMAML (CM Active Message Library) routines. CMAML provides efficient, low-latency interprocessor communication for short messages [28,31]. Active messages are asynchronous (non-blocking) and have very low communication overhead.
Reference: 29. <institution> TMC. </institution> <note> CM-5 User's Guide. CMost Version 7.3. </note> <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: A typical user task consists of a process running on the partition manager and a process running on each of the processing nodes. Though the basic architecture of the CM-5 supports MIMD style programming, it is most often used to run SPMD (Single Program Multiple Data) style programs <ref> [29] </ref>. Both data parallel (SIMD) and message-passing programming on the CM-5 use the SPMD model.
Reference: 30. <author> J. D. Ullman. </author> <title> Principles of Database and Knowledge-Bases Systems. </title> <publisher> Computer Science Press, </publisher> <year> 1988. </year>
Reference-contexts: Recently, we have extended shruti to allow negative literals to occur in facts, queries, and rules [24]. The representation language shares the function-free property of Datalog <ref> [30] </ref> but differs from it in a number of ways. For example, unlike Datalog, our language does not force a dichotomy between extensional and intensional relations and allows both intensional as well as extensional relations to occur in the head of a rule.
Reference: 31. <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the Nineteenth International Symposium on Computer Architecture. </booktitle> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference: 32. <author> D. L. Waltz. </author> <title> Memory-based reasoning. </title> <editor> In M. A. Arbib and J. A. Robinson, editors, </editor> <booktitle> Natural and Artificial Parallel Computation, </booktitle> <pages> pages 251-276. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
References-found: 32

