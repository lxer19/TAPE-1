URL: ftp://archive.cis.ohio-state.edu/pub/neuroprose/drucker.boosting-regression.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00327.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: drucker@monmouth.edu  
Title: Improving Regressors using Boosting Techniques  
Author: Harris Drucker 
Address: West Long Branch, NJ 07764  
Affiliation: Monmouth University  
Abstract: In the regression context, boosting and bagging are techniques to build a committee of regressors that may be superior to a single regressor. We use regression trees as fundamental building blocks in bagging committee machines and boosting committee machines. Performance is analyzed on three non-linear functions and the Boston housing database. In all cases, boosting is at least equivalent, and in most cases better than bagging in terms of prediction error.
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L.(1996a). </author> <title> "Stacked Regressions", </title> <journal> Machine Learning, </journal> <volume> vol. 24, No. 1, </volume> <pages> pp. 41-64. </pages> <note> Also at anonymous ftp site: ftp.stat.berkeley.edu/pub/tech-reports/367.ps.Z. </note>
Reference: <author> Breiman, L. </author> <year> (1996b). </year> <journal> "Bagging Predictors" Machine Learning, </journal> <volume> vol. 26, No. 2, </volume> <pages> pp. 123-140. </pages> <note> Also at anonymous ftp site: ftp.stat.berkeley.edu/pub/tech-reports/421.ps.Z. </note>
Reference: <author> Breiman, L. </author> <title> (1996) "Bias, Variance, </title> <type> and Arcing Classifiers" Technical Report 460, </type> <institution> Department of Statistics, University of California, Berkeley, CA. </institution> <note> Annals of Statistics (to be published) Also at anonymous ftp site: ftp.stat.berkeley.edu/pub/tech-reports/460.ps.Z. </note>
Reference: <author> Breiman, L., and Spector, P. </author> <year> (1992). </year> <title> "Submodel Selection and Evaluation in Regression. The X-Random Case", </title> <journal> International Statistical Review, </journal> <volume> 60, 3, </volume> <month> pp.291-319. </month>
Reference-contexts: Furthermore, if E [n] = 0 and E [n i n j ]=d i j s 2 , then we may take the expectation with respect to (y,x x) and obtain <ref> (Breiman and Spector, 1992) </ref>: E [PE] = s 2 +E [ME] This shows us that even if we know the model exactly, there is a minimum prediction error due to the noise.
Reference: <author> Breiman, L., Friedman, H.H., Olshen, R.A., and Stone, C.J. </author> <year> (1984). </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference: <author> Brodley, C.E., and Utgoff, P.E. </author> <year> (1995), </year> <title> "Multivariate Decision Trees", </title> <journal> Machine Learning, </journal> <volume> 19, </volume> <pages> pp. 45-77. </pages>
Reference: <author> Drucker, H., (1996) and Cortes, C. </author> <title> (1996) "Boosting Decision Trees", Neural Information Processing 8, </title> <editor> ed: D.S. Touretzky, M,C. Mozer and M.E. Hasselmo. </editor> <publisher> Morgan Kaufmann, pp.479-485. </publisher>
Reference: <author> Drucker, H., Cortes, C., Jackel, LD., LeCun, Y., Vapnik V. </author> <year> (1994). </year> <title> "Boosting and Other Ensemble Methods", </title> <note> Drucker, </note> <author> H., Schapire, R.E., Simard, P., </author> <year> (1993), </year> <title> "Boosting Performance in Neural Networks", Efron, </title> <editor> B., and Tibshirani, R. </editor> <year> (1993), </year> <title> An Introduction to the Bootstrap, </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Fisz, M., </author> <year> (1963). </year> <title> Probability Theory and Mathematical Statistics, </title> <publisher> John Wiley. </publisher>
Reference: <author> Freedman, J.H., </author> <year> (1991), </year> <title> "Multivariate Adaptive Regression Splines", </title> <journal> The Annals of Statistics, </journal> <volume> 19, No. 1, </volume> <pages> pp. </pages> <note> 1-82 Freund, </note> <author> Y., and Schapire, R.E. </author> <year> (1996a). </year> <title> "Experiments with a New Boosting Algorithm", </title> <booktitle> Machine Learning: Proceedings of the Thirteenth Conference, </booktitle> <editor> ed: L. Saitta, </editor> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 148-156. </pages> <note> Also at web site: http://www.research.att.com/yoav. or http://www.research.att.com/orgs/ssr/people/yoav. </note>
Reference: <author> Freund, Y., and Schapire, R.E. </author> <year> (1996b). </year> <title> "A decision-theoretic generalization of on-line learning and an application to boosting", at web site: http://www.research.att.com/yoav. or http://www.research.att.com/orgs/ssr/people/yoav. An extended abstract appears in the Proceedings of the Second European Conference on Computational Learning Theory, Barcelona, </title> <publisher> Springer-Verlag, </publisher> <month> March, </month> <year> 1995, </year> <pages> pp. 23-37. </pages>
Reference: <author> Ittner, A., and Schlosser, M.(1996), </author> <title> "Non-Linear Decision Trees-NDT", </title> <booktitle> Machine Learning: Proceedings of the Thirteenth Conference, </booktitle> <editor> ed: L. Saitta, </editor> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 252-257. </pages>
Reference: <author> Kearns, M. </author> <year> (1996). </year> <title> "A Bound on the Error of Cross Validation Using the Approximation and Estimation Rates, with Consequences for Training-Test Split", Neural Information Processing 8, </title> <editor> ed: D.S. Touretzky, M,C. Mozer and M.E. Hasselmo. </editor> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 183-189. </pages>
Reference: <author> Murthy, S., Kasif, S., Salzberg, S., and Beigel, R. </author> <year> (1993). </year> <title> "OC!: </title> <booktitle> Randomized induction of oblique decision trees" Proceeding of the 11th National Conference on Artificial Intelligence (AAAI-93), </booktitle> <publisher> MIT-Press. </publisher>
Reference: <author> Mruth, S., Kasif, S., Salzber, S. </author> <year> (1994), </year> <title> "A System for Induction of Oblique Decision Trees", </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schapire, R.E., </author> <year> (1990), </year> <title> "The Strength of Weak Learnability", </title> <journal> Machine Learning, </journal> <volume> 5, 2, </volume> <pages> pp. 197-227. </pages>
Reference: <author> Wolbert, D.H. </author> <year> (1992), </year> <title> "Stacked Generalization", </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> pp. 241-259. </pages>
References-found: 17

