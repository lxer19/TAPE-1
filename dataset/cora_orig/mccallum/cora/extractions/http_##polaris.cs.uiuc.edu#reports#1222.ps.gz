URL: http://polaris.cs.uiuc.edu/reports/1222.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: PARCEL and MIPRAC: Parallelizers for Symbolic and Numeric Programs  
Author: Williams Ludwell Harrison III and Zahira Ammarguellat 
Date: May 7, 1992  
Abstract-found: 0
Intro-found: 1
Reference: [Amm90] <author> Zahira Ammarguellat. </author> <title> An algorithm for control-flow normalization and its complexity. </title> <type> Technical report, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: This is accomplished in two steps: a syntax-directed translation from the source language, and a subsequent control-flow normalization <ref> [Amm90] </ref>. In the end, the only control structures that remain are sequencing (begin), conditional execution (if), and procedure calls. This gives MIPRAC a procedure orientation, in constrast to the loop orientation of conventional parallelizers. <p> We handle this difficulty in two steps. First, the front-ends for MIPRAC rewrite all iterative structures in terms of gotos and labels. Second, a phase of control-flow normalization <ref> [Amm90] </ref> rids the program of gotos entirely, replacing them with 20 struct foo f int info; struct foo *next; g; struct foo *make list (n) int n; struct foo *s, *t; if (n != 0) t = (struct foo *) malloc (sizeof (struct foo)); t-&gt;info = n; s = make list
Reference: [Ban76] <author> Uptal D. Banerjee. </author> <title> Data dependence in ordinary programs. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> November </month> <year> 1976. </year>
Reference-contexts: In particular, the analysis cannot benefit from even the simplest test for independence among subscript expressions <ref> [Ban76, Ban79] </ref>.
Reference: [Ban79] <author> Uptal D. Banerjee. </author> <title> Speedup of Ordinary Programs. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: In particular, the analysis cannot benefit from even the simplest test for independence among subscript expressions <ref> [Ban76, Ban79] </ref>.
Reference: [BC86] <author> Michael Burke and Ronald G. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the SIGPLAN 1986 Symposium on Compiler Construction, </booktitle> <pages> pages 162-175. </pages> <institution> Association for Computing Machinery, </institution> <month> July </month> <year> 1986. </year>
Reference-contexts: Interprocedural analysis, if performed at all, was ordinarily done to strengthen dependence analysis of loops, to infer aliasing among parameters, and to fold constants across procedure boundaries, but seldom with the goal of extracting high-level parallelism from a Fortran program. (There are exceptions; see <ref> [Tri84, BC86] </ref>.) For this reason, such compilers are typically quite capable of extracting fine-grained parallelism, but do less well in extracting large-grained, high-level parallelism.
Reference: [Cho90] <author> Jhy-Herng Chow. </author> <title> Run-time support for automatically parallelized lisp programs. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: When the nesting of these parallel threads is judged to be sufficient to saturate the machine (this judgement is made according to one of several experimental strategies; see <ref> [Cho90] </ref>), the sequential versions of procedures are invoked, so that no further creation of parallel activity will occur beyond that nesting depth. A traditional microtasking environment [Cra82, EHJP90] associates a stack with every processor. <p> The last processor to finish work on the parallel loop will return its stack to the pool, seize the stack abandoned by the initiating processor, and execute the continuation of the loop. This scheme is described in <ref> [Cho90] </ref>. As an example of the transformations performed in PARCEL, consider the program in Figure 3. The procedure copy is defined, to copy a tree of cons cells. <p> The program achieves a maximum speedup of around 6:5, at a nesting depth of about 4, with 8 processors active. We are experimenting with a number of strategies for automatically selecting between 12 the parallel and sequential procedure versions during execution; the results of these experiments are reported in <ref> [Cho90] </ref>. In Figure 7, a similar execution profile is given for the BOYER benchmark [Gab85], a simple rewriting system designed as a benchmark for prediciting the performance of the Boyer-Moore theorem prover.
Reference: [Cra82] <institution> Cray Research, Mendota Heights, MN. Cray X-MP Series Mainframe Reference Manual (HR-0032), </institution> <year> 1982. </year>
Reference-contexts: A traditional microtasking environment <ref> [Cra82, EHJP90] </ref> associates a stack with every processor. When a processor initiates a parallel loop, the continuation of that loop is on its stack; it must wait for all iterations of the initiated loop to terminate, before it executes the continuation.
Reference: [EHJP90] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> Cedar fortran and its compiler. </title> <type> Technical Report 966, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> jan </month> <year> 1990. </year>
Reference-contexts: A traditional microtasking environment <ref> [Cra82, EHJP90] </ref> associates a stack with every processor. When a processor initiates a parallel loop, the continuation of that loop is on its stack; it must wait for all iterations of the initiated loop to terminate, before it executes the continuation.
Reference: [Gab85] <author> Richard P. Gabriel. </author> <title> Performance and Evaluation of Lisp Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1985. </year>
Reference-contexts: We are experimenting with a number of strategies for automatically selecting between 12 the parallel and sequential procedure versions during execution; the results of these experiments are reported in [Cho90]. In Figure 7, a similar execution profile is given for the BOYER benchmark <ref> [Gab85] </ref>, a simple rewriting system designed as a benchmark for prediciting the performance of the Boyer-Moore theorem prover. In Figure 8, the same profile is shown, except that the speedup, rather than the cpu time, is plotted on the vertical axis.
Reference: [Har89] <author> Williams Ludwell Harrison III. </author> <title> The interprocedural analysis and automatic parallelization of scheme programs. Lisp and Symbolic Computation: </title> <journal> an International Journal, </journal> 2(3/4):179-396, 1989. 
Reference-contexts: 1 Introduction PARCEL is a compiler and run-time system that automatically parallelizes a Scheme program for execution on a shared-memory multiprocessor; it is described at length in <ref> [Har89] </ref>. <p> In essence, the observation applied by this analysis is the following: the visibility of a side-effect is restricted to the subtree of computation that contains the lifetime of the object affected. (The interested reader will find this analysis described and proven correct in <ref> [Har89] </ref>.) From this information, PARCEL constructs dependence graphs of individual procedures, and thereafter parallelizes their control-flow structures regardless of whether they are "outermost" or "innermost" procedures (indeed, such a characterization may be meaningless, as a procedure may be invoked in many different contexts, as well as recursively). <p> This lifetime information can also be used to guide the placement of objects in the hierarchical shared memory of a machine like Cedar. These ideas are developed in detail in <ref> [Har89] </ref>. As an example of this analysis, consider the program in Figure 1. The procedure counter returns an object (a closure that increments a free variable x and returns its updated value).
Reference: [Har90] <author> Williams Ludwell Harrison III. </author> <title> Semantic analysis for automatic paral-lelization of symbolic programs. Technical Report Work In Progress, </title> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: In MIPRAC, this is accomplished by an interprocedural, abstract interpretation over domains in which side-effects, dependences, object lifetimes, and structure sharing (aliasing among pointers) have a natural expression <ref> [Har90] </ref>. <p> The interprocedural analysis is then the following problem: to find the abstract meaning of a program (a fixpoint problem), and to apply theorems to that meaning, that describe the dependences, lifetimes, and structure sharing of the program. This analysis is described in detail in <ref> [Har90] </ref>. It has been implemented in C, and is itself being parallelized to run on an Alliant or on the Cedar, because it is the most expensive phase of MIPRAC.
Reference: [Tri84] <author> Remi Triolet. </author> <title> Contributions to Automatic Parallelization of Fortran Programs with Procedure Calls. </title> <type> PhD thesis, </type> <institution> University of Paris VI (I.P.), </institution> <year> 1984. </year>
Reference-contexts: Interprocedural analysis, if performed at all, was ordinarily done to strengthen dependence analysis of loops, to infer aliasing among parameters, and to fold constants across procedure boundaries, but seldom with the goal of extracting high-level parallelism from a Fortran program. (There are exceptions; see <ref> [Tri84, BC86] </ref>.) For this reason, such compilers are typically quite capable of extracting fine-grained parallelism, but do less well in extracting large-grained, high-level parallelism.
References-found: 11

