URL: http://wwwipd.ira.uka.de/~prechelt/Biblio/wcpc95.ps.gz
Refering-URL: http://wwwipd.ira.uka.de/~hopp/cupit.html
Root-URL: 
Title: Data Locality and Load Balancing for Parallel Neural Network Learning  
Author: Lutz Prechelt 
Keyword: compiler optimizations, high level language, portable machine-independent parallel programming, irregular problems, dynamic data structures, communication optimization  
Address: 76128 Karlsruhe, Germany  
Affiliation: Fakultat Informatik Universitat Karlsruhe  
Email: (prechelt@ira.uka.de)  
Phone: +49/721/608-4068, Fax: +49/721/694092  
Date: April 9, 1995  
Abstract: Compilers for neural network learning algorithms can achieve near-optimal co-locality of data and processes and near-optimal balancing of load over processors for irregular problems. This is impossible for general programs, but restricting programs to that particular problem domain allows for the exploitation of domain-specific properties: The operations performed by neural algorithms are broadcasts, reductions, and object-local operations only; the load distribution is regular with respect to the (perhaps irregular) network topology; changes of network topology occur only from time to time. Compilation techniques and a compiler implementation for the MasPar MP-1 is described and quantitative results for the effects of various optimizations used in the compiler are given. Experiments with weight pruning algorithms yielded speedups of 28% due to load balancing, and of 195% due to data locality. Two other optimizations, connection allocation and selecting the number of replicates, speed programs up by about 50% or 100%, respectively.
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> J.A. Anderson and E. Rosenfeld, editors. Neurocomputing: </editor> <booktitle> Foundations of Research. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference: [2] <author> Guy E. Blelloch, Siddhartha Chatterjee, Jonathan C. Hardwick, Jay Sipelstein, and Marco Zagha. </author> <title> Implementation of a nested data-parallel language. </title> <booktitle> Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), ACM SIGPLAN Notices, </booktitle> <volume> 28(7) </volume> <pages> 102-111, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: On the other hand, this array-based implementation has a better memory utilization and always has perfectly balanced load. This approach to irregular problems is used by languages such as NESL <ref> [2] </ref>. In order to estimate the impact of such array-based implementations on performance, the compiler was instrumented to generate code that simulated no connection object data locality for connection operations (but still avoided parameter broadcast).
Reference: [3] <author> Siddhartha Chatterjee, John R. Gilbert, Robert Schreiber, and Shang-Hua Teng. </author> <title> Automatic array alignment in data parallel programs. </title> <booktitle> In Proc. 20th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL), </booktitle> <pages> pages 16-28, </pages> <address> Charleston, SC, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: then describe CuPit's view of neural networks and neural algorithms, the approach used to achieve data locality and load balancing, some implementation details of a compiler prototype, and the results obtained with the prototype. 2 Related work Data locality can be optimized statically for languages with array-based data parallelism, e.g. <ref> [3, 12] </ref>. Index analysis is used to compute a good data distribution for programs using mostly linear index expressions, which is the most frequent case.
Reference: [4] <author> William Finnoff, Ferdinand Hergert, and Hans Georg Zimmermann. </author> <title> Improving model selection by nonconvergent methods. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 771-783, </pages> <year> 1993. </year>
Reference-contexts: Which connections to remove can be decided in different ways, for instance based on weight (idea: small weights are probably not too important) or based on a statistical measure of significance of being non-zero. The latter method was used in the experiments. See <ref> [4] </ref> for a detailed description. Training started with 4-layer networks with 20+20 hidden nodes and all possible feed forward connections, including all shortcut connections.
Reference: [5] <author> Susan Flynn Hummel, Edith Schonberg, and Lawrence E. Flynn. </author> <title> Factoring | a method for scheduling parallel loops. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 90-101, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: For load balancing (in the context of data parallel programming sometimes called loop scheduling), there are two radically different approaches. Dynamic load balancing is the general approach: Work is distributed as necessary during a parallel section. A variety of methods have been proposed, see <ref> [5] </ref> for an overview. For highly irregular problems with unpredictable run time of the parts, only dynamic methods can guarantee satisfactory balance.
Reference: [6] <author> Robert W. Gray, Vincent P. Heuring, Steven P. Levi, Anthony M. Sloane, and William M. Waite. Eli: </author> <title> A complete, flexible compiler construction system. </title> <journal> Communications of the ACM, </journal> <volume> 35(2) </volume> <pages> 121-131, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Since neural algorithms need a mix of these communication modes, we find a cost on the order of 5 to 20 floating point multiplications for each floating point communication. The CuPit compiler was implemented using the Eli compiler construction system <ref> [6] </ref> and generates MPL code, MasPar's data parallel variant of C. The source code of the compiler is available as a literate programming document [15].
Reference: [7] <author> Bruce Hendrickson and Robert Leland. </author> <note> The Chaco user's guide, version 1.0. </note> <institution> UC-405 SAND93-2339, Sandia National Laboratories, </institution> <address> Albuquerque, NM 87185, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: A similar assumption is used here for neural algorithms. A class of methods trying to solve the data locality and load balancing problem at once is based on graph partitioning; <ref> [7] </ref> gives a good overview and references. These methods assume that the program's communication and computation graph is known in advance. <p> This effect of load balancing is less than 2% weaker than for the normal MasPar implementation. Hence, even for machines that have or simulate zero latency, performance gains due to load balancing will be significant. Further experiments (performed using the the Chaco <ref> [7] </ref> program) showed that using graph partitioning methods to compute the data distributions would not pay off: After pruning two thirds of all connections they could increase remote connection locality only by about 10%, which is not enough to amortize the significant run time they consume on each data redistribution (even
Reference: [8] <author> Christian Jacob and Peter Wilke. </author> <title> A distributed network simulation environment for multiprocessing systems. </title> <booktitle> In Proc. Int. Joint Conf. on Neural Networks (IJCNN), </booktitle> <pages> pages 1178-1183, </pages> <address> Singapore, </address> <year> 1991. </year>
Reference-contexts: mostly concerned with either highly optimized implementations of individual neural algorithms, usually assuming regular neural network topologies (e.g. [9] and references therein), mapping of more general static neural networks to high-latency parallel machines (e.g. [18] and references therein, [17] is a bibliography), or very coarse-grained approaches on workstation clusters (e.g. <ref> [8] </ref>). 3 What is a neural network? Let us define neural networks and neural algorithms (neural network training algorithms) as suited to our needs. Since we are concerned with compilation, the description uses programming language terminology.
Reference: [9] <author> Xiao Liu and George L. Wilcox. </author> <title> Benchmarking of the CM-5 and the Cray machines with a very large backpropagation neural network. </title> <type> Technical Report 93/38, </type> <institution> University of Minnesota Supercomputer Institute, Minneapolis, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: I am not aware of research for optimizing simulations of neural networks with dynamically changing topologies using fine-grained parallelism. Current work is mostly concerned with either highly optimized implementations of individual neural algorithms, usually assuming regular neural network topologies (e.g. <ref> [9] </ref> and references therein), mapping of more general static neural networks to high-latency parallel machines (e.g. [18] and references therein, [17] is a bibliography), or very coarse-grained approaches on workstation clusters (e.g. [8]). 3 What is a neural network? Let us define neural networks and neural algorithms (neural network training algorithms)
Reference: [10] <author> Warren McCulloch and Walter Pitts. </author> <title> A logical calculus of ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5 </volume> <pages> 115-133, </pages> <year> 1943. </year> <note> Reprinted in [1]. </note>
Reference-contexts: In my research, I have used the latter approach, not because it is better, but because it is much easier to implement. The neural network programming language designed for this work is called CuPit, after Warren McCulloch and Walter Pitts who first described a formal neuron in 1943 <ref> [10] </ref>. For a description of CuPit see [13].
Reference: [11] <author> Michael Philippsen, Ernst A. Heinz, and Paul Lukowicz. </author> <title> Compiling machine-independent parallel programs. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 28(8) </volume> <pages> 99-108, </pages> <month> August </month> <year> 1993. </year> <note> Also as report 14/93, </note> <institution> Fakultat fur Informatik, Universitat Karlsruhe. </institution> <month> 16 </month>
Reference-contexts: For this purpose, I compared the run time of a CuPit program to an equivalent Modula-2 fl program. The latter was translated by a compiler that also targets the MasPar and that is known to generate efficient code <ref> [11] </ref>. The problem chosen was backpropagation using the RPROP learning rule [16] for a fully connected 3-layer feed forward network.
Reference: [12] <author> Michael Philippsen and Markus U. Mock. </author> <title> Data and process alignment in Modula-2 fl . In AP '93, Int. Workshop on Automatic Distributed Memory Parallelization, Automatic Data Distribution, </title> <booktitle> and Automatic Parallel Performance Prediction, </booktitle> <pages> pages 141-149, </pages> <address> Saarbrucken, Germany, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: then describe CuPit's view of neural networks and neural algorithms, the approach used to achieve data locality and load balancing, some implementation details of a compiler prototype, and the results obtained with the prototype. 2 Related work Data locality can be optimized statically for languages with array-based data parallelism, e.g. <ref> [3, 12] </ref>. Index analysis is used to compute a good data distribution for programs using mostly linear index expressions, which is the most frequent case.
Reference: [13] <author> Lutz Prechelt. </author> <title> CuPit | a parallel language for neural algorithms: Language reference and tutorial. </title> <type> Technical Report 4/94, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, Germany, </institution> <month> January </month> <year> 1994. </year> <note> Anonymous FTP: /pub/papers/techreports/1994/1994-4.ps.Z on ftp.ira.uka.de. </note>
Reference-contexts: The neural network programming language designed for this work is called CuPit, after Warren McCulloch and Walter Pitts who first described a formal neuron in 1943 [10]. For a description of CuPit see <ref> [13] </ref>.
Reference: [14] <author> Lutz Prechelt. </author> <title> PROBEN1 | A set of benchmarks and benchmarking rules for neural network training algorithms. </title> <type> Technical Report 21/94, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, Germany, </institution> <month> September </month> <year> 1994. </year> <note> Anonymous FTP: /pub/papers/techreports/1994/1994-21.ps.Z on ftp.ira.uka.de. </note>
Reference-contexts: The higher the network irregularity, the more performance can be gained by load balancing. For the experiments reported here, 11 real training problems from 10 different domains were used, all taken from the PROBEN1 benchmark set <ref> [14] </ref>. The name and size of each problem is given in the first four columns of figure 5. The actual datasets for all these problems contain twice as many training examples as indicated in the table.
Reference: [15] <author> Lutz Prechelt. </author> <title> The CuPit compiler for the MasPar | a literate programming document. </title> <type> Technical Report 1/95, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, Germany, </institution> <month> January </month> <year> 1995. </year> <note> Anonymous FTP: /pub/papers/techreports/1995/1995-1.ps.Z on ftp.ira.uka.de. </note>
Reference-contexts: The CuPit compiler was implemented using the Eli compiler construction system [6] and generates MPL code, MasPar's data parallel variant of C. The source code of the compiler is available as a literate programming document <ref> [15] </ref>. Some details of the implementation and a data distribution example are given in the following sections. 5.1 Networks Network replicates are created only on request from a user program. While network replicates exist, no changes in network topology are allowed in CuPit.
Reference: [16] <author> Martin Riedmiller and Heinrich Braun. </author> <title> A direct adaptive method for faster backpropagation learning: The RPROP algorithm. </title> <booktitle> In Proc. of the IEEE Int. Conf. on Neural Networks, </booktitle> <address> San Francisco, CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: For this purpose, I compared the run time of a CuPit program to an equivalent Modula-2 fl program. The latter was translated by a compiler that also targets the MasPar and that is known to generate efficient code [11]. The problem chosen was backpropagation using the RPROP learning rule <ref> [16] </ref> for a fully connected 3-layer feed forward network.
Reference: [17] <author> Tom Tollenaere. </author> <title> Neural network simulations on transputers. </title> <type> Technical Report TR 91 0021, Version 1, </type> <institution> Katholieke Universiteit te Leuven, Leuven, Belgium, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Current work is mostly concerned with either highly optimized implementations of individual neural algorithms, usually assuming regular neural network topologies (e.g. [9] and references therein), mapping of more general static neural networks to high-latency parallel machines (e.g. [18] and references therein, <ref> [17] </ref> is a bibliography), or very coarse-grained approaches on workstation clusters (e.g. [8]). 3 What is a neural network? Let us define neural networks and neural algorithms (neural network training algorithms) as suited to our needs. Since we are concerned with compilation, the description uses programming language terminology.
Reference: [18] <author> Tom Tollenaere and Guy A. Orban. </author> <title> Decomposition and mapping of locally connected layered neural networks on message-passing multiprocessors. </title> <booktitle> Parallel Algorithms and Applications, </booktitle> <volume> 1 </volume> <pages> 43-56, </pages> <year> 1993. </year>
Reference-contexts: Current work is mostly concerned with either highly optimized implementations of individual neural algorithms, usually assuming regular neural network topologies (e.g. [9] and references therein), mapping of more general static neural networks to high-latency parallel machines (e.g. <ref> [18] </ref> and references therein, [17] is a bibliography), or very coarse-grained approaches on workstation clusters (e.g. [8]). 3 What is a neural network? Let us define neural networks and neural algorithms (neural network training algorithms) as suited to our needs. <p> There are two reasons for not doing this: First, little such locality can be obtained in neural networks, since their topology typically exhibits almost no clustering of connections. (An exception are modular neural networks, for which a "locality preference" for the connections holds, e.g. <ref> [18] </ref>). With only small gains in locality, the high running time of the optimization computation takes too long to amortize. Second, arrangement of nodes for optimal extra-object locality interferes with arrangement of nodes for minimum waste of processors within a segment.
References-found: 18

