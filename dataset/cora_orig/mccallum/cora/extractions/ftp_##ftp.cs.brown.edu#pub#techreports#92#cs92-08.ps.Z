URL: ftp://ftp.cs.brown.edu/pub/techreports/92/cs92-08.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-92-08.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [AgV] <author> A. Aggarwal and J. S. Vitter, </author> <title> "The Input/Output Complexity of Sorting and Related Problems," </title> <journal> Communications of the ACM (September 1988), </journal> <pages> 1116-1127. </pages>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel [GHK, GiS, Jil, Mag, PGK, Uni]. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter <ref> [AgV] </ref>. In their model, they considered the parameters N = # records in the file M = # records that can fit in internal memory B = # records per block D = # blocks transferred per I=O (# of disks) where M &lt; N , and 1 DB M=2. <p> Thus, D blocks can be transferred per I/O, as in the <ref> [AgV] </ref> model, but only if no two blocks access the same disk. This assumption is very reasonable in light of the way real systems are constructed. The measure of performance is the number of parallel I/Os required; internal computation time is ignored. <p> Vitter and Shriver presented a randomized version of distribution sort using two complementary partitioning techniques. Their algorithm meets the I/O lower bound (1) for the more lenient model of <ref> [AgV] </ref>, and thus the algorithm is optimal. The randomization was used to distribute each of the buckets evenly over the D disks so that they could be read efficiently with parallel read operations. <p> Theorem 5 The algorithm given is optimal for sorting in the parallel disk model. Proof : The preceding analysis shows that the algorithm achieves the same time bound as was shown in <ref> [AgV] </ref> to be a lower bound for sorting with parallel block transfer. 2 3.3 Memory usage of the sorting algorithm One aspect of the algorithm that we still need to show is that we do not exceed the memory requirements of the computer.
Reference: [BFP] <author> M. Blum, R. Floyd, V. Pratt, R. Rivest, and R. E. Tarjan, </author> <title> "Time Bounds for Selection," </title> <editor> J. </editor> <booktitle> Computer and System Sciences 7 (1973), </booktitle> <pages> 448-461. </pages>
Reference-contexts: t := djS 0 j=M e Read M elements in parallel (dM=DBe tracks; the last may be partial) Sort internally R i := the median element if t = 1 return (kth element) s := the median of R 1 ; . . . ; R t using algorithm from <ref> [BFP] </ref> Partition into two sets S 1 and S 2 such that S 1 &lt; s and S 2 s k 1 := jS 1 j if k k 1 return (Select (S 1 ; k)) else return (Select (S 2 ; k k 1 )) Algorithm 7 [ComputePartitionElts (S) returns
Reference: [CGK] <author> P. Chen, G. Gibson, R. H. Katz, D. A. Patterson, and M. Schulze, </author> <title> "Two Papers on RAIDs," </title> <editor> U. C. </editor> <address> Berkeley, UCB/CSD 88/479, </address> <month> December </month> <year> 1988. </year>
Reference-contexts: Balance Sort can be implemented using only striped writes. This restriction is important in practical parallel disk systems, where striped writes may be required for maintaining redundancy information to make recovery possible in the event of a disk failure <ref> [CGK, PGK, Sch] </ref>. Section 2 describes how to balance the buckets in O (N=DB) parallel I/Os in such a way that each bucket can be read efficiently in parallel at a later stage in the recursion The balancing subroutine is the heart of the overall sorting algorithm.
Reference: [CyP] <author> R. Cypher and C. G. Plaxton, </author> <title> "Deterministic Sorting in Nearly Logarithmic Time on the Hypercube and Related Computers," </title> <note> Journal of Computer and System Sciences (to appear). </note>
Reference-contexts: The adjustments necessary to obtain optimal performance are the subject of the companion paper [NoVa]. 19 An interesting sorting algorithm in the context of fixed interconnection net-works was proposed recently by Cypher and Plaxton <ref> [CyP] </ref>. The algorithm runs in O (log n (log log n) 2 ) time on an n-processor hypercube, shu*e-exchange, or cube-connected cycles network. We are currently exploring whether their techniques would be applicable to our I/O model, and vice versa whether our balancing techniques would yield good sorting algorithms for networks.
Reference: [Flo] <author> R. W. Floyd, </author> <title> "Permuting Information in Idealized Two-Level Storage," in Complexity of Computer Computations, </title> <editor> R. Miller and J. Thatcher, ed., </editor> <publisher> Plenum, </publisher> <year> 1972, </year> <pages> 105-109. </pages>
Reference-contexts: In each I/O, D blocks of B records can be transferred simultaneously, as illustrated in Figure 1. This model generalized the initial work on I/O of Floyd <ref> [Flo] </ref> and Hong and Kung [HoK].
Reference: [GHK] <author> G. Gibson, L. Hellerstein, R. M. Karp, R. H. Katz, and D. A. Patterson, </author> <title> "Coding Techniques for Handling Failures in Large Disk Arrays," </title> <editor> U. C. </editor> <address> Berkeley, UCB/CSD 88/477, </address> <month> December </month> <year> 1988. </year>
Reference-contexts: This approach takes advantage of the fact that the seek time is usually much longer than the time needed for transferring a record of data once the disk read/write head is positioned. An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter [AgV].
Reference: [GiS] <author> D. Gifford and A. Spector, </author> <title> "The TWA Reservation System," </title> <booktitle> Communications of the ACM 27 (July 1984), </booktitle> <pages> 650-665. </pages>
Reference-contexts: This approach takes advantage of the fact that the seek time is usually much longer than the time needed for transferring a record of data once the disk read/write head is positioned. An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter [AgV].
Reference: [HoK] <author> J. W. Hong and H. T. Kung, </author> <title> "I/O Complexity: The Red-Blue Pebble Game," </title> <booktitle> Proc. of the 13th Annual ACM Symposium on the Theory of Computing (May 1981), </booktitle> <pages> 326-333. </pages>
Reference-contexts: In each I/O, D blocks of B records can be transferred simultaneously, as illustrated in Figure 1. This model generalized the initial work on I/O of Floyd [Flo] and Hong and Kung <ref> [HoK] </ref>.
Reference: [IsS] <author> A. Israeli and Y. Shiloach, </author> <title> "An Improved Parallel Algorithm for Maximal Matching," </title> <booktitle> Information Processing Letters 22 (1986), </booktitle> <pages> 57-60. </pages>
Reference-contexts: Likewise, it is not hard to show that any maximal matching in the Rebalance routine is also a maximum matching. Thus, we can use maximal matching in both the Balance and Rebalance routines. Unfortunately, the best known deterministic parallel time for maximal matching is O (log 3 P ) <ref> [IsS] </ref>, which is not good enough to get optimal performance on the parallel memory hierarchies with effective logarithmic cost functions.
Reference: [Jil] <author> W. Jilke, </author> <title> "Disk Array Mass Storage Systems: The New Opportunity," </title> <publisher> Amperif Corporation, </publisher> <month> September </month> <year> 1986. </year>
Reference-contexts: This approach takes advantage of the fact that the seek time is usually much longer than the time needed for transferring a record of data once the disk read/write head is positioned. An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter [AgV].
Reference: [Lei] <author> T. Leighton, </author> <title> "Tight Bounds on the Complexity of Parallel Sorting," </title> <journal> IEEE Transactions on Computers C-34 (April 1985), </journal> <pages> 344-354, </pages> <booktitle> also appears in Proceedings of the 16th Annual ACM Symposium on Theory of Computing, </booktitle> <month> (April </month> <year> 1983), </year> <pages> 71-80. </pages>
Reference-contexts: This problem would have had to have been solved before Greed Sort could have had any applicability to these parallel memory hierarchies. 2. The hidden constants in the big-oh notation are small. The problem with the Greed Sort algorithm of [NoVb] is that it uses Columnsort <ref> [Lei] </ref> as a subroutine, which introduces at least an additional factor of 4 into the constant of proportionality. 3. The algorithm can operate using only striped write operations.
Reference: [Mag] <author> N. B. Maginnis, </author> <title> "Store More, Spend Less: Mid-Range Options Around," Com-puterworld (November 16, </title> <booktitle> 1986), </booktitle> <volume> 71. </volume> <pages> 20 </pages>
Reference-contexts: This approach takes advantage of the fact that the seek time is usually much longer than the time needed for transferring a record of data once the disk read/write head is positioned. An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter [AgV].
Reference: [NoVa] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Optimal Deterministic Sorting on Parallel Memory Hierarchies," </title> <institution> Department of Computer Science, Brown University, CS-92-38, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Balance Sort can be used as the basis of optimal deterministic algorithms for all defined parallel memory hierarchies, as described in the companion paper <ref> [NoVa] </ref>. By contrast, Greed Sort is based on merge sort, and in almost all cases there are no known optimal sorting methods for hierarchical memory based on merge sort, even when there is only a single hierarchy. 2. Balance Sort has smaller constants embedded in the big-oh notation. 3. <p> This algorithm has the following advantages over previously known deterministic algorithms: 1. The new algorithm is also applicable to parallel memory hierarchies, as described in the companion paper <ref> [NoVa] </ref>. The difficulty with Greed Sort is that it was based on merge sort, and in many cases of hierarchical memories, there is no known way to make merge sort work optimally on even a single memory hierarchy. <p> The adjustments necessary to obtain optimal performance are the subject of the companion paper <ref> [NoVa] </ref>. 19 An interesting sorting algorithm in the context of fixed interconnection net-works was proposed recently by Cypher and Plaxton [CyP]. The algorithm runs in O (log n (log log n) 2 ) time on an n-processor hypercube, shu*e-exchange, or cube-connected cycles network.
Reference: [NoVb] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Greed Sort: An Optimal External Sorting Algorithm for Multiple Disks," </title> <institution> Brown University, CS-90-04, </institution> <month> February </month> <year> 1990, </year> <title> also appears in shortened form in "Large-Scale Sorting in Parallel Memories," </title> <booktitle> Proc. 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Hilton Head, SC (July 1991), </address> <pages> 29-39. </pages>
Reference-contexts: They posed as an open problem the question of whether there is an optimal algorithm that is deterministic. That question was answered in the affirmative by Nodine and Vitter using an algorithm called Greed Sort <ref> [NoVb] </ref>. That algorithm was based on merge sort, and was therefore not applicable to finding an optimal deterministic algorithm for parallel memory hierarchies, since there is no known way of using merge sort optimally on even a single memory hierarchy. <p> It provides a more practical and yet deterministic alternative to the optimal randomized algorithm of Vitter and Shriver [ViSa]. Previously there was one known optimal deterministic method for parallel disk sorting, called Greed Sort <ref> [NoVb] </ref>, but Balance Sort has several important advantages over Greed Sort: 1. Balance Sort can be used as the basis of optimal deterministic algorithms for all defined parallel memory hierarchies, as described in the companion paper [NoVa]. <p> At first glance, it seems if D = (M) and B = 1 that we will require (M 3=2 ) storage space in primary memory, which is clearly impossible. However, we can use a partial disk striping method throughout the course of the algorithm, as described in <ref> [NoVb] </ref>. Assume that D grows faster than M fi , where 0 &lt; fi &lt; 1=2. We can cluster our D disks into clusters of D 0 = M fi clusters of B 0 = BD=D 0 disks synchronized together. <p> This problem would have had to have been solved before Greed Sort could have had any applicability to these parallel memory hierarchies. 2. The hidden constants in the big-oh notation are small. The problem with the Greed Sort algorithm of <ref> [NoVb] </ref> is that it uses Columnsort [Lei] as a subroutine, which introduces at least an additional factor of 4 into the constant of proportionality. 3. The algorithm can operate using only striped write operations.
Reference: [PGK] <author> D. A. Patterson, G. Gibson, and R. H. Katz, </author> <title> "A Case for Redundant Arrays of Inexpensive Disks (RAID)," </title> <booktitle> Proceedings ACM SIGMOD Conference (June 1988), </booktitle> <pages> 109-116. </pages>
Reference-contexts: This approach takes advantage of the fact that the seek time is usually much longer than the time needed for transferring a record of data once the disk read/write head is positioned. An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter [AgV]. <p> Balance Sort can be implemented using only striped writes. This restriction is important in practical parallel disk systems, where striped writes may be required for maintaining redundancy information to make recovery possible in the event of a disk failure <ref> [CGK, PGK, Sch] </ref>. Section 2 describes how to balance the buckets in O (N=DB) parallel I/Os in such a way that each bucket can be read efficiently in parallel at a later stage in the recursion The balancing subroutine is the heart of the overall sorting algorithm.
Reference: [Sch] <author> M. E. Schulze, </author> <title> "Considerations in the Design of a RAID Prototype," </title> <editor> U. C. </editor> <address> Berkeley, UCB/CSD 88/448, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: Balance Sort can be implemented using only striped writes. This restriction is important in practical parallel disk systems, where striped writes may be required for maintaining redundancy information to make recovery possible in the event of a disk failure <ref> [CGK, PGK, Sch] </ref>. Section 2 describes how to balance the buckets in O (N=DB) parallel I/Os in such a way that each bucket can be read efficiently in parallel at a later stage in the recursion The balancing subroutine is the heart of the overall sorting algorithm.
Reference: [Uni] <institution> University of California at Berkeley, "Massive Information Storage, Management, and Use (NSF Institutional Infrastructure Proposal)," </institution> <note> Technical Report No. UCB/CSD 89/493, </note> <month> January </month> <year> 1989. </year>
Reference-contexts: This approach takes advantage of the fact that the seek time is usually much longer than the time needed for transferring a record of data once the disk read/write head is positioned. An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter [AgV].
Reference: [ViSa] <author> J. S. Vitter and E. A. M. Shriver, </author> <title> "Optimal Disk I/O with Parallel Block Transfer," </title> <booktitle> Proceedings of the 22nd Annual ACM Symposium on Theory of Computing (May 1990), </booktitle> <pages> 159-169. </pages>
Reference-contexts: They gave two algorithms, a modified merge sort and a distribution sort, that each achieved the optimal I/O bounds. Vitter and Shriver considered a more realistic D-disk model, in which the secondary storage is partitioned into D physically distinct disk drives <ref> [ViSa] </ref>, as in Figure 2. (Note that each head of a multi-head drive could count as a distinct disk in this definition, as long as each could operate independently of the other heads on the drive.) In each I/O operation, each of the D disks can simultaneously transfer one block of <p> Balance Sort is optimal for parallel disk sorting, both in terms of the number of I/O steps and in terms of the amount of internal processing. It provides a more practical and yet deterministic alternative to the optimal randomized algorithm of Vitter and Shriver <ref> [ViSa] </ref>. Previously there was one known optimal deterministic method for parallel disk sorting, called Greed Sort [NoVb], but Balance Sort has several important advantages over Greed Sort: 1.
Reference: [ViSb] <author> J. S. Vitter and E. A. M. Shriver, </author> <title> "Algorithms for Parallel Memory I: Two-Level Memories," </title> <institution> Brown University, CS-90-21, </institution> <month> September </month> <year> 1990. </year> <month> 21 </month>
Reference-contexts: The next few subsections establish the proof of this theorem. 3.1 Finding the partition elements The following procedure for finding the partition elements was presented in <ref> [ViSb] </ref>, and is reproduced here for convenience in Algorithm 7. The algorithm is deterministic and guarantees to find S 1 partition elements such that, for any bucket b, the number of elements in that bucket N b obeys the constraint N N b 2S The algorithm was analyzed in [ViSb] and <p> in <ref> [ViSb] </ref>, and is reproduced here for convenience in Algorithm 7. The algorithm is deterministic and guarantees to find S 1 partition elements such that, for any bucket b, the number of elements in that bucket N b obeys the constraint N N b 2S The algorithm was analyzed in [ViSb] and shown to take O (N=DB) parallel I/O operations which, as we will see in the analysis, is sufficient for achieving optimal performance. The procedure for finding the partitioning elements uses as a subroutine Algorithm 6, which computes the kth smallest of n elements in O (n=DB) I/Os. <p> Let T (N ) denote the amount of time needed for sorting N records. Steps (1) and (2) occur only at the innermost level of recursion. From Theorem 2, the bucket will take no more than 3N=DB reads and writes. Step (3), as mentioned, has been shown by <ref> [ViSb] </ref> to require only O (N=DB) I/Os, regardless of whether S = 2 q M=B or S = 2N=M . Step (4), as shown in Theorem 3, requires no more than 6N=DB I/Os. Step (5) requires writing a set of cardinality DS, which can be done in S=B parallel writes.
References-found: 19

