URL: http://www.cs.rice.edu/~willy/papers/asplos96.ps.gz
Refering-URL: http://www.cs.rice.edu/~willy/TreadMarks/papers.html
Root-URL: 
Email: e-mail: fsandhya, alc, willyg@cs.rice.edu  
Title: An Integrated Compile-Time/Run-Time Software Distributed Shared Memory System compile-time run-time system is 0-29% slower than
Author: Sandhya Dwarkadas, Alan L. Cox, and Willy Zwaenepoel 
Note: mentations of the same applications, the  
Affiliation: Department of Computer Science Rice University  
Abstract: On a distributed memory machine, hand-coded message passing leads to the most efficient execution, but it is difficult to use. Parallelizing compilers can approach the performance of hand-coded message passing by translating data-parallel programs into message passing programs, but efficient execution is limited to those programs for which precise analysis can be carried out. Shared memory is easier to program than message passing and its domain is not constrained by the limitations of parallelizing compilers, but it lags in performance. Our goal is to close that performance gap while retaining the benefits of shared memory. In other words, our goal is (1) to make shared memory as efficient as message passing, whether hand-coded or compiler-generated, (2) to retain its ease of programming, and (3) to retain the broader class of applications it supports. To this end we have designed and implemented an integrated compile-time and run-time software DSM system. The programming model remains identical to the original pure run-time DSM system. No user intervention is required to obtain the benefits of our system. The compiler computes data access patterns for the individual processors. It then performs a source-to-source transformation, inserting in the program calls to inform the run-time system of the computed data access patterns. The run-time system uses this information to aggregate communication, to aggregate data and synchronization into a single message, to eliminate consistency overhead, and to replace global synchronization with point-to-point synchronization wherever possible. We extended the Parascope programming environment to perform the required analysis, and we augmented the TreadMarks run-time DSM library to take advantage of the analysis. We used six Fortran programs to assess the performance benefits: Jacobi, 3D-FFT, Integer Sort, Shallow, Gauss, and Modified Gramm-Schmidt, each with two different data set sizes. The experiments were run on an 8-node IBM SP/2 using user-space communication. Compiler optimization in conjunction with the augmented run-time system achieves substantial execution time improvements in comparison to the base TreadMarks, ranging from 4% to 59% on 8 processors. Relative to message passing imple 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. P. Amarasinghe et al. </author> <title> The SUIF compiler for scalable parallel machines. </title> <booktitle> In Proceedings of the 7th SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: For statement p 1 , the assignment to a (i,j), F prec (p 1 ) contains b2, and S succ (p 1 ) contains b1. For each reference to the array b in the righthand side, a section with a freadg tag is constructed, of the form <ref> [1; M 2 : begin; end] </ref>, [3; M : begin; end], [2; M 1 : begin 1; end 1], and [2; M 1 : begin + 1; end + 1] respectively. <p> Each of these sections is added to the union of the sections for b2, resulting in a final section with a freadg tag and of the form <ref> [1; M : begin 1; end + 1] </ref>. For p 2 , the assignment to b (i,j), F prec (p 2 ) contains b1, and F succ (p 2 ) contains b2. <p> For p 2 , the assignment to b (i,j), F prec (p 2 ) contains b1, and F succ (p 2 ) contains b2. The assignment to the array causes a section with a fwriteg tag of the form <ref> [1; M : begin; end] </ref> to be constructed. Going next to the transformation phase, for b2, the conditions for a Push are satisfied, and b2 is replaced by a Push. <p> Careful optimization to minimize data movement by improving locality is performed, and data and work is distributed according to the owner computes rule. At the other end, compilers such as SUIF <ref> [1] </ref> parallelize directly to shared memory and do not take advantage of bulk transfer capabilities. This work attempts to bridge the gap by providing the flexibility of shared memory while taking advantage of bulk transfer.
Reference: [2] <author> C. Amza et al. TreadMarks: </author> <title> Shared memory computing on networks of workstations. </title> <booktitle> IEEE Computer, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: We extended the Parascope parallel programming environment [18] to analyze and transform explicitly parallel programs. We also extended the interface to the Tread-Marks run-time DSM system <ref> [2] </ref> to take advantage of the compiler analysis. We have measured the performance of these techniques on an 8-node IBM SP/2 for six Fortran applications: Jacobi, 3D-FFT, Integer Sort, Shallow, Gauss, and Modified Gramm-Schmidt, with two data sets for each application. <p> Various techniques have been used to optimize the performance of such DSM systems. To make this discussion specific, we describe the techniques used in TreadMarks <ref> [2] </ref>. TreadMarks uses lazy release consistency [17] to reduce communication and consistency overhead. Lazy release consistency delays consistency-related communication until the time of an acquire synchronization operation [10]. <p> Asynchronous communication is likely to outperform synchronous communication if there is some computation between the fetch and the first access to the shared data. 3.3 Implementation and Limitations The implementation of the interface was done in conjunction with TreadMarks <ref> [2] </ref>. Although in the above we specified the parameters to the augmented run-time system calls as sections, this is done for ease of explanation only. To reduce run-time overhead, in the actual implementation these section parameters are translated by the compiler into a set of contiguous address ranges. <p> For each reference to the array b in the righthand side, a section with a freadg tag is constructed, of the form [1; M 2 : begin; end], [3; M : begin; end], <ref> [2; M 1 : begin 1; end 1] </ref>, and [2; M 1 : begin + 1; end + 1] respectively. <p> For each reference to the array b in the righthand side, a section with a freadg tag is constructed, of the form [1; M 2 : begin; end], [3; M : begin; end], [2; M 1 : begin 1; end 1], and <ref> [2; M 1 : begin + 1; end + 1] </ref> respectively. Each of these sections is added to the union of the sections for b2, resulting in a final section with a freadg tag and of the form [1; M : begin 1; end + 1].
Reference: [3] <author> Applied Parallel Research. </author> <title> FORGE High Performance Fortran User's Guide, </title> <note> version 2.0. </note>
Reference-contexts: For each reference to the array b in the righthand side, a section with a freadg tag is constructed, of the form [1; M 2 : begin; end], <ref> [3; M : begin; end] </ref>, [2; M 1 : begin 1; end 1], and [2; M 1 : begin + 1; end + 1] respectively. <p> We present the performance of these applications in four different versions: 1. The base TreadMarks program executing with the base TreadMarks run-time system. 2. The compiler-optimized TreadMarks program executing with the augmented TreadMarks run-time system. 3. A message passing version automatically generated by the Forge XHPF compiler <ref> [3] </ref> from Applied Parallel Research, Inc. (APR). The results for the XHPF compiler are provided in order to compare performance against a commercial parallelizing compiler for data parallel programs. 4. A hand-coded PVMe message passing [9] version. <p> barriers with a Push are only useful when the data to be communicated is small relative to the overhead of the barrier messages or when there is false sharing. 7 Related Work Research and commercial compilers for distributed memory machines have to date targeted the underlying message passing layer directly <ref> [3, 14] </ref>. Careful optimization to minimize data movement by improving locality is performed, and data and work is distributed according to the owner computes rule. At the other end, compilers such as SUIF [1] parallelize directly to shared memory and do not take advantage of bulk transfer capabilities.
Reference: [4] <author> D. Bailey et al. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report 103863, </type> <institution> NASA, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: As will be seen in Section 6 the performance of this strategy and ours are very similar. Our methods can, however, also be applied to applications for which analysis cannot be made precise. The IS program from the NAS benchmarks <ref> [4] </ref>, discussed in more detail in Section 6, provides a good example. Here a large sub-array is passed between processors under the control of a lock. Our analysis creates a section for the sub-array and issues a Validate when the lock is acquired, resulting in significant performance improvement. <p> Interprocessor communication is accomplished over the IBM SP/2 high-performance two-level cross-bar switch. Unless indicated otherwise, all results are for 8-processor runs. We used six Fortran programs: IS and 3D-FFT from the NAS benchmark suite <ref> [4] </ref>, the Shallow benchmark, and Ja Application Data set size Time (secs) Jacobi - 4Kx4K 4096x4096 288.3 Jacobi - 1Kx1K 1024x1024 17.7 3D-FFT - 6x6x6 2 6 fi 2 6 fi 2 6 9.5 Shallow - 1Kx1K 1024x1024 74.8 Shallow - 1Kx.5K 1024x512 36.9 IS - 23-19 N = 2 23 <p> Second, there is a gain from using Push. With a smaller data set, the cost of the barrier becomes proportionally higher, and hence its elimination results in some improvement in running time (10%). 3D-FFT is the three-dimensional Fast Fourier Transform program from the NAS suite <ref> [4] </ref>. 3D-FFT numerically solves a certain partial differential equation using three dimensional forward and inverse FFTs. The phases of the program are separated by barriers, with a transpose between some of the phases to reduce the array traversal cost. Each processor is assigned a contiguous section of the array. <p> Integer Sort (IS) is another program from the NAS benchmark suite <ref> [4] </ref>. IS ranks an unsorted sequence of N keys. The rank of a key is the index value i that the key would have if the sequence of keys were sorted. All the keys are integers in the range [0, B max ] and the method used is bucket sort.
Reference: [5] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Orca: A language for parallel programming of distributed systems. </title> <type> IEEE-TSE, </type> <month> June </month> <year> 1992. </year>
Reference-contexts: We focus here on explicitly parallel systems that provide a load-store interface to memory, and that do not require annotations (e.g. [6]) or access to shared memory through object methods (e.g. <ref> [5] </ref>). Various techniques have been used to optimize the performance of such DSM systems. To make this discussion specific, we describe the techniques used in TreadMarks [2]. TreadMarks uses lazy release consistency [17] to reduce communication and consistency overhead.
Reference: [6] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In Proceedings of the '93 CompCon Conference, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: Finally, we survey related work in Section 7 and conclude in Section 8. 2 Motivation DSM provides a shared memory abstraction on distributed memory machines. We focus here on explicitly parallel systems that provide a load-store interface to memory, and that do not require annotations (e.g. <ref> [6] </ref>) or access to shared memory through object methods (e.g. [5]). Various techniques have been used to optimize the performance of such DSM systems. To make this discussion specific, we describe the techniques used in TreadMarks [2]. TreadMarks uses lazy release consistency [17] to reduce communication and consistency overhead.
Reference: [7] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Proceedings of ASPLOS-4, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Mowry et al. [23] discuss the design and evaluation of a compiler algorithm for prefetching. Their algorithm concentrates on improving the performance of cache-based systems and issues prefetch requests for data that are likely to incur a cache miss. Porterfield et al. <ref> [7] </ref> present an algorithm for inserting prefetches one loop iteration ahead. Gornish, Granston, and Veidenbaum [11] present an algorithm for de-termining the earliest time when it is safe to prefetch shared data. Our work differs in the granularity of information required, and takes advantage of the software-based consistency maintenance.
Reference: [8] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Techniques for reducing consistency-related information in distributed shared memory systems. </title> <journal> ACM TOCS, </journal> <month> August </month> <year> 1995. </year>
Reference-contexts: The write notices inform the faulting processor whom it needs to communicate with to get the necessary modifications to the page. Tread-Marks uses a multiple-writer protocol, retrieving diffs <ref> [8] </ref> at the time of an access miss rather than whole pages. Diffs are produced by the TreadMarks write detection mechanism. Initially, a page is write-protected. When a processor first writes to the page, it incurs a protection violation and Tread-Marks makes a twin, a copy of the unmodified page. <p> This diff is transmitted to the faulting processor and merged into its copy of the page. In addition to reducing communication, multiple writer protocols have the benefit of reducing false sharing overheads by allowing multiple concurrent writers <ref> [8] </ref>. Recent studies (e.g., [22]) have shown that, for relatively coarse-grained applications, software DSM provides good performance, although there still remains a sizable gap between the performance of DSM and message passing for some applications.
Reference: [9] <author> G.A. Geist and V.S. Sunderam. </author> <title> Network-based concurrent computing on the PVM system. </title> <journal> Concurrency: Practice and Experience, </journal> <month> June </month> <year> 1992. </year>
Reference-contexts: A message passing version automatically generated by the Forge XHPF compiler [3] from Applied Parallel Research, Inc. (APR). The results for the XHPF compiler are provided in order to compare performance against a commercial parallelizing compiler for data parallel programs. 4. A hand-coded PVMe message passing <ref> [9] </ref> version. The results for PVMe are included to estimate the best possible performance that can be achieved on this plat form. All four systems underneath use IBM's user-level Message Passing Library (MPL).
Reference: [10] <author> K. Gharachorloo et al. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of ISCA-17, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: To make this discussion specific, we describe the techniques used in TreadMarks [2]. TreadMarks uses lazy release consistency [17] to reduce communication and consistency overhead. Lazy release consistency delays consistency-related communication until the time of an acquire synchronization operation <ref> [10] </ref>. In Tread-Marks, which uses locks and barriers for synchronization, an acquire corresponds to a lock acquisition or to a departure from a barrier. At that time, the acquiring processor is informed by write notices of modifications to shared pages.
Reference: [11] <author> E. Gornish, E. Granston, and A. Veidenbaum. </author> <title> Compiler-directed data prefetching in multiprocessors with memory hierarchies. </title> <booktitle> In Proceedings of ICS-90, </booktitle> <year> 1990. </year>
Reference-contexts: Their algorithm concentrates on improving the performance of cache-based systems and issues prefetch requests for data that are likely to incur a cache miss. Porterfield et al. [7] present an algorithm for inserting prefetches one loop iteration ahead. Gornish, Granston, and Veidenbaum <ref> [11] </ref> present an algorithm for de-termining the earliest time when it is safe to prefetch shared data. Our work differs in the granularity of information required, and takes advantage of the software-based consistency maintenance. Our optimizations perform aggregation with a view to exploiting the explicit synchronization in relaxed consistency models.
Reference: [12] <author> E. Granston and H. Wijshoff. </author> <title> Managing pages in shared virtual memory systems: Getting the compiler into the game. </title> <booktitle> In Proceedings of ICS-93, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: We focus here on the first three performance issues; false sharing is not directly addressed in this paper. Compiler transformations to reduce false sharing in shared memory programs are discussed elsewhere <ref> [12, 16] </ref>. We illustrate the performance differences between message passing and DSM with a simple example, the Jacobi program. Jacobi is an iterative method for solving partial differential equations, with nearest-neighbor averaging as the main computation. A shared memory version of a parallel Jacobi appears in Figure 1.
Reference: [13] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interpro-cedural bounded regular section analysis. </title> <address> IEEE-TPDS, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: In summary, in the combined system, the run-time library remains the basic vehicle for implementing shared memory, while the compiler performs optimization rather than implementation. Our compiler starts from explicitly parallel shared memory programs written for lazy release consistency (LRC) [17]. We use regular section analysis <ref> [13] </ref> to determine the shared data access patterns between synchronization statements. The resulting regular section descriptors (RSDs) are used to identify opportunities for communication aggregation, consistency overhead elimination, merging synchronization and data messages, and replacing global with point-to-point synchronization. The paper presents the following contributions: 1. <p> - send request for diffs by timestamp on synchronization request - Apply_diffs ( Section ) Write_protect ( Section ) - receive diffs for all pages in Section disable write access for the page apply diffs to pages in Section - - the form of sections, or so called regular sections <ref> [13] </ref> (for a precise definition, see Section 4) 3.1.1 Validate Validate and its variant, Validate w sync, take two parameters: a section and the access pattern, access type, to that section. access type is one of READ, WRITE, READ&WRITE, WRITE ALL, or READ&WRITE ALL. <p> F includes the set S, but in addition includes conditional statements, and, in the absence of interprocedural analysis, procedure calls. Access analysis generates a summary of shared data accesses associated with each element of F , and the type of such accesses. Our main tool is regular section analysis <ref> [13] </ref>. Regular section descriptors (RSDs) are used as the representation to concisely provide information about array accesses in a loop nest. The RSDs represent the accessed data as linear expressions of the upper and lower loop bounds along each dimension, and include stride information.
Reference: [14] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> CACM, </journal> <month> August </month> <year> 1992. </year>
Reference-contexts: 1 Introduction A shared memory programming model for a distributed memory machine can be implemented either solely by run-time methods (e.g., [21]) or solely by compile-time methods (e.g., <ref> [14] </ref>). Distributed shared memory (DSM) run-time libraries dynamically detect shared memory accesses, and send messages accordingly to implement consistency. Compilers use static analysis of the shared memory access patterns to generate a message passing program. <p> barriers with a Push are only useful when the data to be communicated is small relative to the overhead of the barrier messages or when there is false sharing. 7 Related Work Research and commercial compilers for distributed memory machines have to date targeted the underlying message passing layer directly <ref> [3, 14] </ref>. Careful optimization to minimize data movement by improving locality is performed, and data and work is distributed according to the owner computes rule. At the other end, compilers such as SUIF [1] parallelize directly to shared memory and do not take advantage of bulk transfer capabilities.
Reference: [15] <author> T.E. Jeremiassen and S. Eggers. </author> <title> Computing per-process summary side-effect information. </title> <editor> In U. Banerjee et al., editors, </editor> <booktitle> Fifth Workshop on Languages and Compilers for Parallelism, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Our work differs in the granularity of information required, and takes advantage of the software-based consistency maintenance. Our optimizations perform aggregation with a view to exploiting the explicit synchronization in relaxed consistency models. Jeremiassen et al. <ref> [15] </ref> present a static algorithm for computing per-process memory references to shared data in coarse-grained parallel programs. This information is then used to determine cross-process memory references in order to direct the type of coherence protocol to use in a bus-based architecture.
Reference: [16] <author> T.E. Jeremiassen and S. Eggers. </author> <title> Reducing false sharing on shared memory multiprocessors through compile time data transformations. </title> <booktitle> In Proceedings of PPoPP-95, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: We focus here on the first three performance issues; false sharing is not directly addressed in this paper. Compiler transformations to reduce false sharing in shared memory programs are discussed elsewhere <ref> [12, 16] </ref>. We illustrate the performance differences between message passing and DSM with a simple example, the Jacobi program. Jacobi is an iterative method for solving partial differential equations, with nearest-neighbor averaging as the main computation. A shared memory version of a parallel Jacobi appears in Figure 1.
Reference: [17] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of ISCA-19, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: In summary, in the combined system, the run-time library remains the basic vehicle for implementing shared memory, while the compiler performs optimization rather than implementation. Our compiler starts from explicitly parallel shared memory programs written for lazy release consistency (LRC) <ref> [17] </ref>. We use regular section analysis [13] to determine the shared data access patterns between synchronization statements. The resulting regular section descriptors (RSDs) are used to identify opportunities for communication aggregation, consistency overhead elimination, merging synchronization and data messages, and replacing global with point-to-point synchronization. <p> Various techniques have been used to optimize the performance of such DSM systems. To make this discussion specific, we describe the techniques used in TreadMarks [2]. TreadMarks uses lazy release consistency <ref> [17] </ref> to reduce communication and consistency overhead. Lazy release consistency delays consistency-related communication until the time of an acquire synchronization operation [10]. In Tread-Marks, which uses locks and barriers for synchronization, an acquire corresponds to a lock acquisition or to a departure from a barrier. <p> The processor also includes the current vector timestamps <ref> [17] </ref> for the pages in the sections requested to allow other processors to determine what diffs it has and has not seen. These other processors then determine what diffs to communicate to the acquirer.
Reference: [18] <author> K. Kennedy, K. S. McKinley, and C. Tseng. </author> <title> Analysis and transformation in an interactive parallel programming tool. </title> <journal> Concurrency: Practice and Experience, </journal> <month> October </month> <year> 1993. </year>
Reference-contexts: A comparison of the performance of different run-time strategies for taking advantage of the data access patterns provided by the compiler, in particular, synchronous vs. asynchronous data fetching, and combin ing aggregation with synchronization. We extended the Parascope parallel programming environment <ref> [18] </ref> to analyze and transform explicitly parallel programs. We also extended the interface to the Tread-Marks run-time DSM system [2] to take advantage of the compiler analysis. <p> This information is not available at compile-time. The sequential program also has an indirect access to the main array. Hence, it is difficult to express this program in a data parallel style. 4.4 Implementation and Limitations We implemented the analysis using the Parascope paralleliz-ing environment <ref> [18] </ref>. We modified the Parascope analysis to work on explicitly parallel programs written for the release consistency model. We added passes to recognize synchronization calls, and to generate data access summaries at each of these calls. Our current framework does not perform inter-procedural analysis.
Reference: [19] <author> D. Kranz et al. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Proceedings of PPoPP-93, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: This work attempts to bridge the gap by providing the flexibility of shared memory while taking advantage of bulk transfer. Several recent proposals for hardware shared memory machines include a message passing subsystem designed in part to allow applications to take advantage of bulk data transfer <ref> [19, 20] </ref>. Woo et al. [24] evaluate one such design in the context of the Flash system. There are many differences between their work and ours.
Reference: [20] <author> J. Kuskin et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of ISCA-21, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: This work attempts to bridge the gap by providing the flexibility of shared memory while taking advantage of bulk transfer. Several recent proposals for hardware shared memory machines include a message passing subsystem designed in part to allow applications to take advantage of bulk data transfer <ref> [19, 20] </ref>. Woo et al. [24] evaluate one such design in the context of the Flash system. There are many differences between their work and ours.
Reference: [21] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM TOCS, </journal> <month> November </month> <year> 1989. </year>
Reference-contexts: 1 Introduction A shared memory programming model for a distributed memory machine can be implemented either solely by run-time methods (e.g., <ref> [21] </ref>) or solely by compile-time methods (e.g., [14]). Distributed shared memory (DSM) run-time libraries dynamically detect shared memory accesses, and send messages accordingly to implement consistency. Compilers use static analysis of the shared memory access patterns to generate a message passing program.
Reference: [22] <author> H. Lu et al. </author> <title> Message passing versus distributed shared memory on networks of workstations. </title> <booktitle> In Proceedings SuperComputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: For this more general model to be viable, however, it must provide performance competitive with that of parallelizing compilers for data parallel programs. This is exactly one of the areas in which explicitly parallel DSM programs have been lagging <ref> [22] </ref>. Our claim is that compiler support for explicitly parallel DSM programs can close this performance gap for data parallel programs, while the underlying DSM system still retains the advantage of being able to support a wider class of applications. <p> This diff is transmitted to the faulting processor and merged into its copy of the page. In addition to reducing communication, multiple writer protocols have the benefit of reducing false sharing overheads by allowing multiple concurrent writers [8]. Recent studies (e.g., <ref> [22] </ref>) have shown that, for relatively coarse-grained applications, software DSM provides good performance, although there still remains a sizable gap between the performance of DSM and message passing for some applications. In particular, in a comparison of PVM and TreadMarks on a network of workstations [22], a number of issues were <p> Recent studies (e.g., <ref> [22] </ref>) have shown that, for relatively coarse-grained applications, software DSM provides good performance, although there still remains a sizable gap between the performance of DSM and message passing for some applications. In particular, in a comparison of PVM and TreadMarks on a network of workstations [22], a number of issues were identified as contributing to the performance gap between TreadMarks and PVM: absence of bulk data transfer, separation of synchronization and data movement, consistency overhead, and false sharing. <p> The compiler generates a Validate with type READ&WRITE ALL. As a result, no twins or diffs are made. In contrast, TreadMarks suffers from a diff accumulation phenomenon <ref> [22] </ref>. In TreadMarks, if a processor incurs an access miss on a page, it is sent all the diffs created by processors who have modified the data.
Reference: [23] <author> T.C. Mowry, M.S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of ASPLOS-5, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: Finally, while Woo et al. focus on establishing the magnitude of the performance benefits of bulk data transfer, we have explored in addition ways for the compiler to automate the use of the bulk data transfer facility. Mowry et al. <ref> [23] </ref> discuss the design and evaluation of a compiler algorithm for prefetching. Their algorithm concentrates on improving the performance of cache-based systems and issues prefetch requests for data that are likely to incur a cache miss. Porterfield et al. [7] present an algorithm for inserting prefetches one loop iteration ahead.
Reference: [24] <author> S.C. Woo, J.P. Singh, and J.L. Hennessy. </author> <title> The performance advantages of integrating block data transfer in cache-coherent multiprocessors. </title> <booktitle> In Proceedings of ASPLOS-6, </booktitle> <month> Oc-tober </month> <year> 1994. </year>
Reference-contexts: Several recent proposals for hardware shared memory machines include a message passing subsystem designed in part to allow applications to take advantage of bulk data transfer [19, 20]. Woo et al. <ref> [24] </ref> evaluate one such design in the context of the Flash system. There are many differences between their work and ours.
References-found: 24

