URL: http://www.cs.msu.edu/~huangyih/Publications/card_ICPP96.ps
Refering-URL: http://www.cs.msu.edu/~huangyih/Publications/index.html
Root-URL: http://www.cs.msu.edu
Email: fhuangyih,mckinleyg@cps.msu.edu  
Title: Efficient Collective Operations with ATM Network Interface Support  
Author: Y. Huang and P. K. McKinley 
Address: East Lansing, Michigan 48824  
Affiliation: Department of Computer Science Michigan State University  
Abstract: In this paper, we propose a novel approach to implementing collective communication operations across ATM-interconnected workstation clusters. Under this methodology, performance-critical components, and only these components, of the collective operation are moved "close-to-network," that is, they are performed by the ATM network interface. We argue that ATM network interfaces possess unique advantages for performing this task. Simulation results demonstrate that implementations resulting from this approach outperform conventional user-level implementations by a wide margin. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Steffora, </author> <title> "ATM: </title> <booktitle> The year of the trial," IEEE Computer, </booktitle> <pages> pp. 8-10, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: However, this picture is being changed by the emergence of switched local area networks, which exhibit data rates and aggregate bandwidth comparable to those of MPCs. Promising switch-based LANs include those based on asynchronous transfer mode (ATM) <ref> [1, 2] </ref>. The increases in computational power and raw network bandwidth that have made such cluster computing feasible, however, have also revealed new bottlenecks in communication subsystems of workstations, namely system overhead.
Reference: [2] <author> R. Rooholamini, V. Cherkassky, and M. Garver, </author> <title> "Finding the right ATM switch for the market," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 16-28, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: However, this picture is being changed by the emergence of switched local area networks, which exhibit data rates and aggregate bandwidth comparable to those of MPCs. Promising switch-based LANs include those based on asynchronous transfer mode (ATM) <ref> [1, 2] </ref>. The increases in computational power and raw network bandwidth that have made such cluster computing feasible, however, have also revealed new bottlenecks in communication subsystems of workstations, namely system overhead.
Reference: [3] <author> D. C. Schmidt and T. Suda, </author> <title> "Transport system archi-tecture services for high-performance communications systems," </title> <journal> IEEE Journal of Selected Areas in Communications, </journal> <volume> vol. 11, </volume> <pages> pp. 489-506, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: This is because the general-purpose nature of a workstation environment typically demands a full-fledged operating system to provide applications with secure and well-structured services. The problem of reducing system overhead has been addressed in many different ways, including improved buffer management <ref> [3] </ref>, restructured OS kernels [3, 4], and the movement of the protocol stack to the user level [5]. Another solution considered previously [6] is to move the protocol stack (wholly or partially) to the network adapter. <p> This is because the general-purpose nature of a workstation environment typically demands a full-fledged operating system to provide applications with secure and well-structured services. The problem of reducing system overhead has been addressed in many different ways, including improved buffer management [3], restructured OS kernels <ref> [3, 4] </ref>, and the movement of the protocol stack to the user level [5]. Another solution considered previously [6] is to move the protocol stack (wholly or partially) to the network adapter.
Reference: [4] <author> N. C. Hutchinson and L. L. Peterson., </author> <title> "The x-Kernel: An architecture for implementing network protocols," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 17, </volume> <pages> pp. 64-76, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: This is because the general-purpose nature of a workstation environment typically demands a full-fledged operating system to provide applications with secure and well-structured services. The problem of reducing system overhead has been addressed in many different ways, including improved buffer management [3], restructured OS kernels <ref> [3, 4] </ref>, and the movement of the protocol stack to the user level [5]. Another solution considered previously [6] is to move the protocol stack (wholly or partially) to the network adapter.
Reference: [5] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels, "U-Net: </author> <title> A user-level network interface for parallel and distributed computing," </title> <booktitle> in Proc. of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <address> (Copper Mountain, Colorado), </address> <pages> pp. 40-53, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: The problem of reducing system overhead has been addressed in many different ways, including improved buffer management [3], restructured OS kernels [3, 4], and the movement of the protocol stack to the user level <ref> [5] </ref>. Another solution considered previously [6] is to move the protocol stack (wholly or partially) to the network adapter. The intent is to off-load work from the host processor, reduce the rate of interrupts to the host processor, and improve system throughput. <p> Such a downward migration of functionality is not without pitfalls. Since the host processor is typically much more powerful than the adapter processor, the potential advantage of off-loading can be diminished by protracted processing on the adapter <ref> [5] </ref>. However, an observation supported by the results presented in this paper is that, while the above arguments against downward migration hold in a general context, there are special cases in which significant performance gain can be achieved when high-level communication functions are moved close-to-network. <p> For cell level support, an ack merge must be completed within one cell slot time, 2.7 sec in the case of 155 Mbps channels. The performance at the AAL-level implementation depends on the efficiency of segmentation and reassembly. We use as parameters to our simulator the results published in <ref> [5] </ref>, which showed that the round-trip time of a single-cell AAL5 packet is approximately 65 sec, using Sun Sparc 20 workstations, Fore SBA-200 adapters, and the Fore ASX-200 switch. At user space, a necessary simulation parameter is the time for a user process to send an acknowledgment to the NI. <p> The two NI implementations achieve dramatically better performance than do the user-level implementations. Analysis of the costs of NI implementations reveals that a large portion of the time is spent in the transport-to-NI path. Incorporation of recent lightweight messaging systems, such as U-Net <ref> [5] </ref>, should further reduce the time of the NI-supported implementations. merge in various settings. 5 Symmetric Broadcast Next, we discuss low-level ATM NI support for implementing broadcast groups, which allow every member of a process group to broadcast efficiently to the others. <p> In this sense, our approach is more optimistic by focusing on the latency of the collective operation as a whole, rather than the reliability of each constituent message. From another perspective, these modern message transport protocols <ref> [5, 20, 21] </ref> are complementary to the methods described in this paper: While the core suboperations identified here are suitable for low-level implementation, lightweight message transport protocols can further reduce system overhead, which is introduced on a per-operation basis in our approach.
Reference: [6] <author> E. Cooper, P. Steenkiste, R. Sansom, and B. Zill, </author> <title> "Protocol implementation on the Nectar communication processor.," </title> <booktitle> in ACM SIGCOMM'90 Symposium, (Philadelphia), </booktitle> <pages> pp. 135-144, </pages> <month> September </month> <year> 1990. </year> <note> Also published as technical report CMU-CS-90-153. </note>
Reference-contexts: The problem of reducing system overhead has been addressed in many different ways, including improved buffer management [3], restructured OS kernels [3, 4], and the movement of the protocol stack to the user level [5]. Another solution considered previously <ref> [6] </ref> is to move the protocol stack (wholly or partially) to the network adapter. The intent is to off-load work from the host processor, reduce the rate of interrupts to the host processor, and improve system throughput. Such a downward migration of functionality is not without pitfalls.
Reference: [7] <author> Message Passing Interface Forum, </author> <title> "MPI: A message-passing interface standard," </title> <type> tech. rep., </type> <institution> Department of Computer Science, University of Ten-nessee, Knoxville, Tennessee, </institution> <month> May </month> <year> 1994. </year> <note> available via anonymous ftp: ftp.mcs.anl.gov:/pub/mpi/mpi-report.ps.Z. </note>
Reference-contexts: Collective operations may be used for data movement (for example, broadcast, scatter, gather, total exchange), global operations (reduction, scan), and process control (barrier synchronization). The increasing interest in collective operations is evidenced by their inclusion in the Message Passing Interface (MPI) standard <ref> [7] </ref> and by their use in a wide variety of parallel algorithms [8]. A process participating in a collective operation may have to execute multiple communication steps in order to fulfill its responsibility in the operation.
Reference: [8] <author> V. Kumar, A. Grama, A. Gupta, and G. Karypis, </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> Redwood City, California: Ben-jamin/Cummings, </address> <year> 1994. </year>
Reference-contexts: The increasing interest in collective operations is evidenced by their inclusion in the Message Passing Interface (MPI) standard [7] and by their use in a wide variety of parallel algorithms <ref> [8] </ref>. A process participating in a collective operation may have to execute multiple communication steps in order to fulfill its responsibility in the operation.
Reference: [9] <author> ATM Forum, </author> <title> ATM User-Network Interface (UNI) Specification Version 3.1. </title> <publisher> Prentice Hall, </publisher> <month> September </month> <year> 1994. </year>
Reference-contexts: Cells belonging to a VC are tagged with the virtual circuit identifier (VCI). 1 VCs with multiple destinations, or multicast VCs, are also supported by ATM User Network Interface (UNI) standard 3.1 <ref> [9] </ref>. Cells transmitted on multicast VCs are replicated as needed by network switches, resulting in a tree-like connection from the source to the destinations. <p> The M-array topology is a hypergraph, in that every (directed) edge in the topology may have more than one destination. In ATM-networks, multidestination edges can be directly mapped to multicast VCs, which are supported by ATM standards <ref> [9] </ref>. M-arrays of sizes up to 8 are shown in Figure 7. Formally, the M-array is recursively defined as follows: A single node is an M-array of length 2 0 .
Reference: [10] <institution> Fore Systems, Inc., ForeRunner SBA-200 ATM SBus Adapter User Manual, </institution> <year> 1993. </year>
Reference-contexts: Operations adhering to the above guidelines should be able to meet this time limit. Example: Fore SBA-200 NI. In order to help clarify the model, we discuss how the abstract ATM device can be mapped to a particular ATM NI, the SBA-200 (and its driver) from Fore Systems <ref> [10] </ref>. The SBA-200 adapter is equipped with an i960 processor, which executes two important firmware modules, one for the reassembly of input cells and one for the segmentation of out-going packets. An input-cell FIFO and output-cell FIFO form the i960's interface with the ATM physical layer.
Reference: [11] <author> Fore Systems, Inc., </author> <title> ForeRunner ASX-100 ATM Switch Documentation Overview, </title> <year> 1993. </year>
Reference-contexts: In an ATM network, a switch may support hardware replication of cells, such that each copy is transmitted on a different output port. The Fore ASX-100 and ASX-200 switches support this capability <ref> [11] </ref>. However, multicast VCs as defined in the ATM UNI 3.1 standard provide only best-effort delivery; a transport protocol must enforce guaranteed delivery if reliable multicast is desired. In this work, we assume that the transport layer uses a sliding window protocol for retransmission and flow control.
Reference: [12] <author> P. B. Danzig, </author> <title> "Flow Control for Limited Buffer Mul-ticast," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 20, </volume> <pages> pp. 1-12, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: A straightforward way to implement ack merge is to have all destinations send acknowledgments directly back to the source. This method may work well for a small number of destinations, but suffers from acknowledgment implosion at source when the number of destinations is large <ref> [12] </ref>. Alternatively, destinations can be arranged as nodes in a tree rooted at the source [13]. A destination that is a leaf node in the tree simply sends its acknowledgments to its parent.
Reference: [13] <author> C. C. Huang and P. K. McKinley, </author> <title> "Communication issues in parallel computing across ATM networks," </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> vol. 2, no. 4, </volume> <pages> pp. 73-86, </pages> <year> 1994. </year>
Reference-contexts: This method may work well for a small number of destinations, but suffers from acknowledgment implosion at source when the number of destinations is large [12]. Alternatively, destinations can be arranged as nodes in a tree rooted at the source <ref> [13] </ref>. A destination that is a leaf node in the tree simply sends its acknowledgments to its parent. A destination that is an internal node must find the minimum acknowledgment (the one with the smallest sequence number) among its children and itself, and forward this acknowledgment to its parent. <p> The study of cluster computing across ATM networks is growing rapidly, and efficient communication is a key issue <ref> [13, 19, 20] </ref>. In particular, traditional message-passing layers have been recognized as the bottleneck for cluster computing. Researchers have addressed this issue by shortening the application-to-network path. Important contributions in this area include Active Messages (AM) [20] and Fast Messages (FM) [21].
Reference: [14] <author> H. D. Schwetman, "CSIM: </author> <title> A C-based, process-oriented simulation language," </title> <type> Tech. Rep. </type> <institution> PP-080-85, Microelectronics and Computer Technology Corporation, </institution> <year> 1985. </year>
Reference-contexts: The performance of two NI implementations (cell level and AAL level) and two user space implementations (using UDP or Fore AAL5 API) are compared through a simulation. The simulator is based on CSIM <ref> [14] </ref>, an event driven simulation package. For cell level support, an ack merge must be completed within one cell slot time, 2.7 sec in the case of 155 Mbps channels. The performance at the AAL-level implementation depends on the efficiency of segmentation and reassembly.
Reference: [15] <author> K. K. Keeton, T. E. Anderson, and D. A. Patterson, </author> <title> "LogP quantified: The case of low-overhead local area networks," in presented at Hot Interconnects III: A Symposium on High Performance Interconnects, </title> <month> August </month> <year> 1995. </year>
Reference-contexts: The value of this parameter can be affected by the application program interface (API) used by the user process. When the UDP is used, we use the UDP overhead of very short messages, which were reported in <ref> [15] </ref> to be about 1400 sec. When the Fore API is used, we use the figure 1200 sec we obtained in our ATM testbed, which comprises Sparc 10s equipped with Fore SBA-200 adapters and connected by Fore ASX-200 switches. four implementations. <p> Somewhat surprisingly, the curve for ATM cell level support remains flat; long messages should incur long cell delivery delays even with the support of cell-level forwarding. However, further analysis reveals that, since the TCP overhead (for initial sending and final delivery of the message) grows with message length <ref> [15] </ref>, the time for the entire message to be forwarded to the farthest node is still relatively small compared to this overhead. (a) short (8-byte) message. (b) long (48K-byte) message. group. 6 Barrier Synchronization The proposed barrier synchronization algorithm also uses the M-array topology.
Reference: [16] <author> Y. Huang, C. C. Huang, and P. K. McKin-ley, </author> <title> "Multicast virtual topologies for collective communication in MPC and ATM clusters," </title> <booktitle> in On-line Proceedings of Supercomputing'95 at http://www.supercomp.org/sc95/proceedings/, (San Diego, </booktitle> <address> CA), </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Our NI support for such groups uses a virtual topology, called the M-array, which we proposed in an earlier study <ref> [16, 17] </ref>. The M-array topology is a hypergraph, in that every (directed) edge in the topology may have more than one destination. In ATM-networks, multidestination edges can be directly mapped to multicast VCs, which are supported by ATM standards [9]. <p> Similarly, the leftmost node of the right subarray is the source of a multicast channel whose destination nodes are all the nodes in the left subarray. Details of the M-array topology can be found in <ref> [16] </ref>. We emphasize that the choice of a particular virtual topology is not a critical part of the methods proposed in this paper. The concept of NI support for critical suboperations can be applied to collective algorithms that use other virtual topologies.
Reference: [17] <author> C. Huang, Y. Huang, and P. K. McKinley, </author> <title> "A thread-based interface for collective communication on ATM networks," </title> <booktitle> in Proceedings of the 15th IEEE International Conference on Distributed Computing Systems, </booktitle> <address> (Vancouver, British Columbia), </address> <pages> pp. 254-261, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Our NI support for such groups uses a virtual topology, called the M-array, which we proposed in an earlier study <ref> [16, 17] </ref>. The M-array topology is a hypergraph, in that every (directed) edge in the topology may have more than one destination. In ATM-networks, multidestination edges can be directly mapped to multicast VCs, which are supported by ATM standards [9].
Reference: [18] <author> P. K. McKinley, Y.-J. Tsai, and D. Robinson, </author> <title> "Collective communication in wormhole-routed massively parallel computers," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 39-50, </pages> <month> De-cember </month> <year> 1995. </year>
Reference-contexts: This problem does not exist in the barrier operation, resulting in better relative performance. 7 Related Work The subject of collective communication in MPCs has been extensively studied in the last several years, and the contributions are too numerous to list here; a survey can be found in <ref> [18] </ref>. The study of cluster computing across ATM networks is growing rapidly, and efficient communication is a key issue [13, 19, 20]. In particular, traditional message-passing layers have been recognized as the bottleneck for cluster computing. Researchers have addressed this issue by shortening the application-to-network path.
Reference: [19] <author> M. Lin, J. Hsieh, D. H. C. Du, J. P. Thomas, and J. A. MacDonald, </author> <title> "Distributed network computing over local ATM networks," </title> <journal> IEEE Journal of Selected Areas in Communications, </journal> <volume> vol. 13, </volume> <pages> pp. 733-748, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: The study of cluster computing across ATM networks is growing rapidly, and efficient communication is a key issue <ref> [13, 19, 20] </ref>. In particular, traditional message-passing layers have been recognized as the bottleneck for cluster computing. Researchers have addressed this issue by shortening the application-to-network path. Important contributions in this area include Active Messages (AM) [20] and Fast Messages (FM) [21].
Reference: [20] <author> T. von Eicken, V. Avula, A. Basu, and V. </author> <title> Buch, "Low-latency communication over ATM networks using active messages," in Proceedings of Hot Interconnects II, </title> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alami-tos, Calif., </address> <month> Aug. </month> <year> 1994. </year> <note> Also appears in IEEE Micro, </note> <month> February </month> <year> 1995. </year>
Reference-contexts: The study of cluster computing across ATM networks is growing rapidly, and efficient communication is a key issue <ref> [13, 19, 20] </ref>. In particular, traditional message-passing layers have been recognized as the bottleneck for cluster computing. Researchers have addressed this issue by shortening the application-to-network path. Important contributions in this area include Active Messages (AM) [20] and Fast Messages (FM) [21]. <p> In particular, traditional message-passing layers have been recognized as the bottleneck for cluster computing. Researchers have addressed this issue by shortening the application-to-network path. Important contributions in this area include Active Messages (AM) <ref> [20] </ref> and Fast Messages (FM) [21]. Both systems provide reliability and flow control mechanism for message transports and are designed or tailored for short messages. High-level communication functions, such as collective operations, would likely be implemented in applications or in libraries at user level. <p> In this sense, our approach is more optimistic by focusing on the latency of the collective operation as a whole, rather than the reliability of each constituent message. From another perspective, these modern message transport protocols <ref> [5, 20, 21] </ref> are complementary to the methods described in this paper: While the core suboperations identified here are suitable for low-level implementation, lightweight message transport protocols can further reduce system overhead, which is introduced on a per-operation basis in our approach.
Reference: [21] <author> S. Pakin, M. Lauria, and A. Chien, </author> <title> "High performance messaging on workstations: Illi-nois fast messages (FM) for Myrinet," </title> <booktitle> in On-line Proceedings of Supercomputing'95 at http://www.supercom.org/sc95/proceedings/, (San Diego, </booktitle> <address> CA), </address> <year> 1995. </year>
Reference-contexts: In particular, traditional message-passing layers have been recognized as the bottleneck for cluster computing. Researchers have addressed this issue by shortening the application-to-network path. Important contributions in this area include Active Messages (AM) [20] and Fast Messages (FM) <ref> [21] </ref>. Both systems provide reliability and flow control mechanism for message transports and are designed or tailored for short messages. High-level communication functions, such as collective operations, would likely be implemented in applications or in libraries at user level. <p> In this sense, our approach is more optimistic by focusing on the latency of the collective operation as a whole, rather than the reliability of each constituent message. From another perspective, these modern message transport protocols <ref> [5, 20, 21] </ref> are complementary to the methods described in this paper: While the core suboperations identified here are suitable for low-level implementation, lightweight message transport protocols can further reduce system overhead, which is introduced on a per-operation basis in our approach.
Reference: [22] <author> H. Franke, P. Hochschild, P. Pattnaik, and M. Snir, </author> <title> "An efficient implementation of MPI on IBM-SP1," </title> <booktitle> in Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <volume> vol. III, </volume> <pages> pp. 197-209, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: We envision a promising combination of the proposed NI support with the streamlined interfaces to applica tions provided by packages such as AM and FM. Implementations of collective operations across clustered workstations can be found in the implementations of MPI <ref> [22] </ref>. These implementations reside either at user level or in the kernel and are placed above the message transport layer.
References-found: 22

