URL: ftp://ftp.cse.ucsc.edu/pub/ml/minimax.ps.Z
Refering-URL: http://www.cse.ucsc.edu/~haussler/pubs.html
Root-URL: http://www.cse.ucsc.edu
Author: David Haussler 
Keyword: Index terms: minimax theorem, minimax redundancy, minimax risk, Bayes risk, relative entropy, Kullback-Leibler divergence, density estimation, source coding, channel capacity, computational learning theory  
Date: December 29, 1996  
Address: Santa Cruz  
Affiliation: UC  
Abstract: A General Minimax Result for Relative Entropy University of California Technical Report UCSC-CRL-96-26 Baskin Center for Computer Science and Computer Engineering UC Santa Cruz, CA 96064 Abstract: Suppose Nature picks a probability measure P on a complete separable metric space X at random from a measurable set P fi = fP : 2 fig. Then, without knowing , a statistician picks a measure Q on X. Finally, the statistician suffers a loss D(P jjQ), the relative entropy between P and Q. We show that the minimax and maximin values of this game are always equal, and there is always a minimax strategy in the closure of the set of all Bayes strategies. This generalizes previous results of Gallager, and Davisson and Leon-Garcia. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Barron and T. </author> <title> Cover. A bound on the financial value of information. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 34 </volume> <pages> 1097-1100, </pages> <year> 1988. </year>
Reference-contexts: The game has interpretations in other fields as well. For example, in mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution <ref> [1, 3] </ref>. 2 Preliminary Definitions We first briefly review some basic facts about probability measures on complete separable metric spaces; proofs of these can be found in e.g. [8].
Reference: [2] <author> L. L. </author> <title> Cam. An extension of Wald's theory of statistical decision functions. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 26 </volume> <pages> 69-81, </pages> <year> 1955. </year>
Reference-contexts: Computer and Information Sciences, UC Santa Cruz, Santa Cruz, CA 95064. Email addresses: haussler@cse.ucsc.edu 1 a finite set of symbols. The proof of the general result closely follows that of Theorem 2, page 85 in [10], which is based on earlier results of Le Cam <ref> [2] </ref>, with one fairly simple extension to handle the case when fP : 2 fig is not uniformly tight (Lemma 4 below). In the source coding interpretation of this game, the minimax value is the capacity of the channel from fi to X [7, 4]. <p> This, and the comments below, give us the set-up needed to apply Ferguson's theorem, which is based on <ref> [2] </ref>. 5 Let A (fi V ) A (fi) be the set of all priors (probability mass functions) over fi V and let M fi V = fP : 2 A (fi V )g.
Reference: [3] <author> B. Clarke and A. Barron. </author> <title> Information-theoretic asymptotics of Bayes methods. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 36(3) </volume> <pages> 453-471, </pages> <year> 1990. </year> <month> 7 </month>
Reference-contexts: At the end of this time period, the statistician suffers a cumulative relative entropy loss P n which measures the quality of the sequential estimates made. Variants of this game have been studied by several authors (see e.g. <ref> [15, 9, 3, 4, 14] </ref>). If the observations are restricted to a finite alphabet, then the actions of the statistician can be interpreted as adaptive source coding for an unknown source. For fixed , the average loss suffered is the redundancy [6, 3]. <p> If the observations are restricted to a finite alphabet, then the actions of the statistician can be interpreted as adaptive source coding for an unknown source. For fixed , the average loss suffered is the redundancy <ref> [6, 3] </ref>. When we also average over the random choice of according to the prior , we get the risk for this game, which is then average redundancy. This risk is called cumulative relative entropy risk in statistics. <p> Let P = ~ P n be the "true" joint distribution for the observations. Using the chain rule for relative entropy, it is seen that the risk for this game reduces to R D (P jjQ)d () <ref> [3] </ref>. This leads us to a simpler, more general game: Nature picks a prior on fi and then picks a probability measure P on a space X at random (according to ) from a set fP : 2 fig. <p> The game has interpretations in other fields as well. For example, in mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution <ref> [1, 3] </ref>. 2 Preliminary Definitions We first briefly review some basic facts about probability measures on complete separable metric spaces; proofs of these can be found in e.g. [8].
Reference: [4] <author> B. Clarke and A. Barron. </author> <title> Jefferys' prior is asymptotically least favorable under entropy risk. </title> <journal> J. Statistical Planning and Inference, </journal> <volume> 41 </volume> <pages> 37-60, </pages> <year> 1994. </year>
Reference-contexts: At the end of this time period, the statistician suffers a cumulative relative entropy loss P n which measures the quality of the sequential estimates made. Variants of this game have been studied by several authors (see e.g. <ref> [15, 9, 3, 4, 14] </ref>). If the observations are restricted to a finite alphabet, then the actions of the statistician can be interpreted as adaptive source coding for an unknown source. For fixed , the average loss suffered is the redundancy [6, 3]. <p> In the source coding interpretation of this game, the minimax value is the capacity of the channel from fi to X <ref> [7, 4] </ref>. <p> For the general source coding/cumulative relative entropy risk case in which P fi is a family of smooth parametric n-fold product distributions, bounds on the Bayes Risk and the minimax value of the game that hold asymptotically for large n are given in <ref> [4] </ref>. These have a long history, also described there. Bounds on these quantities in a more abstract n-fold product setting are obtained in [14], using the results of this paper. We are unaware of any general bounds for the case when the distributions in fi are not product distributions. <p> Related results are given in [14]. Both <ref> [4] </ref> and [16] also investigate the limiting value of this game in the above case as n ! 1. It is shown in [4] that in this limit Jeffreys' prior achieves the maximin = minimax value asymptotically for smooth, parametric distributions. <p> Related results are given in [14]. Both <ref> [4] </ref> and [16] also investigate the limiting value of this game in the above case as n ! 1. It is shown in [4] that in this limit Jeffreys' prior achieves the maximin = minimax value asymptotically for smooth, parametric distributions. It would be interesting to know the structure of the corresponding "asymptotically least favorable" prior in more abstract settings.
Reference: [5] <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: g is a probability mass distribution and fp i g is any set of nonnegative numbers then X p i log q i i ! X p i : (10) The second claim follows directly from Jensen's inequality, and is a special case of the log sum inequality given in <ref> [5] </ref>.
Reference: [6] <author> L. Davisson. </author> <title> Universal noisless coding. </title> <journal> IEEE transactions on information theory, </journal> <volume> IT-19:783-795, </volume> <year> 1973. </year>
Reference-contexts: If the observations are restricted to a finite alphabet, then the actions of the statistician can be interpreted as adaptive source coding for an unknown source. For fixed , the average loss suffered is the redundancy <ref> [6, 3] </ref>. When we also average over the random choice of according to the prior , we get the risk for this game, which is then average redundancy. This risk is called cumulative relative entropy risk in statistics.
Reference: [7] <author> L. Davisson and A. Leon-Garcia. </author> <title> A source matching approach to finding minimax codes. </title> <journal> IEEE transactions on information theory, </journal> <volume> IT-26:166-174, </volume> <year> 1980. </year>
Reference-contexts: We show that the minimax and maximin values of this game are always equal, and there is always a minimax strategy in the closure of the set of all Bayes strategies. This generalizes results of Gallager [11], and Davisson and Leon-Garcia <ref> [7] </ref>, which were restricted to the case when the observations are chosen from fl Supported by NSF grant IRI-9123692. Computer and Information Sciences, UC Santa Cruz, Santa Cruz, CA 95064. Email addresses: haussler@cse.ucsc.edu 1 a finite set of symbols. <p> In the source coding interpretation of this game, the minimax value is the capacity of the channel from fi to X <ref> [7, 4] </ref>. <p> This generalizes similar results of Gallager [11], and Davisson and Leon-Garcia <ref> [7] </ref>. As in the latter result, the proof closely follows that of Theorem 2, page 85 in [10]. Again, we include it only for completeness 1 . Before we can prove the lemma, we need a few more preliminary definitions.
Reference: [8] <author> R. M. Dudley. </author> <title> Real Analysis and Probability. </title> <publisher> Wadsworth, </publisher> <year> 1989. </year>
Reference-contexts: relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution [1, 3]. 2 Preliminary Definitions We first briefly review some basic facts about probability measures on complete separable metric spaces; proofs of these can be found in e.g. <ref> [8] </ref>. Let (X; ) be a complete separable metric space and let A (X) be the set of all probability measures defined on the -field generated by the open sets of X (i.e. the Borel subsets of X).
Reference: [9] <author> S. Y. Efroimovich. </author> <title> Information contained in a sequence of observations. Problems in Information Transmission, </title> <booktitle> 15 </booktitle> <pages> 178-189, </pages> <year> 1980. </year>
Reference-contexts: At the end of this time period, the statistician suffers a cumulative relative entropy loss P n which measures the quality of the sequential estimates made. Variants of this game have been studied by several authors (see e.g. <ref> [15, 9, 3, 4, 14] </ref>). If the observations are restricted to a finite alphabet, then the actions of the statistician can be interpreted as adaptive source coding for an unknown source. For fixed , the average loss suffered is the redundancy [6, 3].
Reference: [10] <author> T. Ferguson. </author> <title> Mathematical Statistics: A Decision Theoretic Approach. </title> <publisher> Academic Press, </publisher> <year> 1967. </year>
Reference-contexts: Computer and Information Sciences, UC Santa Cruz, Santa Cruz, CA 95064. Email addresses: haussler@cse.ucsc.edu 1 a finite set of symbols. The proof of the general result closely follows that of Theorem 2, page 85 in <ref> [10] </ref>, which is based on earlier results of Le Cam [2], with one fairly simple extension to handle the case when fP : 2 fig is not uniformly tight (Lemma 4 below). <p> The following lemma is a minor variant of the standard minimax theorem for finite fi from <ref> [10] </ref> (see Theorem 1, page 82). We include the proof for completeness. Lemma 2 If fi is finite then V fl V . Proof: Suppose fi = f 1 ; : : : ; k g. <p> Proof: Suppose fi = f 1 ; : : : ; k g. Let S = f (D (P 1 jjQ); : : :; D (P k jjQ)) : Q 2 M fi g; and let co (S) be the convex hull of S. By Helley's theorem (see e.g. <ref> [10] </ref>, Lemma 1, page 65), for every z 2 co (S) there exist s 1 ; : : : ; s k+1 2 S and 1 ; : : : ; k+1 with j 0 and P k+1 j=1 j = 1 such that P k+1 Let s j = (D <p> This generalizes similar results of Gallager [11], and Davisson and Leon-Garcia [7]. As in the latter result, the proof closely follows that of Theorem 2, page 85 in <ref> [10] </ref>. Again, we include it only for completeness 1 . Before we can prove the lemma, we need a few more preliminary definitions. A real-valued function f on a topological space X is lower semicontinuous if for all real r, fx : f (x) &gt; rg is open. <p> Any lower semicontinuous function defined on a compact set achieves its infimum on that set, and if F is any set of lower semicontinuous functions, then g (x) = supff (x) : f 2 F g is lower semicontinuous (see <ref> [10] </ref>). Finally, Posner has shown that the function D (P jjQ) is lower semicontinuous in both its arguments with respect to the topology of weak convergence (or equivalently, w.r.t. the fi metric) [18].
Reference: [11] <author> R. Gallager. </author> <title> Source coding with side information and universal coding. </title> <type> Technical Report LIDS-P-937, </type> <institution> MIT Laboratory for Information and Decision Systems, </institution> <year> 1979. </year>
Reference-contexts: Finally, the statistician suffers a loss D (P jjQ). We show that the minimax and maximin values of this game are always equal, and there is always a minimax strategy in the closure of the set of all Bayes strategies. This generalizes results of Gallager <ref> [11] </ref>, and Davisson and Leon-Garcia [7], which were restricted to the case when the observations are chosen from fl Supported by NSF grant IRI-9123692. Computer and Information Sciences, UC Santa Cruz, Santa Cruz, CA 95064. Email addresses: haussler@cse.ucsc.edu 1 a finite set of symbols. <p> This generalizes similar results of Gallager <ref> [11] </ref>, and Davisson and Leon-Garcia [7]. As in the latter result, the proof closely follows that of Theorem 2, page 85 in [10]. Again, we include it only for completeness 1 . Before we can prove the lemma, we need a few more preliminary definitions.
Reference: [12] <author> D. Haussler and A. Barron. </author> <title> How well do Bayes methods work for on-line prediction of f+1; 1g values? In Proceedings of the Third NEC Symposium on Computation and Cognition. </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: A similar interpretation applies in computational learning theory, where the cumulative relative entropy risk is interpreted as the average additional loss suffered by an adaptive algorithm that predicts each observation before it arrives, based on the previous observations, as compared to an algorithm that makes predictions knowing the true distribution <ref> [12, 13] </ref>.
Reference: [13] <author> D. Haussler, M. Kearns, and R. E. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <journal> Machine Learning, </journal> <volume> 14(1) </volume> <pages> 83-113, </pages> <year> 1994. </year>
Reference-contexts: A similar interpretation applies in computational learning theory, where the cumulative relative entropy risk is interpreted as the average additional loss suffered by an adaptive algorithm that predicts each observation before it arrives, based on the previous observations, as compared to an algorithm that makes predictions knowing the true distribution <ref> [12, 13] </ref>.
Reference: [14] <author> D. Haussler and M. Opper. </author> <title> Mutual information, metric entropy, and cumulative relative entropy risk. </title> <journal> Annals of Statistics, </journal> <note> 1997. to appear. </note>
Reference-contexts: At the end of this time period, the statistician suffers a cumulative relative entropy loss P n which measures the quality of the sequential estimates made. Variants of this game have been studied by several authors (see e.g. <ref> [15, 9, 3, 4, 14] </ref>). If the observations are restricted to a finite alphabet, then the actions of the statistician can be interpreted as adaptive source coding for an unknown source. For fixed , the average loss suffered is the redundancy [6, 3]. <p> These have a long history, also described there. Bounds on these quantities in a more abstract n-fold product setting are obtained in <ref> [14] </ref>, using the results of this paper. We are unaware of any general bounds for the case when the distributions in fi are not product distributions. <p> Related results are given in <ref> [14] </ref>. Both [4] and [16] also investigate the limiting value of this game in the above case as n ! 1. It is shown in [4] that in this limit Jeffreys' prior achieves the maximin = minimax value asymptotically for smooth, parametric distributions.
Reference: [15] <author> I. Ibragimov and R. Hasminskii. </author> <title> On the information in a sample about a parameter. </title> <booktitle> In Second Int. Symp. on Information Theory, </booktitle> <pages> pages 295-309, </pages> <year> 1972. </year>
Reference-contexts: At the end of this time period, the statistician suffers a cumulative relative entropy loss P n which measures the quality of the sequential estimates made. Variants of this game have been studied by several authors (see e.g. <ref> [15, 9, 3, 4, 14] </ref>). If the observations are restricted to a finite alphabet, then the actions of the statistician can be interpreted as adaptive source coding for an unknown source. For fixed , the average loss suffered is the redundancy [6, 3].
Reference: [16] <author> N. Merhav and M. Feder. </author> <title> A strong version of the redundancy-capacity theorem of universal coding. </title> <journal> IEEE Trans. Info Th., </journal> <volume> 41(3):714-, </volume> <year> 1995. </year>
Reference-contexts: These have a long history, also described there. Bounds on these quantities in a more abstract n-fold product setting are obtained in [14], using the results of this paper. We are unaware of any general bounds for the case when the distributions in fi are not product distributions. In <ref> [16] </ref> it is shown that the minimax value of this game is nearly a lower bound on the loss that must be suffered by the statistician for "most" states of Nature, where "most" is defined with respect to a limit of priors that achieve the maximin bound. <p> Related results are given in [14]. Both [4] and <ref> [16] </ref> also investigate the limiting value of this game in the above case as n ! 1. It is shown in [4] that in this limit Jeffreys' prior achieves the maximin = minimax value asymptotically for smooth, parametric distributions.
Reference: [17] <author> M. S. Pinsker. </author> <title> Information and Information Stability of Random Variables and Processes (Transl.). </title> <type> Holden Day, </type> <year> 1964. </year>
Reference-contexts: and Q can be defined by D (P jjQ) = sup k X P (E i ) log Q (E i ) where (X) is the set of all finite partitions of X into Borel sets. (Throughout the paper we define 0 log 0 = 0.) An equivalent definition (see <ref> [17] </ref>) is D (P jjQ) = log dQ (x) dP (x); where dP and dQ are Radon-Nikodym derivatives with respect to a suitable dominating measure for P and Q. Let fi be a set and P be a measure in A (X) for every 2 fi.
Reference: [18] <author> E. Posner. </author> <title> Random coding strategies for minimum entropy. </title> <journal> IEEE Trans. Info. Th., </journal> <volume> IT-21:388-391, </volume> <year> 1975. </year>
Reference-contexts: Finally, Posner has shown that the function D (P jjQ) is lower semicontinuous in both its arguments with respect to the topology of weak convergence (or equivalently, w.r.t. the fi metric) <ref> [18] </ref>. Lemma 3 If P fi is uniformly tight then V fl = V = V , and moreover there exists a minimax strategy in M fi , i.e. there exists Q 0 2 M fi such that V = sup 2A (fi) I (; Q 0 ).
References-found: 18

