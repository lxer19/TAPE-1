URL: http://www-csag.cs.uiuc.edu/papers/ganguly-ms.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Title: CONCURRENT OBJECT-ORIENTED PROGRAMMING ON LARGE SCALED SHARED MEMORY ARCHITECTURES: A STRUCTURED ADAPTIVE MESH REFINEMENT METHOD
Author: BY BISHWAROOP GANGULY 
Degree: THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Date: 1994  
Address: Berkeley,  1998 Urbana, Illinois  
Affiliation: B.S., University of California at  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. Agha. </author> <title> Concurrent object-oriented programming. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 33(9) </volume> <pages> 125-41, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Recent compiler technology advances [6] have found ways to analyze object-based code, breaking down the user abstractions to generate efficient code. A Concurrent Object-Oriented Programing (COOP) <ref> [1] </ref> model distributes objects among processing elements and has support for distributing method invocations to generate parallelism. In the parallel programs, the object model provides flexibility and a means for the user to distribute computation easily, namely distributed objects.
Reference: [2] <author> Gul Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The ICC++ language is derived from the object-oriented C++ language [24]. It provides minimal extensions to the sequential language, and has features supporting concurrent execution based on the Actor model of computation <ref> [2] </ref>. It has been designed to provide C++ programmers with a very familiar syntax to C++ and allow them to express concurrent operations in a simple and intuitive way. <p> The system that we employ in this work contains the runtime support needed. The Illinois Concert system is designed to provide fine-grained parallelism, without forcing the user to explicitly manage the concurrent execution of the code. The Concert system supports fine-grained, concurrent object-oriented programming on Actors <ref> [2] </ref> in an extension of C++ called ICC++ [9, 25]. Computation is expressed as method invocations on objects or collections of objects. Method invocations conceptually operate within dynamically created threads that are inherently concurrent. Thus, the user only thinks about the implementation of the algorithm, and not about managing concurrency.
Reference: [3] <author> American National Standard Institute. </author> <title> Draft Standard for Information Systems Programming Language Fortran, volume 8 of SIGPLAN Special Interest Publication on Fortran. </title> <publisher> ACM Press, </publisher> <month> December </month> <year> 1989. </year>
Reference-contexts: Thus, we see a slew of programs that fail to take advantage of the hardware designed shared memory, simply because the new programming models that have become available are too complex. Solutions to the programming complexity problem have been proposed <ref> [11, 3] </ref>. These systems attempt to automate the process of managing shared address space programming using language features and compiler analysis. The compilers in these systems are generally written to match specific architectures and encode much of the designers knowledge of the hardware.
Reference: [4] <author> Greg L. Bryan. </author> <title> The Numerical Simulation of X-ray Clusters. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: Second, we have used the system to compile and run a Structured Adaptive Mesh Refinement (SAMR) code which is currently being used in research efforts by the Computational Cosmology Group at the National Center for Supercomputing Applications (NCSA) <ref> [4] </ref>. Our goal in doing these experiments is to show that Concerts execution model, and programming interface are appropriate for the new new architecture and for irregular object-oriented "real" codes such as our SAMR application. <p> The router chips handle congestion and implement message priority using a packet aging scheme. 21 Chapter 3 SAMR Implementation Our study is based on an SAMR method written by scientists at NCSA in the Computational Cosmology group. It has been used in a number of cosmology experiments <ref> [4, 23] </ref>. It is written in C++ and uses Fortran 77 kernels to do perform the compute-intensive interpolation and partial differential equation solves. We were motivated to work with a real code (as opposed to a benchmark) for two main reasons. <p> It also initializes some local parameters to be sent to the kernel. The main Fortran kernel is about 20,000 lines of Fortran 77. The actions of the kernel are beyond the scope of this thesis. Interested readers can reference <ref> [4] </ref>. After the solve phase, the boundary refresh phases are repeated, and are just as described above. 3.1.7 Recursive Call At this point, our code executes a recursive call to evolve the next level.
Reference: [5] <author> R. H. Campbell, G. Johnston, K. Kenny, G. Murakami, and V. Russo. </author> <title> Choices (class hierarchical open interface for custom embedded systems). </title> <journal> Operating Systems Review, </journal> <volume> 21(3) </volume> <pages> 9-17, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The concepts of data abstraction, and dynamic method dispatch have resulted in portable, modular code for a large number of application arenas such as databases, operating systems <ref> [5] </ref>, and also scientific applications. While object oriented software environments have been a good solution from a programmability standpoint, they have been perceived as inefficient.
Reference: [6] <author> Craig Chambers. </author> <title> The Cecil language: Specification and rationale, version 2.0. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, Washington, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: Intuitively, this perception stems from the basic premise that abstraction and the creation of interfaces, while providing modularity, add overhead in many common cases thus handicapping performance. Recent compiler technology advances <ref> [6] </ref> have found ways to analyze object-based code, breaking down the user abstractions to generate efficient code. A Concurrent Object-Oriented Programing (COOP) [1] model distributes objects among processing elements and has support for distributing method invocations to generate parallelism.
Reference: [7] <author> Andrew Chien, Julian Dolby, Bishwaroop Ganguly, Vijay Karamcheti, and Xingbin Zhang. </author> <title> Supporting high level programming with high performance: The Illinois Concert system. </title> <booktitle> In Proceedings of the Second International Workshop on High-level Parallel Programming Models and Supportive Environments, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: The Concert system was designed to tackle large programs and to be run on large-scale architectures. It has already demonstrated good performance on distributed memory machines on a number of codes <ref> [7] </ref>, [19]. In this work, we are taking a new direction for the system in two ways. First, we have ported the Concert System to a large-scaled shared memory system. <p> In a later section, we present descriptions of the primitives in more detail as well as some performance measurements. 2.3.1 System Design The following is a brief description of the Illinois Concert System implementation. More in-depth descriptions are available at [15] and <ref> [7] </ref>. Keep in mind that these concepts are at the runtime system level, and are not exposed to the user of the system. The Illinois Concert System uses a dynamic multi-threading execution model. <p> In the case of our application, using the shared memory support was the choice that yielded the best performance, due to the presence of dynamic, communication-only phases. Still other codes <ref> [7] </ref> contain fine overlapping communication and computation that that might benefit more from the distributed memory view that Concert provides as well. 81 Our belief is that future runtime systems will be required to adapt to changing hardware topologies, independent of the language or programming model that is being implemented.
Reference: [8] <author> Andrew Chien, Vijay Karamcheti, and John Plevyak. </author> <title> The Concert system|compiler and runtime support for efficient fine-grained concurrent object-oriented programs. </title> <type> Technical Report UIUCDCS-R-93-1815, </type> <institution> Department of Computer Science, University of Illinois, Urbana, Illinois, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Garbage collection is run whenever certain storage management conditions are detected by the runtime system. the compiler has no control to initiate or terminate garbage collection. Interested readers can get a full description of the Concert runtime design from <ref> [8] </ref>. 4.2 Runtime Support for SAMR Application We now proceed to detail several runtime services that we anticipate will be key to our application in particular. These are: * Parallel Invokes Since our codes parallelism takes the form of parallel independent method calls.
Reference: [9] <author> Andrew A. Chien, Uday S. Reddy, John Plevyak, and Julian Dolby. </author> <title> ICC++ a C++ dialect for high-performance parallel computation. </title> <booktitle> In Proceedings of the 2nd International Symposium on Object Technologies for Advanced Software, </booktitle> <month> March </month> <year> 1996. </year>
Reference-contexts: The Illinois Concert system is designed to provide fine-grained parallelism, without forcing the user to explicitly manage the concurrent execution of the code. The Concert system supports fine-grained, concurrent object-oriented programming on Actors [2] in an extension of C++ called ICC++ <ref> [9, 25] </ref>. Computation is expressed as method invocations on objects or collections of objects. Method invocations conceptually operate within dynamically created threads that are inherently concurrent. Thus, the user only thinks about the implementation of the algorithm, and not about managing concurrency.
Reference: [10] <author> Julian Dolby. </author> <title> Automatic inline allocation of objects. </title> <booktitle> In Proceedings of the 1997 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: We will detail the implementation of some of these runtime services in later sections. The Concert compiler is an aggressive, optimizing Concurrent Object-Oriented Programming language compiler. It performs a number of sequential optimizations such as access region expansion, object inlining <ref> [10] </ref> and method inlining to optimize object-oriented code [20]. In addition, the compiler supports concurrent locality-based optimizations, such as dynamic pointer alignment [26] and view caching [14]. <p> Note that in our efforts to parallelize the code, we have disabled several important optimizations, the most important of which are stack invocations [19] and object inlining <ref> [10] </ref>. While we suspect that these optimizations could assist in gaining sequential performance, the results we have obtained suggest that there is not enough room for improvement to pursue this direction. Our results show that parallelization is an attractive option for our code, from an efficiency standpoint.
Reference: [11] <author> High Performance Fortran Forum. </author> <title> High performance Fortran language specification version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year> <month> 90 </month>
Reference-contexts: Thus, we see a slew of programs that fail to take advantage of the hardware designed shared memory, simply because the new programming models that have become available are too complex. Solutions to the programming complexity problem have been proposed <ref> [11, 3] </ref>. These systems attempt to automate the process of managing shared address space programming using language features and compiler analysis. The compilers in these systems are generally written to match specific architectures and encode much of the designers knowledge of the hardware.
Reference: [12] <editor> J.H. Saltz, et al. </editor> <title> A manual for the CHAOS runtime library. </title> <type> Technical Report CS-TK-3437, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1995. </year>
Reference-contexts: We believe that an efficient implementation of a Concurrent Object-Oriented System with distribution support will provide greater functionality, while maintaining a more general interface and providing comparable performance. 19 2.4.2 CHAOS The CHAOS <ref> [12] </ref> Runtime library uses an Inspector-Executor technique to efficiently schedule communications and computation at runtime of a parallel program where communication patterns are variable. CHAOS attempts to minimize communication by detecting the communication patterns in loops and hoisting it outside.
Reference: [13] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concert|efficient runtime support for concurrent object-oriented programming languages on stock hardware. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/runtime.ps. </note>
Reference-contexts: The Concert system execution model is one of dynamic multithreading. It is implemented with runtime support from the Illinois Concert Runtime Library <ref> [13] </ref>. The Concert Runtime provides efficient primitives for synchronization and communication with remote nodes. It also provides functionality of memory management and garbage collection. We will detail the implementation of some of these runtime services in later sections. The Concert compiler is an aggressive, optimizing Concurrent Object-Oriented Programming language compiler.
Reference: [14] <author> Vijay Karamcheti and Andrew A. Chien. </author> <title> View caching: Efficient software shared memory for dynamic computations. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1997. </year>
Reference-contexts: It performs a number of sequential optimizations such as access region expansion, object inlining [10] and method inlining to optimize object-oriented code [20]. In addition, the compiler supports concurrent locality-based optimizations, such as dynamic pointer alignment [26] and view caching <ref> [14] </ref>. In both the sequential and parallel realms, these features make the Concert compiler state-of-the-art for object-oriented compilers. 1.5 Summary of Results Using our system on the SGI-Cray Origin 2000 shared memory machines, we have achieved good speedups for our SAMR application.
Reference: [15] <author> Vijay Karamcheti, John Plevyak, and Andrew A. Chien. </author> <title> Runtime mechanisms for efficient dynamic multithreading. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 37(1) </volume> <pages> 21-40, </pages> <year> 1996. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/rtperf.ps. </note>
Reference-contexts: In a later section, we present descriptions of the primitives in more detail as well as some performance measurements. 2.3.1 System Design The following is a brief description of the Illinois Concert System implementation. More in-depth descriptions are available at <ref> [15] </ref> and [7]. Keep in mind that these concepts are at the runtime system level, and are not exposed to the user of the system. The Illinois Concert System uses a dynamic multi-threading execution model. <p> In executions of our SAMR code, we have seen that garbage collection takes up an insignificant amount of time, even for the largest hierarchies we create. 43 4.2.3 Thread Scheduling In addition to the runtime support detailed in <ref> [15] </ref>, our shared memory version of the Concert runtime system has the additional dimension of multiple workers per address space. A worker is a kernel-level thread of execution which executes Concert threads (e.g. units of work).
Reference: [16] <author> S.R. Kohn and S.B. Baden. </author> <title> Irregular coarse-grain data parallelism under lparx. </title> <journal> Scientific Programming, </journal> <volume> 5(3), </volume> <month> Fall </month> <year> 1996. </year>
Reference-contexts: For more details, the reader is referred to [19]. 2.4 Related Parallel Systems Several related systems exist whose goal is similar to Concert. That is, they provide runtime support for to capture dynamic, irregular parallelism. We briefly outline two other approaches. 18 2.4.1 LPARX The LPARX <ref> [16] </ref> parallel system is designed to address the problem of block irregular mesh methods and the problems they present. The system is designed to parallelize methods such as SAMR efficiently, using a collaboration of user and runtime support.
Reference: [17] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The directory-based cache coherence protocol for the dash multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Each processor posseses an on-chip L1 cache of size 32 Kbytes. Each processor also has a large L2, off-chip cache of size 4MB. The system uses a directory-based cache coherence scheme utilizing a distributed directory. The coherence scheme is similar to the Stanford DASH <ref> [17] </ref> protocol. It is directory-based and implements a clean-exclusive state for cache lines for single-fetch updates to data. The system also manages the migration of pages using a directory scheme and methods to reduce the code of TLB updates.
Reference: [18] <author> Scott Pakin, Mario Lauria, and Andrew Chien. </author> <title> High performance messaging on workstations: Illinois Fast Messages (FM) for Myrinet. </title> <booktitle> In Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/myrinet-fm-sc95.ps. </note>
Reference-contexts: These data structures are what allow concurrent execution. One can conceptualize a thread as a method call on an object. * message In our execution model, remote accesses and method invocations are performed by sending messages. Concert messages are an implementation of Fast Messages <ref> [18] </ref>. * scheduler Each address space has a scheduler, which contains a list of runnable threads. The Concert virtual machine consists of a number of address spaces, each with the capability sending messages to the other.
Reference: [19] <author> John Plevyak, Vijay Karamcheti, Xingbin Zhang, and Andrew Chien. </author> <title> A hybrid execution model for fine-grained languages on distributed memory multicomputers. </title> <booktitle> In Proceedings of Supercomputing'95, </booktitle> <year> 1995. </year>
Reference-contexts: The Concert system was designed to tackle large programs and to be run on large-scale architectures. It has already demonstrated good performance on distributed memory machines on a number of codes [7], <ref> [19] </ref>. In this work, we are taking a new direction for the system in two ways. First, we have ported the Concert System to a large-scaled shared memory system. <p> In this case, the method's stack is stored in a thread data structure, and a new Concert thread is created, and enqueued for parallel execution. For more details, the reader is referred to <ref> [19] </ref>. 2.4 Related Parallel Systems Several related systems exist whose goal is similar to Concert. That is, they provide runtime support for to capture dynamic, irregular parallelism. <p> Note that in our efforts to parallelize the code, we have disabled several important optimizations, the most important of which are stack invocations <ref> [19] </ref> and object inlining [10]. While we suspect that these optimizations could assist in gaining sequential performance, the results we have obtained suggest that there is not enough room for improvement to pursue this direction.
Reference: [20] <author> John Plevyak, Xingbin Zhang, and Andrew A. Chien. </author> <title> Obtaining sequential efficiency in concurrent object-oriented programs. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 311-321, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: We will detail the implementation of some of these runtime services in later sections. The Concert compiler is an aggressive, optimizing Concurrent Object-Oriented Programming language compiler. It performs a number of sequential optimizations such as access region expansion, object inlining [10] and method inlining to optimize object-oriented code <ref> [20] </ref>. In addition, the compiler supports concurrent locality-based optimizations, such as dynamic pointer alignment [26] and view caching [14].
Reference: [21] <author> Constantine D. Polychronopoulos. </author> <title> Auto-scheduling: Control flow and data flow come together. </title> <type> Technical Report Technical Report No 1058, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, Urbana, Illinois, </institution> <year> 1990. </year>
Reference-contexts: If no local work, check for work in centralized queue. 4. If no centralized work, go to sleep. 5. If centralized work, acquire lock, dequeue max (numthreads/NUM-WORKERS-IN-SPACE, CONSTANT) Concert threads and enqueue them locally and repeat. 44 This algorithm employs a form of guided self-scheduling <ref> [21] </ref>, and is done to minimize con-tention and promote load balance among workers. Note that dequeuing needs to be efficient to minimize the size of the critical region. In practice, we have seen that the load balancing properties are very close to optimal.
Reference: [22] <author> Silicon Graphics, Inc. </author> <title> Origin Servers: Technical Overview of the Origin Family, </title> <note> 1996. Available from http://www.sgi.com/Products/hardware/servers/technology/ overview.html. </note>
Reference-contexts: Our system is also designed to overlap communication with computation as long as there are threads available to run. 2.5 Hardware Description We have run our experiments on the SGI-Cray Origin 2000 Machine at NCSA <ref> [22] </ref>. This is a hardware cache-coherent shared memory machine with up to 1024 MIPS R10000 processors with 1 TB of memory at maximum configurations. The configuration that we are using has 32 processors and a maximum of 12 GB of memory. <p> We expect this matching to yield good performance results. 46 Chapter 5 Performance 5.1 Overview In this chapter, we show the performance of our parallelized ICC++ code. All experiments were run on the SGI-Cray Origin 2000 machine at NCSA <ref> [22] </ref>. The structure of this chapter is as follows. In Section 5.2, we display performance of the ICC++ code running on one processor in reference to the same implementation in C++, compiled using a native compiler.
Reference: [23] <author> Erlendur Steinthorsson, David Modiano, William Y. Crutchfield, John B. Bell, , and Phillip Colella. </author> <title> Three dimensional adapative mesh refinement for hyperbolic conservation law. </title> <booktitle> In Proceedings of the 12th AIAA Computational Fluid Dynamics Conference, </booktitle> <pages> pages 902-912, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The router chips handle congestion and implement message priority using a packet aging scheme. 21 Chapter 3 SAMR Implementation Our study is based on an SAMR method written by scientists at NCSA in the Computational Cosmology group. It has been used in a number of cosmology experiments <ref> [4, 23] </ref>. It is written in C++ and uses Fortran 77 kernels to do perform the compute-intensive interpolation and partial differential equation solves. We were motivated to work with a real code (as opposed to a benchmark) for two main reasons.
Reference: [24] <author> Bjarne Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison Wesley, </publisher> <address> second edition, </address> <year> 1991. </year>
Reference-contexts: The design of the programming system is such that synchronization is done only when analysis of the code deems it necessary, and that the synchronization itself is efficient. The ICC++ language is derived from the object-oriented C++ language <ref> [24] </ref>. It provides minimal extensions to the sequential language, and has features supporting concurrent execution based on the Actor model of computation [2].
Reference: [25] <author> Gregory V. Wilson and Paul Lu, </author> <title> editors. Parallel Programming Using C++, chapter ICC++. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: The Illinois Concert system is designed to provide fine-grained parallelism, without forcing the user to explicitly manage the concurrent execution of the code. The Concert system supports fine-grained, concurrent object-oriented programming on Actors [2] in an extension of C++ called ICC++ <ref> [9, 25] </ref>. Computation is expressed as method invocations on objects or collections of objects. Method invocations conceptually operate within dynamically created threads that are inherently concurrent. Thus, the user only thinks about the implementation of the algorithm, and not about managing concurrency.
Reference: [26] <author> Xingbin Zhang and Andrew A. Chien. </author> <title> Dynamic pointer alignment: Tiling and communication optimizations for parallel pointer-based computations. </title> <booktitle> In Proceedings of ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Las Vegas, Nevada, </address> <month> June </month> <year> 1997. </year> <month> 92 </month>
Reference-contexts: The Concert compiler is an aggressive, optimizing Concurrent Object-Oriented Programming language compiler. It performs a number of sequential optimizations such as access region expansion, object inlining [10] and method inlining to optimize object-oriented code [20]. In addition, the compiler supports concurrent locality-based optimizations, such as dynamic pointer alignment <ref> [26] </ref> and view caching [14]. In both the sequential and parallel realms, these features make the Concert compiler state-of-the-art for object-oriented compilers. 1.5 Summary of Results Using our system on the SGI-Cray Origin 2000 shared memory machines, we have achieved good speedups for our SAMR application.
References-found: 26

