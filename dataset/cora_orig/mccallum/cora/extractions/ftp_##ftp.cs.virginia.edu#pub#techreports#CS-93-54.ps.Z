URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-93-54.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: An Analytic Model of SMC Performance  
Abstract: Sally A. McKee Computer Science Report No. CS-93-54 November 15, 1993 
Abstract-found: 1
Intro-found: 1
Reference: [Don79] <author> Dongarra, J.J., et. al., </author> <title> Linpack Users Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: Scalar and instruction references are assumed to hit in the cache, and all stream references use non-caching loads and stores. Our benchmark suite is depicted in Figure 2. Daxpy, copy, scale, and swap are from the BLAS (Basic Linear Algebra Subroutines) <ref> [Law79, Don79] </ref>. These vector and matrix computations occur frequently in scientific applications, thus they have been collected into a set of library routines that are highly optimized for various host architectures.
Reference: [Don90] <author> Dongarra, J.J., DuCroz, J., Duff, I., and Hammerling, S., </author> <title> A set of Level 3 Basic Linear Algebra Subprograms, </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: Note that although these computations do not reuse vector elements, they are often found in the inner loops of algorithms that do, as with vaxpy for vector-matrix multiply, and blocked algorithms such as those in the Level 3 BLAS <ref> [Don90] </ref>. 7. Results depth and available concurrency, i.e. the number of memory banks.. All results are given as a percentage of the systems peak bandwidth (the bandwidth necessary to allow the processor to perform a memory operation each cycle). The vectors used in these simulations are 10,000 doublewords in length.
Reference: [Gol93] <author> Golub, G., and Ortega, J.M., </author> <title> Scientific Computation: An Introduction with Parallel Computing, </title> <publisher> Academic Press, Inc., </publisher> <year> 1993. </year>
Reference-contexts: Vaxpy is a vector axpy computation that occurs in matrix-vector multiplication by diagonals; this algorithm is useful for the diagonally sparse matrices that arise frequently in the solution of parabolic or elliptic partial differential equations by finite element or finite difference methods <ref> [Gol93] </ref>. Here axpy refers to a computation involving some entity a times a vector x plus a vector y.
Reference: [Hen90] <author> Hennessy, J., and Patterson, D., </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: 1. Introduction The growing disparity between processor speeds and memory speeds is well known <ref> [Kat89, Hen90] </ref>. Memory bandwidth is becoming the limiting performance factor for many applications particularly scientific computations and alleviating this disparity is the subject of much current research.
Reference: [IEEE92] <author> High-speed DRAMs, </author> <title> Special Report, </title> <journal> IEEE Spectrum, </journal> <volume> vol. 29, no. 10, </volume> <month> October </month> <year> 1992. </year>
Reference: [Kat89] <author> Katz, R., and Hennessy, J., </author> <title> High Performance Microprocessor Architectures, </title> <institution> University of California, Berkeley, </institution> <note> Report No. UCB/CSD 89/529, </note> <month> August, </month> <year> 1989. </year>
Reference-contexts: 1. Introduction The growing disparity between processor speeds and memory speeds is well known <ref> [Kat89, Hen90] </ref>. Memory bandwidth is becoming the limiting performance factor for many applications particularly scientific computations and alleviating this disparity is the subject of much current research.
Reference: [Law79] <editor> Lawson, et. al., </editor> <title> Basic Linear Algebra Subprograms for Fortran Usage, </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 5, 3, </volume> <year> 1979. </year>
Reference-contexts: Scalar and instruction references are assumed to hit in the cache, and all stream references use non-caching loads and stores. Our benchmark suite is depicted in Figure 2. Daxpy, copy, scale, and swap are from the BLAS (Basic Linear Algebra Subroutines) <ref> [Law79, Don79] </ref>. These vector and matrix computations occur frequently in scientific applications, thus they have been collected into a set of library routines that are highly optimized for various host architectures.
Reference: [McK93a] <author> McKee, S.A, </author> <title> Hardware Support for Access Ordering: Performance of Some Design Options, </title> <institution> University of Virginia, Department of Computer Science, </institution> <type> Technical Report CS-93-08, </type> <month> August </month> <year> 1993. </year>
Reference-contexts: Access ordering is one technique that can help bridge the processor-memory performance gap. [Moy93] develops and analyzes algorithms to perform access ordering statically at compile time. <ref> [McK93a] </ref> proposes a combined hardware/software scheme for implementing access ordering dynamically at run-time, and presents numerous simulation results demonstrating its effectiveness. The hardware part of this solution is the Stream Memory Controller (SMC) [McK93b]. Here we develop an analytical model to bound SMC performance. 2. <p> Moreover, implementing such an algorithm might be expensive, both in the amount of hardware necessary and in the time required for it to run. We have instead developed a number of heuristic algorithms for dynamic access ordering; simulation results for these are presented in <ref> [McK93a] </ref>. Although we do not know precisely what the optimal ordering algorithm is, we can bound its performance. Taking advantage of the full bandwidth afforded by the memory system requires exploiting the page-mode capabilities of the memory components. <p> Complete uniprocessor results, including a detailed description of each access-ordering heuristic, can be found in <ref> [McK93a] </ref>; highlights of these results are presented in [McK93b, McK93c].
Reference: [McK93b] <author> McKee, S.A., Klenke, R.H., Schwab, A.J., Wulf, Wm.A., Moyer, S.A., Hitchcock, C., Aylor, J.H., </author> <title> Experimental Implementation of Dynamic Access Ordering, </title> <institution> University of Virginia, </institution> <note> TR CS-93-42, August 1993. To appear in Proc. </note> <institution> HICSS-27, </institution> <address> Maui, HI, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: The hardware part of this solution is the Stream Memory Controller (SMC) <ref> [McK93b] </ref>. Here we develop an analytical model to bound SMC performance. 2. <p> The beneficial impact of access ordering on effective memory bandwidth together with the limitations inherent in implementing the technique statically motivate us to consider an implementation that reorders accesses dynamically at run time. What follows is an overview of the architecture proposed in <ref> [McK93b, McK93c] </ref>: see those documents for more details. Our discussion is based on the simplified architecture of Figure 1. In this system, memory is interfaced to the processors through a controller labeled MSU for Memory Scheduling Unit. <p> Complete uniprocessor results, including a detailed description of each access-ordering heuristic, can be found in [McK93a]; highlights of these results are presented in <ref> [McK93b, McK93c] </ref>. Since our concern here is to correlate the performance predictions of our analytic model with our functional simulation results, we present only the maximum percentage of peak bandwidth attained by any order/issue policy simulated for a given memory system and benchmark. 6.
Reference: [McK93c] <author> McKee, S.A., Moyer, S.A., Wulf, Wm.A., Hitchcock, C., </author> <title> Increasing An Analytic Model of SMC Performance 20 Memory Bandwidth for Vector Computations, </title> <institution> University of Virginia, </institution> <note> TR CS-93-34, August 1993. To appear in Proc. Conf. on Prog. </note> <author> Lang. and Sys. </author> <title> Arch., </title> <address> Zurich, Switzerland, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: The beneficial impact of access ordering on effective memory bandwidth together with the limitations inherent in implementing the technique statically motivate us to consider an implementation that reorders accesses dynamically at run time. What follows is an overview of the architecture proposed in <ref> [McK93b, McK93c] </ref>: see those documents for more details. Our discussion is based on the simplified architecture of Figure 1. In this system, memory is interfaced to the processors through a controller labeled MSU for Memory Scheduling Unit. <p> Complete uniprocessor results, including a detailed description of each access-ordering heuristic, can be found in [McK93a]; highlights of these results are presented in <ref> [McK93b, McK93c] </ref>. Since our concern here is to correlate the performance predictions of our analytic model with our functional simulation results, we present only the maximum percentage of peak bandwidth attained by any order/issue policy simulated for a given memory system and benchmark. 6.
Reference: [McM86] <author> McMahon, F.H., </author> <title> The Livermore Fortran Kernels: A Computer Test of the Numerical Performance Range, </title> <institution> Lawrence Livermore National Laboratory, UCRL-53745, </institution> <month> December </month> <year> 1986. </year>
Reference-contexts: These vector and matrix computations occur frequently in scientific applications, thus they have been collected into a set of library routines that are highly optimized for various host architectures. Hydro and tridiag are the first and fifth Livermore Loops <ref> [McM86] </ref>, a set of kernels culled from important scientific computations. The former is a fragment of a hydrodynamics computation, and the latter is a tridiagonal elimination computation. Although the computations differ, their access patterns are identical, thus results for these benchmarks are presented together.
Reference: [Moy93] <author> Moyer, S.A., </author> <title> Access Ordering and Effective Memory Bandwidth, </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, University of Virginia, </institution> <type> Technical Report CS-93-18, </type> <month> April </month> <year> 1993. </year>
Reference-contexts: Memory bandwidth is becoming the limiting performance factor for many applications particularly scientific computations and alleviating this disparity is the subject of much current research. Access ordering is one technique that can help bridge the processor-memory performance gap. <ref> [Moy93] </ref> develops and analyzes algorithms to perform access ordering statically at compile time. [McK93a] proposes a combined hardware/software scheme for implementing access ordering dynamically at run-time, and presents numerous simulation results demonstrating its effectiveness. The hardware part of this solution is the Stream Memory Controller (SMC) [McK93b]. <p> One way to do this is via access ordering, which we define as any technique for changing the order of memory requests to increase bandwidth. Here we are especially concerned with ordering a set of vector-like stream accesses. 3. The SMC <ref> [Moy93] </ref> develops algorithms and analyzes the performance benefits and limitations of doing compile-time access ordering. The beneficial impact of access ordering on effective memory bandwidth together with the limitations inherent in implementing the technique statically motivate us to consider an implementation that reorders accesses dynamically at run time.
Reference: [Qui91] <author> Quinnell, R., </author> <title> High-speed DRAMs, </title> <type> EDN, </type> <month> May 23, </month> <year> 1991. </year>
Reference-contexts: For instance, nearly all current DRAMs implement a form of page-mode operation <ref> [Qui91] </ref>. These devices behave as if implemented with a single on-chip cache line, or page (this should not be confused with a virtual memory page). A memory access falling outside the address range of the current DRAM page forces a new page to be accessed.
Reference: [Ram92] <institution> Architectural Overview, Rambus Inc., Mountain View, </institution> <address> CA, </address> <year> 1992. </year>
References-found: 14

