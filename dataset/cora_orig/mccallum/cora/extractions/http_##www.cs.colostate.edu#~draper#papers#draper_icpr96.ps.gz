URL: http://www.cs.colostate.edu/~draper/papers/draper_icpr96.ps.gz
Refering-URL: http://www.cs.colostate.edu/~draper/
Root-URL: 
Email: bdraper@cs.umass.edu  
Title: Modeling Object Recognition as a Markov Decision Process  
Author: Bruce A. Draper 
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: The field of computer vision has made significant advances over the past twenty years, yet we still have not developed a theoretical or practical understanding of how the many components of vision are combined into coherent, functioning systems. As a result, there are few applications of computer vision technology in the real world, even though the library of available computer vision techniques keeps growing. This paper models the control of visual procedures as a Markov Decision Problem, and presents a version of the Schema Learning System (SLS) that uses this model to assemble object recognition programs from existing computer vision algorithms. An example of SLS learning to recognize rooftops in aerial images is presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Collins, et al. </author> <title> "UMass Progress in 3D Building Model Acquisition," </title> <booktitle> Image Understanding Workshop, </booktitle> <year> 1996. </year>
Reference-contexts: On each trial, the system is given an image containing one or sometimes two buildings, the angle of the sun, and a set of 3D line segments computed as described in <ref> [1] </ref>. The height of the ground plane is also known, and is used to filter the initial set of line segments. This corresponds roughly to the data available to the hand-crafted system described in [1]. <p> the angle of the sun, and a set of 3D line segments computed as described in <ref> [1] </ref>. The height of the ground plane is also known, and is used to filter the initial set of line segments. This corresponds roughly to the data available to the hand-crafted system described in [1]. <p> Each representation has a predefined set of features, as described in Section 5.2. The visual procedures in the library are meant to approximate some of the techniques being used by researchers in the RADIUS project. The procedure for computing 3D line segments is described in <ref> [1] </ref>. The pairwise grouping procedure locates parallel, colinear and orthogonal line pairs, using information about the camera pose (available for all RADIUS images) to remove the effects of perspective distortion.
Reference: [2] <author> B. Draper. </author> <title> "Learning Control Strategies for Object Recognition," in Symbolic Visual Learning, </title> <editor> Ikeuchi and Veloso (eds.), </editor> <publisher> Oxford University Press, </publisher> <year> 1996. </year>
Reference-contexts: Draper <ref> [2] </ref> and Roberts & Brown [5], for example, have built systems that learn to recognize objects from examples, while Peng et al [4] proposes a system for learning to parameterize a complex image segmentation algorithm.
Reference: [3] <author> D.Gerson and S.Wood, </author> <title> "RADIUS Phase II The RADIUS Testbed System," </title> <booktitle> Image Understanding Workshop, </booktitle> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: principle, we should be able to transform the task of constructing a new vision application to one of training the system with a set of representative input-output examples, and the new version of SLS presented here is meant to demonstrate this capability. 3 Learning to Recognize Buildings The RADIUS project <ref> [3] </ref> is an interesting example of both the importance of image understanding technology and the problems with it. Current U.S. military doctrine is to achieve dominant battlefield awareness by digitizing and interpreting ever-increasing numbers of images.
Reference: [4] <author> J. Peng, B. Bhanu and R. Dutta. </author> <title> Delayed Reinforcement Learning for Closed-Loop Object Recognition, </title> <institution> University of California (Riverside), </institution> <year> 1995. </year>
Reference-contexts: Draper [2] and Roberts & Brown [5], for example, have built systems that learn to recognize objects from examples, while Peng et al <ref> [4] </ref> proposes a system for learning to parameterize a complex image segmentation algorithm.
Reference: [5] <author> B. Roberts and C. Brown. </author> <title> "Adaptive Configuration and Control in an ATR System," </title> <booktitle> Image Understanding Workshop, </booktitle> <address> Monterey, CA, </address> <year> 1994. </year>
Reference-contexts: Draper [2] and Roberts & Brown <ref> [5] </ref>, for example, have built systems that learn to recognize objects from examples, while Peng et al [4] proposes a system for learning to parameterize a complex image segmentation algorithm.
Reference: [6] <author> G. Tesauro. </author> <title> "Temporal Difference Learning and TD-Gammon" Communications of the ACM, </title> <booktitle> 38(3) </booktitle> <pages> 58-68 </pages>
Reference-contexts: Although the traditional control-theoretic techniques for solving MDPs (e.g. Dynamic Programming) require a more detailed process model than is generally available for computer vision applications, we believe that recent advances in reinforcement learning and function approximation <ref> [6, 7] </ref> make it possible to learn near-optimal control policies for image understanding. <p> Mathematically, this is a clean formulation of the problem. In practice, it only works if we can learn estimate for the Q fl (s; a) or V fl (s) functions over these infinite state spaces from a finite number of samples. Inspired by Tesauro <ref> [6] </ref> and Zhang and Dietterich [7], we use back-propagation neural networks to learn approximations to the Q fl (s; a) function for each action, as described below in Section 5.4. <p> These algorithms build successively better approximations of Q and V based on experience gained by running the system; Tesauro <ref> [6] </ref> and Zhang and Dietterich [7] have used these techniques with neural network function approximators to learn Q values for systems with continuous state spaces.
Reference: [7] <author> W. Zhang and T. Dietterich. </author> <title> "A Reinforcement Learning Approach to Job-Shop Scheduling," </title> <booktitle> Int. Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year> <month> 5 </month>
Reference-contexts: Although the traditional control-theoretic techniques for solving MDPs (e.g. Dynamic Programming) require a more detailed process model than is generally available for computer vision applications, we believe that recent advances in reinforcement learning and function approximation <ref> [6, 7] </ref> make it possible to learn near-optimal control policies for image understanding. <p> Mathematically, this is a clean formulation of the problem. In practice, it only works if we can learn estimate for the Q fl (s; a) or V fl (s) functions over these infinite state spaces from a finite number of samples. Inspired by Tesauro [6] and Zhang and Dietterich <ref> [7] </ref>, we use back-propagation neural networks to learn approximations to the Q fl (s; a) function for each action, as described below in Section 5.4. The control policies learned by SLS are therefore represented as a set of neural networks, each approximating the Q function for a state/action pair. <p> These algorithms build successively better approximations of Q and V based on experience gained by running the system; Tesauro [6] and Zhang and Dietterich <ref> [7] </ref> have used these techniques with neural network function approximators to learn Q values for systems with continuous state spaces.
References-found: 7

