URL: http://http.cs.berkeley.edu/~miyamoto/classes/spring98/project/papers/wolf91a.ps.gz
Refering-URL: http://http.cs.berkeley.edu/~miyamoto/classes/spring98/project/index.html
Root-URL: http://www.cs.berkeley.edu
Title: A Data Locality Optimizing Algorithm Intermediate Format) compiler, and is successful in optimizing codes such
Author: Michael E. Wolf and Monica S. Lam 
Note: The algorithm has been implemented in the SUIF (Stan-ford University  
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: This paper proposes an algorithm that improves the locality of a loop nest by transforming the code via interchange, reversal, skewing and tiling. The loop transformation algorithm is based on two concepts: a mathematical formulation of reuse and locality, and a loop transformation theory that unifies the various transforms as unimodular matrix transformations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> Nov </month> <year> 1978. </year>
Reference-contexts: 1 Introduction As processor speed continues to increase faster than memory speed, optimizations to use the memory hierarchy efficiently become ever more important. Blocking [9] or tiling [18] is a well-known technique that improves the data locality of numerical algorithms <ref> [1, 6, 7, 12, 13] </ref>. Tiling can be used for different levels of memory hierarchy such as physical memory, caches and registers; multi-level tiling can be used to achieve locality in multiple levels of the memory hierarchy simultaneously. <p> For short, we simply denote such a range with the value itself. There are three common ranges found in practice: <ref> [1; 1] </ref> denoted by `+', [1; 1] denoted by `', and [1; 1] denoted by `'. They correspond to the previously defined directions of `&lt; ', ` &gt; ', and `fl', respectively [17]. We have extended the definition of vector operations to allow for ranges in each of the component. <p> For short, we simply denote such a range with the value itself. There are three common ranges found in practice: <ref> [1; 1] </ref> denoted by `+', [1; 1] denoted by `', and [1; 1] denoted by `'. They correspond to the previously defined directions of `&lt; ', ` &gt; ', and `fl', respectively [17]. We have extended the definition of vector operations to allow for ranges in each of the component. <p> For short, we simply denote such a range with the value itself. There are three common ranges found in practice: <ref> [1; 1] </ref> denoted by `+', [1; 1] denoted by `', and [1; 1] denoted by `'. They correspond to the previously defined directions of `&lt; ', ` &gt; ', and `fl', respectively [17]. We have extended the definition of vector operations to allow for ranges in each of the component.
Reference: [2] <author> U. Banerjee. </author> <title> Data dependence in ordinary programs. </title> <type> Technical Report 76-837, </type> <institution> University of Illinios at Urbana-Champaign, </institution> <month> Nov </month> <year> 1976. </year>
Reference-contexts: This is also equivalent to finding non-rectangular tiles. Figure 2 contains the entire derivation of the tiled code, which we will use to illustrate our locality algorithm in the rest of the paper. There are two major representations used in loop transformations: distance vectors and direction vectors <ref> [2, 17] </ref>. Loops whose dependences can be summarized by distance vectors are special in that it is advantageous, possible and easy to tile all loops [10, 15]. General loop nests, whose dependences are represented by direction vectors, may not be tilable in their entirety.
Reference: [3] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic, </publisher> <year> 1988. </year>
Reference-contexts: Therefore if the transformed dependence vector remains lexicographically positive, the interchange is legal. The loop reversal and skewing transform can similarly be represented as matrices <ref> [3, 4, 16] </ref>. (An example of skewing is shown in Figure 2 (d).) These matrices are uni-modular matrices, that is, they are square matrices with integral components and a determinant of one or negative one.
Reference: [4] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In 3rd Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug </month> <year> 1990. </year>
Reference-contexts: Wolfe also shows that skewing can make a pair of loops tilable. Banerjee discusses general unimodular transforms for two-deep loop nests <ref> [4] </ref>. A technique used in practice to handle general n-dimensional loop nests is to determine a priori the sequence of loop transforms to attempt. This technique is inadequate because certain transformations, such as loop skewing, may not improve code, but may enable other optimizations that do so. <p> Therefore if the transformed dependence vector remains lexicographically positive, the interchange is legal. The loop reversal and skewing transform can similarly be represented as matrices <ref> [3, 4, 16] </ref>. (An example of skewing is shown in Figure 2 (d).) These matrices are uni-modular matrices, that is, they are square matrices with integral components and a determinant of one or negative one.
Reference: [5] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: The SGI 4D/380 has eight MIPS/R3000 processors running at 33 Mhz. Each processor has a 64 KB direct-mapped first-level cache and a 256 KB direct-mapped second-level cache. We ran four different experiments: without tiling, tiling to reuse data in caches, tiling to reuse data in registers <ref> [5] </ref>, and tiling for both register and caches. For cache tiling, the data are copied into consecutive locations to avoid cache interference [12]. Tiling improves the performance on a single processor by a factor of 2:75. <p> This has led to work on combinations of primitive transforms. For example, Wolfe [18] shows how to determine when a loop nest can be tiled; two-dimensional tiling can be achieved via a pair of transformations known as "strip-mine and interchange" [14] or "unroll and jam" <ref> [5] </ref>. Wolfe also shows that skewing can make a pair of loops tilable. Banerjee discusses general unimodular transforms for two-deep loop nests [4]. A technique used in practice to handle general n-dimensional loop nests is to determine a priori the sequence of loop transforms to attempt. <p> The numbers reported below are generated as follows. We used the compiler to generate tiled C code for a single processor. We then performed, by hand, optimizations such as register allocation of array elements <ref> [5] </ref>, moving loop-invariant address calculation code out of the innermost loop, and unrolling the innermost loop. We then compiled the code using the SGI's optimizing compiler. To run the code on multiple processors, we adopt the model of executing the tiles in a DO-ACROSS manner [16].
Reference: [6] <author> J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <pages> pages 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: 1 Introduction As processor speed continues to increase faster than memory speed, optimizations to use the memory hierarchy efficiently become ever more important. Blocking [9] or tiling [18] is a well-known technique that improves the data locality of numerical algorithms <ref> [1, 6, 7, 12, 13] </ref>. Tiling can be used for different levels of memory hierarchy such as physical memory, caches and registers; multi-level tiling can be used to achieve locality in multiple levels of the memory hierarchy simultaneously.
Reference: [7] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh. </author> <title> The impact of hierarchical memory systems on linear algebra algorithm design. </title> <type> Technical report, </type> <institution> University of Illinios, </institution> <year> 1987. </year>
Reference-contexts: 1 Introduction As processor speed continues to increase faster than memory speed, optimizations to use the memory hierarchy efficiently become ever more important. Blocking [9] or tiling [18] is a well-known technique that improves the data locality of numerical algorithms <ref> [1, 6, 7, 12, 13] </ref>. Tiling can be used for different levels of memory hierarchy such as physical memory, caches and registers; multi-level tiling can be used to achieve locality in multiple levels of the memory hierarchy simultaneously.
Reference: [8] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: Porterfield gives an algorithm for estimating the hit rate of a fully-associative LRU (least recently used replacement policy) cache of a given size [14]. Gannon et al. uses reference windows to determine the minimum memory locations necessary to maximize reuse in a loop nest <ref> [8] </ref>. These evaluation functions are useful for comparing the locality performance after applying transformations, but do not suggest the transformations to apply when a series of transformations may first need to be applied before tiling becomes feasible and useful. <p> Such references are known as uniformly generated references. The concept of uniformly generated references is also used by Gannon et al. <ref> [8] </ref> in estimating their reference windows. Definition 4.1 Let n be the depth of a loop nest, and d be the dimensions of an array A.
Reference: [9] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction As processor speed continues to increase faster than memory speed, optimizations to use the memory hierarchy efficiently become ever more important. Blocking <ref> [9] </ref> or tiling [18] is a well-known technique that improves the data locality of numerical algorithms [1, 6, 7, 12, 13].
Reference: [10] <author> F. Irigoin and R. Triolet. </author> <title> Computing dependence direction vectors and dependence cones. </title> <type> Technical Report E94, </type> <institution> Centre D'Automatique et Informatique, </institution> <year> 1988. </year>
Reference-contexts: There are two major representations used in loop transformations: distance vectors and direction vectors [2, 17]. Loops whose dependences can be summarized by distance vectors are special in that it is advantageous, possible and easy to tile all loops <ref> [10, 15] </ref>. General loop nests, whose dependences are represented by direction vectors, may not be tilable in their entirety.
Reference: [11] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Proc. 15th Annual ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <month> Jan-uary </month> <year> 1988. </year>
Reference-contexts: Non-rectangular blocks are obtained by first applying unimodular transformations to the iteration space and then applying tiling. Like all transformations, it is not always possible to tile. Loops I i through I j in a loop nest can be tiled if they are fully permutable <ref> [11, 16] </ref>.
Reference: [12] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: 1 Introduction As processor speed continues to increase faster than memory speed, optimizations to use the memory hierarchy efficiently become ever more important. Blocking [9] or tiling [18] is a well-known technique that improves the data locality of numerical algorithms <ref> [1, 6, 7, 12, 13] </ref>. Tiling can be used for different levels of memory hierarchy such as physical memory, caches and registers; multi-level tiling can be used to achieve locality in multiple levels of the memory hierarchy simultaneously. <p> We ran four different experiments: without tiling, tiling to reuse data in caches, tiling to reuse data in registers [5], and tiling for both register and caches. For cache tiling, the data are copied into consecutive locations to avoid cache interference <ref> [12] </ref>. Tiling improves the performance on a single processor by a factor of 2:75. The effect of tiling on multiple processors is even more significant since it not only reduces the average data access latency but also the required memory bandwidth. <p> Caches usually have small set associativity, so cache data conflicts can cause desired data to be replaced. We have found that the performance of tiled code fluctuates dramatically with the size of the data matrix, due to cache interference <ref> [12] </ref>. We show that this effect can be mitigated by copying reused data to consecutive locations before the computation, or choosing the tile size according to the matrix size. <p> The approach is to first transform the code via interchange, reversal, skewing and tiling, then determine the tile size, taking into account data conflicts due to the set associativity of the cache <ref> [12] </ref>. The loop transformation algorithm is based on two concepts: a mathematical formulation of reuse and locality, and a matrix-based loop transformation theory. While previous work on evaluating locality estimates the number of memory accesses directly for a given transformed code, we break the evaluation into three parts.
Reference: [13] <author> A. C. McKeller and E. G. Coffman. </author> <title> The organization of matrices and matrix operations in a paged multiprogramming environment. </title> <journal> CACM, </journal> <volume> 12(3) </volume> <pages> 153-165, </pages> <year> 1969. </year>
Reference-contexts: 1 Introduction As processor speed continues to increase faster than memory speed, optimizations to use the memory hierarchy efficiently become ever more important. Blocking [9] or tiling [18] is a well-known technique that improves the data locality of numerical algorithms <ref> [1, 6, 7, 12, 13] </ref>. Tiling can be used for different levels of memory hierarchy such as physical memory, caches and registers; multi-level tiling can be used to achieve locality in multiple levels of the memory hierarchy simultaneously.
Reference: [14] <author> A. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: This has led to work on combinations of primitive transforms. For example, Wolfe [18] shows how to determine when a loop nest can be tiled; two-dimensional tiling can be achieved via a pair of transformations known as "strip-mine and interchange" <ref> [14] </ref> or "unroll and jam" [5]. Wolfe also shows that skewing can make a pair of loops tilable. Banerjee discusses general unimodular transforms for two-deep loop nests [4]. <p> On the desirability of tiling, previous work concentrated on how to determine the cache performance and tune the loop parameters for a given loop nest. Porterfield gives an algorithm for estimating the hit rate of a fully-associative LRU (least recently used replacement policy) cache of a given size <ref> [14] </ref>. Gannon et al. uses reference windows to determine the minimum memory locations necessary to maximize reuse in a loop nest [8].
Reference: [15] <author> R. Schreiber and J. Dongarra. </author> <title> Automatic blocking of nested loops. </title> <year> 1990. </year>
Reference-contexts: There are two major representations used in loop transformations: distance vectors and direction vectors [2, 17]. Loops whose dependences can be summarized by distance vectors are special in that it is advantageous, possible and easy to tile all loops <ref> [10, 15] </ref>. General loop nests, whose dependences are represented by direction vectors, may not be tilable in their entirety. <p> The question of whether there exists a unimodular transformation that is legal and creates such an innermost subspace is a difficult one. An existing algorithm that attempts to find such a transform is exponential in the number of loops <ref> [15] </ref>. The general question of finding a legal transformation that minimizes the number of memory accesses as determined by the intersection of the localized and reused vector spaces is even harder. Although the problem is theoretically difficult, loop nests found in practice are generally simple.
Reference: [16] <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> July </month> <year> 1991. </year>
Reference-contexts: Our loop transformation theory offers a foundation for deriving compound transformations efficiently <ref> [16] </ref>. <p> Therefore if the transformed dependence vector remains lexicographically positive, the interchange is legal. The loop reversal and skewing transform can similarly be represented as matrices <ref> [3, 4, 16] </ref>. (An example of skewing is shown in Figure 2 (d).) These matrices are uni-modular matrices, that is, they are square matrices with integral components and a determinant of one or negative one. <p> Non-rectangular blocks are obtained by first applying unimodular transformations to the iteration space and then applying tiling. Like all transformations, it is not always possible to tile. Loops I i through I j in a loop nest can be tiled if they are fully permutable <ref> [11, 16] </ref>. <p> For example, the components of dependences in Figure 2 (b) are all non-negative, and the two loops are therefore fully permutable and tilable. Full permutabil-ity is also very useful for improving parallelism <ref> [16] </ref>, so parallelism and locality are often compatible goals. After tiling, both groups of loops, the loops within a tile and the loops controlling the tiles, remain fully permutable. <p> We first attempt to order the outer loops|the loops in I but not in I. Any transformation applied to the loops in I I that result in these loops being outermost and no dependences being violated by these loops is sufficient <ref> [16] </ref>. If that step succeeds, then we attempt to tile the I loops innermost, which means finding a transformation that turns these loops into a fully permutable loop nest, given the outer nest. Solving these problems exactly is still exponential in the number of dependences. <p> We have developed a heuristic compound transformation, known as the SRP transform, which is useful in both steps of the transformation algorithm. The SRP transformation attempts to make a set of loops fully permutable by applying combinations of permutation, skewing and reversal <ref> [16] </ref>. If it cannot place all loops in a single fully permutable nest, it simply finds the outermost nest, and returns all remaining loops and dependences left to be made lexicographically positive. The algorithm is based upon the observations in Theorem 5.1 and Corollary 5.2. <p> The algorithm is O (n 2 d), where n is the loop nest depth and d is the number of dependence vectors. With the extension of a simple 2D time-cone solver <ref> [16] </ref>, it becomes O (n 3 d) but can find a transformation that makes any two loops fully permutable, and therefore tilable, if some such transformation exists. If there is little reuse, or if data dependences constrain the legal ordering possibilities, the algorithm is fast since I is small. <p> We then compiled the code using the SGI's optimizing compiler. To run the code on multiple processors, we adopt the model of executing the tiles in a DO-ACROSS manner <ref> [16] </ref>. This code has the same structure as the sequential code. <p> None of the original loops is parallelizable. The first verson, labeled "DOALL", is transformed via wavefronting so that the middle loop is a DOALL loop <ref> [16] </ref> (Figure 7 (b)). This transformation unfortunately destroys the original locality within the code. Thus, performance is abysmal on a single processor, and speedup for multiple processors is equally abysmal even though again there is plenty of available parallelism in the I 0 2 loop.
Reference: [17] <author> M. J. Wolfe. </author> <title> Techniques for improving the inherent parallelism in programs. </title> <type> Technical Report UIUCDCS-R-78-929, </type> <institution> University of Illinois, </institution> <year> 1978. </year>
Reference-contexts: This is also equivalent to finding non-rectangular tiles. Figure 2 contains the entire derivation of the tiled code, which we will use to illustrate our locality algorithm in the rest of the paper. There are two major representations used in loop transformations: distance vectors and direction vectors <ref> [2, 17] </ref>. Loops whose dependences can be summarized by distance vectors are special in that it is advantageous, possible and easy to tile all loops [10, 15]. General loop nests, whose dependences are represented by direction vectors, may not be tilable in their entirety. <p> There are three common ranges found in practice: [1; 1] denoted by `+', [1; 1] denoted by `', and [1; 1] denoted by `'. They correspond to the previously defined directions of `&lt; ', ` &gt; ', and `fl', respectively <ref> [17] </ref>. We have extended the definition of vector operations to allow for ranges in each of the component. In this way, we can manipulate a combination of distances and directions simply as vectors. This is needed to support the matrix transform model, discussed below.
Reference: [18] <author> M. J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Supercomputing '89, </booktitle> <month> Nov </month> <year> 1989. </year>
Reference-contexts: 1 Introduction As processor speed continues to increase faster than memory speed, optimizations to use the memory hierarchy efficiently become ever more important. Blocking [9] or tiling <ref> [18] </ref> is a well-known technique that improves the data locality of numerical algorithms [1, 6, 7, 12, 13]. <p> However, in general, it is necessary to apply a series of primitive transformations to achieve goals such as parallelism and data locality. This has led to work on combinations of primitive transforms. For example, Wolfe <ref> [18] </ref> shows how to determine when a loop nest can be tiled; two-dimensional tiling can be achieved via a pair of transformations known as "strip-mine and interchange" [14] or "unroll and jam" [5]. Wolfe also shows that skewing can make a pair of loops tilable.
References-found: 18

