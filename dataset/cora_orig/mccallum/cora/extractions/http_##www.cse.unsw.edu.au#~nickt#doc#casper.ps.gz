URL: http://www.cse.unsw.edu.au/~nickt/doc/casper.ps.gz
Refering-URL: http://www.cse.unsw.edu.au/~nickt/index.html
Root-URL: http://www.cse.unsw.edu.au
Email: -@cse.unsw.edu.au  
Title: A Cascade Network Algorithm Employing Progressive RPROP  
Author: N.K. Treadgold and T.D. Gedeon nickt tom 
Address: Sydney N.S.W. 2052 AUSTRALIA  
Affiliation: School of Computer Science Engineering The University of New South Wales  
Abstract: Cascade Correlation (Cascor) has proved to be a powerful method for training neural networks. Cascor, however, has been shown not to generalise well on regression and some classification problems. A new Cascade network algorithm employing Progressive RPROP (Casper), is proposed. Casper, like Cascor, is a constructive learning algorithm which builds cascade networks. Instead of using weight freezing and a correlation measure to install new neurons, however, Casper uses a variation of RPROP to train the whole network. Casper is shown to produce more compact networks, which generalise better than Cascor. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Adams, A., and Waugh, S. </author> <booktitle> (1995) Function Evaluation and the Cascade-Correlation architecture In Proc. 1995 IEEE Int. Conf. Neural Networks. </booktitle> <pages> pp. 942-946. </pages>
Reference: <author> Fahlman, </author> <title> S.E. (1988) Faster learning variations on backpropagation: An empirical study. </title> <booktitle> In Proc. 1988 Connectionist Models Summer School. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kauffman Fahlman, </publisher> <editor> S.E., and Lebiere, C. </editor> <booktitle> (1990) The cascade-correlation learning architecture. In Advances in Neural Information Processing II, </booktitle> <editor> Touretzky, Ed. </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kauffman, </publisher> <year> 1990, </year> <pages> pp. 524-532. </pages>
Reference-contexts: Thus there are two training phases: the training of hidden neuron weights, and the training of output weights. Both training phases use the QuickProp algorithm <ref> (Fahlman, 1988) </ref>. The separation of training phases allows Cascor to train a pool of neurons (with different random starting weights), and to select the neuron with the best resulting correlation for insertion. <p> In Casper a number of parameters require setting. The following (standard) parameter values were used for RPROP: h + - -6 addition, a constant value of 0.0001 was added to the derivative of the sigmoid in order to overcome the flat spot problem <ref> (Fahlman, 1988) </ref>. The hyperbolic arctan error function (Fahlman, 1988) was used for classification problems, while the standard sum of squares error function was used for regression problems. <p> The following (standard) parameter values were used for RPROP: h + - -6 addition, a constant value of 0.0001 was added to the derivative of the sigmoid in order to overcome the flat spot problem <ref> (Fahlman, 1988) </ref>. The hyperbolic arctan error function (Fahlman, 1988) was used for classification problems, while the standard sum of squares error function was used for regression problems. All weights were initialised to random values in the range -0.7 to 0.7, and the standard symmetric sigmoid non linearity (-0.5, 0.5) was used for the hidden units.
Reference: <author> Hwang, J., Lay, S., Maechler, R. And Martin, D. </author> <title> (1994) Regression Modeling in BackPropagation and Projection Pursuit Learning. </title> <journal> IEEE Trans. Neural Networks vol. </journal> <volume> 5, no. 3. </volume> <pages> pp. 342-353. </pages>
Reference-contexts: The same abscissa values were used for all five functions. The noisy data was created by adding independent and identically distributed (iid) Gaussian noise, with zero mean and unit variance, giving an approximate signal to noise ratio of 4 <ref> (Hwang et al., 1994) </ref>. For each function an independent test set of size 2500 was generated on a regularly spaced grid [0,1] 2 , as used by Hwang et al. (1996). <p> The fraction of variance unexplained (FVU) was the measure chosen to compare the performance of Casper and Cascor on the test set <ref> (Hwang et al. 1994) </ref>. FVU is defined as: For each regression function 100 runs were performed using different random starting weight values. Training was continued for both algorithms until 30 hidden units had been installed. <p> The use of higher order neurons has been utilised successfully in other training algorithms such as Projection Pursuit Learning methods <ref> (Hwang et al, 1994) </ref>. CONCLUSION A new constructive algorithm, Casper, is proposed. We have shown that weight freezing and the correlation measure employed in Cascor are not required to produce networks capable of solving complex problems.
Reference: <author> Hwang, J., You, S., Lay, S., and Jou, I. </author> <title> (1996) The Cascade-Correlation Learning: A Projection Pursuit Learning Perspective. </title> <journal> IEEE Trans. Neural Networks vol. </journal> <volume> 7, no. 2. </volume> <pages> pp. 278-289. </pages>
Reference: <author> Kwok, T., and Yeung, D. </author> <title> (1993) Experimental Analysis of Input Weight Freezing in Constructive Neural Networks. </title> <booktitle> In Proc. 1993 IEEE Int. Conf. Neural Networks. </booktitle> <pages> pp. 511-516. </pages>
Reference: <author> Riedmiller, M. and Braun, H. </author> <title> (1993) A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm. </title> <editor> In: Ruspini, H., (Ed.) </editor> <booktitle> Proc. of the ICNN 93, </booktitle> <address> San Francisco , pp. </address> <pages> 586-591. </pages>
Reference: <author> Riedmiller, M. </author> <title> (1994) Rprop - Description and Implementation Details, </title> <type> Technical Report, </type> <institution> University of Karlsruhe. </institution>
Reference: <author> Treadgold, </author> <title> N.K., and Gedeon, T.D. (1996) A Simulated Annealing Enhancement to Resilient Backpropagation. </title> <booktitle> Proc. Int. Panel Conf. Soft and Intelligent Computing, </booktitle> <pages> Budapest pp. 293-298. </pages>
Reference-contexts: Casper also makes use of weight decay as a means to improve the generalisation properties of the constructed network. After some experimentation we found that the addition of a Simulated Annealing (SA) term applied to the weight decay, as used in the SARPROP algorithm <ref> (Treadgold and Gedeon, 1996) </ref> often improved convergence and generalisation. Each time a new hidden unit is inserted, the weight decay begins with a large magnitude, which is then reduced by the SA term.
References-found: 8

