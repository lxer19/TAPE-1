URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/opitz.tr94.ps
Refering-URL: http://www.lehigh.edu/~ob00/integrated/references-new.html
Root-URL: 
Email: fopitz, shavlikg@cs.wisc.edu  
Title: Dynamically Adding Symbolically Meaningful Nodes to Knowledge-Based Neural Networks  
Author: David W. Opitz and Jude W. Shavlik 
Keyword: network-growing algorithm theory refinement  
Note: the Kbann algorithm computational biology Submitted to the journal Knowledge-Based Systems.  
Address: 1210 W. Dayton St. Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin Madison  
Abstract: Traditional connectionist theory-refinement systems map the dependencies of a domain-specific rule base into a neural network, and then refine this network using neural learning techniques. Most of these systems, however, lack the ability to refine their network's topology and are thus unable to add new rules to the (reformulated) rule base. Therefore, on domain theories that are lacking rules, generalization is poor, and training can corrupt the original rules, even those that were initially correct. We present TopGen, an extension to the Kbann algorithm, that heuristically searches for possible expansions to Kbann's network. TopGen does this by dynamically adding hidden nodes to the neural representation of the domain theory, in a manner analogous to adding rules and conjuncts to the symbolic rule base. Experiments indicate that our method is able to heuristically find effective places to add nodes to the knowledge bases of four real-world problems, as well as an artificial chess domain. The experiments also verify that new nodes must be added in an intelligent manner. Our algorithm showed statistically significant improvements over Kbann in all five domains. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baum, E. & Lang, K. </author> <year> (1991). </year> <title> Constructing hidden units using examples and queries. </title> <editor> In Lipp-mann, R., Moody, J., & Touretzky, D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 3), </booktitle> <pages> (pp. 904-910), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1994). </year> <title> Machine learning approaches to gene recognition. </title> <journal> IEEE Expert, </journal> <volume> 9(2) </volume> <pages> 2-10. </pages>
Reference: <author> Fahlman, S. & Lebiere, C. </author> <year> (1989). </year> <title> The cascade-correlation learning architecture. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2), </booktitle> <pages> (pp. 524-532), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Towell (1991) showed that Kbann was superior to Either on a DNA task, and as reported in Section 5, TopGen outperforms Kbann. A final area related to TopGen is network-growing algorithms, such as Cascade Correlation <ref> (Fahlman & Lebiere, 1989) </ref>, the Tiling algorithm (Mezard & Nadal, 1989), and the Upstart algorithm (Frean, 1990). The most obvious difference between TopGen and these algorithms is that TopGen uses domain knowledge and symbolic rule-refinement techniques to help determine the network's topology.
Reference: <author> Fletcher, J. & Obradovic, Z. </author> <year> (1993). </year> <title> Combining prior symbolic knowledge and constructive neural network learning. </title> <journal> Connection Science, </journal> <volume> 5(4) </volume> <pages> 365-375. </pages>
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2(2) </volume> <pages> 198-209. </pages>
Reference-contexts: Most of these systems also work by first translating the domain knowledge into a network, then modifying its weights with neural learning; however, of these, only Rapture (Mahoney & Mooney, 1993) is able modify its architecture during training. It does this by using the Upstart algorithm <ref> (Frean, 1990) </ref> to add new nodes to the network. Additional related work includes purely symbolic theory-refinement systems. Systems such as Either (Ourston & Mooney, 1994) and Rtls (Ginsberg, 1990) are also propositional in nature. These systems differ from TopGen, in that their approaches are purely symbolic. <p> A final area related to TopGen is network-growing algorithms, such as Cascade Correlation (Fahlman & Lebiere, 1989), the Tiling algorithm (Mezard & Nadal, 1989), and the Upstart algorithm <ref> (Frean, 1990) </ref>. The most obvious difference between TopGen and these algorithms is that TopGen uses domain knowledge and symbolic rule-refinement techniques to help determine the network's topology.
Reference: <author> Fu, L. </author> <year> (1989). </year> <title> Integration of neural heuristics into knowledge-based inference. </title> <journal> Connection Science, </journal> <volume> 1(3) </volume> <pages> 325-340. </pages>
Reference: <author> Fu, L. </author> <year> (1991). </year> <title> Rule learning by searching on adapted nets. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 590-595), </pages> <address> Anaheim, CA. </address> <note> AAAI/MIT Press. 19 Opitz & Shavlik Adding Nodes to Knowledge-Based Neural Networks Ginsberg, </note> <author> A. </author> <year> (1990). </year> <title> Theory reduction, theory revision, </title> <booktitle> and retranslation. In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 777-782), </pages> <address> Boston, MA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference: <author> Hinton, G. E. </author> <year> (1986). </year> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> (pp. 1-12), </pages> <address> Amherst, MA. </address> <publisher> Erlbaum. </publisher>
Reference-contexts: That is, there is a trade-off between changing the domain theory and disregarding the misclassified training examples as noise. To help address this, TopGen uses a variant of weight decay <ref> (Hinton, 1986) </ref>. Weights that are part of the original domain theory decay toward their initial value, while other weights decay toward zero.
Reference: <author> Holland, J. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI. </address>
Reference-contexts: This is important in tasks where the domain theory is far from the true concept. Currently we only consider expansions to the Kbann network; however, we plan to use techniques such as genetic algorithms <ref> (Holland, 1975) </ref> 5 TopGen's runtime is dominated by the training process of each network; thus, TopGen's runtime is longer than Kbann's by approximately the number of networks it considers during its search. 16 Opitz & Shavlik Adding Nodes to Knowledge-Based Neural Networks or simulated annealing (Kirkpatrick et al., 1983) to help
Reference: <author> Kirkpatrick, S., Gelatt, C., & Vecchi, M. </author> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680. </pages>
Reference-contexts: techniques such as genetic algorithms (Holland, 1975) 5 TopGen's runtime is dominated by the training process of each network; thus, TopGen's runtime is longer than Kbann's by approximately the number of networks it considers during its search. 16 Opitz & Shavlik Adding Nodes to Knowledge-Based Neural Networks or simulated annealing <ref> (Kirkpatrick et al., 1983) </ref> to help broaden the types of networks considered during the search. Also, since we are searching through many candidate networks, it is important to be able to recognize the networks that are likely to generalize the best.
Reference: <author> Lacher, R., Hruska, S., & Kuncicky, D. </author> <year> (1992). </year> <title> Back-propagation learning in expert networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(1) </volume> <pages> 62-72. </pages>
Reference: <author> MacKay, D. J. </author> <year> (1992). </year> <title> A practical Bayesian framework for backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 448-472. </pages>
Reference-contexts: Also, since we are searching through many candidate networks, it is important to be able to recognize the networks that are likely to generalize the best. We currently use a validation set; however, a validation set can be a poor estimate of the true error <ref> (MacKay, 1992) </ref>. Also, as we increase the number of networks considered, TopGen may start selecting networks that overfit the validation set. <p> Future work, then, is to investigate selection methods that do not use a validation set, which would also allow us to use all the training instances to train the networks. Such techniques include minimum-description-length methods (Rissanen, 1983), Generalized Prediction Error (Moody, 1991), and Bayesian methods <ref> (MacKay, 1992) </ref>. Future work also includes using a rule-extraction algorithm (Fu, 1991; McMillan et al., 1992; Towell & Shavlik, 1993) to measure the interpretability of a refined TopGen network.
Reference: <author> Maclin, R. & Shavlik, J. </author> <year> (1993). </year> <title> Using knowledge-based neural networks to improve algorithms: Refining the Chou-Fasman algorithm for protein folding. Machine Learning, </title> <publisher> 11(2,3):195-215. </publisher>
Reference: <author> Mahoney, J. J. & Mooney, R. J. </author> <year> (1993). </year> <title> Combining connectionist and symbolic learning to refine certainty-factor rule-bases. Connection Science, </title> <publisher> 5(3,4):339-364. </publisher>
Reference-contexts: Most of these systems also work by first translating the domain knowledge into a network, then modifying its weights with neural learning; however, of these, only Rapture <ref> (Mahoney & Mooney, 1993) </ref> is able modify its architecture during training. It does this by using the Upstart algorithm (Frean, 1990) to add new nodes to the network. Additional related work includes purely symbolic theory-refinement systems.
Reference: <author> McMillan, C., Mozer, M. C., & Smolensky, P. </author> <year> (1992). </year> <title> Rule induction through integrated symbolic and subsymbolic pr ocessing. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 4), </booktitle> <pages> (pp. 969-976), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mezard, M. & Nadal, J.-P. </author> <year> (1989). </year> <title> Learning in feedforward layered networks: The tiling algorithm. </title> <journal> Journal of Physics A, </journal> <volume> 22 </volume> <pages> 2191-2204. </pages>
Reference-contexts: Towell (1991) showed that Kbann was superior to Either on a DNA task, and as reported in Section 5, TopGen outperforms Kbann. A final area related to TopGen is network-growing algorithms, such as Cascade Correlation (Fahlman & Lebiere, 1989), the Tiling algorithm <ref> (Mezard & Nadal, 1989) </ref>, and the Upstart algorithm (Frean, 1990). The most obvious difference between TopGen and these algorithms is that TopGen uses domain knowledge and symbolic rule-refinement techniques to help determine the network's topology.
Reference: <author> Moody, J. </author> <year> (1991). </year> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 4), </booktitle> <pages> (pp. 847-854), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Future work, then, is to investigate selection methods that do not use a validation set, which would also allow us to use all the training instances to train the networks. Such techniques include minimum-description-length methods (Rissanen, 1983), Generalized Prediction Error <ref> (Moody, 1991) </ref>, and Bayesian methods (MacKay, 1992). Future work also includes using a rule-extraction algorithm (Fu, 1991; McMillan et al., 1992; Towell & Shavlik, 1993) to measure the interpretability of a refined TopGen network.
Reference: <author> Noordewier, M. O., Towell, G. G., & Shavlik, J. W. </author> <year> (1990). </year> <title> Training knowledge-based neural networks to recognize genes in DNA sequences. </title> <editor> In Lippmann, R., Moody, J., & Touretzky, D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 3), </booktitle> <pages> (pp. 530-536), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The second domain, splice-junction determination, contains 3,190 examples distributed among three classes, and 23 rules <ref> (Noordewier et al., 1990) </ref>. The third domain, transcription termination sites, contains 142 positive examples, 5,178 negative examples, and 60 rules. Finally, the last domain, ribosome binding sites (RBS), contains 366 positive examples, 1,511 negative examples, and 17 rules. See Craven and Shavlik (1994) for a description of these tasks.
Reference: <author> Omlin, C. & Giles, C. </author> <year> (1992). </year> <title> Training second-order recurrent neural networks using hints. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> (pp. 361-366), </pages> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ourston, D. & Mooney, R. </author> <year> (1994). </year> <title> Theory refinement combining analytical and empirical methods. </title> <journal> Artificial Intelligence, </journal> <volume> 66(2) </volume> <pages> 273-309. </pages> <note> 20 Opitz & Shavlik Adding Nodes to Knowledge-Based Neural Networks Pazzani, </note> <author> M. & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> 9(1):57-94. 
Reference-contexts: It does this by using the Upstart algorithm (Frean, 1990) to add new nodes to the network. Additional related work includes purely symbolic theory-refinement systems. Systems such as Either <ref> (Ourston & Mooney, 1994) </ref> and Rtls (Ginsberg, 1990) are also propositional in nature. These systems differ from TopGen, in that their approaches are purely symbolic.
Reference: <author> Rissanen, J. </author> <year> (1983). </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11(2) </volume> <pages> 416-431. </pages>
Reference-contexts: Future work, then, is to investigate selection methods that do not use a validation set, which would also allow us to use all the training instances to train the networks. Such techniques include minimum-description-length methods <ref> (Rissanen, 1983) </ref>, Generalized Prediction Error (Moody, 1991), and Bayesian methods (MacKay, 1992). Future work also includes using a rule-extraction algorithm (Fu, 1991; McMillan et al., 1992; Towell & Shavlik, 1993) to measure the interpretability of a refined TopGen network.
Reference: <author> Roscheisen, M., Hofmann, R., & Tresp, V. </author> <year> (1991). </year> <title> Neural control for rolling mills: Incorporating domain theories to overcome data deficiency. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 4), </booktitle> <pages> (pp. 659-666), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rumelhart, D., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. & McClelland, J., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the microstructure of cognition. Volume 1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: TopGen uses Kbann's rule-to-network translation algorithm to create an initial guess for the network's topology. This network is trained using backpropagation <ref> (Rumelhart et al., 1986) </ref> and is placed on an OPEN list. In each cycle, TopGen takes the best network (as measured by validation-set-2) from the OPEN list, decides possible ways to add new nodes, trains these new networks, and places them on the OPEN list.
Reference: <author> Scott, G., Shavlik, J., & Ray, W. H. </author> <year> (1992). </year> <title> Refining PID controllers using neural networks. </title> <journal> Neural Computation, </journal> <volume> 5(4) </volume> <pages> 746-757. </pages>
Reference: <author> Towell, G. </author> <year> (1991). </year> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, WI. </institution>
Reference-contexts: It then applies backpropagation to refine these reformulated rules. Kbann has been shown to generalize to previously unseen examples better than many other inductive learning algorithms <ref> (Towell, 1991) </ref>. Towell and Shavlik (1994) attribute Kbann's superiority over other symbolic systems to both its underlying learning algorithm (i.e., backpropagation) and its effective use of domain-specific knowledge. <p> Other possible approaches include: adding them to only a portion of the inputs, adding them to nodes that have a high correlation with the error, or adding them to the next "layer" of nodes. 7 Related Work The most obvious related work is the Kbann system <ref> (Towell, 1991) </ref>, described in detail earlier in this paper. Fletcher and Obradovic (1993) also present an approach that adds nodes to a Kbann network.
Reference: <author> Towell, G. & Shavlik, J. </author> <year> (1992). </year> <title> Using symbolic learning to improve knowledge-based neural networks. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 177-182), </pages> <address> San Jose, CA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Fletcher and Obradovic's approach does not change the weights of the Kbann portion of the network, so modifications to the initial rule base are solely left to the constructed hidden nodes. The Daid algorithm <ref> (Towell & Shavlik, 1992) </ref>, another extension to Kbann, uses the domain knowledge to help train the Kbann network. Because Kbann is more effective at dropping antecedents than adding them, Daid tries to find potentially-useful inputs features not mentioned in the domain theory.
Reference: <author> Towell, G. & Shavlik, J. </author> <year> (1993). </year> <title> Extracting refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 71-101. </pages>
Reference-contexts: Also, large changes to the domain theory can greatly complicate rule extraction following training <ref> (Towell & Shavlik, 1993) </ref>. Hence, our goal is to expand, during the training phase, knowledge-based neural networks | networks whose topology is determined as a result of the direct mapping of the 1 We use generalize to mean accuracy on examples not seen during training. <p> Trained Kbann networks are interpretable because (a) the meaning of its nodes does not significantly shift during training and (b) almost all the nodes are either fully active or inactive <ref> (Towell & Shavlik, 1993) </ref>. Not only does TopGen add nodes in a symbolic fashion, it adds them in a fashion that does not violate these two assumptions. Other future work includes extensively testing other approaches for locating error.
Reference: <author> Towell, G. & Shavlik, J. </author> <year> (1994). </year> <title> Knowledge-based artificial neural networks. </title> <journal> Artificial Intelligence, </journal> <volume> 70. </volume>
Reference-contexts: Most of these systems, however, suffer in that they do not refine the topology of the network they create. We address this problem by presenting a new approach to connectionist theory refinement, particularly focusing on the task of automatically adding nodes to a "knowledge-based" neural network. Kbann <ref> (Towell & Shavlik, 1994) </ref> is an example of a connectionist theory-refinement system; it translates a set of propositional rules into a neural network, thereby determining the network's topology. It then applies backpropagation to refine these reformulated rules.
Reference: <author> Tresp, V., Hollatz, J., & Ahmad, S. </author> <year> (1992). </year> <title> Network structuring and training using rule-based knowledge. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 5), </booktitle> <pages> (pp. 871-878), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Weigend, A., Rumelhart, D., & Huberman, B. </author> <year> (1990). </year> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In Lippmann, R., Moody, J., & Touretzky, D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 3), </booktitle> <pages> (pp. 875-882), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Weights that are part of the original domain theory decay toward their initial value, while other weights decay toward zero. Our weight-decay term, then, decays weights as a function of their distance from their initial value and is a slight variant of the term proposed by Rumelhart in 1987 <ref> (Weigend et al., 1990) </ref>.
Reference: <author> Winston, P. </author> <year> (1975). </year> <title> Learning structural descriptions from examples. </title> <editor> In Winston, P., editor, </editor> <booktitle> The Psychology of Computer Vision, </booktitle> <pages> (pp. 157-210), </pages> <address> New York. McGraw-Hill. </address> <note> 21 Opitz & Shavlik Adding Nodes to Knowledge-Based Neural Networks </note>
Reference-contexts: TopGen increments a node's false-negatives counter in a similar fashion. By checking for single points of failure, TopGen looks for rules that are near misses <ref> (Winston, 1975) </ref>. After the counter values have been determined, TopGen sorts these counters in descending order, while breaking ties by preferring nodes farthest from the output node. TopGen then creates N new networks, where each network contains a single correction (as determined by the first N sorted counters).
References-found: 31

