URL: http://www.ri.cmu.edu/afs/cs/user/kseymore/html/papers/ICSLP_scalelms.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs/user/kseymore/html/papers.html
Root-URL: 
Title: SCALABLE BACKOFF LANGUAGE MODELS  
Author: Kristie Seymore Ronald Rosenfeld 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: When a trigram backoff language model is created from a large body of text, trigrams and bigrams that occur few times in the training text are often excluded from the model in order to decrease the model size. Generally, the elimination of n-grams with very low counts is believed to not significantly affect model performance. This project investigates the degradation of a trigram back-off model's perplexity and word error rates as bigram and trigram cutoffs are increased. The advantage of reduction in model size is compared to the increase in word error rate and perplexity scores. More importantly, this project also investigates alternative ways of excluding bigrams and trigrams from a backoff language model, using criteria other than the number of times an n-gram occurs in the training text. Specifically, a difference method has been investigated where the difference in the logs of the original and backed off trigram and bigram probabilities is used as a basis for n-gram exclusion from the model. We show that excluding trigrams and bigrams based on a weighted version of this difference method results in better perplexity and word error rate performance than excluding trigrams and bigrams based on counts alone. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> F. Jelinek, </author> <title> Self Organized Language Modeling for Speech Recognition, in Readings in Speech Recognition, </title> <editor> Alex Waibel and Kai-Fu Lee (Eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference: 2. <author> S.M. Katz, </author> <title> Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer, </title> <journal> in IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> volume ASSP-35, </volume> <pages> pages 400-401, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: We explore two methods of training text pruning that allow for compact and efficient creation of trigram backoff language models. The effects of the original amount of training data on a scaled-down model is also investigated. 2. THE BACKOFF LANGUAGE MODEL The backoff language model was developed by Katz <ref> [2] </ref> to address the problems associated with sparse training data. Small counts result in unreliable estimates. The backoff model handles this type of sampling error by discounting the probability of low count events and distributing the freed probability mass among unseen events.
Reference: 3. <author> R. Rosenfeld, </author> <title> Optimizing Lexical and N-gram Coverage Via Judicious Use of Linguistic Data, </title> <booktitle> Eurospeech 95, </booktitle> <pages> pp. </pages> <note> 17631766. See file://localhost/afs/cs.cmu.edu/user/roni/WWW/vocov-eurospeech95-proc.ps. </note>
Reference: 4. <author> R. Rosenfeld, </author> <title> The CMU Statistical Language Modeling Toolkit, and its use in the 1994 ARPA CSR Evaluation, </title> <booktitle> in Proc. ARPA Spoken Language Technology Workshop, </booktitle> <address> Austin, TX, </address> <month> January </month> <year> 1995. </year> <note> See http://www.speech.cs.cmu.edu/speech/SLM info.html. </note>
Reference-contexts: # Bigrams # Trigrams Memory (MB) (00) 4,627,551 16,838,937 104 (11) 1,787,935 3,581,187 29 (1010) 347,647 367,928 4 Table 1: Model Cutoffs and Resulting Model Size In order to investigate the effects of raising bigram and trigram cutoffs, several models were created using the Carnegie Mellon Statistical Language Modeling Toolkit <ref> [4] </ref>. The word error rate (WER) and perplexity (PP) were calculated for each model.
Reference: 5. <author> K. Seymore and R. Rosenfeld, </author> <title> Scalable Trigram Backoff Language Models, </title> <institution> Carnegie Mellon University Tech Report CMU-CS-96-139, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: Beyond 25 million words, the amount of training data does not have a noticeable effect. Further analysis, detailed results and ideas for future investigation are presented in <ref> [5] </ref>. 6. ACKNOWLEDGMENTS This material is based upon work supported under a National Science Foundation Graduate Research Fellowship and by the Department of the Navy, Naval Research Laboratory under Grant No. N00014-93-1-2005.
References-found: 5

