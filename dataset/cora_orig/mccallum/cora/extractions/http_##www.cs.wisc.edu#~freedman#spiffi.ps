URL: http://www.cs.wisc.edu/~freedman/spiffi.ps
Refering-URL: http://www.cs.wisc.edu/~bolo/bolo.html
Root-URL: 
Email: ffreedman,bolo,dewittg@cs.wisc.edu  
Title: SPIFFI A Scalable Parallel File System for the Intel Paragon  
Author: Craig S. Freedman Josef Burger David J. DeWitt 
Address: Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin  
Abstract: This paper presents the design and performance of SPIFFI, a scalable high-performance parallel file system intended for use by extremely I/O intensive applications including Grand Challenge scientific applications and multimedia systems. This paper contains experimental results from a SPIFFI prototype on a 64 node/64 disk Intel Paragon. The results show that SPIFFI provides high performance and linear scaleup on real hardware. The paper also explains how shared file pointers (i.e., file pointers that are shared by multiple processes) can simplify the design of a parallel application. By sequentializing I/O accesses and by providing dynamic I/O load balancing, a shared file pointer may even improve an application's performance. This paper also presents the predictions of a SPIFFI simulator that we validated using the prototype. The simulator results show that SPIFFI continues to provide high performance even when it is scaled to configurations with as many as 128 disks or 256 compute nodes.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. G. Baker, J. H. Hartman, M. D. Kupfer, K. W. Shirriff, and J. K. Ousterhout, </author> <title> Measurements of a Distributed File System, </title> <booktitle> Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 198-212, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Though Swift [7] and Zebra [24] do use striping to achieve higher data rates, these and the other distributed file systems were primarily designed to handle a typical UNIX workload of many, relatively small files <ref> [1, 42] </ref>. Such projects have rarely focused on the problems of distributing or striping a few large files across multiple disks within a single, tightly-coupled, massively parallel system. The intent of SPIFFI is to study the performance of a parallel file system within such a massively parallel system.
Reference: [2] <author> C. H. Baldwin and W. C. Nestlerode, </author> <title> A Large Scale File Processing Application on a Hypercube, </title> <booktitle> Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pp. 1400-1404, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: French [19] underscores the importance of load balancing file accesses for large parallel programs but does not propose any solutions to the problem. Kotz and Ellis [30] propose algorithms for deducing global file access patterns and prefetching appropriately. Baldwin and Nestlerode <ref> [2] </ref> investigate read performance using a Unix file pointer. However, they only explore a single workload and place the burden of achieving high performance on the application design rather than on the file system.
Reference: [3] <author> M. L. Best, A. Greenberg, C. Stanfill, and L. W. Tucker, </author> <title> CMMD I/O: A Parallel Unix I/O, </title> <booktitle> Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pp. 489-495, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: While the RAID-II system [18] does provide higher performance through improved hardware and software, it still does not address the parallel application interface problem. Existing parallel I/O systems include the CMMD I/O system <ref> [3] </ref> and the nCUBE parallel I/O system [14]. Existing parallel file systems include RAMA [39], the Bridge parallel file system [16], the IBM Vesta Parallel File System [10, 11, 12], and the Intel Parallel File System (PFS) [28, 29]. The CMMD I/O system [3] provides a parallel I/O interface to parallel <p> I/O systems include the CMMD I/O system <ref> [3] </ref> and the nCUBE parallel I/O system [14]. Existing parallel file systems include RAMA [39], the Bridge parallel file system [16], the IBM Vesta Parallel File System [10, 11, 12], and the Intel Parallel File System (PFS) [28, 29]. The CMMD I/O system [3] provides a parallel I/O interface to parallel applications on the Thinking Machines CM-5. However, the CMMD library does not address the data storage issue. In fact, the CM-5 does not support a parallel file system.
Reference: [4] <institution> Butterfly Parallel Processor Overview, </institution> <type> Technical Report 6149, Version 2, </type> <institution> BBN Laboratories, </institution> <month> June </month> <year> 1986. </year>
Reference-contexts: However, the programmer must use the low-level interface to achieve the best possible performance. A prototype of Bridge was built on a BBN Butterfly <ref> [4] </ref> using simulated disks. The IBM Vesta Parallel File System [10, 11, 12], which runs on the IBM SP machines, stripes files across multiple disks and provides a unique parallel interface in which files are viewed as a collection of logical subfiles and records.
Reference: [5] <author> R. Bordawekar, J. M. del Rosario, and A. Choudhary, </author> <title> Design and Evaluation of Primitives for Parallel I/O, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pp. 452-461, </pages> <month> November </month> <year> 1993. </year>
Reference: [6] <author> P. Brezany, M. Gerndt, P. Mehrotra, and H. Zima, </author> <title> Concurrent File Operations in a High Performance Fortran, </title> <booktitle> Proceedings of Supercomputing '92, </booktitle> <pages> pp. 230-237, </pages> <month> November </month> <year> 1992. </year>
Reference: [7] <author> L.-F. Cabrera and D. D. E. Long, Swift: </author> <title> Using Distributed Disk Striping to Provide High I/O Data Rates, </title> <journal> Computing Systems, </journal> <volume> vol. 4, no. 4, </volume> <pages> pp. 405-436, </pages> <month> Fall </month> <year> 1991. </year>
Reference-contexts: We compare SPIFFI's file pointers to PFS's I/O modes in Section 4.2 and provide some PFS performance data in Section 5.1. 3 Finally, although there has been a great deal of research into distributed operating systems and distributed file systems <ref> [7, 21, 24, 26, 53] </ref>, generally these efforts have focused on issues such as transparent access to or replication of resources across physically distinct and often heterogeneous machines. <p> Though Swift <ref> [7] </ref> and Zebra [24] do use striping to achieve higher data rates, these and the other distributed file systems were primarily designed to handle a typical UNIX workload of many, relatively small files [1, 42].
Reference: [8] <author> P. M. Chen and D. A. Patterson, </author> <title> Maximizing Performance in a Striped Disk Array, </title> <booktitle> Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <pages> pp. 322-331, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Section 8 presents the predictions made by the simulator. Finally, Section 9 contains some conclusions and a discussion of future research directions. 2 Related Work There has been a great deal of research into parallel file systems and parallel I/O in recent years. Chen and Patterson <ref> [8] </ref> use a simulator to investigate how varying striping parameters and degrees of concurrency affects the performance of a random I/O workload on a striped disk array. French [19] underscores the importance of load balancing file accesses for large parallel programs but does not propose any solutions to the problem.
Reference: [9] <author> H-T. Chou, D. J. DeWitt, R. H. Katz, and A. C. Klug, </author> <title> Design and Implementation of the Wisconsin Storage System, </title> <journal> SoftwarePractice and Experience, </journal> <volume> vol. 15, no. 10, </volume> <pages> pp. 943-962, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: A file fragment is the portion of a SPIFFI file stored at one disk. In this study the file fragments were laid out contiguously on disk. Many existing file systems, including the Berkeley fast file system [47] and extent based file systems <ref> [9] </ref>, also attempt to allocate large files in contiguous chunks. The set of disks across which a SPIFFI file is striped and the file's striping granularity are collectively referred to as the file's metadata. This information is sufficient for determining which disk nodes store which blocks of a file.
Reference: [10] <author> P. F. Corbett, S. J. Baylor, and D. G. Feitelson, </author> <title> Overview of the Vesta Parallel File System, </title> <booktitle> Proceedings of IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pp. 7-14, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: RAID devices [45] provide a possible solution to the bandwidth problem but do not address the application interface problem. Furthermore, as RAID sizes increase, a bottleneck may form at the RAID controller or I/O node. Existing parallel file systems include the IBM Vesta Parallel File System <ref> [10, 11, 12] </ref>, the Intel Parallel File System [28, 29], and the Bridge parallel file system [16]. fl This research was partially supported by ARPA under contract number DAAB07-92-C-Q508 and monitored by the US Army Research Laboratory. This research was partially supported by NSF grant number CDA-9024618. <p> Existing parallel I/O systems include the CMMD I/O system [3] and the nCUBE parallel I/O system [14]. Existing parallel file systems include RAMA [39], the Bridge parallel file system [16], the IBM Vesta Parallel File System <ref> [10, 11, 12] </ref>, and the Intel Parallel File System (PFS) [28, 29]. The CMMD I/O system [3] provides a parallel I/O interface to parallel applications on the Thinking Machines CM-5. However, the CMMD library does not address the data storage issue. <p> However, the programmer must use the low-level interface to achieve the best possible performance. A prototype of Bridge was built on a BBN Butterfly [4] using simulated disks. The IBM Vesta Parallel File System <ref> [10, 11, 12] </ref>, which runs on the IBM SP machines, stripes files across multiple disks and provides a unique parallel interface in which files are viewed as a collection of logical subfiles and records. This interface appears to provide powerful primitives for mapping High Performance Fortran arrays to processors. <p> Since the control thread is only involved during file open and closes, it is unlikely to become a performance bottleneck. If the control thread were to become a bottleneck, additional control threads could be started at different nodes and the workload distributed among them <ref> [10] </ref>. The GFP thread provides an atomic read and update operation. Whenever a node wishes to perform an I/O using a GFP or SGFP, it must send a request to this thread with the length of the impending I/O.
Reference: [11] <author> P. F. Corbett, D. G. Feitelson, J.-P. Prost, and S. J. Baylor, </author> <title> Parallel Access to Files in the Vesta File System, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pp. 472-481, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: RAID devices [45] provide a possible solution to the bandwidth problem but do not address the application interface problem. Furthermore, as RAID sizes increase, a bottleneck may form at the RAID controller or I/O node. Existing parallel file systems include the IBM Vesta Parallel File System <ref> [10, 11, 12] </ref>, the Intel Parallel File System [28, 29], and the Bridge parallel file system [16]. fl This research was partially supported by ARPA under contract number DAAB07-92-C-Q508 and monitored by the US Army Research Laboratory. This research was partially supported by NSF grant number CDA-9024618. <p> Existing parallel I/O systems include the CMMD I/O system [3] and the nCUBE parallel I/O system [14]. Existing parallel file systems include RAMA [39], the Bridge parallel file system [16], the IBM Vesta Parallel File System <ref> [10, 11, 12] </ref>, and the Intel Parallel File System (PFS) [28, 29]. The CMMD I/O system [3] provides a parallel I/O interface to parallel applications on the Thinking Machines CM-5. However, the CMMD library does not address the data storage issue. <p> However, the programmer must use the low-level interface to achieve the best possible performance. A prototype of Bridge was built on a BBN Butterfly [4] using simulated disks. The IBM Vesta Parallel File System <ref> [10, 11, 12] </ref>, which runs on the IBM SP machines, stripes files across multiple disks and provides a unique parallel interface in which files are viewed as a collection of logical subfiles and records. This interface appears to provide powerful primitives for mapping High Performance Fortran arrays to processors.
Reference: [12] <author> P. F. Corbett, and D. G. Feitelson, </author> <title> Design and Implementation of the Vesta Parallel File System, </title> <booktitle> Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pp. 63-70, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: RAID devices [45] provide a possible solution to the bandwidth problem but do not address the application interface problem. Furthermore, as RAID sizes increase, a bottleneck may form at the RAID controller or I/O node. Existing parallel file systems include the IBM Vesta Parallel File System <ref> [10, 11, 12] </ref>, the Intel Parallel File System [28, 29], and the Bridge parallel file system [16]. fl This research was partially supported by ARPA under contract number DAAB07-92-C-Q508 and monitored by the US Army Research Laboratory. This research was partially supported by NSF grant number CDA-9024618. <p> Existing parallel I/O systems include the CMMD I/O system [3] and the nCUBE parallel I/O system [14]. Existing parallel file systems include RAMA [39], the Bridge parallel file system [16], the IBM Vesta Parallel File System <ref> [10, 11, 12] </ref>, and the Intel Parallel File System (PFS) [28, 29]. The CMMD I/O system [3] provides a parallel I/O interface to parallel applications on the Thinking Machines CM-5. However, the CMMD library does not address the data storage issue. <p> However, the programmer must use the low-level interface to achieve the best possible performance. A prototype of Bridge was built on a BBN Butterfly [4] using simulated disks. The IBM Vesta Parallel File System <ref> [10, 11, 12] </ref>, which runs on the IBM SP machines, stripes files across multiple disks and provides a unique parallel interface in which files are viewed as a collection of logical subfiles and records. This interface appears to provide powerful primitives for mapping High Performance Fortran arrays to processors.
Reference: [13] <author> R. A. Coyne and H. Hulen, </author> <title> The High Performance Storage System, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pp. 83-92, </pages> <month> November </month> <year> 1993. </year>
Reference: [14] <author> E. DeBenedictis and J. M. del Rosario, </author> <title> nCUBE Parallel I/O Software, </title> <booktitle> Proceedings of the Eleventh Annual International Phoenix Conference on Computers and Communications, </booktitle> <pages> pp. 117-124, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: While the RAID-II system [18] does provide higher performance through improved hardware and software, it still does not address the parallel application interface problem. Existing parallel I/O systems include the CMMD I/O system [3] and the nCUBE parallel I/O system <ref> [14] </ref>. Existing parallel file systems include RAMA [39], the Bridge parallel file system [16], the IBM Vesta Parallel File System [10, 11, 12], and the Intel Parallel File System (PFS) [28, 29]. The CMMD I/O system [3] provides a parallel I/O interface to parallel applications on the Thinking Machines CM-5. <p> However, the CMMD library does not address the data storage issue. In fact, the CM-5 does not support a parallel file system. Instead, data is stored on large high-performance RAID systems and is accessed via the CM-5's interconnection network. nCUBE's parallel I/O system <ref> [14] </ref> uses mapping functions to move data between the processors involved in a parallel computation and a set of disks or other I/O devices. A parallel application provides the mapping between itself and a simple UNIX byte stream view of a file.
Reference: [15] <author> D. J. DeWitt, S. Ghandeharizadeh, D. A. Schneider, A. Bricker, H.-I. Hsiao, and R. Rasmussen, </author> <title> The Gamma Database Machine Project, </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 44-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: The simulator, which is written in the CSIM/C++ process-oriented simulation language [49], is based on a simulator of the Gamma database machine <ref> [15, 37] </ref>. Since we were able to reuse many of the components from the earlier simulator including the disks, buffer pool, files, and network, we were able to get the simulator running in little time.
Reference: [16] <author> P. C. Dibble, M. L. Scott, and C. S. Ellis, </author> <title> Bridge: A High-Performance File System for Parallel Processors, </title> <booktitle> Proceedings of the Eighth International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 154-161, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Furthermore, as RAID sizes increase, a bottleneck may form at the RAID controller or I/O node. Existing parallel file systems include the IBM Vesta Parallel File System [10, 11, 12], the Intel Parallel File System [28, 29], and the Bridge parallel file system <ref> [16] </ref>. fl This research was partially supported by ARPA under contract number DAAB07-92-C-Q508 and monitored by the US Army Research Laboratory. This research was partially supported by NSF grant number CDA-9024618. This material is based on work supported under a National Science Foundation Graduate Fellowship. <p> Existing parallel I/O systems include the CMMD I/O system [3] and the nCUBE parallel I/O system [14]. Existing parallel file systems include RAMA [39], the Bridge parallel file system <ref> [16] </ref>, the IBM Vesta Parallel File System [10, 11, 12], and the Intel Parallel File System (PFS) [28, 29]. The CMMD I/O system [3] provides a parallel I/O interface to parallel applications on the Thinking Machines CM-5. However, the CMMD library does not address the data storage issue. <p> RAMA uses hashing to store and retrieve blocks of a file. Simulation results suggest that poorly written applications may benefit more from hash based disk layouts than from striped disk layouts. The Bridge parallel file system <ref> [16] </ref> provides three interfaces from a high-level Unix-like interface to a low-level interface that provides direct access to the individual disks. However, the programmer must use the low-level interface to achieve the best possible performance. A prototype of Bridge was built on a BBN Butterfly [4] using simulated disks.
Reference: [17] <author> A. L. Drapeau and R. H. Katz, </author> <title> Striping in Large Tape Libraries, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pp. 378-387, </pages> <month> November </month> <year> 1993. </year> <month> 27 </month>
Reference: [18] <author> A. L. Drapeau, K. W. Shirriff, J. H. Hartman, E. L. Miller, S. Seshan, R. H. Katz, K. Lutz, D. A. Patterson, E. K. Lee, P. M. Chen, and G. A. Gibson, </author> <title> RAID-II: A High-Bandwidth Network File Server, </title> <booktitle> Proceedings of the Twenty-First Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 234-244, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In addition, a bottleneck may form since typical RAID devices are attached to a single controller or I/O node. While the RAID-II system <ref> [18] </ref> does provide higher performance through improved hardware and software, it still does not address the parallel application interface problem. Existing parallel I/O systems include the CMMD I/O system [3] and the nCUBE parallel I/O system [14].
Reference: [19] <author> J. C. </author> <title> French, Characterizing the Balance of Parallel I/O Systems, </title> <booktitle> Proceedings of the Sixth Distributed Memory Computing Conference, </booktitle> <pages> pp. 724-727, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Chen and Patterson [8] use a simulator to investigate how varying striping parameters and degrees of concurrency affects the performance of a random I/O workload on a striped disk array. French <ref> [19] </ref> underscores the importance of load balancing file accesses for large parallel programs but does not propose any solutions to the problem. Kotz and Ellis [30] propose algorithms for deducing global file access patterns and prefetching appropriately. Baldwin and Nestlerode [2] investigate read performance using a Unix file pointer.
Reference: [20] <author> N. Galbreath, W. Gropp, and D. Levine, </author> <title> Applications-Driven Parallel I/O, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pp. 462-471, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The intent of SPIFFI is to study the performance of a parallel file system within such a massively parallel system. SPIFFI files will generally be large and immutable including, for instance, scientific data sets or multimedia files <ref> [20, 33, 38, 43, 44] </ref>. 3 The Intel Paragon XP/S The Intel Paragon is a parallel supercomputer capable of scaling to thousands of nodes. The nodes are interconnected by a high-performance mesh network that permits processes at distinct nodes to exchange messages and data efficiently.
Reference: [21] <author> D. K. Gifford, R. M. Needham, and M. D. Schroeder, </author> <title> The Cedar File System, </title> <journal> Communications of the ACM, </journal> <volume> vol. 31, no. 3, </volume> <pages> pp. 288-298, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: We compare SPIFFI's file pointers to PFS's I/O modes in Section 4.2 and provide some PFS performance data in Section 5.1. 3 Finally, although there has been a great deal of research into distributed operating systems and distributed file systems <ref> [7, 21, 24, 26, 53] </ref>, generally these efforts have focused on issues such as transparent access to or replication of resources across physically distinct and often heterogeneous machines.
Reference: [22] <author> J. Gray, B. Horst, and M. Walker, </author> <title> Parity Striping of Disc Arrays: Low-Cost Reliable Storage with Acceptable Throughput, </title> <booktitle> Proceedings of the Sixteenth International Conference on Very Large Data Bases, </booktitle> <pages> pp. 148-159, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: The total number of disks was kept constant at 64. Files were striped across the disks in the same manner as they were with the single disk per node configuration used throughout this paper <ref> [22] </ref>. There were 16 application processes and the application I/O size and SPIFFI striping granularity were set to 32 Kbytes. The read-only application was simulated using all four types of file pointers. performance is nearly constant.
Reference: [23] <author> J. Gray, </author> <title> Transaction Processing: Concepts and Techniques, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993, </year> <pages> pp. 423-424. </pages>
Reference-contexts: Using a LFP in this manner ensures that there is a bounded maximum number of readers and, hence, a bounded maximum workload associated with each disk. If all the application processes can read from any of the disk nodes, there is a tendency for convoys <ref> [23] </ref> of readers to form at some of the disk nodes. These convoys increase the load on some disks while leaving others underutilized (see Section 5.3). In contrast, when using a GFP, SGFP, or DFP, the blocks of the file are automatically distributed among the reader processes. <p> As a result, it is possible for multiple processes to attempt to write to the same disk node at the same time. Since there is a normal variation in disk access latencies, one disk may temporarily perform more slowly than the others. During this period, a convoy <ref> [23] </ref> of writers can form at the slow disk. Convoys are not a problem for the read-only workloads because the prefetching mechanism keeps all the disks active even if queues start to form at some of the disks.
Reference: [24] <author> J. H. Hartman and J. K. Ousterhout, </author> <title> The Zebra Striped Network File System, </title> <booktitle> Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 29-43, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: We compare SPIFFI's file pointers to PFS's I/O modes in Section 4.2 and provide some PFS performance data in Section 5.1. 3 Finally, although there has been a great deal of research into distributed operating systems and distributed file systems <ref> [7, 21, 24, 26, 53] </ref>, generally these efforts have focused on issues such as transparent access to or replication of resources across physically distinct and often heterogeneous machines. <p> Though Swift [7] and Zebra <ref> [24] </ref> do use striping to achieve higher data rates, these and the other distributed file systems were primarily designed to handle a typical UNIX workload of many, relatively small files [1, 42].
Reference: [25] <author> H. Hellwagner, </author> <title> Design Considerations for Scalable Parallel File Systems, </title> <journal> The Computer Journal, </journal> <volume> vol. 36, no. 8, </volume> <pages> pp. 741-755, </pages> <year> 1993. </year>
Reference: [26] <author> J. H. Howard, M. L. Kazar, S. G. Menees, D. A. Nichols, M. Satyanarayanan, R. N. Sidebotham, and M. J. West, </author> <title> Scale and Performance in a Distributed File System, </title> <journal> ACM Transactions of Computer Systems, </journal> <volume> vol. 6, no. 1, </volume> <pages> pp. 51-81, </pages> <month> February </month> <year> 1988. </year> <note> [27] iPSC/2 User's Guide, </note> <institution> Intel Corporation, Beaverton, </institution> <address> OR, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: We compare SPIFFI's file pointers to PFS's I/O modes in Section 4.2 and provide some PFS performance data in Section 5.1. 3 Finally, although there has been a great deal of research into distributed operating systems and distributed file systems <ref> [7, 21, 24, 26, 53] </ref>, generally these efforts have focused on issues such as transparent access to or replication of resources across physically distinct and often heterogeneous machines.
Reference: [28] <institution> Paragon OSF/1 User's Guide, Intel Corporation, Beaverton, </institution> <address> OR, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Furthermore, as RAID sizes increase, a bottleneck may form at the RAID controller or I/O node. Existing parallel file systems include the IBM Vesta Parallel File System [10, 11, 12], the Intel Parallel File System <ref> [28, 29] </ref>, and the Bridge parallel file system [16]. fl This research was partially supported by ARPA under contract number DAAB07-92-C-Q508 and monitored by the US Army Research Laboratory. This research was partially supported by NSF grant number CDA-9024618. <p> Existing parallel I/O systems include the CMMD I/O system [3] and the nCUBE parallel I/O system [14]. Existing parallel file systems include RAMA [39], the Bridge parallel file system [16], the IBM Vesta Parallel File System [10, 11, 12], and the Intel Parallel File System (PFS) <ref> [28, 29] </ref>. The CMMD I/O system [3] provides a parallel I/O interface to parallel applications on the Thinking Machines CM-5. However, the CMMD library does not address the data storage issue. In fact, the CM-5 does not support a parallel file system. <p> This interface appears to provide powerful primitives for mapping High Performance Fortran arrays to processors. However, it is unclear whether Fortran compilers will be able to exploit this interface and whether it will be useful to non-Fortran programmers and for non-array data. The Intel Parallel File System (PFS) <ref> [28, 29] </ref>, which also runs on the Intel Paragon, evolved from the Intel Concurrent File System (CFS) [27]. PFS stripes files across multiple disks or RAID devices, but programmers view PFS files as ordinary Unix byte-stream files. PFS provides a variety of I/O modes including private and shared file pointers. <p> SPIFFI does not use the service nodes. When the experiments in this paper were run 1 , the Paragon was running Release 1.1 of the Paragon OSF/1 operating system <ref> [28] </ref>. This operating system is a parallel version of OSF/1 AD which is based on Carnegie Mellon University's Mach 3.0 [36]. In addition, for compatibility with Intel's hypercubes, Paragon OSF/1 supports Intel's NX message passing system [28]. <p> 1 , the Paragon was running Release 1.1 of the Paragon OSF/1 operating system <ref> [28] </ref>. This operating system is a parallel version of OSF/1 AD which is based on Carnegie Mellon University's Mach 3.0 [36]. In addition, for compatibility with Intel's hypercubes, Paragon OSF/1 supports Intel's NX message passing system [28]. Release 1.1 of Paragon OSF/1 has some significant problems that directly affected SPIFFI design decisions and performance. First, the file system performance lags substantially behind the raw disk performance.
Reference: [29] <institution> Paragon System Software Release 1.1 Release Notes for the Paragon XP/S System, Intel Corporation, Beaverton, </institution> <address> OR, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: Furthermore, as RAID sizes increase, a bottleneck may form at the RAID controller or I/O node. Existing parallel file systems include the IBM Vesta Parallel File System [10, 11, 12], the Intel Parallel File System <ref> [28, 29] </ref>, and the Bridge parallel file system [16]. fl This research was partially supported by ARPA under contract number DAAB07-92-C-Q508 and monitored by the US Army Research Laboratory. This research was partially supported by NSF grant number CDA-9024618. <p> Existing parallel I/O systems include the CMMD I/O system [3] and the nCUBE parallel I/O system [14]. Existing parallel file systems include RAMA [39], the Bridge parallel file system [16], the IBM Vesta Parallel File System [10, 11, 12], and the Intel Parallel File System (PFS) <ref> [28, 29] </ref>. The CMMD I/O system [3] provides a parallel I/O interface to parallel applications on the Thinking Machines CM-5. However, the CMMD library does not address the data storage issue. In fact, the CM-5 does not support a parallel file system. <p> This interface appears to provide powerful primitives for mapping High Performance Fortran arrays to processors. However, it is unclear whether Fortran compilers will be able to exploit this interface and whether it will be useful to non-Fortran programmers and for non-array data. The Intel Parallel File System (PFS) <ref> [28, 29] </ref>, which also runs on the Intel Paragon, evolved from the Intel Concurrent File System (CFS) [27]. PFS stripes files across multiple disks or RAID devices, but programmers view PFS files as ordinary Unix byte-stream files. PFS provides a variety of I/O modes including private and shared file pointers.
Reference: [30] <author> D. Kotz and C. S. Ellis, </author> <title> Practical Prefetching Techniques for Parallel File Systems, </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 33-51, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: French [19] underscores the importance of load balancing file accesses for large parallel programs but does not propose any solutions to the problem. Kotz and Ellis <ref> [30] </ref> propose algorithms for deducing global file access patterns and prefetching appropriately. Baldwin and Nestlerode [2] investigate read performance using a Unix file pointer. However, they only explore a single workload and place the burden of achieving high performance on the application design rather than on the file system.
Reference: [31] <author> D. Kotz, </author> <title> Multiprocessor File System Interfaces, </title> <booktitle> Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pp. 194-201, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Baldwin and Nestlerode [2] investigate read performance using a Unix file pointer. However, they only explore a single workload and place the burden of achieving high performance on the application design rather than on the file system. Kotz <ref> [31] </ref> examines the weaknesses of current parallel file system interfaces and proposes some features including a global file pointer that should be present in any parallel file system. In [32] Kotz compares several parallel file systems by surveying the results from independent performance studies. <p> a SPIFFI file is stored in a central location and updated whenever the file is closed. 6 4.2 File Pointers In a parallel environment a private or local Unix file pointer is no longer adequate; concurrently executing processes on physically separate nodes may wish to share a single file pointer <ref> [31] </ref>. A shared file pointer can simplify the coding and increase the performance of a parallel application. Consider, for example, a parallel matrix addition operation involving large matrices that are stored on disk. Each element of the two matrices must be processed exactly once.
Reference: [32] <author> D. Kotz, </author> <title> Throughput of Existing Multiprocessor File Systems (An Informal Study), </title> <type> Technical Report PCS-TR93-190, </type> <institution> Department of Math and Computer Science, Dartmouth College. </institution>
Reference-contexts: Kotz [31] examines the weaknesses of current parallel file system interfaces and proposes some features including a global file pointer that should be present in any parallel file system. In <ref> [32] </ref> Kotz compares several parallel file systems by surveying the results from independent performance studies. In [34] he introduces a technique called disk-directed I/O that allows disk servers to optimize and improve I/O performance.
Reference: [33] <author> D. Kotz and N. Nieuwejaar, </author> <title> Dynamic File-Access Characteristics of a Production Parallel Scientific Workload, </title> <booktitle> Proceedings of Supercomputing '94, </booktitle> <pages> pp. 640-649, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The intent of SPIFFI is to study the performance of a parallel file system within such a massively parallel system. SPIFFI files will generally be large and immutable including, for instance, scientific data sets or multimedia files <ref> [20, 33, 38, 43, 44] </ref>. 3 The Intel Paragon XP/S The Intel Paragon is a parallel supercomputer capable of scaling to thousands of nodes. The nodes are interconnected by a high-performance mesh network that permits processes at distinct nodes to exchange messages and data efficiently. <p> In each experiment, several application processes collectively read a large file (8 to 24 Gbytes) exactly once. That is, each block of the file is read exactly once by some 8 Although many applications use I/O sizes smaller than 2 Kbytes <ref> [33] </ref>, as the results in Section 5.2.3 show, using an I/O size smaller than 8 Kbytes will lead to extremely poor performance and, therefore, smaller I/O sizes were not explored. 9 As the RAIDs are used to store system files and user's home directories, we were unable to tune the RAID <p> While many applications use I/O sizes smaller than 2 Kbytes <ref> [33] </ref>, as the results below show, using an I/O size smaller than 8 Kbytes will lead to extremely poor performance and, therefore, smaller I/O sizes were not explored. The number of disks and application processes was held constant at 16.
Reference: [34] <author> D. Kotz, </author> <title> Disk-directed I/O for MIMD Multiprocessors, </title> <booktitle> Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pp. 61-74, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Kotz [31] examines the weaknesses of current parallel file system interfaces and proposes some features including a global file pointer that should be present in any parallel file system. In [32] Kotz compares several parallel file systems by surveying the results from independent performance studies. In <ref> [34] </ref> he introduces a technique called disk-directed I/O that allows disk servers to optimize and improve I/O performance.
Reference: [35] <author> J. Krystynak and B. Nitzberg, </author> <title> Performance Characteristics of the iPSC/860 and CM-2 I/O Systems, </title> <booktitle> Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pp. 837-841, </pages> <month> April </month> <year> 1993. </year>
Reference: [36] <author> K. Loepere, </author> <title> Mach 3 Kernel Principles, Open Software Foundation document, </title> <year> 1992. </year>
Reference-contexts: SPIFFI does not use the service nodes. When the experiments in this paper were run 1 , the Paragon was running Release 1.1 of the Paragon OSF/1 operating system [28]. This operating system is a parallel version of OSF/1 AD which is based on Carnegie Mellon University's Mach 3.0 <ref> [36] </ref>. In addition, for compatibility with Intel's hypercubes, Paragon OSF/1 supports Intel's NX message passing system [28]. Release 1.1 of Paragon OSF/1 has some significant problems that directly affected SPIFFI design decisions and performance. First, the file system performance lags substantially behind the raw disk performance.
Reference: [37] <author> M. Mehta and D. J. DeWitt, </author> <title> Dynamic Memory Allocation for Multiple-Query Workloads, </title> <booktitle> Proceedings of the Nineteenth International Conference on Very Large Data Bases, </booktitle> <pages> pp. 354-367, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: The simulator, which is written in the CSIM/C++ process-oriented simulation language [49], is based on a simulator of the Gamma database machine <ref> [15, 37] </ref>. Since we were able to reuse many of the components from the earlier simulator including the disks, buffer pool, files, and network, we were able to get the simulator running in little time.
Reference: [38] <author> E. L. Miller and R. H. Katz, </author> <booktitle> Input/Output Behavior of Supercomputing Applications, Proceedings of Supercomputing '91, </booktitle> <pages> pp. 567-576, </pages> <month> November </month> <year> 1991. </year> <month> 28 </month>
Reference-contexts: The intent of SPIFFI is to study the performance of a parallel file system within such a massively parallel system. SPIFFI files will generally be large and immutable including, for instance, scientific data sets or multimedia files <ref> [20, 33, 38, 43, 44] </ref>. 3 The Intel Paragon XP/S The Intel Paragon is a parallel supercomputer capable of scaling to thousands of nodes. The nodes are interconnected by a high-performance mesh network that permits processes at distinct nodes to exchange messages and data efficiently.
Reference: [39] <author> E. L. Miller and R. H. Katz, </author> <title> RAMA: A File System for Massively-Parallel Computers, </title> <booktitle> Proceedings of the Twelfth IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pp. 163-168, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: While the RAID-II system [18] does provide higher performance through improved hardware and software, it still does not address the parallel application interface problem. Existing parallel I/O systems include the CMMD I/O system [3] and the nCUBE parallel I/O system [14]. Existing parallel file systems include RAMA <ref> [39] </ref>, the Bridge parallel file system [16], the IBM Vesta Parallel File System [10, 11, 12], and the Intel Parallel File System (PFS) [28, 29]. The CMMD I/O system [3] provides a parallel I/O interface to parallel applications on the Thinking Machines CM-5. <p> The operating system provides a second mapping between this canonical byte stream and the disks. This system is only capable of mapping blocks whose sizes are a power of two and the number of processors or disks must also be a power of two. The RAMA parallel file system <ref> [39, 40] </ref> is intended primarily as a cache or staging area for data stored on tertiary storage. RAMA uses hashing to store and retrieve blocks of a file. Simulation results suggest that poorly written applications may benefit more from hash based disk layouts than from striped disk layouts.
Reference: [40] <author> E. L. Miller and R. H. Katz, </author> <title> RAMA: Easy Access to a High-Bandwidth Massively Parallel File System, </title> <booktitle> Proceedings of the 1995 Winter USENIX Conference, </booktitle> <pages> pp. 59-70, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: The operating system provides a second mapping between this canonical byte stream and the disks. This system is only capable of mapping blocks whose sizes are a power of two and the number of processors or disks must also be a power of two. The RAMA parallel file system <ref> [39, 40] </ref> is intended primarily as a cache or staging area for data stored on tertiary storage. RAMA uses hashing to store and retrieve blocks of a file. Simulation results suggest that poorly written applications may benefit more from hash based disk layouts than from striped disk layouts.
Reference: [41] <author> B. Nitzberg, </author> <title> Performance of the iPSC/860 Concurrent File System, </title> <type> Technical Report RND-92-020, </type> <institution> NASA Ames Research Center, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Then, when the request for page 0 arrived, it would be satisfied from the buffer pool. Furthermore, by passing requests to the disks sequentially, SPIFFI ensures that the disk cache operates efficiently. This type of prefetching was used in CFS and can cause thrashing with certain non-sequential workloads <ref> [41] </ref>. To prevent thrashing an application may specify when a file is opened whether file accesses will be globally sequential. 11 SPIFFI uses a similar procedure to improve write performance. The buffer pool delays writes and, if possible, coalesces them into a single large write.
Reference: [42] <author> J. K. Ousterhout, H. Da Costa, D. Harrison, J. A. Kunze, M. Kupfer, and J. G. Thompson, </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System, </title> <booktitle> Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Though Swift [7] and Zebra [24] do use striping to achieve higher data rates, these and the other distributed file systems were primarily designed to handle a typical UNIX workload of many, relatively small files <ref> [1, 42] </ref>. Such projects have rarely focused on the problems of distributing or striping a few large files across multiple disks within a single, tightly-coupled, massively parallel system. The intent of SPIFFI is to study the performance of a parallel file system within such a massively parallel system.
Reference: [43] <author> B. K. Pasquale and G. C. Polyzos, </author> <title> A Static Analysis of I/O Characteristics of Scientific Applications in a Production Workload, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pp. 388-397, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The intent of SPIFFI is to study the performance of a parallel file system within such a massively parallel system. SPIFFI files will generally be large and immutable including, for instance, scientific data sets or multimedia files <ref> [20, 33, 38, 43, 44] </ref>. 3 The Intel Paragon XP/S The Intel Paragon is a parallel supercomputer capable of scaling to thousands of nodes. The nodes are interconnected by a high-performance mesh network that permits processes at distinct nodes to exchange messages and data efficiently.
Reference: [44] <author> B. K. Pasquale and G. C. Polyzos, </author> <title> Dynamic I/O Characterization of I/O Intensive Scientific Applications, </title> <booktitle> Proceedings of Supercomputing '94, </booktitle> <pages> pp. 660-669, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The intent of SPIFFI is to study the performance of a parallel file system within such a massively parallel system. SPIFFI files will generally be large and immutable including, for instance, scientific data sets or multimedia files <ref> [20, 33, 38, 43, 44] </ref>. 3 The Intel Paragon XP/S The Intel Paragon is a parallel supercomputer capable of scaling to thousands of nodes. The nodes are interconnected by a high-performance mesh network that permits processes at distinct nodes to exchange messages and data efficiently.
Reference: [45] <author> D. A. Patterson, G. Gibson, and R. H. Katz, </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proceedings of the 1988 Conference on the Management of Data, </booktitle> <pages> pp. 109-116, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: There are two aspects to the I/O problem in a massively parallel supercomputer. First, applications require enough aggregate I/O bandwidth to ensure that they do not become I/O bound. Second, applications need a parallel interface that will simplify their coding and improve their performance. RAID devices <ref> [45] </ref> provide a possible solution to the bandwidth problem but do not address the application interface problem. Furthermore, as RAID sizes increase, a bottleneck may form at the RAID controller or I/O node. <p> In [34] he introduces a technique called disk-directed I/O that allows disk servers to optimize and improve I/O performance. Finally, the COMFORT project [54] proposes an algorithm for dynamically redistributing data to balance the load across a set of disks. 2 RAID devices <ref> [45] </ref> have the potential to increase I/O bandwidth but do not provide parallel applications with features such as shared file pointers. In addition, a bottleneck may form since typical RAID devices are attached to a single controller or I/O node.
Reference: [46] <author> C. A. Polyzois, A. Bhide, and D. M. Dias, </author> <title> Disk Mirroring with Alternating Deferred Updates, </title> <booktitle> Proceedings of the Nineteenth International Conference on Very Large Databases, </booktitle> <pages> pp. 604-617, </pages> <month> August </month> <year> 1993. </year>
Reference: [47] <author> J. S. Quarterman, A. Silberschatz, and J. L. Peterson, </author> <title> 4.2BSD and 4.3BSD as Examples of the UNIX System, </title> <journal> Computing Surveys, </journal> <volume> vol. 17, no. 4, </volume> <pages> pp. 379-418, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: A file fragment is the portion of a SPIFFI file stored at one disk. In this study the file fragments were laid out contiguously on disk. Many existing file systems, including the Berkeley fast file system <ref> [47] </ref> and extent based file systems [9], also attempt to allocate large files in contiguous chunks. The set of disks across which a SPIFFI file is striped and the file's striping granularity are collectively referred to as the file's metadata.
Reference: [48] <author> D. Ries and R. Epstein, </author> <title> Evaluation of Distribution Criteria for Distributed Database Systems, </title> <type> UCB/ERL Technical Report M78/22, </type> <institution> UC Berkeley, </institution> <month> May </month> <year> 1978. </year>
Reference-contexts: We then discuss the internal operation of SPIFFI including an explanation of SPIFFI's internal processes and buffer pool. Finally, we briefly describe the programming interface exported by SPIFFI. 4.1 File Layout SPIFFI horizontally partitions <ref> [48] </ref> files in a round-robin fashion across selected disks in the system. When creating a file, the user specifies a set of disks and the striping granularity (i.e., the number of bytes that should be stored at each disk before proceeding to the next one).
Reference: [49] <author> H. Schwetman, </author> <title> CSIM Users' Guide, </title> <type> MCC Technical Report No. </type> <institution> ACT-126-90, Microelectronics and Computer Technology Corporation, Austin, TX, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The simulator, which is written in the CSIM/C++ process-oriented simulation language <ref> [49] </ref>, is based on a simulator of the Gamma database machine [15, 37]. Since we were able to reuse many of the components from the earlier simulator including the disks, buffer pool, files, and network, we were able to get the simulator running in little time.
Reference: [50] <author> A. Silberschatz and P. B. Galvin, </author> <title> Operating Systems Concepts, 4th edition, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1994. </year>
Reference-contexts: Since an application may sometimes require that data be written to disk before continuing, for example, after a checkpoint, SPIFFI provides a call to clean all dirty buffers associated with an open file. The buffer pool uses a simple global LRU (least recently used) policy <ref> [50] </ref> to select pages for replacement. Although the buffer pool only replaces clean pages since they are available immediately, dirty pages are also stored in the LRU chain so that once they are cleaned, they will not be given preference over newer clean pages.
Reference: [51] <author> P. Corbett, D. Feitelson, Y. Hsu, J.-P. Prost, M. Snir, S. Fineberg, B. Nitzberg, B. Traversat, and P. Wong, </author> <title> MPI-IO: A Parallel File I/O Interface for MPI, Version 0.3, </title> <type> Technical Report NAS-95-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1995. </year>
Reference: [52] <author> R. Thakur, R. Bordawekar, and A. Choudhary, </author> <title> Compiler and Runtime Support for Out-of-Core HPF Programs, </title> <booktitle> Proceedings of the Eighth ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: These four types of file pointers make SPIFFI's interface flexible and simplify the construction of more sophisticated or specialized I/O libraries. For example, a library for accessing out-of-core High Performance Fortran arrays <ref> [52] </ref> could easily be layered on top of the basic SPIFFI interface. Currently, a SPIFFI prototype is running on a 64 node/64 disk Intel Paragon (one 1 Gbyte disk on each of the 64 nodes) and has been used to validate a SPIFFI simulator.
Reference: [53] <author> B. Walker, G. Popek, R. English, C. Kline, and G. Thiel, </author> <title> The LOCUS Distributed Operating System, </title> <booktitle> Proceedings of the Ninth ACM Symposium on Operating System Principles, </booktitle> <pages> pp. 49-69, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: We compare SPIFFI's file pointers to PFS's I/O modes in Section 4.2 and provide some PFS performance data in Section 5.1. 3 Finally, although there has been a great deal of research into distributed operating systems and distributed file systems <ref> [7, 21, 24, 26, 53] </ref>, generally these efforts have focused on issues such as transparent access to or replication of resources across physically distinct and often heterogeneous machines.
Reference: [54] <author> G. Weikum, C. Hasse, A. Moenkeberg, M. Rys, and P. Zabback, </author> <title> The COMFORT Project, </title> <booktitle> Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pp. 158-161, </pages> <month> January </month> <year> 1993. </year> <month> 29 </month>
Reference-contexts: In [32] Kotz compares several parallel file systems by surveying the results from independent performance studies. In [34] he introduces a technique called disk-directed I/O that allows disk servers to optimize and improve I/O performance. Finally, the COMFORT project <ref> [54] </ref> proposes an algorithm for dynamically redistributing data to balance the load across a set of disks. 2 RAID devices [45] have the potential to increase I/O bandwidth but do not provide parallel applications with features such as shared file pointers.
References-found: 53

