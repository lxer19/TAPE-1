URL: http://www.ri.cmu.edu/afs/cs/user/kseymore/html/SLT_97/H497E.PS
Refering-URL: http://www.ri.cmu.edu/afs/cs/user/kseymore/html/papers.html
Root-URL: 
Title: The 1997 CMU Sphinx-3 English Broadcast News Transcription System  
Author: K. Seymore, S. Chen, S. Doh, M. Eskenazi, E. Gouv ea, B. Raj, M. Ravishankar, R. Rosenfeld, M. Siegler, R. Stern, and E. Thayer 
Address: Pittsburgh, Pennsylvania 15213  
Affiliation: Carnegie Mellon University  
Abstract: This paper describes the 1997 Hub-4 Broadcast News Sphinx-3 speech recognition system. This year's system includes full-bandwidth acoustic models trained on Broadcast News and Wall Street Journal acoustic training data, an expanded vocabulary, and a 4-gram language model for N-best list rescoring. The system structure, acoustic and language models, and adaptation components are described in detail, and results are presented to establish the contributions of multiple recognition passes. Additionally, experimental results are presented for several different acoustic and language model configurations. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. Y. Hwang, </author> <title> Subphonetic Acoustic Modeling for Speaker-Independent Continuous Speech Recognition, </title> <type> PhD. thesis, </type> <institution> Carnegie Mellon University, Computer Science Department tech report CMU-CS-93-230, </institution> <year> 1993. </year>
Reference-contexts: A variety of experimental results on acoustic model and language model variations are presented in Section 4. Evaluation results for each stage of processing are given in Section 5. 2. SYSTEM OVERVIEW The Sphinx-3 system is a fully-continuous Hidden Markov Model-based speech recognizer that uses senonically-tied states <ref> [1] </ref>. Each state is a mixture of a number of diagonal-covariance Gaussian densities. The 1997 Sphinx-3 configuration is similar in many ways to the 1996 system [5]. The recognition process consists of acoustic segmentation, classification and clustering [8], followed by three recognition passes. <p> The means of the baseline acoustic models are transformed for each cluster and the adapted models are used during the next recognition pass. 3. EVALUATION SYSTEM 3.1. Acoustic Models The acoustic models used in the evaluation system are fully-continuous, diagonal-covariance mixture Gaussian models with approximately 6000 senonically-tied <ref> [1] </ref> states. A five-state Bakis model topology is used throughout. Two sets of acoustic models are used: non-telephone (full-bandwidth) models and telephone (narrow-bandwidth) models. The non-telephone models are trained over the Wall Street Journal SI-284 corpus concatenated with the Hub-4 Broadcast News training corpus.
Reference: 2. <author> S. M. Katz, </author> <title> Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer, </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> vol. ASSP-35, no. 3, </volume> <pages> pp. 400-401, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: Language Model Smoothing Two language models were built using different smoothing techniques. The first model was a 51k Good-Turing discounted trigram backoff language model <ref> [2] </ref>, and the second a 51k Kneser-Ney smoothed trigram language model [3]. The Sphinx-3 decoder was run on SET1 with each language model, holding all other parameters constant. Word error rate results are shown in Table 5. The Good-Turing discounted backoff model provided superior performance on this test set. 4.4.
Reference: 3. <author> R. Kneser and H. Ney, </author> <title> Improved Backing-off for M-Gram Language Modeling, </title> <booktitle> Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 181-184, </pages> <year> 1995. </year>
Reference-contexts: Language Model Smoothing Two language models were built using different smoothing techniques. The first model was a 51k Good-Turing discounted trigram backoff language model [2], and the second a 51k Kneser-Ney smoothed trigram language model <ref> [3] </ref>. The Sphinx-3 decoder was run on SET1 with each language model, holding all other parameters constant. Word error rate results are shown in Table 5. The Good-Turing discounted backoff model provided superior performance on this test set. 4.4.
Reference: 4. <author> C. J. Leggetter, and P. C. Woodland, </author> <title> Speaker Adaptation of HMMS using Linear Regression, </title> <institution> Cambridge University Engg. Dept., </institution> <type> F-INFENG, Tech Report 181, </type> <month> June </month> <year> 1994. </year>
Reference-contexts: Each pass consists of a Viterbi decoding using beam search and a best path search of the Viterbi word lattice. The final two passes include N-best list generation and rescoring. Between each pass, acoustic adaptation using a transformation of the mean vectors based on linear regression (MLLR) <ref> [4] </ref> is performed. These steps are summarized in the following list: 1. Automatic data segmentation, classification, and clustering 2. Pass 1 a. Viterbi decoding using beam search b. Best path search of Viterbi word lattice 3. Acoustic adaptation 4. Pass 2 a. Viterbi decoding using beam search b. <p> Acoustic Adaptation Unsupervised adaptation of Gaussian density means in the acoustic model is performed, given the output of the best path or N-best search. In order to obtain larger sample sizes, the test set is clustered as described in Section 2.1. The maximum likelihood linear regression (MLLR) <ref> [4] </ref> approach to mean adaptation is used. A 1-class MLLR transform is obtained for each cluster using the baseline acoustic models and the selected hypotheses. The means of the baseline acoustic models are transformed for each cluster and the adapted models are used during the next recognition pass. 3.
Reference: 5. <author> P. Placeway, S. Chen, M. Eskenazi, U. Jain, V. Parikh, B. Raj, M. Ravishankar, R. Rosenfeld, K. Seymore, M. Siegler, R. Stern, and E. </author> <booktitle> Thayer The 1996 Hub-4 Sphinx-3 System, Proceedings of the 1997 ARPA Speech Recognition Workshop, </booktitle> <pages> pp. 85-89, </pages> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: SYSTEM OVERVIEW The Sphinx-3 system is a fully-continuous Hidden Markov Model-based speech recognizer that uses senonically-tied states [1]. Each state is a mixture of a number of diagonal-covariance Gaussian densities. The 1997 Sphinx-3 configuration is similar in many ways to the 1996 system <ref> [5] </ref>. The recognition process consists of acoustic segmentation, classification and clustering [8], followed by three recognition passes. Each pass consists of a Viterbi decoding using beam search and a best path search of the Viterbi word lattice. The final two passes include N-best list generation and rescoring.
Reference: 6. <author> M. Ravishankar, </author> <title> Efficient Algorithms for Speech Recognition, </title> <type> PhD. thesis, </type> <institution> Carnegie Mellon University, Computer Science Department tech report CMU-CS-96-143, </institution> <year> 1996. </year>
Reference-contexts: Best Path Search: A word graph is constructed from the Viterbi word lattice and then searched for the global best path according to a trigram language model and an empirically determined optimal language weight using a shortest path graph search algorithm <ref> [6] </ref>. The only acoustic scores used in this search are the ones stored in the lattice from the Viterbi recognition. As a result, this search is much quicker than the Viterbi search. A new best-scoring hypothesis transcription is produced.
Reference: 7. <author> K. Seymore, S. Chen, M. Eskenazi and R. Rosenfeld, </author> <title> Language and Pronunciation Modeling in the CMU 1996 Hub-4 Evaluation, </title> <booktitle> Proceedings of the 1997 ARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: The smoothing parameters, language weight, and word insertion penalty are optimized using Powell's algorithm on the entire development test set. Filled pauses are predicted with unigram probabilities that are estimated from the acoustic training data <ref> [7] </ref>. This year, acoustic models were built from scratch for each filled pause event. 3.4. Improvements This year's evaluation system incorporates several improvements over last year's system. The acoustic models are trained on an improved lexicon, and the filler word set introduced last year is trained from scratch. <p> The first language model, noted by S, is a trigram backoff language model trained on language model training text annotated with sentence-boundary tokens. The second language model, XB, contains the sentence-boundary tokens as well as cross-boundary trigrams <ref> [7] </ref>, which are meant to help model the case where sentence boundaries occur inside of an utterance. The third model, NS, is built from the training text without sentence-boundary tokens. Each model is used to decode SET2 using an automatically generated segmentation.
Reference: 8. <author> M. Siegler, U. Jain, B. Raj, and R. Stern, </author> <title> Automatic Segmentation, Classification and Clustering of Broadcast News Audio, </title> <booktitle> Proceedings of the 1997 ARPA Speech Recognition Workshop, </booktitle> <pages> pp. 97-99, </pages> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: Each state is a mixture of a number of diagonal-covariance Gaussian densities. The 1997 Sphinx-3 configuration is similar in many ways to the 1996 system [5]. The recognition process consists of acoustic segmentation, classification and clustering <ref> [8] </ref>, followed by three recognition passes. Each pass consists of a Viterbi decoding using beam search and a best path search of the Viterbi word lattice. The final two passes include N-best list generation and rescoring. <p> Segments are then clustered together into acoustically-similar groups, which is useful for acoustic adaptation. Finally, all segments that encompass more than 30 seconds of data are subsegmented into smaller utterances. These techniques are summarized below; details are available in <ref> [8] </ref>. Automatic Segmentation: The goal of automatic segmentation is to break the audio stream into acoustically homogeneous sections. Ideally, segment boundaries should occur in silence regions so that a word is not split in two.
References-found: 8

