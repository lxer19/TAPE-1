URL: ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/SSP-98.ps
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Email: rich@cs.umass.edu  baveja@cs.colorado.edu  dprecup@cs.colorado.edu  
Title: Improved Switching among Temporally Abstract Actions  
Author: Richard S. Sutton Satinder Singh Doina Precup 
Affiliation: University of Massachusetts  University of Colorado  University of Massachusetts  
Abstract: In robotics and other control applications it is commonplace to have a pre-existing set of controllers for solving subtasks, perhaps handcrafted or previously learned or planned, and yet still face a difficult problem of how best to choose and switch among the controllers to best solve an overall task. In this paper we present a framework for phrasing this problem, a basic theorem regarding the improvement in performance that can be obtained by switching flexibly between given controllers, and example applications of the theorem. Our framework is based on the existing frameworks of reinforcement learning, Markov decision processes (MDPs), and semi-Markov decision processes (SMDPs). The pre-existing controllers are modeled as policies with termination conditions, which we together refer to as options. Options provide temporally abstract actions that can be used interchangably with primitive actions in many learning and planning methods. The primary novelty of our framework is that it overlays an SMDP analysis on top of an MDP. In particular, in this paper we show how planning at the SMDP level can be used to alter action selection at the MDP level to achieve better performance, with negligible additional cost, than can be achieved by SMDP methods alone. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bradtke, S.J., and Duff, M.O. </author> <year> (1995). </year> <title> Reinforcement learning methods for continuous-time Markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7 </booktitle> <pages> 393-400. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Dayan, P. </author> <title> (1993) "Improving generalization for temporal difference learning: </title> <booktitle> The successor representation," Neural Computation 5 </booktitle> <pages> 613-624. </pages>
Reference: <author> Dayan, P., Hinton, G.E. </author> <title> (1993) "Feudal reinforcement learning". </title> <booktitle> In Advances in Neural Information Processing Systems 5 </booktitle> <pages> 271-278. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dietterich, T.G. </author> <title> (1997) "Hierarchical reinforcement learning with the MAXQ value function decomposition". </title> <type> Technical Report, </type> <institution> Department of Computer Science, Oregon State University. </institution>
Reference: <author> Haigh, K.Z., Shewchuk, J., Veloso, M,M, </author> <year> (1997). </year> <title> "Exploring geometry in analogical route planning." </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence 9 </journal> <pages> 509-541. </pages>
Reference: <author> Huber, M., Grupen, R.A. </author> <title> (1997) "A feedback control structure for on-line learning tasks, </title> <booktitle> Robotics and Autonomous Systems 22(3-4):303-315. </booktitle>
Reference: <author> Kaelbling, </author> <title> L.P. (1993) "Hierarchical learning in stochastic domains: Preliminary results," </title> <booktitle> Proc. of the Tenth Int. Conf. on Machine Learning, </booktitle> <pages> 167-173, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kalmar, Z., Szepesvari, C., Lorincz, A. </author> <year> (1997). </year> <title> Module based reinforcement learning for a real robot. </title> <booktitle> In Proceedings of the Sixth European Workshop on Learning Robots, </booktitle> <pages> pp. 22-32. </pages>
Reference: <author> Lin, L.-J. </author> <title> (1993) Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University. </institution> <note> Technical Report CMU-CS-93-103. </note>
Reference: <author> Mahadevan, S., Connell, J. </author> <title> (1992) "Automatic Programming of Behavior-based Robots using Reinforcement Learning." </title> <booktitle> Artificial Intelligence 55(2-3): </booktitle> <pages> 311-365. </pages>
Reference: <author> Mahadevan, S., Marchalleck, N., Das, T., Gosavi, A. </author> <title> (1997) "Self-Improving Factory Simulation using Continuous-Time Average-Reward Reinforcement Learning." </title> <booktitle> In Proceedings of the 14th International Conference on Machine Learning. </booktitle>
Reference: <author> Mataric, M.J. </author> <title> (1997) "Behavior-based control: Examples from navigation, learning, and group behavior." </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence 9(2-3). </journal>
Reference: <author> McGovern, A., Sutton, </author> <title> R.S., Fagg, A.H. (1997) Roles of Macro-Actions in Accelerating Reinforcement Learning. </title> <booktitle> In Proceedings of the 1997 Grace Hopper Celebration of Women in Computing. </booktitle>
Reference: <author> Moore, A.W. </author> <title> (1994) "The parti-game algorithm for variable resolution reinforcement learning in multidimensional spaces", </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pp. 711-718, </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> 15 Parr, </note> <author> R., Russell, S. </author> <year> (1998). </year> <title> Reinforcement learning with hierarchies of machines. </title> <booktitle> In Advances in Neural Information Processing Systems 11. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Rather than work with these low-level actions, infinite in number, we introduce seven landmark locations in the space. For each landmark we define a controller that takes us to the landmark in a direct path <ref> (cf. Moore, 1994) </ref>. Each controller is only applicable within a limited range of states, in this case within a certain distance of the corresponding landmark.
Reference: <author> Parr, R. </author> <title> (in preparation) "Hierarchical control and learning for Markov decision processes, </title> <note> chapter 3." </note> <author> Precup, D., Sutton, R.S., Singh, S. </author> <year> (1997). </year> <title> Planning with closed-loop macro actions. </title> <booktitle> In Working notes of the 1997 AAAI Fall Symposium on Model-directed Autonomous Systems. </booktitle>
Reference: <author> Precup, D., Sutton, R.S., Singh, S. </author> <year> (1998). </year> <title> Theoretical results on reinforcement learning with temporally abstract options. </title> <booktitle> In Proceedings of the Tenth European Conference on Machine Learning. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: <author> Precup, D., Sutton, </author> <title> R.S. (1998) Multi-time models for temporally abstract planning. </title> <booktitle> In Advances in Neural Information Processing Systems 11. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Problems. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: o, the composition of o and , denotes the semi-Markov policy that first follows o until it terminates and then initiates in the resultant state. 3 SMDP Planning Options are closely related to the actions in a special kind of decision problem known as a semi-Markov decision process, or SMDP <ref> (e.g., see Puterman, 1994) </ref>. In fact, any MDP with a fixed set of options is an SMDP. Accordingly, the theory of SMDPs provides an important basis for a theory of options. The limitation of SMDP theory is that it treats options as indivisible units without internal structure.
Reference: <author> Ring, M. </author> <title> (1991) "Incremental development of complex behaviors through automatic construction of sensory-motor hierarchies," </title> <booktitle> Proceedings of the Eighth International Conference on Machine Learning, </booktitle> <pages> 343-347, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sastry, S. </author> <year> (1997). </year> <title> Algorithms for design of hybrid systems. </title> <booktitle> In Proceedings of the International Conference of Information Sciences. </booktitle>
Reference: <author> Singh, S. </author> <title> (1992a) "Reinforcement learning with a hierarchy of abstract models," </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 202-207. </pages> <publisher> MIT/AAAI Press. </publisher>
Reference: <author> Singh, S. </author> <title> (1992b) "Scaling reinforcement learning by learning variable temporal resolution models," </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> 406-415, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R.S. </author> <year> (1995). </year> <title> TD models: Modeling the world at a mixture of time scales. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. 531-539, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In particular, they limit the range over which the option's policy need be defined. For example, a hand-crafted policy for a mobile robot to dock with its battery charger might be 1 The termination condition fi is like the fi in fi-models <ref> (Sutton, 1995) </ref>, but with an opposite sense. That is, fi (s) in this paper corresponds to 1 fi (s) in that earlier paper. 4 defined only for states I in which the battery charger is within sight.
Reference: <author> Sutton, R.S., Barto, A.G. </author> <year> (1998). </year> <title> Reinforcement Learning: An Introduction. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Sutton, R.S., Precup, D., Singh, S. </author> <title> (in preparation) "Between MDPs and semi-MDPs: learning, planning, and representing knowledge at multiple temporal scales." </title> <note> In preparation. </note>
Reference: <author> Thrun, T., Schwartz, A. </author> <title> (1995) "Finding Structure in Reinforcement Learning," </title> <booktitle> in Advances in Neural Information Processing Systems, 7. </booktitle> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 16 </pages>
References-found: 26

