URL: ftp://ftp.cse.cuhk.edu.hk/pub/techreports/96/tr-96-5.ps.gz
Refering-URL: ftp://ftp.cs.cuhk.hk/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: pfsum,csleung,lwchan@cs.cuhk.hk  
Title: Extended Kalman filter in recurrent neural network training and pruning  
Author: John P.F. Sum, C.S. Leung and L.W. Chan 
Date: May 30, 1996  
Address: Shatin, N.T., Hong Kong  
Affiliation: Department of Computer Science and Engineering, Chinese University of Hong Kong  
Pubnum: Technical report CS-TR-96-05  
Abstract: Recently, extended Kalman filter (EKF) based training has been demonstrated to be effective in neural network training. However, its conjunction with pruning methods such as weight decay and optimal brain damage (OBD) has not yet been studied. In this paper, we will elucidate the method of EKF training and propose a pruning method which is based on the results obtained by EKF training. These combined training pruning method is applied to a time series prediction problem.
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson B.D.O. and J. </author> <title> Moore (1979). Optimal Filtering, </title> <publisher> Prentice Hall Inc. </publisher>
Reference-contexts: Owing to this difficulty, we should seek another training method which can prune the RNN efficiently. Recursive least square (RLS) and extended Kalman filter based training <ref> (Anderson and Moore 1979) </ref> is one of the popular methods in training neural network in the past few 1 years and its fast convergence speed has been demonstrated by some researchers (Williams 1992). <p> In fact, there is an additional advantage of using EKF training | EKF is able to closely estimate the hidden neurons' activities in the maximum a posterior (MAP) sense <ref> (Anderson and Moore 1979) </ref>. In such case, the evaluation of the dependent of the output activity to the input-hidden layer weights or the hidden layer self-recurrent weights does not require RTRL algorithm, in contrast to the method Puskorius and Feldkamp (1994).
Reference: <author> LeCun Y. et al. </author> <year> (1990). </year> <title> Optimal brain damage, </title> <booktitle> Advances in Neural Information Processing Systems 2 (D.S. </booktitle> <address> Touretsky, </address> <publisher> ed.) </publisher> <pages> 396-404. </pages>
Reference-contexts: Here E bp is the testing error before pruning. According to our empirical study, the threshold is set to 1:1 fi E bp . It should note that the proposed pruning method is in principle akin to the optimal brain damage proposed by Le Cun et al. <ref> (LeCun et al. 1990) </ref>, flOBD proposed by With Pedersen et al. (With Pedersen et al. 1996) and model structure selection method suggested by Ljung et al. (1992). 4 Simulation results We apply the proposed method to model the USD/DEM exchange rate z (t). <p> We demonstrate the idea and its performance through intensive simulations on the problem of exchange rate prediction. Comparing our method to Hessian-based pruning algorithms such as OBD <ref> (LeCun et al. 1990) </ref> and its variants (With Pedersen and Hansen 1995; With Pedersen et al. 1996), there is one crucial difference between their methods and ours. The evaluation of @ 2 E @ 2 requires tedious recursive computational procedure, (see p.677 of (With Pedersen and Hansen 1995)).
Reference: <author> Mosca E. </author> <year> (1995), </year> <title> Optimal Predictive and Adaptive Control. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: It is just a special case of EKF once we consider the RLS problem as following filtering problem: (t + 1) = (t) where w is a zero mean unit variance Gaussian noise. 4 With reference to <ref> (Mosca 1995) </ref> that the objective function of recursive least square is defined as follows: E () = 2 i=1 P 1 # where 0 and P 0 are the initial value of and P .
Reference: <author> Puskorius G.V. and L.A. </author> <month> Feldkamp </month> <year> (1994), </year> <title> Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> Vol.5(2), </volume> <pages> 279-297. </pages>
Reference-contexts: + 1); (t)); (21) R x (t) = (1 ff R )R x (t 1) + ff R (y fl (t) y (tjt 1)) 2 (23) Advantage of using EKF in RNN training The advantage of using EKF training is mentioned in part in (Williams 1992) and section IVc of <ref> (Puskorius and Feldkamp 1994) </ref> mainly in the computation complexity of the weight values. In fact, there is an additional advantage of using EKF training | EKF is able to closely estimate the hidden neurons' activities in the maximum a posterior (MAP) sense (Anderson and Moore 1979).
Reference: <author> Reed R. </author> <year> (1993), </year> <title> Pruning algorithms A survey, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> Vol.4(5), </volume> <pages> 740-747. </pages>
Reference-contexts: 1 Introduction In the current neural network study, the methods of training and pruning are usually considered as two independent issues. Very few attempts have been made to combine training method with weight pruning <ref> (Reed 1993) </ref>. Recently, Sjoberg and Ljung (1995) and With Ped-ersen et al. (1996) have applied the method of Gauss-Newton to train the feedforward neural network. Then using the Hessian matrix obtained from the training to prune the neural network.
Reference: <author> Sjoberg J. and L. </author> <title> Ljung (1995), Overtraining, regularization and searching for a minimum, with application to neural networks. </title> <journal> Int. J. Control, </journal> <volume> 62, </volume> <pages> 1391-1407. </pages> <note> 8 Weigend A.S. </note> <editor> et al. </editor> <year> (1991), </year> <title> Generalization by weight-elimination applied to currency ex-change rate prediction. </title> <booktitle> Proceeding of IJCNN'91, Vol.I, </booktitle> <pages> 837-841. </pages>
Reference: <author> Williams R.J. </author> <year> (1992), </year> <title> Training recurrent networks using the extended Kalman filter, </title> <booktitle> Proceedings of the IJCNN'92 Baltimore, Vol.IV, </booktitle> <pages> 241-246. </pages>
Reference-contexts: Recursive least square (RLS) and extended Kalman filter based training (Anderson and Moore 1979) is one of the popular methods in training neural network in the past few 1 years and its fast convergence speed has been demonstrated by some researchers <ref> (Williams 1992) </ref>. Therefore, in this paper, we attempt to explore the feasibility of combining both EKF training and pruning as a whole. <p> g (x (tjt); u fl (t + 1); (t)); (21) R x (t) = (1 ff R )R x (t 1) + ff R (y fl (t) y (tjt 1)) 2 (23) Advantage of using EKF in RNN training The advantage of using EKF training is mentioned in part in <ref> (Williams 1992) </ref> and section IVc of (Puskorius and Feldkamp 1994) mainly in the computation complexity of the weight values.
Reference: <author> With Pedersen M. and L.K. </author> <title> Hansen (1995), Recurrent networks: Second order properties and pruning, </title> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <editor> (G. Tesauro et al. </editor> <booktitle> ed.). </booktitle> <pages> 673-680, </pages> <publisher> MIT Press. </publisher>
Reference-contexts: Applying Hessian based pruning method to prune a feedforward neural network have been demonstrated to be effective in these papers. However, it is in general not so effective when it is applied to prune a recurrent neural network since the computation complexity of the Hessian matrix is so large <ref> (With Pedersen and Hansen 1995) </ref>, much larger than the one for feedforward. Owing to this difficulty, we should seek another training method which can prune the RNN efficiently. <p> The evaluation of @ 2 E @ 2 requires tedious recursive computational procedure, (see p.677 of <ref> (With Pedersen and Hansen 1995) </ref>). This recursion makes the evaluation of @ 2 E @ 2 not so feasible especially when the length of the training time series, N , is too long. It should be aware that our major concern in this paper is the way of weight pruning.
Reference: <author> With Pedersen et al. </author> <year> (1996), </year> <title> Pruning with generalization based weight saliencies: </title> <journal> flOBD and flOBS. </journal> <note> To appear in Advances in Neural Information Processing Systems 8, edited by D.S. </note> <editor> Touretzky et al.. </editor> <publisher> MIT Press. </publisher>
Reference-contexts: According to our empirical study, the threshold is set to 1:1 fi E bp . It should note that the proposed pruning method is in principle akin to the optimal brain damage proposed by Le Cun et al. (LeCun et al. 1990), flOBD proposed by With Pedersen et al. <ref> (With Pedersen et al. 1996) </ref> and model structure selection method suggested by Ljung et al. (1992). 4 Simulation results We apply the proposed method to model the USD/DEM exchange rate z (t). The range of the data are from Nov 29 1991 to Nov 3 1994, altogether 764 working days.
Reference: <author> Wu L. and J. </author> <title> Moody (1996), A smoothing regularizer for feedforward and recurrent neural networks, </title> <journal> Neural Computation, </journal> <volume> 8, </volume> <pages> 461-489. 9 </pages>
References-found: 10

