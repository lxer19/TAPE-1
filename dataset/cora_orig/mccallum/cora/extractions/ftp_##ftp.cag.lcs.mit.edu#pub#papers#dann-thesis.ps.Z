URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/dann-thesis.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/dann-thesis.html
Root-URL: 
Title: Run-Time Thread Management for Large-Scale Distributed-Memory Multiprocessors  
Author: by Daniel Nussbaum Anant Agarwal 
Degree: 1988 Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  Signature of Author  Certified by  Associate Professor of Computer Science and Engineering Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, EECS Committee on Graduate Students  
Note: c Massachusetts Institute of Technology  
Date: September 1993  1993  August 31, 1993  
Address: 1985  
Affiliation: B.S., Electrical Engineering and Computer Science Massachusetts Institute of Technology,  S.M., Electrical Engineering and Computer Science Massachusetts Institute of Technology,  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Anant Agarwal. </author> <title> Overview of the Alewife Project. </title> <note> Alewife Systems Memo #10., </note> <month> July </month> <year> 1990. </year>
Reference-contexts: Finally, the mapping of the tree onto the physical processor array enhances communication locality of the application in a natural way, usually causing threads to run on processors near to where they were created. An implementation of XTM has been written for the MIT Alewife Multiprocessor <ref> [1, 2] </ref>. Alewife provides both an efficient implementation of the shared-memory abstraction and efficient interprocessor messages. XTM employs message-passing as its primary communication style. This message-passing implementation is made possible by the static mapping of the X-Tree data structure onto the physical processor array.
Reference: [2] <author> Anant Agarwal, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, Dan Nussbaum, Mike Parkin, and Donald Yeung. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors, </booktitle> <year> 1991. </year>
Reference-contexts: Finally, the mapping of the tree onto the physical processor array enhances communication locality of the application in a natural way, usually causing threads to run on processors near to where they were created. An implementation of XTM has been written for the MIT Alewife Multiprocessor <ref> [1, 2] </ref>. Alewife provides both an efficient implementation of the shared-memory abstraction and efficient interprocessor messages. XTM employs message-passing as its primary communication style. This message-passing implementation is made possible by the static mapping of the X-Tree data structure onto the physical processor array.
Reference: [3] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and Distributed Computing: Numerical Methods. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: Their description is a rather brief part of a larger picture and gives no specific details. However, the settling time for Jacobi Relaxation is proportional to the square of the diameter of the mesh upon which the relaxation takes place <ref> [3] </ref>. More sophisticated relaxation techniques, such as Multigrid methods [3], achieve much better convergence times at the expense of the locality achieved by simple diffusion methods. The XTM algorithm presented by this thesis can be thought of as the multigrid version of diffusion scheduling. <p> Their description is a rather brief part of a larger picture and gives no specific details. However, the settling time for Jacobi Relaxation is proportional to the square of the diameter of the mesh upon which the relaxation takes place <ref> [3] </ref>. More sophisticated relaxation techniques, such as Multigrid methods [3], achieve much better convergence times at the expense of the locality achieved by simple diffusion methods. The XTM algorithm presented by this thesis can be thought of as the multigrid version of diffusion scheduling. We implemented both the simple diffusion scheduling algorithm 24 described here and XTM. <p> The particular choice of the "6" in the expression r = (l 0 l n )+3 6 comes out of a Jacobi Over-Relaxation <ref> [3] </ref> with relaxation parameter fl set to 2 3 . <p> Unfortunately, even when there is no such interaction between integer queue lengths and stability requirements, the relaxation time is still fi , where l is the diameter of the communications network <ref> [3] </ref>. This is very slow compared to the tree algorithms, which balance neighboring tree nodes in time proportional to the distance between the nodes, not the square of that distance. Finally, the overhead of unneeded relaxation steps slows down all processors some fixed amount.
Reference: [4] <author> Lucien M. Censier and Paul Feautrier. </author> <title> A New Solution to Coherence Problems in Mul-ticache Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> December </month> <year> 1978. </year>
Reference-contexts: Four variations on the basic MATMUL were tried: coarse-grained and cached, fine-grained and cached, coarse-grained and uncached, and fine-grained and and uncached. The cached versions simulated full-mapped caches <ref> [4] </ref>. The uncached versions were tested in order to separate out the effect of caching on the application from the thread managers' effects. Two partitioning strategies were employed for this application, as pictured in Figures 6-8 and 6-9. The coarse-grained partitioning strategy takes advantage of locality inherent to the application.
Reference: [5] <author> William J. Dally. </author> <title> Performance Analysis of k-ary n-cube Interconnection Networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-39(6):775-785, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: In this thesis, we examine that class of machines based on k-ary n-dimensional mesh communications networks connecting p processors, where p = k n . Such an architecture is simple and can scale to arbitrarily large sizes without encountering wire-packing or wire-length problems for n 3 <ref> [5] </ref>. Similar analyses can be performed for networks with richer communication structures.
Reference: [6] <author> Alvin M. Despain and David A. Patterson. X-TREE: </author> <title> A Tree Structured MultiProcessor Computer Architecture. </title> <booktitle> In Proceedings of the Fifth International Symposium on Computer Architecture, </booktitle> <year> 1978. </year>
Reference-contexts: If such programs are to be run in an efficient manner on large-scale multiprocessors, then efficient run-time thread placement and scheduling techniques are needed. This thesis examines the problems faced by an on-line thread-management system and presents XTM, an X-Tree-based <ref> [25, 6] </ref> Thread-Management system that attempts to overcome these problems. The general thread-management problem is NP-hard [7]. <p> Guided by the design principles given above, we have developed XTM, a thread management system that is sound from both a theoretical and a practical perspective. XTM solves the sub-problems identified above as follows: 1. Global information is collected and disseminated using an X-Tree <ref> [25, 6] </ref> data structure embedded in the communications network (see Figures 4-1 and 4-2). Each node in the tree contains a "presence bit" whose value indicates whether there are any runnable threads in the sub-tree headed by that node. <p> Stated briefly, the high-level solutions to each of those sub-problems are: 1. Global information is collected and disseminated using an X-Tree <ref> [25, 6] </ref> data structure embedded in the communications network. Each node in the tree contains a "presence bit" whose value indicates whether there are any runnable threads in the sub-tree headed by that node. 27 2. <p> This loss of locality can be alleviated by adding connections in the tree between nodes that are physically near each other. Such a tree, called an X-Tree, is the basic data structure upon which XTM's algorithms are based. Despain, et. al. <ref> [25, 6] </ref>, first introduced the X-Tree data structure as a communications network topology. The particular variant of X-Tree we use is a full-ring X-Tree without end-around connections. The rest of this section discusses the need for global information in solving the dynamic thread management problem. <p> Locality lost in this manner can be regained by adding connections in the tree between nodes that are physically near each other. A tree enhanced with such links, introduced in [25] and <ref> [6] </ref> as a full-ring X-Tree without end-around connections, is the basic data structure upon which XTM's algorithms are based (see Figures 4-1 and 4-2). In Chapter 5, we show that the nearest-neighbor links are needed to get good theoretical behavior. <p> In the next chapter, we show how we assembled these decisions to produce a detailed design of a high-performance thread-management system. 35 Chapter 4 X-Tree-Based Thread Management This thesis presents a thread distribution system based on an X-Tree-driven search. An X-Tree <ref> [25, 6] </ref> is a tree augmented with links between same-level nodes. The particular variant we use contains near-neighbor links between touching same-level nodes. In the parlance used in [25] and [6], this is a full-ring X-Tree without end-around connections (see Figures 4-1 and 4-2). <p> An X-Tree [25, 6] is a tree augmented with links between same-level nodes. The particular variant we use contains near-neighbor links between touching same-level nodes. In the parlance used in [25] and <ref> [6] </ref>, this is a full-ring X-Tree without end-around connections (see Figures 4-1 and 4-2). In this chapter, we describe in detail the algorithms that go into XTM, an X-Tree-based Thread Manager. When a new thread is created, its existence is made public by means of presence bits in the X-Tree.
Reference: [7] <author> Michael R. Garey and David S. Johnson. Computers and Intractability. W. H. Freeman and Company, </author> <year> 1979. </year>
Reference-contexts: This thesis examines the problems faced by an on-line thread-management system and presents XTM, an X-Tree-based [25, 6] Thread-Management system that attempts to overcome these problems. The general thread-management problem is NP-hard <ref> [7] </ref>. The standard problem has the following characteristics: precedence relations are considered, the communications network is flat and infinitely fast, tasks take differing amounts of time to finish, preemptive scheduling is not allowed, and the thread-management algorithm can be sequential and off-line. <p> Even if precedence relations are ignored, the problem is still NP-hard when tasks vary in 11 length and preemption is not allowed: it reduces to the bin-packing problem <ref> [7] </ref>. In the real world, such simplifications often do not apply: real programs contain precedence relations, communications networks are neither flat nor infinitely fast, and in order for thread management algorithms to be useful, they must be distributed and run in real time.
Reference: [8] <author> Robert H. Halstead. </author> <title> Multilisp: A Language for Concurrent Symbolic Computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: This causes threads to be executed in depth-first order locally, thereby minimizing memory consumption, while moving parallel threads around the machine in a more breadth-first fashion, thereby spreading work around the machine efficiently (as suggested in <ref> [8] </ref>). Furthermore, since every processor has its own thread queue, the depth-first order tended to cause threads created on a processor to remain on that processor, reducing demands on the communications network. 6.5 Applications The candidate thread managers described above were tested on a number of example applications.
Reference: [9] <author> Robert H. Halstead and Stephen A. Ward. </author> <title> The MuNet: A Scalable Decentralized Architecture for Parallel Computation. </title> <booktitle> In Proceedings of the Seventh International Symposium on Computer Architecture, </booktitle> <year> 1980. </year>
Reference-contexts: Furthermore, the "draft-age" parameter used to compare drafting candidates does not take the distance between the two processors into account. This lack of attention to communication distances further limits this algorithms effectiveness on large multiprocessors. 2.2.3 Hybrid Methods Diffusion Halstead and Ward proposed diffusion scheduling <ref> [9] </ref> as a means of propagating threads throughout the machine. Their description is a rather brief part of a larger picture and gives no specific details. <p> While RR-1 moves only one thread at a time, RR-2 moves half of the threads from producer processor to consumer processor. Diffusion-1: Diffusion scheduling is suggested in <ref> [9] </ref>. In both Diff-1 and Diff-2, every h cycles, a diffusion step is executed on each processor (the default value for h is 1000, which seems to give the best tradeoff between thread-manager overhead and effectiveness).
Reference: [10] <author> Kirk Johnson. </author> <title> The Impact of Communication Locality on Large-Scale Multiprocessor Performance. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <year> 1992. </year> <month> 214 </month>
Reference-contexts: Similar analyses can be performed for networks with richer communication structures. In our analyses, we ignore the effects that network contention may have on performance. 1 1 In <ref> [10] </ref>, it is shown that for a large class of machines, the effect of network contention on performance is no worse than the effect of network latency, within a constant factor.
Reference: [11] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory; Early Experience. </title> <booktitle> In Proceedings of Practice and Principles of Parallel Programming (PPoPP) 1993, </booktitle> <year> 1993. </year>
Reference-contexts: In many cases, the message overhead required to support the shared-memory abstraction is not needed for correct program behavior. In those cases, a message-passing system can achieve significantly better performance than a shared-memory system <ref> [11] </ref>. When, however, a large fraction of interprocessor communication is in the read, write, or read-modify-write form directly supported by a shared-memory system, then using a shared-memory system may yield better performance because shared-memory systems are usually optimized to support such accesses very efficiently. <p> For the algorithms employed by XTM, the gain can be as high as a factor of log p, where p is the number of processors in the system (see Chapter 5). The advantages of message-passing are not limited to asymptotics. Kranz and Johnson <ref> [11] </ref> show that the cost of certain primitive operations such as thread enqueueing and dequeueing can improve by factors of five or more when message-passing is used. If message-passing is so superior to shared-memory, why use shared-memory at all? This question is also addressed in [11]. <p> Kranz and Johnson <ref> [11] </ref> show that the cost of certain primitive operations such as thread enqueueing and dequeueing can improve by factors of five or more when message-passing is used. If message-passing is so superior to shared-memory, why use shared-memory at all? This question is also addressed in [11]. There are two answers to this question. First, certain types of algorithms are actually more efficient when run on shared-memory systems. Second, it seems that the shared-memory paradigm is easier for programmers to handle, in the same way that virtual-memory systems are easier for programmers than overlay systems.
Reference: [12] <author> David A. Kranz, R. Halstead, and E. Mohr. Mul-T: </author> <title> A High-Performance Parallel Lisp. </title> <booktitle> In Proceedings of SIGPLAN '89, Symposium on Programming Languages Design and Implementation, </booktitle> <year> 1989. </year>
Reference-contexts: These make up a subset of the datatypes provided by Mul-T <ref> [12] </ref>. 91 J-structures are arrays whose elements provide write-once semantics. A read oper-ation on a j-structure element is deferred until a write to that element takes place. A processor issuing such a read is suspended on a queue associated with the element.
Reference: [13] <author> Orly Kremien and Jeff Kramer. </author> <title> Methodical Analysis of Adaptive Load Sharing Algorithms. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 747-760, </pages> <month> Novem-ber </month> <year> 1992. </year>
Reference-contexts: Kremien and Kramer <ref> [13] </ref> state ...a flexible load sharing algorithm is required to be general, adaptable, stable, scalable, transparent to the application, fault tolerant and induce minimum overhead on the system... This chapter explores the work of other investigators in this area and evaluates that work with respect to Kremien and Kramer's requirements. <p> Characteristics of generalized scheduling strategies are discussed in [16], in which the scalability of various candidate load-sharing schemes is examined, and [30], which looks at the effects of processor clustering. In order to meet the criteria given in <ref> [13] </ref>, a thread management system must be dynamic and fully distributed. In this thesis, we are especially interested in dynamic methods because we want to be able to solve problems whose structure is not known at compile time. <p> In the rest of this thesis, we describe and evaluate a new thread management scheme that attempts to overcome all of these objections and tries to meet the requirements set forth by Kremien and Kramer <ref> [13] </ref>. 26 Chapter 3 High-Level System Design In this chapter, we present several high-level decisions that we made early on in the XTM design effort. These decisions were based mainly on the design principles given in Chapter 1: eliminate hot-spots, preserve communication locality and minimize system overhead.
Reference: [14] <author> Clyde P. Kruskal and Alan Weiss. </author> <title> Allocating Independent Subtasks on Parallel Processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(10):1001-1016, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: Thread creation is fully dynamic: a new thread may be created on any processor at any time. 16 themselves determine which thread runs on which processor. 2. Threads follow the Self-Scheduled Model <ref> [14] </ref>, as depicted in Figure 1-1. This means that the processors that execute application code also contend between themselves to decide on which processor the various threads that make up the application are run. There is no fundamental link between threads and processors: any thread can run on any processor. <p> The first widely-discussed dynamic Self-Scheduling schemes were not fully distributed; they employed a central queue, which posed no problem on small machines. 20 2.1 Centralized Self-Scheduling One of the first references to the Self-Scheduling model in the literature appeared in 1985 in <ref> [14] </ref>. In this paper, the authors described Self-Scheduling, although they never explicitly named it such: There are p processors, initially idle. At t = 0, they each take K subtasks from a job queue, each experiencing a delay h in that access. <p> necessary in order to perform effective thread management, how can we manage that information in such a way as to not put an unacceptable load on any processor or any part of the communications network? 29 3.1.2 Prospective Solutions As discussed in Section 1.3, we implement the global Self-Scheduled model <ref> [14, 21] </ref> by maintaining a queue of runnable threads on every processor. Therefore, the global information useful to a thread-management algorithm could potentially include the state of every one of these queues.
Reference: [15] <author> John Kubiatowicz. </author> <title> User's Manual for the A-1000 Communications and Memory Management Unit. Alewife Memo No. </title> <type> 19, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: We give results in terms of maximum latency and bandwidth requirements. 60 Unless otherwise stated, we assume that an efficient message-passing mechanism is em-ployed, i.e., a message can be sent from any processor in the machine to any other with no acknowledgment necessary <ref> [15] </ref>. When such a message represents an active entity moving about the system, then decisions about where it is to be sent next can be made without having to communicate with the processor from which the message first originated. <p> In this manner, each processor scans the machine in its own unique order. This order proceeds from near to far due to the mapping from PID to mesh location used by the Alewife processor <ref> [15] </ref>. In RR-1, when a non-empty queue is found, one thread is moved from that queue to the processor making the request. Round-Robin-2 - steal-half: As was the case for C-Ideal-1 and C-Ideal-2, the only difference between RR-1 and RR-2 concerns the number of threads that are moved.
Reference: [16] <author> Vipin Kumar, Grama Y. Ananth, and Vempaty Nageshwara Rao. </author> <title> Scalable Load Balancing Techniques for Parallel Computers. </title> <type> Technical Report 92-021, </type> <institution> Army High Performance Computing Research Center, University of Minnesota, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Znati, et. al. [31], give a taxonomy of load sharing algorithms, a modified version of which appears in Figure 2-1. Characteristics of generalized scheduling strategies are discussed in <ref> [16] </ref>, in which the scalability of various candidate load-sharing schemes is examined, and [30], which looks at the effects of processor clustering. In order to meet the criteria given in [13], a thread management system must be dynamic and fully distributed.
Reference: [17] <author> Leslie Lamport. </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9), </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: The two kinds of nodes in a shared-memory environment: processing nodes and memory nodes. Throughout this thesis, when we say shared-memory, we really mean the sequentially consistent shared-memory <ref> [17] </ref>. Communication takes place in the form of a request from a processor to a memory node, requiring an acknowledgment when the request has been satisfied. The actual low-level particulars of such a transaction depend on such machine-specific details as the existence of caches and the coherence model employed.
Reference: [18] <author> E. L. Lawler, J. K. Lenstra, A. H. G. Rinnooy Kan, and D. B. Shmoys, </author> <title> editors. The Traveling Salesman Problem. </title> <publisher> John Wiley and Sons, </publisher> <year> 1985. </year>
Reference-contexts: The particular function integrated is x 4 y 4 , over the square bounded by (0:0; 0:0) and (2:0; 2:0). Problem size is determined by the accuracy threshold: higher accuracy requires more work to achieve convergence. Traveling Salesman TSP <ref> [18] </ref> finds the shortest path between randomly placed cities on a two-dimensional surface. In this case, problem size is determined by the number of cities scanned. The search space is pruned using a simple parallel branch-and-bound scheme, where each new "best" path length is broadcast around the machine.
Reference: [19] <author> Frank C. H. Lin and Robert M. Keller. </author> <title> The Gradient Model Load Balancing Method. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):32-38, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: The XTM algorithm presented by this thesis can be thought of as the multigrid version of diffusion scheduling. We implemented both the simple diffusion scheduling algorithm 24 described here and XTM. Results are given in Chapter 7. Gradient Model The Gradient Model proposed in <ref> [19] </ref> has a communication structure similar to that of diffusion scheduling. Processors are defined to be in one of three states: lightly loaded, moderately loaded or heavily loaded.
Reference: [20] <author> Lionel M. Ni, Chong-Wei Xu, and Thomas B. Gendreau. </author> <title> A Distributed Drafting Algorithm for Load Balancing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(10):1153-1161, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: This statement implies that the machines this algorithm is intended for are of limited size (despite the use of "large-scale"), and that the grain size of the tasks is also rather large, minimizing the effect of scheduling overhead on overall performance. 23 2.2.2 Drafting Methods Ni, Xu and Gendreau <ref> [20] </ref> present a drafting-style load-sharing algorithm that also seems to be oriented towards a relatively small number of independent computers connected to a single Local Area Network.
Reference: [21] <author> Constantine D. Polychronopoulos and David J. Kuck. </author> <title> Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: efficiency, for a given problem run on a given number of processors, is defined to be the total number of cycles of useful work performed divided by the total running time (in cycles) times the number 21 at least 1 1+ 2 In 1987, Polychronopoulos and Kuck introduced Guided Self-Scheduling <ref> [21] </ref>. Their scheme, which is oriented towards the efficient scheduling of parallel FORTRAN loops, varies K with time. More specifically, whenever a processor becomes idle, it takes d r p e jobs from the central queue, where r is the number of jobs in the queue at the time. <p> necessary in order to perform effective thread management, how can we manage that information in such a way as to not put an unacceptable load on any processor or any part of the communications network? 29 3.1.2 Prospective Solutions As discussed in Section 1.3, we implement the global Self-Scheduled model <ref> [14, 21] </ref> by maintaining a queue of runnable threads on every processor. Therefore, the global information useful to a thread-management algorithm could potentially include the state of every one of these queues.
Reference: [22] <author> William H. Press, Brian P. Flannery, Saul A Teukolsky, and William T. Vetterling. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: We first consider applications that are relatively fine-grained. Their task graphs are tree-structured; virtually all communication takes place directly between parents and children in the task tree: Numerical Integration AQ makes use of an Adaptive Quadrature algorithm for integrating a function of two variables. This algorithm, given in <ref> [22] </ref>, has a task tree whose shape is determined by the function being integrated. The particular function integrated is x 4 y 4 , over the square bounded by (0:0; 0:0) and (2:0; 2:0). Problem size is determined by the accuracy threshold: higher accuracy requires more work to achieve convergence.
Reference: [23] <author> J. Rees and N. Adams. </author> <title> T: A Dialect of LISP. </title> <booktitle> In Proceedings of Symposium on Lisp and Functional Programming, </booktitle> <year> 1982. </year>
Reference-contexts: The entire system is written in T <ref> [23] </ref>, a dialect of LISP. The multithreader diagrammed in Figure 6-3 supports multiple independent threads of computation executing in a common namespace. Each thread is a T program with its own execution stack.
Reference: [24] <author> Larry Rudolph, Miriam Slivkin-Allalouf, and Eli Upfal. </author> <title> A Simple Load Balancing Scheme for Task Allocation in Parallel Machines. </title> <booktitle> In Proceedings of the Third Annual Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <year> 1991. </year> <month> 215 </month>
Reference-contexts: Random Scheduling Rudolph, et. al. <ref> [24] </ref>, propose a load balancing scheme in which processors periodically balance their workloads with randomly selected partners.
Reference: [25] <author> C. H. Sequin, A. M. Despain, and D. A. Patterson. </author> <title> Communication in X-TREE: A Modular Multiprocessor System. </title> <booktitle> In Proceedings of the 1978 Annual Conference of the Association for Computing Machinery, </booktitle> <year> 1978. </year>
Reference-contexts: If such programs are to be run in an efficient manner on large-scale multiprocessors, then efficient run-time thread placement and scheduling techniques are needed. This thesis examines the problems faced by an on-line thread-management system and presents XTM, an X-Tree-based <ref> [25, 6] </ref> Thread-Management system that attempts to overcome these problems. The general thread-management problem is NP-hard [7]. <p> Guided by the design principles given above, we have developed XTM, a thread management system that is sound from both a theoretical and a practical perspective. XTM solves the sub-problems identified above as follows: 1. Global information is collected and disseminated using an X-Tree <ref> [25, 6] </ref> data structure embedded in the communications network (see Figures 4-1 and 4-2). Each node in the tree contains a "presence bit" whose value indicates whether there are any runnable threads in the sub-tree headed by that node. <p> Stated briefly, the high-level solutions to each of those sub-problems are: 1. Global information is collected and disseminated using an X-Tree <ref> [25, 6] </ref> data structure embedded in the communications network. Each node in the tree contains a "presence bit" whose value indicates whether there are any runnable threads in the sub-tree headed by that node. 27 2. <p> This loss of locality can be alleviated by adding connections in the tree between nodes that are physically near each other. Such a tree, called an X-Tree, is the basic data structure upon which XTM's algorithms are based. Despain, et. al. <ref> [25, 6] </ref>, first introduced the X-Tree data structure as a communications network topology. The particular variant of X-Tree we use is a full-ring X-Tree without end-around connections. The rest of this section discusses the need for global information in solving the dynamic thread management problem. <p> Locality lost in this manner can be regained by adding connections in the tree between nodes that are physically near each other. A tree enhanced with such links, introduced in <ref> [25] </ref> and [6] as a full-ring X-Tree without end-around connections, is the basic data structure upon which XTM's algorithms are based (see Figures 4-1 and 4-2). In Chapter 5, we show that the nearest-neighbor links are needed to get good theoretical behavior. <p> In the next chapter, we show how we assembled these decisions to produce a detailed design of a high-performance thread-management system. 35 Chapter 4 X-Tree-Based Thread Management This thesis presents a thread distribution system based on an X-Tree-driven search. An X-Tree <ref> [25, 6] </ref> is a tree augmented with links between same-level nodes. The particular variant we use contains near-neighbor links between touching same-level nodes. In the parlance used in [25] and [6], this is a full-ring X-Tree without end-around connections (see Figures 4-1 and 4-2). <p> An X-Tree [25, 6] is a tree augmented with links between same-level nodes. The particular variant we use contains near-neighbor links between touching same-level nodes. In the parlance used in <ref> [25] </ref> and [6], this is a full-ring X-Tree without end-around connections (see Figures 4-1 and 4-2). In this chapter, we describe in detail the algorithms that go into XTM, an X-Tree-based Thread Manager.
Reference: [26] <author> H. Sullivan and T. R. Bashkow. </author> <title> A Large Scale, Homogeneous, Fully Distributed Parallel Machine. </title> <booktitle> In Proceedings of the Fourth Annual Symposium on Computer Architecture, </booktitle> <year> 1977. </year>
Reference-contexts: This is true, for example, for the e-cube routing scheme <ref> [26] </ref>.
Reference: [27] <author> Stephen A. Ward and Jr. Robert H. Halstead. </author> <title> Computation Structures. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Since the data is allocated in a static fashion, we say that the application demonstrates static locality. Halstead and Ward <ref> [27] </ref> define locality of reference as follows: Reference to location X at time t implies that the probability of access to loca tion X + X at time t + t increases as X and t approach zero.
Reference: [28] <author> Min-You Wu and Wei Shu. </author> <title> Scatter Scheduling for Problems with Unpredictable Structures. </title> <booktitle> In Sixth Distributed Memory Conference Proceedings, </booktitle> <year> 1991. </year>
Reference-contexts: In hybrid methods, producers and consumers cooperate in the load sharing process. In the rest of this section, we give examples that have appeared in the literature for each of these categories of load-sharing algorithms. 2.2.1 Bidding Methods Wu and Shu <ref> [28] </ref> present a Scatter Scheduling algorithm that is essentially the producer-oriented dual of the round-robin drafting algorithm described in Chapter 6. <p> Assuming that the application is suitably partitioned, this algorithm should get a relatively even load balance on machines of small and moderate size. This expectation is borne out in the results presented in <ref> [28] </ref>. However, since producer-processors send threads to essentially arbitrary destinations over time, any aspect of locality concerning data shared between related threads is lost.
Reference: [29] <author> Pen-Chung Yew, Nian-Feng Tzeng, and Duncan H. Lawrie. </author> <title> Distributing Hot-Spot Addressing in Large-Scale Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):388-395, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Some way to distill this global information is needed, such that the cost of collecting and disseminating the distilled information is acceptable, while keeping around enough information to make good thread-management decisions. Software combining <ref> [29] </ref> presents itself as the obvious way to keep information collection and dissemination costs manageable. When combining techniques are employed, the load on the communications network can be held to an acceptable level.
Reference: [30] <author> Songnian Zhou and Timothy Brecht. </author> <title> Processor Pool-Based Scheduling for Large-Scale NUMA Multiprocessors. </title> <booktitle> In Proceedings of the 1991 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1991. </year>
Reference-contexts: Znati, et. al. [31], give a taxonomy of load sharing algorithms, a modified version of which appears in Figure 2-1. Characteristics of generalized scheduling strategies are discussed in [16], in which the scalability of various candidate load-sharing schemes is examined, and <ref> [30] </ref>, which looks at the effects of processor clustering. In order to meet the criteria given in [13], a thread management system must be dynamic and fully distributed.
Reference: [31] <author> Taieb F. Znati, Rami G. Melhem, and Kirk R. Pruhs. </author> <title> Dilation Based Bidding Schemes for Dynamic Load Balancing on Distributed Processing Systems. </title> <booktitle> In Sixth Distributed Memory Conference Proceedings, </booktitle> <year> 1991. </year> <month> 216 </month>
Reference-contexts: This chapter explores the work of other investigators in this area and evaluates that work with respect to Kremien and Kramer's requirements. Znati, et. al. <ref> [31] </ref>, give a taxonomy of load sharing algorithms, a modified version of which appears in Figure 2-1. Characteristics of generalized scheduling strategies are discussed in [16], in which the scalability of various candidate load-sharing schemes is examined, and [30], which looks at the effects of processor clustering. <p> These assumptions limit this work to be only applicable to small machines. 2.2 Fully Distributed On-Line Algorithms We define fully distributed algorithms to be algorithms that contain no single (or small number of) serialization points on an arbitrarily large multiprocessor. Znati, et. al. <ref> [31] </ref>, divide such algorithms into three sub-categories: bidding methods, drafting methods and hybrid methods. In bidding or producer-oriented methods, the creators of new work push the work off onto processors with lighter loads. <p> Furthermore the cost of thread creation goes up with the diameter of the machine, so on large machines, one would expect this algorithm to behave rather poorly. Znati, et. al. <ref> [31] </ref>, present a bidding scheme that takes distance between sender and receiver into account. When a new thread is created on a given processor, the processor recomputes and broadcasts its load to the rest of the system. <p> In this scheme, each processor has an idea of the state of all other processors by maintaining a "load table," which has an entry for every other processor in the system. Processors can be in one of three states, light load, normal load or heavy load. As in <ref> [31] </ref>, every processor in the system is informed of all changes in load on every other processor by means of broadcasts. <p> P i then determines which candidate will yield the highest benefit and gets work from that processor. The same objections that applied to the scheme proposed in <ref> [31] </ref> apply here: such an algorithm isn't really scalable. Furthermore, the "draft-age" parameter used to compare drafting candidates does not take the distance between the two processors into account.
References-found: 31

