URL: ftp://ftp.cs.utexas.edu/pub/garbage/allocsrv.ps
Refering-URL: http://www.cs.utexas.edu/users/oops/papers.html
Root-URL: 
Email: (wilson|markj|neely@cs.utexas.edu)  
Title: Dynamic Storage Allocation: A Survey and Critical Review analyses and empirical allocator evaluations to date
Author: Paul R. Wilson, Mark S. Johnstone, Michael Neely, and David Boles 
Note: Most theoretical  that must be exploited if allocators are to perform well in practice.  
Address: Austin, Texas, 78751, USA  
Affiliation: Department of Computer Sciences University of Texas at Austin  
Abstract: Dynamic memory allocation has been a fundamental part of most computer systems since roughly 1960, and memory allocation is widely considered to be either a solved problem or an insoluble one. In this survey, we describe a variety of memory allocator designs and point out issues relevant to their design and evaluation. We then chronologically survey most of the literature on allocators between 1961 and 1995. (Scores of papers are discussed, in varying detail, and over 150 references are given.) We argue that allocator designs have been unduly restricted by an emphasis on mechanism, rather than policy, while the latter is more important; higher-level strategic issues are still more important, but have not been given much attention. ? A slightly different version of this paper appears in Proc. 1995 Int'l. Workshop on Memory Management, Kinross, Scotland, UK, September 27-29, 1995, Springer Verlag LNCS. This version differs in several very minor respects, mainly in formatting, correction of several typographical and editing errors, clarification of a few sentences, and addition of a few footnotes and citations. ?? This work was supported by the National Science Foundation under grant CCR-9410026, and by a gift from Novell, Inc. ??? Convex Computer Corporation, Dallas, Texas, USA. (dboles@zeppelin.convex.com) 1 Introduction 
Abstract-found: 1
Intro-found: 1
Reference: [Abr67] <author> John Abramowich. </author> <title> Storage allocation in a certain iterative process. </title> <journal> Communications of the ACM, </journal> <volume> 10(6) </volume> <pages> 368-370, </pages> <month> June </month> <year> 1967. </year>
Reference-contexts: See [WJNB95].) 16 (Other patterns of overall memory usage also occur, but appear less common. As we describe in Section 4, backward ramp functions have been observed [GM85]. Combined forward and backward ramp behavior has also been observed, with one data structure shrinking as another grows <ref> [Abr67] </ref>.) Notice that in the case of ramps and ramp-shaped peaks, looking at the statistical distributions of object lifetimes may be very misleading.
Reference: [AF94] <author> G. Attardi and T. Flagella. </author> <title> A customizable memory management framework. </title> <booktitle> In Proceedings of the USENIX C++ Conference, </booktitle> <address> Cam-bridge, Massachussetts, </address> <year> 1994. </year>
Reference-contexts: real traces, preserving the size and lifetime distributions much more accurately than most synthetic trace generation schemes do.) We found that there was a significant correlation between the results from real traces and those from shu*ed traces, but there were major and systematic 133 See also Delacour's [Del92] and Attardi's <ref> [AF94] </ref> sophisticated systems for low-level storage management in (mostly) garbage-collected systems using mixed lan guages and implementation strategies. errors as well. In an initial test of eight varied alloca-tors, the correlations accounted for only about a third of the observed variation in performance.
Reference: [AS95] <author> Sedat Akyurek and Kenneth Salem. </author> <title> Adaptive block rearrangement. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(2) </volume> <pages> 95-121, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: note that for secondary and tertiary storage more generally, contiguous storage is not strictly required; freedom from this restriction allows schemes that are much more flexible and less vulnerable to fragmentation. (Many systems divide all files into blocks of one or two fixed sizes, and only preserve logical contiguity (e.g., <ref> [RO91, VC90, SKW92, CG91, AS95] </ref>).
Reference: [Bae73] <author> H. D. Baecker. </author> <title> Aspects of reference locality in list structures in virtual memory. </title> <journal> Software Practice and Experience, </journal> <volume> 3(3) </volume> <pages> 245-254, </pages> <year> 1973. </year>
Reference-contexts: The literature on garbage collection is considerably more sophisticated in terms of locality studies than the literature on memory allocation, and should not be overlooked. (See, e.g., <ref> [Bae73, KLS92, Wil90, WLM92, DTM93, Rei94, GA95, Wil95] </ref>.) Many of the same issues must arise in conventionally-managed heaps as well. 26 larger free blocks (coalescing). Equally important are the policy and strategy implications|i.e., whether the allocator properly exploits the regularities in real request streams. <p> Having information about the block stored with the block makes many common operations fast. 50 Briefly, we believe that the allocator should heuristically attempt to cluster objects that are likely to be used at about the same times and in similar ways. This should improve locality <ref> [Bae73, WLM91] </ref>; it should also increase the chances that adjacent objects will die at about the same time, reducing fragmentation. 27 Header fields are usually one machine word; on most modern machines, that is four 8-bit bytes, or 32 bits. (For convenience, we will assume that the word size is 32
Reference: [Bak93] <author> Henry G. Baker. </author> <title> Infant mortality and generational garbage collection. </title> <journal> SIGPLAN Notices, </journal> <volume> 28(4) </volume> <pages> 55-57, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: A note on Markov models. Many probabilistic studies of memory allocation have used first-order 26 We are indebted to Henry Baker, who has made quite similar observations with respect to the use of exponential lifetime distributions to estimate the effectiveness of generational garbage collection schemes <ref> [Bak93] </ref>. 27 In particular, certain effects of randomized traces may (or may not) resemble the cumulative effect of allocator strategy errors over much longer periods.
Reference: [BAO85] <author> B. M. Bigler, S. J. Allan, and R. R. Oldehoeft. </author> <title> Parallel dynamic storage allocation. </title> <booktitle> In 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 272-275, </pages> <year> 1985. </year>
Reference-contexts: To our knowledge, this is by far the most thorough review to date, but it should not be considered detailed or exhaustive; valuable points or papers may have escaped our notice. 95 We have left out work on concurrent and parallel al-locators (e.g., <ref> [GW82, Sto82, BAO85, MK88, EO88, For88, Joh91, JS92, JS92, MS93, Iye93] </ref>), which are beyond the scope of this paper. We have also neglected mainly analytical work (e.g., [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85]) to some degree, be 94 This is not quite necessarily true.
Reference: [Bat76] <author> Alan Batson. </author> <title> Program behavior at the symbolic level. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 21-26, </pages> <month> November </month> <year> 1976. </year>
Reference-contexts: We simply have no theory of program behavior, much less a theory of how allocators exploit that behavior. (Batson made similar comments in 1976, in a slightly different context <ref> [Bat76] </ref>, but after nearly two decades the situation is much the same.) Aside from several useful studies of worst-case performance, most of the analytical work to date seems to be based on several assumptions that turn out to be incorrect, and the results cannot be expected to apply directly to the
Reference: [Bay77] <author> C. Bays. </author> <title> A comparison of next-fit, first-fit and best-fit. </title> <journal> Communications of the ACM, </journal> <volume> 20(3) </volume> <pages> 191-192, </pages> <month> March </month> <year> 1977. </year> <month> 72 </month>
Reference-contexts: Worse, it may affect the locality of the program it allocates for, by scattering objects used by certain phases and intermingling them with objects used by other phases.) In several experiments using both real traces [WJNB95] and synthetic traces (e.g., <ref> [Bay77, Wei76, Pag84, KV85] </ref>), next fit has been shown to cause more other indexing structure. 60 This has also been observed by Ivor Page [Pag82] in randomized simulations, and similar (but possibly weaker) observations were made by Knuth and Shore and others in the late 1960's and 1970's. (Section 4.) 31 <p> Weinstock made the important point that seemingly minor variations in algorithms could have a significant effect on performance; he therefore took great care in the describing of the algorithms he used, and some of the algorithms used in earlier studies. In a brief technical communication, Bays <ref> [Bay77] </ref> replicated some of Shore's results comparing first fit and best fit, and showed that next fit was distinctly inferior when average block sizes were small.
Reference: [BB77] <author> A. P. Batson and R. E. Brundage. </author> <title> Segment sizes and lifetimes in ALGOL 60 programs. </title> <journal> Communications of the ACM, </journal> <volume> 20(1) </volume> <pages> 36-44, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: This effectively smooths the results, making it unclear what the actual distribution is, e.g., whether it is spiky. The general shape (after smoothing) has a rounded peak for the smaller sizes, and is roughly exponential after that. (In a followup study <ref> [BB77] </ref>, described later, Batson and Brundage would find spikes.) A note about Algol-60 is in order here. <p> I saw you. It wasn't fair!" "Be a good sport, Tessie," Mrs Delacroix called, and Mrs. Graves said, "All of us took the same chance." |Shirley Jackson, "The Lottery" Batson and Brundage <ref> [BB77] </ref> reported segment sizes and lifetimes in 34 varied Algol-60 programs. <p> One of the real size-and-lifetime distributions came from the Bliss/11 compiler [WJW + 75], and the other was from Batson and Brundage's measurements of the University of Virginia B5500 system <ref> [BB77] </ref>, described above.
Reference: [BBDT84] <author> G. Bozman, W. Buco, T. P. Daly, and W. H. Tetzlaff. </author> <title> Analysis of free storage algorithms| revisited. </title> <journal> IBM Systems Journal, </journal> <volume> 23(1) </volume> <pages> 44-64, </pages> <year> 1984. </year>
Reference-contexts: This may be a response to the then-unpublished experiments in <ref> [BBDT84] </ref>, but no details are given.) Kaufman [Kau84] presented two buddy system al-locators using deferred coalescing. <p> These results are suspect, however, due to the load-smoothing effects of random traces, which flatter small caches of free blocks (Section 3.11). 123 Bozman et al. <ref> [BBDT84] </ref> studied a wide variety of allocators, including sequential fits, deferred coalescing schemes, buddy systems, and Stephenson's Cartesian tree system. (Not all allocators were compared directly to each other, because some were tailored to an IBM operating system and others were not.) They used synthetic traces based on real lifetime distributions,
Reference: [BC79] <author> Daniel G. Bobrow and Douglas W. Clark. </author> <title> Compact encodings of list structure. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1(2) </volume> <pages> 266-286, </pages> <month> October </month> <year> 1979. </year>
Reference: [BC92] <editor> Yves Bekkers and Jacques Cohen, editors. </editor> <booktitle> International Workshop on Memory Management, number 637 in Lecture Notes in Computer Science, </booktitle> <address> St. Malo, France, </address> <month> September </month> <year> 1992. </year> <note> Springer-Verlag. </note>
Reference: [BCW85] <author> B. S. Baker, E. G. Coffman, Jr., and D. E. Willard. </author> <title> Algorithms for resolving conflicts in dynamic storage allocation. </title> <journal> Journal of the ACM, </journal> <volume> 32(2) </volume> <pages> 327-343, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: We have also neglected mainly analytical work (e.g., <ref> [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85] </ref>) to some degree, be 94 This is not quite necessarily true. For applications that do little freeing, the initial carving of memory requested from the operating system will be a significant fraction of the allocation cost.
Reference: [BDS91] <author> Hans-J. Boehm, Alan J. Demers, and Scott Shenker. </author> <title> Mostly parallel garbage collection. </title> <booktitle> In Proceedings of the 1991 SIGPLAN Conference on Programming Language Design and Implementation [PLD91], </booktitle> <pages> pages 157-164. </pages>
Reference-contexts: In that case, the bitmap will include one bit for each double-word alignment boundary.) To our knowledge, bitmapped allocation has never been used in a conventional allocator, but it is quite common in other contexts, particularly mark-sweep garbage collectors (notably the conservative collectors of Boehm, et al. from Xerox PARC <ref> [BW88, BDS91, DWH + 90] </ref> 84 ) and file systems' disk block managers. We suspect that the main reason it has not been used for conventional memory allocation is that it is perceived as too slow.
Reference: [Bec82] <author> Leland L. Beck. </author> <title> A dynamic storage allocation technique based on memory residence time. </title> <journal> Communications of the ACM, </journal> <volume> 25(10) </volume> <pages> 714-724, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: Also, if the future requests are for 100 blocks of size 10 and 200 blocks of size 20, whether it's a problem may depend on the order in which the requests arrive and the allocator's moment 30 Beck <ref> [Bec82] </ref> makes the only clear statement of this principle which we have found in our exhausting review of the literature. <p> Beck <ref> [Bec82] </ref>, Demers et al. [DWH + 90], and and Barrett and Zorn [BZ93] have developed systems that predict the lifetimes of objects for similar purposes. We note that for our purposes, it is not necessary to predict which groups of objects will die when. <p> We are very far away from this deep understanding at present. Beck <ref> [Bec82] </ref> described the basic issue of fragmentation clearly, and designed two interesting classes of allocators, one idealized and one implementable.
Reference: [Ben81] <author> V. E. </author> <title> Benes. Models and problems of dynamic storage allocation. In Applied Probability and Computer Science|the Interface. </title> <institution> Institute of Management Science and Operations Research Society of America, </institution> <month> January </month> <year> 1981. </year>
Reference-contexts: However, earlier results suggest that small grain sizes are preferred.) He suggests several techniques to make it easier to use somewhat larger models, but had little success with the few he tried. (See also <ref> [Ben81, Ree82, McI82] </ref>.) We are not optimistic that this approach is useful for realistic memory sizes, especially since memory sizes tend to increase rapidly over time. <p> Even with these extremely strong assumptions of randomness, this problem is combinatorially explosive. (This is true even when various symmetries and rotations are exploited to combine (exactly) equiva lent states <ref> [Ben81, McI82] </ref>.) We believe that the only way to make this kind of problem remotely tractable is with powerful abstractions over the possible states of memory.
Reference: [Bet73] <author> Terry Betteridge. </author> <title> An analytical storage allocation model. </title> <journal> Acta Informatica, </journal> <volume> 3 </volume> <pages> 101-122, </pages> <year> 1973. </year>
Reference-contexts: We have also neglected mainly analytical work (e.g., <ref> [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85] </ref>) to some degree, be 94 This is not quite necessarily true. For applications that do little freeing, the initial carving of memory requested from the operating system will be a significant fraction of the allocation cost.
Reference: [Bet82] <author> Terry Betteridge. </author> <title> An Algebraic Analysis of Storage Fragmentation. </title> <publisher> UMI Research Press, </publisher> <address> Ann Arbor, Michigan, </address> <year> 1982. </year>
Reference-contexts: Shore would later show that Knuth's simplifying assumptions about the lack of systematicity in the allocator's placement were also unwarranted. 105 Betteridge <ref> [Bet82] </ref> provides a some 103 One consisted of the six powers of two from 1 to 32, chosen with probability inversely proportional to size, and the other consisted of 22 sizes from 10 to 4000, chosen with equal probability. <p> of the term [Wil95], but it is not uncommon in early papers on allocators. 121 Activation records were apparently allocated on the general heap; presumably this was used to support closures with indefinite extent (i.e., "block retention"), and/or "thunks" (hidden parameterless subroutines) for call by-name parameter passing [Ing61]. 58 Betteridge <ref> [Bet82] </ref> attempted to compute frag-mentation probabilities for different allocators using first-order Markov modeling. (This book is apparently Betteridge's dissertation, completed in 1979.) The basic idea is to model all possible states of memory occupancy (i.e., all arrangements of allocated and free blocks), and the transition probabilities between those states.
Reference: [BJW70] <author> A. P. Batson, S. M. Ju, and D. C. Wood. </author> <title> Measurements of segment size. </title> <journal> Communications of the ACM, </journal> <volume> 13(3) </volume> <pages> 155-159, </pages> <month> March </month> <year> 1970. </year>
Reference-contexts: It is common for block sizes in many modern systems to average on the order of 10 words, give or take a factor of two or so, so a single word per header may increase memory usage by about 10% <ref> [BJW70, Ung86, ZG92, DDZ93, WJNB95] </ref>. Boundary tags. Many allocators that support general coalescing are implemented using boundary tags (due to Knuth [Knu73]) to support the coalescing of free areas. <p> Their model was based on strong assumptions of independence and randomness in the workload, including exponentially distributed random lifetimes. Batson, Ju and Wood <ref> [BJW70] </ref> reported segment size and lifetime distributions in the Univer 50 sity of Virginia B5500 system. Most segments were "small"|about 60 percent of the segments in use were 40 (48-bit) words or less in length.
Reference: [BL92] <author> Ball and Larus. </author> <title> Optimal profiling and tracing of programs. </title> <booktitle> In Conference Record of the Nineteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 59-70. </pages> <publisher> ACM Press, </publisher> <month> January </month> <year> 1992. </year>
Reference-contexts: This simulated program doesn't actually do anything with the allocated blocks, as a real program would, but it imitates the real program's request sequences exactly, which is sufficient for measuring the memory usage. Modern profiling tools <ref> [BL92, CK93] </ref> can also be used with the simulation program to determine how many instruction cycles are spent in the allocator itself. An alternative strategy is to actually link the program with a variety of allocators, and actually re-run the program for each "simulation". <p> Several tools are available to make it relatively easy to gather memory-reference traces, and several cache and virtual memory simulators are available for processing these traces. Larus' QPT tool (a successor to the earlier AE system <ref> [BL92] </ref>) modifies an executable program to make it self-tracing. The Shade tool from SunLabs [CK93] is essentially a CPU emulator, which runs a program in emulation and records various kinds of events in an extremely flexible way. <p> The other simplified quick fit allocator is uses the G++ segregated fits system as its general allocator, and uses quick lists for each size, rounded to the nearest word, up to 8 words (32 bytes). Using Larus' QP tracing tool <ref> [BL92] </ref>, Zorn et al. traced five C programs combined with their five al-locators, and ran the traces through virtual memory and cache simulators.
Reference: [Boz84] <author> Gerald Bozman. </author> <title> The software lookaside buffer reduces search overhead with linked lists. </title> <journal> Communications of the ACM, </journal> <volume> 27(3) </volume> <pages> 222-227, </pages> <month> March </month> <year> 1984. </year>
Reference-contexts: theory; such an omniscient, malevolent opponent is commonly called a "devil" or "evil demon.") Knuth surveyed memory allocation techniques in Volume One of The Art of Computer Programming 101 Comparable schemes were apparently used in other early systems, including one that was integrated with overlaying in the IBM PL/I compiler <ref> [Boz84] </ref>. 102 We do not have a copy of this report at this writing. Our information comes from secondary sources. 48 ([Knu73], first edition 1968), which has been a stan-dard text and reference ever since. <p> highly predictive. (We believe that this conclusion is difficult to support with what amount to two data points, especially since their validation was primarily relevant to variations on a single optimized design, not the wide variety of basic allocators they experimented with using synthetic traces.) In a related paper, Bozman <ref> [Boz84] </ref> described a general "software lookaside buffer" technique for caching search results in data structures. One of his three applications (and empirical evaluations) was for deferred coalescing with best fit and address-ordered first fit allocators.
Reference: [BR64] <author> Daniel G. Bobrow and Bertram Raphael. </author> <title> A comparison of list-processing computer languages. </title> <journal> Communications of the ACM, </journal> <volume> 7(4) </volume> <pages> 231-240, </pages> <month> April </month> <year> 1964. </year>
Reference-contexts: of logical (program and data) segments to physical memory. 97 By the mid-1960's, the problem of managing storage for different-sized objects within the address space of a single process was also recognized as an important one, largely due to the increasing use (and sophistication) of list processing techniques and languages <ref> [Ros61, Com64, BR64] </ref>. 98 Equally important, the 1960's saw the invention of the now-traditional methodology for allocator evaluation. In early papers, the assumptions underlying this scheme were explicit and warned against, but as the decade progressed, the warnings decreased in frequency and seriousness.
Reference: [Bre89] <author> R. Brent. </author> <title> Efficient implementation of the first-fit strategy for dynamic storage allocation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <month> July </month> <year> 1989. </year>
Reference-contexts: Using the standard simulation techniques, and only uniformly distributed sizes and lifetimes, they show that double buddies suffer from somewhat less fragmentation than binary and weighted buddies. They also present an analysis that explains this result. 128 Brent <ref> [Bre89] </ref> presented a scalable algorithm for the address-ordered first fit policy, using a "heap," data structure|i.e., a partially-ordered tree, not to 127 Since memory usage is dominated by a single size, almost all requests can be satisfied by almost any free block; 128 While we believe that double buddies are indeed
Reference: [Bro80] <author> A. G. Bromley. </author> <title> Memory fragmentation in buddy methods for dynamic storage allocation. </title> <journal> Acta Informatica, </journal> <volume> 14(2) </volume> <pages> 107-117, </pages> <month> August </month> <year> 1980. </year>
Reference-contexts: For the plain Fibonacci system, the error was significant (29% predicted, 22% observed). For binary 117 See also Bromley <ref> [Bro80] </ref>. buddy the error was rather large (44% predicted, 30% observed). Russell notes that the CP-67 data do not closely resemble a Zipf distribution, and for this distribution the fragmentation using conventional Fibonacci is in fact lower (at 15%) than his estimated lower bound (24%).
Reference: [Bur76] <author> Warren Burton. </author> <title> A buddy system variation for disk storage allocation. </title> <journal> Communications of the ACM, </journal> <volume> 19(7) </volume> <pages> 416-417, </pages> <month> July </month> <year> 1976. </year>
Reference-contexts: As with binary buddies, the increasing size of successive size ranges limits the number of free lists required. A further refinement, called generalized Fibonacci buddies <ref> [Hir73, Bur76, PN77] </ref> uses a Fibonacci-like number series that starts with a larger number and generates a somewhat more closely-spaced set of sizes. <p> A hybrid strategy might use poor fits, but preserve some larger areas as well. tention than his thorough (and influential) experimentation within the random trace paradigm. Burton introduced a generalization of the Fibo-nacci buddy system <ref> [Bur76] </ref> which is more general than Hirschberg's.
Reference: [BW88] <author> Hans-Juergen Boehm and Mark Weiser. </author> <title> Garbage collection in an uncooperative environment. </title> <journal> Software Practice and Experience, </journal> <volume> 18(9) </volume> <pages> 807-820, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: with a larger constant factor.) In terms of policy, this search order means that smaller blocks are used in preference to larger ones, 74 This invariant can be useful in some kinds of systems, especially systems that provide persistence [SKW92] and/or garbage collection for languages such as C or C++ <ref> [BW88, WDH89, WJ93] </ref>, where pointers may point into the interior parts of objects, and it is important to be able to find the object headers quickly. <p> In that case, the bitmap will include one bit for each double-word alignment boundary.) To our knowledge, bitmapped allocation has never been used in a conventional allocator, but it is quite common in other contexts, particularly mark-sweep garbage collectors (notably the conservative collectors of Boehm, et al. from Xerox PARC <ref> [BW88, BDS91, DWH + 90] </ref> 84 ) and file systems' disk block managers. We suspect that the main reason it has not been used for conventional memory allocation is that it is perceived as too slow.
Reference: [BZ93] <author> David A. Barrett and Bejamin G. Zorn. </author> <title> Using lifetime predictors to improve memory allocation performance. </title> <booktitle> In Proceedings of the 1993 SIGPLAN Conference on Programming Language Design and Implementation [PLD93], </booktitle> <pages> pages 187-196. </pages>
Reference-contexts: likely to be related to type and purpose, so avoiding the intermingling of different sizes (and likely types) of objects may reduce the scattering of long-lived objects among short-lived ones. 35 Barrett and Zorn have recently built an allocator using profile information to heuristically separate long-lived objects from short-lived ones <ref> [BZ93] </ref>. (Section 4.2.) This suggests that objects allocated at about the same time should be allocated adjacent to each other in memory, with the possible amendment that different-sized objects should be segregated [WJNB95]. 36 Implications for strategy. <p> Beck [Bec82], Demers et al. [DWH + 90], and and Barrett and Zorn <ref> [BZ93] </ref> have developed systems that predict the lifetimes of objects for similar purposes. We note that for our purposes, it is not necessary to predict which groups of objects will die when. <p> Effects on larger-scale locality are less clear. Barrett and Zorn <ref> [BZ93] </ref> present a very interesting scheme for avoiding fragmentation by heuristically segregating short-lived objects from other ob jects. Their "lifetime prediction" allocator uses o*ine profile information from "training" runs on sample data to predict which call sites will allocate short-lived objects.
Reference: [BZ95] <author> David A. Barrett and Benjamin G. Zorn. </author> <title> Garbage collection using a dynamic threatening boundary. </title> <booktitle> In Proceedings of the 1995 SIG-PLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 301-314, </pages> <address> La Jolla, California, June 1995. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Many programs build up data structures quickly, and then use those data structures for long periods (often nearly the whole running time of the program). These patterns are well-known, from anecdotal experience by many people (e.g., [Ros67, Han90]), from research on garbage collection (e.g., <ref> [Whi80, WM89, UJ88, Hay91, Hay93, BZ95, Wil95] </ref>), 32 and from a re cent study of C and C++ programs [WJNB95]. 32 It may be thought that garbage collected systems are sufficiently different from those using conventional storage management that these results are not relevant.
Reference: [Cam71] <author> J. A. Campbell. </author> <title> A note on an optimal-fit method for dynamic allocation of storage. </title> <journal> Computer Journal, </journal> <volume> 14(1) </volume> <pages> 7-9, </pages> <month> February </month> <year> 1971. </year>
Reference-contexts: The general idea may have some merit, however, as part of a combination of strategies. Another policy is so-called "optimal fit," where a limited search of the list is usually used to "sample" the list, and a further search finds a fit that is as good or better <ref> [Cam71] </ref>. 63 Another policy is "half fit" [FP74], where the allocator preferentially splits blocks twice the requested size, in hopes that the remainder will come in handy if a similar request occurs soon. <p> Campbell introduced an "optimal fit" policy, which is a variant of next fit intended to improve the chances of a good fit without too much cost in extra searching <ref> [Cam71] </ref>. (It is not optimal in any useful sense.) The basic idea is that the allocator looks forward through the linear list for a bounded number of links, recording the best fit found.
Reference: [CG91] <author> Vincent Cate and Thomas Gross. </author> <title> Combining the concepts of compression and caching for a two-level file system. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 200-209, </pages> <address> Santa Clara, California, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: note that for secondary and tertiary storage more generally, contiguous storage is not strictly required; freedom from this restriction allows schemes that are much more flexible and less vulnerable to fragmentation. (Many systems divide all files into blocks of one or two fixed sizes, and only preserve logical contiguity (e.g., <ref> [RO91, VC90, SKW92, CG91, AS95] </ref>).
Reference: [CK93] <author> Robert Cmelik and David Keppel. Shade: </author> <title> A fast instruction-set simulator for execution profiling. </title> <type> Technical Report UWCSE 93-06-06, </type> <institution> Dept. of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, Washing-ton, </address> <year> 1993. </year>
Reference-contexts: This simulated program doesn't actually do anything with the allocated blocks, as a real program would, but it imitates the real program's request sequences exactly, which is sufficient for measuring the memory usage. Modern profiling tools <ref> [BL92, CK93] </ref> can also be used with the simulation program to determine how many instruction cycles are spent in the allocator itself. An alternative strategy is to actually link the program with a variety of allocators, and actually re-run the program for each "simulation". <p> Larus' QPT tool (a successor to the earlier AE system [BL92]) modifies an executable program to make it self-tracing. The Shade tool from SunLabs <ref> [CK93] </ref> is essentially a CPU emulator, which runs a program in emulation and records various kinds of events in an extremely flexible way. For good performance, it uses dynamic compilation techniques to increase speed relative to a straightford interpretive simulator.
Reference: [CKS85] <author> E. G. Coffman, Jr., T. T. Kadota, and L. A. Shepp. </author> <title> On the asymptotic optimality of first-fit storage allocation. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(2):235-239, </volume> <month> February </month> <year> 1985. </year>
Reference-contexts: With better implementations of the general allocator, this would be less attractive. It also appears that the use of a randomized trace is likely to have a significant effect on the results (Section 3.11). Coffman, Kadota, and Shepp <ref> [CKS85] </ref> have conjectured that address-ordered first fit approaches optimal as the size of memory increases. They make very strong assumptions of randomness and independence, including assuming that lifetimes are unrelated and exponentially distributed.
Reference: [CL89] <author> E. G. Coffman, Jr. and F. T. Leighton. </author> <title> A provably efficient algorithm for dynamic storage allocation. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 38(1) </volume> <pages> 2-35, </pages> <month> February </month> <year> 1989. </year> <month> 73 </month>
Reference-contexts: Hutchison screamed and then they were upon her. |Shirley Jackson, "The Lottery" Coffman and Leighton, in a paper titled "A Provably Efficient Algorithm for Dynamic Storage Allocation" <ref> [CL89] </ref> describe an algorithm combining some characteristics of best fit and address-ordered first fit, and prove that its memory usage is asymptotically optimal as system size increases toward infinity.
Reference: [Col61] <author> G. O. Collins. </author> <title> Experience in automatic stor-age allocation. </title> <journal> Communications of the ACM, </journal> <volume> 4(10) </volume> <pages> 436-440, </pages> <month> October </month> <year> 1961. </year>
Reference-contexts: There was a story that the present box had been made with some pieces of the box that had preceded it, the one that had been constructed when the first people settled down to make a village here. |Shirley Jackson, "The Lottery" Collins <ref> [Col61] </ref> apparently originated the random-trace methodology, and reported on experiments with best fit, worst fit, first fit, and random fit. 47 Collins described his simulations as a "game," in the terminology of game theory.
Reference: [Com64] <author> W. T. </author> <title> Comfort. Multiword list items. </title> <journal> Communications of the ACM, </journal> <volume> 7(6), </volume> <month> June </month> <year> 1964. </year>
Reference-contexts: to programs that allocate large and very long-lived data structures near the beginning of execution. 35 understood for these seemingly simple and very pop-ular policies. 3.6 Segregated Free Lists One of the simplest allocators uses an array of free lists, where each list holds free blocks of a particular size <ref> [Com64] </ref>. When a block of memory is freed, it is simply pushed onto the free list for that size. When a request is serviced, the free list for the appropriate size is used to satisfy the request. There are several important variations on this segregated free lists scheme. <p> Exact Lists. In exact lists systems, where there is (conceptually) a separate free list for each possible block size <ref> [Com64] </ref>. This can result in a very large number of free lists, but the "array" of free lists can be represented sparsely. <p> Scheduling of coalescing. Some allocators defer all coalescing until memory runs out, and then coalesce all coalescable memory. This is most common in early designs, including Comfort's original proposal <ref> [Com64] </ref> 88 and Weinstock's "Quick Fit" scheme [Wei76]. This is not an attractive strategy in most modern systems, however, because in a virtual memory, the program never "runs out of space" until backing store is exhausted. <p> of logical (program and data) segments to physical memory. 97 By the mid-1960's, the problem of managing storage for different-sized objects within the address space of a single process was also recognized as an important one, largely due to the increasing use (and sophistication) of list processing techniques and languages <ref> [Ros61, Com64, BR64] </ref>. 98 Equally important, the 1960's saw the invention of the now-traditional methodology for allocator evaluation. In early papers, the assumptions underlying this scheme were explicit and warned against, but as the decade progressed, the warnings decreased in frequency and seriousness. <p> Given this caveat, best fit worked best, but first fit (apparently address-ordered) was almost equally good. No quantitative results were reported, and the distributions used were not specified. Comfort, in a paper about list processing for different-sized objects <ref> [Com64] </ref>, briefly described the segregated lists technique with splitting and coalescing, as well as address-ordered first fit, using an ordered linear list. 100 (The address order would be used to support coalescing without any additional space overhead.) Comfort did not mention that his "multiple free lists" technique (segregated fits with exact
Reference: [CT75] <author> B. Cranston and R. Thomas. </author> <title> A simplified recombination scheme for the Fibonacci buddy system. </title> <journal> Communications of the ACM, </journal> <volume> 18(6) </volume> <pages> 331-332, </pages> <month> July </month> <year> 1975. </year>
Reference-contexts: This supports splitting and merging nearly as quickly as in the binary buddy scheme. Cranston and Thomas <ref> [CT75] </ref> presented a method for quickly finding the buddy of a block in various buddy systems, using only three bits per block. This reduces the time cost of splitting and merging relative to Hirschberg's scheme, as well as incurring minimal space cost.
Reference: [Dar59] <author> Charles Darwin. </author> <title> The Origin of Species. </title> <type> 1859. </type>
Reference-contexts: DNA-based viruses, or the impact of environmental change on host/parasite inter actions [Gar94]. 18 For example, a single chance mutation that results in an adaptive characteristic in one individual may have a major impact on the subsequent evolution of a species and its entire ecosystem <ref> [Dar59] </ref>. 19 We are also not suggesting that evolutionary theory provides a good paradigm for allocator research; it is just an example of a good scientific paradigm that is very different from the ones typically seen in memory allocation research.
Reference: [DDZ93] <author> David Detlefs, Al Dosser, and Benjamin Zorn. </author> <title> Memory allocation costs in large C and C++ programs. </title> <type> Technical Report CU-CS-665-93, </type> <institution> University of Colorado at Boulder, Dept. of Computer Science, Boulder, Colorado, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: It is common for block sizes in many modern systems to average on the order of 10 words, give or take a factor of two or so, so a single word per header may increase memory usage by about 10% <ref> [BJW70, Ung86, ZG92, DDZ93, WJNB95] </ref>. Boundary tags. Many allocators that support general coalescing are implemented using boundary tags (due to Knuth [Knu73]) to support the coalescing of free areas.
Reference: [DEB94] <author> R. Kent Dybvig, David Eby, and Carl Brugge-man. </author> <title> Don't stop the BIBOP: Flexible and efficient storage management for dynamically typed languages. </title> <type> Technical Report 400, </type> <institution> In-diana University Computer Science Dept., </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: In garbage-collected systems, it is common to segregated objects by type, or by implementation-level characteristics, to facilitate optimizations of type checking and/or garbage collection <ref> [Yua90, Del92, DEB94] </ref>. as with best fit. In some cases, however, the details of the size class system and the searching of size-class lists may cause deviations from the best fit policy. Note that in a segregated fits scheme, coalescing may increase search times.
Reference: [Del92] <author> V. Delacour. </author> <title> Allocation regions and implementation contracts. </title> <booktitle> In Bekkers and Cohen [BC92], </booktitle> <pages> pages 426-439. </pages>
Reference-contexts: In garbage-collected systems, it is common to segregated objects by type, or by implementation-level characteristics, to facilitate optimizations of type checking and/or garbage collection <ref> [Yua90, Del92, DEB94] </ref>. as with best fit. In some cases, however, the details of the size class system and the searching of size-class lists may cause deviations from the best fit policy. Note that in a segregated fits scheme, coalescing may increase search times. <p> simply "shu*ed" the real traces, preserving the size and lifetime distributions much more accurately than most synthetic trace generation schemes do.) We found that there was a significant correlation between the results from real traces and those from shu*ed traces, but there were major and systematic 133 See also Delacour's <ref> [Del92] </ref> and Attardi's [AF94] sophisticated systems for low-level storage management in (mostly) garbage-collected systems using mixed lan guages and implementation strategies. errors as well. In an initial test of eight varied alloca-tors, the correlations accounted for only about a third of the observed variation in performance.
Reference: [Den70] <author> Peter J. Denning. </author> <title> Virtual memory. </title> <journal> Computing Surveys, </journal> <volume> 3(2) </volume> <pages> 153-189, </pages> <month> September </month> <year> 1970. </year>
Reference-contexts: They underscored the difficulty of predicting allocator performance. Unfortunately, though their results and commentary were available in 1974 in a technical report, they were not published in a journal until 1977. Denning <ref> [Den70] </ref> used Knuth's fifty percent rule to derive an "unused memory rule", which states that under assumptions of randomness and steady-state behavior, fragmentation generally increases memory usage by about half; he also pointed out that sequential free list searches tend to be longer when memory is heavily loaded.
Reference: [Den95] <author> Daniel Dennett. Darwin's Dangerous Idea. </author> <year> 1995. </year>
Reference-contexts: for simulations to reliably predict them, many important lower-level issues must be modeled correctly, and sufficient data are usually not available, or suffi 9 evolutionary theory is extremely difficult|and some would say impossible|because too many low-level (or higher-level) details matter, 17 and there may intrinsic unpredictabilities in the systems described <ref> [Den95] </ref>. 18 We are not saying that the development of a good theory of memory allocation is as hard as developing a predictive evolutionary theory|far from it. <p> Computer science has historically been biased toward the paradigms of mathematics and physics|and often a rather naive view of the scientific process in those fields|rather than the "softer" natural sciences. We recommend a more naturalistic approach <ref> [Den95] </ref>, which we believe is more appropriate for complex multilevel systems that are only partly hierarchically decomposable. The fact that fact that we study mostly deterministic processes in formally-describable machines is sometimes irrelevant and misleading.
Reference: [Det92] <author> David L. Detlefs. </author> <title> Garbage collection and run-time typing as a C++ library. </title> <booktitle> In USENIX C++ Conference, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1992. </year> <institution> USENIX Association. </institution>
Reference-contexts: for almost the entire run; it represents the nodes in a hypercube and their interconnections. 43 A very large number of other objects are created, but they are small and very short-lived; they represent messages 40 This program (and the hypercube simulator described below) were also used by Detlefs in <ref> [Det92] </ref> for evaluation of a garbage collector. Based on several kinds of profiles, we now think that Detlefs' choice of test programs may have led to an overestimation of the costs of his garbage collector for C++.
Reference: [Dij69] <author> Edsger W. Dijkstra. </author> <title> Notes on structured programming. In Structured Programming. </title> <publisher> Academic Press, </publisher> <year> 1969. </year>
Reference-contexts: Below that there is an actual mechanism that is intended to implement the policy (and presumably effect the strategy), using whatever algorithms and data structures are deemed appropriate. Mechanisms are often layered, as well, in the usual manner of structured programming <ref> [Dij69] </ref>.
Reference: [Dou93] <author> Fred Douglis. </author> <title> The compression cache: Using on-line compression to extend physical memory. </title> <booktitle> In Proceedings of 1993 Winter USENIX Conference, </booktitle> <pages> pages 519-529, </pages> <address> San Diego, Cali-fornia, </address> <month> January </month> <year> 1993. </year>
Reference: [DTM93] <author> Amer Diwan, David Tarditi, and Eliot Moss. </author> <title> Memory subsystem performance of programs with intensive heap allocation. </title> <note> Submitted for publication, </note> <month> August </month> <year> 1993. </year>
Reference-contexts: The literature on garbage collection is considerably more sophisticated in terms of locality studies than the literature on memory allocation, and should not be overlooked. (See, e.g., <ref> [Bae73, KLS92, Wil90, WLM92, DTM93, Rei94, GA95, Wil95] </ref>.) Many of the same issues must arise in conventionally-managed heaps as well. 26 larger free blocks (coalescing). Equally important are the policy and strategy implications|i.e., whether the allocator properly exploits the regularities in real request streams.
Reference: [DWH + 90] <author> Alan Demers, Mark Weiser, Barry Hayes, Daniel Bobrow, and Scott Shenker. </author> <title> Combining generational and conservative garbage collection: Framework and implementations. </title> <booktitle> In Conference Record of the Seventeenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 261-269, </pages> <address> San Fran-cisco, California, </address> <month> January </month> <year> 1990. </year> <note> ACM Press. </note>
Reference-contexts: Beck [Bec82], Demers et al. <ref> [DWH + 90] </ref>, and and Barrett and Zorn [BZ93] have developed systems that predict the lifetimes of objects for similar purposes. We note that for our purposes, it is not necessary to predict which groups of objects will die when. <p> In that case, the bitmap will include one bit for each double-word alignment boundary.) To our knowledge, bitmapped allocation has never been used in a conventional allocator, but it is quite common in other contexts, particularly mark-sweep garbage collectors (notably the conservative collectors of Boehm, et al. from Xerox PARC <ref> [BW88, BDS91, DWH + 90] </ref> 84 ) and file systems' disk block managers. We suspect that the main reason it has not been used for conventional memory allocation is that it is perceived as too slow. <p> Based on profile information, it predicts whether the lifetimes of objects created by that call pattern can be reliably predicted to be short. (This is essentially a refinement of a similar scheme used by Demers et al. for lifetime prediction in a garbage collector; that scheme <ref> [DWH + 90] </ref> uses only the size and stack pointer, however, not the call chain.) For five test applications, Barrett and Zorn found that examining the stack to a depth of four calls generally worked quite well, enabling discrimination between qualitatively different patterns that result in allocations from the same allocator
Reference: [EO88] <author> C. S. Ellis and T. J. Olson. </author> <title> Algorithms for parallel memory allocation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(4) </volume> <pages> 303-345, </pages> <year> 1988. </year>
Reference-contexts: To our knowledge, this is by far the most thorough review to date, but it should not be considered detailed or exhaustive; valuable points or papers may have escaped our notice. 95 We have left out work on concurrent and parallel al-locators (e.g., <ref> [GW82, Sto82, BAO85, MK88, EO88, For88, Joh91, JS92, JS92, MS93, Iye93] </ref>), which are beyond the scope of this paper. We have also neglected mainly analytical work (e.g., [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85]) to some degree, be 94 This is not quite necessarily true.
Reference: [Fer76] <author> H. R. P. Ferguson. </author> <title> On a generalization of the Fibonacci numbers useful in memory allocation schema. </title> <journal> The Fibonacci Quarterly, </journal> <volume> 14(3) </volume> <pages> 233-243, </pages> <month> October </month> <year> 1976. </year>
Reference-contexts: Delacroix said to Mrs. Graves in the back row. |Shirley Jackson, "The Lottery" Peterson and Norman [PN77] described a very general class of buddy systems, and experimentally compared several varieties of buddy systems: binary, Fibonacci, a generalized Fibonacci <ref> [HS64, Fer76] </ref>, and weighted. They used the usual random trace methodology, with both synthetic (uniform and exponential) and real size distributions.
Reference: [For88] <author> R. Ford. </author> <title> Concurrent algorithms for real-time memory management. </title> <journal> IEEE Software, </journal> <pages> pages 10-23, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: To our knowledge, this is by far the most thorough review to date, but it should not be considered detailed or exhaustive; valuable points or papers may have escaped our notice. 95 We have left out work on concurrent and parallel al-locators (e.g., <ref> [GW82, Sto82, BAO85, MK88, EO88, For88, Joh91, JS92, JS92, MS93, Iye93] </ref>), which are beyond the scope of this paper. We have also neglected mainly analytical work (e.g., [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85]) to some degree, be 94 This is not quite necessarily true.
Reference: [FP74] <author> J. S. Fenton and D. W. Payne. </author> <title> Dynamic storage allocations of arbitrary sized segments. </title> <booktitle> In Proc. IFIPS, </booktitle> <pages> pages 344-348, </pages> <year> 1974. </year>
Reference-contexts: Another policy is so-called "optimal fit," where a limited search of the list is usually used to "sample" the list, and a further search finds a fit that is as good or better [Cam71]. 63 Another policy is "half fit" <ref> [FP74] </ref>, where the allocator preferentially splits blocks twice the requested size, in hopes that the remainder will come in handy if a similar request occurs soon. <p> By default, they used FIFO-ordered free lists. With LIFO-ordered free lists, memory usage was about 3% worse. Using a variation of the random trace methodology intended to approximate a segment-based multiprogramming system, 110 Fenton and Payne <ref> [FP74] </ref> compared best fit (called "least fit"), first fit, next fit, worst fit, and "half fit." The half fit policy allocator attempts to find a block about twice the desired size, in the hopes that if there is a bias toward particular sizes, remainders from splitting will be more likely to
Reference: [FP91] <author> Matthew Farrens and Arvin Park. </author> <title> Dynamic base register caching: A technique for reducing address bus width. </title> <booktitle> In 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 128-137, </pages> <address> Toronto, Canada, May 1991. </address> <publisher> ACM Press. </publisher>
Reference-contexts: instructions 48 Conventional text-string-oriented compression algorithms [Nel91] (e.g, UNIX compress or GNU gzip) work quite well, although we suspect that sophisticated schemes could do significantly better by taking advantage of the numerical properties of object identifiers or addresses; such schemes have been proposed for use in compressed paging and addressing <ref> [WLM91, FP91] </ref>. (Text-oriented compression generally makes Markov-like modeling assumptions, i.e., that literal sequences are likely to recur.
Reference: [GA95] <author> Marcelo J. R. Goncalves and Andrew W. Ap-pel. </author> <title> Cache performance of fast-allocating programs. </title> <booktitle> In FPCA '95, </booktitle> <year> 1995. </year>
Reference-contexts: The literature on garbage collection is considerably more sophisticated in terms of locality studies than the literature on memory allocation, and should not be overlooked. (See, e.g., <ref> [Bae73, KLS92, Wil90, WLM92, DTM93, Rei94, GA95, Wil95] </ref>.) Many of the same issues must arise in conventionally-managed heaps as well. 26 larger free blocks (coalescing). Equally important are the policy and strategy implications|i.e., whether the allocator properly exploits the regularities in real request streams.
Reference: [Gar94] <author> Laurie Garrett. </author> <title> The Coming Plague: Newly Emerging Diseases in a World out of Balance. </title> <editor> Farrar, Straus and Giroux, </editor> <address> New York, </address> <year> 1994. </year>
Reference-contexts: DNA-based viruses, or the impact of environmental change on host/parasite inter actions <ref> [Gar94] </ref>. 18 For example, a single chance mutation that results in an adaptive characteristic in one individual may have a major impact on the subsequent evolution of a species and its entire ecosystem [Dar59]. 19 We are also not suggesting that evolutionary theory provides a good paradigm for allocator research; it
Reference: [Gel71] <author> E. Gelenbe. </author> <title> The two-thirds rule for dynamic storage allocation under equilibrium. </title> <journal> Information Processing Letters, </journal> <volume> 1(2) </volume> <pages> 59-60, </pages> <month> July </month> <year> 1971. </year>
Reference-contexts: Gelenbe also derived a similar "two thirds rule" <ref> [Gel71] </ref> in a somewhat different way. (These essentially identical rules are both subject to the same criticisms as Knuth's original rule.) Purdom and Stigler [PS70] performed statistical analyses of the binary buddy system, and argued that limitations on buddy system coalescing were seldom a problem.
Reference: [GGU72] <author> M. R. Garey, R. L. Graham, and J. D. Ullman. </author> <title> Worst-case analysis of memory allocation algorithms. </title> <booktitle> In Fourth Annual ACM Symposium on the Theory of Computing, </booktitle> <year> 1972. </year>
Reference-contexts: It has been proven that for any possible allocation algorithm, there will always be the possibility that some application program will allocate and deallocate blocks in some fashion that defeats the allocator's strategy, and forces it into severe fragmentation <ref> [Rob71, GGU72, Rob74, Rob77] </ref>. Not only are there no provably good allocation algorithms, there are proofs that any allocator will be "bad" for some possible applications.
Reference: [GM85] <author> S. Gai and M. Mezzalama. </author> <title> Dynamic storage allocation: Experiments using the C language. </title> <journal> Software Practice and Experience, </journal> <volume> 15(7) </volume> <pages> 693-704, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: of perceived space or time costs. 10 It seems significant to us that many articles in non-refereed publications|and a number in refereed publications outside the major journals of operating systems and programming languages|are motivated by extreme concerns about the speed or memory costs of general heap allocation. (One such paper <ref> [GM85] </ref> is discussed in Section 4.1.) Often, ad hoc solutions are used for applications that should not be problematic at all, because at least some well-designed general allocators should do quite well for the workload in question. <p> See [WJNB95].) 16 (Other patterns of overall memory usage also occur, but appear less common. As we describe in Section 4, backward ramp functions have been observed <ref> [GM85] </ref>. Combined forward and backward ramp behavior has also been observed, with one data structure shrinking as another grows [Abr67].) Notice that in the case of ramps and ramp-shaped peaks, looking at the statistical distributions of object lifetimes may be very misleading. <p> Gai and Mezzalama <ref> [GM85] </ref> presented a very simple deferred coalescing scheme, where only one size class is treated specially, and the standard C library allocator routines are used for backing storage. (The algorithms used in this library are not stated, and are not standardized.) Their target application domain was concurrent simulations, where many variations
Reference: [Gra] <author> R. L. Graham. </author> <title> Unpublished technical report on worst-case analysis of memory allocation algorithms, </title> <institution> Bell Labs. </institution>
Reference-contexts: The default allocators included first fit and simple segregated storage. (This is the first published mention of simple segregated storage that we have found, though Comfort's multiple free list scheme is similar.) Graham, in an unpublished technical report <ref> [Gra] </ref>, described the problem of analyzing the worst-case memory use of allocators, and presented lower bounds on worst case fragmentation. 102 (An earlier memo by Doug McIlroy may have motivated this work, as well as Robson's later work.) Graham characterized the problem metaphorically as a board game between an "attacker," who
Reference: [GW82] <author> A. Gottlieb and J. Wilson. </author> <title> Parallelizing the usual buddy algorithm. Technical Report System Software Note 37, </title> <institution> Courant Institute, </institution> <address> New York University, </address> <year> 1982. </year>
Reference-contexts: To our knowledge, this is by far the most thorough review to date, but it should not be considered detailed or exhaustive; valuable points or papers may have escaped our notice. 95 We have left out work on concurrent and parallel al-locators (e.g., <ref> [GW82, Sto82, BAO85, MK88, EO88, For88, Joh91, JS92, JS92, MS93, Iye93] </ref>), which are beyond the scope of this paper. We have also neglected mainly analytical work (e.g., [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85]) to some degree, be 94 This is not quite necessarily true.
Reference: [GZ93] <author> Dirk Grunwald and Benjamin Zorn. </author> <title> CustoMalloc: Efficient synthesized memory allocators. </title> <journal> Software Practice and Experience, </journal> <volume> 23(8) </volume> <pages> 851-869, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: On the other hand, data from a different experiment <ref> [GZ93] </ref> show it being considerably slower than a set of allocators designed primarily for speed. <p> regularities in real workloads, 132 it certainly shows that exploitable regularities exist, and that program behavior is not random in the manner assumed (implicitly or explicitly) by earlier researchers. (Barrett and Zorn found that using only the requested size was less predictive, but still provided useful information.) Zorn and Grunwald <ref> [GZ93] </ref> have investigated the tailoring of allocators to particular programs, primarily to improve speed without undue space cost. One important technique is the use of inlining (incorporating the usual-case allocator code at the point of call, rather than requiring an out-of-line call to a subroutine).
Reference: [GZH93] <author> Dirk Grunwald, Benjamin Zorn, and Robert Henderson. </author> <title> Improving the cache locality of memory allocation. </title> <booktitle> In Proceedings of the 1993 SIGPLAN Conference on Programming Language Design and Implementation [PLD93], </booktitle> <pages> pages 177-186. </pages>
Reference-contexts: incorrectly called a buddy system; we do not use that terminology because simple segregated storage does not use a buddy rule for coalescing|no coalescing is done at all. (Standish [Sta80] refers to simple segregated storage as "partitioned storage.") 36 simple segregated storage (used by Mike Haertel in a fast allocator <ref> [GZH93, Vo95] </ref>, and in several garbage collectors [Wil95]) is to maintain a count of live objects for each page, and notice when a page is entirely empty. <p> In effect, they simply use a non-coalescing segregated lists allocator for small objects and an entirely different allocator for large ones. (Examples include We-instock and Wulf's simplification of their own Quick Fit allocator [WW88], and an allocator developed by Grunwald and Zorn, using Lea's allocator as the general allocator <ref> [GZH93] </ref>.) One of the advantages of such 87 The only deferred coalescing segregated fits algorithm that we know of is Doug Lea's allocator, distributed freely and used in several recent studies (e.g., [GZH93, Vo95, WJNB95]). 43 a scheme is that the minimum block size can be very small|only big enough to <p> Fit allocator [WW88], and an allocator developed by Grunwald and Zorn, using Lea's allocator as the general allocator [GZH93].) One of the advantages of such 87 The only deferred coalescing segregated fits algorithm that we know of is Doug Lea's allocator, distributed freely and used in several recent studies (e.g., <ref> [GZH93, Vo95, WJNB95] </ref>). 43 a scheme is that the minimum block size can be very small|only big enough to hold a header and and a single link pointer. (Doubly-linked lists aren't necessary, since no coalescing is done for small objects.) These simplified designs are not true deferred coalescing allocators, except in <p> It is clear from our review of the literature that there was-and still is|no good model that predicts such a happy coincidence.) 65 Zorn, Grunwald, and Henderson <ref> [GZH93] </ref> mea-sured the locality effects of several allocators: next fit, the G++ segregated fits allocator by Doug Lea, simple segregated storage using powers of two size classes (the Berkeley 4.2 BSD allocator by Chris Kingsley), and two simplified quick fit schemes (i.e., "Quick Fit" in the sense of [WW88], i.e., without <p> Recent work in garbage collection shows this to be true ([WLM92, Wil95, GA95]), but few architects are aware of it, or aware that similar phenomena must occur (to at least some degree) in conventionally-managed memories as well <ref> [GZH93] </ref>. The challenge is to develop a theory that can span all of these levels.
Reference: [Han90] <author> David R. Hanson. </author> <title> Fast allocation and deal-location of memory based on object lifetimes. </title> <journal> 74 Software Practice and Experience, </journal> <volume> 20(1), </volume> <month> Jan--uary </month> <year> 1990. </year>
Reference-contexts: Many programs build up data structures quickly, and then use those data structures for long periods (often nearly the whole running time of the program). These patterns are well-known, from anecdotal experience by many people (e.g., <ref> [Ros67, Han90] </ref>), from research on garbage collection (e.g., [Whi80, WM89, UJ88, Hay91, Hay93, BZ95, Wil95]), 32 and from a re cent study of C and C++ programs [WJNB95]. 32 It may be thought that garbage collected systems are sufficiently different from those using conventional storage management that these results are not <p> These are two-horned peaks, where one (large) size is allocated and deallocated, and much smaller size is allocated and deallocated, out of phase. 39 (This is an unusual feature, in our 37 See the discussion of <ref> [Han90] </ref> (Section 4.1) for a descrip tion of obstacks. 38 We've seen similarly strong peaks in a profile of a compiler of our own, which relies on garbage collection rather than obstacks. 39 Interestingly, the first of the horns usually consists of a size that is specific to that peak|different peaks <p> If systems eventually become ver large (and heterogeneous), locality concerns are likely to be crucial. (Consider the effects on locality in a large system when objects are placed in effectively randomly-generated holes; the scattering of related data seems likely to be a problem.) Hanson <ref> [Han90] </ref> presents a technique for allocating objects and deallocating them en masse. This is often more efficient and convenient than traversing data structures being deallocated, and freeing each object individually. A special kind of heap can be created on demand.
Reference: [Har95] <author> Juris Hartmanis. </author> <title> Turing award lecture: </title> <journal> On computational complexity and the nature of computer science. Computing Surveys, </journal> <volume> 27(1) </volume> <pages> 7-16, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: The history of memory allocation research may serve as a cautionary tale for empirical computer science. Hartmanis has observed that computer science seems less prone to paradigm shifts than most fields <ref> [Har95] </ref>. We agree in part with this sentiment, but the successes of computer science can lead to a false sense 138 Our anonymous FTP repository is on ftp.cs.utexas.edu in the directory pub/garbage. <p> field that attracts a different kind of thinker: : : Such people are especially good at dealing with situations where different rules apply in different cases; they are individuals who can rapidly change levels of abstraction, simultaneously seeing things "in the large" and "in the small." |Donald Knuth, quoted in <ref> [Har95] </ref> Memory management is a fundamental area of computer science, spanning several very different levels of abstraction|from the programmer's strategies for dealing with data, language-level features for expressing those concepts, language implementations for managing actual storage, and the varied hardware memories that real machines contain.
Reference: [Hay91] <author> Barry Hayes. </author> <title> Using key object opportunism to collect old objects. </title> <editor> In Andreas Paepcke, editor, </editor> <booktitle> Conference on Object Oriented Programming Systems, Languages and Applications (OOPSLA '91), </booktitle> <pages> pages 33-46, </pages> <address> Phoenix, Arizona, </address> <month> October </month> <year> 1991. </year> <note> ACM Press. </note>
Reference-contexts: Many programs build up data structures quickly, and then use those data structures for long periods (often nearly the whole running time of the program). These patterns are well-known, from anecdotal experience by many people (e.g., [Ros67, Han90]), from research on garbage collection (e.g., <ref> [Whi80, WM89, UJ88, Hay91, Hay93, BZ95, Wil95] </ref>), 32 and from a re cent study of C and C++ programs [WJNB95]. 32 It may be thought that garbage collected systems are sufficiently different from those using conventional storage management that these results are not relevant.
Reference: [Hay93] <author> Barry Hayes. </author> <title> Key Objects in Garbage Collection. </title> <type> PhD thesis, </type> <institution> Standford University, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Many programs build up data structures quickly, and then use those data structures for long periods (often nearly the whole running time of the program). These patterns are well-known, from anecdotal experience by many people (e.g., [Ros67, Han90]), from research on garbage collection (e.g., <ref> [Whi80, WM89, UJ88, Hay91, Hay93, BZ95, Wil95] </ref>), 32 and from a re cent study of C and C++ programs [WJNB95]. 32 It may be thought that garbage collected systems are sufficiently different from those using conventional storage management that these results are not relevant.
Reference: [Hin75] <author> J. A. Hinds. </author> <title> An algorithm for locating adjacent storage blocks in the buddy system. </title> <journal> Communications of the ACM, </journal> <volume> 18(4) </volume> <pages> 221-222, </pages> <month> April </month> <year> 1975. </year>
Reference-contexts: This models embodies an oversimplification relative to most real systems, in that processes in most systems may have multiple associated segments whose death times cannot be postponed independently. Hinds <ref> [Hin75] </ref> presented a fast scheme for recombination in binary and generalized Fibonacci buddy systems.
Reference: [Hir73] <author> D. S. Hirschberg. </author> <title> A class of dynamic memory allocation algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 16(10) </volume> <pages> 615-618, </pages> <month> October </month> <year> 1973. </year>
Reference-contexts: Fibonacci buddies. This variant of the buddy scheme uses a more closely-spaced set of size classes, based on a Fibonacci series, to reduce internal fragmentation <ref> [Hir73] </ref>. Since each number in the Fibonacci series is the sum of the two previous numbers, a block can always be split (unevenly) to yield two blocks whose sizes are also in the series. <p> As with binary buddies, the increasing size of successive size ranges limits the number of free lists required. A further refinement, called generalized Fibonacci buddies <ref> [Hir73, Bur76, PN77] </ref> uses a Fibonacci-like number series that starts with a larger number and generates a somewhat more closely-spaced set of sizes. <p> The bit for the last word of each block, and the bit for the last word occupied by a block is set. The buddy placement constraint lets these be used as "tail lamps" to look efficiently look through memory to find the ends of preceding blocks. Hirschberg <ref> [Hir73] </ref> followed Knuth's suggestion and devised a Fibonacci buddy system; he compared this experimentally to a binary buddy. His experiment used the usual synthetic trace methodology, using a real distribution of block sizes (from the University of Maryland UNIVAC Exec 8 system [M + 69]) and exponential lifetime distribution.
Reference: [HS64] <author> V. C. Harris and C. C. </author> <title> Styles. A generalization of the Fibonacci numbers. </title> <journal> The Fibonacci Quarterly, </journal> <volume> 2(4) </volume> <pages> 227-289, </pages> <month> December </month> <year> 1964. </year>
Reference-contexts: Delacroix said to Mrs. Graves in the back row. |Shirley Jackson, "The Lottery" Peterson and Norman [PN77] described a very general class of buddy systems, and experimentally compared several varieties of buddy systems: binary, Fibonacci, a generalized Fibonacci <ref> [HS64, Fer76] </ref>, and weighted. They used the usual random trace methodology, with both synthetic (uniform and exponential) and real size distributions.
Reference: [HS89] <author> Mark D. Hill and Alan Jay Smith. </author> <title> Evaluating associativity in CPU caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1612-1629, </pages> <month> De-cember </month> <year> 1989. </year>
Reference-contexts: Efficient cache simulators are available for processing reference traces, including Mark Hill's Tycho and Dinero systems <ref> [HS89] </ref>. 49 3 A Taxonomy of Allocators Allocators are typically categorized by the mechanisms they use for recording which areas of memory are free, and for merging adjacent free blocks into 49 Before attempting locality studies, however, allocation researchers should become familiar with the rather subtle issues in cache design, in
Reference: [IGK71] <author> S. Isoda, E. Goto, and I. Kimura. </author> <title> An efficient bit table technique for dynamic storage allocation of 2 n -word blocks. </title> <journal> Communications of the ACM, </journal> <volume> 14(9) </volume> <pages> 589-592, </pages> <month> September </month> <year> 1971. </year>
Reference-contexts: allocated object|objects must be aligned on word boundaries for architectural reasons, and there is no provision for stealing a bit from the space allocated to an object. 79 Stealing a bit from each object can be avoided, however, by keeping the bits in a separate table "off to the side" <ref> [IGK71] </ref>, but this is fairly awkward, and such a bit table could probably be put to better use with an entirely different basic allocation mechanism. In practical terms, therefore, buddy systems usually require a header word per object, to record the type and/or size. <p> Robson [Rob71] showed that the worst-case performance of a worst-case-optimal algorithm is bounded from below by a function that rises logarithmically with the ratio n (the ratio of the largest and smallest block sizes), i.e., M log 2 n times a constant. Isoda, Goto and Kimura <ref> [IGK71] </ref> introduced a bitmapped technique for keeping track of allocated and unallocated buddies in the (binary) buddy system. Rather than taking a bit (or several, as in Knowl-ton's original scheme) out of the storage for each block, their scheme maintains a bit vector corresponding to the words of memory.
Reference: [IJ62] <author> J. K. Iliffe and J. G. Jodeit. </author> <title> A dynamic storage allocation scheme. </title> <journal> Computer Journal, </journal> <volume> 5(3) </volume> <pages> 200-209, </pages> <month> October </month> <year> 1962. </year>
Reference-contexts: We apologize in advance for a certain amount of redundancy|we have attempted to make this section relatively free-standing, so that it can be read straight through (by a reader with sufficient fortitude) given the basic concepts presented by earlier sections. 96 Several very early papers (e.g., <ref> [Mah61, IJ62] </ref>) discussed memory fragmentation, but in systems where segments could be compacted together or swapped to secondary storage when fragmentation became a problem; these papers generally do not give any quantitative results at all, and few qualitative results comparing different allocation strategies. 46 1960 to 1969. Overview.
Reference: [Ing61] <author> P. Z. Ingerman. </author> <title> Thunks. </title> <journal> Communications of the ACM, </journal> <volume> 4(1) </volume> <pages> 55-58, </pages> <month> January </month> <year> 1961. </year>
Reference-contexts: usual current usage of the term [Wil95], but it is not uncommon in early papers on allocators. 121 Activation records were apparently allocated on the general heap; presumably this was used to support closures with indefinite extent (i.e., "block retention"), and/or "thunks" (hidden parameterless subroutines) for call by-name parameter passing <ref> [Ing61] </ref>. 58 Betteridge [Bet82] attempted to compute frag-mentation probabilities for different allocators using first-order Markov modeling. (This book is apparently Betteridge's dissertation, completed in 1979.) The basic idea is to model all possible states of memory occupancy (i.e., all arrangements of allocated and free blocks), and the transition probabilities between those
Reference: [Iye93] <author> Arun K. Iyengar. </author> <title> Parallel dynamic storage allocation algorithms. </title> <booktitle> In Fifth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <year> 1993. </year>
Reference-contexts: To our knowledge, this is by far the most thorough review to date, but it should not be considered detailed or exhaustive; valuable points or papers may have escaped our notice. 95 We have left out work on concurrent and parallel al-locators (e.g., <ref> [GW82, Sto82, BAO85, MK88, EO88, For88, Joh91, JS92, JS92, MS93, Iye93] </ref>), which are beyond the scope of this paper. We have also neglected mainly analytical work (e.g., [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85]) to some degree, be 94 This is not quite necessarily true.
Reference: [Joh72] <author> G. D. Johnson. </author> <note> Simscript II.5 User's Manual, S/360-370 Version, Release 6, </note> <year> 1972. </year>
Reference-contexts: lists for small block sizes, backed by LIFO-ordered first fit as the general allocator. 115 (Weinstock reported that this scheme was invented several years earlier for use in the Bliss/11 compiler [WJW + 75], and notes that a similar scheme was independently developed and used in the Simscript II.5 language <ref> [Joh72] </ref>. Margolin's prior work was overlooked, however.) Weinstock used the conventional synthetic trace methodology; randomly-ordered synthetic traces were generated, using two real size distributions and four artificial ones.
Reference: [Joh91] <author> Theodore Johnson. </author> <title> A concurrent fast fits memory manager. </title> <type> Technical Report 91-009, </type> <institution> University of Florida, </institution> <year> 1991. </year>
Reference-contexts: To our knowledge, this is by far the most thorough review to date, but it should not be considered detailed or exhaustive; valuable points or papers may have escaped our notice. 95 We have left out work on concurrent and parallel al-locators (e.g., <ref> [GW82, Sto82, BAO85, MK88, EO88, For88, Joh91, JS92, JS92, MS93, Iye93] </ref>), which are beyond the scope of this paper. We have also neglected mainly analytical work (e.g., [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85]) to some degree, be 94 This is not quite necessarily true.
Reference: [JS92] <author> T. Johnson and D. Sasha. </author> <title> Parallel buddy memory management. </title> <journal> Parallel Processing Letters, </journal> <volume> 2(4) </volume> <pages> 391-398, </pages> <year> 1992. </year>
Reference-contexts: To our knowledge, this is by far the most thorough review to date, but it should not be considered detailed or exhaustive; valuable points or papers may have escaped our notice. 95 We have left out work on concurrent and parallel al-locators (e.g., <ref> [GW82, Sto82, BAO85, MK88, EO88, For88, Joh91, JS92, JS92, MS93, Iye93] </ref>), which are beyond the scope of this paper. We have also neglected mainly analytical work (e.g., [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85]) to some degree, be 94 This is not quite necessarily true.
Reference: [Kau84] <author> Arie Kaufman. </author> <title> Tailored-list and recombination-delaying buddy systems. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(4) </volume> <pages> 118-125, </pages> <year> 1984. </year>
Reference-contexts: Empty pages can be transferred from one buddy series to another. To our knowledge, such an optimization has never been implemented for a double buddy scheme. Buddy systems can easily be enhanced with deferred coalescing techniques, as in "recombination delaying" buddy systems <ref> [Kau84] </ref>. <p> Another alternative is to use an allocator which in its usual operation maintains a set of free lists for different sizes or size classes, and simply to defer the coalescing of the blocks on those lists. This may be a buddy system (as in <ref> [Kau84] </ref>) or a segregated lists allocator such as segregated fits. 87 Some allocators, which we will call "simplified quick fit" allocators, are structured similarly but don't do any coalescing for the small blocks on the quick lists. <p> This may be a response to the then-unpublished experiments in [BBDT84], but no details are given.) Kaufman <ref> [Kau84] </ref> presented two buddy system al-locators using deferred coalescing.
Reference: [KLS92] <author> Phillip J. Koopman, Jr., Peter Lee, and Daniel P. Siewiorek. </author> <title> Cache performance of combinator graph reduction. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 14(2) </volume> <pages> 265-297, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: The literature on garbage collection is considerably more sophisticated in terms of locality studies than the literature on memory allocation, and should not be overlooked. (See, e.g., <ref> [Bae73, KLS92, Wil90, WLM92, DTM93, Rei94, GA95, Wil95] </ref>.) Many of the same issues must arise in conventionally-managed heaps as well. 26 larger free blocks (coalescing). Equally important are the policy and strategy implications|i.e., whether the allocator properly exploits the regularities in real request streams.
Reference: [Kno65] <author> Kenneth C. Knowlton. </author> <title> A fast storage allocator. </title> <journal> Communications of the ACM, </journal> <volume> 8(10) </volume> <pages> 623-625, </pages> <month> October </month> <year> 1965. </year>
Reference-contexts: and policies to implement almost any desired strategy. (It seems likely that the original version of boundary tags was initially viewed as too costly in space, in a time when memory was a very scarce resource, and the footer optimization [Sta80] simply never became well-known.) 3.7 Buddy Systems Buddy systems <ref> [Kno65, PN77] </ref> are a variant of segregated lists that supports a limited but efficient kind of splitting and coalescing. <p> Binary buddies are the simplest and best-known kind of buddy system <ref> [Kno65] </ref>. In this scheme, all buddy sizes are a power of two, and each size is divided into two equal parts. <p> Naturally, the "block" sizes here were rather large. Totschek found a roughly trimodal distribution, with most jobs being either around 20,000 words, or either less than half or more than twice that. He did not find a significant correlation between job size and running time. Knowlton <ref> [Kno65] </ref> published the first paper on the 99 We suspect that the history of allocator research might have been quite different if this metaphor had been taken more seriously|the application program in the randomized methodology is a very unstable individual, or one using a very peculiar strategy. 100 Knuth [Knu73] reports
Reference: [Knu73] <author> Donald E. Knuth. </author> <title> The Art of Computer Programming, volume 1: Fundamental Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1973. </year> <note> First edition published in 1968. </note>
Reference-contexts: If the object sizes are smoothly distributed, the requested sizes will almost always be slightly different, increasing the chances of fragmentation. Probabilistic analyses.Since Knuth's derivation of the "fifty percent rule" <ref> [Knu73] </ref> (discussed later, in Section 4), there have been many attempts to reason probabilistically about the interactions between program behavior and allocator policy, and assess the overall cost in terms of fragmentation (usually) and/or CPU time. <p> Boundary tags. Many allocators that support general coalescing are implemented using boundary tags (due to Knuth <ref> [Knu73] </ref>) to support the coalescing of free areas. <p> strategy and policy has sometimes hampered experimental evaluations; even after obviously scalable implementations had been discussed in the literature, later researchers often excluded sequential fit policies from consideration due to their apparent time costs. 58 This potential accumulation of small fragments (often called "splinters" or "sawdust") was noted by Knuth <ref> [Knu73] </ref>, but it seems not to be a serious problem for best fit, with either real or synthetic workloads. 30 satisfy the request. If the block is larger than neces-sary, it is split and the remainder is put on the free list. <p> Next fit. A common "optimization" of first fit is to use a roving pointer for allocation <ref> [Knu73] </ref>. The pointer records the position where the last search was satisfied, and the next search begins from there. Successive searches cycle through the free list, so that searches do not always begin in the same place and result in an accumulation of splinters. <p> In addition, the allocator may choose not to split a block if the remainder is "too small," either in absolute terms <ref> [Knu73] </ref> or relative to the size of the block being split [WJNB95]. This policy is intended to avoid allocating in the remainder a small object that may outlive the large object, and prevent the reclamation of a larger free area. <p> On the other hand, systems based on closer size class spacings may be similarly efficient if lookup tables are used to perform size class mappings quickly. A major problem with binary buddies is that internal fragmentation is usually relatively high|the expected case is (very roughly) about 28% <ref> [Knu73, PN77] </ref>, 81 because any object size must be rounded up to the nearest power of two (minus a word for the header, if the size field is stored). Fibonacci buddies. <p> Knuth <ref> [Knu73] </ref> gives pointers to early history of linked list processing.) In the earliest days, interest was largely in managing memory overlays or segments in segmented operating systems, i.e., managing mappings of logical (program and data) segments to physical memory. 97 By the mid-1960's, the problem of managing storage for different-sized objects <p> well developed, and "user" programs often performed "system level" tasks for themselves. 98 Early list processing systems used only list nodes of one or two sizes, typically containing only two pointers, but later systems supported nodes of arbitrary sizes, to directly support structures that had multiple links. (Again, see Knuth <ref> [Knu73] </ref> for more references.) For example, multitasking may introduce phase behavior, since the segments belonging to a process are usually only released when that process is running, or when it terminates. Between time slices, a program does not generally acquire or release segments. <p> Knowlton [Kno65] published the first paper on the 99 We suspect that the history of allocator research might have been quite different if this metaphor had been taken more seriously|the application program in the randomized methodology is a very unstable individual, or one using a very peculiar strategy. 100 Knuth <ref> [Knu73] </ref> reports that this paper was written in 1961, but unpublished until 1964. (binary) buddy system, although Knuth [Knu73] reports that same idea was independently invented and used by H. Markowitz in the Simscript system around 1963. <p> have been quite different if this metaphor had been taken more seriously|the application program in the randomized methodology is a very unstable individual, or one using a very peculiar strategy. 100 Knuth <ref> [Knu73] </ref> reports that this paper was written in 1961, but unpublished until 1964. (binary) buddy system, although Knuth [Knu73] reports that same idea was independently invented and used by H. Markowitz in the Simscript system around 1963. Knowlton also suggested the use of deferred coalescing to avoid unneeded overheads in the common case where objects of the same size were frequently used. <p> Following Knuth <ref> [Knu73] </ref>, he hypothesized that this was due to its tendency to fit small objects into holes near one end of memory, accumulating larger free areas toward the other end. 111 For partial populations, Shore found that increasing degrees of spikiness seemed to favor best fit over first 111 We are actually <p> This report apparently went unnoticed until well after double buddy was reinvented by Page and Hagins [PH86]. 118 Reeves [Ree79, Ree80, Ree82, Ree83] used analytic techniques to determine the effect of a random fit allocator policy in the face of random workloads, using a "generating function" approach originated by Knuth <ref> [Knu73] </ref>. This work relies extremely heavily on randomness assumptions|usually in both the workload and the allocator|to enable the analyses of memories of significant size. 1980 to 1990.
Reference: [Kri72] <author> Saul A. Kripke. </author> <title> Naming and Necessity. </title> <publisher> Har-vard University Press, </publisher> <year> 1972. </year>
Reference-contexts: We think that this is a strength, however, because it is better to leave a concept somewhat vague than to define it prematurely and incorrectly. It is important to first identify the "natural kinds" in the phenomena under study, and then figure out what their most important characteristics are <ref> [Kri72, Put77, Qui77] </ref>. (We are currently working on developing operational measures of "fragmentation-related" program behavior.) Later in the paper we will express experimental "fragmentation" results as percentages, but this should be viewed as an operational shorthand for the effects of fragmentation on memory usage at whatever point or points in program
Reference: [Kro73] <author> S. Krogdahl. </author> <title> A dynamic storage allocation problem. </title> <journal> Information Processing Letters, </journal> <volume> 2 </volume> <pages> 96-99, </pages> <year> 1973. </year>
Reference-contexts: We have also neglected mainly analytical work (e.g., <ref> [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85] </ref>) to some degree, be 94 This is not quite necessarily true. For applications that do little freeing, the initial carving of memory requested from the operating system will be a significant fraction of the allocation cost.
Reference: [Kuh70] <author> Thomas S. Kuhn. </author> <title> The Structure of Scientific Revolutions (Second Edition, </title> <type> Enlarged). </type> <institution> University of Chicago Press, Chicago, Illinois, </institution> <year> 1970. </year>
Reference-contexts: heavily-timeshared computer.) Later, the emphasis of study shifted away from segment sizes in segmented operating systems, and toward data object sizes in the virtual memories of individual processes running in paged virtual memories. 25 We are unclear on why this should be, except that a particular theoretical and experimental paradigm <ref> [Kuh70] </ref> had simply become thoroughly entrenched in the early 1970's. (It's also somewhat easier than dealing with real data.) 11 of stochastic processes (Markov models, etc.) to derive analytical results about expected behavior. <p> One cause is simply the (short) history of the field, and expectations that computer science issues would be easily formalized, after many striking early successes. (Ullman [Ull95] eloquently describes this phenomenon.) Another is doubtless the same kind of paradigm entrenchment that occurs in other, more mature sciences <ref> [Kuh70] </ref>. Once the received view has been used as a theoretical underpinning of enough seemingly successful experiments, and reiterated in textbooks without the caveats buried in the original research papers, it is very hard for people to see the alternatives.
Reference: [KV85] <author> David G. Korn and Kiem-Phong Vo. </author> <title> In search of a better malloc. </title> <booktitle> In Proc. USENIX Summer 1985, </booktitle> <pages> pages 489-506, </pages> <address> Portland, Oregon, </address> <month> June </month> <year> 1985. </year> <institution> USENIX Association. </institution>
Reference-contexts: This avoids the need to store a separate link field, making the minimum object size quite small. (We've never seen this technique described, but would be surprised if it hasn't been used before, perhaps in some of the allocators described in <ref> [KV85] </ref>.) If used straightforwardly, such a system is likely to scale very poorly, because live blocks must be traversed during search, but this technique might be useful in combination with some In experiments with both real and synthetic traces, it appears that address-ordered first fit may cause significantly less fragmentation than <p> Worse, it may affect the locality of the program it allocates for, by scattering objects used by certain phases and intermingling them with objects used by other phases.) In several experiments using both real traces [WJNB95] and synthetic traces (e.g., <ref> [Bay77, Wei76, Pag84, KV85] </ref>), next fit has been shown to cause more other indexing structure. 60 This has also been observed by Ivor Page [Pag82] in randomized simulations, and similar (but possibly weaker) observations were made by Knuth and Shore and others in the late 1960's and 1970's. (Section 4.) 31 <p> On the other hand, it's not being freed|in a 66 One example is in an early version of the large object manager for the Lucid Common Lisp system (Jon L. White, personal communication, 1991); another is men tioned in <ref> [KV85] </ref> (Section 4.1). 33 sense, the end block has been there all along, ignored until needed. Perhaps it should go on the opposite end of the list because it's conceptually the oldest block| the very large block that contains all as-yet-unused memory. <p> block can also be extended to include more memory by expanding the heap segment, so that the entire area above the high-water mark is viewed as a single huge block. 67 ) Korn and Vo call this a "wilderness preservation heuristic," and report that it is helpful for some allocators <ref> [KV85] </ref> (No quantitative results are given, however.) For policies like best fit and address-ordered first fit, it seems natural to simply put the end block in the indexing structure like any other block. <p> For that workload, Working Set and FIFO performed about equally, and poorly, as would be expected. Effects on actual memory usage were not reported, so the effect of their deferred coalescing on overall memory usage is unknown. Korn and Vo <ref> [KV85] </ref> evaluated a variety of UNIX memory allocators, both production implementations distributed with several UNIX systems, and new implementations and variants.
Reference: [LH82] <author> B. W. Leverett and P. G. Hibbard. </author> <title> An adaptive system for dynamic storage allocation. </title> <journal> Software Practice and Experience, </journal> <volume> 12(6) </volume> <pages> 543-556, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: We then propose a new (though nearly obvious) conception of fragmentation and its causes, and describe more suitable techniques used to study it. (Most of the experiments using sound techniques have been performed in the last few years, but a few notable exceptions were done much earlier, e.g., [MPS71] and <ref> [LH82] </ref>, discussed in Section 4.) 2.1 Internal and External Fragmentation Traditionally, fragmentation is classed as external or internal [Ran69], and is combatted by splitting and coalescing free blocks. <p> fit, both analytically and in randomized simulations. (Only uniformly distributed sizes and lifetimes were used.) The cyclic placement scheme generally resulted in significantly more fragmentation than first fit or best fit. "...over in the north village they're talking of giving up the lottery." |Shirley Jackson, "The Lottery" Leverett and Hibbard <ref> [LH82] </ref> performed one of the all-too-rare studies evaluating memory allo-cators using real traces. Unfortunately, their workload consisted of five very small programs (e.g., towers of Hanoi, knight's tour) coded in Algol-68; none was more than 100 lines.
Reference: [LH83] <author> Henry Lieberman and Carl Hewitt. </author> <title> A real-time garbage collector based on the lifetimes of objects. </title> <journal> Communications of the ACM, </journal> <volume> 26(6) </volume> <pages> 419-429, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: David Moon reports that a similar facility in the Symbolics system often resulted in obscure bugs, and its use was discouraged after an efficient generational garbage collector [Moo84] was developed (Moon, personal communication 1995); generational techniques heuristically exploit the lifetime distributions of typical programs <ref> [LH83, Wil95] </ref>. For systems without garbage collection, however, the resulting problems may be no worse than those introduced by other explicit deallocation strategies, when used care fully and in well-documented ways. 64 4.2 Recent Studies Using Real Traces "Some places have already quit lotteries," Mrs.
Reference: [M + 69] <editor> J. Minker et al. </editor> <title> Analysis of data processing systems. </title> <type> Technical Report 69-99, </type> <institution> University of Maryland, College Park, Maryland, </institution> <year> 1969. </year>
Reference-contexts: Minker et al. <ref> [M + 69] </ref> published a technical report which contained a distribution of "buffer sizes" in the University of Maryland UNIVAC Exec 8 system. 107 Unfortunately, these data are imprecise, because they give counts of buffers within ranges of sizes, not exact sizes. <p> Hirschberg [Hir73] followed Knuth's suggestion and devised a Fibonacci buddy system; he compared this experimentally to a binary buddy. His experiment used the usual synthetic trace methodology, using a real distribution of block sizes (from the University of Maryland UNIVAC Exec 8 system <ref> [M + 69] </ref>) and exponential lifetime distribution. His results agreed well with the analytically derived estimates; Fibo-nacci buddy's fragmentation increased memory usage by about 25%, compared to binary buddy's 38%.
Reference: [Mah61] <author> R. J. Maher. </author> <title> Problems of storage allocation in a multiprocessor multiprogrammed system. </title> <journal> Communications of the ACM, </journal> <volume> 4(10) </volume> <pages> 421-422, </pages> <month> October </month> <year> 1961. </year>
Reference-contexts: We apologize in advance for a certain amount of redundancy|we have attempted to make this section relatively free-standing, so that it can be read straight through (by a reader with sufficient fortitude) given the basic concepts presented by earlier sections. 96 Several very early papers (e.g., <ref> [Mah61, IJ62] </ref>) discussed memory fragmentation, but in systems where segments could be compacted together or swapped to secondary storage when fragmentation became a problem; these papers generally do not give any quantitative results at all, and few qualitative results comparing different allocation strategies. 46 1960 to 1969. Overview.
Reference: [Mar82] <author> David Marr. </author> <title> Vision. </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: For the best fit policies, the general rule is "allocate objects in the smallest free block that's at least big enough to 13 This set of distinctions is doubtless indirectly influenced by work in very different areas, notably Marr's work in natural and artificial visual systems <ref> [Mar82] </ref> and Mc-Clamrock's work in the philosophy of science and cognition [McC91, McC95]. The distinctions are important for understanding a wide variety of complex systems, however. Similar distinctions are made in many fields, including empirical computer science, though often without making them quite clear.
Reference: [McC91] <author> Ronald McClamrock. </author> <title> Marr's three levels: a reevaluation. </title> <journal> Minds and Machines, </journal> <volume> 1 </volume> <pages> 185-196, </pages> <year> 1991. </year>
Reference-contexts: rule is "allocate objects in the smallest free block that's at least big enough to 13 This set of distinctions is doubtless indirectly influenced by work in very different areas, notably Marr's work in natural and artificial visual systems [Mar82] and Mc-Clamrock's work in the philosophy of science and cognition <ref> [McC91, McC95] </ref>. The distinctions are important for understanding a wide variety of complex systems, however. Similar distinctions are made in many fields, including empirical computer science, though often without making them quite clear. <p> The cost may not be negligible, however, especially if splitting and coalescing work too well|in viewed as a strategy or policy. The key point is that there are at least three qualitatively different kinds of levels of abstraction involved <ref> [McC91] </ref>; at the upper levels, there are is the general design goal of exploiting expected regularities, and a set of strategies for doing so; there may be subsidiary strategies, for example to resolve conflicts between strategies in the best possible way.
Reference: [McC95] <author> Ronald McClamrock. </author> <title> Existential Cognition: Computational Minds in the World. </title> <publisher> University of Chicago Press, </publisher> <year> 1995. </year>
Reference-contexts: rule is "allocate objects in the smallest free block that's at least big enough to 13 This set of distinctions is doubtless indirectly influenced by work in very different areas, notably Marr's work in natural and artificial visual systems [Mar82] and Mc-Clamrock's work in the philosophy of science and cognition <ref> [McC91, McC95] </ref>. The distinctions are important for understanding a wide variety of complex systems, however. Similar distinctions are made in many fields, including empirical computer science, though often without making them quite clear.
Reference: [McI82] <author> M. D. McIlroy. </author> <title> The number of states of a dynamic storage allocation system. </title> <journal> Computer Journal, </journal> <volume> 25(3) </volume> <pages> 388-392, </pages> <month> August </month> <year> 1982. </year>
Reference-contexts: We have also neglected mainly analytical work (e.g., <ref> [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85] </ref>) to some degree, be 94 This is not quite necessarily true. For applications that do little freeing, the initial carving of memory requested from the operating system will be a significant fraction of the allocation cost. <p> However, earlier results suggest that small grain sizes are preferred.) He suggests several techniques to make it easier to use somewhat larger models, but had little success with the few he tried. (See also <ref> [Ben81, Ree82, McI82] </ref>.) We are not optimistic that this approach is useful for realistic memory sizes, especially since memory sizes tend to increase rapidly over time. <p> Even with these extremely strong assumptions of randomness, this problem is combinatorially explosive. (This is true even when various symmetries and rotations are exploited to combine (exactly) equiva lent states <ref> [Ben81, McI82] </ref>.) We believe that the only way to make this kind of problem remotely tractable is with powerful abstractions over the possible states of memory.
Reference: [MK88] <author> Marshall Kirk McKusick and Michael J. Karels. </author> <title> Design of a general-purpose memory 75 allocator for the 4.3bsd UNIX kernel. </title> <booktitle> In Pro--ceedings of the Summer 1988 USENIX Conference, </booktitle> <address> San Francisco, California, </address> <month> June </month> <year> 1988. </year> <institution> USENIX Association. </institution>
Reference-contexts: To our knowledge, this is by far the most thorough review to date, but it should not be considered detailed or exhaustive; valuable points or papers may have escaped our notice. 95 We have left out work on concurrent and parallel al-locators (e.g., <ref> [GW82, Sto82, BAO85, MK88, EO88, For88, Joh91, JS92, JS92, MS93, Iye93] </ref>), which are beyond the scope of this paper. We have also neglected mainly analytical work (e.g., [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85]) to some degree, be 94 This is not quite necessarily true.
Reference: [Moo84] <author> David Moon. </author> <title> Garbage collection in a large Lisp system. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on LISP and Functional Programming, </booktitle> <pages> pages 235-246, </pages> <address> Austin, Texas, </address> <month> August </month> <year> 1984. </year> <note> ACM Press. </note>
Reference-contexts: David Moon reports that a similar facility in the Symbolics system often resulted in obscure bugs, and its use was discouraged after an efficient generational garbage collector <ref> [Moo84] </ref> was developed (Moon, personal communication 1995); generational techniques heuristically exploit the lifetime distributions of typical programs [LH83, Wil95].
Reference: [MPS71] <author> B. H. Margolin, R. P. Parme-lee, and M. Schatzoff. </author> <title> Analysis of free-storage algorithms. </title> <journal> IBM Systems Journal, </journal> <volume> 10(4) </volume> <pages> 283-304, </pages> <year> 1971. </year>
Reference-contexts: We then propose a new (though nearly obvious) conception of fragmentation and its causes, and describe more suitable techniques used to study it. (Most of the experiments using sound techniques have been performed in the last few years, but a few notable exceptions were done much earlier, e.g., <ref> [MPS71] </ref> and [LH82], discussed in Section 4.) 2.1 Internal and External Fragmentation Traditionally, fragmentation is classed as external or internal [Ran69], and is combatted by splitting and coalescing free blocks. <p> The most common way of doing this is to keep an array of free lists, often called "quick lists" or "subpools" <ref> [MPS71] </ref>, one for each size of block whose coalescing is to be deferred. Usually, this array is only large enough to have a separate free list for each individual size up to some maximum, such as 10 or 32 words; only those sizes will be treated by deferred coalescing [Wei76]. <p> Another possibility is to periodically flush the quick lists, returning all of the items on the quick lists to the general store for coalescing. This may be done incrementally, removing only the older items from the quick lists. In Margolin et al.'s scheme <ref> [MPS71] </ref>, the lengths of the free lists are bounded, and those lengths are based on the expected usage of different sizes. <p> memory usage, and block sizes infrequently equal to each other) the length of the free list will tend toward being about half the number of blocks actually in use. (All of these assumptions now appear to be false for most programs, as we will explain later in the discussions of <ref> [MPS71] </ref>, [ZG94] and [WJNB95]. <p> now it was no longer completely black but splintered badly among one side to show the original wood color, and in some places faded or stained. |Shirley Jackson, "The Lottery" Margolin et al. used real traces to study memory allocation in the CP-67 control program of an IBM System/360 mainframe <ref> [MPS71] </ref>. (Note that this allocator allocated storage used by the operating system itself, not for application programs.) They warned that examination of their system showed that several assumptions underlying the usual methodology were false, for their system's workload: uncorrelated sizes and lifetimes, independence of successive requests, and well-behaved distributions.
Reference: [MS93] <author> Paul E. McKenney and Jack Slingwine. </author> <title> Efficient kernel memory allocation on shared-memory multiprocessors. </title> <booktitle> In USENIX 1993 Winter Technical Conference, </booktitle> <address> San Diego, Cal-ifornia, </address> <month> January </month> <year> 1993. </year> <institution> USENIX Association. </institution>
Reference-contexts: To our knowledge, this is by far the most thorough review to date, but it should not be considered detailed or exhaustive; valuable points or papers may have escaped our notice. 95 We have left out work on concurrent and parallel al-locators (e.g., <ref> [GW82, Sto82, BAO85, MK88, EO88, For88, Joh91, JS92, JS92, MS93, Iye93] </ref>), which are beyond the scope of this paper. We have also neglected mainly analytical work (e.g., [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85]) to some degree, be 94 This is not quite necessarily true.
Reference: [Nel91] <author> Mark Nelson. </author> <title> The Data Compression Book. M & T Books, </title> <year> 1991. </year>
Reference-contexts: It also has the obvious disadvantage that instructions 48 Conventional text-string-oriented compression algorithms <ref> [Nel91] </ref> (e.g, UNIX compress or GNU gzip) work quite well, although we suspect that sophisticated schemes could do significantly better by taking advantage of the numerical properties of object identifiers or addresses; such schemes have been proposed for use in compressed paging and addressing [WLM91, FP91]. (Text-oriented compression generally makes Markov-like
Reference: [Nie77] <author> N. R. Nielsen. </author> <title> Dynamic memory allocation in computer simulation. </title> <journal> Communications of the ACM, </journal> 20(11) 864-873, November 1977. 
Reference-contexts: He also noted that the roving pointer optimization made next fit's worst case similarly bad|both best fit and next fit can suffer about as much from fragmentation as any allocator with general splitting and coalescing. Nielsen <ref> [Nie77] </ref> studied the performance of memory allocation algorithms for use in simulation programs. His main interest was in finding fast alloca-tors, rather than memory-efficient allocators. He used a variation of the usual random trace methodology intended to model the workloads generated by discrete-event simulation systems.
Reference: [OA85] <author> R. R. Oldehoeft and S. J. Allan. </author> <title> Adaptive exact-fit storage management. </title> <journal> Communications of the ACM, </journal> <volume> 28(5) </volume> <pages> 506-511, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: In Oldehoeft and Allan's system <ref> [OA85] </ref>, the number of quick lists varies over time, according to a FIFO or Working Set policy. This has an adaptive character, especially for the Working Set policy, in that sizes that have not been freed recently are quickly coalesced, while "active" sizes are not. <p> If access times are important, other considerations are likely to be much more significant, such as locality. (For rotating media and especially for tapes, placement has more important effects on speed than on space usage.) Oldehoeft and Allan <ref> [OA85] </ref> experimented with variants of deferred coalescing, using a working-set or FIFO policy to dynamically determine which sizes would be kept on quick lists for for deferred coalescing.
Reference: [Pag82] <author> Ivor P. </author> <title> Page. Optimal fit of arbitrary sized segments. </title> <journal> Computer Journal, </journal> <volume> 25(1), </volume> <month> January </month> <year> 1982. </year>
Reference-contexts: objects used by certain phases and intermingling them with objects used by other phases.) In several experiments using both real traces [WJNB95] and synthetic traces (e.g., [Bay77, Wei76, Pag84, KV85]), next fit has been shown to cause more other indexing structure. 60 This has also been observed by Ivor Page <ref> [Pag82] </ref> in randomized simulations, and similar (but possibly weaker) observations were made by Knuth and Shore and others in the late 1960's and 1970's. (Section 4.) 31 fragmentation than best fit or address-ordered first fit, and the LIFO-order variant may be significantly worse than address order [WJNB95]. <p> The simplest of these is when a program repeatedly does the following: 1. allocates a (short-lived) large object, 2. allocates a long-lived small object, and 63 This is not really optimal in any useful sense, of course. See also Page's critique in <ref> [Pag82] </ref> (Section 4.1). 64 Splay trees are particularly interesting for this application, since they have an adaptive characteristic that may adjust well to the patterns in allocator requests, as well as having amortized complexity within a constant factor of optimal [ST85]. 65 We suspect that earlier researchers often simply didn't worry <p> We have observed that best fit and address-ordered first fit perform quite similarly, for both real and synthetic traces. Page <ref> [Pag82] </ref> has observed that (for random traces using uniform distributions), the short-term placement choices made by best fit and address-ordered 68 It is interesting to note, however, that the direction of the address ordering matters for first fit, if the end block is viewed as the beginning of a very large <p> See also the discussion of <ref> [Pag82] </ref>, later in this section.) Purdom, Stigler, and Cheam [PSC71] introduced segregated fits using size classes with range lists (called "segregated storage" in their paper). The nature and importance of this efficient mechanism for best-fit-like policies was not generally appreciated by later researchers (an exception being Standish [Sta80]). <p> Deferred coalescing greatly improved the speed of their allocator, and usually decreased overall memory usage. Leverett and Hibbard also found that Knuth's roving pointer modification (i.e., next fit) was disappointing; search lengths did not decrease by much, and for some programs got longer. Page <ref> [Pag82] </ref> evaluated Campbell's "optimal fit" method analytically and in randomized trace simulations. (Page's version of optimal fit was somewhat different from Campbell's, of necessity, since Campbell's was intertwined with a particular application program structure.) Page showed that Campbell's analysis erred in assuming randomness in first-fit-like placement policies, and that systematicities in
Reference: [Pag84] <author> Ivor P. </author> <title> Page. Analysis of a cyclic placement scheme. </title> <journal> Computer Journal, </journal> <volume> 27(1) </volume> <pages> 18-25, </pages> <month> Jan-uary </month> <year> 1984. </year>
Reference-contexts: Worse, it may affect the locality of the program it allocates for, by scattering objects used by certain phases and intermingling them with objects used by other phases.) In several experiments using both real traces [WJNB95] and synthetic traces (e.g., <ref> [Bay77, Wei76, Pag84, KV85] </ref>), next fit has been shown to cause more other indexing structure. 60 This has also been observed by Ivor Page [Pag82] in randomized simulations, and similar (but possibly weaker) observations were made by Knuth and Shore and others in the late 1960's and 1970's. (Section 4.) 31 <p> [Tad78], he reported that an experimental evaluation showed this scheme to perform quite similarly to best fit|which is not surprising, because it is best fit, in policy terms|and that it was fast. (These experiments used the usual synthetic trace methodology, and Standish summarized some of Weinstock's results as well.) Page <ref> [Pag84] </ref> analyzed a "cyclic placement" policy similar to next fit, both analytically and in randomized simulations. (Only uniformly distributed sizes and lifetimes were used.) The cyclic placement scheme generally resulted in significantly more fragmentation than first fit or best fit. "...over in the north village they're talking of giving up the
Reference: [PH86] <author> Ivor P. Page and Jeff Hagins. </author> <title> Improving the performance of buddy systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(5):441-447, </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: Double buddies. Double buddy systems use a different technique to allow a closer spacing of size classes <ref> [Wis78, PH86, WJNB95] </ref>. They use two different binary buddy systems, with staggered sizes. For example, one buddy system may use powers-of-two sizes (2, 4, 8, 16...) while another uses a powers-of-two spacing starting at a different size, such as 3. (The resulting sizes are 3, 6, 12, 24 ...). <p> Wise, in an unpublished technical report [Wis78], described a double buddy system and its advantages over Fibonacci systems in terms of external fragmentation (producing free blocks of the same size as requested blocks). This report apparently went unnoticed until well after double buddy was reinvented by Page and Hagins <ref> [PH86] </ref>. 118 Reeves [Ree79, Ree80, Ree82, Ree83] used analytic techniques to determine the effect of a random fit allocator policy in the face of random workloads, using a "generating function" approach originated by Knuth [Knu73]. <p> Page and Hagins <ref> [PH86] </ref> provided the first published double buddy system, and experimentally compared it to binary and weighted buddy systems. Using the standard simulation techniques, and only uniformly distributed sizes and lifetimes, they show that double buddies suffer from somewhat less fragmentation than binary and weighted buddies.
Reference: [PLD91] <institution> Proceedings of the 1991 SIGPLAN Conference on Programming Language Design and Implementation, Toronto, </institution> <address> Ontario, </address> <month> June </month> <year> 1991. </year> <note> ACM Press. Published as SIGPLAN Notices 26(6), </note> <month> June </month> <year> 1992. </year>
Reference: [PLD93] <institution> Proceedings of the 1993 SIGPLAN Conference on Programming Language Design and Implementation, </institution> <address> Albuquerque, New Mexico, June 1993. </address> <publisher> ACM Press. </publisher>
Reference: [PN77] <author> J. L. Peterson and T. A. Norman. </author> <title> Buddy systems. </title> <journal> Communications of the ACM, </journal> <volume> 20(6) </volume> <pages> 421-431, </pages> <month> June </month> <year> 1977. </year>
Reference-contexts: and policies to implement almost any desired strategy. (It seems likely that the original version of boundary tags was initially viewed as too costly in space, in a time when memory was a very scarce resource, and the footer optimization [Sta80] simply never became well-known.) 3.7 Buddy Systems Buddy systems <ref> [Kno65, PN77] </ref> are a variant of segregated lists that supports a limited but efficient kind of splitting and coalescing. <p> On the other hand, systems based on closer size class spacings may be similarly efficient if lookup tables are used to perform size class mappings quickly. A major problem with binary buddies is that internal fragmentation is usually relatively high|the expected case is (very roughly) about 28% <ref> [Knu73, PN77] </ref>, 81 because any object size must be rounded up to the nearest power of two (minus a word for the header, if the size field is stored). Fibonacci buddies. <p> As with binary buddies, the increasing size of successive size ranges limits the number of free lists required. A further refinement, called generalized Fibonacci buddies <ref> [Hir73, Bur76, PN77] </ref> uses a Fibonacci-like number series that starts with a larger number and generates a somewhat more closely-spaced set of sizes. <p> A possible disadvantage of Fibonacci buddies is that when a block is split to satisfy a request for a particular size, the remaining block is of a different 81 This figure varies somewhat depending on the expected range and skew of the size distribution <ref> [PN77] </ref>. 39 size, which is less likely to be useful if the program allocates many objects of the same size [Wis78]. Weighted buddies.Weighted buddy systems [SP74] use a different kind of size class series than either binary or Fibonacci buddy systems. <p> Delacroix said to Mrs. Graves in the back row. |Shirley Jackson, "The Lottery" Peterson and Norman <ref> [PN77] </ref> described a very general class of buddy systems, and experimentally compared several varieties of buddy systems: binary, Fibonacci, a generalized Fibonacci [HS64, Fer76], and weighted. They used the usual random trace methodology, with both synthetic (uniform and exponential) and real size distributions. <p> synthetic traces based on real lifetime distributions, primarily from two installations of the same IBM operating system, VM-SP. (Their main goal was to develop an efficient allocator for that system.) They 122 This tailoring of list length should not be confused with the tailoring of size classes as mentioned in <ref> [PN77] </ref>. 123 The tailored list scheme worked better than the recombination delaying scheme, but this result is especially suspect; the tailored list scheme does not respond dynamically to the changing characteristics of the workload, but this weakness is not stressed by an artificial trace without significant phase behavior. also measured the
Reference: [PS70] <author> P.W. Purdom and S. M. </author> <title> Stigler. Statistical properties of the buddy system. </title> <journal> Journal of the ACM, </journal> <volume> 17(4) </volume> <pages> 683-697, </pages> <month> October </month> <year> 1970. </year>
Reference-contexts: Gelenbe also derived a similar "two thirds rule" [Gel71] in a somewhat different way. (These essentially identical rules are both subject to the same criticisms as Knuth's original rule.) Purdom and Stigler <ref> [PS70] </ref> performed statistical analyses of the binary buddy system, and argued that limitations on buddy system coalescing were seldom a problem. Their model was based on strong assumptions of independence and randomness in the workload, including exponentially distributed random lifetimes.
Reference: [PSC71] <author> P. W. Purdom, S. M. Stigler, and Tat-Ong Cheam. </author> <title> Statistical investigation of three storage allocation algorithms. </title> <journal> BIT, </journal> <volume> 11 </volume> <pages> 187-195, </pages> <year> 1971. </year>
Reference-contexts: segregated storage because the result is that pages (or some other relatively large unit) contain blocks of only one size class. (This differs from the traditional terminology in an important way. "Segregated storage" is commonly used to refer both to this kind of scheme and what we call segregated fits <ref> [PSC71] </ref>. <p> of the tree, and it acts as the head of the doubly-linked list of same-sized blocks. 37 fall into size classes is to allow the lists to con-tain blocks of slightly different sizes, and search the size lists sequentially, using the classic best fit, first fit, or next fit technique <ref> [PSC71] </ref>. (The choice affects the policy implemented, of course, though probably much less than in the case of a single free list.) This could introduce a linear component to search times, though this does not seem likely to be a common problem in practice, at least if size classes are closely <p> An efficient segregated fits scheme with general coalescing (using boundary tags) was described and shown to perform well in 1971 <ref> [PSC71] </ref>, but it did not become well-known; Standish and Tadman's apparently better scheme was published (but only in a textbook) in 1980, and similarly did not become particularly well known, even to the present. <p> See also the discussion of [Pag82], later in this section.) Purdom, Stigler, and Cheam <ref> [PSC71] </ref> introduced segregated fits using size classes with range lists (called "segregated storage" in their paper). The nature and importance of this efficient mechanism for best-fit-like policies was not generally appreciated by later researchers (an exception being Standish [Sta80]).
Reference: [Put77] <author> Hilary Putnam. </author> <title> Meaning and reference. </title> <editor> In Stephen P. Schwartz, editor, </editor> <title> Naming, Necessity, and Natural Kinds. </title> <publisher> Cornell University Press, </publisher> <address> Ithaca, New York, </address> <year> 1977. </year>
Reference-contexts: We think that this is a strength, however, because it is better to leave a concept somewhat vague than to define it prematurely and incorrectly. It is important to first identify the "natural kinds" in the phenomena under study, and then figure out what their most important characteristics are <ref> [Kri72, Put77, Qui77] </ref>. (We are currently working on developing operational measures of "fragmentation-related" program behavior.) Later in the paper we will express experimental "fragmentation" results as percentages, but this should be viewed as an operational shorthand for the effects of fragmentation on memory usage at whatever point or points in program
Reference: [Qui77] <author> W. V. Quine. </author> <title> Natural kinds. </title> <editor> In Stephen P. Schwartz, editor, </editor> <title> Naming, Necessity, and Natural Kinds. </title> <publisher> Cornell University Press, </publisher> <address> Ithaca, New York, </address> <year> 1977. </year>
Reference-contexts: We think that this is a strength, however, because it is better to leave a concept somewhat vague than to define it prematurely and incorrectly. It is important to first identify the "natural kinds" in the phenomena under study, and then figure out what their most important characteristics are <ref> [Kri72, Put77, Qui77] </ref>. (We are currently working on developing operational measures of "fragmentation-related" program behavior.) Later in the paper we will express experimental "fragmentation" results as percentages, but this should be viewed as an operational shorthand for the effects of fragmentation on memory usage at whatever point or points in program
Reference: [Ran69] <author> Brian Randell. </author> <title> A note on storage fragmentation and program segmentation. </title> <journal> Communications of the ACM, </journal> <volume> 12(7) </volume> <pages> 365-372, </pages> <month> July </month> <year> 1969. </year>
Reference-contexts: techniques used to study it. (Most of the experiments using sound techniques have been performed in the last few years, but a few notable exceptions were done much earlier, e.g., [MPS71] and [LH82], discussed in Section 4.) 2.1 Internal and External Fragmentation Traditionally, fragmentation is classed as external or internal <ref> [Ran69] </ref>, and is combatted by splitting and coalescing free blocks. External fragmentation arises when free blocks of memory are available for allocation, but can't be used to hold objects of the sizes actually requested by a program. <p> Nonetheless, next fit became quite popular in real systems. It is unclear whether this is because next fit seems more obviously scalable, or simply because Knuth seemed to favor it and his book was so widely used. Randell <ref> [Ran69] </ref> defined internal and external fragmentation, and pointed out that internal fragmentation can be traded for reduced external fragmentation by allocating memory in multiples of some grain size g; this reduces the effective number of sizes and increases the chances of finding a fit.
Reference: [Ree79] <author> C. M. Reeves. </author> <title> Free store distribution under random-fit allocation. </title> <journal> Computer Journal, </journal> <volume> 22(4) </volume> <pages> 346-351, </pages> <month> November </month> <year> 1979. </year>
Reference-contexts: We have also neglected mainly analytical work (e.g., <ref> [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85] </ref>) to some degree, be 94 This is not quite necessarily true. For applications that do little freeing, the initial carving of memory requested from the operating system will be a significant fraction of the allocation cost. <p> This report apparently went unnoticed until well after double buddy was reinvented by Page and Hagins [PH86]. 118 Reeves <ref> [Ree79, Ree80, Ree82, Ree83] </ref> used analytic techniques to determine the effect of a random fit allocator policy in the face of random workloads, using a "generating function" approach originated by Knuth [Knu73].
Reference: [Ree80] <author> C. M. Reeves. </author> <title> Free store distribution under random-fit allocation: Part 2. </title> <journal> Computer Journal, </journal> <volume> 23(4) </volume> <pages> 298-306, </pages> <month> November </month> <year> 1980. </year>
Reference-contexts: We have also neglected mainly analytical work (e.g., <ref> [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85] </ref>) to some degree, be 94 This is not quite necessarily true. For applications that do little freeing, the initial carving of memory requested from the operating system will be a significant fraction of the allocation cost. <p> This report apparently went unnoticed until well after double buddy was reinvented by Page and Hagins [PH86]. 118 Reeves <ref> [Ree79, Ree80, Ree82, Ree83] </ref> used analytic techniques to determine the effect of a random fit allocator policy in the face of random workloads, using a "generating function" approach originated by Knuth [Knu73].
Reference: [Ree82] <author> C. M. Reeves. </author> <title> A lumped-state model of clustering in dynamic storage allocation. </title> <journal> Computer Journal, </journal> <volume> 27(2) </volume> <pages> 135-142, </pages> <year> 1982. </year>
Reference-contexts: We have also neglected mainly analytical work (e.g., <ref> [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85] </ref>) to some degree, be 94 This is not quite necessarily true. For applications that do little freeing, the initial carving of memory requested from the operating system will be a significant fraction of the allocation cost. <p> This report apparently went unnoticed until well after double buddy was reinvented by Page and Hagins [PH86]. 118 Reeves <ref> [Ree79, Ree80, Ree82, Ree83] </ref> used analytic techniques to determine the effect of a random fit allocator policy in the face of random workloads, using a "generating function" approach originated by Knuth [Knu73]. <p> However, earlier results suggest that small grain sizes are preferred.) He suggests several techniques to make it easier to use somewhat larger models, but had little success with the few he tried. (See also <ref> [Ben81, Ree82, McI82] </ref>.) We are not optimistic that this approach is useful for realistic memory sizes, especially since memory sizes tend to increase rapidly over time.
Reference: [Ree83] <author> C. M. Reeves. </author> <title> Free store distribution under random-fit allocation, part 3. </title> <journal> Computer Journal, </journal> <volume> 26(1) </volume> <pages> 25-35, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: This report apparently went unnoticed until well after double buddy was reinvented by Page and Hagins [PH86]. 118 Reeves <ref> [Ree79, Ree80, Ree82, Ree83] </ref> used analytic techniques to determine the effect of a random fit allocator policy in the face of random workloads, using a "generating function" approach originated by Knuth [Knu73].
Reference: [Rei94] <author> Mark B. Reinhold. </author> <title> Cache performance of garbage-collected programs. </title> <booktitle> In Proceedings of the 1994 SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 206-217, </pages> <address> Orlando, Florida, June 1994. </address> <publisher> ACM Press. </publisher>
Reference-contexts: The literature on garbage collection is considerably more sophisticated in terms of locality studies than the literature on memory allocation, and should not be overlooked. (See, e.g., <ref> [Bae73, KLS92, Wil90, WLM92, DTM93, Rei94, GA95, Wil95] </ref>.) Many of the same issues must arise in conventionally-managed heaps as well. 26 larger free blocks (coalescing). Equally important are the policy and strategy implications|i.e., whether the allocator properly exploits the regularities in real request streams.
Reference: [RO91] <author> Mendel Rosenblum and John K. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 1-15, </pages> <address> Pacific Grove, Califor-nia, </address> <month> October </month> <year> 1991. </year> <note> ACM Press. Published as Operating Systems Review 25(5). </note>
Reference-contexts: note that for secondary and tertiary storage more generally, contiguous storage is not strictly required; freedom from this restriction allows schemes that are much more flexible and less vulnerable to fragmentation. (Many systems divide all files into blocks of one or two fixed sizes, and only preserve logical contiguity (e.g., <ref> [RO91, VC90, SKW92, CG91, AS95] </ref>).
Reference: [Rob71] <author> J. M. Robson. </author> <title> An estimate of the store size necessary for dynamic storage allocation. </title> <journal> Journal of the ACM, </journal> <volume> 18(3) </volume> <pages> 416-423, </pages> <month> July </month> <year> 1971. </year>
Reference-contexts: It has been proven that for any possible allocation algorithm, there will always be the possibility that some application program will allocate and deallocate blocks in some fashion that defeats the allocator's strategy, and forces it into severe fragmentation <ref> [Rob71, GGU72, Rob74, Rob77] </ref>. Not only are there no provably good allocation algorithms, there are proofs that any allocator will be "bad" for some possible applications. <p> fragmentation is generally proportional to the amount of live data 12 multiplied by the logarithm of the ratio between the largest and smallest block sizes, i.e., M log 2 n, where M is the amount of live data and n is the ratio between the smallest and largest object sizes <ref> [Rob71] </ref>. (In discussing worst-case memory costs, we generally assume that all block sizes are evenly divisible by the smallest block size, and n is sometimes simply called "the largest block size," i.e., in units of the smallest.) Of course, for some algorithms, the worst case is much worse, often proportional to <p> In actual tests in the real system, time spent in memory management was cut by about a factor of six. Robson <ref> [Rob71] </ref> showed that the worst-case performance of a worst-case-optimal algorithm is bounded from below by a function that rises logarithmically with the ratio n (the ratio of the largest and smallest block sizes), i.e., M log 2 n times a constant.
Reference: [Rob74] <author> J. M. Robson. </author> <title> Bounds for some functions concerning dynamic storage allocation. </title> <journal> Journal of the ACM, </journal> <volume> 21(3) </volume> <pages> 491-499, </pages> <month> July </month> <year> 1974. </year>
Reference-contexts: It has been proven that for any possible allocation algorithm, there will always be the possibility that some application program will allocate and deallocate blocks in some fashion that defeats the allocator's strategy, and forces it into severe fragmentation <ref> [Rob71, GGU72, Rob74, Rob77] </ref>. Not only are there no provably good allocation algorithms, there are proofs that any allocator will be "bad" for some possible applications. <p> allowing Fibonacci-like series where each size was the sum of the previous size and a size a fixed distance further down in the size series. (For some fixed integer k, the ith size in the series may be split into two blocks of sizes i 1 and i k.) Robson <ref> [Rob74] </ref> put a fairly tight upper and lower bounds on the worst-case performance of the best possible allocation algorithm. He showed that a worst-case-optimal strategy's worst-case memory usage was somewhere between 0:5M log 2 n and about 0:84M log 2 n.
Reference: [Rob77] <author> J. M. Robson. </author> <title> Worst case fragmentation of first fit and best fit storage allocation strategies. </title> <journal> Computer Journal, </journal> <volume> 20(3) </volume> <pages> 242-244, </pages> <month> August </month> <year> 1977. </year>
Reference-contexts: It has been proven that for any possible allocation algorithm, there will always be the possibility that some application program will allocate and deallocate blocks in some fashion that defeats the allocator's strategy, and forces it into severe fragmentation <ref> [Rob71, GGU72, Rob74, Rob77] </ref>. Not only are there no provably good allocation algorithms, there are proofs that any allocator will be "bad" for some possible applications. <p> The worst-case performance of best fit is poor, with its memory usage proportional to the product of the amount of allocated data and the ratio between the largest and smallest object size (i.e., M n) <ref> [Rob77] </ref>. This appears not to happen in practice, or at least not commonly. First fit. <p> Peterson and Norman found that these buddy systems all had similar memory usage; the decreases in internal fragmentation due to more-refined size series were usually offset by similar increases in external fragmentation. Robson <ref> [Rob77] </ref> showed that the worst-case performance of address-ordered first fit is about M log 2 n, while best fit's is far worse, at about M n.
Reference: [Ros61] <author> D. T. Ross. </author> <title> A generalized technique for symbol manipulation and numerical calculation. </title> <journal> Communications of the ACM, </journal> <volume> 4(3) </volume> <pages> 147-150, </pages> <month> March </month> <year> 1961. </year>
Reference-contexts: of logical (program and data) segments to physical memory. 97 By the mid-1960's, the problem of managing storage for different-sized objects within the address space of a single process was also recognized as an important one, largely due to the increasing use (and sophistication) of list processing techniques and languages <ref> [Ros61, Com64, BR64] </ref>. 98 Equally important, the 1960's saw the invention of the now-traditional methodology for allocator evaluation. In early papers, the assumptions underlying this scheme were explicit and warned against, but as the decade progressed, the warnings decreased in frequency and seriousness.
Reference: [Ros67] <author> D. T. Ross. </author> <title> The AED free storage package. </title> <journal> Communications of the ACM, </journal> <volume> 10(8) </volume> <pages> 481-492, </pages> <month> August </month> <year> 1967. </year>
Reference-contexts: Many programs build up data structures quickly, and then use those data structures for long periods (often nearly the whole running time of the program). These patterns are well-known, from anecdotal experience by many people (e.g., <ref> [Ros67, Han90] </ref>), from research on garbage collection (e.g., [Whi80, WM89, UJ88, Hay91, Hay93, BZ95, Wil95]), 32 and from a re cent study of C and C++ programs [WJNB95]. 32 It may be thought that garbage collected systems are sufficiently different from those using conventional storage management that these results are not <p> It is more important to determine what regularities exist in real program behavior, and only then decide which strategies are most 36 We have not found any other mention of these heuristics in the literature, although somewhat similar ideas underlie the "zone" allocator of Ross <ref> [Ros67] </ref> and Hanson's "obstack" system (both discussed later). Beck [Bec82], Demers et al. [DWH + 90], and and Barrett and Zorn [BZ93] have developed systems that predict the lifetimes of objects for similar purposes. <p> Markowitz in the Simscript system around 1963. Knowlton also suggested the use of deferred coalescing to avoid unneeded overheads in the common case where objects of the same size were frequently used. Ross, in <ref> [Ros67] </ref> described a sophisticated storage management system for the AED engineering design support system. While no empirical results were reported, Ross describes different patterns of memory usage that programs may exhibit, such as mostly monotonic accumulation (ramps), and fragmentation caused by different characteristic lifetimes of different-sized objects.
Reference: [Rus77] <author> D. L. Russell. </author> <title> Internal fragmentation in a class of buddy systems. </title> <journal> SIAM J. Comput., </journal> <volume> 76 6(4) </volume> <pages> 607-621, </pages> <month> December </month> <year> 1977. </year>
Reference-contexts: Most empirical studies used synthetic trace techniques, which were refined as more information about real lifetime and size distributions became available, 107 We have not yet obtained a copy of this report| our information is taken from <ref> [Rus77] </ref> and other secondary sources. <p> He did not consider possible systematicities in the application program's allocations and releases, such as patterned births and deaths. (He did aptly note that "the dynamics of memory usage comprise complicated phenomena in which observable effects often have subtle causes.") Russell <ref> [Rus77] </ref> attempted to derive formulas for expected fragmentation in a Fibonacci and a generalized Fibonacci buddy system, 117 based on the assumption that size distributions followed a generalization of Zipf's law (i.e., a decreasing function inversely related to the sizes).
Reference: [Sam89] <author> A. Dain Samples. Mache: </author> <title> No-loss trace compaction. </title> <booktitle> In ACM SIGMETRICS, </booktitle> <pages> pages 89-97, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: This is clearly true to a large degree for allocation and reference traces, but other regularities could probably be exploited as well [WB95].) Dain Samples <ref> [Sam89] </ref> used a simple and effective approach for compressing memory-reference traces; his "Mache" trace compactor used a simple preprocessor to massage the trace into a different format, making the the relevant regularities easier for standard string-oriented compression algorithms to recognize and exploit.
Reference: [Sha88] <author> Robert A. Shaw. </author> <title> Empirical Analysis of a Lisp System. </title> <type> PhD thesis, </type> <institution> Stanford University, Palo Alto, California, </institution> <month> February </month> <year> 1988. </year> <type> Technical Report CSL-TR-88-351, </type> <institution> Stanford University Computer Systems Laboratory. </institution>
Reference-contexts: ordering, but it is unknown whether that applies to items scure issues of reentrancy|the interrupt handler must be careful not to do anything that would interfere with an allocation or deallocation that is interrupted.) 91 This is similar to the "bucket brigade" advancement technique used in some generational garbage collectors <ref> [Sha88, WM89, Wil95] </ref>. A somewhat similar technique is used in Lea's allocator, but for a different purpose.
Reference: [Sho75] <author> J. E. Shore. </author> <title> On the external storage fragmentation produced by first-fit and best-fit allocation strategies. </title> <journal> Communications of the ACM, </journal> <volume> 18(8) </volume> <pages> 433-440, </pages> <month> August </month> <year> 1975. </year>
Reference-contexts: Shore <ref> [Sho75] </ref> designed and implemented a hybrid best fit/first fit policy that outperformed either plain first fit or plain best fit for his randomized workloads. (Discussed in Section 4.1.) The strategic implications of this hybrid policy have not been explored, and it is unclear whether they apply to real workloads. <p> Cranston and Thomas [CT75] presented a method for quickly finding the buddy of a block in various buddy systems, using only three bits per block. This reduces the time cost of splitting and merging relative to Hirschberg's scheme, as well as incurring minimal space cost. Shore <ref> [Sho75] </ref> compared best fit and address-ordered first fit more thoroughly than had been done previously, and also experimented with worst-fit and a novel hybrid of best fit and first fit. He used the then-standard methodology, generating random synthetic traces with (only) uniformly distributed lifetimes. <p> He also showed that the free list for first fit tended toward being roughly sorted in size order. (See also similar but possibly weaker claims in <ref> [Sho75] </ref>, discussed earlier.) 120 A possibly misleading passage says that memory is freed "explicitly," but that is apparently referring to a level of abstraction below the reference counting mechanism.
Reference: [Sho77] <author> J. E. Shore. </author> <title> Anomalous behavior of the fifty-percent rule in dynamic memory allocation. </title> <journal> Communications of the ACM, </journal> 20(11) 558-562, November 1977. 
Reference-contexts: Best fit and address-ordered first fit were among the policies eliminated.) Of the surviving seven allocators, six had poor memory usage. The seventh allocator, which performed quite well in terms of both speed and memory usage, was "multiple free lists," i.e., segregated fits with exact lists. In <ref> [Sho77] </ref>, Shore analyzed address-ordered first fit theoretically, and showed that the allocator itself violates a statistical assumption underlying Knuth's fifty percent rule.
Reference: [SKW92] <author> Vivek Singhal, Sheetal V. Kakkad, and Paul R. Wil-son. </author> <title> Texas: an efficient, portable persistent store. </title> <editor> In Antonio Albano and Ron Morrison, editors, </editor> <booktitle> Fifth International Workshop on Persistent Object Systems, </booktitle> <pages> pages 11-33, </pages> <address> San Mini-ato, Italy, </address> <month> September </month> <year> 1992. </year> <note> Springer-Verlag. </note>
Reference-contexts: that case, the allocator must be designed to work with a potentially non-contiguous set of pages, because there may be intervening pages that belong to different routines. (For example, our Texas persistent store allows the data segment to contain interleaved pages belonging to a persistent heap and a transient heap <ref> [SKW92] </ref>.) Despite this possible interleaving of pages used by different modules, extending the heap will typically just extend the "wilderness block," because it's more likely that successive extensions of the data segment are due to requests by the allocator, than that memory requests from different sources are interleaved. is not viewed <p> a somewhat more refined series, it is still generally logarithmic, but with a larger constant factor.) In terms of policy, this search order means that smaller blocks are used in preference to larger ones, 74 This invariant can be useful in some kinds of systems, especially systems that provide persistence <ref> [SKW92] </ref> and/or garbage collection for languages such as C or C++ [BW88, WDH89, WJ93], where pointers may point into the interior parts of objects, and it is important to be able to find the object headers quickly. <p> note that for secondary and tertiary storage more generally, contiguous storage is not strictly required; freedom from this restriction allows schemes that are much more flexible and less vulnerable to fragmentation. (Many systems divide all files into blocks of one or two fixed sizes, and only preserve logical contiguity (e.g., <ref> [RO91, VC90, SKW92, CG91, AS95] </ref>).
Reference: [SP74] <author> K. K. Shen and J. L. Peterson. </author> <title> A weighted buddy method for dynamic storage allocation. </title> <journal> Communications of the ACM, </journal> <volume> 17(10) </volume> <pages> 558-562, </pages> <month> October </month> <year> 1974. </year>
Reference-contexts: Weighted buddies.Weighted buddy systems <ref> [SP74] </ref> use a different kind of size class series than either binary or Fibonacci buddy systems. Some size classes can be split only one way, while other size classes can be split in two ways. <p> He showed that a worst-case-optimal strategy's worst-case memory usage was somewhere between 0:5M log 2 n and about 0:84M log 2 n. Shen and Peterson introduced the weighted buddy method <ref> [SP74] </ref>, whose allowable block sizes are either powers of two, or three times a power of two. 52 They compared this scheme to binary buddy, using the synthetic trace methodology; they used only a uniform lifetime distributions, and only two size distributions, both smooth (uniform and exponential).
Reference: [ST85] <author> Daniel Dominic Sleator and Robert Endre Tarjan. </author> <title> Self-adjusting binary search trees. </title> <journal> Journal of the ACM, </journal> <volume> 32(3), </volume> <year> 1985. </year>
Reference-contexts: See also Page's critique in [Pag82] (Section 4.1). 64 Splay trees are particularly interesting for this application, since they have an adaptive characteristic that may adjust well to the patterns in allocator requests, as well as having amortized complexity within a constant factor of optimal <ref> [ST85] </ref>. 65 We suspect that earlier researchers often simply didn't worry about this because memory sizes were quite small (and block sizes were often rather large). <p> They were interested in both time and space costs, and in scalability to large heaps. Five of their allocators were variants of next fit. 124 The others included simple segregated storage (with powers of two size classes) 125 address-ordered first fit (using a self-adjusting "splay" tree <ref> [ST85] </ref>), segregated fits (using Fibonacci-spaced size classes), better fit (using Stephenson's Cartesian tree scheme), and two best fit algorithms (one using a balanced binary tree, and the other a splay tree).
Reference: [Sta80] <author> Thomas Standish. </author> <title> Data Structure Techniques. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1980. </year>
Reference-contexts: Blocks are allocated "off by one" from the doubleword boundary, so that the part of the block that actually stores an object is properly aligned. 52 This optimization is described in <ref> [Sta80] </ref>, but it appears not to have been noticed and exploited by most imple block is in use (holding a live object), the size field in the footer is not actually needed|all that is needed is the flag bit saying that the storage is unavailable for coalescing. <p> Our taxonomy is therefore roughly similar to some previous ones (particularly Standish's <ref> [Sta80] </ref>), but more complete.) The basic allocator mechanisms we discuss are: Sequential Fits, including first fit, next fit, best fit, and worst fit, Segregated Free Lists, including simple segregated storage and segregated fits, Buddy Systems, including conventional binary, weighted, and Fibonacci buddies, and double buddies, Indexed Fits, which use structured indexes <p> A crude but possibly effective form of coalescing for 73 Simple segregated storage is sometimes incorrectly called a buddy system; we do not use that terminology because simple segregated storage does not use a buddy rule for coalescing|no coalescing is done at all. (Standish <ref> [Sta80] </ref> refers to simple segregated storage as "partitioned storage.") 36 simple segregated storage (used by Mike Haertel in a fast allocator [GZH93, Vo95], and in several garbage collectors [Wil95]) is to maintain a count of live objects for each page, and notice when a page is entirely empty. <p> Standish and Tadman's "Fast Fits" scheme 75 uses an array of free lists for small size classes, plus a binary tree of free lists for larger sizes (but only the ones that actually occur) <ref> [Sta80, Tad78] </ref>. 76 2. Strict Size Classes with Rounding. When sizes are grouped into size classes (e.g., powers of two), one approach is to maintain an invariant that all blocks on a size list are exactly of the same size. <p> This frees the designer to use more sophisticated higher-level mechanisms and policies to implement almost any desired strategy. (It seems likely that the original version of boundary tags was initially viewed as too costly in space, in a time when memory was a very scarce resource, and the footer optimization <ref> [Sta80] </ref> simply never became well-known.) 3.7 Buddy Systems Buddy systems [Kno65, PN77] are a variant of segregated lists that supports a limited but efficient kind of splitting and coalescing. <p> The nature and importance of this efficient mechanism for best-fit-like policies was not generally appreciated by later researchers (an exception being Standish <ref> [Sta80] </ref>). This may be because their paper's title gave no hint that a novel algorithm was presented. Purdom et al. used the random trace methodology to compare first fit, binary buddy, and segregated fits. (It is unclear which kind of first fit was used, e.g., LIFO-ordered or address-ordered). <p> Among the more interesting designs from this period are Standish and Tadman's exact lists scheme, Page and Hagins' double buddy system, Beck's age-match algorithm, and Hanson's obstack system. Standish surveyed memory allocation research in a (short) chapter of a book on data structures <ref> [Sta80] </ref>, describing segregated fits and introducing a segregated free lists method using exact lists.
Reference: [Ste83] <author> C. J. Stephenson. </author> <title> Fast fits: New methods for dynamic storage allocation. </title> <booktitle> In Proceedings of the Ninth Symposium on Operating Systems Principles, </booktitle> <pages> pages 30-32, </pages> <address> Bretton Woods, New Hampshire, </address> <month> October </month> <year> 1983. </year> <note> ACM Press. Published as Operating Systems Review 17(5), Oc-tober 1983. </note>
Reference-contexts: Such philosophical fine points aside, there is the practical question of how to treat a virgin block of significant size, to minimize fragmentation. (This block is sometimes called "wilderness" <ref> [Ste83] </ref> to signify that it is as yet unspoiled.) Consider what happens if a first fit or next fit policy is being used. <p> The best-known example of an indexed fits scheme is probably Stephenson's "Fast Fits" allocator <ref> [Ste83] </ref>, which uses a Cartesian tree sorted on both size and address. A Cartesian tree [Vui80] encodes two-dimensional information in a binary tree, using two constraints on the tree shape. It is effectively sorted on a primary key and a secondary key. <p> In this case, the age-match heuristic systematically failed, because in that case the age of an object is negatively correlated with the time un 59 til it will die. This should not be surprising. (In this case it might work to reverse the order of estimated death times.) Stephenson <ref> [Ste83] </ref> introduced the "Fast Fits" technique, using a Cartesian tree of free blocks ordered primarily by address and secondarily by block size. He evaluated the leftmost fit (address-ordered first fit) and better fit variants experimentally.
Reference: [Sto82] <author> Harold S. Stone. </author> <title> Parallel memory allocation using the FETCH-AND-ADD instruction. </title> <type> Technical report, </type> <institution> IBM Thomas J. Watson Research Center, </institution> <address> Yorktown Heights, New York, </address> <month> November </month> <year> 1982. </year>
Reference-contexts: To our knowledge, this is by far the most thorough review to date, but it should not be considered detailed or exhaustive; valuable points or papers may have escaped our notice. 95 We have left out work on concurrent and parallel al-locators (e.g., <ref> [GW82, Sto82, BAO85, MK88, EO88, For88, Joh91, JS92, JS92, MS93, Iye93] </ref>), which are beyond the scope of this paper. We have also neglected mainly analytical work (e.g., [Kro73, Bet73, Ree79, Ree80, McI82, Ree82, BCW85]) to some degree, be 94 This is not quite necessarily true.
Reference: [Tad78] <author> M. Tadman. Fast-fit: </author> <title> A new hierarchical dynamic storage allocation technique. </title> <type> Master's thesis, </type> <institution> UC Irvine, Computer Science Dept., </institution> <year> 1978. </year>
Reference-contexts: Standish and Tadman's "Fast Fits" scheme 75 uses an array of free lists for small size classes, plus a binary tree of free lists for larger sizes (but only the ones that actually occur) <ref> [Sta80, Tad78] </ref>. 76 2. Strict Size Classes with Rounding. When sizes are grouped into size classes (e.g., powers of two), one approach is to maintain an invariant that all blocks on a size list are exactly of the same size. <p> Standish surveyed memory allocation research in a (short) chapter of a book on data structures [Sta80], describing segregated fits and introducing a segregated free lists method using exact lists. Citing Tad-man's masters thesis <ref> [Tad78] </ref>, he reported that an experimental evaluation showed this scheme to perform quite similarly to best fit|which is not surprising, because it is best fit, in policy terms|and that it was fast. (These experiments used the usual synthetic trace methodology, and Standish summarized some of Weinstock's results as well.) Page [Pag84]
Reference: [Thi89] <author> Dominique Thiebaut. </author> <title> The fractal dimension of computer programs and its application to the prediction of the cache miss ratio. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1012-1026, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Typically, "fractal" models of program behavior are not infinitely recursive, and are actually graftals or other finite fractal-like recursive entities. 34 We believe that this applies to studies of locality of reference as well. Attempts to characterize memory referencing behavior as fractal-like (e.g., <ref> [VMH + 83, Thi89] </ref>) are ill-conceived or severely limited|if only because memory allocation behavior is not generally fractal, and memory-referencing behavior depends on memory al Ramps, peaks, and plateus have very different implications for fragmentation.
Reference: [Tot65] <author> R. A. Totschek. </author> <title> An empirical investigation into the behavior of the SDC timesharing system. </title> <type> Technical Report SP2191, </type> <institution> Systems Development Corporation, </institution> <year> 1965. </year>
Reference-contexts: Totschek <ref> [Tot65] </ref> reported the distribution of job sizes (i.e., memory associated with each process) in the SDC (Systems Development Corporation) timesharing system. Later papers refer to this as "the SDC distribution". Naturally, the "block" sizes here were rather large. <p> Examining Totschek's distribution, however, it is clear that this is quite small relative to the average "object" (seg ment) size <ref> [Tot65] </ref>. 49 tation (about 11 or 12 percent) than an exponential distribution (about 17 or 18 percent), and both suffer considerably as the grain size is increased.
Reference: [UJ88] <author> David Ungar and Frank Jackson. </author> <title> Tenuring policies for generation-based storage reclamation. </title> <editor> In Norman Meyrowitz, editor, </editor> <booktitle> Conference on Object Oriented Programming Systems, Languages and Applications (OOPSLA '88) Proceedings, </booktitle> <pages> pages 1-17, </pages> <address> San Diego, Cal-ifornia, </address> <month> September </month> <year> 1988. </year> <note> ACM Press. </note>
Reference-contexts: Many programs build up data structures quickly, and then use those data structures for long periods (often nearly the whole running time of the program). These patterns are well-known, from anecdotal experience by many people (e.g., [Ros67, Han90]), from research on garbage collection (e.g., <ref> [Whi80, WM89, UJ88, Hay91, Hay93, BZ95, Wil95] </ref>), 32 and from a re cent study of C and C++ programs [WJNB95]. 32 It may be thought that garbage collected systems are sufficiently different from those using conventional storage management that these results are not relevant.
Reference: [Ull95] <author> Jeffrey D. Ullman. </author> <title> The role of theory today. </title> <journal> Computing Surveys, </journal> <volume> 27(1) </volume> <pages> 43-44, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: We find this curious, and suspect it has two main causes. One cause is simply the (short) history of the field, and expectations that computer science issues would be easily formalized, after many striking early successes. (Ullman <ref> [Ull95] </ref> eloquently describes this phenomenon.) Another is doubtless the same kind of paradigm entrenchment that occurs in other, more mature sciences [Kuh70].
Reference: [Ung86] <author> David Ungar. </author> <title> Design and Evaluation of a High-Performance Smalltalk System. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: It is common for block sizes in many modern systems to average on the order of 10 words, give or take a factor of two or so, so a single word per header may increase memory usage by about 10% <ref> [BJW70, Ung86, ZG92, DDZ93, WJNB95] </ref>. Boundary tags. Many allocators that support general coalescing are implemented using boundary tags (due to Knuth [Knu73]) to support the coalescing of free areas.
Reference: [VC90] <author> P. Vongsathorn and S. D. Carson. </author> <title> A system for adaptive disk rearrangement. </title> <journal> Software Practice and Experience, </journal> <volume> 20(3) </volume> <pages> 225-242, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: note that for secondary and tertiary storage more generally, contiguous storage is not strictly required; freedom from this restriction allows schemes that are much more flexible and less vulnerable to fragmentation. (Many systems divide all files into blocks of one or two fixed sizes, and only preserve logical contiguity (e.g., <ref> [RO91, VC90, SKW92, CG91, AS95] </ref>).
Reference: [VMH + 83] <author> J. Voldman, B. Mandelbrot, L. W. Hoevel, J. Knight, and P. Rosenfeld. </author> <title> Fractal nature of software-cache interaction. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 27(2) </volume> <pages> 164-170, </pages> <month> March </month> <year> 1983. </year>
Reference-contexts: Typically, "fractal" models of program behavior are not infinitely recursive, and are actually graftals or other finite fractal-like recursive entities. 34 We believe that this applies to studies of locality of reference as well. Attempts to characterize memory referencing behavior as fractal-like (e.g., <ref> [VMH + 83, Thi89] </ref>) are ill-conceived or severely limited|if only because memory allocation behavior is not generally fractal, and memory-referencing behavior depends on memory al Ramps, peaks, and plateus have very different implications for fragmentation.
Reference: [Vo95] <author> Kiem-Phong Vo. </author> <title> Vmalloc: A general and efficient memory allocator. </title> <journal> Software Practice and Experience, </journal> <note> 1995. To appear. </note>
Reference-contexts: incorrectly called a buddy system; we do not use that terminology because simple segregated storage does not use a buddy rule for coalescing|no coalescing is done at all. (Standish [Sta80] refers to simple segregated storage as "partitioned storage.") 36 simple segregated storage (used by Mike Haertel in a fast allocator <ref> [GZH93, Vo95] </ref>, and in several garbage collectors [Wil95]) is to maintain a count of live objects for each page, and notice when a page is entirely empty. <p> On the other hand, data from a different experiment [GZ93] show it being considerably slower than a set of allocators designed primarily for speed. Very recent data <ref> [Vo95] </ref> show it being somewhat slower than some other algorithms with similar memory usage, on average. 83 If an algorithm relies on an awkward secondary key, e.g., best fit with address-ordered tie breaking, then it may not make much difference what the ordering function is|one total ordering of blocks is likely <p> Fit allocator [WW88], and an allocator developed by Grunwald and Zorn, using Lea's allocator as the general allocator [GZH93].) One of the advantages of such 87 The only deferred coalescing segregated fits algorithm that we know of is Doug Lea's allocator, distributed freely and used in several recent studies (e.g., <ref> [GZH93, Vo95, WJNB95] </ref>). 43 a scheme is that the minimum block size can be very small|only big enough to hold a header and and a single link pointer. (Doubly-linked lists aren't necessary, since no coalescing is done for small objects.) These simplified designs are not true deferred coalescing allocators, except in <p> In a forthcoming article, Vo reports on the design of a new allocator framework and empirical results comparing several allocators using real traces <ref> [Vo95] </ref>. (Because this is work in progress, we will not report the empirical results in detail.) Vo's vmalloc () allocator is conceptually similar to Ross' zone system, allowing different "regions" of memory to be managed by different policies. 133 (Regions are subsets of the overall heap memory, and are not contiguous
Reference: [Vui80] <author> Jean Vuillemin. </author> <title> A unifying look at data structures. </title> <journal> Communications of the ACM, </journal> <volume> 29(4) </volume> <pages> 229-239, </pages> <month> April </month> <year> 1980. </year>
Reference-contexts: The best-known example of an indexed fits scheme is probably Stephenson's "Fast Fits" allocator [Ste83], which uses a Cartesian tree sorted on both size and address. A Cartesian tree <ref> [Vui80] </ref> encodes two-dimensional information in a binary tree, using two constraints on the tree shape. It is effectively sorted on a primary key and a secondary key. The tree is a normal totally-ordered tree with respect to the primary key.
Reference: [Wal66] <author> B. Wald. </author> <title> Utilization of a multiprocessor in command and control. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 53(12) </volume> <pages> 1885-1888, </pages> <month> December </month> <year> 1966. </year>
Reference-contexts: product of memory usage over time. (This essentially corresponds to the average memory usage, rather than peak usage.) This study was motivated in part by Wald's report of the "somewhat puzzling success" of best fit in actual use in the Automatic Operating and Scheduling Program of the Burroughs D-825 system <ref> [Wal66] </ref>. (Fragmentation was expected to be a problem; plans were made for compaction, but none was needed.) Shore found that best fit and (address-ordered) first fit worked about equally well, but that first fit had an advantage when the distribution included block sizes that were relatively large compared to the memory
Reference: [WB95] <author> Paul R. Wilson and V. B. Balayoghan. </author> <title> Compressed paging. </title> <note> In preparation, </note> <year> 1995. </year>
Reference-contexts: This is clearly true to a large degree for allocation and reference traces, but other regularities could probably be exploited as well <ref> [WB95] </ref>.) Dain Samples [Sam89] used a simple and effective approach for compressing memory-reference traces; his "Mache" trace compactor used a simple preprocessor to massage the trace into a different format, making the the relevant regularities easier for standard string-oriented compression algorithms to recognize and exploit.
Reference: [WDH89] <author> Mark Weiser, Alan Demers, and Carl Hauser. </author> <title> The portable common runtime approach to interoperability. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: with a larger constant factor.) In terms of policy, this search order means that smaller blocks are used in preference to larger ones, 74 This invariant can be useful in some kinds of systems, especially systems that provide persistence [SKW92] and/or garbage collection for languages such as C or C++ <ref> [BW88, WDH89, WJ93] </ref>, where pointers may point into the interior parts of objects, and it is important to be able to find the object headers quickly.
Reference: [Wei76] <author> Charles B. Weinstock. </author> <title> Dynamic Storage Allocation Techniques. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, Pittsburgh, Pennsylvania, </institution> <month> April </month> <year> 1976. </year>
Reference-contexts: a system is likely to scale very poorly, because live blocks must be traversed during search, but this technique might be useful in combination with some In experiments with both real and synthetic traces, it appears that address-ordered first fit may cause significantly less fragmentation than LIFO-ordered first fit (e.g., <ref> [Wei76, WJNB95] </ref>); the address-ordered variant is the most studied, and apparently the most used. Another alternative is to simply push freed blocks onto the rear of a (doubly-linked) list, opposite the end where searches begin. This results in a FIFO (first-in-first-out) queue-like pattern of memory use. <p> Worse, it may affect the locality of the program it allocates for, by scattering objects used by certain phases and intermingling them with objects used by other phases.) In several experiments using both real traces [WJNB95] and synthetic traces (e.g., <ref> [Bay77, Wei76, Pag84, KV85] </ref>), next fit has been shown to cause more other indexing structure. 60 This has also been observed by Ivor Page [Pag82] in randomized simulations, and similar (but possibly weaker) observations were made by Knuth and Shore and others in the late 1960's and 1970's. (Section 4.) 31 <p> Usually, this array is only large enough to have a separate free list for each individual size up to some maximum, such as 10 or 32 words; only those sizes will be treated by deferred coalescing <ref> [Wei76] </ref>. Blocks larger than this maximum size are simply returned directly to the "general" allocator, of whatever type. <p> Scheduling of coalescing. Some allocators defer all coalescing until memory runs out, and then coalesce all coalescable memory. This is most common in early designs, including Comfort's original proposal [Com64] 88 and Weinstock's "Quick Fit" scheme <ref> [Wei76] </ref>. This is not an attractive strategy in most modern systems, however, because in a virtual memory, the program never "runs out of space" until backing store is exhausted. If too much memory remains uncoalesced, wasting virtual memory, locality may be degraded and extra paging could result. <p> is no reason to suppose that stochastic processes could possibly generate the observed request distributions." Though based on a 1974 technical report, this paper was not published until 1977, the same year that saw publication of a flurry of papers based on random traces with well-behaved distributions. (Described below.) Weinstock <ref> [Wei76] </ref> surveyed most of the important work in allocators before 1976, and presented new empirical results.
Reference: [Whi80] <author> Jon L. White. </author> <title> Address/memory management for a gigantic Lisp environment, or, GC considered harmful. </title> <booktitle> In LISP Conference, </booktitle> <pages> pages 119-127, </pages> <address> Redwood Estates, California, </address> <month> August </month> <year> 1980. </year>
Reference-contexts: Many programs build up data structures quickly, and then use those data structures for long periods (often nearly the whole running time of the program). These patterns are well-known, from anecdotal experience by many people (e.g., [Ros67, Han90]), from research on garbage collection (e.g., <ref> [Whi80, WM89, UJ88, Hay91, Hay93, BZ95, Wil95] </ref>), 32 and from a re cent study of C and C++ programs [WJNB95]. 32 It may be thought that garbage collected systems are sufficiently different from those using conventional storage management that these results are not relevant.
Reference: [Wil90] <author> Paul R. Wilson. </author> <title> Some issues and strategies in heap management and memory hierarchies. </title> <booktitle> In OOPSLA/ECOOP '90 Workshop on Garbage Collection in Object-Oriented Systems, Octo-ber 1990. Also appears in SIGPLAN Notices 77 23(3) </booktitle> <pages> 45-52, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: The literature on garbage collection is considerably more sophisticated in terms of locality studies than the literature on memory allocation, and should not be overlooked. (See, e.g., <ref> [Bae73, KLS92, Wil90, WLM92, DTM93, Rei94, GA95, Wil95] </ref>.) Many of the same issues must arise in conventionally-managed heaps as well. 26 larger free blocks (coalescing). Equally important are the policy and strategy implications|i.e., whether the allocator properly exploits the regularities in real request streams.
Reference: [Wil91] <author> Paul R. Wilson. </author> <title> Operating system support for small objects. </title> <booktitle> In International Workshop on Object Orientation in Operating Systems, </booktitle> <pages> pages 80-86, </pages> <address> Palo Alto, California, </address> <month> October </month> <year> 1991. </year> <note> IEEE Press. </note>
Reference: [Wil92] <author> Paul R. Wilson. </author> <title> Uniprocessor garbage collection techniques. </title> <booktitle> In Bekkers and Cohen [BC92], </booktitle> <pages> pages 1-42. </pages>
Reference: [Wil95] <author> Paul R. Wilson. </author> <title> Garbage collection. </title> <journal> Computing Surveys, </journal> <note> 1995. Expanded version of [Wil92]. Draft available via anonymous internet FTP from cs.utexas.edu as pub/garbage/bigsurv.ps. In revision, to appear. </note>
Reference-contexts: Many programs build up data structures quickly, and then use those data structures for long periods (often nearly the whole running time of the program). These patterns are well-known, from anecdotal experience by many people (e.g., [Ros67, Han90]), from research on garbage collection (e.g., <ref> [Whi80, WM89, UJ88, Hay91, Hay93, BZ95, Wil95] </ref>), 32 and from a re cent study of C and C++ programs [WJNB95]. 32 It may be thought that garbage collected systems are sufficiently different from those using conventional storage management that these results are not relevant. <p> The literature on garbage collection is considerably more sophisticated in terms of locality studies than the literature on memory allocation, and should not be overlooked. (See, e.g., <ref> [Bae73, KLS92, Wil90, WLM92, DTM93, Rei94, GA95, Wil95] </ref>.) Many of the same issues must arise in conventionally-managed heaps as well. 26 larger free blocks (coalescing). Equally important are the policy and strategy implications|i.e., whether the allocator properly exploits the regularities in real request streams. <p> not use that terminology because simple segregated storage does not use a buddy rule for coalescing|no coalescing is done at all. (Standish [Sta80] refers to simple segregated storage as "partitioned storage.") 36 simple segregated storage (used by Mike Haertel in a fast allocator [GZH93, Vo95], and in several garbage collectors <ref> [Wil95] </ref>) is to maintain a count of live objects for each page, and notice when a page is entirely empty. <p> amount of allocation) may be sufficient, and may not incur undue costs if the general allocator is reasonably fast. 89 90 89 The issues here are rather analogous to some issues in the design and tuning of generational garbage collectors, particularly the setting of generation sizes and advance ment thresholds <ref> [Wil95] </ref>. 90 If absolute all-out speed is important, Lea's strategy of coalescing only when a search fails may be more attractive|it does not require incrementing or checking an allocation total at each allocation or dealloca-tion. (Another possibility would be to use a timer interrupt, but this is quite awkward. <p> ordering, but it is unknown whether that applies to items scure issues of reentrancy|the interrupt handler must be careful not to do anything that would interfere with an allocation or deallocation that is interrupted.) 91 This is similar to the "bucket brigade" advancement technique used in some generational garbage collectors <ref> [Sha88, WM89, Wil95] </ref>. A somewhat similar technique is used in Lea's allocator, but for a different purpose. <p> On the other hand, recent data for garbage-collected systems <ref> [Wil95] </ref> and for C and C++ programs [WJNB95] suggest that the majority of object lifetimes in modern programs are also tied to the phase structure of pro 109 Algol-60's dynamically sized arrays may complicate this scenario somewhat, requiring general heap allocation, but apparently a large majority of arrays were statically sized <p> Another potentially confusing term, "garbage collection," is used to refer to deferred coalescing where coalescing is performed only when there is no sufficiently large block to satisfy a request. This is very different from the usual current usage of the term <ref> [Wil95] </ref>, but it is not uncommon in early papers on allocators. 121 Activation records were apparently allocated on the general heap; presumably this was used to support closures with indefinite extent (i.e., "block retention"), and/or "thunks" (hidden parameterless subroutines) for call by-name parameter passing [Ing61]. 58 Betteridge [Bet82] attempted to compute <p> In that case, a storage leak results. These kinds of errors (and many others) can usually be avoided if garbage collection <ref> [Wil95] </ref> is used to free objects automatically. Henry Baker reports that the heavy use of an obstack-like scheme used in MIT Lisp machines was a continuing source of bugs (Baker, personal communication 1995). <p> David Moon reports that a similar facility in the Symbolics system often resulted in obscure bugs, and its use was discouraged after an efficient generational garbage collector [Moo84] was developed (Moon, personal communication 1995); generational techniques heuristically exploit the lifetime distributions of typical programs <ref> [LH83, Wil95] </ref>. For systems without garbage collection, however, the resulting problems may be no worse than those introduced by other explicit deallocation strategies, when used care fully and in well-documented ways. 64 4.2 Recent Studies Using Real Traces "Some places have already quit lotteries," Mrs. <p> We agree in part with this sentiment, but the successes of computer science can lead to a false sense 138 Our anonymous FTP repository is on ftp.cs.utexas.edu in the directory pub/garbage. This repository also contains the BibTeX bibliography file used for this paper and <ref> [Wil95] </ref>, several papers on persistence and memory hierarchies, and numerous papers on garbage collection by ourselves and others. of confidence.
Reference: [Wis78] <author> David S. Wise. </author> <title> The double buddy-system. </title> <type> Technical Report 79, </type> <institution> Computer Science Department, Indiana University, Bloomington, Indiana, </institution> <month> December </month> <year> 1978. </year>
Reference-contexts: satisfy a request for a particular size, the remaining block is of a different 81 This figure varies somewhat depending on the expected range and skew of the size distribution [PN77]. 39 size, which is less likely to be useful if the program allocates many objects of the same size <ref> [Wis78] </ref>. Weighted buddies.Weighted buddy systems [SP74] use a different kind of size class series than either binary or Fibonacci buddy systems. Some size classes can be split only one way, while other size classes can be split in two ways. <p> Double buddies. Double buddy systems use a different technique to allow a closer spacing of size classes <ref> [Wis78, PH86, WJNB95] </ref>. They use two different binary buddy systems, with staggered sizes. For example, one buddy system may use powers-of-two sizes (2, 4, 8, 16...) while another uses a powers-of-two spacing starting at a different size, such as 3. (The resulting sizes are 3, 6, 12, 24 ...). <p> We believe that his estimation technique is unreliable, partly because we do not believe that distributions are generally exponential, and partly because of the randomness of request order that he assumes. Wise, in an unpublished technical report <ref> [Wis78] </ref>, described a double buddy system and its advantages over Fibonacci systems in terms of external fragmentation (producing free blocks of the same size as requested blocks).
Reference: [WJ93] <author> Paul R. Wilson and Mark S. Johnstone. </author> <title> Truly real-time non-copying garbage collection. </title> <booktitle> In OOPSLA '93 Workshop on Memory Management and Garbage Collection, </booktitle> <month> December </month> <year> 1993. </year> <note> Expanded version of workshop position paper submitted for publication. </note>
Reference-contexts: with a larger constant factor.) In terms of policy, this search order means that smaller blocks are used in preference to larger ones, 74 This invariant can be useful in some kinds of systems, especially systems that provide persistence [SKW92] and/or garbage collection for languages such as C or C++ <ref> [BW88, WDH89, WJ93] </ref>, where pointers may point into the interior parts of objects, and it is important to be able to find the object headers quickly.
Reference: [WJNB95] <author> Paul R. Wilson, Mark S. Johnstone, Michael Neely, and David Boles. </author> <title> Memory allocation policies reconsidered. </title> <type> Technical report, </type> <institution> University of Texas at Austin Department of Computer Sciences, </institution> <year> 1995. </year>
Reference-contexts: That is, many quite probable kinds of patterns are extremely improbable in a simple Markov model. 13 of simple Markov models <ref> [ZG94, WJNB95] </ref>. 29 2.3 What Fragmentation Really Is, and Why the Traditional Approach is Unsound A single death is a tragedy. <p> They should not be used for this purpose, or any similarly poorly understood purpose, where complex patterns may be very important. (At least, not without extensive validation.) The fact that the regularities are complex and unknown is not a good reason to assume that they're effectively random <ref> [ZG94, WJNB95] </ref> (Section 4.2). design experiments measuring fragmentation, it is worthwhile to stop for a moment and consider what fragmentation really is, and how it arises. Fragmentation is the inability to reuse memory that is free. <p> These patterns are well-known, from anecdotal experience by many people (e.g., [Ros67, Han90]), from research on garbage collection (e.g., [Whi80, WM89, UJ88, Hay91, Hay93, BZ95, Wil95]), 32 and from a re cent study of C and C++ programs <ref> [WJNB95] </ref>. 32 It may be thought that garbage collected systems are sufficiently different from those using conventional storage management that these results are not relevant. <p> See <ref> [WJNB95] </ref>.) 16 (Other patterns of overall memory usage also occur, but appear less common. As we describe in Section 4, backward ramp functions have been observed [GM85]. <p> This may suggest that the allocator should be adaptive, 35 but much simpler strategies also seem likely to work <ref> [WJNB95] </ref>: Objects allocated at about the same time are likely to die together at the end of a phase; if consecutively-allocated objects are allocated in contiguous memory, they will free contiguous memory. Objects of different types may be likely to serve different purposes and die at different times. <p> and Zorn have recently built an allocator using profile information to heuristically separate long-lived objects from short-lived ones [BZ93]. (Section 4.2.) This suggests that objects allocated at about the same time should be allocated adjacent to each other in memory, with the possible amendment that different-sized objects should be segregated <ref> [WJNB95] </ref>. 36 Implications for strategy. The phased behavior of many programs provides an opportunity for the allocator to reduce fragmentation. As we said above, if successive objects are allocated contiguously and freed at about the same time, free memory will again be contiguous. <p> It is common for block sizes in many modern systems to average on the order of 10 words, give or take a factor of two or so, so a single word per header may increase memory usage by about 10% <ref> [BJW70, Ung86, ZG92, DDZ93, WJNB95] </ref>. Boundary tags. Many allocators that support general coalescing are implemented using boundary tags (due to Knuth [Knu73]) to support the coalescing of free areas. <p> a system is likely to scale very poorly, because live blocks must be traversed during search, but this technique might be useful in combination with some In experiments with both real and synthetic traces, it appears that address-ordered first fit may cause significantly less fragmentation than LIFO-ordered first fit (e.g., <ref> [Wei76, WJNB95] </ref>); the address-ordered variant is the most studied, and apparently the most used. Another alternative is to simply push freed blocks onto the rear of a (doubly-linked) list, opposite the end where searches begin. This results in a FIFO (first-in-first-out) queue-like pattern of memory use. <p> This results in a FIFO (first-in-first-out) queue-like pattern of memory use. This variant has not been considered in most studies, but recent results suggest that it can work quite well|better than the LIFO ordering, and perhaps as well as address ordering <ref> [WJNB95] </ref>. A first fit policy may tend over time toward behaving rather like best fit, because blocks near the front of the list are split preferentially, and this may result in a roughly size-sorted list. 60 Whether this happens for real workloads is unknown. Next fit. <p> Worse, it may affect the locality of the program it allocates for, by scattering objects used by certain phases and intermingling them with objects used by other phases.) In several experiments using both real traces <ref> [WJNB95] </ref> and synthetic traces (e.g., [Bay77, Wei76, Pag84, KV85]), next fit has been shown to cause more other indexing structure. 60 This has also been observed by Ivor Page [Pag82] in randomized simulations, and similar (but possibly weaker) observations were made by Knuth and Shore and others in the late 1960's <p> observed by Ivor Page [Pag82] in randomized simulations, and similar (but possibly weaker) observations were made by Knuth and Shore and others in the late 1960's and 1970's. (Section 4.) 31 fragmentation than best fit or address-ordered first fit, and the LIFO-order variant may be significantly worse than address order <ref> [WJNB95] </ref>. As with the other sequential fits algorithms, scalable implementations of next fit are possible using various kinds of trees rather than linear lists. 3.5 Discussion of Sequential Fits and General Policy Issues. <p> In addition, the allocator may choose not to split a block if the remainder is "too small," either in absolute terms [Knu73] or relative to the size of the block being split <ref> [WJNB95] </ref>. This policy is intended to avoid allocating in the remainder a small object that may outlive the large object, and prevent the reclamation of a larger free area. Splitting thresholds do not appear to be helpful in practice, unless (perhaps) they are very small. <p> This may be important if the average object size is very small. Recent studies indicate that in modern programs, the average object size is often quite small by earlier standards (e.g., around 10 words <ref> [WJNB95] </ref>), and that header and footer overheads alone can increase memory usage by ten percent or twenty percent [ZG92, WJNB95]. This is comparable to the "real" fragmentation for good allocators [WJNB95]. <p> Recent studies indicate that in modern programs, the average object size is often quite small by earlier standards (e.g., around 10 words [WJNB95]), and that header and footer overheads alone can increase memory usage by ten percent or twenty percent <ref> [ZG92, WJNB95] </ref>. This is comparable to the "real" fragmentation for good allocators [WJNB95]. Simple segregated storage is quite fast in the usual case, especially when objects of a given size are repeatedly freed and reallocated over short periods of time. <p> indicate that in modern programs, the average object size is often quite small by earlier standards (e.g., around 10 words <ref> [WJNB95] </ref>), and that header and footer overheads alone can increase memory usage by ten percent or twenty percent [ZG92, WJNB95]. This is comparable to the "real" fragmentation for good allocators [WJNB95]. Simple segregated storage is quite fast in the usual case, especially when objects of a given size are repeatedly freed and reallocated over short periods of time. The freed blocks simply wait until the next allocation of the same size, and can be reallocated without splitting. <p> appear to be the case. buddy systems generally exhibit significantly more fragmentation than segregated fits and indexed fits schemes using boundary tags to support general coalescing. (Most of these results come from synthetic trace studies, however; it appears that only two buddy systems have ever been studied using real traces <ref> [WJNB95] </ref>.) Several significant variations on buddy systems have been devised: Binary buddies. Binary buddies are the simplest and best-known kind of buddy system [Kno65]. In this scheme, all buddy sizes are a power of two, and each size is divided into two equal parts. <p> Double buddies. Double buddy systems use a different technique to allow a closer spacing of size classes <ref> [Wis78, PH86, WJNB95] </ref>. They use two different binary buddy systems, with staggered sizes. For example, one buddy system may use powers-of-two sizes (2, 4, 8, 16...) while another uses a powers-of-two spacing starting at a different size, such as 3. (The resulting sizes are 3, 6, 12, 24 ...). <p> Fit allocator [WW88], and an allocator developed by Grunwald and Zorn, using Lea's allocator as the general allocator [GZH93].) One of the advantages of such 87 The only deferred coalescing segregated fits algorithm that we know of is Doug Lea's allocator, distributed freely and used in several recent studies (e.g., <ref> [GZH93, Vo95, WJNB95] </ref>). 43 a scheme is that the minimum block size can be very small|only big enough to hold a header and and a single link pointer. (Doubly-linked lists aren't necessary, since no coalescing is done for small objects.) These simplified designs are not true deferred coalescing allocators, except in <p> block sizes infrequently equal to each other) the length of the free list will tend toward being about half the number of blocks actually in use. (All of these assumptions now appear to be false for most programs, as we will explain later in the discussions of [MPS71], [ZG94] and <ref> [WJNB95] </ref>. <p> On the other hand, recent data for garbage-collected systems [Wil95] and for C and C++ programs <ref> [WJNB95] </ref> suggest that the majority of object lifetimes in modern programs are also tied to the phase structure of pro 109 Algol-60's dynamically sized arrays may complicate this scenario somewhat, requiring general heap allocation, but apparently a large majority of arrays were statically sized and stack-like usage predominated. grams, or to <p> better for real programs behavior as well. (Randomization introduces biases that tend to cancel each other out for most policies tested in earlier work.) The errors produced are still large, however, often comparable to the total fragmentation for real programs, once various overheads are accounted for. (Our own later experiments <ref> [WJNB95] </ref>, described later, show that the random trace methodology can introduce serious and systematic errors for some al-locators which are popular in practice but almost entirely absent in the experimental literature. <p> Wilson, Johnstone, Neely, and Boles. In a forthcoming report <ref> [WJNB95] </ref>, we will present results of a variety of memory allocation experiments using real traces from eight varied C and C++ programs, and more than twenty variants of six general allocator types (first fit, best fit, next fit, buddy systems, and simple segregated storage) [WJNB95]. <p> In a forthcoming report <ref> [WJNB95] </ref>, we will present results of a variety of memory allocation experiments using real traces from eight varied C and C++ programs, and more than twenty variants of six general allocator types (first fit, best fit, next fit, buddy systems, and simple segregated storage) [WJNB95]. We will briefly describe some of the major results of that study here. To test the usual experimental assumptions, we used both real and synthetic traces, and tried to make the synthetic traces as realistic as possible in terms of size and lifetime distributions. <p> Results from most of pre-1992 experiments are therefore highly questionable. Using real traces, we measured fragmentation for our eight programs using our large set of allocators. We will report results for the twelve we consider most interesting here; for more complete and detailed information, see the forthcoming report <ref> [WJNB95] </ref>. <p> We note that some of these numbers may change slightly before <ref> [WJNB95] </ref> appears, due to minor changes in our 134 No significant differences were found between results for variations of best fit using different free list orders. <p> Worse, we found that the variance for some of these allocators was quite high, especially for some of the poorer algorithms. (We are also concerned that any sample of eight programs cannot be considered representative of all real programs, though we have done our best <ref> [WJNB95] </ref>.) The rank ordering here should thus be considered very approximate, especially within clusters. To our great surprise, we found that best fit, address-ordered first fit, and FIFO-ordered first fit all performed extremely well|and nearly identically well.
Reference: [WJW + 75] <author> William A. Wulf, R. K. Johnsson, C. B. We-instock, S. O. Hobbs, and C. M. Geschke. </author> <title> Design of an Optimizing Compiler. </title> <publisher> American El-sevier, </publisher> <year> 1975. </year>
Reference-contexts: He also introduced the "QuickFit" algorithm, a deferred coalescing scheme using size-specific lists for small block sizes, backed by LIFO-ordered first fit as the general allocator. 115 (Weinstock reported that this scheme was invented several years earlier for use in the Bliss/11 compiler <ref> [WJW + 75] </ref>, and notes that a similar scheme was independently developed and used in the Simscript II.5 language [Joh72]. Margolin's prior work was overlooked, however.) Weinstock used the conventional synthetic trace methodology; randomly-ordered synthetic traces were generated, using two real size distributions and four artificial ones. <p> Margolin's prior work was overlooked, however.) Weinstock used the conventional synthetic trace methodology; randomly-ordered synthetic traces were generated, using two real size distributions and four artificial ones. One of the real size-and-lifetime distributions came from the Bliss/11 compiler <ref> [WJW + 75] </ref>, and the other was from Batson and Brundage's measurements of the University of Virginia B5500 system [BB77], described above.
Reference: [WLM91] <author> Paul R. Wilson, Michael S. Lam, and Thomas G. Moher. </author> <title> Effective static-graph reorganization to improve locality in garbage-collected systems. </title> <booktitle> In Proceedings of the 1991 SIGPLAN Conference on Programming Language Design and Implementation [PLD91], </booktitle> <pages> pages 177-191. </pages> <note> Published as SIGPLAN Notices 26(6), </note> <month> June </month> <year> 1992. </year>
Reference-contexts: instructions 48 Conventional text-string-oriented compression algorithms [Nel91] (e.g, UNIX compress or GNU gzip) work quite well, although we suspect that sophisticated schemes could do significantly better by taking advantage of the numerical properties of object identifiers or addresses; such schemes have been proposed for use in compressed paging and addressing <ref> [WLM91, FP91] </ref>. (Text-oriented compression generally makes Markov-like modeling assumptions, i.e., that literal sequences are likely to recur. <p> Having information about the block stored with the block makes many common operations fast. 50 Briefly, we believe that the allocator should heuristically attempt to cluster objects that are likely to be used at about the same times and in similar ways. This should improve locality <ref> [Bae73, WLM91] </ref>; it should also increase the chances that adjacent objects will die at about the same time, reducing fragmentation. 27 Header fields are usually one machine word; on most modern machines, that is four 8-bit bytes, or 32 bits. (For convenience, we will assume that the word size is 32
Reference: [WLM92] <author> Paul R. Wilson, Michael S. Lam, and Thomas G. Moher. </author> <title> Caching considerations for generational garbage collection. </title> <booktitle> In Conference Record of the 1992 ACM Symposium on LISP and Functional Programming, </booktitle> <pages> pages 32-42, </pages> <address> San Francisco, California, June 1992. </address> <publisher> ACM Press. </publisher>
Reference-contexts: The literature on garbage collection is considerably more sophisticated in terms of locality studies than the literature on memory allocation, and should not be overlooked. (See, e.g., <ref> [Bae73, KLS92, Wil90, WLM92, DTM93, Rei94, GA95, Wil95] </ref>.) Many of the same issues must arise in conventionally-managed heaps as well. 26 larger free blocks (coalescing). Equally important are the policy and strategy implications|i.e., whether the allocator properly exploits the regularities in real request streams.
Reference: [WM89] <author> Paul R. Wilson and Thomas G. Moher. </author> <title> Design of the Opportunistic Garbage Collector. </title> <booktitle> In Conference on Object Oriented Programming Systems, Languages and Applications (OOPSLA '89) Proceedings, </booktitle> <pages> pages 23-35, </pages> <address> New Orleans, Louisiana, 1989. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Many programs build up data structures quickly, and then use those data structures for long periods (often nearly the whole running time of the program). These patterns are well-known, from anecdotal experience by many people (e.g., [Ros67, Han90]), from research on garbage collection (e.g., <ref> [Whi80, WM89, UJ88, Hay91, Hay93, BZ95, Wil95] </ref>), 32 and from a re cent study of C and C++ programs [WJNB95]. 32 It may be thought that garbage collected systems are sufficiently different from those using conventional storage management that these results are not relevant. <p> ordering, but it is unknown whether that applies to items scure issues of reentrancy|the interrupt handler must be careful not to do anything that would interfere with an allocation or deallocation that is interrupted.) 91 This is similar to the "bucket brigade" advancement technique used in some generational garbage collectors <ref> [Sha88, WM89, Wil95] </ref>. A somewhat similar technique is used in Lea's allocator, but for a different purpose.
Reference: [Wol65] <author> Eric Wolman. </author> <title> A fixed optimum cell-size for records of various lengths. </title> <journal> Journal of the ACM, </journal> <volume> 12(1) </volume> <pages> 53-70, </pages> <month> January </month> <year> 1965. </year>
Reference: [WW88] <author> Charles B. Weinstock and William A. Wulf. Quickfit: </author> <title> an efficient algorithm for heap storage allocation. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 23(10) </volume> <pages> 141-144, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: In effect, they simply use a non-coalescing segregated lists allocator for small objects and an entirely different allocator for large ones. (Examples include We-instock and Wulf's simplification of their own Quick Fit allocator <ref> [WW88] </ref>, and an allocator developed by Grunwald and Zorn, using Lea's allocator as the general allocator [GZH93].) One of the advantages of such 87 The only deferred coalescing segregated fits algorithm that we know of is Doug Lea's allocator, distributed freely and used in several recent studies (e.g., [GZH93, Vo95, WJNB95]). <p> be a bad case for first fit and best fit. (The two-valued distribution was not used in the final evaluation of allocators.) The Bliss/11 distribution is heavily weighted toward small objects, but is not well-described by an 115 This is not to be confused with the later variant of QuickFit <ref> [WW88] </ref>, which does no coalescing for small objects, or Standish and Tadman's indexed fits allocator. exponential curve. It has distinct spikes at 2 words (44% of all objects) and 9 words (14%). In between those spikes is another elevation at 5 words and 6 words (9% each). <p> and Henderson [GZH93] mea-sured the locality effects of several allocators: next fit, the G++ segregated fits allocator by Doug Lea, simple segregated storage using powers of two size classes (the Berkeley 4.2 BSD allocator by Chris Kingsley), and two simplified quick fit schemes (i.e., "Quick Fit" in the sense of <ref> [WW88] </ref>, i.e., without coalescing for small objects).
Reference: [Yua90] <author> Taichi Yuasa. </author> <title> The design and implementation of Kyoto Common Lisp. </title> <journal> Journal of Information Processing, </journal> <volume> 13(3), </volume> <year> 1990. </year>
Reference-contexts: In garbage-collected systems, it is common to segregated objects by type, or by implementation-level characteristics, to facilitate optimizations of type checking and/or garbage collection <ref> [Yua90, Del92, DEB94] </ref>. as with best fit. In some cases, however, the details of the size class system and the searching of size-class lists may cause deviations from the best fit policy. Note that in a segregated fits scheme, coalescing may increase search times.
Reference: [ZG92] <author> Benjamin Zorn and Dirk Grunwald. </author> <title> Empirical measurements of six allocation-intensive C programs. </title> <type> Technical Report CU-CS-604-92, </type> <institution> University of Colorado at Boulder, Dept. of Computer Science, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: It is common for block sizes in many modern systems to average on the order of 10 words, give or take a factor of two or so, so a single word per header may increase memory usage by about 10% <ref> [BJW70, Ung86, ZG92, DDZ93, WJNB95] </ref>. Boundary tags. Many allocators that support general coalescing are implemented using boundary tags (due to Knuth [Knu73]) to support the coalescing of free areas. <p> Recent studies indicate that in modern programs, the average object size is often quite small by earlier standards (e.g., around 10 words [WJNB95]), and that header and footer overheads alone can increase memory usage by ten percent or twenty percent <ref> [ZG92, WJNB95] </ref>. This is comparable to the "real" fragmentation for good allocators [WJNB95]. Simple segregated storage is quite fast in the usual case, especially when objects of a given size are repeatedly freed and reallocated over short periods of time. <p> In <ref> [ZG92] </ref>, Zorn and Grunwald present various allocation-related statistics on six allocation-intensive C programs, i.e., programs for which the speed of the allocator is important. (Not all of these use large amounts of memory, however.) They found that for each of these programs, the two most popular sizes accounted for at least
Reference: [ZG94] <author> Benjamin Zorn and Dirk Grunwald. </author> <title> Evaluating models of memory allocation. </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> 1(4) </volume> <pages> 107-131, </pages> <year> 1994. </year>
Reference-contexts: That is, many quite probable kinds of patterns are extremely improbable in a simple Markov model. 13 of simple Markov models <ref> [ZG94, WJNB95] </ref>. 29 2.3 What Fragmentation Really Is, and Why the Traditional Approach is Unsound A single death is a tragedy. <p> They should not be used for this purpose, or any similarly poorly understood purpose, where complex patterns may be very important. (At least, not without extensive validation.) The fact that the regularities are complex and unknown is not a good reason to assume that they're effectively random <ref> [ZG94, WJNB95] </ref> (Section 4.2). design experiments measuring fragmentation, it is worthwhile to stop for a moment and consider what fragmentation really is, and how it arises. Fragmentation is the inability to reuse memory that is free. <p> the memory lost to deferred coalescing remains small, however; if the system only frees blocks of a few sizes over a long period of time, uncoalesced blocks may remain on another quick list indefinitely. (This appears to happen for some workloads in a similar system developed by Zorn and Grunwald <ref> [ZG94] </ref>, using a fixed-length LRU queue of quick lists.) Doug Lea's segregated fits allocator uses an unusual and rather complex policy to perform coalescing in small increments. (It is optimized as much for speed as for space.) Coalescing is only performed when a request cannot otherwise be satisfied without obtaining more <p> usage, and block sizes infrequently equal to each other) the length of the free list will tend toward being about half the number of blocks actually in use. (All of these assumptions now appear to be false for most programs, as we will explain later in the discussions of [MPS71], <ref> [ZG94] </ref> and [WJNB95]. <p> In each, the top ten sizes accounted for at least 85% of all allocations. Zorn and Grunwald <ref> [ZG94] </ref> attempted to find fairly conventional models of memory allocation that would allow the generation of synthetic traces useful for evaluating allocators. They used several models of varying degrees of sophistication, some of which modeled phase behavior and one of which modeled fine-grained patterns stochastically (using a first-order Markov model).
Reference: [Zor93] <author> Benjamin Zorn. </author> <title> The measured cost of conservative garbage collection. </title> <journal> Software|Practice and Experience, </journal> <volume> 23(7) </volume> <pages> 733-756, </pages> <month> July </month> <year> 1993. </year> <title> This article was processed using the L a T E X macro package with LLNCS style 78 </title>
Reference-contexts: While Cartesian trees give logarithmic expected search times for random inputs, they may become unbalanced in the face of patterned inputs, and in the worst case provide only linear time searches. 82 82 Data from <ref> [Zor93] </ref> suggest that actual performance is reasonable for real data, being among the faster algo Discussion of indexed fits.
References-found: 164

