URL: ftp://ftp.cs.umass.edu/pub/osl/papers/HCW94.ps.gz
Refering-URL: http://spa-www.cs.umass.edu/bibliography.html
Root-URL: 
Title: Linguistic Support for Heterogeneous Parallel Processing: A Survey and an Approach  
Author: Charles C. Weems Glen E. Weaver Steven G. Dropsho 
Address: Amherst, MA 01003  
Affiliation: Computer Science Department, University of Massachusetts  
Note: Appears in the 1994 Heterogeneous Computing Workshop  
Abstract: Coding a highly parallel application to run on a heterogeneous suite of processors (both metacomputers and mixed-mode computers) with high efficiency, ease of implementation, and portability is a significant challenge. This paper first surveys recently proposed and existing parallel languages from the perspective of programming complex, heterogeneous systems. We then propose two essential features to be included in programming languages that are intended to support heterogeneity. * 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Klietz, A.E., A.V. Malevsky, and K. Chin-Purcell. </author> <title> A Case Sudy in Metacomputing: Distributed Simulations of Mixing in Turbulent Convection. </title> <booktitle> in Workshop on Heterogeneous Processing. </booktitle> <year> 1993. </year>
Reference-contexts: 1. Introduction Recent examples have shown the success of combining heterogeneous computing hardware to solve complex problems <ref> [1, 2] </ref>. When the architecture of the machine matches the structure of the problem, the algorithmic solution is often easier to develop and can execute more efficiently. The currently popular practice of networking existing machines is merely a beginning [3].
Reference: [2] <author> Nicholas, H., et al. </author> <title> Distributing the Comparison of DNA and Protein Sequences Across Heterogeneous Supercomputers. </title> <booktitle> in Proc. of Supercomputing. </booktitle> <year> 1991. </year>
Reference-contexts: 1. Introduction Recent examples have shown the success of combining heterogeneous computing hardware to solve complex problems <ref> [1, 2] </ref>. When the architecture of the machine matches the structure of the problem, the algorithmic solution is often easier to develop and can execute more efficiently. The currently popular practice of networking existing machines is merely a beginning [3].
Reference: [3] <author> Smarr, L. and C.E. Catlett, </author> <title> Metacomputing. </title> <journal> CACM, 1992. </journal> <volume> 35(6): </volume> <pages> p. 45-52. </pages>
Reference-contexts: When the architecture of the machine matches the structure of the problem, the algorithmic solution is often easier to develop and can execute more efficiently. The currently popular practice of networking existing machines is merely a beginning <ref> [3] </ref>. As hardware continues to diminish in size and cost, new possibilities are being created for systems that are heterogeneous by design. Examples of mixed-mode heterogeneous systems are beginning to appear.
Reference: [4] <editor> Weems, C.C., Jr., et al., </editor> <booktitle> The Image Understanding Architecture. IJCV, 1989. 2(3): p. </booktitle> <pages> 251-282. </pages>
Reference-contexts: As hardware continues to diminish in size and cost, new possibilities are being created for systems that are heterogeneous by design. Examples of mixed-mode heterogeneous systems are beginning to appear. The CM-5 and the Image Understanding Architecture (IUA) <ref> [4] </ref> are two examples of architectures that incorporate different modes of parallelism within a single machine. In the future, we can expect even more complex examples of heterogeneity.
Reference: [5] <author> Khokhar, A.A., et al., </author> <title> Heterogeneous Computing: Challenges and Opportunities. </title> <booktitle> Computer, 1993. 26(6): p. </booktitle> <pages> 18-27. </pages>
Reference-contexts: Even if the hardware is built, the application program must be written, debugged, and maintained by humans. A programming facility is needed that simplifies the development task while maximizing performance <ref> [5] </ref>. Programmers must be able to specify algorithms at a sufficiently abstract level so that they are not overwhelmed with trivialities while still having access to those details that are needed by system software (compiler, linker, and runtime support) to efficiently execute the code.
Reference: [6] <author> Weems, C.C., Jr. </author> <title> Image Understanding: A Driving Application for Research in Heterogeneous Parallel Processing. </title> <booktitle> in Workshop on Heterogeneous Processing. </booktitle> <year> 1993. </year>
Reference-contexts: A single application can include many forms of concurrency each requiring a different programming model in order to be expressed in a usable manner <ref> [6] </ref>. These differing models must both be expressible in the same program and in a hierarchical encapsulation of individual models.
Reference: [7] <author> Eshaghian, </author> <title> M.M. and M.E. Shaaban, Cluster-M Parallel Programming Paradigm. </title> <journal> International Journal of High Speed Computing, </journal> <year> 1993. </year> . 
Reference-contexts: Nor are we proposing specific language extensions to support these features. The implementation of the features is an open research issue and our purpose here is merely to motivate their necessity. 2. Language survey 2.1. Cluster-M C l u s t e r - M <ref> [7, 8] </ref> is a program partitioning and placement model for heterogeneous processing, not a language. It uses the tree structure to unify the descriptions of programs and physical systems. A program is represented as a tree structure, called a Cluster-M specification, where each leaf node is a single computation operand.
Reference: [8] <author> Eshaghian, </author> <title> M.M. and R.F. Freund. Cluster-M Paradigms for High-Order Heterogeneous Procedural Specification C o m p u t i n g . in W ork sho p on Het ero gen eo us Processing. </title> <year> 1993. </year>
Reference-contexts: Nor are we proposing specific language extensions to support these features. The implementation of the features is an open research issue and our purpose here is merely to motivate their necessity. 2. Language survey 2.1. Cluster-M C l u s t e r - M <ref> [7, 8] </ref> is a program partitioning and placement model for heterogeneous processing, not a language. It uses the tree structure to unify the descriptions of programs and physical systems. A program is represented as a tree structure, called a Cluster-M specification, where each leaf node is a single computation operand.
Reference: [9] <author> Chen, S., </author> <title> M.M. Eshaghian, and M.E. Shaaban. Automatic Fine Grain Mapping with Cluster-M. </title> <note> To be submitted for publication. </note>
Reference-contexts: The manner in which a program is clustered is specified by the programmer and may be modified during runtime. This gives the user the ability to express how clusters (hence data) are related. Algorithms to automatically map clusters to processors has been proposed in <ref> [9, 10] </ref>. The tree structure representation for programs removes explicit dependence on the underlying machine architecture and aids portability. 2.2. High Performance FORTRAN High Performance FORTRAN (HPF) is a set of extensions to FORTRAN 90 [11] for fine to medium grain data parallelism.
Reference: [10] <editor> DeSouza-Batista, J.C., et al. </editor> <title> A SubOptimal Assignment of Application Tasks onto Heterogeneous Systems. </title> <booktitle> in Heterogeneous Computing Workshop. </booktitle> <year> 1994. </year>
Reference-contexts: The manner in which a program is clustered is specified by the programmer and may be modified during runtime. This gives the user the ability to express how clusters (hence data) are related. Algorithms to automatically map clusters to processors has been proposed in <ref> [9, 10] </ref>. The tree structure representation for programs removes explicit dependence on the underlying machine architecture and aids portability. 2.2. High Performance FORTRAN High Performance FORTRAN (HPF) is a set of extensions to FORTRAN 90 [11] for fine to medium grain data parallelism.
Reference: [11] <author> Loveman, </author> <title> D.B., High Performance Fortran. </title> <booktitle> Parallel & Distributed Technology, 1993. 1(1): p. </booktitle> <pages> 25-42. </pages>
Reference-contexts: Algorithms to automatically map clusters to processors has been proposed in [9, 10]. The tree structure representation for programs removes explicit dependence on the underlying machine architecture and aids portability. 2.2. High Performance FORTRAN High Performance FORTRAN (HPF) is a set of extensions to FORTRAN 90 <ref> [11] </ref> for fine to medium grain data parallelism. HPF inherits synchronous data parallelism from FORTRAN 90 but adds support for data distribution, communication, and the FORALL statement.
Reference: [12] <author> Lucco, S. and O. Sharp. Delirium: </author> <title> An Embedding Coordination Language. </title> <booktitle> in Proc. of Supercomputing. </booktitle> <year> 1990. </year>
Reference-contexts: The programmer expresses parallelism with the FORALL construct, which performs implicit synchronization. HPF does not allow for data type abstraction. 2.3. Delirium Delirium <ref> [12] </ref> is a coordination language that embeds sequential sub-computations written in C or FORTRAN. It is a more restrictive coordination language than Linda or PVM but it has the benefit of deterministic execution and reuse of existing code.
Reference: [13] <author> Gelernter, D., </author> <title> Generative Communication in Linda. </title> <journal> TOPLAS, 1985. </journal> <volume> 7(1): </volume> <pages> p. 80-112. </pages>
Reference-contexts: The user must state all variables each routine will destructively modify so the system can do dynamic dependency analysis and automatic scheduling of parallel processes. Some control of process mapping is possible. 2.4. Linda Linda <ref> [13, 14] </ref> defines a shared, associatively addressed, semi-persistent store called tuple space that holds tuples. Tuples can contain anything and the system transparently manages dispersing the tuples. Tuple space effectively replaces message passing and shared memory. A Linda program is initiated by putting tuples containing code in tuple space.
Reference: [14] <author> Carriero, N., D. Gelernter, and T.G. Mattson. </author> <title> Linda in Heterogeneous Computing Environments. </title> <booktitle> in Workshop on Heterogeneous Processing. </booktitle> <year> 1992. </year>
Reference-contexts: The user must state all variables each routine will destructively modify so the system can do dynamic dependency analysis and automatic scheduling of parallel processes. Some control of process mapping is possible. 2.4. Linda Linda <ref> [13, 14] </ref> defines a shared, associatively addressed, semi-persistent store called tuple space that holds tuples. Tuples can contain anything and the system transparently manages dispersing the tuples. Tuple space effectively replaces message passing and shared memory. A Linda program is initiated by putting tuples containing code in tuple space.
Reference: [15] <author> Foster, I., R. Olson, and S. Tuecke, </author> <title> Productive Parallel Programming: The PCN Approach. </title> <booktitle> Scientific Programming, </booktitle> <year> 1993. </year> . 
Reference-contexts: Linda provides only basic facilities for communication without providing for higher level abstractions of communication patterns. Similarly, synchronization abstractions (e.g. locks) must be built from the atomicity of tuple access. Linda does not address data typing issues. 2.5. PCN PCN <ref> [15] </ref> is a parallel programming language based on C and FORTRAN but adds the extensions of parallel composition and definitional variables. Parallel composition is the use of hierarchical abstractions by the programmer to define an arbitrary parallel model.
Reference: [16] <author> Sunderan, </author> <title> V.S., PVM: A Framework for Parallel Distributed Computing. </title> <journal> Concurrency: Practice and Experience, 1990. </journal> <volume> 2(4): </volume> <pages> p. 315-339. </pages>
Reference-contexts: Communication, synchronization, and scheduling details are implemented by the compiler. The compiler allows users to implement application-specific extensions to the PCN language. 2.6. PVM PVM <ref> [16] </ref> is not a language so much as a runtime facility that provides a standardized set of control parallel programming extensions to sequential languages. PVM provides an abstraction of a network of computers as a single MIMD virtual computer suited for coarse-grain parallelism.
Reference: [17] <author> Butler, R.M. and E.L. Lusk, </author> <title> Monitors, Messages, and Clusters: the p4 Parallel Programming System. </title> <journal> Journal of Parallel Computing, </journal> <note> 1993. To Appear. </note>
Reference-contexts: PVM does not address data type issues. 2.7. p4 P4 <ref> [17] </ref> is a set of macros that provide functionality similar to PVM. P4 also presents a network of computers as a single "virtual" computer. However, p4 is more concerned with efficiency than PVM.
Reference: [18] <author> Larus, J.R., B. Richards, and G. Viswanathan, </author> <title> C**: A Large-Grain, ObjectOriented, Data-Parallel Programming Language. </title> <type> Technical Report 1126, </type> <institution> University of Wisconsin-Madison, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: P4 does add shared memory as a communication abstraction and monitors as a synchronization abstraction to access shared memory, but does not greatly improve the level of abstraction. In p4, users have even less flexibility in placing processes on processors than in PVM. 2.8. C** C** <ref> [18] </ref>, based on C++, is an object oriented data parallel language for fine to coarse grain parallelism. C** offers the SIMD computational model benefit of nearly race-free execution without its restrictions of lockstep processing. The aggregate class declares an ordered collection of values which may be operated upon concurrently.
Reference: [19] <author> Lee, J.K. and D. Gannon. </author> <title> Object Oriented Parallel Programming Experiments and Results. </title> <booktitle> in Proc. of Supercomputing. </booktitle> <year> 1991. </year>
Reference-contexts: The user may slice aggregates along the dimensions thus controlling the grouping of values to be automatically distributed by the system. The code for communication and synchronization to implement parallel functions is automatically added by the compiler. 2.9. PC++ PC++ <ref> [19] </ref> is an object oriented programming language for expressing medium to coarse grain data parallelism.
Reference: [20] <author> Yonezawa, A., J.P. Briot, and E. </author> <title> Shibayama. </title> <booktitle> Object-Oriented Concurrent Programming in ABCL/1. in Proc. of OOPSLA. </booktitle> <year> 1986. </year>
Reference-contexts: There are no rules for structuring such knowledge and there are no mechanisms by which the system might use this knowledge. 2.10. ABCL/1 ABCL/1 <ref> [20] </ref> is an object oriented parallel programming language based on the Actor model for coarse grain control parallelism. In ABCL/1 each object is considered to have its own processing power and persistent memory. Communication is via messages and the objects retrieve messages from queues.
Reference: [21] <author> Rinard, </author> <title> M.C., D.J. Scales, and M.S. Lam, Jade: A High-Level Machine-Independent Language for Parallel Programming. </title> <booktitle> Computer, 1993. 26(6): p. </booktitle> <pages> 28-38. </pages>
Reference-contexts: Although the message abstraction is explicit to the user, its implementation details are hidden. Synchronization is performed through messages. There are no explicit mechanisms for influencing the placement of processes nor enhancements to standard type definition methods. 2.11. Jade Jade <ref> [21] </ref> is an object programming language that is designed for exploiting coarse grain control parallelism in a shared memory system. Parallelism is implicitly stated in a serial program and is dynamically extracted. The programmer, however, must supply information about the data accesses of each task.
Reference: [22] <author> Dobbing, B., </author> <title> Experiences with the Partitions Model. </title> <journal> ACM Ada Letters, </journal> <note> 1993. </note> <author> XIII(2): p. </author> <month> 65-77. </month>
Reference-contexts: The resource construct allows the programmer to dictate the host on which a task should be run for very coarse process assignment control. Based on C, Jade does not extend the type definition capabilities beyond those in C. 2.12. Ada 9X Ada 9X <ref> [22] </ref> augments Ada 83's tightly coupled, medium grain task model with constructs for coarse grain control parallelism and operator overloading for fine grain data parallelism. Used in combination, these features give the programmer a hierarchy of parallelism models. Control parallelism in Ada 9X closely follows a distributed computing model.
Reference: [23] <author> Kale, L.V. and S. Krishnan. CHARM++: </author> <title> A Portable Concurrent Object Oriented System Based on C++. </title> <booktitle> in Proc. of OOPSLA. </booktitle> <year> 1993. </year>
Reference-contexts: However, Ada 9X does specify how the programmer can control the distribution of passive or active partitions. Ada provides data abstraction and overloading of operators, but this does not provide the semantic knowledge needed by the compiler for optimization. 2.13. Charm++ Charm++ <ref> [23] </ref>, which is based on C++, encompasses both control and data parallelism. Charm++ attempts to divide up the work of managing parallelism between the programmer and the runtime system.
Reference: [24] <author> Grimshaw, </author> <title> A.S., MetaSystems: An Approach Combining Parallel Processing and Heterogeneous Distributed Computing Systems. </title> <type> Technical Report TR-92-43, </type> <institution> University of Virginia, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: C++ gives Charm++ the usual and desirable data abstraction capabilities of an objectoriented system, but does not provide for any additional semantic information. Charm++ permits only crude data distribution control and no control over process distribution. 2.14. Mentat Mentat <ref> [24, 25] </ref>, based on C++, is an object oriented language for defining medium to coarse grain control and data parallelism. The model supports only messages between objects, but the compiler automatically generates the code for all communication and synchronization.
Reference: [25] <author> Grimshaw, </author> <title> A.S., Easy-to-Use ObjectOriented Parallel Processing with Mentat. </title> <booktitle> Computer, 1993. </booktitle> <volume> 26( 5): </volume> <pages> p. 39-51. </pages>
Reference-contexts: C++ gives Charm++ the usual and desirable data abstraction capabilities of an objectoriented system, but does not provide for any additional semantic information. Charm++ permits only crude data distribution control and no control over process distribution. 2.14. Mentat Mentat <ref> [24, 25] </ref>, based on C++, is an object oriented language for defining medium to coarse grain control and data parallelism. The model supports only messages between objects, but the compiler automatically generates the code for all communication and synchronization.
Reference: [26] <author> Gelernter, D. and N. Carriero, </author> <title> Coordination Languages and their Significance. </title> <journal> CACM, 1992. </journal> <volume> 35(2): </volume> <pages> p. 97-107. </pages>
Reference-contexts: How the augmentation is achieved raises the second limitation of many of the surveyed languages: support for multiple targets. 3.3 Support for multiple target architectures Most of the languages we surveyed assume a single, homogeneous parallel target architecture. The exceptions are found among the coordination-oriented systems <ref> [26] </ref> such as PVM, P4, Linda, and Cluster-M. Although these enable a heterogeneous ensemble of architectures to work together, they do little to facilitate the portability and retargetting of code between architectures.
References-found: 26

