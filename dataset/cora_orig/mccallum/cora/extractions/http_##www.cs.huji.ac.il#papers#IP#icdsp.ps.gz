URL: http://www.cs.huji.ac.il/papers/IP/icdsp.ps.gz
Refering-URL: http://www.cs.huji.ac.il/papers/IP/index.html
Root-URL: 
Title: Isolating Multiple Image Motions for Enhancement and 3D Analysis  
Author: Michal Irani Benny Rousso Shmuel Peleg 
Address: 91904 Jerusalem, ISRAEL  
Affiliation: Institute of Computer Science The Hebrew University of Jerusalem  
Abstract: Motion computation in scenes having multiple moving objects is performed together with object segmentation by using a temporal integration approach. Using an accurate 2D motion estimate for image regions, they can be enhanced by fusing all successive frames covering the same region. Enhancement includes improvement of image resolution and filling-in occluded regions. It is also shown how an accurate 2D motion estimate for a single planar surface in a general static scene can help to compute the 3D motion performed by the camera. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Adiv. </author> <title> Determining three-dimensional motion and structure from optical flow generated by several moving objects. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(4) </volume> <pages> 384-401, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation [3, 5, 6]. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, [5, 4]) and projective transformation (8 parameters <ref> [1, 4] </ref>). Detecting the First Object. The motion parameters of a single object in the image plane can be recovered by applying the iterative detection method to the entire region of analysis. <p> Previous works on 3D motion estimation use the optical or normal flow field derived between two frames <ref> [1, 2, 8, 17, 20, 21] </ref>, or the correspondence of previously extracted distinguished features (points, lines, contours) [10, 22]. Methods for computing the ego-motion directly from image intensities were also suggested [9, 11, 23], but each method has its limitations. <p> The 3D scene structure can then be reconstructed from the computed 3D motion parameters of the camera (using a scheme similar to that suggested in [9]). 4.1 Projected 2D Motion When the field of view is not very large and the rotation is relatively small <ref> [1] </ref>, a 3D motion of the camera between two image frames creates a 2D displacement (u; v) of an image point (x; y) in the image plane, which can be expressed by [4]: v = f c ( T X Z + y Z x 2 Y f c Z X <p> By perspective projection, this yields: 1 Z = ff+fi x+fl y where: (x; y) are image coordinates, and ff = 1 A ; fi = B f c A : Therefore, Eq. (1) can be rewritten as <ref> [1, 4] </ref>: v s = a + b x + c y + g x 2 + h xy where: a = f c ffT X f c Y e = Z f c fiT Y c = Z f c flT X g = Y d = f c ffT
Reference: [2] <author> Y. Aloimonos and Z. Duric. </author> <title> Active egomotion estimation: A qualitative approach. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 497-510, </pages> <address> Santa Margarita Ligure, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames <ref> [1, 2, 8, 17, 20, 21] </ref>, or the correspondence of previously extracted distinguished features (points, lines, contours) [10, 22]. Methods for computing the ego-motion directly from image intensities were also suggested [9, 11, 23], but each method has its limitations.
Reference: [3] <author> J.R. Bergen and E.H. Adelson. </author> <title> Hierarchical, computationally efficient motion estimation algorithm. </title> <journal> J. Opt. Soc. Am. A., </journal> <volume> 4:35, </volume> <year> 1987. </year>
Reference-contexts: This assumption is valid when the differences in depth caused by the motions are small relative to the distances of the objects from the camera. We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation <ref> [3, 5, 6] </ref>. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, [5, 4]) and projective transformation (8 parameters [1, 4]). Detecting the First Object.
Reference: [4] <author> J.R. Bergen, P. Anandan, K.J. Hanna, and R. Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 237-252, </pages> <address> Santa Margarita Ligure, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation [3, 5, 6]. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, <ref> [5, 4] </ref>) and projective transformation (8 parameters [1, 4]). Detecting the First Object. The motion parameters of a single object in the image plane can be recovered by applying the iterative detection method to the entire region of analysis. <p> We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation [3, 5, 6]. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, [5, 4]) and projective transformation (8 parameters <ref> [1, 4] </ref>). Detecting the First Object. The motion parameters of a single object in the image plane can be recovered by applying the iterative detection method to the entire region of analysis. <p> Projected 2D Motion When the field of view is not very large and the rotation is relatively small [1], a 3D motion of the camera between two image frames creates a 2D displacement (u; v) of an image point (x; y) in the image plane, which can be expressed by <ref> [4] </ref>: v = f c ( T X Z + y Z x 2 Y f c Z X ) x Z + y T Z f c + y 2 X # where, (X; Y; Z) denote the Cartesian coordinates of the scene point projected onto (x; y), (T X <p> By perspective projection, this yields: 1 Z = ff+fi x+fl y where: (x; y) are image coordinates, and ff = 1 A ; fi = B f c A : Therefore, Eq. (1) can be rewritten as <ref> [1, 4] </ref>: v s = a + b x + c y + g x 2 + h xy where: a = f c ffT X f c Y e = Z f c fiT Y c = Z f c flT X g = Y d = f c ffT
Reference: [5] <author> J.R. Bergen, P.J. Burt, K. Hanna, R. Hingorani, P. Jeanne, and S. Peleg. </author> <title> Dynamic multiple-motion computation. In Y.A. </title> <editor> Feldman and A. Bruckstein, editors, </editor> <booktitle> Artificial Intelligence and Computer Vision: Proceedings of the Israeli Conference, </booktitle> <pages> pages 147-156. </pages> <publisher> Elsevier, </publisher> <year> 1991. </year>
Reference-contexts: This assumption is valid when the differences in depth caused by the motions are small relative to the distances of the objects from the camera. We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation <ref> [3, 5, 6] </ref>. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, [5, 4]) and projective transformation (8 parameters [1, 4]). Detecting the First Object. <p> We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation [3, 5, 6]. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, <ref> [5, 4] </ref>) and projective transformation (8 parameters [1, 4]). Detecting the First Object. The motion parameters of a single object in the image plane can be recovered by applying the iterative detection method to the entire region of analysis.
Reference: [6] <author> J.R. Bergen, P.J. Burt, R. Hingorani, and S. Peleg. </author> <title> Computing two motions from three frames. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 27-32, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: This assumption is valid when the differences in depth caused by the motions are small relative to the distances of the objects from the camera. We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation <ref> [3, 5, 6] </ref>. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, [5, 4]) and projective transformation (8 parameters [1, 4]). Detecting the First Object.
Reference: [7] <author> P.J. Burt, R. Hingorani, and R.J. Kolczynski. </author> <title> Mechanisms for isolating component patterns in the sequential analysis of multiple motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 187-193, </pages> <address> Princeton, New Jersey, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: The scene contains four moving objects. b) The temporally integrated image after 5 frames. The tracked motion is that of the ball. All other regions blur out. their regions of support <ref> [7, 16] </ref>. Once a motion has been determined, we would like to identify the region having this motion. To simplify the problem, the two images are registered using the detected motion. The motion of the corresponding region is therefore canceled, and the problem becomes that of identifying the stationary regions.
Reference: [8] <author> R. Guissin and S. Ullman. </author> <title> Direct computation of the focus of expansion from velocity field measurements. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 146-155, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames <ref> [1, 2, 8, 17, 20, 21] </ref>, or the correspondence of previously extracted distinguished features (points, lines, contours) [10, 22]. Methods for computing the ego-motion directly from image intensities were also suggested [9, 11, 23], but each method has its limitations.
Reference: [9] <author> K. Hanna. </author> <title> Direct multi-resolution estimation of ego-motion and structure from motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 156-162, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames [1, 2, 8, 17, 20, 21], or the correspondence of previously extracted distinguished features (points, lines, contours) [10, 22]. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [9, 11, 23] </ref>, but each method has its limitations. In this section we propose the following scheme in order to use the robustness of the 2D motion computation for computing 3D motion: 1. The 2D image motion of a single planar surface is computed (Sect. 2). 2. <p> It is induced by pure translation, and points to the correct FOE (marked by +). 4. The 3D scene structure can then be reconstructed from the computed 3D motion parameters of the camera (using a scheme similar to that suggested in <ref> [9] </ref>). 4.1 Projected 2D Motion When the field of view is not very large and the rotation is relatively small [1], a 3D motion of the camera between two image frames creates a 2D displacement (u; v) of an image point (x; y) in the image plane, which can be expressed <p> T Z was therefore set to the correct size 12cm, and the other parameters were then scaled accordingly). Once the 3D motion parameters of the camera were computed, the 3D scene structure was reconstructed using a scheme similar to that suggested in <ref> [9] </ref>. In Fig. 6, the computed inverse depth map of the scene ( 1 Z (x;y) ) is displayed. 5 Concluding Remarks A method was presented for detecting and tracking several moving objects in dynamic scenes, using 2D parameterization.
Reference: [10] <author> B.K.P. Horn. </author> <title> Relative orientation. </title> <journal> International Journal of Computer Vision, </journal> <volume> 4(1) </volume> <pages> 58-78, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames [1, 2, 8, 17, 20, 21], or the correspondence of previously extracted distinguished features (points, lines, contours) <ref> [10, 22] </ref>. Methods for computing the ego-motion directly from image intensities were also suggested [9, 11, 23], but each method has its limitations. In this section we propose the following scheme in order to use the robustness of the 2D motion computation for computing 3D motion: 1.
Reference: [11] <author> B.K.P. Horn and E.J. Weldon. </author> <title> Direct methods for recovering motion. </title> <journal> International Journal of Computer Vision, </journal> <volume> 2(1) </volume> <pages> 51-76, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames [1, 2, 8, 17, 20, 21], or the correspondence of previously extracted distinguished features (points, lines, contours) [10, 22]. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [9, 11, 23] </ref>, but each method has its limitations. In this section we propose the following scheme in order to use the robustness of the 2D motion computation for computing 3D motion: 1. The 2D image motion of a single planar surface is computed (Sect. 2). 2.
Reference: [12] <author> T.S. Huang and R.Y. Tsai. </author> <title> Multi-frame image restoration and registration. In T.S. </title> <editor> Huang, editor, </editor> <booktitle> Advances in Computer Vision and Image Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 317-339. </pages> <publisher> JAI Press Inc., </publisher> <year> 1984. </year>
Reference-contexts: In [13, 14] an algorithm was presented for processing image sequences to obtain improved resolution of differently moving objects. While earlier research on super-resolution <ref> [12, 13, 18] </ref> has dealt only with static scenes and with pure translational motion of the entire scene in the image plane, we deal with dynamic scenes and with more complex 2D motions.
Reference: [13] <author> M. Irani and S. Peleg. </author> <title> Improving resolution by image registration. CVGIP: Graphical Models and Image Processing, </title> <booktitle> 53 </booktitle> <pages> 231-239, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The method has been applied successfully to 2D affine and projective motions in the image plane. Once an object has been tracked and segmented, it can be enhanced using information from several frames <ref> [13, 14] </ref>. Enhancement includes filling-in occluded regions and improving spatial resolution. The 2D detection and tracking algorithm can also be used for estimating the camera motion (ego-motion) in general static 3D scenes. <p> The methods presented for image enhancement are reconstruction of occluded segments and improvement of spatial resolution. More details can be found in <ref> [13, 14] </ref>. 3.1 Reconstruction of Occlusions When parts of a tracked object are occluded in some frames, but appear in others, a more complete view of the object can be reconstructed. The image frames are registered using the computed motion parameters. <p> Resolution improvement by modifying the sensor can be prohibitive. An increase in the sampling rate could, however, be achieved by obtaining more samples of the imaged object from a sequence of images in which the object appears moving. In <ref> [13, 14] </ref> an algorithm was presented for processing image sequences to obtain improved resolution of differently moving objects. <p> In [13, 14] an algorithm was presented for processing image sequences to obtain improved resolution of differently moving objects. While earlier research on super-resolution <ref> [12, 13, 18] </ref> has dealt only with static scenes and with pure translational motion of the entire scene in the image plane, we deal with dynamic scenes and with more complex 2D motions. <p> The segmentation of the image plane into the differently moving objects and their tracking, using the algorithm mentioned in Section 2 enables processing of each object separately. The algorithm presented in <ref> [13, 14] </ref> for increasing the image resolution is similar to common iterative methods for solving sets of linear equations [19], and has similar properties, such as a rapid convergence (at exponential rate). <p> The difference images fg k g (0) k g are then computed, and are used to improve the resolution of the initial guess by reducing the total energy in those difference images. For details see <ref> [13, 14] </ref>.
Reference: [14] <author> M. Irani and S. Peleg. </author> <title> Image sequence enhancement using multiple motions analysis. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> Champaign, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The method has been applied successfully to 2D affine and projective motions in the image plane. Once an object has been tracked and segmented, it can be enhanced using information from several frames <ref> [13, 14] </ref>. Enhancement includes filling-in occluded regions and improving spatial resolution. The 2D detection and tracking algorithm can also be used for estimating the camera motion (ego-motion) in general static 3D scenes. <p> Sect. 3 describes the algorithms for image enhancement. Sect. 4 describes the method for computing the 3D motion of the camera (the ego-motion) in a static scene. More details can be found in <ref> [14, 15, 16] </ref>. 2 Multiple Motions in Image Sequences To detect differently moving objects in an image pair, a single motion is first computed, and a single object which corresponds to this motion is identified. We call this motion the dominant motion, and the corresponding object the dominant object. <p> The methods presented for image enhancement are reconstruction of occluded segments and improvement of spatial resolution. More details can be found in <ref> [13, 14] </ref>. 3.1 Reconstruction of Occlusions When parts of a tracked object are occluded in some frames, but appear in others, a more complete view of the object can be reconstructed. The image frames are registered using the computed motion parameters. <p> Resolution improvement by modifying the sensor can be prohibitive. An increase in the sampling rate could, however, be achieved by obtaining more samples of the imaged object from a sequence of images in which the object appears moving. In <ref> [13, 14] </ref> an algorithm was presented for processing image sequences to obtain improved resolution of differently moving objects. <p> The segmentation of the image plane into the differently moving objects and their tracking, using the algorithm mentioned in Section 2 enables processing of each object separately. The algorithm presented in <ref> [13, 14] </ref> for increasing the image resolution is similar to common iterative methods for solving sets of linear equations [19], and has similar properties, such as a rapid convergence (at exponential rate). <p> The difference images fg k g (0) k g are then computed, and are used to improve the resolution of the initial guess by reducing the total energy in those difference images. For details see <ref> [13, 14] </ref>.
Reference: [15] <author> M. Irani, B. Rousso, and S. Peleg. </author> <note> Computing occluding and transparent motions. To appear in International Journal of Computer Vision. </note>
Reference-contexts: Sect. 3 describes the algorithms for image enhancement. Sect. 4 describes the method for computing the 3D motion of the camera (the ego-motion) in a static scene. More details can be found in <ref> [14, 15, 16] </ref>. 2 Multiple Motions in Image Sequences To detect differently moving objects in an image pair, a single motion is first computed, and a single object which corresponds to this motion is identified. We call this motion the dominant motion, and the corresponding object the dominant object. <p> Temporal integration is then used to track detected objects throughout the image sequence. More details can be found in <ref> [15] </ref>. It is assumed that the projected 3D motions of the objects can be approximated by some 2D parametric transformation in the image plane. This assumption is valid when the differences in depth caused by the motions are small relative to the distances of the objects from the camera. <p> To simplify the problem, the two images are registered using the detected motion. The motion of the corresponding region is therefore canceled, and the problem becomes that of identifying the stationary regions. Detection of stationary regions is described in <ref> [15] </ref>. Tracking by Temporal Integration. Once an object has been detected, it can be tracked throughout the image sequence. This is done by using temporal integration of images registered with respect to the tracked motion. The temporally integrated image serves as a dynamic internal representation image of the tracked object. <p> The temporally integrated image serves as a dynamic internal representation image of the tracked object. Let fI (t)g denote the image sequence, and let M (t) denote the segmentation mask of the tracked object computed for frame I (t), using the segmentation method described in <ref> [15] </ref>. Initially, M (0) is the entire region of analysis.
Reference: [16] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Detecting and tracking multiple moving objects using temporal integration. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 282-287, </pages> <address> Santa Margarita Ligure, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Sect. 3 describes the algorithms for image enhancement. Sect. 4 describes the method for computing the 3D motion of the camera (the ego-motion) in a static scene. More details can be found in <ref> [14, 15, 16] </ref>. 2 Multiple Motions in Image Sequences To detect differently moving objects in an image pair, a single motion is first computed, and a single object which corresponds to this motion is identified. We call this motion the dominant motion, and the corresponding object the dominant object. <p> The scene contains four moving objects. b) The temporally integrated image after 5 frames. The tracked motion is that of the ball. All other regions blur out. their regions of support <ref> [7, 16] </ref>. Once a motion has been determined, we would like to identify the region having this motion. To simplify the problem, the two images are registered using the detected motion. The motion of the corresponding region is therefore canceled, and the problem becomes that of identifying the stationary regions.
Reference: [17] <author> A.D. </author> <title> Jepson and D.J. Heeger. A fast subspace algorithm for recovering rigid motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 124-131, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames <ref> [1, 2, 8, 17, 20, 21] </ref>, or the correspondence of previously extracted distinguished features (points, lines, contours) [10, 22]. Methods for computing the ego-motion directly from image intensities were also suggested [9, 11, 23], but each method has its limitations.
Reference: [18] <author> S.P. Kim, N.K. Bose, and H.M. valenzuela. </author> <title> Recursive reconstruction of high resolution image from noisy under-sampled multiframes. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 38(6) </volume> <pages> 1013-1027, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In [13, 14] an algorithm was presented for processing image sequences to obtain improved resolution of differently moving objects. While earlier research on super-resolution <ref> [12, 13, 18] </ref> has dealt only with static scenes and with pure translational motion of the entire scene in the image plane, we deal with dynamic scenes and with more complex 2D motions.
Reference: [19] <author> R.L. Lagendijk and J. Biemond. </author> <title> Iterative Identification and Restoration of Images. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston/Dordrecht/London, </address> <year> 1991. </year>
Reference-contexts: The algorithm presented in [13, 14] for increasing the image resolution is similar to common iterative methods for solving sets of linear equations <ref> [19] </ref>, and has similar properties, such as a rapid convergence (at exponential rate).
Reference: [20] <author> D.T. Lawton and J.H. Rieger. </author> <title> The use of difference fields in processing sensor motion. </title> <booktitle> In DARPA IUWorkshop, </booktitle> <pages> pages 78-83, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames <ref> [1, 2, 8, 17, 20, 21] </ref>, or the correspondence of previously extracted distinguished features (points, lines, contours) [10, 22]. Methods for computing the ego-motion directly from image intensities were also suggested [9, 11, 23], but each method has its limitations.
Reference: [21] <author> S. Negahdaripour and S. Lee. </author> <title> Motion recovery from image sequences using first-order optical flow information. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 132-139, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames <ref> [1, 2, 8, 17, 20, 21] </ref>, or the correspondence of previously extracted distinguished features (points, lines, contours) [10, 22]. Methods for computing the ego-motion directly from image intensities were also suggested [9, 11, 23], but each method has its limitations.
Reference: [22] <author> F. Lustman O.D. Faugeras and G. Toscani. </author> <title> Motion and structure from motion from point and line matching. </title> <booktitle> In Proc. 1st International Conference on Computer Vision, </booktitle> <pages> pages 25-34, </pages> <address> London, </address> <year> 1987. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames [1, 2, 8, 17, 20, 21], or the correspondence of previously extracted distinguished features (points, lines, contours) <ref> [10, 22] </ref>. Methods for computing the ego-motion directly from image intensities were also suggested [9, 11, 23], but each method has its limitations. In this section we propose the following scheme in order to use the robustness of the 2D motion computation for computing 3D motion: 1.
Reference: [23] <author> M.A. Taalebinezhaad. </author> <title> Direct recovery of motion and shape in the general case by fixation. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 14 </volume> <pages> 847-853, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames [1, 2, 8, 17, 20, 21], or the correspondence of previously extracted distinguished features (points, lines, contours) [10, 22]. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [9, 11, 23] </ref>, but each method has its limitations. In this section we propose the following scheme in order to use the robustness of the 2D motion computation for computing 3D motion: 1. The 2D image motion of a single planar surface is computed (Sect. 2). 2.
References-found: 23

