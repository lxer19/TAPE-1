URL: http://www.cs.colostate.edu/~ftppub/TechReports/1994/tr-109.ps.Z
Refering-URL: http://www.cs.colostate.edu/~ftppub/
Root-URL: 
Affiliation: Department of Computer Science Colorado State University  
Abstract: On Input Profile Selection For Software Testing Naixin Li and Yashwant K. Malaiya Technical Report CS-94-109 March 15, 1994 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E.N. Adams, </author> <title> Optimizing Preventive Service of Software Products, </title> <journal> IBM Journal of Research and Development, </journal> <volume> Vol. 28, No. 1, </volume> <month> January </month> <year> 1984, </year> <pages> pp. 2-13. 14 </pages>
Reference-contexts: However, even for this case, uniform testing gives better MTTF once the number of tests exceeds about 110. 4 Usage Testing vs. Coverage Testing Adams' study of some real software <ref> [1] </ref> shows that the operational failure rates for different projects follow a similar distribution with the number of faults having a certain failure rate being inversely proportional to the failure rate. Figure 7 plots the relative detectability profiles from two projects and the average detectability profile for 9 other projects. <p> Assumption 3: The failure rate distribution remains the same when testing starts and after testing finishes. Adams' data <ref> [1] </ref> gives the distribution of failures collected from operational use. For an untested software, the distribution of faults over different detectabilities would be more uniform.
Reference: [2] <author> B. Beizer, </author> <title> Software Testing Techniques, 2nd Edition, </title> <publisher> Van Nostrand Reinhold, </publisher> <year> 1990. </year>
Reference: [3] <author> J.R. Brown, and M. </author> <title> Lipow The Quantitative Measurement of Software Safety and Reliability, </title> <type> TRW Report QR 1776, </type> <month> August </month> <year> 1973. </year>
Reference: [4] <author> M.H. Chen, J.R. Horgan, A.P. Mathur and V.J. Rego, </author> <title> "A time/structure based model for estimating software reliability," </title> <institution> SERC-TR-117-P, Purdue University, </institution> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Piwowarski et al [22] observed that error removal rate and code coverage are closely related by a somewhat linear function. Vouk [28] suggested that the relation between software coverage and fault removal rate follows the Weibull distribution. Chen et al <ref> [4] </ref> suggested exclusion of the tests that do not contribute to any type of coverage from consideration when using traditional software reliability models. Malaiya et al [14] proposed a new model to relate test coverage to software reliability. <p> Sneed [25] reported his experience on comparing the effectiveness of branch coverage and data coverage in catching real bugs. Piwowarski et al [22] analytically derived a model characterizing the test coverage growth. Vouk [28] has derived a different model starting with different assumptions. Chen et al <ref> [4] </ref> proposed incorporating test coverage into traditional time-based software reliability models by filtering out the test efforts that contributes to no new coverage. Frankl and Weiss [8] empirically evaluated the fault exposing capability of branch coverage and data flow coverage criteria.
Reference: [5] <author> R. Cobb and H. Mills, </author> <title> Engineering Software under Statistical Quality Control, </title> <journal> IEEE Software, </journal> <month> November </month> <year> 1990, </year> <pages> pp. 44-56. </pages>
Reference-contexts: Musa has given detailed steps for the construction of operational profile and the associated test input selection [19]. Cobb and Mills <ref> [5] </ref> mentioned that operational profile based (usage) testing is 20 times more effective than coverage testing. We examine this aspect of testing in detail here. Another purpose of software testing is to assess the software quality. <p> Figure 7 plots the relative detectability profiles from two projects and the average detectability profile for 9 other projects. Cobb and Mills <ref> [5] </ref> used Adams' data in their computation and came to the conclusion that usage testing testing is about 20 times more effective than (statement) coverage testing. 10 If this is true in general, than there is no need to do coverage testing.
Reference: [6] <author> S.R. Dalal, J.R. Horgan and J.R. Kettenring, </author> <title> "Reliable Software and Communications: Software Quality, Reliability and Safety," </title> <booktitle> Proc. 15th Int. Conf. Software Engineering, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. 425-435 </pages>
Reference-contexts: A similar concept termed accelerating factor was used by Drake and Wolting [7]. Using repair data, they computed the value of acceleration factors for two terminal firmware systems to be 47805 and 45532. Observations <ref> [6, 8, 28] </ref> of significant correlation between structural coverage and error removal growth and work by Malaiya et al [13] also support that real testing can be more effective than random sampling over operational usage distribution.
Reference: [7] <author> H.D.Drake and D.E.Wolting, </author> <title> Reliability Theory Applied to Software Testing" Hewlett-Packard Journal, </title> <month> April, </month> <year> 1987, </year> <pages> pp. 35-39. </pages>
Reference-contexts: Musa et al [17] also noted that equivalence partition testing can increase the test compression factor. A similar concept termed accelerating factor was used by Drake and Wolting <ref> [7] </ref>. Using repair data, they computed the value of acceleration factors for two terminal firmware systems to be 47805 and 45532.
Reference: [8] <author> P.G.Frankl and N.Weiss, </author> <title> "An Experimental Comparison of the Effectiveness of Branch Testing and Data Flow Testing," </title> <journal> IEEE Trans. Soft. Eng., </journal> <month> Aug. </month> <year> 1993, </year> <pages> pp. 774-787. </pages>
Reference-contexts: A similar concept termed accelerating factor was used by Drake and Wolting [7]. Using repair data, they computed the value of acceleration factors for two terminal firmware systems to be 47805 and 45532. Observations <ref> [6, 8, 28] </ref> of significant correlation between structural coverage and error removal growth and work by Malaiya et al [13] also support that real testing can be more effective than random sampling over operational usage distribution. <p> Vouk [28] has derived a different model starting with different assumptions. Chen et al [4] proposed incorporating test coverage into traditional time-based software reliability models by filtering out the test efforts that contributes to no new coverage. Frankl and Weiss <ref> [8] </ref> empirically evaluated the fault exposing capability of branch coverage and data flow coverage criteria. Malaiya et al [14] suggested a hypothesis that different test coverage growths follow an logarithmic trend.
Reference: [9] <author> A.L.Goel, </author> <title> "Software Reliability Models: Assumptions, Limitations, and Applicability", </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> Vol. </volume> <month> December </month> <year> 1985, </year> <pages> p. 1411-1423. </pages>
Reference-contexts: The accuracy 2 of such estimation requires that the software should be exercised during testing phase in a similar way, or following the same input distribution, as the software in operational usage. Indeed, this is an assumption generally made for software reliability models <ref> [9] </ref>. If the input selection during testing phase is different in distribution from that in operation, some adjustment should be made to account for the differences. Musa et al [17] introduce a concept termed test compression factor for this purpose.
Reference: [10] <author> H. Hecht and P. Crane, </author> <title> "Rare Conditions and Their Effect on Software Failures" Proc. </title> <booktitle> Annual Reliability and Maintainability Symposium, </booktitle> <year> 1994, </year> <pages> pp. 334-337. </pages>
Reference-contexts: Observations [6, 8, 28] of significant correlation between structural coverage and error removal growth and work by Malaiya et al [13] also support that real testing can be more effective than random sampling over operational usage distribution. Data gathered by Hecht and Crane <ref> [10] </ref> indicate that code segments for rare conditions, like exception handling, have a much higher failure rate than normal code. Since such code segments are not readily exercised during software testing, relatively more errors (corresponding to higher error rates) are left undetected in such segments.
Reference: [11] <author> Y. Levendal, </author> <title> Improving quality with a Manufacturing Process, </title> <journal> IEEE Software, </journal> <month> March </month> <year> 1991, </year> <pages> pp. 13-25. </pages>
Reference: [12] <author> M.R. Lyu, J.R. Horgan and S. </author> <title> London, "A coverage Analysis Tool for the Effectiveness of Software Testing" IEEE Int. </title> <booktitle> Symp. on Software Reliability Engineering, </booktitle> <year> 1993, </year> <pages> pp. 25-34. </pages>
Reference-contexts: Statement coverage (or block coverage) and branch coverage are the most used coverage measures in practice. Other coverages such as data flow coverages also becomes well-known. Tools are now available for collecting the coverage data of test inputs for some metrics: block, branch, c-use, p-use, all-use <ref> [12] </ref>. Some work is being done to study the test coverage growth and its relation to fault removal rate or software reliability achieved.
Reference: [13] <author> Y. K. Malaiya, A. von Mayrhauser and P. Srimani, </author> <title> "An examination of Fault Exposure Ratio," </title> <note> to appear in IEEE Trans. </note> <institution> Software Engineering, </institution> <year> 1993. </year>
Reference-contexts: Using repair data, they computed the value of acceleration factors for two terminal firmware systems to be 47805 and 45532. Observations [6, 8, 28] of significant correlation between structural coverage and error removal growth and work by Malaiya et al <ref> [13] </ref> also support that real testing can be more effective than random sampling over operational usage distribution. Data gathered by Hecht and Crane [10] indicate that code segments for rare conditions, like exception handling, have a much higher failure rate than normal code.
Reference: [14] <author> Y.K.Malaiya, N. Li, J.Bieman, R. Karcich and B. Skibbe, </author> <title> The Relation Between Software Test Coverage and Reliability, </title> <type> Technical Report, </type> <year> 1994. </year>
Reference-contexts: Vouk [28] suggested that the relation between software coverage and fault removal rate follows the Weibull distribution. Chen et al [4] suggested exclusion of the tests that do not contribute to any type of coverage from consideration when using traditional software reliability models. Malaiya et al <ref> [14] </ref> proposed a new model to relate test coverage to software reliability. <p> Chen et al [4] proposed incorporating test coverage into traditional time-based software reliability models by filtering out the test efforts that contributes to no new coverage. Frankl and Weiss [8] empirically evaluated the fault exposing capability of branch coverage and data flow coverage criteria. Malaiya et al <ref> [14] </ref> suggested a hypothesis that different test coverage growths follow an logarithmic trend. Based on this 13 hypothesis, software fault removal rate and software reliability can be estimated directly from static test coverage measures. Still more empirical data and analytical studies correlating such coverage measures and reliability are needed.
Reference: [15] <author> Y. K. Malaiya and P. Verma, </author> <title> Testability Profile Approach to Software Reliability, Advances in Reliability and Quality Control (Ed. </title> <editor> M.H. Hamza), </editor> <publisher> Acta Press, </publisher> <month> December, </month> <year> 1988, </year> <pages> pp. 67-71. </pages>
Reference-contexts: Obviously op 1 + op 2 = 1. There are exactly 2 faults in the software. Fault 1 can be detected only by inputs from S1, with detectability <ref> [15] </ref> of d 1 = 1 p 1 in S1.
Reference: [16] <author> H.D. Mills, M. Dyer and R.C. </author> <title> Linger, </title> <journal> Cleanroom Software Engineering, IEEE Software, </journal> <month> Nov. </month> <year> 1986, </year> <pages> pp. 19-24. </pages>
Reference-contexts: 1 Introduction Significant effort is now being devoted to develop techniques to deliver reliable software. Methods proposed include well-controlled software development practice such as the clean-room approach <ref> [16, 24] </ref>, formal verification and testing. Cleanroom approach significantly reduces the number of faults introduced during the early phases of software life cycle, but it cannot totally avoid the problem of software faults and failures. <p> For the purpose of reliability certification or prediction, software test input selection should follow the software's operational profile <ref> [16] </ref>. Also, operational profile based testing can be efficient if very limited amount of testing is available. If our main objective of testing is fault removal, operational profile based testing must be supplemented by coverage based testing.
Reference: [17] <author> J.D. Musa, A Iannino, K. Okumoto, </author> <title> Software Reliability, Measurement, Prediction, Application, </title> <publisher> McGraw-Hill, </publisher> <year> 1987. </year> <month> 15 </month>
Reference-contexts: Indeed, this is an assumption generally made for software reliability models [9]. If the input selection during testing phase is different in distribution from that in operation, some adjustment should be made to account for the differences. Musa et al <ref> [17] </ref> introduce a concept termed test compression factor for this purpose. In contrast with real operational use, input states for software during testing phase are generally not repeated or repeated with much lower frequency. <p> In contrast with real operational use, input states for software during testing phase are generally not repeated or repeated with much lower frequency. Thus, actual test inputs are more effective in revealing errors than random sampling according to operational usage patterns. An simple example was given in <ref> [17] </ref> to illustrate the concept of test compression factor, which is quoted below. Assume that a program has only two input states, A and B. Input state A occurs 90 percent of the time; B, 10 percent. All runs take 1 CPU hr. <p> In test, the coverage can be accomplished in 2 CPU hr. The testing compression factor would be 5 in this case. Based on some assumptions, Musa et al <ref> [17] </ref> computed that the test compression factor varies from 8 to 20 for softwares with the number of input states ranging from 10 3 to 10 9 . Musa et al [17] also noted that equivalence partition testing can increase the test compression factor. <p> The testing compression factor would be 5 in this case. Based on some assumptions, Musa et al <ref> [17] </ref> computed that the test compression factor varies from 8 to 20 for softwares with the number of input states ranging from 10 3 to 10 9 . Musa et al [17] also noted that equivalence partition testing can increase the test compression factor. A similar concept termed accelerating factor was used by Drake and Wolting [7]. Using repair data, they computed the value of acceleration factors for two terminal firmware systems to be 47805 and 45532. <p> Quantitative evaluation of their effectiveness remains a problem and calls for more experimentation and experience to fully understand the testing processes. 12 5 Testing For Reliability The operational profile of a software system can be used at different stages in the software's lifetime <ref> [17] </ref>. For the purpose of reliability certification or prediction, software test input selection should follow the software's operational profile [16]. Also, operational profile based testing can be efficient if very limited amount of testing is available.
Reference: [18] <author> J.D. Musa, </author> <title> The Operational Profile in Software Reliability Engineering: An Overview, </title> <booktitle> Proc. of International Symposium on Software Reliability Engineering, </booktitle> <month> October, </month> <year> 1992. </year> <pages> pp. 140-154. </pages>
Reference-contexts: This gives rise to the idea of operational profile-based testing <ref> [18, 19] </ref> which involves partitioning input space into domains and selecting inputs from each domain based on its frequency during operational use. Musa has given detailed steps for the construction of operational profile and the associated test input selection [19]. <p> Typically the number of domains obtained during the construction of operational profile can be hundreds or even thousands for very large projects <ref> [18, 19] </ref>. For such cases, we can still get an analytical optimal distribution for test input selection. Lets assume that a program's input space consists of m domains with one fault associated with each domain with the same detectability (1 p).
Reference: [19] <author> J.D. Musa, </author> <title> Operational Profiles in Software Reliability Engineering, </title> <journal> IEEE Software, </journal> <month> March </month> <year> 1993, </year> <pages> pp. 14-32. </pages>
Reference-contexts: This gives rise to the idea of operational profile-based testing <ref> [18, 19] </ref> which involves partitioning input space into domains and selecting inputs from each domain based on its frequency during operational use. Musa has given detailed steps for the construction of operational profile and the associated test input selection [19]. <p> This gives rise to the idea of operational profile-based testing [18, 19] which involves partitioning input space into domains and selecting inputs from each domain based on its frequency during operational use. Musa has given detailed steps for the construction of operational profile and the associated test input selection <ref> [19] </ref>. Cobb and Mills [5] mentioned that operational profile based (usage) testing is 20 times more effective than coverage testing. We examine this aspect of testing in detail here. Another purpose of software testing is to assess the software quality. <p> Typically the number of domains obtained during the construction of operational profile can be hundreds or even thousands for very large projects <ref> [18, 19] </ref>. For such cases, we can still get an analytical optimal distribution for test input selection. Lets assume that a program's input space consists of m domains with one fault associated with each domain with the same detectability (1 p).
Reference: [20] <author> S.C. Natfos, </author> <title> "An Evaluation of Required Elemente Testing Strategies", </title> <booktitle> 7th Int. Conf. on Software Engineering, </booktitle> <month> March </month> <year> 1984. </year>
Reference-contexts: Tools are now available for collecting the coverage data of test inputs for some metrics: block, branch, c-use, p-use, all-use [12]. Some work is being done to study the test coverage growth and its relation to fault removal rate or software reliability achieved. For example, Ntafos <ref> [20, 21] </ref> compared the effectiveness of random testing with that of branch testing and all-uses testing, and observed that coverage testing is much more effective in revealing errors. Ramsey and Basili [23] empirically investigated the relationship between the number of tests and the test coverage.
Reference: [21] <author> S.C. Ntafos, </author> <title> "On Required Element Testing" IEEE Trans. </title> <booktitle> on Software Engineering, </booktitle> <month> October </month> <year> 1984, </year> <pages> pp. 795-803. </pages>
Reference-contexts: Tools are now available for collecting the coverage data of test inputs for some metrics: block, branch, c-use, p-use, all-use [12]. Some work is being done to study the test coverage growth and its relation to fault removal rate or software reliability achieved. For example, Ntafos <ref> [20, 21] </ref> compared the effectiveness of random testing with that of branch testing and all-uses testing, and observed that coverage testing is much more effective in revealing errors. Ramsey and Basili [23] empirically investigated the relationship between the number of tests and the test coverage.
Reference: [22] <author> P. Piwowarski, M. Ohba and J. Caruso, </author> <title> "Coverage Measurement Experience During Function Test," </title> <booktitle> ICSE'93, </booktitle> <pages> pp. 287-300. </pages>
Reference-contexts: Ramsey and Basili noticed that the number of faults detected in a procedure are independent of the number of times the procedure is executed [23]. Piwowarski et al <ref> [22] </ref> observed that error removal rate and code coverage are closely related by a somewhat linear function. Vouk [28] suggested that the relation between software coverage and fault removal rate follows the Weibull distribution. <p> Ramsey and Basili [23] empirically investigated the relationship between the number of tests and the test coverage. Sneed [25] reported his experience on comparing the effectiveness of branch coverage and data coverage in catching real bugs. Piwowarski et al <ref> [22] </ref> analytically derived a model characterizing the test coverage growth. Vouk [28] has derived a different model starting with different assumptions. Chen et al [4] proposed incorporating test coverage into traditional time-based software reliability models by filtering out the test efforts that contributes to no new coverage.
Reference: [23] <author> J. Ramsey and V.R. Basili, </author> <title> "Analyzing the Test Process Using Structural Coverage," </title> <booktitle> Proc. 8th international conference on Software Engineering, </booktitle> <month> August </month> <year> 1985, </year> <pages> pp. 306-3312. </pages>
Reference-contexts: Ramsey and Basili noticed that the number of faults detected in a procedure are independent of the number of times the procedure is executed <ref> [23] </ref>. Piwowarski et al [22] observed that error removal rate and code coverage are closely related by a somewhat linear function. Vouk [28] suggested that the relation between software coverage and fault removal rate follows the Weibull distribution. <p> For example, Ntafos [20, 21] compared the effectiveness of random testing with that of branch testing and all-uses testing, and observed that coverage testing is much more effective in revealing errors. Ramsey and Basili <ref> [23] </ref> empirically investigated the relationship between the number of tests and the test coverage. Sneed [25] reported his experience on comparing the effectiveness of branch coverage and data coverage in catching real bugs. Piwowarski et al [22] analytically derived a model characterizing the test coverage growth.
Reference: [24] <author> R. Selby, V. Basili, and F. Baker, </author> <title> "Cleanroom Software Development: An Empirical Evaluation," </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> SE-13(9), </volume> <year> 1987, </year> <pages> pp. 1027-1037. </pages>
Reference-contexts: 1 Introduction Significant effort is now being devoted to develop techniques to deliver reliable software. Methods proposed include well-controlled software development practice such as the clean-room approach <ref> [16, 24] </ref>, formal verification and testing. Cleanroom approach significantly reduces the number of faults introduced during the early phases of software life cycle, but it cannot totally avoid the problem of software faults and failures.
Reference: [25] <author> H.M. Sneed, </author> <title> "Data Coverage Measurement in Program Testing," </title> <booktitle> Workshop on Software Testing, </booktitle> <month> July </month> <year> 1986. </year>
Reference-contexts: For example, Ntafos [20, 21] compared the effectiveness of random testing with that of branch testing and all-uses testing, and observed that coverage testing is much more effective in revealing errors. Ramsey and Basili [23] empirically investigated the relationship between the number of tests and the test coverage. Sneed <ref> [25] </ref> reported his experience on comparing the effectiveness of branch coverage and data coverage in catching real bugs. Piwowarski et al [22] analytically derived a model characterizing the test coverage growth. Vouk [28] has derived a different model starting with different assumptions.
Reference: [26] <author> M. Takahashi, and Y. Kamayachi, </author> <title> "An Empirical Study of a Model for Program Error Prediction," </title> <journal> IEEE Trans.l Software Engineering, </journal> <volume> SE-15(1), </volume> <year> 1989, </year> <pages> pp. 82-86. </pages>
Reference: [27] <author> M. Trachtenberg, </author> <title> "Why Failure Rates Observe Zipf's Law in Operational Software," </title> <journal> IEEE Trans. Reliability, </journal> <volume> Vol. 41, No. 3, </volume> <year> 1992, </year> <pages> pp. 386-389. </pages>
Reference-contexts: Assumption 3: The failure rate distribution remains the same when testing starts and after testing finishes. Adams' data [1] gives the distribution of failures collected from operational use. For an untested software, the distribution of faults over different detectabilities would be more uniform. Trachtenberg <ref> [27] </ref> argue that the reason Adams' data follows Zipf's law may be because during software development in IBM, consciously or unconsciously, "the effort to prevent and remove each fault could have been expended in proportion to the fault's potential failure rate".
Reference: [28] <author> M.A. </author> <title> Vouk "Using Reliability Models During Testing With Non-operational Profiles," </title> <booktitle> Proc. 2nd Bellcore/Purdue workshop on issues in Software Reliability Estimation, </booktitle> <month> Oct. </month> <year> 1992, </year> <pages> pp. 103-111 </pages>
Reference-contexts: A similar concept termed accelerating factor was used by Drake and Wolting [7]. Using repair data, they computed the value of acceleration factors for two terminal firmware systems to be 47805 and 45532. Observations <ref> [6, 8, 28] </ref> of significant correlation between structural coverage and error removal growth and work by Malaiya et al [13] also support that real testing can be more effective than random sampling over operational usage distribution. <p> Ramsey and Basili noticed that the number of faults detected in a procedure are independent of the number of times the procedure is executed [23]. Piwowarski et al [22] observed that error removal rate and code coverage are closely related by a somewhat linear function. Vouk <ref> [28] </ref> suggested that the relation between software coverage and fault removal rate follows the Weibull distribution. Chen et al [4] suggested exclusion of the tests that do not contribute to any type of coverage from consideration when using traditional software reliability models. <p> Sneed [25] reported his experience on comparing the effectiveness of branch coverage and data coverage in catching real bugs. Piwowarski et al [22] analytically derived a model characterizing the test coverage growth. Vouk <ref> [28] </ref> has derived a different model starting with different assumptions. Chen et al [4] proposed incorporating test coverage into traditional time-based software reliability models by filtering out the test efforts that contributes to no new coverage.
References-found: 28

