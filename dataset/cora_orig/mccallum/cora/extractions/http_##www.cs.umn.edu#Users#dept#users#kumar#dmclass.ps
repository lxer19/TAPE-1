URL: http://www.cs.umn.edu/Users/dept/users/kumar/dmclass.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Search Framework for Mining Classification Decision Trees  
Author: Eui-Hong (Sam) Han Shashi Shekhar, Vipin Kumar, M. Ganesh, Jaideep Srivastava 
Keyword: Data mining, user interaction, classification, decision trees, algorithm development.  
Note: Contact author: Eui-Hong (Sam) Han  
Address: 4-192 EECS Bldg., 200 Union St. SE  Minneapolis, MN 55455, USA  
Affiliation: Dept. of Computer Information Sciences  University of Minnesota  
Pubnum: Technical Report  
Email: email: han@cs.umn.edu  
Phone: Tel: (612) 626-7515  
Date: 96-023 (April 3, 1996)  
Abstract: Classification-rule-learning task is presented as a search process of finding a classification-decision tree that meets users' preferences and requirements. Users can control the efficiency of the mining process and the quality of the final decision tree through the search parameters. This search framework allows users to easily adapt to different domains and different sets of data by modifying different search parameters. The mining process starts with a specific set of values for the search parameters and the process can be repeated with different search parameters values until a satisfactory result is obtained. This framework also allows the development of new algorithms. A set of search parameters that is frequently and successfully used can define a new algorithm. We present two new algorithms developed in this search framework and compare them to well-known algorithms. BF uses best-first ordering to expand the frontier nodes of the decision trees, instead of the conventional depth-first or breadth-first criteria. CDP+ dynamically adjusts the depth pruning criteria for a node. Experimental results show that BF has a new capability to guide the construction of decision trees based on the overall error-rate criteria. In addition, CDP+ often outperforms several decision-tree learning algorithms in error rate and number of nodes generated. z This work is sponsored (in whole or in part) by the Army High Performance Computing Research Center under the auspices of the Department of the Army, Army Research Laboratory cooperative agreement number DAAH04-95-2-0003/contract number DAAH04-95-C-0008, ARO Grant DA/DAAH04-95-1-0538, the content of which does not necessarily reflect the position or the policy of the government, and no official endorsement should be inferred. 
Abstract-found: 1
Intro-found: 1
Reference: [AIS93] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engg., </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: For any realistic problem domain of the classification-rule learning, the set of possible decision trees is too large to be searched exhaustively. In fact, the computational complexity of finding an optimal classification decision tree is N P - hard. All of the existing algorithms, like C4:5 [Qui93] and CDP <ref> [AIS93] </ref>, are greedy in nature, since a decision to choose an attribute for classification at a node in the decision tree is based on local heuristics. <p> This framework also leads to the development of new algorithms. A set of search parameters that is frequently used and successful can define a new algorithm. We show two new algorithms developed in this search framework and compare them to the well-known algorithms C4:5 [Qui93] and CDP <ref> [AIS93] </ref>. In recent years, substantial advances have been made in the development of data-mining techniques that deal with accuracy, efficiency and robustness. The C4:5 [Qui93] algorithm generates a decision tree for the given training data set by recursive partitioning of the data. <p> The pruning technique has been used to improve the accuracy and robustness. The algorithm tries to find the most accurate decision tree and does not provide any explicit control over efficiency or accuracy. CDP has been developed as part of the Quest project <ref> [AIS93] </ref> at the IBM Almaden Research Center. The CDP algorithm uses the dynamic pruning criterion to stop the generation of decision nodes if the error rate at a node of the tree is below some adaptive precision threshold. <p> The problem space of this search process consists of Model Candidates, a Model Candidate Generator, and Model Constraints. Many existing classification-learning algorithms like C4:5 [Qui93] and CDP <ref> [AIS93] </ref> fit nicely within this search framework. New learning algorithms that fit users' requirements can be developed by defining the components of the problem space. <p> We do not have a copy of the CDP algorithm, and thus we implemented the algorithm based on the description in <ref> [AIS93] </ref>. CDP uses a parameter maxlength for the adaptive precision threshold. We have chosen 10 for the parameter, which is the choice in [AIS93]. We also used the same value for CDP+. In the BF algorithm, we chose a 6% overall error rate as the acceptable error rate. <p> We do not have a copy of the CDP algorithm, and thus we implemented the algorithm based on the description in <ref> [AIS93] </ref>. CDP uses a parameter maxlength for the adaptive precision threshold. We have chosen 10 for the parameter, which is the choice in [AIS93]. We also used the same value for CDP+. In the BF algorithm, we chose a 6% overall error rate as the acceptable error rate. We also perform error-based pruning of CDP+ algorithm using the method used in C4:5P . We have followed the data generation methodology of [AIS93] very closely. <p> choice in <ref> [AIS93] </ref>. We also used the same value for CDP+. In the BF algorithm, we chose a 6% overall error rate as the acceptable error rate. We also perform error-based pruning of CDP+ algorithm using the method used in C4:5P . We have followed the data generation methodology of [AIS93] very closely. Data records were randomly generated and assigned class labels using one of the ten classification functions described in [AIS93]. For each classification function, we used a training set size of 2500 tuples and a test set size of 10000 tuples. <p> We also perform error-based pruning of CDP+ algorithm using the method used in C4:5P . We have followed the data generation methodology of <ref> [AIS93] </ref> very closely. Data records were randomly generated and assigned class labels using one of the ten classification functions described in [AIS93]. For each classification function, we used a training set size of 2500 tuples and a test set size of 10000 tuples. We have performed experiments on 10 different sets of data and report the mean values of the results. <p> Different data sets require different algorithms for the best results, and diverse user requirements put different constraints on the final decision tree. These experiments show that our framework can help find an optimal algorithm for some given data set and user requirements. 1 Note that in <ref> [AIS93] </ref>, the error rate for CDP was somewhat better relative to ID3. This could be due to the fact that C4:5 is an improved version of the ID3 package used in the [AIS93] experiments. 2 The results are not reported here. 7 each problem. <p> can help find an optimal algorithm for some given data set and user requirements. 1 Note that in <ref> [AIS93] </ref>, the error rate for CDP was somewhat better relative to ID3. This could be due to the fact that C4:5 is an improved version of the ID3 package used in the [AIS93] experiments. 2 The results are not reported here. 7 each problem.
Reference: [AS94] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules. </title> <booktitle> In Proc. of the 20th VLDB Conference, </booktitle> <pages> pages 487-499, </pages> <address> Santiago, Chile, </address> <year> 1994. </year>
Reference-contexts: This illustrates the promise of the search-based framework in providing a mechanism for user interaction in data-mining process and a formal mechanism to address accuracy, efficiency, robustness and other related issues. Future work includes the exploration of search frameworks for algorithms discovering other patterns such as associations <ref> [AS94, HS95] </ref>, clustering rules [Fis95] and temporal sequences [MTV95]. Such a framework can be quite useful in a more general environment for visual data-mining in which the user can visualize the intermediate results of the algorithm, and guide the algorithm dynamically by adjusting the model constraints.
Reference: [BEHW87] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24(6) </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: First, the creation of each model atom requires a non-trivial amount of computation, as all the data set contained in the model atom may have to be visited to compute the entropy of different attributes. Second, if the decision tree is too large, then it can over-generalize <ref> [BEHW87] </ref>; i.e., it may give very good performance on the training data set, but very poor performances on the test data sets. In fact, for some algorithms, e.g.
Reference: [Fis95] <author> D. Fisher. </author> <title> Optimization and simplification of hierarchical clusterings. </title> <booktitle> In Proc. of the First Int'l Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 118-123, </pages> <address> Montreal, Quebec, </address> <year> 1995. </year>
Reference-contexts: Future work includes the exploration of search frameworks for algorithms discovering other patterns such as associations [AS94, HS95], clustering rules <ref> [Fis95] </ref> and temporal sequences [MTV95]. Such a framework can be quite useful in a more general environment for visual data-mining in which the user can visualize the intermediate results of the algorithm, and guide the algorithm dynamically by adjusting the model constraints.
Reference: [GHK + 96] <author> M. Ganesh, E.H. Han, V. Kumar, S. Shekhar, and J. Srivastava. </author> <title> Visual data mining: Framework and algorithm development. </title> <type> Technical Report TR-96-021, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1996. </year>
Reference-contexts: Such a framework can be quite useful in a more general environment for visual data-mining in which the user can visualize the intermediate results of the algorithm, and guide the algorithm dynamically by adjusting the model constraints. In <ref> [GHK + 96] </ref> we present our preliminary work on such a visual data-mining system. Acknowledgments We would like to thank the data-mining group at the University of Minnesota, Dept. of Computer Science, for its comments and criticisms of our work.
Reference: [HS95] <author> M. A. W. Houtsma and A. N. Swami. </author> <title> Set-oriented mining for association rules in relational databases. </title> <booktitle> In Proc. of the 11th Int'l Conf. on Data Engg., </booktitle> <pages> pages 25-33, </pages> <address> Taipei, Taiwan, </address> <year> 1995. </year>
Reference-contexts: This illustrates the promise of the search-based framework in providing a mechanism for user interaction in data-mining process and a formal mechanism to address accuracy, efficiency, robustness and other related issues. Future work includes the exploration of search frameworks for algorithms discovering other patterns such as associations <ref> [AS94, HS95] </ref>, clustering rules [Fis95] and temporal sequences [MTV95]. Such a framework can be quite useful in a more general environment for visual data-mining in which the user can visualize the intermediate results of the algorithm, and guide the algorithm dynamically by adjusting the model constraints.
Reference: [KK88] <editor> L. N. Kanal and Vipin Kumar, editors. </editor> <booktitle> Search in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1988. </year>
Reference-contexts: A performance evaluation of the algorithms developed in section 2 is shown in section 3. Section 4 presents our conclusions and future work. 2 Search Framework for Classification-Rule Learning We have modeled the problem of classification-rule-learning as a search <ref> [KK88] </ref> process in the state space of different models. Learning a classification decision tree from a training data set can be regarded as a process of searching for the best decision tree that meets user-provided goal constraints.
Reference: [Lan96] <author> P. Langley. </author> <title> Elements of Machine Learning. </title> <address> Morgan-Kaufman, San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: Another important issue in the classification-rule learning is to allow users to control the computational efficiency of the mining process and the quality of the knowledge to be discovered. Traditional machine-learning <ref> [Lan96] </ref> research has addressed the issues on the quality of the learned knowledge. However, given the huge volume of data in databases and the computational complexity of the learning algorithms, the computational efficiency of mining processes is also a critical issue.
Reference: [MRA95] <author> M. Mehta, J. Rissanen, and R. Agrawal. </author> <title> Mdl-based decision tree pruning. </title> <booktitle> In Proc. of the First Int'l Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 216-221, </pages> <address> Montreal, Quebec, </address> <year> 1995. </year>
Reference: [MTV95] <author> H. Mannila, H. Toivonen, and A. I. Verkamo. </author> <title> Discovering frequent episodes in sequences. </title> <booktitle> In Proc. of the First Int'l Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 210-215, </pages> <address> Montreal, Quebec, </address> <year> 1995. </year>
Reference-contexts: Future work includes the exploration of search frameworks for algorithms discovering other patterns such as associations [AS94, HS95], clustering rules [Fis95] and temporal sequences <ref> [MTV95] </ref>. Such a framework can be quite useful in a more general environment for visual data-mining in which the user can visualize the intermediate results of the algorithm, and guide the algorithm dynamically by adjusting the model constraints.
Reference: [Qui93] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: For any realistic problem domain of the classification-rule learning, the set of possible decision trees is too large to be searched exhaustively. In fact, the computational complexity of finding an optimal classification decision tree is N P - hard. All of the existing algorithms, like C4:5 <ref> [Qui93] </ref> and CDP [AIS93], are greedy in nature, since a decision to choose an attribute for classification at a node in the decision tree is based on local heuristics. <p> This framework also leads to the development of new algorithms. A set of search parameters that is frequently used and successful can define a new algorithm. We show two new algorithms developed in this search framework and compare them to the well-known algorithms C4:5 <ref> [Qui93] </ref> and CDP [AIS93]. In recent years, substantial advances have been made in the development of data-mining techniques that deal with accuracy, efficiency and robustness. The C4:5 [Qui93] algorithm generates a decision tree for the given training data set by recursive partitioning of the data. <p> We show two new algorithms developed in this search framework and compare them to the well-known algorithms C4:5 <ref> [Qui93] </ref> and CDP [AIS93]. In recent years, substantial advances have been made in the development of data-mining techniques that deal with accuracy, efficiency and robustness. The C4:5 [Qui93] algorithm generates a decision tree for the given training data set by recursive partitioning of the data. The algorithm tries to generate a full decision tree which can classify the entire training data set correctly. <p> The problem space of this search process consists of Model Candidates, a Model Candidate Generator, and Model Constraints. Many existing classification-learning algorithms like C4:5 <ref> [Qui93] </ref> and CDP [AIS93] fit nicely within this search framework. New learning algorithms that fit users' requirements can be developed by defining the components of the problem space. <p> Play overcast 83 78 false Play overcast 64 65 true Play overcast 81 75 false Play rain 71 80 true Don't Play rain 65 70 true Don't Play rain 75 80 false Play rain 68 80 false Play rain 70 96 false Play Table 1: A small training data set <ref> [Qui93] </ref> The model candidate generator uses model constraints to provide controls and boundaries to the search space. Model constraints consist of acceptability constraints, expandability constraints, a traversal strategy and a data-entropy-calculation function. <p> The data-entropy-calculation function provides a mechanism for calculating the entropy of training cases with respect to classes. Some examples of the entropy-calculation function are based on information theory <ref> [Qui93] </ref>, and other user-provided functions such as the Gini function [WK91]. 2.2 New Algorithms for Classification-Rule Learning We show how to arrive at an algorithm, BF , based on the Best-First search idea. For acceptability criteria, in addition to the default predicate A1, we also include the acceptability predicate A2. <p> We used the C4:5 algorithm provided with <ref> [Qui93] </ref>. We present two sets of result for C4:5: one in which the resulting classification tree has been pruned as described in [Qui93] (C4:5P ) and the other in which the resulting tree is not pruned (C4:5). <p> We used the C4:5 algorithm provided with <ref> [Qui93] </ref>. We present two sets of result for C4:5: one in which the resulting classification tree has been pruned as described in [Qui93] (C4:5P ) and the other in which the resulting tree is not pruned (C4:5). We do not have a copy of the CDP algorithm, and thus we implemented the algorithm based on the description in [AIS93]. CDP uses a parameter maxlength for the adaptive precision threshold.
Reference: [SAD + 93] <author> M. Stonebraker, R. Agrawal, U. Dayal, E. J. Neuhold, and A. Reuter. </author> <title> Dbms research at a crossroads: The vienna update. </title> <booktitle> In Proc. of the 19th VLDB Conference, </booktitle> <pages> pages 688-692, </pages> <address> Dublin, Ireland, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction One of the important problems in data mining <ref> [SAD + 93] </ref> is the classification-rule learning. The classification-rule learning involves finding rules or decision trees that partition the given data into predefined classes. For any realistic problem domain of the classification-rule learning, the set of possible decision trees is too large to be searched exhaustively.
Reference: [WK91] <author> S. M. Weiss and C. A. </author> <title> Kulikowski. Computer Systems That Learn. </title> <address> Morgan-Kaufman, San Mateo, CA, </address> <year> 1991. </year> <title> 9 The following appendix is for the convenience of the reviewers and is NOT intended to be included in the conference proceedings. </title>
Reference-contexts: The data-entropy-calculation function provides a mechanism for calculating the entropy of training cases with respect to classes. Some examples of the entropy-calculation function are based on information theory [Qui93], and other user-provided functions such as the Gini function <ref> [WK91] </ref>. 2.2 New Algorithms for Classification-Rule Learning We show how to arrive at an algorithm, BF , based on the Best-First search idea. For acceptability criteria, in addition to the default predicate A1, we also include the acceptability predicate A2.
References-found: 13

