URL: http://www.cs.rochester.edu/u/sandhya/papers/sc95.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/sandhya/papers/
Root-URL: 
Email: e-mail: fhhl, sandhya, alc, willyg@cs.rice.edu  
Title: Message Passing Versus Distributed Shared Memory on Networks of Workstations  
Author: Honghui Lu Sandhya Dwarkadas, Alan L. Cox, and Willy Zwaenepoel 
Address: Houston, TX 77005  
Affiliation: Department of Electrical and Computer Engineering  Department of Computer Science Rice University  
Abstract: We compare two paradigms for parallel programming on networks of workstations: message passing and distributed shared memory. We present results for nine applications that were implemented using both paradigms. The message passing programs are executed with the Parallel Virtual Machine (PVM) library and the shared memory programs are executed using TreadMarks. The programs are Water and Barnes-Hut from the SPLASH benchmark suite; 3-D FFT, Integer Sort (IS) and Embarrassingly Parallel (EP) from the NAS benchmarks; ILINK, a widely used genetic linkage analysis program; and Successive Over-Relaxation (SOR), Traveling Salesman (TSP), and Quicksort (QSORT). Two different input data sets were used for Water (Water-288 and Water-1728), IS (IS-Small and IS-Large), and SOR (SOR-Zero and SOR-NonZero). Our execution environment is a set of eight HP735 workstations connected by a 100Mbits per second FDDI network. For Water-1728, EP, ILINK, SOR-Zero, and SOR-NonZero, the performance of TreadMarks is within 10% of PVM. For IS-Small, Water-288, Barnes-Hut, 3-D FFT, TSP, and QSORT, differences are on the order of 10% to 30%. Finally, for IS-Large, PVM performs two times better than TreadMarks. More messages and more data are sent in TreadMarks, explaining the performance differences. This extra communication is caused by 1) the separation of synchronization and data transfer, 2) extra messages to request updates for data by the invalidate protocol used in TreadMarks, 3) false sharing, and 4) diff accumulation for migratory data in TreadMarks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Adve and M. Hill. </author> <title> Weak ordering: A new definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This partial order is known as hb1 <ref> [1] </ref>. Vector timestamps are used to represent the partial order. When a processor executes an acquire, it sends its current timestamp in the acquire message. The previous releaser then piggybacks on its response the set of write notices that have timestamps greater than the timestamp in the acquire message.
Reference: [2] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report 103863, </type> <institution> NASA, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: We ported nine parallel programs to both TreadMarks and PVM: Water and Barnes-Hut from the SPLASH benchmark suite [20]; 3-D FFT, Integer Sort (IS), and Embarrassingly Parallel (EP) from the NAS benchmarks <ref> [2] </ref>; ILINK, a widely used genetic linkage analysis program [8]; and Successive Over-Relaxation (SOR), Traveling Salesman Problem (TSP), and Quicksort (QSORT). Two different input sets were used for Water (Water-288 and Water-1728), IS (IS-Small and IS-Large), and SOR (SOR-Zero and SOR-NonZero). <p> TreadMarks uses light-weight, operation-specific, user-level protocols on top of UDP to ensure reliable delivery. 3.2 Overview We ported nine parallel programs to both TreadMarks and PVM: Water and Barnes-Hut from the SPLASH benchmark suite [20]; 3-D FFT, IS, and EP from the NAS benchmarks <ref> [2] </ref>; ILINK, a widely used genetic linkage analysis program [8]; and SOR, TSP, and QSORT. <p> 33742 57083 3129 16000 Water-288 5028 4945 620 1520 Water-1728 8511 21170 620 9123 Barnes-Hut 62350 18277 280 12704 3-D FFT 14686 28732 1834 25690 ILINK 255047 119969 6615 47583 Table 2 Messages and Data at 8 Processors 3.3 EP The Embarrassingly Parallel program comes from the NAS benchmark suite <ref> [2] </ref>. EP generates pairs of Gaussian random deviates and tabulates the number of pairs in successive square annuli. In the parallel version, the only communication is summing up a ten-integer list at the end of the program. In TreadMarks, updates to the shared list are protected by a lock. <p> As a result of diffing in TreadMarks, much less data is sent in SOR-Zero by TreadMarks than by PVM because most of the pages remain zero. 3.5 Integer Sort Integer Sort (IS) <ref> [2] </ref> from the NAS benchmarks requires ranking an unsorted sequence of keys using bucket sort. The parallel version of IS divides up the keys among the processors. First, each processor counts its keys and writes the result in a private array of buckets. <p> For the same reason, during the force computation, a processor may fault on accessing its own bodies, and bring in unwanted data. 3.10 3-D FFT 3-D FFT, from the NAS <ref> [2] </ref> benchmark suite, numerically solves a partial differential equation using three dimensional forward and inverse FFT's. Assume the input array A is n 1 fi n 2 fi n 3 , organized in row-major order.
Reference: [3] <author> B.N. Bershad and M.J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry con sistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: The programmer needs to keep in mind where the data is, decide when to communicate with other processors, whom to communicate with, and what to communicate, making it hard to program in message passing, especially for applications with complex data structures. Software distributed shared memory (DSM) systems (e.g., <ref> [3, 5, 14, 16] </ref>) provide a shared memory abstraction on top of the native message passing facilities. An application can be written as if it were executing on a shared memory multiprocessor, accessing shared data with ordinary read and write operations.
Reference: [4] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Techniques for reducing consistency-related information in distributed shared memory systems. </title> <note> To appear in ACM Transactions on Computer Systems. </note>
Reference-contexts: Our results show that because of the use of release consistency and the multiple-writer protocol, Tread Marks performs comparably with PVM on a variety of problems in the experimental environment examined. These results are corroborated by those in <ref> [4] </ref>, which performed a similar experiment comparing the Munin DSM system against message passing on the V System [6]. For five out of the twelve experiments, Tread-Marks performed within 10% of PVM.
Reference: [5] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Pro ceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The programmer needs to keep in mind where the data is, decide when to communicate with other processors, whom to communicate with, and what to communicate, making it hard to program in message passing, especially for applications with complex data structures. Software distributed shared memory (DSM) systems (e.g., <ref> [3, 5, 14, 16] </ref>) provide a shared memory abstraction on top of the native message passing facilities. An application can be written as if it were executing on a shared memory multiprocessor, accessing shared data with ordinary read and write operations. <p> With TreadMarks, it is imperative to use explicit synchronization, as data is moved from processor to processor only in response to synchronization calls (see Section 2.2.2). 2.2.2 TreadMarks Implementation TreadMarks uses a lazy invalidate [14] version of release consistency (RC) [10] and a multiple-writer protocol <ref> [5] </ref> to reduce the amount of communication involved in implementing the shared memory abstraction. The virtual memory hardware is used to detect accesses to shared memory. RC is a relaxed memory consistency model.
Reference: [6] <author> D.R. Cheriton and W. Zwaenepoel. </author> <title> The distributed V kernel and its performance for diskless work stations. </title> <booktitle> In Proceedings of the 9th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 129-140, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: These results are corroborated by those in [4], which performed a similar experiment comparing the Munin DSM system against message passing on the V System <ref> [6] </ref>. For five out of the twelve experiments, Tread-Marks performed within 10% of PVM. Of the remaining experiments, Barnes-Hut and to a lesser extent IS-Large exhibit poor performance on both PVM and TreadMarks.
Reference: [7] <author> R. W. Cottingham Jr., R. M. Idury, and A. A. Schaffer. </author> <title> Faster sequential genetic linkage computations. </title> <journal> American Journal of Human Genetics, </journal> <volume> 53 </volume> <pages> 252-263, </pages> <year> 1993. </year>
Reference-contexts: Although the two processors read disjoint parts of the page, the same diff is sent to both of them. As a result, in TreadMarks, 14% more messages and 17% more data are sent at 6 processors than at 8 processors. 3.11 ILINK ILINK <ref> [7, 15] </ref> is a widely used genetic linkage analysis program that locates specific disease genes on chromosomes. The input to ILINK consists of several family trees. The program traverses the family trees and visits each nuclear family. The main data structure in ILINK is a pool of genarrays.
Reference: [8] <author> S. Dwarkadas, A.A. Schaffer, R.W. Cottingham Jr., A.L. Cox, P. Keleher, and W. Zwaenepoel. </author> <title> Paral lelization of general linkage analysis problems. </title> <booktitle> Human Heredity, </booktitle> <volume> 44 </volume> <pages> 127-141, </pages> <year> 1994. </year>
Reference-contexts: We ported nine parallel programs to both TreadMarks and PVM: Water and Barnes-Hut from the SPLASH benchmark suite [20]; 3-D FFT, Integer Sort (IS), and Embarrassingly Parallel (EP) from the NAS benchmarks [2]; ILINK, a widely used genetic linkage analysis program <ref> [8] </ref>; and Successive Over-Relaxation (SOR), Traveling Salesman Problem (TSP), and Quicksort (QSORT). Two different input sets were used for Water (Water-288 and Water-1728), IS (IS-Small and IS-Large), and SOR (SOR-Zero and SOR-NonZero). We ran these programs on eight HP735 workstations connected by a 100Mbits per second FDDI network. <p> operation-specific, user-level protocols on top of UDP to ensure reliable delivery. 3.2 Overview We ported nine parallel programs to both TreadMarks and PVM: Water and Barnes-Hut from the SPLASH benchmark suite [20]; 3-D FFT, IS, and EP from the NAS benchmarks [2]; ILINK, a widely used genetic linkage analysis program <ref> [8] </ref>; and SOR, TSP, and QSORT. <p> The computation either updates a parent's genarray conditioned on the spouse and all children, or updates one child conditioned on both parents and all the other siblings. We use the parallel algorithm described in Dwarkadas et al. <ref> [8] </ref>. Updates to each individual's genarray are parallelized. A master processor assigns the nonzero elements in the parent's genarray to all processors in a round robin fashion.
Reference: [9] <author> G.A. Geist and V.S. Sunderam. </author> <title> Network-based concurrent computing on the PVM system. </title> <journal> Concurrency: Practice and Experience, </journal> <pages> pages 293-311, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Processors in workstation clusters do not share physical memory, so all interprocessor communication between processors must be performed by sending messages over the network. Currently, the prevailing programming model for parallel computing on networks of workstations is message passing, using libraries such as PVM <ref> [9] </ref>, TCGMSG [11] and Express [18]. A message passing standard MPI [17] has also been developed. With the message passing paradigm, the distributed nature of the memory system is fully exposed to the application programmer. <p> The rest of this paper is organized as follows. In Section 2 we introduce the user interfaces and implementations of PVM and TreadMarks. Section 3 presents the application programs and their results. Section 4 concludes the paper. 2 PVM Versus TreadMarks 2.1 PVM PVM <ref> [9] </ref>, standing for Parallel Virtual Machine, is a message passing system originally developed at Oak Ridge National Laboratory. Although other message passing systems such as TCGMSG [11], provide higher bandwidth than PVM, we chose PVM because of its popularity.
Reference: [10] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: With TreadMarks, it is imperative to use explicit synchronization, as data is moved from processor to processor only in response to synchronization calls (see Section 2.2.2). 2.2.2 TreadMarks Implementation TreadMarks uses a lazy invalidate [14] version of release consistency (RC) <ref> [10] </ref> and a multiple-writer protocol [5] to reduce the amount of communication involved in implementing the shared memory abstraction. The virtual memory hardware is used to detect accesses to shared memory. RC is a relaxed memory consistency model.
Reference: [11] <author> R.J. Harrison. </author> <title> Portable tools and applications for parallel computers. </title> <journal> In International Journal of Quantum Chemistry, </journal> <volume> volume 40, </volume> <pages> pages 847-863, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Processors in workstation clusters do not share physical memory, so all interprocessor communication between processors must be performed by sending messages over the network. Currently, the prevailing programming model for parallel computing on networks of workstations is message passing, using libraries such as PVM [9], TCGMSG <ref> [11] </ref> and Express [18]. A message passing standard MPI [17] has also been developed. With the message passing paradigm, the distributed nature of the memory system is fully exposed to the application programmer. <p> Section 3 presents the application programs and their results. Section 4 concludes the paper. 2 PVM Versus TreadMarks 2.1 PVM PVM [9], standing for Parallel Virtual Machine, is a message passing system originally developed at Oak Ridge National Laboratory. Although other message passing systems such as TCGMSG <ref> [11] </ref>, provide higher bandwidth than PVM, we chose PVM because of its popularity. We use PVM version 3.2.6 in our experiments. 2.1.1 PVM Interface With PVM, the user data must be packed into a send buffer before being dispatched.
Reference: [12] <author> J. T. Hecht, Y. Wang, B. Connor, S. H. Blanton, and S. P. Daiger. Non-syndromic cleft lip and palate: </author> <title> No evidence of linkage to hla or factor 13a. </title> <journal> American Journal of Human Genetics, </journal> <volume> 52 </volume> <pages> 1230-1233, </pages> <year> 1993. </year>
Reference-contexts: The diffing mechanism in TreadMarks automatically achieves the same effect. Since only the nonzero elements are modified during each nuclear family update, the diffs transmitted to the master only contain the nonzero elements. We used the CLP data set <ref> [12] </ref>, with an allele product 2 fi 4 fi 4 fi 4. The results are shown in Figure 12. The sequential program runs for 1473 seconds. At 8 processors, TreadMarks achieves a speedup of 5.57, which is 93% of the 5.99 obtained by PVM.
Reference: [13] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The merge is accomplished through the use of diffs. A diff is a runlength encoding of the modifications made to a page, generated by comparing the page to a copy saved prior to the modifications. TreadMarks implements a lazy invalidate version of RC <ref> [13] </ref>. A lazy implementation delays the propagation of consistency information until the time of an acquire. Furthermore, the releaser notifies the acquirer of which pages have been modified, causing the acquirer to invalidate its local copies of these pages.
Reference: [14] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: The programmer needs to keep in mind where the data is, decide when to communicate with other processors, whom to communicate with, and what to communicate, making it hard to program in message passing, especially for applications with complex data structures. Software distributed shared memory (DSM) systems (e.g., <ref> [3, 5, 14, 16] </ref>) provide a shared memory abstraction on top of the native message passing facilities. An application can be written as if it were executing on a shared memory multiprocessor, accessing shared data with ordinary read and write operations. <p> Because PVM is designed to work on a set of heterogeneous machines, it provides conversion to and from an external data representation (XDR). This conversion is avoided if all the machines used are identical. 2.2 TreadMarks TreadMarks <ref> [14] </ref> is a software DSM system built at Rice University. It is an efficient user-level DSM system that runs on commonly available Unix systems. We use TreadMarks version 0.9.4 in our experiments. 2.2.1 TreadMarks Interface TreadMarks provides primitives similar to those used in hardware shared memory machines. <p> They have the same syntax as conventional memory allocation calls. With TreadMarks, it is imperative to use explicit synchronization, as data is moved from processor to processor only in response to synchronization calls (see Section 2.2.2). 2.2.2 TreadMarks Implementation TreadMarks uses a lazy invalidate <ref> [14] </ref> version of release consistency (RC) [10] and a multiple-writer protocol [5] to reduce the amount of communication involved in implementing the shared memory abstraction. The virtual memory hardware is used to detect accesses to shared memory. RC is a relaxed memory consistency model.
Reference: [15] <author> G. M. Lathrop, J. M. Lalouel, C. Julier, and J. Ott. </author> <title> Strategies for multilocus linkage analysis in humans. </title> <booktitle> Proceedings of National Academy of Science, USA, </booktitle> <volume> 81 </volume> <pages> 3443-3446, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Although the two processors read disjoint parts of the page, the same diff is sent to both of them. As a result, in TreadMarks, 14% more messages and 17% more data are sent at 6 processors than at 8 processors. 3.11 ILINK ILINK <ref> [7, 15] </ref> is a widely used genetic linkage analysis program that locates specific disease genes on chromosomes. The input to ILINK consists of several family trees. The program traverses the family trees and visits each nuclear family. The main data structure in ILINK is a pool of genarrays.
Reference: [16] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The programmer needs to keep in mind where the data is, decide when to communicate with other processors, whom to communicate with, and what to communicate, making it hard to program in message passing, especially for applications with complex data structures. Software distributed shared memory (DSM) systems (e.g., <ref> [3, 5, 14, 16] </ref>) provide a shared memory abstraction on top of the native message passing facilities. An application can be written as if it were executing on a shared memory multiprocessor, accessing shared data with ordinary read and write operations.
Reference: [17] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard, </title> <note> version 1.0, </note> <month> May </month> <year> 1994. </year>
Reference-contexts: Currently, the prevailing programming model for parallel computing on networks of workstations is message passing, using libraries such as PVM [9], TCGMSG [11] and Express [18]. A message passing standard MPI <ref> [17] </ref> has also been developed. With the message passing paradigm, the distributed nature of the memory system is fully exposed to the application programmer.
Reference: [18] <institution> Parasoft Corporation, Pasadena, CA. </institution> <note> Express user's guide, version 3.2.5, </note> <year> 1992. </year>
Reference-contexts: Processors in workstation clusters do not share physical memory, so all interprocessor communication between processors must be performed by sending messages over the network. Currently, the prevailing programming model for parallel computing on networks of workstations is message passing, using libraries such as PVM [9], TCGMSG [11] and Express <ref> [18] </ref>. A message passing standard MPI [17] has also been developed. With the message passing paradigm, the distributed nature of the memory system is fully exposed to the application programmer.
Reference: [19] <author> J.P. Singh, J.L. Hennessy, and A. Gupta. </author> <title> Implications of hierarchical n-body methods for multiprocessor architectures. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(2) </volume> <pages> 141-202, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Figure 10 shows the speedups. The sequential program runs for 17 seconds. At 8 processors, PVM and TreadMarks achieve speedups of 3.04 and 2.70 respectively. The low computation to communication ratio and the need for fine-grained communication <ref> [19] </ref> contribute to the poor speedups on both TreadMarks and PVM. In PVM, the network is saturated at 8 processors, because every processor tries to broadcast at the same time. Diff requests and false sharing are the major reasons for TreadMarks' lower performance.

References-found: 19

