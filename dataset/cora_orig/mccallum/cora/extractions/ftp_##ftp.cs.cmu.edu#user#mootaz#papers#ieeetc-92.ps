URL: ftp://ftp.cs.cmu.edu/user/mootaz/papers/ieeetc-92.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/mootaz/ftp/html/pub.html
Root-URL: 
Title: Manetho: Transparent Rollback-Recovery with Low Overhead, Limited Rollback and Fast Output Commit  
Author: Elmootazbellah N. Elnozahy and Willy Zwaenepoel 
Keyword: Index Terms: Antecedence graph, checkpointing, message logging, rollback-recovery, transparent fault tolerance.  
Abstract: Manetho is a new transparent rollback-recovery protocol for long-running distributed computations. It uses a novel combination of antecedence graph maintenance, uncoordinated checkpointing, and sender-based message logging. Manetho simultaneously achieves the advantages of pessimistic message logging, namely limited rollback and fast output commit, and the advantage of optimistic message logging, namely low failure-free overhead. These advantages come at the expense of a complex recovery scheme. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Borg, W. Blau, W. Graetsch, F. Herrmann, and W. Oberle. </author> <title> Fault tolerance under UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(1) </volume> <pages> 1-24, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: These methods achieve only a subset of the goals of reducing the overhead during failure-free operation, limiting the extent of rollback, and reducing the latency of output commit. Pessimistic message logging protocols limit rollback by synchronously logging recovery information on stable storage <ref> [1, 12] </ref>. A failed process is restored to its state before the failure, and processes that survive the failure are not rolled back. In addition, no latency is incurred in sending messages to the outside world. <p> By Lemma 7, the information in that node will no longer be needed during recovery. For the purpose of garbage collection of AG, RU s occasionally exchange the state interval indexes of their most recent checkpoints. 7 Related Work Several systems use message replay for rollback-recovery <ref> [1, 5, 6, 7, 11, 12, 15, 16] </ref>. Except for the Psync recovery protocol [11], none of these systems use a graph that records the "happened before" relation [9] between certain events.
Reference: [2] <author> K.M. Chandy and L. Lamport. </author> <title> Distributed snapshots: Determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: During failure-free operation, a rollback-recovery protocol records information about the computation's execution on stable storage, thereby causing failure-free overhead. The system uses this information after a failure to roll the computation back to a consistent state <ref> [2] </ref>. The system also invokes an output commit algorithm each time the computation sends a message to the "outside world." The outside world consists of the entities that cannot roll their states back (a line printer, for instance). <p> Processes that survive a failure may however be rolled back. Furthermore, the latency of output commit is higher than in pessimistic message logging since a message cannot be sent to the outside world without multi-host coordination. Consistent checkpointing protocols <ref> [2, 4, 8, 10, 17, 18] </ref> do not cause failure-free overhead, except while a 2 consistent checkpoint is being taken. Processes that survive a failure may however be rolled back, and output commit may require taking a multi-host consistent checkpoint, resulting in considerable latency.
Reference: [3] <author> E.N. Elnozahy and W. Zwaenepoel. Manetho: </author> <title> A low overhead rollback-recovery system with fast output commit. </title> <type> Technical Report TR91-152, </type> <institution> Rice University, </institution> <month> March </month> <year> 1991. </year> <month> 18 </month>
Reference-contexts: Nondeterminism is limited to message receipt or other nondeterministic events that can be efficiently recorded in the antecedence graph and replayed during recovery. Experience with an implementation of Manetho shows that the overhead of maintaining the antecedence graph and message logs is small <ref> [3] </ref>. We concentrate in this paper on the protocol description and its correctness. Implementation, performance, and scalability are considered elsewhere [3]. 3 2 Assumptions We assume that the computation consists of a number of fail-stop [14] recovery units (RU s) [16] which communicate only by messages over an asynchronous network. <p> Experience with an implementation of Manetho shows that the overhead of maintaining the antecedence graph and message logs is small <ref> [3] </ref>. We concentrate in this paper on the protocol description and its correctness. Implementation, performance, and scalability are considered elsewhere [3]. 3 2 Assumptions We assume that the computation consists of a number of fail-stop [14] recovery units (RU s) [16] which communicate only by messages over an asynchronous network. An RU consists of one or more threads that manipulate the RU 's internal state. <p> This list is used to reject messages that arrive at their destinations after their copies in their senders' logs have been garbage collected. We discuss recovery of garbage collection information elsewhere <ref> [3] </ref>. To reclaim the space used by the AG, an RU can discard a node that corresponds to q i , if q has taken a checkpoint at state interval q c , where c i.
Reference: [4] <author> F. Jahanian and F. Cristian. </author> <title> A timestamp-based checkpointing protocol for long-lived distributed computations. </title> <booktitle> In Proceedings of the 10th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 12-20, </pages> <address> Bologna, Italy, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Processes that survive a failure may however be rolled back. Furthermore, the latency of output commit is higher than in pessimistic message logging since a message cannot be sent to the outside world without multi-host coordination. Consistent checkpointing protocols <ref> [2, 4, 8, 10, 17, 18] </ref> do not cause failure-free overhead, except while a 2 consistent checkpoint is being taken. Processes that survive a failure may however be rolled back, and output commit may require taking a multi-host consistent checkpoint, resulting in considerable latency.
Reference: [5] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> Sender-based message logging. </title> <booktitle> In Proceedings of the 17th International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 14-19, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: It achieves these goals by using an antecedence graph, which records the "happened before" [9] relationship between certain events in the computation, in combination with uncoordinated checkpointing and sender-based volatile message logging <ref> [5] </ref>. Manetho avoids synchronous logging of recovery information on stable storage most of the time, thereby reducing the overhead during failure-free operation. Manetho also reduces the latency of output commit by allowing messages to be sent to the outside world without multi-host coordination. <p> By Lemma 7, the information in that node will no longer be needed during recovery. For the purpose of garbage collection of AG, RU s occasionally exchange the state interval indexes of their most recent checkpoints. 7 Related Work Several systems use message replay for rollback-recovery <ref> [1, 5, 6, 7, 11, 12, 15, 16] </ref>. Except for the Psync recovery protocol [11], none of these systems use a graph that records the "happened before" relation [9] between certain events.
Reference: [6] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> Recovery in distributed systems using optimistic message logging and checkpointing. </title> <journal> Journal of Algorithms, </journal> <volume> 11(3) </volume> <pages> 462-491, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: In addition, no latency is incurred in sending messages to the outside world. Synchronous logging of recovery information however results in high failure-free overhead, unless special-purpose hardware is used. Optimistic message logging protocols <ref> [6, 7, 15, 16] </ref> reduce failure-free overhead by logging recovery information asynchronously. Processes that survive a failure may however be rolled back. Furthermore, the latency of output commit is higher than in pessimistic message logging since a message cannot be sent to the outside world without multi-host coordination. <p> By Lemma 7, the information in that node will no longer be needed during recovery. For the purpose of garbage collection of AG, RU s occasionally exchange the state interval indexes of their most recent checkpoints. 7 Related Work Several systems use message replay for rollback-recovery <ref> [1, 5, 6, 7, 11, 12, 15, 16] </ref>. Except for the Psync recovery protocol [11], none of these systems use a graph that records the "happened before" relation [9] between certain events.
Reference: [7] <author> T. Juang and S. Venkatesan. </author> <title> Crash recovery with little overhead. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 454-461, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In addition, no latency is incurred in sending messages to the outside world. Synchronous logging of recovery information however results in high failure-free overhead, unless special-purpose hardware is used. Optimistic message logging protocols <ref> [6, 7, 15, 16] </ref> reduce failure-free overhead by logging recovery information asynchronously. Processes that survive a failure may however be rolled back. Furthermore, the latency of output commit is higher than in pessimistic message logging since a message cannot be sent to the outside world without multi-host coordination. <p> By Lemma 7, the information in that node will no longer be needed during recovery. For the purpose of garbage collection of AG, RU s occasionally exchange the state interval indexes of their most recent checkpoints. 7 Related Work Several systems use message replay for rollback-recovery <ref> [1, 5, 6, 7, 11, 12, 15, 16] </ref>. Except for the Psync recovery protocol [11], none of these systems use a graph that records the "happened before" relation [9] between certain events.
Reference: [8] <author> R. Koo and S. Toueg. </author> <title> Checkpointing and rollback-recovery for distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):23-31, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: Processes that survive a failure may however be rolled back. Furthermore, the latency of output commit is higher than in pessimistic message logging since a message cannot be sent to the outside world without multi-host coordination. Consistent checkpointing protocols <ref> [2, 4, 8, 10, 17, 18] </ref> do not cause failure-free overhead, except while a 2 consistent checkpoint is being taken. Processes that survive a failure may however be rolled back, and output commit may require taking a multi-host consistent checkpoint, resulting in considerable latency.
Reference: [9] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: Manetho is a new transparent rollback-recovery protocol that, unlike existing protocols, simultaneously achieves the goals of low overhead, limited rollback, and fast output commit. It achieves these goals by using an antecedence graph, which records the "happened before" <ref> [9] </ref> relationship between certain events in the computation, in combination with uncoordinated checkpointing and sender-based volatile message logging [5]. Manetho avoids synchronous logging of recovery information on stable storage most of the time, thereby reducing the overhead during failure-free operation. <p> It contains a node representing p i and a node for each state interval that "happened before" <ref> [9] </ref> i . Figure 2 shows AG ( p 1 ) corresponding to the example of Figure 1. <p> Except for the Psync recovery protocol [11], none of these systems use a graph that records the "happened before" relation <ref> [9] </ref> between certain events. The combination of the antecedence graph with uncoordinated checkpointing and sender-based volatile message logging allows Manetho to achieve its goals of low failure-free overhead, limited rollback, and fast output commit, albeit at the expense of a more complex recovery protocol.
Reference: [10] <author> K. Li, J.F. Naughton, and J.S. Plank. </author> <title> Checkpointing multicomputer applications. </title> <booktitle> In Proceedings of the 10th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 1-10, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Processes that survive a failure may however be rolled back. Furthermore, the latency of output commit is higher than in pessimistic message logging since a message cannot be sent to the outside world without multi-host coordination. Consistent checkpointing protocols <ref> [2, 4, 8, 10, 17, 18] </ref> do not cause failure-free overhead, except while a 2 consistent checkpoint is being taken. Processes that survive a failure may however be rolled back, and output commit may require taking a multi-host consistent checkpoint, resulting in considerable latency.
Reference: [11] <author> L.L. Peterson, N.C. Bucholz, and R.D. Schlichting. </author> <title> Preserving and using context information in interprocess communication. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(3) </volume> <pages> 217-246, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: By Lemma 7, the information in that node will no longer be needed during recovery. For the purpose of garbage collection of AG, RU s occasionally exchange the state interval indexes of their most recent checkpoints. 7 Related Work Several systems use message replay for rollback-recovery <ref> [1, 5, 6, 7, 11, 12, 15, 16] </ref>. Except for the Psync recovery protocol [11], none of these systems use a graph that records the "happened before" relation [9] between certain events. <p> For the purpose of garbage collection of AG, RU s occasionally exchange the state interval indexes of their most recent checkpoints. 7 Related Work Several systems use message replay for rollback-recovery [1, 5, 6, 7, 11, 12, 15, 16]. Except for the Psync recovery protocol <ref> [11] </ref>, none of these systems use a graph that records the "happened before" relation [9] between certain events. <p> The combination of the antecedence graph with uncoordinated checkpointing and sender-based volatile message logging allows Manetho to achieve its goals of low failure-free overhead, limited rollback, and fast output commit, albeit at the expense of a more complex recovery protocol. Manetho's antecedence graph differs from Psync's context graph <ref> [11] </ref> in that the antecedence graph records the order of message receipt within the same RU, while Psync does not. The order of message receipt is exactly the information required for message replay during recovery. <p> The order of message receipt is exactly the information required for message replay during recovery. The same information can be deduced from the context graph by applying a deterministic ordering filter. This filter delays the delivery of each application message until several subsequent application messages are received <ref> [11] </ref>. Moreover, unlike the antecedence graph, the context graph requires that each process receives and logs every message ex 17 changed in the system.
Reference: [12] <author> M.L. Powell and D.L. Presotto. </author> <title> Publishing: A reliable broadcast communication mechanism. </title> <booktitle> In Proceedings of the 9th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 100-109, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: These methods achieve only a subset of the goals of reducing the overhead during failure-free operation, limiting the extent of rollback, and reducing the latency of output commit. Pessimistic message logging protocols limit rollback by synchronously logging recovery information on stable storage <ref> [1, 12] </ref>. A failed process is restored to its state before the failure, and processes that survive the failure are not rolled back. In addition, no latency is incurred in sending messages to the outside world. <p> By Lemma 7, the information in that node will no longer be needed during recovery. For the purpose of garbage collection of AG, RU s occasionally exchange the state interval indexes of their most recent checkpoints. 7 Related Work Several systems use message replay for rollback-recovery <ref> [1, 5, 6, 7, 11, 12, 15, 16] </ref>. Except for the Psync recovery protocol [11], none of these systems use a graph that records the "happened before" relation [9] between certain events.
Reference: [13] <author> B. Randell. </author> <title> System structure for software fault tolerance. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-1(2):220-232, </volume> <month> June </month> <year> 1975. </year>
Reference-contexts: After a failure, surviving processes are not rolled back, and failed processes are rolled back only to their most recent checkpoints. The protocol tolerates an arbitrary number of fail-stop failures [14], including failures during recovery, and avoids the domino effect <ref> [13] </ref>. Manetho's advantages come at the expense of a complex recovery scheme and some limitations on support for nondeterminism. The expense of recovery should not be a major concern in modern systems where failures are expected to be infrequent.
Reference: [14] <author> R.D. Schlichting and F.B. Schneider. </author> <title> Fail-stop processors: An approach to designing fault-tolerant computing systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(3) </volume> <pages> 222-238, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: After a failure, surviving processes are not rolled back, and failed processes are rolled back only to their most recent checkpoints. The protocol tolerates an arbitrary number of fail-stop failures <ref> [14] </ref>, including failures during recovery, and avoids the domino effect [13]. Manetho's advantages come at the expense of a complex recovery scheme and some limitations on support for nondeterminism. The expense of recovery should not be a major concern in modern systems where failures are expected to be infrequent. <p> We concentrate in this paper on the protocol description and its correctness. Implementation, performance, and scalability are considered elsewhere [3]. 3 2 Assumptions We assume that the computation consists of a number of fail-stop <ref> [14] </ref> recovery units (RU s) [16] which communicate only by messages over an asynchronous network. An RU consists of one or more threads that manipulate the RU 's internal state. Each RU has access to a stable storage device. A failed RU can be restarted on any available machine.
Reference: [15] <author> A.P. Sistla and J.L. Welch. </author> <title> Efficient distributed recovery using message logging. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: In addition, no latency is incurred in sending messages to the outside world. Synchronous logging of recovery information however results in high failure-free overhead, unless special-purpose hardware is used. Optimistic message logging protocols <ref> [6, 7, 15, 16] </ref> reduce failure-free overhead by logging recovery information asynchronously. Processes that survive a failure may however be rolled back. Furthermore, the latency of output commit is higher than in pessimistic message logging since a message cannot be sent to the outside world without multi-host coordination. <p> By Lemma 7, the information in that node will no longer be needed during recovery. For the purpose of garbage collection of AG, RU s occasionally exchange the state interval indexes of their most recent checkpoints. 7 Related Work Several systems use message replay for rollback-recovery <ref> [1, 5, 6, 7, 11, 12, 15, 16] </ref>. Except for the Psync recovery protocol [11], none of these systems use a graph that records the "happened before" relation [9] between certain events.
Reference: [16] <author> R.E. Strom and S.A. Yemini. </author> <title> Optimistic recovery in distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Because the output commit algorithm must ensure that the state from which the message is sent will never be rolled back <ref> [16] </ref>, it may introduce latency in sending messages to the outside world. Existing transparent rollback-recovery methods fall into three classes: pessimistic message logging, optimistic message logging, and consistent checkpointing. <p> In addition, no latency is incurred in sending messages to the outside world. Synchronous logging of recovery information however results in high failure-free overhead, unless special-purpose hardware is used. Optimistic message logging protocols <ref> [6, 7, 15, 16] </ref> reduce failure-free overhead by logging recovery information asynchronously. Processes that survive a failure may however be rolled back. Furthermore, the latency of output commit is higher than in pessimistic message logging since a message cannot be sent to the outside world without multi-host coordination. <p> We concentrate in this paper on the protocol description and its correctness. Implementation, performance, and scalability are considered elsewhere [3]. 3 2 Assumptions We assume that the computation consists of a number of fail-stop [14] recovery units (RU s) <ref> [16] </ref> which communicate only by messages over an asynchronous network. An RU consists of one or more threads that manipulate the RU 's internal state. Each RU has access to a stable storage device. A failed RU can be restarted on any available machine. <p> An RU consists of one or more threads that manipulate the RU 's internal state. Each RU has access to a stable storage device. A failed RU can be restarted on any available machine. The execution of an RU consists of a sequence of piecewise deterministic state intervals <ref> [16] </ref>, each started by a nondeterministic event. <p> This is an instance of the problem of ordering the perception of failures with respect to messages. To solve this problem, each RU starts a new incarnation <ref> [16] </ref> at the beginning of each recovery. Each incarnation is identified by a monotonically increasing incarnation number, and each message is tagged with the current incarnation number of the sender. <p> By Lemma 7, the information in that node will no longer be needed during recovery. For the purpose of garbage collection of AG, RU s occasionally exchange the state interval indexes of their most recent checkpoints. 7 Related Work Several systems use message replay for rollback-recovery <ref> [1, 5, 6, 7, 11, 12, 15, 16] </ref>. Except for the Psync recovery protocol [11], none of these systems use a graph that records the "happened before" relation [9] between certain events.
Reference: [17] <author> K.-L. Wu and W.K. Fuchs. </author> <title> Recoverable distributed shared memory. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4) </volume> <pages> 460-469, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Processes that survive a failure may however be rolled back. Furthermore, the latency of output commit is higher than in pessimistic message logging since a message cannot be sent to the outside world without multi-host coordination. Consistent checkpointing protocols <ref> [2, 4, 8, 10, 17, 18] </ref> do not cause failure-free overhead, except while a 2 consistent checkpoint is being taken. Processes that survive a failure may however be rolled back, and output commit may require taking a multi-host consistent checkpoint, resulting in considerable latency.
Reference: [18] <author> K.-L Wu, W.K. Fuchs, and J.H. Patel. </author> <title> Error recovery in shared memory multiprocessors using private caches. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(2) </volume> <pages> 231-240, </pages> <month> April </month> <year> 1990. </year> <month> 19 </month>
Reference-contexts: Processes that survive a failure may however be rolled back. Furthermore, the latency of output commit is higher than in pessimistic message logging since a message cannot be sent to the outside world without multi-host coordination. Consistent checkpointing protocols <ref> [2, 4, 8, 10, 17, 18] </ref> do not cause failure-free overhead, except while a 2 consistent checkpoint is being taken. Processes that survive a failure may however be rolled back, and output commit may require taking a multi-host consistent checkpoint, resulting in considerable latency.
References-found: 18

