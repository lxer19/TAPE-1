URL: http://www.cs.rutgers.edu/~uli/PACT94.ps
Refering-URL: http://www.cs.rutgers.edu/~uli/pubs.html
Root-URL: http://www.cs.rutgers.edu
Title: Automatic Data Layout Using 0-1 Integer Programming 1  
Author: Robert Bixby a Ken Kennedy b and Ulrich Kremer b 
Affiliation: a Department of Computational and Applied Mathematics, Rice University, U.S.A. b Department of Computer Science, Rice University,  
Date: August 1994.  
Address: (PACT94), Montreal, Canada,  U.S.A.  
Note: Published in the Proceedings of the International Conference on Parallel Architectures and Compilation Techniques  
Abstract: The goal of languages like Fortran D or High Performance Fortran (HPF) is to provide a simple yet efficient machine-independent parallel programming model. By shifting much of the burden of machine-dependent optimization to the compiler, the programmer is able to write data-parallel programs that can be compiled and executed with good performance on many different architectures. However, the choice of a good data layout is still left to the programmer. Even the most sophisticated compiler may not be able to compensate for a poorly chosen data layout since many compiler decisions are driven by the data layout specified in the program. The choice of a good data layout depends on many factors, including the target machine architecture, the compilation system, the problem size, and the number of processors available. The option of remapping arrays at specific points in the program makes the choice even harder. Current programming tools provide little or no support for this difficult selection process. This paper discusses automatic data layout techniques in the context of a programming environment and an advanced compilation system that allows dynamic data remapping. Our proposed framework for automatic data layout builds and examines search spaces of candidate data layouts. A candidate layout is an efficient layout for some part of the program. Choosing a single layout for each program part among its candidate data layouts such that their overall cost is minimal has been shown to be NP-complete. Instead of resorting to heuristics, this paper investigates methods to determine the optimal selection. The data layout selection problem is formulated as a 0-1 integer programming problem, which is then fed to a state-of-the-art, general purpose integer programming solver. Our experiments 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: The discussion of transformations in the context of phase recognition is beyond the scope of this paper. The phase structure of the program is represented in the phase control flow graph, an augmented control flow graph <ref> [1] </ref> where each phase is represented by a single node. The graph is annotated with branch probabilities and loop control information. In the second step, the program alignment space is determined. The program alignment space corresponds to a single HPF template or a single Fortran D decomposition. <p> The costs reflect the frequencies or probabilities of phase execution. A data flow problem over the phase control flow graph can be solved to determine these probabilities. The 4 proposed data flow problem is similar to a reaching definitions problem <ref> [1] </ref> that additionally determines the probability that a definition reaches a given point in the program. The initial formulation of the inter-phase data layout problem as an optimization problem over the data layout graph does not model the possible overlap of communication and computation between phases.
Reference: [2] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Knobe, Lukas, and Dally [19], and Chatterjee, Gilbert, Schreiber, and Teng [9, 10] address the problem of dynamic alignment in a framework particularly suitable for SIMD machines. Anderson and Lam discuss techniques for automatic data layout for distributed and shared address space machines <ref> [2] </ref>. Their approach considers dynamic remapping. Lee and Tsai propose a dynamic programming algorithm to determine a data layout for a sequence of loop nests, allowing remapping between the loop nests [23].
Reference: [3] <author> D. Applegate, R. Bixby, V. Chvatal, and W. Cook. </author> <title> The traveling salesman problem. </title> <note> 1993. In preparation. </note>
Reference-contexts: an integer programming success story exploiting all of the above advances is the recent work of Applegate, Bixby, Cook and Chvatal in which a 4461 city traveling salesman problem was solved to exact optimality using a complex branch-and-cut code running on a network of up to 60 loosely connected workstations <ref> [3] </ref>. 2 Framework for Automatic Data Layout The choice of a good data layout for a program depends on the compilation system, the problem size, the number of processors used, and the performance characteristics of the target machine architecture.
Reference: [4] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The selection process is based on static performance estimates of the candidate data layouts and of data remappings between layouts. A static performance estimator suitable for automatic data layout has been discussed elsewhere <ref> [4, 14] </ref>. The performance estimator will mimic the compilation process for single phases. It will use the training set approach to determine communication and computation costs of the target machine architecture. In the final step, data layout specifications are generated. <p> A static performance estimator together with the computed phase execution frequencies will be used to determine the node and edge weights <ref> [4, 14] </ref>. To solve the inter-phase data layout problem, a single candidate data layout must be chosen for each phase such that the overall cost of the selected layouts is minimal.
Reference: [5] <author> R. Bixby. </author> <title> Implementing the Simplex method: The initial basis. </title> <journal> ORSA Journal on Computing, </journal> <volume> 4(3), </volume> <year> 1992. </year>
Reference-contexts: An instance of the inter-phase data layout problem is translated into a 0-1 integer programming problem suitable to be solved by CPLEX 2 , a linear integer programming tool partly developed by Robert Bixby at Rice University <ref> [5] </ref>. We give experimental results for our 0-1 integer programming formulations for an 800 line benchmark code developed by Thomas Eidson at ICASE. 3 Example Program The following example illustrates the framework for automatic data layout. Figure 1A shows an Alternating Direction Implicit (ADI) integration kernel.
Reference: [6] <author> R. Bixby. </author> <title> Progress in linear programming. </title> <journal> ORSA Journal on Computing, </journal> <volume> 6(1), </volume> <year> 1994. </year>
Reference-contexts: The basic technique for solving integer problems is to apply intelligent branch-and-bound using linear programming at the nodes. Important improvements have come in three areas. First, linear programming codes are on average approximately two orders of magnitude faster than they were five years ago, particularly for larger problems <ref> [6] </ref>. Combined with the improvements in computing speed over that same period these codes represent an approximate four orders of magnitude improvement in our ability to solve linear programming problems. 2 The second major development is in so-called cutting-plane technology.
Reference: [7] <author> R. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <type> Technical Report CRPC-TR93-349-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: However, these other formulations turned out to be inferior to the presented formulation in terms of the time CPLEX needed to compute the optimal solution. A more detailed discussion of the different formulations can be found elsewhere <ref> [7] </ref>. 9 6 Experiments All of our experiments are based on Erlebacher, a 800 line benchmark program written by Thomas Eidson at the Institute for Computer Applications in Science and Engineering (ICASE). The program performs 3-dimensional tridiagonal solves using Alternating Direction Implicit (ADI) integration.
Reference: [8] <author> B. Chapman, H. Herbeck, and H. Zima. </author> <title> Automatic support for data distribution. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference: [9] <author> S. Chatterjee, J.R. Gilbert, and R. Schreiber. </author> <title> The alignment-distribution graph. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Even though many researchers have recognized the need for dynamic remapping and are planning to develop solutions, our work is one of the first to provide a framework for automatic data layout that considers dynamic remapping. Knobe, Lukas, and Dally [19], and Chatterjee, Gilbert, Schreiber, and Teng <ref> [9, 10] </ref> address the problem of dynamic alignment in a framework particularly suitable for SIMD machines. Anderson and Lam discuss techniques for automatic data layout for distributed and shared address space machines [2]. Their approach considers dynamic remapping.
Reference: [10] <author> S. Chatterjee, J.R. Gilbert, R. Schreiber, and S-H. Teng. </author> <title> Automatic array alignment in data-parallel programs. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Albuquerque, NM, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Even though many researchers have recognized the need for dynamic remapping and are planning to develop solutions, our work is one of the first to provide a framework for automatic data layout that considers dynamic remapping. Knobe, Lukas, and Dally [19], and Chatterjee, Gilbert, Schreiber, and Teng <ref> [9, 10] </ref> address the problem of dynamic alignment in a framework particularly suitable for SIMD machines. Anderson and Lam discuss techniques for automatic data layout for distributed and shared address space machines [2]. Their approach considers dynamic remapping.
Reference: [11] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: This restriction can be relaxed by adding additional remapping edges. The placement of these remapping edges is a topic of current research. The inter-phase data layout problem is proven to be NP-complete [21]. The proof is based on a reduction from the 3-CNF satisfiability problem (3-SAT) <ref> [11] </ref>. However, in the special case where each candidate layout specifies a mapping for every array in the program, the inter-phase data layout problem can be solved in polynomial time in the size of the data layout graph.
Reference: [12] <author> G. B. Dantzig, D. R. Fulkerson, and S. M. Johnson. </author> <title> Solution of a large scale traveling salesman problem. </title> <journal> Operations Research, </journal> <volume> 7 </volume> <pages> 58-66, </pages> <year> 1954. </year>
Reference-contexts: Motivated by work of Dantzig, Johnson and Fulkerson in the 50's <ref> [12] </ref>, Padberg, Groetschel and others have shown how cutting-plane techniques could be used to strengthen the linear programming relaxations of many pure 0-1 integer programming problems [25]. The strengthening is effected by studying the facets of the underlying polytope generated by the convex hull of 0-1 solutions.
Reference: [13] <author> M. Gupta. </author> <title> Automatic Data Partitioning on Distributed Memory Multicomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> September </month> <year> 1992. </year> <month> 12 </month>
Reference: [14] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: For example, it is almost essential to consider the program as a whole rather than a series of independent subroutines. Of particular importance and complexity is the problem of determining when dynamic data redistribution will enhance overall performance. Fortunately, the designers of languages like HPF and Fortran D <ref> [14] </ref>, by requiring that data layout specifications be provided by the programmer, have opened the door for powerful new tools which can use intensive computation to determine a first approximation to a good data layout automatically. <p> The selection process is based on static performance estimates of the candidate data layouts and of data remappings between layouts. A static performance estimator suitable for automatic data layout has been discussed elsewhere <ref> [4, 14] </ref>. The performance estimator will mimic the compilation process for single phases. It will use the training set approach to determine communication and computation costs of the target machine architecture. In the final step, data layout specifications are generated. <p> A static performance estimator together with the computed phase execution frequencies will be used to determine the node and edge weights <ref> [4, 14] </ref>. To solve the inter-phase data layout problem, a single candidate data layout must be chosen for each phase such that the overall cost of the selected layouts is minimal.
Reference: [15] <author> D. Hudak and S. Abraham. </author> <title> Compiler techniques for data partitioning of sequentially iterated parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference: [16] <author> K. Kennedy and U. Kremer. </author> <title> Initial framework for automatic data layout in Fortran D: A short update on a case study. </title> <type> Technical Report CRPC-TR93-324-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Then, distribution analysis uses the alignment search spaces to build candidate data layout search spaces of reasonable alignments and distributions for each phase. A preliminary discussion of possible pruning heuristics and the sizes of their resulting search spaces can be found in <ref> [16] </ref>. After the generation of the search spaces, a single candidate data layout is selected for each phase, resulting in a data layout for the entire program. This step solves the so-called inter-phase data layout problem.
Reference: [17] <author> C.W. Keler. </author> <title> Knowledge-based automatic parallelization by pattern recognition. </title> <editor> In C.W. Keler, editor, </editor> <title> Automatic Parallelization | New Approaches to Code Generation, Data Distribution, </title> <booktitle> and Performance Prediction, </booktitle> <pages> pages 110-135. </pages> <publisher> Verlag Vieweg, Wiesbaden, </publisher> <address> Germany, </address> <year> 1993. </year>
Reference: [18] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <year> 1990. </year>
Reference: [19] <author> K. Knobe, J.D. Lukas, and W.J. Dally. </author> <title> Dynamic alignment on distributed memory systems. </title> <booktitle> In Proceedings of the Third Workshop on Compilers for Parallel Computers, </booktitle> <address> Vienna, Austria, </address> <year> 1992. </year>
Reference-contexts: Even though many researchers have recognized the need for dynamic remapping and are planning to develop solutions, our work is one of the first to provide a framework for automatic data layout that considers dynamic remapping. Knobe, Lukas, and Dally <ref> [19] </ref>, and Chatterjee, Gilbert, Schreiber, and Teng [9, 10] address the problem of dynamic alignment in a framework particularly suitable for SIMD machines. Anderson and Lam discuss techniques for automatic data layout for distributed and shared address space machines [2]. Their approach considers dynamic remapping.
Reference: [20] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction The advent of languages like High Performance Fortran (HPF) <ref> [20] </ref>, in which the programmer specifies parallelism implicitly by specifying the layout of an application's data across the processor array, has focused renewed attention on the problem of choosing a good data layout for parallel execution.
Reference: [21] <author> U. Kremer. </author> <title> NP-completeness of dynamic remapping. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: This restriction can be relaxed by adding additional remapping edges. The placement of these remapping edges is a topic of current research. The inter-phase data layout problem is proven to be NP-complete <ref> [21] </ref>. The proof is based on a reduction from the 3-CNF satisfiability problem (3-SAT) [11].
Reference: [22] <author> U. Kremer, J. Mellor-Crummey, K. Kennedy, and A. Carle. </author> <title> Automatic data layout for distributed-memory machines in the D programming environment. </title> <editor> In C.W. Keler, editor, </editor> <title> Automatic Parallelization | New Approaches to Code Generation, Data Distribution, </title> <booktitle> and Performance Prediction, </booktitle> <pages> pages 136-152. </pages> <publisher> Verlag Vieweg, Wiesbaden, </publisher> <address> Germany, </address> <year> 1993. </year>
Reference-contexts: The polynomial time algorithm uses dynamic programming to solve multiple single-source, shortest path problems over the data layout graph <ref> [22] </ref>. In this paper, we focus on methods to compute the optimal solution of the inter-phase data layout problem with only a single copy of each array at any time during the execution of the program.
Reference: [23] <author> P. Lee and T-B. Tsai. </author> <title> Compiling efficient programs for tightly-coupled distributed memory computers. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Anderson and Lam discuss techniques for automatic data layout for distributed and shared address space machines [2]. Their approach considers dynamic remapping. Lee and Tsai propose a dynamic programming algorithm to determine a data layout for a sequence of loop nests, allowing remapping between the loop nests <ref> [23] </ref>. In contrast to most of the previously published work, our framework is designed to work in the context of a programming assistance tool, not inside a compiler. As a consequence, the framework can use techniques that may be too expensive to be included in a compiler. 7 integration kernel.
Reference: [24] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(4) </volume> <pages> 213-221, </pages> <month> August </month> <year> 1991. </year>
Reference: [25] <author> M. Padberg and G. Rinaldi. </author> <title> A branch-and-cut algorithm for the resolution of large-scale symmetric traveling salesman problems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 60-100, </pages> <year> 1991. </year>
Reference-contexts: Motivated by work of Dantzig, Johnson and Fulkerson in the 50's [12], Padberg, Groetschel and others have shown how cutting-plane techniques could be used to strengthen the linear programming relaxations of many pure 0-1 integer programming problems <ref> [25] </ref>. The strengthening is effected by studying the facets of the underlying polytope generated by the convex hull of 0-1 solutions. Knowledge of these facets leads to subroutines for recognizing inequalities violated by the current fractional solution.
Reference: [26] <author> J. Ramanujam and P. Sadayappan. </author> <title> A methodology for parallelizing programs for multicomputers and complex memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference: [27] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, TX, </institution> <month> January </month> <year> 1993. </year> <institution> Rice COMP TR93-199. </institution>
Reference-contexts: However, the static performance estimator for each single phase will take the effect of compiler generated wavefronts inside a phase into account <ref> [27] </ref>. In addition, the initial formulation requires that only a single copy of an array can exist at any time during program execution, unless the array is replicated due to multiple ownership. This restriction can be relaxed by adding additional remapping edges. <p> The corresponding data layout graphs with different weights were generated by hand. Weights were chosen to model different communication costs and the presence or absence of compiler optimizations. For instance, a compiler may be able to generate a coarse-grain pipelined loop if the data layout induces cross-processor dependences <ref> [27] </ref>. Whether the compiler performs such an optimization or not is represented by different weights given to the nodes. We wrote a tool that generates the 0-1 problem formulation (see Section 5) for a given, weighted data layout graph.
Reference: [28] <author> S. Wholey. </author> <title> Automatic Data Mapping for Distributed-Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> May </month> <year> 1991. </year> <month> 13 </month>
References-found: 28

