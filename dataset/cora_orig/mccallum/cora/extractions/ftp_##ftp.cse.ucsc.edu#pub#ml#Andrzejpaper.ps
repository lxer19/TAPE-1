URL: ftp://ftp.cse.ucsc.edu/pub/ml/Andrzejpaper.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00397.html
Root-URL: 
Phone: 2  
Title: Metric Entropy and Minimax Risk in Classification  
Author: David Haussler and Manfred Opper 
Keyword: Vapnik-Chervonenkis dimension.  
Address: Santa Cruz, CA 95064, USA  Wurzburg, Germany  
Affiliation: 1 Computer Science, UC  Dept. of Physics, Universitat  
Abstract: We apply recent results on the minimax risk in density estimation to the related problem of pattern classification. The notion of loss we seek to minimize is an information theoretic measure of how well we can predict the classification of future examples, given the classification of previously seen examples. We give an asymptotic characterization of the minimax risk in terms of the metric entropy properties of the class of distributions that might be generating the examples. We then use these results to characterize the minimax risk in the special case of noisy two-valued classification problems in terms of the Assouad density and the 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: Here we restrict ourselves to a special problem of two-class classification that has been called "noisy concept learning" in the AI and computational learning theory literature <ref> [40, 1, 10, 11, 21] </ref>. We show how fairly precise, general rates can be obtained for this problem based on a combinatorial parameter known as the Assouad density [2], which is related to the VC dimension [44]. <p> For simplicity, in this comparison we restrict ourselves to a special class of classification problems that we call noisy two-class learning (also called "noisy concept learning" in the computational learning theory and AI machine learning literature <ref> [1] </ref>). In noisy two-class learning, the outcome space Y has just two values, which we may designate as +1 and 1, instead of an arbitrary finite set of values, as we have been assuming up to this point. <p> If one does not, then there is no close relationship between these two quantities, even if we use the expected size of F restricted to n random points. For example, if we let X = <ref> [0; 1] </ref>, P be the uniform distribution on X and F be the set of all f1g-valued functions that are +1 on at most d points, then R (x n )jF j x n j n d but K * (F ; D ) = 0, since all functions differ only
Reference: 2. <author> P. Assouad. </author> <title> Densite et dimension. </title> <journal> Annales de l'Institut Fourier, </journal> <volume> 33(3) </volume> <pages> 233-282, </pages> <year> 1983. </year>
Reference-contexts: We show how fairly precise, general rates can be obtained for this problem based on a combinatorial parameter known as the Assouad density <ref> [2] </ref>, which is related to the VC dimension [44]. Finally, we review the implications of these results in the closing section, section 8. <p> Thus F (n) is the maximum number of distinct functions that can be obtained by restricting the domain of the functions in F to n points. From the growth function we can define the Assouad density of F <ref> [2] </ref>, and the Vapnik-Chervonenkis (VC) dimension of F. We treat the Assouad density first, relating it to a certain supremum over the metric dimension, and return to the VC dimension later. Definition 9.
Reference: 3. <author> A. Barron. In T. M. Cover and B. Gopinath, </author> <title> editors, Open Problems in Communication and Computation, chapter 3.20. Are Bayes rules consistent in information?, </title> <address> pages 85-91. </address> <year> 1987. </year>
Reference-contexts: However, it turns out that this effect cannot be very strong. In particular, it can be shown in general that R minimax n is nondecreasing in n, and n X r minimax t R minimax n nr minimax n (see <ref> [3, 13, 6, 28] </ref>.) It follows that R minimax n grows at most linearly in n, since r minimax t log jY j for all t. These inequalities also give fairly tight bounds on R minimax n in terms of r minimax n when r minimax n decreases slowly.
Reference: 4. <author> A. Barron. </author> <title> The exponential convergence of posterior probabilities with implications for Bayes estimators of density functions. </title> <type> Technical Report 7, </type> <institution> Dept. of Statistics, U. Ill. Urbana-Champaign, </institution> <year> 1987. </year>
Reference-contexts: These results were extended to the cumulative relative entropy Bayes and minimax risk in [14] (see also [5]). Related lower bounds, which are often quoted, were obtained by Rissanen [37], based on certain asymptotic normality assumptions. Estimations of the relative entropy risk in nonparametric cases were obtained in <ref> [4, 6, 38, 45, 46] </ref>. General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]).
Reference: 5. <author> A. Barron, B. Clarke, and D. Haussler. </author> <title> Information bounds for the risk of Bayesian predictions and the redundancy of universal codes. </title> <booktitle> In Proc. International Symposium on Information Theory. </booktitle>
Reference-contexts: These results were extended to the cumulative relative entropy Bayes and minimax risk in [14] (see also <ref> [5] </ref>). Related lower bounds, which are often quoted, were obtained by Rissanen [37], based on certain asymptotic normality assumptions. Estimations of the relative entropy risk in nonparametric cases were obtained in [4, 6, 38, 45, 46].
Reference: 6. <author> A. Barron and Y. Yang. </author> <title> Information theoretic lower bounds on convergence rates of nonparametric estimators, 1995. </title> <type> unpublished manuscript. </type>
Reference-contexts: These results were extended to the cumulative relative entropy Bayes and minimax risk in [14] (see also [5]). Related lower bounds, which are often quoted, were obtained by Rissanen [37], based on certain asymptotic normality assumptions. Estimations of the relative entropy risk in nonparametric cases were obtained in <ref> [4, 6, 38, 45, 46] </ref>. General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]). <p> General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]). This approach is further developed in <ref> [7, 8, 25, 41, 9, 6, 45, 42, 28] </ref>. The results of sections 2 through 5 show how this theory can be applied to the classification problem. 2 Using relative entropy and minimax risk to define estimation error Let us summarize the problem we are considering in its abstract setting. <p> However, it turns out that this effect cannot be very strong. In particular, it can be shown in general that R minimax n is nondecreasing in n, and n X r minimax t R minimax n nr minimax n (see <ref> [3, 13, 6, 28] </ref>.) It follows that R minimax n grows at most linearly in n, since r minimax t log jY j for all t. These inequalities also give fairly tight bounds on R minimax n in terms of r minimax n when r minimax n decreases slowly.
Reference: 7. <author> L. Birge. </author> <title> Approximation dans les espaces metriques et theorie de l'estimation. </title> <journal> Zeitschrift fuer Wahrscheinlichkeitstheorie und verwandte gebiete, </journal> <volume> 65 </volume> <pages> 181-237, </pages> <year> 1983. </year>
Reference-contexts: General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]). This approach is further developed in <ref> [7, 8, 25, 41, 9, 6, 45, 42, 28] </ref>. The results of sections 2 through 5 show how this theory can be applied to the classification problem. 2 Using relative entropy and minimax risk to define estimation error Let us summarize the problem we are considering in its abstract setting. <p> The sense of this approximation is given, e.g., in [28]. This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including LeCam [32], Birge <ref> [7, 8] </ref>, Hasminskii and Ibragimov [25], and van de Geer [41]. Now assume that and fl are two joint distributions on X fi Y with a common marginal distribution on X.
Reference: 8. <author> L. Birge. </author> <title> On estimating a density using Hellinger distance and some other strange facts. Probability theory and related fields, </title> <booktitle> 71 </booktitle> <pages> 271-291, </pages> <year> 1986. </year>
Reference-contexts: General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]). This approach is further developed in <ref> [7, 8, 25, 41, 9, 6, 45, 42, 28] </ref>. The results of sections 2 through 5 show how this theory can be applied to the classification problem. 2 Using relative entropy and minimax risk to define estimation error Let us summarize the problem we are considering in its abstract setting. <p> The sense of this approximation is given, e.g., in [28]. This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including LeCam [32], Birge <ref> [7, 8] </ref>, Hasminskii and Ibragimov [25], and van de Geer [41]. Now assume that and fl are two joint distributions on X fi Y with a common marginal distribution on X.
Reference: 9. <author> L. Birge and P. Massart. </author> <title> Rates of convergence for minimum contrast estimators. Probability Theory and Related Fields, </title> <booktitle> 97 </booktitle> <pages> 113-150, </pages> <year> 1993. </year>
Reference-contexts: General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]). This approach is further developed in <ref> [7, 8, 25, 41, 9, 6, 45, 42, 28] </ref>. The results of sections 2 through 5 show how this theory can be applied to the classification problem. 2 Using relative entropy and minimax risk to define estimation error Let us summarize the problem we are considering in its abstract setting. <p> The theory of packing and covering numbers, and the associated metric entropy, was introduced by Kolmogorov and Tikhomirov in [31], and is commonly used in the theory of empirical processes (see e.g. <ref> [19, 36, 24, 9, 42] </ref>). For the following definitions, let (S; ) be any metric space. Definition 1. (Metric entropy, also called Kolmogorov *-entropy [31]) A partition of S is a collection f i g of subsets of S that are pairwise disjoint and whose union is S.
Reference: 10. <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: Here we restrict ourselves to a special problem of two-class classification that has been called "noisy concept learning" in the AI and computational learning theory literature <ref> [40, 1, 10, 11, 21] </ref>. We show how fairly precise, general rates can be obtained for this problem based on a combinatorial parameter known as the Assouad density [2], which is related to the VC dimension [44].
Reference: 11. <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: Here we restrict ourselves to a special problem of two-class classification that has been called "noisy concept learning" in the AI and computational learning theory literature <ref> [40, 1, 10, 11, 21] </ref>. We show how fairly precise, general rates can be obtained for this problem based on a combinatorial parameter known as the Assouad density [2], which is related to the VC dimension [44]. <p> This theorem shows that sup P R minimax n (fi; ) either grows logarithmically or slower, or it grows linearly. There is no rate in between. Results of this type are also available from the standard Vapnik-Chervonenkis theory <ref> [44, 11] </ref>. However, what is novel here is that in the case of logarithmic growth, the best possible constant in front of the logarithm is identified here to be the Assouad density.
Reference: 12. <author> B. Clarke. </author> <title> Asymptotic cumulative risk and Bayes risk under entropy loss with applications. </title> <type> PhD thesis, </type> <institution> Dept. of Statistics, University of Ill., </institution> <year> 1989. </year>
Reference-contexts: In this case they were even able to estimate the lower order additive terms in this approximation, which involve the Fisher information and the entropy of the prior. Further related results were given by Efroimovich [20] and Clarke <ref> [12] </ref>. Clarke and Barron gave a detailed analysis, with applications, of the risk of the Bayes strategy [13], discussing the relation of the cumulative relative entropy loss to the notion of redundancy in information theory, and giving applications to hypothesis testing and portfolio selection theory.
Reference: 13. <author> B. Clarke and A. Barron. </author> <title> Information-theoretic asymptotics of Bayes methods. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 36(3) </volume> <pages> 453-471, </pages> <year> 1990. </year>
Reference-contexts: Further related results were given by Efroimovich [20] and Clarke [12]. Clarke and Barron gave a detailed analysis, with applications, of the risk of the Bayes strategy <ref> [13] </ref>, discussing the relation of the cumulative relative entropy loss to the notion of redundancy in information theory, and giving applications to hypothesis testing and portfolio selection theory. These results were extended to the cumulative relative entropy Bayes and minimax risk in [14] (see also [5]). <p> However, it turns out that this effect cannot be very strong. In particular, it can be shown in general that R minimax n is nondecreasing in n, and n X r minimax t R minimax n nr minimax n (see <ref> [3, 13, 6, 28] </ref>.) It follows that R minimax n grows at most linearly in n, since r minimax t log jY j for all t. These inequalities also give fairly tight bounds on R minimax n in terms of r minimax n when r minimax n decreases slowly.
Reference: 14. <author> B. Clarke and A. Barron. </author> <title> Jefferys' prior is asymptotically least favorable under entropy risk. </title> <journal> J. Statistical Planning and Inference, </journal> <volume> 41 </volume> <pages> 37-60, </pages> <year> 1994. </year>
Reference-contexts: These results were extended to the cumulative relative entropy Bayes and minimax risk in <ref> [14] </ref> (see also [5]). Related lower bounds, which are often quoted, were obtained by Rissanen [37], based on certain asymptotic normality assumptions. Estimations of the relative entropy risk in nonparametric cases were obtained in [4, 6, 38, 45, 46]. <p> separation * goes to zero, the uniform distribution over the maximal *- separated set of models approaches something like a Jeffreys' prior over the model class fi, which is known already to be asymptotically minimax (called "asymptotically least favorable" for technical reasons) for relative entropy risk in smooth parametric cases <ref> [14] </ref>. It is an interesting open problem to determine to what extent this holds for more general fi, and what characterizes asymptotically minimax "generalized Jeffreys' priors". As a practical classification method, the Bayes method using a uniform prior on an *-separated set has two drawbacks 1.
Reference: 15. <author> G. F. Clements. </author> <title> Entropy of several sets of real-valued functions. </title> <journal> Pacific J. Math., </journal> <volume> 13 </volume> <pages> 1085-1095, </pages> <year> 1963. </year>
Reference-contexts: There is a good general theory on approximation error, starting with the fundamental theorems of approximation theory, as given, for example, in the classic book of Lorentz [33] (see also <ref> [31, 15] </ref>). While in specific cases this error depends strongly on the nature of the true distribution, which is unknown, one can still make statements about the general approximability of functions or distributions in one family by functions or distributions in another.
Reference: 16. <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference: 17. <author> L. Devroye and L. Gyorfi. </author> <title> Nonparametric density estimation, the L 1 view. </title> <publisher> Wiley, </publisher> <year> 1985. </year>
Reference-contexts: There is a large statistical literature on minimax rates for estimation error for general statistical problems. However, much of this work has been done using loss functions other than relative entropy, see e.g. the texts <ref> [17, 30] </ref>.
Reference: 18. <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: The theory of making optimal decisions from given these probability distributions is quite simple, and is treated fully in standard texts such as that by Duda and Hart <ref> [18] </ref>, so we will not elaborate on it here. Rather we will focus solely on the problem of obtaining accurate predictive distributions, which is the critical part of the problem. The predictive distribution can be estimated by estimating the joint probability distribution over the random variables X and Y .
Reference: 19. <author> R. M. Dudley. </author> <title> A course on empirical processes. </title> <booktitle> Lecture Notes in Mathematics, </booktitle> <volume> 1097 </volume> <pages> 2-142, </pages> <year> 1984. </year>
Reference-contexts: The theory of packing and covering numbers, and the associated metric entropy, was introduced by Kolmogorov and Tikhomirov in [31], and is commonly used in the theory of empirical processes (see e.g. <ref> [19, 36, 24, 9, 42] </ref>). For the following definitions, let (S; ) be any metric space. Definition 1. (Metric entropy, also called Kolmogorov *-entropy [31]) A partition of S is a collection f i g of subsets of S that are pairwise disjoint and whose union is S. <p> A result of Assouad's, given in a monograph by Dudley <ref> [19] </ref>, relates this scaling, in the worst case over distributions P , to the Assouad density of F fi . For the following definitions and results, let F be any class of f1g-valued functions on a set X and P be any distribution on X. Definition 10. <p> Definition 10. Let s (F ) = inffd &gt; 0 : there is a C &gt; 0 such that for every P and 0 &lt; * 1; M * (F ; D ) C* d g: Theorem 11. (Theorem 9.3.1 of <ref> [19] </ref>) dens (F ) = s (F) Using a similar method, we can also relate s (F) directly to the upper dimen- sion of (F; D ).
Reference: 20. <author> S. Y. Efroimovich. </author> <title> Information contained in a sequence of observations. Problems in Information Transmission, </title> <booktitle> 15 </booktitle> <pages> 178-189, </pages> <year> 1980. </year>
Reference-contexts: In this case they were even able to estimate the lower order additive terms in this approximation, which involve the Fisher information and the entropy of the prior. Further related results were given by Efroimovich <ref> [20] </ref> and Clarke [12]. Clarke and Barron gave a detailed analysis, with applications, of the risk of the Bayes strategy [13], discussing the relation of the cumulative relative entropy loss to the notion of redundancy in information theory, and giving applications to hypothesis testing and portfolio selection theory.
Reference: 21. <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 247-261, </pages> <year> 1989. </year>
Reference-contexts: Here we restrict ourselves to a special problem of two-class classification that has been called "noisy concept learning" in the AI and computational learning theory literature <ref> [40, 1, 10, 11, 21] </ref>. We show how fairly precise, general rates can be obtained for this problem based on a combinatorial parameter known as the Assouad density [2], which is related to the VC dimension [44].
Reference: 22. <author> M. Feder, Y. Freund, and Y. Mansour. </author> <title> Optimal universal learning and prediction of probabilistic concepts. </title> <booktitle> In Proc. of IEEE Information Theory Conference, </booktitle> <pages> page 233. </pages> <publisher> IEEE, </publisher> <year> 1995. </year>
Reference-contexts: Finally, we review the implications of these results in the closing section, section 8. The main results given here are derived from results in [28] (see also <ref> [35, 27, 22, 34] </ref>), where a general theory of minimax estimation error using relative entropy is developed that applies not only to classification problems in the form that we have defined them, but to other important statistical problems, including regression and density estimation.
Reference: 23. <author> A. Gelman. </author> <title> Bayesian Data Analysis. </title> <publisher> Chapman and Hall, </publisher> <address> NY, </address> <year> 1995. </year>
Reference-contexts: Combined with the observed examples, this prior generates a posterior distribution on fi. The predictive distribution is then obtained by integrating over all the conditional distributions in fi, weighted according to this posterior distribution (see e.g. <ref> [23] </ref>). Some of the most successful classification methods are Bayes methods, or computationally efficient approximations to Bayes methods. We will discuss these methods further in the last section of this paper, after we have established the basic theory of estimation error.
Reference: 24. <author> E. Gine and J. Zinn. </author> <title> Some limit theorems for empirical processes. </title> <journal> Annals of Probability, </journal> <volume> 12 </volume> <pages> 929-989, </pages> <year> 1984. </year>
Reference-contexts: The theory of packing and covering numbers, and the associated metric entropy, was introduced by Kolmogorov and Tikhomirov in [31], and is commonly used in the theory of empirical processes (see e.g. <ref> [19, 36, 24, 9, 42] </ref>). For the following definitions, let (S; ) be any metric space. Definition 1. (Metric entropy, also called Kolmogorov *-entropy [31]) A partition of S is a collection f i g of subsets of S that are pairwise disjoint and whose union is S.
Reference: 25. <author> R. Hasminskii and I. Ibragimov. </author> <title> On density estimation in the view of Kol-mogorov's ideas in approximation theory. </title> <journal> Annals of statistics, </journal> <volume> 18 </volume> <pages> 999-1010, </pages> <year> 1990. </year>
Reference-contexts: General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]). This approach is further developed in <ref> [7, 8, 25, 41, 9, 6, 45, 42, 28] </ref>. The results of sections 2 through 5 show how this theory can be applied to the classification problem. 2 Using relative entropy and minimax risk to define estimation error Let us summarize the problem we are considering in its abstract setting. <p> The sense of this approximation is given, e.g., in [28]. This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including LeCam [32], Birge [7, 8], Hasminskii and Ibragimov <ref> [25] </ref>, and van de Geer [41]. Now assume that and fl are two joint distributions on X fi Y with a common marginal distribution on X.
Reference: 26. <author> D. Haussler and A. Barron. </author> <title> How well do Bayes methods work for on-line prediction of f+1; 1g values? In Proceedings of the Third NEC Symposium on Computation and Cognition. </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: It is difficult to identify such constants with the standard Vapnik-Chervonenkis theory, which relies on uniform convergence of empirical estimates, and therefore gives only indirect bounds on the minimax risk. Some tighter upper bounds are known for Theorem 13. In particular, in <ref> [26] </ref> it was shown that Theorem 14.
Reference: 27. <author> D. Haussler and M. Opper. </author> <title> General bounds on the mutual information between a parameter and n conditionally independent observations. </title> <booktitle> In Proceedings of the Seventh Annual ACM Workshop on Computational Learning Theory, </booktitle> <year> 1995. </year>
Reference-contexts: Finally, we review the implications of these results in the closing section, section 8. The main results given here are derived from results in [28] (see also <ref> [35, 27, 22, 34] </ref>), where a general theory of minimax estimation error using relative entropy is developed that applies not only to classification problems in the form that we have defined them, but to other important statistical problems, including regression and density estimation.
Reference: 28. <author> D. Haussler and M. Opper. </author> <title> Mutual information, metric entropy, and risk in estimation of probability distributions. </title> <type> Technical Report UCSC-CRL-96-27, </type> <institution> Univ. of Calif. Computer Research Lab, </institution> <address> Santa Cruz, CA, </address> <year> 1996. </year>
Reference-contexts: Finally, we review the implications of these results in the closing section, section 8. The main results given here are derived from results in <ref> [28] </ref> (see also [35, 27, 22, 34]), where a general theory of minimax estimation error using relative entropy is developed that applies not only to classification problems in the form that we have defined them, but to other important statistical problems, including regression and density estimation. <p> General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]). This approach is further developed in <ref> [7, 8, 25, 41, 9, 6, 45, 42, 28] </ref>. The results of sections 2 through 5 show how this theory can be applied to the classification problem. 2 Using relative entropy and minimax risk to define estimation error Let us summarize the problem we are considering in its abstract setting. <p> We show this formally below. This reduction allows us to use results derived for the more general density estimation problem when analyzing the pattern recognition problem. To see how this reduction works, first let us define the risk function for the density estimation problem as in <ref> [28] </ref>. Denote the estimate for the distribution of Z given a sample S n by ^ P (zjS n ). <p> = yjx)dP (x) log ^ P (Y = yjx; S n )dP (x) Z dP n Z dP (x) y P fl (Y = yjx) = r n+1; ^ P; ( fl ): To complete this reduction, we define the minimax risk for the density esti mation problem as in <ref> [28] </ref> by r minimax;density n (fi) = inf sup r density n; ^ P where the infimum is over all possible estimators of the joint distribution on Z = X fi Y . <p> It is easy to verify that the minimax risk r minimax n is nonincreasing for all n, and in most cases approaches 0 as n goes to infinity <ref> [28] </ref>. The rate at which r minimax n approaches 0 depends primarily on the metric entropy properties of fi, the topic to which we now turn. <p> The Hellinger distance is useful because it is a metric, and the squared Hellinger distance approximates the relative entropy distance, which is not a metric. The sense of this approximation is given, e.g., in <ref> [28] </ref>. This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including LeCam [32], Birge [7, 8], Hasminskii and Ibragimov [25], and van de Geer [41]. <p> This follows directly from Theorem 7 in <ref> [28] </ref>, using claim (1). To see that the conditions of Theorem 7 in [28] hold, suppose jY j = K and let ^ P (Y = yjx) = 1=K for all x 2 X. <p> This follows directly from Theorem 7 in <ref> [28] </ref>, using claim (1). To see that the conditions of Theorem 7 in [28] hold, suppose jY j = K and let ^ P (Y = yjx) = 1=K for all x 2 X. <p> Going to further extremes, convergence is faster than any inverse polynomial for finite model classes fi (it can be shown to be exponential in n <ref> [28] </ref>), but model classes of infinite metric order, or that are not even totally bounded, are essentially "unlearnable" with this definition of estimation error as minimax risk: for any learning method there is a choice of true distribution that makes the convergence slower than 1 n ffi for all positive ffi. <p> When comparing the cumulative minimax risk to the minimax risk defined above, called the instantaneous minimax risk in <ref> [28] </ref> to contrast it with the cumulative minimax risk, note that from the definition of the individual minimax risks r minimax t , for 1 t n, we see that for each separate t we are possibly looking at a different worst case true distribution P fl when we compute the <p> However, it turns out that this effect cannot be very strong. In particular, it can be shown in general that R minimax n is nondecreasing in n, and n X r minimax t R minimax n nr minimax n (see <ref> [3, 13, 6, 28] </ref>.) It follows that R minimax n grows at most linearly in n, since r minimax t log jY j for all t. These inequalities also give fairly tight bounds on R minimax n in terms of r minimax n when r minimax n decreases slowly. <p> If mo (fi; D HL ) = 1 or (fi; D HL ) is not totally bounded, then log R minimax n (fi) ~ log n: Proof. Similar to the proof of Theorem 5, but using Theorem 4 of <ref> [28] </ref>. Furthermore, analogous results using upper and lower dimensions and orders also hold in the situation when the upper and lower dimensions/orders are different. <p> Then lim sup R minimax n (fi) = 2 lim inf R minimax n (fi) = 2 Proof. Let R minimax n = R minimax n (fi) and K (*) = K * (fi; D HL ). By Lemma 7 of <ref> [28] </ref>, there is some positive constant c such that for any n and any * &gt; 0, minfK (*); n* 2 =8g log 2 R minimax n K (*) + c* 2 n log n + c: Here we verify the conditions of the lemma again as in the proof of <p> It turns out that by careful choice of the prior, we can find Bayes methods that get asymptotically close to the best minimax performance. The priors to use can be found by examining the proof of the lower bound given in Lemma 7 of <ref> [28] </ref>, upon which the lower bound in the result given in Theorem 6 above is based. These place a uniform prior distribution on a finite subset of fi, chosen to be a maximal *-separated subset with respect to the Hellinger distance for some suitable *.
Reference: 29. <author> I. Ibragimov and R. Hasminskii. </author> <title> On the information in a sample about a parameter. </title> <booktitle> In Second Int. Symp. on Information Theory, </booktitle> <pages> pages 295-309, </pages> <year> 1972. </year>
Reference-contexts: relative entropy, has its roots in the early work by Ibragimov and Hasminskii, who showed that the cumulative relative entropy risk for Bayes methods for parametric density estimation on the real line is approximately (d=2) log n, where d is the number of parameters and n is the sample size <ref> [29] </ref>. In this case they were even able to estimate the lower order additive terms in this approximation, which involve the Fisher information and the entropy of the prior. Further related results were given by Efroimovich [20] and Clarke [12].
Reference: 30. <author> A. J. Izenman. </author> <title> Recent developments in nonparametric density estimation. </title> <journal> JASA, </journal> <volume> 86(413) </volume> <pages> 205-224, </pages> <year> 1991. </year>
Reference-contexts: There is a large statistical literature on minimax rates for estimation error for general statistical problems. However, much of this work has been done using loss functions other than relative entropy, see e.g. the texts <ref> [17, 30] </ref>.
Reference: 31. <author> A. N. Kolmogorov and V. M. Tihomirov. </author> <title> *-entropy and *-capacity of sets in functional spaces. </title> <journal> Amer. Math. Soc. Translations (Ser. </journal> <volume> 2), 17 </volume> <pages> 277-364, </pages> <year> 1961. </year>
Reference-contexts: There is a good general theory on approximation error, starting with the fundamental theorems of approximation theory, as given, for example, in the classic book of Lorentz [33] (see also <ref> [31, 15] </ref>). While in specific cases this error depends strongly on the nature of the true distribution, which is unknown, one can still make statements about the general approximability of functions or distributions in one family by functions or distributions in another. <p> The rate at which r minimax n approaches 0 depends primarily on the metric entropy properties of fi, the topic to which we now turn. The theory of packing and covering numbers, and the associated metric entropy, was introduced by Kolmogorov and Tikhomirov in <ref> [31] </ref>, and is commonly used in the theory of empirical processes (see e.g. [19, 36, 24, 9, 42]). For the following definitions, let (S; ) be any metric space. Definition 1. (Metric entropy, also called Kolmogorov *-entropy [31]) A partition of S is a collection f i g of subsets of <p> numbers, and the associated metric entropy, was introduced by Kolmogorov and Tikhomirov in <ref> [31] </ref>, and is commonly used in the theory of empirical processes (see e.g. [19, 36, 24, 9, 42]). For the following definitions, let (S; ) be any metric space. Definition 1. (Metric entropy, also called Kolmogorov *-entropy [31]) A partition of S is a collection f i g of subsets of S that are pairwise disjoint and whose union is S. The diameter of a set A S is given by diam (A) = sup x;y2A (x; y). <p> By M * (S; ) we denote the cardinality of the largest finite *-separated subset of S, or 1 if arbitrarily large such sets exist. The following lemma is easily verified <ref> [31] </ref>. Lemma 3. <p> Kolmogorov and Tikhomirov also introduced abstract notions of the dimen-sion and order of metric spaces in their seminal paper <ref> [31] </ref>. These can be used to measure the "massiveness" of both spaces indexed by finite dimensional parameter vectors and infinite dimensional function spaces. In the following, the metric is omitted from the notation, being understood from the context. Definition 4. The upper and lower metric dimensions [31] of S are defined <p> in their seminal paper <ref> [31] </ref>. These can be used to measure the "massiveness" of both spaces indexed by finite dimensional parameter vectors and infinite dimensional function spaces. In the following, the metric is omitted from the notation, being understood from the context. Definition 4. The upper and lower metric dimensions [31] of S are defined by dim (S) = lim sup K * (S) * dim (S) = lim inf K * (S) * respectively. When dim (S) = dim (S), then this value is denoted dim (S) and called the metric dimension of S.
Reference: 32. <author> L. LeCam. </author> <title> Asymptotic methods in statistical decision theory. </title> <publisher> Springer, </publisher> <year> 1986. </year>
Reference-contexts: General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. <ref> [32] </ref>). This approach is further developed in [7, 8, 25, 41, 9, 6, 45, 42, 28]. <p> The sense of this approximation is given, e.g., in [28]. This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including LeCam <ref> [32] </ref>, Birge [7, 8], Hasminskii and Ibragimov [25], and van de Geer [41]. Now assume that and fl are two joint distributions on X fi Y with a common marginal distribution on X.
Reference: 33. <author> G. Lorentz. </author> <title> Approxiamtion of Functions. </title> <publisher> Holt, Rinehart, Winston, </publisher> <year> 1966. </year>
Reference-contexts: There is a good general theory on approximation error, starting with the fundamental theorems of approximation theory, as given, for example, in the classic book of Lorentz <ref> [33] </ref> (see also [31, 15]). While in specific cases this error depends strongly on the nature of the true distribution, which is unknown, one can still make statements about the general approximability of functions or distributions in one family by functions or distributions in another.
Reference: 34. <author> R. Meir and N. Merhav. </author> <title> On the stochastic complexity of learning realizable and unrealizable rules. </title> <type> Unpublished manuscript, </type> <year> 1994. </year>
Reference-contexts: Finally, we review the implications of these results in the closing section, section 8. The main results given here are derived from results in [28] (see also <ref> [35, 27, 22, 34] </ref>), where a general theory of minimax estimation error using relative entropy is developed that applies not only to classification problems in the form that we have defined them, but to other important statistical problems, including regression and density estimation.
Reference: 35. <author> M. Opper and D. Haussler. </author> <title> Bounds for predictive errors in the statistical mechanics of in supervised learning. </title> <journal> Physical Review Letters, </journal> <volume> 75(20) </volume> <pages> 3772-3775, </pages> <year> 1995. </year>
Reference-contexts: Finally, we review the implications of these results in the closing section, section 8. The main results given here are derived from results in [28] (see also <ref> [35, 27, 22, 34] </ref>), where a general theory of minimax estimation error using relative entropy is developed that applies not only to classification problems in the form that we have defined them, but to other important statistical problems, including regression and density estimation.
Reference: 36. <author> D. Pollard. </author> <title> Empirical Processes: Theory and Applications, </title> <booktitle> volume 2 of NSF-CBMS Regional Conference Series in Probability and Statistics. </booktitle> <institution> Institute of Math. Stat. and Am. Stat. Assoc., </institution> <year> 1990. </year>
Reference-contexts: The theory of packing and covering numbers, and the associated metric entropy, was introduced by Kolmogorov and Tikhomirov in [31], and is commonly used in the theory of empirical processes (see e.g. <ref> [19, 36, 24, 9, 42] </ref>). For the following definitions, let (S; ) be any metric space. Definition 1. (Metric entropy, also called Kolmogorov *-entropy [31]) A partition of S is a collection f i g of subsets of S that are pairwise disjoint and whose union is S.
Reference: 37. <author> J. Rissanen. </author> <title> Stochastic complexity and modeling. </title> <journal> The Annals of Statistics, </journal> <volume> 14(3) </volume> <pages> 1080-1100, </pages> <year> 1986. </year>
Reference-contexts: These results were extended to the cumulative relative entropy Bayes and minimax risk in [14] (see also [5]). Related lower bounds, which are often quoted, were obtained by Rissanen <ref> [37] </ref>, based on certain asymptotic normality assumptions. Estimations of the relative entropy risk in nonparametric cases were obtained in [4, 6, 38, 45, 46].
Reference: 38. <author> J. Rissanen, T. Speed, and B. Yu. </author> <title> Density estimation by stochastic complexity. </title> <journal> IEEE Trans. Info. Th., </journal> <volume> 38 </volume> <pages> 315-323, </pages> <year> 1992. </year>
Reference-contexts: These results were extended to the cumulative relative entropy Bayes and minimax risk in [14] (see also [5]). Related lower bounds, which are often quoted, were obtained by Rissanen [37], based on certain asymptotic normality assumptions. Estimations of the relative entropy risk in nonparametric cases were obtained in <ref> [4, 6, 38, 45, 46] </ref>. General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]).
Reference: 39. <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> Journal of Combinatorial Theory (Series A), </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: This result, often cited as Sauer's Lemma <ref> [39] </ref>, was proven independently by Vapnik and Chervonenkis [44] (first in a slightly weaker version). dim V C (F) is called the VC dimension of F. It follows that dens (F ) dim V C (F ): (5) This inequality is often tight, but not always tight.
Reference: 40. <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-42, </pages> <year> 1984. </year>
Reference-contexts: Here we restrict ourselves to a special problem of two-class classification that has been called "noisy concept learning" in the AI and computational learning theory literature <ref> [40, 1, 10, 11, 21] </ref>. We show how fairly precise, general rates can be obtained for this problem based on a combinatorial parameter known as the Assouad density [2], which is related to the VC dimension [44].
Reference: 41. <author> S. van deGeer. </author> <title> Hellinger-consistency of certain nonparametric maximum likelihood estimators. </title> <journal> Annals of Statistics, </journal> <volume> 21 </volume> <pages> 14-44, </pages> <year> 1993. </year>
Reference-contexts: General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]). This approach is further developed in <ref> [7, 8, 25, 41, 9, 6, 45, 42, 28] </ref>. The results of sections 2 through 5 show how this theory can be applied to the classification problem. 2 Using relative entropy and minimax risk to define estimation error Let us summarize the problem we are considering in its abstract setting. <p> The sense of this approximation is given, e.g., in [28]. This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including LeCam [32], Birge [7, 8], Hasminskii and Ibragimov [25], and van de Geer <ref> [41] </ref>. Now assume that and fl are two joint distributions on X fi Y with a common marginal distribution on X.
Reference: 42. <author> A. van der Vaart and J. Wellner. </author> <title> Weak Convergence and Empirical Processes. </title> <publisher> Springer, </publisher> <address> NY, </address> <year> 1996. </year>
Reference-contexts: General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]). This approach is further developed in <ref> [7, 8, 25, 41, 9, 6, 45, 42, 28] </ref>. The results of sections 2 through 5 show how this theory can be applied to the classification problem. 2 Using relative entropy and minimax risk to define estimation error Let us summarize the problem we are considering in its abstract setting. <p> The theory of packing and covering numbers, and the associated metric entropy, was introduced by Kolmogorov and Tikhomirov in [31], and is commonly used in the theory of empirical processes (see e.g. <ref> [19, 36, 24, 9, 42] </ref>). For the following definitions, let (S; ) be any metric space. Definition 1. (Metric entropy, also called Kolmogorov *-entropy [31]) A partition of S is a collection f i g of subsets of S that are pairwise disjoint and whose union is S.
Reference: 43. <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: It turns out that tighter estimates of the convergence rate of the estimation error can be made in this case using the general theory. Following this, in section 7, we compare the classification results obtained this way to the results that can be obtained using the Vapnik-Chervonenkis theory <ref> [43] </ref>. Here we restrict ourselves to a special problem of two-class classification that has been called "noisy concept learning" in the AI and computational learning theory literature [40, 1, 10, 11, 21].
Reference: 44. <author> V. N. Vapnik and A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-80, </pages> <year> 1971. </year>
Reference-contexts: We show how fairly precise, general rates can be obtained for this problem based on a combinatorial parameter known as the Assouad density [2], which is related to the VC dimension <ref> [44] </ref>. Finally, we review the implications of these results in the closing section, section 8. <p> second part is established in a similar manner. 7 Vapnik-Chervonenkis entropy and dimension In this section we examine how the results given here relate to results that can be obtained by another approach that has often been used to analyze the convergence rates of classification methods, namely, the Vapnik-Chervonenkis dimension <ref> [44] </ref>. For simplicity, in this comparison we restrict ourselves to a special class of classification problems that we call noisy two-class learning (also called "noisy concept learning" in the computational learning theory and AI machine learning literature [1]). <p> This result, often cited as Sauer's Lemma [39], was proven independently by Vapnik and Chervonenkis <ref> [44] </ref> (first in a slightly weaker version). dim V C (F) is called the VC dimension of F. It follows that dens (F ) dim V C (F ): (5) This inequality is often tight, but not always tight. <p> This theorem shows that sup P R minimax n (fi; ) either grows logarithmically or slower, or it grows linearly. There is no rate in between. Results of this type are also available from the standard Vapnik-Chervonenkis theory <ref> [44, 11] </ref>. However, what is novel here is that in the case of logarithmic growth, the best possible constant in front of the logarithm is identified here to be the Assouad density.
Reference: 45. <author> W. Wong and X. Shen. </author> <title> Probability inequalities for likelihood ratios and convergence rates for sieve MLE's. </title> <journal> Annals of Statistics, </journal> <volume> 23(2) </volume> <pages> 339-362, </pages> <year> 1995. </year>
Reference-contexts: These results were extended to the cumulative relative entropy Bayes and minimax risk in [14] (see also [5]). Related lower bounds, which are often quoted, were obtained by Rissanen [37], based on certain asymptotic normality assumptions. Estimations of the relative entropy risk in nonparametric cases were obtained in <ref> [4, 6, 38, 45, 46] </ref>. General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]). <p> General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]). This approach is further developed in <ref> [7, 8, 25, 41, 9, 6, 45, 42, 28] </ref>. The results of sections 2 through 5 show how this theory can be applied to the classification problem. 2 Using relative entropy and minimax risk to define estimation error Let us summarize the problem we are considering in its abstract setting.
Reference: 46. <author> B. Yu. </author> <title> Lower bounds on expected redundancy for nonparametric classes. </title> <journal> IEEE Trans. Info. Th., </journal> <volume> 42(1), </volume> <year> 1996. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: These results were extended to the cumulative relative entropy Bayes and minimax risk in [14] (see also [5]). Related lower bounds, which are often quoted, were obtained by Rissanen [37], based on certain asymptotic normality assumptions. Estimations of the relative entropy risk in nonparametric cases were obtained in <ref> [4, 6, 38, 45, 46] </ref>. General approaches, for loss functions other than the relative entropy, to minimax risk in nonparametric density estimation were pioneered by Le Cam, who introduced methods using metric entropy and Hellinger distance (see e.g. [32]).
References-found: 46

