URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tm.outbox/MIT-LCS-TM-521.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/catatm.html
Root-URL: 
Title: Addressing Partitioned Arrays in Distributed Memory Multiprocessors the Software Virtual Memory Approach  
Author: Rajeev Barua David Kranz Anant Agarwal 
Keyword: multiprocessors, compilers, addressing, data partitioning, loop par titioning, pages, virtual memory, locality.  
Note: Authors'  Authors' phone: (617)253-8438.  
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Email: e-mail: fbarua,kranz,agarwalg@lcs.mit.edu.  
Date: October 1993  
Abstract: Partitioning distributed arrays to ensure locality of reference is widely recognized as being critical in obtaining good performance on distributed memory multiprocessors. Data partitioning is the process of tiling data arrays and placing the tiles in memory such that a maximum number of data accesses are satisfied from local memory. Unfortunately, data partitioning makes it difficult to physically locate an element of a distributed array. Data tiles with complicated shapes, such as hyperparallelepipeds, exacerbate this addressing problem. In this paper we propose a simple scheme called software virtual memory that allows flexible addressing of partitioned arrays with low runtime overhead. Software virtual memory implements address translation in software using small, one-dimensional pages, and a compiler-generated software page map. Because page sizes are chosen by the compiler, arbitrarily complex data tiles can be used to maximize locality, and because the pages are one-dimensional, runtime address computations are simple and efficient. One-dimensional pages also ensure that software virtual memory is more efficient than simple blocking for rectangular data tiles. Software virtual memory provides good locality for complicated compile-time partitions, thus enabling the use of sophisticated partitioning schemes appearing in recent literature. Software virtual memory can also be used in systems that provide hardware support for virtual memory. Although hardware virtual memory, when used exclusively, eliminates runtime overhead for addressing, we demonstrate that it does not preserve locality of reference to the same extent as software virtual memory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. G. Abraham and D. E. Hudak. </author> <title> Compile-time partitioning of iterative parallel loops to reduce cache coherency traffic. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 318-328, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 3, 6, 13] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse of data in the cache, subject to the constraint of having good load balance.
Reference: [2] <author> Anant Agarwal, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Ku-biatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, Dan Nussbaum, Mike Parkin, and Donald Yeung. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: Note that in distributed memory machines without a shared address space, software virtual memory has more to offer because hardware virtual memory is not supported. We have implemented the software virtual memory scheme in the compiler and runtime system for the Alewife machine <ref> [2] </ref>, a globally cache-coherent distributed-memory multiprocessor. We use the method of loop and data partitioning described in [3]. In this paper we demonstrate that: * The overhead of software virtual memory is small in general.
Reference: [3] <author> Anant Agarwal, David Kranz, and Venkat Natarajan. </author> <title> Automatic Partitioning of Parallel Loops for Cache-Coherent Multiprocessors. </title> <booktitle> In 22nd International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year> <note> IEEE. A version of this paper appears as MIT/LCS TM-481, </note> <month> December </month> <year> 1992. </year> <month> 14 </month>
Reference-contexts: 1 Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 3, 6, 13] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse of data in the cache, subject to the constraint of having good load balance. <p> We have implemented the software virtual memory scheme in the compiler and runtime system for the Alewife machine [2], a globally cache-coherent distributed-memory multiprocessor. We use the method of loop and data partitioning described in <ref> [3] </ref>. In this paper we demonstrate that: * The overhead of software virtual memory is small in general. <p> These methods only work well when the granularity of the computation is large and regular. Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches <ref> [3, 6] </ref>. In these machines, each processor controls a local portion of the global memory; references to the local portion have lower latency than references that access remote data over the communication network. <p> This is a generalization of blocking. As shown in <ref> [3] </ref>, parallelogram partitions are often required to ensure optimal locality when array accesses contain affine index functions. Let us illustrate the difficulty of addressing parallelogram data tiles with the following example. Consider the nested Doall loop in Figure 2.
Reference: [4] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, God--frey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <journal> IEEE Micro, </journal> <volume> 13(3) </volume> <pages> 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: An add of the offset obtained from the virtual address to the physical page base. the page table is in r1 and the virtual address is in r2. The sequence adds an overhead of only four instructions (the lddf would be done anyway). On the Sparcle processor <ref> [4] </ref> used in Alewife, these 4 instructions require 5 cycles, assuming all instructions and the page table lookup hit in the cache. We expect that the cache hit rate will not degrade significantly even for small page sizes, because the page table entries are small compared to a page.
Reference: [5] <author> Saman P. Amarasinghe and Monica S. Lam. </author> <title> Communication Optimization and Code Generation for Distributed Memory Machines. </title> <booktitle> In Proceedings of SIGPLAN '93, Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 7, 8, 10, 9, 11, 12, 14, 16] </ref>. These methods only work well when the granularity of the computation is large and regular. Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [6] <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of SIGPLAN '93, Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: 1 Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 3, 6, 13] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse of data in the cache, subject to the constraint of having good load balance. <p> These methods only work well when the granularity of the computation is large and regular. Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches <ref> [3, 6] </ref>. In these machines, each processor controls a local portion of the global memory; references to the local portion have lower latency than references that access remote data over the communication network.
Reference: [7] <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On Estimating and Enhancing Cache Effectiveness, </title> <address> pages 328-341. </address> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1991. </year> <booktitle> Lecture Notes in Computer Science: Languages and Compilers for Parallel Computing. </booktitle> <editor> Editors U. Banerjee and D. Gelernter and A. Nicolau and D. </editor> <address> Padua. </address>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 7, 8, 10, 9, 11, 12, 14, 16] </ref>. These methods only work well when the granularity of the computation is large and regular. Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [8] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 7, 8, 10, 9, 11, 12, 14, 16] </ref>. These methods only work well when the granularity of the computation is large and regular. Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [9] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD Distributed Memory Machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 7, 8, 10, 9, 11, 12, 14, 16] </ref>. These methods only work well when the granularity of the computation is large and regular. Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [10] <author> F. Irigoin and R. Triolet. </author> <title> Supernode Partitioning. </title> <booktitle> In 15th Symposium on Principles of Programming Languages (POPL XV), </booktitle> <pages> pages 319-329, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 7, 8, 10, 9, 11, 12, 14, 16] </ref>. These methods only work well when the granularity of the computation is large and regular. Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [11] <author> K. Knobe, J. Lukas, and G. Steele Jr. </author> <title> Data Optimization: Allocation of Arrays to Reduce Communication on SIMD Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <year> 1990. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 7, 8, 10, 9, 11, 12, 14, 16] </ref>. These methods only work well when the granularity of the computation is large and regular. Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [12] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling Global Name-Space Parallel Loops for Distributed Execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> October </month> <year> 1991. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 7, 8, 10, 9, 11, 12, 14, 16] </ref>. These methods only work well when the granularity of the computation is large and regular. Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [13] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-Time Techniques for Data Distribution in Distributed Memory Machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 3, 6, 13] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse of data in the cache, subject to the constraint of having good load balance.
Reference: [14] <author> Anne Rogers and Keshav Pingali. </author> <title> Process Decomposition through Locality of Reference. </title> <booktitle> In SIGPLAN '89, Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 7, 8, 10, 9, 11, 12, 14, 16] </ref>. These methods only work well when the granularity of the computation is large and regular. Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [15] <author> Madhusudhan Talluri, Shing Kong, Mark D. Hill, and David A. Patterson. </author> <title> Tradeoffs in Supporting Two Page Sizes. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: On the other hand, hardware virtual memory systems that support multiple page sizes might reduce some of the problems with fixed page sizes. Although multiple-page-size systems merit further exploration, they do not appear very promising because only the simplest of these solutions are practical to build <ref> [15] </ref>, and the need to support very small pages further complicates the hardware.
Reference: [16] <author> M. Wolfe. </author> <title> More Iteration Space Tiling. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 655-664, </pages> <month> November </month> <year> 1989. </year> <month> 15 </month>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 7, 8, 10, 9, 11, 12, 14, 16] </ref>. These methods only work well when the granularity of the computation is large and regular. Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
References-found: 16

