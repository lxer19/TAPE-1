URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/osullivan.ml95-ebnn.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/full.html
Root-URL: http://www.cs.cmu.edu
Title: Explanation-Based Learning for Mobile Robot Perception  
Author: Tom M. Mitchell Joseph O'Sullivan Sebastian Thrun 
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Explanation-based neural network learning (EBNN) has recently been proposed as a method for reducing the amount of training data required for reliable generalization, by relying instead on approximate, previously learned knowledge. We present first experiments applying EBNN to the problem of learning object recognition for a mobile robot. In these experiments, a mobile robot traveling down a hallway corridor learns to recognize distant doors based on color camera images and sonar sensations. The previously learned knowledge corresponds to a neural network that recognizes nearby doors, and a second network that predicts the state of the world after travelling forward in the corridor. Experimental results show that EBNN is able to use this approximate prior knowledge to significantly reduce the number of training examples required to learn to recognize distant doors. We also present results of experiments in which networks learned by EBNN (e.g., "there is a door 2 meters ahead") are then used as background knowledge for learning subsequent functions (e.g., "there is a door 3 meters ahead"). 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> DeJong, G., and Mooney, R. </author> <title> "Explanation-Based Learning: An Alternative View". </title> <booktitle> Machine Learning 1, 2 (1986), </booktitle> <pages> 145-176. </pages>
Reference-contexts: The analytical component of learning in EBNN is similar to earlier explanation-based learning methods based on symbolic representations <ref> [1, 7] </ref>: given a target function, F, the domain theory is used as an alternative method for computing F, and the dependencies are extracted from this computation. The EBNN algorithm is described in greater detail in [8, 15], and summarized here in Figure 2-1. 2 For each training example: 1.
Reference: 2. <author> Katsushi Ikeuchi and Takashi Suehiro. </author> <title> Towards an assembly plan from observation. </title> <booktitle> Proceedings of IEEE Int. Conf. on Robotics and Automation, </booktitle> <month> May, </month> <year> 1992. </year>
Reference-contexts: One approach to scaling up is to rely on human training (e.g., <ref> [2, 3, 4, 11] </ref>). We are interested here in methods by which the robot can use previously learned knowledge to reduce the need for new data in subsequent learning.
Reference: 3. <author> J. Laird, E. Yager, C. Tuck, and M. Hucka. </author> <title> Learning in tele-autonomous systems using Soar. </title> <booktitle> Proceedings of the 1989 NASA Conference of Space Telerobotics, </booktitle> <year> 1989. </year>
Reference-contexts: One approach to scaling up is to rely on human training (e.g., <ref> [2, 3, 4, 11] </ref>). We are interested here in methods by which the robot can use previously learned knowledge to reduce the need for new data in subsequent learning.
Reference: 4. <author> Long-Ji Lin. </author> <title> Reinforcement Learning for Robots using Neural Networks. </title> <type> Ph.D. </type> <institution> Th., Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA 15232, </address> <year> 1993. </year> <note> Technical Report CMU-CS-93-103. </note>
Reference-contexts: 1. Introduction One crucial problem for robot learning is scaling up. Whereas various research has shown how robots can learn relatively simple strategies from very little initial knowledge (e.g., <ref> [4, 6, 5, 9] </ref>, theoretical and experimental results indicate that simple learning approaches will require unrealistic amounts of training data to learn significantly more complex functions. One approach to scaling up is to rely on human training (e.g., [2, 3, 4, 11]). <p> One approach to scaling up is to rely on human training (e.g., <ref> [2, 3, 4, 11] </ref>). We are interested here in methods by which the robot can use previously learned knowledge to reduce the need for new data in subsequent learning.
Reference: 5. <author> Pattie Maes and Rodney Brooks. </author> <title> Learning to Coordinate Behaviors. </title> <booktitle> Proceedings of AAAI-90, </booktitle> <address> Boston, </address> <month> August, </month> <year> 1990. </year>
Reference-contexts: 1. Introduction One crucial problem for robot learning is scaling up. Whereas various research has shown how robots can learn relatively simple strategies from very little initial knowledge (e.g., <ref> [4, 6, 5, 9] </ref>, theoretical and experimental results indicate that simple learning approaches will require unrealistic amounts of training data to learn significantly more complex functions. One approach to scaling up is to rely on human training (e.g., [2, 3, 4, 11]).
Reference: 6. <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Scaling Reinforcement Learning to Robotics by Exploiting the Subsumption Architecture. </title> <booktitle> Proceedings of Machine Learning Workshop '91, </booktitle> <month> July, </month> <year> 1991. </year>
Reference-contexts: 1. Introduction One crucial problem for robot learning is scaling up. Whereas various research has shown how robots can learn relatively simple strategies from very little initial knowledge (e.g., <ref> [4, 6, 5, 9] </ref>, theoretical and experimental results indicate that simple learning approaches will require unrealistic amounts of training data to learn significantly more complex functions. One approach to scaling up is to rely on human training (e.g., [2, 3, 4, 11]).
Reference: 7. <author> Mitchell, T.M., Keller, R.K., and Kedar-Cabelli, S. </author> <title> "Explanation-Based Generalization: A Unifying View". </title> <booktitle> Machine Learning 1, </booktitle> <month> 1 </month> <year> (1986). </year>
Reference-contexts: The analytical component of learning in EBNN is similar to earlier explanation-based learning methods based on symbolic representations <ref> [1, 7] </ref>: given a target function, F, the domain theory is used as an alternative method for computing F, and the dependencies are extracted from this computation. The EBNN algorithm is described in greater detail in [8, 15], and summarized here in Figure 2-1. 2 For each training example: 1.
Reference: 8. <author> Tom M. Mitchell and Sebastian B. Thrun. </author> <title> Explanation-Based Neural Network Learning for Robot Control. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <editor> J. E. Moody and S. J. Hanson and R. P. Lipmann, Ed., </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: This paper presents early experiments with a robot architecture that utilizes previously learned knowledge about the effects of its actions, to reduce the difficulty of learning to recognize objects that it approaches as it travels through its environment. This architecture is based on explanation-based neural network learning (EBNN) <ref> [8, 15] </ref>, an explanation-based algorithm that represents both prior knowledge and the current target function using neural networks. Initial experiments with EBNN demonstrated its ability to learn robot control knowledge in simulated [8] and real-world robot domains [14]. <p> This architecture is based on explanation-based neural network learning (EBNN) [8, 15], an explanation-based algorithm that represents both prior knowledge and the current target function using neural networks. Initial experiments with EBNN demonstrated its ability to learn robot control knowledge in simulated <ref> [8] </ref> and real-world robot domains [14]. Here we examine its ability to reduce training data required for learning robot perception skills. <p> The EBNN algorithm is described in greater detail in <ref> [8, 15] </ref>, and summarized here in Figure 2-1. 2 For each training example: 1. Explain how the training example satisfied the target function. Chain together the domain theory networks to predict the value of the target function for the training example. 2. Analyze this explanation to determine feature relevance.
Reference: 9. <author> Andrew W. Moore. </author> <title> Efficent Memory-based Learning for Robot Control. </title> <type> Ph.D. </type> <institution> Th., Trinity Hall, University of Cambridge, </institution> <address> England, </address> <year> 1990, 1990. </year>
Reference-contexts: 1. Introduction One crucial problem for robot learning is scaling up. Whereas various research has shown how robots can learn relatively simple strategies from very little initial knowledge (e.g., <ref> [4, 6, 5, 9] </ref>, theoretical and experimental results indicate that simple learning approaches will require unrealistic amounts of training data to learn significantly more complex functions. One approach to scaling up is to rely on human training (e.g., [2, 3, 4, 11]).
Reference: 10. <author> Joseph O'Sullivan. </author> <title> Xavier Manual. </title> <institution> Carnegie Mellon University, Learning Robot Lab Internal Document - contact josullvn@cs.cmu.edu. </institution>
Reference: 11. <author> Dean Pomerleau. </author> <title> Neural Network Perception for Mobile Robot Guidance. </title> <type> Ph.D. </type> <institution> Th., Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA 15232, </address> <year> 1992. </year> <note> Technical Report CMU-CS-92-115. </note>
Reference-contexts: One approach to scaling up is to rely on human training (e.g., <ref> [2, 3, 4, 11] </ref>). We are interested here in methods by which the robot can use previously learned knowledge to reduce the need for new data in subsequent learning.
Reference: 12. <author> David E. Rumelhart, Geoffery E.Hinton and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In David E. Rymelhart and J. L. McClelland, Ed., </editor> <booktitle> Parallel Distributed Processing. Vol I+II, </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: How can the domain theory networks be used to analyze training examples so that information is extracted in a form useful for refining the target network? The key lies in the TangentProp algorithm [13], an extension to Backpropagation <ref> [12] </ref> that is able to adjust network weights to minimize the error in both the values and the derivatives of the function computed by the target network. Consider some target function, F, and a training example, X, consisting is a vector of components x . <p> Given a significant number of training examples, such a network can be learned to a reasonable accuracy using the Backpropagation algorithm <ref> [12] </ref>. We are interested here in comparing such purely inductive methods with EBNN. To apply EBNN to the task of learning Door-ahead , we must have available prior knowledge 1 capable of explaining observed training examples of the target function. We therefore allow Xavier to first learn two neural networks.
Reference: 13. <author> Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. </author> <title> Tangent prop -- a formalism for specifying selected invariances in an adaptive network. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <editor> J. E. Moody and S. J. Hanson and R. P. Lipmann, Ed., </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1992, </year> <pages> pp. </pages> <month> 895--903. </month>
Reference-contexts: How can the domain theory networks be used to analyze training examples so that information is extracted in a form useful for refining the target network? The key lies in the TangentProp algorithm <ref> [13] </ref>, an extension to Backpropagation [12] that is able to adjust network weights to minimize the error in both the values and the derivatives of the function computed by the target network. Consider some target function, F, and a training example, X, consisting is a vector of components x .
Reference: 14. <author> Sebastian B. Thrun. </author> <title> An Approach to Learning Robot Navigation. </title> <booktitle> Proceedings of the IEEE/RSJ/GI International Conference on Intelligent Robots and Systems, </booktitle> <month> September, </month> <year> 1994. </year> <note> (to appear). </note>
Reference-contexts: This architecture is based on explanation-based neural network learning (EBNN) [8, 15], an explanation-based algorithm that represents both prior knowledge and the current target function using neural networks. Initial experiments with EBNN demonstrated its ability to learn robot control knowledge in simulated [8] and real-world robot domains <ref> [14] </ref>. Here we examine its ability to reduce training data required for learning robot perception skills.
Reference: 15. <author> Sebastian B. Thrun, and Tom M. Mitchell. </author> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> Proceedings of IJCAI-93, IJCAI, </booktitle> <address> Chamberry, France, </address> <month> July, </month> <year> 1993. </year>
Reference-contexts: This paper presents early experiments with a robot architecture that utilizes previously learned knowledge about the effects of its actions, to reduce the difficulty of learning to recognize objects that it approaches as it travels through its environment. This architecture is based on explanation-based neural network learning (EBNN) <ref> [8, 15] </ref>, an explanation-based algorithm that represents both prior knowledge and the current target function using neural networks. Initial experiments with EBNN demonstrated its ability to learn robot control knowledge in simulated [8] and real-world robot domains [14]. <p> The EBNN algorithm is described in greater detail in <ref> [8, 15] </ref>, and summarized here in Figure 2-1. 2 For each training example: 1. Explain how the training example satisfied the target function. Chain together the domain theory networks to predict the value of the target function for the training example. 2. Analyze this explanation to determine feature relevance.
References-found: 15

