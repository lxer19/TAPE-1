URL: http://theory.lcs.mit.edu/~shanir/suz.ps
Refering-URL: http://theory.lcs.mit.edu/~shanir/
Root-URL: 
Title: A Steady State Analysis of Diffracting Trees  
Author: Nir Shavit Eli Upfal Asaph Zemach 
Date: February 11, 1997  
Abstract: Diffracting trees are an effective and highly scalable distributed-parallel technique for shared counting and load balancing. This paper presents the first steady-state combinatorial model and analysis for diffracting trees, and uses it to answer several critical algorithmic design questions. Our model is simple and sufficiently high level to overcome many implementation specific details, and yet as we will show it is rich enough to accurately predict empirically observed behaviors. As a result of our analysis we were able to identify starvation problems in the original diffracting tree algorithm and modify it to a create a more stable version. We are also able to identify the range in which the diffracting tree performs most efficiently, and the ranges in which its performance degrades. We believe our model and modeling approach open the way to steady-state analysis of other distributed-parallel structures such as counting networks and elimination trees.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal and M. Cherian. </author> <title> Adaptive Backoff Synchronization Techniques. </title> <booktitle> In Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: If the first compare-and-swap fails, it means that some other processor has already managed to collide with p, so p is diffracted to the next <ref> [1] </ref> balancer (line 10). <p> If it could not remove itself from the set, it follows that some other processor already collided with p, and it exits the balancer, being diffracted to next <ref> [1] </ref> (lines 23-24). 2.2 The Critical Parameters As a rule of thumb, when a large number of processors concurrently enter the balancer, the chances for successful collisions in prism are high, and contention on the toggle bit is unlikely. <p> to balancer) returns ptr to balancer 2 begin 3 location [MYID] = node 4 rand_place = random (node.size) 5 his_id = SWAP (node.prism [rand_place],MYID) 6 if C&S (location [MYID],node,EMPTY) then 7 if C&S (location [his_id],node,EMPTY) then 8 return node.next [0] 9 else location [MYID] = node 10 else return node.next <ref> [1] </ref> 11 repeat forever 12 repeat node.spin times 13 if location [MYID] != node 14 return node.next [1] 15 endrepeat 16 if T&T&S (node.lock) then 17 if C&S (location [MYID],node,EMPTY) then 18 bit_val = node.toggle_bit 19 node.toggle_bit = 1 - bit_val 20 node.lock = OPEN 21 return node.next [bit_val] 22 else <p> (node.size) 5 his_id = SWAP (node.prism [rand_place],MYID) 6 if C&S (location [MYID],node,EMPTY) then 7 if C&S (location [his_id],node,EMPTY) then 8 return node.next [0] 9 else location [MYID] = node 10 else return node.next <ref> [1] </ref> 11 repeat forever 12 repeat node.spin times 13 if location [MYID] != node 14 return node.next [1] 15 endrepeat 16 if T&T&S (node.lock) then 17 if C&S (location [MYID],node,EMPTY) then 18 bit_val = node.toggle_bit 19 node.toggle_bit = 1 - bit_val 20 node.lock = OPEN 21 return node.next [bit_val] 22 else 23 node.lock = OPEN 24 return node.next [1] 25 endif 26 endif 27 endrepeat 28 end 9 <p> 13 if location [MYID] != node 14 return node.next <ref> [1] </ref> 15 endrepeat 16 if T&T&S (node.lock) then 17 if C&S (location [MYID],node,EMPTY) then 18 bit_val = node.toggle_bit 19 node.toggle_bit = 1 - bit_val 20 node.lock = OPEN 21 return node.next [bit_val] 22 else 23 node.lock = OPEN 24 return node.next [1] 25 endif 26 endif 27 endrepeat 28 end 9 short while, reach for the toggle bit and be off, since all "spinning" is done on a locally cached copy of a memory location, it incurs no overhead. <p> [MYID] = node 4 forever /* Moved up to encompass entire algorithm */ 5 rand_place = random (node.size) 6 his_id = SWAP (node.prism [rand_place],MYID) 7 if C&S (location [MYID],node,EMPTY) then 8 if C&S (location [his_id],node,EMPTY) then 9 return node.next [0] 10 else location [MYID] = node 11 else return node.next <ref> [1] </ref> 12 repeat node.spin times 13 if location [MYID] != node then /* diffracted? probably a high load better to spin longer */ 14 if node.spin &lt; MAXSPIN then 15 node.spin = node.spin * 2 16 endif 17 return node.next [1] 18 endif 19 endrepeat 20 if T&T&S (node.lock) then 21 <p> 10 else location [MYID] = node 11 else return node.next <ref> [1] </ref> 12 repeat node.spin times 13 if location [MYID] != node then /* diffracted? probably a high load better to spin longer */ 14 if node.spin &lt; MAXSPIN then 15 node.spin = node.spin * 2 16 endif 17 return node.next [1] 18 endif 19 endrepeat 20 if T&T&S (node.lock) then 21 if C&S (location [MYID],node,EMPTY) then 22 bit_val = node.toggle_bit 23 node.toggle_bit = 1 - bit_val 24 node.lock = OPEN /* toggled? probably a low load better to spin less */ 25 if node.spin &gt; 1 then 26 node.spin = node.spin <p> = node.toggle_bit 23 node.toggle_bit = 1 - bit_val 24 node.lock = OPEN /* toggled? probably a low load better to spin less */ 25 if node.spin &gt; 1 then 26 node.spin = node.spin / 2 27 endif 28 return node.next [bit_val] 29 else 30 node.lock = OPEN 31 return node.next <ref> [1] </ref> 32 endif 33 endif 34 endfor 35 end 12 is constructed around the pairing of canceling tokens, those that leave the bal-ancer through the return node.next [0] of line 9, and canceled tokens, those that leave the balancer through the return node.next [1] of lines 11, 17 or 31. <p> 30 node.lock = OPEN 31 return node.next <ref> [1] </ref> 32 endif 33 endif 34 endfor 35 end 12 is constructed around the pairing of canceling tokens, those that leave the bal-ancer through the return node.next [0] of line 9, and canceled tokens, those that leave the balancer through the return node.next [1] of lines 11, 17 or 31. Since all other tokens go through the toggle bit, showing that the number of canceled tokens is equal to the number of canceling tokens is enough to prove that a balance is maintained on the balancer's output wires.
Reference: [2] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: It is these and similar questions that our work attempts to address. 2.3 The New Algorithm We begin by modifying the diffracting tree algorithm presented in Section 2.1. Touitou [22] reports the following when running a benchmark on the prototype MIT Alewife machine <ref> [2] </ref>. In his benchmark, processors repeatedly attempt to increment a diffracting tree based counter until some fixed number of increments has been performed. During sufficiently long runs, some processors end up performing all the increments, while all others remain "starving" in the tree. <p> the full diffracting tree should feed a binary tree of depth dlog 2 `e with ` counters in the leaves. 4 Experimental Results In order to verify the validity of our theoretical analysis we ran a set of benchmarks on a simulated distributed-shared-memory multiprocessor similar to the MIT Alewife machine <ref> [2] </ref> developed by Agarwal, et. al. Alewife is a large-scale multiprocessor that supports cache-coherent distributed shared memory and user-level message-passing. The nodes communicate via messages on a two-dimensional mesh network.
Reference: [3] <author> B. Aiello, R. Venkatesan and M. Yung. </author> <title> Optimal Depth Counting Networks. </title> <type> personal communication. </type>
Reference-contexts: The closest modeling work related to ours is the amortized contention model of Dwork, Herlihy, and Waarts [9] used in the analysis of counting networks 3 [9] and of the randomized counting networks by Aiello, Venkatesan, Yung <ref> [3] </ref>. However, unlike our work, that analysis is directed at modeling and quantifying contention in the face of a worst case adversary, not the steady state behaviors of the algorithms in normal (i.e. common case) executions.
Reference: [4] <author> J. Aspnes, M.P. Herlihy, and N. Shavit. </author> <title> Counting Networks. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 41, No. </volume> <month> 5 (September </month> <year> 1994), </year> <pages> pp. 1020-1048. 24 </pages>
Reference-contexts: 1 Introduction Diffracting trees [19] are among the most effective and scalable distributed-parallel techniques for shared counting, with a variety of applications to load balancing and concurrent data structure design. Diffracting trees are a special form of the counting networks of Aspnes, Herlihy, and Shavit <ref> [4] </ref>. They are fl MIT and Tel-Aviv University. Supported by National Science Foundation grant CCR-9520298. Contact Author: shanir@theory.lcs.mit.edu. y The Weizmann Institute, Israel, and IBM Almaden Research Center, California. Work at the Weizmann Institute supported in part by the Norman D. <p> To illustrate this property, consider an execution in which tokens traverse the tree sequentially, one completely after the other. Figure 1 shows such an execution on a Binary <ref> [4] </ref> counting tree, the tree moves input tokens to output wires in increasing order modulo w. Trees having this property are called counting trees because they can easily be adapted to count the total number of tokens that have entered the network. <p> The constant hidden by the O notation is small and depends on a particular machine's ability to handle multiple accesses to the same memory location. This is an expected result and fits well with the saturation model of Aspnes, Herlihy, and Shavit for counting networks <ref> [4] </ref>. The following figures show how our model accurately predicts the experimental results. Figure 9 shows the latency of diffracting trees five and six levels deep. In these experiments we use binary trees with d levels (meaning 2 d counters) and L = 2 d .
Reference: [5] <author> J. Aspnes, M.P. Herlihy, and N. Shavit. </author> <title> Counting Networks and Multi--Processor Coordination. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Theory of Computing, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: through benchmarks on a simulated shared memory machine to support the analysis and Section 5 concludes this paper and lists areas of further research. 2 Counting Trees and Diffraction Diffracting trees [19] are counting trees, a special form of the counting network data structures introduced by Aspnes, Herlihy and Shavit <ref> [5] </ref>. They are binary trees of nodes called balancers. A balancer is a computing element with one input wire and two output wires. Tokens arrive on the balancer's input wire at arbitrary times, and are output on its output wires.
Reference: [6] <author> E.A. Brewer, C.N. Dellarocas. </author> <title> Proteus User Documentation. MIT, 545 Technology Square, </title> <address> Cambridge, MA 02139, 0.5 edition, </address> <month> December </month> <year> 1992. </year>
Reference: [7] <author> E.A. Brewer, C.N. Dellarocas, A. Colbrook and W.E. Weihl. Proteus: </author> <title> A High-Performance Parallel-Architecture Simulator. </title> <type> MIT Technical Report /MIT/LCS/TR-561, </type> <month> September </month> <year> 1991. </year>
Reference-contexts: Our experiments make use of the shared memory interface only. To simulate the Alewife we used Proteus 1 , a multiprocessor simulator developed by Brewer, Dellarocas, Colbrook and Weihl <ref> [7] </ref>. Proteus simulates parallel code by multiplexing several parallel threads on a single CPU. Each thread runs on its own virtual CPU with accompanying local memory, cache and communications hardware, keeping track of how much time is spent using each component.
Reference: [8] <author> D.G. </author> <title> Carta Two Fast Implementations of the "Minimal Standard" Random Number Generator. </title> <journal> CACM, </journal> <volume> 33(1), </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: The random number function we used was Proteus' fast random () which is an implementation of the ACM Minimal Standard Random Number Genera 20 tor <ref> [17, 8] </ref>. We now show how our combinatorial model ties together the choice of diffracting tree parameters depth, d, and prism locations per level, L, to the number of processor, P .
Reference: [9] <author> C. Dwork, M. P. Herlihy, and O. Waarts. </author> <title> Contention in shared memory algorithms. </title> <booktitle> In Proceedings of the 25th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 174-183, </pages> <month> May </month> <year> 1993. </year> <note> Expanded version: Digital Equipment Corporation Technical Report CRL 93/12. </note>
Reference-contexts: In the final section of this paper we provide a collection of experimental benchmarks that show how accurately our model fits with actual diffracting tree performance. The closest modeling work related to ours is the amortized contention model of Dwork, Herlihy, and Waarts <ref> [9] </ref> used in the analysis of counting networks 3 [9] and of the randomized counting networks by Aiello, Venkatesan, Yung [3]. <p> The closest modeling work related to ours is the amortized contention model of Dwork, Herlihy, and Waarts <ref> [9] </ref> used in the analysis of counting networks 3 [9] and of the randomized counting networks by Aiello, Venkatesan, Yung [3]. However, unlike our work, that analysis is directed at modeling and quantifying contention in the face of a worst case adversary, not the steady state behaviors of the algorithms in normal (i.e. common case) executions.
Reference: [10] <author> S.N. Ethier and T.G. Kurtz. </author> <title> Markov Processes: Characterization and Convergence. </title> <publisher> John Wiley and Sons, </publisher> <year> 1986. </year>
Reference-contexts: To approximate the performance of the discrete time Markov chain we study a related continuous time, density dependent jump Markov chain (see [15, Chapers 7-8] or <ref> [10, Chapter 11] </ref> for detailed discussion of density dependent jump Markov processes and the convergence theorem we use here). Processors in the continuous Markov process execute the same steps as in the discrete process.
Reference: [11] <author> J.R. Goodman, M.K. Vernon, and P.J. Woest. </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent multiprocessors. </title> <booktitle> In Proceedings of the 3rd ASPLOS, </booktitle> <pages> pages 64-75. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1989. </year>
Reference-contexts: The depth 5 tree also shows how latency increases again as concurrency increases. Note that the calibration of our graphs, and hence the phenomena we are modeling, are very fine relative to the changes in latency for other types of data structures. For example, in [19], combining trees <ref> [11] </ref> are shown to have a latency increase by 2500 units over the tested concurrency range, and so the 300 unit change in latency of diffracting trees would be considered almost constant. See [19] for details. in a level of the diffracting tree while keeping the other parameters constant.
Reference: [12] <author> M.P. Herlihy. </author> <title> Wait-Free Synchronization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(1) </volume> <pages> 123-149, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Given appropriate hardware primitives, diffracting trees can be imple 2 mented in a lock-free manner. In fact, assuming a hardware Fetch&Complement operation allows making diffracting trees wait-free <ref> [12] </ref>, that is, for each increment operation termination is guaranteed in a bounded number of steps even if all other processors fail. When implementing diffracting trees [13], the following type of questions are of critical importance.
Reference: [13] <author> S. </author> <title> Kahan - TERA Computer Company. </title> <type> Personal communication, </type> <month> May </month> <year> 1995. </year>
Reference-contexts: In fact, assuming a hardware Fetch&Complement operation allows making diffracting trees wait-free [12], that is, for each increment operation termination is guaranteed in a bounded number of steps even if all other processors fail. When implementing diffracting trees <ref> [13] </ref>, the following type of questions are of critical importance. <p> Furthermore, it was observed by [19, 21] that too many concurrent processors can also cause performance degradation. This brings us to the questions most often asked by practitioners implementing diffracting trees <ref> [13] </ref>.
Reference: [14] <author> V.F. Kolchin, B.A. Senast'yanov, </author> <title> and V.P. Chistyakov. Random Allocation. </title> <publisher> V.H. Winston & Sons, </publisher> <address> Washington D.C. </address> <year> 1978. </year>
Reference: [15] <author> T.G. Kurtz. </author> <title> Approximation of Population Processes. </title> <booktitle> CBMS-NSF Reginal Conf. Series in Applied Math. </booktitle> <publisher> SIAM, </publisher> <year> 1981. </year>
Reference-contexts: To approximate the performance of the discrete time Markov chain we study a related continuous time, density dependent jump Markov chain (see <ref> [15, Chapers 7-8] </ref> or [10, Chapter 11] for detailed discussion of density dependent jump Markov processes and the convergence theorem we use here). Processors in the continuous Markov process execute the same steps as in the discrete process. <p> The solution of the above system gives: m j = ff 2 + 2ff + 1 The density dependent jump Markov process satisfies the conditions of Kurtz's convergence theorem (see Theorem 8.1 in <ref> [15] </ref>). Thus, as L; d ! 1 the behavior of the Markov process converges to that of the deterministic process. We can now use the above analysis to characterize the performance of the diffracting tree.
Reference: [16] <author> J.M. Mellor-Crummey and M.L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <type> Technical Report 342, </type> <institution> University of Rochester, Rochester, </institution> <address> NY 14627, </address> <month> April </month> <year> 1990. </year>
Reference: [17] <author> S.K. Park and K.W. Miller. </author> <title> Random number generators: Good ones are hard to find. </title> <journal> CACM, </journal> <month> 31(10),October </month> <year> 1988. </year>
Reference-contexts: The random number function we used was Proteus' fast random () which is an implementation of the ACM Minimal Standard Random Number Genera 20 tor <ref> [17, 8] </ref>. We now show how our combinatorial model ties together the choice of diffracting tree parameters depth, d, and prism locations per level, L, to the number of processor, P .
Reference: [18] <author> L. Rudolph, </author> <title> Decentralized cache scheme for an MIMD parallel processor. </title> <booktitle> In 11th Annual Computing Architecture Conference, </booktitle> <year> 1983, </year> <pages> pp. 340-347. 25 </pages>
Reference-contexts: [0; n 1]. * SWAP (a,x) Atomically writes x to address a, and returns the previous value there. * C&S (a,p,n) Atomically compares the value at address a to p, if they match, writes n to a and returns TRUE, otherwise returns FALSE. * T&T&S (l) Performs a Test&Test&Set operation <ref> [18] </ref> on the lock, l, re turns TRUE if the lock was captured. The counters at the tree's leaves are implemented using a hardware F&I operation.
Reference: [19] <author> N. Shavit and A. Zemach. </author> <title> Diffracting Trees. </title> <booktitle> In Proceedings of the 6th ACM Annual Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pp. 167-174, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Diffracting trees <ref> [19] </ref> are among the most effective and scalable distributed-parallel techniques for shared counting, with a variety of applications to load balancing and concurrent data structure design. Diffracting trees are a special form of the counting networks of Aspnes, Herlihy, and Shavit [4]. They are fl MIT and Tel-Aviv University. <p> Our model is simple and sufficiently high level to overcome many implementation specific details, and yet as we will show it is rich enough to accurately predict empirically observed behaviors. As a result of our analysis we were able to identify starvation problems in the algorithm of <ref> [19] </ref> and thus introduce a more stable diffracting balancer algorithm (see section 2.3). We were also able to identify the range (as a function of P , work, d and L) in which the diffracting tree performs most efficiently, and the ranges in which its performance degrades. <p> In Section 4 we present empirical evidence collected through benchmarks on a simulated shared memory machine to support the analysis and Section 5 concludes this paper and lists areas of further research. 2 Counting Trees and Diffraction Diffracting trees <ref> [19] </ref> are counting trees, a special form of the counting network data structures introduced by Aspnes, Herlihy and Shavit [5]. They are binary trees of nodes called balancers. A balancer is a computing element with one input wire and two output wires. <p> The data structures of a diffracting tree of width 4 are depicted graphically in Figure 4. used in <ref> [19] </ref>. * MYID The ID of the processor executing the code. * random (n) Returns an integer number in the range [0; n 1]. * SWAP (a,x) Atomically writes x to address a, and returns the previous value there. * C&S (a,p,n) Atomically compares the value at address a to p, <p> However, there is a large range of concurrency levels where there are moderate numbers of processors, and yet it is far from clear what level of diffraction is achieved. Furthermore, it was observed by <ref> [19, 21] </ref> that too many concurrent processors can also cause performance degradation. This brings us to the questions most often asked by practitioners implementing diffracting trees [13]. <p> The method suggested in <ref> [19] </ref> to overcome starvation was to allow processors waiting for the toggle bit to rewrite their IDs to prism so that later arriving processors might diffract them (this is equivalent to adding the code node.prism [rand place]=MYID between lines 26 and 27 of Figure 5). <p> In the next section we prove that the new algorithm, when run with the optimal tree of depth d and the optimal prism width L/2 i , does not suffer from this starvation phenomenon. (lines 14-16 and 25-27), a performance enhancement technique that was used both in <ref> [19] </ref> and here. The spin variable serves both as a delay in which a processor may be diffracted and as a method to exponentially back-off from the toggle bit. Spin time is doubled when a processor is diffracted and halved if it captures the lock on the toggle bit. <p> The depth 5 tree also shows how latency increases again as concurrency increases. Note that the calibration of our graphs, and hence the phenomena we are modeling, are very fine relative to the changes in latency for other types of data structures. For example, in <ref> [19] </ref>, combining trees [11] are shown to have a latency increase by 2500 units over the tested concurrency range, and so the 300 unit change in latency of diffracting trees would be considered almost constant. See [19] for details. in a level of the diffracting tree while keeping the other parameters <p> For example, in <ref> [19] </ref>, combining trees [11] are shown to have a latency increase by 2500 units over the tested concurrency range, and so the 300 unit change in latency of diffracting trees would be considered almost constant. See [19] for details. in a level of the diffracting tree while keeping the other parameters constant. Here we used a tree of depth 3 and 64 processors, with almost no work. The number of counters in the tree does not change, it remains 2 d = 8.
Reference: [20] <author> N. Shavit and A. Zemach. </author> <title> Diffracting Trees. </title> <journal> In ACM Transactions on Computer Systems, </journal> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: If no collision occurs within time spin, P toggles the shared bit and leaves the balancer accordingly. 2.1 The Original Diffracting Tree Implementation Zemach <ref> [20] </ref>. Each balancer record consists of a toggle bit (with accompanying 6 type balancer is size: integer spin: integer prism: array [1..size] of integer lock: boolean toggle: boolean next: array [0..1] of ptr to balancer endtype location: global array [1..NUMPROCS] of ptr to balancer lock) and a prism array. <p> MAXSPIN is a system dependent constant which defines the maximum amount of time a processor might spin. The method used in <ref> [20] </ref> to prove correctness is based on analysis of the different values taken by the elements of the location array during the execution of the algorithm. Those methods carry over to the new algorithm with only slight modifications. We will show this for the most important lemma of [20], the rest <p> used in <ref> [20] </ref> to prove correctness is based on analysis of the different values taken by the elements of the location array during the execution of the algorithm. Those methods carry over to the new algorithm with only slight modifications. We will show this for the most important lemma of [20], the rest of the proof can be deduced in a similar manner. <p> Lemma 2.1 Given processors q 6= r, if q performs C&S q (location [r]; b; EMPTY) = true , then the token currently shepherded by r through b is a canceled token. Proof: Lemmas 5.8 and 5.10 of <ref> [20] </ref> prove that if q's C&S operation, on r's element of location was successful, then r was in fact shepherding a token though b at that same time. This token is performing operations in the code somewhere between lines 4 and 34.
Reference: [21] <author> N. Shavit, and D. </author> <title> Touitou. </title> <booktitle> Elimination Trees and the Construction of Pools and Stacks In Proceedings of the 7th Annual Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 54-63, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: However, there is a large range of concurrency levels where there are moderate numbers of processors, and yet it is far from clear what level of diffraction is achieved. Furthermore, it was observed by <ref> [19, 21] </ref> that too many concurrent processors can also cause performance degradation. This brings us to the questions most often asked by practitioners implementing diffracting trees [13]. <p> The solution was to add a second layer prism between the first layer and the toggle bit, a method which empirically exhibits more stability at the price of slightly increased latency <ref> [21] </ref>. The combinatorial model of the next section shows that this form of starvation is an inherent phenomenon in the old code due to the fact that processors that do not diffract can leave the balancer only by toggling the shared bit, that is, by passing through a sequential bottleneck. <p> Our analysis shows that in sufficiently 10 long runs one will reach a permanent global state in which processors are piled up at the toggle bits. This would also be true of the method of <ref> [21] </ref> unless many levels of prisms a re used, resulting in poor latency. The improved algorithm presented in this article solves this problem by allowing processors to repeatedly return to attempt diffractions on the prism after failing to acquire the toggle bit. <p> The improved algorithm presented in this article solves this problem by allowing processors to repeatedly return to attempt diffractions on the prism after failing to acquire the toggle bit. It is a dynamic form of the method used by <ref> [21] </ref>, but does not suffer from the same latency increase since it always uses the "right" number of prisms. Figure 6 shows the new algorithm. <p> We strongly believe our model and modeling approach pave the way to steady-state combinatorial analysis of other distributed-parallel data structures such as counting networks and other diffracting tree based data structures such as elimination trees <ref> [21] </ref>, pools [21], priority queues, and so on. <p> We strongly believe our model and modeling approach pave the way to steady-state combinatorial analysis of other distributed-parallel data structures such as counting networks and other diffracting tree based data structures such as elimination trees <ref> [21] </ref>, pools [21], priority queues, and so on.
Reference: [22] <author> D. </author> <title> Touitou - Tel-Aviv University. </title> <type> Personal communication, </type> <month> October </month> <year> 1994. </year> <month> 26 </month>
Reference-contexts: It is these and similar questions that our work attempts to address. 2.3 The New Algorithm We begin by modifying the diffracting tree algorithm presented in Section 2.1. Touitou <ref> [22] </ref> reports the following when running a benchmark on the prototype MIT Alewife machine [2]. In his benchmark, processors repeatedly attempt to increment a diffracting tree based counter until some fixed number of increments has been performed.
References-found: 22

