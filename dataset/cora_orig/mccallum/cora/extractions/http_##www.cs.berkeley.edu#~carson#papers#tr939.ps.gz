URL: http://www.cs.berkeley.edu/~carson/papers/tr939.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~carson/papers/tr939.html
Root-URL: 
Email: fsjb,carson,hayit,malikg@cs.berkeley.edu  
Title: Recognition of Images in Large Databases Using a Learning Framework  
Author: Serge Belongie, Chad Carson, Hayit Greenspan, and Jitendra Malik 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division, University of California at Berkeley  
Abstract: Retrieving images from very large collections using image content as a key is becoming an important problem. Classifying images into visual categories and finding objects in image databases are two major challenges in the field. This paper describes our approach toward the first of the two tasks, the generalization of which we believe will assist in the second task as well. We define a blobworld representation which provides a transition from the raw pixel data to a small set of localized coherent regions in color and texture space. Learning is then utilized to extract a probabilistic interpretation of the scene. Experimental results are presented for more than 1000 images from the Corel photo collection. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> J. Ashley et al. </editor> <title> Automatic and semiautomatic methods for image annotation and retrieval in QBIC. </title> <booktitle> In SPIE Proc. Storage and Retrieval for Still Image and Video Databases III, </booktitle> <year> 1995. </year>
Reference-contexts: The system then displays a selection of potential matches to those criteria, sorted by a score of the appropriateness of the match. Region segmentation is largely manual, but the most recent versions of QBIC <ref> [1] </ref> contain simple automated segmentation facilities. Photobook [19] largely shares QBIC's model of an image as a collage of flat, homogeneous frontally presented regions, but incorporates more sophisticated representations of texture and a degree of automatic segmentation.
Reference: [2] <author> J. Bach et al. </author> <title> The Virage image search engine: An open framework for image management. </title> <booktitle> In SPIE Proc. Storage and Retrieval for Still Image and Video Databases IV, </booktitle> <year> 1996. </year>
Reference-contexts: Photobook [19] largely shares QBIC's model of an image as a collage of flat, homogeneous frontally presented regions, but incorporates more sophisticated representations of texture and a degree of automatic segmentation. Further examples of systems that identify materials using low-level image properties include Virage <ref> [2] </ref>, Candid [13], and Chabot [17]. None of these systems codes spatial organization in a way that supports object queries.
Reference: [3] <author> J. Bigun. </author> <title> Local symmetry features in image processing. </title> <type> PhD thesis, </type> <institution> Linkoping University, </institution> <year> 1988. </year>
Reference-contexts: Consider a fixed scale and pixel location, let 1 and 2 denote the eigenvalues of M at that location ( 1 2 ), and let denote the argument of the principal eigenvector. The relation between the eigenstructure of M and the local image structure it describes is well known <ref> [3, 7] </ref>. In particular, when 1 is large compared to 2 , the local neighborhood possesses a dominant orientation (as specified by ), and can be characterized as 1D-textured. When the eigenvalues are comparable, there is no preferred orientation. <p> In other words, the hue is set to be twice the orientation angle, the saturation is assigned the value of the anisotropy, and the value (or brightness) is associated with the texture contrast. (A similar color coding was suggested in <ref> [3] </ref>.) In this manner, the line of grays associated with the zero-saturation axis of the HSV-cone corresponds to 2D textures of varying contrast. Textures possessing a preferred orientation correspond to saturated, colorful regions in color space.
Reference: [4] <author> T. Binford. </author> <title> Survey of model-based image analysis systems. </title> <journal> Int. J. Rob. Res., </journal> <pages> pages 18-64, </pages> <year> 1982. </year>
Reference-contexts: As discussed in a review of model-based vision in the late 70s and early 80s <ref> [4] </ref>, a number of researchers were addressing the task of scene labeling using hand-crafted image models. For example, the system developed by Ohta [18] seeks to label image regions as one of four object classes: sky, trees, buildings (with windows) and roads (with cars).
Reference: [5] <author> C. Carson and V. Ogle. </author> <title> Storage and retrieval of feature data for a very large online image collection. </title> <journal> IEEE Data Engineering Bulletin, </journal> <note> to appear. </note>
Reference-contexts: For simplicity we have assumed that the blobs which support a given class are mutually independent. The above scheme represents a nave Bayes classifier [23]. 6. Experiments 6.1. The image collection Our image collection consists of 28,000 of the Corel images online in a database <ref> [5] </ref>. The images are arranged in sets of 100, each with a category name; some of these categories are visually meaningful (e.g., Cheetahs, Leopards, and Jaguars), while others are not (e.g., Tour through Eu-rope).
Reference: [6] <author> A. Dempster, N. Laird, and D. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Soc., Ser. B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: The initial color labels of each pixel are subsequently dropped, and the i 's and S i 's are used to initialize a parameter search for a mixture of Gaussians using the Expectation Maximization (EM) algorithm <ref> [6, 22] </ref>. An iterative procedure forms the basis of the algorithm: the Expectation or E-step 4 image. (b) The color channels; channels that initialize EM are indicated. (c) Support maps for the EM results; each gray level represents a different connected component. (d) The texture channels.
Reference: [7] <author> W. Forstner. </author> <title> A framework for low level feature extraction. </title> <booktitle> In Proc. European Conf. Comp. Vis., </booktitle> <year> 1994. </year>
Reference-contexts: The texture space Texture is a well-researched property of image regions, and many texture descriptors have been proposed, including multi-orientation filter banks [14, 9] and the second-moment matrix <ref> [7, 8] </ref>. We will not elaborate here on the classical approaches to texture segmentation and classification, both of which are challenging and well-studied tasks. Rather, we introduce a new perspective related to texture descriptors and texture grouping motivated by the content-based retrieval task. <p> is a 9 fi 9 separable binomial approximation to a Gaussian smoothing kernel with variance 2. (Note that at each pixel location, M (x; y) is a 2 fi 2 symmetric positive semidefinite matrix.) The variance of G has been called the integration scale or artificial scale by various authors <ref> [7, 8] </ref> to distinguish it from the scale parameter used in linear smoothing of raw image intensities. Consider a fixed scale and pixel location, let 1 and 2 denote the eigenvalues of M at that location ( 1 2 ), and let denote the argument of the principal eigenvector. <p> Consider a fixed scale and pixel location, let 1 and 2 denote the eigenvalues of M at that location ( 1 2 ), and let denote the argument of the principal eigenvector. The relation between the eigenstructure of M and the local image structure it describes is well known <ref> [3, 7] </ref>. In particular, when 1 is large compared to 2 , the local neighborhood possesses a dominant orientation (as specified by ), and can be characterized as 1D-textured. When the eigenvalues are comparable, there is no preferred orientation.
Reference: [8] <author> J. Garding and T. Lindeberg. </author> <title> Direct computation of shape cues using scale-adapted spatial derivative operators. </title> <journal> Int. J. of Comp. Vis., </journal> <volume> 17, </volume> <month> Feb </month> <year> 1996. </year>
Reference-contexts: The texture space Texture is a well-researched property of image regions, and many texture descriptors have been proposed, including multi-orientation filter banks [14, 9] and the second-moment matrix <ref> [7, 8] </ref>. We will not elaborate here on the classical approaches to texture segmentation and classification, both of which are challenging and well-studied tasks. Rather, we introduce a new perspective related to texture descriptors and texture grouping motivated by the content-based retrieval task. <p> is a 9 fi 9 separable binomial approximation to a Gaussian smoothing kernel with variance 2. (Note that at each pixel location, M (x; y) is a 2 fi 2 symmetric positive semidefinite matrix.) The variance of G has been called the integration scale or artificial scale by various authors <ref> [7, 8] </ref> to distinguish it from the scale parameter used in linear smoothing of raw image intensities. Consider a fixed scale and pixel location, let 1 and 2 denote the eigenvalues of M at that location ( 1 2 ), and let denote the argument of the principal eigenvector. <p> These expressions are used to define texture categories: non-textured (featureless), 2D texture, and 1D texture. The latter is subdivided into 4 possible orientations: horizontally oriented texture, vertically oriented texture, and two diagonally oriented textures. 2 These are related to derived quantities reported in <ref> [8] </ref>. Visualizing texture data: the texture cone Since M (x; y) possesses three values of interest (for a fixed scale) at each pixel, it is not immediately obvious how it should be visualized.
Reference: [9] <author> H. Greenspan et al. </author> <title> Learning texture discrimination rules in a multiresolution system. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), </journal> <volume> 16(9) </volume> <pages> 894-901, </pages> <year> 1994. </year>
Reference-contexts: However, this scheme ignores the fact that for moderate and large values and saturations, hue is more perceptually relevant than saturation and value. 3.2. The texture space Texture is a well-researched property of image regions, and many texture descriptors have been proposed, including multi-orientation filter banks <ref> [14, 9] </ref> and the second-moment matrix [7, 8]. We will not elaborate here on the classical approaches to texture segmentation and classification, both of which are challenging and well-studied tasks. Rather, we introduce a new perspective related to texture descriptors and texture grouping motivated by the content-based retrieval task.
Reference: [10] <author> C. Jacobs, A. Finkelstein, and D. Salesin. </author> <title> Fast multires-olution image querying. </title> <booktitle> In Proc. SIGGRAPH-95, </booktitle> <pages> pages 277-285, </pages> <year> 1995. </year>
Reference-contexts: Object-oriented queries search for images that contain particular objects; such queries can be seen either as constructs on material queries [20], as essentially textual matters [21], or as the proper domain of object recognition. A third query mode looks for images that are iconic matches of a given image <ref> [10] </ref>.
Reference: [11] <author> A. Jain. </author> <title> Fundamentals of digital image processing. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: A hue-based space such as HSV is superior to RGB in these respects <ref> [11] </ref>.
Reference: [12] <author> K. Kelly and D. Judd. </author> <title> Color: Universal Language and Dictionary of Names. </title> <type> U.S. </type> <institution> National Bureau of Standards, </institution> <month> Dec </month> <year> 1976. </year> <title> Spec. </title> <journal> Publ. </journal> <volume> 440. </volume>
Reference-contexts: Our color processing is based on partitioning the color space into perceptually meaningful channels in order to aid grouping and recognition. The perceptual channels we use loosely follow the color naming system of the Inter-Society Color Council and National Bureau of Standards <ref> [12] </ref>, which uses six levels of detail to designate colors. These levels range from broad perceptual color names such as red, blue, and gray (13 colors) to about five million color designations defined by spectrophotometric measurements [26]. Only the first three levels correspond to human color names.
Reference: [13] <author> P. Kelly, M. Cannon, and D. Hush. </author> <title> Query by image example: the comparison algorithm for navigating digital image databases (CANDID) approach. </title> <booktitle> In SPIE Proc. Storage and Retrieval for Image and Video Databases III, </booktitle> <pages> pages 238-249, </pages> <year> 1995. </year>
Reference-contexts: Photobook [19] largely shares QBIC's model of an image as a collage of flat, homogeneous frontally presented regions, but incorporates more sophisticated representations of texture and a degree of automatic segmentation. Further examples of systems that identify materials using low-level image properties include Virage [2], Candid <ref> [13] </ref>, and Chabot [17]. None of these systems codes spatial organization in a way that supports object queries. Variations on Photobook [15] use a form of supervised learning known in the information retrieval community as relevance feedback to adjust segmentation and classification parameters for various forms of textured region.
Reference: [14] <author> J. Malik and P. Perona. </author> <title> Preattentive texture discrimination with early vision mechanisms. </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> 7(5) </volume> <pages> 923-932, </pages> <year> 1990. </year>
Reference-contexts: However, this scheme ignores the fact that for moderate and large values and saturations, hue is more perceptually relevant than saturation and value. 3.2. The texture space Texture is a well-researched property of image regions, and many texture descriptors have been proposed, including multi-orientation filter banks <ref> [14, 9] </ref> and the second-moment matrix [7, 8]. We will not elaborate here on the classical approaches to texture segmentation and classification, both of which are challenging and well-studied tasks. Rather, we introduce a new perspective related to texture descriptors and texture grouping motivated by the content-based retrieval task.
Reference: [15] <author> T. Minka. </author> <title> An image database browser that learns from user interaction. </title> <type> Technical Report 365, </type> <institution> MIT Media Lab, </institution> <year> 1995. </year>
Reference-contexts: Further examples of systems that identify materials using low-level image properties include Virage [2], Candid [13], and Chabot [17]. None of these systems codes spatial organization in a way that supports object queries. Variations on Photobook <ref> [15] </ref> use a form of supervised learning known in the information retrieval community as relevance feedback to adjust segmentation and classification parameters for various forms of textured region. When a user is available to tune queries, supervised learning algorithms can clearly improve performance, given appropriate object and image representations.
Reference: [16] <author> W. Niblack et al. </author> <title> The QBIC project: querying images by content using colour, texture and shape. </title> <note> In IS and T/SPIE 1993 Int. Symp. </note> <institution> Electr. Imaging: Science and Technology, </institution> <note> Conf. </note> <year> 1908, </year> <title> Storage and Retrieval for Image and Video Databases, </title> <year> 1993. </year>
Reference-contexts: As one might expect, the rules in this system are very image dependent and burdensome to itemize. Ohta's work nonetheless represents a significant step toward understanding scenes using properties of color and texture. The best-known image database system is QBIC <ref> [16] </ref>, which allows an operator to specify various properties of a desired image. The system then displays a selection of potential matches to those criteria, sorted by a score of the appropriateness of the match. <p> The color space Color is an important cue in extracting information from images. Color histograms provide a global image color characterization and are commonly used in content-based retrieval systems <ref> [16, 17, 24] </ref>. They have proven to be very useful. Still, the global characterization is poor at, for example, distinguishing between a field of flowers and a single large flower, because it lacks information about how the color is distributed spatially.
Reference: [17] <author> V. Ogle and M. Stonebraker. Chabot: </author> <title> Retrieval from a relational database of images. </title> <journal> IEEE Computer, </journal> <volume> 28(9), </volume> <month> Sep </month> <year> 1995. </year>
Reference-contexts: Photobook [19] largely shares QBIC's model of an image as a collage of flat, homogeneous frontally presented regions, but incorporates more sophisticated representations of texture and a degree of automatic segmentation. Further examples of systems that identify materials using low-level image properties include Virage [2], Candid [13], and Chabot <ref> [17] </ref>. None of these systems codes spatial organization in a way that supports object queries. Variations on Photobook [15] use a form of supervised learning known in the information retrieval community as relevance feedback to adjust segmentation and classification parameters for various forms of textured region. <p> The color space Color is an important cue in extracting information from images. Color histograms provide a global image color characterization and are commonly used in content-based retrieval systems <ref> [16, 17, 24] </ref>. They have proven to be very useful. Still, the global characterization is poor at, for example, distinguishing between a field of flowers and a single large flower, because it lacks information about how the color is distributed spatially.
Reference: [18] <author> Y. Ohta. </author> <title> A region-oriented image-analysis system by computer. </title> <type> PhD thesis, </type> <institution> Kyoto University, </institution> <year> 1980. </year>
Reference-contexts: As discussed in a review of model-based vision in the late 70s and early 80s [4], a number of researchers were addressing the task of scene labeling using hand-crafted image models. For example, the system developed by Ohta <ref> [18] </ref> seeks to label image regions as one of four object classes: sky, trees, buildings (with windows) and roads (with cars).
Reference: [19] <author> A. Pentland, R. Picard, and S. Sclaroff. Photobook: </author> <title> content-based manipulation of image databases. </title> <type> Technical Report 255, </type> <institution> MIT Media Lab Perceptual Computing, </institution> <year> 1993. </year>
Reference-contexts: The system then displays a selection of potential matches to those criteria, sorted by a score of the appropriateness of the match. Region segmentation is largely manual, but the most recent versions of QBIC [1] contain simple automated segmentation facilities. Photobook <ref> [19] </ref> largely shares QBIC's model of an image as a collage of flat, homogeneous frontally presented regions, but incorporates more sophisticated representations of texture and a degree of automatic segmentation. Further examples of systems that identify materials using low-level image properties include Virage [2], Candid [13], and Chabot [17].
Reference: [20] <author> R. Picard and T. Minka. </author> <title> Vision texture for annotation. </title> <journal> J. Multimedia Sys., </journal> <volume> 3 </volume> <pages> 3-14, </pages> <year> 1995. </year>
Reference-contexts: Background Existing content-based retrieval systems can handle queries about specific colors and textures but are far from achieving the higher-level content extraction that users prefer. Object-oriented queries search for images that contain particular objects; such queries can be seen either as constructs on material queries <ref> [20] </ref>, as essentially textual matters [21], or as the proper domain of object recognition. A third query mode looks for images that are iconic matches of a given image [10].
Reference: [21] <author> R. Price, T.-S. Chua, and S. Al-Hawamdeh. </author> <title> Applying relevance feedback to a photo-archival system. </title> <journal> J. Information Sci., </journal> <volume> 18 </volume> <pages> 203-215, </pages> <year> 1992. </year>
Reference-contexts: Object-oriented queries search for images that contain particular objects; such queries can be seen either as constructs on material queries [20], as essentially textual matters <ref> [21] </ref>, or as the proper domain of object recognition. A third query mode looks for images that are iconic matches of a given image [10].
Reference: [22] <author> R. Redner and H. Walker. </author> <title> Mixture densities, maximum-likelihood estimation and the EM algorithm (review). </title> <journal> SIAM Review, </journal> <volume> 26(2) </volume> <pages> 195-237, </pages> <year> 1984. </year>
Reference-contexts: The initial color labels of each pixel are subsequently dropped, and the i 's and S i 's are used to initialize a parameter search for a mixture of Gaussians using the Expectation Maximization (EM) algorithm <ref> [6, 22] </ref>. An iterative procedure forms the basis of the algorithm: the Expectation or E-step 4 image. (b) The color channels; channels that initialize EM are indicated. (c) Support maps for the EM results; each gray level represents a different connected component. (d) The texture channels.
Reference: [23] <author> B. Ripley. </author> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cam-bridge University Press, </publisher> <year> 1996. </year>
Reference-contexts: In these cases, we make the approximation of adding the blob probabilities Pr (blob k jC j ). For simplicity we have assumed that the blobs which support a given class are mutually independent. The above scheme represents a nave Bayes classifier <ref> [23] </ref>. 6. Experiments 6.1. The image collection Our image collection consists of 28,000 of the Corel images online in a database [5].
Reference: [24] <author> M. Swain and D. Ballard. </author> <title> Color indexing. </title> <journal> Int. J. Comp. Vis., </journal> <volume> 7(1) </volume> <pages> 11-32, </pages> <year> 1991. </year>
Reference-contexts: The color space Color is an important cue in extracting information from images. Color histograms provide a global image color characterization and are commonly used in content-based retrieval systems <ref> [16, 17, 24] </ref>. They have proven to be very useful. Still, the global characterization is poor at, for example, distinguishing between a field of flowers and a single large flower, because it lacks information about how the color is distributed spatially.
Reference: [25] <author> M. Swain and M. Stricker. </author> <title> The capacity and the sensitivity of color histogram indexing. </title> <type> Technical Report 94-05, </type> <institution> University of Chicago, </institution> <month> Mar </month> <year> 1994. </year>
Reference-contexts: Comparison with global color histograms In order to obtain a baseline for the retrieval results, we performed an experiment using color histogram comparisons <ref> [25] </ref>. We first found the color histogram for each image using the 13 perceptual color channels. (Using a low-dimensional histogram is necessary since there is so much variability among the images in each category.) We then found average histograms for each category using all combinations of the cross-validation partitions.
Reference: [26] <author> G. Wyszecki and W. Stiles. </author> <title> Color science: concepts and methods, quantitative data and formulae. </title> <publisher> Wiley, </publisher> <address> second edition, </address> <year> 1982. </year> <month> 8 </month>
Reference-contexts: These levels range from broad perceptual color names such as red, blue, and gray (13 colors) to about five million color designations defined by spectrophotometric measurements <ref> [26] </ref>. Only the first three levels correspond to human color names.
References-found: 26

