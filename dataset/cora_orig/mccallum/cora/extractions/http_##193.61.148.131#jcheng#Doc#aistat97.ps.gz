URL: http://193.61.148.131/jcheng/Doc/aistat97.ps.gz
Refering-URL: http://193.61.148.131/jcheng/bniordabs.htm
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: -j.cheng, da.bell, w.liu-@ulst.ac.uk  
Title: An Algorithm for Bayesian Belief Network Construction from Data  
Author: Jie Cheng, David A. Bell, Weiru Liu 
Address: Northern Ireland, UK, BT37 0QB  
Affiliation: School of Information and Software Engineering University of Ulster at Jordanstown  
Abstract: This paper presents an efficient algorithm for constructing Bayesian belief networks from databases. The algorithm takes a database and an attributes ordering (i.e., the causal attributes of an attribute should appear earlier in the order) as input and constructs a belief network structure as output. The construction process is based on the computation of mutual information of attribute pairs. Given a data set which is large enough and has a DAG-Isomorphic probability distribution, this algorithm guarantees that the perfect map [1] of the underlying dependency tests. To evaluate this algorithm, we present the experimental results on three versions of the well-known ALARM network database, which has 37 attributes and 10,000 records. The correctness proof and the analysis of computational complexity are also presented. We also discuss the features of our work and relate it to previous works. model is generated, and at the same time, enjoys the time complexity of O N( ) 2 on conditional independence (CI)
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Pearl, J. </author> , <title> Probabilistic reasoning in intelligent systems: networks of plausible inference, </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: algorithms has less time complexity in the worst case (when the underlying DAG is densely connected), but it may not find the best solution due to its heuristic nature; The second category of algorithms is usually asymptotically correct when the probability distribution of data is DAG-Isomorphic (The definition is in <ref> [1] </ref>), but as Cooper et al. pointed out in [5], CI tests with large conditionsets may be unreliable unless the volume of data is enormous. On developing this algorithm, we take the following two facts into consideration. <p> It uses DAG to represent dependency relationships between variables. Since every independence statement in belief networks satisfies a group of axioms (See <ref> [1] </ref> for details), we can construct belief networks from data by analyzing conditional independence relationships. This CI test based method is used by all the algorithms of the second category described in Section 1. To introduce our approach, we first review the concept of d-separation [1], which plays an important role <p> a group of axioms (See <ref> [1] </ref> for details), we can construct belief networks from data by analyzing conditional independence relationships. This CI test based method is used by all the algorithms of the second category described in Section 1. To introduce our approach, we first review the concept of d-separation [1], which plays an important role in our algorithm. For any three disjoint node sets X, Y, and Z in a belief network, X is said to be d-separated from Y by Z if there is no active undirected path between X and Y. <p> In the second phase, the algorithm adds arcs when the pairs of nodes cannot be d-separated. The result of Phase II is an independence map (I-map) <ref> [1] </ref> of the underlying dependency model. In the third phase, each arc of the I-map is examined using CI tests and will be removed if the two nodes of the arc can be d-separated. The result of Phase III is the minimal I-map [1]. 3.1 The Algorithm Phase I: (Drafting) 1. <p> Phase II is an independence map (I-map) <ref> [1] </ref> of the underlying dependency model. In the third phase, each arc of the I-map is examined using CI tests and will be removed if the two nodes of the arc can be d-separated. The result of Phase III is the minimal I-map [1]. 3.1 The Algorithm Phase I: (Drafting) 1. Initiate a graph G V E ( , ) where V=-all the nodes of a data set, E=- -. Initiate two empty ordered set S, R. 2. <p> Q.E.D. Proposition 2 Graph G3 generated after Phase III is a perfect map of M. Proof: Since an arc is removed in Phase III only if the pair of nodes are d-separated, G3 is an I-map of M. Next, we shall prove that this also a Dependent-map (D-map) <ref> [1] </ref>. Suppose G3 is not a D-map, then there must exist an arc ( a, b) which is in G3 and the two nodes a and b are actually independent in the underlying model M.
Reference: [2] <author> Neapolitan, R.E. </author> , <title> Probabilistic reasoning in expert systems: theory and algorithms, </title> <publisher> John Wiley & Sons, </publisher> <year> 1990. </year>
Reference: [3] <author> Spirtes, P., Glymour, C. and Scheines, R., </author> <title> Causation, Prediction, and Search (Book), </title> <address> http://hss.cmu.edu/html/departments/philosophy/TETRAD.BOOK/book.html,1996. </address>
Reference-contexts: To understand d-Separation, we can use an analogy, which is similar to the one suggested in <ref> [3] </ref>. We view a belief network as a network system of information channels, where each node is a valve that is either active or inactive and the valves are connected by noisy information channels. The information flow can pass through an active valve but not an inactive one. <p> Repeat step 4 until S is empty. In order to illustrate this algorithms working mechanism, we use a simple multi-connected network example borrowed from <ref> [3] </ref>. Suppose we have a database that has underlying Bayesian network as Figure 1.a; and we also have a nodes order as A, B, C, D, E. Our task is to find out the exact network structure.
Reference: [4] <author> Spirtes, P., Glymour, C. and Scheines, R., </author> <title> An algorithm for fast recovery of sparse causal graphs, </title> <journal> Social Science Computer Review, </journal> <volume> 9, </volume> <pages> 62-72, </pages> <year> 1991. </year>
Reference-contexts: 2 2 in the worst case. 5 Discussion Some of the belief network construction algorithms require nodes ordering, such as the algorithms presented in [5,10,18,20,21] and the proposed algorithm in this paper; Others do not require nodes ordering and can orient the edges automatically, such as the algorithms presented in <ref> [4, 12] </ref>. The former group of algorithms can be viewed as a special case of the latter group of algorithms where the nodes ordering is known. Based on the same ideas presented in this paper, we also developed a correct algorithm that does not require nodes ordering [22].
Reference: [5] <author> Cooper, G.F., Herskovits, E., </author> <title> A Bayesian Method for the induction of probabilistic networks from data, </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: (when the underlying DAG is densely connected), but it may not find the best solution due to its heuristic nature; The second category of algorithms is usually asymptotically correct when the probability distribution of data is DAG-Isomorphic (The definition is in [1]), but as Cooper et al. pointed out in <ref> [5] </ref>, CI tests with large conditionsets may be unreliable unless the volume of data is enormous. On developing this algorithm, we take the following two facts into consideration. <p> We call them dataset1, dataset2 and dataset3 i . Each of them has 10,000 cases. Since this algorithm requires nodes ordering, we use the ordering described in the web page of Norsys Software Corp. for dataset1 and dataset2 and use the ordering described in <ref> [5] </ref> for dataset3. Please note that the actual orderings make no difference to the algorithm as long as they preserve the cause and effect relationships.
Reference: [6] <author> Heckerman, D., Geiger, D. and Chickering, </author> <title> D.M., Learning Bayesian networks: the combination of knowledge and statistical data, </title> <type> Technical Report MSR-TR-94-09, </type> <institution> Microsoft Research, </institution> <year> 1994. </year>
Reference-contexts: When the underlying graph is sparse, Phase I can construct a graph very close to the original one. In fact, if the underlying graph is a singly connected graph (a graph without undirected cycle), Phase I of this algorithm is essentially the algorithm of Chow and Liu <ref> [6] </ref>, and it guarantees the constructed network is the same as the original one. Our algorithm can be viewed as an extension of Chow and Lius algorithm to multi-connect networks. In this example, (B,E) is wrongly added and (D,E) is missing because of the existing open path (D-B-E) and (D-B-C-E).
Reference: [7] <author> Chow, C.K. and Liu, C.N., </author> <title> Approximating discrete probability distributions with dependence trees. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 14, </volume> <pages> 462-467, </pages> <year> 1968. </year>
Reference-contexts: On dataset3, it has one missing arc (excluding 12-32, 21-31 for the reason given in Section 4) and five not oriented arcs. Both of our algorithms can be viewed as extension of the algorithm of <ref> [7] </ref> to multi-connected networks. One merit of the proposed algorithm is that it preserves the O N ( ) 2 complexity on CI tests. Algorithms described in [4,8,10] can also construct a belief network whose structure is a minimal I-map of the underlying dependency model.
Reference: [8] <author> Wermuth, N. and Lauritzen, S., </author> <title> Graphical and recursive models for contingency tables. </title> <journal> Biometrika, </journal> <volume> 72, </volume> <pages> 537-552, </pages> <year> 1983. </year>
Reference: [9] <author> Fung, R.M. and Crawford, </author> <title> S.L., Constructor: a system for the induction of probabilistic models. </title> <booktitle> Proceedings of AAAI (pp. </booktitle> <pages> 762-769), </pages> <address> Boston, MA: </address> <publisher> MIT Press. </publisher>
Reference: [10] <author> Srinivas, S. Russell, S. and Agogino, A., </author> <title> Automated construction of sparse Bayesian networks from unstructured probabilistic models and domain information, </title> <editor> In Henrion, M., Shachter, R.D., Kanal, L.N. and Lemmer, J.F. (Eds.), </editor> <booktitle> Uncertainty in artificial intelligence 5, </booktitle> <address> Amsterdam: </address> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference: [11] <author> Buntine, W., </author> <title> A guide to the literature on learning probabilistic networks from data. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 8(2), </volume> <pages> 195-210, </pages> <year> 1996. </year>
Reference: [12] <author> Singh, M. and Valtorta, M. </author> <title> Construction of Bayesian network structures from data: a brief survey and an efficient algorithm, </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 12, </volume> <pages> 111-131, </pages> <year> 1995. </year>
Reference-contexts: 2 2 in the worst case. 5 Discussion Some of the belief network construction algorithms require nodes ordering, such as the algorithms presented in [5,10,18,20,21] and the proposed algorithm in this paper; Others do not require nodes ordering and can orient the edges automatically, such as the algorithms presented in <ref> [4, 12] </ref>. The former group of algorithms can be viewed as a special case of the latter group of algorithms where the nodes ordering is known. Based on the same ideas presented in this paper, we also developed a correct algorithm that does not require nodes ordering [22].
Reference: [13] <author> Wong, S.K.M. and Xiang, Y. </author> <title> Construction of a Markov network from data for probabilistic inference, </title> <booktitle> Proc. Third International Workshop on Rough Sets and Soft Computing, </booktitle> <address> San Jose, CA, 562-569, </address> <year> 1994. </year>
Reference: [14] <author> Sarkar, S. and Murthy, I. </author> <title> Constructing efficient belief network structures with expert provided information, </title> <journal> IEEE Transactions on knowledge and data engineering, </journal> <pages> 8-1, </pages> <year> 1996. </year>
Reference: [15] <author> Beinlich, I.A., Suermondt, H.J., Chavez, R.M. and Cooper, </author> <title> G.F., The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks. </title> <booktitle> Proceedings of the Second European Conference on Artificial Intelligence in Medicine (pp.247-256), </booktitle> <address> London, England, </address> <year> 1989. </year>
Reference-contexts: In the worst case, when all the CI tests require conditionsets on all the other nodes, the time complexity on basic operation is O N r N ( ) 2 . 4 Results on ALARM Network ALARM network <ref> [15] </ref> is a medical diagnostic alarm message system for patient monitoring, it contains 37 nodes and 46 arcs (see Figure 2). This belief network has become the de facto benchmark for evaluating belief network construction algorithms.
Reference: [16] <author> Bouckaert, </author> <title> R.R., Properties of Bayesian belief network learning algorithms. </title> <booktitle> In Proc. of tenth conference on uncertainty in artificial intelligence, </booktitle> <editor> Mantaras, R. and Poole, D. (Ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference: [17] <author> Madigan, D., Mosurski, K. and Almond R.G., </author> <title> Explanation in belief networks. </title> <type> Technical Report, </type> <institution> Department of Statistics, University of Washington, </institution> <year> 1994. </year>
Reference: [18] <author> Acid, S., and Campos, L.M., BENEDICT: </author> <title> An Algorithm for Learning Probabilistic Belief Networks, </title> <booktitle> Sixth International Conference IPMU'96, </booktitle> <year> 1996. </year>
Reference: [19] <author> Acid, S., and Campos, L.M., </author> <title> An Algorithm for Finding Minimum d-Separating Sets in Belief Networks. </title> <booktitle> Proceedings of UAI'96, </booktitle> <year> 1996. </year>
Reference-contexts: Arc (B,E) is removed because B and E are independent given - C,D-. This procedure generates the perfect I-map of the underlying dependency model. A B D C E C E A B D (a) (b) Finding Minimum Block Set As suggested by Acid et al. in <ref> [19] </ref>, knowing the minimum block set of two nodes in belief networks can be very useful in several ways. In our algorithm, we try to avoid CI tests with large conditionsets by finding minimum block sets. The following simple procedure uses a heuristicsearch method to find the block set. <p> In our algorithm, we try to avoid CI tests with large conditionsets by finding minimum block sets. The following simple procedure uses a heuristicsearch method to find the block set. An algorithm for finding minimum d-Separation sets can be found in <ref> [19] </ref>.
Reference: [20] <author> Suzuki, J., </author> <title> Learning Bayesian belief networks based on the MDL principle: An efficient algorithm using the branch and bound technique, </title> <booktitle> Proceedings of the international conference on machine learning, </booktitle> <address> Bally, Italy, </address> <year> 1996. </year>
Reference-contexts: This process continues until the score of the new model is not significantly better than the old one. Different scoring criteria have been applied in these algorithms, such as, Bayesian scoring method [5,6], entropy based method [21], and minimum description length method <ref> [20] </ref>. The other category of algorithms constructs Bayesian networks by analyzing dependency relationships among nodes. The dependency relationships are measured by using some kind of conditional independence (CI) test. The algorithms described in [4,8,9,10] and the proposed algorithm in this paper belong to this category.
Reference: [21] <author> Herskovits, </author> <title> E.H., Computer-based probabilistic network construction, </title> <type> Doctoral dissertation, </type> <institution> Medical information sciences, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1991. </year>
Reference-contexts: This process continues until the score of the new model is not significantly better than the old one. Different scoring criteria have been applied in these algorithms, such as, Bayesian scoring method [5,6], entropy based method <ref> [21] </ref>, and minimum description length method [20]. The other category of algorithms constructs Bayesian networks by analyzing dependency relationships among nodes. The dependency relationships are measured by using some kind of conditional independence (CI) test.
Reference: [22] <author> Cheng, J., Bell, DA and Liu W., </author> <title> Learning belief networks from data: an information theory based approach, forth coming. i Dataset1 has the underlying belief network described in the web page of Norsys Software Corp. Dataset2 has the underlying belief network used by David Heckerman. Dataset3 is generated by Gregory F. Cooper and Edward Herskovits. We generated dataset1 and dataset2 using a Monte Carlo technique. ii The CI tests are grouped by the cardinalities of conditionsets. </title>
Reference-contexts: The former group of algorithms can be viewed as a special case of the latter group of algorithms where the nodes ordering is known. Based on the same ideas presented in this paper, we also developed a correct algorithm that does not require nodes ordering <ref> [22] </ref>. Since dropping the requirement of nodes ordering means many more possibilities to be considered, the new algorithm requires CI tests O N ( ) 4 times. Of course, the complexity on basic operations is exponential. <p> Phase II and III, and also reduce the number of CI test needed in Phase III. (2) By using mutual information as a measure of dependency relationship, our algorithms can compare two relationships quantitatively, and therefore avoid exponential complexity in the case of no nodes ordering (the detail is in <ref> [22] </ref>.) Our subsequent research will focus on handling continuous variable nodes and missing values. We also plan to develop a commercial software based on our algorithms. Acknowledgments We wish to thank G. Cooper and E. Herskovits for providing their ALARM network data. We also thank D.
References-found: 22

