URL: ftp://ftp.cse.unsw.edu.au/pub/doc/papers/UNSW/9708.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/research/tr.html
Root-URL: http://www.cse.unsw.edu.au
Email: E-mail: fmbh,claudeg@cse.unsw.edu.au  E-mail: kim@rmb.com.au  
Title: Extracting Hidden Context  
Author: Michael Harries Claude Sammut Kim Horn 
Note: Submitted to the Machine Learning Journal for the special issue on Context-Sensitive Learning (1998).  
Date: November 1997  
Address: Sydney 2052, Australia.  Level 5 Underwood House, 37-47 Pitt Street, Sydney 2000, Australia.  Sydney 2052, Australia  
Affiliation: Department of Artificial Intelligence School of Computer Science and Engineering The University of New South Wales,  RMB Australia Limited,  THE UNIVERSITY OF NEW SOUTH WALES School of Computer Science and Engineering The University of New South Wales  
Pubnum: UNSW-CSE-TR-9708  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D.W. Aha, D. Kibler, and M.K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: Context can also be interpreted as the effect of one (contextual) attribute on the interpretation of another (context-sensitive) attribute. Classifier accuracy can be improved when context-sensitive attributes are present for both instance based learning <ref> [1] </ref> and multivariate regression by the methods of contextual normalisation, contextual expansion and contextual weighting [26, 27, 28]. Instance based learning could be used as the Splice-2R underlying learner, any hidden contexts thereby recognised by Splice-2R could then be utilised with these techniques for on-line prediction.
Reference: [2] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference-contexts: The ability of decision trees to capture context is associated with the fact that decision tree algorithms use a form of context-sensitive feature selection (CSFS) [4]. A number of machine learning algorithms can be regarded as using CSFS including decision tree algorithms [19], rule induction algorithms <ref> [2] </ref> and ILP systems [18]. All of these systems produce concepts containing local information about context. When contiguous intervals of time reflect a hidden attribute or context, we call time the environmental attribute.
Reference: [3] <author> Scott Clearwater, Tze-Pin Cheng, and Haym Hirsh. </author> <title> Incremental batch learning. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 366-370. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Many other on-line learners use an explicit window heuristic similar to that used in FLORA [11] [12] [13] [10]. Batch learners can also be used for on-line learning on domains with concept drift by repeatedly learning from a window of recent instances [7] <ref> [3] </ref>. The window update mechanism need not use a first in, first out, organisation, [22] discards older examples only when a new item appears in a similar region of attribute space. On-line learners for domains with hidden changes in context assume that context will tend to be contiguous over time.
Reference: [4] <author> Pedro Domingos. </author> <title> Context-sensitive feature selection for lazy learners. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11 </volume> <pages> 227-253, </pages> <year> 1997. </year> <note> Special issue on lazy learning, edited by David Aha. </note>
Reference-contexts: The ability of decision trees to capture context is associated with the fact that decision tree algorithms use a form of context-sensitive feature selection (CSFS) <ref> [4] </ref>. A number of machine learning algorithms can be regarded as using CSFS including decision tree algorithms [19], rule induction algorithms [2] and ILP systems [18]. All of these systems produce concepts containing local information about context.
Reference: [5] <author> Usama M. Fayyad and Keki B. Irani. </author> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1022-1027, </pages> <address> California, 1993. </address> <publisher> Morgan Kaufmann. </publisher> <address> REFERENCES 32 </address>
Reference-contexts: While the use of an existing CSFS machine learning system to provide partitioning is elegant, a better solution may be to implement a specialised method designed to deal with additional complexity on environmental attributes. One approach to this is to augment a decision tree algorithm to allow many splits <ref> [5] </ref> on selected attributes. Neither Splice-1 or Splice-2 provide a direct comparison of the relative advantage of dividing the domain into one set of contexts over another. One comparison method that could be used is the minimum description length (MDL) [21] heuristic.
Reference: [6] <author> Usama M. Fayyad, Gregory Piatsky-Shapiro, and Padhraic Smyth. </author> <title> From data mining to knowledge discovery: An overview. </title> <editor> In Usama M. Fayyad, Gregory Piatsky-Shapiro, Padhraic Smyth, and Ramasamy Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Any such attributes could then be incorporated with the current attribute set allowing a bootstrapping of the domain representation. This could be used within the Knowledge Discovery in Databases (KDD) approach <ref> [6] </ref> which includes the notion that analysts can reiterate the data selection and learning (data mining) tasks.
Reference: [7] <author> M. Harries and K. Horn. </author> <title> Detecting concept drift in financial time series prediction using symbolic machine learning. </title> <editor> In Xin Yao, editor, </editor> <booktitle> Eighth Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 91-98, </pages> <address> Singapore, 1995. </address> <publisher> World Scientific Publishing. </publisher>
Reference-contexts: Many other on-line learners use an explicit window heuristic similar to that used in FLORA [11] [12] [13] [10]. Batch learners can also be used for on-line learning on domains with concept drift by repeatedly learning from a window of recent instances <ref> [7] </ref> [3]. The window update mechanism need not use a first in, first out, organisation, [22] discards older examples only when a new item appears in a similar region of attribute space.
Reference: [8] <author> M. Harries and K. Horn. </author> <title> Learning stable concepts in domains with hidden changes in context. </title> <editor> In M. Kubat and G Widmer, editors, </editor> <booktitle> Learning in context-sensitive domains (Workshop Notes). 13th International Conference on Machine Learning, </booktitle> <address> Bari, Italy, </address> <year> 1996. </year>
Reference-contexts: The initial set of identified context changes can be refined by contextual clustering. Contextual clustering combines similar intervals of the dataset, where the similarity of two intervals is based upon the degree to which a partial model is accurate on both intervals. 2 SPLICE-1 6 2 Splice-1 Splice-1 <ref> [8] </ref> is designed to recognise stable context and extract local concepts from domains with hidden changes in context. Splice-1 can use any ordinal attribute as the environmental attribute, in order to preserve clarity in the following discussion we have substituted Time for the broader term environmental attribute.
Reference: [9] <author> A.J. Katz, M.T. Gately, and Collins D.R. </author> <title> Robust classifiers without robust features. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 472-479, </pages> <year> 1990. </year>
Reference-contexts: This method has been applied to the learning to fly domain [23] and to target recognition <ref> [9] </ref>. The application of local concepts for prediction and classification in this article used a similar model switching approach. The transfer of knowledge learnt in one context to a new, previously unseen, context is similar to an on-line adaption to a hidden change of context.
Reference: [10] <author> F. Kilander and C. G. Jansson. </author> <title> COBBIT a control procedure for COBWEB in the presence of concept drift. </title> <editor> In Pavel B. Brazdil, editor, </editor> <booktitle> European Conference on Machine Learning, </booktitle> <pages> pages 244-261, </pages> <address> Berlin, 1993. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The failed characterisation is removed from the search frontier and replaced by alternate characterisations. Discarding characterisations that are no longer effective provides a decay in the importance of older information. Many other on-line learners use an explicit window heuristic similar to that used in FLORA [11] [12] [13] <ref> [10] </ref>. Batch learners can also be used for on-line learning on domains with concept drift by repeatedly learning from a window of recent instances [7] [3].
Reference: [11] <author> M. </author> <title> Kubat. Floating approximation in time-varying knowledge bases. </title> <journal> Pattern Recognition Letters, </journal> <volume> 10 </volume> <pages> 223-227, </pages> <year> 1989. </year>
Reference-contexts: The failed characterisation is removed from the search frontier and replaced by alternate characterisations. Discarding characterisations that are no longer effective provides a decay in the importance of older information. Many other on-line learners use an explicit window heuristic similar to that used in FLORA <ref> [11] </ref> [12] [13] [10]. Batch learners can also be used for on-line learning on domains with concept drift by repeatedly learning from a window of recent instances [7] [3].
Reference: [12] <author> M. </author> <title> Kubat. A machine learning based approach to load balancing in computer networks. </title> <journal> Cybernetics and Systems Journal, </journal> <year> 1992. </year>
Reference-contexts: The failed characterisation is removed from the search frontier and replaced by alternate characterisations. Discarding characterisations that are no longer effective provides a decay in the importance of older information. Many other on-line learners use an explicit window heuristic similar to that used in FLORA [11] <ref> [12] </ref> [13] [10]. Batch learners can also be used for on-line learning on domains with concept drift by repeatedly learning from a window of recent instances [7] [3].
Reference: [13] <author> M. Kubat and G. </author> <title> Widmer. Adapting to drift in continuous domains. </title> <booktitle> In Proceedings of the 8th European Conference on Machine Learning, </booktitle> <pages> pages 307-310, </pages> <address> Berlin, 1995. </address> <publisher> Springer. </publisher>
Reference-contexts: The failed characterisation is removed from the search frontier and replaced by alternate characterisations. Discarding characterisations that are no longer effective provides a decay in the importance of older information. Many other on-line learners use an explicit window heuristic similar to that used in FLORA [11] [12] <ref> [13] </ref> [10]. Batch learners can also be used for on-line learning on domains with concept drift by repeatedly learning from a window of recent instances [7] [3].
Reference: [14] <author> Miroslav Kubat. </author> <title> Second tier for decision trees. </title> <booktitle> In Machine Learning: Proceedings of the 13th International Conference, </booktitle> <pages> pages 293-301, </pages> <address> California, 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The transfer of knowledge learnt in one context to a new, previously unseen, context is similar to an on-line adaption to a hidden change of context. Knowledge embedded in a decision tree can be transfered to a new context <ref> [14] </ref> by applying a two tiered structure. The fixed decision tree is used as the first tier. The second tier, providing soft matching and weights for each leaf of the decision tree, is trained on the second context.
Reference: [15] <author> R.S. Michalski. </author> <title> Learning flexible concepts: Fundimental ideas and a method based on two-tiered representation. </title> <editor> In Y. Kodratoff and R.S. Michalski, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Vol. III. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The fixed decision tree is used as the first tier. The second tier, providing soft matching and weights for each leaf of the decision tree, is trained on the second context. This is similar to the two tiered structure originally proposed <ref> [15] </ref> for dealing with flexible contexts.
Reference: [16] <author> L. Y. Pratt. </author> <title> Discriminability-based transfer between neural networks. </title> <editor> In S. J. Hanson, C. L. Giles, and J. D. Cowan, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 204-211. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year> <note> REFERENCES 33 </note>
Reference-contexts: This is similar to the two tiered structure originally proposed [15] for dealing with flexible contexts. Knowledge from an existing network can be used to significantly increase the speed of learning in a new context <ref> [16, 17] </ref> by using weights from the existing network to initialise the new neural network. 8 CONCLUSION 29 Methods for the transfer of knowledge from one context to another could be used to adapt Splice local concepts on-line in a manner analogous to that used by FLORA3.
Reference: [17] <author> Lorien Y. Pratt and Candace A. Kamm. </author> <title> Direct transfer of learned information amoung neural networks. </title> <booktitle> In Proceedings 9th National Conference on Artificial Intelligence (AAAI-91), </booktitle> <year> 1991. </year>
Reference-contexts: This is similar to the two tiered structure originally proposed [15] for dealing with flexible contexts. Knowledge from an existing network can be used to significantly increase the speed of learning in a new context <ref> [16, 17] </ref> by using weights from the existing network to initialise the new neural network. 8 CONCLUSION 29 Methods for the transfer of knowledge from one context to another could be used to adapt Splice local concepts on-line in a manner analogous to that used by FLORA3.
Reference: [18] <author> J. R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: A number of machine learning algorithms can be regarded as using CSFS including decision tree algorithms [19], rule induction algorithms [2] and ILP systems <ref> [18] </ref>. All of these systems produce concepts containing local information about context. When contiguous intervals of time reflect a hidden attribute or context, we call time the environmental attribute.
Reference: [19] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kauf-mann Publishers Inc., </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: The ability of decision trees to capture context is associated with the fact that decision tree algorithms use a form of context-sensitive feature selection (CSFS) [4]. A number of machine learning algorithms can be regarded as using CSFS including decision tree algorithms <ref> [19] </ref>, rule induction algorithms [2] and ILP systems [18]. All of these systems produce concepts containing local information about context. When contiguous intervals of time reflect a hidden attribute or context, we call time the environmental attribute. <p> Splice-1 is a meta-level algorithm that incorporates an existing batch learner. In this study the underlying learner is the decision tree learner, C4.5 <ref> [19] </ref>, with no modifications. The underlying learner for Splice-1 could, in principle, be replaced by any other CSFS machine learner able to provide splits on time. As we expect the underlying learner to deal with noise, Splice-1 does not have (or need) a mechanism to deal with noise directly. <p> The difficulty of inducing the correct classifier has been related to the number of regions <ref> [19] </ref> or peaks [20] that must be described. Another measure of domain difficulty is the average information entropy in the concept over all relevant attributes [20]. <p> Most previous work with hidden changes in context has used an on-line learning approach. The new approach, Splice, uses off-line, batch, meta-learning to extract hidden context and induce the associated local concepts. It incorporates existing machine learning systems (in this article, C4.5 <ref> [19] </ref>). Two implementations of Splice were presented. The evaluation of the Splice approach 8 CONCLUSION 30 included an on-line prediction task, a series of hidden context recognition tasks, and a complex control task.
Reference: [20] <author> Larry Rendell and Harish Ragavan. </author> <title> Improving the design of induction methods by analyzing algorithm functionality and data-based concept complexity. </title> <booktitle> In 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 952-958, </pages> <address> California, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The difficulty of inducing the correct classifier has been related to the number of regions [19] or peaks <ref> [20] </ref> that must be described. Another measure of domain difficulty is the average information entropy in the concept over all relevant attributes [20]. <p> The difficulty of inducing the correct classifier has been related to the number of regions [19] or peaks <ref> [20] </ref> that must be described. Another measure of domain difficulty is the average information entropy in the concept over all relevant attributes [20]. Drawing upon these definitions for domain difficulty, we anticipate that the CSFS algorithm will be more likely to miss context changes as the following domain characteristics increase: * Context repetition. * Irrelevant attributes. * Noise. Splice-2 is designed to minimise the impact of poor initial partitioning.
Reference: [21] <author> J. Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11(2) </volume> <pages> 416-431, </pages> <year> 1983. </year>
Reference-contexts: Neither Splice-1 or Splice-2 provide a direct comparison of the relative advantage of dividing the domain into one set of contexts over another. One comparison method that could be used is the minimum description length (MDL) <ref> [21] </ref> heuristic. The MDL principle is that the best theory for a given concept will minimise the amount of information that need be sent from a sender to a receiver so that the receiver can correctly classify items in a shared dataset.
Reference: [22] <author> M. Salganicoff. </author> <title> Density adaptive learning and forgetting. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <pages> pages 276-283, </pages> <address> San Mateo, California, 1993. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Batch learners can also be used for on-line learning on domains with concept drift by repeatedly learning from a window of recent instances [7] [3]. The window update mechanism need not use a first in, first out, organisation, <ref> [22] </ref> discards older examples only when a new item appears in a similar region of attribute space. On-line learners for domains with hidden changes in context assume that context will tend to be contiguous over time.
Reference: [23] <author> Claude Sammut, Scott Hurst, Dana Kedzier, and Donald Michie. </author> <title> Learning to fly. </title> <booktitle> In Machine Learning: Proceedings of the Ninth International Conference, </booktitle> <pages> pages 385-393, </pages> <address> San Mateo, California, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: We had available, data collected from flight simulation experiments used in be-havioural cloning <ref> [23] </ref>. Previous work on this domain found it necessary to explicitly divide the domain into a series of individual learning tasks or stages. Splice-2 was able to induce an effective pilot for a substantial proportion of the original flight plan with no explicitly provided stages. <p> Splice-2 was able to induce an effective pilot for a substantial proportion of the original flight plan with no explicitly provided stages. In the following sections we briefly describe the problem domain and the application of Splice-2. 6.1 Domain The "Learning to Fly" experiments <ref> [23] </ref> were intended to demonstrate that it is possible to build controllers for complex dynamic systems by recording the actions of a skilled operator in response to the current state of the system. <p> Such a process is rarely made explicit. One paper that does explicitly refer to an augmentation of the domain representation with a previously hidden context is <ref> [23] </ref> in which the task of learning to fly could not be achieved until the domain representation was augmented by splitting the learning task into sub-tasks. There has been substantial work on dealing with known changes in context. <p> One approach to a known context is to divide the domain into a series of different learning tasks, induce different classifiers for each task, then switch between these classifiers according to the current context. This method has been applied to the learning to fly domain <ref> [23] </ref> and to target recognition [9]. The application of local concepts for prediction and classification in this article used a similar model switching approach. The transfer of knowledge learnt in one context to a new, previously unseen, context is similar to an on-line adaption to a hidden change of context.
Reference: [24] <author> J. C. Schlimmer and R. I. Granger, Jr. </author> <title> Beyond incremental processing: Tracking concept drift. </title> <booktitle> In Proceedings AAAI-86, </booktitle> <pages> pages 502-507, </pages> <address> Los Altos, California, 1986. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Splice extends the concept of storing stable concepts as an adjunct to on-line learning to be the primary focus of an off-line batch learning approach. Most on-line learning methods that deal with concept drift decay the importance of older instances. STAGGER <ref> [24, 25] </ref>, for instance, was probably the first machine learning system dealing with concept drift. As the system moves forward over the data series, a search frontier of conjunctive and disjunctive features is altered according to changes in statistics measuring logical sufficiency (LS) and logical necessity (LN).
Reference: [25] <author> Jeffory Schlimmer and Richard Granger, Jr. </author> <title> Incremental learning from noisy data. </title> <journal> Machine Learning, </journal> <volume> 1(3) </volume> <pages> 317-354, </pages> <year> 1986. </year>
Reference-contexts: through of the Splice-1 algorithm on a problem drawn from the sample domain, then describes investigations into Splice-1 performance in a prediction task and Splice-1 accuracy in local concept identification. 3.1 STAGGER Data Set The data sets used in the following experiments are based on those used in evaluating STAGGER <ref> [25] </ref> and subsequently used by [30]. While our approach and underlying philosophy are substantially different, this allows some comparison of results. The domain chosen is artificial and a program was used to generate the data. <p> noise of between 0 and 30% is testament to its stability in adverse situations. 3 SPLICE-1 PERFORMANCE 14 0 40 80 0 50 100 150 200 250 300 % correct test series Splice C4.5 This task is similar to the on-line learning task tackled by both FLORA [30] and STAGGER <ref> [25] </ref>. The combination of Splice-1 with a simple strategy for selection of the current local concept is effective on a simple context sensitive prediction task. <p> Splice extends the concept of storing stable concepts as an adjunct to on-line learning to be the primary focus of an off-line batch learning approach. Most on-line learning methods that deal with concept drift decay the importance of older instances. STAGGER <ref> [24, 25] </ref>, for instance, was probably the first machine learning system dealing with concept drift. As the system moves forward over the data series, a search frontier of conjunctive and disjunctive features is altered according to changes in statistics measuring logical sufficiency (LS) and logical necessity (LN).
Reference: [26] <author> P. D. Turney. </author> <title> Exploiting context when learning to classify. </title> <editor> In Pavel B. Brazdil, editor, </editor> <booktitle> European Conference on Machine Learning, </booktitle> <pages> pages 402-407, </pages> <address> Berlin, 1993. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Context can also be interpreted as the effect of one (contextual) attribute on the interpretation of another (context-sensitive) attribute. Classifier accuracy can be improved when context-sensitive attributes are present for both instance based learning [1] and multivariate regression by the methods of contextual normalisation, contextual expansion and contextual weighting <ref> [26, 27, 28] </ref>. Instance based learning could be used as the Splice-2R underlying learner, any hidden contexts thereby recognised by Splice-2R could then be utilised with these techniques for on-line prediction. A somewhat different on-line method designed to detect and exploit contextual attributes is MetaL (B) [31].
Reference: [27] <author> P. D. Turney. </author> <title> Robust classification with context sensitive features. </title> <booktitle> In Paper presented at the Industrial and Engineering Applications of Artificial Intel ligence and Expert Systems, </booktitle> <year> 1993. </year>
Reference-contexts: Context can also be interpreted as the effect of one (contextual) attribute on the interpretation of another (context-sensitive) attribute. Classifier accuracy can be improved when context-sensitive attributes are present for both instance based learning [1] and multivariate regression by the methods of contextual normalisation, contextual expansion and contextual weighting <ref> [26, 27, 28] </ref>. Instance based learning could be used as the Splice-2R underlying learner, any hidden contexts thereby recognised by Splice-2R could then be utilised with these techniques for on-line prediction. A somewhat different on-line method designed to detect and exploit contextual attributes is MetaL (B) [31].
Reference: [28] <author> Peter Turney and Michael Halasz. </author> <title> Contextual normalization applied to aircraft gas turbine engine diagnosis. </title> <journal> Journal of Applied Intelligence, </journal> <volume> 3 </volume> <pages> 109-129, </pages> <year> 1993. </year> <note> REFERENCES 34 </note>
Reference-contexts: Context can also be interpreted as the effect of one (contextual) attribute on the interpretation of another (context-sensitive) attribute. Classifier accuracy can be improved when context-sensitive attributes are present for both instance based learning [1] and multivariate regression by the methods of contextual normalisation, contextual expansion and contextual weighting <ref> [26, 27, 28] </ref>. Instance based learning could be used as the Splice-2R underlying learner, any hidden contexts thereby recognised by Splice-2R could then be utilised with these techniques for on-line prediction. A somewhat different on-line method designed to detect and exploit contextual attributes is MetaL (B) [31].
Reference: [29] <author> G. Widmer and M. </author> <title> Kubat. Effective learning in dynamic environments by explicit concept tracking. </title> <editor> In Pavel B. Brazdil, editor, </editor> <booktitle> European Conference on Machine Learning, </booktitle> <pages> pages 227-243, </pages> <address> Berlin, 1993. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Rapid adaption to changes in context is assured by altering the window size in response to changes in prediction accuracy and concept complexity. One version, FLORA3, <ref> [29] </ref> was adapted for domains with recurring hidden context by storing stable concepts for possible re-use when a context change is suspected.
Reference: [30] <author> G. Widmer and M. </author> <title> Kubat. Learning in the presence of concept drift and hidden contexts. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 69-101, </pages> <year> 1996. </year>
Reference-contexts: on a problem drawn from the sample domain, then describes investigations into Splice-1 performance in a prediction task and Splice-1 accuracy in local concept identification. 3.1 STAGGER Data Set The data sets used in the following experiments are based on those used in evaluating STAGGER [25] and subsequently used by <ref> [30] </ref>. While our approach and underlying philosophy are substantially different, this allows some comparison of results. The domain chosen is artificial and a program was used to generate the data. This program allows us to control recurrence of contexts and other factors such as noise 2 and duration. <p> Colour has three possible values: red, green and blue. Shape also has three possible values: circular, triangular, and square. 2 In the following experiments, n% noise implies that the class was randomly selected with a probability of n%. This method for generating noise was chosen to be consistent with <ref> [30] </ref>. 3 SPLICE-1 PERFORMANCE 9 Table 1: Local Accuracy Matrix: Partial concept error on individual inter vals (%). <p> a range of noise of between 0 and 30% is testament to its stability in adverse situations. 3 SPLICE-1 PERFORMANCE 14 0 40 80 0 50 100 150 200 250 300 % correct test series Splice C4.5 This task is similar to the on-line learning task tackled by both FLORA <ref> [30] </ref> and STAGGER [25]. The combination of Splice-1 with a simple strategy for selection of the current local concept is effective on a simple context sensitive prediction task. <p> As the selection mechanism assumes that at least one of the local concepts will be correct, Splice-1 almost immediately moves to its maximum accuracy on each new local concept. On a similar domain the FLORA family <ref> [30] </ref> (in particular FLORA3, the learner designed to exploit recurring context) appear to reach much the same level of accuracy as Splice-1, although as an on-line learning method, FLORA requires some time to fully reflect changes in context. This comparison is problematic for a number of reasons. <p> for extending context selection methods by reasoning about context. 7 RELATED WORK 27 Splice-2 -45000 -40000 -35000 -30000 -25000 -20000 -15000 -10000 -5000 0 2000 4000 6000 500 1500 2500 North/South (feet) East/West (feet) Height (feet) C4.5 Sample flight 7 Related Work Splice is most closely related to the FLORA <ref> [30] </ref> family of on-line learners designed to adapt to hidden changes in context by drawing changes to the concept from a window of recent instances. Rapid adaption to changes in context is assured by altering the window size in response to changes in prediction accuracy and concept complexity.
Reference: [31] <author> Gerhard Widmer. </author> <title> Recognition and exploitation of contextual clues via incremental meta-learning. </title> <editor> In Lorenza Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Workshop, </booktitle> <pages> pages 525-533, </pages> <address> San Francisco, 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Instance based learning could be used as the Splice-2R underlying learner, any hidden contexts thereby recognised by Splice-2R could then be utilised with these techniques for on-line prediction. A somewhat different on-line method designed to detect and exploit contextual attributes is MetaL (B) <ref> [31] </ref>. In this case, contextual attributes are considered to be predictive of the relevance of other attributes. MetaL (B) works by using the detected contextual attributes to trigger changes to the set of features presented to the classifier.
References-found: 31

