URL: ftp://ftp.cs.umass.edu/pub/osl/papers/Penalties-TR-95-110.ps.Z
Refering-URL: http://spa-www.cs.umass.edu/bibliography.html
Root-URL: 
Email: dropsho@cs.umass.edu  
Title: Real-Time Penalties in RISC Processing  
Author: Steve Dropsho 
Date: December 12, 1995  
Affiliation: Department of Computer Science University of Masschusetts-Amherst  
Abstract: The RISC processor features that provide high performance are probabilistic (e.g., cache, TLB, writebuffers, branch prediction, etc.), so worst-case analysis in real-time systems must regularly assume the pathological conditions that make these features perform poorly (e.g., every cache access conflicts). This report presents analytical results of performance penalties due to worst-case execution time (WCET) estimates for RISC processors in real-time systems. The results clearly indicate where efforts should be made to reduce variability in processor designs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Donald Alpert and Dror Avnon. </author> <title> Architecture of the Pentium Microprocessor. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 11-21, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Division is not pipelined and requires a fixed 18 cycles for single precision and 33 for double. Single cycle latency with multiple cycle throughput on floating point multiplication is standard in the popular processors as is a longer, but fixed delay for division (DEC Alpha 21064 [4], Intel Pentium <ref> [1] </ref>, MIPS R4400 [18]). The conclusions we can draw on variability due to instruction data dependencies is that most instructions add no variability. Shift, multiplication, and division instructions can contribute variance in some processors, however, if one or more operands are constants then compilers can predict the execution time.
Reference: [2] <author> S. Basumallick and K. Nilsen. </author> <title> Cache Issues in Real-Time Systems. </title> <booktitle> ACM PLDI Workshop on Language, Compiler, and Tool Support for Real-Time Systems, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: Data loads can also be a source of large variability. Work in predicting the hit rates in the data cache <ref> [15, 2] </ref> have not been nearly as successful as similar work for instruction references. Experimental results have shown miss rate predictions ranging from 30% up to 100% for applications with very low actual miss rates. Because of the large variance we shall assume a 100% miss rate in the WCET.
Reference: [3] <author> Brad Burgess, Nasr Ullah, Peter Van Overen, and Deene Ogden. </author> <title> The PowerPC 603 Microprocessor. </title> <journal> Communications of the ACM, </journal> <pages> pages 34-42, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Today's microprocessor's use aggressive 2 hardware designs to minimize the cycle time of integer multiply, but opt for longer and less hardware intensive integer division implementations. For example, the PowerPC 603 requires between 2 to 6 cycles <ref> [3] </ref>, a 1:3 ratio, for multiply depending on the operands while division is a fixed 37 cycles. In contrast, the Alpha 21064 uses a software division algorithm with a best-case of 16 cycles and a worst-case of 144 cycles [4].
Reference: [4] <author> Digital Equipment Corporation, </author> <title> editor. DECchip 21064-AA Microprocessor Hardware Reference Manual. </title> <institution> Digital Equipment Corporation, </institution> <year> 1992. </year>
Reference-contexts: In contrast, the Alpha 21064 uses a software division algorithm with a best-case of 16 cycles and a worst-case of 144 cycles <ref> [4] </ref>. This is a 1:9 ratio and the largest ALU instruction best-case to worst-case ratio we know of in current processors. For floating point operations the PowerPC 603 has a single-cycle throughput with a fixed three cycle latency for all operations except for division. <p> Division is not pipelined and requires a fixed 18 cycles for single precision and 33 for double. Single cycle latency with multiple cycle throughput on floating point multiplication is standard in the popular processors as is a longer, but fixed delay for division (DEC Alpha 21064 <ref> [4] </ref>, Intel Pentium [1], MIPS R4400 [18]). The conclusions we can draw on variability due to instruction data dependencies is that most instructions add no variability.
Reference: [5] <author> M.T. Franklin, W.P. Alexander, R. Jauhari, A.M.G. Maynard, and B.R. Olszewski. </author> <title> Commercial workload performance in the IBM POWER2 RISC System/6000 processor. </title> <journal> IBM Journal of Research and Development, </journal> <pages> pages 555-562, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: For the average-case we assume a 4.0% miss rate for both the instruction and data caches. This is a high miss ratio for today's microprocessors which have demonstrated significantly lower values in the SPEC92 suite [9] and other commercial benchmarks <ref> [5] </ref> that include transaction processing, file servers, and multiuser development environments. For loads that might displace a dirty cache line we assume that only 50% of the cache lines are dirty.
Reference: [6] <author> Christopher A. Healy, David B. Whalley, and Marion G. Harmon. </author> <title> Integrating the Timing Analysis of Pipelining and Instruction Caching. </title> <booktitle> Proc. of the IEEE Real-Time Systems Symposium, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: This is considerably smaller than the superscalar PowerPC 604 and indicates that as designs incorporate more parallelism into the pipelines the gap between best- and worst-case will grow. Pipeline timing analysis has been used by Harmon et al. <ref> [6] </ref> and their results show exact matches between the observed timing and predictions when only variations due to pipeline effects are considered. <p> To simplify the discussion, little has been said about the interactions between the hardware features. Indeed, these interactions can have significant impact on the degree to which features affect the run-times. For example, Healy et al. <ref> [6] </ref> show how memory latency can be overlapped with pipeline stalls and Saavedra and Smith [16] show that the loose coupling of functional units in superscalar designs can result in an overlap between computation and data fetching resulting in limited prefetching.
Reference: [7] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> second edition, </address> <year> 1996. </year>
Reference-contexts: There are many different translation methods used across the different microprocessors and each makes tradeoffs between potential memory fragmentation, simplicity of translation, size of translation table information, and protection. The second edition of Hennessy and Patterson's architecture book <ref> [7] </ref> gives a good survey of the issues and possible solutions. All methods of translation follow pointers to one or more tables that contain information for translating between the virtual address and the corresponding physical address. <p> This might be as few as one or potentially a large number, however, practical considerations usually limit the number to two (POWER2 architecture [17]) or three accesses (Alpha AXP architecture <ref> [7] </ref>) per miss. Actual measured times by Saavedra and Smith [16] on a variety of systems support this relationship. Some RISC chips such as the Alpha 21064 trap on a TLB miss to run specialized code for loading the TLB. This adds additional time that is not considered here.
Reference: [8] <author> Israel Koren. </author> <title> Computer Arithmetic Algorithms. </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: While there are many algorithms for implementing these functions <ref> [8] </ref> each involves a tradeoff between time and hardware resources. Today's microprocessor's use aggressive 2 hardware designs to minimize the cycle time of integer multiply, but opt for longer and less hardware intensive integer division implementations.
Reference: [9] <author> Alvin R. Lebeck and David A. Wood. </author> <title> Cache Profiling and the SPEC Benchmarks: A Case Study. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 15-26, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Column 3 in table 2 shows the average- to best-case results. For the average-case we assume a 4.0% miss rate for both the instruction and data caches. This is a high miss ratio for today's microprocessors which have demonstrated significantly lower values in the SPEC92 suite <ref> [9] </ref> and other commercial benchmarks [5] that include transaction processing, file servers, and multiuser development environments. For loads that might displace a dirty cache line we assume that only 50% of the cache lines are dirty.
Reference: [10] <author> Larry McMahan and Ruby Lee. </author> <title> Pathlengths of SPEC Benchmarks for PA-RISC, MIPS, and SPARC. </title> <booktitle> IEEE COMPCON, </booktitle> <pages> pages 481-490, </pages> <year> 1993. </year>
Reference-contexts: The largest best-case to worst-case ratios we have seen for shift, multiplication, and division are 1:2 (MIPS R4000), 1:3 (PowerPC 603), and 1:9 (MIPS R4400), respectively. 2.3 Observations The fraction of shifts, multiplications, and divisions in the SPEC89 benchmarks are 0.012, 0.030, and 0.005, respectively <ref> [10] </ref>. Assuming no performance penalties from other factors (cache misses, exceptions, branch mispredictions, etc.) we can show the impact of ALU instruction variability on an application with similar instruction mix. We shall use a best-case cost of one cycle for shifts, 5 for multiplication, and 16 for division. <p> Experimental results have shown miss rate predictions ranging from 30% up to 100% for applications with very low actual miss rates. Because of the large variance we shall assume a 100% miss rate in the WCET. From the SPEC89 suite <ref> [10] </ref>, the frequency of data loads is about 35% and the frequency of data stores about 10%. The effective miss rate per instruction for data loads is the miss rate of loads multiplied by the fraction of data loads occurring in code segments. <p> RP = 1 + F racBr fi max (AddrCalcP enalty; M ispredictP enalty) fi InstIssueN um 1 F racBr (10) In the SPEC89 codes branches are almost 15% of the instructions <ref> [10] </ref>. For an example we shall use 15% for F racBr. The IBM's Power2 does not use a BTB (it always calculates the target address) but has a one cycle delay on mispredictions and can issue up to four instructions simultaneously, giving a penalty factor of 1.9.
Reference: [11] <author> Sunil Mirapuri, Michael Woodacre, and Nader Vasseghi. </author> <title> The Mips R4000 Processor. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 10-22, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: The R4000, however, allowed up to 64 bit shifts to be specified but only allowed up to 32 bit shifts in a single cycle with larger shifts causing a slip in the pipeline of one cycle <ref> [11] </ref>. 2.2 Arithmetic Instructions The multiply and divide instructions still suffer from data dependent execution times in some processors. While there are many algorithms for implementing these functions [8] each involves a tradeoff between time and hardware resources. <p> Thus, the relative performance factor between worst-case and best-case in the PowerPC due to pipeline conflicts is (1 + 2) fi 4 = 12. An interesting contrast is the MIPS R4000 <ref> [11] </ref> which can issue only a single instruction per cycle and has a maximum stall penalty of two cycles (e.g., a load whose result is required immediately) for a relative performance factor of 3. <p> The MIPS R4000 also does not use a BTB but has a three cycle penalty and can issue only a single instruction per cycle <ref> [11] </ref> for a similar performance penalty factor of 1.7. 6.1 Observations The effect of control transfer instructions on the worst-case to best-case ratio is less than 2.0 for current architectures. This ratio reflects the low frequency of branches and the relatively small penalty for a branch misprediction.
Reference: [12] <author> Frank Mueller, David B. Whalley, and Marion Marmon. </author> <title> Predicting Instruction Cache Behavior. </title> <booktitle> ACM SIGPLAN Workshop on Language, Compiler and Tool Support for Real-Time Systems, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: For worst-case timing analysis a miss penalty must be assumed if a cache hit cannot be guaranteed. Current work in instruction cache performance prediction <ref> [12] </ref> has accurately determined hit rates of 70% during compile time for instruction cache accesses. We must assume the remaining 30% as instruction cache misses.
Reference: [13] <author> Inc. </author> <title> NEC Electronics, editor. Memory Products Data Book, DRAMs, DRAM Modules, </title> <journal> Video RAMs, </journal> <volume> volume 2. </volume> <publisher> NEC Electronics, Inc., </publisher> <year> 1993. </year>
Reference-contexts: Equation 14 gives relative performance due to DRAM refresh effects provided some system parameters are known. An industry constant in DRAM memory modules (DRAM SIMMs) is that a single SIMM must receive a refresh operation at least once every 15.625 microseconds on average <ref> [13] </ref>. Thus, each SIMM must receive a refresh operation every 15.625 microseconds and multiple banks may be refreshed in parallel (if permitted by power constraints).
Reference: [14] <author> D. B. Whalley R. Arnold, F. Mueller and M. Harmon. </author> <title> Bounding Worst-Case Instruction Cache Performance. </title> <booktitle> IEEE Symposium on Real-Time Systems, </booktitle> <pages> pages 172-181, </pages> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: For typical applications there are 1.45 memory references per instruction. 5.1 Observations Unfortunately, we are not familiar with any work that parallels the work done in instruction cache worst-case performance prediction <ref> [14] </ref> and consequently we do not have numbers showing what reasonable analysis can predict for TLB behavior. It is also unfortunate that the TLB miss rate can not be bounded by the instruction or data cache miss rates.
Reference: [15] <author> Jai Rawat. </author> <title> Static Analysis of Cache Performance for Real-Time Programming. </title> <type> Technical Report TR93-19, </type> <institution> Iowa State University of Science and Technology, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Data loads can also be a source of large variability. Work in predicting the hit rates in the data cache <ref> [15, 2] </ref> have not been nearly as successful as similar work for instruction references. Experimental results have shown miss rate predictions ranging from 30% up to 100% for applications with very low actual miss rates. Because of the large variance we shall assume a 100% miss rate in the WCET.
Reference: [16] <author> R.H. Saavedra and A.J. Smith. </author> <title> Measuring Cache and TLB Performance and Their Effect on Benchmark Runtimes. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1223-1235, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: This might be as few as one or potentially a large number, however, practical considerations usually limit the number to two (POWER2 architecture [17]) or three accesses (Alpha AXP architecture [7]) per miss. Actual measured times by Saavedra and Smith <ref> [16] </ref> on a variety of systems support this relationship. Some RISC chips such as the Alpha 21064 trap on a TLB miss to run specialized code for loading the TLB. This adds additional time that is not considered here. <p> However, separate instruction TLBs and larger page sizes should allow good worst-case miss rate prediction. Average-case data of TLB miss rates in actual machines show miss rates for the SPEC Benchmarks ranging from 0.0% up to 10.0% <ref> [16] </ref>. 6 Control Transfer Instructions Control transfer instructions change the control flow of programs. Generally, there are two problems in tracking control flow. One is the instruction flow problem. When control transfers from one sequence to another the instruction fetch unit must adjust for this change in flow. <p> For average-case TLB effects we will select the TLB miss rate of 3.0%. This is approximately the median miss rate for the HP 9000/720 on the SPEC Benchmarks <ref> [16] </ref>. The HP 9000/720 was chosen because it is most representative of current microprocessor systems in the study. With a TLB miss penalty of three memory accesses or 60 cycles, equation 9 shows a performance ratio of 3.6. <p> Indeed, these interactions can have significant impact on the degree to which features affect the run-times. For example, Healy et al. [6] show how memory latency can be overlapped with pipeline stalls and Saavedra and Smith <ref> [16] </ref> show that the loose coupling of functional units in superscalar designs can result in an overlap between computation and data fetching resulting in limited prefetching.
Reference: [17] <author> D.J. Shippy and T.W. Griffith. </author> <title> POWER2 fixed point, data cache, and storage control units. </title> <journal> IBM Journal of Research and Development, </journal> <pages> pages 503-524, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: TLB miss times depend heavily on the number of pointers that have to be followed in determining a physical address. This might be as few as one or potentially a large number, however, practical considerations usually limit the number to two (POWER2 architecture <ref> [17] </ref>) or three accesses (Alpha AXP architecture [7]) per miss. Actual measured times by Saavedra and Smith [16] on a variety of systems support this relationship. Some RISC chips such as the Alpha 21064 trap on a TLB miss to run specialized code for loading the TLB.
Reference: [18] <author> S. Simha. </author> <title> R4400 Microprocessor Product Information. </title> <type> Technical report, </type> <institution> MIPS Technologies Inc., </institution> <month> September 27 </month> <year> 1993. </year>
Reference-contexts: Single cycle latency with multiple cycle throughput on floating point multiplication is standard in the popular processors as is a longer, but fixed delay for division (DEC Alpha 21064 [4], Intel Pentium [1], MIPS R4400 <ref> [18] </ref>). The conclusions we can draw on variability due to instruction data dependencies is that most instructions add no variability. Shift, multiplication, and division instructions can contribute variance in some processors, however, if one or more operands are constants then compilers can predict the execution time.
Reference: [19] <author> S. Peter Song, Marvin Denman, and Joe Chang. </author> <title> The PowerPC 604 RISC Microprocessor. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 8-17, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: example, in the PowerPC 604 conflicts between the branch unit and the dispatch unit result in a single cycle pipeline stall, data dependencies in the integer unit, complex integer unit, floating point unit, and store unit can cause pipeline stalls of zero (no delay), one, two, and two cycles, respectively <ref> [19] </ref> (note, the units have latencies of 1, 2, 3, and 3 cycles and can stall the pipeline up to one cycle less than the latency). Therefore, a single cycle instruction in the PowerPC 604 can stall the pipeline for a maximum of two cycles.
Reference: [20] <author> Chip Weems and Steve Dropsho. </author> <title> Real-Time RISC Processing. </title> <type> Technical Report TR-95-41, </type> <institution> University of Massachusetts- Amherst, </institution> <year> 1995. </year> <month> 20 </month>
Reference-contexts: This report looks at the potential penalties that must be assumed when calculating worst-case execution time (WCET) estimates of real-time code. The importance of general purpose processing in real-time computing requires predictable RISC processors <ref> [20] </ref>. The results here demonstrate where effort should be spent on redesign to significantly improve WCET estimates. We list the hardware features of interest and perform a first order analysis of their potential effects on code execution times.
References-found: 20

