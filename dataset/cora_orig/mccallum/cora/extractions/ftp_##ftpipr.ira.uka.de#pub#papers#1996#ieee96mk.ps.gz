URL: ftp://ftpipr.ira.uka.de/pub/papers/1996/ieee96mk.ps.gz
Refering-URL: ftp://ftpipr.ira.uka.de/.public_html/papersna.html
Root-URL: 
Title: Building Elementary Robot Skills from Human Demonstration  
Author: M. Kaiser R. Dillmann 
Address: D-76128 Karlsruhe, Germany  
Affiliation: University of Karlsruhe Institute for Real-Time Computer Systems Robotics  
Date: April 1996  
Note: In: IEEE International Conference on Robotics and Automation, Minneapolis, Minnesota,  
Abstract: This paper presents a general approach to the acquisition of sensor-based robot skills from human demonstrations. Since human-generated examples cannot be assumed to be optimal with respect to the robot, adaptation of the initially acquired skill is explicitly considered. Results for acquiring and refining manipulation skills for a Puma 260 manipulator are given. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Asada and B.-H. Yang. </author> <title> Skill acquisition from human experts through pattern processing of teaching data. </title> <booktitle> In Proceedings of the 1989 IEEE International Conference on Robotics and Automation, </booktitle> <year> 1989. </year>
Reference-contexts: However, it turns out that it is extremely difficult to duplicate this elementary operative intelligence, which is used by humans unconsciously, in a computer-controlled robot [10]. This observation motivates research in the field of Robot Skill Acquisition via Human Demonstration <ref> [1, 11, 7] </ref>, which is an extension of Robot Programming by Human Demonstration [9] that deals with the aquisition of sensor-based robot skills from human demonstrations (Fig. 1). Puma 260 manipulator. Most approaches to skill acquisition rely on application-specific solutions [1, 11, 15, 16]. <p> Puma 260 manipulator. Most approaches to skill acquisition rely on application-specific solutions <ref> [1, 11, 15, 16] </ref>. They do not consider human deficiencies, which occur especially in service scenarios with skills demonstrated by untrained users. Consequently, skill adaptation is only rarely performed [11, 7].
Reference: [2] <author> C. Baroglio, A. Giordana, M. Kaiser, M. Nuttin, and R. Pi-ola. </author> <title> Learning controllers for industrial robots. </title> <booktitle> Machine Learning, </booktitle> <year> 1996. </year>
Reference-contexts: For solving this task, several algorithms have been proposed [12, 13], among those a supervised clustering algorithm specifically designed for constructing networks to approximate continuous functions <ref> [2] </ref>. This algorithm is based on the idea that the amount of overlap between individual clusters of an RBF network should depend on the similarity of the actions associated to these clusters. <p> Additionally, it allows to determine the amount of generalization done by the network by specifying the maximum network activ ity in regions of the input space that are not covered by examples (for further details see <ref> [2] </ref>). Following the construction, the network's output weights w i are trained using gradient descent. 4 Skill refinement The refinement of skills requires to change actions that are associated to known states (skill adaptation) as well as to assign appropriate actions to states that were not encountered yet (skill extension).
Reference: [3] <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <pages> pages 835-846, </pages> <year> 1983. </year>
Reference-contexts: In the general setting of skill acquisition from human demonstration, both tasks have to be solved based on a delayed scalar evaluation (a delayed reward) of the skill execution. This is a typical setting for the application of reinforcement learning methods <ref> [3] </ref> and especially direct adaptation methods (Fig. 4, [4]). These methods do not rely on a model of the plant but try to directly estimate necessary changes in the calculated action u 0 on-line. To this aim, the generated action is systematically altered by an exploratory element (EE).
Reference: [4] <author> V. Gullapalli, J. A. Franklin, and H. Benbrahim. </author> <title> Acquiring robot skills via reinforcement learning. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 14(1):13 - 24, </volume> <year> 1994. </year>
Reference-contexts: In the general setting of skill acquisition from human demonstration, both tasks have to be solved based on a delayed scalar evaluation (a delayed reward) of the skill execution. This is a typical setting for the application of reinforcement learning methods [3] and especially direct adaptation methods (Fig. 4, <ref> [4] </ref>). These methods do not rely on a model of the plant but try to directly estimate necessary changes in the calculated action u 0 on-line. To this aim, the generated action is systematically altered by an exploratory element (EE). <p> The correlation between the change of the obtained evaluation (the reinforcement signal) and the change in the original action is exploited for adaptation (Fig. 4). The technique used for skill refinement is based on Gullapalli's Stochastic Real-Valued units (SRV units, <ref> [4] </ref>) that allow for reinforcement learning of real-valued functions. The basic idea behind the SRV algorithm is to use a Gaussian distribution to produce a stochastic control signal u fl given an input x 0 , i.e. u fl = (C s (x 0 ); ).
Reference: [5] <author> G. Hirzinger and J. Heindl. </author> <title> Sensor programming anew way for teaching robot paths and forces/torques simultaneously. </title> <booktitle> In Third International Conference on Robot Vision and Sensory Controls, </booktitle> <address> Cambridge, Mass., </address> <year> 1983. </year>
Reference-contexts: However, since the demonstrated actions were valid, after 15 trials the robot managed the task (Fig. 9) even without explicit sample correction (as in <ref> [5] </ref>). Until then, 189 neurons were added. opening under neural network control (after 15 runs). 6 Summary and Conclusion This paper presented an approach to the acquisition of robot skills from human demonstrations.
Reference: [6] <author> M. Kaiser, A. Retey, and R. Dillmann. </author> <title> Designing neural networks for adaptive control. </title> <booktitle> In IEEE International Conference on Decision and Control, </booktitle> <year> 1995. </year>
Reference-contexts: Like multilayer perceptrons, they are universal approxima tors, and similar to MLPs, they can use time-delays for handling spatio-temporal data (Fig. 3), making them applicable also for identification and control <ref> [6] </ref>. 3.1 Network construction and training To construct an RBF network, the number of clusters, their centers and widths have to be determined. For solving this task, several algorithms have been proposed [12, 13], among those a supervised clustering algorithm specifically designed for constructing networks to approximate continuous functions [2].
Reference: [7] <author> M. Kaiser, A. Retey, and R. Dillmann. </author> <title> Robot skill acquisition via human demonstration. </title> <booktitle> In Proceedings of the International Conference on Advanced Robotics, </booktitle> <year> 1995. </year>
Reference-contexts: However, it turns out that it is extremely difficult to duplicate this elementary operative intelligence, which is used by humans unconsciously, in a computer-controlled robot [10]. This observation motivates research in the field of Robot Skill Acquisition via Human Demonstration <ref> [1, 11, 7] </ref>, which is an extension of Robot Programming by Human Demonstration [9] that deals with the aquisition of sensor-based robot skills from human demonstrations (Fig. 1). Puma 260 manipulator. Most approaches to skill acquisition rely on application-specific solutions [1, 11, 15, 16]. <p> Puma 260 manipulator. Most approaches to skill acquisition rely on application-specific solutions [1, 11, 15, 16]. They do not consider human deficiencies, which occur especially in service scenarios with skills demonstrated by untrained users. Consequently, skill adaptation is only rarely performed <ref> [11, 7] </ref>. In this paper, an approach to skill acquisition from human demonstration is presented that takes the characteristics of human-generated data, the existence of irrelevant perception and action components, and robustness requirements into account. <p> is above a threshold min ; 0 &lt; min 1: Depending on the quality of the human demonstration, min is chosen to be min 2 [0:9; 1:0]. 2.2.2 Identifying relevant perceptions If sufficiently many samples (y; u) are available from a single demonstration, relevant perception components can be identified statistically <ref> [7] </ref>. <p> (t) u fl (t) as output error, and a discounted learning rate. (c) Train the network representing r s with x 0 (t) as input, r r (t) as output error, and a dis counted learning rate. reaction time was identified as 0:25 [s] such that d o = 7 <ref> [7] </ref>.
Reference: [8] <author> R. Koeppe and G. Hirzinger. </author> <title> Learning compliant motions by task-demonstration in virtual environments. </title> <booktitle> In Fourth Int. Symp. on Experimental Robotics, </booktitle> <year> 1995. </year>
Reference-contexts: We don't expect that skills acquired via an interactive learning approach are comparable to those originating from an in-depth task analysis and explicit robot programming, although virtual reality techniques <ref> [8] </ref> as well as advanced user interfaces and teaching devices may help the user to produce better examples. However, if robots are to become consumer products, they will be exposed to users who are not familiar with computers or robots.
Reference: [9] <author> Y. Kuniyoshi, M. Inaba, and H. Inoue. </author> <title> Learning by watching: Extracting reusable task knowledge from visual observation of human performance. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10(6) </volume> <pages> 799-822, </pages> <year> 1994. </year>
Reference-contexts: This observation motivates research in the field of Robot Skill Acquisition via Human Demonstration [1, 11, 7], which is an extension of Robot Programming by Human Demonstration <ref> [9] </ref> that deals with the aquisition of sensor-based robot skills from human demonstrations (Fig. 1). Puma 260 manipulator. Most approaches to skill acquisition rely on application-specific solutions [1, 11, 15, 16]. They do not consider human deficiencies, which occur especially in service scenarios with skills demonstrated by untrained users.
Reference: [10] <author> J.-C. Latombe. </author> <title> Robot Motion Planning. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: However, it turns out that it is extremely difficult to duplicate this elementary operative intelligence, which is used by humans unconsciously, in a computer-controlled robot <ref> [10] </ref>. This observation motivates research in the field of Robot Skill Acquisition via Human Demonstration [1, 11, 7], which is an extension of Robot Programming by Human Demonstration [9] that deals with the aquisition of sensor-based robot skills from human demonstrations (Fig. 1). Puma 260 manipulator.
Reference: [11] <author> S. Liu and H. Asada. </author> <title> Teaching and learning of debur-ring robots using neural networks. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <address> Atlanta, Georgia, </address> <year> 1993. </year>
Reference-contexts: However, it turns out that it is extremely difficult to duplicate this elementary operative intelligence, which is used by humans unconsciously, in a computer-controlled robot [10]. This observation motivates research in the field of Robot Skill Acquisition via Human Demonstration <ref> [1, 11, 7] </ref>, which is an extension of Robot Programming by Human Demonstration [9] that deals with the aquisition of sensor-based robot skills from human demonstrations (Fig. 1). Puma 260 manipulator. Most approaches to skill acquisition rely on application-specific solutions [1, 11, 15, 16]. <p> Puma 260 manipulator. Most approaches to skill acquisition rely on application-specific solutions <ref> [1, 11, 15, 16] </ref>. They do not consider human deficiencies, which occur especially in service scenarios with skills demonstrated by untrained users. Consequently, skill adaptation is only rarely performed [11, 7]. <p> Puma 260 manipulator. Most approaches to skill acquisition rely on application-specific solutions [1, 11, 15, 16]. They do not consider human deficiencies, which occur especially in service scenarios with skills demonstrated by untrained users. Consequently, skill adaptation is only rarely performed <ref> [11, 7] </ref>. In this paper, an approach to skill acquisition from human demonstration is presented that takes the characteristics of human-generated data, the existence of irrelevant perception and action components, and robustness requirements into account. <p> Second, the formalism must allow for incremental learning, since online adaptation and extension of the initially learned skill is mandatory. Both requirements exclude, for instance, multilayer perceptrons (MLPs) <ref> [15, 11] </ref> from being used in a general approach. A function approximation technique whose structure can be generated from examples and which supports incremental learning of both numerical and structural parameters are neural networks based on local receptive fields, such as Radial-Basis Function Networks (RBFs) [14].
Reference: [12] <author> J. Moody and C. Darken. </author> <title> Learning with localized receptive fields. </title> <booktitle> In Proceedings of the Connectionist Models Summer School. </booktitle> <institution> Carnegie Mellon University, </institution> <year> 1988. </year>
Reference-contexts: For solving this task, several algorithms have been proposed <ref> [12, 13] </ref>, among those a supervised clustering algorithm specifically designed for constructing networks to approximate continuous functions [2]. This algorithm is based on the idea that the amount of overlap between individual clusters of an RBF network should depend on the similarity of the actions associated to these clusters.
Reference: [13] <author> M.T. Musavi, W. Ahmed, K.H. Chan, </author> <title> K.B. Faris, and D.M. Hummels. On the training of radial basis function classifiers. Neural Networks, </title> <address> 5:595 - 603, </address> <year> 1992. </year>
Reference-contexts: For solving this task, several algorithms have been proposed <ref> [12, 13] </ref>, among those a supervised clustering algorithm specifically designed for constructing networks to approximate continuous functions [2]. This algorithm is based on the idea that the amount of overlap between individual clusters of an RBF network should depend on the similarity of the actions associated to these clusters.
Reference: [14] <author> T. Poggio and F. Girosi. </author> <title> Networks for approximation and learning. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(9) </volume> <pages> 1481-1497, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: A function approximation technique whose structure can be generated from examples and which supports incremental learning of both numerical and structural parameters are neural networks based on local receptive fields, such as Radial-Basis Function Networks (RBFs) <ref> [14] </ref>. These networks calculate the output u for a given input vector x as the weighted sum of the activity of the local receptive fields.
Reference: [15] <author> D. A. Pomerleau. </author> <title> Efficient training of artificial neural networks for autonomous navigation. </title> <journal> Neural Computation, </journal> <volume> 3:88 - 97, </volume> <year> 1991. </year>
Reference-contexts: Puma 260 manipulator. Most approaches to skill acquisition rely on application-specific solutions <ref> [1, 11, 15, 16] </ref>. They do not consider human deficiencies, which occur especially in service scenarios with skills demonstrated by untrained users. Consequently, skill adaptation is only rarely performed [11, 7]. <p> Second, the formalism must allow for incremental learning, since online adaptation and extension of the initially learned skill is mandatory. Both requirements exclude, for instance, multilayer perceptrons (MLPs) <ref> [15, 11] </ref> from being used in a general approach. A function approximation technique whose structure can be generated from examples and which supports incremental learning of both numerical and structural parameters are neural networks based on local receptive fields, such as Radial-Basis Function Networks (RBFs) [14].
Reference: [16] <author> P. Reignier, V. Hansen, and J.L. Crowley. </author> <title> Incremental supervised learning for mobile robot reactive control. </title> <booktitle> In Intelligent Autonomous Systems 4, </booktitle> <year> 1995. </year>
Reference-contexts: Puma 260 manipulator. Most approaches to skill acquisition rely on application-specific solutions <ref> [1, 11, 15, 16] </ref>. They do not consider human deficiencies, which occur especially in service scenarios with skills demonstrated by untrained users. Consequently, skill adaptation is only rarely performed [11, 7].
References-found: 16

