URL: ftp://ftp.cse.ucsc.edu/pub/hsnlab/ucsc-crl-93-41.ps.Z
Refering-URL: http://www.cse.ucsc.edu/research/hsnlab/publications/publications_sorted_by_subject.html
Root-URL: http://www.cse.ucsc.edu
Title: Selective Victim Caching: A Method to Improve the Performance of Direct-Mapped Caches  
Author: Dimitrios Stiliadis Anujan Varma 
Note: Research supported by NSF Young Investigator Award MIP-9257103 and NSF Grant No. MIP-9111241.  
Address: Santa Cruz, CA 95064  
Affiliation: Board of Studies in Computer Engineering University of California, Santa Cruz  
Date: October 6, 1994  
Pubnum: UCSC-CRL-93-41  
Abstract-found: 0
Intro-found: 1
Reference: [Agarwal and Pudar, 1993] <author> Agarwal, A. and Pudar, S. </author> <year> (1993). </year> <title> "Column-associative caches: a technique for reducing the miss rate of direct-mapped caches." </title> <booktitle> In Proc. 20th Int'l. Symposium on Computer Architecture, </booktitle> <pages> pages 179-190. </pages>
Reference-contexts: This motivated researchers to investigate techniques to reduce conflict-misses in a direct-mapped cache without affecting the access time on a cache hit <ref> [Jouppi, 1990, McFarling, 1992, Agarwal and Pudar, 1993] </ref>. One approach, proposed by Jouppi [Jouppi, 1990], is victim caching, where a small fully-associative "victim cache" is introduced between the first-level direct-mapped cache and the main memory.
Reference: [Ball and Larus, 1992] <author> Ball, T. and Larus, J. </author> <year> (1992). </year> <title> "Optimally profiling and tracing programs." </title> <booktitle> In Conference Record, 19th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 59-70. 29 </pages>
Reference-contexts: Even for trace driven simulations, the choice of the input traces is an important factor in the validity of the results. 12 Traces There are several methods for producing memory-access traces of programs. In this study we used traces generated by two separate techniques. We used the QPT profiler <ref> [Ball and Larus, 1992] </ref> to generate traces from the SPEC '92 benchmark programs. QPT modifies the object code in order to identify the basic blocks.
Reference: [Belady, 1966] <author> Belady, L. </author> <year> (1966). </year> <title> "A study of replacement algorithms for a virtual-storage Com--puter." </title> <journal> IBM Systems Journal, </journal> <volume> 5(2) </volume> <pages> 78-101. </pages>
Reference-contexts: This anomaly can be avoided by the use of the OPT algorithm instead of LRU as the basis for classifying cache misses [Sugumar and Abraham, 1993]. The OPT algorithm, first studied in <ref> [Belady, 1966] </ref>, always replaces the block that will be referenced the farthest in the future. Such an algorithm can be shown to be optimal for all reference patterns, that is, its miss rate is the minimum among all replacement policies.
Reference: [DEC, 1992] <institution> DEC (1992). DECchip 21064-AA microprocessor hardware reference manual. Digital Equipment Corporation. </institution>
Reference-contexts: In addition, as the processor cycle-time shrinks, single-cycle access to the cache can often be provided only by a direct-mapped configuration. Indeed, both the instruction and data caches in the DEC Alpha 21064 chip are implemented as direct-mapped to accommodate the 5 ns processor-cycle time <ref> [DEC, 1992] </ref>. Other architectures employing a direct-mapped L1 cache include the MIPS R4000 [Mirapuri et al., 1992] and the TI microSparc. Another advantage of direct-mapped caches is their compatibility with processor pipelines.
Reference: [Flanagan, 1993] <author> Flanagan, J. </author> <year> (1993). </year> <title> A new methodology for accurate trace collection and its application to memory hierarchy performance modeling. </title> <type> Ph.D. dissertation, </type> <institution> Brigham Young University. </institution>
Reference-contexts: The second class of traces were produced from a multiprogramming workload by the BACH hardware tracing tool <ref> [Flanagan, 1993, Grimsrud et al., 1993] </ref>. BACH is a hardware monitor that captures complete traces on different architectures. It consists of a hardware interface that is attached directly to the CPU and has the ability to record all memory references.
Reference: [Gee et al., 1993] <author> Gee, J., Hill, M., Pnevmatikatos, D., and Smith, A. </author> <year> (1993). </year> <title> "Cache performance of the SPEC-92 benchmark suite." </title> <journal> IEEE Micro, </journal> <volume> 13(4) </volume> <pages> 17-27. </pages>
Reference-contexts: This approach is currently the most popular in generating traces since it is relatively easy to use; several cache performance studies are based on this approach <ref> [McFarling, 1992, Jouppi, 1990, Gee et al., 1993] </ref>. Using QPT, we traced all the SPEC '92 benchmark programs running in a SUN Sparc workstation under SUN OS Release 4.1.3. The programs were compiled with the standard SPARC compilers with the optimization level specified by SPEC [SPECin, 1992, SPECfp, 1992]. <p> The data traces consisted of the references generated by the first 100 million instruction in each program; the lengths of the individual traces are shown in Table 3.3. Although the miss ratios of most of the SPEC benchmarks are relatively low <ref> [Gee et al., 1993] </ref>, they have been used widely in the past for evaluating cache organizations and algorithms by researchers. Therefore, we found it important to use them in our simulations to compare with previous results.
Reference: [Grimsrud et al., 1993] <author> Grimsrud, K., Archibald, J., Ripley, M., and Flanagan, K. </author> <year> (1993). </year> <title> "BACH: A hardware monitor for tracing microprocessor-based systems." </title> <journal> Microprocessors and Microsystems, </journal> <volume> 17(8) </volume> <pages> 443-59. </pages>
Reference-contexts: The second class of traces were produced from a multiprogramming workload by the BACH hardware tracing tool <ref> [Flanagan, 1993, Grimsrud et al., 1993] </ref>. BACH is a hardware monitor that captures complete traces on different architectures. It consists of a hardware interface that is attached directly to the CPU and has the ability to record all memory references.
Reference: [Hennessy and Patterson, 1991] <author> Hennessy, J. and Patterson, D. </author> <year> (1991). </year> <title> Computer architecture: A quantitative approach. </title> <publisher> Morgan Kaufman Publishers, Inc. </publisher>
Reference-contexts: The conflict misses are usually calculated as the additional misses of a cache compared to a fully-associative cache of the same size and employing the same replacement algorithm. The replacement algorithm usually used is Least Recently Used (LRU) <ref> [Hill and Smith, 1989, Hennessy and Patterson, 1991] </ref>. Although this model seems intuitively correct, it can sometimes yield misleading results; for example, the total miss rate can sometimes actually increase when the associativity is increased, causing the conflict misses to be negative.
Reference: [Hill, 1987] <author> Hill, M. </author> <year> (1987). </year> <title> Aspects of cache memory and instruction buffer performance. </title> <type> Ph.D. dissertation, </type> <institution> University of California, Berkeley. </institution>
Reference: [Hill, 1988] <author> Hill, M. </author> <year> (1988). </year> <title> "A case for direct-mapped caches." </title> <journal> IEEE Computer, </journal> <volume> 21(12) </volume> <pages> 25-40. </pages>
Reference-contexts: Realizing the performance potential of these processors requires careful design of the first-level cache. The design of an on-chip first-level cache involves a fundamental tradeoff between miss-rate and access time <ref> [Hill, 1988, Przybylski et al., 1988] </ref>. A direct-mapped cache results in the lowest access time, but often suffers from high miss rates due to conflicts among memory references. Set-associative caches improve the miss rate at the expense of increasing the access time. <p> Set-associative caches improve the miss rate at the expense of increasing the access time. Hill argued that direct-mapped caches often afford better performance in terms of average memory-access time over set-associative caches when both the miss rate and the cache access time are taken into account <ref> [Hill, 1988] </ref>. In addition, as the processor cycle-time shrinks, single-cycle access to the cache can often be provided only by a direct-mapped configuration. Indeed, both the instruction and data caches in the DEC Alpha 21064 chip are implemented as direct-mapped to accommodate the 5 ns processor-cycle time [DEC, 1992].
Reference: [Hill and Smith, 1989] <author> Hill, M. and Smith, A. </author> <year> (1989). </year> <title> "Evaluating associativity in CPU caches." </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1612-1630. </pages>
Reference-contexts: The conflict misses are usually calculated as the additional misses of a cache compared to a fully-associative cache of the same size and employing the same replacement algorithm. The replacement algorithm usually used is Least Recently Used (LRU) <ref> [Hill and Smith, 1989, Hennessy and Patterson, 1991] </ref>. Although this model seems intuitively correct, it can sometimes yield misleading results; for example, the total miss rate can sometimes actually increase when the associativity is increased, causing the conflict misses to be negative.
Reference: [Jouppi, 1990] <author> Jouppi, N. </author> <year> (1990). </year> <title> "Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers." </title> <booktitle> In Proc. 17th Int'l. Symposium on Computer Architecture, </booktitle> <pages> pages 364-373. </pages>
Reference-contexts: This motivated researchers to investigate techniques to reduce conflict-misses in a direct-mapped cache without affecting the access time on a cache hit <ref> [Jouppi, 1990, McFarling, 1992, Agarwal and Pudar, 1993] </ref>. One approach, proposed by Jouppi [Jouppi, 1990], is victim caching, where a small fully-associative "victim cache" is introduced between the first-level direct-mapped cache and the main memory. <p> This motivated researchers to investigate techniques to reduce conflict-misses in a direct-mapped cache without affecting the access time on a cache hit [Jouppi, 1990, McFarling, 1992, Agarwal and Pudar, 1993]. One approach, proposed by Jouppi <ref> [Jouppi, 1990] </ref>, is victim caching, where a small fully-associative "victim cache" is introduced between the first-level direct-mapped cache and the main memory. <p> Extension of the scheme to data caches is straightforward. 2.1 Cache Organization The memory hierarchy under selective victim caching is illustrated in Figure 2.1. The first-level (L1) cache consists of a direct-mapped main cache and a small fully-associative victim cache as proposed in <ref> [Jouppi, 1990] </ref>. The next lower level of the memory hierarchy can be an L2 cache or the main memory. In the original victim cache configuration described by Jouppi, which we call simple victim cache, the victim cache holds only blocks discarded from the main cache as a result of replacements. <p> We compared the selective victim caching scheme against three other cache configurations: a direct-mapped cache, a simple victim cache configuration as proposed in <ref> [Jouppi, 1990] </ref>, and, in some cases, a two-way set-associative cache. <p> This approach is currently the most popular in generating traces since it is relatively easy to use; several cache performance studies are based on this approach <ref> [McFarling, 1992, Jouppi, 1990, Gee et al., 1993] </ref>. Using QPT, we traced all the SPEC '92 benchmark programs running in a SUN Sparc workstation under SUN OS Release 4.1.3. The programs were compiled with the standard SPARC compilers with the optimization level specified by SPEC [SPECin, 1992, SPECfp, 1992].
Reference: [Jouppi, 1993] <author> Jouppi, N. </author> <year> (1993). </year> <title> "Cache write policies and performance." </title> <booktitle> In Proc. 20th Int'l. Symposium on Computer Architecture, </booktitle> <pages> pages 191-201. </pages>
Reference-contexts: If a miss occurs, processing of the instruction is aborted at a later stage in the pipeline. This idea can also be applied to writes in a data cache <ref> [Jouppi, 1993] </ref>.
Reference: [Jouppi and Wilton, 1994a] <author> Jouppi, N. and Wilton, S. </author> <year> (1994). </year> <title> "Tradeoffs in two-level on-chip caching." </title> <booktitle> In Proc. 21st Annual Int'l. Symposium on Computer Architecture, </booktitle> <pages> pages 34-45. </pages>
Reference-contexts: Necessary to our calculations was a timing model that captures the influence of the different parameters | cache size, block size, associativity, etc. | on the cache access time. We used the model presented in [Wada et al., 1992] and later expanded in <ref> [Jouppi and Wilton, 1994a] </ref>; the model provides an estimate of cache access time as a function of cache size, block size, associativity, and the physical organization of the cache.
Reference: [Jouppi and Wilton, 1994b] <author> Jouppi, N. and Wilton, S. </author> <year> (1994). </year> <title> "An enhanced access and cycle time model for on-chip caches." </title> <institution> Digital Equipment Corporation, Western Research Laboratory, </institution> <note> Research Report 93/5. </note>
Reference-contexts: A generic 0:8 m CMOS process was used and the required circuit parameters were obtained from the corresponding SPICE transistor model in <ref> [Jouppi and Wilton, 1994b] </ref>. For each cache configuration used, we iterated over all different physical organizations and estimated the lowest possible cache access time. The resulting access times are presented in Table 3.1.
Reference: [McDonell, 1992] <author> McDonell, K. </author> <year> (1992). </year> <title> "An introduction to the KENBUS benchmark." Technical Report SPEC-SDM1.1, System Performance Evaluation Cooperative (SPEC). </title>
Reference: [McFarling, 1992] <author> McFarling, S. </author> <year> (1992). </year> <title> "Cache replacement with dynamic exclusion." </title> <booktitle> In Proc. 19th Int'l. Symposium on Computer Architecture, </booktitle> <pages> pages 192-200. </pages>
Reference-contexts: This motivated researchers to investigate techniques to reduce conflict-misses in a direct-mapped cache without affecting the access time on a cache hit <ref> [Jouppi, 1990, McFarling, 1992, Agarwal and Pudar, 1993] </ref>. One approach, proposed by Jouppi [Jouppi, 1990], is victim caching, where a small fully-associative "victim cache" is introduced between the first-level direct-mapped cache and the main memory. <p> These state bits provide information on the history of the block the last time it was in the main cache. This idea was first proposed by McFarling, who used the history information to exclude certain blocks from a direct-mapped cache, reducing cyclic replacements involving the same block <ref> [McFarling, 1992] </ref>. This scheme, called dynamic exclusion, reduces conflict misses in many cases. Its main drawback, however, is that a wrong prediction results in an access to the next level of the memory hierarchy, offsetting some of the performance gain. <p> This is the basic idea behind our scheme. The prediction algorithm is based on the dynamic exclusion algorithm proposed by McFar-ling <ref> [McFarling, 1992] </ref>. The algorithm uses two state bits associated with every cache block, called the hit bit and the sticky bit. The hit bit is logically associated with a L1-cache block as it resides in L2-cache or main memory. <p> Therefore, the prediction algorithm would be unable to determine whether the block was referenced repeatedly within a program loop, or whether more than one word from the same block were fetched from the cache without an intervening reference to another block. Similar solutions to this problem were proposed in <ref> [McFarling, 1992] </ref>. The complete selective victim-caching algorithm is outlined in Figure 2.3. Three separate cases are considered: In the first case, a hit in the main cache simply sets the hit and sticky bits. In the second case, the accessed block, say fi, is in the victim cache. <p> This approach is currently the most popular in generating traces since it is relatively easy to use; several cache performance studies are based on this approach <ref> [McFarling, 1992, Jouppi, 1990, Gee et al., 1993] </ref>. Using QPT, we traced all the SPEC '92 benchmark programs running in a SUN Sparc workstation under SUN OS Release 4.1.3. The programs were compiled with the standard SPARC compilers with the optimization level specified by SPEC [SPECin, 1992, SPECfp, 1992].
Reference: [McLellan, 1993] <author> McLellan, E. </author> <year> (1993). </year> <title> "The Alpha AXP Architecture and 21064 processor." </title> <journal> IEEE Micro, </journal> <volume> 13(3) </volume> <pages> 36-47. </pages>
Reference-contexts: The "Sequential Access" signal in Figure 2.1 is activated by the CPU when the current reference is to the same block as the previous one. Current CPUs include separate address units with dedicated displacement adders <ref> [McLellan, 1993] </ref> that can be easily modified to provide the Sequential Access signal; the signal is used by the cache to avoid update of state bits used by the prediction algorithm on sequential references to the same block.
Reference: [Mirapuri et al., 1992] <author> Mirapuri, S., Woodcare, M., and Vasseghi, N. </author> <year> (1992). </year> <title> "The MIPS R4000 processor." </title> <journal> IEEE Micro, </journal> <volume> 12(2) </volume> <pages> 10-22. </pages>
Reference-contexts: Indeed, both the instruction and data caches in the DEC Alpha 21064 chip are implemented as direct-mapped to accommodate the 5 ns processor-cycle time [DEC, 1992]. Other architectures employing a direct-mapped L1 cache include the MIPS R4000 <ref> [Mirapuri et al., 1992] </ref> and the TI microSparc. Another advantage of direct-mapped caches is their compatibility with processor pipelines. On an instruction fetch or data read, data can be fetched to the CPU before the outcome of the tag-check is known.
Reference: [Prince, 1991] <author> Prince, B. </author> <year> (1991). </year> <title> Semiconductor Memories. </title> <publisher> Wiley Publishers. </publisher>
Reference-contexts: Processor clock rates increased approximately by a factor of 10 during the 1980's, while DRAM access time decreased only by a factor of two or three <ref> [Prince, 1991] </ref>. This trend is continuing into the 1990's. The common way of overcoming the bottleneck due to slow main memory is by organizing the memory system in a hierarchy, with a fast SRAM-based cache close to the CPU and the slower DRAM-based main memory behind it.
Reference: [Przybylski et al., 1988] <author> Przybylski, S., Horowitz, M., and Hennessy, J. </author> <year> (1988). </year> <title> "Performance tradeoffs in cache design." </title> <booktitle> In Proc. 15th Int'l. Symposium on Computer Architecture, </booktitle> <pages> pages 290-298. </pages>
Reference-contexts: Realizing the performance potential of these processors requires careful design of the first-level cache. The design of an on-chip first-level cache involves a fundamental tradeoff between miss-rate and access time <ref> [Hill, 1988, Przybylski et al., 1988] </ref>. A direct-mapped cache results in the lowest access time, but often suffers from high miss rates due to conflicts among memory references. Set-associative caches improve the miss rate at the expense of increasing the access time.
Reference: [Smith, 1982] <author> Smith, A. </author> <year> (1982). </year> <title> "Cache memories." </title> <journal> Computing Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530. </pages>
Reference-contexts: Owing to the increasing gap between the processor cycle-time and the access time of main memory, influence of the memory hierarchy performance, of cache memory in particular, on program execution is considerably stronger now than it was at the beginning of the last decade <ref> [Smith, 1982] </ref>. This trend is likely to continue with the new generation of processors such as the DEC Alpha, PowerPC, Intel Pentium, etc. Realizing the performance potential of these processors requires careful design of the first-level cache.
Reference: [SPECfp, 1992] <author> SPECfp (1992). </author> <title> Introduction to SPEC CPU Floating Point Benchmark Suite. SPEC Steering Committee. </title>
Reference-contexts: Using QPT, we traced all the SPEC '92 benchmark programs running in a SUN Sparc workstation under SUN OS Release 4.1.3. The programs were compiled with the standard SPARC compilers with the optimization level specified by SPEC <ref> [SPECin, 1992, SPECfp, 1992] </ref>. The characteristics of programs of the SPEC '92 benchmark suite used for our simulations are presented in Table 3.3. For simulations of instruction caches, we used only 10 of the programs with significant miss ratios for caches larger than 8 Kbytes, as indicated in Table 3.3.
Reference: [SPECin, 1992] <author> SPECin (1992). </author> <title> Introduction to SPEC CPU Integer Benchmark Suite. SPEC Steering Committee. </title>
Reference-contexts: Using QPT, we traced all the SPEC '92 benchmark programs running in a SUN Sparc workstation under SUN OS Release 4.1.3. The programs were compiled with the standard SPARC compilers with the optimization level specified by SPEC <ref> [SPECin, 1992, SPECfp, 1992] </ref>. The characteristics of programs of the SPEC '92 benchmark suite used for our simulations are presented in Table 3.3. For simulations of instruction caches, we used only 10 of the programs with significant miss ratios for caches larger than 8 Kbytes, as indicated in Table 3.3.
Reference: [Stiliadis, 1994] <author> Stiliadis, D.(1994). </author> <title> "Selective victim caching: A method to improve the performance of direct-mapped caches," M.S. </title> <type> Dissertation, </type> <institution> University of California, Santa Cruz. </institution>
Reference: [Sugumar and Abraham, 1993] <author> Sugumar, R. and Abraham, S. </author> <year> (1993). </year> <title> "Efficient simulation of caches under optimal replacement with applications to miss characterization." </title> <journal> Performance Evaluation Review, </journal> <volume> 21(1) </volume> <pages> 24-35. </pages>
Reference-contexts: The problem arises from the replacement algorithm being used; the LRU replacement policy is far from optimal for some programs. This anomaly can be avoided by the use of the OPT algorithm instead of LRU as the basis for classifying cache misses <ref> [Sugumar and Abraham, 1993] </ref>. The OPT algorithm, first studied in [Belady, 1966], always replaces the block that will be referenced the farthest in the future. Such an algorithm can be shown to be optimal for all reference patterns, that is, its miss rate is the minimum among all replacement policies. <p> We 9 Cache access time (ns) Cache size (Kbytes) Direct mapped 2-way 4 6.657 9.447 16 7.998 10.742 64 9.818 14.089 Table 3.1: Estimated cache access times. used the cheetah simulator <ref> [Sugumar and Abraham, 1993] </ref> for measuring the miss ratios of a fully-associative cache with optimal replacement.
Reference: [Wada et al., 1992] <author> Wada, T., Rajan, S., and Przybylski, S. </author> <year> (1992). </year> <title> "An analytical access time model for on-chip cache memories." </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 27(8). </volume>
Reference-contexts: Necessary to our calculations was a timing model that captures the influence of the different parameters | cache size, block size, associativity, etc. | on the cache access time. We used the model presented in <ref> [Wada et al., 1992] </ref> and later expanded in [Jouppi and Wilton, 1994a]; the model provides an estimate of cache access time as a function of cache size, block size, associativity, and the physical organization of the cache.
Reference: [Wall et al., 1990] <author> Wall, D., Borg, A., and Kessler, R. </author> <year> (1990). </year> <title> "Generation and analysis of very long address traces." </title> <booktitle> In Proc. 17th Int'l. Symposium on Computer Architecture, </booktitle> <pages> pages 290-298. 31 </pages>
Reference-contexts: A block size of 32 bytes is assumed, except when stated otherwise. The miss penalty of 10-50 cycles for 32-byte blocks can be considered as modest when no L2 cache is present. Several studies, for example <ref> [Wall et al., 1990] </ref>, report that the miss penalty can be as high as 100-200 cycles if a second-level cache is not included. The notations we use are summarized in Table 3.2. In all cases, R is the total number of references generated by the program trace.
References-found: 28

