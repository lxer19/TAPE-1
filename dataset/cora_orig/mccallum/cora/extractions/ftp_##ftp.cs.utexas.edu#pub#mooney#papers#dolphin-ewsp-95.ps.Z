URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/dolphin-ewsp-95.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: festlin,mooneyg@cs.utexas.edu  
Title: Hybrid Learning of Search Control for Partial-Order Planning  
Author: Tara A. Estlin and Raymond J. Mooney 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas at Austin  
Note: Appears in New Directions in AI Planning, IOS Press, 1996, pp. 129-140  
Abstract: This paper presents results on applying a version of the Dolphin search-control learning system to speed up a partial-order planner. Dolphin integrates explanation-based and inductive learning techniques to acquire effective clause-selection rules for Prolog programs. A version of the UCPOP partial-order planning algorithm has been implemented as a Prolog program and Dolphin used to automatically learn domain-specific search control rules that help eliminate backtracking. The resulting system is shown to produce significant speedup in several planning domains.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Barrett and D. Weld. </author> <title> Partial order planning: Evaluating possible efficiency gains. </title> <journal> Artificial Intelligence, </journal> <volume> 67 </volume> <pages> 71-112, </pages> <year> 1994. </year>
Reference-contexts: More recently, the problem of learning search control for a nonlinear planner has been presented [3, 10, 18]. Nonlinear planners have been accepted as superior to linear planners for many years, and recent experimental results support that partial-order planners are more efficient than totally-ordered planners in most domains <ref> [1, 15, 9] </ref>. Though some past work has addressed this problem [5, 7], there has been little recent work in learning control knowledge for current partial-order planning systems [10].
Reference: [2] <author> D. Borrajo and M. Veloso. </author> <title> Incremental learning of control knowledge for improvment of planning efficieny and plan quality. </title> <booktitle> In AAAI-94 Fall Symposium, </booktitle> <pages> pages 5-9, </pages> <address> New Orleans, LA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Dolphin should also be tested on domains which contain conditional effects, universal quantification, and other more-expressive planning constructs. We also plan to examine ways of using Dolphin to improve plan quality as well as planner efficiency. Borrajo and Veloso <ref> [2] </ref> and Perez and Carbonell [18] have used learned control information to guide a planner towards better solutions. Dolphin could be modified to collect positive control examples in a different method so that control rules are focused on quality issues as well as speedup.
Reference: [3] <author> D. Borrajo and M. Veloso. </author> <title> Incremental learning of control knowledge for nonlinear problem solving. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, ECML-94, </booktitle> <pages> pages 64-82, </pages> <publisher> Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: However, most work in learning and planning has been in the context of linear, state-based planners [14, 8, 12]. More recently, the problem of learning search control for a nonlinear planner has been presented <ref> [3, 10, 18] </ref>. Nonlinear planners have been accepted as superior to linear planners for many years, and recent experimental results support that partial-order planners are more efficient than totally-ordered planners in most domains [1, 15, 9]. <p> SNLP+EBL also requires additional domain knowledge which was not provided to Dolphin. Hamlet <ref> [3] </ref> is another related system which learns control knowledge for the nonlinear planner underlying Prodigy4.0. Hamlet also combines induction with EBL but uses a very different approach than Dolphin. It is diffcult to directly compare Hamlet and Dolphin for several reasons.
Reference: [4] <author> W. W. Cohen. </author> <title> Learning approximate control rules of high utility. </title> <booktitle> In ML-90, </booktitle> <pages> pages 268-276, </pages> <address> Austin, TX, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Standard explanation-based learning can frequently produce complex, overly-specific control rules that decrease rather than improve overall planning performance [13]. However, incorporating induction to learn simpler, approximate control rules can improve the utility of acquired knowledge <ref> [4, 12] </ref>. Dolphin combines explanation-based generalization (EBG) [20, 6] with techniques from inductive logic programming (ILP) [19, 16, 11] to learn effective, approximate search-control rules. <p> Unfortunately, the output program is not guaranteed to be complete. Some clauses could be too specialized and thus the final program may not solve all problems solvable by the original program. In order to guarantee the completeness of the new program, a strategy from <ref> [4] </ref> is adopted. If the final program fails to find a solution to a test problem, the initial program is used to solve the problem.
Reference: [5] <author> D. Croft. </author> <title> Choices made by a planner: Identifying them, and improving the way in which they are made. </title> <type> Master's thesis, </type> <institution> University of Edinburgh, </institution> <year> 1984. </year>
Reference-contexts: Nonlinear planners have been accepted as superior to linear planners for many years, and recent experimental results support that partial-order planners are more efficient than totally-ordered planners in most domains [1, 15, 9]. Though some past work has addressed this problem <ref> [5, 7] </ref>, there has been little recent work in learning control knowledge for current partial-order planning systems [10]. Zelle and Mooney [23] introduced the Dolphin speedup-learning system (Dynamic Optimization of Logic Programs through Heuristics INduction), which combines explanation-based learning with induction to learn clause-selection heuristics for Prolog programs.
Reference: [6] <author> G. F. DeJong and R. J. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: Standard explanation-based learning can frequently produce complex, overly-specific control rules that decrease rather than improve overall planning performance [13]. However, incorporating induction to learn simpler, approximate control rules can improve the utility of acquired knowledge [4, 12]. Dolphin combines explanation-based generalization (EBG) <ref> [20, 6] </ref> with techniques from inductive logic programming (ILP) [19, 16, 11] to learn effective, approximate search-control rules. <p> Examples of correct and incorrect clause decisions are collected for all clauses specified for learning. These examples are later used as positive and negative control examples in the induction component. In order to produce generalized proofs for the training problems, standard EBG techniques <ref> [6, 20] </ref> are applied to the original problem proofs. These proofs are tree-structured explanations that trace through the steps made during problem solving. In generalization, elements of the proof that are dependent on specific example facts are removed while the structure of the original proof is maintained.
Reference: [7] <author> R. Desimone. </author> <title> Learning control knowledge within an explanation-based learning framework. </title> <booktitle> In Proceedings of 2nd European Working Session on Learning, </booktitle> <address> EWSL-87, Bled,Yugoslavia, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: Nonlinear planners have been accepted as superior to linear planners for many years, and recent experimental results support that partial-order planners are more efficient than totally-ordered planners in most domains [1, 15, 9]. Though some past work has addressed this problem <ref> [5, 7] </ref>, there has been little recent work in learning control knowledge for current partial-order planning systems [10]. Zelle and Mooney [23] introduced the Dolphin speedup-learning system (Dynamic Optimization of Logic Programs through Heuristics INduction), which combines explanation-based learning with induction to learn clause-selection heuristics for Prolog programs.
Reference: [8] <author> J. Gratch and G. DeJong. COMPOSER: </author> <title> A probabilistic solution to the utility problem in speed-up learning. </title> <booktitle> In AAAI-92, </booktitle> <pages> pages 235-240, </pages> <address> San Jose, CA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Research in learning and planning attempts to address this important problem by developing methods that automatically acquire search-control knowledge from experience. However, most work in learning and planning has been in the context of linear, state-based planners <ref> [14, 8, 12] </ref>. More recently, the problem of learning search control for a nonlinear planner has been presented [3, 10, 18].
Reference: [9] <author> S. Kambhampati and J. Chen. </author> <title> Relative utility of EBG based plan reuse in partial ordering vs. total ordering. </title> <booktitle> In AAAI-93, </booktitle> <pages> pages 514-519, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: More recently, the problem of learning search control for a nonlinear planner has been presented [3, 10, 18]. Nonlinear planners have been accepted as superior to linear planners for many years, and recent experimental results support that partial-order planners are more efficient than totally-ordered planners in most domains <ref> [1, 15, 9] </ref>. Though some past work has addressed this problem [5, 7], there has been little recent work in learning control knowledge for current partial-order planning systems [10].
Reference: [10] <author> S. Katukam and S. Kambhampati. </author> <title> Learning explanation-based search control for partial order planning. </title> <booktitle> In AAAI-94, </booktitle> <pages> pages 582-587, </pages> <address> Seattle,WA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: However, most work in learning and planning has been in the context of linear, state-based planners [14, 8, 12]. More recently, the problem of learning search control for a nonlinear planner has been presented <ref> [3, 10, 18] </ref>. Nonlinear planners have been accepted as superior to linear planners for many years, and recent experimental results support that partial-order planners are more efficient than totally-ordered planners in most domains [1, 15, 9]. <p> Though some past work has addressed this problem [5, 7], there has been little recent work in learning control knowledge for current partial-order planning systems <ref> [10] </ref>. Zelle and Mooney [23] introduced the Dolphin speedup-learning system (Dynamic Optimization of Logic Programs through Heuristics INduction), which combines explanation-based learning with induction to learn clause-selection heuristics for Prolog programs. Standard explanation-based learning can frequently produce complex, overly-specific control rules that decrease rather than improve overall planning performance [13]. <p> Most related learning systems have been tested on different planning algorithms. For example, SNLP+EBL <ref> [10] </ref> learns search-control rules for SNLP, a predecessor of UCPOP. SNLP does not handle universal quantification or conditional effects, however, a comparison can still be made between the two learning systems on a domain which does not employ these extra features. <p> SNLP does not handle universal quantification or conditional effects, however, a comparison can still be made between the two learning systems on a domain which does not employ these extra features. For these tests, we followed a similar experimental method to that used in <ref> [10] </ref>. Problems were randomly generated from the blocksworld domain that contained between three to eight blocks and two to six goals. 3 Dolphin was trained on a set of 50 problems. The test set contained 100 problems and a CPU time limit of 120 seconds was imposed during testing.
Reference: [11] <author> N. Lavrac and S. Dzeroski, </author> <title> editors. Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: However, incorporating induction to learn simpler, approximate control rules can improve the utility of acquired knowledge [4, 12]. Dolphin combines explanation-based generalization (EBG) [20, 6] with techniques from inductive logic programming (ILP) <ref> [19, 16, 11] </ref> to learn effective, approximate search-control rules.
Reference: [12] <author> C. Leckie and I. Zuckerman. </author> <title> An inductive approach to learning search control rules for planning. </title> <booktitle> In IJCAI-93, </booktitle> <pages> pages 1100-1105, </pages> <address> Chamberry,France, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Research in learning and planning attempts to address this important problem by developing methods that automatically acquire search-control knowledge from experience. However, most work in learning and planning has been in the context of linear, state-based planners <ref> [14, 8, 12] </ref>. More recently, the problem of learning search control for a nonlinear planner has been presented [3, 10, 18]. <p> Standard explanation-based learning can frequently produce complex, overly-specific control rules that decrease rather than improve overall planning performance [13]. However, incorporating induction to learn simpler, approximate control rules can improve the utility of acquired knowledge <ref> [4, 12] </ref>. Dolphin combines explanation-based generalization (EBG) [20, 6] with techniques from inductive logic programming (ILP) [19, 16, 11] to learn effective, approximate search-control rules.
Reference: [13] <author> S. Minton. </author> <title> Quantitative results concerning the utility of explanation-based learning. </title> <booktitle> In AAAI-88, </booktitle> <pages> pages 564-569, </pages> <address> St. Paul, MN, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: Zelle and Mooney [23] introduced the Dolphin speedup-learning system (Dynamic Optimization of Logic Programs through Heuristics INduction), which combines explanation-based learning with induction to learn clause-selection heuristics for Prolog programs. Standard explanation-based learning can frequently produce complex, overly-specific control rules that decrease rather than improve overall planning performance <ref> [13] </ref>. However, incorporating induction to learn simpler, approximate control rules can improve the utility of acquired knowledge [4, 12]. Dolphin combines explanation-based generalization (EBG) [20, 6] with techniques from inductive logic programming (ILP) [19, 16, 11] to learn effective, approximate search-control rules.
Reference: [14] <author> S. Minton. </author> <title> Explanation-based learning: A problem solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 63-118, </pages> <year> 1989. </year>
Reference-contexts: Research in learning and planning attempts to address this important problem by developing methods that automatically acquire search-control knowledge from experience. However, most work in learning and planning has been in the context of linear, state-based planners <ref> [14, 8, 12] </ref>. More recently, the problem of learning search control for a nonlinear planner has been presented [3, 10, 18].
Reference: [15] <author> S. Minton, M. Drummond, J. L. Bresina, and A. B. Phillips. </author> <title> Total order vs. partial order planning: Factors influencing performance. </title> <booktitle> In KR-92, </booktitle> <pages> pages 83-92, </pages> <address> Cambridge,CA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: More recently, the problem of learning search control for a nonlinear planner has been presented [3, 10, 18]. Nonlinear planners have been accepted as superior to linear planners for many years, and recent experimental results support that partial-order planners are more efficient than totally-ordered planners in most domains <ref> [1, 15, 9] </ref>. Though some past work has addressed this problem [5, 7], there has been little recent work in learning control knowledge for current partial-order planning systems [10].
Reference: [16] <editor> S. Muggleton, editor. </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press, </publisher> <address> NY, NY, </address> <year> 1992. </year>
Reference-contexts: However, incorporating induction to learn simpler, approximate control rules can improve the utility of acquired knowledge [4, 12]. Dolphin combines explanation-based generalization (EBG) [20, 6] with techniques from inductive logic programming (ILP) <ref> [19, 16, 11] </ref> to learn effective, approximate search-control rules.
Reference: [17] <author> J.S. Penberthy and D. S. Weld. UCPOP: </author> <title> A sound, complete, partial order planner for ADL. </title> <booktitle> In KR-92, </booktitle> <pages> pages 113-114, </pages> <address> Cambridge,MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: This paper significantly extends these results by applying Dolphin to speed up a partial-order planner. A version of the UCPOP planning algorithm <ref> [17] </ref> was implemented as a Prolog program in which plan-refinement decisions are coded as clause-selection points. This allows Dolphin to learn heuristics for picking appropriate refinement operators during planning. <p> Section 5 presents experimental results on speedup learning in two planning domains. Section 6 reviews related work, Section 7 discusses limitations and future directions, and Section 8 presents our conclusions. 2 Background 2.1 The UCPOP Planner The base planner we chose for experimentation is UCPOP <ref> [17] </ref>, a partial-order planner whose step descriptions can include conditional effects and universal quantification.
Reference: [18] <author> M. A. Perez and J. Carbonell. </author> <title> Control knowledge to improve the plan quality. </title> <booktitle> In AIPS-94, </booktitle> <address> Chicago, IL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: However, most work in learning and planning has been in the context of linear, state-based planners [14, 8, 12]. More recently, the problem of learning search control for a nonlinear planner has been presented <ref> [3, 10, 18] </ref>. Nonlinear planners have been accepted as superior to linear planners for many years, and recent experimental results support that partial-order planners are more efficient than totally-ordered planners in most domains [1, 15, 9]. <p> Dolphin should also be tested on domains which contain conditional effects, universal quantification, and other more-expressive planning constructs. We also plan to examine ways of using Dolphin to improve plan quality as well as planner efficiency. Borrajo and Veloso [2] and Perez and Carbonell <ref> [18] </ref> have used learned control information to guide a planner towards better solutions. Dolphin could be modified to collect positive control examples in a different method so that control rules are focused on quality issues as well as speedup.
Reference: [19] <author> J.R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: However, incorporating induction to learn simpler, approximate control rules can improve the utility of acquired knowledge [4, 12]. Dolphin combines explanation-based generalization (EBG) [20, 6] with techniques from inductive logic programming (ILP) <ref> [19, 16, 11] </ref> to learn effective, approximate search-control rules. <p> The resulting generalized proofs are used in the control rule induction phase to guide an inductive search. Induction is used to build an operational definition of the class of subgoals to which a certain clause should be applied. The inductive search is based upon the Foil algorithm <ref> [19] </ref>; however, instead of searching through all possible literals, the search is modified to only consider the operational literals contained in the generalized proofs of the training examples. Dolphin builds a subgoal definition by finding clauses which cover some positive control examples but no negatives.
Reference: [20] <author> R. M. Keller T. Mitchell and S. T. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: Standard explanation-based learning can frequently produce complex, overly-specific control rules that decrease rather than improve overall planning performance [13]. However, incorporating induction to learn simpler, approximate control rules can improve the utility of acquired knowledge [4, 12]. Dolphin combines explanation-based generalization (EBG) <ref> [20, 6] </ref> with techniques from inductive logic programming (ILP) [19, 16, 11] to learn effective, approximate search-control rules. <p> Examples of correct and incorrect clause decisions are collected for all clauses specified for learning. These examples are later used as positive and negative control examples in the induction component. In order to produce generalized proofs for the training problems, standard EBG techniques <ref> [6, 20] </ref> are applied to the original problem proofs. These proofs are tree-structured explanations that trace through the steps made during problem solving. In generalization, elements of the proof that are dependent on specific example facts are removed while the structure of the original proof is maintained.
Reference: [21] <author> M. Veloso. </author> <title> Learning by Analogical Reasoning in General Problem Solving. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: However, we feel many of them would also be beneficial in learning search control for other types of problem solving. 5 Experimental Evaluation 5.1 Experimental Design The blocksworld and logistics transportation domains were used to test the final system. In the logistics domain <ref> [21] </ref>, packages must be delivered to different locations in several cities. Packages are transported between cities by airplane and within a city by truck. In both domains, a test set of 100 independently generated problems was used to evaluate performance. Dolphin was trained on separate example sets of increasing size.
Reference: [22] <author> J. M. Zelle. </author> <title> Learning search-control heuristics for logic programs: Applications to speedup learning and language acquisition. </title> <type> Technical Report AI93-200, </type> <institution> University of Texas, Austin, TX, </institution> <year> 1993. </year>
Reference-contexts: The output is a modified program which incorporates learned search-control information. The original Dolphin algorithm has been explained in detail in <ref> [22] </ref> and [23].
Reference: [23] <author> J. M. Zelle and R. J. Mooney. </author> <title> Combining FOIL and EBG to speed-up logic programs. </title> <booktitle> In IJCAI-93, </booktitle> <pages> pages 1106-1111, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: Though some past work has addressed this problem [5, 7], there has been little recent work in learning control knowledge for current partial-order planning systems [10]. Zelle and Mooney <ref> [23] </ref> introduced the Dolphin speedup-learning system (Dynamic Optimization of Logic Programs through Heuristics INduction), which combines explanation-based learning with induction to learn clause-selection heuristics for Prolog programs. Standard explanation-based learning can frequently produce complex, overly-specific control rules that decrease rather than improve overall planning performance [13]. <p> However, incorporating induction to learn simpler, approximate control rules can improve the utility of acquired knowledge [4, 12]. Dolphin combines explanation-based generalization (EBG) [20, 6] with techniques from inductive logic programming (ILP) [19, 16, 11] to learn effective, approximate search-control rules. Zelle and Mooney <ref> [23] </ref> fl This research was supported by the NASA Graduate Student Researchers Program under grant number NGT-51332 and by the National Science Foundation under grant IRI-9310819. present results verifying that this approach outperforms standard explanation-based methods at improving the performance of a simple, linear planner written in Prolog. <p> The output is a modified program which incorporates learned search-control information. The original Dolphin algorithm has been explained in detail in [22] and <ref> [23] </ref>. <p> The problem of adding required parameters to predicates for control purposes is discussed in future work. 4 Extending DOLPHIN Previously, Dolphin was shown effective at speeding up the N-Queens problem, a symbolic integration program, and planning in the STRIPS robot world and the blocksworld <ref> [23] </ref>. The planning domains employed a linear, means-ends analysis planner. Dolphin has now been extended so that is can successfully speedup a partial-order planner. The search space of a partial-order planner greatly differs from that of a standard linear planner.
References-found: 23

