URL: http://www.cs.huji.ac.il/~feit/parsched/p-95-19.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~feit/parsched/parsched95.html
Root-URL: http://www.cs.huji.ac.il
Email: feit@watson.ibm.com  nitzberg@nas.nasa.gov  
Phone: 2  
Title: Job Characteristics of a Production Parallel Scientific Workload on the NASA Ames iPSC/860  
Author: Dror G. Feitelson and Bill Nitzberg ? P. O. 
Address: Box 218, Yorktown Heights, NY 10598  M/S 258-6, Moffett Field, CA 94035  
Affiliation: 1 IBM T. J. Watson Research Center  NASA Ames Research Center  
Abstract: Statistics of a parallel workload on a 128-node iPSC/860 located at NASA Ames are presented. It is shown that while the number of sequential jobs dominates the number of parallel jobs, most of the resources (measured in node-seconds) were consumed by parallel jobs. Moreover, most of the sequential jobs were for system administration. The average runtime of jobs grew with the number of nodes used, so the total resource requirements of large parallel jobs were larger by more than the number of nodes they used. The job submission rate during peak day activity was somewhat lower than one every two minutes, and the average job size was small. At night, submission rate was low but job sizes and system utilization were high, mainly due to NQS. Submission rate and utilization over the weekend were lower than on weekdays. The overall utilization was 50%, after accounting for downtime. About 2/3 of the applications were executed repeatedly, some for a significant number of times.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. K. Agrawala, J. M. Mohr, and R. M. Bryant, </author> <title> "An approach to the workload characterization problem". </title> <booktitle> Computer 9(6), </booktitle> <pages> pp. 18-32, </pages> <month> Jun </month> <year> 1976. </year>
Reference-contexts: 1 Introduction Since the dawn of (computer) time, it has been recognized that performance analysis and modeling of computer systems hinges on using a representative workload <ref> [9, 1] </ref>. The approach is typically to measure the workload on a real system, analyze it, and use the obtained parameters to drive a simulation or as input for analytical modeling.
Reference: 2. <author> G. M. </author> <title> Amdahl, "Validity of the single processor approach to achieving large scale computer capabilities". </title> <booktitle> In AFIPS Spring Joint Comput. Conf., </booktitle> <volume> vol. 30, </volume> <pages> pp. 483-485, </pages> <month> Apr </month> <year> 1967. </year>
Reference-contexts: This assumes that the work done by a job is fixed, and parallelism is used to solve the same problems faster. Therefore the runtime is assumed to be inversely proportional to the degree of parallelism. This model is the basis for Amdahl's law <ref> [2] </ref>. Fixed time [10, 11]. Here it is assumed that parallelism is used to solve increasingly larger problems, under the constraint that the total runtime stays fixed. In this case, the runtime distribution is independent of the degree of parallelism. Memory bound [26].
Reference: 3. <author> E. Barszcz, </author> <title> "Intercube communication for the iPSC/860". </title> <booktitle> In Scalable High-Performance Comput. Conf., </booktitle> <pages> pp. 307-313, </pages> <year> 1992. </year>
Reference-contexts: Cumulative time spent in each multiprogramming level. It should be noted that a package for inter-partition communication has been developed at NASA Ames, and is used on this system. This package allows distinct jobs, running concurrently on disjoint subcubes, to communicate with each other <ref> [3] </ref>. Use of this package obviously boosts the measured multiprogramming level, because sets of jobs that are actually part of the same application are measured as independent jobs.
Reference: 4. <author> M. Calzarossa and G. </author> <title> Serazzi, "A characterization of the variation in time of workload arrival patterns". </title> <journal> IEEE Trans. Comput. </journal> <volume> C-34(2), </volume> <pages> pp. 156-162, </pages> <month> Feb </month> <year> 1985. </year>
Reference-contexts: On weekdays, a daily cycle is obvious, with a high submission rate during the working day and a low rate at night. The peak is in the late morning, with a noticeable drop during lunch. This pattern is similar to known results from uniprocessor interactive systems <ref> [4] </ref>. The peak value is about one job every 2.6 minutes, on average. The implication is that the load on the system does not change too often, so scheduling schemes can afford to optimize the way resources are shared.
Reference: 5. <author> M. Calzarossa and G. </author> <title> Serazzi, "Workload characterization: a survey". </title> <booktitle> Proc. IEEE 81(8), </booktitle> <pages> pp. 1136-1150, </pages> <month> Aug </month> <year> 1993. </year>
Reference-contexts: The approach is typically to measure the workload on a real system, analyze it, and use the obtained parameters to drive a simulation or as input for analytical modeling. While there are some measurement-based results on workload modeling for uniprocessors <ref> [12, 5] </ref>, there is very little hard evidence from real measurements on parallel machines. The little work that has been done concentrates on modeling single applications, e.g. showing how a specific algorithm leads to changes in the degree of parallelism in different phases of the computation [18, 24, 20].
Reference: 6. <author> S-H. Chiang, R. K. Mansharamani, and M. K. Vernon, </author> <title> "Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 33-44, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The overheads associated with parallelism always grow superlinearly. Thus the total execution time actually increases with added parallelism. The first two models are typically used in studies of multiprocessor scheduling <ref> [21, 19, 17, 6] </ref>. However, these models are inconsistent with our results. The memory bound model is consistent in terms of its runtime predictions, but we do not have any data on memory usage by the applications.
Reference: 7. <author> M. V. Devarakonda and R. K. Iyer, </author> <title> "Predictability of process resource usage: a measurement-based study on UNIX ". IEEE Trans. </title> <journal> Softw. Eng. </journal> <volume> 15(12), </volume> <pages> pp. 1579-1586, </pages> <month> Dec </month> <year> 1989. </year>
Reference-contexts: The results are shown in Fig. 18. The majority of cases had a coefficient of variation smaller than 1, which is promising. The predictive power can be improved by using a more sophisticated model of job behavior, rather than just using the mean of previous runs <ref> [7] </ref>. Fig. 18. Histogram of the coefficient of variation of runtimes of applications that were executed repeatedly. Fig. 19. Histogram of the runlengths of applications that were executed repeatedly by the same user using the same subcube size.
Reference: 8. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Wasted resources in gang scheduling". </title> <booktitle> In 5th Jerusalem Conf. Information Technology, </booktitle> <pages> pp. 127-136, </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Oct </month> <year> 1990. </year>
Reference-contexts: Thus if a 128-node partition is not available, 124 nodes can be used instead. If 64 nodes are not available, a partition of 60 can still be used. In principle, the utilization of hypercubes can be boosted if time-slicing is used <ref> [8] </ref>. However, this requires gang scheduling to be implemented, and increases the requirement for memory at each node, because multiple applications have to be memory-resident at the same time. Multiprogramming level On uniprocessors, multiprogramming is used to improve system utilization by overlapping the execution of jobs with complementary requirements.
Reference: 9. <author> D. Ferrari, </author> <title> "Workload characterization and selection in computer performance measurement". </title> <booktitle> Computer 5(4), </booktitle> <pages> pp. 18-24, </pages> <month> Jul/Aug </month> <year> 1972. </year>
Reference-contexts: 1 Introduction Since the dawn of (computer) time, it has been recognized that performance analysis and modeling of computer systems hinges on using a representative workload <ref> [9, 1] </ref>. The approach is typically to measure the workload on a real system, analyze it, and use the obtained parameters to drive a simulation or as input for analytical modeling.
Reference: 10. <author> J. L. Gustafson, </author> <title> "Reevaluating Amdahl's law". </title> <journal> Comm. ACM 31(5), </journal> <pages> pp. 532-533, </pages> <month> May </month> <year> 1988. </year> <note> See also Comm. ACM 32(2), pp. 262-264, </note> <month> Feb </month> <year> 1989, </year> <journal> and Comm. ACM 32(8), </journal> <pages> pp. 1014-1016, </pages> <month> Aug </month> <year> 1989. </year>
Reference-contexts: This assumes that the work done by a job is fixed, and parallelism is used to solve the same problems faster. Therefore the runtime is assumed to be inversely proportional to the degree of parallelism. This model is the basis for Amdahl's law [2]. Fixed time <ref> [10, 11] </ref>. Here it is assumed that parallelism is used to solve increasingly larger problems, under the constraint that the total runtime stays fixed. In this case, the runtime distribution is independent of the degree of parallelism. Memory bound [26].
Reference: 11. <author> J. L. Gustafson, G. R. Montry, and R. E. Benner, </author> <title> "Development of parallel methods for a 1024-processor hypercube". </title> <journal> SIAM J. Sci. Statist. Comput. </journal> <volume> 9(4), </volume> <pages> pp. 609-638, </pages> <month> Jul </month> <year> 1988. </year>
Reference-contexts: This assumes that the work done by a job is fixed, and parallelism is used to solve the same problems faster. Therefore the runtime is assumed to be inversely proportional to the degree of parallelism. This model is the basis for Amdahl's law [2]. Fixed time <ref> [10, 11] </ref>. Here it is assumed that parallelism is used to solve increasingly larger problems, under the constraint that the total runtime stays fixed. In this case, the runtime distribution is independent of the degree of parallelism. Memory bound [26].
Reference: 12. <author> P. Heidelberger and S. S. Lavenberg, </author> <title> "Computer performance evaluation methodology". </title> <journal> IEEE Trans. Comput. </journal> <volume> C-33(12), </volume> <pages> pp. 1195-1220, </pages> <month> Dec </month> <year> 1984. </year>
Reference-contexts: The approach is typically to measure the workload on a real system, analyze it, and use the obtained parameters to drive a simulation or as input for analytical modeling. While there are some measurement-based results on workload modeling for uniprocessors <ref> [12, 5] </ref>, there is very little hard evidence from real measurements on parallel machines. The little work that has been done concentrates on modeling single applications, e.g. showing how a specific algorithm leads to changes in the degree of parallelism in different phases of the computation [18, 24, 20].
Reference: 13. <author> Intel Corp., </author> <title> iPSC/860 Multi-User Accounting, Control, and Scheduling Utilities Manual. Order number 312261-002, </title> <month> May </month> <year> 1992. </year>
Reference-contexts: NQS queues used on the traced machine. Jobs can be submitted directly or through the NQS batch queueing facility. NQS is integrated with MACS (Multi-user Accounting, Control, and Scheduling utilities), which is part of the system software on the iPSC/860 <ref> [13] </ref>. To use NQS, system administrators define multiple queues with different resource limits. The combinations used on the iPSC/860 at NASA Ames are shown in Table 1: there are 4 possible values for the subcube size, and 3 for the runtime. <p> In certain circumstances, it may even kill direct (interactive) jobs that exceed their resource limits <ref> [13] </ref>. Only queues q16s, q16m, q32s, and q32m are enabled during prime time (from 6 AM to 8 PM), effectively limiting NQS jobs to 32 nodes and 1 hour. Any larger jobs that are submitted are queued until the other queues become active at night. <p> This leaves enough of the machine free for all the small interactive jobs. In fact, interactive users are encouraged to use the clrcube command to kill large batch jobs that get in their way <ref> [13] </ref>. More on the resource use of interactive vs. batch is given below. job size numb er jobs 1 2 4 8 16 32 64 128 1000 3000 29000 direct NQS Unix Fig. 4.
Reference: 14. <author> D. Kotz and N. Nieuwejaar, </author> <title> "Dynamic file-access characteristics of a production parallel scientific workload". </title> <booktitle> In Supercomputing '94, </booktitle> <pages> pp. 640-649, </pages> <month> Nov </month> <year> 1994. </year>
Reference-contexts: Trace Availability The trace used in this study is available upon request from Bill Nitzberg, at nitzberg@nas.nasa.gov. Acknowledgements This study was initiated using the CHARISMA traces, collected by David Kotz and Nils Nieuwejaar of Dartmouth College to study I/O behavior of applications <ref> [14] </ref>. These traces were later replaced by an accounting trace collected at NASA Ames, which had the advantage that it included user and job information, and that it covered a much longer period.
Reference: 15. <author> S. </author> <title> Krakowiak, </title> <booktitle> Principles of Operating Systems. </booktitle> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: In the exponential distribution, it has a value of 1. Fig. 10. Distribution of runtimes for jobs with different degrees of parallelism, and normalized cumulative distributions of runtimes. variation is always larger than 1, lending credence to a hyperexponential model, as is typically expected <ref> [15] </ref>. The mean is especially small, and the coefficient of variation especially large, for system support jobs. due to the large number of these jobs, it is important to exclude them when investigating statistics of user activity.
Reference: 16. <author> P. Krueger and R. Chawla, </author> <title> "The stealth distributed scheduler". </title> <booktitle> In 11th Intl. Conf. Distributed Comput. Syst., </booktitle> <pages> pp. 336-343, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The transition from night to day is more gradual, as NQS gradually stops scheduling new jobs that are not expected to complete before the night shift is over. This daily cycle should be contrasted with the cycle observed on interactive workstations, which displays very low utilization at night <ref> [16] </ref>. After accounting for downtime, the overall average utilization according to the traces was 52.4% on weekdays, 42.1% on weekends, and 50.0% overall. This is considerably lower than the 80% utilization reported for the Touchstone Delta, also accounting for downtime [22]. <p> This is considerably lower than the 80% utilization reported for the Touchstone Delta, also accounting for downtime [22]. However, it is in line with the results of [17], which are specific to subcube allocation. It is much higher than the 9% average utilization measured for workstations <ref> [16] </ref>. The reason for the difference from the Delta is that in general there are not enough small jobs to fill in the gaps left by large ones. In hypercube machines, jobs require partitions that are subcubes. Thus if a 4-node job is running, a 128-node job is blocked.
Reference: 17. <author> P. Krueger, T-H. Lai, and V. A. Radiya, </author> <title> "Processor allocation vs. job scheduling on hypercube computers". </title> <booktitle> In 11th Intl. Conf. Distributed Comput. Syst., </booktitle> <pages> pp. 394-401, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: After accounting for downtime, the overall average utilization according to the traces was 52.4% on weekdays, 42.1% on weekends, and 50.0% overall. This is considerably lower than the 80% utilization reported for the Touchstone Delta, also accounting for downtime [22]. However, it is in line with the results of <ref> [17] </ref>, which are specific to subcube allocation. It is much higher than the 9% average utilization measured for workstations [16]. The reason for the difference from the Delta is that in general there are not enough small jobs to fill in the gaps left by large ones. <p> The overheads associated with parallelism always grow superlinearly. Thus the total execution time actually increases with added parallelism. The first two models are typically used in studies of multiprocessor scheduling <ref> [21, 19, 17, 6] </ref>. However, these models are inconsistent with our results. The memory bound model is consistent in terms of its runtime predictions, but we do not have any data on memory usage by the applications.
Reference: 18. <author> M. Kumar, </author> <title> "Measuring parallelism in computation-intensive scientific/engineering applications". </title> <journal> IEEE Trans. Comput. </journal> <volume> 37(9), </volume> <pages> pp. 1088-1098, </pages> <month> Sep </month> <year> 1988. </year>
Reference-contexts: The little work that has been done concentrates on modeling single applications, e.g. showing how a specific algorithm leads to changes in the degree of parallelism in different phases of the computation <ref> [18, 24, 20] </ref>. There is practically no work relating to the mix of different jobs that are found on parallel machines. This makes it hard to study and compare operating system policies for scheduling and processor allocation. ? Computer Sciences Corporation, NASA Contract NAS 2-12961.
Reference: 19. <author> S. T. Leutenegger and M. K. Vernon, </author> <title> "The performance of multiprogrammed mul-tiprocessor scheduling policies". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The overheads associated with parallelism always grow superlinearly. Thus the total execution time actually increases with added parallelism. The first two models are typically used in studies of multiprocessor scheduling <ref> [21, 19, 17, 6] </ref>. However, these models are inconsistent with our results. The memory bound model is consistent in terms of its runtime predictions, but we do not have any data on memory usage by the applications.
Reference: 20. <author> S. Majumdar, D. L. Eager, and R. B. Bunt, </author> <title> "Characterisation of programs for scheduling in multiprogrammed parallel systems". </title> <booktitle> Performance Evaluation 13(2), </booktitle> <pages> pp. 109-130, </pages> <year> 1991. </year>
Reference-contexts: The little work that has been done concentrates on modeling single applications, e.g. showing how a specific algorithm leads to changes in the degree of parallelism in different phases of the computation <ref> [18, 24, 20] </ref>. There is practically no work relating to the mix of different jobs that are found on parallel machines. This makes it hard to study and compare operating system policies for scheduling and processor allocation. ? Computer Sciences Corporation, NASA Contract NAS 2-12961.
Reference: 21. <author> S. Majumdar, D. L. Eager, and R. B. Bunt, </author> <title> "Scheduling in multiprogrammed parallel systems". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 104-113, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: The overheads associated with parallelism always grow superlinearly. Thus the total execution time actually increases with added parallelism. The first two models are typically used in studies of multiprocessor scheduling <ref> [21, 19, 17, 6] </ref>. However, these models are inconsistent with our results. The memory bound model is consistent in terms of its runtime predictions, but we do not have any data on memory usage by the applications.
Reference: 22. <author> P. Messina, </author> <title> "The Concurrent Supercomputing Consortium: </title> <booktitle> year 1". IEEE Parallel & Distributed Technology 1(1), </booktitle> <pages> pp. 9-16, </pages> <month> Feb </month> <year> 1993. </year>
Reference-contexts: After accounting for downtime, the overall average utilization according to the traces was 52.4% on weekdays, 42.1% on weekends, and 50.0% overall. This is considerably lower than the 80% utilization reported for the Touchstone Delta, also accounting for downtime <ref> [22] </ref>. However, it is in line with the results of [17], which are specific to subcube allocation. It is much higher than the 9% average utilization measured for workstations [16].
Reference: 23. <author> V. K. Naik, S. K. Setia, and M. S. Squillante, </author> <title> Performance Analysis of Job Scheduling Policies in Parallel Supercomputer Environments. </title> <type> Research Report RC 19138 (82333), </type> <institution> IBM T. J. Watson Research Center, </institution> <month> Sep </month> <year> 1993. </year> <note> Also in Supercomputing '93. </note>
Reference-contexts: An independent study of 23 CFD applications (which is the typical application domain for the studied system) also found that large jobs tend to run longer, despite the fact that they use more processors <ref> [23] </ref>. Interactive vs. batch The distribution of runtimes also allows one to distinguish between interactive and batch jobs. For example, jobs taking less than 10 seconds could be called interactive, and those taking more time called batch.
Reference: 24. <author> K. C. Sevcik, </author> <title> "Characterization of parallelism in applications and their use in scheduling ". In SIGMETRICS Conf. </title> <booktitle> Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: The little work that has been done concentrates on modeling single applications, e.g. showing how a specific algorithm leads to changes in the degree of parallelism in different phases of the computation <ref> [18, 24, 20] </ref>. There is practically no work relating to the mix of different jobs that are found on parallel machines. This makes it hard to study and compare operating system policies for scheduling and processor allocation. ? Computer Sciences Corporation, NASA Contract NAS 2-12961.
Reference: 25. <author> J. P. Singh, J. L. Hennessy, and A. Gupta, </author> <title> "Scaling parallel programs for multiprocessors: methodology and examples". </title> <booktitle> Computer 26(7), </booktitle> <pages> pp. 42-50, </pages> <month> Jul </month> <year> 1993. </year>
Reference-contexts: Runtime statistics for jobs with different degrees of parallelism. increased available parallelism. Three models have been proposed <ref> [27, 25] </ref>: Fixed work. This assumes that the work done by a job is fixed, and parallelism is used to solve the same problems faster. Therefore the runtime is assumed to be inversely proportional to the degree of parallelism. This model is the basis for Amdahl's law [2].
Reference: 26. <author> X-H. Sun and L. M. Ni, </author> <title> "Scalable problems and memory-bounded speedup". </title> <journal> J. Parallel & Distributed Comput. </journal> <volume> 19(1), </volume> <pages> pp. 27-37, </pages> <month> Sep </month> <year> 1993. </year>
Reference-contexts: Fixed time [10, 11]. Here it is assumed that parallelism is used to solve increasingly larger problems, under the constraint that the total runtime stays fixed. In this case, the runtime distribution is independent of the degree of parallelism. Memory bound <ref> [26] </ref>. If the problem size is increased to fill the available memory on the larger machine, the amount of productive work typically grows at least linearly with the parallelism. The overheads associated with parallelism always grow superlinearly. Thus the total execution time actually increases with added parallelism.
Reference: 27. <author> P. H. Worley, </author> <title> "The effect of time constraints on scaled speedup". </title> <journal> SIAM J. Sci. Statist. Comput. </journal> <volume> 11(5), </volume> <pages> pp. 838-858, </pages> <month> Sep </month> <year> 1990. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Runtime statistics for jobs with different degrees of parallelism. increased available parallelism. Three models have been proposed <ref> [27, 25] </ref>: Fixed work. This assumes that the work done by a job is fixed, and parallelism is used to solve the same problems faster. Therefore the runtime is assumed to be inversely proportional to the degree of parallelism. This model is the basis for Amdahl's law [2].
References-found: 27

