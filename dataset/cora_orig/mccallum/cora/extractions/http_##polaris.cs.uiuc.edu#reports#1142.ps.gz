URL: http://polaris.cs.uiuc.edu/reports/1142.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: MCSPARSE: A Parallel Sparse Unsymmetric Linear System Solver  
Author: K. A. Gallivan, B. A. Marsolf, and H. A. G. Wijshoff 
Address: 1308 West Main Street Urbana, Illinois 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Date: August 1991  
Pubnum: CSRD Report No. 1142  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. Alaghband, </author> <title> A parallel pivoting algorithm on a shared memory multiprocessor, </title> <booktitle> in Proceedings of the 1988 International Conference on Parallel Processing, Volume3: Applications and Algorithms, </booktitle> <volume> vol. 3, </volume> <year> 1988, </year> <pages> pp. 177-180. </pages>
Reference-contexts: This approach if organized correctly can provide large and medium grain parallelism. However, the method tends to work well on matrices with a near-symmetric structure and the pivot sequence is constrained. Another approach to parallel sparse solvers exploits the dynamic identification and application of parallel pivots <ref> [1, 4, 15] </ref>. At each stage these algorithms construct a set of pivots that can be applied in parallel and perform the appropriate updates. These codes typically concentrate on medium and fine grain parallelism and tend to be most efficient on a moderate number of processors with fairly tight synchronization.
Reference: [2] <author> C. C. Ashcraft, R. G. Grimes, J. G. Lewis, B. W. Peyton, and H. D. Simon, </author> <title> Progress in sparse matrix methods for large linear systems on vector supercomputers, </title> <journal> Intl. J. Supercomputing Appl., </journal> <volume> 1 (1987), </volume> <pages> pp. 10-30. </pages>
Reference-contexts: These codes typically concentrate on medium and fine grain parallelism and tend to be most efficient on a moderate number of processors with fairly tight synchronization. There is also previous work on performance improvements of direct sparse solvers on vector supercomputers <ref> [2] </ref>. The results indicate that vectorization can sometimes be used to improve the performance. Both of these approaches can be used as part of an algorithm which exploits multiple levels of parallelism.
Reference: [3] <author> J. R. Bunch, </author> <title> Analysis of sparse elimination, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 11 (1974), </volume> <pages> pp. 847-873. </pages>
Reference-contexts: The extra factor of 2 is due to the two pivots per column. In the case of sparse matrices, the growth factor bound can be reduced significantly and depends upon the sparsity of the matrix <ref> [3, 16] </ref>. When the partial pivoting conditions are relaxed or another pivoting strategy 31 is combined with casting, the bound on the growth factor is easily deduced from modifications to the standard analysis. 5. MCSPARSE Factorization/Solve Description. 5.1. Stability control.
Reference: [4] <author> T. Davis, </author> <title> A parallel algorithm for sparse unsymmetric LU factorization, </title> <type> Tech. Report CSRD Report No. 907, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1989. </year> <type> PhD. thesis. </type>
Reference-contexts: This approach if organized correctly can provide large and medium grain parallelism. However, the method tends to work well on matrices with a near-symmetric structure and the pivot sequence is constrained. Another approach to parallel sparse solvers exploits the dynamic identification and application of parallel pivots <ref> [1, 4, 15] </ref>. At each stage these algorithms construct a set of pivots that can be applied in parallel and perform the appropriate updates. These codes typically concentrate on medium and fine grain parallelism and tend to be most efficient on a moderate number of processors with fairly tight synchronization. <p> This row permutation also destroys the structure of the matrix. Row permutations with the border, at the appropriate point in the factorization, do in fact preserve the bordered block upper triangular structure. For example, pairwise pivoting could be used to eliminate the rows of the border in parallel <ref> [4] </ref>.
Reference: [5] <author> I. S. Duff, </author> <title> Ma28- a set of fortran subroutines for sparse unsymmetric linear equations, </title> <type> Tech. Report Report AERE R8730, </type> <address> HMSO, London, </address> <year> 1977. </year> <title> [6] , Algorithm 575. permutations for a zero-free diagonal, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 7 (1981), </volume> <pages> pp. </pages> <month> 387-390. </month> <title> [7] , On algorithms for obtaining a maximum transversal, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 7 (1981), </volume> <pages> pp. </pages> <month> 315-330. </month> <title> [8] , Parallel implementation of multifrontal schemes, </title> <booktitle> Parallel Computing, 3 (1986), </booktitle> <pages> pp. 193-204. </pages>
Reference-contexts: The stability results so far have compared different versions of the solver to indicate when the stability of the solver has improved. However, it is also important to compare the solver against other known solvers. For these comparisons the solver ma28, <ref> [5] </ref>, was chosen and 80 matrices were solved. ma28 was run with the stability factor (u) at 1.0 (the most stable value) and with a value of 0.1 (which is less stable but allows better control of fill-in). mcsparse was run with the diagonal casting (ff = 10 5 ) and <p> Alliant FX/80 Performance Results and MA28 Comparisons. In this section we give performance results for mcsparse on the Alliant FX/80, and compare its effectiveness against a known sequential sparse solver, ma28 <ref> [5] </ref>. The solution times of the large matrices from the RUA collection for both the mcsparse and ma28 solvers are presented in Table 21. This table contains the user process times for the solutions as collected in single-user mode on the Alliant FX/80.
Reference: [9] <author> I. S. Duff, A. M. Erisman, and J. K. Reid, </author> <title> Direct Methods for Sparse Matrices, </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: The initial phase of the ordering, H0, attempts to adaptively find a weighted transversal. A transversal is defined by row and column permutations which ensure that all of the diagonal elements of the permuted matrix are nonzero. If such permutations do not exist then the matrix is structurally singular <ref> [9] </ref>. When determining this transversal H0 attempts to select elements for the diagonal that will enhance the stability of the factorization. The second phase of the ordering applies Tarjan's Algorithm to determine the strongly connected components of the adjacency graph of the matrix.
Reference: [10] <author> I. S. Duff and J. K. Reid, </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 9 (1983), </volume> <pages> pp. 302-325. </pages>
Reference-contexts: As a result, for unsymmetric systems on a range of parallel architectures it is often necessary to carefully mix a priori static and dynamic runtime decisions. One approach that has been tried for parallel sparse system solvers is the multifrontal scheme <ref> [8, 10] </ref>. A multifrontal scheme constructs an elimination tree to organize the parallel work. A node in the tree represents a certain computation, which may include handling the information from the node's children and performing some pivot eliminations.
Reference: [11] <author> A. M. Erisman, R. G. Grimes, J. G. Lewis, W. G. Poole, and H. D. Simon, </author> <title> Evaluation of orderings for unsymmetric sparse matrices, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8 (1987), </volume> <pages> pp. 600-624. </pages>
Reference-contexts: The effect of these constraints, for unsymmetric problems, can be seen by considering tearing techniques. These have been proposed to expose large-grain structure and parallelism by reordering the matrix into a bordered block triangular matrix <ref> [11, 22] </ref>. This effectively partitions the problem into small subproblems (the diagonal blocks) and then eliminates all connections between the subproblems (the border blocks). Unfortunately, the associated factorization routines are often unable to preserve stability and sparsity without destroying this structure.
Reference: [12] <author> K. Gallivan, B. Marsolf, and H. Wijshoff, </author> <title> A large-grain parallel sparse system solver, </title> <booktitle> in Proc. Fourth SIAM Conf. on Parallel Proc. for Scient. Comp., </booktitle> <address> Chicago, IL, </address> <year> 1989, </year> <pages> pp. 23-28. </pages>
Reference-contexts: The large and medium grain parallelism (parallel subsystems of various sizes) exposed by H* is combined with medium (various parallel row updates strategies) and 2 fine grain (vectorization) parallelism to form a multi grain parallel solver mcsparse which allows adaptation to a wide range of multiprocessor architectures. In <ref> [12] </ref> initial results with mcsparse are presented and more details can be found in [30]. The paper is organized as follows. In Section 2 a global overview of the procedures in mcsparse is given. The details of the ordering H* are presented in Section 3.
Reference: [13] <author> K. Gallivan, A. Sameh, and Z. Zlatev, </author> <title> Parallel hybrid sparse linear system solver, </title> <booktitle> Computing systems in engineering, 1 (1990), </booktitle> <pages> pp. </pages> <month> 183-195. </month> <title> [14] , Solving general sparse linear systems using conjugate gradient-type methods, </title> <booktitle> in Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <address> New York, 1990, </address> <publisher> ACM Press, </publisher> <pages> pp. 132-139. </pages> <month> June 11-15, </month> <year> 1990, </year> <title> Amsterdam, The Netherlands. [15] , Parallel direct method codes for general sparse matrices, </title> <type> Tech. Report CSRD Report No. 1143, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1991. </year> <note> To appear in Proceedings of NATO ASI on Linear Systems. </note> <editor> Bertocchi, Spedicato and Vespucci, eds., </editor> <year> 1991. </year>
Reference-contexts: Initial results, [33], indicate that mcsparse can be adapted to use a combination of positional dropping, i.e., ignoring a fill-in element due to its position in the matrix, and numerical dropping, i.e., ignoring a fill-in element because of its relative magnitude <ref> [14, 13] </ref>, to produce a preconditioner for conjugate gradient-like algorithms. Finally, the techniques used in mcsparse should be considered for use with more conventional approaches to solving systems with tearing techniques, e.g., exploiting the Sherman-Morrison-Woodbury formula.
Reference: [16] <author> C. W. Gear, </author> <title> Numerical errors in sparse linear equations, </title> <type> Tech. Report UIUDCS-F-75-885, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <year> 1975. </year>
Reference-contexts: The extra factor of 2 is due to the two pivots per column. In the case of sparse matrices, the growth factor bound can be reduced significantly and depends upon the sparsity of the matrix <ref> [3, 16] </ref>. When the partial pivoting conditions are relaxed or another pivoting strategy 31 is combined with casting, the bound on the growth factor is easily deduced from modifications to the standard analysis. 5. MCSPARSE Factorization/Solve Description. 5.1. Stability control.
Reference: [17] <author> A. George, </author> <title> Nested dissection of a regular finite element mesh, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 10 (1973), </volume> <pages> pp. </pages> <month> 345-363. </month> <title> [18] , An automatic one-way dissection algorithm for irregular finite element problems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 17 (1980), </volume> <pages> pp. 740-751. </pages>
Reference-contexts: It is also possible to approach the problem of transforming the matrix to block upper triangular form starting from the standard techniques used to produce separator sets for structurally symmetric matrices, e.g., nested dissection <ref> [17, 20] </ref>. The ordering H2 starts with the construction of separator sets of the adjacency matrix of A+ A T as in the standard approaches. For the implementation of H2 we used a straight-forward implementation of automatic nested dissection [19].
Reference: [19] <author> A. George and J. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice Hall, </publisher> <year> 1981. </year>
Reference-contexts: The ordering H2 starts with the construction of separator sets of the adjacency matrix of A+ A T as in the standard approaches. For the implementation of H2 we used a straight-forward implementation of automatic nested dissection <ref> [19] </ref>. However, other initial orderings could have been used such as one-way dissection [18], more sophisticated implementations of automatic nested dissection [27], or the graph bisection heuristics as proposed by [26]. In fact, the initial algorithm used for finding these separator sets does not appear to be very important.
Reference: [20] <author> A. George and J. W. H. Liu, </author> <title> An automatic nested dissection algorithm for irregular finite-element problems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 15 (1978), </volume> <pages> pp. 1053-1069. </pages>
Reference-contexts: It is also possible to approach the problem of transforming the matrix to block upper triangular form starting from the standard techniques used to produce separator sets for structurally symmetric matrices, e.g., nested dissection <ref> [17, 20] </ref>. The ordering H2 starts with the construction of separator sets of the adjacency matrix of A+ A T as in the standard approaches. For the implementation of H2 we used a straight-forward implementation of automatic nested dissection [19].
Reference: [21] <author> F. Gustavson, </author> <title> Finding the block lower triangular form of a matrix, in Sparse Matrix Computations, </title> <note> 60 J. </note> <editor> Bunch and D. Rose, eds., </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: Algorithms for finding set representation [28] or solutions to the assignment problem [25] could be used to find the transversal. An alternative algorithm involves finding maximal matchings in bipartite graphs [23]. The algorithm chosen for the transversal is based on work of Duff and Gustavson <ref> [7, 6, 21] </ref>. The algorithm uses a depth first search of the matrix to determine a series of column interchanges. The algorithm creates a transversal by assigning a unique diagonal position to each column of the matrix. These assignments determine a column permutation which places nonzero elements on the diagonal.
Reference: [22] <author> E. Hellerman and D. C. Rarick, </author> <title> The partitioned preassigned pivot procedure (P 4 ), in Sparse Matrices and their Applications, </title> <editor> D. J. Rose and R. A. Willoughby, eds., </editor> <publisher> Plenum, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: The effect of these constraints, for unsymmetric problems, can be seen by considering tearing techniques. These have been proposed to expose large-grain structure and parallelism by reordering the matrix into a bordered block triangular matrix <ref> [11, 22] </ref>. This effectively partitions the problem into small subproblems (the diagonal blocks) and then eliminates all connections between the subproblems (the border blocks). Unfortunately, the associated factorization routines are often unable to preserve stability and sparsity without destroying this structure.
Reference: [23] <author> J. E. Hopcroft and R. M. Karp, </author> <title> An n 5 2 algorithm for maximum matchings in bipartite graphs, </title> <journal> SIAM J. Comput., </journal> <volume> 2 (1973), </volume> <pages> pp. 225-231. </pages>
Reference-contexts: Algorithms for finding set representation [28] or solutions to the assignment problem [25] could be used to find the transversal. An alternative algorithm involves finding maximal matchings in bipartite graphs <ref> [23] </ref>. The algorithm chosen for the transversal is based on work of Duff and Gustavson [7, 6, 21]. The algorithm uses a depth first search of the matrix to determine a series of column interchanges.
Reference: [24] <author> D. Kuck, E. Davidson, D. Lawrie, and A. Sameh, </author> <title> Parallel supercomputing today and the Cedar approach, </title> <booktitle> Science, 231 (1986), </booktitle> <pages> pp. 967-974. </pages>
Reference-contexts: The value of the border column count is also constrained to prevent the count from becoming larger than the number of rows in the border. 5.3. Implementation. In this section, we discuss the present implementation of mc-sparse for the Cedar multiprocessor <ref> [32, 24, 35] </ref>. It is assumed that both the matrix A and the right-hand side vector b of the system to be solved are available. The solver is composed of two phases.
Reference: [25] <author> H. W. Kuhn, </author> <title> The hungarian method for the assignment problem, </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 2 (1955), </volume> <pages> pp. 83-97. </pages>
Reference-contexts: The Transversal. The transversal ordering is a matching between the columns and the diagonals and could be found using many different algorithms. Algorithms for finding set representation [28] or solutions to the assignment problem <ref> [25] </ref> could be used to find the transversal. An alternative algorithm involves finding maximal matchings in bipartite graphs [23]. The algorithm chosen for the transversal is based on work of Duff and Gustavson [7, 6, 21].
Reference: [26] <author> C. Leiserson and J. Lewis, </author> <title> Orderings for parallel sparse symmetric factorization, </title> <booktitle> in Proc. Third SIAM Conf. on Parallel Proc. for Scient. Comp., </booktitle> <address> Los Angeles, CA., </address> <year> 1987, </year> <pages> pp. 27-31. </pages>
Reference-contexts: For the implementation of H2 we used a straight-forward implementation of automatic nested dissection [19]. However, other initial orderings could have been used such as one-way dissection [18], more sophisticated implementations of automatic nested dissection [27], or the graph bisection heuristics as proposed by <ref> [26] </ref>. In fact, the initial algorithm used for finding these separator sets does not appear to be very important. Nested dissection preserves structural symmetry and therefore results in a matrix with an arrowhead form.
Reference: [27] <author> R. Lipton, D. Rose, and R. Tarjan, </author> <title> Generalized nested dissection, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 16 (1979), </volume> <pages> pp. 346-358. </pages>
Reference-contexts: For the implementation of H2 we used a straight-forward implementation of automatic nested dissection [19]. However, other initial orderings could have been used such as one-way dissection [18], more sophisticated implementations of automatic nested dissection <ref> [27] </ref>, or the graph bisection heuristics as proposed by [26]. In fact, the initial algorithm used for finding these separator sets does not appear to be very important. Nested dissection preserves structural symmetry and therefore results in a matrix with an arrowhead form.
Reference: [28] <author> J. M. Hall, </author> <title> An algorithm for distinct representatives, </title> <journal> The American Mathematical Monthly, </journal> <volume> 63 (1956), </volume> <pages> pp. 716-717. </pages>
Reference-contexts: The Transversal. The transversal ordering is a matching between the columns and the diagonals and could be found using many different algorithms. Algorithms for finding set representation <ref> [28] </ref> or solutions to the assignment problem [25] could be used to find the transversal. An alternative algorithm involves finding maximal matchings in bipartite graphs [23]. The algorithm chosen for the transversal is based on work of Duff and Gustavson [7, 6, 21].
Reference: [29] <author> H. M. Markowitz, </author> <title> The elimination form of the inverse and its application to linear programming, </title> <booktitle> Management Science, 3 (1957), </booktitle> <pages> pp. 255-269. </pages>
Reference-contexts: An important part of any sparse solver is the algorithm for controlling the amount of fillin that is generated. Most sequential sparse matrix packages, such as MA28, used a simple strategy proposed by Markowitz <ref> [29] </ref>.
Reference: [30] <author> B. Marsolf, </author> <title> Large grain parallel sparse system solver, </title> <type> Tech. Report CSRD Report No. 1125, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1991. </year> <type> Master Thesis. </type>
Reference-contexts: In [12] initial results with mcsparse are presented and more details can be found in <ref> [30] </ref>. The paper is organized as follows. In Section 2 a global overview of the procedures in mcsparse is given. The details of the ordering H* are presented in Section 3. Casting is introduced and discussed from an algebraic point of view in Section 4.
Reference: [31] <author> O. Osterby and Z. Zlatev, </author> <title> Direct methods for sparse matrices, </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1983. </year>
Reference-contexts: The stability and sparsity tests for pivot selection are often contradictory and most strategies involve some combination of the two, e.g., the generalized Markowitz strategy, <ref> [31] </ref>. Parallel solvers add a third constraint to pivot selection. For the medium and fine grain algorithms mentioned above, these three constraints can be considered in a reasonably straightforward way potentially with respect to the entire active portion of the matrix.
Reference: [32] <author> Staff, </author> <title> The Cedar Project, </title> <type> Tech. Report CSRD Report No. 1122, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1991. </year>
Reference-contexts: The value of the border column count is also constrained to prevent the count from becoming larger than the number of rows in the border. 5.3. Implementation. In this section, we discuss the present implementation of mc-sparse for the Cedar multiprocessor <ref> [32, 24, 35] </ref>. It is assumed that both the matrix A and the right-hand side vector b of the system to be solved are available. The solver is composed of two phases.
Reference: [33] <author> X. Wang, </author> <title> private communication, </title> <month> April </month> <year> 1991. </year>
Reference-contexts: Further work in this area has already shown that the effectiveness H1 ordering can be improved by removing rows which are above a specified density threshold as is done in the H2 ordering <ref> [33] </ref>. The H2 ordering is applied recursively until the diagonal block size is reduced to a specified level. As a result, the size of the largest diagonal block for the H2 ordering may be selected when the test is run. <p> A parallel implementation of the H* ordering would improve further the overall performance of mcsparse. The code could be adapted to map its multilevel parallelism onto other multivec 59 tor processors and to exploit their architectures efficiently. Initial results, <ref> [33] </ref>, indicate that mcsparse can be adapted to use a combination of positional dropping, i.e., ignoring a fill-in element due to its position in the matrix, and numerical dropping, i.e., ignoring a fill-in element because of its relative magnitude [14, 13], to produce a preconditioner for conjugate gradient-like algorithms.
Reference: [34] <author> H. A. G. Wijshoff, </author> <title> Symmetric orderings for unsymmetric sparse matrices, </title> <type> Tech. Report CSRD Report No. 901, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1989. </year>
Reference-contexts: The structure of the matrix after the application of the permutations is a bordered block upper triangular matrix. Further, the rows of the border are sorted based on the column index of their leftmost nonzero entry. A preliminary algorithmic description of the H* ordering can be found in <ref> [34] </ref>. 2.2. Matrix Structure. The structure of the reordered matrix is shown in Figure 1. Note that a block upper triangular form is assumed without losing generality. The interaction of diagonal blocks D 1 through D m is confined to the off-diagonal blocks C 1 through C m1 .
Reference: [35] <author> P. Yew, </author> <title> Architecture of the Cedar parallel supercomputer, </title> <type> Tech. Report CSRD Report No. 609, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1986. </year> <month> 61 </month>
Reference-contexts: The value of the border column count is also constrained to prevent the count from becoming larger than the number of rows in the border. 5.3. Implementation. In this section, we discuss the present implementation of mc-sparse for the Cedar multiprocessor <ref> [32, 24, 35] </ref>. It is assumed that both the matrix A and the right-hand side vector b of the system to be solved are available. The solver is composed of two phases.
References-found: 29

