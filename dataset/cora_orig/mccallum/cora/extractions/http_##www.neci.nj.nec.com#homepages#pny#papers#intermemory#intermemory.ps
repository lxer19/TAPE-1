URL: http://www.neci.nj.nec.com/homepages/pny/papers/intermemory/intermemory.ps
Refering-URL: http://www.neci.nj.nec.com/homepages/pny/papers/intermemory/main.html
Root-URL: 
Title: Towards an Archival Intermemory  
Author: Andrew V. Goldberg and Peter N. Yianilos 
Keyword: Archival Storage, Distributed Redundant Databases, Electronic Publishing, Distributed Algorithms, Error Correcting Codes, Erasure-Resilient Codes, Information Dispersal Algorithm, Digital Library, Internet.  
Address: 4 Independence Way Princeton, NJ 08540  
Affiliation: NEC Research Institute, Inc.  
Date: 1998  
Note: Advances in Digital Libraries ADL'98, April 22-24, Santa Barbara, California,  
Abstract: We propose a self-organizing archival Intermem-ory. That is, a noncommercial subscriber-provided distributed information storage service built on the existing Internet. Given an assumption of continued growth in the memory's total size, a subscriber's participation for only a finite time can nevertheless ensure archival preservation of the subscriber's data. Information disperses through the network over time and memories become more difficult to erase as they age. The probability of losing an old memory given random node failures is vanishingly small and an adversary would have to corrupt hundreds of thousands of nodes to destroy a very old memory. This paper presents a framework for the design of an Intermemory, and considers certain aspects of the design in greater detail. In particular, the aspects of addressing, space efficiency, and redundant coding are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agrawal and P. Jalote. </author> <title> Coding-based replication schemes for distributed systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(3) </volume> <pages> 240-251, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: So while this work is clearly related to our problem it does not appear to be directly relevant. The topic of [9] is a discussion, in general terms, of several design issues and tradeoffs relating to the implementation of redundant distributed databases. Similar issues are discussed in <ref> [1] </ref> and both papers cite Rabin's work [13], which introduced the idea of coding-based redundancy to the database community. He in turn recognizes the earlier related work [5]. Odlyzko is also interested in perpetual storage [11], where his focus is on preservation of and access to research literature.
Reference: [2] <author> N. Alon and M. Luby. </author> <title> A linear time erasure-resilient code with nearly optimal recovery. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(6) </volume> <pages> 1732-1736, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: Since these algorithms are also efficient in practice, encoding and decoding complexity does not limit our choice of N . We remark that more complex coding schemes <ref> [2] </ref> might be considered that are asymptotically faster yet but require slightly more than N words to reconstruct the original. Also, the topic of reconstruction given some number of erroneous values has been considered in the literature.
Reference: [3] <author> R. J. Anderson. </author> <title> The eternity service. </title> <booktitle> In Pragocrypt 96, </booktitle> <pages> pages 242-252. </pages> <publisher> CTU Publishing, </publisher> <year> 1996. </year>
Reference-contexts: It is worthwhile noting that Intermemory solves the preservation problem associated with the ephemeral nature of computer storage media. As new subscribers replace old ones, memories are automatically copied onto the latest medium. Anderson's work <ref> [3] </ref> parallels our own, but his emphasis is on the freedom-of-expression and individual rights rationale, and less on the specific technical and architectural issues involved. The problem of efficiently distributing a file within a distributed system having connectivity described by a given undirected graph is considered in [10].
Reference: [4] <author> T. E. Anderson, M. D. Dahlin, J. M. Neefe, D. A. Patterson, D. S. Roselli, and R. Y. Wang. </author> <title> Serverless network file systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(1) </volume> <pages> 41-79, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Redundancy is provided by variations of a simple parity approach. Further distributing not only the storage, but also the control of a file system, is the subject of numerous papers including <ref> [4] </ref>, which describes the xFS system now under development. This system distributes a file system over cooperating workstations while retaining high performance.
Reference: [5] <author> C. A. Asmuth and G. R. Blakley. </author> <title> Pooling splitting and restituting information to overcome total failure of some channels of communications. </title> <booktitle> In Proceedings of the 1982 Symposium on Security and Privacy, </booktitle> <pages> pages 156-169, </pages> <address> New York, 1982. </address> <publisher> IEEE Society. </publisher>
Reference-contexts: Similar issues are discussed in [1] and both papers cite Rabin's work [13], which introduced the idea of coding-based redundancy to the database community. He in turn recognizes the earlier related work <ref> [5] </ref>. Odlyzko is also interested in perpetual storage [11], where his focus is on preservation of and access to research literature. The redundant array of inexpensive disks (RAID) approach [12] distributes a file system over an array of disks directly attached to a host computer. <p> and perhaps other functions built on top of the simple substrate we consider | as well as interpretation of specific data types (such as 2 by a French engineering officer in August of 1799 while strengthening a fort located on the west bank of the Rosetta arm of the Nile <ref> [5] </ref>. It records a bilingual decree promulgated by the whole of the priesthood of Egypt in the ninth year of the reign of Ptolemy V (B.C. 196).
Reference: [6] <author> E. R. Berlekamp. </author> <title> Algebraic Coding Theory. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: These words may then be regarded as the coefficients of a polynomial of degree N 1 over F . Then the value assumed by this polynomial at any N distinct points suffice to uniquely identify it. This classical observation is a key idea in coding theory <ref> [6] </ref> and corresponds to the Vandermonde matrix case of Rabin's information dispersal framework [13] in the extreme setting where each block contains a single word.
Reference: [7] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms, chapter 32. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The second level of replication takes four times the original block size. The total memory requirement is nine times the original block size. When the dispersal is completed, over 2 18 processors need to be disabled for the data to be lost. one may perform both evaluation <ref> [7] </ref> and interpolation in O (N log 2 N ) time. Since these algorithms are also efficient in practice, encoding and decoding complexity does not limit our choice of N .
Reference: [8] <author> B. Kahle. </author> <title> Preserving the internet. </title> <publisher> Scientific American, </publisher> <pages> pages 82-83, </pages> <month> March </month> <year> 1997. </year>
Reference-contexts: Web-indexing approaches dispense with the second role and allow a user to view all the world has to offer. They fail, however, to deal with the first. Creating a record of everything on the Internet <ref> [8] </ref> represents, by contrast, an emphasis on the first role and eliminates entirely the second. Our concept of In-termemory combines the archival function with what amounts to a self-selecting publication process. It is worthwhile noting that Intermemory solves the preservation problem associated with the ephemeral nature of computer storage media.
Reference: [9] <author> R. Mukkamala. </author> <title> Storage efficient and secure replicated distributed databases. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 6(2), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: By contrast, we assume that the world network provides a complete graph. So while this work is clearly related to our problem it does not appear to be directly relevant. The topic of <ref> [9] </ref> is a discussion, in general terms, of several design issues and tradeoffs relating to the implementation of redundant distributed databases. Similar issues are discussed in [1] and both papers cite Rabin's work [13], which introduced the idea of coding-based redundancy to the database community.
Reference: [10] <author> M. Naor and R. M. Roth. </author> <title> Optimal file sharing in distributed networks. </title> <journal> SIAM Journal on Computing, </journal> <volume> 24(1) </volume> <pages> 158-183, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Anderson's work [3] parallels our own, but his emphasis is on the freedom-of-expression and individual rights rationale, and less on the specific technical and architectural issues involved. The problem of efficiently distributing a file within a distributed system having connectivity described by a given undirected graph is considered in <ref> [10] </ref>. That is, a node can reconstruct data from its neighbors. This idea is related to that of diversity coding (see [14] for recent work) in which there are multiple encoders and each of several decoders has access to some fixed subset of the encoders.
Reference: [11] <author> A. Odlyzko. </author> <title> Tragic loss or good riddance? the impending demise of traditional scholarly journals. </title> <journal> Notices Amer. Math. Soc., </journal> <month> January </month> <year> 1995. </year>
Reference-contexts: Similar issues are discussed in [1] and both papers cite Rabin's work [13], which introduced the idea of coding-based redundancy to the database community. He in turn recognizes the earlier related work [5]. Odlyzko is also interested in perpetual storage <ref> [11] </ref>, where his focus is on preservation of and access to research literature. The redundant array of inexpensive disks (RAID) approach [12] distributes a file system over an array of disks directly attached to a host computer. Redundancy is provided by variations of a simple parity approach.
Reference: [12] <author> D. A. Patterson, G. Gibson, and H. Katz. </author> <title> A case for redundant arrays of inexpensive disks (raid). </title> <booktitle> In Proc. ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: He in turn recognizes the earlier related work [5]. Odlyzko is also interested in perpetual storage [11], where his focus is on preservation of and access to research literature. The redundant array of inexpensive disks (RAID) approach <ref> [12] </ref> distributes a file system over an array of disks directly attached to a host computer. Redundancy is provided by variations of a simple parity approach.
Reference: [13] <author> M. O. Rabin. </author> <title> Efficient dispersal of information for security, load balancing, and fault tolerance. </title> <journal> Journal of the ACM, </journal> <volume> 36(2) </volume> <pages> 335-348, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The topic of [9] is a discussion, in general terms, of several design issues and tradeoffs relating to the implementation of redundant distributed databases. Similar issues are discussed in [1] and both papers cite Rabin's work <ref> [13] </ref>, which introduced the idea of coding-based redundancy to the database community. He in turn recognizes the earlier related work [5]. Odlyzko is also interested in perpetual storage [11], where his focus is on preservation of and access to research literature. <p> Then the value assumed by this polynomial at any N distinct points suffice to uniquely identify it. This classical observation is a key idea in coding theory [6] and corresponds to the Vandermonde matrix case of Rabin's information dispersal framework <ref> [13] </ref> in the extreme setting where each block contains a single word.
Reference: [14] <author> J. R. Roche, R. W. Yeung, and K. P. Hau. </author> <title> Symmetrical multilevel diversity coding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 43(3) </volume> <pages> 1059-1064, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: The problem of efficiently distributing a file within a distributed system having connectivity described by a given undirected graph is considered in [10]. That is, a node can reconstruct data from its neighbors. This idea is related to that of diversity coding (see <ref> [14] </ref> for recent work) in which there are multiple encoders and each of several decoders has access to some fixed subset of the encoders. By contrast, we assume that the world network provides a complete graph.
Reference: [15] <author> J. Rothenberg. </author> <title> Ensuring the longevity of digital documents. </title> <publisher> Scientific American, </publisher> <pages> pages 42-47, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Specific designs for distributed redundant data storage. 4. A specific approach to the addressing problem. The growing interest in digital libraries was recently surveyed [16] and a general discussion of the problem of preserving digital documents is contained in <ref> [15] </ref>. We observe that libraries serve two distinct roles: maintenance of a historical record, and selection of appropriate materials. Web-indexing approaches dispense with the second role and allow a user to view all the world has to offer. They fail, however, to deal with the first.
Reference: [16] <author> B. Schatz and H. Chen. </author> <title> Building large-scale digital libraries. </title> <journal> Computer, </journal> <volume> 29(5) </volume> <pages> 22-26, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: General development of the notion of Intermemory and its architecture. 2. The idea of trading a finite-term donation for un bounded storage rights. 3. Specific designs for distributed redundant data storage. 4. A specific approach to the addressing problem. The growing interest in digital libraries was recently surveyed <ref> [16] </ref> and a general discussion of the problem of preserving digital documents is contained in [15]. We observe that libraries serve two distinct roles: maintenance of a historical record, and selection of appropriate materials.
Reference: [17] <author> N. J. A. Sloane. </author> <title> Proposal for an internet service: The eternal home page. </title> <type> Technical report, </type> <institution> AT&T Labs - Information Sciences Research Center, </institution> <note> 1996 (Revised 1997). </note>
Reference-contexts: The data would be widely distributed, and financial reserves established to care for it in the future. Sloan suggests a simpler alternative in <ref> [17] </ref>; that such a service might be provided by a single major university or other institution. These approaches are, however, inconsistent with the essential spirit of the Internet, i.e. growth and effectiveness in the absence of strong central control or commercial organization.
References-found: 17

