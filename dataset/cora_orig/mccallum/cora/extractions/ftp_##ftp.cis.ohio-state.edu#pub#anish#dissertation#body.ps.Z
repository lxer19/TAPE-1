URL: ftp://ftp.cis.ohio-state.edu/pub/anish/dissertation/body.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~anish/pub.html
Root-URL: http://www.cis.ohio-state.edu
Note: Chapter 1  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> T. Anderson and P. A. Lee, </author> <title> "Fault tolerance terminology proposals", </title> <booktitle> Proceedings of FTCS-12 (1982), </booktitle> <pages> pp. 29-33. </pages>
Reference-contexts: One is based on a distinction between the notions of faults, errors, and failures: faults in a physical domain can cause errors in an information domain, whereas errors in an information domain can 1 2 cause failures in an external domain <ref> [1, 10, 41] </ref>. (Unfortunately, these notions are subjective: "what one person call a failure, a second person calls a fault, and a third person might call an error" [18].) The other is based on what type of fault is tolerated, for example, stuck-at, crash, fail-stop, omission, timing, or byzantine faults [27,
Reference: [2] <author> Y. Afek, B. Awerbuch, and E. Gafni, </author> <title> "Applying static network protocols to dynamic networks", </title> <booktitle> Proceedings of 28th IEEE Symposium on Foundations of Computer Science (1987). </booktitle>
Reference-contexts: The design consists of three major components: a leader election, a spanning tree construction, and a diffusing computation. Each of these components is global stabilizing, tolerates process and channel failures and repairs, and admits bounded-space implementations. These features distinguish our design of these components from earlier designs <ref> [2, 26, 25] </ref> and redress the following comment made by Lamport and Lynch [39, page 1193]: "A [global] stabilizing algorithm [that translates a distributed system designed for a fixed but arbitrary network into one that works for a changing network] using a finite number of identifiers would be quite useful, but
Reference: [3] <author> A. Arora, P. C. Attie, M. Evangelist, and M. G. Gouda, </author> <title> "Convergence of iteration systems", Distributed Computing, to appear, 1992; extended abstract in: </title> <booktitle> Proceedings of Concur'90: Theories of Concurrency, Lecture Notes in Computer Science 458, </booktitle> <publisher> Springer-Verlag (1990), </publisher> <pages> pp. 70-82. </pages>
Reference-contexts: Towards meeting this obligation, we develop below a little theory. (We have developed a more elaborate theory for proving convergence elsewhere <ref> [3] </ref>). We build a directed graph that has one edge for each constraint in S. Edges are directed as follows: Let P and Q be constraints in S. <p> Second, our design remains correct even if we weaken the interleaving requirement as follows: in each step, an arbitrary subset of the processes can each execute some enabled action as long as no two executed actions access the same shared variable <ref> [3, 15] </ref>. Issues for further investigation include: transformation of our read-write atomicity programs into message-passing programs while preserving fault-tolerance, analysis of the communication complexity of the resulting message-passing programs, and design of a mechanism for maintaining a timely and consistent state of neighboring process indices.
Reference: [4] <author> A. Arora, S. Dolev and M. G. Gouda, </author> <title> "Maintaining digital clocks in step", </title> <journal> Parallel Processing Letters, </journal> <volume> 1(1) (1991), </volume> <pages> pp. </pages> <booktitle> 11-18; extended abstract in: Proceedings of 5th International Workshop on Distributed Algorithms, Delphi, </booktitle> <address> Greece (1991). </address>
Reference-contexts: Thus, S 0 is not closed under the execution of actions in C-maj-element and F 2. Hence, C-maj-element is not masking F 2-tolerant for S 0 . 37 3.6 Maintaining Digital Clocks in Step Specification <ref> [4] </ref> Consider an undirected and connected graph.
Reference: [5] <author> A. Arora and M. G. Gouda, </author> <title> "Closure and convergence: A formulation of fault-tolerant computing", </title> <booktitle> Proceedings of the 22nd International Symposium on Fault-Tolerant Computing (1992), to appear. </booktitle>
Reference: [6] <author> A. Arora and M. G. Gouda, </author> <title> "Distributed reset", revised for IEEE Transactions on Computers; extended abstract in: </title> <booktitle> Proceedings of the 10th Conference on 127 128 Foundations of Software Technology and Theoretical Computer Science, Lecture Notes in Computer Science 472, </booktitle> <publisher> Springer-Verlag (1990), </publisher> <pages> pp. 316-331. </pages>
Reference-contexts: Even so, self-stabilizing systems are mainly being designed to tolerate arbitrary transient faults, whereas they can be designed to tolerate a variety of fault types <ref> [6, 36, 60] </ref>. <p> The first of these programs maintains a diffusing computation, and is a simplified version of a program in <ref> [6] </ref>. The remaining two are token ring programs, due originally to Dijkstra [20]. 4.3.1 Stabilizing Diffusing Computations Specification : Consider a finite out-tree. <p> The augmentation does not introduce new processes or new communication channels to the system. It merely introduces 68 69 additional modules to the existing processes. The added modules, communicating with one another over existing channels, comprise what we call a reset subsystem <ref> [6] </ref>. 6.1 Distributed Reset Ideally, resetting a distributed system to a given global state implies resuming the execution of the system starting from the given state. With this characterization, however, each reset of a distributed system can be achieved only by a "global freeze" of the system.
Reference: [7] <author> A. Arora and M. G. Gouda, </author> <title> "Load balancing: An exercise in constrained convergence", </title> <note> submitted for publication. </note>
Reference: [8] <author> A. Arora, M. G. Gouda, and T. Herman, </author> <title> "Composite routing protocols", </title> <booktitle> Proceedings of the 2nd IEEE Symposium on Parallel and Distributed Processing (1990), </booktitle> <pages> pp. 70-78. </pages>
Reference-contexts: It will therefore come as no surprise to our reader that the notions of closure and convergence have applications other than fault-tolerance. For example, we have shown in joint work with Mohamed Gouda and Ted Herman <ref> [8] </ref> how closure and convergence can be used to design composite routing protocols for communication networks. In this case, the perturbing actions correspond to changes in network characteristics, e.g. communication delay, availability, waiting times for service, etc.
Reference: [9] <author> A. Arora, M. G. Gouda, and G. Varghese, </author> <title> "Distributed constraint satisfaction", </title> <note> submitted for publication. </note>
Reference-contexts: Given T , closure and convergence actions can then be designed under the assumption that T always holds, and with the obligation that executing these actions does not violate T . We conclude with a nice observation pointed out to us by George Varghese <ref> [9] </ref>: Let p be a program whose graph is an out-tree and whose actions can only violate the constraints of edges associated with some node in the out-tree. Then p can be translated into an equivalent global stabilizing program.
Reference: [10] <author> A. Avizienis, </author> <title> "The four-universe information system model for the study of fault tolerance", </title> <booktitle> Proceedings of 12th International Symposium on Fault-Tolerant Computing (1982), </booktitle> <pages> pp. 6-13. </pages>
Reference-contexts: One is based on a distinction between the notions of faults, errors, and failures: faults in a physical domain can cause errors in an information domain, whereas errors in an information domain can 1 2 cause failures in an external domain <ref> [1, 10, 41] </ref>. (Unfortunately, these notions are subjective: "what one person call a failure, a second person calls a fault, and a third person might call an error" [18].) The other is based on what type of fault is tolerated, for example, stuck-at, crash, fail-stop, omission, timing, or byzantine faults [27,
Reference: [11] <author> F. B. Bastani, I.-L. Yen, and I.-R. Chen, </author> <title> "A class of inherently fault-tolerant distributed programs", </title> <journal> IEEE Transactions on Software Engg. </journal> <volume> 14(10) (1988), </volume> <pages> pp. 1431-1442. </pages>
Reference-contexts: While self-stabilization was first studied in computing science in 1973 [20], and its application to fault-tolerance was strongly endorsed in 1983 [37], it is only in the last few years that concerted efforts have been made to relate self-stabilization to fault-tolerance <ref> [11, 13, 16] </ref>. Even so, self-stabilizing systems are mainly being designed to tolerate arbitrary transient faults, whereas they can be designed to tolerate a variety of fault types [6, 36, 60].
Reference: [12] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman, </author> <title> Concurrency Control and Recovery in Database Systems, Chapter 7, </title> <publisher> Addison-Wesley (1987). </publisher>
Reference-contexts: Each problem is presented as follows. First, we specify the problem. We then exhibit a program and a set of fault actions, and prove that the program both meets its specification and tolerates its set of fault actions. 12 13 3.1 Atomic Commitment Specification <ref> [12] </ref> Each process casts one of two votes, Yes or No, and subsequently reaches one of two decisions, Commit or Abort. It is required that: 1. If no faults occur and all processes vote Yes, all processes reach a Commit decision. 2.
Reference: [13] <author> G. M. Brown, M. G. Gouda, and C.-L. Wu, </author> <title> "Token systems that self-stabilize", </title> <journal> IEEE Transactions on Computers 38(6) (1989), </journal> <pages> pp. 845-852. </pages>
Reference-contexts: While self-stabilization was first studied in computing science in 1973 [20], and its application to fault-tolerance was strongly endorsed in 1983 [37], it is only in the last few years that concerted efforts have been made to relate self-stabilization to fault-tolerance <ref> [11, 13, 16] </ref>. Even so, self-stabilizing systems are mainly being designed to tolerate arbitrary transient faults, whereas they can be designed to tolerate a variety of fault types [6, 36, 60].
Reference: [14] <author> M. Breuer and A. Friedman, </author> <title> Diagnosis and Reliable Design of Digital Systems, </title> <publisher> Computer Science Press (1976). </publisher>
Reference-contexts: One consequence of this tradition is that several subdisciplines of fault-tolerant computing have emerged that are apparently unrelated to each other: these subdisciplines deal with specific classes of faults, employ distinct models and design methods, and have their own terminology and classification <ref> [14, 40, 58] </ref>. As a result, the discipline itself appears to be fragmented. Another consequence of this tradition is that verification of fault-tolerant systems is often based on implementation-specific artifacts|such as stable storage, timeouts, and shadow registers|without explicitly specifying what properties of these artifacts are necessary.
Reference: [15] <author> J. E. Burns, M. G. Gouda, and R. E. Miller, </author> <title> "On relaxing interleaving assumptions", </title> <type> Technical Report GIT-ICS-88/29, </type> <institution> School of ICS, Georgia Institute of Technology. </institution>
Reference-contexts: Second, our design remains correct even if we weaken the interleaving requirement as follows: in each step, an arbitrary subset of the processes can each execute some enabled action as long as no two executed actions access the same shared variable <ref> [3, 15] </ref>. Issues for further investigation include: transformation of our read-write atomicity programs into message-passing programs while preserving fault-tolerance, analysis of the communication complexity of the resulting message-passing programs, and design of a mechanism for maintaining a timely and consistent state of neighboring process indices.
Reference: [16] <author> J. E. Burns and J. Pachl, </author> <title> "Uniform stabilizing rings," </title> <booktitle> ACM Transactions on Programming Languages and Systems 11(2) (1989), </booktitle> <pages> pp. 330-344. 129 </pages>
Reference-contexts: While self-stabilization was first studied in computing science in 1973 [20], and its application to fault-tolerance was strongly endorsed in 1983 [37], it is only in the last few years that concerted efforts have been made to relate self-stabilization to fault-tolerance <ref> [11, 13, 16] </ref>. Even so, self-stabilizing systems are mainly being designed to tolerate arbitrary transient faults, whereas they can be designed to tolerate a variety of fault types [6, 36, 60].
Reference: [17] <author> K. M. Chandy and J. Misra, </author> <title> Parallel Program Design: A Foundation, </title> <publisher> Addison-Wesley (1988). </publisher>
Reference-contexts: One process g is distinguished, and has associated with it a boolean value B. It is required that: 1. If g is Reliable, the decision value of each Reliable process is B. 2. All Reliable processes eventually reach the same decision. Faults may make Reliable processes Unreliable. Program <ref> [17, 53] </ref> We assume authenticated communication: messages sent by Reliable processes are correctly received by Reliable processes, and Unreliable processes do not forge messages on behalf of Reliable processes. Agreement is reached within N +1 rounds of communication, where N is the maximum number of processes that can be Unreliable. <p> true ! d r :j := d r1 :j _ (c r :j:fl r ^ c r :j:g) [] :b:j ! d r :j := ? end 26 Faults : F = f (sum k : :b:k : 1) &lt; N ^ b:j ! b:j := f alse g Proof <ref> [17] </ref> Let S = (sum j : :b:j : 1) N b:j ) (j = g ) d 0 :j = B) ^ (j 6= g ) :d 0 :j) ^ :c 0 :j:k ^ b:j ^ 0 &lt; q r ) c q :k:j ) d q :j We show
Reference: [18] <author> F. Cristian, </author> <title> "Understanding fault-tolerant distributed systems", </title> <booktitle> Communications of the ACM 34(2) (1991), </booktitle> <pages> pp. 56-78. </pages>
Reference-contexts: can cause errors in an information domain, whereas errors in an information domain can 1 2 cause failures in an external domain [1, 10, 41]. (Unfortunately, these notions are subjective: "what one person call a failure, a second person calls a fault, and a third person might call an error" <ref> [18] </ref>.) The other is based on what type of fault is tolerated, for example, stuck-at, crash, fail-stop, omission, timing, or byzantine faults [27, 45, 49, 52].
Reference: [19] <author> F. Cristian, </author> <title> "A rigorous approach to fault-tolerant programming", </title> <journal> IEEE Transactions on Software Engg. </journal> <month> 11(1) </month> <year> (1985). </year>
Reference-contexts: A few efforts have also been made to formally define and verify system fault-tolerance <ref> [19, 44, 45, 49] </ref>, but these efforts have been limited in scope. Specifically, they have considered systems that recover from the occurrence of faults, and terminate properly. In other words, they have considered systems whose input-output relation masks faults. <p> Following this method, we require that for each fault-tolerant system there exist a predicate S that is invariant throughout fault-free system execution. Next, we observe that faults|be they stuck-at, crash, fail-stop, omission, timing, or byzantine|can be systematically represented as actions that upon execution perturb the system state <ref> [19] </ref>. Consider, for example, a wire that can potentially be stuck-at-low-voltage. Such a wire can be represented by the following program. Let in and out be two variables that range over f0; 1g, and let broken be a boolean variable.
Reference: [20] <author> E. W. Dijkstra, </author> <title> "Self-stabilizing systems in spite of distributed control", </title> <booktitle> Communications of the ACM 17(11) (1974). </booktitle>
Reference-contexts: An extreme form of fault-tolerance that does not always mask faults is self-stabilization. While self-stabilization was first studied in computing science in 1973 <ref> [20] </ref>, and its application to fault-tolerance was strongly endorsed in 1983 [37], it is only in the last few years that concerted efforts have been made to relate self-stabilization to fault-tolerance [11, 13, 16]. <p> The first of these programs maintains a diffusing computation, and is a simplified version of a program in [6]. The remaining two are token ring programs, due originally to Dijkstra <ref> [20] </ref>. 4.3.1 Stabilizing Diffusing Computations Specification : Consider a finite out-tree. It is required to design a stabilizing program in which, upon starting from a state where all tree nodes are colored green, the root node initiates a diffusing computation.
Reference: [21] <author> E. W. Dijkstra, </author> <title> A Discipline of Programming, </title> <publisher> Prentice-Hall (1976). </publisher>
Reference-contexts: Our definition consists of two conditions: one of closure and another of convergence. To motivate the closure condition, let us observe that a well-established method for verifying fault-free systems is to exhibit a predicate that is true throughout system execution <ref> [21, 31] </ref>. Such an "invariant" predicate identifies the "legal" system states, and asserts that the set of legal states is closed under system execution. Following this method, we require that for each fault-tolerant system there exist a predicate S that is invariant throughout fault-free system execution.
Reference: [22] <author> E. W. Dijkstra, </author> <title> "Solution of a problem in concurrent programming control", </title> <booktitle> Communications of the ACM 17(11) (1965), </booktitle> <pages> pp. 569. </pages>
Reference-contexts: Proof: Let p be an arbitrary read-write program for mutual exclusion, and let S be the intended domain of execution of p. That is, S is a closed state predicate of p such that all computations of p starting in S satisfy the following two properties <ref> [22] </ref>. * Safety : at most one process is "privileged" at each state in the computation, and * Deadlock-Freedom : if the computation starts at a state where some process has requested the privilege, then there exists a subsequent state in the computation where some process that previously requested the privilege
Reference: [23] <author> E. W. Dijkstra, </author> <title> "Hierarchical ordering of processes," in Operating Systems Techniques, </title> <publisher> Academic Press (1972), </publisher> <pages> pp. 72-93. </pages>
Reference: [24] <author> E. W. Dijkstra and C. S. Scholten, </author> <title> Predicate Calculus and Program Semantics, </title> <publisher> Springer-Verlag (1990). </publisher>
Reference-contexts: S ) T s = f definition of T s g S ) (8T : T ) = f predicate calculus g (8T : S ) T ) true T s is closed in F = f definition of closed, expressed in terms of weakest precondition, wp, <ref> [24] </ref> g (8 B ! st in F : T s ) wp:(B ! st):T s) = f definition of T s g (8 B ! st in F : (8T : T ) ) wp:(B ! st):(8T : T )) = f wp:(B ! st) is universally conjunctive if st <p> ! st in F : (8T : T ) ) wp:(B ! st):(8T : T )) = f wp:(B ! st) is universally conjunctive if st always terminates g (8 B ! st in F : (8T : T ) ) (8T : wp:(B ! st):T )) ( f Leibniz <ref> [24] </ref> g (8 B ! st in F : (8T : T ) wp:(B ! st):T )) = f T is closed in F , for all T g true T s converges to S in p since: (i) S is closed in p, (ii) T s is closed in p <p> predicate calculusg (9T : S ) T ) true T w is closed in F = f definition of closed g (8 B ! st in F : T w ) wp:(B ! st):T w) ( f T ) T w for all T , wp:(B ! st) is monotonic <ref> [24] </ref>, predicate calculus g (8 B ! st in F : T w ) (9T : wp:(B ! st):T )) = f definition of T w g (8 B ! st in F : (9T : T ) ) (9T : wp:(B ! st):T )) = f predicate calculus g (8
Reference: [25] <author> E. W. Dijkstra, and C. S. Scholten, </author> <title> "Termination detection for diffusing computations", </title> <booktitle> Information Processing Letters 11(1) (1980), </booktitle> <pages> pp. 1-4. </pages>
Reference-contexts: The design consists of three major components: a leader election, a spanning tree construction, and a diffusing computation. Each of these components is global stabilizing, tolerates process and channel failures and repairs, and admits bounded-space implementations. These features distinguish our design of these components from earlier designs <ref> [2, 26, 25] </ref> and redress the following comment made by Lamport and Lynch [39, page 1193]: "A [global] stabilizing algorithm [that translates a distributed system designed for a fixed but arbitrary network into one that works for a changing network] using a finite number of identifiers would be quite useful, but <p> We have not made this assumption: if a root process fails, then the remaining up processes elect a new root. 84 6.4 The Wave Layer As outlined in Section 6.2, the task of the wave layer is to perform a diffusing computation <ref> [25] </ref> in which each appl:i module resets its state. The diffusing computation uses the spanning tree maintained by the tree layer, and consists of three phases.
Reference: [26] <author> S. Dolev, A. Israeli, and S. Moran, </author> <title> "Self-stabilization of dynamic systems assuming only read-write atomicity", </title> <booktitle> Proceedings of the 9th ACM Symposium on Principles of Distributed Computing (1990), </booktitle> <pages> pp. 91-101. </pages>
Reference-contexts: The design consists of three major components: a leader election, a spanning tree construction, and a diffusing computation. Each of these components is global stabilizing, tolerates process and channel failures and repairs, and admits bounded-space implementations. These features distinguish our design of these components from earlier designs <ref> [2, 26, 25] </ref> and redress the following comment made by Lamport and Lynch [39, page 1193]: "A [global] stabilizing algorithm [that translates a distributed system designed for a fixed but arbitrary network into one that works for a changing network] using a finite number of identifiers would be quite useful, but <p> This restriction is too severe for our purpose, and we have lifted it by designing the tree layer to be global stabilizing; i.e., insensitive to the initial state. We note that a global stabilizing spanning tree algorithm has been recently described in <ref> [26] </ref>. However, the algorithm in [26] is based on the simplifying assumption that, at all times, there exists a special process which knows that it is the root. <p> This restriction is too severe for our purpose, and we have lifted it by designing the tree layer to be global stabilizing; i.e., insensitive to the initial state. We note that a global stabilizing spanning tree algorithm has been recently described in <ref> [26] </ref>. However, the algorithm in [26] is based on the simplifying assumption that, at all times, there exists a special process which knows that it is the root.
Reference: [27] <author> P. Ezhilchelvan and S. K. Shrivastava, </author> <title> "A characterization of faults in systems", </title> <booktitle> Proceedings of the 5th Symposium on Reliability in Distributed Software and Database Systems (1986). </booktitle> <pages> 130 </pages>
Reference-contexts: [1, 10, 41]. (Unfortunately, these notions are subjective: "what one person call a failure, a second person calls a fault, and a third person might call an error" [18].) The other is based on what type of fault is tolerated, for example, stuck-at, crash, fail-stop, omission, timing, or byzantine faults <ref> [27, 45, 49, 52] </ref>. A few efforts have also been made to formally define and verify system fault-tolerance [19, 44, 45, 49], but these efforts have been limited in scope. Specifically, they have considered systems that recover from the occurrence of faults, and terminate properly.
Reference: [28] <author> M. J. Fischer, N. A. Lynch, and M. S. Paterson, </author> <title> "Impossibility of distributed consensus with one faulty process", </title> <journal> Journal of the ACM, </journal> <volume> 32(2)2 (1985), </volume> <pages> pp. 374-382. </pages>
Reference-contexts: Several results in the literature on impossibility of fault-tolerance [43] can be proven using this method, including the wellknown impossibility of distributed consensus with one faulty process <ref> [28] </ref>. Some of these results involve special kinds of fault-tolerance, such as masking or global stabilizing fault-tolerance.
Reference: [29] <author> N. Francez, </author> <title> Fairness, </title> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference: [30] <author> M. G. Gouda, and N. J. Multari, </author> <title> "Stabilizing communication protocols," </title> <journal> IEEE Transactions on Computers 40(4) (1991), </journal> <pages> pp. 448-458. </pages>
Reference-contexts: Therefore, a rooted spanning tree exists. Also, note that each state in G is a fixed-point; i.e., once the tree:i modules reach a state in G, no action in any of the tree:i modules is enabled. 77 Our proof employs the "convergence stair" method <ref> [30] </ref>: we exhibit a finite sequence of state predicates H:0; H:1; :::; H:K such that (i) H:0 j true (ii) H:K j G (iii) For each l such that 0 l K, H:l is closed under system execution. (iv) For each l such that 0 l &lt; K, H:l converges to
Reference: [31] <editor> D. Gries, </editor> <booktitle> The Science of Programming, </booktitle> <publisher> Springer-Verlag (1981). </publisher>
Reference-contexts: Our definition consists of two conditions: one of closure and another of convergence. To motivate the closure condition, let us observe that a well-established method for verifying fault-free systems is to exhibit a predicate that is true throughout system execution <ref> [21, 31] </ref>. Such an "invariant" predicate identifies the "legal" system states, and asserts that the set of legal states is closed under system execution. Following this method, we require that for each fault-tolerant system there exist a predicate S that is invariant throughout fault-free system execution.
Reference: [32] <author> P. Gronning, T. Nielsen, and H. Lovengreen, </author> <title> "Stepwise development of a distributed load balancing algorithm", </title> <booktitle> Proceedings of the Fourth International Workshop on Distributed Algorithms (1990). </booktitle>
Reference: [33] <author> T. Herman, </author> <title> "Probabilistic self-stabilization," </title> <booktitle> Information Processing Letters 35 (1990), </booktitle> <pages> pp. 63-67. </pages>
Reference: [34] <author> C.-Y. Hsu and J. Liu, </author> <title> "Dynamic load balancing algorithms in homogeneous distributed systems", </title> <booktitle> Proceedings of the Sixteenth International Conference on Distributed Computer Systems (1986), </booktitle> <pages> pp. 216-223. </pages>
Reference-contexts: For instance, the amount of the work load that is created or the rate at which it is created is typically not known a priori. Traditionally, this problem has been dealt with using probabilistic analysis <ref> [34] </ref>, graph-theoretic flow models [55], simulations [59], or heuristic methods [54]. Unfortunately, these approaches are limited in scope. For one, they do not prove that a given load balancing program is correct, i.e., that the program actually balances work load for each possible distribution of work load.
Reference: [35] <author> B. Johnson, </author> <title> The Design and Analysis of Fault-Tolerant Digital Systems, </title> <publisher> Addison-Wesley (1989). </publisher>
Reference: [36] <author> S. Katz and K. J. Perry, </author> <title> "Self-stabilizing extensions for message-passing systems", </title> <booktitle> Proceedings of the 9th ACM Symposium on Principles of Distributed Computing (1990), </booktitle> <pages> pp. 91-101. </pages>
Reference-contexts: Even so, self-stabilizing systems are mainly being designed to tolerate arbitrary transient faults, whereas they can be designed to tolerate a variety of fault types <ref> [6, 36, 60] </ref>. <p> The situations in which distributed resets are required are application-specific. One such situation, however, is when the global state of the application layer is not within its domain of execution. Such states may be detected by periodically executing a stabilizing global state detection algorithm <ref> [36] </ref>. To this end, we note that it is possible to implement a stabilizing global state detection with minor modifications to our reset subsystem.
Reference: [37] <author> L. Lamport, </author> <title> "Solved problems, unsolved problems and non-problems in con-currency", invited talk, </title> <booktitle> Proceedings of the 3rd Annual ACM Symposium on Principles of Distributed Computing (1984), </booktitle> <pages> pp. 1-11. </pages>
Reference-contexts: An extreme form of fault-tolerance that does not always mask faults is self-stabilization. While self-stabilization was first studied in computing science in 1973 [20], and its application to fault-tolerance was strongly endorsed in 1983 <ref> [37] </ref>, it is only in the last few years that concerted efforts have been made to relate self-stabilization to fault-tolerance [11, 13, 16].
Reference: [38] <author> L. Lamport, </author> <title> "The mutual exclusion problem: Part II|Statements and solutions", </title> <journal> Journal of the ACM 33(2) (1986), </journal> <pages> pp. 327-348. 131 </pages>
Reference: [39] <author> L. Lamport, and N. A. Lynch, </author> <title> "Distributed computing: Models and methods", </title> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <volume> Volume 2, Chapter 18, </volume> <publisher> Elsevier Science Publishers (1990), </publisher> <pages> pp. 1158-1199. </pages>
Reference-contexts: Each of these components is global stabilizing, tolerates process and channel failures and repairs, and admits bounded-space implementations. These features distinguish our design of these components from earlier designs [2, 26, 25] and redress the following comment made by Lamport and Lynch <ref> [39, page 1193] </ref>: "A [global] stabilizing algorithm [that translates a distributed system designed for a fixed but arbitrary network into one that works for a changing network] using a finite number of identifiers would be quite useful, but we know of no such algorithm." The rest of the chapter is organized <p> Hence, the convergence rate is O (K + (deg fi dia)) rounds. 2 We conclude this section with the remark that the problems of leader election and spanning tree construction have received considerable attention in the literature (see, for example, <ref> [39, 48, 57] </ref>). Most of these algorithms are based on the assumption that all processes start execution in some designated initial state. This restriction is too severe for our purpose, and we have lifted it by designing the tree layer to be global stabilizing; i.e., insensitive to the initial state.
Reference: [40] <author> B. W. Lampson and H. E. Sturgis, </author> <title> "Crash recovery in a distributed storage system", </title> <type> Xerox Park Tech. Report, </type> <institution> Xerox Palo Alto Research Center (1979). </institution>
Reference-contexts: One consequence of this tradition is that several subdisciplines of fault-tolerant computing have emerged that are apparently unrelated to each other: these subdisciplines deal with specific classes of faults, employ distinct models and design methods, and have their own terminology and classification <ref> [14, 40, 58] </ref>. As a result, the discipline itself appears to be fragmented. Another consequence of this tradition is that verification of fault-tolerant systems is often based on implementation-specific artifacts|such as stable storage, timeouts, and shadow registers|without explicitly specifying what properties of these artifacts are necessary.
Reference: [41] <author> J.-C. Laprie, </author> <title> "Dependable computing and fault tolerance: Concepts and terminology", </title> <booktitle> Proceedings of the 15th International Symposium on Fault-Tolerant Computing (1985), </booktitle> <pages> pp. 2-11. </pages>
Reference-contexts: One is based on a distinction between the notions of faults, errors, and failures: faults in a physical domain can cause errors in an information domain, whereas errors in an information domain can 1 2 cause failures in an external domain <ref> [1, 10, 41] </ref>. (Unfortunately, these notions are subjective: "what one person call a failure, a second person calls a fault, and a third person might call an error" [18].) The other is based on what type of fault is tolerated, for example, stuck-at, crash, fail-stop, omission, timing, or byzantine faults [27,
Reference: [42] <author> F. Lin and R. Keller, </author> <title> "Gradient model: A demand driven load balancing scheme", </title> <booktitle> Proceedings of the Sixteenth International Conference on Distributed Computer Systems (1986), </booktitle> <pages> pp. 216-223. </pages>
Reference: [43] <author> N. A. Lynch, </author> <title> "A hundred impossibility proofs for distributed computing" invited talk, </title> <booktitle> Proceedings of the 8th Annual ACM Symposium on Principles of Distributed Computing (1989), </booktitle> <pages> pp. 1-29. </pages>
Reference-contexts: Several results in the literature on impossibility of fault-tolerance <ref> [43] </ref> can be proven using this method, including the wellknown impossibility of distributed consensus with one faulty process [28]. Some of these results involve special kinds of fault-tolerance, such as masking or global stabilizing fault-tolerance.
Reference: [44] <author> A. Mili, </author> <title> An Introduction to Program Fault-Tolerance, </title> <publisher> Prentice-Hall (1990). </publisher>
Reference-contexts: A few efforts have also been made to formally define and verify system fault-tolerance <ref> [19, 44, 45, 49] </ref>, but these efforts have been limited in scope. Specifically, they have considered systems that recover from the occurrence of faults, and terminate properly. In other words, they have considered systems whose input-output relation masks faults.
Reference: [45] <author> C. Mohan, R. Strong, and S. Finkelstein, </author> <title> "Methods for distributed transaction commit and recovery using byzantine agreement within clusters of processes", </title> <booktitle> Proceedings of the 2nd ACM Symposium on Principles of Distributed Computing (1983), </booktitle> <pages> pp. 29-43. </pages>
Reference-contexts: [1, 10, 41]. (Unfortunately, these notions are subjective: "what one person call a failure, a second person calls a fault, and a third person might call an error" [18].) The other is based on what type of fault is tolerated, for example, stuck-at, crash, fail-stop, omission, timing, or byzantine faults <ref> [27, 45, 49, 52] </ref>. A few efforts have also been made to formally define and verify system fault-tolerance [19, 44, 45, 49], but these efforts have been limited in scope. Specifically, they have considered systems that recover from the occurrence of faults, and terminate properly. <p> A few efforts have also been made to formally define and verify system fault-tolerance <ref> [19, 44, 45, 49] </ref>, but these efforts have been limited in scope. Specifically, they have considered systems that recover from the occurrence of faults, and terminate properly. In other words, they have considered systems whose input-output relation masks faults.
Reference: [46] <author> G. Neiger and S. Toeug, </author> <title> "Automatically increasing the fault-tolerance of distributed algorithms", </title> <booktitle> Journal of Algorithms 11(3) (1990), </booktitle> <pages> pp. 374-419. </pages>
Reference: [47] <author> J. von Neumann, </author> <title> "Probabilistic logics and the synthesis of reliable organisms from unreliable components," in Automata Studies, </title> <publisher> Princeton University Press (1956), </publisher> <pages> pp. 43-98. 132 </pages>
Reference: [48] <author> R. Perlman, </author> <title> "An algorithm for distributed computation of a spanning tree in an extended LAN", </title> <booktitle> Proceedings of the 9th ACM Data Communications Symposium (1985), </booktitle> <pages> pp. 44-52. </pages>
Reference-contexts: Hence, the convergence rate is O (K + (deg fi dia)) rounds. 2 We conclude this section with the remark that the problems of leader election and spanning tree construction have received considerable attention in the literature (see, for example, <ref> [39, 48, 57] </ref>). Most of these algorithms are based on the assumption that all processes start execution in some designated initial state. This restriction is too severe for our purpose, and we have lifted it by designing the tree layer to be global stabilizing; i.e., insensitive to the initial state.
Reference: [49] <author> R. D. Schlichting and F. B. Schneider, </author> <title> "Fail-stop processors: An approach to designing fault-tolerant computing systems", </title> <journal> ACM Transactions on Computers (1983), </journal> <pages> pp. 222-238. </pages>
Reference-contexts: [1, 10, 41]. (Unfortunately, these notions are subjective: "what one person call a failure, a second person calls a fault, and a third person might call an error" [18].) The other is based on what type of fault is tolerated, for example, stuck-at, crash, fail-stop, omission, timing, or byzantine faults <ref> [27, 45, 49, 52] </ref>. A few efforts have also been made to formally define and verify system fault-tolerance [19, 44, 45, 49], but these efforts have been limited in scope. Specifically, they have considered systems that recover from the occurrence of faults, and terminate properly. <p> A few efforts have also been made to formally define and verify system fault-tolerance <ref> [19, 44, 45, 49] </ref>, but these efforts have been limited in scope. Specifically, they have considered systems that recover from the occurrence of faults, and terminate properly. In other words, they have considered systems whose input-output relation masks faults. <p> The other conditions are trivially checked. Since T = S, program Majority is masking and, since true does not converge to S, it is local stabilizing. 2 3.4 Atomic Action Specification <ref> [49] </ref> Design a program Atomic that satisfies the Hoare-triple: fx = M ^ y = N g Atomic fx = h:M ^ y = h:N g , where x and y are integer variables, M and N are integer constants, and h is a 31 function from integers to integers. <p> Lastly, true does not converge to S, and the closure of S is violated by executing actions in F . Hence, program Atomic has nonmasking and local stabilizing fault-tolerance. 2 Remarks This problem is due to Schlichting and Schneider <ref> [49] </ref> who propose that one way to program fail-stop processors is to design sequences of "fault-tolerant actions"; if a processor fails while executing a fault-tolerant action FTA and subsequently repairs, then the action FTA (or occasionally, the recovery protocol of FTA ) is restarted. <p> Subsequently, we refine our program for the C-element based on the observation that the C-element can be implemented using a 3-input majority function. We verify (the well-known fact) that this implementation tolerates one type of delay, but does not tolerate another type of delay. Specification <ref> [49] </ref> A C-element with boolean inputs x and y and a boolean output z is specified as follows: (i) Input x (respectively, y) changes only if x j z (respectively, y j z) holds ; (ii) Output z becomes true only if x ^ y holds, and becomes f alse only
Reference: [50] <author> C. Seitz, </author> <title> "System timing", in Introduction to VLSI Systems, </title> <publisher> Addison-Wesley (1980). </publisher>
Reference-contexts: Specifically, we verify that a delay-insensitive circuit, the Muller C-element, tolerates arbitrary delays in the arrival of its input signals <ref> [50] </ref>. Subsequently, we refine our program for the C-element based on the observation that the C-element can be implemented using a 3-input majority function. We verify (the well-known fact) that this implementation tolerates one type of delay, but does not tolerate another type of delay. <p> In particular, it is both global stabilizing and masking fault-tolerant. Implementation Consider a majority circuit with three boolean inputs x, y, and u and one boolean output v. To implement the C-element using this majority circuit, it suffices to 35 connect v to z and feedback v to u <ref> [50] </ref>.
Reference: [51] <author> D. P. Siewiorek, </author> <title> "Architecture of fault-tolerant computers", </title> <booktitle> in Fault-Tolerant Computing: Volume II, Prentice-Hall (1986). </booktitle>
Reference: [52] <author> D. Skeen and M. Stonebraker, </author> <title> "A formal model of crash recovery in a distributed system", </title> <journal> IEEE Transactions on Software Engg. </journal> <year> (1983), </year> <pages> pp. 219-228. </pages>
Reference-contexts: [1, 10, 41]. (Unfortunately, these notions are subjective: "what one person call a failure, a second person calls a fault, and a third person might call an error" [18].) The other is based on what type of fault is tolerated, for example, stuck-at, crash, fail-stop, omission, timing, or byzantine faults <ref> [27, 45, 49, 52] </ref>. A few efforts have also been made to formally define and verify system fault-tolerance [19, 44, 45, 49], but these efforts have been limited in scope. Specifically, they have considered systems that recover from the occurrence of faults, and terminate properly.
Reference: [53] <author> T. K. Srikanth and S. Toeug, </author> <title> "Simulating authenticated broadcast to derive simple fault tolerant algorithms", </title> <booktitle> Distributed Computing 2(2) (1987), </booktitle> <pages> pp. 80-94. </pages>
Reference-contexts: One process g is distinguished, and has associated with it a boolean value B. It is required that: 1. If g is Reliable, the decision value of each Reliable process is B. 2. All Reliable processes eventually reach the same decision. Faults may make Reliable processes Unreliable. Program <ref> [17, 53] </ref> We assume authenticated communication: messages sent by Reliable processes are correctly received by Reliable processes, and Unreliable processes do not forge messages on behalf of Reliable processes. Agreement is reached within N +1 rounds of communication, where N is the maximum number of processes that can be Unreliable.
Reference: [54] <author> J. A. Stankovik, </author> <title> "A perspective on distributed computer systems", </title> <journal> IEEE Transactions on Computers 33(12) (1984) , pp. </journal> <pages> 1102-1115. </pages>
Reference-contexts: For instance, the amount of the work load that is created or the rate at which it is created is typically not known a priori. Traditionally, this problem has been dealt with using probabilistic analysis [34], graph-theoretic flow models [55], simulations [59], or heuristic methods <ref> [54] </ref>. Unfortunately, these approaches are limited in scope. For one, they do not prove that a given load balancing program is correct, i.e., that the program actually balances work load for each possible distribution of work load.
Reference: [55] <author> H. S. Stone, </author> <title> "Multiprocessor scheduling with the aid of network flow algorithms", </title> <journal> IEEE Transactions on Computers 4(3) (1978), </journal> <pages> pp. 254-258. </pages>
Reference-contexts: For instance, the amount of the work load that is created or the rate at which it is created is typically not known a priori. Traditionally, this problem has been dealt with using probabilistic analysis [34], graph-theoretic flow models <ref> [55] </ref>, simulations [59], or heuristic methods [54]. Unfortunately, these approaches are limited in scope. For one, they do not prove that a given load balancing program is correct, i.e., that the program actually balances work load for each possible distribution of work load.
Reference: [56] <author> B. Randell, </author> <title> "System structure for software fault tolerance", </title> <journal> IEEE Transactions on Software Engg. </journal> <year> (1975), </year> <pages> pp. 220-232. </pages>
Reference-contexts: How can we reason about the fault-tolerance of program interfaces? A program interface specifies the program behavior that is observable by some environment. This specification consists of a set of program variables and a set of constraints on how these variables may be updated <ref> [56] </ref>. In our approach, reasoning about interfaces is simple: Associated with each interface of a program p is some state predicate R that is closed under program execution. An interface is fault-tolerant with respect to some set of fault actions F iff p is F -tolerant for R.
Reference: [57] <author> W. D. Tajibnapis, </author> <title> "A correctness proof of a topology information maintenance protocol or a distributed computer network", </title> <booktitle> Communications of the ACM 20(7) (1977), </booktitle> <pages> pp. 477-485. </pages>
Reference-contexts: Hence, the convergence rate is O (K + (deg fi dia)) rounds. 2 We conclude this section with the remark that the problems of leader election and spanning tree construction have received considerable attention in the literature (see, for example, <ref> [39, 48, 57] </ref>). Most of these algorithms are based on the assumption that all processes start execution in some designated initial state. This restriction is too severe for our purpose, and we have lifted it by designing the tree layer to be global stabilizing; i.e., insensitive to the initial state.
Reference: [58] <author> A. S. Tanenbaum, </author> <title> Computer Networks, </title> <booktitle> Prentice-Hall (1981). </booktitle> <pages> 133 </pages>
Reference-contexts: One consequence of this tradition is that several subdisciplines of fault-tolerant computing have emerged that are apparently unrelated to each other: these subdisciplines deal with specific classes of faults, employ distinct models and design methods, and have their own terminology and classification <ref> [14, 40, 58] </ref>. As a result, the discipline itself appears to be fragmented. Another consequence of this tradition is that verification of fault-tolerant systems is often based on implementation-specific artifacts|such as stable storage, timeouts, and shadow registers|without explicitly specifying what properties of these artifacts are necessary.
Reference: [59] <author> D. L. Eager, E. D. Lazowska and J. Zahorjan, </author> <title> "A comparison of receiver-initiated and sender-initiated dynamic load sharing", </title> <booktitle> Performance Evaluation 6(1) (1986), </booktitle> <pages> pp. 53-68. </pages>
Reference-contexts: For instance, the amount of the work load that is created or the rate at which it is created is typically not known a priori. Traditionally, this problem has been dealt with using probabilistic analysis [34], graph-theoretic flow models [55], simulations <ref> [59] </ref>, or heuristic methods [54]. Unfortunately, these approaches are limited in scope. For one, they do not prove that a given load balancing program is correct, i.e., that the program actually balances work load for each possible distribution of work load.
Reference: [60] <author> Y. Zhao and F. B. Bastani, </author> <title> "A self-stabilizing algorithm for byzantine agreement", </title> <institution> University of Houston Tech. </institution> <type> Rep. </type> <month> UH-CS-87-6 </month> <year> (1987). </year>
Reference-contexts: Even so, self-stabilizing systems are mainly being designed to tolerate arbitrary transient faults, whereas they can be designed to tolerate a variety of fault types <ref> [6, 36, 60] </ref>.
References-found: 60

