URL: http://fas.sfu.ca/cs/people/GradStudents/melli/DBPredictor/ICML98.ps.gz
Refering-URL: http://fas.sfu.ca/cs/people/GradStudents/melli/DBPredictor/
Root-URL: 
Email: melli@cs.sfu.ca  
Title: Advances in Lazy Model-Based Induction  
Author: Gabor Melli 
Date: February 27, 1998  
Affiliation: Simon Fraser University  
Abstract: This paper demonstrates the value of lazy model-based induction to the opportunistic classification of a single unlabeled event. Three areas are investigated with DBPredic-tor [14] as the reference classifier. Firstly, the susceptibility of overfitting is shown to be significant to this classifier and a proposed enhancement of (pre)pruning is shown to mitigate this weakness. Secondly, the appropriateness of evaluation functions based on the parent-child difference (as proposed for LazyDT [9]) is rejected based on its consistent degradation of the algorithm's empirical accuracy. Finally, the empirical running time complexity of these algorithms is shown to be significantly faster than the eager model-based C4.5 [18] classifier. The results show that DBPredictor can perform a significant number of classifications before C4.5 can classify a single event. Furthermore, this difference appears to grow linearly with respect to the number of instances in the dataset. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> AAAI. </editor> <booktitle> Thirteenth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference: [2] <author> D. W. Aha, </author> <title> editor. Lazy Learning. </title> <publisher> Kluwer Academic, </publisher> <month> May </month> <year> 1997. </year>
Reference-contexts: Furthermore, instance-based learning (IBL) [3] algorithms do not return a justification of their class predictions that a person can easily interpret. To resolve this gap two recent algorithms have been developed that are lazy <ref> [2] </ref> but remain model-based: LazyDT [9] and DBPredictor [13, 14].
Reference: [3] <author> D. W. Aha, D. Kibler, and M. K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: A common requirement of these tasks is to predict the class for only one event. Unfortunately, the running time of eager classification algorithms such as C4.5 [18] and CN2 [5] does not lend itself to these on-line classification tasks. Furthermore, instance-based learning (IBL) <ref> [3] </ref> algorithms do not return a justification of their class predictions that a person can easily interpret. To resolve this gap two recent algorithms have been developed that are lazy [2] but remain model-based: LazyDT [9] and DBPredictor [13, 14]. <p> This approach allows the algorithm to more quickly predict our particular mushroom's edibility. A lazy non-model-based classifier such as IB1's k nearest-neighbour, on the other, will return a prediction very quickly, however its accuracy and the understandability of its results can be problematic <ref> [3] </ref>. Irrelevant attributes, for example, are common to opportunistic classification but the accuracy of IB1-like classifiers degrades in these domains [7]. Furthermore an instance-based result (of k instances) may not provide a justification to this prediction that can be easily interpreted by an individual.
Reference: [4] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: Note that this approach has been exten sively researched for eager model-based in duction <ref> [17, 4] </ref>. 3.1.1 Average Impurity The entropy function i () 1 is commonly used to determine the information gain i () of selecting a hypothesis [17, 16]. This measure is based on information-theory [20] and will be informally referred to in this paper as entropy ().
Reference: [5] <author> P. Clark and R. Boswell. </author> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Machine Learning Proceedings of the Fifth European Conference (EWSL-91), </booktitle> <pages> pages 151-163. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Opportunistic classification occurs against datasets that were not explicitly constructed for the classification task at hand. A common requirement of these tasks is to predict the class for only one event. Unfortunately, the running time of eager classification algorithms such as C4.5 [18] and CN2 <ref> [5] </ref> does not lend itself to these on-line classification tasks. Furthermore, instance-based learning (IBL) [3] algorithms do not return a justification of their class predictions that a person can easily interpret. <p> Algorithm lower avg err. clear on rate (%) wins DBPredictor 12 16.94 6 C4.5 10 16.61 6 Subtraction 2 -0.33 0 naive algorithm, it is likely due to overfit-ting <ref> [5] </ref>. With no pruning, the most accurate version of DBPredictor achieves a higher error rate than the naive algorithm on five of the twenty three datasets: liver-disease, hepatitis, heart-c, credit-g, and echocardio-gram.
Reference: [6] <author> P. Domingos. </author> <title> Unifying instance-based and rule-based induction. </title> <journal> Machine Learning, </journal> <volume> 24(2) </volume> <pages> 141-168, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Does DBPredictor perform on-line clas sification tasks faster than C4.5? 5.1 Accuracy Twenty three real-world data sets were selected to perform this study's investigation into DBPredictor's accuracy. An attempt was made to include datasets that have been widely used in other classification studies <ref> [9, 6] </ref>. Furthermore, the datasets were required to contain a sampling of sizes, data types, and application areas. Table 1 lists and summarizes the characteristics of these datasets. All datasets were retrieved from the UCI repository [15].
Reference: [7] <author> P. Domingos. </author> <title> Context-sensitive feature selection for lazy learners. </title> <booktitle> In Aha [2], </booktitle> <pages> pages 227-253. </pages>
Reference-contexts: Irrelevant attributes, for example, are common to opportunistic classification but the accuracy of IB1-like classifiers degrades in these domains <ref> [7] </ref>. Furthermore an instance-based result (of k instances) may not provide a justification to this prediction that can be easily interpreted by an individual. A lazy model-based classifier on the other hand performs dynamic relevance analysis that avoids irrelevant attributes by iteratively generating and testing hypotheses within its model.
Reference: [8] <author> U. M. Fayyad and K. B. Irani. </author> <title> The attribute selection problem in decision 14 tree generation. </title> <booktitle> In Proceedings of the Tenth National Conference of Artificial Intelligence, </booktitle> <pages> pages 104-110. </pages> <publisher> AAAI Press, </publisher> <year> 1992. </year>
Reference-contexts: This function falls into the category of class separation functions <ref> [8] </ref>. In our example this function calculates the distance between ~r 2 and ~ r 0 1 . This distance can be validated against Figure 2.
Reference: [9] <author> J. H. Friedman, R. Kohavi, and Y. Yun. </author> <title> Lazy decision trees. </title> <booktitle> [1], </booktitle> <pages> pages 717-724. </pages>
Reference-contexts: Furthermore, instance-based learning (IBL) [3] algorithms do not return a justification of their class predictions that a person can easily interpret. To resolve this gap two recent algorithms have been developed that are lazy [2] but remain model-based: LazyDT <ref> [9] </ref> and DBPredictor [13, 14]. Consider the following example: A person is about to eat a mushroom but wants to confirm whether it is edible or poisonous. 1 After a quick search the person finds a very large database on the World Wide Web that includes data on mushroom edibility. <p> The details of this classifier are presented in Melli, 1998 [14]. The working principle of this algorithm is to locate a predictive classification rule in ways that expends effort strictly on the single classification task at hand. As such the most similar classifier is the LazyDT algorithm proposed in <ref> [9] </ref>. The main differences between the two are DBPredictor's avoidance of a dis-cretization step for numerical attributes and its IF-THEN rule based output rather than LazyDT's decision tree path result. DBPredictor requires three input parameters. <p> It may instead be more valid to focus attention on the different between the parent rule and the sibling rule that applies to ~e. In our example this is the difference between rules r 0 and r 2 . This variation is implicitly proposed for the LazyDT classifier <ref> [9] </ref> against the entropy () measure. This section explicitly presents this variation and also extends it to the Eu-clidean distance based measure DI (). <p> variation of the entropy based function entropy + () (formally i + ()) subtracts the entropy i () of the parent rule from the entropy of the child rule. i + (~r 0 ; ~r 2 ) = i (~r 0 ) i (~r 2 ) Unfortunately, as documented in <ref> [9] </ref> this simplistic approach leads to problems. Our example highlights this problem because the entropy measures of both parent and child distributions are equal (0.88) and will thus cancel themselves out. <p> To overcome this problem the distribution of r 0 is flattened (made impure) and the distribution of r 2 is updated accordingly. Since this normalization process is not described in detail in <ref> [9] </ref> 6 the following process is assumed: ~r parent+ ( 8i [r pi+ ] = 1=c ~r child+ ( 8i [r ci+ ] = r ci =r pi j=1 r cj =r pj When applied to our example, r 0 and r 2 are updated to: ~r 0 + = [1=2; <p> Does DBPredictor perform on-line clas sification tasks faster than C4.5? 5.1 Accuracy Twenty three real-world data sets were selected to perform this study's investigation into DBPredictor's accuracy. An attempt was made to include datasets that have been widely used in other classification studies <ref> [9, 6] </ref>. Furthermore, the datasets were required to contain a sampling of sizes, data types, and application areas. Table 1 lists and summarizes the characteristics of these datasets. All datasets were retrieved from the UCI repository [15].
Reference: [10] <author> R. V. Hogg and J. Ledolter. </author> <title> Engineering Statistics. </title> <publisher> Macmillan Publishing Company, </publisher> <year> 1987. </year>
Reference-contexts: error rate on more datasets than A 2 . * avg: A 1 achieves a lower average error rate than A 2 . * clear wins: A 1 achieves a lower error rate, with a 99.5% confidence level, on more datasets than A 2 (based on a one tailed t-test <ref> [10] </ref>). 5.1.2 Parameter Refinement The purpose of the first empirical study was to determine what combination of values for the two pruning parameters (min cov, min meas) maximized DBPredictor's accuracy. Five datasets were selected for this study (they are marked in Table 1 with a * symbol beside their name).
Reference: [11] <author> P. J. Huber. </author> <title> From large to huge: A statistician's reactions to KDD & DM. </title> <booktitle> In The Third International Conference on Knowledge Discovery & Data Mining, </booktitle> <pages> pages 304-308, </pages> <year> 1997. </year>
Reference-contexts: 1 Introduction With the ease of access to a growing amount of structured observations, such as those stored in relational databases, classification algorithms are being used in very "opportunistic" ways <ref> [11] </ref>. Opportunistic classification occurs against datasets that were not explicitly constructed for the classification task at hand. A common requirement of these tasks is to predict the class for only one event.
Reference: [12] <author> R. Kohavi. </author> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <editor> In C. S. Mellish, editor, </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1137-1143, </pages> <year> 1995. </year>
Reference-contexts: Error rate estimates from multiple cross-validation studies have been recently shown to achieve both a conservative bias and a low variance <ref> [12] </ref>.
Reference: [13] <author> G. Melli. </author> <title> Ad hoc attribute-value prediction. </title> <booktitle> [1], </booktitle> <pages> page 1396. </pages>
Reference-contexts: Furthermore, instance-based learning (IBL) [3] algorithms do not return a justification of their class predictions that a person can easily interpret. To resolve this gap two recent algorithms have been developed that are lazy [2] but remain model-based: LazyDT [9] and DBPredictor <ref> [13, 14] </ref>. Consider the following example: A person is about to eat a mushroom but wants to confirm whether it is edible or poisonous. 1 After a quick search the person finds a very large database on the World Wide Web that includes data on mushroom edibility.
Reference: [14] <author> G. Melli. </author> <title> Knowledge based on-line classification. </title> <type> Master's thesis, </type> <institution> Simon Fraser University, School of Computing Science, </institution> <month> April </month> <year> 1998. </year>
Reference-contexts: Furthermore, instance-based learning (IBL) [3] algorithms do not return a justification of their class predictions that a person can easily interpret. To resolve this gap two recent algorithms have been developed that are lazy [2] but remain model-based: LazyDT [9] and DBPredictor <ref> [13, 14] </ref>. Consider the following example: A person is about to eat a mushroom but wants to confirm whether it is edible or poisonous. 1 After a quick search the person finds a very large database on the World Wide Web that includes data on mushroom edibility. <p> Finally, Section 6 discusses the lessons learned and concludes with future research opportunities. 2 DBPredictor Overview This section provides an overview of the reference lazy model-based classifier: DBPre-dictor. The details of this classifier are presented in Melli, 1998 <ref> [14] </ref>. The working principle of this algorithm is to locate a predictive classification rule in ways that expends effort strictly on the single classification task at hand. As such the most similar classifier is the LazyDT algorithm proposed in [9].
Reference: [15] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1995. </year>
Reference-contexts: For this example assume that three rules are generated at this specialization step: r 1 : (A 1 = a 1 ) ! A 4 2 <ref> [15; 35] </ref> r 3 : (A 3 = a 3 ) ! A 4 2 [30; 70] Note that as required all three rules continue to apply to the event vector. <p> Furthermore, the datasets were required to contain a sampling of sizes, data types, and application areas. Table 1 lists and summarizes the characteristics of these datasets. All datasets were retrieved from the UCI repository <ref> [15] </ref>. The C4.5 classification algorithm will be the benchmark system that DBPredictor will be tested against. C4.5 is a state-of-the-art classification algorithm that constructs decision trees that can then be used to classify new cases [18]. <p> The dataset is composed of N =199,523 instances and n=37 predicting attributes (with 13 of these being numerical). The class attribute of this dataset has c=2 classes of relatively equal proportion and few missing attribute-values. This dataset is also located at the UCI repository <ref> [15] </ref>. Finally, all the experiments in this section were performed on a computer with the following configuration: 133Mhz Pen-tium CPU, 64MB of memory, and Linux 2.0.27. Furthermore both classifiers were implemented with the same (GNU) ANSI-C compiler.
Reference: [16] <author> S K. Murthy. </author> <title> On Growing Better Decision Trees from Data. </title> <type> PhD thesis, </type> <institution> John Hopkins University, </institution> <year> 1995. </year>
Reference-contexts: infer the distribution of the complementary 4 F (~r 0 ; ~r 2 ). 0.3 0.7~r 0 = p r2 0 = 80% Z Z Z~ 16 64 2 = 0.7 0.3 ~r 2 = rule r 0 2 : (A 2 6= a 2 ) ! A 4 2 <ref> [16; 64] </ref> by subtracting from the parent's distribution. The standard approach to evaluation functions is to determine the difference between the class probability distribution vector of rule r 2 (i.e. ~r 2 ) and its complement's vector ~ r 0 2 [17]. <p> Note that this approach has been exten sively researched for eager model-based in duction [17, 4]. 3.1.1 Average Impurity The entropy function i () 1 is commonly used to determine the information gain i () of selecting a hypothesis <ref> [17, 16] </ref>. This measure is based on information-theory [20] and will be informally referred to in this paper as entropy (). <p> Specifically, further specialization will be restricted to rules that match a minimum number of instances in the dataset and that achieve a minimum evaluation function F () value. This approach is related to the use of pre-pruning (also known as stop-splitting rules) in top-down induction of decision trees <ref> [21, 18, 16] </ref>. Both the sensitivity of overfitting and the impact of pruning will be tested empirically in the Section 5. Two threshold parameters are integrated in the evaluation function F (): min cov and min meas.
Reference: [17] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: A rule r 0 i is taken to be more predictive than rule r 0 j if F (r 0 j ; r). Evaluation function F () can be based on one of several standard measures. This section presents versions of F () based two distinct measures: entropy () <ref> [17] </ref> and DI () [21]. For each measure a sibling-sibling and parent-child version is described for a total of four versions of F (). The parent-child approach is a novel variation that is implicitly used in the LazyDT algorithm. <p> The standard approach to evaluation functions is to determine the difference between the class probability distribution vector of rule r 2 (i.e. ~r 2 ) and its complement's vector ~ r 0 2 <ref> [17] </ref>. This approach will be referred to as sibling-sibling functions. The variation informally presented for LazyDT on the other hand evaluates the difference between r 2 and its parent rule r 0 . This approach will be referred to as parent-child functions. <p> Note that this approach has been exten sively researched for eager model-based in duction <ref> [17, 4] </ref>. 3.1.1 Average Impurity The entropy function i () 1 is commonly used to determine the information gain i () of selecting a hypothesis [17, 16]. This measure is based on information-theory [20] and will be informally referred to in this paper as entropy (). <p> Note that this approach has been exten sively researched for eager model-based in duction [17, 4]. 3.1.1 Average Impurity The entropy function i () 1 is commonly used to determine the information gain i () of selecting a hypothesis <ref> [17, 16] </ref>. This measure is based on information-theory [20] and will be informally referred to in this paper as entropy (). <p> Several future research directions are apparent. First a more formal approach to pre-pruning should be investigated now that its usefulness has been presented to this type of classifier. One possibility that comes to mind is the chi-square test for stochastic independence proposed in <ref> [17] </ref>. Also, while LazyDT's parent-child variation has been rejected in this paper, two of its other proposals remain untested. Its use of very conservative specialization steps and one-level lookahead (to resolve ties) should be investigated to understand their impact on accuracy and running time.
Reference: [18] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Opportunistic classification occurs against datasets that were not explicitly constructed for the classification task at hand. A common requirement of these tasks is to predict the class for only one event. Unfortunately, the running time of eager classification algorithms such as C4.5 <ref> [18] </ref> and CN2 [5] does not lend itself to these on-line classification tasks. Furthermore, instance-based learning (IBL) [3] algorithms do not return a justification of their class predictions that a person can easily interpret. <p> Specifically, further specialization will be restricted to rules that match a minimum number of instances in the dataset and that achieve a minimum evaluation function F () value. This approach is related to the use of pre-pruning (also known as stop-splitting rules) in top-down induction of decision trees <ref> [21, 18, 16] </ref>. Both the sensitivity of overfitting and the impact of pruning will be tested empirically in the Section 5. Two threshold parameters are integrated in the evaluation function F (): min cov and min meas. <p> All datasets were retrieved from the UCI repository [15]. The C4.5 classification algorithm will be the benchmark system that DBPredictor will be tested against. C4.5 is a state-of-the-art classification algorithm that constructs decision trees that can then be used to classify new cases <ref> [18] </ref>.
Reference: [19] <author> J. R. Quinlan. </author> <title> Improved use of continuous attributes in C4.5. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 77-90, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: C4.5 is a state-of-the-art classification algorithm that constructs decision trees that can then be used to classify new cases [18]. The version of C4.5 used in this study was release 8 which improves C4.5's performance on datasets with continuous attributes <ref> [19] </ref>. 5.1.1 Accuracy Testing Methodology All error rate results in this study are based on the average of at least five runs of a stratified 10-fold cross-validation (SCV-10) resampling study [22].
Reference: [20] <author> P. Smyth and R.M. Goodman. </author> <title> An information theoretic approch to rule induction. </title> <journal> IEEE Trans. Knowledge and Data Engineering, </journal> <volume> 4 </volume> <pages> 301-316, </pages> <year> 1992. </year>
Reference-contexts: Note that this approach has been exten sively researched for eager model-based in duction [17, 4]. 3.1.1 Average Impurity The entropy function i () 1 is commonly used to determine the information gain i () of selecting a hypothesis [17, 16]. This measure is based on information-theory <ref> [20] </ref> and will be informally referred to in this paper as entropy ().
Reference: [21] <author> R. Uthurusamy, U. M. Fayyad, and S. Spangler. </author> <title> Learning useful rules from inconclusive data. </title> <editor> In G. Piatetsky-Shapiro and W. J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 141-157. </pages> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Evaluation function F () can be based on one of several standard measures. This section presents versions of F () based two distinct measures: entropy () [17] and DI () <ref> [21] </ref>. For each measure a sibling-sibling and parent-child version is described for a total of four versions of F (). The parent-child approach is a novel variation that is implicitly used in the LazyDT algorithm. Thus to understand the value of this variation a concerted investigation is performed. <p> the entropy of the parent rule: i ( ~r 0 ; ~r 2 ) = i ( ~r 0 ) p r2 i ( ~r 2 ) p r2 0 i ( ~ r 0 0:88 0:2 (0:88) 0:8 (0:72) 13bits 3.1.2 Class Separation The DI () function presented in <ref> [21] </ref> bases its prediction by measuring the Euclidean distance between two class probability distribution vectors. This function falls into the category of class separation functions [8]. In our example this function calculates the distance between ~r 2 and ~ r 0 1 . <p> Specifically, further specialization will be restricted to rules that match a minimum number of instances in the dataset and that achieve a minimum evaluation function F () value. This approach is related to the use of pre-pruning (also known as stop-splitting rules) in top-down induction of decision trees <ref> [21, 18, 16] </ref>. Both the sensitivity of overfitting and the impact of pruning will be tested empirically in the Section 5. Two threshold parameters are integrated in the evaluation function F (): min cov and min meas.
Reference: [22] <author> S. M. Weiss and C. A. </author> <title> Kulikowski. Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. </title> <publisher> Morgan Kauf-man, </publisher> <year> 1991. </year> <month> 15 </month>
Reference-contexts: The version of C4.5 used in this study was release 8 which improves C4.5's performance on datasets with continuous attributes [19]. 5.1.1 Accuracy Testing Methodology All error rate results in this study are based on the average of at least five runs of a stratified 10-fold cross-validation (SCV-10) resampling study <ref> [22] </ref>. Error rate estimates from multiple cross-validation studies have been recently shown to achieve both a conservative bias and a low variance [12].
References-found: 22

