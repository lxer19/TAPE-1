URL: ftp://ftp.cs.washington.edu/tr/1993/09/UW-CSE-93-09-05.PS.Z
Refering-URL: http://www.cs.washington.edu/research/arch/meerkat.html
Root-URL: 
Email: robertb,cpbrown@cs.washington.edu  
Title: The Meerkat Multicomputer  
Author: Robert Bedichek Curtis Brown 
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Meerkat is a distributed memory multicomputer architecture that scales to hundreds of processors. Meerkat uses a two dimensional passive backplane to connect nodes composed of processors, memory, and I/O devices. The interconnect is conceptually simple, inexpensive to design and build, has low latency, and provides high bandwidth on long messages. However, it does not scale to thousands of processors, does not provide high bandwidth on short messages, and does not provide cache coherent shared memory. Our hypothesis is that many general-purpose, database, and parallel numerical workloads work well on systems with Meerkat's characteristics. We describe the Meerkat architecture, the niche that Meerkat fills, the motivation behind our design choices, and give performance results obtained from our hardware prototype and a calibrated simulator. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ramune Arlauskas. </author> <title> iPSC/2 System: A Second Gen eration Hypercube, </title> <month> January </month> <year> 1988. </year>
Reference-contexts: If double precision numbers are used, each message will be 1024 bytes. 4.4 Throughput under light load nodes for both systems as a function of message size. We include in these graphs the throughput measured on the Intel Hypercube iPSC/2 <ref> [1] </ref> and iPSC/860. In this test the bandwidths reported by the Meerkat simulator and by the hardware differed by about one percent and so the Meerkat curve can be viewed both 5 as measurements of a real system and as simulation results.
Reference: [2] <author> Robert Bedichek. </author> <title> Some efficient architecture simula tion techniques. </title> <booktitle> In Proceedings of the Winter 1990 USENIX Conference, </booktitle> <pages> pages 53-63, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The simulator extends our performance results to systems with up to 256 nodes. While the simulator is slow compared to the hardware, its on-the-fly generation of threaded code allows it to simulate roughly a million processor cycles per second on a modern workstation <ref> [2] </ref>. The Meerkat simulator allows us to do detailed simulations of Meerkat systems with 256 nodes running significant parallel programs. Meerkat's simplicity allows implementations that have high performance without great design effort. The Meerkat hardware prototype supports this result.
Reference: [3] <author> Robert Bedichek and Curtis Brown. </author> <title> The Meerkat multicomputer. </title> <institution> University of Washington CSE Technical Report 92-09-05, </institution> <year> 1992. </year>
Reference: [4] <author> Michael Carlton and Alvin Despain. </author> <title> Aquarius project. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 80-83, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The connections between the backplane segments would be designed to match the backplane's target impedance as closely as possible. 9 7 Related Work The diagram for Meerkat resembles those of the Wisconsin Multicube [5] and Aquarius Multi-multi <ref> [4] </ref>. However, the latter two machines put their buses to different uses than does Meerkat. The Multicube and Aquarius implement a coherent shared memory system in hardware. Therefore, transactions on their buses are initiated by memory reference instructions and cache coherence operations.
Reference: [5] <author> James R. Goodman and Philip J. Woest. </author> <title> The Wis consin multicube: A new large-scale cache-coherent multiprocessor. </title> <booktitle> In Proceedings 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 422-431, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The connections between the backplane segments would be designed to match the backplane's target impedance as closely as possible. 9 7 Related Work The diagram for Meerkat resembles those of the Wisconsin Multicube <ref> [5] </ref> and Aquarius Multi-multi [4]. However, the latter two machines put their buses to different uses than does Meerkat. The Multicube and Aquarius implement a coherent shared memory system in hardware. Therefore, transactions on their buses are initiated by memory reference instructions and cache coherence operations.
Reference: [6] <author> Bill Gunning, Leo Yuan, Trung Nguyen, and Tony Wong. </author> <title> A CMOS low-voltage-swing transmission-line transceiver. </title> <booktitle> IEEE International Solid-State Circuits Conference, </booktitle> <pages> pages 58-59, </pages> <year> 1992. </year>
Reference-contexts: With recent advances in CMOS technology, it is possible today to fabricate a single low cost CMOS gate array that drives over 100 terminated bus wires at 100 MHz <ref> [6] </ref>. Fat-tree networks are planar in theory, but in practice manufacturers such as Thinking Machines are not able to take advantage of this property. The CM-5's fat-tree network, for example, uses coaxial cables to connect nodes of the fat tree. <p> without much difficulty: * internode bus lengths of 75 cm, stub lengths 5 cm or less * a maximum of sixteen taps (each node having a tap) per internode bus * internode buses clocked at 100 MHz with one 32 bit word transferred per clock tick * use of GTL <ref> [6] </ref> logic levels for internode commu nication These characteristics yield a system that can have 256 nodes and a raw per-bus internode transfer rate of 400 MB/sec. We estimate that under heavy load such a system would provide 20 MB/sec per node assuming that most communication requires two buses.
Reference: [7] <institution> Intel Corp, </institution> <address> 2065 Bowers Avenue, Santa Clara, </address> <note> Cali fornia 95051. Touchstone Delta System User's Guide, </note> <year> 1991. </year>
Reference-contexts: The stand-alone environment implements a subset of the Intel message passing library and thus allows programs written for this subset to execute on both Meerkat and Intel parallel computers such as the 1 Intel Touchstone Delta <ref> [7] </ref>. The stand-alone environment is small and makes it easier to measure the performance of the architecture in isolation. All of our measurements reported here use the stand-alone environment. The Mach and OSF-based system, on the other hand, challenges us to support a large operating system.
Reference: [8] <institution> Intel Supercomputer Systems Division. Paragon XP/S Product Overview, </institution> <year> 1991. </year>
Reference-contexts: There is no lack of demand for the greater computational power of larger systems, rather, size is limited by economics. Nodes of commercial multi-computers, such as the TMC CM-5 [13] and the Intel Paragon <ref> [8] </ref>, use powerful microprocessors and large memories and therefore cost in the range of $30K to $100K per node. This puts the cost of a 256 node system, for example, in the range of $7.5M to $25M.
Reference: [9] <author> Daniel Lenoski, James Laudon, Kourosh Gharachor loo, Anoop Gupta, and John Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This is on par with the cost of an Ethernet adapter while the bandwidth of Meerkat is 64 times higher and the processor overhead is lower than that of Ethernet. The overhead of the interconnect and distributed shared memory hardware in DASH <ref> [9] </ref> is about 20 percent of the system. While this is reasonable for a machine of its class, it represents a substantially higher system cost than Meerkat's two percent.
Reference: [10] <author> R.M. Metcalfe and D.R. Boggs. </author> <title> Ethernet: Distributed packet switching for local computer networks. </title> <journal> Communications of the ACM, </journal> <volume> 19(7) </volume> <pages> 395-404, </pages> <month> July </month> <year> 1976. </year>
Reference-contexts: For both of these reasons Meerkat's low level arbitration software releases the first bus in a failed 2-bus arbitration and tries again a short time later. The length of the back-off interval is random to prevent live-lock. This back-off scheme is akin to that used by Ethernet <ref> [10] </ref>, except that it is done in software and the Meerkat bus arbitration circuit grants ownership of a particular bus to one node at a time; in Ethernet collisions occur when two nodes try to use the wire at the same time.
Reference: [11] <author> Youcef Saad and Martin H. Schultz. </author> <title> Topological Properties of Hypercube. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <volume> vol. 1, </volume> <pages> pages 867-872, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: The wiring density of cables is approximately 10 wires per inch. 3 In addition to their order of magnitude advantage in density, printed circuit wires are less expensive and more reliable than cables. Nonplanar interconnect topologies, such as the hypercube <ref> [11] </ref>, cannot be wired using low cost, high density printed circuit wires. Instead, nodes of these systems are connected by links that use cables which have relatively few wires, often just one. In order to maintain the same data rates, these cables must use much higher clock rates.
Reference: [12] <author> Charles L. Seitz and Wen-King Su. </author> <title> A family of rout ing and communication chips based on the mossaic. </title> <booktitle> Research on Integrated Systems, </booktitle> <pages> pages 320-337, </pages> <year> 1993. </year>
Reference-contexts: On long messages both Meerkat and Delta are limited by their different abilities to drive their interconnects. Meerkat's internode bandwidth reaches 83 percent of its peak rate of 80 MB/sec, whereas the Delta reaches ten percent of its theoretical rate, which is also 80 MB/sec <ref> [12] </ref>. While Meerkat's maximum interconnect performance is seen at a message size that is longer than most applications will generate, its performance on shorter messages is still high. It may make sense, however, to reduce the per-message overhead by moving logic from low-level software into hardware.
Reference: [13] <institution> Thinking Machines Corp., </institution> <address> 245 First St., Cambridge MA 02142. </address> <note> CM-5 Technical Summary. 11 </note>
Reference-contexts: There is no lack of demand for the greater computational power of larger systems, rather, size is limited by economics. Nodes of commercial multi-computers, such as the TMC CM-5 <ref> [13] </ref> and the Intel Paragon [8], use powerful microprocessors and large memories and therefore cost in the range of $30K to $100K per node. This puts the cost of a 256 node system, for example, in the range of $7.5M to $25M.
References-found: 13

