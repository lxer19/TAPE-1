URL: ftp://ftp.idsia.ch/pub/techrep/IDSIA-09-98.ps.gz
Refering-URL: http://www.idsia.ch/techrep.html
Root-URL: http://www.idsia.ch/techrep.html
Email: nic@idsia.ch  
Title: Local Gain Adaptation for Multi-Layer Perceptrons  
Author: Nicol N. Schraudolph 
Date: March 27, 1998 revised May 15, 1998  
Web: http://www.idsia.ch/  
Address: Corso Elvezia 36 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Note: Online  
Abstract: Technical Report IDSIA-09-98 Abstract We introduce a new method for adapting the step size of each individual weight in a multi-layer perceptron trained by stochastic gradient descent. Our technique derives from the K1 algorithm for linear systems (Sutton, 1992b), which in turn is based on a diagonalized Kalman Filter. We expand upon Sutton's work in two regards: K1 is a) extended to multi-layer perceptrons, and b) made more efficient by linearizing an exponentiation operation. The resulting elk1 (extended, linearized K1) algorithm is computationally little more expensive than alternative proposals (Zimmermann, 1994; Almeida et al., 1997, 1998), and does not require an arbitrary smoothing parameter. In our benchmark experiments, elk1 consistently outperforms these alternatives, as well as stochastic gradient descent with momentum, even when the number of floating-point operations required per weight update is taken into account. Unlike the method of Almeida et al. (1997, 1998), elk1 does not require statistical independence between successive training patterns, and handles large initial learning rates well. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Almeida, L. B., Langlois, T., & Amaral, J. D. </author> <year> (1997). </year> <title> On-line step size adaptation. </title> <type> Tech. rep. </type> <address> RT07/97, INESC, 9 Rua Alves Redol, 1000 Lisboa, Portugal, http://146.193.1.145/tl/RT0797/ fl. </address>
Reference: <author> Almeida, L. B., Langlois, T., Amaral, J. D., & Plakhov, A. </author> <year> (1998). </year> <title> Parameter adaptation in stochastic optimization. </title> <type> Tech. rep., </type> <address> INESC, 9 Rua Alves Redol, 1000 Lisboa, Portugal, ftp://146.193.2.131/pub/lba/- papers/adsteps.ps.gz fl. </address>
Reference-contexts: As a subjective guide, Table 1 lists the CPU time (in milliseconds per pattern) consumed by each algorithm in our prototype implementation. There are other differences between the above algorithms. For instance, alap is derived under the assumption that successive training patterns are statistically independent <ref> (Almeida et al., 1998) </ref> | a condition that may be hard to meet e.g. in situated applications, where the input is provided by a real-time environment over which the learner does not have complete control.
Reference: <author> Battiti, R. </author> <year> (1989). </year> <title> Accelerated back-propagation learning: Two optimization methods. </title> <journal> Complex Systems, </journal> <volume> 3, </volume> <pages> 331-342. </pages>
Reference: <author> Jacobs, R. </author> <year> (1988). </year> <title> Increased rates of convergence through learning rate adaptation. </title> <booktitle> Neural Networks, </booktitle> <volume> 1, </volume> <pages> 295-307. </pages>
Reference: <author> Lapedes, A., & Farber, R. </author> <year> (1986). </year> <title> A self-optimizing, nonsymmetrical neural net for content addressable memory and pattern recognition. </title> <journal> Physica, </journal> <volume> D 22, </volume> <pages> 247-259. </pages>
Reference: <author> LeCun, Y., Simard, P. Y., & Pearlmutter, B. </author> <year> (1993). </year> <title> Automatic learning rate maximization in large adaptive machines. </title> <editor> In Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 5, </volume> <pages> pp. 156-163. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Matthews, M. B. </author> <year> (1990). </year> <title> Neural network nonlinear adaptive filtering using the extended Kalman filter algorithm. </title> <booktitle> In Proceedings of the International Neural Networks Conference, </booktitle> <volume> Vol. I, </volume> <pages> pp. </pages> <address> 115-119 Paris, France. </address>
Reference: <author> Murata, N., Muller, K.-R., Ziehe, A., & Amari, S.-i. </author> <year> (1997). </year> <title> Adaptive on-line learning in changing environments. </title> <editor> In Mozer, M. C., Jordan, M. I., & Petsche, T. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 9, </volume> <pages> pp. 599-605. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Plumer, E. S. </author> <year> (1995). </year> <title> Training neural networks using sequential-update forms of the extended Kalman filter. </title> <type> Tech. rep. </type> <institution> LA-UR-95-422, Los Alamos National Laboratory. </institution>
Reference: <author> Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. </author> <year> (1992). </year> <title> Numerical Recipes in C: The Art of Scientific Computing (Second edition). </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: In order to explore the sensitivity of the present algorithms to dependencies between successive training patterns, we repeated the above experiment while sampling the input space according to the Sobol sequence <ref> (Press et al., 1992, pp. 311-314) </ref>. This deterministic, fractal sequence is designed to cover the input space super-uniformly (see Figure 3, center), and thus can sometimes accelerate convergence relative to a naive Monte Carlo approach.
Reference: <author> Puskorius, G. V., & Feldkamp, L. A. </author> <year> (1991). </year> <title> Decoupled extended Kalman filter training of feedforward layered networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> Vol. I, </volume> <pages> pp. </pages> <address> 771-777 Seattle, WA. </address> <note> IEEE. 10 Schraudolph, </note> <author> N. N. </author> <year> (1998). </year> <title> A fast, compact approximation of the exponen-tial function. </title> <type> Tech. rep. </type> <institution> IDSIA-07-98, Istituto Dalle Molle di Studi sull'Intelligenza Artificiale, </institution> <note> Corso Elvezia 36, 6900 Lugano, Switzer-land. To appear in Neural Computation. ftp://ftp.idsia.ch/pub/nic/- exp.ps.gz fl. </note>
Reference: <author> Schraudolph, N. N., & Sejnowski, T. J. </author> <year> (1996). </year> <title> Tempering backpropagation networks: Not all weights are created equal. </title> <editor> In Touretzky, D. S., Mozer, M. C., & Hasselmo, M. E. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 8, </volume> <pages> pp. 563-569. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: 1 Introduction For neural network learning algorithms based on simple gradient descent, an appropriate choice of step size for each weight is crucial to achieve efficient learning <ref> (Schraudolph & Sejnowski, 1996) </ref>.
Reference: <author> Shah, S., Palmieri, F., & Datum, M. </author> <year> (1992). </year> <title> Optimal filtering algorithms for fast learning in feedforward neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 779-787. </pages>
Reference: <author> Silva, F. M., & Almeida, L. B. </author> <year> (1990). </year> <title> Speeding up back-propagation. </title> <editor> In Eck-miller, R. (Ed.), </editor> <booktitle> Advanced Neural Computers, </booktitle> <pages> pp. </pages> <address> 151-158 Amsterdam. </address> <publisher> Elsevier. </publisher>
Reference: <author> Singhal, S., & Wu, L. </author> <year> (1989). </year> <title> Training multilayer perceptrons with the extended Kalman filter. </title> <editor> In Touretzky, D. S. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems. Proceedings of the 1988 Conference, </booktitle> <pages> pp. </pages> <address> 133-140 San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: in this order: 1. propagate activity forward (1), 2. backpropagate error to obtain ffi, 3. update learning rates ~p (5), 4. calculate gain vectors ~ k (6), 5. update the ~w (2) and ~ h (9). 3 Benchmark Comparison We evaluated elk1 on the "four regions" classification task due to <ref> (Singhal & Wu, 1989) </ref>, a well-established benchmark problem (Puskorius & Feldkamp, 4 1991; Shah et al., 1992; Yu et al., 1993; Plumer, 1995).
Reference: <author> Sutton, R. S. </author> <year> (1992a). </year> <title> Adapting bias by gradient descent: an incremental version of delta-bar-delta. </title> <booktitle> In Proc. 10th National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 171-176 ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/- sutton-92a.ps.gz fl. </address> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Sutton, R. S. </author> <year> (1992b). </year> <title> Gain adaptation beats least squares?. </title> <booktitle> In Proc. 7th Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pp. </pages> <address> 161-166 ftp://ftp.cs.- umass.edu/pub/anw/pub/sutton/sutton-92b.ps.gz fl. </address>
Reference-contexts: K1 <ref> (Sutton, 1992b) </ref> uses the diagonal approximation P j diag (e ~q ) whose parameter vector ~q is adapted online. <p> Note that each element of this factor is guaranteed to lie between zero and one; a positive-bounding operation as in <ref> (Sutton, 1992b) </ref> is therefore not necessary. Initially we set ~ h 0. In order to make immediate use of the latest information available, the gain is adapted before the weights for each new pattern.
Reference: <author> Vogl, T. P., Mangis, J. K., Rigler, A. K., Zink, W. T., & Alkon, D. L. </author> <year> (1988). </year> <title> Accelerating the convergence of the back-propagation method. </title> <journal> Biological Cybernetics, </journal> <volume> 59, </volume> <pages> 257-263. </pages>
Reference: <author> Williams, R. J. </author> <year> (1992). </year> <title> Some observations on the use of the extended Kalman filter as a recurrent network learning algorithm. </title> <type> Tech. rep. </type> <institution> NU-CCS-92-1, College of Computer Science, Northeastern University, </institution> <address> Boston, MA 02115. </address>
Reference: <author> Yu, X.-H., Chen, G.-A., & Cheng, S.-X. </author> <year> (1993). </year> <title> Acceleration of backpropagation learning using optimised learning rate and momentum. </title> <journal> Electronics Letters, </journal> <volume> 29 (14), </volume> <pages> 1288-1290. </pages>
Reference: <author> Zimmermann, H. G. </author> <year> (1994). </year> <editor> Neuronale Netze als Entscheidungskalkul. In Rehkugler, H., & Zimmermann, H. G. (Eds.), Neuronale Netze in der Okonomie: </editor> <booktitle> Grundlagen und finanzwirtschaftliche Anwendungen, </booktitle> <pages> pp. 1-87. </pages> <publisher> Vahlen Verlag, </publisher> <address> Munich. </address> <month> 11 </month>
Reference-contexts: Given the inherent efficiency of stochastic gradient descent, it would be desirable to have comparable online methods to adapt local step sizes. The little-known vario-j approach <ref> (Zimmermann, 1994, page 48) </ref> has been found to work well in practice, but is a gradient normalization rather than gain adaptation technique. A local gain adaptation method for online optimization has recently been proposed by Almeida et al. (1997, 1998) but has not yet been widely tested. <p> We used "softmax" output units and a negated cross-entropy objective function E. We compared four online algorithms: gradient descent with momentum, vario-j <ref> (Zimmermann, 1994, page 48) </ref>, alap | normalized local step size adaptation as proposed by Almeida et al. (1998), and elk1.
References-found: 21

