URL: http://www.cs.tamu.edu/faculty/bhuyan/papers/hpca98r.ps
Refering-URL: http://www.cs.tamu.edu/faculty/bhuyan/
Root-URL: http://www.cs.tamu.edu
Email: E-mail: fravi,bhuyang@cs.tamu.edu  
Title: Switch Cache: A Framework for Improving the Remote Memory Access Latency of CC-NUMA Multiprocessors  
Author: Ravi Iyer and Laxmi Narayan Bhuyan 
Address: College Station, TX 77843-3112, USA.  
Affiliation: Department of Computer Science Texas A&M University  
Abstract: Cache coherent non-uniform memory access (CC-NUMA) multiprocessors continue to suffer from remote memory access latencies due to comparatively slow memory technology and data transfer latencies in the interconnection network. In this paper, we propose a novel hardware caching technique, called switch cache. The main idea is to implement small fast caches in crossbar switches of the interconnect medium to capture and store shared data as they flow from the memory module to the requesting processor. This stored data acts as a cache for subsequent requests, thus reducing the latency of remote memory accesses tremendously. The implementation of a cache in a crossbar switch needs to be efficient and robust, yet flexible for changes in the caching protocol. The design and implementation details of a CAche Embedded Switch ARchitecture, CAESAR, using wormhole routing with virtual channels is presented. Using detailed execution-driven simulations, we find that the CAESAR switch cache is capable of improving the performance of CC-NUMA multiprocessors by reducing the number of reads served at distant remote memories by up to 45% and improving the application execution time by as high as 20%. We conclude that the switch caches provide a cost-effective solution for designing high performance CC-NUMA multiprocessors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Astfalk et al., </author> <title> An Overview of the HP/Convex Exemplar Hardware, </title> <address> http://www.convex.com/tech cache/ps/hw ov.ps. </address>
Reference-contexts: 1. Introduction To alleviate the problem of high memory access latencies, shared memory multiprocessors employ processors with small fast on-chip caches and additionally larger off-chip caches. To build high performance systems that are highly scalable, several current systems <ref> [1, 9, 11, 12] </ref> employ the CC-NUMA architecture. In such a system, the shared memory is distributed among all the nodes in the system to provide a closer local memory and several remote memories. <p> Another alternative is the use of network caches or remote data caches [13, 22]. The HP Exemplar <ref> [1] </ref> implements the network cache as a configurable partition of the local memory. Sequent's NUMA-Q [12] dedicates a 32MB DRAM memory for the network cache. The DASH multiprocessor [11] has provision for a network cache called the remote access cache.
Reference: [2] <author> BBN Laboratories Inc., </author> <title> Butterfly Parallel Processor Overview, </title> <type> version 1, </type> <month> Dec. </month> <year> 1985. </year>
Reference-contexts: These features of the MIN make it (a) FWA (b) GS (c) GAUSS (d) SOR (e) FFT (f) MATMUL very attractive as scalable high performance interconnects for commercial systems. Existing systems such as Butterfly <ref> [2] </ref>, CM-5 [10] and IBM SP2 [19] employ a bidirectional MIN. In this paper, the switch cache interconnect is a bidirectional MIN to take advantage of the inherent tree structure.
Reference: [3] <author> L. Bhuyan, et al., </author> <title> The Impact of Switch Design on the Application Performance of Shared Memory Multiprocessors, </title> <booktitle> International Parallel Processing Symposium, </booktitle> <month> Mar </month> <year> 1998. </year>
Reference-contexts: The use of a switch cache relaxes the time constraint, enables several memory read requests to be satisfied in the switch and avoids the need to access the slow main memory. Our recent study <ref> [3] </ref> indicates that increasing the buffer size beyond a certain value in a switch does not have much impact on the application performance for a shared memory multiprocessor. Thus we think that the large amount of buffers in current switches, such as SPIDER [7], is an overkill.
Reference: [4] <author> J. Carbonaro and F. Verhoorn, Cavallino: </author> <title> The Teraflops Router and NIC, </title> <booktitle> Proc. Symp. High Performance Interconnects (Hot Interconnects 4), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: The caching technique is implemented at the send module of the network interface where messages are prepared to be sent over the network. In a wormhole-routed network, messages are made up of flow control digits or flits. Each flit is 8 bytes as in Spider [7] and Cavallino <ref> [4] </ref>. The message header contains the routing information, while data flits follow the path created by the header. The format of the message header is shown in Figure 7. To implement the caching technique, we require that the header consist of 3 additional bits of information. <p> Crossbar switches mainly differ in two design issues: switching technique and buffer management. As mentioned earlier, we use wormhole routing as the switching technique and input buffering with virtual channels [6] since these are prevalent in current commercial crossbar switches <ref> [4, 7] </ref>. 3.1. The Switch Cache Organization Our base bi-directional crossbar switch has four inputs and four outputs as shown in Figure 8. Each input link in the crossbar switch has two virtual channels thus providing 8 possible input candidates for arbitration. <p> The arbitration process is the age technique, similar to that employed in the SGI Spider Switch [7]. At each arbitration cycle, a maximum of 4 highest age flits are selected from 8 possible arbitration candidates. The internal switch core and link transmission operates at 200MHz like Cavallino <ref> [4] </ref>. The wire width at the link is w = 16 bits. The 8 fi 4 crossbar takes 4 cycles to arbitrate and move flits from the input to the link transmitter at the output. <p> Thus, it takes four cycles for the link to transmit a flit from one switch to another. The resultant throughput of the switch is 1 flit served per cycle. Note that the wire width and flit size parameters are the same as in the Cav-allino switch <ref> [4] </ref>. Buffering in the crossbar switch is provided at the input block at each link. The input block is organized as a fixed size FIFO buffer for each virtual channel that stores flits belonging to a single message at a time.
Reference: [5] <author> L. M. Censier and P. Feautrier, </author> <title> A New Solution to Coherence Problems in Multicache Systems, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-27, no. 12, </volume> <pages> pp. 11121118, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: The Caching Protocol The introduction of processor caches in a multiprocessor introduces the cache coherence problem. Many hardware cache-coherent systems employ a full-map directory scheme <ref> [5] </ref>, In this scheme, each node maintains a bit vector to keep track of all the sharers of each block in its local shared memory space. <p> The raw memory access time is 40 cycles, but it takes more than 50 cycles to submit the request to the memory subsystem and read the data over the memory bus. The system employs the full-map three-state directory protocol <ref> [5] </ref> and the MSI cache protocol to maintain cache coherence. The system uses a release consistency model. The system employs a wormhole routed bidirectional MIN using 4 fi 4 switches organized in 4 stages as shown earlier in Figure 5.
Reference: [6] <author> W. J. Dally, </author> <title> Virtual-Channel Flow Control, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 3, no. 2, </volume> <pages> pp. </pages> <address> 194205, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: The contribution of this paper is the detailed design and performance evaluation of a switch cache interconnect employing CAESAR, a CAche Embedded Switch ARchitecture. The CAESAR switch cache is a dual-ported SRAM cache operating at the same speed as a wormhole routed crossbar switch with virtual channels <ref> [6] </ref>. The switch design is optimized to maintain crossbar bandwidth and throughput, while at the same time providing sufficient switch cache throughput and improved remote access performance. The performance evaluation of the switch cache interconnect is conducted using six scientific applications. <p> In general, an N -node system using a BMIN comprises of N=k switching elements (a 2k fi 2k crossbar) in each of the log k N stages connected by bidirectional links. We chose wormhole routing with virtual channels <ref> [6] </ref> as the switching technique because it is prevalent in current systems such as the SGI Origin [9]. In a shared memory system, communication between nodes is accomplished via read/write transactions and coherence requests/acknowledgements. <p> Crossbar Switch Cache Design Crossbar switches provide an excellent building block for scalable high performance interconnects. Crossbar switches mainly differ in two design issues: switching technique and buffer management. As mentioned earlier, we use wormhole routing as the switching technique and input buffering with virtual channels <ref> [6] </ref> since these are prevalent in current commercial crossbar switches [4, 7]. 3.1. The Switch Cache Organization Our base bi-directional crossbar switch has four inputs and four outputs as shown in Figure 8.
Reference: [7] <author> M. Galles, </author> <title> Scalable Pipelined Interconnect for Distributed Endpoint Routing: The SGI SPIDER Chip, </title> <booktitle> Proc. Symp. High Performance Interconnects (Hot Interconnects 4), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Our recent study [3] indicates that increasing the buffer size beyond a certain value in a switch does not have much impact on the application performance for a shared memory multiprocessor. Thus we think that the large amount of buffers in current switches, such as SPIDER <ref> [7] </ref>, is an overkill. A better utilization of these buffers can be accomplished by organizing them as a switch cache. The contribution of this paper is the detailed design and performance evaluation of a switch cache interconnect employing CAESAR, a CAche Embedded Switch ARchitecture. <p> The caching technique is implemented at the send module of the network interface where messages are prepared to be sent over the network. In a wormhole-routed network, messages are made up of flow control digits or flits. Each flit is 8 bytes as in Spider <ref> [7] </ref> and Cavallino [4]. The message header contains the routing information, while data flits follow the path created by the header. The format of the message header is shown in Figure 7. To implement the caching technique, we require that the header consist of 3 additional bits of information. <p> Crossbar switches mainly differ in two design issues: switching technique and buffer management. As mentioned earlier, we use wormhole routing as the switching technique and input buffering with virtual channels [6] since these are prevalent in current commercial crossbar switches <ref> [4, 7] </ref>. 3.1. The Switch Cache Organization Our base bi-directional crossbar switch has four inputs and four outputs as shown in Figure 8. Each input link in the crossbar switch has two virtual channels thus providing 8 possible input candidates for arbitration. <p> Each input link in the crossbar switch has two virtual channels thus providing 8 possible input candidates for arbitration. The arbitration process is the age technique, similar to that employed in the SGI Spider Switch <ref> [7] </ref>. At each arbitration cycle, a maximum of 4 highest age flits are selected from 8 possible arbitration candidates. The internal switch core and link transmission operates at 200MHz like Cavallino [4]. The wire width at the link is w = 16 bits.
Reference: [8] <author> A. Gottlieb et al., </author> <title> The NYU Ultracomputer: Designinga MIMD Shared-Memory Parallel Computer, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-32, no. 2, </volume> <pages> pp. 175-189, </pages> <month> Feb. </month> <year> 1983. </year>
Reference-contexts: Such a scheme can be considered as a multi-level caching scheme without inclusion property. Our studies on application behavior indicate that there is enough spatial and temporal locality between requests from processors. The request-combining technique employed in the NYU Ultracomputer <ref> [8] </ref> was also based on such an observation. However, their technique relies heavily on requests being generated simultaneously and is suitable more to synchronization traffic.
Reference: [9] <author> J. Laudon and D. Lenoski, </author> <title> The SGI Origin: A ccNUMA Highly Scalable Server, </title> <booktitle> Proceedings of 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 241-251, </pages> <year> 1997. </year>
Reference-contexts: 1. Introduction To alleviate the problem of high memory access latencies, shared memory multiprocessors employ processors with small fast on-chip caches and additionally larger off-chip caches. To build high performance systems that are highly scalable, several current systems <ref> [1, 9, 11, 12] </ref> employ the CC-NUMA architecture. In such a system, the shared memory is distributed among all the nodes in the system to provide a closer local memory and several remote memories. <p> In this paper, the switch cache interconnect is a bidirectional MIN to take advantage of the inherent tree structure. Note, however, that logical trees can also be embedded on other popular direct networks like the hypercube used in SGI Origin <ref> [9] </ref>. The baseline topology of the 16-node bidirectional MIN (BMIN) is shown in Figure 5a. In general, an N -node system using a BMIN comprises of N=k switching elements (a 2k fi 2k crossbar) in each of the log k N stages connected by bidirectional links. <p> We chose wormhole routing with virtual channels [6] as the switching technique because it is prevalent in current systems such as the SGI Origin <ref> [9] </ref>. In a shared memory system, communication between nodes is accomplished via read/write transactions and coherence requests/acknowledgements. The read/write requests and coherence replies from the processor to the memory use the forward links to traverse through the switches.
Reference: [10] <author> C. E. Leiserson etal, </author> <title> The Network Architecture of the Connection Machine CM-5, </title> <booktitle> Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 272-285, </pages> <year> 1992. </year>
Reference-contexts: Consequently, interconnect topologies based on tree structures can be considered to be the next best alternative to global caching schemes. Tree-based networks like the fat tree <ref> [10] </ref> , the hierarchical bus network [20] and the multistage interconnection network (MIN) [14] provide hierarchical topologies suitable for global caching. While offering an inherent tree-structure, the MIN is also highly scalable, and it provides a bisection bandwidth that scales linearly with the number of nodes in the system. <p> These features of the MIN make it (a) FWA (b) GS (c) GAUSS (d) SOR (e) FFT (f) MATMUL very attractive as scalable high performance interconnects for commercial systems. Existing systems such as Butterfly [2], CM-5 <ref> [10] </ref> and IBM SP2 [19] employ a bidirectional MIN. In this paper, the switch cache interconnect is a bidirectional MIN to take advantage of the inherent tree structure. Note, however, that logical trees can also be embedded on other popular direct networks like the hypercube used in SGI Origin [9].
Reference: [11] <author> D. Lenoski et al., </author> <title> The Stanford DASH Multiprocessor, </title> <journal> IEEE Computer, </journal> <volume> 25(3), </volume> <pages> pages 63-79, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: 1. Introduction To alleviate the problem of high memory access latencies, shared memory multiprocessors employ processors with small fast on-chip caches and additionally larger off-chip caches. To build high performance systems that are highly scalable, several current systems <ref> [1, 9, 11, 12] </ref> employ the CC-NUMA architecture. In such a system, the shared memory is distributed among all the nodes in the system to provide a closer local memory and several remote memories. <p> Another alternative is the use of network caches or remote data caches [13, 22]. The HP Exemplar [1] implements the network cache as a configurable partition of the local memory. Sequent's NUMA-Q [12] dedicates a 32MB DRAM memory for the network cache. The DASH multiprocessor <ref> [11] </ref> has provision for a network cache called the remote access cache. Moga et al.[13] explore the use of SRAM network caches integrated with a page cache.
Reference: [12] <author> T. Lovett and R. Clapp., STiNG: </author> <title> A CC-NUMA Computer System for the Commercial Marketplace., </title> <booktitle> Proceedings of the 23rd Annual International Symposium on Computer Architecture., </booktitle> <pages> pages 308-317. </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: 1. Introduction To alleviate the problem of high memory access latencies, shared memory multiprocessors employ processors with small fast on-chip caches and additionally larger off-chip caches. To build high performance systems that are highly scalable, several current systems <ref> [1, 9, 11, 12] </ref> employ the CC-NUMA architecture. In such a system, the shared memory is distributed among all the nodes in the system to provide a closer local memory and several remote memories. <p> Another alternative is the use of network caches or remote data caches [13, 22]. The HP Exemplar [1] implements the network cache as a configurable partition of the local memory. Sequent's NUMA-Q <ref> [12] </ref> dedicates a 32MB DRAM memory for the network cache. The DASH multiprocessor [11] has provision for a network cache called the remote access cache. Moga et al.[13] explore the use of SRAM network caches integrated with a page cache.
Reference: [13] <author> A. Moga and M. Dubois., </author> <title> The Effectiveness of SRAM Network Caches on Clustered DSMs, </title> <booktitle> Proceesings of the 4th International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 103-112, </pages> <month> Feb. </month> <year> 1998. </year>
Reference-contexts: To reduce the impact of remote memory access latencies, researchers have proposed improved caching strategies <ref> [13, 15, 22] </ref> within each cluster of the multiprocessor. These caching techniques are primarily based on data sharing among multiple processors within the same cluster. <p> Nayfeh et al. [15] explore the use of shared L2 caches to reduce communication misses between processors within the fl This research has been supported by NSF grants MIP CCR-9622740 and CCR-9810205. cluster. Another alternative is the use of network caches or remote data caches <ref> [13, 22] </ref>. The HP Exemplar [1] implements the network cache as a configurable partition of the local memory. Sequent's NUMA-Q [12] dedicates a 32MB DRAM memory for the network cache. The DASH multiprocessor [11] has provision for a network cache called the remote access cache.
Reference: [14] <author> A. Nanda and L. Bhuyan, </author> <title> Design and Analysis of Cache Coherent Multistage Interconnection Networks, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 42, no. 4, </volume> <month> April </month> <year> 1993. </year>
Reference-contexts: Consequently, interconnect topologies based on tree structures can be considered to be the next best alternative to global caching schemes. Tree-based networks like the fat tree [10] , the hierarchical bus network [20] and the multistage interconnection network (MIN) <ref> [14] </ref> provide hierarchical topologies suitable for global caching. While offering an inherent tree-structure, the MIN is also highly scalable, and it provides a bisection bandwidth that scales linearly with the number of nodes in the system.
Reference: [15] <author> B. Nayfeh, et al., </author> <title> The Impact of Shared-Cache Clustering in Small-Scale Shared-Memory Multiprocessors, </title> <booktitle> Pro-ceesings of the 2nd International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 74-84, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: To reduce the impact of remote memory access latencies, researchers have proposed improved caching strategies <ref> [13, 15, 22] </ref> within each cluster of the multiprocessor. These caching techniques are primarily based on data sharing among multiple processors within the same cluster. <p> To reduce the impact of remote memory access latencies, researchers have proposed improved caching strategies [13, 15, 22] within each cluster of the multiprocessor. These caching techniques are primarily based on data sharing among multiple processors within the same cluster. Nayfeh et al. <ref> [15] </ref> explore the use of shared L2 caches to reduce communication misses between processors within the fl This research has been supported by NSF grants MIP CCR-9622740 and CCR-9810205. cluster. Another alternative is the use of network caches or remote data caches [13, 22].
Reference: [16] <author> V. Pai et al., </author> <note> RSIM Reference Manual. Version 1.0, </note> <institution> Department of Electrical and Computer Engineering, Rice University. </institution> <type> Technical Report 9705. </type> <month> July </month> <year> 1997. </year>
Reference-contexts: Simulation Methodology To evaluate the performance impact of switch caches on the application performance of CC-NUMA multiprocessors, we use a modified version of Rice Simulator for ILP Multiprocessors (RSIM) <ref> [16] </ref>. RSIM is an execution driven simulator for shared memory multiprocessors with accurate models of current processors that exploit instruction-level parallelism. The base system configuration consists of 16 nodes.
Reference: [17] <author> T. Shanley, </author> <title> Pentium Pro Processor System Architecture, </title> <publisher> MindShare Inc., Addison-Wesley Publishing Company, </publisher> <month> April </month> <year> 1997. </year>
Reference: [18] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory, </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(1):544, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: These applications are Floyd-Warshall's all-pair-shortest-path algorithm, Gaussian elimination (GE), QR factorization using the Gram-Schmidt Algorithm (GS) and the multiplication of 2D matrices (MATMUL), successive over-relaxation of a grid (SOR) and the six-step 1D fast fourier transform (FFT) from SPLASH <ref> [18] </ref>. The input data sizes are shown in Table 1 and the sharing characteristics were discussed in Section 2.1. 4.2.
Reference: [19] <author> C. B. Stunkel et al., </author> <title> The SP2 High Performance Switch, </title> <journal> IBM Systems Journal, </journal> <volume> vol. 34, no. 2, </volume> <pages> pp. 185-204, </pages> <year> 1995 </year>
Reference-contexts: These features of the MIN make it (a) FWA (b) GS (c) GAUSS (d) SOR (e) FFT (f) MATMUL very attractive as scalable high performance interconnects for commercial systems. Existing systems such as Butterfly [2], CM-5 [10] and IBM SP2 <ref> [19] </ref> employ a bidirectional MIN. In this paper, the switch cache interconnect is a bidirectional MIN to take advantage of the inherent tree structure. Note, however, that logical trees can also be embedded on other popular direct networks like the hypercube used in SGI Origin [9].
Reference: [20] <author> A.W. Wilson, </author> <title> Hierarchical cache/bus architecture for shared memory multiprocessors, </title> <booktitle> Proc.14th Ann. Int'l. Symp. on Comp. Arch., </booktitle> <pages> pp. 244-252, </pages> <year> 1987. </year>
Reference-contexts: Consequently, interconnect topologies based on tree structures can be considered to be the next best alternative to global caching schemes. Tree-based networks like the fat tree [10] , the hierarchical bus network <ref> [20] </ref> and the multistage interconnection network (MIN) [14] provide hierarchical topologies suitable for global caching. While offering an inherent tree-structure, the MIN is also highly scalable, and it provides a bisection bandwidth that scales linearly with the number of nodes in the system.
Reference: [21] <author> S. Wilton and N. Jouppi, </author> <title> An Enhanced Access and Cycle Time Model for On-Chip Caches, </title> <type> Technical Report #93/5, </type> <institution> DEC-Western Research Lab, </institution> <year> 1994. </year>
Reference-contexts: Thus the maximum number of switch cache inputs are 4 requests, as can be seen in Figure 9. The cycle time and access time of an SRAM cache depends on several factors such as associativity, cache output width, number of wordlines, number of bitlines and cache size <ref> [21] </ref>. The CACTI model [21] shows that direct mapped caches have low cycle times since a direct indexing method is used to locate the line, but have poor hit ratios due to mapping conflicts in the cache. <p> The cycle time and access time of an SRAM cache depends on several factors such as associativity, cache output width, number of wordlines, number of bitlines and cache size <ref> [21] </ref>. The CACTI model [21] shows that direct mapped caches have low cycle times since a direct indexing method is used to locate the line, but have poor hit ratios due to mapping conflicts in the cache. <p> Furthermore, most current processors employ multi-ported two-way set associative L1 caches operating within a single processor cycle. Cache output width is also an important issue that primarily affects the data read/write delay. As studied by Wilton et al. <ref> [21] </ref>, the increase in data array width increases the number of sense amplifiers required. The organization of the cache can also make a significant difference in terms of chip area. Narrower caches provide data in multiple cycles, thus increasing the cache access time for an average read request.
Reference: [22] <author> Z. Zhang and J. Torellas., </author> <title> Reducing Remote Conflict Misses: NUMA with Remote Cache versus COMA, </title> <booktitle> Proceedings of the 3rd International Symposium on High Performance Computer Architecture, </booktitle> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: To reduce the impact of remote memory access latencies, researchers have proposed improved caching strategies <ref> [13, 15, 22] </ref> within each cluster of the multiprocessor. These caching techniques are primarily based on data sharing among multiple processors within the same cluster. <p> Nayfeh et al. [15] explore the use of shared L2 caches to reduce communication misses between processors within the fl This research has been supported by NSF grants MIP CCR-9622740 and CCR-9810205. cluster. Another alternative is the use of network caches or remote data caches <ref> [13, 22] </ref>. The HP Exemplar [1] implements the network cache as a configurable partition of the local memory. Sequent's NUMA-Q [12] dedicates a 32MB DRAM memory for the network cache. The DASH multiprocessor [11] has provision for a network cache called the remote access cache.
References-found: 22

