URL: http://www.cs.caltech.edu/~eric/papers/telescope.ps
Refering-URL: http://www.cs.caltech.edu/~eric/papers/papers.html
Root-URL: http://www.cs.caltech.edu
Email: eric@cs.caltech.edu  
Title: Improved Hoeffding-Style Performance Guarantees for Accurate Classifiers  
Author: Eric Bax 
Date: March 5, 1998  
Address: Pasadena, CA 91125  
Affiliation: Computer Science Dept. California Institute of Technology 256-80  
Abstract: We extend Hoeffding bounds to develop superior probabilistic performance guarantees for accurate classifiers. The original Hoeffding bounds on classifier accuracy depend on the accuracy itself as a parameter. Since the accuracy is not known a priori, the parameter value that gives the weakest bounds is used. We present a method that loosely bounds the accuracy using the old method and uses the loose bound as an improved parameter value for tighter bounds. We show how to use the bounds in practice, and we generalize the bounds for individual classifiers to form uniform bounds over multiple classifiers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Abu-Mostafa, </author> <title> What you need to know about the VC Inequality, Class notes from CS156, </title> <institution> California Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: The uniform bounds are derived using the fact that the sum of event probabilities bounds the probability of the union of events. P rf 1 x 1 t or : : : or M x M tg (13) M e (15) In practice, this bound is used as follows <ref> [1, 2, 3, 4] </ref>. Several classifiers are developed using the training data. Then the classifier with the highest validation success rate is used, and the others are discarded.
Reference: [2] <author> E. Bax, Z. Cataltepe, and J. Sill, </author> <title> Alternative error bounds for the classifier chosen by early stopping, </title> <booktitle> Proc. IEEE Pacific Rim Conf. on Communications, Computers, and Signal Processing, </booktitle> <address> Victoria, B.C., Canada. </address> (1997):811-814. 
Reference-contexts: The uniform bounds are derived using the fact that the sum of event probabilities bounds the probability of the union of events. P rf 1 x 1 t or : : : or M x M tg (13) M e (15) In practice, this bound is used as follows <ref> [1, 2, 3, 4] </ref>. Several classifiers are developed using the training data. Then the classifier with the highest validation success rate is used, and the others are discarded.
Reference: [3] <author> E. Bax, </author> <title> Validation of voting committees, </title> <note> to appear in Neural Computation. </note>
Reference-contexts: The uniform bounds are derived using the fact that the sum of event probabilities bounds the probability of the union of events. P rf 1 x 1 t or : : : or M x M tg (13) M e (15) In practice, this bound is used as follows <ref> [1, 2, 3, 4] </ref>. Several classifiers are developed using the training data. Then the classifier with the highest validation success rate is used, and the others are discarded.
Reference: [4] <author> E. Bax, </author> <title> Partition-based uniform error bounds, </title> <booktitle> to appear in the Proceedings of the 1998 IEEE World Congress on Computational Intelligence, </booktitle> <address> Anchorage, Alaska. </address>
Reference-contexts: The uniform bounds are derived using the fact that the sum of event probabilities bounds the probability of the union of events. P rf 1 x 1 t or : : : or M x M tg (13) M e (15) In practice, this bound is used as follows <ref> [1, 2, 3, 4] </ref>. Several classifiers are developed using the training data. Then the classifier with the highest validation success rate is used, and the others are discarded.
Reference: [5] <author> W. Feller, </author> <title> An Introduction to Probability Theory and Its Applications, </title> <publisher> John Wiley and Sons, Inc. </publisher> <year> 1968. </year>
Reference-contexts: We call this quantity the true success rate. Consider the random variable that has value 1 if the trained classifier and target function agree for a random input and 0 otherwise. This is a Bernoulli variable <ref> [5] </ref>. The mean is the true success rate. Each validation example supplies a sample of this random variable since we can check whether or not the classifier agrees with the target function.
Reference: [6] <author> W. Hoeffding, </author> <title> Probability inequalities for sums of bounded random variables, </title> <journal> Am. Stat. Assoc. J., </journal> <volume> 58 </volume> (1963):13-30. 
Reference-contexts: Each validation example supplies a sample of this random variable since we can check whether or not the classifier agrees with the target function. The average over these samples, which is the validation success rate, is an unbiased estimate of the true success rate. Hoeffding <ref> [6] </ref> gave bounds for the probability that the true success rate is less than the validation success rate by more than a given amount. This paper extends Hoeffding's bounds to develop bounds that are superior for classifiers that have a high success rate. <p> Then we generalize the bounds for single classifiers to form uniform bounds for several classifiers. We conclude with several challenges for future research. 2 Derivation First, we review the Hoeffding bounds <ref> [6] </ref>. Following Hoeffding's notation, let be the true success rate, let x be the validation success rate, and let n be the number of validation examples. Then for 0 &lt; t &lt; 1 , the following results are derived in [6]. <p> future research. 2 Derivation First, we review the Hoeffding bounds <ref> [6] </ref>. Following Hoeffding's notation, let be the true success rate, let x be the validation success rate, and let n be the number of validation examples. Then for 0 &lt; t &lt; 1 , the following results are derived in [6]. P rf x tg f ( + t +t 1 ) g (1) nt 2 g () where g () = 1 2 1 for 0 &lt; &lt; 2 and 1 for 2 Note that g () attains its minimum at = 1 2 . <p> Also, through scaling, the bounds may be applied to any independent variables with bounded ranges. (See Hoeffding's paper <ref> [6] </ref> for details.) 5 Conclusion We have extended the Hoeffding bounds to derive bounds that are superior for accurate classifiers. We have shown how to apply these bounds in practice, and we have shown that these bounds can be used for uniform bounds over multiple classifiers.
Reference: [7] <author> V. N. Vapnik, </author> <title> Estimation of Dependences Based on Empirical Data p.31, </title> <publisher> Springer-Verlag New York, Inc. </publisher> <year> 1982. </year>
Reference-contexts: For each s, there is a grid search for the minimum t in f0:0000; 0:0001; : : : ; sg that gives at least 95% confidence. 4 Generalizations The Hoeffding bound is the basis for the Vapnik-Chervonenkis bounds <ref> [7, 8, 9] </ref> in learning theory. The Vapnik-Chervonenkis bound is a uniform bound over multiple classifiers, i.e., it is a bound on the probability that one or more true success rates are at least t less than their corresponding validation success rates.
Reference: [8] <author> V. N. Vapnik, </author> <title> The Nature of Statistical Learning Theory, </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address> <year> 1995. </year>
Reference-contexts: For each s, there is a grid search for the minimum t in f0:0000; 0:0001; : : : ; sg that gives at least 95% confidence. 4 Generalizations The Hoeffding bound is the basis for the Vapnik-Chervonenkis bounds <ref> [7, 8, 9] </ref> in learning theory. The Vapnik-Chervonenkis bound is a uniform bound over multiple classifiers, i.e., it is a bound on the probability that one or more true success rates are at least t less than their corresponding validation success rates.
Reference: [9] <author> V. N. Vapnik and A. Chervonenkis, </author> <title> On the uniform convergence of relative frequencies of events to their probabilities, </title> <journal> Theory Prob. Appl., </journal> <volume> 16(1971) </volume> <pages> 264-280. 8 </pages>
Reference-contexts: For each s, there is a grid search for the minimum t in f0:0000; 0:0001; : : : ; sg that gives at least 95% confidence. 4 Generalizations The Hoeffding bound is the basis for the Vapnik-Chervonenkis bounds <ref> [7, 8, 9] </ref> in learning theory. The Vapnik-Chervonenkis bound is a uniform bound over multiple classifiers, i.e., it is a bound on the probability that one or more true success rates are at least t less than their corresponding validation success rates.
References-found: 9

