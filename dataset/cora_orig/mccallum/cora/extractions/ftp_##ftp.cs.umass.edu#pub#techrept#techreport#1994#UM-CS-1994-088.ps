URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1994/UM-CS-1994-088.ps
Refering-URL: http://ccs-www.cs.umass.edu/lory/publications.html
Root-URL: 
Email: e-mail: lory@cs.umass.edu, krithi@cs.umass.edu  
Title: Recovery Protocols for Cache-Coherent Shared Memory Operating Systems  
Author: Lory D. Molesky and Krithi Ramamritham 
Keyword: Shared Memory, Operating Systems, Cache Coherency, Crash Recovery, Volatile Logging  
Date: December 9, 1994  
Address: Amherst MA 01003-4610  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: Significant performance advantages can be achieved by implementing certain kernel data structures in the cache-coherent memory of a shared memory multiprocessor. However, even in multiprocessor systems which are capable of detecting and isolating node failures, implementing kernel data structures in shared memory can cause recovery problems. For example, in the disk buffer, when disk blocks are buffered in cache-coherent memory, unnecessary dependencies may form between processes which access these buffers. Because of these dependencies, the crash of one node may adversely affect the integrity of processes which execute on nodes other than the one(s) which failed, violating the forward progress of these processes. Moreover, resources held by processes running on crashed nodes may fail to be released, violating system liveness properties. In this paper, we propose low overhead recovery techniques which mask these dependencies. These techniques are based on compile time, runtime, and recovery time techniques. Low overheads are achieved by using protocols that to not need stable storage, since this may involve the more costly disk I/O. We consider the application of these recovery techniques to those kernel data structures which are suitable for implementation in cache-coherent shared memory. These kernel data structures include semaphores, maps, used to catalog disk usage, and the disk buffer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Archibald and J. Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: The shared memory is made coherent using a hardware-based cache coherency protocol. Hardware-based cache coherency provides low latency, high bandwidth access to shared memory, and ensures that each node reads the most recent version of the data in the shared address space <ref> [1, 9] </ref>. Each node has its own cache, and before an operation is performed on a data item, the data item must first be brought into the cache. <p> While the unit of I/O is a page, the unit of coherency is a cache line, and is typically smaller than a page. We consider recovery issues in the context the two major cache coherency protocols, write-invalidate and write-broadcast <ref> [1, 9] </ref>. In both these protocols, a cache line could be valid at multiple nodes after a series of read requests to the line have been issued.
Reference: [2] <author> M. Bach. </author> <title> The Design of the Unix Operating System. </title> <publisher> Prentice-Hall, </publisher> <year> 1991. </year>
Reference-contexts: Using the recovery techniques developed in sections 4.1 and 4.2, section 4.3 shows how the disk buffer can be recovered. 4.1 Binary Semaphores Binary semaphores are often used in multiprocessor operating systems to ensure that resource allocation is performed mutually exclusively <ref> [2, 3] </ref>. In section 4.1.1, we consider the approach of using dedicated cache lines to solve the update anomalies caused by cache coherency. <p> Specifically, a map is an array where each entry consists of an address of an allocatable resource and the number of resource units available there. In the Unix operating system, maps are used to control the allocation of resources on disk partitions designated as swap devices <ref> [2] </ref>. Swap devices are used both for swapping, where an entire process is transferred between primary memory and the swap device, and for demand paging, where individual pages are transferred between primary memory and the swap device. <p> This in-memory buffering of disk blocks is temporary since (a) the kernel may issue a flush command to force the block to disk, and (b) the buffer size is fixed at kernel initialization time, so only the most recently used disk blocks are buffered <ref> [2] </ref>. By maintaining the disk buffer in coherent shared memory, file system integrity can be maintained by serializing access by multiple processes to the same disk block. This strategy was adopted in the multiprocessor operating system of the AT&T 32B multiprocessor [3]. <p> For example, modifications to the free list may occur at the head, when any free block of the 3 Another name for the disk buffer is the buffer cache <ref> [2] </ref>. 15 disk buffer is required, or to the middle, when a specific free block is required 4 .
Reference: [3] <author> M. Bach and S. Buroff. </author> <title> Multiprocessor Unix Systems. </title> <journal> AT&T Bell Laboratories Technical Journal, </journal> <volume> 68(8) </volume> <pages> 1733-1750, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: Using the recovery techniques developed in sections 4.1 and 4.2, section 4.3 shows how the disk buffer can be recovered. 4.1 Binary Semaphores Binary semaphores are often used in multiprocessor operating systems to ensure that resource allocation is performed mutually exclusively <ref> [2, 3] </ref>. In section 4.1.1, we consider the approach of using dedicated cache lines to solve the update anomalies caused by cache coherency. <p> By maintaining the disk buffer in coherent shared memory, file system integrity can be maintained by serializing access by multiple processes to the same disk block. This strategy was adopted in the multiprocessor operating system of the AT&T 32B multiprocessor <ref> [3] </ref>. However, when implemented in coherent shared memory, we must consider the recovery problems which may be caused by cache coherency. Before discussing these recovery issues, we first discuss the structure of the disk buffer. The data structures used to structure the disk buffer support two basic types of requests. <p> When a buffer is allocated, the kernel marks it used. When the kernel is finished reading or writing the block, the used flag is cleared and it is returned to the free list. We next consider the recovery issues for the shared memory implementation of the disk buffer. In <ref> [3] </ref>, a semaphore is associated with each buffer in order to serialize access to a particular buffer. Since we have already discussed the recovery issues for semaphores, we focus on the free list and the hash table.
Reference: [4] <author> Motorola Inc. </author> <title> MC68020 32-Bit Microprocessor User's Manual. </title> <publisher> Prentice-hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1985. </year>
Reference-contexts: In these examples, we assume that x and y are different nodes. The crash of a particular node y is indicated with Crash y . Although semaphores are typically implemented with multiprocessor synchronization primitives, such as test-and-set (TAS) or compare-and-swap (CAS) <ref> [4] </ref>, for clarity of the presentation, we denote a successful semaphore acquisition with an increment (inc) operation. <p> Note that for redo logging, it is not necessary to guarantee logging before migration. For semaphores, this guarantee of undo logging before migration can be implemented with existing multiprocessor synchronization techniques, such as and cache line locks [13], and compare-and-swap (CAS) <ref> [4] </ref>. The line lock is a primitive which locks a line into the cache until it is released. Ensuring that the log record is written prior to migration can be accomplished as follows.
Reference: [5] <author> D. Johnson and W. Zwaenepoel. </author> <title> Sender-Based Message Logging. </title> <booktitle> Proceedings of the 17th International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 14-19, </pages> <year> 1987. </year>
Reference-contexts: Alternatively, the use of (volatile) redo logging provides sufficient information for restart recovery to restore lost updates. 6 Related Work While our work addresses recovery issues in the context of cache-coherent shared memory multiprocessors, recovery issues in the context of distributed computations in message-passing systems have been studied in <ref> [5, 14] </ref>. The process checkpointing schemes of [5, 14] ensure a consistent state of a distributed computation without necessitating the rollback of any processes other than ones that failed. <p> logging provides sufficient information for restart recovery to restore lost updates. 6 Related Work While our work addresses recovery issues in the context of cache-coherent shared memory multiprocessors, recovery issues in the context of distributed computations in message-passing systems have been studied in <ref> [5, 14] </ref>. The process checkpointing schemes of [5, 14] ensure a consistent state of a distributed computation without necessitating the rollback of any processes other than ones that failed. <p> In this case, a checkpoint consists of the necessary process state for restarting execution, such as the program counter, process identifier, and register contents. In <ref> [5, 14] </ref>, to ensure the forward progress of a distributed computation, messages sent between processes trigger log records to be written to volatile memory. Recovery of a failed process is achieved by restarting the failed process from its checkpoint and replaying the message from the sender's logs.
Reference: [6] <author> P. Keleher, A. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> 1994 Winter Usenix Conference, </booktitle> <year> 1994. </year>
Reference-contexts: At present, most shared memory systems are constructed as tightly-coupled multiprocessors. However, current technological trends, such as advances in processor and network technology [7], are enabling the emergence of loosely-coupled shared memory systems connected by local or wide area networks <ref> [8, 6, 15] </ref>. With current mechanisms, a single node failure is likely to require a reboot of the entire shared memory system.
Reference: [7] <author> H. T. Kung. </author> <title> Gigabit Local Area Networks: A Systems Perspective. </title> <journal> IEEE Communications Magazine, </journal> <pages> pages 79-89, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: These systems have been successful in supporting both parallel and timesharing applications. At present, most shared memory systems are constructed as tightly-coupled multiprocessors. However, current technological trends, such as advances in processor and network technology <ref> [7] </ref>, are enabling the emergence of loosely-coupled shared memory systems connected by local or wide area networks [8, 6, 15]. With current mechanisms, a single node failure is likely to require a reboot of the entire shared memory system.
Reference: [8] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> Proceedings of the 21th International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This recovery strategy is motivated by proposed enhancements to multiprocessor failure models where certain low-level mechanisms are in place to detect and isolate hardware faults <ref> [8] </ref>. However, these alone are not enough to guarantee that unnecessary failure dependencies do not form between nodes. Our recovery techniques consist of low overhead software mechanisms which compensate for the unwanted and deleterious side effects of the cache coherency protocol. <p> The crash of node x does not destroy the contents of volatile memory located on other nodes. This assumption is motivated by proposed enhancements to shared memory architectures whereby individual node failures are independent <ref> [8] </ref>. In the event of the crash of some node x, our recovery objective is to abort the kernel on node x, but allow all kernels executing on surviving nodes to continue execution in a consistent global system state. <p> The drawback of dedicated cache lines is that a lot of space will be wasted. For example, on the KSR-1 and KSR-2 multiprocessors [13], and on Stanford's FLASH <ref> [8] </ref> distributed shared memory machine, the cache line size is 128 bytes. On these architectures, dedicating an entire line for a semaphore (which typically consume 1 - 4 bytes), would waste the better portion of the cache line. <p> At present, most shared memory systems are constructed as tightly-coupled multiprocessors. However, current technological trends, such as advances in processor and network technology [7], are enabling the emergence of loosely-coupled shared memory systems connected by local or wide area networks <ref> [8, 6, 15] </ref>. With current mechanisms, a single node failure is likely to require a reboot of the entire shared memory system.
Reference: [9] <author> D. Lilja. </author> <title> Cache Coherence in Large-Scale Shared-Memory Multiprocessors: Issues and Comparisons. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(3) </volume> <pages> 303-338, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The shared memory is made coherent using a hardware-based cache coherency protocol. Hardware-based cache coherency provides low latency, high bandwidth access to shared memory, and ensures that each node reads the most recent version of the data in the shared address space <ref> [1, 9] </ref>. Each node has its own cache, and before an operation is performed on a data item, the data item must first be brought into the cache. <p> While the unit of I/O is a page, the unit of coherency is a cache line, and is typically smaller than a page. We consider recovery issues in the context the two major cache coherency protocols, write-invalidate and write-broadcast <ref> [1, 9] </ref>. In both these protocols, a cache line could be valid at multiple nodes after a series of read requests to the line have been issued.
Reference: [10] <author> L. Lin and M. Ahamad. </author> <title> Checkpointing and Rollback-Recovery in Distributed Object Based Systems. </title> <booktitle> Proceedings of the 20th International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 97-104, </pages> <year> 1990. </year>
Reference-contexts: In this system, checkpointing is used to prevent rollback propagation between nodes before a requested page is sent so some process q, if it has been modified by p since p's last checkpoint, then a checkpoint of p is initiated. In <ref> [10] </ref>, a process checkpointing scheme for avoiding rollback dependencies in distributed object based systems is discussed. In this work, dependencies between processors form when threads are invoked in the address space of an object located on a remote node.
Reference: [11] <author> C. Mohan, D. Haderle, B. Lindsay, H. Pirahesh, and P. Schwarz. </author> <title> ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 17 </volume> <pages> 94-162, </pages> <month> March </month> <year> 1992. </year> <month> 21 </month>
Reference-contexts: Similar issues arise in database systems, when transaction checkpoints are issued, or active transactions commit updates to internal system structures regardless of the commit or abort of the updating transaction <ref> [11] </ref>. The recovery methods for cache-coherent shared memory operating systems presented in this paper are suitable for providing low-level recovery support for applications such as database systems. However, when applications exploit shared memory, additional recovery problems arise. For example, lost or spurious record updates, inserts, or deletes may a*ict transactions.
Reference: [12] <author> L. Molesky and K. Ramamritham. </author> <title> Recovery Protocols for Shared Memory Database Systems. </title> <booktitle> Submitted to the 1995 ACM SIGMOD International Conference on Management of Data. </booktitle>
Reference-contexts: Instead, we wish to guarantee that, if the active allocation (an allocation remains active until the corresponding deallocation is issued) migrates, so does the undo log record corresponding to this active allocation. One technique for implementing this undo logging technique is to use in-line logging <ref> [12] </ref>. With in-line logging, undo log records are stored in the same cache line where the allocation has been performed. In most cases, an in-line log record requires only a small amount of information to be written, such as a node identifier and an undo operation code. <p> However, when applications exploit shared memory, additional recovery problems arise. For example, lost or spurious record updates, inserts, or deletes may a*ict transactions. In a related paper <ref> [12] </ref>, we discuss methods for avoiding unnecessary transaction 19 aborts in a cache-coherent shared memory database system. 7 Concluding Remarks Shared memory systems offer significant performance advantages for applications which share data. These systems have been successful in supporting both parallel and timesharing applications.
Reference: [13] <author> Kendall Square Research. </author> <title> KSR1 Principles of Operation. KSR Research, </title> <address> Waltham, Mass., </address> <year> 1992. </year>
Reference-contexts: Dedicated cache lines are appropriate if the cache line size is relatively small, and/or the number of semaphores in the system is small. The drawback of dedicated cache lines is that a lot of space will be wasted. For example, on the KSR-1 and KSR-2 multiprocessors <ref> [13] </ref>, and on Stanford's FLASH [8] distributed shared memory machine, the cache line size is 128 bytes. On these architectures, dedicating an entire line for a semaphore (which typically consume 1 - 4 bytes), would waste the better portion of the cache line. <p> If this did not hold, a spurious update could occur. Note that for redo logging, it is not necessary to guarantee logging before migration. For semaphores, this guarantee of undo logging before migration can be implemented with existing multiprocessor synchronization techniques, such as and cache line locks <ref> [13] </ref>, and compare-and-swap (CAS) [4]. The line lock is a primitive which locks a line into the cache until it is released. Ensuring that the log record is written prior to migration can be accomplished as follows.
Reference: [14] <author> R. Strom, D. Bacon, and S. Yemini. </author> <title> Volatile Logging in n-Fault-Tolerant Distributed Systems. </title> <booktitle> Proceedings of the 18th International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 44-49, </pages> <year> 1988. </year>
Reference-contexts: Alternatively, the use of (volatile) redo logging provides sufficient information for restart recovery to restore lost updates. 6 Related Work While our work addresses recovery issues in the context of cache-coherent shared memory multiprocessors, recovery issues in the context of distributed computations in message-passing systems have been studied in <ref> [5, 14] </ref>. The process checkpointing schemes of [5, 14] ensure a consistent state of a distributed computation without necessitating the rollback of any processes other than ones that failed. <p> logging provides sufficient information for restart recovery to restore lost updates. 6 Related Work While our work addresses recovery issues in the context of cache-coherent shared memory multiprocessors, recovery issues in the context of distributed computations in message-passing systems have been studied in <ref> [5, 14] </ref>. The process checkpointing schemes of [5, 14] ensure a consistent state of a distributed computation without necessitating the rollback of any processes other than ones that failed. <p> In this case, a checkpoint consists of the necessary process state for restarting execution, such as the program counter, process identifier, and register contents. In <ref> [5, 14] </ref>, to ensure the forward progress of a distributed computation, messages sent between processes trigger log records to be written to volatile memory. Recovery of a failed process is achieved by restarting the failed process from its checkpoint and replaying the message from the sender's logs.
Reference: [15] <author> M. Tam. </author> <title> CapNet Using Gigabit Networks as a High Speed Backplane. </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer and Information Science, University of Pennsylvania, </institution> <year> 1994. </year>
Reference-contexts: At present, most shared memory systems are constructed as tightly-coupled multiprocessors. However, current technological trends, such as advances in processor and network technology [7], are enabling the emergence of loosely-coupled shared memory systems connected by local or wide area networks <ref> [8, 6, 15] </ref>. With current mechanisms, a single node failure is likely to require a reboot of the entire shared memory system.
Reference: [16] <author> K. Wu, W. Fuchs, and J. Patel. </author> <title> Recoverable Distributed Shared Virtual Memory. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4) </volume> <pages> 460-469, </pages> <month> April </month> <year> 1990. </year> <month> 22 </month>
Reference-contexts: Recovery of a failed process is achieved by restarting the failed process from its checkpoint and replaying the message from the sender's logs. Other related work on distributed computations require that checkpoints are taken to disk to avoid rollback propagation. In <ref> [16] </ref>, a process checkpointing scheme for distributed shared virtual memory is presented. A centralized page manager implements coherency in 18 a loosely coupled multicomputer system.
References-found: 16

