URL: http://www.eecs.umich.edu/techreports/cse/1997/CSE-TR-344-97.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse97.html
Root-URL: http://www.cs.umich.edu
Email: thalerd@eecs.umich.edu ravi@eecs.umich.edu  
Title: An Architecture for Inter-Domain Troubleshooting  
Author: David G. Thaler and Chinya V. Ravishankar 
Address: Ann Arbor, Michigan 48109-2122  
Affiliation: Electrical Engineering and Computer Science Department The University of Michigan,  
Abstract: In this paper, we explore the constraints of a new problem: that of coordinating network troubleshooting among peer administrative domains or Internet Service Providers, and untrusted observers. Allowing untrusted observers permits any entity to report problems, whether it is a Network Operations Center (NOC), end-user, or application. Our goals here are to define the inter-domain coordination problem clearly, and to develop an architecture which allows observers to report problems and receive timely feedback, regardless of their own locations and identities. By automating this process, we also relieve human bottlenecks at help desks and NOCs whenever possible. We begin by presenting a troubleshooting methodology for coordinating problem diagnosis. We then describe GDT, a distributed protocol which realizes this methodology. We show through simulation that GDT performs well as the number of observers and problems grows, and continues to function robustly amidst heavy packet loss. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Shri K. Goyal and Ralph W. Worrest. </author> <title> Expert systems in network maintenance and management. </title> <booktitle> In IEEE International Conference on Communications, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: To our knowledge, little has been done to address the problem of coordinated network management across administrative domains, although the need for such a global coordination system has long been recognized <ref> [1, 2, 3, 4] </ref>.
Reference: [2] <author> Makoto Yoshida, Makoto Kobayashi, and Haruo Yamaguchi. </author> <title> Customer control of network management from the service provider's perspective. </title> <journal> IEEE Communications Magazine, </journal> <pages> pages 35-40, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: To our knowledge, little has been done to address the problem of coordinated network management across administrative domains, although the need for such a global coordination system has long been recognized <ref> [1, 2, 3, 4] </ref>.
Reference: [3] <author> Kraig R. Meyer and Dale S. Johnson. </author> <title> Experience in network management: The Merit network operations center. In Integrated Network Management, II. </title> <booktitle> IFIP TC6/WG6.6, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: To our knowledge, little has been done to address the problem of coordinated network management across administrative domains, although the need for such a global coordination system has long been recognized <ref> [1, 2, 3, 4] </ref>.
Reference: [4] <author> Alan Hannan. Inter-provider outage notification. </author> <title> North American Network Operator's Group, </title> <month> May </month> <year> 1996. </year> <note> http://www.academ.com/nanog/may1996/outage.html. </note>
Reference-contexts: To our knowledge, little has been done to address the problem of coordinated network management across administrative domains, although the need for such a global coordination system has long been recognized <ref> [1, 2, 3, 4] </ref>.
Reference: [5] <author> Craig Labovitz. </author> <title> Routing stability analysis. North American Network Operator's Group, </title> <month> October </month> <year> 1996. </year> <note> http://www.academ.com/nanog/oct1996/routing-stability.html. </note>
Reference-contexts: To our knowledge, little has been done to address the problem of coordinated network management across administrative domains, although the need for such a global coordination system has long been recognized [1, 2, 3, 4]. In one recent informal study of routing instability <ref> [5] </ref>, it was found that while the majority of catastrophic routing problems could be identified as software and configuration errors, about 10% of the problems could only be classified as "somebody else's problem", since all parties questioned pointed to another party as the cause. <p> Cycles in cause-effect graphs are particularly important. They may lead all administrations involved to conclude it is "somebody else's problem", as observed in the informal routing instability study <ref> [5] </ref>, resulting in no action taken at all. We will return to this issue in Section 4.3. 3.2.1 Constructing cause-effect graphs Given an initial problem (leaf effect) report, a cause-effect graph can be constructed according to the following procedure: 1.
Reference: [6] <author> Merit/ISI. Inter-provider notification. </author> <note> http://compute.merit.edu/ipn.html. </note>
Reference-contexts: We leave inter-administration negotiation as an opportunity for future work. We also leave the issue of pre-notification, or notifying organizations of downtime scheduled in the future, to other mechanisms. While some work currently in progress, such as IPN <ref> [6] </ref>, addresses the issue of pre-notification, we observe that pre-notification does not solve the problem of coordinating troubleshooting during a problem, since past announcements (if any) may have been lost, ignored, or forgotten, and may be inaccessible during the problem. fl This work was supported in part by National Science Foundation
Reference: [7] <author> Metin Feridun. </author> <title> Diagnosis of connectivity problems in the internet. In Integrated Network Management, II. </title> <booktitle> IFIP TC6/WG6.6, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Our framework consists of three parts: 1. Domain-expertise modules: These are existing tools upon which we build. They apply traditional management techniques, usually within an administrative domain. Example domain-expertise modules include existing Network Management Systems (NMS's), connectivity diagnosis tools such as CT <ref> [7] </ref>, up-down and congestion experts as used by ANM [8], and agents employing newer techniques such as Anomaly Signature Matching [9]. 2. Troubleshooting Methodology: This is the theory and algorithm behind the protocol which allows effective troubleshooting in an inter-domain environment. 3.
Reference: [8] <author> M. Feridun, M. Leib, M. Nodine, and J. Ong. ANM: </author> <title> Automated network management system. </title> <journal> IEEE Network, </journal> <volume> 2(2) </volume> <pages> 13-19, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Domain-expertise modules: These are existing tools upon which we build. They apply traditional management techniques, usually within an administrative domain. Example domain-expertise modules include existing Network Management Systems (NMS's), connectivity diagnosis tools such as CT [7], up-down and congestion experts as used by ANM <ref> [8] </ref>, and agents employing newer techniques such as Anomaly Signature Matching [9]. 2. Troubleshooting Methodology: This is the theory and algorithm behind the protocol which allows effective troubleshooting in an inter-domain environment. 3.
Reference: [9] <author> Frank Feather, Dan Slewlorek, and Roy Maxion. </author> <title> Fault detection in an ethernet network using anomaly signature matching. </title> <booktitle> In Proceedings of the ACM SIGCOMM, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: They apply traditional management techniques, usually within an administrative domain. Example domain-expertise modules include existing Network Management Systems (NMS's), connectivity diagnosis tools such as CT [7], up-down and congestion experts as used by ANM [8], and agents employing newer techniques such as Anomaly Signature Matching <ref> [9] </ref>. 2. Troubleshooting Methodology: This is the theory and algorithm behind the protocol which allows effective troubleshooting in an inter-domain environment. 3. Coordination Protocol: This protocol conveys information between management entities which may be in different administrative domains, and enables the methodology.
Reference: [10] <author> David D. Clark. </author> <title> The design philosophy of the DARPA Internet protocols. </title> <booktitle> Proc. of ACM SIGCOMM '88, </booktitle> <pages> pages 106-114, </pages> <year> 1988. </year>
Reference-contexts: The second part implies that problems reported by untrusted sources must be confirmed before being acted upon. 2 2.2 Architectural Constraints Our architecture follows the Internet design philosophy described in <ref> [10] </ref>. We summarize this philosophy with the following set of constraints ranked in order of importance: high availability, allowing multiple services, networks, and centers of administration, cost-effectiveness, low-effort deployment, and accountability. We now adapt these constraints to the problem of network troubleshooting as discussed below.
Reference: [11] <author> Marshall T. Rose. </author> <title> The Simple Book. </title> <publisher> Prentice Hall, </publisher> <address> 2nd edition, </address> <year> 1994. </year>
Reference-contexts: We now adapt these constraints to the problem of network troubleshooting as discussed below. We paraphrase our foremost constraint (from Rose <ref> [11] </ref>) as follows: Constraint 1 (Reliability): When all else fails, troubleshooting must continue to function, if at all possible. It is instructive to contrast the reliability requirements of troubleshooting services with those of ordinary distributed services.
Reference: [12] <author> Zheng Wang. </author> <title> Model of network faults. In Integrated Network Management, I. </title> <booktitle> IFIP TC6/WG6.6, </booktitle> <month> April </month> <year> 1989. </year>
Reference-contexts: A good introduction to fault propagation can be found in <ref> [12] </ref>, which describes how faults propagate both vertically, as well as horizontally through the network. For the horizontal directions, we use the term downstream to denote the direction of data flow, and upstream to denote the reverse direction.
Reference: [13] <author> ISO. </author> <title> Information processing systems open systems interconnection basic reference model part 4: Management framework, </title> <booktitle> 1989. </booktitle> <pages> ISO 7498-4. </pages>
Reference-contexts: For the horizontal directions, we use the term downstream to denote the direction of data flow, and upstream to denote the reverse direction. In the vertical direction, up and down are defined with respect to the seven-layer protocol stack defined by the ISO <ref> [13] </ref>. To expand upon this notion, we propose the use of resource dependency graphs. We first use a resource dependency graph to describe the network, in which each node represents an object, and directed edges denote dependencies. Figure 1 shows one such example, depicting both vertical and horizontal dependencies. <p> This is important, since an observer may not be able to distinguish between the two, and our definition allows us to coordinate information relating to both fault management and performance management, as defined by the ISO <ref> [13] </ref>. Our methodology requires creating and maintaining, for each class of objects, means for determining an instance's capacity, utilization, and health, and methods for determining the set of instances above, below, upstream, and downstream from a given instance.
Reference: [14] <author> German Goldszmidt and Yechiam Yemini. </author> <title> Evaluating management decisions via delegation. In Integrated Network Management, </title> <booktitle> III. IFIP TC6/WG6.6, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: For example, a link's capacity might be measured in Mbps, and a file system's capacity might be measured in Gbytes. Let an object's utilization denote the total amount of its resources in use. Finally, we adopt the concept of a "health function" from <ref> [14] </ref>. We let an object's health be a measure of its performance and its ability to adequately meet imposed demands. Low health is thus an indication of degraded performance. We will use the term problem to denote an object experiencing low health.
Reference: [15] <author> Willis Stinson and Shaygan Kheradpir. </author> <title> A state-based approach to real-time telecommunications network management. </title> <booktitle> In NOMS, </booktitle> <year> 1992. </year>
Reference-contexts: Observation 3 High utilization can cause low health, as utilization approaches the object's capacity. That is, low health may arise from soft failures (congestion) in addition to hard failures (hardware or software faults). 3.2 Cause-effect graphs Previous studies (e.g., <ref> [15] </ref>) have typically only looked at one direction of fault propagation (i.e., "up"). We introduce cause-effect graphs as a more comprehensive model for representing fault propagation. Each node in a cause-effect graph represents a problem, and directed edges lead from effects to causes.
Reference: [16] <author> D. Thaler. Globally-distributed troubleshooting (GDT): </author> <title> Protocol specification. </title> <booktitle> Work in progress, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: The state transition table seen by each problem (i.e., each node in the cause-effect graph) is shown in Figure 7. State transitions correspond to timer expirations and receipt of protocol messages. Details can be found in <ref> [16] </ref>. Each expert then locally follows the methodology of Section 3 for the nodes of the global cause-effect graph which it holds, and GDT protocol messages are exchanged between experts to create and maintain the cause-effect graph. <p> If the client never refreshes the deferred state (see Section 4.3), the tests need not be performed. 4.3 Protocol Overview In this section we give a brief overview of the GDT protocol. A detailed specification can be found elsewhere <ref> [16] </ref>. Any entity may report a problem, whether the entity is a client perceiving a problem, or an expert hypothesizing about potential causes of a known problem.
Reference: [17] <author> A. Guttman. R-trees: </author> <title> A dynamic index structure for spatial searching. </title> <booktitle> In Proceedings of the 1984 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 47-57, </pages> <month> June </month> <year> 1984. </year> <month> 15 </month>
Reference-contexts: This problem is analogous to that of performing a point query in a spatial database to get a list of regions covering the given point. Traditional spatial database techniques such as R-trees <ref> [17] </ref> are not directly applicable, however, since scalability requires that the database of regions be physically distributed. In addition, it doesn't matter whether a region is matched if the associated expert isn't reachable.
Reference: [18] <author> Paul Mockapetris. </author> <title> Domain names concepts and facilities, </title> <month> November </month> <year> 1987. </year> <month> RFC-1034. </month>
Reference-contexts: Maintain low bandwidth and memory overhead (thus trying not to exacerbate congestion problems, and interfering as little as possible with other objects). The first constraint suggests that a hierarchy of servers corresponding to a hierarchy in the namespace (as is used by DNS <ref> [18] </ref>, X.500 [19], etc) will not work, since we must have successful queries even when we are partitioned from a large part of the network. Replicating such servers everywhere will not keep the bandwidth overhead low.
Reference: [19] <author> Gerald W. Neufeld. </author> <title> Descriptive names in X.500. </title> <booktitle> In Proceedings of the ACM SIGCOMM, </booktitle> <pages> pages 64-70, </pages> <year> 1989. </year>
Reference-contexts: Maintain low bandwidth and memory overhead (thus trying not to exacerbate congestion problems, and interfering as little as possible with other objects). The first constraint suggests that a hierarchy of servers corresponding to a hierarchy in the namespace (as is used by DNS [18], X.500 <ref> [19] </ref>, etc) will not work, since we must have successful queries even when we are partitioned from a large part of the network. Replicating such servers everywhere will not keep the bandwidth overhead low. We also want to avoid mandating a hierarchical namespace to preserve domain autonomy and class independence.
Reference: [20] <author> Larry L. Peterson. </author> <title> The profile naming service. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(4) </volume> <pages> 341-364, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: On the other hand, we desire some structure to the servers so that expert location can provide higher availability, and be easily adapted to changing conditions without manual reconfiguration. Many existing attribute-based naming schemes (e.g., <ref> [20] </ref>) provide no structure to servers and hence rely on manual configuration. The solution we adopt is as follows. Expert location servers (ELS's) are organized into a hierarchy according to their location.
Reference: [21] <author> Mic Bowman, Saumya K. Debray, and Larry L. Peterson. </author> <title> Reasoning about naming systems. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(5) </volume> <pages> 795-825, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This procedure ensures that closer experts will be found before more distant experts. This approach both helps to ensure availability of experts matched, and minimizes latency and bandwidth used. A second issue is the ordering of the list of experts. Bowman, et al. <ref> [21] </ref> describe a framework for reasoning about naming systems and describe how ordering rules can be expressed in terms of a "preference hierarchy". The constraints listed above lead us to the following preference hierarchy for ordering the list of experts. 1. Prefer closer experts first to achieve availability.
Reference: [22] <author> D. Thaler and C.V. Ravishankar. </author> <title> Using name-based mappings to increase hit rates. </title> <journal> ACM/IEEE Transactions on Networking, </journal> <note> to appear. </note>
Reference-contexts: Prefer experts which match more of the optional attributes. If a region's description does not specify an optional attribute contained in the request, it is not considered to match when counting matched attributes. 5. Finally, to break ties, we use the Highest Random Weight (HRW) algorithm described in <ref> [22] </ref>. If two requests for the same object retrieve the same set S of servers, HRW generates the same ordering of servers in S for both requests.
Reference: [23] <author> Reid G. Smith. </author> <title> The contract net protocol: High-level communication and control in a distributed problem solver. </title> <journal> ACM Transactions on Computers, </journal> <pages> pages 1104-1113, </pages> <month> December </month> <year> 1980. </year>
Reference-contexts: We also do not want the world to be flooded with requests for experts, as is done in the Contract Net protocol <ref> [23] </ref>.
Reference: [24] <author> D. Eastlake and C. Kaufman. </author> <title> Domain name system security extensions, </title> <month> January </month> <year> 1997. RFC-2065. </year>
Reference-contexts: To ensure integrity of capability advertisements and authentication of their origin, we adopt the current model recommended by the IETF for use with nameservice-like applications, which is known as DNSsec <ref> [24] </ref>. Briefly, a public/private key pair is associated with each domain, and all capabilities are signed with a domain key. To reliably learn the public key of a domain, the key itself must be signed. <p> A resolver must therefore be configured with at least the public key of one domain that it can use to authenticate signatures. It can then securely read the public keys of other domains if the intervening domains in the ELS tree are secure and their signed keys accessible. See <ref> [24, 25] </ref> for a more detailed discussion of the security model and associated concerns. A second security issue is denial-of-service attacks by observers reporting non-existent problems. Such attacks can be combatted in GDT by deferring tests and repairs once such an attack is suspected.
Reference: [25] <author> D. </author> <title> Eastlake. Secure domain name system dynamic update, </title> <month> April </month> <year> 1997. </year> <month> RFC-2137. </month>
Reference-contexts: A resolver must therefore be configured with at least the public key of one domain that it can use to authenticate signatures. It can then securely read the public keys of other domains if the intervening domains in the ELS tree are secure and their signed keys accessible. See <ref> [24, 25] </ref> for a more detailed discussion of the security model and associated concerns. A second security issue is denial-of-service attacks by observers reporting non-existent problems. Such attacks can be combatted in GDT by deferring tests and repairs once such an attack is suspected.
Reference: [26] <institution> Lawrence Berkeley National Labs. ns software. </institution> <note> http://www-nrg.ee.lbl.gov/ns/. </note>
Reference-contexts: To do this, we implemented GDT clients and experts using "ns", the LBNL Network Simulator <ref> [26] </ref>. Our goal will be to test its performance and reliability under conditions particularly adverse to GDT. It is first useful to understand the effects on GDT of hard and soft failures.
Reference: [27] <author> Sally Floyd and Van Jacobson. </author> <title> Random early detection gateways for congestion avoidance. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(4) </volume> <pages> 397-413, </pages> <month> August </month> <year> 1993. </year> <month> 16 </month>
Reference-contexts: We assume that clients start with a warm cache of expert capabilities, and vary the loss rate of GDT messages by artificially injecting loss. Our simulations assumed that all messages had an equal probability of being lost when the aggregate bandwidth demand is constant (e.g., as with RED <ref> [27] </ref>). When routers with a drop-tail queueing policy exist in the network, this assumption is invalid; the policy is biased against bursty streams. Since GDT is somewhat bursty (e.g., several hypotheses may be sent out when a problem is confirmed), GDT is unfairly penalized and hence performs less well.
References-found: 27

