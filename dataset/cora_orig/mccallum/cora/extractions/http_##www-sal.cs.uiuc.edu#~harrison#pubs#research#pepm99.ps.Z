URL: http://www-sal.cs.uiuc.edu/~harrison/pubs/research/pepm99.ps.Z
Refering-URL: http://www-sal.cs.uiuc.edu/~harrison/pubs/research/research.html
Root-URL: http://www.cs.uiuc.edu
Email: fharrison,kaming@cs.uiuc.edu  
Title: Compilation as Metacomputation: Binding Time Separation in Modular Compilers (extended abstract) is taken as input
Author: William L. Harrison Samuel N. Kamin 
Note: The idea of metacomputation|computations of computations|arises naturally in the compi lation of programs. Figure 1 illustrates this idea. In Figure 1, the source language program s  
Address: Urbana, Illinois 61801-2987  
Affiliation: Department of Computer Science University of Illinois, Urbana-Champaign  
Abstract: This paper presents a modular and extensible style of language specification based on metacom-putations. This style uses two monads to factor the static and dynamic parts of the specification, thereby staging the specification and achieving strong binding-time separation. Because meta-computations are defined in terms of monads, they can be constructed modularly and extensibly using monad transformers. Consequently, metacomputation-style specifications are modular and extensible. A number of language constructs are specified: expressions, control-flow, imperative features, block structure, and higher-order functions and recursive bindings. Because of the strong binding-time separation, metacomputation-style specification lends itself to semantics-directed compilation, which we demonstrate by creating a modular compiler for a higher-order, imperative, Algol-like language. Keywords: Compilers, Partial Evaluation, Semantics-Based Compilation, Programming Language Semantics, Monads, Monad Transformers, Pass Separation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. </author> <title> Ullman Compilers: Principles, Techniques, and Tools, </title> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Block structure [ Booleans Equations = Eq Expressions [ Eq Imperative [ Eq Control-flow [ Eq Block structure [ Eq Booleans Source Code: new x in x := 5; y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := <ref> [1] </ref>; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := [2] * [3]; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; Future work focuses on two areas. Firstly, specifying other language constructs like objects, classes, and exceptions.
Reference: [2] <author> A. Appel, </author> <title> Modern Compiler Implementation in ML, </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1998. </year>
Reference-contexts: As an example, consider the three definitions of the conditional if-then statement in Figure 6. The first is a dual continuation "control-flow" semantics, found commonly in compilers <ref> [2] </ref>. If B is true, then the first continuation, [[c]] ? I , is executed, otherwise c is skipped and just is executed. <p> Control-flow [ Eq Block structure [ Eq Booleans Source Code: new x in x := 5; y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := [1]; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := <ref> [2] </ref> * [3]; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; Future work focuses on two areas. Firstly, specifying other language constructs like objects, classes, and exceptions. Secondly, exploring the impact of the metacomputation-style on compiler correctness. <p> x in x := 5; y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := [1]; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := <ref> [2] </ref> * [3]; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; Future work focuses on two areas. Firstly, specifying other language constructs like objects, classes, and exceptions. Secondly, exploring the impact of the metacomputation-style on compiler correctness. <p> 5; y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := [1]; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := <ref> [2] </ref> * [3]; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; Future work focuses on two areas. Firstly, specifying other language constructs like objects, classes, and exceptions. Secondly, exploring the impact of the metacomputation-style on compiler correctness. A question of particular interest is how modular are the correctness proofs of modular compilers.
Reference: [3] <author> A. Appel, </author> <title> Compiling with Continuations, </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Compiler specifications written in continuation passing style may be vastly simpler when structured with metacomputations than those in the monolithic computation style. There is a considerable body of work which structures compilers with continuations <ref> [3, 8, 21, 24] </ref>, frequently because continuations can play the role of machine language sequences. In the context of semantics-directed compilation, continuations which include only dynamic computations are much easier to treat as code that those containing both static and dynamic computations|hence, the usefulness of staging specifications with metacomputations. <p> Eq Block structure [ Eq Booleans Source Code: new x in x := 5; y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := [1]; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := [2] * <ref> [3] </ref>; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; Future work focuses on two areas. Firstly, specifying other language constructs like objects, classes, and exceptions. Secondly, exploring the impact of the metacomputation-style on compiler correctness. <p> x := 5; y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := [1]; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := [2] * <ref> [3] </ref>; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; Future work focuses on two areas. Firstly, specifying other language constructs like objects, classes, and exceptions. Secondly, exploring the impact of the metacomputation-style on compiler correctness. A question of particular interest is how modular are the correctness proofs of modular compilers. <p> y := 1; while (1 x) do y := y*x; x := x-1; Target Code: 0 := 5; 2: 2 := [1]; 3: halt; 1 := 1; 3 := [0]; jump 1; 1 := [2] * <ref> [3] </ref>; 2 := [0]; 3 := [0]; 0 := [2] - [3] BRLEQ [2] [3] 2 3; jump 1; Future work focuses on two areas. Firstly, specifying other language constructs like objects, classes, and exceptions. Secondly, exploring the impact of the metacomputation-style on compiler correctness. A question of particular interest is how modular are the correctness proofs of modular compilers.
Reference: [4] <author> O. Danvy, </author> <title> "Type-Directed Partial Evaluation," </title> <booktitle> Proceedings of the ACM Conference on the Principles of Programming Languages, </booktitle> <year> 1996. </year>
Reference: [5] <author> O. Danvy and R. Vestergaard, </author> <title> "Semantics-Based Compiling: A Case Study in Type-Directed Partial Evaluation," </title> <booktitle> Eighth International Symposium on Programming Language Implementation and Logic Programming, </booktitle> <year> 1996, </year> <pages> pages 182-497. </pages>
Reference: [6] <author> R. Davies and F. Pfenning, </author> <title> "A Modal Analysis of Staged Computation," </title> <booktitle> Proceedings of the ACM Conference on the Principles of Programming Languages, </booktitle> <year> 1996. </year>
Reference: [7] <author> D. Espinosa, </author> <title> "Semantic Lego," </title> <type> Doctoral Dissertation, </type> <institution> Columbia University, </institution> <year> 1995. </year>
Reference-contexts: 1 Monads and Monad Transformers In this section, we review the theory of monads [16, 23] and monad transformers <ref> [7, 13] </ref>. Readers familiar with these topics may skip the section, except for the last paragraph. <p> Unfortunately, M St (M St t ) and M 2St t are very different types. This points to a difficulty with monads: they do not compose in this simple manner. The key contribution of the recent work <ref> [7, 13] </ref> on monad transformers is to solve this composition problem. When applied to a monad M, a monad transformer T creates a new monad M 0 . For example, the state monad transformer, T St store, is shown in Figure 3. <p> This separation of the store into two components is really a "staging transformation;" it separates the static part of the state (the free locations) from the dynamic part (the stored values). 1.1 A Semantics for Metacomputation We can formalize this notion of metacomputation using monads <ref> [7, 13, 16, 23] </ref> and use the resulting framework as a basis for staging computations. Given a monad M, the computations of type a is the type M a. <p> This process is illustrated in Figure 2. Compilers constructed in the manner are modular because our language specifications are structured by monad transformers <ref> [7, 13] </ref>. In [7, 13], modular interpreters are constructed by applying monad transformers associated with each language feature, and our compilers are modular for precisely the same reason. <p> This process is illustrated in Figure 2. Compilers constructed in the manner are modular because our language specifications are structured by monad transformers <ref> [7, 13] </ref>. In [7, 13], modular interpreters are constructed by applying monad transformers associated with each language feature, and our compilers are modular for precisely the same reason. <p> In our approach to compilation, monad transformers provide modularity, extensibility, and reusability, and monadic structure provides binding-time separation. 2.1 Integer Expressions Compiler Building Block Consider the standard monadic-style specification of negation <ref> [7, 13, 23] </ref> displayed in Figure 4. <p> Observe that this implementation-oriented definition calculates the same value as the standard definition, but it stores the intermediate value i as well. But where do addresses and storage come from? One choice is to add them to the Interp monad using monad transformers <ref> [7, 13] </ref> as in the "Implementation-oriented" specification in Figure 4. In that definition, rdAddr reads the current top of stack address a, inAddr increments the top of stack, and Thread stores i at a. <p> Because meta-computations are defined in terms of monads, they can be constructed modularly and extensibly using monad transformers. Consequently, metacomputation-style specifications are modular and extensible just as the modular interpreter constructions in <ref> [7, 13] </ref> are. We exploit this fact to create modular compilers, as we did in Section 2.
Reference: [8] <author> W. Harrison and S. Kamin, </author> <title> "Modular Compilers Based on Monad Transformers," </title> <booktitle> Proceedings of the IEEE International Conference on Programming Languages, </booktitle> <year> 1998, </year> <pages> pages 122-131. </pages>
Reference-contexts: Generally, T Env must be applied after T CPS . Cf. Liang, et al.[13]. generally a different notion altogether from computations. 2 A Case Study in Metacomputation-style Staging: Modular Compilation for the While Language Traditional compilers are structured by phase|parsing, type checking, code generation and optimization|while modular compilers <ref> [8] </ref> are factored by source language feature (e.g., expressions, block structure, procedures, etc.) as well. Each source feature is compiled in isolation by a reusable compiler building block, so that compilers for languages with many features are built by combining compiler building blocks for each feature. <p> In that definition, rdAddr reads the current top of stack address a, inAddr increments the top of stack, and Thread stores i at a. This was the approach taken in our previous work <ref> [8] </ref> and is typical of the pass separation [10] Standard: Interp = Id [[ : intexp]] : Interp (int) [[t]] = unitI (i) Implementation-oriented/Monolithic: Interp = T Env Addr (T St Sto Id) Addr = int; Sto = Addr ! int [[ : intexp]] : Interp (int) Thread (i : int; <p> Compiler specifications written in continuation passing style may be vastly simpler when structured with metacomputations than those in the monolithic computation style. There is a considerable body of work which structures compilers with continuations <ref> [3, 8, 21, 24] </ref>, frequently because continuations can play the role of machine language sequences. In the context of semantics-directed compilation, continuations which include only dynamic computations are much easier to treat as code that those containing both static and dynamic computations|hence, the usefulness of staging specifications with metacomputations. <p> There are two distinct kinds of computation in the compilation of if-then, and following Moggi's idea that a monad encapsulates a notion of computation [16], two monads are necessary. 2 A full description of newSegment is found in <ref> [8] </ref>.
Reference: [9] <author> N. D. Jones, C. K. Gomard, and P. Sestoft, </author> <title> Partial Evaluation and Automatic Program Generation, </title> <publisher> Prentice-Hall 1993. </publisher>
Reference: [10] <author> U. Jorring and W. Scherlis, </author> <title> "Compilers and Staging Transformations," </title> <booktitle> Proceedings of the ACM Conference on the Principles of Programming Languages, </booktitle> <year> 1986. </year>
Reference-contexts: In that definition, rdAddr reads the current top of stack address a, inAddr increments the top of stack, and Thread stores i at a. This was the approach taken in our previous work [8] and is typical of the pass separation <ref> [10] </ref> Standard: Interp = Id [[ : intexp]] : Interp (int) [[t]] = unitI (i) Implementation-oriented/Monolithic: Interp = T Env Addr (T St Sto Id) Addr = int; Sto = Addr ! int [[ : intexp]] : Interp (int) Thread (i : int; a : Addr) = updateSto [a 7! i] <p> With this approach, one adds whatever intermediate data structure <ref> [10] </ref> is needed to a semantics to make it more implementation-oriented, and then partial evaluation is used to transform the denotations of terms into a machine language-like form.
Reference: [11] <author> P. Lee, </author> <title> Realistic Compiler Generation, </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: The beauty of the monadic form is that the meaning of [[]] can be reinterpreted in a variety of monads. Monadic semantics separate the description of a language from its denotation. In this sense, it is similar to action semantics [17] and high-level semantics <ref> [11] </ref>. The simplest monad is the identity monad, shown in Figure 3. Given the identity monad, we can define add as ordinary addition. [[]] would have type Expression ! int .
Reference: [12] <author> S. Liang, </author> <title> "A Modular Semantics for Compiler Generation," </title> <institution> Yale University Department of Computer Science Technical Report TR-1067, </institution> <month> February </month> <year> 1995. </year>
Reference: [13] <author> S. Liang, P. Hudak, and M. Jones, </author> <title> Monad Transformers and Modular Interpreters. </title> <booktitle> Proceedings of the ACM Conference on the Principles of Programming Languages, </booktitle> <year> 1995. </year>
Reference-contexts: 1 Monads and Monad Transformers In this section, we review the theory of monads [16, 23] and monad transformers <ref> [7, 13] </ref>. Readers familiar with these topics may skip the section, except for the last paragraph. <p> Unfortunately, M St (M St t ) and M 2St t are very different types. This points to a difficulty with monads: they do not compose in this simple manner. The key contribution of the recent work <ref> [7, 13] </ref> on monad transformers is to solve this composition problem. When applied to a monad M, a monad transformer T creates a new monad M 0 . For example, the state monad transformer, T St store, is shown in Figure 3. <p> This separation of the store into two components is really a "staging transformation;" it separates the static part of the state (the free locations) from the dynamic part (the stored values). 1.1 A Semantics for Metacomputation We can formalize this notion of metacomputation using monads <ref> [7, 13, 16, 23] </ref> and use the resulting framework as a basis for staging computations. Given a monad M, the computations of type a is the type M a. <p> This process is illustrated in Figure 2. Compilers constructed in the manner are modular because our language specifications are structured by monad transformers <ref> [7, 13] </ref>. In [7, 13], modular interpreters are constructed by applying monad transformers associated with each language feature, and our compilers are modular for precisely the same reason. <p> This process is illustrated in Figure 2. Compilers constructed in the manner are modular because our language specifications are structured by monad transformers <ref> [7, 13] </ref>. In [7, 13], modular interpreters are constructed by applying monad transformers associated with each language feature, and our compilers are modular for precisely the same reason. <p> In our approach to compilation, monad transformers provide modularity, extensibility, and reusability, and monadic structure provides binding-time separation. 2.1 Integer Expressions Compiler Building Block Consider the standard monadic-style specification of negation <ref> [7, 13, 23] </ref> displayed in Figure 4. <p> Observe that this implementation-oriented definition calculates the same value as the standard definition, but it stores the intermediate value i as well. But where do addresses and storage come from? One choice is to add them to the Interp monad using monad transformers <ref> [7, 13] </ref> as in the "Implementation-oriented" specification in Figure 4. In that definition, rdAddr reads the current top of stack address a, inAddr increments the top of stack, and Thread stores i at a. <p> Because meta-computations are defined in terms of monads, they can be constructed modularly and extensibly using monad transformers. Consequently, metacomputation-style specifications are modular and extensible just as the modular interpreter constructions in <ref> [7, 13] </ref> are. We exploit this fact to create modular compilers, as we did in Section 2.
Reference: [14] <author> S. Liang, </author> <title> "Modular Monadic Semantics and Compilation," </title> <type> Doctoral Thesis, </type> <institution> Yale University, </institution> <year> 1997. </year>
Reference: [15] <author> T. Mogensen. </author> <title> "Separating Binding Times in Language Specifications," </title> <booktitle> Proceedings of the ACM Conference on Functional Programming and Computer Architecture, </booktitle> <pages> pp 12-25, </pages> <year> 1989. </year>
Reference: [16] <author> E. Moggi, </author> <title> "Notions of Computation and Monads," </title> <booktitle> Information and Computation 93(1), </booktitle> <pages> pp. 55-92, </pages> <year> 1991. </year>
Reference-contexts: 1 Monads and Monad Transformers In this section, we review the theory of monads <ref> [16, 23] </ref> and monad transformers [7, 13]. Readers familiar with these topics may skip the section, except for the last paragraph. <p> This separation of the store into two components is really a "staging transformation;" it separates the static part of the state (the free locations) from the dynamic part (the stored values). 1.1 A Semantics for Metacomputation We can formalize this notion of metacomputation using monads <ref> [7, 13, 16, 23] </ref> and use the resulting framework as a basis for staging computations. Given a monad M, the computations of type a is the type M a. <p> We refer therefore to this approach as "monolithic" to distinguish it from the present work. But we wish to maintain clear separation of the static parts from the dynamic parts of the specification, and here the monadic structure becomes crucial. Monads, which encapsulate kinds or notions of computation <ref> [16] </ref>, provide a natural and intuitive semantics for metacomputation, and metacomputations, in turn, allow the static parts to be factored out from the dynamic parts in language specifications. <p> There are two distinct kinds of computation in the compilation of if-then, and following Moggi's idea that a monad encapsulates a notion of computation <ref> [16] </ref>, two monads are necessary. 2 A full description of newSegment is found in [8].
Reference: [17] <author> P. Mosses, </author> <title> Action Semantics, </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: The beauty of the monadic form is that the meaning of [[]] can be reinterpreted in a variety of monads. Monadic semantics separate the description of a language from its denotation. In this sense, it is similar to action semantics <ref> [17] </ref> and high-level semantics [11]. The simplest monad is the identity monad, shown in Figure 3. Given the identity monad, we can define add as ordinary addition. [[]] would have type Expression ! int .
Reference: [18] <author> H. Nielson and F. Nielson, </author> <title> "Code Generation from two-level denotational metalanguages," in Programs as Data Objects, </title> <booktitle> Lecture Notes in Computer Science 217 (Springer, </booktitle> <address> Berlin, </address> <year> 1986). </year>
Reference: [19] <author> H. Nielson and F. Nielson, </author> <title> "Automatic Binding Time Analysis for a Typed -calculus," </title> <booktitle> Science of Computer Programming 10, </booktitle> <month> 2 (April </month> <year> 1988), </year> <pages> pp 139-176. </pages>
Reference: [20] <author> J. Reynolds. </author> <title> "The Essence of Algol," </title> <booktitle> Algorithmic Languages, Proceedings of the International Symposium on Algorithmic Languages, </booktitle> <pages> pp. 345-372, </pages> <year> 1981. </year>
Reference-contexts: The compiler building block for this language appears in Figure 9. The static part of this specification allocates a free stack location a, and the program variable x is bound to an accepter-expresser pair <ref> [20] </ref> in the current environment . <p> The compiler building block for this language appears in Figure 9. For sequencing, the static part of the specification compiles c 1 and c 2 in succession, while the dynamic (boxed) part runs them in succession. For assignment, the static part of the specification retrieves the accepter <ref> [20] </ref> acc for program variable x from the current environment and compiles t, while the dynamic part calculates the value of t and passes it to acc. 2.5 Combining Compiler Building Blocks monad transformers to create the Comp and Interp monads for the combined language.
Reference: [21] <author> J. Reynolds, </author> <title> "Using Functor Categories to Generate Intermediate Code," </title> <booktitle> Proceedings of the ACM Conference on the Principles of Programming Languages, </booktitle> <pages> pages 25-36, </pages> <year> 1995. </year>
Reference-contexts: Compiler specifications written in continuation passing style may be vastly simpler when structured with metacomputations than those in the monolithic computation style. There is a considerable body of work which structures compilers with continuations <ref> [3, 8, 21, 24] </ref>, frequently because continuations can play the role of machine language sequences. In the context of semantics-directed compilation, continuations which include only dynamic computations are much easier to treat as code that those containing both static and dynamic computations|hence, the usefulness of staging specifications with metacomputations.
Reference: [22] <author> J. E. Stoy, </author> <title> Denotational Semantics: the Scott-Strachey Approach to Programming Language Theory, </title> <publisher> MIT Press, </publisher> <year> 1977. </year>
Reference: [23] <author> P. Wadler, </author> <title> "The essence of functional programming," </title> <booktitle> Proceedings of the ACM Conference on the Principles of Programming Languages, </booktitle> <pages> pages 1-14, </pages> <year> 1992. </year>
Reference-contexts: 1 Monads and Monad Transformers In this section, we review the theory of monads <ref> [16, 23] </ref> and monad transformers [7, 13]. Readers familiar with these topics may skip the section, except for the last paragraph. <p> This separation of the store into two components is really a "staging transformation;" it separates the static part of the state (the free locations) from the dynamic part (the stored values). 1.1 A Semantics for Metacomputation We can formalize this notion of metacomputation using monads <ref> [7, 13, 16, 23] </ref> and use the resulting framework as a basis for staging computations. Given a monad M, the computations of type a is the type M a. <p> In our approach to compilation, monad transformers provide modularity, extensibility, and reusability, and monadic structure provides binding-time separation. 2.1 Integer Expressions Compiler Building Block Consider the standard monadic-style specification of negation <ref> [7, 13, 23] </ref> displayed in Figure 4.
Reference: [24] <author> M. Wand, </author> <title> "Deriving Target Code as a Representation of Continuation Semantics," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 4, No. 3, </volume> <pages> pp. 496-517, </pages> <year> 1982. </year>
Reference-contexts: Compiler specifications written in continuation passing style may be vastly simpler when structured with metacomputations than those in the monolithic computation style. There is a considerable body of work which structures compilers with continuations <ref> [3, 8, 21, 24] </ref>, frequently because continuations can play the role of machine language sequences. In the context of semantics-directed compilation, continuations which include only dynamic computations are much easier to treat as code that those containing both static and dynamic computations|hence, the usefulness of staging specifications with metacomputations.
References-found: 24

