URL: ftp://ftp.cs.rochester.edu/pub/u/kyros/iuw96.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/kyros/publicat.htm
Root-URL: 
Title: Affine Object Representations for Calibration-Free Augmented Reality  
Author: Kiriakos N. Kutulakos, James Vallino 
Address: Rochester, NY 14627-0226  
Affiliation: Computer Science Department University of Rochester  
Abstract: We describe the design and implementation of a video-based augmented reality system capable of overlaying three-dimensional graphical objects on live video of dynamic environments. The key feature of the system is that it is completely uncalibrated: it does not rely on any metric information about the calibration parameters of the camera or on the existence of 3D models for objects physically present in the environment.
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> W. Grimson et al., </editor> <title> An automatic registration method for frame-less stereotaxy, image guided surgery, and enhanced reality visualization, </title> <booktitle> Proc. CVPR, </booktitle> <pages> pp. 430436, </pages> <year> 1994. </year>
Reference-contexts: In practice, camera calibration and position tracking are prone to errors which accumulate in the augmented display. Furthermore, initialization of virtual objects requires additional calibration stages <ref> [1, 8] </ref>, and camera calibration must be performed whenever its intrinsic parameters (e.g., focal length) change. This paper presents a new approach for embedding 3D virtual objects into live video.
Reference: [2] <author> M. Uenohara and T. Kanade, </author> <title> Vision-based object registration for real-time image overlay, </title> <booktitle> Proc. CVRMED, </booktitle> <pages> pp. 1422, </pages> <year> 1995. </year>
Reference-contexts: Very little work has been published on augmented reality systems that reduce the effects of calibration errors through real-time processing of the live video stream <ref> [2, 6, 8, 18] </ref>. To our knowledge, only two systems have been reported [2, 8] that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed [5] or perfectly calibrated. <p> Very little work has been published on augmented reality systems that reduce the effects of calibration errors through real-time processing of the live video stream [2, 6, 8, 18]. To our knowledge, only two systems have been reported <ref> [2, 8] </ref> that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed [5] or perfectly calibrated. The system of Mellor [8] is capable of overlaying 3D medical data over live video of patients in a surgical environment. <p> The most closely related work to our own is the work of Uenohara and Kanade <ref> [2] </ref>. Their system allows overlay of planar diagrams onto live video by tracking feature points in an unknown configuration that lie on the same plane as the diagram. Calibration is avoided by expressing diagram points as linear combinations of the feature points. <p> On a theoretical level, we are extending the basic approach by representing virtual objects in a projective reference frame and investigating the use of image synthesis techniques for shading non-Euclidean virtual objects. On a practical level, we are planning to use gray level feature trackers <ref> [2] </ref> to increase tracking accuracy and versatility. 0 50 100 150 200 250 300 350 400 450 -100 0 100 200 300 Frame Number Projection Matrix Variation X Z Origin (a) (b) (c) (d) box. (b) Image overlay taken from a different position of the camera. (c) Variation of the elements
Reference: [3] <author> M. Bajura, H. Fuchs, and R. Ohbuchi, </author> <title> Merging virtual objects with the real world: Seeing ultrasound imagery within the patient, </title> <booktitle> Proc. SIGGRAPH'92, </booktitle> <pages> pp. </pages> <address> 203210, </address> <year> 1992. </year>
Reference: [4] <author> S. Feiner, B. MacIntyre, and D. Soligmann, </author> <title> Knowledge-based augmented reality, </title> <journal> CACM, v. </journal> <volume> 36, </volume> <editor> n. </editor> <volume> 7, </volume> <pages> pp. 5362, </pages> <year> 1993. </year>
Reference-contexts: Applications of this powerful visualization tool include overlaying clinical 3D data with live video of patients during surgical planning [13] as well as developing three-dimensional user interfaces <ref> [4] </ref>.
Reference: [5] <author> T. Darrell, P. Maes, B. Blumberg, and A. P. Pentland, </author> <title> A novel environment for situated vision and action, </title> <booktitle> IEEE Workshop on Visual Behaviors, </booktitle> <pages> pp. 6872, </pages> <year> 1994. </year>
Reference-contexts: Applications of this powerful visualization tool include overlaying clinical 3D data with live video of patients during surgical planning [13] as well as developing three-dimensional user interfaces [4]. While several approaches have demonstrated the potential of augmented reality systems for human-computer interaction <ref> [5] </ref>, the process of embedding three-dimensional virtual objects into a user's environment raises three issues unique to augmented reality: * Establishing 3D geometric relationships between physical and virtual objects: The locations of virtual objects must be initialized in the user's environment before user interaction can take place. * Rendering virtual objects: <p> To our knowledge, only two systems have been reported [2, 8] that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed <ref> [5] </ref> or perfectly calibrated. The system of Mellor [8] is capable of overlaying 3D medical data over live video of patients in a surgical environment. The system tracks circular features in an known 3D configuration to invert the object-to-image transformation using a linear method.
Reference: [6] <author> M. M. Wloka and B. G. Anderson, </author> <title> Resolving occlusion in augmented reality, </title> <booktitle> Proc. Symp. on Interactive 3D Graphics, </booktitle> <pages> pp. 512, </pages> <year> 1995. </year>
Reference-contexts: Z840902, and of Honeywell under Research Contract No. 304931455 are gratefully acknowledged. At the heart of these issues is the ability to describe the camera's motion, the user's environment and the embedded virtual objects in the same frame of reference. Typical approaches rely on 3D position tracking devices <ref> [6] </ref> and precise calibration [7] to ensure that the entire sequence of transformations between the internal reference frames of the virtual and physical objects, the camera tracking device, and the user's display is known exactly. <p> Very little work has been published on augmented reality systems that reduce the effects of calibration errors through real-time processing of the live video stream <ref> [2, 6, 8, 18] </ref>. To our knowledge, only two systems have been reported [2, 8] that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed [5] or perfectly calibrated. <p> The ellipsoids were defined with respect to those points (see Section 5). The wireframe was rotated clockwise between frames. Note that hidden-surface elimination occurs only between virtual objects; correct occlusion resolution between real and virtual objects requires information about the real objects' 3D structure <ref> [6] </ref>. expressed mathematically by representing the optical axis of the camera with the homogeneous vector [ T 0] T where is given by the cross product = 4 u b 2 u p o 3 2 v b 1 v p o v b 3 v p o 5 (3) and
Reference: [7] <author> M. Tuceyran et al., </author> <title> Calibration requirements and procedures for a monitor-based augmented reality system, </title> <journal> IEEE Trans. Visualization and Computer Graphics, v. </journal> <volume> 1, </volume> <pages> pp. 255273, </pages> <year> 1995. </year>
Reference-contexts: At the heart of these issues is the ability to describe the camera's motion, the user's environment and the embedded virtual objects in the same frame of reference. Typical approaches rely on 3D position tracking devices [6] and precise calibration <ref> [7] </ref> to ensure that the entire sequence of transformations between the internal reference frames of the virtual and physical objects, the camera tracking device, and the user's display is known exactly. In practice, camera calibration and position tracking are prone to errors which accumulate in the augmented display.
Reference: [8] <author> J. Mellor, </author> <title> Enhanced reality visualization in a surgical environment, </title> <type> Master's thesis, </type> <institution> MIT, </institution> <year> 1995. </year>
Reference-contexts: In practice, camera calibration and position tracking are prone to errors which accumulate in the augmented display. Furthermore, initialization of virtual objects requires additional calibration stages <ref> [1, 8] </ref>, and camera calibration must be performed whenever its intrinsic parameters (e.g., focal length) change. This paper presents a new approach for embedding 3D virtual objects into live video. <p> Very little work has been published on augmented reality systems that reduce the effects of calibration errors through real-time processing of the live video stream <ref> [2, 6, 8, 18] </ref>. To our knowledge, only two systems have been reported [2, 8] that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed [5] or perfectly calibrated. <p> Very little work has been published on augmented reality systems that reduce the effects of calibration errors through real-time processing of the live video stream [2, 6, 8, 18]. To our knowledge, only two systems have been reported <ref> [2, 8] </ref> that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed [5] or perfectly calibrated. The system of Mellor [8] is capable of overlaying 3D medical data over live video of patients in a surgical environment. <p> To our knowledge, only two systems have been reported [2, 8] that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed [5] or perfectly calibrated. The system of Mellor <ref> [8] </ref> is capable of overlaying 3D medical data over live video of patients in a surgical environment. The system tracks circular features in an known 3D configuration to invert the object-to-image transformation using a linear method.
Reference: [9] <author> L. S. Shapiro, A. Zisserman, and M. Brady, </author> <title> 3D Motion recovery via affine epipolar geometry, </title> <editor> IJCV, v. </editor> <volume> 16, </volume> <pages> pp. 147182, </pages> <year> 1995. </year>
Reference-contexts: The only requirement is the ability to track across frames at least four features (points or lines) that are specified by the user during system initialization and whose world coordinates are unknown. Our approach is motivated by recent work on non-metric scene reconstruction <ref> [9, 10] </ref>, uncalibrated active vision [11], as well as recent human-computer interaction techniques [12, 13]. We show that the task of overlaying 3D virtual objects onto live video becomes considerably simplified when the camera, the virtual objects, and the environment are represented in an affine frame of reference. <p> Property 2 shows that this process can be inverted if at least four non-coplanar 3D points can be tracked across frames as the camera moves. Throughout this paper we assume the weak perspective projection model <ref> [9] </ref>. 3 Affine Augmented Reality The previous section suggests that once the affine coordinates of points on a virtual object are determined relative to four features in the environment, the points' projection becomes trivial to compute. <p> In general, the correspondence induced by q L and q R may not define a physical point in space. Once p's projection is specified in one image, its projection in the second image must lie on a line satisfying the epipolar constraint <ref> [9] </ref>. This line is computed automatically and is used to constrain the user's selection of q R in the second image (Figure 2).
Reference: [10] <author> J. J. Koenderink and A. J. van Doorn, </author> <title> Affine structure from motion, </title> <editor> J. Opt. Soc. Am., v. A, n. </editor> <volume> 2, </volume> <pages> pp. 377385, </pages> <year> 1991. </year>
Reference-contexts: The only requirement is the ability to track across frames at least four features (points or lines) that are specified by the user during system initialization and whose world coordinates are unknown. Our approach is motivated by recent work on non-metric scene reconstruction <ref> [9, 10] </ref>, uncalibrated active vision [11], as well as recent human-computer interaction techniques [12, 13]. We show that the task of overlaying 3D virtual objects onto live video becomes considerably simplified when the camera, the virtual objects, and the environment are represented in an affine frame of reference. <p> We show that the task of overlaying 3D virtual objects onto live video becomes considerably simplified when the camera, the virtual objects, and the environment are represented in an affine frame of reference. In particular, initialization of virtual objects is reduced to the operation of affine reconstruction <ref> [10] </ref>, visible-surface rendering is reduced to re-projection [14, 15], and rigid animation is reduced to the problem of synthesizing conjugate rotation transformations [16]. Our approach is based on a well-known property of affine point representations [10]: Given a set of four or more noncoplanar 3D points represented in an affine reference <p> In particular, initialization of virtual objects is reduced to the operation of affine reconstruction <ref> [10] </ref>, visible-surface rendering is reduced to re-projection [14, 15], and rigid animation is reduced to the problem of synthesizing conjugate rotation transformations [16]. Our approach is based on a well-known property of affine point representations [10]: Given a set of four or more noncoplanar 3D points represented in an affine reference frame, the projection of all points in the set can be computed as a linear combination of the projections of just four of the points. <p> Affine object representations have been applied extensively to 3D reconstruction and recognition tasks <ref> [10, 17] </ref>. While our results draw heavily from this research, the use of affine 3D graphical models for interactive graphics and augmented reality applications has not been previously studied. <p> We use the following two properties of affine point representations <ref> [10] </ref>: Property 1 (Re-Projection Property) When the weak perspective projection of the origin and basis points is known in an image I m , we can compute the projection of a point p from its affine coordinates: " p p # " b 1 u m b 3 u m v
Reference: [11] <author> G. D. Hager, </author> <title> Calibration-free visual control using projective invariance, </title> <booktitle> Proc. </booktitle> <address> ICCV, </address> <year> 1995. </year>
Reference-contexts: The only requirement is the ability to track across frames at least four features (points or lines) that are specified by the user during system initialization and whose world coordinates are unknown. Our approach is motivated by recent work on non-metric scene reconstruction [9, 10], uncalibrated active vision <ref> [11] </ref>, as well as recent human-computer interaction techniques [12, 13]. We show that the task of overlaying 3D virtual objects onto live video becomes considerably simplified when the camera, the virtual objects, and the environment are represented in an affine frame of reference. <p> We use a simple observation that has also been used for un-calibrated visual servoing <ref> [11] </ref>: The location of a 3D point is uniquely determined by the point's projection on two nonparallel image planes. <p> Affine object representations lead naturally to an interactive method for constraining further the user degrees of freedom during the initialization of virtual objects. Such constraints have been used for hand-eye coordination <ref> [11] </ref> and vision-based pointing interfaces [12].
Reference: [12] <author> R. Cipolla, P. A. Hadfield, and N. J. Hollinghurst, </author> <title> Uncalibrated stereo vision with pointing for a man-machine interface, </title> <booktitle> Proc. IAPR Workshop on Machine Vision Applications, </booktitle> <year> 1994. </year>
Reference-contexts: Our approach is motivated by recent work on non-metric scene reconstruction [9, 10], uncalibrated active vision [11], as well as recent human-computer interaction techniques <ref> [12, 13] </ref>. We show that the task of overlaying 3D virtual objects onto live video becomes considerably simplified when the camera, the virtual objects, and the environment are represented in an affine frame of reference. <p> Affine object representations lead naturally to an interactive method for constraining further the user degrees of freedom during the initialization of virtual objects. Such constraints have been used for hand-eye coordination [11] and vision-based pointing interfaces <ref> [12] </ref>.
Reference: [13] <author> A. Azarbayejani, T. Starner, B. Horowitz, and A. Pent-land, </author> <title> Visually controlled graphics, </title> <journal> IEEE Trans. PAMI, v. </journal> <volume> 15, </volume> <pages> pp. 602605, </pages> <year> 1993. </year>
Reference-contexts: Applications of this powerful visualization tool include overlaying clinical 3D data with live video of patients during surgical planning <ref> [13] </ref> as well as developing three-dimensional user interfaces [4]. <p> Our approach is motivated by recent work on non-metric scene reconstruction [9, 10], uncalibrated active vision [11], as well as recent human-computer interaction techniques <ref> [12, 13] </ref>. We show that the task of overlaying 3D virtual objects onto live video becomes considerably simplified when the camera, the virtual objects, and the environment are represented in an affine frame of reference.
Reference: [14] <author> A. Shashua, </author> <title> A geometric invariant for visual recognition and 3d reconstruction from two perspective/orthographic views, </title> <booktitle> Proc. Qualitative Vision Workshop, </booktitle> <pages> pp. 107117, </pages> <year> 1993. </year>
Reference-contexts: In particular, initialization of virtual objects is reduced to the operation of affine reconstruction [10], visible-surface rendering is reduced to re-projection <ref> [14, 15] </ref>, and rigid animation is reduced to the problem of synthesizing conjugate rotation transformations [16]. <p> Third, efficient execution of computer graphics operations on affine virtual objects is achieved by implementing affine projection computations directly on dedicated graphics hardware. 2 Affine Point Representations A basic operation in our method for computing the projection of a virtual object is that of re-projection <ref> [14, 15] </ref>: given the projection of a collection of 3D points at two positions of the camera, compute the projection of these points at a third camera position.
Reference: [15] <author> E. B. Barrett, M. H. Brill, N. N. Haag, and P. M. Payton, </author> <title> Invariant linear methods in photogrammetry and model-matching, in Geometric Invariance in Computer Vision, </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: In particular, initialization of virtual objects is reduced to the operation of affine reconstruction [10], visible-surface rendering is reduced to re-projection <ref> [14, 15] </ref>, and rigid animation is reduced to the problem of synthesizing conjugate rotation transformations [16]. <p> Third, efficient execution of computer graphics operations on affine virtual objects is achieved by implementing affine projection computations directly on dedicated graphics hardware. 2 Affine Point Representations A basic operation in our method for computing the projection of a virtual object is that of re-projection <ref> [14, 15] </ref>: given the projection of a collection of 3D points at two positions of the camera, compute the projection of these points at a third camera position.
Reference: [16] <author> P. A. Beardsley, I. D. Reid, A. Zisserman, and D. W. Murray, </author> <title> Active visual navigation using non-metric structure, </title> <booktitle> Proc. ICCV, </booktitle> <pages> pp. 5864, </pages> <year> 1995. </year>
Reference-contexts: In particular, initialization of virtual objects is reduced to the operation of affine reconstruction [10], visible-surface rendering is reduced to re-projection [14, 15], and rigid animation is reduced to the problem of synthesizing conjugate rotation transformations <ref> [16] </ref>. <p> By repeating this process three times, the user provides enough information to span the entire space of rotations. More precisely, we use the following two theorems which exploit the special structure of matrix T. This matrix is a conjugate rotation transformation <ref> [16] </ref> that allows generation of all 4 fi 4 transformations corresponding to rigid rotations about the axis of rotation of T: Theorem 1 (Rotation about the axis of T) Transformations corresponding to rotation about the axis of T are of the form T () = Q cos + i sin 0 <p> In practice, the value of i is found numerically by minimizing the residual of Eq. (7) in the interval [0; 2). Theorems 1 and 2 are closely related to recent techniques for Euclidean and affine camera calibration that exploit the special structure of conjugate rotation transformations <ref> [16, 24] </ref>. Intuitively, the problem of synthesizing conjugate rotation transformations is equivalent to the indirect computation of metric quantities.
Reference: [17] <author> Y. Lamdan, J. T. Schwartz, and H. J. Wolfson, </author> <title> Object recognition by affice invariant matching, </title> <booktitle> Proc. CVPR, </booktitle> <pages> pp. 335344, </pages> <year> 1988. </year>
Reference-contexts: Affine object representations have been applied extensively to 3D reconstruction and recognition tasks <ref> [10, 17] </ref>. While our results draw heavily from this research, the use of affine 3D graphical models for interactive graphics and augmented reality applications has not been previously studied.
Reference: [18] <author> M. Bajura and U. Neumann, </author> <title> Dynamic registration correction in video-based augmented reality systems, </title> <journal> IEEE Computer Graphics and Applcations, v. </journal> <volume> 15, </volume> <editor> n. </editor> <volume> 5, </volume> <pages> pp. 5260, </pages> <year> 1995. </year>
Reference-contexts: Very little work has been published on augmented reality systems that reduce the effects of calibration errors through real-time processing of the live video stream <ref> [2, 6, 8, 18] </ref>. To our knowledge, only two systems have been reported [2, 8] that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed [5] or perfectly calibrated.
Reference: [19] <author> J. D. Foley, A. van Dam, S. K. Feiner, and J. F. Hughes, </author> <title> Computer Graphics Principles and Practice. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: First, we show that by representing virtual objects in an affine reference frame and by performing computer graphics operations such as projection and visible-surface determination directly on affine models, the entire video overlay process is described by a single 4 fi 4 homogeneous view transformation matrix <ref> [19] </ref>. Furthermore, the elements of this matrix are simply the image x- and y- coordinates of feature points. <p> One of the key aspects of affine object representations is that even though they are nonEuclidean, they nevertheless allow rendering operations <ref> [19] </ref> such as z-buffering and clipping to be performed accurately. This is because both depth order as well as the intersection of lines and planes is preserved under affine transformations.
Reference: [20] <author> Y. Bar-Shalom and T. E. Fortmann, </author> <title> Tracking and Data Association. </title> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: Furthermore, the elements of this matrix are simply the image x- and y- coordinates of feature points. This not only enables the efficient estimation of the view transformation matrix but also leads to the use of optimal estimators such as the Kalman filter <ref> [20] </ref> to both track the feature points and compute the matrix. Second, the use of affine models leads to a simple through-the-lens method [21] for interactively placing virtual objects within the user's 3D environment and for animating them relative to other physical or virtual objects. <p> Upon initialization of the affine basis, the linear features defining the basis are tracked automatically. Line tracking runs on a single processor at a rate of 60Hz for approximately 12 lines and provides updated Kalman filter estimates for the elements of the projection matrix <ref> [20, 22] </ref>. Conceptually, the tracking subsystem can be thought of as an affine camera position tracker that returns the current affine projection matrix asynchronously upon request. For each new video frame, the rows of the projection matrix are used to build the view transformation matrix.
Reference: [21] <author> M. Gleicher and A. Witkin, </author> <title> Through-the-lens camera control, </title> <booktitle> Proc. SIGGRAPH'92, </booktitle> <pages> pp. 331340, </pages> <year> 1992. </year>
Reference-contexts: This not only enables the efficient estimation of the view transformation matrix but also leads to the use of optimal estimators such as the Kalman filter [20] to both track the feature points and compute the matrix. Second, the use of affine models leads to a simple through-the-lens method <ref> [21] </ref> for interactively placing virtual objects within the user's 3D environment and for animating them relative to other physical or virtual objects.
Reference: [22] <author> K. N. Kutulakos and J. Vallino, </author> <title> Affine object representations for calibration-free augmented reality, </title> <booktitle> Proc. IEEE Virtual Reality Annual Symp., </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Our system tracks lines in real time and uses line intersections to define the basis points (see <ref> [22] </ref> for details). 4 Visible Surface Rendering The projection of points on an affinely-represented virtual object is completely determined by the location of the feature points defining the affine frame. <p> Upon initialization of the affine basis, the linear features defining the basis are tracked automatically. Line tracking runs on a single processor at a rate of 60Hz for approximately 12 lines and provides updated Kalman filter estimates for the elements of the projection matrix <ref> [20, 22] </ref>. Conceptually, the tracking subsystem can be thought of as an affine camera position tracker that returns the current affine projection matrix asynchronously upon request. For each new video frame, the rows of the projection matrix are used to build the view transformation matrix.
Reference: [23] <author> S. M. Seitz and C. R. Dyer, </author> <title> Complete scene structure from four point correspondences, </title> <booktitle> Proc. ICCV, </booktitle> <pages> pp. 330337, </pages> <year> 1995. </year>
Reference-contexts: In particular, if L ; R are the projection matrices associated with the first and second image, respectively, and L ; R are the corresponding optical axes defined by Eq. (3), the epipolar line can be parameterized by the set <ref> [23] </ref> By taking the epipolar constraint into account, we can ensure that the points specified by the user provide a physically-valid placement of the virtual object. Affine object representations lead naturally to an interactive method for constraining further the user degrees of freedom during the initialization of virtual objects.
Reference: [24] <author> R. Horaud, F. Dornaika, B. Boufama, and R. Mohr, </author> <title> Self calibration of a stereo head mounted onto a robot arm, </title> <booktitle> Proc. ECCV, </booktitle> <pages> pp. 455462, </pages> <year> 1994. </year>
Reference-contexts: In practice, the value of i is found numerically by minimizing the residual of Eq. (7) in the interval [0; 2). Theorems 1 and 2 are closely related to recent techniques for Euclidean and affine camera calibration that exploit the special structure of conjugate rotation transformations <ref> [16, 24] </ref>. Intuitively, the problem of synthesizing conjugate rotation transformations is equivalent to the indirect computation of metric quantities.
References-found: 24

