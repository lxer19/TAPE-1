URL: ftp://cns.brown.edu/nin/papers/comay.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Title: Ensemble Training: Some Recent Experiments with Postal Zip Data  
Author: Oded Comay Nathan Intrator 
Address: Ramat-Aviv 69978, Israel  
Affiliation: Computer Science Department Tel-Aviv University  
Abstract: Recent findings suggest that a classification scheme based on an ensemble of networks is an effective way to address overfitting. We study optimal methods for training an ensemble of networks. Some recent experiments on Postal Zip-code character data suggest that weight decay may not be an optimal method for controlling the variance of a classifier.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Harris Drucker, Robert Schapire, and Patrice Simard. </author> <title> Improving performance in neural networks using a boosting algorithm. </title> <editor> In Steven J. Hanson, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 42-49. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Single nearest neighbor classification error on our test set was 7% (compared with a single nearest neighbor misclassification error of 5.9% obtained by Drucker et al. <ref> [1] </ref> on similar data with 2000 test patterns. The training data set was fed into back-propagation feed forward fully connected sig-moidal networks with one hidden layer and bias. The networks' sizes were varied from 15 hidden units (1080 weights) to 200 hidden units (14400 weights).
Reference: [2] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias-variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: Recently, it has been observed that an ensemble of networks may have better generalization error than the average of the performance of single networks in the ensemble e.g. [5, 7, 8, 6]. A compelling example of this effect is given in <ref> [2] </ref>. where an ensemble of classification networks is trained on a binary classification task with sinusoidal boundaries (see also [7].) While the boundaries found by each of the classifiers do not resemble the sinusoidal boundary, an average of several networks does.
Reference: [3] <author> Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel. </author> <title> Handwritten digit recognition with a back-propagation network. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 396-404, </pages> <address> San Mateo, 1990. (Denver 1989), </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This reduced the number of connections by about 60% while keeping error relatively the same. On a similar classification problem, Le Cun et al. <ref> [3] </ref> used weight sharing to reduce a network of 98442 connections to 2578 free parameters using prior knowledge on the more likely weights to be shared.
Reference: [4] <author> Y. Le Cun, J.S. Denker, and S.A. Solla. </author> <title> Optimal brain damage. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605, </pages> <address> San Mateo, 1990. (Denver 1989), </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1/3 of the data (3479 digits). 2.1 Heuristic dimensionality reduction Various approaches have been used for reducing the number of parameters in the classification scheme so as to alleviate the overfitting problem associated with large number of parameters compared with relatively small number of training samples; Le Cun et al. <ref> [4] </ref> have proposed an "optimal brain damage" technique, in which they erased connections with weights falling below a predefined threshold. This reduced the number of connections by about 60% while keeping error relatively the same.
Reference: [5] <author> W. P. Lincoln and J. Skrzypek. </author> <title> Synergy of clustering multiple back-propagation net-works. </title> <editor> In D. S. Touretzky and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 650-657. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: A manifestation of this problem is the poor MSE generalization which is usually attributed to overfitting. Recently, it has been observed that an ensemble of networks may have better generalization error than the average of the performance of single networks in the ensemble e.g. <ref> [5, 7, 8, 6] </ref>.
Reference: [6] <author> D. J. C. MacKay. </author> <title> A practical Bayesian framework for backprop networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 448-472, </pages> <year> 1992. </year>
Reference-contexts: A manifestation of this problem is the poor MSE generalization which is usually attributed to overfitting. Recently, it has been observed that an ensemble of networks may have better generalization error than the average of the performance of single networks in the ensemble e.g. <ref> [5, 7, 8, 6] </ref>.
Reference: [7] <author> B. A. Pearlmutter and R. Rosenfeld. </author> <title> Chaitin-Kolmogorov complexity and generalization in neural networks. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 925-931. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: A manifestation of this problem is the poor MSE generalization which is usually attributed to overfitting. Recently, it has been observed that an ensemble of networks may have better generalization error than the average of the performance of single networks in the ensemble e.g. <ref> [5, 7, 8, 6] </ref>. <p> A compelling example of this effect is given in [2]. where an ensemble of classification networks is trained on a binary classification task with sinusoidal boundaries (see also <ref> [7] </ref>.) While the boundaries found by each of the classifiers do not resemble the sinusoidal boundary, an average of several networks does. These findings suggest that an ensemble of networks is an effective way to address overfitting.
Reference: [8] <author> Michael P. Perrone. </author> <title> Improving Regression Estimation: Averaging Methods for Variance Reduction with Extensions to General Convex Measure Optimization. </title> <type> PhD thesis, </type> <institution> Brown University, Institute for Brain and Neural Systems, </institution> <year> 1993. </year>
Reference-contexts: A manifestation of this problem is the poor MSE generalization which is usually attributed to overfitting. Recently, it has been observed that an ensemble of networks may have better generalization error than the average of the performance of single networks in the ensemble e.g. <ref> [5, 7, 8, 6] </ref>.
Reference: [9] <author> D. C. Plaut, S. J. Nowlan, and G. E. Hinton. </author> <title> Experiments on learning by backpropagation. </title> <type> Technical Report CMU-CS-86-126, </type> <institution> Carnegie-Mellon University, </institution> <year> 1986. </year>
Reference-contexts: When training single networks for best generalization the overfitting problem is addressed by adding various constraints to the the objective function minimization such as weight decay <ref> [9, 10] </ref>. Other methods use early stopping based on cross validation misclassification. In this paper we study the effect of averaging on the ability to predict generalization error from MSE on the training set.
Reference: [10] <author> A. S. Weigend, D. E. Rumelhart, and B. A. Huberman. </author> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 875-882. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: When training single networks for best generalization the overfitting problem is addressed by adding various constraints to the the objective function minimization such as weight decay <ref> [9, 10] </ref>. Other methods use early stopping based on cross validation misclassification. In this paper we study the effect of averaging on the ability to predict generalization error from MSE on the training set.
References-found: 10

