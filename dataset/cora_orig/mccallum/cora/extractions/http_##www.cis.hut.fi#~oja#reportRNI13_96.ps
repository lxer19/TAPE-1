URL: http://www.cis.hut.fi/~oja/reportRNI13_96.ps
Refering-URL: http://www.cis.hut.fi/~oja/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Comparison of Neural and Statistical Classifiers| Theory and Practice  
Author: Lasse Holmstrom Rolf Nevanlinna Institute Petri Koistinen Rolf Nevanlinna Institute Jorma Laaksonen Erkki Oja 
Note: ISBN 952-9528-32-9 ISSN 0787-8338 YLIOPISTOPAINO  
Address: P.O. Box 4 (Yliopistonkatu 5) 00014 University of Helsinki, Finland  
Affiliation: Laboratory of Computer and Information Science, Helsinki University of Technology  Laboratory of Computer and Information Science, Helsinki University of Technology Rolf Nevanlinna Institute  
Abstract: Research Reports A13 January 1996 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. S. Baird. </author> <title> Recognition technology frontiers. </title> <journal> Pattern Recognition Letters, </journal> <volume> 14(4) </volume> <pages> 327-334, </pages> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: In statistics literature supervised classification is often called discriminant analysis [68]. A popular application of pattern recognition is o*ine character recognition, where research during the last two decades has focused on handwritten characters <ref> [99, 65, 1 50, 71, 101, 100, 96, 30, 107, 1] </ref>. This is mainly due to the fact that while recognition of machine printed characters is considered a solved problem, the reliability achieved with handwritten text has not yet reached the level required in practical applications. <p> When r (x; t) is linear in its parameters, one can use the methodology developed for generalized linear models [66] to estimate the parameter vector. Note that the estimated posterior probability then takes values in the interval <ref> [0; 1] </ref> as a probability should. Note also that the least squares fitting criterion (6) can be thought to rise from using the maximum likelihood principle to estimate a regression model where errors are distributed normally, whereas the logistic approach uses binomially distributed error, clearly the statistically correct model.
Reference: [2] <author> R. Becker, J. Chambers, and A. Wilks. </author> <title> The NEW S Language. </title> <publisher> Chapman and Hall, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The tree classifier therefore uses the Bayes rule with the class posterior probabilities estimated by locally constant functions. The particular tree classifier used here is available as part of the S-Plus statistical software package <ref> [2, 8, 106] </ref>. This implementation uses a likelihood function to select the optimal splits [10]. Pruning was performed by the minimal cost-complexity method.
Reference: [3] <author> C. M. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, </publisher> <year> 1995. </year>
Reference-contexts: In another approach the densities are estimated as finite mixtures of some standard probability densities by using the expectation-maximization (EM) algorithm or some other method [79, 103, 77, 78, 38]. Such an approach can be viewed as an economized KDA or an instance of the RBF approach <ref> [3] </ref>. The Self-organizing Reduced Kernel Density Estimator introduced in [45] estimates densities in the spirit of radial basis functions and we refer to the corresponding classification method as Reduced Kernel Discriminant Analysis (RKDA). <p> In parametric regression one models the posterior probabilities using a family of functions described by a finite number of parameters. Examples are linear and logistic regression, as well as a Multi-Layer Perceptron (MLP) <ref> [40, 3] </ref> with a fixed architecture, i.e., fixed layer sizes. In nonparametric regression no fixed functional form is assumed. <p> The Nadaraya-Watson kernel regression estimator is also called the General Regression Neural Network [94]. Other neural network approaches include Multi-Layer Perceptrons with a flexible architecture and Radial Basis Function expansions <ref> [40, 3] </ref>. 5 One way to fit a regression model is to use binary least squares regression. In a two class case we define the response y i = 1 or 0 depending on whether the ith training vector x i comes from class 1 or 2. <p> j (x) = P c :(9) Then a natural fitting criterion is to maximize the conditional log-likelihood n X c X y i log q j (x i ) = i=1 r2R In the case of two classes this approach is equivalent to the use of the cross-entropy fitting criterion <ref> [3] </ref>. A very natural approach would be a regression technique that uses the number of misclassified patterns as the fitting criterion to be minimized. This provides for example one viable way of selecting the smoothing parameters in a classifier based on kernel regression. <p> from the point of view of integrated squared error and not discrimination performance which is the true focus of interest. 3.4 MLP We used a standard Multi-Layer Perceptron with d inputs, ` hidden units and c output units and with the logistic activation function in the hidden and output units <ref> [40, 3] </ref>. Such a network has (d + 1)` + (` + 1)c adaptable weights which in our experiments were determined by minimizing the sum of squared errors criterion using a conjugate gradient method [88]. <p> It seems difficult to select a network architecture which is flexible enough for the task and at the same time is parameterized parsimoniously enough so that the parameters can be reliably determined from the training data. Some kind of regularization such as weight decay <ref> [40, 3] </ref> might help in reducing the overfitting. Excluding the committee classifier, the best results were obtained with Local Linear Regression (LLR). Unfortunately, this method is impractically slow since the weighted linear least squares problem has to be solved separately at each pattern to be classified.
Reference: [4] <author> J. L. Blue, G. T. Candela, P. J. Grother, R. Chellappa, and C. L. Wilson. </author> <title> Evaluation of pattern classifiers for fingerprint and OCR applications. </title> <journal> Pattern Recognition, </journal> <volume> 27(4) </volume> <pages> 485-501, </pages> <year> 1994. </year>
Reference-contexts: One of the databases used in that study consisted of handwritten digits. The good performance of nearest neighbor classifiers for handwritten digits was also confirmed by Blue et al. <ref> [4] </ref>, who however only compared the result to Radial Basis Function and Multi-Layer Perceptron (MLP) neural classifiers. A kernel discriminant analysis type method, the Probabilistic Neural Network, also did very well in that study. <p> Further, Local Linear Regression has been rarely used previously in a classification context. Besides a different set of classifiers selected for comparison, the present study also differs from <ref> [4] </ref> in our systematic use of training set cross-validation in all classifier design. The final performance estimates are based on an independent testing set that had no role in classifier construction, including the choice of optimal pattern vector dimension. <p> The bounding boxes of the digits were normalized to 32 fi 32 pixels and the slant of the images was removed. No normalization of the line width was performed. The preprocessing procedure closely resembles that of [29], <ref> [4] </ref>, and it produced a sample of 17880 binary vectors, each 1024-dimensional. Some examples are displayed in the leftmost column of Figure 2. <p> The KL transform was derived from the estimated covariance matrix of the training set. This approach is similar to that adopted in [29], <ref> [4] </ref>, and it is based on the assumption that the essential information needed in classification is concentrated along the directions which correspond to the largest variations in the data. In practice, it is possible that the classification accuracy in fact improves when the dimensionality of the data is reduced.
Reference: [5] <author> L. Bottou, C. Cortes, J. S. Denker, H. Drucker, I. Guyon, L. D. Jackel, Y. Le-Cun, U. A. Muller, E. Sackinger, P. Y. Simard, and V. Vapnik. </author> <title> Comparison of classifier methods: A case study in handwritten digit recognition. </title> <booktitle> In Proceedings of 12th International Conference on Pattern Recognition, </booktitle> <volume> volume II, </volume> <pages> pages 77-82. </pages> <address> IAPR, </address> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: Recently, many benchmark and comparison studies have been published on neural and statistical classifiers <ref> [81, 9, 49, 5] </ref>. One of the most extensive was the Statlog project in which statistical methods, machine learning and neural networks were compared [69]. <p> Perhaps the best example of a clean-cut neural network classifier is the LeNet system <ref> [62, 5] </ref> for handwritten digit recognition. Such a computational model supports well the implementation in regular VLSI circuits.
Reference: [6] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Chapman & Hall, </publisher> <year> 1984. </year>
Reference-contexts: While such techniques have been proposed also in parametric regression settings [41] they are seldom used in practice. Classification and Regression Trees (CART) are an example of a nonparametric technique that estimates the posterior probabilities directly but uses neither the binary nor the logistic regression approach <ref> [6] </ref>. 2.3 Other Classifiers In addition to using the Bayes rule combined with either density estimation or regression, a third popular approach is to model the classes using prototypes and classify according to the shortest distance (defined in a suitable sense) to a prototype. <p> the same and that the local constant regression estimator uses the kernel K of the KDA classifier as its weight function. 3.6 Tree classifier, MARS and FDA The introduction of tree based models in statistics dates back to [70] although their current popularity is largely due to the seminal book <ref> [6] </ref>. For Euclidean pattern vectors x = [x 1 ; : : : ; x d ] T , a classification tree is a binary tree where at each node the decision to branch either to left or right is based on a test of the form x i .
Reference: [7] <author> J. Bridle. </author> <title> Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. </title> <editor> In D. Touretzky, </editor> <booktitle> 29 editor, Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 211-217, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: One natural way of generalizing the logistic regression approach 6 is to model the posterior probabilities as the softmax <ref> [7] </ref> of the components of an R c -valued function r, P (J = j j X = x) = q j (x) = P c :(9) Then a natural fitting criterion is to maximize the conditional log-likelihood n X c X y i log q j (x i ) =
Reference: [8] <author> J. Chambers and T. Hastie, </author> <title> editors. Statistical Models in S. </title> <publisher> Chapman and Hall, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: The tree classifier therefore uses the Bayes rule with the class posterior probabilities estimated by locally constant functions. The particular tree classifier used here is available as part of the S-Plus statistical software package <ref> [2, 8, 106] </ref>. This implementation uses a likelihood function to select the optimal splits [10]. Pruning was performed by the minimal cost-complexity method.
Reference: [9] <author> B. Cheng and D. Titterington. </author> <title> Neural networks: A review from a statistical perspective. </title> <journal> Statistical Science, </journal> <volume> 9(1) </volume> <pages> 2-54, </pages> <year> 1994. </year>
Reference-contexts: Recently, many benchmark and comparison studies have been published on neural and statistical classifiers <ref> [81, 9, 49, 5] </ref>. One of the most extensive was the Statlog project in which statistical methods, machine learning and neural networks were compared [69].
Reference: [10] <author> L. Clark and D. </author> <title> Pregibon. Tree-based models. </title> <note> Chapter 9 of [8]. </note>
Reference-contexts: The tree classifier therefore uses the Bayes rule with the class posterior probabilities estimated by locally constant functions. The particular tree classifier used here is available as part of the S-Plus statistical software package [2, 8, 106]. This implementation uses a likelihood function to select the optimal splits <ref> [10] </ref>. Pruning was performed by the minimal cost-complexity method.
Reference: [11] <author> W. Cleveland and S. Devlin. </author> <title> Locally weighted regression: an approach to regression analysis by local fitting. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 83 </volume> <pages> 596-610, </pages> <year> 1988. </year>
Reference-contexts: In nonparametric regression no fixed functional form is assumed. Examples of nonparametric methodologies are Projection Pursuit [23, 19], additive models [37], Multivariate Adaptive Regression Splines (MARS) [22], the Nadaraya-Watson kernel regression estimator [108, 56] and Local Linear Regression (LLR) <ref> [11, 12, 108] </ref>. The Nadaraya-Watson kernel regression estimator is also called the General Regression Neural Network [94]. Other neural network approaches include Multi-Layer Perceptrons with a flexible architecture and Radial Basis Function expansions [40, 3]. 5 One way to fit a regression model is to use binary least squares regression. <p> Such estimators have received more attention recently, see e.g. [36]. The particular version described below is also called 13 LOESS <ref> [11, 12] </ref>. Local Linear Regression models the regression function in the neighborhood of each point x by means of a linear function z 7! b 0 + B (z x). <p> As the function w we used the tricube weight function <ref> [11] </ref> w (u) = max ((1 juj 3 ) 3 ; 0):(21) The local bandwidth h (x) is controlled by a neighborhood size parameter 0 &lt; ff 1: one takes k equal to ffn rounded to the nearest integer and then takes h (x) equal to the distance to the kth
Reference: [12] <author> W. Cleveland and C. Loader. </author> <title> Smoothing by local regression: Principles and methods. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1995. </year> <note> available as 95.3.ps in http://netlib.att.com/netlib/att/stat/doc/. </note>
Reference-contexts: In nonparametric regression no fixed functional form is assumed. Examples of nonparametric methodologies are Projection Pursuit [23, 19], additive models [37], Multivariate Adaptive Regression Splines (MARS) [22], the Nadaraya-Watson kernel regression estimator [108, 56] and Local Linear Regression (LLR) <ref> [11, 12, 108] </ref>. The Nadaraya-Watson kernel regression estimator is also called the General Regression Neural Network [94]. Other neural network approaches include Multi-Layer Perceptrons with a flexible architecture and Radial Basis Function expansions [40, 3]. 5 One way to fit a regression model is to use binary least squares regression. <p> for the sum of squared errors to prevent the network from converging to a shallow local minimum. 3.5 LLR Local Linear Regression (LLR) (or more generally local polynomial regression) is a nonparametric regression method which has its roots in classical methods proposed for the smoothing of time series data, see <ref> [12] </ref>. Such estimators have received more attention recently, see e.g. [36]. The particular version described below is also called 13 LOESS [11, 12]. <p> Such estimators have received more attention recently, see e.g. [36]. The particular version described below is also called 13 LOESS <ref> [11, 12] </ref>. Local Linear Regression models the regression function in the neighborhood of each point x by means of a linear function z 7! b 0 + B (z x).
Reference: [13] <author> P. Craven and G. Wahba. </author> <title> Smoothing noisy data with spline functions. </title> <journal> Numerical Mathematics, </journal> <volume> 31 </volume> <pages> 317-403, </pages> <year> 1979. </year>
Reference-contexts: The maximum order of variable interactions (products of variables) allowed in the functions B k , as well the maximum value of M allowed in the forward stage, are parameters that need to be tuned experimentally. Backward model selection uses the generalized cross-validation criterion (GCV) introduced in <ref> [13] </ref>. GCV (M ) = P n [1 C (M )=n] 2 : Here the numerator is the lack-of-fit based on the training data and the denominator imposes a penalty for increasing the model complexity, C (M ).
Reference: [14] <author> P. Devijver and J. Kittler. </author> <title> Pattern Recognition: A Statistical Approach. </title> <booktitle> Prentice-Hall International, </booktitle> <year> 1982. </year>
Reference-contexts: In a narrow sense, the pattern recognition problem can be parceled to data acquisition and preprocessing, feature extraction, and classification. In addition to general references such as <ref> [17, 117, 14, 102, 25, 68, 84, 24] </ref> questions of pattern recognition are extensively covered in the literature of many specific application areas such as speech and image recognition. The performance of statistical and neural network methods is usually discussed in connection with feature extraction and classification. <p> Feature extraction involves the determination of those characteristics of the data that make subsequent reliable classification possible. Often one is able to take advantage of problem dependent features of the data but several general purpose approaches are available as well <ref> [14] </ref>. Efficient feature extraction is crucial for reliable classification and, if possible, these two subsystems should be matched optimally in the design of a complete pattern recognition system. In this article we concentrate on classification only. <p> More often the individual classes overlap and the smallest achievable error, the Bayes classification error, is positive. The Bayes classifier that minimizes the misclassification error <ref> [14] </ref> is defined by g BAYES (x) = argmax j=1;::: ;c Here "argmax" denotes the maximizing argument, i.e., x is classified to the class that maximizes the product P j f j (x). <p> Kernel or Parzen estimates as well as k-nearest neighbor methods (at least when k is large) are examples of popular nonparametric density estimation methods [90, 86, 108]. They give rise to such classification methods as Kernel Discriminant Analysis (KDA) [18, 33, 89] and k-Nearest Neighbor (k-NN) rules <ref> [18, 14] </ref>. The Probabilistic Neural Network (PNN) [95] is the neural network counterpart of KDA. In another approach the densities are estimated as finite mixtures of some standard probability densities by using the expectation-maximization (EM) algorithm or some other method [79, 103, 77, 78, 38]. <p> regression function output space R c onto a lower dimensional feature space R ` in a manner that optimally facilitates prototype classification based on the transformed class means A (r ( b j )) and a weighted Euclidean distance function. 3.7 k-NN and Learning k-NN Classifiers In a k-NN classifier <ref> [14] </ref> each class is represented by a set of prototype vectors. The k closest neighbors of a pattern vector are found from among all the prototypes and the class label is decided by the majority rule. <p> The whole process of classifier design should then be based strictly on the training sample only. In addition to parameter estimation, the design of some classifiers involves the choice of various tuning parameters and model or architecture selection. To utilize the training sample efficiently, cross-validation [98] (or "rotation", cf. <ref> [14, Ch. 10.6.4] </ref>) can be used. In v-fold cross-validation the training sample is first divided into v disjoint subsets. One subset at a time is then put aside, a classifier is designed based on the union of the remaining v 1 subsets and then tested for the subset left out. <p> The optimal cross-validated parameters are also shown for reference. To assess the statistical accuracy of the results, one can obtain a nominal estimate for the standard deviation of the error percentage by using the binomial distribution, cf. <ref> [14, Ch. 10.3] </ref>. In the range of 2.5-4.0%, where the results for our best classifiers lie, this gives a standard deviation of approximately 0.2 percentage points. Then, e.g., a nominal 95% confidence interval for the error rate of the LLR classifier would be given by 2:8 0:4%.
Reference: [15] <author> H. Drucker, C. Cortes, L. D. Jackel, Y. LeCun, and V. Vapnik. </author> <title> Boosting and other ensemble methods. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1289-1301, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Given a fixed feature extraction method one can either use a common training set to design a number of different types of classifiers [48] or alternatively use different training sets to design several versions of one type of classifier <ref> [16, 15, 42, 85, 93] </ref>. 19 5 Case Study: Classification of Handwritten Dig- its 5.1 Data and its Preprocessing We tested the performance of the classifiers defined previously on a realistic pattern recognition problem, the classification of handwritten digits. To obtain the test material, we used a set of forms.
Reference: [16] <author> H. Drucker, R. Schapire, and P. Simard. </author> <title> Boosting performance in neural networks. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 7(4) </volume> <pages> 705-719, </pages> <year> 1993. </year>
Reference-contexts: Given a fixed feature extraction method one can either use a common training set to design a number of different types of classifiers [48] or alternatively use different training sets to design several versions of one type of classifier <ref> [16, 15, 42, 85, 93] </ref>. 19 5 Case Study: Classification of Handwritten Dig- its 5.1 Data and its Preprocessing We tested the performance of the classifiers defined previously on a realistic pattern recognition problem, the classification of handwritten digits. To obtain the test material, we used a set of forms.
Reference: [17] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: In a narrow sense, the pattern recognition problem can be parceled to data acquisition and preprocessing, feature extraction, and classification. In addition to general references such as <ref> [17, 117, 14, 102, 25, 68, 84, 24] </ref> questions of pattern recognition are extensively covered in the literature of many specific application areas such as speech and image recognition. The performance of statistical and neural network methods is usually discussed in connection with feature extraction and classification.
Reference: [18] <author> E. Fix and J. Hodges. </author> <title> Discriminatory analysis|nonparametric discrimination: Consistency properties. </title> <type> Technical Report Number 4, </type> <institution> Project Number 21-49-004, USAF School of Aviation Medicine, Randolph Field, Texas, </institution> <year> 1951. </year> <note> reprinted in [89]. </note>
Reference-contexts: Kernel or Parzen estimates as well as k-nearest neighbor methods (at least when k is large) are examples of popular nonparametric density estimation methods [90, 86, 108]. They give rise to such classification methods as Kernel Discriminant Analysis (KDA) <ref> [18, 33, 89] </ref> and k-Nearest Neighbor (k-NN) rules [18, 14]. The Probabilistic Neural Network (PNN) [95] is the neural network counterpart of KDA. <p> Kernel or Parzen estimates as well as k-nearest neighbor methods (at least when k is large) are examples of popular nonparametric density estimation methods [90, 86, 108]. They give rise to such classification methods as Kernel Discriminant Analysis (KDA) [18, 33, 89] and k-Nearest Neighbor (k-NN) rules <ref> [18, 14] </ref>. The Probabilistic Neural Network (PNN) [95] is the neural network counterpart of KDA. In another approach the densities are estimated as finite mixtures of some standard probability densities by using the expectation-maximization (EM) algorithm or some other method [79, 103, 77, 78, 38].
Reference: [19] <author> T. Flick, L. Jones, R. Priest, and C. Herman. </author> <title> Pattern classification using projection pursuit. </title> <journal> Pattern Recognition, </journal> <volume> 23(12) </volume> <pages> 1367-1376, </pages> <year> 1990. </year>
Reference-contexts: Examples are linear and logistic regression, as well as a Multi-Layer Perceptron (MLP) [40, 3] with a fixed architecture, i.e., fixed layer sizes. In nonparametric regression no fixed functional form is assumed. Examples of nonparametric methodologies are Projection Pursuit <ref> [23, 19] </ref>, additive models [37], Multivariate Adaptive Regression Splines (MARS) [22], the Nadaraya-Watson kernel regression estimator [108, 56] and Local Linear Regression (LLR) [11, 12, 108]. The Nadaraya-Watson kernel regression estimator is also called the General Regression Neural Network [94].
Reference: [20] <author> J. Franke and E. Mandler. </author> <title> A comparison of two approaches for combining the votes of cooperating classifiers. </title> <booktitle> In Proceedings of the 11th International Conference on Pattern Recognition, </booktitle> <volume> volume II, </volume> <pages> pages 611-614, </pages> <address> Hague, </address> <month> August </month> <year> 1992. </year> <month> IAPR. </month>
Reference-contexts: A genetic algorithm can search for optimal weights to combine the classifier outputs [60]. Theoretically more advanced methods may be derived from the EM-algorithm [43, 44, 113, 114, 52] or from the Dempster-Shafer theory of evidence <ref> [116, 20] </ref>. The outputs of several classifiers may be combined linearly [51] or nonlinearly [104] to reduce the variance of the posterior probability estimates. A more general case is variance reduction in continuous function estimation.
Reference: [21] <author> J. Friedman. </author> <title> Regularized discriminant analysis. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 84(405) </volume> <pages> 165-175, </pages> <year> 1989. </year>
Reference-contexts: In case of equal class covariance matrices a linear function results and the term Linear Discriminant Analysis (LDA) is used. A recent development is Regularized Discriminant Analysis (RDA) <ref> [21] </ref> which interpolates between LDA and QDA. A thorough account of these and other classifiers is provided by McLachlan [68]. <p> the previous formula simplifies to the Linear Discriminant Analysis (LDA) rule g LDA (x) = argmax j=1;:::;c log ^ P j + b T b (x 2 10 where a natural estimate for is the pooled covariance matrix estimate b = j=1 b j :(13) Friedman's Regularized Discriminant Analysis (RDA) <ref> [21] </ref> is a compromise between LDA and QDA. The decision rule is otherwise the same as (11) but instead of b j one uses regularized covariance estimates b j (; fl) with two regularizing parameters. <p> Parameter controls the shrinkage of the class conditional covariance estimates toward the pooled estimate and fl controls the shrinkage toward a multiple of the identity matrix, see <ref> [21] </ref> for the exact formulas. One can obtain the QDA and LDA with certain choices of the regularizing parameters. 3.2 KDA In the Kernel Discriminant Analysis (KDA) approach one forms kernel estimates ^ f j of the class conditional densities and then applies rule (1).
Reference: [22] <author> J. Friedman. </author> <title> Multivariate adaptive regression splines. </title> <journal> The Annals of Statistics, </journal> <volume> 19 </volume> <pages> 1-141, </pages> <year> 1991. </year> <title> with discussion. </title> <type> 30 </type>
Reference-contexts: Examples are linear and logistic regression, as well as a Multi-Layer Perceptron (MLP) [40, 3] with a fixed architecture, i.e., fixed layer sizes. In nonparametric regression no fixed functional form is assumed. Examples of nonparametric methodologies are Projection Pursuit [23, 19], additive models [37], Multivariate Adaptive Regression Splines (MARS) <ref> [22] </ref>, the Nadaraya-Watson kernel regression estimator [108, 56] and Local Linear Regression (LLR) [11, 12, 108]. The Nadaraya-Watson kernel regression estimator is also called the General Regression Neural Network [94]. <p> An overfitted tree is pruned by giving ff increasingly large values and selecting nested subtrees that minimize R ff (T ). MARS <ref> [22] </ref> is a regression method that shares features with tree based modeling. MARS estimates an unknown function r using an expansion ^r (x) = a 0 + k=1 where the functions B k are multivariate splines.
Reference: [23] <author> J. Friedman and W. Stuetzle. </author> <title> Projection pursuit regression. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 76(376) </volume> <pages> 817-823, </pages> <year> 1981. </year>
Reference-contexts: Examples are linear and logistic regression, as well as a Multi-Layer Perceptron (MLP) [40, 3] with a fixed architecture, i.e., fixed layer sizes. In nonparametric regression no fixed functional form is assumed. Examples of nonparametric methodologies are Projection Pursuit <ref> [23, 19] </ref>, additive models [37], Multivariate Adaptive Regression Splines (MARS) [22], the Nadaraya-Watson kernel regression estimator [108, 56] and Local Linear Regression (LLR) [11, 12, 108]. The Nadaraya-Watson kernel regression estimator is also called the General Regression Neural Network [94].
Reference: [24] <author> K. S. Fu. </author> <title> Syntactic pattern recognition and applications. </title> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference-contexts: In a narrow sense, the pattern recognition problem can be parceled to data acquisition and preprocessing, feature extraction, and classification. In addition to general references such as <ref> [17, 117, 14, 102, 25, 68, 84, 24] </ref> questions of pattern recognition are extensively covered in the literature of many specific application areas such as speech and image recognition. The performance of statistical and neural network methods is usually discussed in connection with feature extraction and classification. <p> In this article we concentrate on classification only. Further, only supervised classification using statistical and neural network methods is considered thus leaving out both unsupervised classification (clustering [35]) and syntactic pattern recognition <ref> [24] </ref>. In statistics literature supervised classification is often called discriminant analysis [68]. A popular application of pattern recognition is o*ine character recognition, where research during the last two decades has focused on handwritten characters [99, 65, 1 50, 71, 101, 100, 96, 30, 107, 1].
Reference: [25] <author> K. Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> 2nd edition, </address> <year> 1990. </year>
Reference-contexts: In a narrow sense, the pattern recognition problem can be parceled to data acquisition and preprocessing, feature extraction, and classification. In addition to general references such as <ref> [17, 117, 14, 102, 25, 68, 84, 24] </ref> questions of pattern recognition are extensively covered in the literature of many specific application areas such as speech and image recognition. The performance of statistical and neural network methods is usually discussed in connection with feature extraction and classification. <p> Some examples are displayed in the leftmost column of Figure 2. The sample was then divided into two sets of equal size, one for training and one for testing the classifiers. 5.2 Feature Selection As pattern vectors were used Karhunen-Loeve (KL) transforms <ref> [25] </ref> of the original preprocessed images. The KL transform was derived from the estimated covariance matrix of the training set.
Reference: [26] <author> K. Fukunaga and R. R. Hayes. </author> <title> The reduced Parzen classifier. </title> <journal> IEEE Trans. Pattern Anal. and Machine Intell., </journal> <volume> PAMI-11(4):423-425, </volume> <month> April </month> <year> 1989. </year>
Reference-contexts: One solution is to reduce the estimate, that is, to use fewer kernels but to place them at optimal locations. One can also introduce kernel dependent weights and smoothing parameters. Various reduction approaches were described in <ref> [26, 27, 31, 77, 78, 103, 92, 112] </ref>. In some cases the methods suggested have a close connection with radial basis function and mixture density estimation techniques.
Reference: [27] <author> K. Fukunaga and J. M. Mantock. </author> <title> Nonparametric data reduction. </title> <journal> IEEE Trans. Pattern Anal. and Machine Intell., </journal> <volume> PAMI-6(1):115-118, </volume> <month> January </month> <year> 1984. </year>
Reference-contexts: One solution is to reduce the estimate, that is, to use fewer kernels but to place them at optimal locations. One can also introduce kernel dependent weights and smoothing parameters. Various reduction approaches were described in <ref> [26, 27, 31, 77, 78, 103, 92, 112] </ref>. In some cases the methods suggested have a close connection with radial basis function and mixture density estimation techniques.
Reference: [28] <author> K. Fukushima. </author> <title> Neocognitron: A hierarchical neural network capable of visual pattern recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 1(2) </volume> <pages> 119-130, </pages> <year> 1988. </year>
Reference-contexts: This is mainly due to the fact that while recognition of machine printed characters is considered a solved problem, the reliability achieved with handwritten text has not yet reached the level required in practical applications. Handwritten character recognition has become a popular application area for neural network classifiers <ref> [28, 32, 63] </ref>, which through their adaptive capabilities have often been able to achieve better reliability than classical statistical or knowledge-based structural recognition methods. Recently, many benchmark and comparison studies have been published on neural and statistical classifiers [81, 9, 49, 5].
Reference: [29] <author> M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson. </author> <title> NIST form-based handprint recognition system. </title> <type> Technical report, </type> <institution> National Institute of Standards and Technology, </institution> <year> 1994. </year>
Reference-contexts: In this resolution each digit was printed approximately in a 75fi100 pixel rectangle. The bounding boxes of the digits were normalized to 32 fi 32 pixels and the slant of the images was removed. No normalization of the line width was performed. The preprocessing procedure closely resembles that of <ref> [29] </ref>, [4], and it produced a sample of 17880 binary vectors, each 1024-dimensional. Some examples are displayed in the leftmost column of Figure 2. <p> The KL transform was derived from the estimated covariance matrix of the training set. This approach is similar to that adopted in <ref> [29] </ref>, [4], and it is based on the assumption that the essential information needed in classification is concentrated along the directions which correspond to the largest variations in the data. In practice, it is possible that the classification accuracy in fact improves when the dimensionality of the data is reduced.
Reference: [30] <author> M. Gilloux. </author> <title> Research into the new generation of character and mailing address recognition systems at the French post office research center. </title> <journal> Pattern Recognition Letters, </journal> <volume> 14(4) </volume> <pages> 267-276, </pages> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: In statistics literature supervised classification is often called discriminant analysis [68]. A popular application of pattern recognition is o*ine character recognition, where research during the last two decades has focused on handwritten characters <ref> [99, 65, 1 50, 71, 101, 100, 96, 30, 107, 1] </ref>. This is mainly due to the fact that while recognition of machine printed characters is considered a solved problem, the reliability achieved with handwritten text has not yet reached the level required in practical applications.
Reference: [31] <author> I. Grabec. </author> <title> Self-organization of neurons described by the maximum-entropy principle. </title> <journal> Biol. Cybern., </journal> <volume> 63 </volume> <pages> 403-409, </pages> <year> 1990. </year>
Reference-contexts: One solution is to reduce the estimate, that is, to use fewer kernels but to place them at optimal locations. One can also introduce kernel dependent weights and smoothing parameters. Various reduction approaches were described in <ref> [26, 27, 31, 77, 78, 103, 92, 112] </ref>. In some cases the methods suggested have a close connection with radial basis function and mixture density estimation techniques.
Reference: [32] <author> I. Guyon. </author> <title> Applications of neural networks to character recognition. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 5(1-2):353-382, </volume> <year> 1991. </year>
Reference-contexts: This is mainly due to the fact that while recognition of machine printed characters is considered a solved problem, the reliability achieved with handwritten text has not yet reached the level required in practical applications. Handwritten character recognition has become a popular application area for neural network classifiers <ref> [28, 32, 63] </ref>, which through their adaptive capabilities have often been able to achieve better reliability than classical statistical or knowledge-based structural recognition methods. Recently, many benchmark and comparison studies have been published on neural and statistical classifiers [81, 9, 49, 5].
Reference: [33] <author> D. </author> <title> Hand. Kernel Discriminant Analysis. </title> <publisher> Research Studies Press, </publisher> <address> Cichester, </address> <year> 1982. </year>
Reference-contexts: Kernel or Parzen estimates as well as k-nearest neighbor methods (at least when k is large) are examples of popular nonparametric density estimation methods [90, 86, 108]. They give rise to such classification methods as Kernel Discriminant Analysis (KDA) <ref> [18, 33, 89] </ref> and k-Nearest Neighbor (k-NN) rules [18, 14]. The Probabilistic Neural Network (PNN) [95] is the neural network counterpart of KDA.
Reference: [34] <author> L. K. Hansen and P. Salamon. </author> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(10) </volume> <pages> 993-1001, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: A more general case is variance reduction in continuous function estimation. A set of MLPs can be combined into a committee classifier with reduced output variance and thus smaller classification error <ref> [76, 34, 57, 111] </ref>. A separate confidence function may also be incorporated in each of the MLPs [91].
Reference: [35] <author> J. Hartigan. </author> <title> Clustering Algorithms. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: In this article we concentrate on classification only. Further, only supervised classification using statistical and neural network methods is considered thus leaving out both unsupervised classification (clustering <ref> [35] </ref>) and syntactic pattern recognition [24]. In statistics literature supervised classification is often called discriminant analysis [68]. A popular application of pattern recognition is o*ine character recognition, where research during the last two decades has focused on handwritten characters [99, 65, 1 50, 71, 101, 100, 96, 30, 107, 1].
Reference: [36] <author> T. Hastie and C. Loader. </author> <title> Local regression: Automatic kernel carpentry. </title> <journal> Statistical Science, </journal> <volume> 8 </volume> <pages> 120-143, </pages> <year> 1993. </year> <title> with discussion. </title>
Reference-contexts: Such estimators have received more attention recently, see e.g. <ref> [36] </ref>. The particular version described below is also called 13 LOESS [11, 12]. Local Linear Regression models the regression function in the neighborhood of each point x by means of a linear function z 7! b 0 + B (z x).
Reference: [37] <author> T. Hastie and R. Tibshirani. </author> <title> Generalized Additive Models. </title> <publisher> Chapman & Hall, </publisher> <year> 1990. </year>
Reference-contexts: Examples are linear and logistic regression, as well as a Multi-Layer Perceptron (MLP) [40, 3] with a fixed architecture, i.e., fixed layer sizes. In nonparametric regression no fixed functional form is assumed. Examples of nonparametric methodologies are Projection Pursuit [23, 19], additive models <ref> [37] </ref>, Multivariate Adaptive Regression Splines (MARS) [22], the Nadaraya-Watson kernel regression estimator [108, 56] and Local Linear Regression (LLR) [11, 12, 108]. The Nadaraya-Watson kernel regression estimator is also called the General Regression Neural Network [94].
Reference: [38] <author> T. Hastie and R. Tibshirani. </author> <title> Discriminant analysis by Gaussian mixtures. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1994. </year> <note> available as 94.6.ps in http://netlib.att.com/netlib/att/stat/doc/. </note>
Reference-contexts: The Probabilistic Neural Network (PNN) [95] is the neural network counterpart of KDA. In another approach the densities are estimated as finite mixtures of some standard probability densities by using the expectation-maximization (EM) algorithm or some other method <ref> [79, 103, 77, 78, 38] </ref>. Such an approach can be viewed as an economized KDA or an instance of the RBF approach [3].
Reference: [39] <author> T. Hastie, R. Tibshirani, and A. Buja. </author> <title> Flexible discriminant analysis by optimal scoring. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 89 </volume> <pages> 1255-1270, </pages> <year> 1994. </year>
Reference-contexts: The original MARS algorithm fits only scalar valued functions and it is therefore not well-suited to discrimination tasks with more than two classes. A recent proposal called Flexible Discriminant Analysis (FDA) <ref> [39] </ref> with its publicly available S-Plus implementation in the StatLib program library contains vector valued MARS as one of its ingredients. However, FDA is not limited to just MARS as it allows the use of other regression techniques as its building blocks as well.
Reference: [40] <author> S. Haykin. </author> <title> Neural Networks: A Comprehensive Foundation. </title> <publisher> Macmillan College Publishing Company, Inc., </publisher> <address> New York, </address> <year> 1994. </year> <month> 31 </month>
Reference-contexts: In parametric regression one models the posterior probabilities using a family of functions described by a finite number of parameters. Examples are linear and logistic regression, as well as a Multi-Layer Perceptron (MLP) <ref> [40, 3] </ref> with a fixed architecture, i.e., fixed layer sizes. In nonparametric regression no fixed functional form is assumed. <p> The Nadaraya-Watson kernel regression estimator is also called the General Regression Neural Network [94]. Other neural network approaches include Multi-Layer Perceptrons with a flexible architecture and Radial Basis Function expansions <ref> [40, 3] </ref>. 5 One way to fit a regression model is to use binary least squares regression. In a two class case we define the response y i = 1 or 0 depending on whether the ith training vector x i comes from class 1 or 2. <p> from the point of view of integrated squared error and not discrimination performance which is the true focus of interest. 3.4 MLP We used a standard Multi-Layer Perceptron with d inputs, ` hidden units and c output units and with the logistic activation function in the hidden and output units <ref> [40, 3] </ref>. Such a network has (d + 1)` + (` + 1)c adaptable weights which in our experiments were determined by minimizing the sum of squared errors criterion using a conjugate gradient method [88]. <p> It seems difficult to select a network architecture which is flexible enough for the task and at the same time is parameterized parsimoniously enough so that the parameters can be reliably determined from the training data. Some kind of regularization such as weight decay <ref> [40, 3] </ref> might help in reducing the overfitting. Excluding the committee classifier, the best results were obtained with Local Linear Regression (LLR). Unfortunately, this method is impractically slow since the weighted linear least squares problem has to be solved separately at each pattern to be classified.
Reference: [41] <author> W. Highleyman. </author> <title> Linear decision functions with application to pattern recogni-tion. </title> <journal> Proc. IRE, </journal> <volume> 50 </volume> <pages> 1501-1514, </pages> <year> 1962. </year>
Reference-contexts: This provides for example one viable way of selecting the smoothing parameters in a classifier based on kernel regression. While such techniques have been proposed also in parametric regression settings <ref> [41] </ref> they are seldom used in practice.
Reference: [42] <author> G. E. Hinton, M. Revow, and P. Dayan. </author> <title> Recognizing handwritten digits using mixtures of linear models. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Neural Information Processing Systems 7, </booktitle> <pages> pages 1015-1022, </pages> <address> Cambridge, MA, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Given a fixed feature extraction method one can either use a common training set to design a number of different types of classifiers [48] or alternatively use different training sets to design several versions of one type of classifier <ref> [16, 15, 42, 85, 93] </ref>. 19 5 Case Study: Classification of Handwritten Dig- its 5.1 Data and its Preprocessing We tested the performance of the classifiers defined previously on a realistic pattern recognition problem, the classification of handwritten digits. To obtain the test material, we used a set of forms.
Reference: [43] <author> T. K. Ho, J. J. Hull, and S. N. Srihari. </author> <title> A regression approach to combination of decisions by multiple character recognition algorithms. </title> <editor> In D. P. D'Amato, W.-E. Blanz, B. E. Dom, and S. N. Srihari, editors, </editor> <booktitle> Proceedings of SPIE Conference on Machine Vision Applications in Character Recognition and Industrial Inspection, number 1661 in SPIE, </booktitle> <pages> pages 137-145. SPIE, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: The classifier may also propose a set of classes in the order of decreasing certainty or a measure of decision certainty may be associated with all possible classes. Various ways to combine classifiers with such different types of output information are analyzed in <ref> [116, 43, 44, 47] </ref>. The simplest decision rule is to use a majority rule among the classifiers in the committee, possibly ignoring the opinion of some of the classifiers [115]. <p> Two or more classifiers using different sets of features may be combined to implement the rejection of ambiguous patterns [72, 53, 101, 59]. A genetic algorithm can search for optimal weights to combine the classifier outputs [60]. Theoretically more advanced methods may be derived from the EM-algorithm <ref> [43, 44, 113, 114, 52] </ref> or from the Dempster-Shafer theory of evidence [116, 20]. The outputs of several classifiers may be combined linearly [51] or nonlinearly [104] to reduce the variance of the posterior probability estimates. A more general case is variance reduction in continuous function estimation.
Reference: [44] <author> T. K. Ho, J. J. Hull, and S. N. Srihari. </author> <title> Decision combination in multiple classifier systems. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(1) </volume> <pages> 66-75, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: The classifier may also propose a set of classes in the order of decreasing certainty or a measure of decision certainty may be associated with all possible classes. Various ways to combine classifiers with such different types of output information are analyzed in <ref> [116, 43, 44, 47] </ref>. The simplest decision rule is to use a majority rule among the classifiers in the committee, possibly ignoring the opinion of some of the classifiers [115]. <p> Two or more classifiers using different sets of features may be combined to implement the rejection of ambiguous patterns [72, 53, 101, 59]. A genetic algorithm can search for optimal weights to combine the classifier outputs [60]. Theoretically more advanced methods may be derived from the EM-algorithm <ref> [43, 44, 113, 114, 52] </ref> or from the Dempster-Shafer theory of evidence [116, 20]. The outputs of several classifiers may be combined linearly [51] or nonlinearly [104] to reduce the variance of the posterior probability estimates. A more general case is variance reduction in continuous function estimation.
Reference: [45] <author> L. Holmstrom and A. Hamalainen. </author> <title> The self-organizing reduced kernel density estimator. </title> <booktitle> In Proceedings of the 1993 IEEE International Conference on Neural Networks, </booktitle> <address> San Francisco, California, March 28 - April 1, </address> <booktitle> volume 1, </booktitle> <pages> pages 417-421, </pages> <year> 1993. </year>
Reference-contexts: Such an approach can be viewed as an economized KDA or an instance of the RBF approach [3]. The Self-organizing Reduced Kernel Density Estimator introduced in <ref> [45] </ref> estimates densities in the spirit of radial basis functions and we refer to the corresponding classification method as Reduced Kernel Discriminant Analysis (RKDA). It should be pointed out that good density estimates are not always necessary for the design of an efficient classifier. <p> One can also introduce kernel dependent weights and smoothing parameters. Various reduction approaches were described in [26, 27, 31, 77, 78, 103, 92, 112]. In some cases the methods suggested have a close connection with radial basis function and mixture density estimation techniques. The Self-organizing Reduced Kernel Density Estimate <ref> [45] </ref> has the form ^ f (x) = k=1 12 where m 1 ; : : : ; m ` are the reference vectors of a Self-Organizing Map [55], w 1 ; : : : ; w ` are nonnegative weights with P ` k=1 w k = 1, and h
Reference: [46] <author> L. Holmstrom, S. Sain, and H. Miettinen. </author> <title> A new multivariate technique for top quark search. </title> <journal> Computer Physics Communications, </journal> <volume> 88 </volume> <pages> 195-210, </pages> <year> 1995. </year>
Reference-contexts: We elaborate upon these two approaches further in Sections 2.1 and 2.2 below. 3 Other criteria than minimum classification error can be important in practice. Sometimes a class-dependent misclassification cost is needed. In certain types of applications it is essential to use Neyman-Pearson style classification <ref> [117, 46] </ref> where the classification error of one class is minimized given a fixed error for another class. The use of a reject class can help reduce the misclassification rate in tasks where exceptional handling (e.g. by a human expert) of particularly ambiguous cases is feasible.
Reference: [47] <author> Y. S. Huang, K. Liu, and C. Y. Suen. </author> <title> The combination of multiple classifiers by a neural network approach. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 9(3) </volume> <pages> 579-597, </pages> <year> 1995. </year>
Reference-contexts: The classifier may also propose a set of classes in the order of decreasing certainty or a measure of decision certainty may be associated with all possible classes. Various ways to combine classifiers with such different types of output information are analyzed in <ref> [116, 43, 44, 47] </ref>. The simplest decision rule is to use a majority rule among the classifiers in the committee, possibly ignoring the opinion of some of the classifiers [115].
Reference: [48] <author> Y. Idan and J.-M. Auger. </author> <title> Pattern recognition by cooperating neural networks. </title> <editor> In S.-S. Chen, editor, </editor> <booktitle> Proceedings of SPIE Conference on Neural and Stochastic Methods in Image and Signal Processing, number 1766 in SPIE, </booktitle> <pages> pages 437-443. SPIE, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: A separate confidence function may also be incorporated in each of the MLPs [91]. Given a fixed feature extraction method one can either use a common training set to design a number of different types of classifiers <ref> [48] </ref> or alternatively use different training sets to design several versions of one type of classifier [16, 15, 42, 85, 93]. 19 5 Case Study: Classification of Handwritten Dig- its 5.1 Data and its Preprocessing We tested the performance of the classifiers defined previously on a realistic pattern recognition problem, the
Reference: [49] <author> Y. Idan, J.-M. Auger, N. Darbel, M. Sales, R. Chevallier, B. Dorizzi, and G. Cazuguel. </author> <title> Comparative study of neural networks and non parametric statistical methods for off-line handwritten character recognition. </title> <editor> In I. Aleksander and J. Taylor, editors, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 1607-1610, </pages> <address> Brighton, </address> <month> September </month> <year> 1992. </year> <title> ENNS, </title> <publisher> North-Holland. </publisher>
Reference-contexts: Recently, many benchmark and comparison studies have been published on neural and statistical classifiers <ref> [81, 9, 49, 5] </ref>. One of the most extensive was the Statlog project in which statistical methods, machine learning and neural networks were compared [69].
Reference: [50] <author> S. Impedovo, L. Ottaviano, and S. Occhinegro. </author> <title> Optical character recognition-a survey. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 5(1-2):1-24, </volume> <year> 1991. </year>
Reference: [51] <author> R. A. Jacobs. </author> <title> Methods for combining experts' probability assessments. </title> <journal> Neural Computation, </journal> <volume> 7(5) </volume> <pages> 867-888, </pages> <month> September </month> <year> 1995. </year> <month> 32 </month>
Reference-contexts: A genetic algorithm can search for optimal weights to combine the classifier outputs [60]. Theoretically more advanced methods may be derived from the EM-algorithm [43, 44, 113, 114, 52] or from the Dempster-Shafer theory of evidence [116, 20]. The outputs of several classifiers may be combined linearly <ref> [51] </ref> or nonlinearly [104] to reduce the variance of the posterior probability estimates. A more general case is variance reduction in continuous function estimation. A set of MLPs can be combined into a committee classifier with reduced output variance and thus smaller classification error [76, 34, 57, 111].
Reference: [52] <author> M. I. Jordan and R. A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 181-214, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Two or more classifiers using different sets of features may be combined to implement the rejection of ambiguous patterns [72, 53, 101, 59]. A genetic algorithm can search for optimal weights to combine the classifier outputs [60]. Theoretically more advanced methods may be derived from the EM-algorithm <ref> [43, 44, 113, 114, 52] </ref> or from the Dempster-Shafer theory of evidence [116, 20]. The outputs of several classifiers may be combined linearly [51] or nonlinearly [104] to reduce the variance of the posterior probability estimates. A more general case is variance reduction in continuous function estimation.
Reference: [53] <author> F. Kimura and M. Shridhar. </author> <title> Handwritten numerical recognition based on multiple algorithms. </title> <journal> Pattern Recognition, </journal> <volume> 24(10) </volume> <pages> 969-983, </pages> <year> 1991. </year>
Reference-contexts: The simplest decision rule is to use a majority rule among the classifiers in the committee, possibly ignoring the opinion of some of the classifiers [115]. Two or more classifiers using different sets of features may be combined to implement the rejection of ambiguous patterns <ref> [72, 53, 101, 59] </ref>. A genetic algorithm can search for optimal weights to combine the classifier outputs [60]. Theoretically more advanced methods may be derived from the EM-algorithm [43, 44, 113, 114, 52] or from the Dempster-Shafer theory of evidence [116, 20].
Reference: [54] <author> T. Kohonen. </author> <title> The `neural' phonetic typewriter. </title> <journal> Computer, </journal> <volume> 21(3) </volume> <pages> 11-22, </pages> <year> 1988. </year>
Reference-contexts: The goal remains, however, of building much more complex artificial neural systems for demanding tasks like speech recognition <ref> [54] </ref> or computer vision [61], in which it is difficult or eventually impossible to state the exact optimization criteria for all the consequent processing stages. Section 3.
Reference: [55] <author> T. Kohonen. </author> <title> Self-Organizing Maps. </title> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Examples are Learning Vector Quantization (LVQ) <ref> [55] </ref> and the k-Nearest Neighbor classifiers with a small k. A distinct family of discrimination methods are the sub-space classifiers [73] that model the individual classes using linear subspaces and classify a pattern according to its shortest distance to the model subspaces. <p> The Self-organizing Reduced Kernel Density Estimate [45] has the form ^ f (x) = k=1 12 where m 1 ; : : : ; m ` are the reference vectors of a Self-Organizing Map <ref> [55] </ref>, w 1 ; : : : ; w ` are nonnegative weights with P ` k=1 w k = 1, and h k is a smoothing parameter associated with the kth kernel. In order to achieve substantial reduction one takes ` t n. <p> In such a case the Learning k-NN rule is better able to utilize the available data by using the whole size n training set to optimize the smaller set of ` &lt; n prototype vectors. 3.8 Variations of LVQ The Learning Vector Quantization (LVQ) algorithm <ref> [55] </ref> produces a set of prototype or codebook pattern vectors that can be used in a 1-NN classifier. Training consists of moving a fixed number ` of codebook vectors iteratively toward or away from the training samples x i . <p> The size of the codebook ` was selected with cross-validation and the codebook was initialized using `=10 pattern vectors from each class. The classifier was trained using 10 epochs of LVQ1 and a varying number of LVQ2 epochs (for definitions of LVQ variants, see <ref> [55] </ref>). During each epoch the number of training vectors presented to the classifier was equal to the number of vectors in the actual training set and the learning coefficient ff was linearly decreased from 0.2 to zero. LVQ2 used relative window width of 0.5.
Reference: [56] <author> P. Koistinen and L. Holmstrom. </author> <title> Kernel regression and backpropagation training with noise. </title> <editor> In J. Moody, S. Hanson, and R. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 1033-1039, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: In nonparametric regression no fixed functional form is assumed. Examples of nonparametric methodologies are Projection Pursuit [23, 19], additive models [37], Multivariate Adaptive Regression Splines (MARS) [22], the Nadaraya-Watson kernel regression estimator <ref> [108, 56] </ref> and Local Linear Regression (LLR) [11, 12, 108]. The Nadaraya-Watson kernel regression estimator is also called the General Regression Neural Network [94].
Reference: [57] <author> A. Krogh and J. Vedelsby. </author> <title> Neural network ensembles, cross validation, and active learning. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Neural Information Processing Systems 7, </booktitle> <pages> pages 231-238, </pages> <address> Cambridge, MA, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: A more general case is variance reduction in continuous function estimation. A set of MLPs can be combined into a committee classifier with reduced output variance and thus smaller classification error <ref> [76, 34, 57, 111] </ref>. A separate confidence function may also be incorporated in each of the MLPs [91].
Reference: [58] <author> J. Laaksonen and E. Oja. </author> <title> Classification with learning k-nearest neighbors. </title> <booktitle> In Proceedings of the International Conference of Neural Networks, </booktitle> <year> 1996. </year> <note> to be published. </note>
Reference-contexts: A possible tie of two or more classes is broken by decreasing k by one and re-voting. Recently, two of the authors have introduced a set of adaptation rules that can be used in iterative training of a k-NN classifier <ref> [58] </ref>. The learning rules of the proposed Learning k-NN resemble those of LVQ but at the same time the classifier utilizes the improved classification accuracy provided by majority voting.
Reference: [59] <author> L. Lam and C. Y. Suen. </author> <title> A theoretical analysis of the application of majority voting to pattern recognition. </title> <booktitle> In Proceedings of 12th International Conference on Pattern Recognition, </booktitle> <volume> volume II, </volume> <pages> pages 418-420. </pages> <address> IAPR, </address> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: The simplest decision rule is to use a majority rule among the classifiers in the committee, possibly ignoring the opinion of some of the classifiers [115]. Two or more classifiers using different sets of features may be combined to implement the rejection of ambiguous patterns <ref> [72, 53, 101, 59] </ref>. A genetic algorithm can search for optimal weights to combine the classifier outputs [60]. Theoretically more advanced methods may be derived from the EM-algorithm [43, 44, 113, 114, 52] or from the Dempster-Shafer theory of evidence [116, 20].
Reference: [60] <author> L. Lam and C. Y. Suen. </author> <title> Optimal combinations of pattern classifiers. </title> <journal> Pattern Recognition Letters, </journal> <volume> 16 </volume> <pages> 945-954, </pages> <year> 1995. </year>
Reference-contexts: Two or more classifiers using different sets of features may be combined to implement the rejection of ambiguous patterns [72, 53, 101, 59]. A genetic algorithm can search for optimal weights to combine the classifier outputs <ref> [60] </ref>. Theoretically more advanced methods may be derived from the EM-algorithm [43, 44, 113, 114, 52] or from the Dempster-Shafer theory of evidence [116, 20]. The outputs of several classifiers may be combined linearly [51] or nonlinearly [104] to reduce the variance of the posterior probability estimates.
Reference: [61] <author> J. Lampinen and E. Oja. </author> <title> Distortion tolerant pattern recognition based on self-organizing feature extraction. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 6(3) </volume> <pages> 539-547, </pages> <year> 1995. </year>
Reference-contexts: The goal remains, however, of building much more complex artificial neural systems for demanding tasks like speech recognition [54] or computer vision <ref> [61] </ref>, in which it is difficult or eventually impossible to state the exact optimization criteria for all the consequent processing stages. Section 3. The horizontal axis measures the flexibility of a classifier architecture in the sense of the richness of the discriminant function family encompassed by a particular method.
Reference: [62] <author> Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. </author> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 541-551, </pages> <year> 1989. </year>
Reference-contexts: Perhaps the best example of a clean-cut neural network classifier is the LeNet system <ref> [62, 5] </ref> for handwritten digit recognition. Such a computational model supports well the implementation in regular VLSI circuits.
Reference: [63] <author> D.-S. Lee, S. N. Srihari, and R. Gaborski. </author> <title> Bayesian and neural network pattern recognition: a theoretical connection and empirical results with handwritten characters. </title> <editor> In I. K. Sethi and A. K. Jain, editors, </editor> <booktitle> Artificial Neural Networks and Statistical Pattern Recognition, </booktitle> <pages> pages 89-108. </pages> <publisher> Elsevier Science Publishers, </publisher> <year> 1991. </year>
Reference-contexts: This is mainly due to the fact that while recognition of machine printed characters is considered a solved problem, the reliability achieved with handwritten text has not yet reached the level required in practical applications. Handwritten character recognition has become a popular application area for neural network classifiers <ref> [28, 32, 63] </ref>, which through their adaptive capabilities have often been able to achieve better reliability than classical statistical or knowledge-based structural recognition methods. Recently, many benchmark and comparison studies have been published on neural and statistical classifiers [81, 9, 49, 5].
Reference: [64] <author> J. MacQueen. </author> <title> Some methods for classification and analysis of multivariate observations. </title> <editor> In L. M. LeCam and J. Neyman, editors, </editor> <booktitle> Proc. Fifth Berkeley Symp. Math. Stat. and Prob., </booktitle> <pages> pages 281-297. </pages> <address> Berkeley, CA: </address> <institution> U.C. Berkeley Press, </institution> <year> 1967. </year>
Reference-contexts: Simulations have shown that when the underlying density f is multimodal, the use of the feature map algorithm gives better density estimates than k-means clustering, the approach proposed in <ref> [64] </ref>. Reduced Kernel Discriminant Analysis (RKDA) constitutes using estimates (19) for the class-conditional densities in the classifier (1).
Reference: [65] <author> J. Mantas. </author> <title> An overview of character recognition methodologies. </title> <journal> Pattern Recog 33 nition, </journal> <volume> 19(6) </volume> <pages> 425-430, </pages> <year> 1986. </year>
Reference-contexts: In statistics literature supervised classification is often called discriminant analysis [68]. A popular application of pattern recognition is o*ine character recognition, where research during the last two decades has focused on handwritten characters <ref> [99, 65, 1 50, 71, 101, 100, 96, 30, 107, 1] </ref>. This is mainly due to the fact that while recognition of machine printed characters is considered a solved problem, the reliability achieved with handwritten text has not yet reached the level required in practical applications.
Reference: [66] <author> P. McCullagh and J. Nelder. </author> <title> Generalized Linear Models. </title> <publisher> Chapman & Hall, </publisher> <address> second edition, </address> <year> 1989. </year>
Reference-contexts: When r (x; t) is linear in its parameters, one can use the methodology developed for generalized linear models <ref> [66] </ref> to estimate the parameter vector. Note that the estimated posterior probability then takes values in the interval [0; 1] as a probability should.
Reference: [67] <author> W. S. McCulloch and W. Pitts. </author> <title> A logical calculus of the idea immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5 </volume> <pages> 115-133, </pages> <year> 1943. </year>
Reference-contexts: However, this abstract point of view fails to identify some key features of neural networks that characterize them as a distinct methodology. From the very beginning of neural network research <ref> [67, 82, 83] </ref> the goal was to demonstrate problem-solving without explicit programming. The neurons and networks were supposed to learn from examples and store this knowledge in a distributed way among the connection weights.
Reference: [68] <author> G. McLachlan. </author> <title> Discriminant Analysis and Statistical Pattern Recognition. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1992. </year>
Reference-contexts: In a narrow sense, the pattern recognition problem can be parceled to data acquisition and preprocessing, feature extraction, and classification. In addition to general references such as <ref> [17, 117, 14, 102, 25, 68, 84, 24] </ref> questions of pattern recognition are extensively covered in the literature of many specific application areas such as speech and image recognition. The performance of statistical and neural network methods is usually discussed in connection with feature extraction and classification. <p> In this article we concentrate on classification only. Further, only supervised classification using statistical and neural network methods is considered thus leaving out both unsupervised classification (clustering [35]) and syntactic pattern recognition [24]. In statistics literature supervised classification is often called discriminant analysis <ref> [68] </ref>. A popular application of pattern recognition is o*ine character recognition, where research during the last two decades has focused on handwritten characters [99, 65, 1 50, 71, 101, 100, 96, 30, 107, 1]. <p> In case of equal class covariance matrices a linear function results and the term Linear Discriminant Analysis (LDA) is used. A recent development is Regularized Discriminant Analysis (RDA) [21] which interpolates between LDA and QDA. A thorough account of these and other classifiers is provided by McLachlan <ref> [68] </ref>. Modeling the class-conditional densities as multivariate normals is an example of parametric density estimation, where the densities are assumed to belong to a 4 family of functions described by a finite set of parameters. In nonparametric density estimation no such fixed family of possible densities is assumed. <p> While the least squares fitting criterion can be motivated by the above reasoning, perhaps a more natural approach is to use logistic regression methodology <ref> [68, Ch. 8] </ref> and model the posterior probability directly as P (J = 1 j X = x) = 1 + exp (r (x; t)) One can then estimate the unknown parameter t by maximizing the conditional log-likelihood of y 1 ; : : : ; y n given that X <p> Several approaches have been proposed in the literature, see <ref> [90, 86, 68, 108] </ref>. We used two methods based on cross-validated error count. In the first method, KDA1, we restrict all the smoothing parameters h j to be equal to a parameter h, and evaluate the number of misclassified vectors using cross-validation (see Section 4.1).
Reference: [69] <editor> D. Michie, D. J. Spiegelhalter, and C. C. Taylor, editors. </editor> <title> Machine learning, neural and statistical classification. </title> <publisher> Ellis Horwood Limited, </publisher> <year> 1994. </year>
Reference-contexts: Recently, many benchmark and comparison studies have been published on neural and statistical classifiers [81, 9, 49, 5]. One of the most extensive was the Statlog project in which statistical methods, machine learning and neural networks were compared <ref> [69] </ref>. As a general conclusion of that study, good statistical classifiers included the k-Nearest Neighbor (k-NN) rule, and good neural network classifiers included Learning Vector Quantization (LVQ) and Radial Basis Function (RBF) classifiers. One of the databases used in that study consisted of handwritten digits.
Reference: [70] <author> J. Morgan and J. Sonquist. </author> <title> Problems in the analysis of survey data and a proposal. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58 </volume> <pages> 415-434, </pages> <year> 1963. </year>
Reference-contexts: hence the same classifier as the KDA provided the bandwidths h are the same and that the local constant regression estimator uses the kernel K of the KDA classifier as its weight function. 3.6 Tree classifier, MARS and FDA The introduction of tree based models in statistics dates back to <ref> [70] </ref> although their current popularity is largely due to the seminal book [6].
Reference: [71] <author> S. Mori, C. Y. Suen, and K. Yamamoto. </author> <title> Historical review of OCR research and development. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1029-1058, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: In statistics literature supervised classification is often called discriminant analysis [68]. A popular application of pattern recognition is o*ine character recognition, where research during the last two decades has focused on handwritten characters <ref> [99, 65, 1 50, 71, 101, 100, 96, 30, 107, 1] </ref>. This is mainly due to the fact that while recognition of machine printed characters is considered a solved problem, the reliability achieved with handwritten text has not yet reached the level required in practical applications.
Reference: [72] <author> C. Nadal, R. Legault, and C. Y. Suen. </author> <title> Complementary algorithms for the recognition of totally unconstrained handwritten numerals. </title> <booktitle> In Proceedings of the 10th International Conference on Pattern Recognition, </booktitle> <pages> pages 443-449, </pages> <address> At-lantic City, NJ, </address> <month> June </month> <year> 1990. </year> <month> IAPR. </month>
Reference-contexts: The simplest decision rule is to use a majority rule among the classifiers in the committee, possibly ignoring the opinion of some of the classifiers [115]. Two or more classifiers using different sets of features may be combined to implement the rejection of ambiguous patterns <ref> [72, 53, 101, 59] </ref>. A genetic algorithm can search for optimal weights to combine the classifier outputs [60]. Theoretically more advanced methods may be derived from the EM-algorithm [43, 44, 113, 114, 52] or from the Dempster-Shafer theory of evidence [116, 20].
Reference: [73] <author> E. Oja. </author> <title> Subspace Methods of Pattern Recognition. </title> <publisher> Research Studies Press, </publisher> <address> Letchworth, </address> <year> 1983. </year>
Reference-contexts: Examples are Learning Vector Quantization (LVQ) [55] and the k-Nearest Neighbor classifiers with a small k. A distinct family of discrimination methods are the sub-space classifiers <ref> [73] </ref> that model the individual classes using linear subspaces and classify a pattern according to its shortest distance to the model subspaces. Examples include the classical CLAFIC (CLAss-Featuring Information Compression) method [109] and the Averaged Learning Subspace Method (ALSM) that features incremental learning [73]. 2.4 What are neural classifiers? In the <p> of discrimination methods are the sub-space classifiers <ref> [73] </ref> that model the individual classes using linear subspaces and classify a pattern according to its shortest distance to the model subspaces. Examples include the classical CLAFIC (CLAss-Featuring Information Compression) method [109] and the Averaged Learning Subspace Method (ALSM) that features incremental learning [73]. 2.4 What are neural classifiers? In the previous discussion we characterized some popular classification techniques in terms of the mathematical principles they are based on. In this general view many neural networks can be seen as representatives of certain larger families of statistical 7 techniques. <p> The classification rule of the CLAFIC [109] algorithm can then be expressed as g CLAFIC (x) = argmax j=1;::: ;c j xk 2 :(25) The Averaged Learning Subspace Method (ALSM) introduced by one of the authors <ref> [73] </ref> is an iterative learning version of CLAFIC in which the unnormalized sample class correlation matrices b S j = P n j ij are slightly modified according to the correctness of the classifications, b S j (k + 1) = b S j (k) + ff i2A j i fi
Reference: [74] <author> E. Oja. </author> <title> Self-organizing maps and computer vision. </title> <editor> In H. Wechsler, editor, </editor> <title> Neural Networks for Perception, volume I, </title> <booktitle> chapter II.9, </booktitle> <pages> pages 368-385. </pages> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Realistic systems for computer vision, even for intrinsically two-dimensional problems, are hybrids of many methodologies like signal processing, classification, and relational matching. It seems that neural networks can be used to an advantage in certain subproblems, especially in feature extraction and classification <ref> [74] </ref>. These are also problems amenable to statistical techniques, because the data representations are real vectors of measurements or feature values, and it is possible to collect training samples on which regression analysis or density estimation become feasible.
Reference: [75] <author> M. P. Perrone. </author> <title> Pulling it all together: Methods for combining neural networks. </title> <editor> In J. D.Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Neural Information Processing Systems 6, </booktitle> <pages> pages 1188-1189, </pages> <address> Cambridge, MA, 1994. </address> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: It is then quite possible that combining the opinions of several parallel methods results in improved classification performance. Such hybrid classifiers, classifier ensembles, or committees, have been studied intensively in recent years <ref> [75] </ref>. Besides improved classification performance, there are other reasons to use a committee classifier. The pattern vectors may be composed of components that originate from very diverse domains.
Reference: [76] <author> M. P. Perrone and L. N. Cooper. </author> <title> When networks disagree: Ensemble methods for hybrid neural networks. </title> <editor> In R. J. Mammone, editor, </editor> <booktitle> Artificial Neural Networks for Speech and Vision, </booktitle> <pages> pages 126-142. </pages> <publisher> Chapman & Hall, </publisher> <year> 1993. </year>
Reference-contexts: A more general case is variance reduction in continuous function estimation. A set of MLPs can be combined into a committee classifier with reduced output variance and thus smaller classification error <ref> [76, 34, 57, 111] </ref>. A separate confidence function may also be incorporated in each of the MLPs [91].
Reference: [77] <author> C. E. Priebe and D. J. Marchette. </author> <title> Adaptive mixtures: Recursive nonparametric pattern recognition. </title> <journal> Pattern Recognition, </journal> <volume> 24(12) </volume> <pages> 1197-1209, </pages> <year> 1991. </year>
Reference-contexts: The Probabilistic Neural Network (PNN) [95] is the neural network counterpart of KDA. In another approach the densities are estimated as finite mixtures of some standard probability densities by using the expectation-maximization (EM) algorithm or some other method <ref> [79, 103, 77, 78, 38] </ref>. Such an approach can be viewed as an economized KDA or an instance of the RBF approach [3]. <p> One solution is to reduce the estimate, that is, to use fewer kernels but to place them at optimal locations. One can also introduce kernel dependent weights and smoothing parameters. Various reduction approaches were described in <ref> [26, 27, 31, 77, 78, 103, 92, 112] </ref>. In some cases the methods suggested have a close connection with radial basis function and mixture density estimation techniques.
Reference: [78] <author> C. E. Priebe and D. J. Marchette. </author> <title> Adaptive mixture density estimation. </title> <journal> Pattern Recognition, </journal> <volume> 26(5) </volume> <pages> 771-785, </pages> <year> 1993. </year>
Reference-contexts: The Probabilistic Neural Network (PNN) [95] is the neural network counterpart of KDA. In another approach the densities are estimated as finite mixtures of some standard probability densities by using the expectation-maximization (EM) algorithm or some other method <ref> [79, 103, 77, 78, 38] </ref>. Such an approach can be viewed as an economized KDA or an instance of the RBF approach [3]. <p> One solution is to reduce the estimate, that is, to use fewer kernels but to place them at optimal locations. One can also introduce kernel dependent weights and smoothing parameters. Various reduction approaches were described in <ref> [26, 27, 31, 77, 78, 103, 92, 112] </ref>. In some cases the methods suggested have a close connection with radial basis function and mixture density estimation techniques.
Reference: [79] <author> R. Redner and H. Walker. </author> <title> Mixture densities, maximum likelihood and the EM algorithm. </title> <journal> SIAM Review, </journal> <volume> 26(2), </volume> <month> April </month> <year> 1984. </year>
Reference-contexts: The Probabilistic Neural Network (PNN) [95] is the neural network counterpart of KDA. In another approach the densities are estimated as finite mixtures of some standard probability densities by using the expectation-maximization (EM) algorithm or some other method <ref> [79, 103, 77, 78, 38] </ref>. Such an approach can be viewed as an economized KDA or an instance of the RBF approach [3].
Reference: [80] <author> M. D. Richard and R. P. Lippman. </author> <title> Neural network classifiers estimate bayesian a posteriori probabilities. </title> <journal> Neural Computation, </journal> <volume> 3(4) </volume> <pages> 461-483, </pages> <year> 1991. </year>
Reference-contexts: However, the conditional expectation m may or may not belong to the family R, and besides, sampling variation will anyhow prevent us from estimating m exactly even when it does belong to R <ref> [110, 80] </ref>.
Reference: [81] <author> B. Ripley. </author> <title> Neural networks and related methods for classification. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 56(3) </volume> <pages> 409-456, </pages> <year> 1994. </year> <title> with discussion. </title> <type> 34 </type>
Reference-contexts: Recently, many benchmark and comparison studies have been published on neural and statistical classifiers <ref> [81, 9, 49, 5] </ref>. One of the most extensive was the Statlog project in which statistical methods, machine learning and neural networks were compared [69].
Reference: [82] <author> F. Rosenblatt. </author> <title> The perceptron: A probabilistic model for information storage and organization in brain. </title> <journal> Psychological Review, </journal> <volume> 65 </volume> <pages> 386-408, </pages> <year> 1958. </year>
Reference-contexts: However, this abstract point of view fails to identify some key features of neural networks that characterize them as a distinct methodology. From the very beginning of neural network research <ref> [67, 82, 83] </ref> the goal was to demonstrate problem-solving without explicit programming. The neurons and networks were supposed to learn from examples and store this knowledge in a distributed way among the connection weights.
Reference: [83] <author> F. Rosenblatt. </author> <title> Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. </title> <publisher> Spartan Books, </publisher> <address> Washington, DC., </address> <year> 1961. </year>
Reference-contexts: However, this abstract point of view fails to identify some key features of neural networks that characterize them as a distinct methodology. From the very beginning of neural network research <ref> [67, 82, 83] </ref> the goal was to demonstrate problem-solving without explicit programming. The neurons and networks were supposed to learn from examples and store this knowledge in a distributed way among the connection weights.
Reference: [84] <author> R. J. Schalkoff. </author> <title> Pattern Recognition: Statistical, Structural and Neural Approaches. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1992. </year>
Reference-contexts: In a narrow sense, the pattern recognition problem can be parceled to data acquisition and preprocessing, feature extraction, and classification. In addition to general references such as <ref> [17, 117, 14, 102, 25, 68, 84, 24] </ref> questions of pattern recognition are extensively covered in the literature of many specific application areas such as speech and image recognition. The performance of statistical and neural network methods is usually discussed in connection with feature extraction and classification.
Reference: [85] <author> H. Schwenk and M. Milgram. </author> <title> Transformation invariant autoassociation with application to handwritten character recognition. </title> <editor> In G. Tesauro, D. S. Touret-zky, and T. K. Leen, editors, </editor> <booktitle> Neural Information Processing Systems 7, </booktitle> <pages> pages 991-998, </pages> <address> Cambridge, MA, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Given a fixed feature extraction method one can either use a common training set to design a number of different types of classifiers [48] or alternatively use different training sets to design several versions of one type of classifier <ref> [16, 15, 42, 85, 93] </ref>. 19 5 Case Study: Classification of Handwritten Dig- its 5.1 Data and its Preprocessing We tested the performance of the classifiers defined previously on a realistic pattern recognition problem, the classification of handwritten digits. To obtain the test material, we used a set of forms.
Reference: [86] <author> D. Scott. </author> <title> Multivariate Density Estimation: Theory, Practice, and Visualization. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1992. </year>
Reference-contexts: In nonparametric density estimation no such fixed family of possible densities is assumed. Kernel or Parzen estimates as well as k-nearest neighbor methods (at least when k is large) are examples of popular nonparametric density estimation methods <ref> [90, 86, 108] </ref>. They give rise to such classification methods as Kernel Discriminant Analysis (KDA) [18, 33, 89] and k-Nearest Neighbor (k-NN) rules [18, 14]. The Probabilistic Neural Network (PNN) [95] is the neural network counterpart of KDA. <p> Several approaches have been proposed in the literature, see <ref> [90, 86, 68, 108] </ref>. We used two methods based on cross-validated error count. In the first method, KDA1, we restrict all the smoothing parameters h j to be equal to a parameter h, and evaluate the number of misclassified vectors using cross-validation (see Section 4.1).
Reference: [87] <author> T. J. Sejnowski and C. R. Rosenberg. </author> <title> Parallel networks that learn to pronounce english text. </title> <journal> Journal of Complex Systems, </journal> <volume> 1(1) </volume> <pages> 145-168, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: In neural networks, the approach has been bottom-up: starting from a very simple linear neuron that computes a weighted sum of its inputs, adding a saturating smooth nonlinearity, and constructing layers of similar parallel units, it turned out that "intelligent" behavior like speech synthesis <ref> [87] </ref> emerged by simple learning rules. The computational aspect has always been central.
Reference: [88] <author> D. Shanno and K. Phua. </author> <title> Remark on algorithm 500. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 6(4) </volume> <pages> 618-622, </pages> <year> 1980. </year>
Reference-contexts: Such a network has (d + 1)` + (` + 1)c adaptable weights which in our experiments were determined by minimizing the sum of squared errors criterion using a conjugate gradient method <ref> [88] </ref>. Using the notation of Section 2.2, we used the vector 0:1+0:8y i as the desired output for input x i , i.e., the vectors y i were scaled to better fit within the range of the logistic function.
Reference: [89] <author> B. Silverman and M. Jones. E. Fix and J.L. </author> <title> Hodges (1951): An important contribution to nonparametric discriminant analysis and density estimation| commentary on Fix and Hodges (1951). </title> <journal> International Statistical Review, </journal> <volume> 57(3) </volume> <pages> 233-247, </pages> <year> 1989. </year>
Reference-contexts: Kernel or Parzen estimates as well as k-nearest neighbor methods (at least when k is large) are examples of popular nonparametric density estimation methods [90, 86, 108]. They give rise to such classification methods as Kernel Discriminant Analysis (KDA) <ref> [18, 33, 89] </ref> and k-Nearest Neighbor (k-NN) rules [18, 14]. The Probabilistic Neural Network (PNN) [95] is the neural network counterpart of KDA.
Reference: [90] <author> B. W. Silverman. </author> <title> Density Estimation for Statistics and Data Analysis. </title> <publisher> Chap-man and Hall, </publisher> <year> 1986. </year>
Reference-contexts: In nonparametric density estimation no such fixed family of possible densities is assumed. Kernel or Parzen estimates as well as k-nearest neighbor methods (at least when k is large) are examples of popular nonparametric density estimation methods <ref> [90, 86, 108] </ref>. They give rise to such classification methods as Kernel Discriminant Analysis (KDA) [18, 33, 89] and k-Nearest Neighbor (k-NN) rules [18, 14]. The Probabilistic Neural Network (PNN) [95] is the neural network counterpart of KDA. <p> Several approaches have been proposed in the literature, see <ref> [90, 86, 68, 108] </ref>. We used two methods based on cross-validated error count. In the first method, KDA1, we restrict all the smoothing parameters h j to be equal to a parameter h, and evaluate the number of misclassified vectors using cross-validation (see Section 4.1).
Reference: [91] <author> F. Smieja. </author> <title> The Pandemonium system of reflective agents. </title> <type> Technical Report 1994/2, </type> <institution> German National Research Center for Computer Science (GMD), </institution> <year> 1994. </year> <note> available in http://borneo.gmd.de/AS/janus/publi/publi.html. </note>
Reference-contexts: A more general case is variance reduction in continuous function estimation. A set of MLPs can be combined into a committee classifier with reduced output variance and thus smaller classification error [76, 34, 57, 111]. A separate confidence function may also be incorporated in each of the MLPs <ref> [91] </ref>.
Reference: [92] <author> P. Smyth and J. Mellstrom. </author> <title> Fault diagnosis of antenna pointing systems using hybrid neural network and signal processing models. </title> <editor> In J. Moody, S. Hanson, and R. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 667-674. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: One solution is to reduce the estimate, that is, to use fewer kernels but to place them at optimal locations. One can also introduce kernel dependent weights and smoothing parameters. Various reduction approaches were described in <ref> [26, 27, 31, 77, 78, 103, 92, 112] </ref>. In some cases the methods suggested have a close connection with radial basis function and mixture density estimation techniques.
Reference: [93] <author> P. Sollich and A. Krogh. </author> <title> Learning with ensembles: How over-fitting can be useful. </title> <booktitle> In Neural Information Processing Systems 8, </booktitle> <year> 1995. </year> <note> to appear. </note>
Reference-contexts: Given a fixed feature extraction method one can either use a common training set to design a number of different types of classifiers [48] or alternatively use different training sets to design several versions of one type of classifier <ref> [16, 15, 42, 85, 93] </ref>. 19 5 Case Study: Classification of Handwritten Dig- its 5.1 Data and its Preprocessing We tested the performance of the classifiers defined previously on a realistic pattern recognition problem, the classification of handwritten digits. To obtain the test material, we used a set of forms.
Reference: [94] <author> D. Specht. </author> <title> A general regression neural network. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(6) </volume> <pages> 568-576, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Examples of nonparametric methodologies are Projection Pursuit [23, 19], additive models [37], Multivariate Adaptive Regression Splines (MARS) [22], the Nadaraya-Watson kernel regression estimator [108, 56] and Local Linear Regression (LLR) [11, 12, 108]. The Nadaraya-Watson kernel regression estimator is also called the General Regression Neural Network <ref> [94] </ref>. Other neural network approaches include Multi-Layer Perceptrons with a flexible architecture and Radial Basis Function expansions [40, 3]. 5 One way to fit a regression model is to use binary least squares regression.
Reference: [95] <author> D. F. Specht. </author> <title> Probabilistic neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 3(1) </volume> <pages> 109-118, </pages> <year> 1990. </year>
Reference-contexts: They give rise to such classification methods as Kernel Discriminant Analysis (KDA) [18, 33, 89] and k-Nearest Neighbor (k-NN) rules [18, 14]. The Probabilistic Neural Network (PNN) <ref> [95] </ref> is the neural network counterpart of KDA. In another approach the densities are estimated as finite mixtures of some standard probability densities by using the expectation-maximization (EM) algorithm or some other method [79, 103, 77, 78, 38].
Reference: [96] <author> S. N. Srihari. </author> <title> Recognition of handwritten and machineprinted text for postal address interpretation. </title> <journal> Pattern Recognition Letters, </journal> <volume> 14(4) </volume> <pages> 291-302, </pages> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: In statistics literature supervised classification is often called discriminant analysis [68]. A popular application of pattern recognition is o*ine character recognition, where research during the last two decades has focused on handwritten characters <ref> [99, 65, 1 50, 71, 101, 100, 96, 30, 107, 1] </ref>. This is mainly due to the fact that while recognition of machine printed characters is considered a solved problem, the reliability achieved with handwritten text has not yet reached the level required in practical applications.
Reference: [97] <author> K. Stokbro, D. Umberger, and J. Hertz. </author> <title> Exploiting neurons with localized 35 receptive fields to learn chaos. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> <pages> 603-622, </pages> <year> 1990. </year>
Reference-contexts: However, a classifier operating on roughly the same principles should achieve good performance. A particularly promising candidate in this respect would be the local linear radial basis function method used in <ref> [97] </ref>. The performance of the tree classifier was disappointing. The natural approach is to use full 64-component pattern vectors and to let the algorithm itself decide how many variables (pattern components) x i an optimal tree needs.
Reference: [98] <author> C. Stone. </author> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 36 </volume> <pages> 111-147, </pages> <year> 1974. </year>
Reference-contexts: The whole process of classifier design should then be based strictly on the training sample only. In addition to parameter estimation, the design of some classifiers involves the choice of various tuning parameters and model or architecture selection. To utilize the training sample efficiently, cross-validation <ref> [98] </ref> (or "rotation", cf. [14, Ch. 10.6.4]) can be used. In v-fold cross-validation the training sample is first divided into v disjoint subsets.
Reference: [99] <author> C. Y. Suen, M. Berthold, and S. Mori. </author> <title> Automatic recognition of handprinted characters-the state of the art. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 68(4) </volume> <pages> 469-487, </pages> <month> April </month> <year> 1980. </year>
Reference-contexts: In statistics literature supervised classification is often called discriminant analysis [68]. A popular application of pattern recognition is o*ine character recognition, where research during the last two decades has focused on handwritten characters <ref> [99, 65, 1 50, 71, 101, 100, 96, 30, 107, 1] </ref>. This is mainly due to the fact that while recognition of machine printed characters is considered a solved problem, the reliability achieved with handwritten text has not yet reached the level required in practical applications.
Reference: [100] <author> C. Y. Suen, R. Legault, C. Nadal, M. Cheriet, and L. Lam. </author> <title> Building a new generation of handwriting recognition systems. </title> <journal> Pattern Recognition Letters, </journal> <volume> 14(4) </volume> <pages> 303-315, </pages> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: In statistics literature supervised classification is often called discriminant analysis [68]. A popular application of pattern recognition is o*ine character recognition, where research during the last two decades has focused on handwritten characters <ref> [99, 65, 1 50, 71, 101, 100, 96, 30, 107, 1] </ref>. This is mainly due to the fact that while recognition of machine printed characters is considered a solved problem, the reliability achieved with handwritten text has not yet reached the level required in practical applications.
Reference: [101] <author> C. Y. Suen, C. Nadal, R. Legault, T. A. Mai, and L. Lam. </author> <title> Computer recognition of unconstrained handwritten numerals. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1162-1180, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: In statistics literature supervised classification is often called discriminant analysis [68]. A popular application of pattern recognition is o*ine character recognition, where research during the last two decades has focused on handwritten characters <ref> [99, 65, 1 50, 71, 101, 100, 96, 30, 107, 1] </ref>. This is mainly due to the fact that while recognition of machine printed characters is considered a solved problem, the reliability achieved with handwritten text has not yet reached the level required in practical applications. <p> The simplest decision rule is to use a majority rule among the classifiers in the committee, possibly ignoring the opinion of some of the classifiers [115]. Two or more classifiers using different sets of features may be combined to implement the rejection of ambiguous patterns <ref> [72, 53, 101, 59] </ref>. A genetic algorithm can search for optimal weights to combine the classifier outputs [60]. Theoretically more advanced methods may be derived from the EM-algorithm [43, 44, 113, 114, 52] or from the Dempster-Shafer theory of evidence [116, 20].
Reference: [102] <author> C. Therrien. </author> <title> Decision, Estimation and Classification: An Introduction to Pattern Recognition and Related Topics. </title> <publisher> John Wiley & Sons, </publisher> <year> 1989. </year>
Reference-contexts: In a narrow sense, the pattern recognition problem can be parceled to data acquisition and preprocessing, feature extraction, and classification. In addition to general references such as <ref> [17, 117, 14, 102, 25, 68, 84, 24] </ref> questions of pattern recognition are extensively covered in the literature of many specific application areas such as speech and image recognition. The performance of statistical and neural network methods is usually discussed in connection with feature extraction and classification.
Reference: [103] <author> H. Tr-aven. </author> <title> A neural network approach to statistical pattern classification by "semiparametric" estimation of probability density functions. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(3) </volume> <pages> 366-377, </pages> <year> 1991. </year>
Reference-contexts: The Probabilistic Neural Network (PNN) [95] is the neural network counterpart of KDA. In another approach the densities are estimated as finite mixtures of some standard probability densities by using the expectation-maximization (EM) algorithm or some other method <ref> [79, 103, 77, 78, 38] </ref>. Such an approach can be viewed as an economized KDA or an instance of the RBF approach [3]. <p> One solution is to reduce the estimate, that is, to use fewer kernels but to place them at optimal locations. One can also introduce kernel dependent weights and smoothing parameters. Various reduction approaches were described in <ref> [26, 27, 31, 77, 78, 103, 92, 112] </ref>. In some cases the methods suggested have a close connection with radial basis function and mixture density estimation techniques.
Reference: [104] <author> V. Tresp and M. Taniguchi. </author> <title> Combining estimators using non-constant weighting functions. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Neural Information Processing Systems 7, </booktitle> <pages> pages 419-426, </pages> <address> Cambridge, MA, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Theoretically more advanced methods may be derived from the EM-algorithm [43, 44, 113, 114, 52] or from the Dempster-Shafer theory of evidence [116, 20]. The outputs of several classifiers may be combined linearly [51] or nonlinearly <ref> [104] </ref> to reduce the variance of the posterior probability estimates. A more general case is variance reduction in continuous function estimation. A set of MLPs can be combined into a committee classifier with reduced output variance and thus smaller classification error [76, 34, 57, 111].
Reference: [105] <author> G. Tutz. </author> <title> An alternative choice of smoothing for kernel-based density estimates in discrete discriminant analysis. </title> <journal> Biometrika, </journal> <volume> 73 </volume> <pages> 405-411, </pages> <year> 1986. </year>
Reference-contexts: In the second method the nonsmoothness of the object function causes more trouble. Instead of minimizing the error count directly, we found it advantageous to minimize a smoothed version of it. The particular smoothing described below is inspired by that used in <ref> [105] </ref>.
Reference: [106] <author> W. Venables and B. Ripley. </author> <title> Modern Applied Statistics with S-Plus. </title> <address> Springer-Verlagi New York, </address> <publisher> Inc., </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The tree classifier therefore uses the Bayes rule with the class posterior probabilities estimated by locally constant functions. The particular tree classifier used here is available as part of the S-Plus statistical software package <ref> [2, 8, 106] </ref>. This implementation uses a likelihood function to select the optimal splits [10]. Pruning was performed by the minimal cost-complexity method.
Reference: [107] <author> T. Wakahara. </author> <title> Towards robust handwritten character recognition. </title> <journal> Pattern Recognition Letters, </journal> <volume> 14(4) </volume> <pages> 345-354, </pages> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: In statistics literature supervised classification is often called discriminant analysis [68]. A popular application of pattern recognition is o*ine character recognition, where research during the last two decades has focused on handwritten characters <ref> [99, 65, 1 50, 71, 101, 100, 96, 30, 107, 1] </ref>. This is mainly due to the fact that while recognition of machine printed characters is considered a solved problem, the reliability achieved with handwritten text has not yet reached the level required in practical applications.
Reference: [108] <author> M. Wand and M. Jones. </author> <title> Kernel Smoothing. </title> <publisher> Chapman & Hall, </publisher> <year> 1995. </year>
Reference-contexts: In nonparametric density estimation no such fixed family of possible densities is assumed. Kernel or Parzen estimates as well as k-nearest neighbor methods (at least when k is large) are examples of popular nonparametric density estimation methods <ref> [90, 86, 108] </ref>. They give rise to such classification methods as Kernel Discriminant Analysis (KDA) [18, 33, 89] and k-Nearest Neighbor (k-NN) rules [18, 14]. The Probabilistic Neural Network (PNN) [95] is the neural network counterpart of KDA. <p> In nonparametric regression no fixed functional form is assumed. Examples of nonparametric methodologies are Projection Pursuit [23, 19], additive models [37], Multivariate Adaptive Regression Splines (MARS) [22], the Nadaraya-Watson kernel regression estimator <ref> [108, 56] </ref> and Local Linear Regression (LLR) [11, 12, 108]. The Nadaraya-Watson kernel regression estimator is also called the General Regression Neural Network [94]. <p> In nonparametric regression no fixed functional form is assumed. Examples of nonparametric methodologies are Projection Pursuit [23, 19], additive models [37], Multivariate Adaptive Regression Splines (MARS) [22], the Nadaraya-Watson kernel regression estimator [108, 56] and Local Linear Regression (LLR) <ref> [11, 12, 108] </ref>. The Nadaraya-Watson kernel regression estimator is also called the General Regression Neural Network [94]. Other neural network approaches include Multi-Layer Perceptrons with a flexible architecture and Radial Basis Function expansions [40, 3]. 5 One way to fit a regression model is to use binary least squares regression. <p> Several approaches have been proposed in the literature, see <ref> [90, 86, 68, 108] </ref>. We used two methods based on cross-validated error count. In the first method, KDA1, we restrict all the smoothing parameters h j to be equal to a parameter h, and evaluate the number of misclassified vectors using cross-validation (see Section 4.1). <p> However, in our application we simply used the Euclidean metric. At a given x, the weighted linear least squares problem can be reduced to inverting a (d + 1) fi (d + 1) matrix, where d is the dimensionality of x, see e.g. <ref> [108, Ch. 5] </ref>. It would also be possible to use a bandwidth h which does not depend on x and the weight function could be chosen differently. This would lead to local linear (or local polynomial) kernel regression [108, Ch. 5]. <p> + 1) matrix, where d is the dimensionality of x, see e.g. <ref> [108, Ch. 5] </ref>. It would also be possible to use a bandwidth h which does not depend on x and the weight function could be chosen differently. This would lead to local linear (or local polynomial) kernel regression [108, Ch. 5]. There is an interesting connection between local constant kernel regression and KDA. First of all, the local constant regression with kernel weights is identical with the Nadaraya-Watson kernel regression estimator.
Reference: [109] <author> S. Watanabe, P. F. Lambert, C. A. Kulikowski, J. L. Buxton, and R. Walker. </author> <title> Evaluation and selection of variables in pattern recognition. </title> <editor> In J. Tou, editor, </editor> <booktitle> Computer and Information Sciences II. </booktitle> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1967. </year>
Reference-contexts: A distinct family of discrimination methods are the sub-space classifiers [73] that model the individual classes using linear subspaces and classify a pattern according to its shortest distance to the model subspaces. Examples include the classical CLAFIC (CLAss-Featuring Information Compression) method <ref> [109] </ref> and the Averaged Learning Subspace Method (ALSM) that features incremental learning [73]. 2.4 What are neural classifiers? In the previous discussion we characterized some popular classification techniques in terms of the mathematical principles they are based on. <p> For each class j the correlation matrix c R j is estimated and its first few eigenvectors u 1j ; : : : ; u ` j j are used as columns of a basis matrix U j . The classification rule of the CLAFIC <ref> [109] </ref> algorithm can then be expressed as g CLAFIC (x) = argmax j=1;::: ;c j xk 2 :(25) The Averaged Learning Subspace Method (ALSM) introduced by one of the authors [73] is an iterative learning version of CLAFIC in which the unnormalized sample class correlation matrices b S j = P
Reference: [110] <author> H. White. </author> <title> Learning in artificial neural networks: A statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 425-464, </pages> <year> 1989. </year>
Reference-contexts: However, the conditional expectation m may or may not belong to the family R, and besides, sampling variation will anyhow prevent us from estimating m exactly even when it does belong to R <ref> [110, 80] </ref>.
Reference: [111] <author> D. H. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259, </pages> <year> 1992. </year>
Reference-contexts: A more general case is variance reduction in continuous function estimation. A set of MLPs can be combined into a committee classifier with reduced output variance and thus smaller classification error <ref> [76, 34, 57, 111] </ref>. A separate confidence function may also be incorporated in each of the MLPs [91].
Reference: [112] <author> J. X. Wu and C. Chan. </author> <title> A three-layer adaptive network for pattern density estimation and classification. </title> <journal> International Journal of Neural Systems, </journal> <volume> 2(3) </volume> <pages> 211-220, </pages> <year> 1991. </year>
Reference-contexts: One solution is to reduce the estimate, that is, to use fewer kernels but to place them at optimal locations. One can also introduce kernel dependent weights and smoothing parameters. Various reduction approaches were described in <ref> [26, 27, 31, 77, 78, 103, 92, 112] </ref>. In some cases the methods suggested have a close connection with radial basis function and mixture density estimation techniques.
Reference: [113] <author> L. Xu and M. I. Jordan. </author> <title> EM learning on a generalized finite mixture model for 36 combining multiple classifiers. </title> <booktitle> In Proceedings of the World Congress on Neural Networks, volume IV, </booktitle> <pages> pages 227-230, </pages> <year> 1993. </year>
Reference-contexts: Two or more classifiers using different sets of features may be combined to implement the rejection of ambiguous patterns [72, 53, 101, 59]. A genetic algorithm can search for optimal weights to combine the classifier outputs [60]. Theoretically more advanced methods may be derived from the EM-algorithm <ref> [43, 44, 113, 114, 52] </ref> or from the Dempster-Shafer theory of evidence [116, 20]. The outputs of several classifiers may be combined linearly [51] or nonlinearly [104] to reduce the variance of the posterior probability estimates. A more general case is variance reduction in continuous function estimation.
Reference: [114] <author> L. Xu, M. I. Jordan, and G. E. Hinton. </author> <title> An alternative model for mixtures of experts. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Neural Information Processing Systems 7, </booktitle> <pages> pages 633-640, </pages> <address> Cambridge, MA, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Two or more classifiers using different sets of features may be combined to implement the rejection of ambiguous patterns [72, 53, 101, 59]. A genetic algorithm can search for optimal weights to combine the classifier outputs [60]. Theoretically more advanced methods may be derived from the EM-algorithm <ref> [43, 44, 113, 114, 52] </ref> or from the Dempster-Shafer theory of evidence [116, 20]. The outputs of several classifiers may be combined linearly [51] or nonlinearly [104] to reduce the variance of the posterior probability estimates. A more general case is variance reduction in continuous function estimation.
Reference: [115] <author> L. Xu, A. Krzyzak, and C. Y. Suen. </author> <title> Associative switch for combining multiple classifiers. </title> <booktitle> In Proceedings of 1991 International Joint Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 43-48. </pages> <publisher> IEEE, </publisher> <address> INNS, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Various ways to combine classifiers with such different types of output information are analyzed in [116, 43, 44, 47]. The simplest decision rule is to use a majority rule among the classifiers in the committee, possibly ignoring the opinion of some of the classifiers <ref> [115] </ref>. Two or more classifiers using different sets of features may be combined to implement the rejection of ambiguous patterns [72, 53, 101, 59]. A genetic algorithm can search for optimal weights to combine the classifier outputs [60].
Reference: [116] <author> L. Xu, A. Krzyzak, and C. Y. Suen. </author> <title> Methods of combining multiple classifiers and their applications to handwriting recognition. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 22(3) </volume> <pages> 418-435, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The classifier may also propose a set of classes in the order of decreasing certainty or a measure of decision certainty may be associated with all possible classes. Various ways to combine classifiers with such different types of output information are analyzed in <ref> [116, 43, 44, 47] </ref>. The simplest decision rule is to use a majority rule among the classifiers in the committee, possibly ignoring the opinion of some of the classifiers [115]. <p> A genetic algorithm can search for optimal weights to combine the classifier outputs [60]. Theoretically more advanced methods may be derived from the EM-algorithm [43, 44, 113, 114, 52] or from the Dempster-Shafer theory of evidence <ref> [116, 20] </ref>. The outputs of several classifiers may be combined linearly [51] or nonlinearly [104] to reduce the variance of the posterior probability estimates. A more general case is variance reduction in continuous function estimation.
Reference: [117] <author> T. Y. Young and T. W. Calvert. </author> <title> Classification, Estimation and Pattern Recognition. </title> <publisher> American Elsevier Publishing Co., Inc., </publisher> <address> New York, </address> <year> 1974. </year> <month> 37 </month>
Reference-contexts: In a narrow sense, the pattern recognition problem can be parceled to data acquisition and preprocessing, feature extraction, and classification. In addition to general references such as <ref> [17, 117, 14, 102, 25, 68, 84, 24] </ref> questions of pattern recognition are extensively covered in the literature of many specific application areas such as speech and image recognition. The performance of statistical and neural network methods is usually discussed in connection with feature extraction and classification. <p> We elaborate upon these two approaches further in Sections 2.1 and 2.2 below. 3 Other criteria than minimum classification error can be important in practice. Sometimes a class-dependent misclassification cost is needed. In certain types of applications it is essential to use Neyman-Pearson style classification <ref> [117, 46] </ref> where the classification error of one class is minimized given a fixed error for another class. The use of a reject class can help reduce the misclassification rate in tasks where exceptional handling (e.g. by a human expert) of particularly ambiguous cases is feasible.
References-found: 117

