URL: ftp://ftp.cs.toronto.edu/pub/parallel/Kulkarni_Stumm_292.ps.Z
Refering-URL: http://www.cs.toronto.edu/~kulki/pubs_abs.html
Root-URL: 
Email: Email: kulki@csri.toronto.edu  
Title: Alignment: A New, Unified Program Transformation for Local and Global Optimization  
Author: Dattatraya Kulkarni and Michael Stumm 
Date: January, 1994  
Address: Toronto, Toronto, Canada, M5S 1A4  
Affiliation: Department of Computer Science, and Department of Electrical and Computer Engineering University of  
Note: Computational  
Abstract: Technical Report CSRI-292, Computer Systems Research Institute, University of Toronto, January 1994.Technical Report CSRI-292, Computer Systems Research Institute, University of Toronto, January 1994. Abstract Computational Alignment is a new class of program transformations suitable for both local and global optimization. Computational Alignment transforms all of the computations of a portion of the loop body in order to align them to other computations either in the same loop or in another loop. It extends along a new dimension and is significantly more powerful than linear transformations because i) it can transform subsets of dependences and references; ii) it is sensitive to the location of data in that it can move the computation relative to data; iii) it applies to imperfect loop nests; and iv) it is the first loop transformation that can change access vectors. Linear transformations are just a special case of Computational Alignment. Computational Alignment is highly suitable for global optimization because it can transform given loops to access data in similar ways. Two important subclasses of Computational Alignment are presented as well, namely, Freeing and Isomerizing Computational Alignment.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Michael Wolfe. </author> <title> Optimizing supercompilers for supercomputers. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: They map computations and data onto processors to extract parallelism and minimize data movement. From early on, nested loops have been recognized as a major source of parallelism. Consequently, there is a large body of work on transforming nested loops <ref> [1, 2, 3, 4, 5, 6, 7, 8] </ref> and the arrays they access [9, 10, 11] for the purpose of improving locality, parallelism and load balance. Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling [1, 2, 3, 4, 6, 12, 13, 14, 15, 16]. <p> Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling <ref> [1, 2, 3, 4, 6, 12, 13, 14, 15, 16] </ref>. A data transformation applied often is data alignment, which maps one array onto another. 1 In practice, when optimizing a loop, it is generally necessary to apply a sequence of several transformations. <p> That is, the loop bounds, dependences, and references of the transformed loop can be computed directly from the transformation matrix, and the original bounds matrix, dependences, and reference matrices. The framework is unifying in that, any sequence of linear transformations such as reversal, interchange, and skew <ref> [1] </ref> can be combined and represented by a single linear transformation. Suppose I is the iteration vector, fi the bounds matrix, D the dependence matrix where each column is a dependence distance vector, and R an array reference in an n-dimensional nested loop. <p> Example 9 Consider the alignment of Example 4 with n of 100. The crossover occurs at i =0, and 100. If we include the i bounds, the ranges for X 1 = f100; 0; 100g would be R 1 = h <ref> [100; 1] </ref>; [0; 100]i, and the computation they are associated with would be L 2 and L 1 &L 2 respectively. The the range [100; 1] contains only L 2 . We therefore, generate a double loop containing L 2 (Step 15). <p> If we include the i bounds, the ranges for X 1 = f100; 0; 100g would be R 1 = h <ref> [100; 1] </ref>; [0; 100]i, and the computation they are associated with would be L 2 and L 1 &L 2 respectively. The the range [100; 1] contains only L 2 . We therefore, generate a double loop containing L 2 (Step 15). Iterator i varies from -100 to -1 (with a step of 1). The j bounds are used as given. The second range, [0; 100], has both L 1 and L 2 computations.
Reference: [2] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: They map computations and data onto processors to extract parallelism and minimize data movement. From early on, nested loops have been recognized as a major source of parallelism. Consequently, there is a large body of work on transforming nested loops <ref> [1, 2, 3, 4, 5, 6, 7, 8] </ref> and the arrays they access [9, 10, 11] for the purpose of improving locality, parallelism and load balance. Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling [1, 2, 3, 4, 6, 12, 13, 14, 15, 16]. <p> Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling <ref> [1, 2, 3, 4, 6, 12, 13, 14, 15, 16] </ref>. A data transformation applied often is data alignment, which maps one array onto another. 1 In practice, when optimizing a loop, it is generally necessary to apply a sequence of several transformations. <p> The iteration space corresponds to the integer space described by the loop bounds, and is defined as <ref> [2] </ref>: Definition 1 (Iteration space) An iteration space is a set I Z n such that I = f (i 1 ; :::; i n ) j l 1 i 1 u 1 ; :::; l n (i 1 ; :::; i n1 ) i n u n (i 1 ; <p> The first condition follows from the first principle of program transformation <ref> [2, 17] </ref>. It ensures that the new data dependence relations will lexicographically have the same sign as the original dependences so that they have the same temporal order, ensuring that correct values are computed.
Reference: [3] <author> J.R. Allen and Ken Kennedy. </author> <title> Automatic loop interchange. </title> <booktitle> In Proceedings of the ACM SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <volume> volume 19, </volume> <pages> pages 233-246, </pages> <year> 1984. </year>
Reference-contexts: They map computations and data onto processors to extract parallelism and minimize data movement. From early on, nested loops have been recognized as a major source of parallelism. Consequently, there is a large body of work on transforming nested loops <ref> [1, 2, 3, 4, 5, 6, 7, 8] </ref> and the arrays they access [9, 10, 11] for the purpose of improving locality, parallelism and load balance. Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling [1, 2, 3, 4, 6, 12, 13, 14, 15, 16]. <p> Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling <ref> [1, 2, 3, 4, 6, 12, 13, 14, 15, 16] </ref>. A data transformation applied often is data alignment, which maps one array onto another. 1 In practice, when optimizing a loop, it is generally necessary to apply a sequence of several transformations.
Reference: [4] <author> A. Aiken and A. Nicolau. </author> <title> Optimal loop parallelization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 23, </volume> <pages> pages 308-317, </pages> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: They map computations and data onto processors to extract parallelism and minimize data movement. From early on, nested loops have been recognized as a major source of parallelism. Consequently, there is a large body of work on transforming nested loops <ref> [1, 2, 3, 4, 5, 6, 7, 8] </ref> and the arrays they access [9, 10, 11] for the purpose of improving locality, parallelism and load balance. Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling [1, 2, 3, 4, 6, 12, 13, 14, 15, 16]. <p> Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling <ref> [1, 2, 3, 4, 6, 12, 13, 14, 15, 16] </ref>. A data transformation applied often is data alignment, which maps one array onto another. 1 In practice, when optimizing a loop, it is generally necessary to apply a sequence of several transformations.
Reference: [5] <author> E.H. D'Hollander. </author> <title> Partitioning and labeling of index sets in do loops with constant dependence vectors. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages 139-144, </pages> <year> 1989. </year>
Reference-contexts: They map computations and data onto processors to extract parallelism and minimize data movement. From early on, nested loops have been recognized as a major source of parallelism. Consequently, there is a large body of work on transforming nested loops <ref> [1, 2, 3, 4, 5, 6, 7, 8] </ref> and the arrays they access [9, 10, 11] for the purpose of improving locality, parallelism and load balance. Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling [1, 2, 3, 4, 6, 12, 13, 14, 15, 16].
Reference: [6] <author> L. Lamport. </author> <title> The parallel execution of do loops. </title> <journal> Communications of the ACM, </journal> <volume> 17(2), </volume> <year> 1974. </year>
Reference-contexts: They map computations and data onto processors to extract parallelism and minimize data movement. From early on, nested loops have been recognized as a major source of parallelism. Consequently, there is a large body of work on transforming nested loops <ref> [1, 2, 3, 4, 5, 6, 7, 8] </ref> and the arrays they access [9, 10, 11] for the purpose of improving locality, parallelism and load balance. Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling [1, 2, 3, 4, 6, 12, 13, 14, 15, 16]. <p> Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling <ref> [1, 2, 3, 4, 6, 12, 13, 14, 15, 16] </ref>. A data transformation applied often is data alignment, which maps one array onto another. 1 In practice, when optimizing a loop, it is generally necessary to apply a sequence of several transformations.
Reference: [7] <author> W. Kelly and W. Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> Technical Report UMIACS-TR-92-126, </type> <institution> University of Maryland, </institution> <year> 1992. </year>
Reference-contexts: They map computations and data onto processors to extract parallelism and minimize data movement. From early on, nested loops have been recognized as a major source of parallelism. Consequently, there is a large body of work on transforming nested loops <ref> [1, 2, 3, 4, 5, 6, 7, 8] </ref> and the arrays they access [9, 10, 11] for the purpose of improving locality, parallelism and load balance. Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling [1, 2, 3, 4, 6, 12, 13, 14, 15, 16].
Reference: [8] <author> V. Sarkar and R. Thekkath. </author> <title> A general framework for iteration-reordering loop transformations (technical summary). </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 27, </volume> <pages> pages 175-187, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: They map computations and data onto processors to extract parallelism and minimize data movement. From early on, nested loops have been recognized as a major source of parallelism. Consequently, there is a large body of work on transforming nested loops <ref> [1, 2, 3, 4, 5, 6, 7, 8] </ref> and the arrays they access [9, 10, 11] for the purpose of improving locality, parallelism and load balance. Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling [1, 2, 3, 4, 6, 12, 13, 14, 15, 16].
Reference: [9] <author> K. Knobe and V. Natarajan. </author> <title> Data optimization: Minimizing residual interprocessor data motion on simd machines. </title> <booktitle> In Proceedings of the Symposium on frontiers of massively parallel computation, </booktitle> <pages> pages 416-423, </pages> <year> 1990. </year>
Reference-contexts: From early on, nested loops have been recognized as a major source of parallelism. Consequently, there is a large body of work on transforming nested loops [1, 2, 3, 4, 5, 6, 7, 8] and the arrays they access <ref> [9, 10, 11] </ref> for the purpose of improving locality, parallelism and load balance. Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling [1, 2, 3, 4, 6, 12, 13, 14, 15, 16].
Reference: [10] <author> M.F.P O'Boyle and G.A. Hedayat. </author> <title> Data alignment: Transformations to reduce communication on distributed memory architectures. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <publisher> IEE Press, </publisher> <address> Williamsburg, </address> <year> 1992. </year>
Reference-contexts: From early on, nested loops have been recognized as a major source of parallelism. Consequently, there is a large body of work on transforming nested loops [1, 2, 3, 4, 5, 6, 7, 8] and the arrays they access <ref> [9, 10, 11] </ref> for the purpose of improving locality, parallelism and load balance. Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling [1, 2, 3, 4, 6, 12, 13, 14, 15, 16]. <p> The objective in aligning two arrays is to reorganize the data in one array with respect to that in the other in order to improve locality, assuming the arrays will be distributed the same way. O'Boyle and Hedayat <ref> [10] </ref> formalize alignment of a pair of arrays and provide a heuristic algorithm to do so. Li and Chen [11] provide a heuristic algorithm for the much more general task of aligning a group of arrays in a complete program.
Reference: [11] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed memory machines. </title> <journal> Journal of parallel and distributed computing, </journal> <volume> 13 </volume> <pages> 213-221, </pages> <year> 1991. </year>
Reference-contexts: From early on, nested loops have been recognized as a major source of parallelism. Consequently, there is a large body of work on transforming nested loops [1, 2, 3, 4, 5, 6, 7, 8] and the arrays they access <ref> [9, 10, 11] </ref> for the purpose of improving locality, parallelism and load balance. Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling [1, 2, 3, 4, 6, 12, 13, 14, 15, 16]. <p> In fact, traditional linear loop transformations are just a special case of Computational Alignment in our framework. Computational Alignment enables us to perform local optimizations that were not possible with earlier techniques. In addition, it is highly suitable for global optimizations <ref> [23, 11, 24, 25] </ref>. In contrast, existing loop and data transformations only consider constraints relative to a single loop, and can therefore only be applied as local optimizations. <p> O'Boyle and Hedayat [10] formalize alignment of a pair of arrays and provide a heuristic algorithm to do so. Li and Chen <ref> [11] </ref> provide a heuristic algorithm for the much more general task of aligning a group of arrays in a complete program. An alignment of an array B to array A maps the elements of B to the elements of A.
Reference: [12] <author> Randy Allen, David Callahan, and Ken Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Conference Record of the 14th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 63-76, </pages> <address> Munich, West Germany, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling <ref> [1, 2, 3, 4, 6, 12, 13, 14, 15, 16] </ref>. A data transformation applied often is data alignment, which maps one array onto another. 1 In practice, when optimizing a loop, it is generally necessary to apply a sequence of several transformations. <p> We presented techniques that compute tight bounds and generate guard-free code for arbitrary alignment functions, but causing imperfect nesting in the process. In related work, Allen et al. discuss loop alignment, a transformation that is similar to FCA in eliminating dependences <ref> [12] </ref>. However the scope of their work is limited to obtaining independent partitions in a single loop, and does not have the algebraic framework provided with Computational Alignment. Access Normalization [37] is similar to ICA with ff set to the identity matrix, and treating the entire loop body uniformly.
Reference: [13] <author> Utpal Banerjee. </author> <title> A theory of loop permutations. </title> <booktitle> In Proceedings of Second Workshop on Programming Languag es and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1989. </year> <month> 38 </month>
Reference-contexts: Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling <ref> [1, 2, 3, 4, 6, 12, 13, 14, 15, 16] </ref>. A data transformation applied often is data alignment, which maps one array onto another. 1 In practice, when optimizing a loop, it is generally necessary to apply a sequence of several transformations.
Reference: [14] <author> J. Ramanujam. </author> <title> Tiling of iteration spaces for multicomputers. </title> <booktitle> In Proceedings of the Supercomputing 1992, </booktitle> <pages> pages 179-186, </pages> <year> 1992. </year>
Reference-contexts: Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling <ref> [1, 2, 3, 4, 6, 12, 13, 14, 15, 16] </ref>. A data transformation applied often is data alignment, which maps one array onto another. 1 In practice, when optimizing a loop, it is generally necessary to apply a sequence of several transformations.
Reference: [15] <author> M.S. Lam, E.E. Rothberg, and M.E. Wolf. </author> <title> The cache performance and optimizations of block algorithms. </title> <booktitle> In 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling <ref> [1, 2, 3, 4, 6, 12, 13, 14, 15, 16] </ref>. A data transformation applied often is data alignment, which maps one array onto another. 1 In practice, when optimizing a loop, it is generally necessary to apply a sequence of several transformations.
Reference: [16] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Conference Record of the 15th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 319-329, </pages> <address> San Diego, CA, </address> <year> 1988. </year>
Reference-contexts: Some of the basic loop transformations are interchange, skew, reversal, wavefront, permutation, and tiling <ref> [1, 2, 3, 4, 6, 12, 13, 14, 15, 16] </ref>. A data transformation applied often is data alignment, which maps one array onto another. 1 In practice, when optimizing a loop, it is generally necessary to apply a sequence of several transformations.
Reference: [17] <author> Utpal Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of Third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Deriving the appropriate sequence of transformations is non-trivial and early work in the area did not address this issue. Linear loop transformations <ref> [17, 18, 19, 20] </ref> provide a unifying framework in which a sequence of transformations is represented by a non-singular matrix. The algebraic framework allows us to derive the transformed loop structure in a systematic way in one step from the given transformation matrix and the original loop structure. <p> The introduction of linear loop transformations <ref> [17, 18, 19, 20] </ref> was a major step in unifying most existing loop transformations. In this framework, a non-singular integer matrix completely characterizes the transformation. <p> the same in both the original program and the transformed program, and 2. there is a one to one correspondence between the set of computations in the original program and the transformed program. 2 Proof : Since the theorem is a slight modification of the one in linear transformations theory <ref> [17] </ref>, we only provide an informal proof here. The first condition follows from the first principle of program transformation [2, 17]. <p> The first condition follows from the first principle of program transformation <ref> [2, 17] </ref>. It ensures that the new data dependence relations will lexicographically have the same sign as the original dependences so that they have the same temporal order, ensuring that correct values are computed.
Reference: [18] <author> M.E. Wolf and M.S. Lam. </author> <title> An algorithmic approach to compound loop transformation. </title> <booktitle> In Proceedings of Third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Deriving the appropriate sequence of transformations is non-trivial and early work in the area did not address this issue. Linear loop transformations <ref> [17, 18, 19, 20] </ref> provide a unifying framework in which a sequence of transformations is represented by a non-singular matrix. The algebraic framework allows us to derive the transformed loop structure in a systematic way in one step from the given transformation matrix and the original loop structure. <p> The introduction of linear loop transformations <ref> [17, 18, 19, 20] </ref> was a major step in unifying most existing loop transformations. In this framework, a non-singular integer matrix completely characterizes the transformation.
Reference: [19] <author> D. Kulkarni, K.G. Kumar, A. Basu, and A. Paulraj. </author> <title> Loop partitioning for distributed memory multiprocessors as unimodular transformations. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Deriving the appropriate sequence of transformations is non-trivial and early work in the area did not address this issue. Linear loop transformations <ref> [17, 18, 19, 20] </ref> provide a unifying framework in which a sequence of transformations is represented by a non-singular matrix. The algebraic framework allows us to derive the transformed loop structure in a systematic way in one step from the given transformation matrix and the original loop structure. <p> In our earlier work, we posed the problem of finding the best linear transformation of nested loops as an optimization problem, and presented techniques to obtain near optimal solutions <ref> [19, 21, 22] </ref>. The framework of linear transformations is very general; even data alignment can be thought of as a linear transformation. In this paper, we present a new class of linear transformations that extends traditional linear loop transformations along a new dimension. <p> The introduction of linear loop transformations <ref> [17, 18, 19, 20] </ref> was a major step in unifying most existing loop transformations. In this framework, a non-singular integer matrix completely characterizes the transformation. <p> Traditional linear transformation can not eliminate dependences. A transformation, such as the one in this example is useful because it allows, for example, a subsequent transformation to internalize <ref> [19, 20, 21] </ref> the (1,1) dependence to the inner loop in order to obtain parallelism that did not exist in the original organization of the statements, and could not have been obtained by traditional linear transformations alone. 11 To illustrate how Computational Alignment can be used for global optimization, consider the
Reference: [20] <author> K.G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Generalized unimodular loop transformations for distributed memory multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <address> Chicago, MI, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Deriving the appropriate sequence of transformations is non-trivial and early work in the area did not address this issue. Linear loop transformations <ref> [17, 18, 19, 20] </ref> provide a unifying framework in which a sequence of transformations is represented by a non-singular matrix. The algebraic framework allows us to derive the transformed loop structure in a systematic way in one step from the given transformation matrix and the original loop structure. <p> The introduction of linear loop transformations <ref> [17, 18, 19, 20] </ref> was a major step in unifying most existing loop transformations. In this framework, a non-singular integer matrix completely characterizes the transformation. <p> Traditional linear transformation can not eliminate dependences. A transformation, such as the one in this example is useful because it allows, for example, a subsequent transformation to internalize <ref> [19, 20, 21] </ref> the (1,1) dependence to the inner loop in order to obtain parallelism that did not exist in the original organization of the statements, and could not have been obtained by traditional linear transformations alone. 11 To illustrate how Computational Alignment can be used for global optimization, consider the
Reference: [21] <author> K.G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Deriving good transformations for mapping nested loops on hierarchical parallel machines in polynomial time. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: In our earlier work, we posed the problem of finding the best linear transformation of nested loops as an optimization problem, and presented techniques to obtain near optimal solutions <ref> [19, 21, 22] </ref>. The framework of linear transformations is very general; even data alignment can be thought of as a linear transformation. In this paper, we present a new class of linear transformations that extends traditional linear loop transformations along a new dimension. <p> Traditional linear transformation can not eliminate dependences. A transformation, such as the one in this example is useful because it allows, for example, a subsequent transformation to internalize <ref> [19, 20, 21] </ref> the (1,1) dependence to the inner loop in order to obtain parallelism that did not exist in the original organization of the statements, and could not have been obtained by traditional linear transformations alone. 11 To illustrate how Computational Alignment can be used for global optimization, consider the
Reference: [22] <author> K.G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Mapping nested loops on hierarchical parallel machines using unimodular transformations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> page (submitted). </note>
Reference-contexts: In our earlier work, we posed the problem of finding the best linear transformation of nested loops as an optimization problem, and presented techniques to obtain near optimal solutions <ref> [19, 21, 22] </ref>. The framework of linear transformations is very general; even data alignment can be thought of as a linear transformation. In this paper, we present a new class of linear transformations that extends traditional linear loop transformations along a new dimension.
Reference: [23] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 28, </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: In fact, traditional linear loop transformations are just a special case of Computational Alignment in our framework. Computational Alignment enables us to perform local optimizations that were not possible with earlier techniques. In addition, it is highly suitable for global optimizations <ref> [23, 11, 24, 25] </ref>. In contrast, existing loop and data transformations only consider constraints relative to a single loop, and can therefore only be applied as local optimizations.
Reference: [24] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: In fact, traditional linear loop transformations are just a special case of Computational Alignment in our framework. Computational Alignment enables us to perform local optimizations that were not possible with earlier techniques. In addition, it is highly suitable for global optimizations <ref> [23, 11, 24, 25] </ref>. In contrast, existing loop and data transformations only consider constraints relative to a single loop, and can therefore only be applied as local optimizations.
Reference: [25] <author> D. Kulkarni and M. Stumm. </author> <title> Global optimizations by computational alignment. </title> <note> In In preparation. </note>
Reference-contexts: In fact, traditional linear loop transformations are just a special case of Computational Alignment in our framework. Computational Alignment enables us to perform local optimizations that were not possible with earlier techniques. In addition, it is highly suitable for global optimizations <ref> [23, 11, 24, 25] </ref>. In contrast, existing loop and data transformations only consider constraints relative to a single loop, and can therefore only be applied as local optimizations.
Reference: [26] <author> A. Schrijver. </author> <title> Theory of linear and integer programming. </title> <publisher> Wiley, </publisher> <year> 1986. </year> <month> 39 </month>
Reference-contexts: Because the limits of a loop have to be expressed as functions of only the enclosing loop indices, if any at all, and fi U 1 may not conform to this requirement, we can apply the Fourier-Motzkin variable elimination technique <ref> [26] </ref> to fi U 1 to obtain the bounds matrix fi 0 for the transformed loop. When U is not unimodular the transformed loop will have non-unit strides in some dimensions and extended Fourier-Motzkin variable elimination [27, 28] may have to be applied to get the exact transformed bounds. <p> matrix. /* nothing is transformed */ 2. for i = p, p+r-1 f 0 end for 3. for i = p, p+r-1 c (i; n + 1) f c (i p + 1; r + 1) /* I p to I p+r1 are transformed */ end Then Extended Fourier-Motzkin elimination <ref> [27, 28, 26] </ref> can be applied to fi 0 2 to obtain the new L 2 loop bounds. These new bounds are from the perspective of transformed L 2 . They can be used directly if there are no enclosing iterators common to both L 1 and L 2 .
Reference: [27] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with DO loops. </title> <booktitle> In Proceedings of the 3rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <volume> volume 26, </volume> <pages> pages 39-50, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: When U is not unimodular the transformed loop will have non-unit strides in some dimensions and extended Fourier-Motzkin variable elimination <ref> [27, 28] </ref> may have to be applied to get the exact transformed bounds. <p> matrix. /* nothing is transformed */ 2. for i = p, p+r-1 f 0 end for 3. for i = p, p+r-1 c (i; n + 1) f c (i p + 1; r + 1) /* I p to I p+r1 are transformed */ end Then Extended Fourier-Motzkin elimination <ref> [27, 28, 26] </ref> can be applied to fi 0 2 to obtain the new L 2 loop bounds. These new bounds are from the perspective of transformed L 2 . They can be used directly if there are no enclosing iterators common to both L 1 and L 2 .
Reference: [28] <author> J. Ramanujam. </author> <title> Non-singular transformations of nested loops. </title> <booktitle> In Supercomputing 92, </booktitle> <pages> pages 214-223, </pages> <year> 1992. </year>
Reference-contexts: When U is not unimodular the transformed loop will have non-unit strides in some dimensions and extended Fourier-Motzkin variable elimination <ref> [27, 28] </ref> may have to be applied to get the exact transformed bounds. <p> matrix. /* nothing is transformed */ 2. for i = p, p+r-1 f 0 end for 3. for i = p, p+r-1 c (i; n + 1) f c (i p + 1; r + 1) /* I p to I p+r1 are transformed */ end Then Extended Fourier-Motzkin elimination <ref> [27, 28, 26] </ref> can be applied to fi 0 2 to obtain the new L 2 loop bounds. These new bounds are from the perspective of transformed L 2 . They can be used directly if there are no enclosing iterators common to both L 1 and L 2 .
Reference: [29] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the fortran d programming system. </title> <type> Technical Report CRPC-TR91121, </type> <institution> Dept of computer Science, Rice University, </institution> <year> 1991. </year>
Reference-contexts: If we 7 8 consider each element of array A to be on a different processor, then the alignment determines the elements of A and B that are assigned to the same processor. If we assume the familiar ownership-rule <ref> [29, 30] </ref> to determine which processor execute which computations, then an alignment will implicitly determine the placement of the computations in an iteration onto the processors. <p> Not shown in the above examples is the fact that Computational Alignment can also be used to move computations relative to data, allowing Computational Alignment to be the dual of data alignment. 1. Computational Alignment can drastically reduce the number of ownership tests <ref> [31, 29, 30] </ref> required and enable a better computation partitioning. Just as data alignment moves the data relative to other data in order to make computations in the body execute on the same processor, Computational Alignment can move computations relative to other computations and achieve the same objective.
Reference: [30] <author> HPF Forum. </author> <title> Hpf: High performance fortran language specification. </title> <type> Technical report, HPF Forum, </type> <year> 1993. </year>
Reference-contexts: If we 7 8 consider each element of array A to be on a different processor, then the alignment determines the elements of A and B that are assigned to the same processor. If we assume the familiar ownership-rule <ref> [29, 30] </ref> to determine which processor execute which computations, then an alignment will implicitly determine the placement of the computations in an iteration onto the processors. <p> Not shown in the above examples is the fact that Computational Alignment can also be used to move computations relative to data, allowing Computational Alignment to be the dual of data alignment. 1. Computational Alignment can drastically reduce the number of ownership tests <ref> [31, 29, 30] </ref> required and enable a better computation partitioning. Just as data alignment moves the data relative to other data in order to make computations in the body execute on the same processor, Computational Alignment can move computations relative to other computations and achieve the same objective.
Reference: [31] <author> V. Bala, J. Ferrante, and L. Carter. </author> <title> Explicit data placement (xdp): A methodology for explicit compile-time representation and optimization of data movement. </title> <booktitle> In Proceedings of the 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <volume> volume 28, </volume> <pages> pages 139-149, </pages> <address> San Diego, CA, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Not shown in the above examples is the fact that Computational Alignment can also be used to move computations relative to data, allowing Computational Alignment to be the dual of data alignment. 1. Computational Alignment can drastically reduce the number of ownership tests <ref> [31, 29, 30] </ref> required and enable a better computation partitioning. Just as data alignment moves the data relative to other data in order to make computations in the body execute on the same processor, Computational Alignment can move computations relative to other computations and achieve the same objective. <p> As future work, we are extending the framework developed here to singular reference matrices and general non-unimodular alignments. We are integrating the techniques of Computational Alignment with flexible ownership rules and XDP <ref> [31] </ref> directives. We are also 36 developing a parameterized model of parallel machines so that the effectiveness of candidate linear transformations and Computational Alignments can be evaluated quantitatively.
Reference: [32] <author> W. Li and K. Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <booktitle> In Proceedings of the Fifth Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: A mismatch between the loop structure and the distributions results in unnecessary data movements and one way to alleviate the problem is to transform the references so as to reflect the distributions <ref> [32] </ref>. Computational Alignment is better than the only existing technique capable of doing this, namely Access Normalization [32], because it can transform subsets of references in the aligned computations. A final important feature of Computational Alignment is that it can be applied to computations in an imperfectly nested loop. <p> A mismatch between the loop structure and the distributions results in unnecessary data movements and one way to alleviate the problem is to transform the references so as to reflect the distributions <ref> [32] </ref>. Computational Alignment is better than the only existing technique capable of doing this, namely Access Normalization [32], because it can transform subsets of references in the aligned computations. A final important feature of Computational Alignment is that it can be applied to computations in an imperfectly nested loop.
Reference: [33] <author> F.P. Preparata and M.I. Shamos. </author> <title> Computational Geometry an Introduction. </title> <publisher> Springer-verlag, </publisher> <year> 1985. </year>
Reference-contexts: The run-time overhead due to empty iterations and guards can be considerable. In Section 7 we discuss an algorithm that provides tight loop bounds by computing the convex-hull <ref> [33] </ref> of the union of the aligned computation sets. This reduces the number of empty iterations to a minimum for perfectly nested transformed loop. <p> The basic idea in Algorithm CA-bounds-optimal () of Figure 9 is to find the convex hull <ref> [33] </ref> of the union of the aligned computations. From the definition of the convex hull, the bounds provided by the description of this convex hull are tight. <p> such that u 2 I 1 or u 2 I 0 2 6. for each bounding hyperplane (h (I) = 0) 2 H if h (u) 0 then fi fi [ (h (I) 0) else fi fi [ (h (I) 0) end for end the gift-wrapping or the beneath-beyond method <ref> [33] </ref>. The convex hull, i.e. set H, is the set of bounding hyperplanes of the form h (I) = 0. A given h (I) is either a lower bound or an upper bound depending on which side of the hyperplane an arbitrary point in the union lies.
Reference: [34] <author> V. Balasundaram and Ken Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 24, </volume> <pages> pages 41-53, </pages> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The algorithm CA-bounds-optimal () is also useful in the context of computing the Data Access Descriptors <ref> [34] </ref> as summaries of the data access in a loop or a procedure. It is also useful in other cases that require the computation of unions such as array privatization [35, 36]. <p> These subclasses play an important role in global optimization as well. Other contributions of the paper include an algorithm to compute tight loop bounds which can be used to obtain more accurate data access descriptors <ref> [34] </ref> and to improve techniques for inter-procedural analysis and array privatization [35]. The flexibility and power of Computational Alignment comes, however, with additional execution overhead for empty iterations and guards when perfect nests are desired.
Reference: [35] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings of Sixth Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <year> 1993. </year>
Reference-contexts: The algorithm CA-bounds-optimal () is also useful in the context of computing the Data Access Descriptors [34] as summaries of the data access in a loop or a procedure. It is also useful in other cases that require the computation of unions such as array privatization <ref> [35, 36] </ref>. In both cases, our algorithm provides tighter descriptions than the existing algorithms. 7.2 Guard Optimizations The algorithm CA-bounds-optimal () discussed above produces the tight convex loop bounds for the aligned program. Although this reduces the number of empty iterations, guards are still required for two reasons. <p> These subclasses play an important role in global optimization as well. Other contributions of the paper include an algorithm to compute tight loop bounds which can be used to obtain more accurate data access descriptors [34] and to improve techniques for inter-procedural analysis and array privatization <ref> [35] </ref>. The flexibility and power of Computational Alignment comes, however, with additional execution overhead for empty iterations and guards when perfect nests are desired. We presented techniques that compute tight bounds and generate guard-free code for arbitrary alignment functions, but causing imperfect nesting in the process.
Reference: [36] <author> P. Feautrier. </author> <title> Array expansion. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <year> 1988. </year>
Reference-contexts: The algorithm CA-bounds-optimal () is also useful in the context of computing the Data Access Descriptors [34] as summaries of the data access in a loop or a procedure. It is also useful in other cases that require the computation of unions such as array privatization <ref> [35, 36] </ref>. In both cases, our algorithm provides tighter descriptions than the existing algorithms. 7.2 Guard Optimizations The algorithm CA-bounds-optimal () discussed above produces the tight convex loop bounds for the aligned program. Although this reduces the number of empty iterations, guards are still required for two reasons.
Reference: [37] <author> W. Li and K. Pingali. </author> <title> Access normalization: loop restructuring for numa compilers. </title> <booktitle> In 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1992. </year>
Reference-contexts: In related work, Allen et al. discuss loop alignment, a transformation that is similar to FCA in eliminating dependences [12]. However the scope of their work is limited to obtaining independent partitions in a single loop, and does not have the algebraic framework provided with Computational Alignment. Access Normalization <ref> [37] </ref> is similar to ICA with ff set to the identity matrix, and treating the entire loop body uniformly.
Reference: [38] <author> J. Torres, E. Ayguade, J. Labarta, and M. Valero. </author> <title> Align and distribute-based linear loop transformations. </title> <booktitle> In Proceedings of Sixth Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <year> 1993. </year> <month> 40 </month>
Reference-contexts: The subclass ICA is more effective than Access Normalization, because it can selectively transform a subset of the references instead of all of them, and because it can handle both alignment and distribution specifications. Torres et al. <ref> [38] </ref> independently formalized a notion similar to FCA, but they only consider alignments that are shifts. Our framework is much more general in that we consider arbitrary alignments. In contrast to our framework, their loop bounds are not tight, and the code is not guard-free.
References-found: 38

