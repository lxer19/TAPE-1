URL: http://www.cs.bu.edu/techreports/93-013-gb-parser.ps.Z
Refering-URL: http://cs-www.bu.edu/techreports/Home.html
Root-URL: 
Email: shaban@cs.bu.edu  
Title: A Minimal GB Parser needed in a GB parser, and has fairly good coverage of
Author: Marwan Shaban 
Note: The parser is minimal in the sense that it implements the major principles  
Address: 111 Cummington Street Boston, MA 02215  
Affiliation: Computer Science Department Boston University  
Abstract: October 26, 1993 BU-CS Tech Report # 93-013 Abstract: We describe a GB parser implemented along the lines of those written by Fong [4] and Dorr [2]. The phrase structure recovery component is an implementation of Tomita's generalized LR parsing algorithm (described in [10]), with recursive control flow (similar to Fong's implementation). The major principles implemented are government, binding, bounding, trace theory, case theory, -theory, and barriers. The particular version of GB theory we use is that described by Haegeman [5]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> Abney, Steven, Ed.: </editor> <booktitle> The MIT Parsing Volume, </booktitle> <address> 1987-1988; Cambridge, </address> <institution> MIT Center for Cognitive Science, </institution> <year> 1988. </year>
Reference: [2] <author> Dorr, Bonnie: UNITRAN: </author> <title> A Principle-Based Approach to Machine Translation; Cambridge, </title> <type> MIT AI Technical Report No. 1000, </type> <year> 1987. </year>
Reference-contexts: implementing our own parser was to garner experience in writing a principle-based parser, and to have in hand a parser that can be easily modified for future research. 2 Overall Architecture of The Parser The implemented parser follows closely the GB parsing model pioneered in the late 1980s by Dorr <ref> [2] </ref>, Fong [4], Wehrli [11], Kashket [6], and others. <p> Theta roles are not transmitted from antecedents to traces since the linguistic theory asserts that theta role assignment occurs at d-structure, thus the assignment takes place before movement (Dorr <ref> [2] </ref>). 6.3 The Theta Criterion The theta criterion well-formedness check is applied to all hypothesized parse trees.
Reference: [3] <author> Dorr, Bonnie: </author> <title> Lexical Conceptual Structure and Machine Translation; Cambridge, Mass., </title> <type> Ph.D. Dissertation, </type> <institution> Department of Electrical Engineering and Computer Science, MIT, </institution> <year> 1990. </year>
Reference-contexts: Lexical information about case is used to assign inherent case. 2. Structural case is assigned according to these rules (from <ref> [3] </ref>): * Objective case is assigned to the object governed by transitive P or V (passive verbs, however, cannot assign case). * Possessive case is assigned to the object governed by transitive N (Currently, no transitive nouns are handled by the parser). * Nominative case is assigned to the subject governed
Reference: [4] <author> Fong, Sandiway. </author> <title> Computational Properties of Principle-Based Grammatical Theories. Cambridge, Mass., </title> <type> Ph.D. Dissertation, </type> <institution> Department of Electrical Engineering and Computer Science, MIT, </institution> <year> 1991. </year>
Reference-contexts: own parser was to garner experience in writing a principle-based parser, and to have in hand a parser that can be easily modified for future research. 2 Overall Architecture of The Parser The implemented parser follows closely the GB parsing model pioneered in the late 1980s by Dorr [2], Fong <ref> [4] </ref>, Wehrli [11], Kashket [6], and others. <p> Apart from certain inter-module dependencies, these modules can be applied in any order. A good study of the ordering of principles within a GB parser was done by Fong <ref> [4] </ref>. Our current parser is "minimal" in the sense that it contains enough components to achieve broad coverage of natural language, and contains most of the components ("principles") usually associated with the current linguistic theory. Our parser uses fairly standard definitions of the principles. <p> This helps to weed out bad parses early. 9 The Binding Module The binding module performs the following tasks: * Takes each tree and expands it to include all possible NP coindexations, producing one or more possible parse trees. Fong <ref> [4] </ref> gave an account of the characteristics (and complexity) of the NP coindexation problem. <p> In order to handle the presence of such rules in the s-structure grammar, we have implemented the fix used by Fong (described in <ref> [4] </ref>). The LR parser is augmented with an "environment" stack which holds information about the parse in progress. This new stack is used in two ways: 1.
Reference: [5] <author> Haegeman, Liliane: </author> <title> Introduction to Government & Binding Theory. </title> <publisher> Oxford, Basil Blackwell, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction The purpose of this paper is to describe the architecture of a GB (Government-Binding) parser which was recently implemented. The reader is assumed to be familiar with some version of the linguistic theory of GB. Our primary sources for the linguistic theory were Haegeman's text <ref> [5] </ref>, and the overview by Sells [7]. Appendix D shows the English corpus which was used to test the English language coverage of the parser. Each sentence in the corpus is marked as to whether it is currently parsed correctly. <p> Every maximal projection dominating fi dominates ff. The c-command relation is defined as follows (from <ref> [5] </ref>): A node ff c-commands a node fi iff: 1. ff does not dominate fi. 2. fi does not dominate ff. 3. The first branching node dominating ff also dominates fi. We did not implement the above definitions directly. <p> Go through the subtree again, and set the government-related features of the governed nodes. 4.2 Theta Government Theta government relations are needed when setting up barriers (see below). The definition of Theta Government is as follows (from <ref> [5] </ref>): When a V governs an element and assigns an internal theta role to it we say that it theta-governs this element. The theta module (see below) sets up theta government while processing internal theta roles. 4.3 Barriers Barriers are needed when setting up government relations. <p> The theta module (see below) sets up theta government while processing internal theta roles. 4.3 Barriers Barriers are needed when setting up government relations. The definition of a barrier is as follows (from <ref> [5] </ref>): ff is a barrier for fi iff at least one of the following two conditions is met: 1. ff is a maximal projection and ff immediately dominates fl, fl is a BC (blocking category) for fi. 2. ff is a BC for fi, ff is not IP. <p> Blocking categories (BCs) are defined as follows (from <ref> [5] </ref>): fl is a BC for fi iff fl is not L-marked and fl dominates fi. L-marking is defined as follows (from [5]): 7 ff L-marks fi iff ff is a lexical category that theta-governs fi. <p> Blocking categories (BCs) are defined as follows (from <ref> [5] </ref>): fl is a BC for fi iff fl is not L-marked and fl dominates fi. L-marking is defined as follows (from [5]): 7 ff L-marks fi iff ff is a lexical category that theta-governs fi. Three passes are performed on each hypothesized parse tree to set up L-marking relations, blocking categories, and barrier nodes. <p> Three passes are performed on each hypothesized parse tree to set up L-marking relations, blocking categories, and barrier nodes. Currently, the barrier information within a tree is only used in setting up government relations, although it could also be used in the bounding module as well (as discussed in <ref> [5] </ref>). 4.4 Government Government is defined as follows (from [5]): X governs Y iff: 1. <p> Currently, the barrier information within a tree is only used in setting up government relations, although it could also be used in the bounding module as well (as discussed in <ref> [5] </ref>). 4.4 Government Government is defined as follows (from [5]): X governs Y iff: 1. X is either (a) one of the categies A, N, V, P, or I; (b) or, X and Y are coindexed (i.e. the governor is allowed to be a maximal projection if it is coindexed with the governed element. <p> The minimality condition on government is not implemented in the current system. Because of this, certain incorrect parses are generated by the system. For example, the parser accepts "Who do you think that came" (see discussion of that-trace effect in <ref> [5] </ref>, pp. 456-7). 8 4.5 Proper Government Proper government is used in processing the Empty Category Principle (ECP, see below). <p> Otherwise, we insert "PRO". * If the empty NP is not governed by the Inflection (I) node, then it is either an NP-trace or a wh-trace. After movement chains are constructed, we can distinguish between wh-traces and NP-traces using the following facts found in Haegeman <ref> [5] </ref>: An np-trace must not have case. A wh-trace must not have case, unless its antecedent is an NP, in which case it must have case. The antecedent of an np-trace must be an NP. The antecedent of a wh-trace can be any maximal projection type. <p> We distinguish between intermediate traces and traces that head a movement chain. In our system, the ECP does not apply to intermediate traces. This simplification is mostly valid. Haegeman <ref> [5] </ref> (on p. 465) claims that having intermediate nodes immune to the ECP produces almost correct results. <p> In fact, Haegeman couldn't give an example of a non-sentence that fails purely on the basis of an intermediate trace being ruled out by the ECP. 7.3 Control The only aspect of the theory of Control that we implement is the condition that "PRO" must be ungoverned (Haegeman <ref> [5] </ref>, p. 251). 8 The Bounding Module The bounding module performs the following tasks: * Takes each tree and expands it to include all possible movement chain combinations, producing one or more possible parse trees. <p> Because of this, ungrammat-ical sentences such as "Who do you think that came" are accepted by the parser. For an analysis of this sentence and a discussion of the "that-trace effect", see Haegeman <ref> [5] </ref>, pp. 456-7. 20 * The sentence "Poirot said that he is ill" produces three parses, including an incorrect parse.
Reference: [6] <author> Kashket, Michael. </author> <title> Parsing Warlpiri, a Free-Word Order Language in The MIT Parsing Volume 1987-1988. </title> <address> Cambridge, Mass., </address> <institution> MIT Center for Cognitive Science, </institution> <year> 1988. </year> <month> 31 </month>
Reference-contexts: garner experience in writing a principle-based parser, and to have in hand a parser that can be easily modified for future research. 2 Overall Architecture of The Parser The implemented parser follows closely the GB parsing model pioneered in the late 1980s by Dorr [2], Fong [4], Wehrli [11], Kashket <ref> [6] </ref>, and others.
Reference: [7] <author> Sells, Peter: </author> <title> Lectures on Contemporary Syntactic Theories; Stanford, Cen--ter for the Study of Language and Information, </title> <year> 1985. </year>
Reference-contexts: The reader is assumed to be familiar with some version of the linguistic theory of GB. Our primary sources for the linguistic theory were Haegeman's text [5], and the overview by Sells <ref> [7] </ref>. Appendix D shows the English corpus which was used to test the English language coverage of the parser. Each sentence in the corpus is marked as to whether it is currently parsed correctly. <p> Several types of government are distinguished, each having a separate role in the overall parsing process. 4.1 Head Government Head-Government is set up using the following definition (from <ref> [7] </ref>): ff head-governs fi iff: 1. ff c-commands fi. 2. ff is a minimal projection. 3. Every maximal projection dominating fi dominates ff.
Reference: [8] <author> Tenny, Carol, Ed.: </author> <title> The MIT Parsing Volume, </title> <address> 1988-1989; Cambridge, </address> <institution> MIT Center for Cognitive Science, </institution> <month> January </month> <year> 1990. </year>
Reference: [9] <author> Tenny, Carol, Ed.: </author> <title> The MIT Parsing Volume, </title> <address> 1989-1990; Cambridge, </address> <institution> MIT Center for Cognitive Science, </institution> <month> July </month> <year> 1990. </year>
Reference: [10] <author> Tomita, Masaru: </author> <title> Efficient Parsing for Natural Language; Boston, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1986. </year>
Reference-contexts: Fong uses an LR (1) parser, Dorr uses a modified Earley Algorithm, and Wehrli uses a bottom-up chart parser. We decided to implement Tomita's modified LR (1) parsing algorithm, which is the algorithm that Fong uses for his phrase structure recovery. Tomita <ref> [10] </ref> makes the claim that his algorithm 5 is more efficient than Earley's algorithm in the case where the grammar being used is "close" to LR. 2 As a simple measure of how close our present English s-structure grammar is to being LR, 12% of its action table entries have multiple <p> `T', this node is L-marked. * BLOCKING-CATEGORY? : If this feature's value is `T', this node is a blocking category. * TRANSMITTED-FEATURES : Tells what features of this node have been transmitted (the possible values of this feature are "case" and "theta") 13 *-grammar Handling Tomita's algorithm, as described in <ref> [10] </ref>, isn't able to handle arbitrary grammars containing *-productions. In order to handle the presence of such rules in the s-structure grammar, we have implemented the fix used by Fong (described in [4]). The LR parser is augmented with an "environment" stack which holds information about the parse in progress.
Reference: [11] <author> Wehrli, Eric: </author> <title> A Government-Binding Parser for French; Geneva, </title> <institution> University of Geneva, </institution> <year> 1984. </year> <month> 32 </month>
Reference-contexts: was to garner experience in writing a principle-based parser, and to have in hand a parser that can be easily modified for future research. 2 Overall Architecture of The Parser The implemented parser follows closely the GB parsing model pioneered in the late 1980s by Dorr [2], Fong [4], Wehrli <ref> [11] </ref>, Kashket [6], and others.
References-found: 11

