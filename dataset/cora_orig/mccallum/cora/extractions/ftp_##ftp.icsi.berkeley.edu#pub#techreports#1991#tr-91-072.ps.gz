URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1991/tr-91-072.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1991.html
Root-URL: http://www.icsi.berkeley.edu
Title: SPERT: A VLIW/SIMD Microprocessor for Artificial Neural Network Computations  
Author: Krste Asanovic James Beck Brian E. D. Kingsbury Phil Kohn Nelson Morgan John Wawrzynek 
Address: Berkeley, Berkeley, CA 94720  
Affiliation: EECS Department, University of California at  
Date: January 1992  
Pubnum: TR-91-072  
Abstract: SPERT (Synthetic PERceptron Testbed) is a fully programmable single chip microprocessor designed for efficient execution of artificial neural network algorithms. The first implementation will be in a 1.2 m CMOS technology with a 50MHz clock rate, and a prototype system is being designed to occupy a double SBus slot within a Sun Sparcstation. SPERT will sustain over 300 fi 10 6 connections per second during pattern classification, and around 100 fi 10 6 connection updates per second while running the popular error backpropagation training algorithm. This represents a speedup of around two orders of magnitude over a Sparcstation-2 for algorithms of interest. An earlier system produced by our group, the Ring Array Processor (RAP), used commercial DSP chips. Compared with a RAP multiprocessor of similar performance, SPERT represents over an order of magnitude reduction in cost for problems where fixed-point arithmetic is satisfactory. fl International Computer Science Institute, 1947 Center Street, Berkeley, CA 94704
Abstract-found: 1
Intro-found: 1
Reference: [AM91] <author> Krste Asanovic and Nelson Morgan. </author> <title> Experimental Determination of Precision Requirements for Back-Propagation Training of Artificial Neural Networks. </title> <booktitle> In Proceedings 2nd International Conference on Microelectronics for Neural Networks, </booktitle> <address> Mu-nich, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: The RAP was used to simulate reduced precision arithmetic while training large networks on large, real world datasets <ref> [AM91] </ref>. Connection weights dominate storage and memory bandwidth requirements, so we first investigated the effects of reducing only weight precision. The tests were performed by adapting an existing program to call a weight quantization routine after each training pattern.
Reference: [ASPF92] <author> Krste Asanovic, Klaus Erik Schauser, David A. Patterson, and Edward H. Frank. </author> <title> Evaluation of a Stall Cache: An Efficient Restricted On-Chip Instruction Cache. </title> <booktitle> In Proceedings 25th Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: However, typical SPERT applications are dominated by small inner loops, and a relatively small on-chip instruction cache gives excellent performance. The instruction cache holds 16 instructions and is direct mapped with a separate tag per cached instruction. The instruction cache on SPERT has been organized as a stall cache <ref> [ASPF92] </ref> to take advantage of the single cycle external memory. During instruction fetch, the stall cache bypasses the instruction cache whenever the instruction currently in the memory access stage of the instruction pipeline does not use the memory port, and fetches the instruction direct from memory.
Reference: [BH88] <author> Tom Baker and Dan Hammerstrom. </author> <title> Modifications to Artificial Neural Network Models for Digital Hardware Implementation. </title> <type> Technical Report CS/E 88-035, </type> <institution> Department of Computer Science and Engineering, Oregon Graduate Center, </institution> <year> 1988. </year>
Reference-contexts: A second set of experiments varied unit output precision with 25b and 16b weights, and demonstrated that 4-8 bits of output precision were sufficient for training and classification. These results are consistent with other studies in a number of different application areas <ref> [BH88] </ref>. Based on these precision experiments, our requirements for the arithmetic units in SPERT were that they provide fast 16bfi8b fixed-point multiplication, with efficient handling of larger (24-32b) intermediary results. We chose a uniform register and datapath width of 32b for SPERT.
Reference: [Ham90] <author> Dan Hammerstrom. </author> <title> A VLSI architecture for High-Performance, Low-Cost, On-Chip Learning. </title> <booktitle> In Proc. International Joint Conference on Neural Networks, </booktitle> <pages> pages II-537-543, </pages> <year> 1990. </year>
Reference-contexts: Some parallel systems train multiple copies of a network, then periodically pool the weight updates. This can lead to reduced training performance, counteracting the benefits of increased parallelism. 8 Related Work Several related efforts are underway to construct programmable digital neurocomputers, most notably the CNAPS chip from Adaptive Solutions <ref> [Ham90] </ref> and the MA-16 chip from Siemens [RBR + 91]. Adaptive Solutions provides a SIMD array with 64 processing elements per chip, in a system with four chips on a board controlled by a common microcode sequencer.
Reference: [KAW + 91] <author> Brian E. D. Kingsbury, Krste Asanovic, John Wawrzynek, Bertrand Irissou, and Nelson Morgan. </author> <title> Recent Work in VLSI Elements for Digital Implementations of Artificial Neural Networks. </title> <type> Technical Report TR-91-074, </type> <institution> International Computer Science Institute, </institution> <year> 1991. </year>
Reference-contexts: The multiplier design has been completed and is out for fabrication. The custom cells we are developing are being made available as Lager library cells. A full description of the VLSI cell library work is available in <ref> [KAW + 91] </ref>. 5 SPERT SBus Board We are designing an SBus board around the SPERT chip. The board will contain a SPERT chip clocked at 33MHz, 2 MB of 20ns SRAM, and the SBus to JTAG interface.
Reference: [Koh91] <author> P. Kohn. </author> <title> CLONES: Connectionist Layered Object-oriented NEtwork Simulator. </title> <type> Technical Report TR-91-073, </type> <institution> International Computer Science Institute, </institution> <year> 1991. </year>
Reference-contexts: The Connectionist Layered Object-oriented NEtwork Simulator (CLONES) is based on this interface and supports training and interconnect of a variety of network types including arbitrarily shaped backpropagation networks <ref> [Koh91] </ref>. The matrix vector library is in turn implemented on top of another shared interface: the Common Server Interface (or CSI). Object classes (such as matrix and vector) that insulate the user from the hardware configuration are built on top of the CSI class.
Reference: [MB90] <author> N. Morgan and H. Bourlard. </author> <title> Continuous speech recognition using Multilayer Perceptrons with Hidden Markov models. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, & Signal Processing, </booktitle> <pages> pages 413-416, </pages> <address> Albuquerque, New Mexico, USA, </address> <year> 1990. </year>
Reference-contexts: Special emphasis is being placed on support for variants of the commonly used backpropagation training algorithm for multi-layer feed-forward networks [RHW86, Wer74]. This class of networks is of interest in the speech recognition task that is the focus of our applications work <ref> [MB90] </ref>. In this work, networks are trained to estimate phonetic probabilities for use in a Hidden Markov Model based continuous speech recognizer.
Reference: [MBAB90] <author> N. Morgan, J. Beck, E. Allman, and J. Beer. </author> <title> RAP: A Ring Array Processor for Multilayer Perceptron Applications. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, & Signal Processing, </booktitle> <pages> pages 1005-1008, </pages> <address> Albuquerque, New Mexico, USA, </address> <year> 1990. </year>
Reference-contexts: In earlier work, our group designed a programmable machine called the Ring Array Processor (RAP) for general backpropagation training of layered feed-forward networks <ref> [MBAB90, MBKB92] </ref>. This used multiple TMS320C30 DSP chips [Tex88] connected in a ring array. SPERT's performance is comparable to that of a 40 node RAP multiprocessor.
Reference: [MBKB92] <author> N. Morgan, J. Beck, P. Kohn, and J. Bilmes. </author> <title> Neurocomputing on the RAP. </title> <editor> In K. W. Przytula and V. K. Prasanna, editors, </editor> <title> Digital Parallel Implementations of Neural Networks. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year> <note> In Press. </note>
Reference-contexts: In earlier work, our group designed a programmable machine called the Ring Array Processor (RAP) for general backpropagation training of layered feed-forward networks <ref> [MBAB90, MBKB92] </ref>. This used multiple TMS320C30 DSP chips [Tex88] connected in a ring array. SPERT's performance is comparable to that of a 40 node RAP multiprocessor.
Reference: [RBR + 91] <author> U. Ramacher, J. Beichter, W. Raab, J. Anlauf, N. Bruls, M. Hachmann, and M. </author> <title> Wesseling. Design of a 1st Generation Neurocomputer. In VLSI Design of Neural Networks. </title> <publisher> Kluwer Academic, </publisher> <year> 1991. </year> <month> 10 </month>
Reference-contexts: This can lead to reduced training performance, counteracting the benefits of increased parallelism. 8 Related Work Several related efforts are underway to construct programmable digital neurocomputers, most notably the CNAPS chip from Adaptive Solutions [Ham90] and the MA-16 chip from Siemens <ref> [RBR + 91] </ref>. Adaptive Solutions provides a SIMD array with 64 processing elements per chip, in a system with four chips on a board controlled by a common microcode sequencer. As with SPERT, processing elements are similar to general purpose DSPs with reduced precision multipliers.
Reference: [RHW86] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning Internal Representations by Error Propagation. In Parallel Distributed Processing. </title> <journal> Exploration of the Microstructure of Cognition, </journal> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The architecture is fully programmable, and executes a wide range of connectionist computations efficiently. Special emphasis is being placed on support for variants of the commonly used backpropagation training algorithm for multi-layer feed-forward networks <ref> [RHW86, Wer74] </ref>. This class of networks is of interest in the speech recognition task that is the focus of our applications work [MB90]. In this work, networks are trained to estimate phonetic probabilities for use in a Hidden Markov Model based continuous speech recognizer.
Reference: [Tex88] <institution> Texas Instruments, Houston, Texas, USA. </institution> <note> Third-Generation TMS320 User's Guide, </note> <year> 1988. </year>
Reference-contexts: In earlier work, our group designed a programmable machine called the Ring Array Processor (RAP) for general backpropagation training of layered feed-forward networks [MBAB90, MBKB92]. This used multiple TMS320C30 DSP chips <ref> [Tex88] </ref> connected in a ring array. SPERT's performance is comparable to that of a 40 node RAP multiprocessor. Such a RAP system occupies ten 9U VME boards, while in contrast the initial SPERT system design will be a double SBus card that will fit into a workstation.
Reference: [Waw92] <author> J. Wawrzynek. </author> <title> A 250MHz 64-bit Datap-ath in 1.2 m CMOS. </title> <note> Technical Report In Preparation, </note> <institution> Computer Science Division (EECS), University of California, Berke-ley, </institution> <year> 1992. </year>
Reference-contexts: Our VLSI group has been experimenting with the "True Single Phase Clocking" (TSPC) proposed in [YS89] for CMOS circuits, gaining experience with the technique for larger and more complex systems <ref> [Waw92] </ref>. These designs result in reduced complexity, higher density, and higher speeds than conventional CMOS clocking methodologies. In TSPC designs, there is a single clock signal distributed to two complementary latch types; one is transparent on clock low, the other on clock high.
Reference: [Wer74] <author> P.J. Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Dept. of Applied Mathematics, Harvard University, </institution> <year> 1974. </year>
Reference-contexts: The architecture is fully programmable, and executes a wide range of connectionist computations efficiently. Special emphasis is being placed on support for variants of the commonly used backpropagation training algorithm for multi-layer feed-forward networks <ref> [RHW86, Wer74] </ref>. This class of networks is of interest in the speech recognition task that is the focus of our applications work [MB90]. In this work, networks are trained to estimate phonetic probabilities for use in a Hidden Markov Model based continuous speech recognizer.
Reference: [YS89] <author> Jiren Yuan and Christer Svensson. </author> <title> High-Speed CMOS Circuit Technique. </title> <journal> IEEE JSSC, </journal> <volume> 24(1) </volume> <pages> 62-70, </pages> <month> February </month> <year> 1989. </year> <month> 11 </month>
Reference-contexts: Our VLSI group has been experimenting with the "True Single Phase Clocking" (TSPC) proposed in <ref> [YS89] </ref> for CMOS circuits, gaining experience with the technique for larger and more complex systems [Waw92]. These designs result in reduced complexity, higher density, and higher speeds than conventional CMOS clocking methodologies.
References-found: 15

