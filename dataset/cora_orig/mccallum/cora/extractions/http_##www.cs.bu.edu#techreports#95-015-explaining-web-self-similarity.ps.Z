URL: http://www.cs.bu.edu/techreports/95-015-explaining-web-self-similarity.ps.Z
Refering-URL: http://www.cs.gatech.edu/~zou/cs7100/proposal3.html
Root-URL: 
Email: fcrovella,bestg@cs.bu.edu  
Title: Explaining World Wide Web Traffic Self-Similarity  
Author: Mark E. Crovella and Azer Bestavros 
Date: October 12, 1995  Revised  
Address: 111 Cummington St, Boston, MA 02215  
Affiliation: Computer Science Department Boston University  
Pubnum: Technical Report TR-95-015  
Abstract: Recently the notion of self-similarity has been shown to apply to wide-area and local-area network traffic. In this paper we examine the mechanisms that give rise to self-similar network traffic. We present an explanation for traffic self-similarity by using a particular subset of wide area traffic: traffic due to the World Wide Web (WWW). Using an extensive set of traces of actual user executions of NCSA Mosaic, reflecting over half a million requests for WWW documents, we show evidence that WWW traffic is self-similar. Then we show that the self-similarity in such traffic can be explained based on the underlying distributions of WWW document sizes, the effects of caching and user preference in file transfer, the effect of user think time, and the superimposition of many such transfers in a local area network. To do this we rely on empirically measured distributions both from our traces and from data independently collected at over thirty WWW sites. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jan Beran. </author> <title> Statistics for Long-Memory Processes. Monographs on Statistics and Applied Probability. </title> <publisher> Chapman and Hall, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: In addition, using our measurements of user inter-request times, we explore reasons for the heavy-tailed distribution of quiet times needed for self-similarity. 2 Background 2.1 Definition of Self-Similarity For detailed discussion of self-similarity in timeseries data and the accompanying statistical tests, see <ref> [1, 27] </ref>. Our discussion in this subsection and the next closely follows those sources. <p> These methods are described fully in <ref> [1] </ref> and are the same methods described and used in [15]. A summary of the relative accuracy of these methods on synthetic datasets is presented in [24]. The first method, the variance-time plot, relies on the slowly decaying variance of a self-similar series. <p> The two forms that are most commonly used are fractional Gaussian noise (FGN) with parameter 1=2 &lt; H &lt; 1, and Fractional ARIMA (p; d; q) with 0 &lt; d &lt; 1=2 (for details see <ref> [1, 3] </ref>). These two models differ in their assumptions about the short-range dependences in the datasets; FGN assumes no short-range dependence while Fractional ARIMA can assume a fixed degree of short-range dependence.
Reference: [2] <author> T. Berners-Lee, L. Masinter, and M.McCahill. </author> <title> Uniform resource locators. </title> <type> RFC 1738, </type> <note> http://www.ics.uci.edu/pub/ietf/uri/rfc1738.txt, December 1994. </note>
Reference-contexts: A complete description of our data collection methods and the format of the log files is given in [6]; here we only give a high-level summary. We modified Mosaic to record the Uniform Resource Locator (URL) <ref> [2] </ref> of each file accessed by the Mosaic user, as well as the time the file was accessed and the time required to transfer the file from its server (if necessary).
Reference: [3] <author> Peter J. Brockwell and Richard A. Davis. </author> <title> Time Series: Theory and Methods. </title> <booktitle> Springer Series in Statistics. </booktitle> <publisher> Springer-Verlag, </publisher> <address> second edition, </address> <year> 1991. </year>
Reference-contexts: The two forms that are most commonly used are fractional Gaussian noise (FGN) with parameter 1=2 &lt; H &lt; 1, and Fractional ARIMA (p; d; q) with 0 &lt; d &lt; 1=2 (for details see <ref> [1, 3] </ref>). These two models differ in their assumptions about the short-range dependences in the datasets; FGN assumes no short-range dependence while Fractional ARIMA can assume a fixed degree of short-range dependence.
Reference: [4] <author> Lara D. Catledge and James E. Pitkow. </author> <title> Characterizing browsing strategies in the World-Wide Web. </title> <booktitle> In Proceedings of the Third WWW Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Previous measurement studies of the Web have focused on reference patterns established based on logs of proxies [9, 23], or servers [21]. The authors in <ref> [4] </ref> captured client traces, but they concentrated on events at the user interface level in order to study browser and page design. In contrast, our goal in data collection was to acquire a complete picture of the reference behavior and timing of user accesses to the WWW.
Reference: [5] <institution> Netscape Communications Corp. </institution> <note> Netscape Navigator software. Available from http:- //www.netscape.com. </note>
Reference-contexts: At the time of our study (January and February 1995) Mosaic was the WWW browser preferred by nearly all users at our site. Hence our data consists of nearly all of the WWW traffic at our site. Since the time of our study, the preferred browser has become Netscape <ref> [5] </ref>, which is not available in source form. As a result, capturing an equivalent set of WWW user traces at the current time would be significantly more difficult. The data captured consists of the sequence of WWW file requests performed during each session.
Reference: [6] <author> Carlos R. Cunha, Azer Bestavros, and Mark E. Crovella. </author> <title> Characteristics of WWW client-based traces. </title> <type> Technical Report BU-CS-95-010, </type> <institution> Boston University Computer Science Department, </institution> <year> 1995. </year>
Reference-contexts: The browser we used was Mosaic, since its source was publicly available and permission has been granted for using and modifying the code for research purposes. A complete description of our data collection methods and the format of the log files is given in <ref> [6] </ref>; here we only give a high-level summary. We modified Mosaic to record the Uniform Resource Locator (URL) [2] of each file accessed by the Mosaic user, as well as the time the file was accessed and the time required to transfer the file from its server (if necessary). <p> In fact, in other work <ref> [6] </ref> we show that the rule known as Zipf's Law (degree of popularity is exactly inversely proportional to rank of popularity) applies quite strongly to Web documents. The heavy-tailed distribution of user think times also seems to be a feature of human information processing (e.g., [21]).
Reference: [7] <author> Richard A. Floyd. </author> <title> Short-term file reference patterns in a UNIX environment. </title> <type> Technical Report 177, </type> <institution> Computer Science Dept., U. Rochester, </institution> <year> 1986. </year>
Reference-contexts: In the approximate range of 30,000 to 300,000 bytes, tail weight is increased mainly by audio files. Beyond 300,000 bytes, tail weight is increased mainly by video files. Heavy-tailed file size distributions have been noted before, particularly in filesystem studies <ref> [7, 18] </ref>, although measurements of ff values have been absent. As an example, we compare the distribution of Web files found in our logs with an overall distribution of files found in a survey of Unix filesystems.
Reference: [8] <institution> National Center for Supercomputing Applications. </institution> <note> Mosaic software. Available at ftp:- //ftp.ncsa.uiuc.edu/Mosaic. </note>
Reference-contexts: To study the traffic patterns of the WWW we collected reference data reflecting actual WWW use at our site. We instrumented NCSA Mosaic <ref> [8] </ref> to capture user access patterns to the Web.
Reference: [9] <author> Steven Glassman. </author> <title> A Caching Relay for the World Wide Web. </title> <booktitle> In First International Conference on the World-Wide Web, CERN, </booktitle> <address> Geneva (Switzerland), May 1994. </address> <publisher> Elsevier Science. </publisher>
Reference-contexts: Previous measurement studies of the Web have focused on reference patterns established based on logs of proxies <ref> [9, 23] </ref>, or servers [21]. The authors in [4] captured client traces, but they concentrated on events at the user interface level in order to study browser and page design.
Reference: [10] <author> B. M. Hill. </author> <title> A simple general approach to inference about the tail of a distribution. </title> <journal> The Annals of Statistics, </journal> <volume> 3 </volume> <pages> 1163-1174, </pages> <year> 1975. </year>
Reference-contexts: In all our ff estimates for file sizes we use = 1000 meaning that we consider tails to be the portions of the distributions for files of 1,000 bytes or greater. An alternative approach to estimating tail weight, used in [28], is the Hill estimator <ref> [10] </ref>. The Hill estimator does not give a single estimate of ff, but can be used to gauge the general range of ffs that are reasonable.
Reference: [11] <institution> Merit Network Inc. NSF Network statistics. Available at ftp://nis.nsf.net/statistics /nsfnet/, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: Our data complements those studies by providing a view of WWW (HTTP) traffic at a stub network. Since WWW traffic accounts for more than 25% of the traffic on the Internet and is currently growing more rapidly than any other traffic type <ref> [11] </ref>, understanding the nature of WWW traffic is important and is expected to increase in importance. The benchmark study of self-similarity in network traffic is [13, 15], and our study uses many of the same methods used in that work.
Reference: [12] <author> Gordon Irlam. </author> <title> Unix file size survey 1993. </title> <note> Available at http://www.base.com/gordoni /ufs93.html, </note> <month> September </month> <year> 1994. </year>
Reference-contexts: While there is no truly typical Unix file system, an aggregate picture of file sizes on over 1000 different Unix file systems is reported in <ref> [12] </ref>. In Figure 11 we compare the distribution of document sizes we found in the Web with that data. The Figure plots the two histograms on the same, log-log scale. <p> First, the generalization from Web traffic to aggregated wide-area traffic is not obvious. While other authors have noted the heavy tailed distribution of Unix files <ref> [12] </ref> and of FTP transfers [20], extending our approach to wide-area traffic in general is difficult because of the many sources of traffic and determiners of traffic demand. A second question concerns the amount of demand required to observe self-similarity in a traffic series.
Reference: [13] <author> W. Leland, M. Taqqu, W. Willinger, and D. Wilson. </author> <title> On the self-similar nature of Ethernet traffic. </title> <booktitle> In Proceedings of SIGCOMM '93, </booktitle> <pages> pages 183-193, </pages> <month> September </month> <year> 1993. </year> <month> 18 </month>
Reference-contexts: The benchmark study of self-similarity in network traffic is <ref> [13, 15] </ref>, and our study uses many of the same methods used in that work. However, the goal of that study was to demonstrate the self-similarity of network traffic; to do that, many large datasets taken from a multi-year span were used. <p> While these four hours are self-similar, many less-busy hours in our logs do not show self-similar characteristics. We feel that this is only the result of the traffic demand present in our logs, which is much lower than that used in <ref> [13, 15] </ref>; this belief is supported by the findings in that study, which showed that the intensity of self-similarity 5 increases as the aggregate traffic level increases. Our work is most similar in intent to [28].
Reference: [14] <author> W. E. Leland and D. V. Wilson. </author> <title> High time-resolution measurement and analysis of LAN traffic: Implications for LAN interconnection. </title> <booktitle> In Proceeedings of IEEE Infocomm '91, </booktitle> <pages> pages 1360-1366, </pages> <address> Bal Harbour, FL, </address> <year> 1991. </year>
Reference-contexts: Surprisingly (given the counterintuitive aspects of long-range dependence) the self-similarity of Ethernet network traffic has been rigorously established [15]. The importance of long-range dependence in network traffic is beginning to be observed in studies such as <ref> [14] </ref>, which show that packet loss and delay behavior is radically different in simulations using real traffic data rather than traditional network models. 1 However, the reasons behind self-similarity in network traffic have not been clearly identified.
Reference: [15] <author> W.E. Leland, M.S. Taqqu, W. Willinger, and D.V. Wilson. </author> <title> On the self-similar nature of Ethernet traffic (extended version). </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 2 </volume> <pages> 1-15, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Understanding the nature of network traffic is critical in order to properly design and implement computer networks and network services like the World Wide Web. Recent examinations of LAN traffic <ref> [15] </ref> and wide area network traffic [20] have challenged the commonly assumed models for network traffic, e.g., the Poisson distribution. <p> Since a self-similar process has observable bursts on all time scales, it exhibits long-range dependence; values at any instant are typically correlated with values at all future instants. Surprisingly (given the counterintuitive aspects of long-range dependence) the self-similarity of Ethernet network traffic has been rigorously established <ref> [15] </ref>. <p> To bridge the gap between studying network traffic on one hand and high-level system characteristics on the other, we make use of two essential tools. First, to explain self-similar network traffic in terms of individual transmission lengths, we employ the mechanism introduced in [16] and described in <ref> [15] </ref>. That paper points out that self-similar traffic can be constructed by multiplexing a large number of ON/OFF sources that have ON and OFF period lengths that are heavy-tailed, as defined in Section 2.3. <p> The paper takes two parts. First, we establish the self-similarity of Web traffic for the busiest hours we measured. To do so we use analyses very similar to those performed in <ref> [15] </ref>. These analyses strongly support the notion that Web traffic is self-similar, at least when demand is high enough. This result in itself has implications for designers of systems that attempt to improve performance characteristics of the WWW. <p> These methods are described fully in [1] and are the same methods described and used in <ref> [15] </ref>. A summary of the relative accuracy of these methods on synthetic datasets is presented in [24]. The first method, the variance-time plot, relies on the slowly decaying variance of a self-similar series. <p> The benchmark study of self-similarity in network traffic is <ref> [13, 15] </ref>, and our study uses many of the same methods used in that work. However, the goal of that study was to demonstrate the self-similarity of network traffic; to do that, many large datasets taken from a multi-year span were used. <p> While these four hours are self-similar, many less-busy hours in our logs do not show self-similar characteristics. We feel that this is only the result of the traffic demand present in our logs, which is much lower than that used in <ref> [13, 15] </ref>; this belief is supported by the findings in that study, which showed that the intensity of self-similarity 5 increases as the aggregate traffic level increases. Our work is most similar in intent to [28]. <p> The estimates of H given by these plots are in the range 0.7 to 0.8, consistent with the values for a lightly loaded network measured in <ref> [15] </ref>. Moving from the busier hours to the less-busy hours, the estimates of H seem to decline somewhat, and the variance in the estimate of H increases, which are also conclusions consistent with previous research. <p> This section provides an explanation, based on measured characteristics of the Web. 5.1 Superimposing Heavy-Tailed Renewal Processes Our starting point is the method of constructing self-similar processes described by Mandelbrot [16] and Taqqu and Levy [26] and summarized in <ref> [15] </ref>. A self-similar process may be constructed by superimposing many simple renewal reward processes, in which the rewards are restricted to the values 0 and 1, and in which the inter-renewal times are heavy-tailed.
Reference: [16] <author> Benoit B. Mandelbrot. </author> <title> Long-run linearity, locally Gaussian processes, H-spectra and infinite variances. </title> <journal> Intern. Econom. Rev., </journal> <volume> 10 </volume> <pages> 82-113, </pages> <year> 1969. </year>
Reference-contexts: To bridge the gap between studying network traffic on one hand and high-level system characteristics on the other, we make use of two essential tools. First, to explain self-similar network traffic in terms of individual transmission lengths, we employ the mechanism introduced in <ref> [16] </ref> and described in [15]. That paper points out that self-similar traffic can be constructed by multiplexing a large number of ON/OFF sources that have ON and OFF period lengths that are heavy-tailed, as defined in Section 2.3. <p> This section provides an explanation, based on measured characteristics of the Web. 5.1 Superimposing Heavy-Tailed Renewal Processes Our starting point is the method of constructing self-similar processes described by Mandelbrot <ref> [16] </ref> and Taqqu and Levy [26] and summarized in [15]. A self-similar process may be constructed by superimposing many simple renewal reward processes, in which the rewards are restricted to the values 0 and 1, and in which the inter-renewal times are heavy-tailed.
Reference: [17] <author> Benoit B. Mandelbrot. </author> <title> The Fractal Geometry of Nature. </title> <editor> W. H. </editor> <publisher> Freedman and Co., </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Thus, as ff decreases, an arbitrarily large portion of the probability mass may be present in the tail of the distribution. In practical terms a random variable that follows a heavy-tailed distribution can give rise to extremely large values with non-negligible probability (see [20] and <ref> [17] </ref> for details and examples). 2.3.1 Estimating Tail Weight Our results are based on estimating the values of ff for a number of empirically measured distributions, such as the lengths of World Wide Web file transmission times. To do so, we employ log-log complementary distribution (LLCD) plots. <p> The heavy-tailed distribution of file sizes we have observed seems similar in spirit to other Pareto distributions noted in the social sciences, such as the distribution of lengths of books on library shelves, and the distribution of word lengths in sample texts (for a discussion of these effects, see <ref> [17] </ref> and citations therein). In fact, in other work [6] we show that the rule known as Zipf's Law (degree of popularity is exactly inversely proportional to rank of popularity) applies quite strongly to Web documents.
Reference: [18] <author> John K. Ousterhout, Herve Da Costa, David Harrison, John A. Kunze, Michael Kupfer, and James G. Thompson. </author> <title> A trace-driven analysis of the UNIX 4.2BSD file system. </title> <type> Technical Report CSD-85-230, </type> <institution> Dept. of Computer Science, University of California at Berkeley, </institution> <year> 1985. </year>
Reference-contexts: In the approximate range of 30,000 to 300,000 bytes, tail weight is increased mainly by audio files. Beyond 300,000 bytes, tail weight is increased mainly by video files. Heavy-tailed file size distributions have been noted before, particularly in filesystem studies <ref> [7, 18] </ref>, although measurements of ff values have been absent. As an example, we compare the distribution of Web files found in our logs with an overall distribution of files found in a survey of Unix filesystems.
Reference: [19] <author> Vern Paxson. </author> <title> Empirically-derived analytic models of wide-area TCP connections. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 2(4) </volume> <pages> 316-336, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: used the Hill estimator to check that the estimates of ff obtained using the LLCD method were within range; in all cases they were. 2.3.2 Testing for Infinite Variance There is evidence that, over their entire range, many of the distributions we study may be well characterized using lognormal distributions <ref> [19] </ref>. However, lognormal distributions do not have infinite variance, and hence are not heavy-tailed. In our work, we are not concerned with distributions over their entire range only their tails. <p> As a result, we collected a large dataset of client-based traces. A full description of our traces (which are available for anonymous FTP) is given in [citation omitted]. Previous wide-area traffic studies have studied FTP, TELNET, NNTP, and SMTP traffic <ref> [19, 20] </ref>. Our data complements those studies by providing a view of WWW (HTTP) traffic at a stub network.
Reference: [20] <author> Vern Paxson and Sally Floyd. </author> <title> Wide-area traffic: The failure of poisson modeling. </title> <booktitle> In Proceedings of SIGCOMM '94, </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction Understanding the nature of network traffic is critical in order to properly design and implement computer networks and network services like the World Wide Web. Recent examinations of LAN traffic [15] and wide area network traffic <ref> [20] </ref> have challenged the commonly assumed models for network traffic, e.g., the Poisson distribution. Were traffic to follow a Poisson or Markovian arrival process, it would have a characteristic burst length which would tend to be smoothed by averaging over a long enough time scale. <p> Thus, as ff decreases, an arbitrarily large portion of the probability mass may be present in the tail of the distribution. In practical terms a random variable that follows a heavy-tailed distribution can give rise to extremely large values with non-negligible probability (see <ref> [20] </ref> and [17] for details and examples). 2.3.1 Estimating Tail Weight Our results are based on estimating the values of ff for a number of empirically measured distributions, such as the lengths of World Wide Web file transmission times. To do so, we employ log-log complementary distribution (LLCD) plots. <p> As a result, we collected a large dataset of client-based traces. A full description of our traces (which are available for anonymous FTP) is given in [citation omitted]. Previous wide-area traffic studies have studied FTP, TELNET, NNTP, and SMTP traffic <ref> [19, 20] </ref>. Our data complements those studies by providing a view of WWW (HTTP) traffic at a stub network. <p> Interestingly, the authors in <ref> [20] </ref> found that the upper tail of the distribution of data bytes in FTP bursts was well fit to a Pareto distribution with 0:9 ff 1:1. <p> First, the generalization from Web traffic to aggregated wide-area traffic is not obvious. While other authors have noted the heavy tailed distribution of Unix files [12] and of FTP transfers <ref> [20] </ref>, extending our approach to wide-area traffic in general is difficult because of the many sources of traffic and determiners of traffic demand. A second question concerns the amount of demand required to observe self-similarity in a traffic series.
Reference: [21] <author> James E. Pitkow and Margaret M. Recker. </author> <title> A Simple Yet Robust Caching Algorithm Based on Dynamic Access Patterns. </title> <booktitle> In Electronic Proc. of the 2nd WWW Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Previous measurement studies of the Web have focused on reference patterns established based on logs of proxies [9, 23], or servers <ref> [21] </ref>. The authors in [4] captured client traces, but they concentrated on events at the user interface level in order to study browser and page design. <p> The heavy-tailed distribution of user think times also seems to be a feature of human information processing (e.g., <ref> [21] </ref>).
Reference: [22] <institution> Regents of the University of California. </institution> <note> www-stat 1.0 software. Available from http:- //www.ics.uci.edu/WebSoft/wwwstat/. </note>
Reference-contexts: For comparison purposes we surveyed 32 Web servers scattered throughout North America. These servers were chosen because they provided a usage report based on www-stat 1.0 <ref> [22] </ref>. These usage reports provide information sufficient to determine the distribution of file sizes on the server (for files accessed during the reporting period). In each case we obtained the most recent usage reports (as of July 1995), for an entire month if possible.
Reference: [23] <author> Jeff Sedayao. </author> <title> Mosaic Will Kill My Network! Studying Network Traffic Patterns of Mosaic Use. </title> <booktitle> In Electronic Proceedings of the Second World Wide Web Conference '94: Mosaic and the Web, </booktitle> <address> Chicago, Illinois, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Previous measurement studies of the Web have focused on reference patterns established based on logs of proxies <ref> [9, 23] </ref>, or servers [21]. The authors in [4] captured client traces, but they concentrated on events at the user interface level in order to study browser and page design.
Reference: [24] <author> M. S. Taqqu, V. Teverovsky, and W. Willinger. </author> <title> Estimators for long-range dependence: an empirical study, 1995. </title> <type> Preprint. </type>
Reference-contexts: These methods are described fully in [1] and are the same methods described and used in [15]. A summary of the relative accuracy of these methods on synthetic datasets is presented in <ref> [24] </ref>. The first method, the variance-time plot, relies on the slowly decaying variance of a self-similar series.
Reference: [25] <author> Murad Taqqu. </author> <type> Personal communication. </type>
Reference: [26] <author> Murad S. Taqqu and Joshua B. Levy. </author> <title> Using renewal processes to generate long-range dependence and high variability. </title> <editor> In Ernst Eberlein and Murad S. Taqqu, editors, </editor> <booktitle> Dependence in Probability and Statistics, </booktitle> <pages> pages 73-90. </pages> <publisher> Birkhauser, </publisher> <year> 1986. </year>
Reference-contexts: This section provides an explanation, based on measured characteristics of the Web. 5.1 Superimposing Heavy-Tailed Renewal Processes Our starting point is the method of constructing self-similar processes described by Mandelbrot [16] and Taqqu and Levy <ref> [26] </ref> and summarized in [15]. A self-similar process may be constructed by superimposing many simple renewal reward processes, in which the rewards are restricted to the values 0 and 1, and in which the inter-renewal times are heavy-tailed.
Reference: [27] <author> Walter Willinger, Murad S. Taqqu, Will E. Leland, and Daniel V. Wilson. </author> <title> Self-similarity in high-speed packet traffic: Analysis and modeling of Ethernet traffic measurements. </title> <journal> Statistical Science, </journal> <volume> 10(1) </volume> <pages> 67-85, </pages> <year> 1995. </year>
Reference-contexts: In addition, using our measurements of user inter-request times, we explore reasons for the heavy-tailed distribution of quiet times needed for self-similarity. 2 Background 2.1 Definition of Self-Similarity For detailed discussion of self-similarity in timeseries data and the accompanying statistical tests, see <ref> [1, 27] </ref>. Our discussion in this subsection and the next closely follows those sources. <p> Thus, for self-similar series, 1=2 &lt; H &lt; 1. As H ! 1, the degree of self-similarity increases. Thus the fundamental test for self-similarity of a series reduces to the question of whether H is significantly different from 1=2. The effect of self-similarity in network traffic is shown in <ref> [27] </ref>, which compares a self-similar series with a compound Poisson series with the same distributional characteristics.
Reference: [28] <author> Walter Willinger, Murad S. Taqqu, Robert Sherman, and Daniel V. Wilson. </author> <title> Self-similarity through high-variability: Statistical analysis of Ethernet LAN traffic at the source level. </title> <booktitle> In Proceedings of SIGCOMM '95, </booktitle> <pages> pages 100-113, </pages> <address> Boston, MA, </address> <year> 1995. </year> <month> 19 </month>
Reference-contexts: In all our ff estimates for file sizes we use = 1000 meaning that we consider tails to be the portions of the distributions for files of 1,000 bytes or greater. An alternative approach to estimating tail weight, used in <ref> [28] </ref>, is the Hill estimator [10]. The Hill estimator does not give a single estimate of ff, but can be used to gauge the general range of ffs that are reasonable. <p> Our work is most similar in intent to <ref> [28] </ref>. That paper looked at network traffic at the packet level, identified the flows between individual source/destination pairs, and showed that transmission and idle times for those flows were heavy-tailed. In contrast, our paper is based on data collected at the application level rather than the network level. <p> As a result we are able to examine the relationship between transmission times and file sizes, and are able to assess the effects of caching and user preference on these distributions. These observations allow us to build on the conclusions presented in <ref> [28] </ref> by showing that the heavy-tailed nature of transmission and idle times is not primarily a result of network protocols or user preference, but rather stems from more basic properties of information storage and processing: both file sizes and user think times are themselves strongly heavy-tailed. 4 Examining Web Traffic Self-Similarity
References-found: 28

