URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr93/tr93-018.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr93-abstracts.html
Root-URL: http://www.cis.ufl.edu
Title: An unsymmetric-pattern multifrontal method for sparse LU factorization  
Author: Timothy A. Davis Iain S. Duff 
Keyword: Matrix Analysis and Applications. Key words. LU factorization, unsymmetric sparse matrices, parallel algorithms, multifrontal methods  
Note: European Center for Research and Advanced Training in Scientific  Submitted to the SIAM Journal on  AMS (MOS) subject classifications. 65F50, 65W05, 65F05, 65-04, 68-04. Abbreviated title. Unsymmetric-pattern multifrontal  
Address: Gainesville, Florida, USA  Toulouse, France  Oxon. 0X11 0QX England.  
Affiliation: Computer and Information Sciences Department University of Florida,  Computation (CERFACS)  also Rutherford Appleton Laboratory, Chilton, Didcot,  
Email: email: davis@cis.ufl.edu.  
Phone: phone: (904) 392-1481,  
Date: March 1993  
Web: method  
Abstract: Technical Report TR-93-018 Computer and Information Sciences Department University of Florida Gainesville, FL, 32611 USA 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, M. R. Garey, and J. D. Ullman. </author> <title> The transitive reduction of a directed graph. </title> <journal> SIAM J. Comput., </journal> <volume> 1 </volume> <pages> 131-137, </pages> <year> 1972. </year>
Reference-contexts: In the outer-product formulation of Gaussian elimination, A is transformed into the product L [k] A [k] U [k] after step k 1 and just before step k (1 k n; A <ref> [1] </ref> = A; L [n+1] = L; U [n+1] = U ). Throughout this paper, the notation X [k] will refer to the state of X just before step k, where X is a matrix, set, graph, scalar, etc. <p> That is, F [k] = F L [ F U [k] n [k] o [k] n ht; column ji j j 2 U [k] o These edges are referred to as active L-edges and active U-edges, respectively. 4.1 The basic graph Before factorization starts, the dag G <ref> [1] </ref> is empty and A [1] is the bipartite graph of the original matrix, A. The pivot search selects a pivot and permutes it to the first row and column (renaming them as row and column one, in our notation). <p> F [k] = F L [ F U [k] n [k] o [k] n ht; column ji j j 2 U [k] o These edges are referred to as active L-edges and active U-edges, respectively. 4.1 The basic graph Before factorization starts, the dag G <ref> [1] </ref> is empty and A [1] is the bipartite graph of the original matrix, A. The pivot search selects a pivot and permutes it to the first row and column (renaming them as row and column one, in our notation). <p> The first node of G is constructed. This node refers to the newly factorized frontal matrix E 1 . The column pattern U 1 of E 1 is given by the set of edges in A <ref> [1] </ref> incident to row nodes 1 through g 1 . Similarly, the row pattern of E 1 (L 1 ) is given by the set of edges in A [1] incident to column nodes 1 through g 1 . <p> The column pattern U 1 of E 1 is given by the set of edges in A <ref> [1] </ref> incident to row nodes 1 through g 1 . Similarly, the row pattern of E 1 (L 1 ) is given by the set of edges in A [1] incident to column nodes 1 through g 1 . Pivot row and column nodes 1 through g 1 (having been renamed) and edges in A [1] incident to these nodes are then removed from A. <p> Similarly, the row pattern of E 1 (L 1 ) is given by the set of edges in A <ref> [1] </ref> incident to column nodes 1 through g 1 . Pivot row and column nodes 1 through g 1 (having been renamed) and edges in A [1] incident to these nodes are then removed from A. The pattern L 00 1 defines new active L-edges in F [g 1 +1] L that are added from node 1 of G to row nodes in A. <p> Transitive reduction <ref> [1] </ref> could remove more edges, but would be too costly. 4.3 Additional amalgamation No-fill amalgamation has already been described. Additional amalgamation can improve the unsymmetric-pattern multifrontal method by increasing the ratio of Level-3 BLAS to Level-2 BLAS operations and by simplifying the symbolic computations.
Reference: [2] <author> P. R. Amestoy and I. S. Duff. </author> <title> Vectorization of a multiprocessor multifrontal code. </title> <journal> Int. J. Supercomputer Appl., </journal> <volume> 3(3) </volume> <pages> 41-59, </pages> <year> 1989. </year>
Reference-contexts: Near the end of the factorization, the active submatrix is considered as a dense matrix (and factorized with dense matrix kernels). The multifrontal method of Duff and Reid (MA37) <ref> [2, 9, 10, 16] </ref> will be referred to as the classical multifrontal method. The method takes more advantage of dense matrix kernels than D2, but is unsuitable when the pattern of the matrix is very unsymmetric. <p> This would not occur if the pattern of A was symmetric. Row 7 of E 1 can be assembled into E 2 , since U <ref> [2] </ref> 1 U 2 , even though the contribution that E 1 makes to row 7 is not needed to factorize E 2 . <p> The process repeats until the desired assembly tree is obtained. Elimination and assembly trees are typically defined only for symmetric-patterned LU factors, with the exception of partial-pivoting methods [21, 22]. A more general graph is needed for the unsymmetric-pattern multifrontal method. The classical multifrontal method <ref> [2, 9, 10, 15, 16] </ref> is based on the assembly tree. It has a similar formulation as the general frontal matrix formulation described in the previous section, except that the analysis is performed on the pattern of A + A T . Frontal matrices are square. <p> The resulting G [g 1 +1] contains a single node. The factorization of the first frontal matrix satisfies steps 1 through g 1 of the LU factorization. The next step of LU factorization will be step g 1 + 1. An example of a graph D <ref> [2] </ref> is shown in Figure 2 for the matrix A in Equation 6 in Section 2.1. The first frontal matrix E 1 consists of only one pivot and is the first node in the graph G [2] . To avoid too many lines, edges in the implicit bipartite graph A [2] <p> An example of a graph D <ref> [2] </ref> is shown in Figure 2 for the matrix A in Equation 6 in Section 2.1. The first frontal matrix E 1 consists of only one pivot and is the first node in the graph G [2] . To avoid too many lines, edges in the implicit bipartite graph A [2] are shown 14 dag,G [2] bipartite graph, A [2] 15 in array form. <p> <ref> [2] </ref> is shown in Figure 2 for the matrix A in Equation 6 in Section 2.1. The first frontal matrix E 1 consists of only one pivot and is the first node in the graph G [2] . To avoid too many lines, edges in the implicit bipartite graph A [2] are shown 14 dag,G [2] bipartite graph, A [2] 15 in array form. <p> The first frontal matrix E 1 consists of only one pivot and is the first node in the graph G <ref> [2] </ref> . To avoid too many lines, edges in the implicit bipartite graph A [2] are shown 14 dag,G [2] bipartite graph, A [2] 15 in array form. An original edge between a row i and a column j is shown as an fi, a fill-in edge is shown as a , and a zero is shown as a small ffi (for which no edge in A exists). <p> The first frontal matrix E 1 consists of only one pivot and is the first node in the graph G <ref> [2] </ref> . To avoid too many lines, edges in the implicit bipartite graph A [2] are shown 14 dag,G [2] bipartite graph, A [2] 15 in array form. An original edge between a row i and a column j is shown as an fi, a fill-in edge is shown as a , and a zero is shown as a small ffi (for which no edge in A exists). The two graphs, G [2] and <p> A <ref> [2] </ref> 15 in array form. An original edge between a row i and a column j is shown as an fi, a fill-in edge is shown as a , and a zero is shown as a small ffi (for which no edge in A exists). The two graphs, G [2] and A [2] , are separated by a dashed line. Edges in F [k] cross this dashed line, while edges in E [k] will be placed to the upper-left of this line (E [2] is empty). <p> An original edge between a row i and a column j is shown as an fi, a fill-in edge is shown as a , and a zero is shown as a small ffi (for which no edge in A exists). The two graphs, G <ref> [2] </ref> and A [2] , are separated by a dashed line. Edges in F [k] cross this dashed line, while edges in E [k] will be placed to the upper-left of this line (E [2] is empty). <p> The two graphs, G <ref> [2] </ref> and A [2] , are separated by a dashed line. Edges in F [k] cross this dashed line, while edges in E [k] will be placed to the upper-left of this line (E [2] is empty). At a subsequent step k, a new pivot row i and column j are selected, and frontal matrix E k is created. Additional pivots with identical pattern are also included, so that E k contains pivots k through k + g k 1. <p> When E 2 is created, its column pattern U 2 = f2; 3; 4; 5; 7g is a superset of the pattern U 1 = f4; 5g because of fill-in. Row i = 7 is in both row patterns of L <ref> [2] </ref> 2 . Thus, the contribution that E 1 makes to row 7 is assembled into E 2 (before row 7 becomes pivotal). The active L-edge from node 1 to row 7 is assembled and deleted. Similarly, in Figure 4, node 4 2 G is a U-parent of node 2. <p> In this section, we compare their performance with the MA28 algorithm [14], sequential versions of the classical multifrontal method (Mups) <ref> [2] </ref>, and the D2 algorithm [5]. Table 1 summarizes our results [3] for eighty-six matrices from the Harwell/Boeing collection [12, 13] and other sources. Twenty-seven of these are symmetric positive-definite. Only matrices of order 500 or larger were considered. The Z matrices are chemical engineering problems from S.
Reference: [3] <author> T. A. Davis. </author> <title> Performance of an unsymmetric-pattern multifrontal method for sparse LU factorization. </title> <type> Technical Report TR-92-014, </type> <institution> Comp. and Info. Sci. Dept., Univ. </institution> <note> of Florida (anonymous ftp to cis.ufl.edu:cis/tech-reports/tr92/tr92-014.ps.Z), </note> <institution> Gainesville, </institution> <address> FL, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: In this section, we compare their performance with the MA28 algorithm [14], sequential versions of the classical multifrontal method (Mups) [2], and the D2 algorithm [5]. Table 1 summarizes our results <ref> [3] </ref> for eighty-six matrices from the Harwell/Boeing collection [12, 13] and other sources. Twenty-seven of these are symmetric positive-definite. Only matrices of order 500 or larger were considered. The Z matrices are chemical engineering problems from S. Zitney and others [29] (Z/m2 is from a PDE).
Reference: [4] <author> T. A. Davis and I. S. Duff. </author> <title> Unsymmetric-pattern multifrontal methods for parallel sparse LU factorization. </title> <type> Technical Report TR-91-023, </type> <institution> CIS Dept., Univ. </institution> <note> of Florida (anonymous ftp to cis.ufl.edu:cis/tech-reports/tr91/tr91-023.ps.Z), </note> <institution> Gainesville, </institution> <address> FL, </address> <year> 1991. </year>
Reference-contexts: If this assumption is not made, the frontal matrices are rectangular instead of square, a directed acyclic graph (called the assembly dag) replaces the assembly tree, and frontal matrices are no longer assembled by a single parent. A new unsymmetric-pattern multifrontal approach respecting these constraints is presented <ref> [4] </ref>. It builds the assembly dag either during factorization or in a preprocessing phase. As in the symmetric multifrontal case, advantage is taken of repetitive structure in the matrix by amalgamating nodes in the assembly dag. <p> ] D 0 [k+g k ] 2.1 Example matrix Consider the matrix A = 6 6 6 6 6 6 p 1 fi fi fi fi p 3 fi fi fi fi fi fi fi fi fi 7 7 7 7 7 7 (6) with the partially factorized matrix L <ref> [4] </ref> A [4] U [4] 2 6 6 6 6 6 4 fi p 2 fi fi fi fi fi p 4 fi fi fi 3 7 7 7 7 7 5 just before the fourth step of LU factorization. <p> 0 [k+g k ] 2.1 Example matrix Consider the matrix A = 6 6 6 6 6 6 p 1 fi fi fi fi p 3 fi fi fi fi fi fi fi fi fi 7 7 7 7 7 7 (6) with the partially factorized matrix L <ref> [4] </ref> A [4] U [4] 2 6 6 6 6 6 4 fi p 2 fi fi fi fi fi p 4 fi fi fi 3 7 7 7 7 7 5 just before the fourth step of LU factorization. <p> k ] 2.1 Example matrix Consider the matrix A = 6 6 6 6 6 6 p 1 fi fi fi fi p 3 fi fi fi fi fi fi fi fi fi 7 7 7 7 7 7 (6) with the partially factorized matrix L <ref> [4] </ref> A [4] U [4] 2 6 6 6 6 6 4 fi p 2 fi fi fi fi fi p 4 fi fi fi 3 7 7 7 7 7 5 just before the fourth step of LU factorization. <p> The active submatrix just before step 4 is A 4::7;4::7 = M <ref> [4] </ref> + E 1 + E 2 A 4::7;4::7 = 4 fi fi 6 fi fi + 4 fi fi + 5 fi fi fi : Note that E 3 is not generated, since it lies entirely within E 2 . If the entry a [4] 4;4 (labeled as p 4 <p> is A 4::7;4::7 = M <ref> [4] </ref> + E 1 + E 2 A 4::7;4::7 = 4 fi fi 6 fi fi + 4 fi fi + 5 fi fi fi : Note that E 3 is not generated, since it lies entirely within E 2 . If the entry a [4] 4;4 (labeled as p 4 ) is selected at step 4 as the fourth pivot, then the resulting element E 4 would be E 4 = 4 p 4 fi 7 fi fi Also note that element E 1 makes a contribution to the pivot row of E 4 which <p> Row 7 of E 1 can be assembled into E 2 , since U [2] 1 U 2 , even though the contribution that E 1 makes to row 7 is not needed to factorize E 2 . If this assembly is performed, row 7 is removed from L <ref> [4] </ref> 1 since the notation E [4] 1 refers only to portions of E 1 that are not yet assembled into subsequent frontal matrices just before step 4. <p> If this assembly is performed, row 7 is removed from L <ref> [4] </ref> 1 since the notation E [4] 1 refers only to portions of E 1 that are not yet assembled into subsequent frontal matrices just before step 4. <p> The active L-edges are a subset of the nonzero pattern of L [k] k::n;1::k1 , and the active U-edges are a subset of the nonzero pattern of U [k] has been factorized. The single inactive L-edge h1; 2i in G <ref> [4] </ref> is shown in bold (node 2 is an L-parent of its L-child node 1). <p> This L-edge is due to the nonzero entries l 2;1 and l 3;1 , and (at this point in the discussion) represents the assembly of two rows from the contribution block of E 1 into the pivot rows of E 2 . The resulting bipartite graph A <ref> [4] </ref> is also shown. U-edge h2; 4i in G [5] represents the assembly into E 4 of the contribution that E 2 makes to row 4. It is present because of the nonzeros u 2;4 and u 3;4 . <p> It is present because of the nonzeros l 4;1 and u 1;4 . In the next section these inactive edges will be used to represent the assembly of additional contributions, and some of the active edges in Figures 3 and 4 will be removed. 16 dag,G <ref> [4] </ref> bipartite graph, A [4] 17 dag,G [5] bipartite graph, A [5] 18 4.2 Edge reductions Potentially, every off-diagonal nonzero in the LU factors can define a single, unique edge in the assembly dag G. <p> In the next section these inactive edges will be used to represent the assembly of additional contributions, and some of the active edges in Figures 3 and 4 will be removed. 16 dag,G <ref> [4] </ref> bipartite graph, A [4] 17 dag,G [5] bipartite graph, A [5] 18 4.2 Edge reductions Potentially, every off-diagonal nonzero in the LU factors can define a single, unique edge in the assembly dag G. Some of the these potential edges are never created, due to the no-fill amalgamation described in the previous section.
Reference: [5] <author> T. A. Davis and P. C. Yew. </author> <title> A nondeterministic parallel algorithm for general unsym-metric sparse LU factorization. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11(3) </volume> <pages> 383-402, </pages> <year> 1990. </year>
Reference-contexts: As in the symmetric multifrontal case, advantage is taken of repetitive structure in the matrix by amalgamating nodes in the assembly dag. Thus the algorithm uses dense matrix kernels in its innermost loops, giving it high performance on parallel-vector supercomputers. 1.1 Previous work The D2 algorithm <ref> [5] </ref> is based on a non-deterministic parallel pivot search that constructs a set of independent pivots (m, say) followed by a parallel rank-m update of the active submatrix. Near the end of the factorization, the active submatrix is considered as a dense matrix (and factorized with dense matrix kernels). <p> The resulting bipartite graph A [4] is also shown. U-edge h2; 4i in G <ref> [5] </ref> represents the assembly into E 4 of the contribution that E 2 makes to row 4. It is present because of the nonzeros u 2;4 and u 3;4 . <p> In the next section these inactive edges will be used to represent the assembly of additional contributions, and some of the active edges in Figures 3 and 4 will be removed. 16 dag,G [4] bipartite graph, A [4] 17 dag,G <ref> [5] </ref> bipartite graph, A [5] 18 4.2 Edge reductions Potentially, every off-diagonal nonzero in the LU factors can define a single, unique edge in the assembly dag G. Some of the these potential edges are never created, due to the no-fill amalgamation described in the previous section. <p> In the next section these inactive edges will be used to represent the assembly of additional contributions, and some of the active edges in Figures 3 and 4 will be removed. 16 dag,G [4] bipartite graph, A [4] 17 dag,G <ref> [5] </ref> bipartite graph, A [5] 18 4.2 Edge reductions Potentially, every off-diagonal nonzero in the LU factors can define a single, unique edge in the assembly dag G. Some of the these potential edges are never created, due to the no-fill amalgamation described in the previous section. <p> In this section, we compare their performance with the MA28 algorithm [14], sequential versions of the classical multifrontal method (Mups) [2], and the D2 algorithm <ref> [5] </ref>. Table 1 summarizes our results [3] for eighty-six matrices from the Harwell/Boeing collection [12, 13] and other sources. Twenty-seven of these are symmetric positive-definite. Only matrices of order 500 or larger were considered. The Z matrices are chemical engineering problems from S.
Reference: [6] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and I. S. Duff. </author> <title> A set of level-3 basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: The assembly tree (a variant of the elimination tree [26]) controls parallelism across multiple frontal matrices, while dense matrix operations <ref> [6] </ref> provide parallelism and vectorization within each frontal matrix. However, this method is based on an assumption of a symmetric nonzero pattern, and so has a poor performance on matrices whose patterns are very unsymmetric. <p> The entire contribution block D j of a child can always be assembled into its parent, since all the rows and columns that are affected by D j are present in E k . The method takes advantage of the dense matrix kernels <ref> [6, 7] </ref> to factorize E k : the Level-2 BLAS for simple nodes (g k = 1) or the Level-3 BLAS for supernodes (g k &gt; 1). The classical multifrontal method is not the only method based on the elimination tree or its variants.
Reference: [7] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and R. J. Hanson. </author> <title> An extended set of Fortran basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 14 </volume> <pages> 1-32, </pages> <year> 1988. </year>
Reference-contexts: The entire contribution block D j of a child can always be assembled into its parent, since all the rows and columns that are affected by D j are present in E k . The method takes advantage of the dense matrix kernels <ref> [6, 7] </ref> to factorize E k : the Level-2 BLAS for simple nodes (g k = 1) or the Level-3 BLAS for supernodes (g k &gt; 1). The classical multifrontal method is not the only method based on the elimination tree or its variants.
Reference: [8] <author> I. S. Duff. </author> <title> On algorithms for obtaining a maximum transversal. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7 </volume> <pages> 315-330, </pages> <year> 1981. </year>
Reference-contexts: The remaining 44.4 million operations are performed in only 116 frontal matrices, (a typical one of which is 69-by-65 with 31 pivots). The largest frontal matrix constructed by AFstack is 216-by-281, with 171 pivots. For this matrix, Mups is directed to first find a maximum transversal <ref> [8] </ref>, otherwise excessive fill-in is obtained. MA28 and D2 both find a poor pivot ordering for rdist1, when compared with Mups and the AF algorithms.
Reference: [9] <author> I. S. Duff. </author> <title> Parallel implementation of multifrontal schemes. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 193-204, </pages> <year> 1986. </year>
Reference-contexts: Near the end of the factorization, the active submatrix is considered as a dense matrix (and factorized with dense matrix kernels). The multifrontal method of Duff and Reid (MA37) <ref> [2, 9, 10, 16] </ref> will be referred to as the classical multifrontal method. The method takes more advantage of dense matrix kernels than D2, but is unsuitable when the pattern of the matrix is very unsymmetric. <p> The process repeats until the desired assembly tree is obtained. Elimination and assembly trees are typically defined only for symmetric-patterned LU factors, with the exception of partial-pivoting methods [21, 22]. A more general graph is needed for the unsymmetric-pattern multifrontal method. The classical multifrontal method <ref> [2, 9, 10, 15, 16] </ref> is based on the assembly tree. It has a similar formulation as the general frontal matrix formulation described in the previous section, except that the analysis is performed on the pattern of A + A T . Frontal matrices are square.
Reference: [10] <author> I. S. Duff, A. M. Erisman, and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> London: Oxford Univ. Press, </publisher> <year> 1986. </year>
Reference-contexts: Near the end of the factorization, the active submatrix is considered as a dense matrix (and factorized with dense matrix kernels). The multifrontal method of Duff and Reid (MA37) <ref> [2, 9, 10, 16] </ref> will be referred to as the classical multifrontal method. The method takes more advantage of dense matrix kernels than D2, but is unsuitable when the pattern of the matrix is very unsymmetric. <p> The process repeats until the desired assembly tree is obtained. Elimination and assembly trees are typically defined only for symmetric-patterned LU factors, with the exception of partial-pivoting methods [21, 22]. A more general graph is needed for the unsymmetric-pattern multifrontal method. The classical multifrontal method <ref> [2, 9, 10, 15, 16] </ref> is based on the assembly tree. It has a similar formulation as the general frontal matrix formulation described in the previous section, except that the analysis is performed on the pattern of A + A T . Frontal matrices are square. <p> Three approaches for performing additional amalgamation are described below (the AFup, AFdown, and AFstack algorithms). Before using any of these algorithms, we perform an (optional) preprocessing stage that scales the matrix and permutes it to upper-block-triangular form (the diagonal blocks are factorized independently) <ref> [10] </ref>. The performance of these algorithms is presented in Section 8. 7.1 Data structures All data structures are allocated out of a pair of one-dimensional real and integer arrays. <p> The pivot search is assisted by a set of n linked lists. The d-th linked list holds those columns with upper bound degree d, that is fj j upper (c [k] The pivot a [k] ij at step k must also satisfy the threshold partial-pivoting criterion <ref> [10] </ref>: ja ij j u max ja sj j; 0 &lt; u 1: (16) The candidate pivot is the numerically acceptable entry a [k] ij with lowest approximate Mark owitz cost using the true column degree and the upper bound row degree, (upper (r [k] [k] The approximate degree update finds <p> The sherman5 and mahindasb matrices contain 1674 and 669 singletons, respectively, which are 1-by-1 diagonal blocks arising from a permutation to upper-block-triangular form <ref> [10] </ref>. These singletons are included in the count of frontal matrices for the AF algorithms. The dags are unconnected for these matrices. The number of delayed pivots in AFdown and AFup, although acceptable for many matrices, can be quite high.
Reference: [11] <author> I. S. Duff, N. I. M. Gould, J. K. Reid, J. A. Scott, and K. Turner. </author> <title> The factorization of sparse symmetric indefinite matrices. </title> <journal> IMA Journal of Numerical Analysis, </journal> <volume> 11 </volume> <pages> 181-204, </pages> <year> 1991. </year>
Reference-contexts: However, we can apply recent work of Duff and Reid [17] based on algorithms developed by Duff, Gould, Reid, Scott, and Turner <ref> [11] </ref> to obtain a suitable analysis. We discuss the use of their algorithms in this section. Methods based on the elimination dag are presented in [18, 23]. Duff et al. [11] design algorithms for factorizing symmetric indefinite matrices which use block pivots of order 1 or 2, chosen from the diagonal <p> can apply recent work of Duff and Reid [17] based on algorithms developed by Duff, Gould, Reid, Scott, and Turner <ref> [11] </ref> to obtain a suitable analysis. We discuss the use of their algorithms in this section. Methods based on the elimination dag are presented in [18, 23]. Duff et al. [11] design algorithms for factorizing symmetric indefinite matrices which use block pivots of order 1 or 2, chosen from the diagonal to preserve symmetry, and are suitable even when there are zero entries on the diagonal. <p> the augmented system " B 0 x # " b (14) where B is a nonsingular unsymmetric matrix of order n, the first n components of the solution are just the solution of the set of unsymmetric linear equations Bx = b: Furthermore, if the algorithm of Duff et al. <ref> [11] </ref> is used to choose pivots from the coefficient matrix of Equation 14, then n oxo pivots will be chosen. <p> Thus we can use the symmetric minimum degree ordering of Duff et al. <ref> [11] </ref> to obtain a symbolic analysis of an unsymmetric matrix. The code of Duff and Reid [17] will also produce the equivalent of our directed acyclic graph (G) which could, after suitable modification, be used as input to the factor-only algorithm of this paper.
Reference: [12] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 15 </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: In this section, we compare their performance with the MA28 algorithm [14], sequential versions of the classical multifrontal method (Mups) [2], and the D2 algorithm [5]. Table 1 summarizes our results [3] for eighty-six matrices from the Harwell/Boeing collection <ref> [12, 13] </ref> and other sources. Twenty-seven of these are symmetric positive-definite. Only matrices of order 500 or larger were considered. The Z matrices are chemical engineering problems from S. Zitney and others [29] (Z/m2 is from a PDE). The Hm matrices are circuit simulation matrices from S. Hamm (Motorola). <p> The new method (AFstack) is faster than Mups, D2 and MA28 for only 13 out of 86 matrices. However, these matrices include nearly all of the large unsymmetric matrices which are not extremely sparse. The present release of the Harwell/Boeing collection is very weak in this class <ref> [12, 13] </ref>. Also, the method demonstrates a consistent performance for the entire range of matrices, as can be observed in Figures 7 through 9. It usually takes no more than twice the time as Mups for symmetric-patterned matrices, and is even occasionally faster (for bcsstk08 and the two Hm/add matrices). <p> Steve Zitney at Cray and Steve Hamm at Motorola provided us with several large unsymmetric matrices, a class of matrices that is weak in the present version of the Harwell/Boeing collection <ref> [12, 13] </ref>. Portions of this work were supported by a post-doctoral grant from CERFACS, September 1989 to December 1990. Support for this project also provided by the National Science Foundation (ASC-9111263), and by Cray Research, Inc. and Florida State University through the allocation of supercomputer resources. 46
Reference: [13] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Users' guide for the Harwell-Boeing sparse matrix collection. </title> <type> Technical Report TR/PA/92/86, </type> <institution> CERFACS, Toulouse, France, </institution> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: In this section, we compare their performance with the MA28 algorithm [14], sequential versions of the classical multifrontal method (Mups) [2], and the D2 algorithm [5]. Table 1 summarizes our results [3] for eighty-six matrices from the Harwell/Boeing collection <ref> [12, 13] </ref> and other sources. Twenty-seven of these are symmetric positive-definite. Only matrices of order 500 or larger were considered. The Z matrices are chemical engineering problems from S. Zitney and others [29] (Z/m2 is from a PDE). The Hm matrices are circuit simulation matrices from S. Hamm (Motorola). <p> The new method (AFstack) is faster than Mups, D2 and MA28 for only 13 out of 86 matrices. However, these matrices include nearly all of the large unsymmetric matrices which are not extremely sparse. The present release of the Harwell/Boeing collection is very weak in this class <ref> [12, 13] </ref>. Also, the method demonstrates a consistent performance for the entire range of matrices, as can be observed in Figures 7 through 9. It usually takes no more than twice the time as Mups for symmetric-patterned matrices, and is even occasionally faster (for bcsstk08 and the two Hm/add matrices). <p> Steve Zitney at Cray and Steve Hamm at Motorola provided us with several large unsymmetric matrices, a class of matrices that is weak in the present version of the Harwell/Boeing collection <ref> [12, 13] </ref>. Portions of this work were supported by a post-doctoral grant from CERFACS, September 1989 to December 1990. Support for this project also provided by the National Science Foundation (ASC-9111263), and by Cray Research, Inc. and Florida State University through the allocation of supercomputer resources. 46
Reference: [14] <author> I. S. Duff and J. K. Reid. </author> <title> Some design features of a sparse matrix code. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 5(1) </volume> <pages> 18-35, </pages> <year> 1979. </year>
Reference-contexts: If the true degree is calculated during factorization, the two bounds are set equal to the true degree. Only the first few columns with minimum upper bound degree are searched (not unlike the truncated pivot search options in <ref> [14] </ref> and [31]), and the true degrees of these columns are computed. The pivot search is assisted by a set of n linked lists. <p> In this section, we compare their performance with the MA28 algorithm <ref> [14] </ref>, sequential versions of the classical multifrontal method (Mups) [2], and the D2 algorithm [5]. Table 1 summarizes our results [3] for eighty-six matrices from the Harwell/Boeing collection [12, 13] and other sources. Twenty-seven of these are symmetric positive-definite. Only matrices of order 500 or larger were considered.
Reference: [15] <author> I. S. Duff and J. K. Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 9(3) </volume> <pages> 302-325, </pages> <year> 1983. </year>
Reference-contexts: This assembly is performed by the edge reductions described in Section 4.2. 3 Elimination and assembly trees The elimination tree, T , [26] and its variants (such as the assembly tree <ref> [15] </ref>) are used either explicitly or implicitly in most parallel sparse matrix algorithms [25]: T = (T V ; T E ) T E = fhi; ji j j = parent (i)g 10 parent (i) = minfj j i &lt; j; l ji 6= 0g: Starting with the elimination tree, T <p> The process repeats until the desired assembly tree is obtained. Elimination and assembly trees are typically defined only for symmetric-patterned LU factors, with the exception of partial-pivoting methods [21, 22]. A more general graph is needed for the unsymmetric-pattern multifrontal method. The classical multifrontal method <ref> [2, 9, 10, 15, 16] </ref> is based on the assembly tree. It has a similar formulation as the general frontal matrix formulation described in the previous section, except that the analysis is performed on the pattern of A + A T . Frontal matrices are square.
Reference: [16] <author> I. S. Duff and J. K. Reid. </author> <title> The multifrontal solution of unsymmetric sets of linear equations. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 5(3) </volume> <pages> 633-641, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction Conventional sparse matrix factorization algorithms rely heavily on indirect addressing. This gives them an irregular memory access pattern that limits their performance on typical parallel-vector supercomputers. In contrast, the multifrontal method of Duff and Reid <ref> [16] </ref> is designed with regular memory access in the innermost loops. Its kernel is one or more steps of LU factorization within each square, dense frontal matrix defined by the nonzero pattern of a pivot row and column. <p> Near the end of the factorization, the active submatrix is considered as a dense matrix (and factorized with dense matrix kernels). The multifrontal method of Duff and Reid (MA37) <ref> [2, 9, 10, 16] </ref> will be referred to as the classical multifrontal method. The method takes more advantage of dense matrix kernels than D2, but is unsuitable when the pattern of the matrix is very unsymmetric. <p> The process repeats until the desired assembly tree is obtained. Elimination and assembly trees are typically defined only for symmetric-patterned LU factors, with the exception of partial-pivoting methods [21, 22]. A more general graph is needed for the unsymmetric-pattern multifrontal method. The classical multifrontal method <ref> [2, 9, 10, 15, 16] </ref> is based on the assembly tree. It has a similar formulation as the general frontal matrix formulation described in the previous section, except that the analysis is performed on the pattern of A + A T . Frontal matrices are square.
Reference: [17] <author> I. S. Duff and J. K. Reid. MA47, </author> <title> a Fortran code for direct solution of indefinite sparse symmetric linear systems. </title> <note> to appear, </note> <year> 1993. </year>
Reference-contexts: We did start to design such an analysis phase but were not convinced of its utility because of the problems with perturbing the data structures that we mention in Section 6. However, we can apply recent work of Duff and Reid <ref> [17] </ref> based on algorithms developed by Duff, Gould, Reid, Scott, and Turner [11] to obtain a suitable analysis. We discuss the use of their algorithms in this section. Methods based on the elimination dag are presented in [18, 23]. <p> Thus we can use the symmetric minimum degree ordering of Duff et al. [11] to obtain a symbolic analysis of an unsymmetric matrix. The code of Duff and Reid <ref> [17] </ref> will also produce the equivalent of our directed acyclic graph (G) which could, after suitable modification, be used as input to the factor-only algorithm of this paper.
Reference: [18] <author> S. C. Eisenstat and J. W. H. Liu. </author> <title> Exploiting structural symmetry in unsymmetric sparse symbolic factorization. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13(1) </volume> <pages> 202-211, </pages> <year> 1992. </year>
Reference-contexts: The method takes more advantage of dense matrix kernels than D2, but is unsuitable when the pattern of the matrix is very unsymmetric. Many methods for symmetric matrices use dense kernels; a survey may be found in [25]. Most recently, Gilbert and Liu [23] and Eisenstat and Liu <ref> [18] </ref> have presented symbolic factorization algorithms for unsymmetric matrices, assuming that the pivot ordering is known a priori. The algorithms are based on the elimination directed acyclic graph (dag) and its reductions, which are similar to the assembly dag presented in this paper. <p> If the assembly tree is unsuitable, what kind of graph can guide the unsymmetric-pattern multifrontal method? The elimination dag <ref> [18, 23] </ref> is one possibility. Define G (L) as the directed graph associated with L. That is, hi; ji is an edge of G (L) if and only if l ji is nonzero. <p> That is, hi; ji is an edge of G (L) if and only if l ji is nonzero. Similarly, hi; ji is an edge of G (U T ) if and only if u ij is nonzero. Several reductions to these graphs are described in <ref> [18, 23] </ref> (such as transitive reduction, which leads to the elimination dag). If the graphs are constructed for a symmetric-patterned matrix and a simple reduction is applied (namely, symmetric reduction), then the graphs become equivalent to the elimination tree. The next section extends the results in [18, 23] by allowing for <p> graphs are described in <ref> [18, 23] </ref> (such as transitive reduction, which leads to the elimination dag). If the graphs are constructed for a symmetric-patterned matrix and a simple reduction is applied (namely, symmetric reduction), then the graphs become equivalent to the elimination tree. The next section extends the results in [18, 23] by allowing for arbitrary sparsity preserving and numerically acceptable pivoting during the factorization. Further extensions are discussed in Section 7. 12 4 The assembly graph and assembly dag The assembly graph D = (A; G; F ) is constructed during the factorization as the pivot sequence is determined. <p> Some of the these potential edges are never created, due to the no-fill amalgamation described in the previous section. Additional edge reductions are described in this section. Some of these are in the style of Eisenstat, Liu, and Gilbert <ref> [18, 23] </ref>, except that we apply them to our partially constructed graph when the pivot order is not fully known. A class of edge reductions that goes beyond transitive reduction is applied during the approximate degree update phase described in Section 7.2. <p> However, we can apply recent work of Duff and Reid [17] based on algorithms developed by Duff, Gould, Reid, Scott, and Turner [11] to obtain a suitable analysis. We discuss the use of their algorithms in this section. Methods based on the elimination dag are presented in <ref> [18, 23] </ref>. Duff et al. [11] design algorithms for factorizing symmetric indefinite matrices which use block pivots of order 1 or 2, chosen from the diagonal to preserve symmetry, and are suitable even when there are zero entries on the diagonal.
Reference: [19] <author> A. George, M. T. Heath, J. W. H. Liu, and E. Ng. </author> <title> Solution of sparse positive definite systems on a shared-memory multiprocessor. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 15(4) </volume> <pages> 309-325, </pages> <year> 1986. </year>
Reference-contexts: The classical multifrontal method is not the only method based on the elimination tree or its variants. In the sparse column-Cholesky factorization of George et al. <ref> [19] </ref>, the work at node k in the elimination tree is the computation of column k of L. The work at node k modifies column k with columns corresponding to a subset of the descendants of node k in the elimination tree.
Reference: [20] <author> A. George and J. W. H. Liu. </author> <title> The evolution of the minimum degree ordering algorithm. </title> <journal> SIAM Review, </journal> <volume> 31(1) </volume> <pages> 1-19, </pages> <year> 1989. </year>
Reference-contexts: Frontal matrices are square. The method is usually divided into two phases: symbolic analysis and numerical factorization. The symbolic phase finds a suitable pivot ordering using a sparsity preserving heuristic such as minimum degree <ref> [20] </ref>, determines the assembly tree, and finds the patterns of L and U . Each node k in the assembly tree represents the work associated with element E k . The assembly tree describes the large-grain parallelism between the nodes.
Reference: [21] <author> A. George and E. Ng. </author> <title> Parallel sparse gaussian elimination with partial pivoting. </title> <journal> Annals of Operation Research, </journal> <volume> 22 </volume> <pages> 219-240, </pages> <year> 1990. </year>
Reference-contexts: The process repeats until the desired assembly tree is obtained. Elimination and assembly trees are typically defined only for symmetric-patterned LU factors, with the exception of partial-pivoting methods <ref> [21, 22] </ref>. A more general graph is needed for the unsymmetric-pattern multifrontal method. The classical multifrontal method [2, 9, 10, 15, 16] is based on the assembly tree.
Reference: [22] <author> J. R. Gilbert. </author> <title> An efficient parallel sparse partial pivoting algorithm. </title> <type> Technical Report 88/45052-1, </type> <institution> Center for Computer Science, Chr. Michelsen Institute, Bergen, Norway, </institution> <year> 1988. </year>
Reference-contexts: The process repeats until the desired assembly tree is obtained. Elimination and assembly trees are typically defined only for symmetric-patterned LU factors, with the exception of partial-pivoting methods <ref> [21, 22] </ref>. A more general graph is needed for the unsymmetric-pattern multifrontal method. The classical multifrontal method [2, 9, 10, 15, 16] is based on the assembly tree.
Reference: [23] <author> J. R. Gilbert and J. W. H. Liu. </author> <title> Elimination structures for unsymmetric sparse LU factors. </title> <type> Technical Report CS-90-11, </type> <institution> Dept. of Computer Sci., York Univ., </institution> <address> North York, Ontario, </address> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: The method takes more advantage of dense matrix kernels than D2, but is unsuitable when the pattern of the matrix is very unsymmetric. Many methods for symmetric matrices use dense kernels; a survey may be found in [25]. Most recently, Gilbert and Liu <ref> [23] </ref> and Eisenstat and Liu [18] have presented symbolic factorization algorithms for unsymmetric matrices, assuming that the pivot ordering is known a priori. The algorithms are based on the elimination directed acyclic graph (dag) and its reductions, which are similar to the assembly dag presented in this paper. <p> If the assembly tree is unsuitable, what kind of graph can guide the unsymmetric-pattern multifrontal method? The elimination dag <ref> [18, 23] </ref> is one possibility. Define G (L) as the directed graph associated with L. That is, hi; ji is an edge of G (L) if and only if l ji is nonzero. <p> That is, hi; ji is an edge of G (L) if and only if l ji is nonzero. Similarly, hi; ji is an edge of G (U T ) if and only if u ij is nonzero. Several reductions to these graphs are described in <ref> [18, 23] </ref> (such as transitive reduction, which leads to the elimination dag). If the graphs are constructed for a symmetric-patterned matrix and a simple reduction is applied (namely, symmetric reduction), then the graphs become equivalent to the elimination tree. The next section extends the results in [18, 23] by allowing for <p> graphs are described in <ref> [18, 23] </ref> (such as transitive reduction, which leads to the elimination dag). If the graphs are constructed for a symmetric-patterned matrix and a simple reduction is applied (namely, symmetric reduction), then the graphs become equivalent to the elimination tree. The next section extends the results in [18, 23] by allowing for arbitrary sparsity preserving and numerically acceptable pivoting during the factorization. Further extensions are discussed in Section 7. 12 4 The assembly graph and assembly dag The assembly graph D = (A; G; F ) is constructed during the factorization as the pivot sequence is determined. <p> Some of the these potential edges are never created, due to the no-fill amalgamation described in the previous section. Additional edge reductions are described in this section. Some of these are in the style of Eisenstat, Liu, and Gilbert <ref> [18, 23] </ref>, except that we apply them to our partially constructed graph when the pivot order is not fully known. A class of edge reductions that goes beyond transitive reduction is applied during the approximate degree update phase described in Section 7.2. <p> However, we can apply recent work of Duff and Reid [17] based on algorithms developed by Duff, Gould, Reid, Scott, and Turner [11] to obtain a suitable analysis. We discuss the use of their algorithms in this section. Methods based on the elimination dag are presented in <ref> [18, 23] </ref>. Duff et al. [11] design algorithms for factorizing symmetric indefinite matrices which use block pivots of order 1 or 2, chosen from the diagonal to preserve symmetry, and are suitable even when there are zero entries on the diagonal.
Reference: [24] <author> S. Hadfield and T. A. Davis. </author> <title> Analysis of potential parallel implementations of the unsymmetric-pattern multifrontal method for sparse LU factorization. </title> <type> Technical Report TR-92-017, </type> <institution> CIS Dept., Univ. </institution> <note> of Florida (anonymous ftp to cis.ufl.edu:cis/tech-reports/tr92/tr92-017.ps.Z), </note> <institution> Gainesville, </institution> <address> FL, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Compute the update terms with a rank-g i update to the contribution block D i , using the Level-2 BLAS if g i = 1, or Level-3 if g i &gt; 1. Parallelism can occur within these kernels, as well as between independent nodes <ref> [24] </ref>. 5. If node i is the last child to complete for a parent j then enable node j. Numerical pivoting considerations might not allow the expected number of pivots to be chosen from a pivot block F i .
Reference: [25] <author> M. T. Heath, E. Ng, and B. W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33(3) </volume> <pages> 420-460, </pages> <year> 1991. </year>
Reference-contexts: The method takes more advantage of dense matrix kernels than D2, but is unsuitable when the pattern of the matrix is very unsymmetric. Many methods for symmetric matrices use dense kernels; a survey may be found in <ref> [25] </ref>. Most recently, Gilbert and Liu [23] and Eisenstat and Liu [18] have presented symbolic factorization algorithms for unsymmetric matrices, assuming that the pivot ordering is known a priori. <p> This assembly is performed by the edge reductions described in Section 4.2. 3 Elimination and assembly trees The elimination tree, T , [26] and its variants (such as the assembly tree [15]) are used either explicitly or implicitly in most parallel sparse matrix algorithms <ref> [25] </ref>: T = (T V ; T E ) T E = fhi; ji j j = parent (i)g 10 parent (i) = minfj j i &lt; j; l ji 6= 0g: Starting with the elimination tree, T , an assembly tree is constructed by amalgamating a connected (node-induced) subgraph into
Reference: [26] <author> J. W. H. Liu. </author> <title> The role of elimination trees in sparse factorization. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11(1) </volume> <pages> 134-172, </pages> <year> 1990. </year>
Reference-contexts: These steps of LU factorization compute a submatrix of update terms that are held within the frontal matrix until they are assembled (added) into the frontal matrix of its parent in the assembly tree. The assembly tree (a variant of the elimination tree <ref> [26] </ref>) controls parallelism across multiple frontal matrices, while dense matrix operations [6] provide parallelism and vectorization within each frontal matrix. However, this method is based on an assumption of a symmetric nonzero pattern, and so has a poor performance on matrices whose patterns are very unsymmetric. <p> This assembly is performed by the edge reductions described in Section 4.2. 3 Elimination and assembly trees The elimination tree, T , <ref> [26] </ref> and its variants (such as the assembly tree [15]) are used either explicitly or implicitly in most parallel sparse matrix algorithms [25]: T = (T V ; T E ) T E = fhi; ji j j = parent (i)g 10 parent (i) = minfj j i &lt; j; l
Reference: [27] <author> H. M. Markowitz. </author> <title> The elimination form on the inverse and its application to linear programming. </title> <journal> Management Science, </journal> <volume> 3 </volume> <pages> 255-269, </pages> <month> Apr </month> <year> 1957. </year> <month> 48 </month>
Reference-contexts: data structures in the algorithm. 7.2 Pivot search, degree update, and edge reductions The three algorithms use a similar global pivot search strategy and degree update phase (although AFstack combines the numerical assembly with this phase), and perform similar edge reductions. 26 The pivot search is based on Markowitz' strategy <ref> [27] </ref>, which selects the pivot a [k] ij with minimum upper bound on fill-in (or cost), (r i 1)(c j 1): (15) The rows and columns of the active matrix are not held explicitly, rather, they are held as a set of contribution blocks and entries from the original matrix A.
Reference: [28] <author> P. Matstoms. </author> <title> Sparse QR factorization in MATLAB. </title> <type> Technical Report LiTH-MAT-R--1992-05, </type> <institution> Dept. of Mathematics, Linkoping Univ., Linkoping, Sweden, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: The algorithms are based on the elimination directed acyclic graph (dag) and its reductions, which are similar to the assembly dag presented in this paper. Moreover, we also indicate how our graphs can be applied to the case where the pivot ordering is not known a priori. Also, Matstoms <ref> [28] </ref> has recently developed a multifrontal QR factorization algorithm. 5 1.2 Outline The following sections present the unsymmetric-pattern multifrontal method and three sequential algorithms based on the method. Section 2 describes LU factorization in terms of general frontal matrices.
Reference: [29] <author> S. E. Zitney and M. A. Stadtherr. </author> <title> A frontal algorithm for equation-based chemical process flowsheeting on vector and parallel computers. </title> <booktitle> In Proc. AIChE Annual Meeting, </booktitle> <address> Washington, DC, </address> <year> 1988. </year>
Reference-contexts: Table 1 summarizes our results [3] for eighty-six matrices from the Harwell/Boeing collection [12, 13] and other sources. Twenty-seven of these are symmetric positive-definite. Only matrices of order 500 or larger were considered. The Z matrices are chemical engineering problems from S. Zitney and others <ref> [29] </ref> (Z/m2 is from a PDE). The Hm matrices are circuit simulation matrices from S. Hamm (Motorola). Table 1 lists the results for these matrices obtained on a Cray YMP-8/128, sorted by asymmetry (and by order if tied).
Reference: [30] <author> S.E. Zitney. </author> <title> A frontal code for aspen plus on advanced architecture computers. </title> <booktitle> In American Inst. of Chemical Eng. Annual Meeting, Symposium on Parallel Computing, </booktitle> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: It is usually less than the number of nonzeros in the LU factors (the exceptions are for very sparse matrices). This result demonstrates the effectiveness of our edge reductions. The Z/rdist1 matrix is a distillation column with 19 components and 100 stages <ref> [30] </ref>. Reactions occur in stages 35 to 70. Both Mups and AFstack can take advantage of its unsymmetric block structure, which occurs within each stage. For example, of the 44.5 million floating-point operations in AFstack, only 80,255 are done in frontal matrices with 1 or 2 pivots (428 fronts).
Reference: [31] <author> Z. Zlatev, J. Wasniewski, and K. Schaumburg. Y12M: </author> <title> Solution of Large and Sparse Systems of Linear Algebraic Equations, </title> <booktitle> Lecture Notes in Computer Science 121. </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag, </publisher> <year> 1981. </year> <month> 49 </month>
Reference-contexts: If the true degree is calculated during factorization, the two bounds are set equal to the true degree. Only the first few columns with minimum upper bound degree are searched (not unlike the truncated pivot search options in [14] and <ref> [31] </ref>), and the true degrees of these columns are computed. The pivot search is assisted by a set of n linked lists.
References-found: 31

