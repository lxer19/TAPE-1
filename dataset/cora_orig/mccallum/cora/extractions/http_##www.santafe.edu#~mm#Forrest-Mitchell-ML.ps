URL: http://www.santafe.edu/~mm/Forrest-Mitchell-ML.ps
Refering-URL: http://www.santafe.edu/~mm/paper-abstracts.html
Root-URL: 
Email: Email: forrest@unmvax.cs.unm.edu  Email: melaniem@eecs.umich..  
Author: Melanie Mitchell 
Address: Albuquerque, N.M. 87131-1386  Ann Arbor, MI 48109-2110  
Affiliation: Dept. of Computer Science University of New Mexico  Artificial Intelligence Laboratory University of Michigan  
Note: Stephanie Forrest  
Abstract: What Makes a Problem Hard for a Genetic Algorithm? Some Anomalous Results and Their Explanation Abstract What makes a problem easy or hard for a genetic algorithm (GA)? This question has become increasingly important as people have tried to apply the GA to ever more diverse types of problems. Much previous work on this question has studied the relationship between GA performance and the structure of a given fitness function when it is is expressed as a Walsh polynomial. The work of Bethke, Goldberg, and others has produced certain theoretical results about this relationship. In this paper we review these theoretical results, and then discuss a number of seemingly anomalous experimental results reported by Tanese concerning the performance of the GA on a subclass of Walsh polynomials, some members of which were expected to be easy for the GA to optimize. Tanese found that the GA was poor at optimizing all functions in this subclass, that a partitioning of a single large population into a number of smaller independent populations seemed to improve performance, and that hillclimbing outperformed both the original and partitioned forms of the GA on these functions. These results seemed to contradict several commonly held expectations about GAs. We begin by reviewing schema processing in GAs. We then give an informal description of how Walsh analysis and Bethke's Walsh-Schema transform relate to GA performance, and we discuss the relevance of this analysis for GA applications in optimization and machine learning. We then describe Tanese's surprising results, examine them experimentally and theoretically, and propose and evaluate some explanations. These explanations lead to a more fundamental question about GAs: what are the features of problems that determine the likelihood of successful GA performance? 
Abstract-found: 1
Intro-found: 1
Reference: <editor> Belew, R. K. and Booker, L. B. (Eds.) </editor> <booktitle> (1991). Proceedings of the Fourth International Conference on Genetic Algorithms. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bergman, A. and Feldman, M. W. </author> <year> (1990). </year> <title> More on selection for and against recombination. </title> <booktitle> Theoretical Population Biology 38 (1), </booktitle> <pages> 68-92. </pages>
Reference-contexts: The appeal of GAs comes from their simplicity and elegance as algorithms as well as from their power to discover good solutions rapidly for difficult high-dimensional problems. In addition, GAs are idealized computational models of evolution that are being used to study questions in evolutionary biology and population genetics <ref> (Bergman and Feldman, 1990) </ref>. In the simplest form of the GA, bit strings play the role of chromosomes, with individual bits playing the role of genes.
Reference: <author> Bethke, A. D. </author> <year> (1980). </year> <title> Genetic Algorithms as Function Optimizers. </title> <type> (Doctoral dissertation, </type> <institution> University of Michigan.) </institution> <note> Dissertation Abstracts International, 41(9), 3503B. </note>
Reference: <author> Das, R. and Whitley, L. D. </author> <year> (1991). </year> <title> The only challenging problems are deceptive: Global search by solving order-1 hyperplanes. </title> <editor> In R. K. Belew and L. B Booker (Eds.), </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: The Tanese functions thus provide an important counter-example to a prevailing belief that deception is the crucial feature in characterizing what types of functions are difficult for the GA to optimize <ref> (Das & Whitley, 1991) </ref>.
Reference: <author> Davis, L. D. (Ed.) </author> <year> (1987). </year> <title> Genetic Algorithms and Simulated Annealing. </title> <booktitle> Research Notes in Artificial Intelligence. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kauffmann. </publisher>
Reference: <editor> Davis, L. D. (Ed.) </editor> <booktitle> (1991). The Handbook of Genetic Algorithms. </booktitle> <address> New York: </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference: <author> De Jong, K. A. </author> <year> (1975). </year> <title> An Analysis of the Behavior of a Class of Genetic Adaptive Systems. </title> <type> Unpublished doctoral dissertation, </type> <institution> University of Michigan, </institution> <address> Ann Arbor, MI. </address>
Reference: <author> De Jong, K. A. (Ed.) </author> <year> (1990a). </year> <note> Special issue on genetic algorithms. Machine Learning 5(4). </note>
Reference: <author> De Jong, K. A. </author> <year> (1990b). </year> <title> Introduction to the second special issue on genetic algorithms. </title> <journal> Machine Learning, </journal> <volume> 5(4), </volume> <pages> 351-353. </pages>
Reference-contexts: If the space to be searched is not so well understood and relatively unstructured, and if an effective GA representation of that space can be developed, then GAs provide a surprisingly powerful search heuristic for large, complex spaces <ref> (De Jong, 1990b, p. 351, italics added) </ref>. It is of central importance to understand what constitutes "an effective GA representation".
Reference: <author> De Jong, K. A. and Spears, W. </author> <year> (1991). </year> <title> Learning concept classification rules using genetic algo-rithms. </title> <booktitle> In Proceedings of the Twelfth International Conference on Artificial Intelligence, </booktitle> <pages> 651-656. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Forrest, S. </author> <year> (1985). </year> <title> Documentation for prisoner's dilemma and norms programs that use the genetic algorithm. </title> <type> Unpublished report, </type> <institution> University of Michigan, </institution> <address> Ann Arbor, MI. </address>
Reference-contexts: Experimental Setup The experiments we report in this paper were performed with a similar GA and identical parameter values to those used by Tanese (1989a) <ref> (and also in previous work by Forrest, 1985) </ref>. All of Tanese's experiments used strings of length 32 and populations of 256 individuals. The population was sometimes subdivided into a number of smaller subpopulations.
Reference: <author> Forrest, S. and Mitchell, M. </author> <year> (1991). </year> <title> The performance of genetic algorithms on Walsh polynomials: Some anomalous results and their explanation. </title> <editor> In R. K. Belew and L. B. Booker (Eds). </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Forrest, S. and Mitchell, M. </author> <year> (1992). </year> <title> Towards a stronger building-blocks hypothesis: Effects of relative building-block fitness on GA performance. </title> <note> To appear in L. </note> <editor> D. Whitley (ed.), </editor> <booktitle> Foundations of Genetic Algorithms 2. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Other such factors include sampling error (Grefenstette & Baker, 1989), the number of local optima in the landscape (Schaffer et al., 1989), and the relative differences in fitness between disjoint desirable schemas <ref> (Mitchell, Forrest, & Holland, 1992) </ref>. Most efforts at characterizing the ease or difficulty of problems for the GA have concentrated on deception, but these other factors have to be taken into account as well.
Reference: <author> Goldberg, D. E. </author> <year> (1985). </year> <title> Optimal initial population size for binary-coded genetic algorithms. </title> <type> Technical Report TCGA Report No. 85001, </type> <institution> University of Alabama, Tuscaloosa, AL. </institution>
Reference-contexts: As Tanese points out, these results run against conventional wisdom about GAs: it has been thought that on difficult problems a large population is needed for processing a sufficient number of schemas <ref> (Goldberg, 1985) </ref>. Tanese proposes three main reasons for this surprising result. 1. Tanese functions have a large number of local optima and the GA tends to converge on one.
Reference: <author> Goldberg, D. E. </author> <year> (1987). </year> <title> Simple genetic algorithms and the minimal deceptive problem. </title> <editor> In L. </editor> <address> D. </address>
Reference: <author> Davis (Ed.), </author> <title> Genetic Algorithms and Simulated Annealing. </title> <booktitle> Research Notes in Artificial Intelligence. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Goldberg, D. E. </author> <year> (1989d). </year> <title> Sizing populations for serial and parallel genetic algorithms. </title> <editor> In J. </editor> <address> D. </address>
Reference: <editor> Schaffer (Ed.) </editor> <booktitle> (1989). Proceedings of the Third International Conference on Genetic Algorithms. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Other such factors include sampling error (Grefenstette & Baker, 1989), the number of local optima in the landscape <ref> (Schaffer et al., 1989) </ref>, and the relative differences in fitness between disjoint desirable schemas (Mitchell, Forrest, & Holland, 1992). Most efforts at characterizing the ease or difficulty of problems for the GA have concentrated on deception, but these other factors have to be taken into account as well.
Reference: <author> Goldberg, D. E. </author> <year> (1989a). </year> <title> Genetic algorithms and Walsh functions: Part I, A gentle introduction. </title> <booktitle> Complex Systems 3, </booktitle> <pages> 129-152. </pages>
Reference-contexts: ! 01 ! 10 ) = 9 (14=4 + 6=4 + 12=4) And to check: F (11) = ! 00 ! 01 ! 10 + ! 11 = 14=4 + 6=4 + 12=4 + 4=4 = 9: In general, ! j = 2 l x=0 This is the Walsh transform <ref> (it is derived more formally in Goldberg, 1989a) </ref>. Once the ! j 's have been determined, F can be calculated as F (x) = j=0 This expression is called the Walsh polynomial representing F (x).
Reference: <author> Goldberg, D. E. </author> <year> (1989b). </year> <title> Genetic algorithms and Walsh functions: Part II, Deception and its analysis. </title> <booktitle> Complex Systems 3, </booktitle> <pages> 153-171. </pages>
Reference: <author> Goldberg, D. E. </author> <year> (1989c). </year> <title> Genetic Algorithms in Search, Optimization, </title> <booktitle> and Machine Learning. </booktitle> <address> Reading, MA: </address> <publisher> Addison Wesley. </publisher>
Reference-contexts: identify the fittest part of the space very quickly is a powerful property; on the other hand, since the GA always operates on finite size populations, there is inherently some sampling error in the search, and in some cases the GA can magnify a small sampling error, causing premature convergence <ref> (Goldberg, 1989c) </ref>. Also, in some cases strong convergence is inappropriate, for example, in classifier systems (Holland, 1986), in which the GA is trying to evolve a set of co-adapted rules, each one specialized for a specific but different task, rather than a population of similar rules.
Reference: <author> Goldberg, D. E. </author> <year> (1991). </year> <title> Construction of high-order deceptive functions using low-order Walsh coefficients. </title> <type> Technical Report 90002, </type> <institution> Illinois Genetic Algorithms Laboratory, Dept. of General Engineering, University of Illinois, Urbana, IL. </institution>
Reference-contexts: Are the Tanese Functions Deceptive? None of the explanations given so far for Tanese's anomalous results relies on the Tanese functions being deceptive. However, the poor performance of the GA led Goldberg to propose that "deception or partial deception" was lurking in the Tanese functions <ref> (Goldberg, 1991) </ref>. In this section we examine the question of whether or not it is possible for a Tanese functions to be deceptive.
Reference: <author> Goldberg, D. E. and Bridges, C. L. </author> <year> (1988). </year> <title> An analysis of a reordering operator on a GA-hard problem. </title> <type> Technical Report 88005, </type> <institution> The Clearinghouse for Genetic Algorithms, Dept. of Engineering Mechanics, University of Alabama, Tuscaloosa, AL. </institution>
Reference: <author> Goldberg, D. E. and Holland, J. H. (Eds.) </author> <year> (1988a). </year> <note> Special issue on genetic algorithms. Machine Learning 3(2-3). </note>
Reference: <author> Goldberg, D. E. and Holland, J. H. </author> <year> (1988b). </year> <title> Introduction to the first special issue on genetic algorithms. </title> <booktitle> Machine Learning 3(2-3), </booktitle> <pages> 95-99. </pages>
Reference-contexts: The promise of GAs is based on the belief that "discovery and recombination of building blocks, allied with speedup provided by implicit parallelism, provides a powerful tool for learning in complex domains" <ref> (Goldberg & Holland, 1988b) </ref>, but a better understanding of the details of building-block processing and implicit parallelism, and how they are affected by the structure of a given problem, is needed in order to use GAs most effectively.
Reference: <author> Goldberg, D. E., Korb, B., and Deb, K. </author> <year> (1990). </year> <title> Messy genetic algorithms: Motivation, analysis, and first results. </title> <booktitle> Complex Systems 3, </booktitle> <pages> 493-530. </pages>
Reference: <editor> Grefenstette, J. J. (Ed.) </editor> <booktitle> (1985). Proceedings of the First International Conference on Genetic Algorithms and Their Applications. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <editor> Grefenstette, J. J. (Ed.) </editor> <booktitle> (1987). Genetic Algorithms and Their Applications: Proceedings of the Second International Conference on Genetic Algorithms. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Grefenstette, J. J. and Baker, J. E. </author> <year> (1989). </year> <title> How genetic algorithms work: A critical look at implicit parallelism. </title> <editor> In J. D. Schaffer (Ed.), </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Other such factors include sampling error <ref> (Grefenstette & Baker, 1989) </ref>, the number of local optima in the landscape (Schaffer et al., 1989), and the relative differences in fitness between disjoint desirable schemas (Mitchell, Forrest, & Holland, 1992).
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <booktitle> Adaptation in Natural and Artificial Systems. </booktitle> <address> Ann Arbor, MI: </address> <publisher> University of Michigan Press Holland, </publisher> <editor> J. H. </editor> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. </editor> <publisher> M. </publisher>
Reference: <editor> Mitchell (Eds.), </editor> <booktitle> Machine Learning II, </booktitle> <pages> 593-623. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Holland, J. H. </author> <year> (1988). </year> <title> The dynamics of searches directed by genetic algorithms. </title> <editor> In Y. C. Lee (Ed.), </editor> <title> Evolution, Learning, </title> <journal> and Cognition, </journal> <pages> 111-128. </pages> <address> Teaneck, NJ: </address> <publisher> World Scientific. </publisher>
Reference-contexts: That is, 0**** and *0*** are partial solutions. Schemas induce a partitioning of the search space <ref> (Holland, 1988) </ref>. For example, as seen in Figure 1, the partition d**** (where "d" means "defined bit") divides the search space into two halves, corresponding to the schemas 1**** and 0****. <p> The purpose of the Walsh functions j (x) is to provide such a consistent way of assigning signs to ! j 's, via bitwise AND and parity. This is not the only possible method; a slightly different method is given by Holland for his hyperplane transform <ref> (Holland, 1988) </ref>|an alternative formulation of the Walsh-schema transform, described in the next section. 3.2 The Walsh-Schema Transform There is a close connection between the Walsh transform and schemas. The Walsh-Schema transform formalizes this connection. <p> These results also apparently contradict some other beliefs about the GA|that it will routinely outperform hillclimbing and other gradient descent methods on hard problems such as those with nonlinear interactions <ref> (Holland, 1988) </ref>; and that a population must be of a sufficient size to support effective schema processing (Goldberg, 1985; Goldberg, 1989d). In order to better understand the sources of Tanese's results, we performed a number of additional experiments, which are described in the following sections. 5.
Reference: <author> Holland, J. H. </author> <year> (1989). </year> <title> Using classifier systems to study adaptive nonlinear networks. </title> <editor> In D. </editor> <publisher> L. </publisher>
Reference-contexts: Another hypothesized contributing factor is the degree to which the fitness landscape contains different "mountainous" regions of high-fitness points that are separated by "deserts" of low-fitness points <ref> (Holland, 1989) </ref>. In order to travel from a low mountainous region to a higher one, a low-fitness desert must be crossed.
Reference: <editor> Stein (Ed.), </editor> <booktitle> Lectures in the Sciences of Complexity (Vol. </booktitle> <volume> 1, </volume> <pages> pp. 463-499). </pages> <address> Reading, MA. </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Koza, J. R. </author> <year> (1990). </year> <title> Genetic programming: A paradigm for genetically breeding populations of computer programs to solve problems. </title> <type> Technical Report STAN-CS-90-1314, </type> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <address> CA. </address>
Reference-contexts: The GA has been used in many machine-learning contexts, such as evolving classification rules (e.g., Packard, 1990; De Jong and Spears, 1991), evolving neural networks (e.g., Miller, Todd, & Hegde, 1989; Whitley, Dominic, & Das, 1991), classifier systems (e.g., Holland, 1986; Smith, 1980), and automatic programming <ref> (e.g., Koza, 1990) </ref>. In many of these cases there is no closed-form "fitness function"; the evaluation of each individual (or collection of individuals) is obtained by "running" it on the particular task being learned.
Reference: <author> Liepins, G. E. and Vose, M. D. </author> <year> (1990). </year> <title> Representational issues in genetic optimization. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence 2, </journal> <pages> 101-115. </pages>
Reference-contexts: Moreover, it could be argued that in general the GA is more suited to finding good solutions quickly than to finding the absolute best solution to a problem. But some researchers define deception solely in terms of hyperplanes leading toward or away from the global optimum <ref> (e.g., see Liepins & Vose, 1990) </ref>. This narrows the concept of deception and makes it less generally useful for characterizing problems according to probable GA success or failure, especially when "success" does not require that the exact optimum point be found.
Reference: <author> Miller, G. F., Todd, P. M., and Hegde, S. U. </author> <year> (1989). </year> <title> Designing neural networks using genetic algorithms. </title> <editor> In J. D. Schaffer (Ed.), </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> 379-384. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mitchell, M., Forrest, S., and Holland, J. H. </author> <year> (1992). </year> <title> The royal road for genetic algorithms: Fitness landscapes and GA performance. </title> <booktitle> In Proceedings of the First European Conference on Artificial Life. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press/Bradford Books. </publisher>
Reference-contexts: Other such factors include sampling error (Grefenstette & Baker, 1989), the number of local optima in the landscape (Schaffer et al., 1989), and the relative differences in fitness between disjoint desirable schemas <ref> (Mitchell, Forrest, & Holland, 1992) </ref>. Most efforts at characterizing the ease or difficulty of problems for the GA have concentrated on deception, but these other factors have to be taken into account as well.
Reference: <author> Packard, N. H. </author> <year> (1990). </year> <title> A genetic learning algorithm for the analysis of complex data. </title> <journal> Complex Systems 4(5). </journal>
Reference: <editor> Schaffer, J. D. (Ed.) </editor> <booktitle> (1989). Proceedings of the Third International Conference on Genetic Algorithms. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Other such factors include sampling error (Grefenstette & Baker, 1989), the number of local optima in the landscape <ref> (Schaffer et al., 1989) </ref>, and the relative differences in fitness between disjoint desirable schemas (Mitchell, Forrest, & Holland, 1992). Most efforts at characterizing the ease or difficulty of problems for the GA have concentrated on deception, but these other factors have to be taken into account as well.
Reference: <author> Smith, S. F. </author> <year> (1980). </year> <title> A learning system based on genetic adaptive algorithms. </title> <type> Unpublished Ph.D. dissertation, </type> <institution> Computer Science Department, University of Pittsburgh, </institution> <address> Pittsburgh, PA. </address>
Reference: <author> Tanese, R. </author> <year> (1989a). </year> <title> Distributed Genetic Algorithms for Function Optimization. </title> <type> Unpublished doctoral dissertation, </type> <institution> University of Michigan, </institution> <address> Ann Arbor, MI. </address>
Reference-contexts: The shorter runs were sufficient for determining the comparative effects of the various manipulations we performed on the parameters. Tanese also compared her GA results with the results of running an iterated hillclimbing algorithm on randomly generated Tanese functions. The iterated hillclimbing algorithm works as follows <ref> (Tanese, 1989a) </ref>. Repeat the following until a specified number of function evaluations have been performed. 1. Choose a random string x in the search space, and calculate its fitness. 2. Calculate the fitness of every one-bit mutation of x.
Reference: <author> Tanese, R. </author> <year> (1989b). </year> <title> Distributed genetic algorithms. </title> <editor> In J. D. Schaffer (Ed.), </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Walsh, J. L. </author> <year> (1923). </year> <title> A closed set of orthogonal functions. </title> <journal> American Journal of Mathematics, </journal> <volume> 55, </volume> <pages> 5-24. </pages>
Reference-contexts: Walsh-Schema Analysis Two goals for a theory of genetic algorithms are (1) to describe in detail how schemas are processed, and (2) to predict the degree to which a given problem will be easy or difficult for the GA. Bethke's dissertation (1980) addressed these issues by applying Walsh functions <ref> (Walsh, 1923) </ref> to the study of schema processing in GAs. In particular, Bethke developed the Walsh-Schema transform, in which discrete versions of Walsh functions are used to calculate schema average fitnesses efficiently. He then used this transform to characterize functions as easy or hard for the GA to optimize.
Reference: <author> Whitley, L. D. </author> <year> (1991). </year> <title> Fundamental principles of deception in genetic search. </title> <editor> In G. Rawlins (Ed.), </editor> <booktitle> Foundations of Genetic Algorithms. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The Tanese functions thus provide an important counter-example to a prevailing belief that deception is the crucial feature in characterizing what types of functions are difficult for the GA to optimize <ref> (Das & Whitley, 1991) </ref>.
Reference: <author> Whitley, L. D., Dominic, S., and Das, R. </author> <year> (1991). </year> <title> Genetic reinforcement learning with multilayer neural networks. </title> <editor> In R. K. Belew and L. B. Booker (Eds.), </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 562-569. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The Tanese functions thus provide an important counter-example to a prevailing belief that deception is the crucial feature in characterizing what types of functions are difficult for the GA to optimize <ref> (Das & Whitley, 1991) </ref>.
Reference: <author> Wilson, S. W. </author> <year> (1991). </year> <title> GA-easy does not imply steepest-ascent optimizable. </title> <editor> In R. K. Belew and L. B. Booker (Eds.), </editor> <booktitle> Proceedings of The Fourth International Conference on Genetic Algorithms. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 47

