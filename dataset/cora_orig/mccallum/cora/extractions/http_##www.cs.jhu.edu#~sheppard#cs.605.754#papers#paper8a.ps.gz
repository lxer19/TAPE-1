URL: http://www.cs.jhu.edu/~sheppard/cs.605.754/papers/paper8a.ps.gz
Refering-URL: http://www.cs.jhu.edu/~sheppard/cs.605.754/sched.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: (pete@turing.ac.uk)  (tim@turing.ac.uk)  
Title: The CN2 Induction Algorithm  
Author: Peter Clark Tim Niblett 
Date: October 1988  
Address: 36 N. Hanover St., Glasgow, G1 2AD, U.K.  
Affiliation: The Turing Institute,  
Web: http://www.cs.utexas.edu/users/pclark/papers/cn2.ps  
Note: In: Machine Learning Journal, 3 (4), pp261-283, Netherlands: Kluwer (1989)  
Abstract: Systems for inducing concept descriptions from examples are valuable tools for assisting in the task of knowledge acquisition for expert systems. This paper presents a description and empirical evaluation of a new induction system, cn2, designed for the efficient induction of simple, comprehensible production rules in domains where problems of poor description language and/or noise may be present. Implementations of the cn2, id3 and aq algorithms are compared on three medical classification tasks. Keywords: concept learning, rule induction, noise, comprehensibility, cn2. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Peter Mowforth. </author> <title> Some applications with inductive expert system shells. </title> <type> TIOP 86-002, </type> <institution> Turing Institute, Glasgow, UK, </institution> <year> 1986. </year>
Reference-contexts: 1 Introduction In the task of constructing expert systems, systems for inducing concept descriptions from examples have proved useful in easing the bottleneck of knowledge acquisition <ref> [1] </ref>. Two families of systems, based on the id3 [2] and aq [3] algorithms, have been especially successful. These basic algorithms assume no noise in the domain, searching for a concept description that classifies training data perfectly.
Reference: [2] <author> J. Ross Quinlan. </author> <title> Learning efficient classification procedures and their application to chess endgames. </title> <editor> In J. G. Carbonell, R. S. Michalski, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> vol. 1. </volume> <publisher> Tioga, </publisher> <address> Palo Alto, Ca, </address> <year> 1983. </year>
Reference-contexts: 1 Introduction In the task of constructing expert systems, systems for inducing concept descriptions from examples have proved useful in easing the bottleneck of knowledge acquisition [1]. Two families of systems, based on the id3 <ref> [2] </ref> and aq [3] algorithms, have been especially successful. These basic algorithms assume no noise in the domain, searching for a concept description that classifies training data perfectly. However for the application of systems based on these algorithms to real-world domains, methods for handling noisy data are required.
Reference: [3] <author> R. S. Michalski. </author> <title> On the quasi-minimal solution of the general covering problem. </title> <booktitle> In Proceedings of the 5th international symposium on Information Processing (FCIP 69), Vol. A3 (Switching circuits), Bled, Yugoslavia, </booktitle> <pages> pages 125-128, </pages> <year> 1969. </year>
Reference-contexts: 1 Introduction In the task of constructing expert systems, systems for inducing concept descriptions from examples have proved useful in easing the bottleneck of knowledge acquisition [1]. Two families of systems, based on the id3 [2] and aq <ref> [3] </ref> algorithms, have been especially successful. These basic algorithms assume no noise in the domain, searching for a concept description that classifies training data perfectly. However for the application of systems based on these algorithms to real-world domains, methods for handling noisy data are required. <p> If assistant is to generate an `unpruned' tree, the termination criterion TE (E) is satisfied if all the examples E have the same class value. 2.2 AQR aqr is an induction system that uses the basic aq algorithm <ref> [3] </ref> to generate a set of classification rules.
Reference: [4] <author> J. R. Quinlan. </author> <title> Simplifying decision trees. </title> <journal> Int. Journal of Man-Machine Studies, </journal> <volume> 27(3) </volume> <pages> 221-234, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: Fortunately the id3 algorithm lends itself to easy modification allowing this constraint to be relaxed, by the nature of its general-to-specific search. Tree pruning techniques (e.g. <ref> [4, 5] </ref>), as used for example in the systems c4 [6] and assistant [7], have proved to be effective methods of avoiding overfitting. The aq algorithm, however, is less easy to modify due to its dependence on specific training examples during its search.
Reference: [5] <author> Tim Niblett. </author> <title> Constructing decision trees in noisy domains. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning (proceedings of the 2nd European Working Session on Learning), </booktitle> <pages> pages 67-78. </pages> <address> Sigma, Wilmslow, UK, </address> <year> 1987. </year>
Reference-contexts: Fortunately the id3 algorithm lends itself to easy modification allowing this constraint to be relaxed, by the nature of its general-to-specific search. Tree pruning techniques (e.g. <ref> [4, 5] </ref>), as used for example in the systems c4 [6] and assistant [7], have proved to be effective methods of avoiding overfitting. The aq algorithm, however, is less easy to modify due to its dependence on specific training examples during its search.
Reference: [6] <author> J. R. Quinlan, P. J. Compton, K. A. Horn, and L. Lazarus. </author> <title> Inductive knowledge acquisition: a case study. </title> <booktitle> In Applications of Expert Systems, </booktitle> <pages> pages 157-173. </pages> <publisher> Addison-Wesley, </publisher> <address> Wokingham, UK, </address> <year> 1987. </year>
Reference-contexts: Fortunately the id3 algorithm lends itself to easy modification allowing this constraint to be relaxed, by the nature of its general-to-specific search. Tree pruning techniques (e.g. [4, 5]), as used for example in the systems c4 <ref> [6] </ref> and assistant [7], have proved to be effective methods of avoiding overfitting. The aq algorithm, however, is less easy to modify due to its dependence on specific training examples during its search.
Reference: [7] <author> Igor Kononenko, Ivan Bratko, and Egidija Roskar. </author> <title> Experiments in automatic learning of medical diagnostic rules. </title> <type> Technical report, </type> <institution> Faculty of Electircal Engineering, E. Kardelj University, Ljubljana, </institution> <year> 1984. </year> <month> 27 </month>
Reference-contexts: Fortunately the id3 algorithm lends itself to easy modification allowing this constraint to be relaxed, by the nature of its general-to-specific search. Tree pruning techniques (e.g. [4, 5]), as used for example in the systems c4 [6] and assistant <ref> [7] </ref>, have proved to be effective methods of avoiding overfitting. The aq algorithm, however, is less easy to modify due to its dependence on specific training examples during its search. <p> In all of our experiments, the example description language consisted of attributes, attribute values and user-specified classes. This language was the same for each algorithm. 2.1 Assistant The assistant algorithm <ref> [7] </ref> is a descendant of Quinlan's id3 (1983), and incorporates a tree pruning mechanism for handling noisy data. 3 Let: E be a set of examples A be a set of attributes for describing examples T E (E) be a termination criterion IDM (a i ; E) be an evaluation function <p> These data were obtained from the Institute of Oncology at the University Medical Center in Ljubljana, Yugoslavia <ref> [7] </ref>. In each test, 70% of the training examples were selected at random from the entire data set, and the remaining 30% of the data were used for testing. The algorithms were all run on the same training data and their induced knowledge structures tested using the same test data.
Reference: [8] <author> R. S. Michalski and J. Larson. </author> <title> Incremental generation of vl 1 hypotheses: the underly-ing methodology and the description of program aq11. </title> <type> ISG 83-5, </type> <institution> Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, Urbana, </institution> <year> 1983. </year>
Reference-contexts: The aq algorithm, however, is less easy to modify due to its dependence on specific training examples during its search. Existing implementations (e.g. aq11 <ref> [8] </ref> and aq15 [9]) deal with noisy data by using pre- and post-processing techniques while leaving the basic aq algorithm intact. <p> Many systems use this algorithm in a more sophisticated manner than aqr to improve predictive accuracy and rule simplicity (e.g., aq11 <ref> [8] </ref> uses a more complex method of rule interpretation that involves degrees of confirmation). aqr is a reconstruction of a straightforward aq-based system. 2.2.1 Concept Description Language and Interpretation aqr induces a set of decision rules, one for each class. <p> If the example is not covered by any rule, then it is assigned by default to the class that occurred most frequently in the training examples. 2.2.2 The Learning Algorithm The aq rule-generation algorithm has been described elsewhere (e.g. <ref> [8, 12, 13] </ref>), and the aqr system is an instance of this general algorithm. aqr generates a decision rule for each class in turn.
Reference: [9] <author> R. S. Michalski, I. Mozetic, J. Hong, and N. Lavrac. </author> <title> The AQ15 inductive learning system : an overview and experiments. </title> <booktitle> In Proceedings of IMAL 1986, </booktitle> <address> Orsay, France, </address> <year> 1986. </year> <institution> Universite de Paris-Sud. </institution>
Reference-contexts: The aq algorithm, however, is less easy to modify due to its dependence on specific training examples during its search. Existing implementations (e.g. aq11 [8] and aq15 <ref> [9] </ref>) deal with noisy data by using pre- and post-processing techniques while leaving the basic aq algorithm intact. Our objective in designing cn2 is to modify the aq algorithm itself in such a way that this dependence on specific examples is removed and the space of rules searched is increased. <p> The algorithms were all run on the same training data and their induced knowledge structures tested using the same test data. Five such tests were performed for each of the three domains, and the results were averaged. These data are thus identical to those used to test aq15 in <ref> [9] </ref>, though the particular random 70% and 30% samples are different. Both cn2 and aqr were given a value of 15 for maxstar in all runs. 4.2.1 Three Medical Domains Table 4 summarizes the characteristics of the three medical domains used in the experiments. The first of these involved lymphography. <p> It was included in these experiments to examine the basic aq algorithm's sensitivity to noise. In practice it is rarely used on its own, and instead enhanced by a number of pre- and post-rule-generation techniques. Experiments with the aq15 system <ref> [9] </ref> show that with post-pruning of the rules and a probability-based or `flexible matching' rule application method, one can achieve results similar to those of cn2 and assistant in terms of accuracy and complexity.
Reference: [10] <author> Wayne Iba, James Wogulis, and Pat Langley. </author> <title> Trading off simplicity and coverage in incremental concept learning. </title> <editor> In John Laird, editor, </editor> <booktitle> Proc. 5th Int. Conf. on Machine Learning, </booktitle> <pages> pages 73-79. </pages> <publisher> Kaufmann, </publisher> <address> Ca, </address> <year> 1988. </year>
Reference-contexts: Thus, to induce short rules, one must usually relax the requirement that the induced rules be consistent with all the training data. The choice of how much to relax this requirement involves a trade-off between accuracy and simplicity <ref> [10] </ref>. Efficient rule generation. If one expects to use large example sets, it is important that the algorithm scales up to complex situations.
Reference: [11] <author> R. L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(3) </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: It combines the efficiency and ability to cope with noisy data of id3 with the if-then rule form and flexible search strategy of the aq family. The representation for rules output by cn2 is an ordered set of if-then rules, also known as a `decision list' <ref> [11] </ref>. cn2 uses a heuristic function to terminate search during rule construction, based on an estimate of the noise present in the data. This results in rules that do not necessarily classify all the training examples correctly, but that perform well on new data.
Reference: [12] <author> R. S. Michalski and R. Chilausky. </author> <title> Learning by being told and learning from examples: an experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean diagnosis. </title> <journal> Policy Analysis and Information Systems, </journal> <volume> 4(2) </volume> <pages> 125-160, </pages> <year> 1980. </year>
Reference-contexts: If the example is not covered by any rule, then it is assigned by default to the class that occurred most frequently in the training examples. 2.2.2 The Learning Algorithm The aq rule-generation algorithm has been described elsewhere (e.g. <ref> [8, 12, 13] </ref>), and the aqr system is an instance of this general algorithm. aqr generates a decision rule for each class in turn.
Reference: [13] <author> P. O'Rorke. </author> <title> A comparative study of inductive learning systems AQ11P and ID3 using a chess end-game test problem. </title> <type> ISG 82-2, </type> <institution> Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, Urbana, </institution> <year> 1982. </year>
Reference-contexts: If the example is not covered by any rule, then it is assigned by default to the class that occurred most frequently in the training examples. 2.2.2 The Learning Algorithm The aq rule-generation algorithm has been described elsewhere (e.g. <ref> [8, 12, 13] </ref>), and the aqr system is an instance of this general algorithm. aqr generates a decision rule for each class in turn.
Reference: [14] <author> A. Wald. </author> <title> Sequential Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1947. </year>
Reference-contexts: Note that, unlike the other algorithms we have discussed, our implementation of the Bayesian classifier requires one to examine the values of all attributes when making a prediction. We should note that there also exist more sophisticated applications of the Bayes rule in which the attribute tests are ordered <ref> [14] </ref>. Such a sequential technique adds the contribution of each test to a total; when this score exceeds a threshold, the algorithm exits with a class prediction.
Reference: [15] <author> J. Kalbfleish. </author> <title> Probability and Statistical Inference, volume 2. </title> <publisher> Springer-Verlag, </publisher> <address> NY, </address> <year> 1979. </year>
Reference-contexts: The issue is whether the observed differences are too great to be accounted for purely by chance. If so, cn2 assumes that the complex reflects a genuine correlation between attributes and classes. To test significance, the system uses the likelihood ratio statistic <ref> [15] </ref>.
Reference: [16] <author> A. Paterson and T. Niblett. </author> <title> ACLS Manual. </title> <type> Version 1. </type> <institution> Glasgow, </institution> <year> 1982. </year>
Reference-contexts: Thus the amount of time taken by assistant for the basic attribute selection operation is a linear function of the number of examples, when the number of classes and attributes are held constant. We should note that extensions to this algorithm that use real-valued attributes (such as acls <ref> [16] </ref>) must sort the examples by attribute value at the first stage. This increases the overall time bound to o (a e log e). 3.2 Time Complexity of CN2 The basic operation in cn2 is the specialization of the complexes in the current star.
Reference: [17] <author> Philip K. Chan. </author> <title> A critical review of cn2: A polythetic classifier system. </title> <type> Technical Report CS-88-09, </type> <institution> Dept. of Computer Science, Vanderbild Univ., Tennessee, </institution> <year> 1988. </year>
Reference-contexts: When this ideal is not met (e.g. when a concept description classifying the training data perfectly is sought for), cn2 would at worst induce e rules of length a giving an overall time complexity of o (a 2 :e 2 :s) <ref> [17] </ref>. assistant sorts a total of e examples among a attributes for each level of the tree, giving an overall time complexity of o (a 2 :e) as the tree depth is bounded by a. A similar worst-case time complexity to cn2 holds for aqr. <p> These comparative results and behavior as noise level approaches 100% suggests that the thresholding methods used in both cn2 and assistant need to be more sensitive to the properties of the application domain. Research on improvements to cn2's significance test <ref> [17] </ref> and assistant's pruning mechanism [19] is currently being conducted. Finally, we consider the accuracy of cn2's individual rules as well as that of the whole rule set.
Reference: [18] <author> J.A. Jackson. </author> <title> Economics of automatic generation of rules from examples in a Chess end-game. </title> <type> UIUCDCS-F 85-932, </type> <institution> Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, Urbana, </institution> <year> 1985. </year>
Reference: [19] <author> Bojan Cestnik, Igor Kononenko, and Ivan Bratko. </author> <title> Assistant 86: A knowledge-elicitation tool for sophisticated users. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning (proceedings of the 2nd European Working Session on Learning), </booktitle> <pages> pages 31-45. </pages> <address> Sigma, Wilmslow, UK, </address> <year> 1987. </year> <month> 28 </month>
Reference-contexts: These comparative results and behavior as noise level approaches 100% suggests that the thresholding methods used in both cn2 and assistant need to be more sensitive to the properties of the application domain. Research on improvements to cn2's significance test [17] and assistant's pruning mechanism <ref> [19] </ref> is currently being conducted. Finally, we consider the accuracy of cn2's individual rules as well as that of the whole rule set.
Reference: [20] <author> J. Ross Quinlan. </author> <title> Generating production rules from decision trees. </title> <editor> In J. McDermott, editor, </editor> <booktitle> IJCAI-87, </booktitle> <pages> pages 304-307, </pages> <address> Ca, 1987. </address> <publisher> Kaufmann. </publisher>
Reference: [21] <author> T. Niblett and I. Bratko. </author> <title> Learning decision rules in noisy domains. </title> <editor> In M. A. Bramer, editor, </editor> <booktitle> Research and Development in Expert Systems III, </booktitle> <pages> pages 25-34. </pages> <publisher> Cambridge University Press, </publisher> <year> 1987. </year> <month> 29 </month>
References-found: 21

