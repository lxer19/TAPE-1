URL: http://www.nbi.dk/~randlov/bicycle.ps.gz
Refering-URL: http://www.cs.wisc.edu/icml98/schedule.html
Root-URL: 
Email: randlov@nbi.dk  alstrom@cats.nbi.dk  
Title: Learning to Drive a Bicycle using Reinforcement Learning and Shaping  
Author: Jette Randlv CATS, Niels Bohr Preben Alstrm, 
Address: Blegdamsvej 17, DK-2100 Copenhagen Denmark  
Affiliation: Institute, University of Copenhagen,  
Abstract: We present and solve a real-world problem of learning to drive a bicycle. We solve the problem by online reinforcement learning using the Sarsa()-algorithm. Then we solve the composite problem of learning to balance a bicycle and then drive to a goal. In our approach the reinforcement function is independent of the task the agent tries to learn to solve.
Abstract-found: 1
Intro-found: 1
Reference: [Atkinson et al., 1996] <author> Atkinson, R. L., Atkinson, R. C., Smith, E. E., Bem, D. J., and Nolen-Hoeksema, S. </author> <year> (1996). </year> <title> Hilgard's Introduction to Psychology. </title> <publisher> Harcourt Brace College Publishers, 12'th edition. </publisher>
Reference-contexts: brought to pecking a selected spot [Skinner, 1953, p. 93], horses to do clever tricks in a circus like seemingly recognize flags of nations or numbers and to do calculation [Jrgensen, 1962, pp. 137-139], and pigs to perform complex acts as eating breakfast at a table and vacuuming the floor <ref> [Atkinson et al., 1996, p. 242] </ref>. Staddon notes that human education as well is built up as a process of shaping if behavior is taken to include understanding [Staddon, 1983, p. 458]. agent can balance the bicycle for 3040 meters.
Reference: [Bertsekas and Tsitsiklis, 1996] <author> Bertsekas, D. P. and Tsitsiklis, J. N. </author> <year> (1996). </year> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific. </publisher>
Reference-contexts: No evaluative feedback from the system other than the failure signal is available. The goal of the agent is to learn a mapping from states to actions that maximizes the agent's discounted reward over time <ref> [Bertsekas and Tsitsiklis, 1996, Sutton and Barto, 1998] </ref>. The discounted reward is the sum P 1 i=0 fl i r t+i , where fl is the discount parameter. A lot of techniques have been developed to find near optimal mappings on a trial-and-error basis.
Reference: [Colombetti et al., 1996] <author> Colombetti, M., Dorigo, M., and Borghi, G. </author> <year> (1996). </year> <title> Robot shaping: The hamster experiment. </title> <type> Technical Report TR/IRIDIA/1996-6, </type> <institution> Universit Libre de Bruxelles. </institution>
Reference-contexts: They use reinforcement learning as a mean to translate suggestions from an external trainer. The trainer is a programme in itself with a high-level representation of the desired behavior that provided immediate reinforcement. For instance in the The Hamster Experiment <ref> [Colombetti et al., 1996] </ref> the robot's task is to collect pieces of food (colored cans) and bring them to its nest. The trainer provides the agent with a reinforcement signal for approaching the food. <p> The complexity of the task then grows as the agent gets better at playing. In Gullapalli's experiments [Gullapalli, 1992] and Selfridge, Sutton and Barto's [Selfridge et al., 1985], as well as in Dorigo, Colombetti and Borghi's <ref> [Colombetti et al., 1996, Dorigo and Colombetti, 1997] </ref>, the agent received a different reinforcement signal over time for the same behavior. This is not in agreement with the original inspiration of the reinforcement signal as being a hardwired signal inside the brain of a animal. <p> The agent is punished when driving away from the goal and rewarded when driving towards it. This reinforcement function is inspired by the signal used by Colombetti, Dorigo and Borghi <ref> [Colombetti et al., 1996] </ref> mentioned earlier. Note that the agent still have to solve the delayed reinforcement problem. As one can see, the numerical value of this signal is quite small.
Reference: [Dorigo and Colombetti, 1993] <author> Dorigo, M. and Colombetti, M. </author> <year> (1993). </year> <title> Robot shaping: Developing autonomous agents though learning. </title> <type> Technical Report TR-92-040, </type> <institution> International Computer Science Institute, Berkeley. </institution> <note> Labeled: To appear in Artificial Intelligence Journal. </note>
Reference: [Dorigo and Colombetti, 1997] <author> Dorigo, M. and Colombetti, M. </author> <year> (1997). </year> <title> Prcis of Robot Shaping: An Experiment in Behavior Engineering. Adaptive Behavior, 5(34). </title> <publisher> Prcis of the book from MIT Press, </publisher> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: The complexity of the task then grows as the agent gets better at playing. In Gullapalli's experiments [Gullapalli, 1992] and Selfridge, Sutton and Barto's [Selfridge et al., 1985], as well as in Dorigo, Colombetti and Borghi's <ref> [Colombetti et al., 1996, Dorigo and Colombetti, 1997] </ref>, the agent received a different reinforcement signal over time for the same behavior. This is not in agreement with the original inspiration of the reinforcement signal as being a hardwired signal inside the brain of a animal.
Reference: [Gullapalli, 1992] <author> Gullapalli, V. </author> <year> (1992). </year> <title> Reinforcement Learning and Its Application to Control. </title> <type> PhD thesis, </type> <institution> University of Massachusetts. </institution> <type> COINS Technical Report 9210. </type>
Reference-contexts: Again the theory was tested on a real robot moving cans to a nest. Here the constructed function did point is an average of 30 simulations. not eliminate the need for solving the delayed reinforcement problem. Gullapalli has studied two implementations of shaping <ref> [Gullapalli, 1992] </ref>. In the first the complexity of the control task is gradually increased during learning, and the reinforcement function used is changed accordingly. In this way most of a training run is used in learning the approximation to the current target behavior. <p> Self-play is a sort of shaping, since at first the agent plays against a nearly random opponent and thereby solves an easy task. The complexity of the task then grows as the agent gets better at playing. In Gullapalli's experiments <ref> [Gullapalli, 1992] </ref> and Selfridge, Sutton and Barto's [Selfridge et al., 1985], as well as in Dorigo, Colombetti and Borghi's [Colombetti et al., 1996, Dorigo and Colombetti, 1997], the agent received a different reinforcement signal over time for the same behavior.
Reference: [Jrgensen, 1962] <author> Jrgensen, J. </author> <year> (1962). </year> <institution> Psykologi paa biolo-gisk Grundlag. Scandinavian University Books. Munksgaard, Kbenhavn. </institution>
Reference-contexts: By rewarding successive approximations to the desired behavior, pigeons can be brought to pecking a selected spot [Skinner, 1953, p. 93], horses to do clever tricks in a circus like seemingly recognize flags of nations or numbers and to do calculation <ref> [Jrgensen, 1962, pp. 137-139] </ref>, and pigs to perform complex acts as eating breakfast at a table and vacuuming the floor [Atkinson et al., 1996, p. 242].
Reference: [Mataric, 1994] <author> Mataric, M. J. </author> <year> (1994). </year> <title> Reward functions for accelerated learning. </title> <editor> In Cohen, W. W. and Hirsh, H., editors, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> CA. </address>
Reference-contexts: Mataric has studied the possibility of putting implicit domain knowledge into the agent by construction a more complex reinforcement function than commonly used <ref> [Mataric, 1994] </ref>. Again the theory was tested on a real robot moving cans to a nest. Here the constructed function did point is an average of 30 simulations. not eliminate the need for solving the delayed reinforcement problem. Gullapalli has studied two implementations of shaping [Gullapalli, 1992]. <p> We agree with Mataric <ref> [Mataric, 1994] </ref> that these heterogeneous reinforcement functions have to be designed with great care. In our first experiments we rewarded the agent for driving towards the goal but did not punish it for driving away from it.
Reference: [McGovern et al., 1997] <author> McGovern, A., Sutton, R. S., and Fagg, A. H. </author> <year> (1997). </year> <title> Roles of macro-actions in accelerating reinforcement learning. </title> <booktitle> In 1997 Grace Hopper Celebration of Women in Computing. </booktitle>
Reference-contexts: McGovern, Sutton and Fagg have tested macro-actions in a gridworld and found that in some cases they accelerate the learning process <ref> [McGovern et al., 1997] </ref>. Dorigo, Colombetti and Borghi have worked with shaping for real robots [Dorigo and Colombetti, 1993, Colombetti et al., 1996, Dorigo and Colombetti, 1997]. They use reinforcement learning as a mean to translate suggestions from an external trainer.
Reference: [Rummery, 1995] <author> Rummery, G. A. </author> <year> (1995). </year> <title> Problem Solving with Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> Cambridge University Engineering Department. </institution>
Reference-contexts: Otherwise perform the learning (point 4) with Q t = 0. Niranjan [Rummery and Niranjan, 1994, Rummery, 1995, Singh and Sutton, 1996, Sutton and Barto, 1998], because empirical studies seem to suggest that this algorithm is the best so far <ref> [Rummery and Niranjan, 1994, Rummery, 1995, Sutton and Barto, 1998] </ref>. Figure 1 shows the Sarsa ()-algorithm. We have modified the algorithm slightly by cutting of eligibility traces that fall below 10 7 in order to save calculation time.
Reference: [Rummery and Niranjan, 1994] <author> Rummery, G. A. and Niranjan, M. </author> <year> (1994). </year> <title> On-line Q-learning using connectionist systems. </title> <type> Technical Report CUED/F-INFENG/TR 166, </type> <institution> Engineering Department, Cambridge University. </institution>
Reference-contexts: Otherwise perform the learning (point 4) with Q t = 0. Niranjan [Rummery and Niranjan, 1994, Rummery, 1995, Singh and Sutton, 1996, Sutton and Barto, 1998], because empirical studies seem to suggest that this algorithm is the best so far <ref> [Rummery and Niranjan, 1994, Rummery, 1995, Sutton and Barto, 1998] </ref>. Figure 1 shows the Sarsa ()-algorithm. We have modified the algorithm slightly by cutting of eligibility traces that fall below 10 7 in order to save calculation time.
Reference: [Santamara et al., 1996] <author> Santamara, J. C., Sutton, R. S., and Ram, A. </author> <year> (1996). </year> <title> Experiments with reinforcement learning in problems with continuous states and action spaces. </title> <type> Technical Report 96-088, COINS. </type>
Reference: [Selfridge et al., 1985] <author> Selfridge, O. G., Sutton, R. S., and Barto, A. G. </author> <year> (1985). </year> <title> Training and tracking in robotics. </title> <booktitle> In Proceedings of the Ninth International Joint Conference in Artificial Intelligence, </booktitle> <pages> pages 670672. </pages> <publisher> Morgan Kaufmann, </publisher> <address> CA. </address>
Reference-contexts: Selfridge, Sutton and Barto showed that transferring knowledge from solving an easy version of a problem such as the classical pole mounted on a cart can ease learning a more difficult version <ref> [Selfridge et al., 1985] </ref>. McGovern, Sutton and Fagg have tested macro-actions in a gridworld and found that in some cases they accelerate the learning process [McGovern et al., 1997]. <p> Self-play is a sort of shaping, since at first the agent plays against a nearly random opponent and thereby solves an easy task. The complexity of the task then grows as the agent gets better at playing. In Gullapalli's experiments [Gullapalli, 1992] and Selfridge, Sutton and Barto's <ref> [Selfridge et al., 1985] </ref>, as well as in Dorigo, Colombetti and Borghi's [Colombetti et al., 1996, Dorigo and Colombetti, 1997], the agent received a different reinforcement signal over time for the same behavior.
Reference: [Singh and Sutton, 1996] <author> Singh, S. P. and Sutton, R. S. </author> <year> (1996). </year> <title> Reinforcement learning with replacing eligibility traces. Machine Learning, </title> <publisher> 22:123158. </publisher>
Reference-contexts: We have modified the algorithm slightly by cutting of eligibility traces that fall below 10 7 in order to save calculation time. For replacing traces we allowed the trace for each state-action pair to continue until that pair occurred again, contrary to Singh and Sutton <ref> [Singh and Sutton, 1996] </ref>. 2 Learning to balance on a bicycle Our first task is to learn to balance. <p> At each time step a line is drawn between the points where the tyres touch the ground. Both accumulating and replacing eligibility traces were tried. The results are shown in figure 5. The results found support the general conclusions drawn by Singh and Sutton <ref> [Singh and Sutton, 1996] </ref>: Replacing traces make the agent perform much better than conventional, accumulating traces.
Reference: [Skinner, 1938] <author> Skinner, B. F. </author> <year> (1938). </year> <title> The Behavior of Organisms: An Experimental Analysis. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey. </address>
Reference-contexts: The term originates from the psychologist Skinner <ref> [Skinner, 1938] </ref>, who studied the effect on animals, especially pigeons and rats. To train an animal to produce a certain behavior, the trainer must find out what subtasks constitute an approximation of the desired behavior, and how these should be reinforced [Staddon, 1983].
Reference: [Skinner, 1953] <author> Skinner, B. F. </author> <year> (1953). </year> <booktitle> Science and Human Behavior. </booktitle> <address> Collier-Macmillian, New York. </address>
Reference-contexts: To train an animal to produce a certain behavior, the trainer must find out what subtasks constitute an approximation of the desired behavior, and how these should be reinforced [Staddon, 1983]. By rewarding successive approximations to the desired behavior, pigeons can be brought to pecking a selected spot <ref> [Skinner, 1953, p. 93] </ref>, horses to do clever tricks in a circus like seemingly recognize flags of nations or numbers and to do calculation [Jrgensen, 1962, pp. 137-139], and pigs to perform complex acts as eating breakfast at a table and vacuuming the floor [Atkinson et al., 1996, p. 242].
Reference: [Staddon, 1983] <author> Staddon, J. E. R. </author> <year> (1983). </year> <title> Adaptive Behavior and Learning. </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: The term originates from the psychologist Skinner [Skinner, 1938], who studied the effect on animals, especially pigeons and rats. To train an animal to produce a certain behavior, the trainer must find out what subtasks constitute an approximation of the desired behavior, and how these should be reinforced <ref> [Staddon, 1983] </ref>. <p> Staddon notes that human education as well is built up as a process of shaping if behavior is taken to include understanding <ref> [Staddon, 1983, p. 458] </ref>. agent can balance the bicycle for 3040 meters. The agent starts each trial in a equilibrium position (; _ ; !; _!; !) = (0; 0; 0; 0; 0).
Reference: [Sutton, 1996] <author> Sutton, R. S. </author> <year> (1996). </year> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 10381044. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge. </address>
Reference: [Sutton and Barto, 1998] <author> Sutton, R. S. and Barto, A. G. </author> <year> (1998). </year> <title> Introduction to Reinforcement Learning. </title> <publisher> MIT Press/Bradford Books. </publisher>
Reference-contexts: No evaluative feedback from the system other than the failure signal is available. The goal of the agent is to learn a mapping from states to actions that maximizes the agent's discounted reward over time <ref> [Bertsekas and Tsitsiklis, 1996, Sutton and Barto, 1998] </ref>. The discounted reward is the sum P 1 i=0 fl i r t+i , where fl is the discount parameter. A lot of techniques have been developed to find near optimal mappings on a trial-and-error basis. <p> Otherwise perform the learning (point 4) with Q t = 0. Niranjan [Rummery and Niranjan, 1994, Rummery, 1995, Singh and Sutton, 1996, Sutton and Barto, 1998], because empirical studies seem to suggest that this algorithm is the best so far <ref> [Rummery and Niranjan, 1994, Rummery, 1995, Sutton and Barto, 1998] </ref>. Figure 1 shows the Sarsa ()-algorithm. We have modified the algorithm slightly by cutting of eligibility traces that fall below 10 7 in order to save calculation time. <p> Long traces help the agent best. 3 Shaping The idea of shaping, which is borrowed from behavioural psychology, is to give the learning agent a series of relatively easy problems building up to the harder problem of ultimate interest <ref> [Sutton and Barto, 1998] </ref>. The term originates from the psychologist Skinner [Skinner, 1938], who studied the effect on animals, especially pigeons and rats.
Reference: [Tesauro, 1992] <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. Machine Learning, </title> <publisher> 8:257277. </publisher>
Reference-contexts: The actual task consisted of six subtasks. Secondly Gullapalli considered structural shaping: An incremental development of the learning system where a multi-level architecture is trained in parts. Gerald Tesauro's Backgammon playing agent achieved master level play through self-play <ref> [Tesauro, 1992, Tesauro, 1994, Tesauro, 1995] </ref>. This can be considered as a very succesfull example of the use of shaping. Self-play is a sort of shaping, since at first the agent plays against a nearly random opponent and thereby solves an easy task.
Reference: [Tesauro, 1994] <author> Tesauro, G. </author> <year> (1994). </year> <title> TD-Gammon, a self-teaching backgammon program, achieves master-level play. </title> <type> Technical report, </type> <institution> IBM, Thomas J. Watson Research Center, </institution> <address> Yorktown Heights, NY 10598. </address>
Reference-contexts: The actual task consisted of six subtasks. Secondly Gullapalli considered structural shaping: An incremental development of the learning system where a multi-level architecture is trained in parts. Gerald Tesauro's Backgammon playing agent achieved master level play through self-play <ref> [Tesauro, 1992, Tesauro, 1994, Tesauro, 1995] </ref>. This can be considered as a very succesfull example of the use of shaping. Self-play is a sort of shaping, since at first the agent plays against a nearly random opponent and thereby solves an easy task.
Reference: [Tesauro, 1995] <author> Tesauro, G. </author> <year> (1995). </year> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <volume> 38. </volume>
Reference-contexts: The actual task consisted of six subtasks. Secondly Gullapalli considered structural shaping: An incremental development of the learning system where a multi-level architecture is trained in parts. Gerald Tesauro's Backgammon playing agent achieved master level play through self-play <ref> [Tesauro, 1992, Tesauro, 1994, Tesauro, 1995] </ref>. This can be considered as a very succesfull example of the use of shaping. Self-play is a sort of shaping, since at first the agent plays against a nearly random opponent and thereby solves an easy task.
Reference: [Watkins, 1989] <author> Watkins, C. J. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University. </institution>
References-found: 23

