URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn14.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: MATRIX MULTIPLICATION ON THE INTEL TOUCHSTONE DELTA* Summary Access to the Intel Touchstone Delta System
Author: STEVEN HUSS-LEDERMAN ELAINE M. JACOBSON ANNA TSAO and GUODONG ZHANG 
Keyword: Key words and phrases. matrix multiplication, scalable algorithm, parallel algorithm, distributed memory, two dimensional mesh topology, MIMD.  
Note: This research was done while author was  This research is being partially supported by ARPA's Applied and Computational Mathematics Program, under contract DM28E04120.  Supercomputing Consortium is being provided by NSF. 1991 Mathematics Subject Classification.  Typeset by A  
Address: 17100 Science Drive Bowie, MD 20715-4300  3000 Waterview Parkway Richardson, TX 75083-3851  College Park, MD 20742.  
Affiliation: Supercomputing Research Center  CONVEX Computer Corporation  at Department of Mathematics, University of Maryland,  M S-T E X  
Pubnum: SRC-TR-93-101  
Date: (revised)  
Web: 65Y05, 65Y10.  
Abstract: Matrix multiplication is a key primitive in block matrix algorithms such as those found in LAPACK. We present results from our study of matrix multiplication algorithms on the Intel Touchstone Delta, a distributed memory message-passing architecture with a two-dimensional mesh topology. We analyze and compare three algorithms and obtain an implementation, BiMMeR, that uses communication primitives highly suited to the Delta and exploits the single node assembly-coded matrix multiplication. Our algorithm is completely general, i.e., able to deal with various data layouts as well as arbitrary mesh aspect ratios and matrix dimensions, and has achieved parallel efficiency of 86%, with overall peak performance in excess of 8 Gflops on 256 nodes for an 8800 fi 8800 matrix. We describe BiMMeR's design and implementation and present performance results that demonstrate scalability and robust behavior over varying mesh topologies. fl This paper is PRISM Working Note #14, available via anonymous ftp to ftp.super.org in the directory pub/prism. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Anderson, E., Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, & D. Sorensen, </author> <title> LAPACK: A portable linear algebra library for high-performance computers, </title> <booktitle> Proceedings, Supercomputing `90, </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1990, </year> <pages> pp. 2-11. </pages>
Reference: 2. <author> Anderson, E., Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, & D. Sorensen, </author> <title> LAPACK Users' Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: This is not yet the case for parallel architectures, where the set of primitives can so readily change from one machine to another, but the block algorithms of LAPACK ([1], <ref> [2] </ref>, [6]) and ScaLAPACK [9] are one step in this direction. In this paper we study algorithms for matrix multiplication on distributed-memory message-passing architectures with a two-dimensional mesh topology and develop an implementation on one such machine, the i860-based Intel Touchstone Delta.
Reference: 3. <author> Ashcraft, C. C., </author> <title> The distributed solution of linear systems using the torus wrap data mapping, </title> <institution> Engineering Computing and Analysis Technical Report ECA-TR-147, Boeing Computer Services (1990). </institution>
Reference: 4. <author> Auslander, L. & A. Tsao, </author> <title> On parallelizable eigensolvers, </title> <booktitle> Advances in Applied Mathematics 13 (1992), </booktitle> <pages> 253-261, </pages> <note> (also appeared as Technical Report SRC-TR-91-028, </note> <institution> Supercomputing Research Center). </institution>
Reference: 5. <author> Barnett, M., D. G. Payne, & R. van de Geijn, </author> <title> Optimal broadcasting in mesh-connected architectures, </title> <type> preprint, </type> <note> also appears as University of Texas Computer Science Technical Report TR-91-38 (December, </note> <year> 1991). </year>
Reference-contexts: It can also handle linear arrays and rings embedded in the two-dimensional mesh. Our broadcast code also operates on arbitrary subsets of single rows (columns) of processors and on all rows (columns) in a rectangular submesh. It is a generalization of a similar algorithm given in <ref> [5] </ref>, free of contention and concurrency. In our design, the number of processors in a row (column) can be any size and is not restricted to powers of two; we have also optimized the lengths of communication paths.
Reference: 6. <author> Bischof, C. H., </author> <title> LAPACK: Linear Algebra Software for Supercomputers, </title> <type> Preprint MCS-P236-0491, </type> <institution> Argonne National Laboratory (July, </institution> <year> 1991). </year>
Reference-contexts: This is not yet the case for parallel architectures, where the set of primitives can so readily change from one machine to another, but the block algorithms of LAPACK ([1], [2], <ref> [6] </ref>) and ScaLAPACK [9] are one step in this direction. In this paper we study algorithms for matrix multiplication on distributed-memory message-passing architectures with a two-dimensional mesh topology and develop an implementation on one such machine, the i860-based Intel Touchstone Delta.
Reference: 7. <author> Bischof, C. H., S. Huss-Lederman, E. M. Jacobson, X. Sun, & A. Tsao, </author> <title> On the impact of HPF data layout on the design of efficient and maintainable parallel linear algebra libraries, </title> <institution> Argonne National Lab ANL/MCS-TM-184, </institution> <note> (available from the archives of the HPF Forum). </note>
Reference-contexts: This data layout has been previously discussed in [16] and <ref> [7] </ref>.
Reference: 8. <author> Cannon, L.E., </author> <title> A cellular computer to implement the Kalman filter algorithm, </title> <type> Ph.D. Thesis (1969), </type> <institution> Montana State University. </institution>
Reference: 9. <author> Choi, J., J. J. Dongarra, R. Pozo, & D. W. Walker, </author> <title> ScaLAPACK: A scalable linear algebra library for distributed memory concurrent computers, </title> <booktitle> Proceedings, Fourth Symposium on the Frontiers of Massively Parallel Computation (McLean, </booktitle> <address> Virginia, </address> <month> October 19-21, </month> <title> 1992), </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1992, </year> <pages> pp. 120-127. </pages>
Reference-contexts: This is not yet the case for parallel architectures, where the set of primitives can so readily change from one machine to another, but the block algorithms of LAPACK ([1], [2], [6]) and ScaLAPACK <ref> [9] </ref> are one step in this direction. In this paper we study algorithms for matrix multiplication on distributed-memory message-passing architectures with a two-dimensional mesh topology and develop an implementation on one such machine, the i860-based Intel Touchstone Delta. <p> Finally, while our orientation was machine-specific and aimed at optimization rather than portability, the resulting kernel could still be useful to future portable distributed-memory software such as ScaLAPACK <ref> [9] </ref>, being an optimized version of one of the core Level 3 BLAS building blocks [12]. Indeed, BiMMeR parallels PUMMA, the prototype matrix multiplication software of ScaLAPACK, but differs from it in several respects. A comparison can be found in [21]. <p> However, many other algorithms that might call such a matrix multiplication routine utilize a scattered data layout. One scattered distribution scheme is the block scattered layout ([3], <ref> [9] </ref>, [10]), where successive matrix blocks of size r fi s are assigned to successive physical processors in both mesh/matrix dimensions.
Reference: 10. <author> Choi, J., J. J. Dongarra, & D. W. Walker, </author> <title> Level 3 BLAS for distributed memory concurrent computers, </title> <booktitle> CNRS-NSF Workshop on Environments and Tools for Parallel Scientific Computing (Saint Hilaire du Touvet, </booktitle> <address> France, September 7-8, 1992), </address> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference-contexts: However, many other algorithms that might call such a matrix multiplication routine utilize a scattered data layout. One scattered distribution scheme is the block scattered layout ([3], [9], <ref> [10] </ref>), where successive matrix blocks of size r fi s are assigned to successive physical processors in both mesh/matrix dimensions. <p> The r and s are application-dependent panel widths determined 7 to be advantageous for local computation, while scattering the blocks across physical processors helps maintain good load balance in applications such as eigensolvers [23] and LU factorization [20]. Unfortunately, as noted in <ref> [10] </ref>, for the important basic operation of matrix multiplication, the contiguous layout is optimal; the algorithm is already load balanced, and spreading the data to the level achieved by the block scattered scheme often incurs extra communication overhead.
Reference: 11. <author> Dally, W. J., </author> <title> Performance analysis of k-ary n-cube interconnection networks, </title> <journal> IEEE Transactions on Computers 39 (1990), </journal> <volume> no. 6, </volume> <pages> 775-785. </pages>
Reference: 12. <author> Dongarra, J. J., J. Du Croz, S. Hammarling, & I. Duff, </author> <title> A set of level 3 basic linear algebra subprograms, </title> <journal> ACM Transactions on Mathematical Software 16 (1990), </journal> <pages> 1-17. </pages>
Reference-contexts: Finally, while our orientation was machine-specific and aimed at optimization rather than portability, the resulting kernel could still be useful to future portable distributed-memory software such as ScaLAPACK [9], being an optimized version of one of the core Level 3 BLAS building blocks <ref> [12] </ref>. Indeed, BiMMeR parallels PUMMA, the prototype matrix multiplication software of ScaLAPACK, but differs from it in several respects. A comparison can be found in [21].
Reference: 13. <author> Dongarra, J. J., I. S. Duff, D. C. Sorensen, & H. A. van der Vorst, </author> <title> Solving Linear Systems on Vector and Shared Memory Computers, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: Second, our present target machine represented a new architecture on which the costs of data motion were not clear. Just as reducing the cost of hierarchical memory data access is crucial to the design of efficient algorithms for vector and shared-memory machines <ref> [13] </ref>, the same is true in a multicomputer environment, where we want to reduce the cost of data movement across the distributed memory.
Reference: 14. <author> Dongarra, J., C. B. Moler, J. R. Bunch, & G. W. Stewart, </author> <title> LINPACK User's Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: 1. Introduction The multiplication of two matrices is one of the most basic operations of scientific computing. Versions for serial computers have long been based on optimized primitives embodied in the kernels of standard software packages, such as LINPACK <ref> [14] </ref>. A stable and fairly uniform set of appropriate kernels well-suited to most serial machines makes these implementations hard to beat.
Reference: 15. <author> Dunigan, T. H., </author> <title> Communication performance of the Intel Touchstone Delta mesh, </title> <type> Technical Report ORNL/TM-11983, </type> <institution> Oak Ridge National Laboratory (January, </institution> <year> 1992). </year>
Reference-contexts: One immediate consequence is that these routing techniques should limit communication costs due to internode distance, i.e., there should be little difference in delivery time between a message sent to a nearest neighbor or to a node several `hops' away. This has been confirmed in timing studies <ref> [15] </ref>. Thus we may assume hops are nearly free, and our communications cost formulas will not include internode distance as a parameter. In addition, although the Delta is a true two-dimensional mesh, the above indicates that we may also assume it is effectively a two-dimensional torus, i.e., has wraparound wires.
Reference: 16. <author> Falgout, R. D., A. Skjellum, S. G. Smith, & C. H. </author> <title> Still, The Multicomputer Toolbox approach to concurrent BLAS and LACS, </title> <booktitle> Proceedings, Scalable High Performance Computing Conference (Williamsburg, </booktitle> <address> VA, April 26-29, 1992), </address> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1992, </year> <pages> pp. 121-28. </pages>
Reference-contexts: In addition, panels can be arbitrarily assigned to virtual processors, and a very useful case giving further spreading across physical processors can be obtained by specifying spacing parameters s r and s c in the row and column directions, respectively. This data layout has been previously discussed in <ref> [16] </ref> and [7].
Reference: 17. <author> Fox, G., </author> <title> Domain decomposition in distributed and shared memory environments, </title> <booktitle> Lecture Notes in Comput. Sci. 297: Proc., 1987 Intl. Conf. Supercomputing, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1987, </year> <pages> pp. 1042-1073. </pages>
Reference: 18. <author> Fox, G., M. Johnson, G. Lyzenga, S. Otto, J. Salmon, & D. Walker, </author> <title> Solving Problems on Concurrent Processors, Vol. I, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year> <month> 20 </month>
Reference-contexts: We consider three standard algorithms: 1D-systolic (1D) [20], 2D-systolic or Cannon's algorithm (2D) ([8],[20]), and Broadcast-Multiply-Roll (BMR) ([17], <ref> [18] </ref>, [19]). We quickly review the algorithms while examining their communication requirements. Descriptions of the algorithms could be given from the matrix point-of-view, i.e., seeing first the matrix layout and marking entries with the processor location where the entry is stored.
Reference: 19. <author> Fox, G., S. Otto, & A. Hey, </author> <title> Matrix algorithms on a hypercube I: Matrix multiplication, </title> <booktitle> Parallel Computing 4 (1987), </booktitle> <pages> 17-31. </pages>
Reference-contexts: We consider three standard algorithms: 1D-systolic (1D) [20], 2D-systolic or Cannon's algorithm (2D) ([8],[20]), and Broadcast-Multiply-Roll (BMR) ([17], [18], <ref> [19] </ref>). We quickly review the algorithms while examining their communication requirements. Descriptions of the algorithms could be given from the matrix point-of-view, i.e., seeing first the matrix layout and marking entries with the processor location where the entry is stored.
Reference: 20. <author> Golub, G. & C. F. Van Loan, </author> <title> Matrix Computations, 2nd ed., </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: We consider three standard algorithms: 1D-systolic (1D) <ref> [20] </ref>, 2D-systolic or Cannon's algorithm (2D) ([8],[20]), and Broadcast-Multiply-Roll (BMR) ([17], [18], [19]). We quickly review the algorithms while examining their communication requirements. <p> The r and s are application-dependent panel widths determined 7 to be advantageous for local computation, while scattering the blocks across physical processors helps maintain good load balance in applications such as eigensolvers [23] and LU factorization <ref> [20] </ref>. Unfortunately, as noted in [10], for the important basic operation of matrix multiplication, the contiguous layout is optimal; the algorithm is already load balanced, and spreading the data to the level achieved by the block scattered scheme often incurs extra communication overhead.
Reference: 21. <author> Huss-Lederman, S., E. M. Jacobson, & A. Tsao, </author> <title> Comparison of scalable parallel matrix multiplication libraries, </title> <booktitle> Proceedings, Scalable Parallel Libraries Conference (Starksville, </booktitle> <address> MS, </address> <month> Oct. </month> <pages> 6-8, </pages> <year> 1993), </year> <note> IEEE, </note> <year> 1993, </year> <pages> pp. 142-149, </pages> <note> (also PRISM Working Note #13, also appears as Technical Report SRC-TR-93-108, </note> <institution> Supercomputing Research Center, </institution> <year> 1993). </year>
Reference-contexts: Indeed, BiMMeR parallels PUMMA, the prototype matrix multiplication software of ScaLAPACK, but differs from it in several respects. A comparison can be found in <ref> [21] </ref>. Our analysis of matrix multiplication on the Intel Delta assumes an optimized single-processor matrix multiplication subroutine and uses the i860 assembly-coded implementation of DGEMM for this purpose [27]. Thus, we can focus our efforts on gaining performance through careful choice of distributed algorithm, distributed data layout, and message-passing primitives.
Reference: 22. <author> Huss-Lederman, S., E. M. Jacobson, A. Tsao, & G. Zhang, </author> <title> Optimizing communication primitives on the Intel Touchstone Delta, </title> <type> Technical Report, </type> <note> Supercomputing Research Center (to appear). </note>
Reference-contexts: Over p processors its contention level is p=2; as such it can cost no more than a factor of p=2 over the time required for a contention-free communication on the slowest node, since we could instead perform p=2 sequential contention-free steps. It is observed <ref> [22] </ref> that costs for this pattern increase linearly with p, and cost factors approach this upper bound of p=2 for large data sizes. Thus, our cost estimate for this high contention skew over p processors on large matrices is 2 (p=2) = p sequential communication steps. <p> Thus, our cost estimate for this high contention skew over p processors on large matrices is 2 (p=2) = p sequential communication steps. Further details on these primitives can be found in Section 4 and <ref> [22] </ref>. We have already seen that internode distance is not an important parameter, so the cost of sending w data elements can be modeled as b + cw, where b is a constant start-up cost and the remaining term is linear in the message size. <p> Since our design is to favor large problems, startup should be small compared to the remaining costs for sending a large amount of data. This is borne out in performance studies <ref> [22] </ref>. Thus, to simplify formulas we will neglect startup and set b = 0. Our results would also hold for non-zero b. Also, since our objective is comparison, without loss of generality we may assume c = 1. <p> We are thus pushed towards reliance on the broadcast rather than the skew primitive and conclude that BMR is our algorithm of choice. The BiMMeR software is thus based on the BMR algorithm and optimized implementations of its two communication primitives: roll and 1D broadcast. See <ref> [22] </ref> for details on the design and optimization of these primitives on the Delta. We now summarize with a few descriptive comments. <p> this occurs (a zero-length message sent from receiver to sender will do), this is a small price to pay for the large gain in performance that can be seen when forced-type is used (<ref> [22] </ref>,[29],[32]). As an illustration, we tested the two-step roll primitive using 10 different versions of message passing [22]. The three with best performance for moderate to large messages are all variations of forced-type communication. We thus use forced-type in our implementation.
Reference: 23. <author> Huss-Lederman, S., A. Tsao, & G. Zhang, </author> <title> A parallel implementation of the Invariant Subspace Decomposition Algorithm for dense symmetric matrices, </title> <booktitle> Proceedings, Sixth SIAM Conference on Parallel Processing for Scientific Computing (Norfolk, </booktitle> <address> Virginia, </address> <month> March 22-24, </month> <editor> 1993) (R. F. Sincovec, ed.), </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993, </year> <pages> pp. 367-374, </pages> <note> (also PRISM Working Note #9, also appears as Technical Report SRC-TR-93-091, </note> <institution> Supercomputing Research Center, </institution> <year> 1993). </year>
Reference-contexts: The r and s are application-dependent panel widths determined 7 to be advantageous for local computation, while scattering the blocks across physical processors helps maintain good load balance in applications such as eigensolvers <ref> [23] </ref> and LU factorization [20]. Unfortunately, as noted in [10], for the important basic operation of matrix multiplication, the contiguous layout is optimal; the algorithm is already load balanced, and spreading the data to the level achieved by the block scattered scheme often incurs extra communication overhead.
Reference: 24. <author> Intel Corporation, </author> <title> i860 64-Bit Microprocessor Programmer's Reference Manual, </title> <year> 1990. </year>
Reference: 25. <institution> Intel Supercomputing Systems Division, Intel Touchstone Delta System Description, Advanced Information, Intel Corporation (February, </institution> <year> 1991). </year>
Reference-contexts: This explains the highly synchronous nature of all three candidate algorithms. The compute processors are on node boards that also contain a FIFO interface to a backplane of separate routing chips (MRCs) that form the actual two-dimensional mesh <ref> [25] </ref>. Communication between MRCs occurs on two byte-wide communication channels, one in and one out, which operate at 65 MB/s aggregate bandwidth. In addition, switching is fast; MRC latency is 75 nanoseconds to move in the same direction, 150 nanoseconds to change direction.
Reference: 26. <institution> Intel Supercomputing Systems Division, Paragon Supercomputers, Order Number 203/6/92/ 10K/GA, Intel Corporation (1992). </institution>
Reference-contexts: While these had a hypercube topology, the Delta is a two-dimensional mesh and represents the alternate design choice of having more wires between connected processors but less connectivity than the earlier machines ([11],[25]). The Delta is a prototype for Intel's follow-on machine, the Paragon <ref> [26] </ref>. Developed for the Concurrent Supercomputing Consortium and housed at the California Institute of Technology, the Delta consists of 528 i860 XR compute processors ([24], [30], [34]).
Reference: 27. <author> Kuck & Associates, Inc., CLASSPACK, </author> <title> Basic Math Library User's Guide, Release 1.2, Document # 9202003, </title> <year> 1992. </year>
Reference-contexts: A comparison can be found in [21]. Our analysis of matrix multiplication on the Intel Delta assumes an optimized single-processor matrix multiplication subroutine and uses the i860 assembly-coded implementation of DGEMM for this purpose <ref> [27] </ref>. Thus, we can focus our efforts on gaining performance through careful choice of distributed algorithm, distributed data layout, and message-passing primitives.
Reference: 28. <author> Lillevik, S. L., </author> <title> The Touchstone 30 Gigaflop DELTA Prototype, </title> <booktitle> Proceedings, Sixth Distributed Memory Computing Conference (Portland, </booktitle> <address> Oregon, </address> <month> April 28 May 1, </month> <note> 1991) (Stout, </note> <editor> Q. & M. Wolfe, eds.), </editor> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1991, </year> <pages> pp. 671-677. </pages>
Reference-contexts: The Intel Touchstone Delta In this section we present an overview of the Intel Delta architecture, focusing primarily on its characteristics that influenced our choice of algorithm. The Delta is a descendant of the earlier Intel iPSC/1, iPSC/2, and iPSC/860 (Gamma) machines <ref> [28] </ref>. While these had a hypercube topology, the Delta is a two-dimensional mesh and represents the alternate design choice of having more wires between connected processors but less connectivity than the earlier machines ([11],[25]). The Delta is a prototype for Intel's follow-on machine, the Paragon [26].
Reference: 29. <author> Littlefield, R., </author> <title> Characterizing and tuning communications performance for real applications, presentation overheads, </title> <booktitle> Proceedings, First Intel Delta Applications Workshop, </booktitle> <month> CCSF-14-92 (February, </month> <year> 1992), </year> <title> Caltech Concurrent Supercomputing Facilities, </title> <address> Pasadena, California, </address> <year> 1992, </year> <pages> pp. 179-190. </pages>
Reference-contexts: One important feature of the i860 that influenced our design is that it has a write-back cache, requiring the processor to mediate all memory requests. Context-switching delays thus thwart attempts to overlap well-tuned communication with computation <ref> [29] </ref>. This explains the highly synchronous nature of all three candidate algorithms. The compute processors are on node boards that also contain a FIFO interface to a backplane of separate routing chips (MRCs) that form the actual two-dimensional mesh [25]. <p> The roll primitive required by all the algorithms is shown in Figure 11 (a). Its inherent concurrency explains the measured result that for moderate to large messages it is more efficient to guarantee that any given processor is only sending or only receiving at any given time ([22], <ref> [29] </ref>). Thus, for example, odd-numbered nodes can send first, and then receive, so that a roll costs two sequential communication steps. The 1D broadcast primitive used by BMR does not exhibit problems with concurrency, but could have problems with contention.
Reference: 30. <author> Margulis, Neal, </author> <title> i860 Microprocessor Architecture, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: The Delta is a prototype for Intel's follow-on machine, the Paragon [26]. Developed for the Concurrent Supercomputing Consortium and housed at the California Institute of Technology, the Delta consists of 528 i860 XR compute processors ([24], <ref> [30] </ref>, [34]). Each has a peak double precision rate of 60 Mflops, 40 Mflops if the number of multiplications and additions is equal, as in standard matrix multiplication.
Reference: 31. <author> Mathur, K. K. & S. L. Johnsson, </author> <title> Multiplication of matrices of arbitrary shape on a data parallel computer (1992), Thinking Machines Corporation, </title> <type> preprint, </type> <note> also released as Technical Report TR-216. </note>
Reference-contexts: B 53 B 54 B 55 While direct imitation of the 2D systolic algorithm on the virtual mesh leads to a large amount of skewing on multiple columns (rows) of blocks in single processors, as well as fine-grained local multiplications on matrices of size N=ff fi N=ff, as shown in <ref> [31] </ref>, it is possible to reduce the skewing costs and increase granularity. We first skew all the A data in processor row i left by (ff=p) i virtual steps and all the B data in processor column j up (ff=q) j virtual steps. <p> In virtual 2D torus wrap, we assign successive blocks to virtual processors in both mesh dimensions. If assignment is to successive virtual processors and r = s = N=ff, we obtain a contiguous data layout. This contiguous virtual layout is used, for example, in <ref> [31] </ref>. Smaller r; s, 1 r; s N=ff, results in more spreading of data across physical processors; for example, suppose r = s = N=12, so that N = 12r = 12s, and M contains 12 panels in each dimension. Figure 9 shows assignment of blocks to successive virtual processors.
Reference: 32. <author> Regnier, G., </author> <title> Intel Touchstone Delta message passing performance, Intel Supercomputing Systems Division (August, 1991), </title> <type> preprint. </type>
Reference: 33. <author> Regnier, G., </author> <title> Delta message passing protocol, presentation overheads, </title> <booktitle> Proceedings, First Intel Delta Applications Workshop, </booktitle> <month> CCSF-14-92 (February, </month> <year> 1992), </year> <title> Caltech Concurrent Supercomputing Facilities, </title> <address> Pasadena, California, </address> <year> 1992, </year> <pages> pp. 173-178. </pages>
Reference-contexts: Packetized, e-cube (x-direction first) circuit-switched wormhole routing is used, together with additional protocols to avoid deadlock by, for the most part, guaranteeing that messages are `consumed' by receiving nodes <ref> [33] </ref>. This takes the form of system buffering and system flow control (handshaking) and incurs extra message-passing overhead. As we will see later, if the user can guarantee that receiving nodes are ready to `consume' messages, this overhead can be avoided.
Reference: 34. <author> Scott, D. S., & G. R. Withers, </author> <title> Performance and assembly language programming of the iPSC/860 System, </title> <booktitle> Proceedings, Sixth Distributed Memory Computing Conference (Portland, </booktitle> <address> Oregon, </address> <month> April 28 May 1, </month> <note> 1991) (Stout, </note> <editor> Q. & M. Wolfe, eds.), </editor> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1991, </year> <pages> pp. 534-541. </pages>
Reference-contexts: The Delta is a prototype for Intel's follow-on machine, the Paragon [26]. Developed for the Concurrent Supercomputing Consortium and housed at the California Institute of Technology, the Delta consists of 528 i860 XR compute processors ([24], [30], <ref> [34] </ref>). Each has a peak double precision rate of 60 Mflops, 40 Mflops if the number of multiplications and additions is equal, as in standard matrix multiplication.
Reference: 35. <author> Van de Geijn, R. A., </author> <title> Massively parallel LINPACK benchmark on the Intel Touchstone Delta and iPSC/860 systems: </title> <type> Progress Report, </type> <institution> Computer Science Technical Report TR-91-28, University of Texas (1991). </institution> <month> 21 </month>
References-found: 35

