URL: http://www.cs.berkeley.edu/~cpwen/cpwen/phd.ps
Refering-URL: http://www.cs.berkeley.edu/~cpwen/
Root-URL: http://www.cs.berkeley.edu
Title: Portable Library Support for Irregular Applications  
Author: by Chih-Po Wen 
Degree: 1988 M.S. (University of California at Berkeley) 1992 A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at BERKELEY Committee in charge: Professor Katherine A. Yelick, Chair Professor Susan L. Graham Professor Stuart Dreyfus  
Date: 1995  
Affiliation: B.A. (National Chiao-Tung University, Taiwan)  
Abstract-found: 0
Intro-found: 1
Reference: [ABB + 92] <author> E. Anderson, Z. Bai, C. Bischoff, James Demmel, Jack J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchev, and D. Sorenson. </author> <note> LAPACK Users' Guide. SIAM, </note> <year> 1992. </year>
Reference-contexts: The library is also extensible and can provide domain-specific abstractions. This approach has been used successfully in several areas including dense matrix algorithms <ref> [Dem89, ABB + 92, CDPW92] </ref>, unstructured mesh computation [DHU + 93, DUSH94, MSH + 95], and adaptive mesh computation [KB95]. While their applications include some degree of irregularity, these libraries do not handle task parallelism or asynchronous communication. <p> Our approach provides more insight into the techniques for optimizing irregular applications. There has been a great deal of work on specialized libraries for scientific applications. Examples include LAPACK <ref> [Dem89, ABB + 92] </ref> and ScaLAPACK [CDPW92], which are libraries of SPMD algorithms for solving linear algebra problems such as matrix multiplication, matrix factorization, and eigenvalue computations. Unlike the Multipol library, they contain standalone procedures rather than data structures and they solve mostly regular problems.
Reference: [ABC + 88] <author> Frances E. Allen, Michael Burke, Philippe Charles, Ron Cytron, and Jeanne Ferrante. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 617-640, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: One approach to parallel programming that addresses the portability problem is to use so-called "heroic" compilers such as parallelizing Fortran compilers <ref> [HKT91, ABC + 88, PGH + 90] </ref> or optimizing compilers for data parallel languages [CMF92, Hig92]. The programmer relies on the compiler to discover parallelism and to schedule communication and computation.
Reference: [AL90] <author> T. Anderson and E. Lazowska. Quartz: </author> <title> A tool for tuning parallel program performance. </title> <booktitle> In Proc. 1990 ACM SIGMETRICS Conference on the Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: To make performance prediction possible, the statistics in their cost models cannot be computed values. Hollingsworth [HM92] compared several performance metrics for detecting problematic program components which include the critical path metric [YM88] and the NPT metric <ref> [AL90] </ref>. The critical path metric measures the amount of time spent by each procedure in the longest running path of the execution.
Reference: [BCF + 95] <author> N. J. Baden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. L. Seitz, J. N. Seizovic, and W. K. Su. </author> <title> Myrinet a gigabit-per-second local-area network. </title> <journal> IEEE-Micro, </journal> <volume> 15, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: The bandwidth is the peak point-to-point bandwidth attainable by the communication library. The processors can be connected by a variety of networks such as the fat-tree interconnect of the CM5 [LAD + 92], the mesh interconnect of the Paragon [LC95], and the switched LAN (e.g., Myrinet <ref> [BCF + 95] </ref>) of the Sparc cluster built by the NOW [CLMY96] group at Berkeley. The native communication libraries on these machines typically have high start-up overhead, and therefore bandwidth increases with the size of the physical message.
Reference: [BDG + 91] <author> A. Beguelin, J. Dongarra, A. Geist, R. Manchek, and V. Sunderam. </author> <title> A users' guide to PVM parallel virtual machine. </title> <type> Technical Report ORNL/TM-11826, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Examples of these irregular applications include unstructured mesh computations, divide and conquer algorithms, and discrete event simulation. Another approach to parallel programming involves explicit message passing with message passing layers such as PVM <ref> [BDG + 91] </ref> and MPI [For94]. Such message passing layers are often used when performance is the most important concern, because they allow the programmer to have complete control over parallelism and communication.
Reference: [BJK + 95] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Principles and Practice of Parallel Programming, </booktitle> <year> 1995. </year>
Reference-contexts: Many load balancing protocols for distributed memory platforms have been proposed. Their major differences are not in the algorithms, but in the surrounding software support. Languages and runtime layers such as Charm [SK91, KK93] and Cilk <ref> [BJK + 95] </ref> 40 have built-in language mechanisms to allocate parallel tasks that are automatically migrated by a embedded load balancer. These systems have a fixed set of load balancing and scheduling policies that cannot be easily customized for a particular application. <p> Threads and fibers are used only for latency hiding. They are not meant to be used as units of true parallelism or load balance and they do not migrate across processors. Unlike other compilers or runtime systems with built-in load balancers <ref> [SK91, BJK + 95, GSC95, MKH91, CR95] </ref>, load balancing mechanisms are left out of the Multipol runtime layer so that the programmer can have more control over locality and scheduling. Load balancing can be performed by the task stealer data structure or other data structures implemented by the user. <p> However, sockets are rather heavy-weight, and sending a message incurs at least 800 microseconds of overhead on the Sparc cluster. 3.6 Related Work The fiber abstraction in the Multipol runtime layer is similar to the thread abstraction in TAM [CSS + 91], Cilk <ref> [BJK + 95] </ref>, and Nexus [FKOT91], and the chare abstraction in Charm++ [SK91, KK93]. TAM is a threaded abstraction machine for data-flow computation.
Reference: [BL94] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Scheduling multithreaded computations by work stealing. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science (FOCS '94), </booktitle> <pages> pages 356-368, </pages> <address> Santa Fe, New Mexico, </address> <month> November </month> <year> 1994. </year> <month> 117 </month>
Reference-contexts: Task pushing has been found to achieve better load balance for certain applications we encountered (such as the Eigenvalue program described below). However, the theoretical optimality of both types of load balancing protocols have been reported <ref> [BL94, CRY94] </ref>. Task migration creates problems for termination detection, because some migrated tasks may be in transit in the network layer when all local task pools are empty.
Reference: [BRB90] <author> K.S. Brace, R.L. Rudell, and R.E. Bryant. </author> <title> Efficient implementation of a BDD package. </title> <booktitle> In 27th ACM/IEEE Design Automation Conference, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The data structure is used to implement a distributed set of key/value pairs. A set can be used to cache the result of computation or to guarantee the uniqueness of data elements. Its applications include logic verification algorithms <ref> [BRB90] </ref> and path counting problems such as the Tripuzzle application described in Section 2.2.2.3. 2.2.2.1 Interface A hash table contains a collection of bins for storing distinct data entries.
Reference: [Bre94] <author> Eric Brewer. </author> <title> Portable High-Performance Supercomputing: High-Level Platform-Dependent Optimization. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> Septem-ber </month> <year> 1994. </year>
Reference-contexts: Although direct use of the 2 probability is difficult, we can use it to compare the quality of different instantiations of the cost models. * The mean relative error (MRE) of the fit <ref> [Bre94] </ref>. MRE is defined as the geometric mean of the error ratios (1 + jt e x t e j=t e ) over all samples. <p> The statistical approach to modeling the performance of parallel programs has been taken by other researchers such as Brewer <ref> [Bre94] </ref> and Crovella [Cro94]. However, their work targeted standalone procedures and a fixed set of overhead categories, and they did not address the composition of concurrent data structures. Also, their applications are regular, bulk-synchronous programs whose performance can be predicted prior to the computation. <p> Their work was restricted to shared memory platforms where global variables can be updated and probed with low overhead, and they did not address the interaction of data structures. The statistical approach to performance modeling has been taken by Brewer <ref> [Bre94] </ref> and Crovella [Cro94] in predicting the performance of bulk-synchronous programs. As in Mprof, they assume the cost models are linear combinations of terms. Brewer used statistical models to predict the performance of different implementations of the same procedure.
Reference: [BSS91] <author> H. Berryman, J. Saltz, and J. Scroggs. </author> <title> Execution time support for adaptive scientific algorithms on distributed memory multiprocessors. </title> <journal> Concurrency: Practice and Experience, </journal> <pages> pages 159-178, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The use of a library allows for portability, because the high-level programming interface provided by the library shields the application programmers from the implementation details. The library approach has been adopted by PARTI <ref> [BSS91] </ref>, CHAOS [DHU + 93, DUSH94, MSH + 95] and LPARX [KB95], which are specialized libraries for unstructured and adaptive mesh applications. They handle certain irregular data structures, but are limited to bulk synchronous computation patterns. <p> Examples include LAPACK [Dem89, ABB + 92] and ScaLAPACK [CDPW92], which are libraries of SPMD algorithms for solving linear algebra problems such as matrix multiplication, matrix factorization, and eigenvalue computations. Unlike the Multipol library, they contain standalone procedures rather than data structures and they solve mostly regular problems. PARTI <ref> [BSS91] </ref> and CHAOS [DHU + 93, DUSH94, MSH + 95] (an enhanced version of PARTI) are runtime layers designed to support simulations on spatially irregular data structures. They generate optimized communication schedules to reduce the overhead of accessing such data structures. LPARX [KB95] is a library for adaptive mesh computation.
Reference: [CDG + 93] <author> David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishna-murthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: For each data structure, we describe the programming interface, the implementation techniques, and the driving applications. For each example application, we describe its irregularities and execution behaviors using measurements from parallel executions. Several of the applications are described more completely elsewhere <ref> [CDG + 93, DDR94, JY95, WY95] </ref>. The data structures and applications are then summarized and compared in Section 2.3. <p> In the sections that follow, we use these statistics to characterize the application workload. 2.2.1 Bipartite Graph The bipartite graph data structure was implemented by Etienne Deprit [YCD + 95] based on an implementation by the Split-C group <ref> [CDG + 93] </ref>. It is used to perform bulk-synchronous computation over an unstructured mesh. Each node in the graph has a color, which is either red or black and, by construction, edges only connect nodes of different colors. <p> The fiber may reply to the sender with an acknowledgement message, although a more typical scenario is to acknowledge multiple store operations at the end of a phase, such as in the EM3D program. The bulk accesses in our runtime layer are modeled after the Split-C <ref> [CDG + 93] </ref> bulk accesses and the GAM [CKK + 94] store operation, except that our primitives have a split-phase interface that is integrated with the Multipol thread layer. 3.3.3 Implementation and Optimization The communication primitives are translated into the machine primitives by the communication layer, which handles buffer allocation and <p> The runtime layer has been ported on three types of communication libraries: active messages (CMAML), cooperative message passing (MPLp, NX), and BSD sockets using the TCP/IP protocol stack. We reuse code from GAM [LC95, CKK + 94] and libsplit-c [Lun94], the communication library for the Split-C language <ref> [CDG + 93] </ref>. Most modifications to their code are for integrating the the communication primitives with the Multipol thread layer. In the paragraphs that follow, we sketch the implementation of these ports and describe the source of their overheads.
Reference: [CDPW92] <author> J. Choi, J. Dongarra, R. Pozo, and D. Walker. </author> <title> ScaLAPACK: A scalable linear algebra library for distributed memory concurrent computers. </title> <booktitle> In Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The library is also extensible and can provide domain-specific abstractions. This approach has been used successfully in several areas including dense matrix algorithms <ref> [Dem89, ABB + 92, CDPW92] </ref>, unstructured mesh computation [DHU + 93, DUSH94, MSH + 95], and adaptive mesh computation [KB95]. While their applications include some degree of irregularity, these libraries do not handle task parallelism or asynchronous communication. <p> Our approach provides more insight into the techniques for optimizing irregular applications. There has been a great deal of work on specialized libraries for scientific applications. Examples include LAPACK [Dem89, ABB + 92] and ScaLAPACK <ref> [CDPW92] </ref>, which are libraries of SPMD algorithms for solving linear algebra problems such as matrix multiplication, matrix factorization, and eigenvalue computations. Unlike the Multipol library, they contain standalone procedures rather than data structures and they solve mostly regular problems.
Reference: [CKK + 94] <author> David Culler, Kim Keeton, Cedric Krumbein, Lok Tin Liu, Alan Mainwar-ing, Rich Martin, Steve Rodrigues, Kristin Wright, and Chad Yoshikawa. </author> <title> The generic active message interface specification. </title> <type> Technical report, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1994. </year>
Reference-contexts: Communication latency varies with the physical distance between the nodes, but the difference is usually negligible compared to the communication overhead. work. For portability, we use the communication libraries provided by the vendors instead of the research prototypes such as the generic active messages (GAM) <ref> [CKK + 94] </ref> developed by the NOW group. The measurements for the CM5, the Paragon, and the SP1 are from Luna [Lun94], and the measurements for the Sparc cluster are from Keeton et al [KAP95]. <p> First, active messages take a fixed number of arguments (typically 4 words) for efficiency, and leave the problems of buffer allocation and fragmentation to the upper-level software. A later version of active messages called generic active messages (GAM) <ref> [CKK + 94] </ref> allows the programmer to invoke the message handler with an arbitrary number of arguments, but the upper-level software is still required to allocate the storage prior to the communication. Second, the handler code must follow a request/reply protocol to avoid network level deadlock [vECGS92]. <p> The bulk accesses in our runtime layer are modeled after the Split-C [CDG + 93] bulk accesses and the GAM <ref> [CKK + 94] </ref> store operation, except that our primitives have a split-phase interface that is integrated with the Multipol thread layer. 3.3.3 Implementation and Optimization The communication primitives are translated into the machine primitives by the communication layer, which handles buffer allocation and flow control. <p> The runtime layer has been ported on three types of communication libraries: active messages (CMAML), cooperative message passing (MPLp, NX), and BSD sockets using the TCP/IP protocol stack. We reuse code from GAM <ref> [LC95, CKK + 94] </ref> and libsplit-c [Lun94], the communication library for the Split-C language [CDG + 93]. Most modifications to their code are for integrating the the communication primitives with the Multipol thread layer.
Reference: [CL85] <author> K. M. Chandy and L. Lamport. </author> <title> Distributed snapshots: Determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <year> 1985. </year>
Reference-contexts: The state of the individual task pools can then be probed and combined to determine global termination. If tasks become available when termination detection is taking place, they can be processed immediately without delay. Although our termination detection protocol is not fully incremental <ref> [CL85] </ref> in that it blocks task migration, it does not impact performance in practice because the protocol is rarely invoked. <p> The solid arrows denote the dependencies between threads. The snapshot obtained is the object state after access A completes but before access B takes place. 61 freeze. Many distributed snapshot algorithms have been proposed, notably the algorithm by Chandy and Lamport <ref> [CL85] </ref> for distributed FIFO channels. The Multipol snapshot data structure is different from these algorithms in that it has stronger semantics.
Reference: [CL93] <author> M. Crovella and T. LeBlanc. </author> <title> Performance debugging using parallel performance predicates. </title> <booktitle> In 3rd ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: However, Mprof provides summary information about the entire execution, and it cannot provide information about a particular time period in the execution. Instrumentation and sampling methods are popular alternatives to event tracing. Research in this area includes Paradyne [MCC + 95] and the work by Crovella and LeBlanc <ref> [CL93] </ref>. The Paradyne toolkit instruments the program binaries to collect performance statistics. It allows the programmer to specify conditions to dynamically control the degree of instrumentation. Uninstrumented code can be used for program components that do not contribute to performance inefficiencies.
Reference: [CLMY96] <author> David Culler, Lok T. Liu, Richard Martin, and Chad Yoshikawa. </author> <title> LogP performance assessment of fast network interfaces. </title> <booktitle> IEEE Micro, </booktitle> <year> 1996. </year> <month> 118 </month>
Reference-contexts: The processors can be connected by a variety of networks such as the fat-tree interconnect of the CM5 [LAD + 92], the mesh interconnect of the Paragon [LC95], and the switched LAN (e.g., Myrinet [BCF + 95]) of the Sparc cluster built by the NOW <ref> [CLMY96] </ref> group at Berkeley. The native communication libraries on these machines typically have high start-up overhead, and therefore bandwidth increases with the size of the physical message. Communication latency varies with the physical distance between the nodes, but the difference is usually negligible compared to the communication overhead. work.
Reference: [CM81] <author> K. M. Chandy and J. Misra. </author> <title> Asynchronous distributed simulation via a sequence of parallel computations. </title> <journal> Communications of the ACM, </journal> <volume> 24(11), </volume> <month> April </month> <year> 1981. </year>
Reference-contexts: The simulation algorithm has been shown to be both accurate and efficient in the work by Lin et al. [LMSK91]. The parallel implementation of the CSWEC program is a classical example of parallel discrete event simulation. We adopt the conservative approach to parallelizing asynchronous simulation <ref> [CM81] </ref>. A graph node is allocated for each input signal port and each voltage point of a subcircuit. A thread is created for each subcircuit to simulate its time-varying state. The threads communicate and synchronize via the event graph data structure, which encapsulates the connectivity structure of the circuit.
Reference: [CMF92] <institution> Thinking Machines Corporation. </institution> <note> CM Fortran Reference Manual, </note> <month> December </month> <year> 1992. </year>
Reference-contexts: One approach to parallel programming that addresses the portability problem is to use so-called "heroic" compilers such as parallelizing Fortran compilers [HKT91, ABC + 88, PGH + 90] or optimizing compilers for data parallel languages <ref> [CMF92, Hig92] </ref>. The programmer relies on the compiler to discover parallelism and to schedule communication and computation.
Reference: [CR95] <author> Martin C. Carlisle and Anne Rogers. </author> <title> Software caching and computation migration in Olden. </title> <booktitle> In Proceedings of the 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Threads and fibers are used only for latency hiding. They are not meant to be used as units of true parallelism or load balance and they do not migrate across processors. Unlike other compilers or runtime systems with built-in load balancers <ref> [SK91, BJK + 95, GSC95, MKH91, CR95] </ref>, load balancing mechanisms are left out of the Multipol runtime layer so that the programmer can have more control over locality and scheduling. Load balancing can be performed by the task stealer data structure or other data structures implemented by the user.
Reference: [Cro94] <author> M. Crovella. </author> <title> Performance Prediction and Tuning of Parallel Programs. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: The statistical approach to modeling the performance of parallel programs has been taken by other researchers such as Brewer [Bre94] and Crovella <ref> [Cro94] </ref>. However, their work targeted standalone procedures and a fixed set of overhead categories, and they did not address the composition of concurrent data structures. Also, their applications are regular, bulk-synchronous programs whose performance can be predicted prior to the computation. <p> Their work was restricted to shared memory platforms where global variables can be updated and probed with low overhead, and they did not address the interaction of data structures. The statistical approach to performance modeling has been taken by Brewer [Bre94] and Crovella <ref> [Cro94] </ref> in predicting the performance of bulk-synchronous programs. As in Mprof, they assume the cost models are linear combinations of terms. Brewer used statistical models to predict the performance of different implementations of the same procedure. The prediction is used to select the best implementation for a given input.
Reference: [CRY94] <author> Soumen Chakrabarti, Abhiram Ranade, and Katherine Yelick. </author> <title> Randomized load balancing for tree-structured computation. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Task pushing has been found to achieve better load balance for certain applications we encountered (such as the Eigenvalue program described below). However, the theoretical optimality of both types of load balancing protocols have been reported <ref> [BL94, CRY94] </ref>. Task migration creates problems for termination detection, because some migrated tasks may be in transit in the network layer when all local task pools are empty.
Reference: [CSS + 91] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proc. of 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa-Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The port can be used for almost any UNIX workstation. However, sockets are rather heavy-weight, and sending a message incurs at least 800 microseconds of overhead on the Sparc cluster. 3.6 Related Work The fiber abstraction in the Multipol runtime layer is similar to the thread abstraction in TAM <ref> [CSS + 91] </ref>, Cilk [BJK + 95], and Nexus [FKOT91], and the chare abstraction in Charm++ [SK91, KK93]. TAM is a threaded abstraction machine for data-flow computation.
Reference: [CY93] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> Implementing an irregular application on a distributed memory multiprocessor. </title> <booktitle> In ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The program uses a kind of speculative parallelism that is similar to a search problem, but with more subtle forms of pruning <ref> [CY93] </ref>. 2.4 Related Work Fox [Fox92] classified parallel applications based on their temporal structures. He put all applications in three categories: synchronous, loosely synchronous, and asynchronous. He described example applications in each category and discussed their degree of irregularity.
Reference: [DDR94] <author> J. Demmel, I. Dhillon, and H. Ren. </author> <title> On the correctness of parallel bisection in floating point. </title> <type> Technical Report UCB//CSD-94-805, </type> <institution> UC Berkeley Computer Science Division, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: For each data structure, we describe the programming interface, the implementation techniques, and the driving applications. For each example application, we describe its irregularities and execution behaviors using measurements from parallel executions. Several of the applications are described more completely elsewhere <ref> [CDG + 93, DDR94, JY95, WY95] </ref>. The data structures and applications are then summarized and compared in Section 2.3. <p> The snapshot mechanism used for suspending task migration is described in Section 3.4.2. 2.2.3.3 Example Application Eigenvalue The Eigenvalue program computes the eigenvalues of an N by N symmetric tridi-agonal matrix, which is known to have N real eigenvalues. The program uses the bisection algorithm <ref> [DDR94] </ref> to approximate the eigenvalues to an arbitrary precision. Given an input matrix, it first computes an initial interval of real numbers that contains all possible eigenvalues for the matrix. <p> The input matrix is replicated on all processors, so the bisection computation is local once the interval is obtained. The computation kernel of the Eigenvalue program was written by Soumen Chakrabarti [YCD + 95]. Inderjit Dhillion implemented a CM5 version of the bisection algorithm prior to the Multipol implementation <ref> [DDR94] </ref>. is used to set up the initial interval and perform post-processing after the computation. A computation thread is created on each processor to process the intervals. Either task pushing or task stealing can be used to perform dynamic load balancing.
Reference: [Dem89] <author> James Demmel. </author> <title> LAPACK: A portable linear algebra library for supercomputers. </title> <booktitle> In Proceedings of the 1989 IEEE Control Systems Society Workshop on Computer-Aided Control System Design, </booktitle> <address> Tampa, FL, </address> <month> Dec </month> <year> 1989. </year>
Reference-contexts: The library is also extensible and can provide domain-specific abstractions. This approach has been used successfully in several areas including dense matrix algorithms <ref> [Dem89, ABB + 92, CDPW92] </ref>, unstructured mesh computation [DHU + 93, DUSH94, MSH + 95], and adaptive mesh computation [KB95]. While their applications include some degree of irregularity, these libraries do not handle task parallelism or asynchronous communication. <p> Our approach provides more insight into the techniques for optimizing irregular applications. There has been a great deal of work on specialized libraries for scientific applications. Examples include LAPACK <ref> [Dem89, ABB + 92] </ref> and ScaLAPACK [CDPW92], which are libraries of SPMD algorithms for solving linear algebra problems such as matrix multiplication, matrix factorization, and eigenvalue computations. Unlike the Multipol library, they contain standalone procedures rather than data structures and they solve mostly regular problems.
Reference: [DHU + 93] <author> R. Das, Y. Hwang, M. Uysal, J. Saltz, and A. Sussman. </author> <title> Applying the CHAOS/PARTI library to irregular problems in computational chemistry and 119 computational aerodynamics. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <address> Starkville, MS, </address> <year> 1993. </year>
Reference-contexts: The library is also extensible and can provide domain-specific abstractions. This approach has been used successfully in several areas including dense matrix algorithms [Dem89, ABB + 92, CDPW92], unstructured mesh computation <ref> [DHU + 93, DUSH94, MSH + 95] </ref>, and adaptive mesh computation [KB95]. While their applications include some degree of irregularity, these libraries do not handle task parallelism or asynchronous communication. <p> The use of a library allows for portability, because the high-level programming interface provided by the library shields the application programmers from the implementation details. The library approach has been adopted by PARTI [BSS91], CHAOS <ref> [DHU + 93, DUSH94, MSH + 95] </ref> and LPARX [KB95], which are specialized libraries for unstructured and adaptive mesh applications. They handle certain irregular data structures, but are limited to bulk synchronous computation patterns. <p> Unlike the Multipol library, they contain standalone procedures rather than data structures and they solve mostly regular problems. PARTI [BSS91] and CHAOS <ref> [DHU + 93, DUSH94, MSH + 95] </ref> (an enhanced version of PARTI) are runtime layers designed to support simulations on spatially irregular data structures. They generate optimized communication schedules to reduce the overhead of accessing such data structures. LPARX [KB95] is a library for adaptive mesh computation.
Reference: [DUSH94] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> September </month> <year> 1994. </year>
Reference-contexts: The library is also extensible and can provide domain-specific abstractions. This approach has been used successfully in several areas including dense matrix algorithms [Dem89, ABB + 92, CDPW92], unstructured mesh computation <ref> [DHU + 93, DUSH94, MSH + 95] </ref>, and adaptive mesh computation [KB95]. While their applications include some degree of irregularity, these libraries do not handle task parallelism or asynchronous communication. <p> The use of a library allows for portability, because the high-level programming interface provided by the library shields the application programmers from the implementation details. The library approach has been adopted by PARTI [BSS91], CHAOS <ref> [DHU + 93, DUSH94, MSH + 95] </ref> and LPARX [KB95], which are specialized libraries for unstructured and adaptive mesh applications. They handle certain irregular data structures, but are limited to bulk synchronous computation patterns. <p> Unlike the Multipol library, they contain standalone procedures rather than data structures and they solve mostly regular problems. PARTI [BSS91] and CHAOS <ref> [DHU + 93, DUSH94, MSH + 95] </ref> (an enhanced version of PARTI) are runtime layers designed to support simulations on spatially irregular data structures. They generate optimized communication schedules to reduce the overhead of accessing such data structures. LPARX [KB95] is a library for adaptive mesh computation.
Reference: [FKOT91] <author> Ian Foster, Carl Kesselman, Robert Olson, and Steve Tuccke. </author> <title> Nexus: An interoperability toolkit for parallel and distributed computer systems. </title> <type> Technical Report ANL/MCS-TM-189, </type> <institution> Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: However, sockets are rather heavy-weight, and sending a message incurs at least 800 microseconds of overhead on the Sparc cluster. 3.6 Related Work The fiber abstraction in the Multipol runtime layer is similar to the thread abstraction in TAM [CSS + 91], Cilk [BJK + 95], and Nexus <ref> [FKOT91] </ref>, and the chare abstraction in Charm++ [SK91, KK93]. TAM is a threaded abstraction machine for data-flow computation. TAM threads are nonblocking units of computation for latency hiding which are generated by a compiler (each thread corresponds to a basic block in the data-flow graph of a function).
Reference: [For94] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard. </title> <institution> Technical Report Computer Science Department Technical Report CS-94-230, University of Tennessee, Knoxville, TN, </institution> <month> May 5 </month> <year> 1994. </year> <note> Also appeared in the International Journal of Supercomputing Applications, Volume 8, Number 3/4, </note> <year> 1994. </year>
Reference-contexts: Examples of these irregular applications include unstructured mesh computations, divide and conquer algorithms, and discrete event simulation. Another approach to parallel programming involves explicit message passing with message passing layers such as PVM [BDG + 91] and MPI <ref> [For94] </ref>. Such message passing layers are often used when performance is the most important concern, because they allow the programmer to have complete control over parallelism and communication. However, the programming is fairly low-level, and the primitives are designed for cooperative message passing which makes irregular communication patterns awkward. <p> Portability is also a concern in developing irregular applications. Irregular parallel programs are mostly written with explicit communication primitives from the vendor's communication library, which varies with the machine. The differences in programming environments create problems for code sharing across machines. MPI <ref> [For94] </ref> is a message passing standard which provides a uniform communication interface on distributed memory platforms. But the MPI primitives are designed for cooperative message passing and therefore do not support irregular communication patterns well. <p> Like Cilk threads, Charm chares are also units of load balance and they may migrate across processors. The Charm system provides little support for using customized scheduling policies for the entry function invocations. There exist many portable communication layers such as MPI (Message Passing Interface <ref> [For94] </ref> and active messages [vECGS92]. MPI provides cooperative message passing primitives for point-to-point and group communication. However, it does not provide irregular communication primitives such as remote fiber invocation. Active messages are similar to remote fiber invocation, but impose more restrictions (see section 3.3.1 for detailed comparison).
Reference: [Fox92] <author> G. C. Fox. </author> <title> Hardware and software architectures for irregular problems. Unstructured Scientific Computation on Scalable Multiprocessors, </title> <year> 1992. </year>
Reference-contexts: The program uses a kind of speculative parallelism that is similar to a search problem, but with more subtle forms of pruning [CY93]. 2.4 Related Work Fox <ref> [Fox92] </ref> classified parallel applications based on their temporal structures. He put all applications in three categories: synchronous, loosely synchronous, and asynchronous. He described example applications in each category and discussed their degree of irregularity.
Reference: [GHPW90] <author> G. A. Geist, M. T. Heath, B. W. Peyton, and P. H. Worley. </author> <title> A user's guide to PICL: A portable instrumented communication library. </title> <type> Technical Report ORNL/TM-11616, </type> <institution> Oak Ridge National Laboratory, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: They include PICL/ParaGraph <ref> [GHPW90] </ref>, Pablo [RAN + 93], and Vista [Hal95]. 95 These tools collect a trace of communication activities and other user-defined events from the execution and use the trace to generate multiple views of the program's activities over time. Such tools feature large quantities of low-level profiling information.
Reference: [GSC95] <author> Seth C. Goldstein, Klaus E. Schauser, and David E. Culler. </author> <title> Enabling primitives for compiling parallel languages. </title> <booktitle> In Third Workshop on Languages, Compilers, and Run-Time Systems fo r Scalable Computers, </booktitle> <institution> Rensselaer Polytechnic Institute, </institution> <address> NY, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Threads and fibers are used only for latency hiding. They are not meant to be used as units of true parallelism or load balance and they do not migrate across processors. Unlike other compilers or runtime systems with built-in load balancers <ref> [SK91, BJK + 95, GSC95, MKH91, CR95] </ref>, load balancing mechanisms are left out of the Multipol runtime layer so that the programmer can have more control over locality and scheduling. Load balancing can be performed by the task stealer data structure or other data structures implemented by the user. <p> The Cilk runtime layer uses a fixed load balancing and scheduling policy based on task stealing, which is similar to the works on lazy task creation [MKH91] and stacklet <ref> [GSC95] </ref>. In contrast, Multipol fibers are used for latency hiding and do not migrate across processors. Furthermore, the scheduling policies for Multipol fibers can be customized for a particular application.
Reference: [Hal95] <author> Robert H. Halstead Jr. </author> <title> Understanding the performance of parallel symbolic programs. </title> <booktitle> In Proceedings of the Parallel Symbolic Languages and Systems Workshop, </booktitle> <address> Beaune France, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: They include PICL/ParaGraph [GHPW90], Pablo [RAN + 93], and Vista <ref> [Hal95] </ref>. 95 These tools collect a trace of communication activities and other user-defined events from the execution and use the trace to generate multiple views of the program's activities over time. Such tools feature large quantities of low-level profiling information.
Reference: [Hig92] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <note> Version 0.4, 1992. 120 </note>
Reference-contexts: One approach to parallel programming that addresses the portability problem is to use so-called "heroic" compilers such as parallelizing Fortran compilers [HKT91, ABC + 88, PGH + 90] or optimizing compilers for data parallel languages <ref> [CMF92, Hig92] </ref>. The programmer relies on the compiler to discover parallelism and to schedule communication and computation.
Reference: [HKT91] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machine. </title> <booktitle> Supercomputing, </booktitle> <pages> pages 86-100, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: One approach to parallel programming that addresses the portability problem is to use so-called "heroic" compilers such as parallelizing Fortran compilers <ref> [HKT91, ABC + 88, PGH + 90] </ref> or optimizing compilers for data parallel languages [CMF92, Hig92]. The programmer relies on the compiler to discover parallelism and to schedule communication and computation. <p> Unpredictable communication schedules are expensive to implement because they require hand-shaking between the sender and the receiver for each communication event. Furthermore, on distributed memory architectures with large per-message overheads, an irregular communication schedule makes it impossible to apply compile-time loop transformations such as message vectorization <ref> [HKT91] </ref> or runtime preprocessing techniques such as the inspector primitive to reduce communication overhead. Dynamic computation granularities arise when the control structure of the computation is highly dependent on the data.
Reference: [HM92] <author> Jeffrey K. Hollingsworth and Barton P. Miller. </author> <title> Parallel program performance metrics: A comparison and validation. </title> <booktitle> In Supercomputing '92, </booktitle> <address> Minneapolis, </address> <year> 1992. </year>
Reference-contexts: On the other hand, their toolkits provide performance prediction, while Mprof only provides post-mortem performance profiles. To make performance prediction possible, the statistics in their cost models cannot be computed values. Hollingsworth <ref> [HM92] </ref> compared several performance metrics for detecting problematic program components which include the critical path metric [YM88] and the NPT metric [AL90]. The critical path metric measures the amount of time spent by each procedure in the longest running path of the execution.
Reference: [Ho94] <author> Kinson Ho. </author> <title> High-level Abstractions for Symbolic Parallel Programming (Parallel Lisp Hacking Made Easy). </title> <type> PhD thesis, </type> <institution> Computer Science Division, University of California at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: The interface is different from the sequential interface proposed by Ho for task parallelism <ref> [Ho94] </ref>. An application can add and remove tasks in the task pools. Along with the content of the task, the user may also provide hints for scheduling and load balancing. <p> In contrast, Multipol is designed for a more general class of irregular problems which includes asynchronous algorithms. Out of the five applications we examined, only the EM3D application can be optimized using libraries such as CHAOS or LPARX. Ho <ref> [Ho94] </ref> developed a toolbox for parallelizing symbolic applications in Lisp. His toolbox provides two types of abstractions: parallelism and data sharing abstractions. The parallelism abstraction is similar to our task stealer data structure. The data sharing abstractions include automatically locked objects and speculative read-modify-write operations.
Reference: [Jai91] <author> Raj Jain. </author> <title> The Art of Computer Systems Performance Analysis. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1991. </year>
Reference-contexts: 90% confidence interval for the fitted parameter c with mean u c and variance v c (estimated from the samples) is u c + v c fi t [0:95; n m], where t [0:95; n m] is the 95% quantile of the Student's T-distribution with n m degree of freedom <ref> [Jai91] </ref>. 5 If the confidence interval contains 0, the parameter is not significantly different from zero, and the corresponding term should be omitted from the model. The statistical approach to modeling the performance of parallel programs has been taken by other researchers such as Brewer [Bre94] and Crovella [Cro94].
Reference: [Jef85] <author> D. R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3), </volume> <month> July </month> <year> 1985. </year>
Reference-contexts: Programming and optimizing such applications is also considerably more difficult. Finally, applications that exploit speculative parallelism may perform redundant computation. For example, a search application using imperfect heuristics may explore parts of the search space that do not contribute to the solution [JY95], and an optimistic discrete event simulator <ref> [Jef85, WY93] </ref> may waste time on computations that use stale data. Redundant computation presents a tradeoff between parallelism and overhead, because an optimization that increases parallelism may not necessarily improve performance due to 8 a proportional increase in redundant work.
Reference: [Joh91] <author> M. Johnson. </author> <title> Superscalar Microprocessor Design. </title> <publisher> Prentice-Hall, </publisher> <year> 1991. </year>
Reference-contexts: Therefore, instead of eliminating the flow control dependencies, we try to hide their latency using a technique called software pipelining, which is also used in advanced compilers for exploiting instruction-level parallelism <ref> [Lam87, Joh91] </ref>. 92 % pipe-cm5 32 10000 10 100 1000 100 1000 *** Cost Profile *** Id 0: THREAD= 38.905 IDLE= 51.779 COMM_ALPHA= 6.753 COMM_BETA= 52.388 Id 3: COMP=149.078 Id 1: ACCESS= 15.518 DATA= 4.713 Time: Model= 319.134, Total= 307.305 *** Observed Latency **** ENQ1 = 1.723 -- DEQ1: 1.723 produce3
Reference: [Joh93] <author> K. L. Johnson. </author> <title> Private communication. </title> <note> Available via anonymous ftp from cag.lcs.mit.edu as /pub/tuna/tripuz-entry, </note> <month> November </month> <year> 1993. </year>
Reference-contexts: A solution is a sequence of legal moves that leads to a board configuration with only one peg. The Multipol implementation of the Tripuzzle program was written by Etienne Deprit [YCD + 95] based on a CM-5 implementation by Kirk Johnson <ref> [Joh93] </ref>. The program performs an exhaustive, breadth-first search to count all possible solutions. The search space is a graph of board configurations where each directed edge represents a legal move. The graph is acyclic because all moves are irreversible (pegs taken out of the board are never put back).
Reference: [JP95] <author> J. Jones and V. Papavassiliou. </author> <title> A compiler for Multipol. Project report for CS264: </title> <booktitle> Implementation of Programming Languages, </booktitle> <year> 1995. </year>
Reference-contexts: The non-blocking semantics of fibers also facilitate scheduling and performance profiling. Although the runtime layer does not provide internal support for thread context management, we provide a set of macros to help the construction of threads from the fibers. A prototype compiler was developed by Jones and Papavassil-iou <ref> [JP95] </ref> to automatically converts programs with blocking accesses into programs with only split-phase accesses and fibers, but in this thesis, all of our programs use hand-coded fibers or fibers generated by the macros. To create a fiber, the programmer specifies a function and its actual arguments. <p> Therefore, the programmer is burdened with task of decomposing long-running threads into fibers with reasonable granularities. Better linguistic support and compilation tools would ease the programming task. The prototype compiler developed by Jones and Papavassiliou <ref> [JP95] </ref> is the first step towards such a goal. The reusability of the cost models depends on the availability of a non-blocking communication interface on the machine so that the overhead of sending a message of a given size can be approximated by a fixed value.
Reference: [JY95] <author> J. Jones and K. Yelick. </author> <title> Parallelizing the phylogeny problem. </title> <booktitle> In Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Programming and optimizing such applications is also considerably more difficult. Finally, applications that exploit speculative parallelism may perform redundant computation. For example, a search application using imperfect heuristics may explore parts of the search space that do not contribute to the solution <ref> [JY95] </ref>, and an optimistic discrete event simulator [Jef85, WY93] may waste time on computations that use stale data. Redundant computation presents a tradeoff between parallelism and overhead, because an optimization that increases parallelism may not necessarily improve performance due to 8 a proportional increase in redundant work. <p> For each data structure, we describe the programming interface, the implementation techniques, and the driving applications. For each example application, we describe its irregularities and execution behaviors using measurements from parallel executions. Several of the applications are described more completely elsewhere <ref> [CDG + 93, DDR94, JY95, WY95] </ref>. The data structures and applications are then summarized and compared in Section 2.3. <p> That is, once a species loses a certain trait, its descendent species can never regain it. The Phylogeny program finds the maximal character subsets that have a perfect phylogeny tree for a given set of species. The problem is important to a branch of biology known as systematics <ref> [JY95] </ref>. The Multipol implementation of the Phylogeny program is based on the CM-5 implementation written by Jeff Jones [JY95]. The program performs a search on the space of character subsets to find the subsets that have a perfect phylogeny tree. <p> The problem is important to a branch of biology known as systematics <ref> [JY95] </ref>. The Multipol implementation of the Phylogeny program is based on the CM-5 implementation written by Jeff Jones [JY95]. The program performs a search on the space of character subsets to find the subsets that have a perfect phylogeny tree. The program takes advantage of two properties of perfect phylogeny trees in pruning the search space. <p> We did not perform the optimizations on the task pushing version, because task stealing is clearly the better load balancing strategy to use. The Split-C implementation of the Phylogeny program <ref> [JY95] </ref> showed better speedup than the Multipol implementation on the CM5. We noticed that the performance of the Multipol implementation is very sensitive to the load balancing and scheduling parameters.
Reference: [KAP95] <author> K. Keeton, T. Anderson, and D. Patterson. </author> <title> LogP quantified: The case for low-overhead local area networks. </title> <booktitle> In Proceedings of Hot Interconnects III: A Symposium on High Performance Interconnects, </booktitle> <address> Stanford, CA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: The measurements for the CM5, the Paragon, and the SP1 are from Luna [Lun94], and the measurements for the Sparc cluster are from Keeton et al <ref> [KAP95] </ref>. On every machine, the communication time for small messages is entirely dominated by the send and receive overheads on each end. 3.1.2 Our approach Although distributed memory architectures provide high performance and scalability, they do not provide an adequate programming model for irregular applications.
Reference: [KB95] <author> Scott R. Kohn and Scott B. Baden. </author> <title> A parallel software infrastructure for structured adaptive mesh methods. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year> <month> 121 </month>
Reference-contexts: The library is also extensible and can provide domain-specific abstractions. This approach has been used successfully in several areas including dense matrix algorithms [Dem89, ABB + 92, CDPW92], unstructured mesh computation [DHU + 93, DUSH94, MSH + 95], and adaptive mesh computation <ref> [KB95] </ref>. While their applications include some degree of irregularity, these libraries do not handle task parallelism or asynchronous communication. Multipol [YCD + 95] is a parallel data structure library designed for irregular applications with pointer-based data structures, asynchronous communication, and unpredictable task dependences or computation times. <p> The use of a library allows for portability, because the high-level programming interface provided by the library shields the application programmers from the implementation details. The library approach has been adopted by PARTI [BSS91], CHAOS [DHU + 93, DUSH94, MSH + 95] and LPARX <ref> [KB95] </ref>, which are specialized libraries for unstructured and adaptive mesh applications. They handle certain irregular data structures, but are limited to bulk synchronous computation patterns. In this chapter, we present an overview of the Multipol data structure library and some of the applications that use the library. <p> PARTI [BSS91] and CHAOS [DHU + 93, DUSH94, MSH + 95] (an enhanced version of PARTI) are runtime layers designed to support simulations on spatially irregular data structures. They generate optimized communication schedules to reduce the overhead of accessing such data structures. LPARX <ref> [KB95] </ref> is a library for adaptive mesh computation. It provides primitives for building adaptive meshes, partitioning the computation, and accessing the mesh state.
Reference: [KK93] <author> L. V. Kale and Sanjeev Krishnan. </author> <title> Charm++ : A portable concurrent object oriented system based on C++. </title> <booktitle> In Proceedings of the Conference on Object Oriented Programming Systems, Languages and Applications, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: Many load balancing protocols for distributed memory platforms have been proposed. Their major differences are not in the algorithms, but in the surrounding software support. Languages and runtime layers such as Charm <ref> [SK91, KK93] </ref> and Cilk [BJK + 95] 40 have built-in language mechanisms to allocate parallel tasks that are automatically migrated by a embedded load balancer. These systems have a fixed set of load balancing and scheduling policies that cannot be easily customized for a particular application. <p> and sending a message incurs at least 800 microseconds of overhead on the Sparc cluster. 3.6 Related Work The fiber abstraction in the Multipol runtime layer is similar to the thread abstraction in TAM [CSS + 91], Cilk [BJK + 95], and Nexus [FKOT91], and the chare abstraction in Charm++ <ref> [SK91, KK93] </ref>. TAM is a threaded abstraction machine for data-flow computation. TAM threads are nonblocking units of computation for latency hiding which are generated by a compiler (each thread corresponds to a basic block in the data-flow graph of a function).
Reference: [LAD + 92] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: The bandwidth is the peak point-to-point bandwidth attainable by the communication library. The processors can be connected by a variety of networks such as the fat-tree interconnect of the CM5 <ref> [LAD + 92] </ref>, the mesh interconnect of the Paragon [LC95], and the switched LAN (e.g., Myrinet [BCF + 95]) of the Sparc cluster built by the NOW [CLMY96] group at Berkeley.
Reference: [Lam87] <author> Monica S. Lam. </author> <title> A Systolic Array Optimizing Compiler. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: Therefore, instead of eliminating the flow control dependencies, we try to hide their latency using a technique called software pipelining, which is also used in advanced compilers for exploiting instruction-level parallelism <ref> [Lam87, Joh91] </ref>. 92 % pipe-cm5 32 10000 10 100 1000 100 1000 *** Cost Profile *** Id 0: THREAD= 38.905 IDLE= 51.779 COMM_ALPHA= 6.753 COMM_BETA= 52.388 Id 3: COMP=149.078 Id 1: ACCESS= 15.518 DATA= 4.713 Time: Model= 319.134, Total= 307.305 *** Observed Latency **** ENQ1 = 1.723 -- DEQ1: 1.723 produce3
Reference: [LC95] <author> Lok T. Liu and David E. Culler. </author> <title> Evaluation of the Intel Paragon on active message communication. </title> <booktitle> In Proceedings of Intel Supercomputer Users Group Conference, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The bandwidth is the peak point-to-point bandwidth attainable by the communication library. The processors can be connected by a variety of networks such as the fat-tree interconnect of the CM5 [LAD + 92], the mesh interconnect of the Paragon <ref> [LC95] </ref>, and the switched LAN (e.g., Myrinet [BCF + 95]) of the Sparc cluster built by the NOW [CLMY96] group at Berkeley. The native communication libraries on these machines typically have high start-up overhead, and therefore bandwidth increases with the size of the physical message. <p> The runtime layer has been ported on three types of communication libraries: active messages (CMAML), cooperative message passing (MPLp, NX), and BSD sockets using the TCP/IP protocol stack. We reuse code from GAM <ref> [LC95, CKK + 94] </ref> and libsplit-c [Lun94], the communication library for the Split-C language [CDG + 93]. Most modifications to their code are for integrating the the communication primitives with the Multipol thread layer.
Reference: [LMSK91] <author> Shen Lin, M. Marek-Sadowska, and E.S. Kuh. SWEC: </author> <title> A stepwise equivalent conductance timing simulator for CMOS VLSI circuits. </title> <booktitle> In Proceedings of the European Conference on Design Automation, </booktitle> <address> Amsterdam, Netherlands, </address> <month> Febru-ary </month> <year> 1991. </year>
Reference-contexts: The collapsing of messages significantly reduces the amount of communication if the nodes 31 have a large number of fanouts, such as in the CSWEC program described below. 2.2.4.3 Example Application - CSWEC The CSWEC program was implemented by the author, based on the sequential SWEC program written by Lin <ref> [LMSK91] </ref>. The program simulates the voltage output of combinational digital circuits, that is, circuits without feedback signal paths. The program partitions the circuit into loosely coupled subcircuits that can be simulated independently within a time step. <p> The propagation of subcircuit state is called an event. The event-driven approach significantly reduces computation because digital signals change infrequently. The simulation algorithm has been shown to be both accurate and efficient in the work by Lin et al. <ref> [LMSK91] </ref>. The parallel implementation of the CSWEC program is a classical example of parallel discrete event simulation. We adopt the conservative approach to parallelizing asynchronous simulation [CM81]. A graph node is allocated for each input signal port and each voltage point of a subcircuit.
Reference: [Lun94] <author> Steve Luna. </author> <title> Implementing an efficient portable global memory layer on distributed memory multiprocessors. </title> <type> Master's thesis, </type> <institution> Computer Science Division, University of California at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: For portability, we use the communication libraries provided by the vendors instead of the research prototypes such as the generic active messages (GAM) [CKK + 94] developed by the NOW group. The measurements for the CM5, the Paragon, and the SP1 are from Luna <ref> [Lun94] </ref>, and the measurements for the Sparc cluster are from Keeton et al [KAP95]. <p> The runtime layer has been ported on three types of communication libraries: active messages (CMAML), cooperative message passing (MPLp, NX), and BSD sockets using the TCP/IP protocol stack. We reuse code from GAM [LC95, CKK + 94] and libsplit-c <ref> [Lun94] </ref>, the communication library for the Split-C language [CDG + 93]. Most modifications to their code are for integrating the the communication primitives with the Multipol thread layer. In the paragraphs that follow, we sketch the implementation of these ports and describe the source of their overheads.
Reference: [MCC + 95] <author> Barton P. Miller, Mark D. Callaghan, Jonathan M. Cargille, Jeffrey K. Hollingsworth, R. Bruce Irvin, Karen L. Karavanic, Krishna Kunchithapadam, and Tia Newhall. </author> <title> The Paradyn parallel performance measurement tools. </title> <booktitle> IEEE Computer, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: Furthermore, Mprof provides high-level profiling information instead of raw event traces. However, Mprof provides summary information about the entire execution, and it cannot provide information about a particular time period in the execution. Instrumentation and sampling methods are popular alternatives to event tracing. Research in this area includes Paradyne <ref> [MCC + 95] </ref> and the work by Crovella and LeBlanc [CL93]. The Paradyne toolkit instruments the program binaries to collect performance statistics. It allows the programmer to specify conditions to dynamically control the degree of instrumentation.
Reference: [MKH91] <author> E. Mohr, D. A. Kranz, and R. H. Halstead Jr. </author> <title> Lazy task creation: a technique for increasing the granularity of parallel programs. </title> <booktitle> IEEE Transaction on Parallel and Distributed Systems, </booktitle> <year> 1991. </year>
Reference-contexts: Threads and fibers are used only for latency hiding. They are not meant to be used as units of true parallelism or load balance and they do not migrate across processors. Unlike other compilers or runtime systems with built-in load balancers <ref> [SK91, BJK + 95, GSC95, MKH91, CR95] </ref>, load balancing mechanisms are left out of the Multipol runtime layer so that the programmer can have more control over locality and scheduling. Load balancing can be performed by the task stealer data structure or other data structures implemented by the user. <p> The Cilk runtime layer uses a fixed load balancing and scheduling policy based on task stealing, which is similar to the works on lazy task creation <ref> [MKH91] </ref> and stacklet [GSC95]. In contrast, Multipol fibers are used for latency hiding and do not migrate across processors. Furthermore, the scheduling policies for Multipol fibers can be customized for a particular application.
Reference: [MSH + 95] <author> S. Mukherjee, S. Sharma, M. Hill, J. Larus, A. Rogers, and J. Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In Pro 122 ceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: The library is also extensible and can provide domain-specific abstractions. This approach has been used successfully in several areas including dense matrix algorithms [Dem89, ABB + 92, CDPW92], unstructured mesh computation <ref> [DHU + 93, DUSH94, MSH + 95] </ref>, and adaptive mesh computation [KB95]. While their applications include some degree of irregularity, these libraries do not handle task parallelism or asynchronous communication. <p> The use of a library allows for portability, because the high-level programming interface provided by the library shields the application programmers from the implementation details. The library approach has been adopted by PARTI [BSS91], CHAOS <ref> [DHU + 93, DUSH94, MSH + 95] </ref> and LPARX [KB95], which are specialized libraries for unstructured and adaptive mesh applications. They handle certain irregular data structures, but are limited to bulk synchronous computation patterns. <p> Unlike the Multipol library, they contain standalone procedures rather than data structures and they solve mostly regular problems. PARTI [BSS91] and CHAOS <ref> [DHU + 93, DUSH94, MSH + 95] </ref> (an enhanced version of PARTI) are runtime layers designed to support simulations on spatially irregular data structures. They generate optimized communication schedules to reduce the overhead of accessing such data structures. LPARX [KB95] is a library for adaptive mesh computation.
Reference: [Ous94] <author> John K. Ousterhout. </author> <title> Tcl and the Tk Toolkit. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1994. </year>
Reference-contexts: The high-level statistics are not detailed enough for making performance tradeoffs. 1 In reality, the performance interface for each data structure is stored in a different file. 2 The transformation is done with Tcl scripts <ref> [Ous94] </ref>. 75 # Declare statistics for "pipe" P -- Partitions per processor (= 1) T -- Computation N -- Size of a enqueue or dequeue batch M -- Total number of items produced # Declare statistics for "rts" P -- Partitions per processor (= 1) Sched -- Number of customized schedulers
Reference: [PGH + 90] <author> Constantine Polychronopoulos, Milind B. Girkar, Mohammad R. Haghighat, Chia L. Lee, Bruce P. Leung, and Dale A. Schouten. </author> <title> The structure of Parafrase-2: An advanced parallelizing compiler for C and Fortran. </title> <booktitle> In Languages and Compilers for Parallel Computing. </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: One approach to parallel programming that addresses the portability problem is to use so-called "heroic" compilers such as parallelizing Fortran compilers <ref> [HKT91, ABC + 88, PGH + 90] </ref> or optimizing compilers for data parallel languages [CMF92, Hig92]. The programmer relies on the compiler to discover parallelism and to schedule communication and computation.
Reference: [PTVF92] <author> W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: section, we describe our choice of algorithm for solving the system of equations, and how feedback from the solutions can be used to improve the quality of the parameterized cost models. 4.4.3 Instantiating the Cost Models We use the Singular Value Decomposition method (SVD) to find the best c r <ref> [PTVF92] </ref>. The method is robust in the presence of linearly dependent terms and nearly identical samples. Linearly dependent terms do not contribute to the expressiveness of the model, and should be merged into fewer terms.
Reference: [RAN + 93] <author> Daniel A. Reed, Ruth A. Aydt, Roger J. Noe, Phillip C. Roth, Keith A. Shields, Bradley Schwartz, and Luis F. Tavera. </author> <title> Scalable performance analysis: The Pablo performance analysis environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <year> 1993. </year>
Reference-contexts: They include PICL/ParaGraph [GHPW90], Pablo <ref> [RAN + 93] </ref>, and Vista [Hal95]. 95 These tools collect a trace of communication activities and other user-defined events from the execution and use the trace to generate multiple views of the program's activities over time. Such tools feature large quantities of low-level profiling information.
Reference: [SK91] <author> Wei Shu and L. V. Kale. </author> <title> Chare kernel a runtime support system for parallel computations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> March </month> <year> 1991. </year>
Reference-contexts: Many load balancing protocols for distributed memory platforms have been proposed. Their major differences are not in the algorithms, but in the surrounding software support. Languages and runtime layers such as Charm <ref> [SK91, KK93] </ref> and Cilk [BJK + 95] 40 have built-in language mechanisms to allocate parallel tasks that are automatically migrated by a embedded load balancer. These systems have a fixed set of load balancing and scheduling policies that cannot be easily customized for a particular application. <p> Threads and fibers are used only for latency hiding. They are not meant to be used as units of true parallelism or load balance and they do not migrate across processors. Unlike other compilers or runtime systems with built-in load balancers <ref> [SK91, BJK + 95, GSC95, MKH91, CR95] </ref>, load balancing mechanisms are left out of the Multipol runtime layer so that the programmer can have more control over locality and scheduling. Load balancing can be performed by the task stealer data structure or other data structures implemented by the user. <p> and sending a message incurs at least 800 microseconds of overhead on the Sparc cluster. 3.6 Related Work The fiber abstraction in the Multipol runtime layer is similar to the thread abstraction in TAM [CSS + 91], Cilk [BJK + 95], and Nexus [FKOT91], and the chare abstraction in Charm++ <ref> [SK91, KK93] </ref>. TAM is a threaded abstraction machine for data-flow computation. TAM threads are nonblocking units of computation for latency hiding which are generated by a compiler (each thread corresponds to a basic block in the data-flow graph of a function).
Reference: [vECGS92] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: A distributed memory machine consists of a collection of processors, each of which has its own memory module. Without a shared address space, accessing remote memory requires executing code on the processor owning the memory. The accesses can be performed by cooperative message passing or active messages <ref> [vECGS92] </ref>. To support messages of an arbitrary size, the buffer space used for each message must be explicitly allocated before the communication takes place. 43 Machine Network Nodes Comm. <p> Other than that basic similarity, the two mechanisms are quite different. Active messages are designed as an efficient network layer, not as a computational abstraction <ref> [vECGS92] </ref>, and are insufficient for implementing remote operations in the follow respects. First, active messages take a fixed number of arguments (typically 4 words) for efficiency, and leave the problems of buffer allocation and fragmentation to the upper-level software. <p> Second, the handler code must follow a request/reply protocol to avoid network level deadlock <ref> [vECGS92] </ref>. For example, a handler invoked by a request message cannot send another request message. The protocol limits the types of accesses that can execute in a handler. <p> The segment is used to check the completion of the transfer because the network may reorder active messages. The store operation then returns immediately so that the program can proceed with other computation. The segment implementation is modeled after the implementation of active messages by Thorsten von Eicken <ref> [vECGS92] </ref>. Upon receiving a send request, the receiver sets up a segment and replies the sender with the address of the segment using another active message. When the sender receives the reply, it fragments the message into 4-word packets and injects them into the network. <p> Like Cilk threads, Charm chares are also units of load balance and they may migrate across processors. The Charm system provides little support for using customized scheduling policies for the entry function invocations. There exist many portable communication layers such as MPI (Message Passing Interface [For94] and active messages <ref> [vECGS92] </ref>. MPI provides cooperative message passing primitives for point-to-point and group communication. However, it does not provide irregular communication primitives such as remote fiber invocation. Active messages are similar to remote fiber invocation, but impose more restrictions (see section 3.3.1 for detailed comparison).
Reference: [WHJ + 95] <author> Deborah A. Wallach, Wilson C. Hsieh, Kirk Johnson, M. Frans Kaashoek, and William E. Weihl. </author> <title> Optimistic active messages: A mechanism for scheduling communication with computation. </title> <booktitle> In Proceedings of the 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1995. </year>
Reference-contexts: If the fiber is scheduled by the preemptive scheduler (called a small preemptive message), the active message handler may directly execute the fiber code to avoid the costs of creating a new fiber. This optimization is similar to the idea of optimistic active messages <ref> [WHJ + 95] </ref>. 3.4 System Data Structures The runtime layer provides two system data structures: the distributed object manager for naming distributed objects and the snapshot data structure for taking snapshots of distributed objects. 57 shows the reduction of running time (percentage) due to message aggregation. 3.4.1 Distributed Object Manager The <p> However, it does not provide irregular communication primitives such as remote fiber invocation. Active messages are similar to remote fiber invocation, but impose more restrictions (see section 3.3.1 for detailed comparison). Neither MPI nor active messages have an integrated thread layer. Optimistic active messages <ref> [WHJ + 95] </ref> are similar to the small preemptive messages in Multipol in that they both reduce the thread creation overhead.
Reference: [WY93] <author> Chih-Po Wen and Katherine Yelick. </author> <title> Parallel timing simulation on a distributed memory multiprocessor. </title> <booktitle> In International Conference on CAD, </booktitle> <address> Santa Clara, CA, </address> <month> November </month> <year> 1993. </year> <note> An earlier version appeared as UCB Technical Report CSD-93-723. 123 </note>
Reference-contexts: Programming and optimizing such applications is also considerably more difficult. Finally, applications that exploit speculative parallelism may perform redundant computation. For example, a search application using imperfect heuristics may explore parts of the search space that do not contribute to the solution [JY95], and an optimistic discrete event simulator <ref> [Jef85, WY93] </ref> may waste time on computations that use stale data. Redundant computation presents a tradeoff between parallelism and overhead, because an optimization that increases parallelism may not necessarily improve performance due to 8 a proportional increase in redundant work.
Reference: [WY95] <author> Chih-Po Wen and Katherine Yelick. </author> <title> Portable runtime support for asynchronous simulation. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <address> Oconomowoc, Wisconsin, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: For each data structure, we describe the programming interface, the implementation techniques, and the driving applications. For each example application, we describe its irregularities and execution behaviors using measurements from parallel executions. Several of the applications are described more completely elsewhere <ref> [CDG + 93, DDR94, JY95, WY95] </ref>. The data structures and applications are then summarized and compared in Section 2.3. <p> The speedup of the parallel execution is taken with respect to the sequential execution that uses the same program on one processor, because the sequential implementations of the programs are either unavailable, or they do not show consistent performance gain over the parallel implementation (such as the CSWEC program <ref> [WY95] </ref>). <p> The performance degradation on the Paragon results from the increase in idle time, which is already high in the initial implementation. These results agree with the results from our prior work on the CSWEC program <ref> [WY95] </ref>, 1 which shows that the optimal degree of message aggregation varies with the platform. For the C2670 circuit, the best implementation of the program achieves speedups of 26.8, 117 (super-linear due to paging), and 7.3 for the CM5, Paragon, and SP1, respectively.
Reference: [YCD + 95] <author> K. A. Yelick, S. Chakrabarti, E. Deprit, J. Jones, A. Krishnamurthy, and C. Wen. </author> <title> Parallel data structures for symbolic computation. </title> <booktitle> In Workshop on Parallel Symbolic Languages and Systems, </booktitle> <month> October </month> <year> 1995. </year>
Reference-contexts: While their applications include some degree of irregularity, these libraries do not handle task parallelism or asynchronous communication. Multipol <ref> [YCD + 95] </ref> is a parallel data structure library designed for irregular applications with pointer-based data structures, asynchronous communication, and unpredictable task dependences or computation times. Multipol applications include unstructured mesh computations, search problems, discrete event simulation, and symbolic algebra problems. <p> In the sections that follow, we use these statistics to characterize the application workload. 2.2.1 Bipartite Graph The bipartite graph data structure was implemented by Etienne Deprit <ref> [YCD + 95] </ref> based on an implementation by the Split-C group [CDG + 93]. It is used to perform bulk-synchronous computation over an unstructured mesh. Each node in the graph has a color, which is either red or black and, by construction, edges only connect nodes of different colors. <p> bipartite graph data structure is able to apply runtime preprocessing to optimize the communication schedule. 13 for profiling the fiber granularity is 100 microseconds, and the resolution for the communi cation event size is 16 bytes. 14 2.2.2 Hash Table The hash table data structure was implemented by Etienne Deprit <ref> [YCD + 95] </ref>. The data structure is used to implement a distributed set of key/value pairs. A set can be used to cache the result of computation or to guarantee the uniqueness of data elements. <p> After the move, the peg crossed over by the moved peg is taken out of the board. A solution is a sequence of legal moves that leads to a board configuration with only one peg. The Multipol implementation of the Tripuzzle program was written by Etienne Deprit <ref> [YCD + 95] </ref> based on a CM-5 implementation by Kirk Johnson [Joh93]. The program performs an exhaustive, breadth-first search to count all possible solutions. The search space is a graph of board configurations where each directed edge represents a legal move. <p> Each task repeatedly performs bisection on its interval until two useful sub-intervals are obtained. The input matrix is replicated on all processors, so the bisection computation is local once the interval is obtained. The computation kernel of the Eigenvalue program was written by Soumen Chakrabarti <ref> [YCD + 95] </ref>. Inderjit Dhillion implemented a CM5 version of the bisection algorithm prior to the Multipol implementation [DDR94]. is used to set up the initial interval and perform post-processing after the computation. A computation thread is created on each processor to process the intervals.
Reference: [YM88] <author> C. Yang and B. P. Miller. </author> <title> Critical path analysis for the execution of parallel and distributed programs. </title> <booktitle> In 8th International Conference on Distributed Computing Systems, </booktitle> <address> San Jose, CA, </address> <year> 1988. </year>
Reference-contexts: On the other hand, their toolkits provide performance prediction, while Mprof only provides post-mortem performance profiles. To make performance prediction possible, the statistics in their cost models cannot be computed values. Hollingsworth [HM92] compared several performance metrics for detecting problematic program components which include the critical path metric <ref> [YM88] </ref> and the NPT metric [AL90]. The critical path metric measures the amount of time spent by each procedure in the longest running path of the execution.
References-found: 65

