URL: http://ic-www.arc.nasa.gov/ic/jair-www/volume2/donoho95a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/donoho95a.html
Root-URL: 
Email: donoho@cs.uiuc.edu  rendell@cs.uiuc.edu  
Title: Rerepresenting and Restructuring Domain Theories: A Constructive Induction Approach  
Author: Steven K. Donoho Larry A. Rendell 
Address: 405 N. Mathews Ave., Urbana, IL 61801 USA  
Affiliation: Department of Computer Science, Univeristy of Illinois  
Note: Journal of Artificial Intelligence Research 2 (1995) 411-446 Submitted 11/94; published 4/95  
Abstract: Theory revision integrates inductive learning and background knowledge by combining training examples with a coarse domain theory to produce a more accurate theory. There are two challenges that theory revision and other theory-guided systems face. First, a representation language appropriate for the initial theory may be inappropriate for an improved theory. While the original representation may concisely express the initial theory, a more accurate theory forced to use that same representation may be bulky, cumbersome, and difficult to reach. Second, a theory structure suitable for a coarse domain theory may be insufficient for a fine-tuned theory. Systems that produce only small, local changes to a theory have limited value for accomplishing complex structural alterations that may be required. Consequently, advanced theory-guided learning systems require flexible representation and flexible structure. An analysis of various theory revision systems and theory-guided learning systems reveals specific strengths and weaknesses in terms of these two desired properties. Designed to capture the underlying qualities of each system, a new system uses theory-guided constructive induction. Experiments in three domains show improvement over previous theory-guided systems. This leads to a study of the behavior, limitations, and potential of theory-guided constructive induction.
Abstract-found: 1
Intro-found: 1
Reference: <author> Baffes, P., & Mooney, R. </author> <year> (1993). </year> <title> Symbolic revision of theories with M-of-N rules. </title> <booktitle> In Proceedings of the 1993 IJCAI. </booktitle>
Reference-contexts: Furthermore, arriving at the final theory using the refinement operators most suitable for DNF (drop-condition, add-condition, modify-condition) would be a cumbersome task. But when an M-of-N representation is adopted, the refinement simply involves empirically finding the appropriate M, and the final theory can be expressed concisely <ref> (Baffes & Mooney, 1993) </ref>. Similarly, the second desirable quality, flexibility of structure, arises because the theory structure that was suitable for a coarse domain theory may be insufficient for a fine-tuned theory. In order to achieve the desired accuracy, a restructuring of the initial theory may be necessary. <p> Although the full extent of Kbann's power may be needed for some problems, many important problems may be solvable by applying Kbann's principles at the symbolic level using less expensive tools. Neither-MofN <ref> (Baffes & Mooney, 1993) </ref>, a descendant of Either, is a second example of a system that allows a theory to be revised in a representation other than that of the initial theory. The domain theory input into Neither-MofN is expressed in propositional logic as an AND/OR tree. <p> The M-of-N representation it employs is not as big a change from the original representation as the neural net representation which Kbann employs yet it achieves similar results and arrives at them much more quickly than Kbann <ref> (Baffes & Mooney, 1993) </ref>. A shortcoming of Neither-MofN is that since it acts by making local changes in an initial theory, it can still become trapped by the structure of the initial theory.
Reference: <author> Bloedorn, E., Michalski, R., & Wnek, J. </author> <year> (1993). </year> <title> Multistrategy constructive induction: </title> <booktitle> AQ17-MCI. In Proceeding of the second international workshop on multistrategy learning. </booktitle>
Reference: <author> Clark, P., & Matwin, S. </author> <year> (1993). </year> <title> Using qualitative models to guide inductive learning. </title> <booktitle> In Proceedings of the 1993 International Conference on Machine Learning. </booktitle>
Reference: <author> Cohen, W. </author> <year> (1992). </year> <title> Compiling prior knowledge into an explicit bias. </title> <booktitle> In Proceedings of the 1992 International Conference on Machine Learning. </booktitle>
Reference: <author> Craven, M. W., & Shavlik, J. W. </author> <year> (1995). </year> <title> Investigating the value of a good input representation. Computational Learning Theory and Natural Learning Systems, 3. </title> <publisher> Forthcoming. </publisher>
Reference-contexts: Eliminating these anomalies would remove this bias. 5. Experiments and Analysis This section presents the results of applying theory-guided constructive induction to three domains: the promoter domain (Harley et al., 1990), the primate splice-junction domain (No-ordewier, Shavlik, & Towell, 1992), and the gene identification domain <ref> (Craven & Shavlik, 1995) </ref>. In each case the Tgci1 interpreter was applied to the domain's theory and examples in order to redescribe the examples using new features. <p> Theory-guided constructive induction has an advantage of speed over Kbann because C4.5, its underlying learner, runs much more quickly than backpropagation, Kbann's underlying learning algorithm. 5.3 The Gene Identification Domain The gene identification domain <ref> (Craven & Shavlik, 1995) </ref> involves classifying a given DNA segment as a coding sequence (one that codes a protein) or a non-coding sequence.
Reference: <author> Drastal, G., & Raatz, S. </author> <year> (1989). </year> <title> Empirical results on learning in an abstraction space. </title> <booktitle> In Proceedings of the 1989 IJCAI. </booktitle>
Reference-contexts: In this section we analyze Miro, Either, Focl, Labyrinth K , Kbann, Neither-MofN, and Grendel to discuss their related underlying contributions in relationship to our perspective. 2.1 Miro Miro <ref> (Drastal & Raatz, 1989) </ref> is a seminal work in knowledge-guided constructive induction. It takes knowledge about how low-level features interact and uses this knowledge to construct high-level features for its training examples. A standard learning algorithm is then run on these examples described using the new features.
Reference: <author> Dzerisko, S., & Lavrac, N. </author> <year> (1991). </year> <title> Learning relations from noisy examples: An empirical comparison of LINUS and FOIL. </title> <booktitle> In Proceedings of the 1991 International Conference on Machine Learning. 444 Rerepresenting and Restructuring Domain Theories Feldman, </booktitle> <editor> R., Serge, A., & Koppel, M. </editor> <year> (1991). </year> <title> Incremental refinement of approximate domain theories. </title> <booktitle> In Proceedings of the 1991 International Conference on Machine Learning. </booktitle>
Reference: <author> Flann, N., & Dietterich, T. </author> <year> (1989). </year> <title> A study of explanation-based methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 187-226. </pages>
Reference: <author> Fu, L. M., & Buchanan, B. G. </author> <year> (1985). </year> <title> Learning intermediate concepts in constructing a hierarchical knowledge base. </title> <booktitle> In Proceedings of the 1985 IJCAI. </booktitle>
Reference: <author> Harley, C., Reynolds, R., & Noordewier, M. </author> <year> (1990). </year> <title> Creators of original promoter dataset. </title>
Reference-contexts: This is a case of the principle of flexible structure. In the following section we introduce the DNA Promoter Recognition domain in order to illustrate tangibly how some of the systems discussed above integrate knowledge and induction. 3. Demonstrations of Related Work This section introduces the Promoter Recognition domain <ref> (Harley, Reynolds, & Noordewier, 1990) </ref> and briefly illustrates how a Miro-like system, Either, Kbann, and Neither-MofN behave in this domain. We implemented a Miro-like system for the promoter domain; versions of Either and Neither-MofN were available from Ray Mooney's group; Kbann's behavior is described by analyzing (Towell & Shavlik, 1994). <p> Eliminating these anomalies would remove this bias. 5. Experiments and Analysis This section presents the results of applying theory-guided constructive induction to three domains: the promoter domain <ref> (Harley et al., 1990) </ref>, the primate splice-junction domain (No-ordewier, Shavlik, & Towell, 1992), and the gene identification domain (Craven & Shavlik, 1995). In each case the Tgci1 interpreter was applied to the domain's theory and examples in order to redescribe the examples using new features.
Reference: <author> Hirsh, H., & Noordewier, M. </author> <year> (1994). </year> <title> Using background knowledge to improve inductive learning of DNA sequences. </title> <booktitle> In Tenth IEEE Conference on AI for Applications San Antonio, </booktitle> <address> TX. </address>
Reference: <author> Matheus, C. J., & Rendell, L. A. </author> <year> (1989). </year> <title> Constructive induction on decision trees. </title> <booktitle> In Proceedings of the 1989 IJCAI. </booktitle>
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20 (2), </volume> <pages> 111-161. </pages>
Reference: <author> Mitchell, T. </author> <year> (1977). </year> <title> Version spaces: A candidate elimination approach to rule learning. </title> <booktitle> In Proceedings of the 1977 IJCAI. </booktitle>
Reference-contexts: New features would similarly be created for the minus 10 rules and the conformation rules, and a standard induction algorithm could then be applied. We implemented a Miro-like system; Figure 7 gives an example theory created by it. (Drastal's original Miro used the candidate elimination algorithm <ref> (Mitchell, 1977) </ref> as its underlying induction algorithm. We used C4.5 (Quinlan, 1993).) As opposed to theory revision systems that incrementally modify the domain theory, Miro has broken the theory down into its components and has fashioned these components into a new theory using a standard induction program.
Reference: <author> Mooney, R. J. </author> <year> (1993). </year> <title> Induction over the unexplained: Using overly-general domain theories to aid concept learning. </title> <journal> Machine Learning, </journal> <volume> 10 (1), </volume> <pages> 79-110. </pages>
Reference-contexts: Furthermore, arriving at the final theory using the refinement operators most suitable for DNF (drop-condition, add-condition, modify-condition) would be a cumbersome task. But when an M-of-N representation is adopted, the refinement simply involves empirically finding the appropriate M, and the final theory can be expressed concisely <ref> (Baffes & Mooney, 1993) </ref>. Similarly, the second desirable quality, flexibility of structure, arises because the theory structure that was suitable for a coarse domain theory may be insufficient for a fine-tuned theory. In order to achieve the desired accuracy, a restructuring of the initial theory may be necessary. <p> Although the full extent of Kbann's power may be needed for some problems, many important problems may be solvable by applying Kbann's principles at the symbolic level using less expensive tools. Neither-MofN <ref> (Baffes & Mooney, 1993) </ref>, a descendant of Either, is a second example of a system that allows a theory to be revised in a representation other than that of the initial theory. The domain theory input into Neither-MofN is expressed in propositional logic as an AND/OR tree. <p> The M-of-N representation it employs is not as big a change from the original representation as the neural net representation which Kbann employs yet it achieves similar results and arrives at them much more quickly than Kbann <ref> (Baffes & Mooney, 1993) </ref>. A shortcoming of Neither-MofN is that since it acts by making local changes in an initial theory, it can still become trapped by the structure of the initial theory.
Reference: <author> Mooney, R. J., Shavlik, J. W., Towell, G. G., & Gove, A. </author> <year> (1989). </year> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <booktitle> In Proceedings of the 1989 IJCAI. </booktitle>
Reference-contexts: Tgci runs as much as 100 times faster than Neither-MofN on large datasets. A strict quantitative comparison of the speeds of Tgci and Kbann was not made because 1) backpropagation is known to be much slower than decision trees <ref> (Mooney, Shavlik, Towell, & Gove, 1989) </ref>, 2) Kbann uses multiple hidden layers which makes its training time even longer (Towell & Shavlik, 1994), and 3) Towell and Shavlik (1994) point out that each run of Kbann must be made multiple times with different initial random weights, whereas a single run of
Reference: <author> Murphy, P., & Pazzani, M. </author> <year> (1991). </year> <title> ID2-of-3: Constructive induction of M-of-N concepts for discriminators in decision trees. </title> <booktitle> In Proceedings of the 1991 International Conference on Machine Learning. </booktitle>
Reference-contexts: The most accurate theory, though, is one in which any M of these N conditions holds. Expressing this more accurate theory in the DNF representation used to describe the initial theory would be cumbersome and unwieldy <ref> (Murphy & Pazzani, 1991) </ref>. Furthermore, arriving at the final theory using the refinement operators most suitable for DNF (drop-condition, add-condition, modify-condition) would be a cumbersome task.
Reference: <author> Noordewier, M., Shavlik, J., & Towell, G. </author> <year> (1992). </year> <note> Donors of original primate splice-junction dataset. </note>
Reference-contexts: We speculate this because we conducted some experiments that allowed bias-guided dropping and adding of conditions within Tgci. We found that these techniques actually reduced accuracy in this domain. main. 5.2 The Primate Splice-junction Domain The primate splice-junction domain <ref> (Noordewier et al., 1992) </ref> involves analyzing a DNA sequence and identifying boundaries between introns and exons. Exons are the parts of a DNA sequence kept after splicing; introns are spliced out.
Reference: <author> Ourston, D., & Mooney, R. </author> <year> (1990). </year> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the 1990 National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: Figure 1 illustrates this situation. Minor revisions have been made conditions have been added, dropped, and modified but the refined theory is trapped by the backbone structure of the initial theory. When only local changes are needed, these techniques have proven useful <ref> (Ourston & Mooney, 1990) </ref>, but often more is required. When more is required, these systems often move to the other extreme; they drop entire rules and groups of rules and then build entire new rules and groups of rules from scratch to replace them. <p> Also the representation of Miro's constructed features was primitive | either an example met the conditions of a high-level feature or did not. An example of Miro's behavior is given in Section 3.2. 2.2 Either, Focl, and Labyrinth K The Either <ref> (Ourston & Mooney, 1990) </ref>, Labyrinth K (Thompson et al., 1991), and Focl (Pazzani & Kibler, 1992) systems represent a broad spectrum of theory revision work. They make steps toward effective integration of background knowledge and inductive learning.
Reference: <author> Pagallo, G., & Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 (1), </volume> <pages> 71-99. </pages>
Reference: <author> Pazzani, M., & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 (1), </volume> <pages> 57-94. </pages>
Reference-contexts: An example of Miro's behavior is given in Section 3.2. 2.2 Either, Focl, and Labyrinth K The Either (Ourston & Mooney, 1990), Labyrinth K (Thompson et al., 1991), and Focl <ref> (Pazzani & Kibler, 1992) </ref> systems represent a broad spectrum of theory revision work. They make steps toward effective integration of background knowledge and inductive learning.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. 445 Donoho & Rendell Ragavan, </publisher> <editor> H., & Rendell, L. </editor> <year> (1993). </year> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> In Proceedings of the 1993 International Conference on Machine Learning. </booktitle>
Reference-contexts: The constructed features need not be expressed in the same representational language as the initial theory and can be refined to better match the training examples. Finally, a standard inductive learning algorithm, C4.5 <ref> (Quinlan, 1993) </ref>, is applied to the redescribed examples. We begin by analyzing how landmark theory revision and learning systems have exhibited flexibility in handling a domain theory and what part this has played in their performance. <p> We implemented a Miro-like system; Figure 7 gives an example theory created by it. (Drastal's original Miro used the candidate elimination algorithm (Mitchell, 1977) as its underlying induction algorithm. We used C4.5 <ref> (Quinlan, 1993) </ref>.) As opposed to theory revision systems that incrementally modify the domain theory, Miro has broken the theory down into its components and has fashioned these components into a new theory using a standard induction program. <p> In each case the Tgci1 interpreter was applied to the domain's theory and examples in order to redescribe the examples using new features. Then C4.5 <ref> (Quinlan, 1993) </ref> was applied to the redescribed examples. 5.1 The Promoter Domain domain accompanied by curves for Either, Labyrinth K , Kbann, and Neither-MofN.
Reference: <author> Rumelhart, D. E., Hinton, G. E., & McClelland, J. L. </author> <year> (1986). </year> <title> A general framework for parallel distributed processing. </title> <editor> In Rumelhart, D. E., & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microarchitecture of Cognition, Volume I. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Kbann takes an initial domain theory described symbolically in logic and creates a neural network whose structure and initial weights encode this theory. Backpropagation <ref> (Rumelhart, Hinton, & McClelland, 1986) </ref> is then applied as a refinement tool for fine-tuning the network weights. Kbann has been empirically shown to give 416 Rerepresenting and Restructuring Domain Theories significant improvement over many theory revision systems for the widely-used Promoter Recognition domain.
Reference: <author> Schlimmer, J. C. </author> <year> (1987). </year> <title> Learning and representation change. </title> <editor> In Kaufmann, M. (Ed.), </editor> <booktitle> Proceedings of the 1987 National Conference on Artificial Intelligence. </booktitle>
Reference: <author> Thompson, K., Langley, P., & Iba, W. </author> <year> (1991). </year> <title> Using background knowledge in concept formation. </title> <booktitle> In Proceedings of the 1991 International Conference on Machine Learning. </booktitle>
Reference-contexts: Also the representation of Miro's constructed features was primitive | either an example met the conditions of a high-level feature or did not. An example of Miro's behavior is given in Section 3.2. 2.2 Either, Focl, and Labyrinth K The Either (Ourston & Mooney, 1990), Labyrinth K <ref> (Thompson et al., 1991) </ref>, and Focl (Pazzani & Kibler, 1992) systems represent a broad spectrum of theory revision work. They make steps toward effective integration of background knowledge and inductive learning.
Reference: <author> Towell, G., & Shavlik, J. </author> <year> (1994). </year> <title> Knowledge-based artificial neural networks. </title> <journal> Artificial Intelligence, </journal> <volume> 70, </volume> <pages> 119-165. </pages>
Reference-contexts: We implemented a Miro-like system for the promoter domain; versions of Either and Neither-MofN were available from Ray Mooney's group; Kbann's behavior is described by analyzing <ref> (Towell & Shavlik, 1994) </ref>. <p> A strict quantitative comparison of the speeds of Tgci and Kbann was not made because 1) backpropagation is known to be much slower than decision trees (Mooney, Shavlik, Towell, & Gove, 1989), 2) Kbann uses multiple hidden layers which makes its training time even longer <ref> (Towell & Shavlik, 1994) </ref>, and 3) Towell and Shavlik (1994) point out that each run of Kbann must be made multiple times with different initial random weights, whereas a single run of Tgci is sufficient.
Reference: <author> Towell, G., Shavlik, J., & Noordeweir, M. </author> <year> (1990). </year> <title> Refinement of approximately correct domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the 1990 National Conference on Artificial Intelligence. </booktitle>
Reference: <author> Utgoff, P. E. </author> <year> (1986). </year> <title> Shift of bias for inductive concept learning. </title> <editor> In Michalski, Carbonell, & Mitchell (Eds.), </editor> <booktitle> Machine Learning, </booktitle> <volume> Vol. 2, chap. 5, </volume> <pages> pp. 107-148. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It takes knowledge about how low-level features interact and uses this knowledge to construct high-level features for its training examples. A standard learning algorithm is then run on these examples described using the new features. The domain theory is used to shift the bias of the induction problem <ref> (Utgoff, 1986) </ref>. Empirical results showed that describing the examples in these high-level, abstract terms improved learning accuracy. The Miro approach provides a means of utilizing knowledge in a domain theory without being restricted by the structure of that theory.
Reference: <author> Wogulis, J. </author> <year> (1991). </year> <title> Revising relational domain theories. </title> <booktitle> In Proceedings of the 1991 International Conference on Machine Learning. </booktitle> <pages> 446 </pages>
References-found: 29

