URL: http://www.cs.wisc.edu/~solodov/mvs95increm.ps.Z
Refering-URL: 
Root-URL: 
Email: Email solodov@impa.br  
Title: INCREMENTAL GRADIENT ALGORITHMS WITH STEPSIZES BOUNDED AWAY FROM ZERO  
Author: M. V. Solodov 
Keyword: Key words. incremental gradient methods, perturbed gradient methods, approximate solutions, backpropagation, neural network training  
Address: RJ 22460-320, Brazil.  
Affiliation: Instituto de Matematica Pura e Aplicada, Estrada Dona Castorina 110, Jardim Bot^anico, Rio de Janeiro,  
Note: Computational Optimization and Applications (to appear)  This material is based on research supported in part by CNPq grant number 300734/95-6.  
Abstract: We consider the class of incremental gradient methods for minimizing a sum of continuously differentiable functions. An important novel feature of our analysis is that the stepsizes are kept bounded away from zero. We derive the first convergence results of any kind for this computationally important case. In particular, we show that a certain "-approximate solution can be obtained and establish the linear dependence of " on the stepsize limit. Incremental gradient methods are particularly well-suited for large neural network training problems where obtaining an approximate solution is typically sufficient and often is preferable to computing an exact solution. Thus, in the context of neural networks, the approach presented here is related to the principle of tolerant training. Our results justify numerous stepsize rules that were derived on the basis of extensive numerical experimentation but for which no theoretical analysis was previously available. In addition, convergence to (exact) stationary points is established when the gradient satisfies a certain growth property. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.P. Bertsekas. </author> <title> A new class of incremental gradient methods for least squares problems. </title> <note> SIAM J. on Optimization, to appear. 13 </note>
Reference-contexts: This requirement usually drives the stepsizes to zero, unless some additional assumptions are satisfied (we emphasize that we do not make these assumptions for the main result of this paper). In <ref> [1] </ref> a hybrid algorithm is proposed which is aimed at accelerating (local) convergence of IGA-type methods. This new algorithm essentially works just like IGA when far from the eventual limit, and it gradually transforms into the steepest descent as the iterates approach a stationary point of the problem. <p> We refer the reader to artificial intelli gence literature for a discussion of overfitting and other related issues (see [20] and references 3 therein). Motivated by the above considerations, we adopt a slightly different point of view on the issues related to convergence of IGA-type techniques than that in <ref> [13, 10, 1, 21] </ref>. We note that tolerant training permits certain errors in fitting the training data which, in the context of this paper, can be viewed as solving the problem (1.1) inexactly.
Reference: [2] <author> D.P. Bertsekas. </author> <title> Nonlinear programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1995. </year>
Reference-contexts: In applications where K is large, the following incremental gradient algorithm (IGA) proved to be very useful (see also <ref> [2, Section 1.5.2] </ref>). Algorithm 1.1 (IGA) Choose any x 0 2 &lt; n . Having x i , check a stopping criterion. <p> This fact makes it difficult to apply standard Lyapunov-type techniques [14, 15] to the analysis of IGA. Thus a new approach had to be developed. This problem has recently attracted a lot of interest. The first deterministic results were obtained in [13, 10] (see also <ref> [2, 4] </ref>), where the stepsizes are chosen so that to satisfy the following condition 1 X i = 1; i=0 i &lt; 1: (1.2) In particular, it was shown that if (1.2) is satisfied then the sequence ff (x i )g converges and the sequence frf (x i )g converges to
Reference: [3] <author> D.P. Bertsekas. </author> <title> Incremental least squares methods and the extended Kalman filter. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 6 </volume> <pages> 807-822, </pages> <year> 1996. </year>
Reference-contexts: Stochastic analysis under conditions similar to (1.2) can be found in [22, 6]. Error-stability properties of a very general class of algorithms, which includes IGA, are analyzed in [18]. And in <ref> [3] </ref> a related least-squares incremental method is considered. The results just cited, though significant, still left a certain gap between theoretical convergence analysis of incremental algorithms and computational practice.
Reference: [4] <author> D.P. Bertsekas and J.N. Tsitsiklis. </author> <title> Neuro-Dynamic programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1996. </year>
Reference-contexts: This fact makes it difficult to apply standard Lyapunov-type techniques [14, 15] to the analysis of IGA. Thus a new approach had to be developed. This problem has recently attracted a lot of interest. The first deterministic results were obtained in [13, 10] (see also <ref> [2, 4] </ref>), where the stepsizes are chosen so that to satisfy the following condition 1 X i = 1; i=0 i &lt; 1: (1.2) In particular, it was shown that if (1.2) is satisfied then the sequence ff (x i )g converges and the sequence frf (x i )g converges to
Reference: [5] <author> A. Cichocki and R. Unbehauen. </author> <title> Neural Networks for optimization and signal processing. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: On the domain of large neural network training problems, this algorithm is known to be often superior to standard optimization techniques which process all the partial objective functions before adjusting the variables (see <ref> [5, 7] </ref> for a discussion of this issue). <p> In artificial intelligence literature IGA is usually referred to as online backpropagation training [16]. In addition to being faster, IGA has some other advantages over standard optimization methods when considered in the machine learning context. For example, it can be used in real-time on-chip operation <ref> [5] </ref>. Despite the popularity of incremental methods within the artificial intelligence community and their wide use in practice, until very recently there existed no rigorous convergence analysis for this class of algorithms. <p> Fortunately, in neural network applications one is typically not interested in computing an exact solution of (1.1) (more on this later). The results in this paper provide theoretical foundation for a number of heuristic stepsize rules that satisfy (1.3) but not (1.2) (see <ref> [5] </ref>). We finally mention some interesting recent work on incremental algorithms. In [21] new adaptive stepsize rules are proposed and analyzed. These rules are much in the spirit of heuristics that are used in practice.
Reference: [6] <author> A.A. Gaivoronski. </author> <title> Convergence properties of backpropagation for neural networks via theory of stochastic gradient methods. Part I. </title> <journal> Optimization Methods and Software, </journal> <volume> 4 </volume> <pages> 117-134, </pages> <year> 1994. </year>
Reference-contexts: Stochastic analysis under conditions similar to (1.2) can be found in <ref> [22, 6] </ref>. Error-stability properties of a very general class of algorithms, which includes IGA, are analyzed in [18]. And in [3] a related least-squares incremental method is considered. The results just cited, though significant, still left a certain gap between theoretical convergence analysis of incremental algorithms and computational practice.
Reference: [7] <author> T. Khanna. </author> <title> Foundations of neural networks. </title> <publisher> Addison-Wesley, </publisher> <address> New Jersey, </address> <year> 1989. </year>
Reference-contexts: On the domain of large neural network training problems, this algorithm is known to be often superior to standard optimization techniques which process all the partial objective functions before adjusting the variables (see <ref> [5, 7] </ref> for a discussion of this issue).
Reference: [8] <author> K. Lang, A. Waibel, and G. Hinton. </author> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 23-43, 90. </pages>
Reference-contexts: Thus state-of-the-art neural network training systems almost always use some kind of early stopping criteria that terminate training before an exact solution to (1.1) is attained. For example, the use of tuning sets is popular <ref> [8] </ref>. We refer the reader to artificial intelli gence literature for a discussion of overfitting and other related issues (see [20] and references 3 therein).
Reference: [9] <author> Z.-Q. Luo. </author> <title> On the convergence of the LMS algorithm with adaptive learning rate for linear feedforward networks. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 226-245, </pages> <year> 1991. </year>
Reference-contexts: In particular, (1.2) implies that the stepsizes tend to zero in the limit, while many heuristic rules used by practitioners keep them bounded away from zero. It is therefore of importance to study the 2 behaviour of IGA when lim i = &gt; 0: (1.3) An example in <ref> [9] </ref> shows that in general, under the condition of (1.3), one cannot expect convergence to an exact solution even in the simple case when f () is given by a sum of two strongly convex quadratic (not identical) functions.
Reference: [10] <author> Z.-Q. Luo and P. Tseng. </author> <title> Analysis of an approximate gradient projection method with applications to the backpropagation algorithm. </title> <journal> Optimization Methods and Software, </journal> <volume> 4 </volume> <pages> 85-101, </pages> <year> 1994. </year>
Reference-contexts: This fact makes it difficult to apply standard Lyapunov-type techniques [14, 15] to the analysis of IGA. Thus a new approach had to be developed. This problem has recently attracted a lot of interest. The first deterministic results were obtained in <ref> [13, 10] </ref> (see also [2, 4]), where the stepsizes are chosen so that to satisfy the following condition 1 X i = 1; i=0 i &lt; 1: (1.2) In particular, it was shown that if (1.2) is satisfied then the sequence ff (x i )g converges and the sequence frf (x <p> &lt; 1: (1.2) In particular, it was shown that if (1.2) is satisfied then the sequence ff (x i )g converges and the sequence frf (x i )g converges to zero (in [13], furthermore, the use of a momentum term and a parallel version of IGA were considered, while in <ref> [10] </ref> a constrained version of IGA was studied). Stochastic analysis under conditions similar to (1.2) can be found in [22, 6]. Error-stability properties of a very general class of algorithms, which includes IGA, are analyzed in [18]. And in [3] a related least-squares incremental method is considered. <p> We refer the reader to artificial intelli gence literature for a discussion of overfitting and other related issues (see [20] and references 3 therein). Motivated by the above considerations, we adopt a slightly different point of view on the issues related to convergence of IGA-type techniques than that in <ref> [13, 10, 1, 21] </ref>. We note that tolerant training permits certain errors in fitting the training data which, in the context of this paper, can be viewed as solving the problem (1.1) inexactly. <p> are contained in some set fx j f (x) 1 g + fx j kxk 2 g which is bounded if the level set fx j f (x) 1 g is bounded for some 1 &gt; f (x 0 ), as is the typical case with neural network training (see <ref> [10, x3] </ref>). It is also easy to see that the gradient of the neural network training function is Lipschitz continuous and bounded on any bounded set. <p> Proof. The result follows from combining Proposition 2.1 and Theorem 2.1. The analysis presented here can be applied to a variety of modifications and extensions of Algorithm 1.1. For example, using the approach of [19], we could treat the projection version of IGA described in <ref> [10, 21] </ref>. At the expense of introducing considerably more notation and some technical details, we could also consider the parallel and momentum term modifications given in [13, 21], as well as algorithms with noisy data along the lines of [18].
Reference: [11] <author> O.L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: learning (in particular, neural network) applications, where weights and thresholds of the network comprise the problem variable x 2 &lt; n , K is the number of training samples, and f j () represents the error associated with the j-th sample, j = 1; : : : ; K (see <ref> [11] </ref> for a detailed description). In applications where K is large, the following incremental gradient algorithm (IGA) proved to be very useful (see also [2, Section 1.5.2]). Algorithm 1.1 (IGA) Choose any x 0 2 &lt; n . Having x i , check a stopping criterion.
Reference: [12] <editor> O.L. Mangasarian and M.V. Solodov. </editor> <title> Backpropagation convergence via deterministic nonmonotone perturbed minimization. </title> <editor> In G. Tesauro J.D. Cowan and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 383-390, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann Publishers. </publisher> <pages> 14 </pages>
Reference-contexts: This approach would be very close to heuristics used in practice. As an alternative to decreasing the stepsize, one could dynamically aggregate partial objective functions into (larger) groups (as mentioned in <ref> [12] </ref>) which is also likely to reduce the right-hand-side of (2.7). As a side result, we now establish convergence of IGA to exact stationary points of (1.1) under a growth condition on the gradients very similar to that used in [21].
Reference: [13] <editor> O.L. Mangasarian and M.V. Solodov. </editor> <title> Serial and parallel backpropagation convergence via nonmonotone perturbed minimization. </title> <journal> Optimization Methods and Software, </journal> <volume> 4 </volume> <pages> 103-116, </pages> <year> 1994. </year>
Reference-contexts: This fact makes it difficult to apply standard Lyapunov-type techniques [14, 15] to the analysis of IGA. Thus a new approach had to be developed. This problem has recently attracted a lot of interest. The first deterministic results were obtained in <ref> [13, 10] </ref> (see also [2, 4]), where the stepsizes are chosen so that to satisfy the following condition 1 X i = 1; i=0 i &lt; 1: (1.2) In particular, it was shown that if (1.2) is satisfied then the sequence ff (x i )g converges and the sequence frf (x <p> the stepsizes are chosen so that to satisfy the following condition 1 X i = 1; i=0 i &lt; 1: (1.2) In particular, it was shown that if (1.2) is satisfied then the sequence ff (x i )g converges and the sequence frf (x i )g converges to zero (in <ref> [13] </ref>, furthermore, the use of a momentum term and a parallel version of IGA were considered, while in [10] a constrained version of IGA was studied). Stochastic analysis under conditions similar to (1.2) can be found in [22, 6]. <p> We refer the reader to artificial intelli gence literature for a discussion of overfitting and other related issues (see [20] and references 3 therein). Motivated by the above considerations, we adopt a slightly different point of view on the issues related to convergence of IGA-type techniques than that in <ref> [13, 10, 1, 21] </ref>. We note that tolerant training permits certain errors in fitting the training data which, in the context of this paper, can be viewed as solving the problem (1.1) inexactly. <p> It is worth to point out that the main results of this paper cannot be obtained using the approach of <ref> [13] </ref>. As an aside, we show that under a certain additional assumption on the growth property on the gradients (similar to the one used in [21]), IGA converges to (exact) stationary points. We briefly describe our notation. <p> For example, using the approach of [19], we could treat the projection version of IGA described in [10, 21]. At the expense of introducing considerably more notation and some technical details, we could also consider the parallel and momentum term modifications given in <ref> [13, 21] </ref>, as well as algorithms with noisy data along the lines of [18]. Although specific stepsize rules are not a subject of this paper, we shall make a few remarks concerning this issue.
Reference: [14] <author> E. Polak. </author> <title> Computational methods in optimization: A unified approach. </title> <publisher> Academic Press, </publisher> <address> New York, New York, </address> <year> 1971. </year>
Reference-contexts: This fact makes it difficult to apply standard Lyapunov-type techniques <ref> [14, 15] </ref> to the analysis of IGA. Thus a new approach had to be developed. This problem has recently attracted a lot of interest.
Reference: [15] <author> B.T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: This fact makes it difficult to apply standard Lyapunov-type techniques <ref> [14, 15] </ref> to the analysis of IGA. Thus a new approach had to be developed. This problem has recently attracted a lot of interest.
Reference: [16] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <pages> pages 318-362, </pages> <address> Cambridge, Massachusetts, 1986. </address> <publisher> MIT Press. </publisher>
Reference-contexts: For problems of this class, incremental methods have to be used. In artificial intelligence literature IGA is usually referred to as online backpropagation training <ref> [16] </ref>. In addition to being faster, IGA has some other advantages over standard optimization methods when considered in the machine learning context. For example, it can be used in real-time on-chip operation [5].
Reference: [17] <author> S. Shah, F. Palmieri, and M. Datum. </author> <title> Optimal filtering algorithms for fast learning in feedforward neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 779-787, </pages> <year> 1992. </year>
Reference-contexts: Unfortunately, this often seems to be the case in machine learning. For many practical neural network systems, standard optimization methods require storage and/or computational cost which can become unmanageable even for a moderate network size, provided the training set (i.e. the number K) is large enough <ref> [17] </ref>. For problems of this class, incremental methods have to be used. In artificial intelligence literature IGA is usually referred to as online backpropagation training [16]. In addition to being faster, IGA has some other advantages over standard optimization methods when considered in the machine learning context.
Reference: [18] <author> M. V. Solodov and S. K. Zavriev. </author> <title> Error-stabilty properties of generalized gradient-type algorithms. </title> <journal> Journal of Optimization Theory and Applications, </journal> <note> to appear. </note>
Reference-contexts: Stochastic analysis under conditions similar to (1.2) can be found in [22, 6]. Error-stability properties of a very general class of algorithms, which includes IGA, are analyzed in <ref> [18] </ref>. And in [3] a related least-squares incremental method is considered. The results just cited, though significant, still left a certain gap between theoretical convergence analysis of incremental algorithms and computational practice. <p> At the expense of introducing considerably more notation and some technical details, we could also consider the parallel and momentum term modifications given in [13, 21], as well as algorithms with noisy data along the lines of <ref> [18] </ref>. Although specific stepsize rules are not a subject of this paper, we shall make a few remarks concerning this issue. In practice, one usually starts with a fixed intuitively reasonable stepsize value and uses it as long as the algorithm makes sufficient progress according to some chosen criterion.
Reference: [19] <author> M.V. Solodov. </author> <title> Convergence analysis of perturbed feasible descent methods. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 93: </volume> <pages> 337-353, </pages> <year> 1997. </year>
Reference-contexts: Our analysis is by virtue of characterizing IGA as a special perturbed gradient method (Proposition 2.1), and it makes use of some of the ideas employed in <ref> [19] </ref> where general perturbed feasible descent algorithms are studied. It is worth to point out that the main results of this paper cannot be obtained using the approach of [13]. <p> Proof. The result follows from combining Proposition 2.1 and Theorem 2.1. The analysis presented here can be applied to a variety of modifications and extensions of Algorithm 1.1. For example, using the approach of <ref> [19] </ref>, we could treat the projection version of IGA described in [10, 21].
Reference: [20] <author> W.N. </author> <title> Street and O.L. Mangasarian. Improved generalization via tolerant training. </title> <type> Technical Report 95-11, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, U.S.A., </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: This phenomenon is known as overtraining or overfitting. Fitting the data very accurately can be particularly harmful in the presence of noise. It is a widely accepted heuristic in machine learning that tolerant training should be employed to avoid overfitting <ref> [20] </ref>. Thus state-of-the-art neural network training systems almost always use some kind of early stopping criteria that terminate training before an exact solution to (1.1) is attained. For example, the use of tuning sets is popular [8]. <p> For example, the use of tuning sets is popular [8]. We refer the reader to artificial intelli gence literature for a discussion of overfitting and other related issues (see <ref> [20] </ref> and references 3 therein). Motivated by the above considerations, we adopt a slightly different point of view on the issues related to convergence of IGA-type techniques than that in [13, 10, 1, 21]. <p> Furthermore, we establish at least linear dependence of " on the limiting value of the sequence of stepsizes. It can be argued that computing an approximate solution falls within the tolerant training principle <ref> [20] </ref> of machine learning and, consequently, that having stepsizes bounded away from zero is, in some sense, sufficient for solving a neural network training problem. We first show that IGA can be regarded as a perturbed gradient algorithm with a certain special structure.
Reference: [21] <author> P. Tseng. </author> <title> Incremental gradient(-projection) method with momentum term and adaptive stepsize rule. </title> <note> SIAM J. on Optimization, to appear. </note>
Reference-contexts: The results in this paper provide theoretical foundation for a number of heuristic stepsize rules that satisfy (1.3) but not (1.2) (see [5]). We finally mention some interesting recent work on incremental algorithms. In <ref> [21] </ref> new adaptive stepsize rules are proposed and analyzed. These rules are much in the spirit of heuristics that are used in practice. However, the rules in [21] are designed so that to find an exact solution (stationary point) of the problem. <p> We finally mention some interesting recent work on incremental algorithms. In <ref> [21] </ref> new adaptive stepsize rules are proposed and analyzed. These rules are much in the spirit of heuristics that are used in practice. However, the rules in [21] are designed so that to find an exact solution (stationary point) of the problem. This requirement usually drives the stepsizes to zero, unless some additional assumptions are satisfied (we emphasize that we do not make these assumptions for the main result of this paper). <p> We refer the reader to artificial intelli gence literature for a discussion of overfitting and other related issues (see [20] and references 3 therein). Motivated by the above considerations, we adopt a slightly different point of view on the issues related to convergence of IGA-type techniques than that in <ref> [13, 10, 1, 21] </ref>. We note that tolerant training permits certain errors in fitting the training data which, in the context of this paper, can be viewed as solving the problem (1.1) inexactly. <p> It is worth to point out that the main results of this paper cannot be obtained using the approach of [13]. As an aside, we show that under a certain additional assumption on the growth property on the gradients (similar to the one used in <ref> [21] </ref>), IGA converges to (exact) stationary points. We briefly describe our notation. The usual inner product of two vectors x 2 &lt; n , y 2 &lt; n is denoted by hx; yi. The Euclidean 2-norm of x 2 &lt; n is given by kxk 2 = hx; xi. <p> A remark about the assumptions of Theorem 2.2 below is in order. In this theorem, we explicitly assume that the sequence fx i g generated by IGA is bounded. We note that this is not restrictive since it can be shown (see <ref> [21] </ref>) that the iterates are contained in some set fx j f (x) 1 g + fx j kxk 2 g which is bounded if the level set fx j f (x) 1 g is bounded for some 1 &gt; f (x 0 ), as is the typical case with neural <p> Proof. The result follows from combining Proposition 2.1 and Theorem 2.1. The analysis presented here can be applied to a variety of modifications and extensions of Algorithm 1.1. For example, using the approach of [19], we could treat the projection version of IGA described in <ref> [10, 21] </ref>. At the expense of introducing considerably more notation and some technical details, we could also consider the parallel and momentum term modifications given in [13, 21], as well as algorithms with noisy data along the lines of [18]. <p> For example, using the approach of [19], we could treat the projection version of IGA described in [10, 21]. At the expense of introducing considerably more notation and some technical details, we could also consider the parallel and momentum term modifications given in <ref> [13, 21] </ref>, as well as algorithms with noisy data along the lines of [18]. Although specific stepsize rules are not a subject of this paper, we shall make a few remarks concerning this issue. <p> From (2.7) we can see that decreasing the stepsize will, indeed, yield a better approximate solution (in some sense). As a practical matter, we would suggest using a stepsize rule similar to that proposed in <ref> [21] </ref> with a slight modification consisting of imposing a lower bound on the stepsize. On one hand, this modification will prevent the stepsize from becoming too small and, on the other hand, it may also help to avoid overfitting by computing an approximate rather than an exact solution. <p> As a side result, we now establish convergence of IGA to exact stationary points of (1.1) under a growth condition on the gradients very similar to that used in <ref> [21] </ref>.
Reference: [22] <author> H. White. </author> <title> Some asymptotic results for learning in single hidden-layer feedforward network models. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 84(408) </volume> <pages> 1003-1013, </pages> <year> 1989. </year> <month> 15 </month>
Reference-contexts: Stochastic analysis under conditions similar to (1.2) can be found in <ref> [22, 6] </ref>. Error-stability properties of a very general class of algorithms, which includes IGA, are analyzed in [18]. And in [3] a related least-squares incremental method is considered. The results just cited, though significant, still left a certain gap between theoretical convergence analysis of incremental algorithms and computational practice.
References-found: 22

