URL: http://playfair.Stanford.EDU/reports/chen_s/BasisPursuit.ps.Z
Refering-URL: http://www.cs.wisc.edu/~paulb/research.html
Root-URL: 
Phone: 94305. Telephone: (415) 723-3350.  
Title: Atomic Decomposition by Basis Pursuit  
Author: Scott Shaobing Chen David L. Donoho Michael A. Saunders 
Keyword: Key Words and Phrases. Overcomplete signal representation, De-Noising, Time-Frequency Analysis, Time-Scale Analysis, 1 norm optimization, Matching Pursuit, Wavelets, Wavelet Packets, Cosine Packets, Interior-point methods for linear programming, Total Variation De-Noising, Multi-Scale Edges.  
Address: Stanford, CA  
Note: Acknowledgements. This research was partially supported by NSF DMS-92-09130 and DMI-92-04208, by the NASA Astrophysical Data Program, by ORN grant N00014-90-J1242, and by other sponsors. Contact Information. Email address: donoho@playfair.stanford.edu. Mail address:  
Date: February, 1996  
Affiliation: IBM T.J. Watson Research Center  Dept. of Statistics, Stanford University  Dept. of Operations Research, Stanford University  Department of Statistics, Stanford University,  
Abstract: The Time-Frequency and Time-Scale communities have recently developed a large number of overcomplete waveform dictionaries | stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcom-plete systems is not unique, and several methods for decomposition have been proposed, including the Method of Frames (MOF), Matching Pursuit (MP), and, for special dictionaries, the Best Orthogonal Basis (BOB). Basis Pursuit (BP) is a principle for decomposing a signal into an "optimal" superposition of dictionary elements, where optimal means having the smallest l 1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP&lt; and BOB, including better sparsity, and super-resolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation de-noising, and multi-scale edge de-noising. Basis Pursuit in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.E. Bixby. </author> <title> Commentary: Progress in linear programming. </title> <journal> ORSA Journal on Computing, </journal> <volume> vol. 6, </volume> <pages> pp. 15-22, </pages> <year> 1994. </year>
Reference-contexts: A good overview of the recent rapid progress in this field and the current state of the art is afforded by the article of Lustig, 23 Marsten and Shanno [21] and the accompanying discussions by Bixby <ref> [1] </ref>, Saunders [31], Todd [33], and Vanderbei [34]. Much of the rapid expansion in the size of linear programs solved is due to the "Interior Point revolution" initiated by Karmarkar's proof that a pseudo-polynomial time algorithm could be based on an interior-point method [18].
Reference: [2] <author> Peter Bloomfield and William Steiger. </author> <title> Least Absolute Deviations: Theory, Applications, and Algorithms. </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1983. </year>
Reference-contexts: x , (u; v) ; c , (1; 1) ; A , (; ) ; b , s : Hence, the solution of (3.1) can be obtained by solving an equivalent linear program. (The equivalence of minimum ` 1 optimizations with linear programming has been known since the 1950's; see <ref> [2] </ref>). The connection between Basis Pursuit and linear programming is useful in several ways. 3.1.1 Solutions as Bases In the linear programming problem (3.2), suppose A is an n by m matrix with m &gt; n, and suppose an optimal solution exists.
Reference: [3] <editor> J. Buckheit and D.L. Donoho. WaveLab and reproducible research. In A. Antoniadis, Editor, </editor> <booktitle> Wavelets and Statistics, </booktitle> <address> Springer, </address> <note> to appear, </note> <year> 1995. </year>
Reference-contexts: Gong signal of length 1024 with a cosine packet dictionary at the parameter setting 10 3 ; it takes about 8 hours to perform BP on the signal Gong at the same parameter setting. 6.6 Reproducible Research This paper has been written following the discipline of Reproducible Research described in <ref> [3] </ref>. As a complement to this article, we are releasing the underlying software environment by placing it on internet for access either by anonymous FTP or WWW browsers.
Reference: [4] <author> S. Chen, </author> <title> Basis Pursuit. </title> <type> Ph.D. Thesis, </type> <institution> Department of Statistics, Stanford University, </institution> <year> 1995. </year> <note> http://playfair.stanford.edu/~schen/. 28 </note>
Reference-contexts: FeaTol = 10 1 and PDGapTol = 10 1 would usually suffice for this. * Each barrier iteration involves approximate solution of the central equations (6.4) using the conjugate-gradient method, e. g. at accuracy CGAccuracy = 10 1 . We refer the reader to <ref> [4] </ref> for more detailed discussion of the our implementation. 6.5 Complexity Analysis Table 1 displays the CPU times in seconds spent in running various atomic decomposition techniques in our experiments; all computation was done on a Sun Sparc20 workstation. <p> BP is also slower than MP (which has a quasi-linear complexity, depending on the number of chosen atoms) except on the FM-Cosine signal in Figure 3.2. Several factors influence the running time of Basis Pursuit: 1. Problem Sizes. The complexity goes up "quasi-linearly" as the problem size increases <ref> [4] </ref>. 2. Parameter Settings. The complexity of our primal-dual logarithmic barrier interior-point implementation depends on both the the accuracy of the solution and the accuracy 27 of the conjugate-gradient solver. <p> The choice ffi = 1 helps: it regularizes the central equations to be solved at each barrier iteration. Thus the BPDN implementation seems to converge more quickly than the BP implementation. For example, according to our experiments <ref> [4] </ref>, it takes only 3 minutes to perform BPDN on the noisy Gong signal of length 1024 with a cosine packet dictionary at the parameter setting 10 3 ; it takes about 8 hours to perform BP on the signal Gong at the same parameter setting. 6.6 Reproducible Research This paper <p> As a complement to this article, we are releasing the underlying software environment by placing it on internet for access either by anonymous FTP or WWW browsers. Web Browser: http://playfair.stanford.edu/~schen/Atomizer.html FTP Client: playfair.stanford.edu file: pub/chen_s/Atomizer0600.tar.Z 7 Discussion For reasons of space we refer the reader to <ref> [4] </ref> for a discussion of related work in statistics and elsewhere.
Reference: [5] <author> R.R. Coifman, Y. Meyer. </author> <title> Remarques sur l'analyze de Fourier a Fen^etre. </title> <journal> Comptes Rendus Acad. Sci. Paris (A), </journal> <volume> vol. 312, </volume> <pages> pp. 259-261, </pages> <year> 1991. </year>
Reference-contexts: For fixed ffit, discrete dictionaries can be built from time-frequency lattices, ! k = k! and t ` = `t , and 2 f0; =2g; with t and ! chosen sufficiently fine these are complete. For further discussions see e.g. [8]. Recently, Coifman and Meyer <ref> [5] </ref> developed the wavelet packet and cosine packet dictionaries especially to meet the computational demands of discrete-time signal processing. For 1-d discrete time signals of length n, these dictionaries each contain about n log 2 (n) waveforms.
Reference: [6] <author> R.R. Coifman and M.V. Wickerhauser. </author> <title> Entropy-based algorithms for best-basis selection. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 38, </volume> <pages> pp. 713-718, </pages> <year> 1992. </year>
Reference-contexts: These range from general approaches, like the Method of Frames [8], and the method of Matching Pursuit [23], to clever schemes derived for specialized dictionaries, like 2 the method of Best Orthogonal Basis <ref> [6] </ref>. These methods are described briefly in Section 2.3. In our view, these methods have both advantages and shortcomings. The principal emphasis of the proposers of these methods is in achieving sufficient computational speed. <p> A simple block-diagram helps us visualize the atoms appearing in the decomposition. This diagram, adapted from Coifman and Wickerhauser <ref> [6] </ref>, associates with each cosine packet or wavelet packet a rectangle in the time-frequency phase plane. The association is illustrated in Figure 2.1 for a certain wavelet packet. <p> Certain special subcollections of the elements in these dictionaries amount to orthogonal bases; one gets in this way a wide range of orthonormal bases (in fact 2 n such orthogonal bases for signals of length n). Coifman and Wickerhauser <ref> [6] </ref> have proposed a method of adaptively picking from among these many bases a single orthogonal basis that is the "best basis". <p> is a sort of build-up approach, while BP-Simplex is a sort of swap-down approach. 3.3.2 Best Orthogonal Basis To make BP and BOB most comparable, suppose that they are both working with a cosine packet dictionary, and note that the ` 1 -norm of coefficients is what Coifman and Wickerhauser <ref> [6] </ref> call an "additive measure of information". So suppose we apply the Coifman-Wickerhauser Best Basis algorithm with entropy E = ` 1 . <p> We employ a conjugate-gradient solver for the generalized inverse in the MOF solution (2.4); the resulting algorithm for MOF has a complexity order O (n log (n)). We implement Coifman and Wickerhauser's BOB algorithm <ref> [6] </ref>, which also has a complexity of order O (n log (n)). We observe that BP is typically slower than MOF and BOB. BP is also slower than MP (which has a quasi-linear complexity, depending on the number of chosen atoms) except on the FM-Cosine signal in Figure 3.2.
Reference: [7] <author> G.B. Dantzig. </author> <title> Linear Programming and Extensions. </title> <publisher> Princeton University Press, </publisher> <year> 1963. </year>
Reference-contexts: In contrast, Basis Pursuit requires the solution of a convex, nonquadratic optimization problem, which involves considerably more effort and sophistication. 3.1 Linear Programming To explain the last comment, and the name Basis Pursuit, we develop a connection with linear programming (LP). The linear program in so-called standard form <ref> [7, 16] </ref> is a constrained optimization problem defined in terms of a variable x 2 R m by min c T x subject to Ax = b; x 0 ; (3.2) where c T x is the objective function, Ax = b is a collection of equality constraints, and x 0
Reference: [8] <author> I. Daubechies. </author> <title> Time-frequency localization operators: a geometric phase space approach. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 34, </volume> <pages> pp. 605-612, </pages> <year> 1988. </year>
Reference-contexts: It should be possible to obtain a representation in order O (n) or O (n log (n)) time. 1.2 Finding a Representation Several methods have been proposed for obtaining signal representations in overcomplete dictionaries. These range from general approaches, like the Method of Frames <ref> [8] </ref>, and the method of Matching Pursuit [23], to clever schemes derived for specialized dictionaries, like 2 the method of Best Orthogonal Basis [6]. These methods are described briefly in Section 2.3. In our view, these methods have both advantages and shortcomings. <p> For fixed ffit, discrete dictionaries can be built from time-frequency lattices, ! k = k! and t ` = `t , and 2 f0; =2g; with t and ! chosen sufficiently fine these are complete. For further discussions see e.g. <ref> [8] </ref>. Recently, Coifman and Meyer [5] developed the wavelet packet and cosine packet dictionaries especially to meet the computational demands of discrete-time signal processing. For 1-d discrete time signals of length n, these dictionaries each contain about n log 2 (n) waveforms. <p> In Panel 2.2d we compare the sorted coefficients of the overcomplete representation (synthesis) with the analysis coefficients. 2.3 Existing Decomposition Methods There are several currently popular approaches to obtaining solutions to (2.2). 2.3.1 Frames The Method of Frames (MOF) <ref> [8] </ref> picks out, among all solutions of (2.2), one whose coefficients have minimum l 2 norm: min kffk 2 subject to ff = s: (2.3) The solution of this problem is unique; label it ff y .
Reference: [9] <author> I. Daubechies. </author> <title> Ten Lectures on Wavelets. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: It is called stationary since the whole dictionary is invariant under circulant shift. A variety of other wavelet bases are possible. The most important variations are smooth wavelet bases, using splines or using wavelets defined recursively from two-scale filtering relations <ref> [9] </ref>. Although the rules of construction are more complicated (boundary conditions [25], orthogonality versus bi-orthogonality [9], etc.), these have the same indexing structure as the standard Haar dictionary. In this paper, we use Symmlet-8 smooth wavelets, i.e., Daubechies Nearly Symmetric wavelets with eight vanishing moments; see [9] for examples. 2.1.4 Time-Frequency <p> A variety of other wavelet bases are possible. The most important variations are smooth wavelet bases, using splines or using wavelets defined recursively from two-scale filtering relations <ref> [9] </ref>. Although the rules of construction are more complicated (boundary conditions [25], orthogonality versus bi-orthogonality [9], etc.), these have the same indexing structure as the standard Haar dictionary. In this paper, we use Symmlet-8 smooth wavelets, i.e., Daubechies Nearly Symmetric wavelets with eight vanishing moments; see [9] for examples. 2.1.4 Time-Frequency Dictionaries Much recent activity in the wavelet communities has focused on the study of time-frequency <p> from two-scale filtering relations <ref> [9] </ref>. Although the rules of construction are more complicated (boundary conditions [25], orthogonality versus bi-orthogonality [9], etc.), these have the same indexing structure as the standard Haar dictionary. In this paper, we use Symmlet-8 smooth wavelets, i.e., Daubechies Nearly Symmetric wavelets with eight vanishing moments; see [9] for examples. 2.1.4 Time-Frequency Dictionaries Much recent activity in the wavelet communities has focused on the study of time-frequency phenomena.
Reference: [10] <author> R.A. DeVore and V.N. Temlyakov. </author> <title> Some remarks on greedy algorithms. </title> <type> Manuscript, </type> <year> 1995. </year>
Reference-contexts: DeVore and Temlyakov's Example. Vladimir Temlyakov, in a talk at the IEEE Conference on Information Theory and Statistics, October 1994, described an example in which the straightforward greedy algorithm is not sparsity-preserving. In our adaptation of this example, based on Temlyakov's joint work with R.A. DeVore <ref> [10] </ref>, one constructs a dictio 9 nary having n + 1 atoms. The first n are the Dirac basis; the final atom involves a linear combination of the first n with decaying weights.
Reference: [11] <author> D.L. Donoho. </author> <title> De-Noising by soft thresholding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 41, </volume> <pages> pp. 613-627, </pages> <year> 1995. </year>
Reference-contexts: This can be motivated as follows. In the case of a dictionary that is an orthonormal basis, a number of papers <ref> [11, 14] </ref> have carefully studied an approach to de-noising by so-called "soft-thresholding in an orthonormal basis".
Reference: [12] <author> D.L. Donoho and I.M. Johnstone. </author> <title> Ideal de-noising in an orthonormal basis chosen from a library of bases. </title> <journal> C. R. Acad. Sci. Paris, Ser. </journal> <volume> I, vol. 319, </volume> <pages> pp. 1317-1322, </pages> <year> 1994. </year>
Reference-contexts: Matching Pursuit De-Noising (MPDN) runs Matching Pursuit until the coefficient associated with the selected atom gets below the threshold p 2 log (p) . The Best Orthogonal Basis De-Noising (BOBDN) is a thresholding scheme in the best orthogonal basis chosen by the BOB algorithm with a special entropy <ref> [12] </ref>. 5.3.1 Gong cosine packet dictionary. Panel a) displays the noiseless signal and panel b) displays a noisy version. Panels c)-f) display de-noising results for MOF, BOB, MP, and BP, respectively. BP outperforms the other methods visually. 5.3.2 TwinSine in the noisy case.
Reference: [13] <author> D.L. Donoho and I.M. Johnstone. </author> <title> Empirical atomic decomposition. </title> <type> Manuscript, </type> <year> 1995. </year>
Reference-contexts: The scheme we have suggested here to be applied in overcomplete as well as orthogonal settings therefore includes soft-thresholding in ortho-bases as a special case. Formal arguments similar to those in <ref> [13] </ref> can be used to give a proof that mean-squared error properties of the resulting procedure are near-optimal under certain conditions. 20 5.3 Examples We present two examples of BPDN in action with time-frequency dictionaries. We compare BPDN with three other de-noising methods adapted from MOF, MP and BOB.
Reference: [14] <author> D.L. Donoho, I.M. Johnstone, G. Kerkyacharian, and D. </author> <title> Picard. Wavelet shrinkage: </title> <journal> asymptopia? Journal of the Royal Statistical Society, Series B, </journal> <volume> vol. 57, </volume> <pages> pp. 301-369, </pages> <year> 1995. </year>
Reference-contexts: This can be motivated as follows. In the case of a dictionary that is an orthonormal basis, a number of papers <ref> [11, 14] </ref> have carefully studied an approach to de-noising by so-called "soft-thresholding in an orthonormal basis".
Reference: [15] <author> P.E. Gill, W. Murray, D.B. Ponceleon and M. Saunders. </author> <title> Solving reduced KKT systems in barrier methods for linear and quadratic programming. </title> <type> Technical Report SOL 91-7, </type> <institution> Stanford University, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: We cannot summarize all these ideas here; many of them are mentioned in [21] and others are covered in the references of that article. Our approach is based on a primal-dual log-barrier algorithm. In order to regularize standard LP, Gill et al. <ref> [15] </ref> proposed solving the following perturbed LP: min c T x + 2 kflxk 2 + 1 kpk 2 subject to Ax + ffip = b; x 0 ; (6.3) where fl and ffi are normally small (e.g. 10 4 ) regularization parameters. (We comment that such a perturbed LP with <p> Until the following three conditions are satisfied: (a) Primal Infeasibility = krk 2 &lt; FeaTol. (b) Dual Infeasibility: = ktk 2 1+kyk 2 &lt; FeaTol. (c) Duality Gap = z T x 1+kzk 2 kxk 2 &lt; PDGapTol: 25 For fuller discussions of this and related algorithms, again see <ref> [15] </ref> or references there. While in principle we could have based our approach on other interior-point schemes, the primal-dual approach naturally incorporates several features we found useful. First, the iterates x; y; z do not have to be feasible. <p> carried out for whatever length of time we are willing to invest in it, makes a useful improvement over the Method of Frames. 6.4 Routine Settings For BP Our strategy for routine signal processing by BP is as follows: * We employ the "primal-dual logarithmic barrier method" for perturbed LP <ref> [15] </ref>. * We suppose fast implicit algorithms for Aa and A T s. * We only aim to reach an approximate optimum.
Reference: [16] <author> P.E. Gill, W. Murray and M.H. Wright. </author> <title> Numerical Linear Algebra and Optimization. </title> <publisher> Addison Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1991. </year>
Reference-contexts: In contrast, Basis Pursuit requires the solution of a convex, nonquadratic optimization problem, which involves considerably more effort and sophistication. 3.1 Linear Programming To explain the last comment, and the name Basis Pursuit, we develop a connection with linear programming (LP). The linear program in so-called standard form <ref> [7, 16] </ref> is a constrained optimization problem defined in terms of a variable x 2 R m by min c T x subject to Ax = b; x 0 ; (3.2) where c T x is the objective function, Ax = b is a collection of equality constraints, and x 0 <p> There always exists a swap that improves or maintains the objective value, except at the optimal solution. Moreover, LP researchers have shown how one can select terms to swap in such a way as to guarantee convergence to an optimal solution (anti-cycling rules) <ref> [16] </ref>. Hence the simplex algorithm is explicitly a process of "Basis Pursuit": iterative improvement of a basis until no improvement is possible, at which point the solution is achieved. Translating this LP algorithm into BP terminology, one starts from any linearly independent collection of n atoms from the dictionary. <p> Of course, in general solving systems of equations is not rapid: a general n by n system Bx = c takes order O (n 3 ) time to solve by standard elimination methods or by modern stable factorization schemes <ref> [17, 16] </ref>. In order for practical algorithms to be based on the interior-point heuristic, it is necessary to be able to solve the systems of equations much more rapidly than one could solve general systems.
Reference: [17] <author> Gene Golub and Charles Van Loan, </author> <title> Matrix Computations, 2nd edition. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: Of course, in general solving systems of equations is not rapid: a general n by n system Bx = c takes order O (n 3 ) time to solve by standard elimination methods or by modern stable factorization schemes <ref> [17, 16] </ref>. In order for practical algorithms to be based on the interior-point heuristic, it is necessary to be able to solve the systems of equations much more rapidly than one could solve general systems.
Reference: [18] <author> N. Karmarkar. </author> <title> A new polynomial-time algorithm for linear programming. </title> <journal> Combina-torica, </journal> <volume> vol. 4, </volume> <year> 1984. </year>
Reference-contexts: Much of the rapid expansion in the size of linear programs solved is due to the "Interior Point revolution" initiated by Karmarkar's proof that a pseudo-polynomial time algorithm could be based on an interior-point method <ref> [18] </ref>. Since then a very wide array of interior-point algorithms have been proposed and considerable practical [21] and theoretical [27] understanding is now available.
Reference: [19] <author> M. Kojima, S. Mizuno, and A. Yoshise. </author> <title> A primal-dual interior point algorithm for linear programming. In Progress in Mathematical Programming: Interior Point and Related Methods, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference: [20] <author> Y. Li and F. Santosa. </author> <title> An affine scaling algorithm for minimizing total variation in image enhancement. </title> <type> Manuscript, </type> <year> 1994. </year>
Reference-contexts: More specifically, they propose the optimization problem min 1 ky gk 2 where T V (g) is a discrete measure of the total variation of g. A solution of this problem is the de-noised object. Li and Santosa <ref> [20] </ref> have developed an alternative algorithm for this problem based on interior-point methods for convex optimization. For the 1-dimensional case (signals rather than images) it is possible to implement what amounts to total variation de-noising by applying BPDN with a Heaviside dictionary.
Reference: [21] <author> I.J. Lustig, R.E. Marsten and D.F. Shanno. </author> <title> Interior point methods for linear programming: computational state of the art. </title> <journal> ORSA Journal on Computing, </journal> <volume> vol. 6, </volume> <pages> pp. 1-14, </pages> <year> 1994. </year>
Reference-contexts: A good overview of the recent rapid progress in this field and the current state of the art is afforded by the article of Lustig, 23 Marsten and Shanno <ref> [21] </ref> and the accompanying discussions by Bixby [1], Saunders [31], Todd [33], and Vanderbei [34]. <p> Since then a very wide array of interior-point algorithms have been proposed and considerable practical <ref> [21] </ref> and theoretical [27] understanding is now available. <p> We cannot summarize all these ideas here; many of them are mentioned in <ref> [21] </ref> and others are covered in the references of that article. Our approach is based on a primal-dual log-barrier algorithm. <p> solution c T x fl = b T y fl , and the duality gap c T x b T y x T z quantifies the distance from this ideal. 6.3 Implementation Heuristics The primal-dual log barrier algorithm we just described works in a fashion similar to other interior-point methods <ref> [21] </ref>. It starts from an initial feasible (or nearly feasible) solution located at or near the "center" of the feasible region, and iteratively improves the current solution until the iterates (x; y; z) achieve the desired accuracy. <p> In the current state of the art of linear programming [31], one attempts to do this by exploiting sparsity of the underlying matrix A. However, the optimization problems we are interested in have a key difference from the successful large-scale applications outlined in <ref> [21] </ref>. The matrix A we deal with is not at all sparse; it is generally completely dense. For example, if A is generated from a Fourier dictionary, most of the elements of A will be of the same order of magnitude.
Reference: [22] <author> S. Mallat and W.L. Hwang. </author> <title> Singularity detection and processing with wavelets. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 38, </volume> <pages> pp. 617-643, </pages> <year> 1992. </year> <month> 29 </month>
Reference-contexts: There is a surprisingly close agreement of the BP representation in a stationary wavelet dictionary with ideas about signal representation associated with the "Multi-Scale Edges" ideas of Mallat and Hwang <ref> [22] </ref>. The Multi-Scale Edge method analyzes the continuous wavelet transform (CWT) at scale 2 j and identifies the maxima of this transform. Then it selects maxima that are "important" by thresholding based on amplitude. These "important" maxima identify important features of the signal.
Reference: [23] <author> S. Mallat and Z. Zhang. </author> <title> Matching Pursuit in a time-frequency dictionary. </title> <journal> IEEE Trans--actions on Signal Processing, </journal> <volume> vol. 41, </volume> <pages> pp. 3397-3415, </pages> <year> 1993. </year>
Reference-contexts: These range from general approaches, like the Method of Frames [8], and the method of Matching Pursuit <ref> [23] </ref>, to clever schemes derived for specialized dictionaries, like 2 the method of Best Orthogonal Basis [6]. These methods are described briefly in Section 2.3. In our view, these methods have both advantages and shortcomings. The principal emphasis of the proposers of these methods is in achieving sufficient computational speed. <p> We use terminology introduced by Mallat and Zhang <ref> [23] </ref>. A dictionary is a collection of parameterized waveforms D = ( fl : fl 2 ). The waveforms fl are discrete-time signals of length n called atoms. <p> The result is the synthesis from coefficients with a broad oscillatory appearance, consisting not of two but of many frequencies, and giving no visual clue that the object may be synthesized from two frequencies alone. 2.3.2 Matching Pursuit Mallat and Zhang <ref> [23] </ref> have discussed a general method for approximate decomposition (1.2) that addresses the sparsity issue directly. Starting from an initial approximation s (0) = 0 8 and residual R (0) = s, it builds up a sequence of sparse approximations stepwise.
Reference: [24] <author> N. Megiddo. </author> <title> On finding primal- and dual- optimal bases, </title> <journal> ORSA Journal on Computing, </journal> <volume> vol. 3, </volume> <pages> pp. 63-65, </pages> <year> 1991. </year>
Reference: [25] <author> Y. Meyer. </author> <title> Ondelettes sur l'intervalle. </title> <journal> Revista Mat. Ibero-Americana, </journal> <volume> vol. 7, </volume> <pages> pp. 115-134, </pages> <year> 1991. </year>
Reference-contexts: A variety of other wavelet bases are possible. The most important variations are smooth wavelet bases, using splines or using wavelets defined recursively from two-scale filtering relations [9]. Although the rules of construction are more complicated (boundary conditions <ref> [25] </ref>, orthogonality versus bi-orthogonality [9], etc.), these have the same indexing structure as the standard Haar dictionary.
Reference: [26] <author> Y. Meyer. </author> <title> Wavelets: Algorithms and Applications. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993. </year>
Reference-contexts: So in a stationary wavelet dictionary, the global optimization principle BP yields results that are close to certain heuristic methods. An important contrast: Meyer has a counterexample to multi-scale edge approaches, showing that the Mallat-Hwang approach may fail in certain cases <ref> [26] </ref>; but there can be no such counterexamples to BP. 4.2 Dictionary Mergers An important methodological tool is the ability to combine dictionaries to make bigger, more expressive dictionaries. We mention here two possibilities. Examples of such decompositions are given in Section 5 below. Jump+Sine.
Reference: [27] <author> Y. Nesterov and A. Nemirovskii. </author> <title> Interior-Point Polynomial Algorithms in Convex programming. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1994. </year>
Reference-contexts: Since then a very wide array of interior-point algorithms have been proposed and considerable practical [21] and theoretical <ref> [27] </ref> understanding is now available. In this section we describe our algorithm and our experience with it. 6.1 Duality Theory We consider the linear program in the standard form min c T x subject to Ax = b; x 0: (6.1) This is often called the primal linear program.
Reference: [28] <author> L.J. Rudin, S. Osher and E. Fatemi. </author> <title> Nonlinear total-variation-based noise removal algorithms. </title> <journal> Physica D, </journal> <volume> vol. 60, </volume> <pages> pp. 259-268, </pages> <year> 1992. </year>
Reference-contexts: As in the noiseless case, MP gives a reconstruction that goes wrong at step 1 it selects the average of the two frequencies in the TwinSine signal. BP correctly resolves the non-negative doublet structure. 21 5.4 Total Variation De-Noising Recently, Rudin, Osher and Fatemi <ref> [28] </ref> have called attention to the possibility of de-noising images using total-variation penalized least-squares. More specifically, they propose the optimization problem min 1 ky gk 2 where T V (g) is a discrete measure of the total variation of g. A solution of this problem is the de-noised object.
Reference: [29] <author> Y.C. Pati, R. Rezaiifar and P.S. Krishnaprasad. </author> <title> Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition. </title> <editor> In A. Singh, editor, </editor> <booktitle> Proceedings of 27th Asilomar Conference on Signals, Systems and Computers, </booktitle> <year> 1993. </year>
Reference-contexts: Shaobing Chen's Example. The DeVore-Temlyakov example applies to the original MP algorithm as announced by Mallat and Zhang in 1992. A later refinement (see also Pati <ref> [29] </ref>) involves an extra step of orthogonalization. One takes all m terms that have entered at stage m and solves the least squares problem min ks i=1 for coefficients (a (m) i ). <p> Then one forms the residual R [m] = s P m (m) i fl i , which will be orthogonal to all terms currently in the model. This method is called Orthogonal Matching Pursuit (OMP) by Pati <ref> [29] </ref>. The DeVore-Temlyakov example does not apply to OMP, but Shaobing Chen found in Summer 1993 an example of similar flavor that does. In this example, a special signal and dictionary are constructed, with the following flavor.
Reference: [30] <author> Shie Qian and Dapang Chen. </author> <title> Signal representation using adaptive normalized Gaussian functions. </title> <booktitle> Signal Processing, </booktitle> <volume> vol. 36, </volume> <pages> pp. 1-11, </pages> <year> 1994. </year>
Reference-contexts: After m steps, one has a representation of the form (1.2), with residual R = R (m) . A similar algorithm was proposed for Gabor dictionaries by S. Qian and D. Chen <ref> [30] </ref>. An intrinsic feature of the algorithm is that when stopped after a few steps, it yields an approximation using only a few atoms. When the dictionary is orthogonal, the method works perfectly.
Reference: [31] <author> M.A. Saunders. </author> <title> Commentary: Major Cholesky would feel proud. </title> <journal> ORSA Journal on Computing, </journal> <volume> vol. 6, </volume> <pages> pp. 23-27, </pages> <year> 1994. </year>
Reference-contexts: A good overview of the recent rapid progress in this field and the current state of the art is afforded by the article of Lustig, 23 Marsten and Shanno [21] and the accompanying discussions by Bixby [1], Saunders <ref> [31] </ref>, Todd [33], and Vanderbei [34]. Much of the rapid expansion in the size of linear programs solved is due to the "Interior Point revolution" initiated by Karmarkar's proof that a pseudo-polynomial time algorithm could be based on an interior-point method [18]. <p> In order for practical algorithms to be based on the interior-point heuristic, it is necessary to be able to solve the systems of equations much more rapidly than one could solve general systems. In the current state of the art of linear programming <ref> [31] </ref>, one attempts to do this by exploiting sparsity of the underlying matrix A. However, the optimization problems we are interested in have a key difference from the successful large-scale applications outlined in [21]. The matrix A we deal with is not at all sparse; it is generally completely dense.
Reference: [32] <author> E.P. Simoncelli, W.T. Freeman, </author> <title> E.H. Adelson, and D.J. Heeger. Shiftable multiscale transforms. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 38, </volume> <pages> pp. 587-607, </pages> <year> 1992. </year>
Reference-contexts: The wavelets at this scale are all circulant shifts of each other, the shift being n=2 j samples. Some authors <ref> [32] </ref> have suggested that this scheme can be less than satisfactory, essentially because the shift between adjacent wavelets is too large.
Reference: [33] <author> M.J. Todd. </author> <title> Commentary: Theory and practice for interior point methods. </title> <journal> ORSA Journal on Computing, </journal> <volume> vol. 6, </volume> <pages> pp. 28-31, </pages> <year> 1994. </year>
Reference-contexts: A good overview of the recent rapid progress in this field and the current state of the art is afforded by the article of Lustig, 23 Marsten and Shanno [21] and the accompanying discussions by Bixby [1], Saunders [31], Todd <ref> [33] </ref>, and Vanderbei [34]. Much of the rapid expansion in the size of linear programs solved is due to the "Interior Point revolution" initiated by Karmarkar's proof that a pseudo-polynomial time algorithm could be based on an interior-point method [18].
Reference: [34] <author> R.J. Vanderbei. </author> <title> Commentary: Interior point methods: algorithms and formulations. </title> <journal> ORSA Journal on Computing, </journal> <volume> vol. 6, </volume> <pages> pp. 32-34, </pages> <year> 1994. </year> <month> 30 </month>
Reference-contexts: A good overview of the recent rapid progress in this field and the current state of the art is afforded by the article of Lustig, 23 Marsten and Shanno [21] and the accompanying discussions by Bixby [1], Saunders [31], Todd [33], and Vanderbei <ref> [34] </ref>. Much of the rapid expansion in the size of linear programs solved is due to the "Interior Point revolution" initiated by Karmarkar's proof that a pseudo-polynomial time algorithm could be based on an interior-point method [18].
References-found: 34

