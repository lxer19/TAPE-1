URL: http://www.cs.ucl.ac.uk/staff/G.Koufakis/eusipco98.ps.gz
Refering-URL: http://www.cs.ucl.ac.uk/staff/G.Koufakis/main.html
Root-URL: http://www.cs.ucl.ac.uk
Email: e-mail: g.koufakis@cs.ucl.ac.uk, b.buxton@cs.ucl.ac.uk  
Phone: Tel: +44 171 380 7214 fax: +44 171 387 1397  
Title: LINEAR COMBINATION OF FACE VIEWS FOR LOW BIT RATE FACE VIDEO COMPRESSION  
Author: Ioannis Koufakis Bernard Buxton 
Address: Gower Street, London WC1E 6BT, UNITED KINGDOM  
Affiliation: Dept. of Computer Science, University College London,  
Abstract: We present a technique for very low bit rate encoding of faces for videoconferencing applications over the Internet or mobile communication networks. The proposed scheme represents each new face view as a linear combination of three basis face views of the same person. The background area of the target face view is also encoded by the same method using the same basis views as used for the face area. Changes in the eyes and mouth regions are encoded using principal components analysis with three training sets of eyes and mouth images of the same person. The scheme results in a very compact representation of the face video data, and good quality images are reconstructed with an estimated bit rate from 1422 to 1620 bits/frame. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Liou. </author> <title> Overview of the px64kbits/s Video Coding Standards. </title> <journal> Communications of the ACM, </journal> <volume> 34(4) </volume> <pages> 59-63, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Hence, the desired compression ratios are from 600:1 to 160:1. Achieving these compression ratios is a very difficult task. In conventional DCT block-based compression schemes, such as H.261 <ref> [1] </ref>, the performance deteriorates significantly at bit rates below 64 Kbits/sec and the decoded video stream suffers from significant coding artifacts. Most current implementations eliminate these effects by lowering the spatial (frame size) and temporal (frame rate) resolution.
Reference: [2] <author> D.E. Pearson. </author> <title> Developments in Model-Based Video Coding. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 83(6) </volume> <pages> 893-905, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: In order to achieve higher compression ratios, it seems that knowledge of the image content, for example that the video data contains faces, needs to be used. Such model-based image coding techniques have recently received a lot of attention for the encoding of face images for videoconferencing applications <ref> [2, 3] </ref>. Although these methods can achieve high compression ratios, they require complex 3D models of the face or explicit 3D information of the scene content to be used. The proposed encoding scheme is based on the mod-elling of objects using linear combination of images [4].
Reference: [3] <author> K. Aizawa and T.S. Huang. </author> <title> Model-Based Image Coding: Advanced Video Coding Techniques for Very Low Bit Rate Applications. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 83(2) </volume> <pages> 259-271, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: In order to achieve higher compression ratios, it seems that knowledge of the image content, for example that the video data contains faces, needs to be used. Such model-based image coding techniques have recently received a lot of attention for the encoding of face images for videoconferencing applications <ref> [2, 3] </ref>. Although these methods can achieve high compression ratios, they require complex 3D models of the face or explicit 3D information of the scene content to be used. The proposed encoding scheme is based on the mod-elling of objects using linear combination of images [4].
Reference: [4] <author> S. Ullman and R. Basri. </author> <title> Recognition by Linear Combination of Models. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10) </volume> <pages> 992-1006, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Although these methods can achieve high compression ratios, they require complex 3D models of the face or explicit 3D information of the scene content to be used. The proposed encoding scheme is based on the mod-elling of objects using linear combination of images <ref> [4] </ref>. Each novel face view of the person who uses the video-conferencing system is expressed as a linear combination of three basis face views of the same person. <p> regions can not be encoded by the linear combination method, therefore, we decide to use principal components analysis for encoding these facial features. 2 LINEAR COMBINATION OF FACE VIEWS 2.1 Geometry The modelling of objects using linear combination of images was initially proposed by Ullman and Basri for object recognition <ref> [4] </ref>. They found that all the possible images of an object can, to a good approximation, often be expressed as a linear combination of a small number of "basis" 2D views.
Reference: [5] <author> A. Shashua. </author> <title> Photometry and Geometry in 3D Visual Recognition. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: For objects with smooth boundaries, such as human faces, undergoing rigid 3D transformations and scaling under orthographic or weak perspective projection <ref> [5] </ref>, as in videoconferencing applications, three views of the object are sufficient to synthe-sise any other novel view of the same object.
Reference: [6] <author> G.D. Hager and M. Toyama. Xvision: </author> <title> A General Purpose Substrate for Real-Time Vision Applications. To appear in Computer Vision and Image Understanding. </title>
Reference-contexts: In our experiments, the control points were located manually. However, various face detection and feature tracking systems have been developed by other researchers and could easily be combined with the proposed encoding scheme <ref> [6] </ref>. 2.2 Texture Rendering The texture of the new face view is reconstructed by the texture of the three basis views using a weighted interpolation technique.
Reference: [7] <author> A. Goshtasby. </author> <title> Piecewise Linear Mapping Functions for Image Registration. </title> <journal> Pattern Recognition, </journal> <volume> 19(6) </volume> <pages> 459-466, </pages> <year> 1986. </year>
Reference-contexts: In particular, for each pixel in the novel face view, the corresponding pixels in the three basis views are found by using the "piecewise linear mapping" technique <ref> [7] </ref>. The control points divide the face in triangular regions. In our experiments, the triangulation of the face has been done manually for the three basis views and remains the same for all novel face views.
Reference: [8] <author> S. Avidan and A. Shashua. </author> <title> Novel View Synthesis in Tensor Space. </title> <booktitle> Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 1034-1040, </pages> <year> 1997. </year>
Reference-contexts: Furthermore, the texture reconstruction method described above does not require dense correspondence between the novel face view and the basis views, in contrast to a similar approach that uses the trilinear tensor in face view synthesis <ref> [8] </ref>. 3 EXPERIMENTS ON FACE RECONSTRU CTION In our experiments, we selected the three basis face views to be one frontal view, one oriented to the left and one to the right with almost 45 ffi orientation change around the vertical axis.
Reference: [9] <author> D. Shah, S. Marshall, and W.J. Welsh. </author> <title> Principal Components Analysis of Mouth Sequences. </title> <booktitle> Proceedings of International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> 5 </volume> <pages> 373-376, </pages> <year> 1993. </year>
Reference-contexts: The linear combination technique fails to encode the eyes when the person talks, opens/closes their eyes or changes facial expression, since such changes are not included in the manifold spanned by the basis face views. However, we found that principal component analysis (PCA) techniques <ref> [9] </ref> can successfully encode the eyes and mouth of the face using a small number (around 10) of eigenvectors. We, therefore, combined both techniques and found by experiments that we can encode the eyes and mouth in each frame of colour video data with good quality using only 1080 bits.
Reference: [10] <author> S. Ullman. </author> <title> High-Level Vision: Object Recognition and Visual Cognition, chapter 5. Recognition by the Combination of Views, </title> <address> pages 95-157. </address> <publisher> MIT Press, </publisher> <year> 1996. </year> <month> 4 </month>
Reference-contexts: The quality of the reconstructed face images can be further improved by eliminating self-occlusion effects by hidden surface removal <ref> [10] </ref>.
References-found: 10

