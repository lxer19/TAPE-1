URL: http://www.cs.ucsd.edu/users/berman/cse260/chapter.ps
Refering-URL: http://www.cs.ucsd.edu/users/berman/cse260/index.html
Root-URL: http://www.cs.ucsd.edu
Title: High-Performance Schedulers  
Author: Francine Berman 
Date: January 26, 1998  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> J. Arabe, A. Beguelin, B. Lowekamp, E. Seligman, M. Starkey, and P. Stephan. Dome: </author> <title> Parallel programming in a heterogeneous multi-user environment. </title> <type> Technical Report TR CMU-CS-95-137, </type> <institution> Carnegie Mellon University, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: Dataflow-style program graphs are a common representation for grid programs. Dome <ref> [1] </ref> and SPP (X) [2] provide a language abstraction for the program which is compiled into a low-level program dependency graph representation. MARS [20] assumes programs to be phased (represented by a sequence of program stages or phases), and builds a program dependency graph as part of the scheduling process. <p> SPP (X) base serial language compositional determination of skeleton perf. [2] X and performance model performance models can structured based on skeletons model for be derived coordination associated with candidate schedules automatically language program with minimal from program structure execution time structure Dome SPMD C++ program rebalanced globally-controlled initial bench <ref> [1] </ref> PVM programs based on past or locally-controlled mark data performance, after load balancing based on short some number of initial phase Dome operations with uniform data distribution 0.4. Current Efforts 13 a set of possible image-processing algorithms. <p> SPP (X) [2] derives "skeleton" performance models from programs developed using a Structured Coordination Language. Similarly, MARS [20] uses the program dependency graph built during an iterative execution process and parameterized by dynamic information as its performance model for the next iteration. Dome <ref> [1] </ref> uses the last program iteration as a benchmark for its SPMD programs, and as a predictor of future performance. IOS [7] associates a set of algorithms with each fine-grained task in the program graph, and evaluates pre-stored off-line mappings for this graph indexed by dynamic information. <p> VDCE [45] uses a list scheduling algorithm to match resources with application tasks. The performance criteria is minimal execution time. Dome <ref> [1] </ref> focuses on load-balancing as a scheduling policy for its SPMD PVM programs with the performance goal of minimal execution time. The load-balancing policy used by Dome can be globally-controlled or locally-controlled and after a short initial benchmark, uses dynamic capacity information to re-balance at Dome-specified or programmer-specified intervals. <p> the MARS project [20] with respect to the retention of performance information for the purpose of rescheduling, in the AppLeS project [4] in terms of the use of dynamic system information via the Network Weather Service, and in terms of the load-balancing approach performed as part of the Dome system <ref> [1] </ref>. Using MetaInformation to Improve Performance A number of scheduling projects use information from various sources as well as meta-information in the form of an assessment of the quality of the information given. This meta-information can be used in various ways. <p> However, some researchers are obtaining useful information for scheduling from programming languages and abstractions. Efforts to develop languages that incorporate information about task decomposition, data decomposition and location, resource requirements, etc. assist in automating the scheduling process. Projects such as SPP (X) [2] and Dome <ref> [1] </ref> provide a promising approach to obtaining performance information relevant to computational grids from high-level language abstractions. 0.7.2 Challenges While current efforts to develop high-performance schedulers promise improved application performance, there are still a number of challenges to be addressed. Portability vs.
Reference: [2] <author> P. Au, J. Darlington, M. Ghanem, Y. Guo, H. To, and J. Yang. </author> <title> Coordinating heterogeneous parallel computation. </title> <booktitle> Proceedings of the 1996 Euro-Par Conference. </booktitle>
Reference-contexts: One approach to developing grid models is to compose models from constituent components which reflect application performance activities. This approach is being taken by a number of researchers <ref> [37, 44, 2, 50] </ref>, etc. <p> Dataflow-style program graphs are a common representation for grid programs. Dome [1] and SPP (X) <ref> [2] </ref> provide a language abstraction for the program which is compiled into a low-level program dependency graph representation. MARS [20] assumes programs to be phased (represented by a sequence of program stages or phases), and builds a program dependency graph as part of the scheduling process. <p> automatic target as a dependency graph mappings indexed by uses dynamic recognition of subtasks, each of dynamic params. used parameters applications which can be assigned to determine mapping to index off-line one of several for current mappings possible algs. iteration SPP (X) base serial language compositional determination of skeleton perf. <ref> [2] </ref> X and performance model performance models can structured based on skeletons model for be derived coordination associated with candidate schedules automatically language program with minimal from program structure execution time structure Dome SPMD C++ program rebalanced globally-controlled initial bench [1] PVM programs based on past or locally-controlled mark data performance, <p> Approaches differ in terms of who supplies the performance model (the system, the programmer, some combination), its form, and its parameterization. We describe some of the general approaches in the following paragraphs. On one end of the spectrum are scheduler-derived performance models. SPP (X) <ref> [2] </ref> derives "skeleton" performance models from programs developed using a Structured Coordination Language. Similarly, MARS [20] uses the program dependency graph built during an iterative execution process and parameterized by dynamic information as its performance model for the next iteration. <p> However, some researchers are obtaining useful information for scheduling from programming languages and abstractions. Efforts to develop languages that incorporate information about task decomposition, data decomposition and location, resource requirements, etc. assist in automating the scheduling process. Projects such as SPP (X) <ref> [2] </ref> and Dome [1] provide a promising approach to obtaining performance information relevant to computational grids from high-level language abstractions. 0.7.2 Challenges While current efforts to develop high-performance schedulers promise improved application performance, there are still a number of challenges to be addressed. Portability vs.
Reference: [3] <editor> General Chair B. Miller. </editor> <booktitle> Proceedings of the acm/onr workshops on parallel and distributed debugging. </booktitle>
Reference-contexts: Many ingenious approaches have been developed to attack the repeatability problem for parallel programs targeted to MPPs <ref> [3] </ref>, however most assume that resources are uniform, enjoy the same performance characteristics, and can be loosely synchronized with respect to one another.
Reference: [4] <author> F. Berman and R. Wolski. </author> <title> The apples project: A status report. </title> <booktitle> In Proceedings of the NEC Symposium on Metacomputing, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: SEA [43] and VDCE [45] represent the program as dependency graphs of coarse-grained tasks. In the case of VDCE, each of the tasks are from a mathematical task library. In other efforts, the program is represented by its characteristics. AppLeS <ref> [4] </ref> and I-SOFT [17] take this approach, representing programs in terms of their resource requirements. AppLeS and I-SOFT focus on coarse-grained grid applications. IOS [7] on the other hand, represents real-time fine-grained iterative automatic target recognition applications. <p> AppLeS and I-SOFT focus on coarse-grained grid applications. IOS [7] on the other hand, represents real-time fine-grained iterative automatic target recognition applications. Each task in IOS is associated with 12 Project Program Performance Scheduling Remarks Model Model Policy AppLeS communicating application best of candidate Network Weather <ref> [4] </ref> tasks performance schedules based Service ([47]) used model parameterized on user's to forecast resource by dynamic resource performance load and performance capacities criteria availability MARS phased dependency graph determines candidate program history [20] message-passing built from schedule which information used programs program and used minimizes execution to improve to determine <p> VDCE uses the program graph as a performance model, in which the scheduler will evaluate candidate schedules based on predicted task execution times. All of these approaches require little intervention from the user. On the other end of the spectrum are user-derived performance models. AppLeS <ref> [4] </ref> assumes that the performance model will be provided by the user. Current AppLeS applications rely on structural performance models [37] which compose performance activities (parameterized by static and dynamic information) into a prediction of application performance. <p> The scheduling policy is then to choose the "best" (according to the performance criteria) among the candidate choices. Note that some schedulers perform resource selection as a preliminary step to filter the candidate resource sets to a managable number, and some schedulers do not. AppLeS <ref> [4] </ref> performs resource selection as an initial step and its default scheduling policy chooses the best schedule among the resulting candidates (scheduled by a "Planner" subsystem) based on the user's performance criteria (which may not be minimal execution time). Other scheduling policies may be provided by the user. <p> AppLeS (Application-Level Scheduler) <ref> [4] </ref> is a high-performance scheduler targeted to multi-user distributed heterogeneous environments. Each grid application is scheduled by its own AppLeS which determines and "actuates" a schedule customized for the individual application and the target computational grid at schedule-time. <p> Such schedules better reflect the dynamics of changing application resource requirements and dynamically varying system state. This can be seen, for example, in the MARS project [20] with respect to the retention of performance information for the purpose of rescheduling, in the AppLeS project <ref> [4] </ref> in terms of the use of dynamic system information via the Network Weather Service, and in terms of the load-balancing approach performed as part of the Dome system [1]. <p> This meta-information can be used in various ways. Autopilot [34] incorporates meta-information in the fuzzy logic values used for determining resource allocation decisions. AppLeS <ref> [4] </ref> uses quality of information ("QoIn") measures to evaluate the quality of predictions derived by structural performance models. Such uses of meta-information parallel some of the important ideas on "meta-data" emerging from the data analysis and data mining communities. <p> Siegel, Valerie Taylor, Jon Weissman, and the members of the U.C.S.D. AppLeS group for useful comments on previous drafts. Finally, I am grateful to NSF, DARPA, and the DoD Modernization Program for support during the development of this chapter. Further Reading 1. <ref> [4] </ref> 30 4. [5]
Reference: [5] <author> F. Berman, R. Wolski, S. Figueira, J. Schopf, and G Shao. </author> <title> Application level scheduling on distributed heterogeneous networks. </title> <booktitle> In Proceedings of Supercomputing 1996. </booktitle>
Reference-contexts: Performance at schedule time was modeled using a compositional performance model parameterized by dynamic parameters representing CPU load and available bandwidth. The computation and communication component models were chosen to reflect resource performance in the distributed environment. More information about the Jacobi2D AppLeS can be found in <ref> [5] </ref>. <p> Although much of the information that would be required by the DSSA AppLeS would be dynamic or application-specific, experience shows that it could be obtained efficiently during scheduling or off-line <ref> [5] </ref>. Since the DSSA application will be performed repeatedly by researchers, the time spent building an AppLeS scheduler for it by application developers would be amortized by improved application execution performance for users. <p> Siegel, Valerie Taylor, Jon Weissman, and the members of the U.C.S.D. AppLeS group for useful comments on previous drafts. Finally, I am grateful to NSF, DARPA, and the DoD Modernization Program for support during the development of this chapter. Further Reading 1. [4] 30 4. <ref> [5] </ref>
Reference: [6] <author> N. Bowen, C. Nikolaou, and A. Ghafoor. </author> <title> On the assignment problem of arbitrary process systems to heterogeneous distributed computer systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(3), </volume> <month> March </month> <year> 1992. </year>
Reference: [7] <author> J. Budenske, R. Ramanujan, and H.J. Siegel. </author> <title> On-line use of off-line derived mappings for iterative automatic target recognition tasks and a particular class of hardware platforms. </title> <booktitle> In Proceedings of the Heterogeneous Computing Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: In other efforts, the program is represented by its characteristics. AppLeS [4] and I-SOFT [17] take this approach, representing programs in terms of their resource requirements. AppLeS and I-SOFT focus on coarse-grained grid applications. IOS <ref> [7] </ref> on the other hand, represents real-time fine-grained iterative automatic target recognition applications. <p> select own [17] supercomputers users, maintained user queues resources, sched. remote instruments, static capacity info. and static capacities, approach used immersive envts., used for scheduling apps scheduled as "first for I-Way data systems some applications come, first served" at SC '95 IOS real-time, iterative app. represented off-line genetic alg. approach <ref> [7] </ref> automatic target as a dependency graph mappings indexed by uses dynamic recognition of subtasks, each of dynamic params. used parameters applications which can be assigned to determine mapping to index off-line one of several for current mappings possible algs. iteration SPP (X) base serial language compositional determination of skeleton perf. <p> Similarly, MARS [20] uses the program dependency graph built during an iterative execution process and parameterized by dynamic information as its performance model for the next iteration. Dome [1] uses the last program iteration as a benchmark for its SPMD programs, and as a predictor of future performance. IOS <ref> [7] </ref> associates a set of algorithms with each fine-grained task in the program graph, and evaluates pre-stored off-line mappings for this graph indexed by dynamic information. VDCE uses the program graph as a performance model, in which the scheduler will evaluate candidate schedules based on predicted task execution times. <p> I-WAY was a successful "proof-of-concept" experiment in grid computing at Supercomputing '95. The I-SOFT scheduler [17] was centralized and operated on a "first come, first served" policy. Information about static capacities and user queues was used by many users to develop schedules for their own applications. Finally, IOS <ref> [7] </ref> uses a novel approach for scheduling fine-grained automatic target recognition (ATR) applications. Off-line genetic algorithm mappings are developed for different configurations of program parameters prior to execution. Dynamic information is then used to select a performance-efficient mapping at 0.5. Case Study in High-Performance Schedulers: The AppLeS Project 15 run-time. <p> Restricted-Domain Schedulers One way of deriving information about application behavior and performance is to restrict the applications to be scheduled to those which lie within a well-defined domain. Several high-performance scheduling efforts target a particular class of programs. IOS <ref> [7] </ref> targets iterative automatic target recognition programs, whereas Prophet [46] targets SPMD and parallel pipelined Mentat applications. In addition, the Phase [21] system performs resource selection to to support the efficient execution of pharmaceutical applications on computational grids.
Reference: [8] <author> Henri Casanova and Jack Dongarra. Netsolve: </author> <title> A network server for solving computational science problems. </title> <type> Technical Report cs-95-313, </type> <institution> University of Tennessee, </institution> <month> November </month> <year> 1995. </year> <note> 31 32 BIBLIOGRAPHY </note>
Reference-contexts: Autopilot controls resource allocation based on application-driven events. A fuzzy logic model is used to determine allocation decisions based on the "quality" of monitored performance data. A discussion of Autopilot and additional related work can be found in Chapter ??. Ninf [30], NetSolve <ref> [8] </ref>, Nile [29], and NEOS [10] represent problem solving environments which can benefit from high-performance scheduling to achieve performance. For example, Ninf incorporates "metaserver" agents to gather network information to optimize client-server-based applications. <p> Coordinating the activities of compiling and scheduling would enhance them both. Compiling applications for computational grids is addressed in Chapter ??. In addition, adaptive scheduling can enhance the performance of PSEs. Tools such as SCIrun [35] and NetSolve <ref> [8] </ref> can themselves be scheduled to achieve better performance. The integration of problem solving environments and high-performance schedulers would provide an execution and development environment in which the quality of application results, as well as application execution performance could be addressed.
Reference: [9] <author> T. Cassavant and J. Kuhl. </author> <title> A taxonomy of scheduling in general-purpose distributed computing systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(2) </volume> <pages> 141-154, </pages> <month> February </month> <year> 1988. </year>
Reference: [10] <author> J. Czyzyk, M. Mesnier, and J. </author> <title> More. The network-enabled optimization system (neos) server. </title> <type> Technical Report MCS-P615-1096, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: Autopilot controls resource allocation based on application-driven events. A fuzzy logic model is used to determine allocation decisions based on the "quality" of monitored performance data. A discussion of Autopilot and additional related work can be found in Chapter ??. Ninf [30], NetSolve [8], Nile [29], and NEOS <ref> [10] </ref> represent problem solving environments which can benefit from high-performance scheduling to achieve performance. For example, Ninf incorporates "metaserver" agents to gather network information to optimize client-server-based applications. The system schedules accesses from a client application to remote libraries with the performance goal of optimizing application performance.
Reference: [11] <author> DOCT. </author> <title> Distributed object computation testbed. </title> <address> http://www.sdsc.edu/doct/. </address>
Reference-contexts: No resource management system is assumed, however AppLeS applications are currently being targeted to the Globus [18] and Legion [24] software systems and their testbeds, the DOCT testbed <ref> [11] </ref>, and the NPACI metasystem [33]. AppLeS operates at the user level and does not assume any special permissions. Neither computation nor communication resources are assumed to be homogeneous, nor are they assumed to be under uniform administrative control.
Reference: [12] <author> M. Eshagian and R. Freund. </author> <title> Cluster-m paradigms for high-order heterogeneous procedural specification computing. </title> <booktitle> In Proceedings of the Heterogeneous Computing Workshop, </booktitle> <year> 1992. </year>
Reference: [13] <author> Dan Reed et al. Pablo project. </author> <note> http://bugle.cs.uiuc.edu/#overview. </note>
Reference-contexts: Problem solving environments for computational grids are addressed in Chapter ??. Finally, programmers rely on support for both performance monitoring and evaluation to improve the performance of their applications. Tools such as Pablo <ref> [13] </ref>, and Paradyn [14] can be used to develop performance-efficient programs. High-performance schedulers often require similar sorts of dynamic performance information to make scheduling decisions.
Reference: [14] <editor> B.P. Miller et. al. </editor> <title> The Paradyn Parallel Performance Measurement Tool. </title> <journal> IEEE-COMPUTER, </journal> <volume> 28(11) </volume> <pages> 37-46, </pages> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: Problem solving environments for computational grids are addressed in Chapter ??. Finally, programmers rely on support for both performance monitoring and evaluation to improve the performance of their applications. Tools such as Pablo [13], and Paradyn <ref> [14] </ref> can be used to develop performance-efficient programs. High-performance schedulers often require similar sorts of dynamic performance information to make scheduling decisions. Coordination between performance monitoring, evaluation, and scheduling activities would allow such facilities to leverage each other's information 28 more efficiently, and utilize adaptive techniques for improving application performance.
Reference: [15] <author> D. Feitelson. </author> <title> A survey of scheduling in multiprogrammed parallel systems. </title> <type> Technical Report RC 19790 (87657), </type> <institution> IBM Research Division, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: This seemed reasonable since applications in both MPP and grid environments require careful coordination of processing, communication and data to achieve performance. Moreover, in the MPP environment, strategies such as gang scheduling <ref> [15] </ref> provide a method by which both application and system behavior can be optimized (under the assumption that achieving good throughput, utilization and/or fairness for uniform resources will promote good application performance on average). However, MPP scheduling models generally produce poor grid schedules in practice.
Reference: [16] <author> S. M. Figueira and F. Berman. </author> <title> Modeling the effects of contention on the performance of heterogeneous applications. </title> <booktitle> Proceedings of the High Performance Distributed Computing Conference, </booktitle> <year> 1996. </year>
Reference-contexts: In grid environments, networked resources may be shared, and AP P 1 and AP P 2 may both be able to obtain the same resources concurrently. However, each application may "slow down" or degrade the performance of the other <ref> [16] </ref>, diminishing the resulting performance of both applications. 6 * Load-balancing may not provide the optimal application schedul ing policy. In grid environments, the performance deliverable by a given resource will vary over time, depending on the fraction of the resource allocated to other programs which share the system.
Reference: [17] <author> I. Foster, J. Geisler, W. Smith, and S. Tuecke. </author> <title> Software infrastructure for the i-way high-performance distributed computing experiment. </title> <booktitle> Proceedings of the 5th High-Performance Distributed Computing Conference, </booktitle> <year> 1996. </year>
Reference-contexts: SEA [43] and VDCE [45] represent the program as dependency graphs of coarse-grained tasks. In the case of VDCE, each of the tasks are from a mathematical task library. In other efforts, the program is represented by its characteristics. AppLeS [4] and I-SOFT <ref> [17] </ref> take this approach, representing programs in terms of their resource requirements. AppLeS and I-SOFT focus on coarse-grained grid applications. IOS [7] on the other hand, represents real-time fine-grained iterative automatic target recognition applications. <p> model in next version SEA dataflow-style expert system "ready" tasks resource selection [43] program dependence which evaluates enabled in program performed by a graph "ready" tasks in graph are next to Mapping Expert program graph be scheduled Advisor (MEA) I-SOFT apps. which couple developed by centralized scheduler users select own <ref> [17] </ref> supercomputers users, maintained user queues resources, sched. remote instruments, static capacity info. and static capacities, approach used immersive envts., used for scheduling apps scheduled as "first for I-Way data systems some applications come, first served" at SC '95 IOS real-time, iterative app. represented off-line genetic alg. approach [7] automatic target <p> AppLeS [4] assumes that the performance model will be provided by the user. Current AppLeS applications rely on structural performance models [37] which compose performance activities (parameterized by static and dynamic information) into a prediction of application performance. The I-SOFT scheduler <ref> [17] </ref> assumed that both the performance model and the resulting schedule were determined by the programmer. Information about system characteristics was available, but usage of those characteristics was left up to the programmer. Some approaches combine both programmer-provided and scheduler-provided performance components. <p> The scheduling policy used by SEA is embodied in its expert system approach: the application is scheduled by enabling tasks as they become "ready" in the program graph. I-WAY was a successful "proof-of-concept" experiment in grid computing at Supercomputing '95. The I-SOFT scheduler <ref> [17] </ref> was centralized and operated on a "first come, first served" policy. Information about static capacities and user queues was used by many users to develop schedules for their own applications. Finally, IOS [7] uses a novel approach for scheduling fine-grained automatic target recognition (ATR) applications.
Reference: [18] <author> I. Foster and C. Kesselman. Globus: </author> <title> A metacomputing infrastructure toolkit. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 11(2) </volume> <pages> 115-128, </pages> <year> 1997. </year>
Reference-contexts: The target platform for AppLeS applications is intended to be a distributed wide-area and/or local-area network which connects computational resources, data resources, visualization resources, and "smart instruments". No resource management system is assumed, however AppLeS applications are currently being targeted to the Globus <ref> [18] </ref> and Legion [24] software systems and their testbeds, the DOCT testbed [11], and the NPACI metasystem [33]. AppLeS operates at the user level and does not assume any special permissions. <p> Mechanisms which retain dynamic information must be extensible so that new categories of information relevant to an application's resource usage can be stored, and flexible so that information can be gathered and accessed in a variety of ways. The Metacomputing Directory System (MDS), developed for Globus <ref> [18] </ref>, provides an example of a database facility which retains dynamic resource information such as the number of nodes currently available and the status of the resource that can be used by schedulers. The MDS is discussed in Chapter ??.
Reference: [19] <author> R. Freund. </author> <title> Optimal selection theory for superconcurrency. </title> <booktitle> In Proceedings of Supercomputing '89. </booktitle>
Reference-contexts: There are many ways to develop application scheduling models for the computational grid, many of which are documented in the literature. Early recognition of the multi-parametered nature of performance and program models can be seen in the work on Optimal Selection Theory <ref> [19] </ref>. In addition, a number of sophisticated scheduling policies have been devised to address the grid scheduling problem [42, 9, 6, 27, 39, 32, 28, 22, 25, 12, 26, 41], etc.
Reference: [20] <author> J. Gehrinf and A. Reinfeld. </author> <title> Mars a framework for minimizing the job execution time in a metacomputing environment. </title> <booktitle> Proceedings of Future General Computer Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Dataflow-style program graphs are a common representation for grid programs. Dome [1] and SPP (X) [2] provide a language abstraction for the program which is compiled into a low-level program dependency graph representation. MARS <ref> [20] </ref> assumes programs to be phased (represented by a sequence of program stages or phases), and builds a program dependency graph as part of the scheduling process. SEA [43] and VDCE [45] represent the program as dependency graphs of coarse-grained tasks. <p> 12 Project Program Performance Scheduling Remarks Model Model Policy AppLeS communicating application best of candidate Network Weather [4] tasks performance schedules based Service ([47]) used model parameterized on user's to forecast resource by dynamic resource performance load and performance capacities criteria availability MARS phased dependency graph determines candidate program history <ref> [20] </ref> message-passing built from schedule which information used programs program and used minimizes execution to improve to determine time successive task migration executions Prophet Mentat SPMD execution time = sum determines schedule focuses primarily [46] and parallel of comm. and comp. with the minimal on workstation pipeline programs parameterized by predicted <p> We describe some of the general approaches in the following paragraphs. On one end of the spectrum are scheduler-derived performance models. SPP (X) [2] derives "skeleton" performance models from programs developed using a Structured Coordination Language. Similarly, MARS <ref> [20] </ref> uses the program dependency graph built during an iterative execution process and parameterized by dynamic information as its performance model for the next iteration. Dome [1] uses the last program iteration as a benchmark for its SPMD programs, and as a predictor of future performance. <p> Other scheduling policies may be provided by the user. SPP (X), Prophet [46] and MARS <ref> [20] </ref> use similar approaches although they do not provide as much latitude for user-provided scheduling policies or performance criteria the performance goal for all applications is minimal execution time. VDCE [45] uses a list scheduling algorithm to match resources with application tasks. The performance criteria is minimal execution time. <p> Consequently, many high-performance schedulers utilize dynamic information to create adaptive schedules. Such schedules better reflect the dynamics of changing application resource requirements and dynamically varying system state. This can be seen, for example, in the MARS project <ref> [20] </ref> with respect to the retention of performance information for the purpose of rescheduling, in the AppLeS project [4] in terms of the use of dynamic system information via the Network Weather Service, and in terms of the load-balancing approach performed as part of the Dome system [1].
Reference: [21] <author> J. Gehring, A. Reinefeld, and A. Weber. </author> <title> Phase and mica: Application specific metacomputing. </title> <booktitle> Proceedings of the Euro-Par Conference, </booktitle> <year> 1997. </year> <note> BIBLIOGRAPHY 33 </note>
Reference-contexts: Several high-performance scheduling efforts target a particular class of programs. IOS [7] targets iterative automatic target recognition programs, whereas Prophet [46] targets SPMD and parallel pipelined Mentat applications. In addition, the Phase <ref> [21] </ref> system performs resource selection to to support the efficient execution of pharmaceutical applications on computational grids. Restricting the program domain enables a scheduler to better predict application behavior, and to use specialized scheduling policies to achieve performance.
Reference: [22] <author> A. Ghafoor and J. Yang. </author> <title> A distributed heterogeneous supercomputing man-agment system. </title> <booktitle> Computer, </booktitle> <year> 1993. </year>
Reference: [23] <institution> Globus Resource Allocation Manager (GRAM). </institution> <note> http://www.globus.org/scheduler/grm_spec.html. </note>
Reference-contexts: Multischeduling Finally, high-performance schedulers will not operate "in a vacuum" they will co-exist with a number of scheduling mechanisms including local resource schedulers, high-throughput schedulers, and perhaps other high-performance schedulers. Scheduling different resources simultaneously (or multischeduling) (also known as co-allocation <ref> [23] </ref>) is difficult. In particular, developing an integrated scheduling subsystem in which each scheduler is able to promote the performance of the programs or resources in its domain provides the ultimate scheduling challenge. <p> High-performance schedulers, resource schedulers and job schedulers will share the same resources and in many cases, use the same grid infrastructure to manage communication, store or access information, reserve resources, etc. One approach to providing a uniform interface for multiple schedulers is the Globus Resource Allocation Manager (GRAM) <ref> [23] </ref>, which is being designed to provide services which support resource discovery, resource inquiry, MDS access, and other activities useful for scheduling on computational grids. When multiple schedulers work together (both high-performance and otherwise), consistency of information and meta-information across the grid becomes especially important.
Reference: [24] <author> A. S. Grimshaw, W. A. Wulf, </author> <title> and the Legion Team. The legion vision of a worldwide virtual computer. </title> <journal> Communications of the ACM, </journal> <year> 1997. </year>
Reference-contexts: The target platform for AppLeS applications is intended to be a distributed wide-area and/or local-area network which connects computational resources, data resources, visualization resources, and "smart instruments". No resource management system is assumed, however AppLeS applications are currently being targeted to the Globus [18] and Legion <ref> [24] </ref> software systems and their testbeds, the DOCT testbed [11], and the NPACI metasystem [33]. AppLeS operates at the user level and does not assume any special permissions. Neither computation nor communication resources are assumed to be homogeneous, nor are they assumed to be under uniform administrative control. <p> High-Level Language Support High-level language support for scheduling can assist the programmer in the process of developing grid-aware applications and provide important 0.8. System Support for High-Performance Schedulers 27 information for high-performance schedulers. The Legion system <ref> [24] </ref> provides an example of such an approach. High-level language primitives (data streams, object method invocations, etc.) provide support for building high-level semantic objects. These objects can be incorporated in various language paradigms and used for scheduling. High-level language support ensures uniform semantics across the computational grid.
Reference: [25] <author> B. Hamidzadeh, D. Lilja, and Y. Arif. </author> <title> Dynamic scheduling techniques for heterogeneous computing systems. </title> <journal> Concurrency, Practice and Experience, </journal> <volume> 7(7), </volume> <month> October </month> <year> 1995. </year>
Reference: [26] <author> A. Kokhar, V. Prasanna, M. Shaaban, and C. Wang. </author> <title> Heterogeneous computing: Challenges and opportunities. </title> <journal> IEEE Computer, </journal> <volume> 26(6), </volume> <month> June </month> <year> 1993. </year>
Reference: [27] <author> C. Leangsuksun, J. Potter, and S. Scott. </author> <title> Dynamic task mapping algorithms for a distributed heterogeneous computing environment. </title> <booktitle> In Proceedings of the Heterogeneous Computing Workshop, </booktitle> <year> 1995. </year>
Reference: [28] <author> D. Lilja. </author> <title> Experiments with a task partitioning model for heterogeneous computing. </title> <booktitle> In Proceedings of the Heterogeneous Computing Workshop, </booktitle> <month> April </month> <year> 1993. </year>
Reference: [29] <author> K. Marzullo, M. Ogg, A. Ricciardi, A. Amoroso, F. Calkins, and E. Rothfus. Nile: </author> <title> Wide-area computing for high energy physics. </title> <booktitle> Proceedings of the 1996 SIGOPS Conference. </booktitle>
Reference-contexts: Autopilot controls resource allocation based on application-driven events. A fuzzy logic model is used to determine allocation decisions based on the "quality" of monitored performance data. A discussion of Autopilot and additional related work can be found in Chapter ??. Ninf [30], NetSolve [8], Nile <ref> [29] </ref>, and NEOS [10] represent problem solving environments which can benefit from high-performance scheduling to achieve performance. For example, Ninf incorporates "metaserver" agents to gather network information to optimize client-server-based applications. The system schedules accesses from a client application to remote libraries with the performance goal of optimizing application performance.
Reference: [30] <author> S. Matsuoka, U. Nagashima, and H. Nakada. </author> <title> Ninf : Network based information library for globally high performance computing. </title> <booktitle> In Methods and Applications (POOMA), </booktitle> <address> Santa Fe, </address> <year> 1996. </year>
Reference-contexts: Autopilot controls resource allocation based on application-driven events. A fuzzy logic model is used to determine allocation decisions based on the "quality" of monitored performance data. A discussion of Autopilot and additional related work can be found in Chapter ??. Ninf <ref> [30] </ref>, NetSolve [8], Nile [29], and NEOS [10] represent problem solving environments which can benefit from high-performance scheduling to achieve performance. For example, Ninf incorporates "metaserver" agents to gather network information to optimize client-server-based applications.
Reference: [31] <author> C. R. Mechoso, J. D. Farrara, and J. A. Spahr. </author> <title> Running a climate model in a heterogeneous, distributed computer environment. </title> <booktitle> In Proceedings of the Third IEEE International Symposium on High Performance and Distributed Computing, </booktitle> <pages> pages 79-84, </pages> <month> August </month> <year> 1994. </year>
Reference: [32] <author> D. Menasce, S. da Silva Porto, and S. Tripathi. </author> <title> Processor assignment in heterogeneous parallel architectures. </title> <type> Technical Report UMIACS-TR-91-131 CS-TR-2765, </type> <institution> University of Maryland, </institution> <month> September </month> <year> 1991. </year>
Reference: [33] <editor> NPACI. </editor> <booktitle> National partnership for advanced computational infrastructure, </booktitle> <address> http://www.npaci.edu. 34 BIBLIOGRAPHY </address>
Reference-contexts: No resource management system is assumed, however AppLeS applications are currently being targeted to the Globus [18] and Legion [24] software systems and their testbeds, the DOCT testbed [11], and the NPACI metasystem <ref> [33] </ref>. AppLeS operates at the user level and does not assume any special permissions. Neither computation nor communication resources are assumed to be homogeneous, nor are they assumed to be under uniform administrative control.
Reference: [34] <author> R. Ribler and D. Reed. </author> <title> The autopilot performance-directed adaptive control system. </title> <booktitle> International Conference on Suptercomputing, Workshop on Performance Data Mining: Automated Diagnosis, Adaption and Optimization, </booktitle> <year> 1997. </year>
Reference-contexts: There are a number of problem solving environments, program development tools, and network-based libraries which act as "applications" and use high-performance scheduling techniques to achieve performance. Application-centric resource management systems such as Autopilot <ref> [34] </ref> seek to control resource allocation based on application performance and represent application-aware systems. Autopilot controls resource allocation based on application-driven events. A fuzzy logic model is used to determine allocation decisions based on the "quality" of monitored performance data. <p> Using MetaInformation to Improve Performance A number of scheduling projects use information from various sources as well as meta-information in the form of an assessment of the quality of the information given. This meta-information can be used in various ways. Autopilot <ref> [34] </ref> incorporates meta-information in the fuzzy logic values used for determining resource allocation decisions. AppLeS [4] uses quality of information ("QoIn") measures to evaluate the quality of predictions derived by structural performance models. <p> Information on QoS and resource reservation for computational grids can be found in Chapter ??. 26 2. Mechanisms for Monitoring and Storing Dynamic Resource In formation Information about dynamic system state and application resource usage can be used effectively by high-performance and other schedulers to derive performance-efficient schedules. Autopilot <ref> [34] </ref> (Chapter ??) provides an example of a mechanism which gathers resource information (for the purpose of managing resource allocation) based on application-driven events; the Network Weather Service [47] provides an example of dynamic resource information which is gathered at regular intervals.
Reference: [35] <author> D. Weinstein S. Parker and C. Johnson. </author> <title> The scirun computational steering software system. </title> <editor> In E. Arge, A. Bruaset, and H. Langtangen, editors, </editor> <booktitle> Modern Software Tools in Scientific Computing, </booktitle> <year> 1997. </year>
Reference-contexts: Conversely, scheduling directives within the application can provide useful information to the compiler. Coordinating the activities of compiling and scheduling would enhance them both. Compiling applications for computational grids is addressed in Chapter ??. In addition, adaptive scheduling can enhance the performance of PSEs. Tools such as SCIrun <ref> [35] </ref> and NetSolve [8] can themselves be scheduled to achieve better performance. The integration of problem solving environments and high-performance schedulers would provide an execution and development environment in which the quality of application results, as well as application execution performance could be addressed.
Reference: [36] <author> SARA. </author> <title> Synthetic apperture radar atlas. </title> <address> http://sara.sdsc.edu/. </address>
Reference: [37] <author> J. Schopf. </author> <title> Structural prediction models for high-performance distributed applications. </title> <booktitle> Proceedings of the Cluster Computing Conference, </booktitle> <year> 1997. </year>
Reference-contexts: One approach to developing grid models is to compose models from constituent components which reflect application performance activities. This approach is being taken by a number of researchers <ref> [37, 44, 2, 50] </ref>, etc. <p> All of these approaches require little intervention from the user. On the other end of the spectrum are user-derived performance models. AppLeS [4] assumes that the performance model will be provided by the user. Current AppLeS applications rely on structural performance models <ref> [37] </ref> which compose performance activities (parameterized by static and dynamic information) into a prediction of application performance. The I-SOFT scheduler [17] assumed that both the performance model and the resulting schedule were determined by the programmer.
Reference: [38] <author> J. Schopf and F. Berman. </author> <title> Performance prediction in production environments. </title> <type> Technical Report CS97-558, </type> <address> U. C. San Diego, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: predict application execution time within acceptable accuracy, and it may be possible to combine the accuracies of each of the predictions of CompA (t 1 ), CompB (t 3 ) and Comm (t 2 ) to deliver a performance prediction of ExecT ime (t 1 ) with the desired accuracy <ref> [38] </ref>. Note that the accuracy, lifetime, and other characteristics of performance parameters and predictions constitute meta-information (attributes which describe the determination or content of information) which provides a qualitative measure of the performance information being used in/produced by the model.
Reference: [39] <author> S. Selvakumar and C. Siva Ram Murthy. </author> <title> Static task allocation of concurrent programs for distributed computing systems with processor and resource heterogeneity. </title> <journal> Parallel Computing, </journal> <volume> 20, </volume> <year> 1994. </year>
Reference: [40] <author> B. Shirazi, A. Hurson, and K. Kavi. </author> <title> Scheduling and Load Balancing in Parallel and Distributed Systems. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference: [41] <author> H.J. Siegel, John Antonio, Richard Metzger, Min Tan, and Yan Alexander Li. </author> <title> Heterogeneous computing. </title> <editor> In A. Zomaya, editor, </editor> <booktitle> Parallel and Distributed Computing Handbook. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1996. </year>
Reference: [42] <author> G. Sih and E. Lee. </author> <title> A compile-time scheduling heuristic for interconnection-constrained heterogeneous processor architectures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 175-187, </pages> <month> February </month> <year> 1993. </year>
Reference: [43] <author> M. Sirbu and D. Marinescu. </author> <title> A scheduling expert advisor for heterogeneous environments. </title> <booktitle> In Proceedings of the Heterogeneous Computing Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: MARS [20] assumes programs to be phased (represented by a sequence of program stages or phases), and builds a program dependency graph as part of the scheduling process. SEA <ref> [43] </ref> and VDCE [45] represent the program as dependency graphs of coarse-grained tasks. In the case of VDCE, each of the tasks are from a mathematical task library. In other efforts, the program is represented by its characteristics. <p> dependency graph list scheduling communication [45] of tasks from weighted by dedicated used to match weighted as 0 mathematical task task benchmarks and resources with in current version, libraries dynamic load info. application tasks param. by exp., anal. model in next version SEA dataflow-style expert system "ready" tasks resource selection <ref> [43] </ref> program dependence which evaluates enabled in program performed by a graph "ready" tasks in graph are next to Mapping Expert program graph be scheduled Advisor (MEA) I-SOFT apps. which couple developed by centralized scheduler users select own [17] supercomputers users, maintained user queues resources, sched. remote instruments, static capacity info. <p> Some approaches combine both programmer-provided and scheduler-provided performance components. Prophet [46] provides a more generic performance model (ExecutionT ime = Computation + Communication) for its SPMD programs, parameterized by benchmark, static, and dynamic program capacity information. SEA <ref> [43] </ref> uses its dataflow-style program graph as input to an expert 14 system which evaluates which tasks are currently "ready". These approaches require both programmer and scheduler information. 0.4.3 Scheduling Policies The goal of a high-performance scheduler is to determine a schedule which optimizes the application's performance goal.
Reference: [44] <author> V. Taylor, J. Chen, T. Disz, M. Papka, and R. Stevens. </author> <title> Interactive virtual reality in simulations: Exploring lag time. </title> <journal> IEEE Computational Science and Engineering, </journal> <year> 1996. </year>
Reference-contexts: One approach to developing grid models is to compose models from constituent components which reflect application performance activities. This approach is being taken by a number of researchers <ref> [37, 44, 2, 50] </ref>, etc. <p> In this way, the performance model can be adapted to reflect the performance characteristics of the application with respect to a particular execution environment. Such an approach is taken in <ref> [44] </ref> which describes a compositional model for predicting lagtime in interactive virtual reality simulations. Adaptation can also be used to weight the relative impact of the performance activities represented in a performance model.
Reference: [45] <author> H. Topcuoglu, S. Hariri, W. Furmanski, J. Valente, I. Ra, D. Kim, Y. Kim, X. Bing, and B. Ye. </author> <title> The software architecture of a virtual distributed computing environment. </title> <booktitle> Proceedings of the High-Performance Distributed Computing Conference, </booktitle> <year> 1997. </year> <note> BIBLIOGRAPHY 35 </note>
Reference-contexts: MARS [20] assumes programs to be phased (represented by a sequence of program stages or phases), and builds a program dependency graph as part of the scheduling process. SEA [43] and VDCE <ref> [45] </ref> represent the program as dependency graphs of coarse-grained tasks. In the case of VDCE, each of the tasks are from a mathematical task library. In other efforts, the program is represented by its characteristics. <p> determine time successive task migration executions Prophet Mentat SPMD execution time = sum determines schedule focuses primarily [46] and parallel of comm. and comp. with the minimal on workstation pipeline programs parameterized by predicted execution clusters static and dynamic inf. time VDCE programs composed task dependency graph list scheduling communication <ref> [45] </ref> of tasks from weighted by dedicated used to match weighted as 0 mathematical task task benchmarks and resources with in current version, libraries dynamic load info. application tasks param. by exp., anal. model in next version SEA dataflow-style expert system "ready" tasks resource selection [43] program dependence which evaluates enabled <p> Other scheduling policies may be provided by the user. SPP (X), Prophet [46] and MARS [20] use similar approaches although they do not provide as much latitude for user-provided scheduling policies or performance criteria the performance goal for all applications is minimal execution time. VDCE <ref> [45] </ref> uses a list scheduling algorithm to match resources with application tasks. The performance criteria is minimal execution time. Dome [1] focuses on load-balancing as a scheduling policy for its SPMD PVM programs with the performance goal of minimal execution time.
Reference: [46] <author> J. Weissman and X. Zhao. </author> <title> Runtime support for scheduling parallel applications in heterogeneous nows. </title> <booktitle> Proceedings of the High-Performance Distributed Computing Conference, </booktitle> <year> 1997. </year>
Reference-contexts: resource performance load and performance capacities criteria availability MARS phased dependency graph determines candidate program history [20] message-passing built from schedule which information used programs program and used minimizes execution to improve to determine time successive task migration executions Prophet Mentat SPMD execution time = sum determines schedule focuses primarily <ref> [46] </ref> and parallel of comm. and comp. with the minimal on workstation pipeline programs parameterized by predicted execution clusters static and dynamic inf. time VDCE programs composed task dependency graph list scheduling communication [45] of tasks from weighted by dedicated used to match weighted as 0 mathematical task task benchmarks and <p> The I-SOFT scheduler [17] assumed that both the performance model and the resulting schedule were determined by the programmer. Information about system characteristics was available, but usage of those characteristics was left up to the programmer. Some approaches combine both programmer-provided and scheduler-provided performance components. Prophet <ref> [46] </ref> provides a more generic performance model (ExecutionT ime = Computation + Communication) for its SPMD programs, parameterized by benchmark, static, and dynamic program capacity information. SEA [43] uses its dataflow-style program graph as input to an expert 14 system which evaluates which tasks are currently "ready". <p> Other scheduling policies may be provided by the user. SPP (X), Prophet <ref> [46] </ref> and MARS [20] use similar approaches although they do not provide as much latitude for user-provided scheduling policies or performance criteria the performance goal for all applications is minimal execution time. VDCE [45] uses a list scheduling algorithm to match resources with application tasks. <p> Restricted-Domain Schedulers One way of deriving information about application behavior and performance is to restrict the applications to be scheduled to those which lie within a well-defined domain. Several high-performance scheduling efforts target a particular class of programs. IOS [7] targets iterative automatic target recognition programs, whereas Prophet <ref> [46] </ref> targets SPMD and parallel pipelined Mentat applications. In addition, the Phase [21] system performs resource selection to to support the efficient execution of pharmaceutical applications on computational grids. Restricting the program domain enables a scheduler to better predict application behavior, and to use specialized scheduling policies to achieve performance.
Reference: [47] <author> R. Wolski. </author> <title> Dynamically forecasting network performance to support dynamic scheduling using the network weather service. </title> <booktitle> Proceedings of the High-Performance Distributed Computing Conference, </booktitle> <year> 1997. </year>
Reference-contexts: The goal of the AppLeS agent is to achieve performance for its application in the dynamic grid environment. The AppLeS approach is to model the user's scheduling process, while incorporating additional information and operating at machine speeds. A facility called the Network Weather Service <ref> [47] </ref> is used to provide dynamic forecasts of resource load and availability to each AppLeS. Dynamic Network Weather Service information is used to parameterize performance models, and to predict the state of grid resources at the time the application will be scheduled. <p> Autopilot [34] (Chapter ??) provides an example of a mechanism which gathers resource information (for the purpose of managing resource allocation) based on application-driven events; the Network Weather Service <ref> [47] </ref> provides an example of dynamic resource information which is gathered at regular intervals. It is often useful for dynamic resource information to be persistent. Time series analyses and predictive models utilize such information to promote good schedules, and require the retention of dynamic information.
Reference: [48] <author> R. Wolski. </author> <title> Forecasting network performance to support dynamic scheduling using the network weather service. </title> <booktitle> Proceedings of the High Performance Distributed Computing Conference, </booktitle> <year> 1997. </year>
Reference: [49] <author> M. Wu and A. Kuppermann. </author> <title> Casa quantum chemical reaction dynamics. In CASA Gigabit Network Testbed Annual Report, </title> <year> 1994. </year>
Reference: [50] <author> Y. Yan, X. Zhang, and M. Yang. </author> <title> An effective and practical performance prediction model for parallel computing on non-dedicated heterogeneous nows. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 35(2), </volume> <year> 1996. </year>
Reference-contexts: One approach to developing grid models is to compose models from constituent components which reflect application performance activities. This approach is being taken by a number of researchers <ref> [37, 44, 2, 50] </ref>, etc.
References-found: 50

