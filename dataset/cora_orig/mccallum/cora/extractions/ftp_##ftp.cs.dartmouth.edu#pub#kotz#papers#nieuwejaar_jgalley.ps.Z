URL: ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/nieuwejaar:jgalley.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~dfk/papers/jgalley.html
Root-URL: http://www.cs.dartmouth.edu
Title: The Galley Parallel File System  
Author: Nils Nieuwejaar, David Kotz 
Keyword: Key words: Parallel I/O. Multiprocessor file system. Performance evaluation. IBM  
Address: College, Hanover, NH 03755-3510  
Affiliation: Department of Computer Science, Dartmouth  
Note: To appear in Parallel Computing, 1997. Available at  SP-2. Scientific Computing.  
Email: fnils,dfkg@cs.dartmouth.edu  
Web: URL ftp://ftp.cs.dartmouth.edu/kotz/papers/nieuwejaar:jgalley.ps.Z  
Abstract: Most current multiprocessor file systems are designed to use multiple disks in parallel, using the high aggregate bandwidth to meet the growing I/O requirements of parallel scientific applications. Many multiprocessor file systems provide applications with a conventional Unix-like interface, allowing the application to access multiple disks transparently. This interface conceals the parallelism within the file system, increasing the ease of programmability, but making it difficult or impossible for sophisticated programmers and libraries to use knowledge about their I/O needs to exploit that parallelism. In addition to providing an insufficient interface, most current multiprocessor file systems are optimized for a different workload than they are being asked to support. We introduce Galley, a new parallel file system that is intended to efficiently support realistic scientific multiprocessor workloads. We discuss Galley's file structure and application interface, as well as the performance advantages offered by that interface.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> James W. Arendt. </author> <title> Parallel genome sequence comparison using a concurrent file system. </title> <type> Technical Report UIUCDCS-R-91-1674, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: Such a library 5 could store the compressed data chunks in one fork and index information in another. Another instance where this type of file structure may be useful is in the problem of genome-sequence comparison. This problem requires searching a large database to find approximate matches between strings <ref> [1] </ref>. The raw database used in [1] contained thousands of genetic sequences, each of which was composed of hundreds or thousands of bases. To reduce the amount of time required to identify potential matches, the authors constructed an index of the database that was specific to their needs. <p> Another instance where this type of file structure may be useful is in the problem of genome-sequence comparison. This problem requires searching a large database to find approximate matches between strings <ref> [1] </ref>. The raw database used in [1] contained thousands of genetic sequences, each of which was composed of hundreds or thousands of bases. To reduce the amount of time required to identify potential matches, the authors constructed an index of the database that was specific to their needs.
Reference: [2] <author> Sandra Johnson Baylor, Caroline B. Benveniste, and Yarson Hsu. </author> <title> Performance evaluation of a parallel I/O architecture. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 404-413, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference: [3] <author> Michael L. Best, Adam Greenberg, Craig Stanfill, and Lewis W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: Other multiprocessor file systems with this 29 Fig. 14. Increase in throughput for write requests using the strided interface when overwriting an existing file. style of interface are SUNMOS and its successor, PUMA [32], sfs [15], and CMMD <ref> [3] </ref>. Like the systems mentioned above, PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface [10]. In PPFS, however, the basic transfer unit is an application-defined record rather than a byte.
Reference: [4] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 63-70, </pages> <year> 1994. </year>
Reference: [5] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, George S. Almasi, Sandra Johnson Baylor, Anthony S. Bolmarcich, Yarsun Hsu, Julian Satran, Marc Snir, Robert Colao, Brian Herr, Joseph Kavaky, Thomas R. Morgan, and Anthony Zlotek. </author> <title> Parallel file systems for the IBM SP computers. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 222-248, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: One such library implements a Unix-like file model, which should reduce the effort required to port legacy applications to Galley [21]. Other libraries that have been implemented on top of Galley provide Panda [27,30] and Vesta <ref> [5] </ref> interfaces, as well as support for ViC*, a variant of C* designed for out-of-core computations [6,7]. 4.2 I/O Processors Galley's I/O servers are composed of several functional units, which are described in detail below.
Reference: [6] <author> Thomas H. Cormen and Alex Colvin. </author> <title> ViC*: A preprocessor for virtual-memory C*. </title> <type> Technical Report PCS-TR94-243, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> November </month> <year> 1994. </year>
Reference: [7] <author> Thomas H. Cormen and Melissa Hirschl. </author> <title> Early experiences in evaluating the Parallel Disk Model with the ViC* implementation. </title> <type> Technical Report PCS-TR96-293, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> August </month> <year> 1996. </year> <note> To appear in Parallel Computing. </note>
Reference: [8] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution> <note> Revised as Dartmouth PCS-TR93-188 on 9/20/94. </note>
Reference-contexts: To avoid the limitations of the linear file model, Galley does not impose a declustering strategy on an application's data. Instead, Galley provides applications with the ability to fully control this declustering according to their own needs. This control is particularly important when implementing I/O-optimal algorithms <ref> [8] </ref>. Applications are also able to explicitly indicate which disk they wish to access in each request. To allow this behavior, files are composed of one or more subfiles, which may be directly addressed by the application.
Reference: [9] <institution> Hewlett Packard. </institution> <note> HP97556/58/60 5.25-inch SCSI Disk Drives Technical Reference Manual, second edition, </note> <month> June </month> <year> 1991. </year> <title> HP Part number 5960-0115. </title>
Reference-contexts: To avoid these inflated results, we examined Galley's performance using a simulation of an HP 97560 SCSI hard disk, which has an average seek time of 13.5 ms and a maximum sustained throughput of 2.2 MB/s <ref> [9] </ref>. Our implementation of the disk model was based on earlier implementations [26,13] 1 .
Reference: [10] <author> Jay Huber, Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Like the systems mentioned above, PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface <ref> [10] </ref>. In PPFS, however, the basic transfer unit is an application-defined record rather than a byte. PPFS maps requests against the logical, linear stream of records to an underlying two-dimensional model, indexed with a (disk, record) pair. Several different mapping functions, corresponding to common data distributions, are built into PPFS.
Reference: [11] <author> IBM. </author> <title> AIX Version 3.2 General Programming Concepts, </title> <booktitle> twelfth edition, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: An example of when this interface is useful is shown in [21]. 5.4 List Requests Finally, in addition to these structured operations, Galley provides a simple, more general file interface, called the list interface, which has functionality similar to the POSIX lio listio () interface <ref> [11] </ref>. This interface allows an application to simply specify an array of (file offset, memory offset, size) triples that it would like transferred between memory and disk. This interface is useful for applications with access patterns that do not have any inherently regular structure.
Reference: [12] <author> David Kotz and Nils Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 640-649, </pages> <month> November </month> <year> 1994. </year>
Reference: [13] <author> David Kotz, Song Bac Toh, and Sriram Radhakrishnan. </author> <title> A detailed simulation model of the HP 97560 disk drive. </title> <type> Technical Report PCS-TR94-220, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference: [14] <author> Thomas T. Kwan and Daniel A. Reed. </author> <title> Performance of the CM-5 scalable file system. </title> <booktitle> In Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <pages> pages 156-165, </pages> <month> July </month> <year> 1994. </year> <month> 33 </month>
Reference: [15] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference-contexts: Other multiprocessor file systems with this 29 Fig. 14. Increase in throughput for write requests using the strided interface when overwriting an existing file. style of interface are SUNMOS and its successor, PUMA [32], sfs <ref> [15] </ref>, and CMMD [3]. Like the systems mentioned above, PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface [10]. In PPFS, however, the basic transfer unit is an application-defined record rather than a byte.
Reference: [16] <author> Ethan L. Miller and Randy H. Katz. </author> <title> Input/output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 567-576, </pages> <month> November </month> <year> 1991. </year>
Reference: [17] <author> Jason A. Moore, Phil Hatcher, and Michael J. Quinn. </author> <title> Efficient data-parallel files via automatic mode detection. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 1-14, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Under Galley, this index could be stored in one fork, while the database itself could be stored in a second fork. A final example of the use of forks is Stream*, a parallel file abstraction for the data-parallel language C* <ref> [17] </ref>. Briefly, Stream* divides a file into three distinct segments, each of which corresponds to a particular set of access semantics. While the current implementation of Stream* stores all the segments in a single file, one could use a different fork for each segment.
Reference: [18] <author> Nils Nieuwejaar and David Kotz. </author> <title> Low-level interfaces for high-level parallel I/O. </title> <editor> In Ravi Jain, John Werth, and James C. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, chapter 9, </booktitle> <pages> pages 205-223. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference: [19] <author> Nils Nieuwejaar and David Kotz. </author> <title> Performance of the Galley parallel file system. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 83-94, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Throughput for write requests using the traditional Unix-like interface when overwriting an existing file. There were 16 CPs in every case. able to receive data and issue new requests to the IOPs in a timely fashion <ref> [19] </ref>. As a result, the fortuitous synchronization discussed above broke down, so the DiskManagers on the IOPs were unable to make intelligent disk scheduling decisions, causing excess disk-head seeks and thrashing of the on-disk cache.
Reference: [20] <author> Nils Nieuwejaar, David Kotz, Apratim Purakayastha, Carla Schlatter Ellis, and Michael Best. </author> <title> File-access characteristics of parallel scientific workloads. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1996. To appear. </note>
Reference-contexts: Studies of two different multiprocessor file-system workloads, running a variety of applications in a variety of scientific domains, on two architectures, under both data-parallel and control-parallel programming models, show that many applications make many small, regular, but non-consecutive requests to the file system <ref> [20] </ref>. These studies suggest that the workload that most multiprocessor file systems were optimized for is very different than the workloads they are actually being asked to serve. <p> The drawback of this approach is that most multiprocessor file systems use a declustering unit size measured in kilobytes (e.g., 4 KB in Intel's CFS [23]), but our workload characterization studies show that the typical request size in a parallel application is much smaller: frequently under 200 bytes <ref> [20] </ref>. This disparity between the request size and 3 the declustering unit size means that most of the individual requests generated by parallel applications are not being executed in parallel. <p> Although it may seem counterintuitive for an application to access large, contiguous regions of a file in small chunks, we observed such behavior in practice <ref> [20] </ref>. One likely reason that data would be accessed in this fashion is that records stored contiguously on disk are to be stored non-contiguously in memory. Another possible cause for such behavior is that the I/O was added to an existing loop as an afterthought.
Reference: [21] <author> Nils A. Nieuwejaar. </author> <title> Galley: A New Parallel File System For Scientic Applications. </title> <type> PhD thesis, </type> <institution> Dartmouth College, </institution> <year> 1996. </year>
Reference-contexts: Although applications may interact directly with Galley's interface, we expect that most applications will use a higher-level library or language layered on top of the Galley run-time library. One such library implements a Unix-like file model, which should reduce the effort required to port legacy applications to Galley <ref> [21] </ref>. <p> One example of such an application is given in <ref> [21] </ref>. For those applications, we provide a nested-batched interface. <p> The sub-request can be a simple data transfer (in the case of a standard or a simple-strided request), or it can be a vector of gfs batch structures (in the case of a nested-strided, or more complex request). An example of when this interface is useful is shown in <ref> [21] </ref>. 5.4 List Requests Finally, in addition to these structured operations, Galley provides a simple, more general file interface, called the list interface, which has functionality similar to the POSIX lio listio () interface [11].
Reference: [22] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Galley's DiskManager does not attempt to prefetch data for two reasons. First, indiscriminate prefetching can cause thrashing in the buffer cache <ref> [22] </ref>. Second, prefetching is based on the assumption that the system can intelligently guess what an application is going to request next.
Reference: [23] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: The drawback of this approach is that most multiprocessor file systems use a declustering unit size measured in kilobytes (e.g., 4 KB in Intel's CFS <ref> [23] </ref>), but our workload characterization studies show that the typical request size in a parallel application is much smaller: frequently under 200 bytes [20].
Reference: [24] <author> Terrence W. Pratt, James C. French, Phillip M. Dickens, and Stanley A. Janet, Jr. </author> <title> A comparison of the architecture and performance of two parallel file systems. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 161-166, </pages> <year> 1989. </year>
Reference: [25] <author> Apratim Purakayastha, Carla Schlatter Ellis, David Kotz, Nils Nieuwejaar, and Michael Best. </author> <title> Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <pages> pages 165-172, </pages> <month> April </month> <year> 1995. </year>
Reference: [26] <author> Chris Ruemmler and John Wilkes. </author> <title> An introduction to disk drive modeling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: To validate our model, we used a trace-driven simulation, using data provided by Hewlett-Packard and used by Ruemmler and Wilkes in their study. 2 Comparing the results of this trace-driven simulation with the measured results from the actual disk, we obtained a demerit figure (see <ref> [26] </ref> for a discussion of this measure) of 5.0%, indicating that our model was extremely accurate. The simulated disk is integrated into Galley by creating a new thread on each IOP to execute the simulation.
Reference: [27] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year> <month> 34 </month>
Reference: [28] <author> K. E. Seamons and M. Winslett. </author> <title> A data management approach for handling large compressed arrays in high performance computing. </title> <booktitle> In Proceedings of the Seventh Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 119-128, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: In addition to storing data in the traditional sense, many libraries also need to store persistent, library-specific `metadata' independently of the data proper. One example of such a library would be a compression library similar to that described in <ref> [28] </ref>, which compresses a data file in multiple independent chunks. Such a library 5 could store the compressed data chunks in one fork and index information in another. Another instance where this type of file structure may be useful is in the problem of genome-sequence comparison.
Reference: [29] <author> Margo Seltzer, Peter Chen, and John Ousterhout. </author> <title> Disk scheduling revisited. </title> <booktitle> In Proceedings of the 1990 Winter USENIX Conference, </booktitle> <pages> pages 313-324, </pages> <year> 1990. </year>
Reference-contexts: The DiskManager maintains a list of blocks that the CacheManager has requested to be read or written. As new requests arrive from the CacheManager, they are placed into the list according to the disk scheduling algorithm. The DiskManager currently uses a Cyclical Scan algorithm <ref> [29] </ref>. When using either simulated disks or raw devices, this disk scheduling helps deliver high performance.
Reference: [30] <author> Joel T. Thomas. </author> <title> The Panda array I/O library on the Galley parallel file system. </title> <type> Technical Report PCS-TR96-288, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> June </month> <year> 1996. </year> <note> Senior Honors Thesis. </note>
Reference: [31] <author> Sivan Toledo and Fred G. Gustavson. </author> <title> The design and implementation of SOLAR, a portable library for scalable out-of-core linear algebra computations. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 28-40, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: One such example is the two-dimensional, cyclically-shifted block layout scheme for matrices, shown in Figure 1, which was designed for SOLAR, a portable, out-of-core linear-algebra library <ref> [31] </ref>. This data layout is intended to efficiently support a wide variety of out-of-core algorithms. In particular, it allows blocks of rows and columns to be transferred efficiently, as well as square or nearly-square submatrices. Fig. 1. An example of a 2-dimensional, cyclically-shifted block layout, as described in [31]. <p> linear-algebra library <ref> [31] </ref>. This data layout is intended to efficiently support a wide variety of out-of-core algorithms. In particular, it allows blocks of rows and columns to be transferred efficiently, as well as square or nearly-square submatrices. Fig. 1. An example of a 2-dimensional, cyclically-shifted block layout, as described in [31]. In this example there are 6 disks, logically arranged into a 2-by-3 grid, and a 6-by-12 block matrix. The number in each square indicates the disk on which that block is stored.
Reference: [32] <author> Stephen R. Wheat, Arthur B. Maccabe, Rolf Riesen, David W. van Dresser, and T. Mack Stallcup. PUMA: </author> <title> An operating system for massively parallel systems. </title> <booktitle> In Proceedings of the Twenty-Seventh Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 56-65, </pages> <year> 1994. </year> <month> 35 </month>
Reference-contexts: Other multiprocessor file systems with this 29 Fig. 14. Increase in throughput for write requests using the strided interface when overwriting an existing file. style of interface are SUNMOS and its successor, PUMA <ref> [32] </ref>, sfs [15], and CMMD [3]. Like the systems mentioned above, PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface [10].
References-found: 32

