URL: http://www.cs.indiana.edu/hyplan/kksiazek/mypapers/clu.ps.Z
Refering-URL: http://www.cs.indiana.edu/hyplan/kksiazek/pardis.html
Root-URL: http://www.cs.indiana.edu
Email: E-mail: keahey@cs.indiana.edu  
Title: Cluster  Developing and Evaluating Abstractions for Distributed Supercomputing  
Author: Katarzyna Keahey Dennis Gannon 
Address: 215 Lindley Hall Bloomington, IN 47405  
Affiliation: Department of Computer Science Indiana University  
Date: 0 (1997) 1  
Pubnum: Computing  
Abstract: To fully realize its potential, distributed supercomputing requires abstractions and environments facilitating development of efficient applications. In this paper we present PARDIS, a system which addresses this demand by providing support for interoperability of PARallel DIStributed applications. The design of PARDIS is based on the Common Object Request Broker Architecture (CORBA). Like CORBA, it provides interoperability between heterogeneous components by specifying their interfaces in a meta-language, the CORBA IDL, which can be translated into the language of interacting components. However, PARDIS extends the CORBA object model by introducing SPMD objects representing data-parallel computations. This extension allows us to build interactions involving data-parallel components, which exchange distributed data structures whose definitions are captured by distributed sequences. We present microbench-mark results which evaluate the performance potential of SPMD objects for data structures of diverse complexity and different network configurations. Based on these results, we conclude that while encapsulating the existence of multiple interactions SPMD objects also allow their efficient utilization, and therefore constitute a useful abstraction. Keywords: parallel, distributed, heterogeneous, metacomputing, CORBA, interoperability 
Abstract-found: 1
Intro-found: 1
Reference: [ABC + 95] <author> S. Atlas, S. Banerjee, J.C. Cummings, P. J. Hinker, M. Srikant, J. V. W. Reynders, and M. Tholburn, POOMA: </author> <title> A High Performance Distributed Simulation Environment for Scientific Applications, </title> <booktitle> Supercomputing '95 Proceedings, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: and to fully use the advantage of meta-language definitions the sequence should map directly to to constructs present in the programmer's package. [KG97b] describes our experiments with providing a direct mapping of the distributed sequence to the distributed vector of HPC++ [GBJ + ar] and the field in the POOMA <ref> [ABC + 95] </ref> library. 2.3. General Design Components of PARDIS PARDIS is a distributed software system consisting of an IDL compiler, communication libraries, object repository databases and facilities responsible for locating and activating objects. The relationship between these components is depicted in figure 1.
Reference: [BG96] <author> P. Beckman and D. Gannon, Tulip: </author> <title> A Portable Run-Time System for Object-Parallel Systems, </title> <booktitle> Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996, </year> <pages> pp. 532-536. </pages>
Reference-contexts: Keahey, D. Gannon / Developing and Evaluating Abstractions for Distributed Supercomputing areas in the picture denote the PARDIS run-time system inter face. terface has been specified; it encompasses the functionality of message-passing libraries and has been tested using applications based on MPI [For95] and the Tulip <ref> [BG96] </ref> run-time system. In the future PARDIS will provide an alternative run-time system interface capturing the functionality of the more flexible one-sided run-time systems. 3.
Reference: [BWF + 96] <author> F. Berman, R. Wolski, S. Figueira, J. Schopf, and G. Shao, </author> <title> Application-Level Scheduling on Distributed Heterogeneous Networks, </title> <booktitle> Supercomputing '96 Proceedings, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: While our results are useful for small-scale proprietary networks and as a best performance estimate, they will not satisfy an application developer faced with heavy traffic of wide area networks. A good understanding of performance prediction [FB96] and performance related scheduling <ref> [BWF + 96] </ref> in this environment is essential. Parallel data transfer between data-parallel computation units has been investigated before; [FKKC96] describes how an MPI-based approach can extend HPF to combine task and data parallelism and presents its performance results on IBM SP/2.
Reference: [DFP + 96] <author> T. DeFanti, I. Foster, M. Papka, R. Stevens, and T. Kuhfuss, </author> <title> Overview of the I-Way: Wide-Area Visual Supercomputing, </title> <booktitle> The International Journal of Supercomputer Applications and High Performance Computing 10 (1996), </booktitle> <volume> no. 2, </volume> <pages> 123-131. </pages>
Reference-contexts: These applications make use of the combined computational power of several resources to increase their performance, and exploit the heterogeneity of diverse architectures and software systems by assigning selected tasks to platforms which can best support them. Experiences of the I-WAY <ref> [DFP + 96] </ref> networking experiment demonstrated that this way of approaching high-performance computing has enormous potential for solving important scientific problems. <p> Related Work and Discussion The I-WAY <ref> [DFP + 96] </ref> networking experiment combined many systems which proved that distributed supercomputing has high potential for developing unique and efficient applications. However, building those applications and environments supporting their development requires the understanding of many issues. In this section we will outline ongoing research addressing 12 K. Keahey, D.
Reference: [FB96] <author> S. M. Figueira and F. Berman, </author> <title> Modeling the Effects of Contention on the Performance of Heterogeneous Applications, </title> <booktitle> Proceedings of the 5th IEEE International Symposium on High Performance Distributed Computation, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: The experiments presented in this paper were run on dedicated networks. While our results are useful for small-scale proprietary networks and as a best performance estimate, they will not satisfy an application developer faced with heavy traffic of wide area networks. A good understanding of performance prediction <ref> [FB96] </ref> and performance related scheduling [BWF + 96] in this environment is essential. Parallel data transfer between data-parallel computation units has been investigated before; [FKKC96] describes how an MPI-based approach can extend HPF to combine task and data parallelism and presents its performance results on IBM SP/2.
Reference: [FGKT96] <author> I. Foster, J. Geisler, C. Kesselman, and S. Tuecke, </author> <title> Multimethod Communication for High-Performance Metacomputing Applications, </title> <booktitle> Supercomputing '96 Proceedings, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: The TAO project [SGHP97] focuses on developing a high-performance, real-time ORB providing quality of service guarantees, optimizing the performance of network interfacing and ORB components. Another interesting approach to distributed supercomputing is offered by multi-method run-time systems, such as Nexus <ref> [FGKT96] </ref> and Horus [vRBF + 95].
Reference: [FK97] <author> I. Foster and C. Kesselman, Globus: </author> <title> A Metacomput-ing Infrastructure Toolkit, </title> <booktitle> The International Journal of Supercomputer Applications and High Performance Computing 11 (1997), </booktitle> <volume> no. 2, </volume> <pages> 115-128. </pages>
Reference-contexts: In this section we will outline ongoing research addressing 12 K. Keahey, D. Gannon / Developing and Evaluating Abstractions for Distributed Supercomputing these problems and relate it to our work. Metacomputing systems and environments such as Legion [GW96] and Globus <ref> [FK97] </ref> unite efforts investigating problems of resource configuration and management, security, scheduling, fault tolerance and similar issues related to negotiating participation of high-performance resources in a distributed system.
Reference: [FKKC96] <author> I. Foster, D. Kohr, R. Krishnaiyer, and A. Choud-hary, </author> <title> Double Standards: Bringing Task Parallelism to HPF via the Message Passing Interface, </title> <booktitle> Supercomputing '96 Proceedings, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: A good understanding of performance prediction [FB96] and performance related scheduling [BWF + 96] in this environment is essential. Parallel data transfer between data-parallel computation units has been investigated before; <ref> [FKKC96] </ref> describes how an MPI-based approach can extend HPF to combine task and data parallelism and presents its performance results on IBM SP/2. Our approach is intended to provide interoperability in a distributed environment, and consequently the experiments we present were conducted between heterogeneous remote multiprocessors.
Reference: [FKOT94] <author> I. Foster, C. Kesselman, R. Olson, and S. Tuecke, </author> <title> Nexus: An Interoperability Layer for Parallel and Distributed Computer Systems, </title> <note> Technical Memorandum ANL/MCS-TM-189 (1994). </note>
Reference-contexts: Refer to <ref> [FKOT94] </ref> for details on Nexus implementation. 3.3. Results We will now present data obtained in four experiments evaluating argument transfer in different conditions. The first three experiments were conducted on SGI multi-processors connected by a single network connection; we varied the type of the elements of the distributed sequence.
Reference: [For95] <author> Message Passing Interface Forum, </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> June </month> <year> 1995. </year>
Reference-contexts: Keahey, D. Gannon / Developing and Evaluating Abstractions for Distributed Supercomputing areas in the picture denote the PARDIS run-time system inter face. terface has been specified; it encompasses the functionality of message-passing libraries and has been tested using applications based on MPI <ref> [For95] </ref> and the Tulip [BG96] run-time system. In the future PARDIS will provide an alternative run-time system interface capturing the functionality of the more flexible one-sided run-time systems. 3. <p> All of our experiments were run on dedicated networks. The client and the server were relying on the MPICH [GLDS96] (v 1.0.12) implementation of MPI <ref> [For95] </ref> for their internal communication; all of our experiments were based on a distributed memory model.
Reference: [GBJ + ar] <author> D. Gannon, P. Beckman, E. Johnson, T. Green, and M. Levine, </author> <title> HPC++ and the HPC++Lib Toolkit, Languages, Compilation Techniques and Run Time Systems (Recent Advances and Future Perspectives) (1997 (To appear)). </title>
Reference-contexts: Based on this specification, the IDL compiler will generate the client's and the server's stubs translating the definitions above into the language of the package in which the client and server are implemented (for example HPC++ <ref> [GBJ + ar] </ref>). The stub code contains calls to communication libraries provided by PARDIS. Linked to the object's implementation, it allows the request broker to invoke methods on the object; the client can use the stub code to invoke methods on remote objects. <p> been developed to capture abstractions associated with parallel programming and to fully use the advantage of meta-language definitions the sequence should map directly to to constructs present in the programmer's package. [KG97b] describes our experiments with providing a direct mapping of the distributed sequence to the distributed vector of HPC++ <ref> [GBJ + ar] </ref> and the field in the POOMA [ABC + 95] library. 2.3. General Design Components of PARDIS PARDIS is a distributed software system consisting of an IDL compiler, communication libraries, object repository databases and facilities responsible for locating and activating objects.
Reference: [GLDS96] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum, </author> <title> A High Performance, Portable Implementation of the MPI Message Passing Interface Standard, </title> <type> Tech. Report ANL/MCS-TM-213, </type> <institution> Argonne National Laboratory, </institution> <year> 1996. </year>
Reference-contexts: All of our experiments were run on dedicated networks. The client and the server were relying on the MPICH <ref> [GLDS96] </ref> (v 1.0.12) implementation of MPI [For95] for their internal communication; all of our experiments were based on a distributed memory model.
Reference: [GW96] <author> A. S. Grimshaw and W. A. Wulf, </author> <title> Legion | A View From 50,000 Feet, </title> <booktitle> Proceedings of the 5th IEEE International Symposium on High Performance Distributed Computation, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: In this section we will outline ongoing research addressing 12 K. Keahey, D. Gannon / Developing and Evaluating Abstractions for Distributed Supercomputing these problems and relate it to our work. Metacomputing systems and environments such as Legion <ref> [GW96] </ref> and Globus [FK97] unite efforts investigating problems of resource configuration and management, security, scheduling, fault tolerance and similar issues related to negotiating participation of high-performance resources in a distributed system.
Reference: [KG97a] <author> K. Keahey and D. Gannon, PARDIS: </author> <title> A Parallel Approach to CORBA, </title> <booktitle> Proceedings of the 6th IEEE International Symposium on High Performance Distributed Computation, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: The results for multi-port method show that invocation times have an overall decreasing tendency with increasing n and m. In <ref> [KG97a] </ref> we have examined the different cost components of the invocation time and shown that in the presence of more than one active connection between client and server the sends are interleaved so that more than one communication operation can be active at the same time.
Reference: [KG97b] <author> K. Keahey and D. Gannon, PARDIS: </author> <title> CORBA-based Architecture for Application-Level Parallel Distributed Computation, </title> <booktitle> Supercomputing '97 Proceedings, </booktitle> <month> November </month> <year> 1997. </year>
Reference-contexts: PARDIS also allows the server to interrupt its computation in order to process outstanding requests; full discussion of these capabilities is beyond the scope of this paper, for details refer to <ref> [KG97b] </ref>. Principles applied in this simple scenario can be used to construct more complex interactions composed of multiple parallel applications, as well as units visualizing or otherwise monitoring their progress (see [KG97b] for an examples). <p> process outstanding requests; full discussion of these capabilities is beyond the scope of this paper, for details refer to <ref> [KG97b] </ref>. Principles applied in this simple scenario can be used to construct more complex interactions composed of multiple parallel applications, as well as units visualizing or otherwise monitoring their progress (see [KG97b] for an examples). Interoperability with CORBA will eventually enable PARDIS to integrate many existing systems based on this technology. 2.2. <p> Many object-oriented systems have been developed to capture abstractions associated with parallel programming and to fully use the advantage of meta-language definitions the sequence should map directly to to constructs present in the programmer's package. <ref> [KG97b] </ref> describes our experiments with providing a direct mapping of the distributed sequence to the distributed vector of HPC++ [GBJ + ar] and the field in the POOMA [ABC + 95] library. 2.3.
Reference: [NBB + 96] <author> M. L. Norman, P. Beckman, G. L. Bryan, J. Du-binski, D. Gannon, L. Hernquist, K. Keahey, J. P. Ostriker, J. Shalf, J. Welling, and S. Yang, </author> <title> Galaxies Collide on the I-WAY: An Example of Heterogeneous Wide-Area Collaborative Supercomputing, </title> <booktitle> The International Journal of Supercomputer Applications and High Performance Computing 10 (1996), </booktitle> <volume> no. 2, </volume> <pages> 132-144. </pages>
Reference-contexts: High performance applications composed of many distributed, heterogeneous components have previously been developed in an ad hoc fashion trying to explicitly combine different communication libraries and languages and developing special-case tools <ref> [NBB + 96] </ref>. Systems constructed in this way usually require extensive modifications to the original application code and result in software which is difficult to maintain and reuse. Implementing these systems requires sub 2 K. Keahey, D.
Reference: [OEPW96] <author> W.G. O'Farrell, F. Ch. Eigler, S. D. Pullara, and G. V. Wilson, </author> <title> Parallel Programming Using C++, ch. ABC++, </title> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: In the case of both the client and the server the generated stub code contains all the code necessary to perform argument marshaling. As the example of the client's stub shows, PAR-DIS supports non-blocking invocations returning futures (similar to ABC++ futures <ref> [OEPW96] </ref>) as its "out" arguments. This allows the client to use remote resources concurrently with its own, and provides the programmer with an elegant way of representing results which are not yet available.
Reference: [OMG95] <author> OMG, </author> <title> The Common Object Request Broker: Architecture and Specification. Revision 2.0, OMG Document, </title> <month> June </month> <year> 1995. </year>
Reference-contexts: Experiences of the I-WAY [DFP + 96] networking experiment demonstrated that this way of approaching high-performance computing has enormous potential for solving important scientific problems. At the same time another development in distributed object-oriented technology, the Common Object Request Broker Architecture (CORBA) <ref> [OMG95] </ref> has made it possible to seamlessly integrate heterogeneous distributed objects within one system. CORBA provides interoperability between different components by specifying their interfaces in a meta-language, the CORBA Interface Definition Language (IDL), which is translated into the language of interacting components by a compiler. <p> This kind of interaction can be useful to parallel clients which want to interact in parallel with multiple distributed objects, possibly encapsulating data parallel computations. On the server's side, PARDIS uses the mechanism of the CORBA C++ mapping through inheritance <ref> [OMG95] </ref> to invoke operations on the object. All the programmer of the server needs to do is provide the implementation of an object computing diffusion simulation and instantiate that object. <p> Similarly, the local access operations can be used to convert a sequence to the programmer's memory management scheme. An "in" argument on the client's side must set the length and distribution of a distributed sequence before it can be used. An "out" argument (represented as a managed type <ref> [OMG95] </ref>) should be initialized by a distribution template before calling the operation which returns it; otherwise a uniform blockwise distribution will be assumed. The distribution of return values is always assumed to be blockwise.
Reference: [SGHP97] <author> D. Schmidt, A. Gokhale, T. Harrison, and G. Parulkar, </author> <title> A High-performance Endsystem Architecture for Real-time CORBA, </title> <journal> IEEE Communications Magazine 14 (1997), </journal> <volume> no. </volume> <pages> 2. </pages>
Reference-contexts: Our research if focused on investigating ways of modifying CORBA to include support for parallel computation. Active research is also being done on optimizing the performance of CORBA itself for high-speed networks. The TAO project <ref> [SGHP97] </ref> focuses on developing a high-performance, real-time ORB providing quality of service guarantees, optimizing the performance of network interfacing and ORB components. Another interesting approach to distributed supercomputing is offered by multi-method run-time systems, such as Nexus [FGKT96] and Horus [vRBF + 95].
Reference: [vRBF + 95] <author> R. van Renesse, K. P. Birman, R. Friedman, M. Hay-den, and D. A. Karr, </author> <title> A Framework for Protocol Composition in Horus, </title> <booktitle> Proceedings of Principles of Distributed Computing, </booktitle> <year> 1995. </year>
Reference-contexts: The TAO project [SGHP97] focuses on developing a high-performance, real-time ORB providing quality of service guarantees, optimizing the performance of network interfacing and ORB components. Another interesting approach to distributed supercomputing is offered by multi-method run-time systems, such as Nexus [FGKT96] and Horus <ref> [vRBF + 95] </ref>.
References-found: 20

