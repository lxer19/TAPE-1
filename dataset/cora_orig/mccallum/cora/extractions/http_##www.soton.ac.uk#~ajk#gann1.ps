URL: http://www.soton.ac.uk/~ajk/gann1.ps
Refering-URL: http://www.soton.ac.uk/~ajk/opt/welcome.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: step@bsdi.iem.pw.edu.pl  andy.keane@soton.ac.uk  
Title: Pruning backpropagation neural networks using modern stochastic optimization techniques  
Author: Slawomir W. Stepniewski Andy J. Keane 
Keyword: optimization, neural network, pruning, genetic algorithm, simulated annealing.  
Address: ul. Koszykowa 75, 00-662 Warszawa, Poland  Highfield, Southampton, SO17 1BJ, U.K.  
Affiliation: Department of Electrical Engineering (IETiME) Warsaw University of Technology  Department of Mechanical Engineering University of Southampton  
Abstract: Approaches combining genetic algorithms and neural networks have received a great deal of attention in recent years. As a result, much work has been reported in two major areas of neural network design: training and topology optimization. This paper focuses on the key issues associated with the problem of pruning a multilayer perceptron using genetic algorithms and simulated annealing. The study presented considers a number of aspects associated with network training that may alter the behavior of a stochastic topology optimizer. Enhancements are discussed that can improve topology searches. Simulation results for the two mentioned stochastic optimization methods applied to nonlinear system identification are presented and compared with a simple random search. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bck T., Hoffmeister F., </author> <title> Extended Selection Mechanisms in Genetic Algorithms, </title> <booktitle> Proc. 4th Int. Conf. on Genetic Algorithms, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991, </year> <pages> pp. 92-99. </pages>
Reference-contexts: One may expect that some optimization strategy that is more sophisticated than a random search works, but only comparison with the latter reveals if this initial hypothesis is correct. Terminology used to describe selection follows definitions presented by Bck and Hoffmeister in <ref> [1] </ref>. 13 0 2 4 6 8 50 150 250 unpruned network average fitness 3.46 fitness function (eq. 4) number of networks pruning a fully connected MLP 2-8-8-8-1.
Reference: [18] <author> Kuscu I., Thornton C., </author> <title> Design of Artificial Neural Networks Using Genetic Algorithms: </title> <journal> review and prospect, Cognitive and Computing Sciences, </journal> <note> University of Sussex, </note> <author> 1994.[2] Beaufays F., Abdel-Mogid Y., Widrow B., </author> <title> Appication of Neural Networks to Load-Frequency Control in Power Systems, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 7., No. 1., </volume> <year> 1994, </year> <pages> pp. 183-194. </pages>
Reference-contexts: The methods often differ in the way they encode solutions and also sample the search space. A more comprehensive discussion of genetic algorithms applied to designing neural networks is carried out in the retrospective paper <ref> [18] </ref>. In this paper we focus attention on reducing the size of neural networks (without compromising network generalization ability or learning accuracy) using stochastic optimization techniques. We investigate the problem of pruning feedforward neural networks that are used to carry out nonlinear identification.
Reference: [19] <author> Miller G.F., Todd P.M., Hegde S.U., </author> <title> Designing neural networks using genetic algorithms, </title> <booktitle> Proc. 3rd Int. Conf. on Genetic Algorithms, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1989, </year> <pages> pp. </pages> <note> 379-384.[3] Bhat V.H., Minderman A.P., </note> <author> McAvoy T., Wang N. S., </author> <title> Modeling Chemical Process Systems via Neural Computation, </title> <journal> IEEE Control Systems Magazine, </journal> <month> April </month> <year> 1990, </year> <pages> pp. 24-29 </pages>
Reference-contexts: These methods are additionally compared to a simple random search. As is shown later in this paper, such a comparison provides a rather severe benchmark for the methods under investigation. The methodology presented here is similar to that described in <ref> [19] </ref> and [26], however, this study focuses on solving another, distinct class of problems. In the cited reports, topology optimization was tested on such tasks as the boolean XOR function, four-quadrant classification or the binary adder.
Reference: [20] <author> Naidu R.S., Zafiriou E., McAvoy T.J., </author> <title> Use of Neural Networks for Sensor Failure Detection in a Control System, </title> <journal> IEEE Control Systems Magazine, </journal> <month> April </month> <year> 1990, </year> <pages> pp. </pages> <editor> 49-55.[4] Billings S.A., Jamaluddin H.B., Chen S., </editor> <title> Properties of neural networks with applications to modeling nonlinear dynamical systems, </title> <journal> Int. J. Control, 1992, </journal> <volume> Vol. 55, No. 1, </volume> <pages> pp. 193-224. </pages>
Reference-contexts: 1. INTRODUCTION e.g., process age, oxygen consumption, carbon dioxide production, etc. Besides these basic plant models, other specially designed neural networks may operate as sensor/actuator failure detectors <ref> [20] </ref> or even function as direct controllers when a stable, inverse plant model exists [14]. Feedforward networks are perhaps the most commonly used among all types of neural networks.
Reference: [21] <author> Narendra K. S., Parthasarathy K., </author> <title> Identyfication and Control of Dynamical Systems Using Neural Networks, </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> Vol. 1, No. 1, </volume> <year> 1990, </year> <title> pp. 4-27.[5] Bishop C.M., Novelty Detection and Neural Network Validation, </title> <booktitle> IEE Proc.: Vision, Image and Signal Processing, </booktitle> <volume> 141 (4), </volume> <pages> pp. 217-222. </pages>
Reference-contexts: Neural models may be used in a number of ways in control systems. They may be employed as multi-step ahead predictors in optimal control systems [9, 14] or in transportation delay compensators (Smith predictors). One step-ahead neural predictors can be employed in reference model control schemes or related approaches <ref> [21, 14] </ref>. In some cases neural models function as Jacobian approximators of the controlled plant [2, 25]. Such Jacobians are required, for example, when the controller is also built using neural network techniques and needs to be trained while reflecting the influence of the plant.
Reference: [22] <author> Reed R., </author> <title> Pruning Algorithms - A Survey, </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 4, No. 5, </volume> <year> 1993. </year>
Reference-contexts: A series of methods that attempt to shrink or prune some given original architecture are described in <ref> [22] </ref>. For this 1 task, one possible approach is to estimate the importance of each individual connection or whole node inside the network and then gradually to remove unnecessary components. A comparison of two such algorithms, Optimal Brain Damage (OBD) and skeletonization is covered for example in [8]. <p> A more general approach than OBD is called Optimal Brain Surgeon (OBS) and is presented in [12]. Reduction in network size can be also combined with the process of network training. Weight decay and weight elimination approaches <ref> [13, 22] </ref> modify the learning law in such a way that it tends to lower the absolute values of connection weights so the weakest synapses can be removed if identified as being redundant. cost function used for topology optimization need not be the same as that used for the training phase. <p> These effects are especially problematic in 'wide' network architectures due to so called derivative dilution. In contrast to stochastic optimizers, OBD or OBS attempt to remove one link at a time. As was noted in <ref> [22] </ref> such techniques may not estimate correctly the importance of some configuration of connections such as where two weights cancel each other exactly.
Reference: [6] <author> Chen S., Billings A., </author> <title> Neural Networks for nonlinear dynamic system modeling and identification, </title> <journal> Int. J. Control, </journal> <volume> Vol. 56, No. 2, </volume> <year> 1992, </year> <pages> pp. 319-346. </pages>
Reference: [23] <author> Schiffmann W., Joost M., Werner R., </author> <title> Optimization of the Backpropagation Algorithm for Training Multilayer Perceptrons, </title> <type> Technical Report, </type> <note> University of Koblenz, </note> <author> 1992.[7] Cichocki A., Unbehauen R., </author> <title> Neural Networks for Optimization and Signal Processing, 4th ed., </title> <publisher> Wiley & Sons, </publisher> <year> 1994. </year>
Reference-contexts: This reduction in problem dimensionality significantly eases the learning process. feedforward network with mixed configuration of biased and unbiased nodes. 6. NETWORK TRAINING Here, neural networks are trained in a batch mode using the RPROP <ref> [23, 7] </ref> algorithm. This method individually adjusts each weight according to the sign of the weight gradients evaluated during the two most recent iterations. The RPROP algorithm used in our experiments may be summarized as follows: 1. <p> Although this modified gradient evaluation has been used in conjunction with the RPROP algorithm, the same modification may be applied to virtually any training paradigm that utilizes gradients. Our brief experiments indicate that such training methods as SuperSAAB, Delta-BarDelta, Silvia-Almeida or adaptive learning with momentum <ref> [23, 7] </ref> are stable and work well with this modification. Typically, at the beginning of training, the validation error oscillates rapidly. Later, the training process stabilizes and the changes in validation error become smaller.
Reference: [24] <editor> Stpniewski S.W., Keane A.J., </editor> <title> Topology design of feedforward neural networks by genetic algorithms, </title> <editor> (submitted).[8] Eigel-Danielson V., Augustejin M. F., </editor> <title> Neural Network Pruning and its Effect on Generalization - some experimental results, </title> <booktitle> Neural Parallel & Scientific Computation 1, </booktitle> <year> 1993, </year> <pages> pp. 59-70. </pages>
Reference-contexts: Our experiments with stochastic optimizers employed to design neural networks show that using different random starting points for training a given network at each stage of the topology search severely deceives such methods <ref> [24] </ref>. Under these circumstances even the best architecture found so far by a topology optimizer may perform quite indifferently when trained in the next generation from another starting point. <p> The second parameter R ( R 1 ) may be also used to scale the network performance evaluation. In some cases setting these parameters to other values than assumed here as default ( R = 1, S = 1) may considerably improve the efficentcy of the topology search <ref> [24] </ref>. In some cases, especially when the initial, unpruned network is small, this penalty factor may be difficult to adjust precisely using the parameter S. The penalty function may still overly reward small networks and 12 remove too many links.
Reference: [25] <author> Wang D., Chai T., </author> <title> Multivariable Adaptive Control of Unknown Nonlinear Dynamic Systems Using Neural Networks, </title> <booktitle> Proc. 33rd Conf. on Decision and Control, </booktitle> <address> Lake Buena Vista, FL, </address> <year> 1994, </year> <pages> pp. 2500-2505. </pages>
Reference-contexts: One step-ahead neural predictors can be employed in reference model control schemes or related approaches [21, 14]. In some cases neural models function as Jacobian approximators of the controlled plant <ref> [2, 25] </ref>. Such Jacobians are required, for example, when the controller is also built using neural network techniques and needs to be trained while reflecting the influence of the plant.
Reference: [9] <author> Evans, J.T., Gomm J.B., Williams D. ,Lisboa P.J.B., </author> <title> To Q.S., A practical application of neural modelling and predictive control, chapter 6 in Application of Neural Networks to Modelling and Control, Page G.F, </title> <editor> Gomm J.B., Williams D. (eds.), </editor> <publisher> Chapman & Hall, </publisher> <year> 1994 </year>
Reference-contexts: Neural models may be used in a number of ways in control systems. They may be employed as multi-step ahead predictors in optimal control systems <ref> [9, 14] </ref> or in transportation delay compensators (Smith predictors). One step-ahead neural predictors can be employed in reference model control schemes or related approaches [21, 14]. In some cases neural models function as Jacobian approximators of the controlled plant [2, 25].
Reference: [26] <author> Whitley D., Starkweather T., Bogart C., </author> <title> Genetic Algorithms and Neural Networks: optimizing connections and connectivity, </title> <booktitle> Parallel Computing 14, </booktitle> <year> 1990, </year> <pages> pp. 347-361. </pages>
Reference-contexts: These methods are additionally compared to a simple random search. As is shown later in this paper, such a comparison provides a rather severe benchmark for the methods under investigation. The methodology presented here is similar to that described in [19] and <ref> [26] </ref>, however, this study focuses on solving another, distinct class of problems. In the cited reports, topology optimization was tested on such tasks as the boolean XOR function, four-quadrant classification or the binary adder. <p> In our approach we label each node with an integer number. This number is then used to sort nodes belonging to each layer. Using a modified initial architecture may be also beneficial in other ways. For example, it was observed <ref> [26] </ref> that if direct connections exist between the network input and output, the process of training was usually substantially shorter and therefore the whole pruning procedure may be completed in less time.
Reference: [10] <author> Fahlman S. E., Lebiere C., </author> <title> The cascade-correlation learning architecture, </title> <type> Technical Report CMU-CS-90-100, </type> <institution> Carnegie Mellon University, </institution> <year> 1990. </year>
Reference-contexts: A GA method can also be very easily distributed among several processors or computers as it operates on populations of solutions that may be evaluated concurrently. Probably the most well known example of a method that utilizes the expansion strategy is cascade correlation, designed by Falhman et al. <ref> [10] </ref>. This method progressively adds hidden nodes until the output error of the network decreases to a desired level.
Reference: [27] <author> Willis M.J., Massimo C., </author> <title> Montague G.A, Tham M.T., </title> <booktitle> Morris A.J., Artificial neural networks in process engineering, IEE Proc. Pt. D, </booktitle> <volume> Vol. 138, No. 3, </volume> <year> 1991, </year> <pages> pp. </pages> <editor> 256-266.[11] Goldberg, D. E., </editor> <title> Genetic Algorithms in Search, Optimization and Machine Learning, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Neural models may also be used to estimate otherwise unavailable data for use by external controllers. Such problems occur, for instance, in technological processes that involve biomass fermentation <ref> [27] </ref>. In such cases, laboratory analyses introduce delays in sampling and are performed over such long intervals that they may cause operability problems for a given control system.

References-found: 14

