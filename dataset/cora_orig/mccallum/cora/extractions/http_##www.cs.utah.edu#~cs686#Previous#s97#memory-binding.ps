URL: http://www.cs.utah.edu/~cs686/Previous/s97/memory-binding.ps
Refering-URL: http://www.cs.utah.edu/~cs686/Previous/s97/
Root-URL: 
Email: Email: zzhang,torrella@csrd.uiuc.edu  
Title: Speeding up Irregular Applications in Shared-Memory Multiprocessors: Memory Binding and Group Prefetching 1  
Author: Zheng Zhang and Josep Torrellas 
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: While many parallel applications exhibit good spatial locality, other important codes in areas like graph problem-solving or CAD do not. Often, these irregular codes contain small records accessed via pointers. Consequently, while the former applications benefit from long cache lines, the latter prefer short lines. One good solution is to combine short lines with prefetching. In this way, each application can exploit the amount of spatial locality that it has. However, prefetching, if provided, should also work for the irregular codes. This paper presents a new prefetching scheme that, while usable by regular applications, is specifically targeted to irregular ones: Memory Binding and Group Prefetching. The idea is to hardware-bind and prefetch together groups of data that the programmer suggests are strongly related to each other. Examples are the different fields in a record or two records linked by a permanent pointer. This prefetching scheme, combined with short cache lines, results in a memory hierarchy design that can be exploited by both regular and irregular applications. Overall, it is better to use a system with short lines (16-32 bytes) and our prefetching than a system with long lines (128 bytes) with or without our prefetching. The former system runs 6 out of 7 Splash-class applications faster. In particular, some of the most irregular applications run 25-40% faster. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. L. Baer and T. F. Chen. </author> <title> An Effective On-Chip Preloading Scheme to Reduce Data Access Penalty. </title> <booktitle> In Supercomputing '91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: If prefetching support is provided, however, we would like it to work for both regular and irregular applications. Unfortunately, it is hard for prefetching to work well for our irregular applications. On the one hand, traditional prefetching hardware is effective for fixed-stride access patterns only <ref> [1, 4] </ref>. Therefore, it will not work very well for irregular, pointer-based codes. On the other hand, purely software-based prefetching driven by the compiler [14] does not seem applicable either: the compiler analysis required is very complex. Finally, software-based prefetching with hand-inserted prefetches is a possibility. <p> Unfortunately, the processor itself is likely to dereference P1 very soon and, therefore, not much latency will be hidden. This problem occurs when prefetching irregular pointer structures; regular access patterns can be easily learned and the data structures prefetched in advance <ref> [1] </ref>. Instead, to use off-the-shelf processors and to generate the prefetches earlier, we place the prefetcher in the memory system. More specifically, the prefetching logic is integrated in the network processors and in the mechanism that performs L1 Pipelined Prefetching between the secondary and primary caches. <p> In a network with less bandwidth, long lines would perform relatively worse. 6 Comparison to Other Prefetch ing Schemes There has been much work on data prefetching, both hardware-based <ref> [1, 6, 8, 11] </ref> and software-based [2, 5, 10, 12, 13, 14]. Typically, hardware-based schemes only work well for data structures that are accessed with a fixed stride.
Reference: [2] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software Prefetching. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: In a network with less bandwidth, long lines would perform relatively worse. 6 Comparison to Other Prefetch ing Schemes There has been much work on data prefetching, both hardware-based [1, 6, 8, 11] and software-based <ref> [2, 5, 10, 12, 13, 14] </ref>. Typically, hardware-based schemes only work well for data structures that are accessed with a fixed stride. Therefore, they do not do well in irregular applications like some of ours, where many pointers are dereferenced and references have no clear pattern.
Reference: [3] <author> R. Chandra, K. Gharachorloo, V. Soundararajan, and A. Gupta. </author> <title> Performance Evaluation of Hybrid Hardware and Software Distributed Shared Memory Protocols. </title> <booktitle> In Proceedings of the 1994 International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Again, this scheme adapts to the object size. Unfortunately, both schemes fail to use any object information that could be extracted, often very easily, from the source code. Instead of using help from the compiler or programmer, all is left to the hardware. Two other techniques, regions <ref> [3] </ref> and block transfers [19], use the opposite approach, namely they use the help of the software to identify objects. However, given the overheads involved, these techniques are designed for large objects. In addition, identifying the regions or blocks and annotating the application is likely to require significant programmer involvement.
Reference: [4] <author> T. F. Chen and J. L. Baer. </author> <title> A Performance Study of Software and Hardware Data Prefetching Schemes. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 223-232, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: If prefetching support is provided, however, we would like it to work for both regular and irregular applications. Unfortunately, it is hard for prefetching to work well for our irregular applications. On the one hand, traditional prefetching hardware is effective for fixed-stride access patterns only <ref> [1, 4] </ref>. Therefore, it will not work very well for irregular, pointer-based codes. On the other hand, purely software-based prefetching driven by the compiler [14] does not seem applicable either: the compiler analysis required is very complex. Finally, software-based prefetching with hand-inserted prefetches is a possibility.
Reference: [5] <author> W. Y. Chen, S. A. Mahlke, P. P. Chang, and W. W. Hwu. </author> <title> Data Access Microarchitectures for Superscalar Processors with Compiler-Assisted Data Prefetching. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Microarchitecture, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: In a network with less bandwidth, long lines would perform relatively worse. 6 Comparison to Other Prefetch ing Schemes There has been much work on data prefetching, both hardware-based [1, 6, 8, 11] and software-based <ref> [2, 5, 10, 12, 13, 14] </ref>. Typically, hardware-based schemes only work well for data structures that are accessed with a fixed stride. Therefore, they do not do well in irregular applications like some of ours, where many pointers are dereferenced and references have no clear pattern.
Reference: [6] <author> F. Dahlgren, M. Dubois, and P. Stenstrom. </author> <title> Fixed and Adaptive Sequential Prefetching in Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <pages> pages I:56-63, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: One of them is to have an adjustable cache line size [7]. The idea is to let the hardware adapt to changing object sizes by automatically merging or splitting cache lines dynamically. A second approach is to combine short cache lines and adaptive prefetching <ref> [6] </ref>. In adaptive prefetching, every miss fetches a certain number of contiguous lines. The hardware then monitors the use of the lines. If they are used, more lines will be fetched in future misses; if they are not used, fewer lines will be fetched per miss. <p> In a network with less bandwidth, long lines would perform relatively worse. 6 Comparison to Other Prefetch ing Schemes There has been much work on data prefetching, both hardware-based <ref> [1, 6, 8, 11] </ref> and software-based [2, 5, 10, 12, 13, 14]. Typically, hardware-based schemes only work well for data structures that are accessed with a fixed stride.
Reference: [7] <author> C. Dubnicki and T. LeBlanc. </author> <title> Adjustable Block Size Coherent Caches. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 170-180, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: It seems, therefore, that if we want to design general purpose computers, we need to accommodate these two contradictory demands on the cache line size. Several techniques have been proposed to address this problem. One of them is to have an adjustable cache line size <ref> [7] </ref>. The idea is to let the hardware adapt to changing object sizes by automatically merging or splitting cache lines dynamically. A second approach is to combine short cache lines and adaptive prefetching [6]. In adaptive prefetching, every miss fetches a certain number of contiguous lines.
Reference: [8] <author> J. W. C. Fu and J. H. Patel. </author> <title> Data Prefetching in Multiprocessor Vector Cache Memories. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In a network with less bandwidth, long lines would perform relatively worse. 6 Comparison to Other Prefetch ing Schemes There has been much work on data prefetching, both hardware-based <ref> [1, 6, 8, 11] </ref> and software-based [2, 5, 10, 12, 13, 14]. Typically, hardware-based schemes only work well for data structures that are accessed with a fixed stride.
Reference: [9] <author> S. Goldschmidt. </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: In the following, we start by describing our evaluation methodology and then evaluate each of the three proposed optimizations in turn. 5.1 Evaluation Methodology Our evaluation is based on execution-driven simulations of a 32-processor architecture running the parallel applications of Section 3. We use TangoLite <ref> [9] </ref> to simulate a 32-node NUMA multiprocessor. Each node contains a 200 MHz processor, a 64 Kbyte primary cache, a 256 Kbyte secondary cache, a portion of the global memory with its corresponding directory and a network processor. All caches are direct-mapped.
Reference: [10] <author> E. H. Gornish, E. D. Granston, and A. V. Veidenbaum. </author> <title> Compiler-Directed Data Prefetching in Multiprocessors with Memory Hierarchies. </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pages 11-15, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In a network with less bandwidth, long lines would perform relatively worse. 6 Comparison to Other Prefetch ing Schemes There has been much work on data prefetching, both hardware-based [1, 6, 8, 11] and software-based <ref> [2, 5, 10, 12, 13, 14] </ref>. Typically, hardware-based schemes only work well for data structures that are accessed with a fixed stride. Therefore, they do not do well in irregular applications like some of ours, where many pointers are dereferenced and references have no clear pattern. <p> In these systems, however, the window of instructions considered tends to be relatively small and, therefore, these systems issue fewer useful prefetches than our system. Furthermore, the hardware can be quite complex. Software-based prefetching driven by a compiler has been successfully used in regular applications <ref> [10, 14] </ref>. However, it cannot be easily applied to irregular applications, where it is hard for compilers to analyze the pointers.
Reference: [11] <author> N. Jouppi. </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In a network with less bandwidth, long lines would perform relatively worse. 6 Comparison to Other Prefetch ing Schemes There has been much work on data prefetching, both hardware-based <ref> [1, 6, 8, 11] </ref> and software-based [2, 5, 10, 12, 13, 14]. Typically, hardware-based schemes only work well for data structures that are accessed with a fixed stride.
Reference: [12] <author> A. C. Klaiber and H. M. Levy. </author> <title> Architecture for Software-Controlled Data Prefetching. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 43-63, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In a network with less bandwidth, long lines would perform relatively worse. 6 Comparison to Other Prefetch ing Schemes There has been much work on data prefetching, both hardware-based [1, 6, 8, 11] and software-based <ref> [2, 5, 10, 12, 13, 14] </ref>. Typically, hardware-based schemes only work well for data structures that are accessed with a fixed stride. Therefore, they do not do well in irregular applications like some of ours, where many pointers are dereferenced and references have no clear pattern.
Reference: [13] <author> T. Mowry and A. Gupta. </author> <title> Tolerating Latency through Software-Controlled Prefetching in Shared-Memory Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Finally, software-based prefetching with hand-inserted prefetches is a possibility. However, for irregular applications, it is hard and very time consuming to get good speedups with hand-inserted prefetches <ref> [13] </ref>. This paper addresses the prefetching problem by proposing a new prefetching scheme that, while usable by applications with good spatial locality, is specifically targeted to irregular applications. We call the scheme Memory Binding and Group Prefetching. <p> We mark this link as a prefetching link. This helps prefetch the cell earlier. This is a fairly obvious optimization to perform, and is equivalent to the software pipelining performed by Mowry and Gupta <ref> [13] </ref> in the same code. If we are willing to add additional pointers between groups as we did for MP3D, the rest of the applications could be easily optimized too. <p> In a network with less bandwidth, long lines would perform relatively worse. 6 Comparison to Other Prefetch ing Schemes There has been much work on data prefetching, both hardware-based [1, 6, 8, 11] and software-based <ref> [2, 5, 10, 12, 13, 14] </ref>. Typically, hardware-based schemes only work well for data structures that are accessed with a fixed stride. Therefore, they do not do well in irregular applications like some of ours, where many pointers are dereferenced and references have no clear pattern. <p> While advanced pointer analysis together with the use of profile information may eventually provide some gains, it will surely not be easy to successfully use compiler prefetches for irregular applications. For irregular applications, instead, researchers have looked at software-based prefetching with hand-inserted prefetches <ref> [13] </ref>. Compared to our scheme, however, hand-inserting prefetches has four disadvantages. The first one is the overhead of the prefetch instructions and the instructions that generate the addresses for the prefetches. In irregular applications, the latter is likely to be large. <p> This is because the programmer only needs to check for groups of data that are accessed together. For example, to choose our groups and prefetching links, we did not use the extensive profiling used in <ref> [13] </ref>. Furthermore, adding the prefetches takes longer than adding our system calls. We are currently trying to quantify the difference in effort between the two schemes. Finally, a fourth disadvantage of hand-inserting prefetches is that it may not be very effective when prefetching records linked by pointers. <p> Finally, a disadvantage of the scheme presented is the hardware support required. We note, however, that we can still use off-the-shelf processors. We have attempted to compare our scheme to hand-inserting prefetches. An evaluation of the latter, performed by Mowry and Gupta <ref> [13] </ref>, included MP3D and Pthor among the applications. Unfortunately, it is hard to compare the quantitative results reported by these authors to ours since they simulated 16 processors, used only 1-Kbyte primary and 2-Kbyte secondary caches for lock-up free caches, and simulated a different architecture.
Reference: [14] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: On the one hand, traditional prefetching hardware is effective for fixed-stride access patterns only [1, 4]. Therefore, it will not work very well for irregular, pointer-based codes. On the other hand, purely software-based prefetching driven by the compiler <ref> [14] </ref> does not seem applicable either: the compiler analysis required is very complex. Finally, software-based prefetching with hand-inserted prefetches is a possibility. However, for irregular applications, it is hard and very time consuming to get good speedups with hand-inserted prefetches [13]. <p> In a network with less bandwidth, long lines would perform relatively worse. 6 Comparison to Other Prefetch ing Schemes There has been much work on data prefetching, both hardware-based [1, 6, 8, 11] and software-based <ref> [2, 5, 10, 12, 13, 14] </ref>. Typically, hardware-based schemes only work well for data structures that are accessed with a fixed stride. Therefore, they do not do well in irregular applications like some of ours, where many pointers are dereferenced and references have no clear pattern. <p> In these systems, however, the window of instructions considered tends to be relatively small and, therefore, these systems issue fewer useful prefetches than our system. Furthermore, the hardware can be quite complex. Software-based prefetching driven by a compiler has been successfully used in regular applications <ref> [10, 14] </ref>. However, it cannot be easily applied to irregular applications, where it is hard for compilers to analyze the pointers.
Reference: [15] <author> T. C. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Unfortunately, it is hard to compare the quantitative results reported by these authors to ours since they simulated 16 processors, used only 1-Kbyte primary and 2-Kbyte secondary caches for lock-up free caches, and simulated a different architecture. Similarly, all relevant data in <ref> [15] </ref> corresponds 16 processors. Interestingly, we do not think that hand-inserted prefetch-ing and our scheme are completely exclusive alternatives. Instead, they can be combined and can complement each other. For example, some hand-inserted prefetches can be used to trigger a group prefetch without requiring the initial miss.
Reference: [16] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stan-ford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Often, the dominant data structures or objects in these applications are small records no more than a few tens of bytes in size. Furthermore, these objects are frequently interconnected and accessed via pointers. For example, Splash's Pthor application <ref> [16] </ref> is an event-driven simulator whose data structures are small records representing gates or nets. Gates and nets are linked by pointers to form the circuit. <p> The applications include most of the Splash <ref> [16] </ref> applications plus one pointer-intensive application called Maxflow [18]. The fraction of pointer-chasing references varies quite a bit among the applications. Using this metric, we classify Barnes, Maxflow, Pthor and, to a lesser extent, MP3D as pointer-intensive or irregular applications. They often access structures of records via pointers.
Reference: [17] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> False Sharing and Spatial Locality in Multiprocessor Caches. </title> <journal> In IEEE Trans. on Computers, </journal> <pages> pages 651-663, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Irregular applications run more efficiently with short cache lines. Indeed, short lines bring less useless data into the cache and, therefore, cause less traffic. In addition, they are likely to cause fewer cache conflicts and less false sharing <ref> [17] </ref>. It seems, therefore, that if we want to design general purpose computers, we need to accommodate these two contradictory demands on the cache line size. Several techniques have been proposed to address this problem. One of them is to have an adjustable cache line size [7].
Reference: [18] <author> W. Weber and A. Gupta. </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: This is useful in non record-based programs when the compiler alone cannot figure out the groups. We will see an example in Splash's Cholesky application. While records are hints of inter-related data, permanent pointers between records are strong hints of groups that go together. Consider, for instance, Maxflow <ref> [18] </ref>, a graph problem-solving application. The nodes and edges in the graph are represented by records. The record of a node and the records of the edges connected to it are linked by permanent pointers in the application. <p> The applications include most of the Splash [16] applications plus one pointer-intensive application called Maxflow <ref> [18] </ref>. The fraction of pointer-chasing references varies quite a bit among the applications. Using this metric, we classify Barnes, Maxflow, Pthor and, to a lesser extent, MP3D as pointer-intensive or irregular applications. They often access structures of records via pointers.
Reference: [19] <author> S. Woo, J. Singh, and J. Hennessy. </author> <title> The Performance Advantages of Integrating Block Data Transfer in Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 219-229, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Unfortunately, both schemes fail to use any object information that could be extracted, often very easily, from the source code. Instead of using help from the compiler or programmer, all is left to the hardware. Two other techniques, regions [3] and block transfers <ref> [19] </ref>, use the opposite approach, namely they use the help of the software to identify objects. However, given the overheads involved, these techniques are designed for large objects. In addition, identifying the regions or blocks and annotating the application is likely to require significant programmer involvement.
References-found: 19

