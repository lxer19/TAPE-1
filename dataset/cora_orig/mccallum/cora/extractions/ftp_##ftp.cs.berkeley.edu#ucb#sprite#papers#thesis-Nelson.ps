URL: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/thesis-Nelson.ps
Refering-URL: http://www.cs.berkeley.edu/projects/sprite/sprite.papers.html
Root-URL: http://www.cs.berkeley.edu
Title: Physical Memory Management in a Network Operating System  
Author: Michael Newell Nelson 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division Electrical Engineering and Computer Sciences University of California  
Abstract-found: 0
Intro-found: 1
Reference: [Akh87] <author> P. Akhtar, </author> <title> ``A Replacement for Berkeley Memory Management'', </title> <booktitle> Proceedings of the USENIX 1987 Summer Conference, </booktitle> <month> JUNE </month> <year> 1987, </year> <pages> 69-79. </pages>
Reference-contexts: Previous Work The original idea of copy-on-write emerged over 15 years ago with TENEX [BBM72, Mur72]. Since then it has been implemented in several systems <ref> [Akh87, GMS87, Ras87, SCC86] </ref>. The Mach operating system [Ras87] is one of the most recent systems to implement copy-on-write, and is one of the few whose implementation of copy-on-write has been published in detail.
Reference: [BLM87] <author> M. J. Bach, M. W. Luppi, A. S. Melamed and K. Yueh, </author> <title> ``A Remote-File Cache for RFS'', </title> <booktitle> Proceedings of the USENIX Summer 1987 Conference, </booktitle> <month> June </month> <year> 1987, </year> <pages> 275-280. </pages>
Reference-contexts: The simplest policy for managing modified data blocks is to write them through to the server and/or the disk as soon as they are placed into the cache. NFS uses write-through on the server and RFS <ref> [BLM87] </ref> uses write-through on clients. The advantage of a write-through policy is its reliability: little information is lost when a client or server crashes. However, each write must wait for the data to be written to the server and/or disk, which results in poor write performance.
Reference: [BCD72] <author> A. Bensoussan, C. T. Clingen and R. C. Daley, </author> <title> ``The MULTICS Virtual Memory: Concepts and Design'', </title> <journal> Comm. of the ACM 15, </journal> <month> 5 (May </month> <year> 1972). </year>
Reference-contexts: This approach eliminates the file cache entirely; the standard page replacement mechanisms automatically balance physical memory usage between file and program information. Mapped files were first used in Multics <ref> [BCD72, DaD68] </ref> and TENEX [BBM72, Mur72]. More recently they have been implemented in Pilot [Red80], Accent [RaR81, RaF86], Apollo [LLH85, Lea83] and Mach [Ras87]. Mapped files present a much different interface than systems such as UNIX that keep the file system and virtual memory system separate.
Reference: [BiN84] <author> A. D. Birrell and B. J. Nelson, </author> <title> ``Implementing Remote Procedure Calls'', </title> <journal> ACM Transactions on Computer Systems 2, </journal> <month> 1 (Feb. </month> <year> 1984), </year> <pages> 39-59. </pages>
Reference-contexts: In particular, Sprite's implementation is based around a simple kernel-to-kernel remote-procedure-call (RPC) facility [Wel86], which allows kernels on different workstations to request services of each other using a protocol similar to the one described by Birrell and Nel-son <ref> [BiN84] </ref>. The Sprite file system uses the RPC mechanism extensively for cache management. 1.3. Thesis Overview This dissertation covers three areas: file caching, virtual memory, and the interaction between the two. The first part of the dissertation (Chapters 2 through 5) covers issues in file caching.
Reference: [BBM72] <author> D. G. Bobrow, J. D. Burchfiel, D. L. Murphy and R. S. Tomlinson, ``TENEX, </author> <title> a Paged Time Sharing System for the PDP-10'', </title> <journal> Comm. of the ACM 15, </journal> <month> 3 (Mar. </month> <year> 1972), </year> <pages> 1135-143. </pages>
Reference-contexts: This approach eliminates the file cache entirely; the standard page replacement mechanisms automatically balance physical memory usage between file and program information. Mapped files were first used in Multics [BCD72, DaD68] and TENEX <ref> [BBM72, Mur72] </ref>. More recently they have been implemented in Pilot [Red80], Accent [RaR81, RaF86], Apollo [LLH85, Lea83] and Mach [Ras87]. Mapped files present a much different interface than systems such as UNIX that keep the file system and virtual memory system separate. <p> Copy-on-write saves not only copying of pages in memory, but also copying of pages that are on backing store. Copy-on-write has been implemented in several systems, with the earliest being TENEX <ref> [BBM72, Mur72] </ref> and one of the most recent being Mach [Ras87]. This chapter describes a simple copy-on-write mechanism that I have implemented as part of Sprite. <p> If neither the parent nor the child modify many pages between the fork and the exec, then copy-on-write may be able to save many page 136 copy operations. 7.3. Previous Work The original idea of copy-on-write emerged over 15 years ago with TENEX <ref> [BBM72, Mur72] </ref>. Since then it has been implemented in several systems [Akh87, GMS87, Ras87, SCC86]. The Mach operating system [Ras87] is one of the most recent systems to implement copy-on-write, and is one of the few whose implementation of copy-on-write has been published in detail.
Reference: [BKT85] <author> M. R. Brown, K. N. Kolling and E. A. Taft, </author> <title> ``The Alpine File System'', </title> <journal> Trans. Computer Systems 3, </journal> <volume> 4 (Nov. </volume> <year> 1985), </year> <pages> 261-293. </pages>
Reference-contexts: The solutions that have been used in other file systems to provide a higher measure of reliability than Sprite's are based on file versions [CaW86, SGN85] or atomic transactions <ref> [BKT85, PoW85] </ref>. The systems that use file versions create a new version each time that a file is written. Thus, files will never be destroyed as a result of client or server crashes, because old versions of files will remain safely on disk.
Reference: [CaW86] <author> L. F. Cabrera and J. Wylie, </author> <title> ``QuickSilver Distributed File Services: An Architecture for Horizontal Growth'', </title> <type> Research Report RJ 5578 (56697), </type> <address> San Jose, California, </address> <month> June </month> <year> 1986. </year> <month> 166 </month>
Reference-contexts: The solutions that have been used in other file systems to provide a higher measure of reliability than Sprite's are based on file versions <ref> [CaW86, SGN85] </ref> or atomic transactions [BKT85, PoW85]. The systems that use file versions create a new version each time that a file is written. Thus, files will never be destroyed as a result of client or server crashes, because old versions of files will remain safely on disk.
Reference: [ChR85] <author> D. R. Cheriton and P. J. Roy, </author> <title> ``Performance of the V Storage Server: A Preliminary Report'', </title> <booktitle> Proc. of the 1985 ACM Computer Science Conference, </booktitle> <month> Mar. </month> <year> 1985, </year> <pages> 302-308. </pages>
Reference-contexts: When it detects this, it forces all reads and writes to go through to the server for the file that is being shared. 2.5.1.7. V Storage Server The V Storage Server at Stanford <ref> [ChR85] </ref> provides multiple approaches to consistency. One approach is called T-consistency and is used for immutable files. The data pages read from an immutable cached file are consistent with some version of the file, either the current version or a version that is at most T milliseconds out of date.
Reference: [DaD68] <author> R. C. Daley and J. B. Dennis, </author> <title> ``Virtual Memory, Processes and Sharing in MULTICS'', </title> <journal> Comm. of the ACM 11, </journal> <month> 5 (May </month> <year> 1968), </year> <pages> 306-312. </pages>
Reference-contexts: This approach eliminates the file cache entirely; the standard page replacement mechanisms automatically balance physical memory usage between file and program information. Mapped files were first used in Multics <ref> [BCD72, DaD68] </ref> and TENEX [BBM72, Mur72]. More recently they have been implemented in Pilot [Red80], Accent [RaR81, RaF86], Apollo [LLH85, Lea83] and Mach [Ras87]. Mapped files present a much different interface than systems such as UNIX that keep the file system and virtual memory system separate.
Reference: [Flo86] <author> R. Floyd, </author> <title> ``Short-Term File Reference Patterns in a UNIX Environment'', </title> <type> Technical Report Tech. Rep. 177, </type> <institution> The University of Rochester, </institution> <month> Mar. </month> <year> 1986. </year>
Reference-contexts: However, results from four studies of UNIX timesharing traces can be used to help predict the best writing policy for clients and servers. In addition to the two previously-mentioned studies by Ousterhout and Kent there are also studies that were done by Floyd <ref> [Flo86] </ref> and Thompson [Tho87]. Floyd's 15 studies are nearly identical to Ousterhout's studies so I will not mention them further. Thompson's study was a follow-on study to the study done by Ousterhout et al.; Thompson's results are based on very detailed traces of UNIX timesharing systems.
Reference: [GMS87] <author> R. A. Gingell, J. P. Moran and W. A. Shannon, </author> <title> ``Virtual Memory Architecture in SunOS'', </title> <booktitle> Proceedings of the USENIX 1987 Summer Conference, </booktitle> <month> JUNE </month> <year> 1987, </year> <pages> 81-94. </pages>
Reference-contexts: Previous Work The original idea of copy-on-write emerged over 15 years ago with TENEX [BBM72, Mur72]. Since then it has been implemented in several systems <ref> [Akh87, GMS87, Ras87, SCC86] </ref>. The Mach operating system [Ras87] is one of the most recent systems to implement copy-on-write, and is one of the few whose implementation of copy-on-write has been published in detail.
Reference: [Gus87] <author> R. Gusella, </author> <title> ``The Analysis of Diskless Workstation Traffic on the Ethernet'', </title> <note> Technical Report UCB/Computer Science Dpt. 87/379, </note> <institution> University of California, Berkeley, </institution> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: After that, higher-performance networks will become essential. Ricardo Gusella in an analysis of diskless workstation Ethernet traffic also noticed that Ethernets are becoming heavily loaded with the introduction of faster machines <ref> [Gus87] </ref>. He measured the traffic on a 10-Mbit Ethernet over a 24 hour period. He determined that two Sun-3 workstations (a Sun-3/180 server and a Sun-3/50 client each with 4 Mbytes of memory) running UNIX with Sun's Network File System (NFS) [San85] can utilize over 20% of the Ethernet.
Reference: [Hil86] <author> M. D. Hill, et al., </author> <title> ``SPUR: A VLSI Multiprocessor Workstation'', </title> <booktitle> IEEE Computer 19, </booktitle> <address> 11 (Nov. </address> <year> 1986), </year> <pages> 8-22. </pages>
Reference-contexts: In the rest of the dissertation where I describe work that I did on my own I will use ``I''. 1.2. Overview of Sprite Sprite [OCD88] is a new operating system implemented at the University of Cali-fornia at Berkeley as part of the development of SPUR <ref> [Hil86] </ref>, a high-performance multiprocessor workstation. A preliminary version of Sprite is currently running on Sun-2 and Sun-3 workstations, which have about 1-2 MIPS processing power and 4-16 Mbytes of main memory. <p> Acquiring ownership causes the page to be removed from all other workstation's memories. Another potential problem with mapped files is that the hardware may make mapping difficult. Some newer workstations use a virtually addressed hardware cache <ref> [Hil86, Kel86, SSS85] </ref>. These caches do not support synonyms multiple virtual addresses pointing to the same physical address. <p> I chose the COW-COR mechanism for two reasons: virtually-addressed caches and simplicity. The SPUR hardware <ref> [Hil86] </ref>, which is one of Sprite's target machines, uses virtually-addressed caches that do not provide efficient support for copy-on-write; expensive cache flushing operations are required in order to implement copy-on-write on a SPUR. 134 As I will explain later, the Sprite COW-COR scheme can be implemented on architectures such as a <p> The last column is the percentage of pages that would have been copied under a pure copy-on-write scheme; it is the second column added to the product of the third and fourth columns. architectures with virtually addressed caches, such as the Sun-3 [SSS85], Sun-4 [Kel86] and SPUR <ref> [Hil86] </ref> architectures. In these machines, protection bits are stored along with the data in individual cache lines. To change the protection on a page, the operat ing system must first modify the page table entry, then flush all of the page's lines from the cache.
Reference: [How88] <author> J. Howard, et al., </author> <title> ``Scale and Performance in a Distributed File System'', </title> <journal> Trans. Computer Systems 6, </journal> <month> 1 (Feb. </month> <year> 1988), </year> <pages> 51-81. </pages>
Reference-contexts: As memories get larger, main-memory caches will grow to achieve even higher hit ratios. Although several systems have implemented client caching in various forms, none of these systems has been analyzed to determine the impact of caching on system performance. For example, Howard et al. <ref> [How88] </ref> showed that with caches on clients, the load placed on the server by each client is very small. However, they did not determine what the load would have been if there had been no caches on the client workstations. <p> Most systems verify consistency when a file is opened or locked. The Andrew file system initially verified consistency when a file was opened, but, after discovering that their servers were becoming seriously overloaded, they changed to use the second approach <ref> [How88] </ref>. 2.6. Trace-Driven Analyses of Client Caching Jim Thompson used UNIX traces gathered from a single timeshared machine to perform a trace-driven simulation of the impact of client caching on performance [Tho87]. In his simulations every user on the timesharing system represents a different client. <p> Second, it allowed us to build a very simple data cache consistency algorithm. However, it had the potential to increase server load, as was discovered by the Andrew file system when its authors also required that the server be contacted on each file open <ref> [How88] </ref>. The next chapter will include a discussion of the impact of this decision on Sprite file system performance. <p> Developed by M. Satyanarayanan for benchmark-ing the Andrew file system; see <ref> [How88] </ref> for de tails. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Vm-make 42.3 25.9Use the ``make'' program to recompile the Sprite virtual memory system: 14 source files, 12600 lines of C source code. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Sort 46.4 89.9Sort a 1-Mbyte file. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Diff 452.2 4.3Compare 2 identical 1-Mbyte files. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Ditroff 7.0 10.4Format a paper which contains both <p> Since the average I/O rate for the Andrew benchmark was 90 Kbytes/second, it corresponds to about 5-20 users. This estimate is consistent with independent estimates made by Howard et al., who estimated that one instance of the Andrew benchmark corresponds to five average users <ref> [How88] </ref>, and by Lazowska et al., who estimated about 4 Kbytes/second of I/O per user on slower Sun-2 workstations [LZC86]. The server capacity should not change much with increasing CPU speeds, as long as both client and server CPU speeds increase at about the same rate. <p> Comparison to Other Systems benchmark. The measurements for the NFS and Andrew file systems were obtained from <ref> [How88] </ref>. Unfortunately, the measurements in [How88] were taken using Sun-3/50 clients, whereas I had only Sun-3/75 clients available for the Sprite measurements; the Sun-3/75 is about 30% faster than the Sun-3/50. <p> Comparison to Other Systems benchmark. The measurements for the NFS and Andrew file systems were obtained from <ref> [How88] </ref>. Unfortunately, the measurements in [How88] were taken using Sun-3/50 clients, whereas I had only Sun-3/75 clients available for the Sprite measurements; the Sun-3/75 is about 30% faster than the Sun-3/50. <p> Another difference between my measurements and the ones in <ref> [How88] </ref> is that the NFS and Andrew measurements were made using local disks for program binaries, paging, and temporary files. For Sprite, all of this information was accessed remotely from the server. about 35% faster than Andrew. <p> The Andrew and NFS numbers were taken from <ref> [How88] </ref> and are based Sun-3/50 clients. The Sprite numbers were taken from Table 4-5 and re-normalized for Sun-3/50 clients. 2.4% server CPU utilization, vs. 7.5% in Sprite and 20% in NFS. I attribute Andrew's low server CPU utilization to its use of callbacks. <p> Developed by M. Satyanarayanan for benchmark-ing the Andrew file system; see <ref> [How88] </ref> for de tails. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Vm-make 42.3 25.9Use the ``make'' program to recompile the Sprite virtual memory system: 15 source files, 11,250 lines of C source code. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Sort 46.4 89.9Sort a 1-Mbyte file. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c
Reference: [Kel86] <author> E. Kelly, </author> <title> Sun-4 Architecture Manual, </title> <publisher> Sun Microsystems Inc., </publisher> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: Acquiring ownership causes the page to be removed from all other workstation's memories. Another potential problem with mapped files is that the hardware may make mapping difficult. Some newer workstations use a virtually addressed hardware cache <ref> [Hil86, Kel86, SSS85] </ref>. These caches do not support synonyms multiple virtual addresses pointing to the same physical address. <p> The last column is the percentage of pages that would have been copied under a pure copy-on-write scheme; it is the second column added to the product of the third and fourth columns. architectures with virtually addressed caches, such as the Sun-3 [SSS85], Sun-4 <ref> [Kel86] </ref> and SPUR [Hil86] architectures. In these machines, protection bits are stored along with the data in individual cache lines. To change the protection on a page, the operat ing system must first modify the page table entry, then flush all of the page's lines from the cache.
Reference: [Kel88] <author> E. Kelly, </author> <type> Personal Communication, </type> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: Attributes of the Sun-4 architecture <ref> [Kel88] </ref>. When a cache line is copied at fork time or because of a copy-on-write or copy-on-reference fault, the destination of the copy will not be present in the cache.
Reference: [Ken86] <author> C. A. Kent, </author> <title> Cache Coherence in Distributed Systems, </title> <type> Phd Thesis, </type> <institution> Purdue University, </institution> <year> 1986. </year> <month> 167 </month>
Reference-contexts: The actual improvement that can be gained from caching depends on the writing policy, which will be explained below. A study very similar to Ousterhout's study was done by Kent at Purdue <ref> [Ken86] </ref>. He also did a trace-driven analysis of file activity in a timeshared UNIX 4.2 BSD system, and his results were nearly identical to Ousterhout's results. One other study of disk caching was done by Smith, who used trace data from IBM mainframes [Smi85]. <p> We chose the disk block size based on the results obtained by McKusick et al. [MJL84], who determined that large block sizes on the order of 4 Kbytes result in sub 31 stantially better file system performance than smaller block sizes. In addition, studies by Kent <ref> [Ken86] </ref> and Ousterhout [Ous85] also demonstrate the virtues of a large block size. Whether the disk block size should be even larger is an open question which we will address as we gain more experience with the system.
Reference: [LZC86] <author> E. Lazowska, J. Zahorjan, D. Cheriton and W. Zwaenepoel, </author> <title> ``File Access Performance of Diskless Workstations'', </title> <journal> Trans. Computer Systems, </journal> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: A reduction in the load on the network and the server will result in greater system scalability because there can be more clients per network and more clients per server. The relation between server load and system scalability was shown by Lazowska et al. <ref> [LZC86] </ref> in a study of remote file access where they concluded that the server CPU is the primary bottleneck that limits system scalability. Caches can be used on clients for two purposes: to cache file data and to cache naming information. <p> Network Utilization In their analysis of diskless file access, based on Sun-2 workstations, Lazowska et al. concluded that network loading was not yet a major factor in network file systems <ref> [LZC86] </ref>. However, as CPU speeds increase, the network bandwidth is becoming more and more of an issue. Figure 4-1 (b) plots network traffic as a function of cache size for the benchmarks running on Sun-3's. Without client caching the benchmarks averaged 7.8% utilization of the 10-Mbit/second Ethernet. <p> This estimate is consistent with independent estimates made by Howard et al., who estimated that one instance of the Andrew benchmark corresponds to five average users [How88], and by Lazowska et al., who estimated about 4 Kbytes/second of I/O per user on slower Sun-2 workstations <ref> [LZC86] </ref>. The server capacity should not change much with increasing CPU speeds, as long as both client and server CPU speeds increase at about the same rate. In a system with servers that are more powerful than clients, the server capacity should be even higher than this. 4.4.
Reference: [LLH85] <author> P. Leach, P. Levine, J. Hamilton and B. Stumpf, </author> <title> ``The File System of an Integrated Local Network'', </title> <booktitle> Proc. of the 1985 ACM Computer Science Conference, </booktitle> <month> Mar. </month> <year> 1985, </year> <pages> 309-324. </pages>
Reference-contexts: When a token is released, the file that the token pertains to must be written back to the server and invalidated from the cache. The algorithm must ensure that all sharers of a file get a fair chance at accessing the file. 2.5.1.5. Apollo The Apollo Aegis file system <ref> [LLH85, Lea83] </ref> uses file locking to guarantee consistency; consistency is not guaranteed unless clients lock files before they perform read or write operations. A file can be locked by multiple clients when there are only readers, and by only a single client if the file is locked for writing. <p> This approach eliminates the file cache entirely; the standard page replacement mechanisms automatically balance physical memory usage between file and program information. Mapped files were first used in Multics [BCD72, DaD68] and TENEX [BBM72, Mur72]. More recently they have been implemented in Pilot [Red80], Accent [RaR81, RaF86], Apollo <ref> [LLH85, Lea83] </ref> and Mach [Ras87]. Mapped files present a much different interface than systems such as UNIX that keep the file system and virtual memory system separate. Under the UNIX approach, users use system calls such as read and write to access file data.
Reference: [Lea83] <author> P. J. Leach, et al., </author> <title> ``The Architecture of an Integrated Local Network'', </title> <journal> IEEE Journal on Selected Areas in Communications SAC-1, </journal> <volume> 5 (Nov. </volume> <year> 1983), </year> <pages> 842-857. </pages>
Reference-contexts: When a token is released, the file that the token pertains to must be written back to the server and invalidated from the cache. The algorithm must ensure that all sharers of a file get a fair chance at accessing the file. 2.5.1.5. Apollo The Apollo Aegis file system <ref> [LLH85, Lea83] </ref> uses file locking to guarantee consistency; consistency is not guaranteed unless clients lock files before they perform read or write operations. A file can be locked by multiple clients when there are only readers, and by only a single client if the file is locked for writing. <p> This approach eliminates the file cache entirely; the standard page replacement mechanisms automatically balance physical memory usage between file and program information. Mapped files were first used in Multics [BCD72, DaD68] and TENEX [BBM72, Mur72]. More recently they have been implemented in Pilot [Red80], Accent [RaR81, RaF86], Apollo <ref> [LLH85, Lea83] </ref> and Mach [Ras87]. Mapped files present a much different interface than systems such as UNIX that keep the file system and virtual memory system separate. Under the UNIX approach, users use system calls such as read and write to access file data.
Reference: [Li86] <author> K. Li, </author> <title> Shared Virtual Memory on a Loosely Coupled Multiprocessor, </title> <type> PhD Thesis, </type> <institution> Yale University, </institution> <year> 1986. </year>
Reference-contexts: This would make it impossible to use Sprite's simple cache consistency algorithm, which requires caches to be disabled under some conditions. 111 One scheme that keeps mapped file caches consistent without requiring users to lock their files is one that has been implemented by Kai Li <ref> [Li86] </ref>. His scheme provides cache consistency at the page level. It is a complex scheme in which each page is ``owned'' by a workstation. Whenever a workstation wishes to read a page that is not already in its memory, a copy of the page is fetched from the page's owner.
Reference: [MJL84] <author> M. K. McKusick, W. N. Joy, S. J. Leffler and R. S. Fabry, </author> <title> ``A Fast File System for UNIX'', </title> <journal> Trans. Computer Systems 2, </journal> <month> 3 (Aug. </month> <year> 1984), </year> <month> 181-197.. </month>
Reference-contexts: The cache block size corresponds to the disk block size, which is also 4 Kbytes. We chose the disk block size based on the results obtained by McKusick et al. <ref> [MJL84] </ref>, who determined that large block sizes on the order of 4 Kbytes result in sub 31 stantially better file system performance than smaller block sizes. In addition, studies by Kent [Ken86] and Ousterhout [Ous85] also demonstrate the virtues of a large block size. <p> This is different from the UNIX 4.2 BSD implementation, which puts file descriptors, indirect blocks and data blocks for a file within the same group of cylinders on disk <ref> [MJL84] </ref>. The result is that Sprite may have to perform longer seeks between reads and writes of the three types of disk data. Because all three types of data are cached by Sprite, reading the data from disk should not be a problem. <p> Effect of Disk Layout on Write Performance As mentioned before, the Sprite file system's disk reading and writing performance is not as good as that of other systems such as UNIX 4.2 BSD <ref> [MJL84] </ref>. Sprite's poor writing performance could potentially contribute to the extra client degradation, server utilization and disk utilization when more reliable server writing policies are used. There are two areas where Sprite disk writing performance could be improved.
Reference: [Mor86] <author> J. H. Morris, et al., ``Andrew: </author> <title> A Distributed Personal Computing Environment'', </title> <journal> Comm. of the ACM 29, </journal> <month> 3 (Mar. </month> <year> 1986), </year> <pages> 184-201. </pages>
Reference-contexts: Systems that have implemented client caching have taken one of two approaches: cache file blocks in memory (e.g. LOCUS [PoW85, Wal83] and Sun's Network File System (NFS) [San85]) or cache whole files on a local disk (e.g. Andrew <ref> [Mor86, Sat85] </ref> and Cedar [SGN85]). The advantage of caching on a local disk is that local disks are generally much larger than physical memories. However, caching in main memory has numerous advantages over caching on a local disk. First, main-memory caches permit workstations to be diskless. <p> Note that Cedar does not satisfy my definition of cache consistency because once a file is open reads are not guaranteed to return the most recently written data. 2.5.1.3. Andrew Andrew <ref> [Mor86, Sat85] </ref> only supports sequential write-sharing. If two clients are undergoing concurrent write-sharing, then clients will not see a consistent view of the file.
Reference: [Mur72] <author> D. L. Murphy, </author> <title> ``Storage organization and management in TENEX'', </title> <booktitle> Proceedings AFIPS Fall Joint Computer Conference 15, 3 (1972), </booktitle> <pages> 23-32. </pages>
Reference-contexts: This approach eliminates the file cache entirely; the standard page replacement mechanisms automatically balance physical memory usage between file and program information. Mapped files were first used in Multics [BCD72, DaD68] and TENEX <ref> [BBM72, Mur72] </ref>. More recently they have been implemented in Pilot [Red80], Accent [RaR81, RaF86], Apollo [LLH85, Lea83] and Mach [Ras87]. Mapped files present a much different interface than systems such as UNIX that keep the file system and virtual memory system separate. <p> Copy-on-write saves not only copying of pages in memory, but also copying of pages that are on backing store. Copy-on-write has been implemented in several systems, with the earliest being TENEX <ref> [BBM72, Mur72] </ref> and one of the most recent being Mach [Ras87]. This chapter describes a simple copy-on-write mechanism that I have implemented as part of Sprite. <p> If neither the parent nor the child modify many pages between the fork and the exec, then copy-on-write may be able to save many page 136 copy operations. 7.3. Previous Work The original idea of copy-on-write emerged over 15 years ago with TENEX <ref> [BBM72, Mur72] </ref>. Since then it has been implemented in several systems [Akh87, GMS87, Ras87, SCC86]. The Mach operating system [Ras87] is one of the most recent systems to implement copy-on-write, and is one of the few whose implementation of copy-on-write has been published in detail.
Reference: [Nel86] <author> M. N. Nelson, </author> <title> ``The Sprite Virtual Memory System'', </title> <note> Technical Report UCB/Computer Science Dpt. 86/301, </note> <institution> University of California, Berkeley, </institution> <month> June </month> <year> 1986. </year>
Reference-contexts: In the Sprite mechanism, the file system module and the virtual memory module each manage a separate pool of physical memory pages. Virtual memory keeps its pages in approximate LRU order through a version of the clock algorithm <ref> [Nel86] </ref>. The file system keeps its cache blocks in perfect LRU order since all block accesses are made through the ``read'' and ``write'' system calls. Each system keeps a time-of-last-access for each page or block. <p> The approach just described has two potential problems: double-caching and multi-block pages. Double-caching can occur because virtual memory is a user of the file system: backing storage is implemented using ordinary files, and read-only code is demand-loaded directly from executable files <ref> [Nel86] </ref>.
Reference: [OCD88] <author> J. K. Ousterhout, A. R. Cherenson, F. Douglis, M. N. Nelson and B. B. Welch, </author> <title> ``The Sprite Network Operating System'', </title> <journal> IEEE Computer 21, </journal> <volume> 2 168 (Feb. </volume> <year> 1988), </year> <pages> 23-36. </pages>
Reference-contexts: The method that I used to perform this research was to design, build and measure the Sprite file system caching mechanism and the Sprite virtual memory system as part of the Sprite operating system <ref> [OCD88] </ref>. In addition to measuring the mechanisms used daily in Sprite, I also measured a variety of alternative mechanisms; these measurements provide the first quantitative comparisons between many of the popular memory-management techniques. <p> In the rest of the dissertation where I describe work that I did on my own I will use ``I''. 1.2. Overview of Sprite Sprite <ref> [OCD88] </ref> is a new operating system implemented at the University of Cali-fornia at Berkeley as part of the development of SPUR [Hil86], a high-performance multiprocessor workstation.
Reference: [Ous85] <author> J. K. Ousterhout, et al., </author> <title> ``A Trace-Driven Analysis of the 4.2 BSD UNIX File System'', </title> <booktitle> Proceedings of the 10th Symp. on Operating System Prin., </booktitle> <month> Dec. </month> <year> 1985, </year> <pages> 15-24. </pages>
Reference-contexts: Chapter 2 introduces the problems in file caching and discusses previous work in this area. This includes a discussion of an important set of trace-driven analyses that measured file activity in several timeshared UNIX 4.2 BSD systems <ref> [Ous85] </ref>. These simulations yielded two important results which motivated the Sprite 6 caching design. First, they demonstrated the potential performance improvements possible through caching; they found that even small caches can greatly improve performance. <p> One study of server caching was a trace-driven analysis of file activity in several timeshared UNIX 4.2 BSD systems <ref> [Ous85] </ref>. This study provided the main motivation for the Sprite cache design and I will refer to it extensively throughout this chapter. The systems studied by Ousterhout et al. were used for program development, text formatting, and computer-aided design. <p> This is the method used in most existing file systems and is practical because studies have shown that files are generally read and written in their entirety <ref> [Ous85] </ref>. Per-file approaches are simpler and can potentially lower 19 the cost of consistency by requiring fewer consistency actions (one per file rather than one per block). It is important to distinguish between consistency and correct synchronization. <p> We chose the disk block size based on the results obtained by McKusick et al. [MJL84], who determined that large block sizes on the order of 4 Kbytes result in sub 31 stantially better file system performance than smaller block sizes. In addition, studies by Kent [Ken86] and Ousterhout <ref> [Ous85] </ref> also demonstrate the virtues of a large block size. Whether the disk block size should be even larger is an open question which we will address as we gain more experience with the system. The choice to use a fixed block size was dictated by our striving for simplicity. <p> I also estimated the overall effectiveness of client caches. The traces were collected over 3-day mid-week intervals on 3 VAX-11/780s running 4.2 BSD UNIX for program development, text processing, and computer-aided design applications; see <ref> [Ous85] </ref> for more details. The data were used as input to a simulator that treated each timesharing user as a separate client workstation in a network with a single file server. The results are shown in Table 3-2. <p> This estimate is based on the study of UNIX done by Ousterhout et al. <ref> [Ous85] </ref>, which reported average file I/O rates per active user of 0.5-1.8 Kbytes/second. <p> However, the two types of data are actually quite different. The sequential nature of file accesses <ref> [Ous85] </ref> means that a low file hit ratio should have a much smaller impact on system performance than a low virtual-memory hit ratio. Also, the level of interactive response relies almost entirely on virtual memory system performance, not on the performance of the file system.
Reference: [PoW85] <author> G. Popek and B. Walker, </author> <title> editors, The LOCUS Distributed System Architecture, </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: In this section I will concentrate on data caching, and in Section 2.5 I will explore the impact of name caching. Systems that have implemented client caching have taken one of two approaches: cache file blocks in memory (e.g. LOCUS <ref> [PoW85, Wal83] </ref> and Sun's Network File System (NFS) [San85]) or cache whole files on a local disk (e.g. Andrew [Mor86, Sat85] and Cedar [SGN85]). The advantage of caching on a local disk is that local disks are generally much larger than physical memories. <p> Sequential write-sharing is supported by guaranteeing that, once a file is closed, all data is back on the server, and by ensuring that a client is notified by the server whenever the client's cached copy becomes out-of-date. 2.5.1.4. LOCUS LOCUS <ref> [PoW85, Wal83] </ref> supports both concurrent and sequential write-sharing. It uses a complex mechanism based on passing tokens between workstations that are accessing the file. There are two types of tokens: read and write. A client must possess a token in order to access a file. <p> The solutions that have been used in other file systems to provide a higher measure of reliability than Sprite's are based on file versions [CaW86, SGN85] or atomic transactions <ref> [BKT85, PoW85] </ref>. The systems that use file versions create a new version each time that a file is written. Thus, files will never be destroyed as a result of client or server crashes, because old versions of files will remain safely on disk.
Reference: [RaR81] <author> R. F. Rashid and G. G. Robertson, </author> <title> ``Accent: A communication oriented network operating system kernel'', </title> <booktitle> Proceedings of the 8th Symposium on Operating Systems Principles, </booktitle> <year> 1981, </year> <pages> 164-175. </pages>
Reference-contexts: This approach eliminates the file cache entirely; the standard page replacement mechanisms automatically balance physical memory usage between file and program information. Mapped files were first used in Multics [BCD72, DaD68] and TENEX [BBM72, Mur72]. More recently they have been implemented in Pilot [Red80], Accent <ref> [RaR81, RaF86] </ref>, Apollo [LLH85, Lea83] and Mach [Ras87]. Mapped files present a much different interface than systems such as UNIX that keep the file system and virtual memory system separate. Under the UNIX approach, users use system calls such as read and write to access file data.
Reference: [RaF86] <author> R. F. Rashid and R. Fitzgerald, </author> <title> ``The Integration of Virtual Memory Management and Interprocess Communication in Accent'', </title> <journal> Trans. Computer Systems 4, </journal> <month> 2 (May </month> <year> 1986), </year> <pages> 147-177. </pages>
Reference-contexts: This approach eliminates the file cache entirely; the standard page replacement mechanisms automatically balance physical memory usage between file and program information. Mapped files were first used in Multics [BCD72, DaD68] and TENEX [BBM72, Mur72]. More recently they have been implemented in Pilot [Red80], Accent <ref> [RaR81, RaF86] </ref>, Apollo [LLH85, Lea83] and Mach [Ras87]. Mapped files present a much different interface than systems such as UNIX that keep the file system and virtual memory system separate. Under the UNIX approach, users use system calls such as read and write to access file data.
Reference: [Ras88] <author> R. Rashid, </author> <type> Personal Communication, </type> <month> Mar. </month> <year> 1988. </year>
Reference-contexts: Figure 7-4 shows that, with this fault cost, COW-COR provides slightly worse performance than copy-on-fork. The fault cost in Sprite is much higher than the fault cost in the Mach operating system <ref> [Ras88] </ref>. In Mach the page fault cost is less than 10% of the copying cost. If Sprite were able to attain the same low fault cost as Mach, forks would be 15 to 20 percent faster with COW-COR than with copy-on-fork.
Reference: [Ras87] <author> R. Rashid, et al., </author> <title> ``Machine-Independent Virtual Memory Management for Paged Uniprocessor and Multiprocessor Architectures'', </title> <booktitle> Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS II), </booktitle> <month> Oct. </month> <year> 1987, </year> <pages> 31-39. </pages>
Reference-contexts: Mapped files were first used in Multics [BCD72, DaD68] and TENEX [BBM72, Mur72]. More recently they have been implemented in Pilot [Red80], Accent [RaR81, RaF86], Apollo [LLH85, Lea83] and Mach <ref> [Ras87] </ref>. Mapped files present a much different interface than systems such as UNIX that keep the file system and virtual memory system separate. Under the UNIX approach, users use system calls such as read and write to access file data. <p> Copy-on-write saves not only copying of pages in memory, but also copying of pages that are on backing store. Copy-on-write has been implemented in several systems, with the earliest being TENEX [BBM72, Mur72] and one of the most recent being Mach <ref> [Ras87] </ref>. This chapter describes a simple copy-on-write mechanism that I have implemented as part of Sprite. <p> Previous Work The original idea of copy-on-write emerged over 15 years ago with TENEX [BBM72, Mur72]. Since then it has been implemented in several systems <ref> [Akh87, GMS87, Ras87, SCC86] </ref>. The Mach operating system [Ras87] is one of the most recent systems to implement copy-on-write, and is one of the few whose implementation of copy-on-write has been published in detail. <p> Previous Work The original idea of copy-on-write emerged over 15 years ago with TENEX [BBM72, Mur72]. Since then it has been implemented in several systems [Akh87, GMS87, Ras87, SCC86]. The Mach operating system <ref> [Ras87] </ref> is one of the most recent systems to implement copy-on-write, and is one of the few whose implementation of copy-on-write has been published in detail. Copy-on-write is an integral part of Mach; it is the basis for both efficient message transmission and efficient process creation. <p> Much of the complexity involved in Mach memory management is involved in preventing long chains of shadow objects <ref> [Ras87] </ref>. In particular, the extraneous shadow objects shown in Figure 7-1 that are left over after a child exits can be eliminated by moving the pages in the shadow objects into the original object. 138 7.4.
Reference: [Red80] <author> D. D. Redell, et al., </author> <title> ``Pilot: An Operating System for a Personal Computer'', </title> <journal> Communications of the ACM 23, </journal> <month> 2 (Feb. </month> <year> 1980), </year> <pages> 81-92. </pages>
Reference-contexts: This approach eliminates the file cache entirely; the standard page replacement mechanisms automatically balance physical memory usage between file and program information. Mapped files were first used in Multics [BCD72, DaD68] and TENEX [BBM72, Mur72]. More recently they have been implemented in Pilot <ref> [Red80] </ref>, Accent [RaR81, RaF86], Apollo [LLH85, Lea83] and Mach [Ras87]. Mapped files present a much different interface than systems such as UNIX that keep the file system and virtual memory system separate. Under the UNIX approach, users use system calls such as read and write to access file data.
Reference: [Rif86] <author> A. P. Rifkin, et al., </author> <title> ``RFS Architectural Overview'', </title> <booktitle> USENIX Association 1986 Summer Conference Proceedings, </booktitle> <year> 1986. </year> <month> 169 </month>
Reference-contexts: When a client locks a file, it compares its version number for the file with the version number returned by the server. If the version numbers do not match, then the client removes the file's blocks from its memory. 2.5.1.6. RFS The RFS system <ref> [Rif86] </ref> handles both sequential and concurrent write-sharing. Sequential write-sharing is handled by using a write-through writing policy and by contacting the server whenever a file is opened to ensure that the cached copy is up to date. 24 RFS handles concurrent write-sharing by disabling client caching when it occurs.
Reference: [RiT74] <author> D. M. Ritchie and K. Thompson, </author> <title> ``The UNIX Time-Sharing System'', </title> <journal> Comm. of the ACM 17, </journal> <month> 7 (July </month> <year> 1974), </year> <month> 365-375.. </month>
Reference-contexts: We hope that Sprite will be suitable for networks of up to a few hundred of these workstations. The interface that Sprite provides to user processes is much like that provided by UNIX <ref> [RiT74] </ref>. The file system appears as a single shared hierarchy accessible equally by processes on any workstation in the network (see [WeO86] for information on how the name space is managed). The user interface to the file system is through UNIX-like system calls such as open, close, read, and write.
Reference: [San85] <author> R. Sandberg, et al., </author> <title> ``Design and Implementation of the Sun Network Filesystem'', </title> <booktitle> Proceedings of the USENIX 1985 Summer Conference, </booktitle> <month> JUNE </month> <year> 1985, </year> <pages> 119-130. </pages>
Reference-contexts: Client caches reduced the server utilization from about 5-27% per active client to only about 1-12% per active client. Since normal users are rarely active, my measurements suggest that a single server should be able to support at least 30 clients. In comparisons with Sun's Network File System <ref> [San85] </ref> and the Andrew file system [Sat85], Sprite completed a file-intensive benchmark 30-35% faster than the other systems. Sprite's server utilization was three times less than NFS but three times higher than Andrew. <p> In this section I will concentrate on data caching, and in Section 2.5 I will explore the impact of name caching. Systems that have implemented client caching have taken one of two approaches: cache file blocks in memory (e.g. LOCUS [PoW85, Wal83] and Sun's Network File System (NFS) <ref> [San85] </ref>) or cache whole files on a local disk (e.g. Andrew [Mor86, Sat85] and Cedar [SGN85]). The advantage of caching on a local disk is that local disks are generally much larger than physical memories. However, caching in main memory has numerous advantages over caching on a local disk. <p> He measured the traffic on a 10-Mbit Ethernet over a 24 hour period. He determined that two Sun-3 workstations (a Sun-3/180 server and a Sun-3/50 client each with 4 Mbytes of memory) running UNIX with Sun's Network File System (NFS) <ref> [San85] </ref> can utilize over 20% of the Ethernet. Since the workstations that Gusella measured had smaller memories than the Sprite workstations and NFS does not utilize file data caches as effectively as Sprite, I would not expect Sprite to exhibit the same loads that were measured by Gusella. <p> In addition to measuring the absolute performance of Sprite, I also compared the performance of the Sprite file system, the Andrew file system [Sat85], and Sun's Network File System <ref> [San85] </ref> for a particular file-intensive benchmark. I showed that Sprite completes the benchmark 30-35% faster than the other systems. Sprite's server utilization was one-third of NFS's utilization but three times Andrew's utilization. 73 CHAPTER 5 Writing Policies 5.1.
Reference: [Sat85] <author> M. Satyanarayanan, et al., </author> <title> ``The ITC Distributed File System: </title> <booktitle> Principles and Design'', Proceedings of the 10th Symp. on Operating System Prin., </booktitle> <year> 1985, </year> <pages> 35-50. </pages>
Reference-contexts: Since normal users are rarely active, my measurements suggest that a single server should be able to support at least 30 clients. In comparisons with Sun's Network File System [San85] and the Andrew file system <ref> [Sat85] </ref>, Sprite completed a file-intensive benchmark 30-35% faster than the other systems. Sprite's server utilization was three times less than NFS but three times higher than Andrew. <p> Systems that have implemented client caching have taken one of two approaches: cache file blocks in memory (e.g. LOCUS [PoW85, Wal83] and Sun's Network File System (NFS) [San85]) or cache whole files on a local disk (e.g. Andrew <ref> [Mor86, Sat85] </ref> and Cedar [SGN85]). The advantage of caching on a local disk is that local disks are generally much larger than physical memories. However, caching in main memory has numerous advantages over caching on a local disk. First, main-memory caches permit workstations to be diskless. <p> Note that Cedar does not satisfy my definition of cache consistency because once a file is open reads are not guaranteed to return the most recently written data. 2.5.1.3. Andrew Andrew <ref> [Mor86, Sat85] </ref> only supports sequential write-sharing. If two clients are undergoing concurrent write-sharing, then clients will not see a consistent view of the file. <p> Since normal users are rarely active, my measurements suggest that a single server should be able to support at least 30 clients. In addition to measuring the absolute performance of Sprite, I also compared the performance of the Sprite file system, the Andrew file system <ref> [Sat85] </ref>, and Sun's Network File System [San85] for a particular file-intensive benchmark. I showed that Sprite completes the benchmark 30-35% faster than the other systems. Sprite's server utilization was one-third of NFS's utilization but three times Andrew's utilization. 73 CHAPTER 5 Writing Policies 5.1.
Reference: [SGN85] <author> M. Schroeder, D. Gifford and R. Needham, </author> <title> ``A Caching File System for a Programmer's Workstation'', </title> <booktitle> Proceedings of the 10th Symp. on Operating System Prin., </booktitle> <month> Dec. </month> <year> 1985, </year> <pages> 25-34. </pages>
Reference-contexts: Systems that have implemented client caching have taken one of two approaches: cache file blocks in memory (e.g. LOCUS [PoW85, Wal83] and Sun's Network File System (NFS) [San85]) or cache whole files on a local disk (e.g. Andrew [Mor86, Sat85] and Cedar <ref> [SGN85] </ref>). The advantage of caching on a local disk is that local disks are generally much larger than physical memories. However, caching in main memory has numerous advantages over caching on a local disk. First, main-memory caches permit workstations to be diskless. <p> Otherwise it will verify its version with the file's server and flush its cache if necessary. 22 2.5.1.2. Cedar The Cedar file system <ref> [SGN85] </ref> provides consistency through the use of ``immutable files.'' Each time that a file is modified, a new version of the file is created. When a file is opened, a user specifies which version of the file to use. <p> The solutions that have been used in other file systems to provide a higher measure of reliability than Sprite's are based on file versions <ref> [CaW86, SGN85] </ref> or atomic transactions [BKT85, PoW85]. The systems that use file versions create a new version each time that a file is written. Thus, files will never be destroyed as a result of client or server crashes, because old versions of files will remain safely on disk.
Reference: [Smi85] <author> A. J. Smith, </author> <title> ``Disk Cache Miss Ratio Analysis and Design Considerations'', </title> <journal> Trans. Computer Systems 3, </journal> <month> 3 (Aug. </month> <year> 1985), </year> <pages> 161-203. </pages>
Reference-contexts: He also did a trace-driven analysis of file activity in a timeshared UNIX 4.2 BSD system, and his results were nearly identical to Ousterhout's results. One other study of disk caching was done by Smith, who used trace data from IBM mainframes <ref> [Smi85] </ref>. Smith reported reductions in disk traffic similar to those reported in Ousterhout's study even though his data was much different. Unfortunately Smith's data did not distinguish read accesses from write accesses. Thus, he did not determine the impact of the writing policy on the traffic ratio.
Reference: [SSS85] <institution> Sun-3 Architecture Manual, Sun Microsystems Inc., </institution> <month> July </month> <year> 1985. </year>
Reference-contexts: Acquiring ownership causes the page to be removed from all other workstation's memories. Another potential problem with mapped files is that the hardware may make mapping difficult. Some newer workstations use a virtually addressed hardware cache <ref> [Hil86, Kel86, SSS85] </ref>. These caches do not support synonyms multiple virtual addresses pointing to the same physical address. <p> The last column is the percentage of pages that would have been copied under a pure copy-on-write scheme; it is the second column added to the product of the third and fourth columns. architectures with virtually addressed caches, such as the Sun-3 <ref> [SSS85] </ref>, Sun-4 [Kel86] and SPUR [Hil86] architectures. In these machines, protection bits are stored along with the data in individual cache lines. To change the protection on a page, the operat ing system must first modify the page table entry, then flush all of the page's lines from the cache.
Reference: [SCC86] <author> E. W. Sznyter, P. Clancy and J. Crossland, </author> <title> ``A New Virtual-Memory Implementation for Unix'', </title> <booktitle> Proceedings of the USENIX 1986 Summer Conference, </booktitle> <month> JUNE </month> <year> 1986, </year> <pages> 81-88. </pages>
Reference-contexts: Previous Work The original idea of copy-on-write emerged over 15 years ago with TENEX [BBM72, Mur72]. Since then it has been implemented in several systems <ref> [Akh87, GMS87, Ras87, SCC86] </ref>. The Mach operating system [Ras87] is one of the most recent systems to implement copy-on-write, and is one of the few whose implementation of copy-on-write has been published in detail.
Reference: [Tho87] <author> J. G. Thompson, </author> <title> Efficient Analysis of Caching Systems, </title> <type> Phd Thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1987. </year>
Reference-contexts: However, results from four studies of UNIX timesharing traces can be used to help predict the best writing policy for clients and servers. In addition to the two previously-mentioned studies by Ousterhout and Kent there are also studies that were done by Floyd [Flo86] and Thompson <ref> [Tho87] </ref>. Floyd's 15 studies are nearly identical to Ousterhout's studies so I will not mention them further. Thompson's study was a follow-on study to the study done by Ousterhout et al.; Thompson's results are based on very detailed traces of UNIX timesharing systems. <p> The amount of file sharing that occurs has an impact on the importance of cache consistency. Jim Thompson analyzed the amount of file sharing that occurred in a UNIX environment <ref> [Tho87] </ref> and got several interesting results: 20 Time C1 has file open for writingwriting open for C2 has file Time reading open for C1 has file C1 has file open for readingwriting open for C2 has file reading open for C1 has file Concurrent Write Sharing Sequential Write Sharing sequential write <p> Trace-Driven Analyses of Client Caching Jim Thompson used UNIX traces gathered from a single timeshared machine to perform a trace-driven simulation of the impact of client caching on performance <ref> [Tho87] </ref>. In his simulations every user on the timesharing system represents a different client. His measurements depend on which of 5 cache consistency algorithms are used; all of his algorithms provide consistency for both concurrent and sequential write-sharing. <p> These results strengthened our hypotheses about the effectiveness of client caching and our simple cache consistency algorithm, and indicated to us that we should proceed with the implementation. 3.3.3.2. Simulation of Several Mechanisms Jim Thompson <ref> [Tho87] </ref> did a much more detailed simulation of cache consistency policies than we did. He simulated not only the Sprite policy, but several other policies as well.
Reference: [Wal83] <editor> B. Walker, et al., </editor> <booktitle> ``The LOCUS Distributed Operating System'', Proceedings of the 9th Symp. on Operating System Prin. </booktitle> <volume> 17, </volume> <month> 5 (Nov. </month> <year> 1983), </year> <pages> 49-70. 170 </pages>
Reference-contexts: In this section I will concentrate on data caching, and in Section 2.5 I will explore the impact of name caching. Systems that have implemented client caching have taken one of two approaches: cache file blocks in memory (e.g. LOCUS <ref> [PoW85, Wal83] </ref> and Sun's Network File System (NFS) [San85]) or cache whole files on a local disk (e.g. Andrew [Mor86, Sat85] and Cedar [SGN85]). The advantage of caching on a local disk is that local disks are generally much larger than physical memories. <p> Sequential write-sharing is supported by guaranteeing that, once a file is closed, all data is back on the server, and by ensuring that a client is notified by the server whenever the client's cached copy becomes out-of-date. 2.5.1.4. LOCUS LOCUS <ref> [PoW85, Wal83] </ref> supports both concurrent and sequential write-sharing. It uses a complex mechanism based on passing tokens between workstations that are accessing the file. There are two types of tokens: read and write. A client must possess a token in order to access a file.
Reference: [Wel86] <author> B. B. Welch, </author> <title> ``The Sprite Remote Procedure Call System'', </title> <note> Technical Report UCB/Computer Science Dpt. 86/302, </note> <institution> University of California, Berkeley, </institution> <month> June </month> <year> 1986. </year>
Reference-contexts: Although Sprite appears similar in function to UNIX, we have completely re-implemented the kernel in order to provide better network integration. In particular, Sprite's implementation is based around a simple kernel-to-kernel remote-procedure-call (RPC) facility <ref> [Wel86] </ref>, which allows kernels on different workstations to request services of each other using a protocol similar to the one described by Birrell and Nel-son [BiN84]. The Sprite file system uses the RPC mechanism extensively for cache management. 1.3.
Reference: [WeO86] <author> B. B. Welch and J. K. Ousterhout, </author> <title> ``Prefix Tables: A Simple Mechanism for Locating Files in a Distributed Filesystem'', </title> <booktitle> Proc. of the 6th Int'l Conf. on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1986, </year> <pages> 184-189. </pages>
Reference-contexts: The interface that Sprite provides to user processes is much like that provided by UNIX [RiT74]. The file system appears as a single shared hierarchy accessible equally by processes on any workstation in the network (see <ref> [WeO86] </ref> for information on how the name space is managed). The user interface to the file system is through UNIX-like system calls such as open, close, read, and write. Although Sprite appears similar in function to UNIX, we have completely re-implemented the kernel in order to provide better network integration.

References-found: 45

