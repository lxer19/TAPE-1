URL: http://www.cs.washington.edu/homes/eggers/Research/vso.ps
Refering-URL: http://www.cs.washington.edu/homes/eggers/Research/dc.html
Root-URL: 
Title: Evaluating Runtime-Compiled Value-Specific Optimizations 1 Evaluating Runtime-Compiled Value-Specific Optimizations  
Author: David Keppel, Susan J. Eggers and Robert R. Henry 
Date: 93-11-02  
Pubnum: Technical Report  
Abstract: Traditional compiler optimizations are either data-independent or optimize around common data values while retaining correct behavior for uncommon values. This paper examines value-specific data-dependent optimizations (VSO), where code is optimized at runtime around particular input values. Because VSO optimizes for the specific case, the resulting code is more efficient. However, since optimization is performed at runtime, the performance improvement must more than pay for the runtime compile costs. We describe two VSO implementation techniques and compare the performance of applications that have been implemented using both VSO and static code. The results demonstrate that VSO produces better code and often for reasonable input sizes. The machine-independent implementations showed speedups of up to 1.5 over static C code, and the machine-dependent versions showed speedups of up to 4.3 over static assembly code. 
Abstract-found: 1
Intro-found: 1
Reference: [Bro76] <author> P. J. Brown. </author> <title> Throw-away Compiling. </title> <journal> Software - Practice and Experience, </journal> <volume> 6 </volume> <pages> 423-434, </pages> <year> 1976. </year>
Reference-contexts: In this paper we use runtime code generation (RTCG) to produce code that is better than the best possible static code. Some previous studies have used RTCG to dynamically improve unoptimized code produced by a fast compiler for an edit-compile-debug environment <ref> [Han73, Bro76] </ref>. In those cases, however, the best dynamically-generated code was only as good as the best statically-generated code. Other studies have used dynamic type information to eliminate the high overheads of type dispatch in dynamically-typed languages [Mit70, DB77, GW78, DS84, CUL89].
Reference: [CK93] <author> R. F. Cmelik and D. Keppel. Shade: </author> <title> A Fast Instruction-Set Simulator for Execution Profiling. </title> <type> Technical Report UWCSE 93-06-06, </type> <institution> University of Washington, </institution> <year> 1993. </year>
Reference-contexts: In addition, despite the cost of optimizing at runtime, the speedups were usually achieved with reasonable input sizes. Third, al though VSO has usually been performed with machine-dependent code <ref> [Tho68, PLR85, Loc87, MP89, KEH91, CK93] </ref>, we show the above speedups using retargetable code generation, linking and instruction cache flushing. Fourth, we describe what optimizations are actually performed and show that runtime-compiled VSO can enable optimizations that can't be performed statically. <p> As in Figure 2, p0 and p1 are pointers to the records being compared. parts of the compiler. Template compilers have been used by most systems that perform VSO <ref> [Tho68, PLR85, Hol87, Loc87, KL89, MP89, Kes90, CK93] </ref>. INT fragment in Figure 2.
Reference: [CUL89] <author> C. Chambers, D. Ungar, and E. Lee. </author> <title> An Efficient Implementation of SELF, a Dynamically-Typed Object-Oriented Language Based on Prototypes. </title> <booktitle> OOPSLA '89 Proceedings, </booktitle> <pages> pages 49-70, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: In those cases, however, the best dynamically-generated code was only as good as the best statically-generated code. Other studies have used dynamic type information to eliminate the high overheads of type dispatch in dynamically-typed languages <ref> [Mit70, DB77, GW78, DS84, CUL89] </ref>. With VSO, types are known statically; yet we can still optimize using the specific values of variables. 1 The rest of this paper is organized as follows: Section 2 briefly describes why VSO fell from favor and why it is again useful. <p> The application programmer still writes an application-specific compiler, but its output is a machine-independent IR. Executable machine code is generated from a directed acyclic graph IR using a retargetable code generator. IR code generators are primarily used by dynamic compilers, which are already IR-based <ref> [Han73, CUL89, VP89] </ref>; a few systems that are not inherently IR-based also use IR code generators [PHH88]. The IR compiler has a number of advantages over a template compiler. First, a portable IR frees the application programmer from writing machine-dependent code.
Reference: [DB77] <author> E. J. Van Dyke and K. A. Van Bree. </author> <title> A Dynamic Incremental Compiler for an Interpretive Language. </title> <journal> Hewlett-Packard Journal, </journal> <pages> pages 17-24, </pages> <month> July </month> <year> 1977. </year>
Reference-contexts: In those cases, however, the best dynamically-generated code was only as good as the best statically-generated code. Other studies have used dynamic type information to eliminate the high overheads of type dispatch in dynamically-typed languages <ref> [Mit70, DB77, GW78, DS84, CUL89] </ref>. With VSO, types are known statically; yet we can still optimize using the specific values of variables. 1 The rest of this paper is organized as follows: Section 2 briefly describes why VSO fell from favor and why it is again useful.
Reference: [DS84] <author> P. Deutsch and A. M. Schiffman. </author> <title> Efficient Implementation of the Smalltalk-80 System. </title> <booktitle> 11th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 297-302, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: In those cases, however, the best dynamically-generated code was only as good as the best statically-generated code. Other studies have used dynamic type information to eliminate the high overheads of type dispatch in dynamically-typed languages <ref> [Mit70, DB77, GW78, DS84, CUL89] </ref>. With VSO, types are known statically; yet we can still optimize using the specific values of variables. 1 The rest of this paper is organized as follows: Section 2 briefly describes why VSO fell from favor and why it is again useful.
Reference: [FH91] <author> C. W. Fraser and R. R. Henry. </author> <title> Hard-Coding Bottom-Up Code Generation Tables to Save Time and Space. </title> <journal> Software Practice and Experience, </journal> <volume> 21(1) </volume> <pages> 1-12, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Finally, there were still many opportunities for improving static code, so development time was best spent improving static optimizations. Technology and workload changes make VSO attractive once again. First, advances in software methodology have solved some portability problems; for example, machine dependencies have been encapsulated with re-targetable code generators <ref> [Sta89, FH91] </ref> and portable interfaces for instruction cache flushing [Kep91]. Moreover, these tools are now available as library services rather than being bundled in compilers and so on. Sec 1 We note that there is a fuzzy distinction between type and value; for example, VSO can also use range information. <p> We estimate that generating machine code directly from IR takes one thousand instructions per generated instruction. This is conservative, since locally-optimal code generators can produce assembly code in a few hundred instructions per generated instruction <ref> [FH91] </ref>. Evaluating Runtime-Compiled Value-Specific Optimizations 7 Our code generation cost estimates use a fast and simple code generator rather than a slower code generator with procedural optimizations, because, as we show in Section 5.3, a simple code generator can produce good code with VSO.
Reference: [FST91] <author> A. Fyfe, I. Soleimanipour, and V. Tatkar. </author> <title> Compiling from Saved State: Fast Incremental Compilation with Traditional Unix Compilers. </title> <booktitle> USENIX, </booktitle> <pages> pages 161-171, </pages> <month> Winter </month> <year> 1991. </year>
Reference-contexts: into an "empty" function fragment to build an IR tree that represents a predicate like that in the corresponding parameters in the blank function fragment. off is a runtime constant filled in during runtime compilation. ment to IR and symbol table information which would be saved away for later use <ref> [FST91] </ref>. At runtime the fragments would be combined as with our IR compiler. Since current languages don't provide constructs for building fragments, we built the IR fragments by hand, avoiding the need to write or extend a compiler.
Reference: [GW78] <author> L. J. Guibas and D. K. Wyatt. </author> <title> Compilation and Delayed Evaluation in APL. </title> <booktitle> Fifth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1-8, </pages> <year> 1978. </year>
Reference-contexts: In those cases, however, the best dynamically-generated code was only as good as the best statically-generated code. Other studies have used dynamic type information to eliminate the high overheads of type dispatch in dynamically-typed languages <ref> [Mit70, DB77, GW78, DS84, CUL89] </ref>. With VSO, types are known statically; yet we can still optimize using the specific values of variables. 1 The rest of this paper is organized as follows: Section 2 briefly describes why VSO fell from favor and why it is again useful.
Reference: [Han73] <author> G. J. Hansen. </author> <title> Adaptive Systems for the Dynamic Run-Time Optimization of Programs. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <month> March </month> <year> 1973. </year>
Reference-contexts: In this paper we use runtime code generation (RTCG) to produce code that is better than the best possible static code. Some previous studies have used RTCG to dynamically improve unoptimized code produced by a fast compiler for an edit-compile-debug environment <ref> [Han73, Bro76] </ref>. In those cases, however, the best dynamically-generated code was only as good as the best statically-generated code. Other studies have used dynamic type information to eliminate the high overheads of type dispatch in dynamically-typed languages [Mit70, DB77, GW78, DS84, CUL89]. <p> The application programmer still writes an application-specific compiler, but its output is a machine-independent IR. Executable machine code is generated from a directed acyclic graph IR using a retargetable code generator. IR code generators are primarily used by dynamic compilers, which are already IR-based <ref> [Han73, CUL89, VP89] </ref>; a few systems that are not inherently IR-based also use IR code generators [PHH88]. The IR compiler has a number of advantages over a template compiler. First, a portable IR frees the application programmer from writing machine-dependent code.
Reference: [Hen87] <author> R. R. Henry. </author> <title> Code Generation by Table Lookup. </title> <type> Technical Report 87-07-07, </type> <institution> University of Washing-ton Computer Science, </institution> <year> 1987. </year>
Reference-contexts: Specializing the retargetable code generator and IR to the application <ref> [Hen87] </ref> and target machine would improve the template compilers' performance. Our reported template compiler costs are conservative. We focused on code quality and ease of development, and ignored compiler speed.
Reference: [HLL + 93] <author> M. D. Hill, J. R. Larus, A. R. Leback, M. Tal-luri, and D. A. Wood. </author> <title> Wisconsin Architectural Research Tool Set. </title> <journal> Computer Architecture News, </journal> <volume> 21(4), </volume> <month> August </month> <year> 1993. </year>
Reference-contexts: For cache simulation, the asymptotic speedup is smaller, only 5-10%, but the breakeven point is at only 1,000 references. Since memory reference traces are typically millions of references, VSO "always" pays. Many other cache simulators read text input that is then converted <ref> [HLL + 93] </ref>. Our simulators read trace input in a compact, binary form that is shared from run to run of a simulator. The binary form reduces both the I/O and per-run conversion costs.
Reference: [HO91] <author> W. Wilson Ho and Ronald A Olsson. </author> <title> An Approach to Genuine Dynamic Linking. </title> <journal> Software-Practice and Experience, </journal> <volume> 21(4) </volume> <pages> 375-390, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: VSO IR manipulation is more complicated than with a traditional IR, because the VSO IR supports holes and hole filling operations; our IR is even more general, supporting hole deletion, which is unused in practice. Similarly, the linker <ref> [HO91] </ref> links files, rather than code generated in-core, so there is overhead for opening and reading files, converting disk formats to core formats, etc. (For example, 60% of the linker time is typically spent in the open and read system calls.) Thus, we expect that a linker designed for in-core linking
Reference: [Hol87] <author> G. J. Holzmann. </author> <title> PICO A Picture Editor. </title> <journal> AT&T Technical Journal, </journal> <volume> 66(2) </volume> <pages> 2-13, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: As in Figure 2, p0 and p1 are pointers to the records being compared. parts of the compiler. Template compilers have been used by most systems that perform VSO <ref> [Tho68, PLR85, Hol87, Loc87, KL89, MP89, Kes90, CK93] </ref>. INT fragment in Figure 2.
Reference: [KEH91] <author> D. Keppel, S. J. Eggers, and R. R. Henry. </author> <title> A Case for Runtime Code Generation. </title> <type> Technical Report UWCSE 91-11-04, </type> <institution> University of Washington Department of Computer Science and Engineering, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: In addition, despite the cost of optimizing at runtime, the speedups were usually achieved with reasonable input sizes. Third, al though VSO has usually been performed with machine-dependent code <ref> [Tho68, PLR85, Loc87, MP89, KEH91, CK93] </ref>, we show the above speedups using retargetable code generation, linking and instruction cache flushing. Fourth, we describe what optimizations are actually performed and show that runtime-compiled VSO can enable optimizations that can't be performed statically. <p> VSO was used where performance was important; implemented as ad hoc self-modifying code, these sequences were often smaller and faster <ref> [KEH91] </ref>. Changing technology reduced the demand for VSO. Memories grew, reducing the I/O component of memory access times. The proliferation of architectures and implementations, coupled with software maintainance costs, meant portability became an issue. Portability was achieved using high-level languages, and most lacked constructs for expressing VSO.
Reference: [Kep91] <author> D. Keppel. </author> <title> A Portable Interface for On-The-Fly Instruction Space Modification. </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 86-95, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Technology and workload changes make VSO attractive once again. First, advances in software methodology have solved some portability problems; for example, machine dependencies have been encapsulated with re-targetable code generators [Sta89, FH91] and portable interfaces for instruction cache flushing <ref> [Kep91] </ref>. Moreover, these tools are now available as library services rather than being bundled in compilers and so on. Sec 1 We note that there is a fuzzy distinction between type and value; for example, VSO can also use range information. <p> For implementations based on the IR compiler, the cost is composed of the times to build the IR (build in Table 2), generate machine code (cgen) using a 20 MIPS processor, 9 and link the code into the running program (link). Cache flush times were negligible <ref> [Kep91] </ref>. For the debugger, there is an additional cost (reflected in the total column) to move code and data from the debugger to the debuggee. Build, code generation and link costs have similar ratios for all programs, with code generation time typically the smallest.
Reference: [Kes90] <author> P. Kessler. </author> <title> Fast Breakpoints: </title> <booktitle> Design and Implementation. Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 78-84, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The code that resumes execution also needs to simulate the instruction that was displaced by the patch. Static code alternatives, therefore, must use an interpreter to evaluate the breakpoint conditional and simulate the displaced instruction. VSO code evaluates the conditional directly and often executes the displaced instruction directly <ref> [Kes90] </ref>. Data Decompression The compression algorithm is optimized for fast decompression using table lookup. The table maps bytes in the compressed input data stream to strings of bytes in the decompressed output stream. <p> As in Figure 2, p0 and p1 are pointers to the records being compared. parts of the compiler. Template compilers have been used by most systems that perform VSO <ref> [Tho68, PLR85, Hol87, Loc87, KL89, MP89, Kes90, CK93] </ref>. INT fragment in Figure 2. <p> Thus, using VSO does not affect the observed time to set a breakpoint. Previous conditional breakpoint studies have compared VSO with implementations that require several protection domain crossings on each invocation of the conditional expression <ref> [Kes90] </ref>. Even with faster IPC mechanisms, crossing costs would still dominate. Here, we make the fairest comparison by implementing both static and VSO'd versions so that the conditional expression executes in the debuggee, without domain crossings. Thus, we report just the speedup due to VSO.
Reference: [KL89] <author> P. J. Koopman, Jr. and P. Lee. </author> <title> A Fresh Look at Combinator Graph Reduction (Or, Having a TI-GRE by the Tail). </title> <booktitle> ACM SIGPLAN 1989 Symposium on Programming Language Design and Implementation, </booktitle> <pages> pages 110-119, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: As in Figure 2, p0 and p1 are pointers to the records being compared. parts of the compiler. Template compilers have been used by most systems that perform VSO <ref> [Tho68, PLR85, Hol87, Loc87, KL89, MP89, Kes90, CK93] </ref>. INT fragment in Figure 2.
Reference: [Loc87] <author> B. N. Locanthi. </author> <title> Fast BitBlt With asm() and CPP. </title> <booktitle> European Unix Users Group Conference Proceedings (EUUG), </booktitle> <month> September </month> <year> 1987. </year>
Reference-contexts: In addition, despite the cost of optimizing at runtime, the speedups were usually achieved with reasonable input sizes. Third, al though VSO has usually been performed with machine-dependent code <ref> [Tho68, PLR85, Loc87, MP89, KEH91, CK93] </ref>, we show the above speedups using retargetable code generation, linking and instruction cache flushing. Fourth, we describe what optimizations are actually performed and show that runtime-compiled VSO can enable optimizations that can't be performed statically. <p> Bitblt The bitblt (bit block transfer) algorithm is typically used as a graphics kernel operation, and is an example where VSO has been used successfully in the past <ref> [Loc87, PLR85] </ref>. The bitblt function has several parameters (source and destination alignment; transfer operation) that alter control flow in the innermost loop of a loop nest. These parameters are constant across any given call; therefore, control flow is the same for all iterations in any given invocation. <p> As in Figure 2, p0 and p1 are pointers to the records being compared. parts of the compiler. Template compilers have been used by most systems that perform VSO <ref> [Tho68, PLR85, Hol87, Loc87, KL89, MP89, Kes90, CK93] </ref>. INT fragment in Figure 2. <p> Here, VSO would pay only for atypically large regions or if the VSO'd bitblt could be cached for later reuse. 8 VSO'd bitblt was profitable in other experiments <ref> [PLR85, Loc87] </ref>, because they used less-optimized static code and their compiler was hand-coded and machine-dependent and thus fast; yet even there, small regions used static code.
Reference: [May92] <author> T. </author> <month> May. </month> <type> Personal Communication, </type> <month> May </month> <year> 1992. </year>
Reference-contexts: Some versions of bitblt can merge regions with different bit depths, color operations, and so on <ref> [May92] </ref>. Since case-by-case specialization leads to combinatorial space explosion, advanced static optimizations are probably impossible with these versions of bitblt. 9 The estimated code generation costs are computed by multiplying the estimated instruction count by the processor's speed. are specialized to both the application and the target machine.
Reference: [Mit70] <author> J. G. Mitchell. </author> <title> The Design and Construction of Flexible and Efficient Interactive Programming Systems. </title> <type> Technical Report Ph.D. Dissertation, </type> <institution> Carnegie-Mellon University, </institution> <year> 1970. </year>
Reference-contexts: In those cases, however, the best dynamically-generated code was only as good as the best statically-generated code. Other studies have used dynamic type information to eliminate the high overheads of type dispatch in dynamically-typed languages <ref> [Mit70, DB77, GW78, DS84, CUL89] </ref>. With VSO, types are known statically; yet we can still optimize using the specific values of variables. 1 The rest of this paper is organized as follows: Section 2 briefly describes why VSO fell from favor and why it is again useful.
Reference: [MP89] <author> H. Massalin and C. Pu. </author> <title> Threads and Input/Output in the Synthesis Kernel. </title> <booktitle> Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 191-201, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: In addition, despite the cost of optimizing at runtime, the speedups were usually achieved with reasonable input sizes. Third, al though VSO has usually been performed with machine-dependent code <ref> [Tho68, PLR85, Loc87, MP89, KEH91, CK93] </ref>, we show the above speedups using retargetable code generation, linking and instruction cache flushing. Fourth, we describe what optimizations are actually performed and show that runtime-compiled VSO can enable optimizations that can't be performed statically. <p> As in Figure 2, p0 and p1 are pointers to the records being compared. parts of the compiler. Template compilers have been used by most systems that perform VSO <ref> [Tho68, PLR85, Hol87, Loc87, KL89, MP89, Kes90, CK93] </ref>. INT fragment in Figure 2.
Reference: [Par91] <institution> Proceedings of the Symposium on Partial Evaluation and Semantics-Based Program Manipulation. SIGPLAN Notices, </institution> <month> 26(9), September </month> <year> 1991. </year>
Reference-contexts: Thus, for sufficiently large inputs, x, the VSO'd code will be executed enough times that the overall cost, y, will be lower than for statically-optimized code. VSO is partial evaluation <ref> [Par91] </ref> applied at program Evaluating Runtime-Compiled Value-Specific Optimizations 2 runtime. However, all data values are available at run-time; applying partial evaluation blindly can cause the partial evaluator to slowly consume all input values, instead of producing code optimized around the key values.
Reference: [PHH88] <author> S. Przybylski, M. Horowitz, and J. Hennessy. </author> <title> Performance Tradeoffs in Cache Design. </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 290-298, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: The simulation produces cache behavior metrics, such as cache hit ratios. The core table lookup must be flexible, to simulate a variety of cache sizes, associativities, etc. VSO builds a cache lookup function that is optimized to the particular line size and associativity 2 of the cache being simulated <ref> [PHH88] </ref>. Debugging Debuggers implement conditional break points, which halt a program if certain program variables satisfy a user-specified condition. The debugger detects that a conditional breakpoint has been reached, evaluates the conditional expression, and, if the program is continuing, resumes execution. <p> Executable machine code is generated from a directed acyclic graph IR using a retargetable code generator. IR code generators are primarily used by dynamic compilers, which are already IR-based [Han73, CUL89, VP89]; a few systems that are not inherently IR-based also use IR code generators <ref> [PHH88] </ref>. The IR compiler has a number of advantages over a template compiler. First, a portable IR frees the application programmer from writing machine-dependent code. Second, simple optimizers can rewrite the IR, which also frees the programmer from worrying about detailed optimizations.
Reference: [PLR85] <author> R. Pike, B. N. Locanthi, and J. F. Reiser. </author> <title> Hardware/Software Trade-offs for Bitmap Graphics on the Blit. </title> <journal> Software Practice and Experience, </journal> <volume> 15(2) </volume> <pages> 131-151, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: In addition, despite the cost of optimizing at runtime, the speedups were usually achieved with reasonable input sizes. Third, al though VSO has usually been performed with machine-dependent code <ref> [Tho68, PLR85, Loc87, MP89, KEH91, CK93] </ref>, we show the above speedups using retargetable code generation, linking and instruction cache flushing. Fourth, we describe what optimizations are actually performed and show that runtime-compiled VSO can enable optimizations that can't be performed statically. <p> Bitblt The bitblt (bit block transfer) algorithm is typically used as a graphics kernel operation, and is an example where VSO has been used successfully in the past <ref> [Loc87, PLR85] </ref>. The bitblt function has several parameters (source and destination alignment; transfer operation) that alter control flow in the innermost loop of a loop nest. These parameters are constant across any given call; therefore, control flow is the same for all iterations in any given invocation. <p> As in Figure 2, p0 and p1 are pointers to the records being compared. parts of the compiler. Template compilers have been used by most systems that perform VSO <ref> [Tho68, PLR85, Hol87, Loc87, KL89, MP89, Kes90, CK93] </ref>. INT fragment in Figure 2. <p> Here, VSO would pay only for atypically large regions or if the VSO'd bitblt could be cached for later reuse. 8 VSO'd bitblt was profitable in other experiments <ref> [PLR85, Loc87] </ref>, because they used less-optimized static code and their compiler was hand-coded and machine-dependent and thus fast; yet even there, small regions used static code.
Reference: [Sta87] <author> R. M. Stallman. </author> <title> GDB Manual: The GNU Source-Level Debugger. Free Software Foundation, </title> <address> Cam-bridge, Massachusetts, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: REFERENCES 12 7 Acknowledgements Craig Chambers, Jim Larus and David Wall reviewed earlier versions of this paper and greatly improved its presentation. Thanks also to Robert Bedicheck for explaining the internals of gdb, from which our debugger is derived <ref> [Sta87] </ref>. This work was supported by NSF PYI Award #MIP-9058-439, NSF #CDA-8619-663 and Sun Microsystems.
Reference: [Sta89] <author> R. M. Stallman. </author> <title> Using and Porting GNU CC. Free Software Foundation, </title> <address> Cambridge, Mas-sachusetts, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: Finally, there were still many opportunities for improving static code, so development time was best spent improving static optimizations. Technology and workload changes make VSO attractive once again. First, advances in software methodology have solved some portability problems; for example, machine dependencies have been encapsulated with re-targetable code generators <ref> [Sta89, FH91] </ref> and portable interfaces for instruction cache flushing [Kep91]. Moreover, these tools are now available as library services rather than being bundled in compilers and so on. Sec 1 We note that there is a fuzzy distinction between type and value; for example, VSO can also use range information. <p> The VSO implementa tions using a template compiler provide a measure of the best possible performance. 4.1 Static Code The static code versions of each application were extensively hand-optimized. For example, key loops were unrolled by hand in order to turn our state-of-the-practice static compiler, GCC <ref> [Sta89] </ref>, into a state-of-the-art compiler. We feel this yields code close to the best code a static compiler could produce, and represents what real programmers do when faced with the challenge of speeding up their programs. <p> On the other hand, debugging has high enough speedups that its relatively high compile costs are paid back quickly. 5.3 Code Generation Strategies Our estimated code generation costs are based on the cost of a fast, locally-optimal code generator. The goal of -O0 optimization is "to produce code quickly" <ref> [Sta89] </ref>. Code is generated on a statement-by-statement basis and has many redundant loads and stores. It is thus worse than that produced by a fast, locally-optimal code generator. -O1 optimization performs common optimizations, such as CSE, jump optimization, register allocation, and delay slot filling.
Reference: [Tho68] <author> K. Thompson. </author> <title> Regular Expression Search Algorithm. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 11(6) </volume> <pages> 419-422, </pages> <month> June </month> <year> 1968. </year>
Reference-contexts: In addition, despite the cost of optimizing at runtime, the speedups were usually achieved with reasonable input sizes. Third, al though VSO has usually been performed with machine-dependent code <ref> [Tho68, PLR85, Loc87, MP89, KEH91, CK93] </ref>, we show the above speedups using retargetable code generation, linking and instruction cache flushing. Fourth, we describe what optimizations are actually performed and show that runtime-compiled VSO can enable optimizations that can't be performed statically. <p> As in Figure 2, p0 and p1 are pointers to the records being compared. parts of the compiler. Template compilers have been used by most systems that perform VSO <ref> [Tho68, PLR85, Hol87, Loc87, KL89, MP89, Kes90, CK93] </ref>. INT fragment in Figure 2.
Reference: [VP89] <author> S. Vegdahl and U. F. Pleban. </author> <title> The Runtime Environment for Screme, a Scheme Implementation on the 88000. </title> <booktitle> Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 172-182, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The application programmer still writes an application-specific compiler, but its output is a machine-independent IR. Executable machine code is generated from a directed acyclic graph IR using a retargetable code generator. IR code generators are primarily used by dynamic compilers, which are already IR-based <ref> [Han73, CUL89, VP89] </ref>; a few systems that are not inherently IR-based also use IR code generators [PHH88]. The IR compiler has a number of advantages over a template compiler. First, a portable IR frees the application programmer from writing machine-dependent code.
References-found: 28

