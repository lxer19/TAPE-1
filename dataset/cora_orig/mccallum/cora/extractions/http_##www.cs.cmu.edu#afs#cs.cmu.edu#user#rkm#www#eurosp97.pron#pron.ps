URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/rkm/www/eurosp97.pron/pron.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/rkm/www/speechbib.html
Root-URL: 
Email: Email: rkm@cs.cmu.edu  
Phone: Tel. +1 412 268 3344, FAX: +1 412 268 5576,  
Title: AUTOMATIC GENERATION OF CONTEXT-DEPENDENT PRONUNCIATIONS  
Author: Ravishankar, M. and Eskenazi, M. 
Address: Pittsburgh, PA-15213, USA.  
Affiliation: School of Computer Science Carnegie Mellon University,  
Abstract: We describe experiments in modelling the dynamics of fluent speech in which word pronunciations are modified by neighbouring context. Based on all-phone decoding of large volumes of training data, we automatically derive new word pronunciation, and context-dependent transformation rules for phone sequences. In contrast to existing techniques, the rules can be applied even to words not in the training set, and across word boundaries, thus modelling context-dependent behavior. We use the technique on the Wall Street Journal (WSJ) training data and apply the new pronunciations and rules to WSJ and broadcast news tests. The changes correct a significant portion of the errors they could potentially correct. But the transformations introduce a comparable number of new errors, indicating that perhaps stronger constraints on the application of such rules are needed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rabiner, </author> <title> L.R., A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, Readings in Speech Recognition, </title> <editor> Ed. </editor> <booktitle> Waibel&Lee, </booktitle> <pages> pp. 267-296. </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [2] <author> Katz, </author> <title> S.M., Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer, </title> <journal> IEEE Trans. on ASSP, </journal> <volume> Vol. ASSP-35, </volume> <month> Mar. </month> <pages> 87, pp. 400-401. </pages>
Reference: [3] <author> Ljolje, A. et al, </author> <title> The AT&T 60,000 Word Speech-ToText system, </title> <booktitle> Proc. DARPA Spoken Lang. Sys. Tech. Workshop, </booktitle> <month> Jan </month> <year> 1995, </year> <pages> pp. 162-165. </pages>
Reference: [4] <author> Sloboda, T., </author> <title> Dictionary Learning for Spontaneous Speech Recognition, </title> <booktitle> Proc. ICSLP, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: The basic principle relies on statistics gathered by processing a large set of training data using an all-phone recognizer. It has been tried in the past, for example in <ref> [4] </ref>, to tune word pronunciations. Our approach produces a set of word-independent phonetic transformation rules that capture the ways in which sequences of phones in the training set are transformed into other sequences. Moreover, the transformations can be context-dependent. <p> Processing of Training Data Our procedure for the identification of pronunciation errors is straightforward and has been used before in <ref> [4] </ref>, as mentioned. We extend it to generate word-independent pronunciation transformation rules that are context-dependent. This training process is applied to a large volume of pre-transcribed data. It consists of the following steps: 1. Perform a forced-recognition of the training speech data using the corresponding transcripts and an initial lexicon.
Reference: [5] <editor> Placeway, P. et al, </editor> <booktitle> The 1996 Hub-4 Sphinx-3 System, Proc. DARPA Speech Recognition Workshop, </booktitle> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: Handselected transformation rules were applied to chosen words of the test lexicon (without reference to context), and tested. The test sets were chosen from the following: 1. The DARPA 1996 broadcast news development and test sets F0 and F1 conditions <ref> [5] </ref>. F0 is clean, high quality, prepared speech, and F1 is similar but spontaneous speech. Uses a 51K word vocabulary. 2. The DARPA 1994 H1-C0 test set [7]; read speech from business news; predefined 20K vocabulary. They were decoded using the Sphinx-3 decoder with fully continuous acoustic models ([5]).
Reference: [6] <author> Gauvain, J-L. et al, </author> <title> Acoustic Modelling in the LIMSI Nov96 Hub4 System, </title> <booktitle> Proc. DARPA Speech Recognition Workshop, </booktitle> <month> Feb. </month> <year> 1997. </year>
Reference: [7] <author> Kubala, F. </author> <title> Design of the 1994 CSR Benchmark Tests, </title> <booktitle> Proc. DARPA Spoken Language Systems Technology Workshop, </booktitle> <pages> pp. 41-46, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: The test sets were chosen from the following: 1. The DARPA 1996 broadcast news development and test sets F0 and F1 conditions [5]. F0 is clean, high quality, prepared speech, and F1 is similar but spontaneous speech. Uses a 51K word vocabulary. 2. The DARPA 1994 H1-C0 test set <ref> [7] </ref>; read speech from business news; predefined 20K vocabulary. They were decoded using the Sphinx-3 decoder with fully continuous acoustic models ([5]).
Reference: [8] <author> Ravishankar, M., </author> <title> Efficient Algorithms for Speech Recognition, </title> <type> Ph.D. thesis, TR. </type> <institution> CMU-CS-96-143, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: The remaining list checked by hand and unlikely pronunciations were dropped. As a result, 144 new pronunciations were selected for addition to the testing lexicon. Table 2 lists a few examples (using the CMU Sphinx phone set, see <ref> [8] </ref>). 3.1.2. Context-Dependent Transformations Count Lexical phone Sequence All-phone sequence 790 1fl''fl6 1fl6 171 ,+fl7'fl,; ,+fl';fl,; Table 3: Sample phone sequence transformations. Similarly, we obtained pronunciation transformation rules from the high-count error regions. About 200 of them occurred 100 or more times.
Reference: [9] <author> Chase, L., </author> <title> Error-Response Feedback Mechanisms for Speech Recognizers, </title> <type> Ph.D. thesis, TR CMU-RI-TR-97-18, </type> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: The word error rates for the two conditions changed from 28.9 and 33.6 in the baseline to 28.8 and 34.0, respectively. 3.3. Discussion Overall, the experimental results are inconclusive. However, from a detailed analysis of the errors, similar to <ref> [9] </ref>, we obtained the following insights. The generation of new word pronunciations does work. There is a small overall gain on the three test sets. Moreover, even though the same words may be recognized, the new pronunciations are preferred in about 2.3% of the total words.
References-found: 9

