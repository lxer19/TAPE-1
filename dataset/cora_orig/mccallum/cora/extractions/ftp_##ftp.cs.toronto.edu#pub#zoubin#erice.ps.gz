URL: ftp://ftp.cs.toronto.edu/pub/zoubin/erice.ps.gz
Refering-URL: http://www.cs.utoronto.ca/~zoubin/
Root-URL: 
Email: fhinton,sallans,zoubing@cs.toronto.edu  
Title: A HIERARCHICAL COMMUNITY OF EXPERTS  
Author: GEOFFREY E. HINTON BRIAN SALLANS AND ZOUBIN GHAHRAMANI 
Address: Toronto, Ontario, Canada M5S 3H5  
Affiliation: Department of Computer Science University of Toronto  
Abstract: We describe a directed acyclic graphical model that contains a hierarchy of linear units and a mechanism for dynamically selecting an appropriate subset of these units to model each observation. The non-linear selection mechanism is a hierarchy of binary units each of which gates the output of one of the linear units. There are no connections from linear units to binary units, so the generative model can be viewed as a logistic belief net (Neal 1992) which selects a skeleton linear model from among the available linear units. We show that Gibbs sampling can be used to learn the parameters of the linear and binary units even when the sampling is so brief that the Markov chain is far from equilibrium. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Everitt, B. S. </author> <year> (1984). </year> <title> An Introduction to Latent Variable Models. </title> <publisher> Chapman and Hall, London. </publisher>
Reference-contexts: The state of unit j is then picked randomly from a Gaussian distribution with mean ^y j and a variance 2 j that is learned from data. The generative model underlying factor analysis <ref> (Everitt, 1984) </ref> consists of one hidden layer of linear-Gaussian units (the factors) that send weighted connections (the factor loadings) to a visible layer of linear-Gaussian units.
Reference: <author> Ghahramani, Z. and Hinton, G. E. </author> <year> (1996). </year> <title> The EM algorithm for mixtures of factor analyzers. </title> <type> Technical Report CRG-TR-96-1 [ftp://ftp.cs.toronto.edu/pub/zoubin/tr-96-1.ps.gz], </type> <institution> Department of Computer Science, University of Toronto. </institution>
Reference: <author> Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. </author> <year> (1995). </year> <title> The wake-sleep algorithm for unsupervised neural networks. </title> <journal> Science, </journal> <volume> 268 </volume> <pages> 1158-1161. </pages>
Reference: <author> Hinton, G. E., Dayan, P., and Revow, M. </author> <year> (1997). </year> <title> Modeling the manifolds of Images of handwritten digits. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 8(1) </volume> <pages> 65-74. </pages>
Reference: <author> Hinton, G. E. and Ghahramani, Z. </author> <year> (1997). </year> <title> Generative models for discovering sparse distributed representations. </title> <journal> Phil. Trans. Roy. Soc. London B: Biol. Sci. </journal>
Reference: <author> Hull, J. J. </author> <year> (1994). </year> <title> A database for handwritten text recognition research. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(5) </volume> <pages> 550-554. </pages>
Reference-contexts: After the final iteration, the top level binary unit was found to be off for 90% of vertical images, and on for 84% of horizontal images. 5. Results on handwritten digits We trained a similar three-layer network on handwritten twos and threes from the CEDAR CDROM 1 database <ref> (Hull, 1994) </ref>. The digits were scaled 10 GEOFFREY E. HINTON ET AL. to an 8 fi 8 grid, and the 256-gray-scale pixel values were rescaled to lie within [0; 1].
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixture of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87. </pages>
Reference: <author> Jordan, M. I. and Jacobs, R. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214. </pages>
Reference: <author> Neal, R. M. </author> <year> (1992). </year> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56 </volume> <pages> 71-113. </pages>
Reference: <author> Neal, R. M. and Hinton, G. E. </author> <year> (1993). </year> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <type> Unpublished manuscript [ftp://ftp.cs.utoronto.ca/pub/radford/em.ps.z], </type> <institution> Department of Computer Science, University of Toronto. </institution>
Reference-contexts: Fortunately, the EM algorithm can be generalized so that each iteration improves a lower bound on the log likelihood <ref> (Neal and Hinton, 1993) </ref>. In this form, the only property required of Gibbs sampling is that it get closer to equilibrium on each iteration. There is a sensible objective function for the learning that can be improved even if the sampling is far from equilibrium. <p> Otherwise, F exceeds the negative log probability of visible configuration by the Kullback-Leibler divergence between Q and P : F = ln p (visible) + X Q ff ln P ff The EM algorithm consists of coordinate descent in F <ref> (Neal and Hinton, 1993) </ref>: a full M step minimizes F with respect to the parameters that determine E, and a full E step minimizes F with respect to Q, which is achieved by setting Q equal to the posterior distribution over the hidden configurations given E.
References-found: 10

