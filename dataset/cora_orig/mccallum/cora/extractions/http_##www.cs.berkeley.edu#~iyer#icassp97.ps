URL: http://www.cs.berkeley.edu/~iyer/icassp97.ps
Refering-URL: http://www.cs.berkeley.edu/~iyer/
Root-URL: 
Email: fbilmes,krste,cheewhye,demmelg@cs.berkeley.edu  
Title: USING PHIPAC TO SPEED ERROR BACK-PROPAGATION LEARNING  
Author: Jeff Bilmes Krste Asanovic Chee-whye Chin and Jim Demmel 
Address: Berkeley, CA 94704, USA  1947 Center Street, Suite 600 Berkeley, CA 94704, USA  
Affiliation: Department of Electrical Engineering and Computer Sciences University of California at Berkeley  International Computer Science Institute  
Abstract: We introduce PHiPAC, a coding methodology for developing portable high-performance numerical libraries in ANSI C. Using this methodology, we have developed code for optimized matrix multiply routines. These routines can achieve over 90% of peak performance on a variety of current workstations, and are often faster than vendor-supplied optimized libraries. We then describe the bunch-mode back-propagation algorithm and how it can use the PHiPAC derived matrix multiply routines. Using a set of plots, we investigate the tradeoffs between bunch size, convergence rate, and training speed using a standard speech recognition data set and show how use of the PHiPAC routines can lead to a significantly faster back-propagation learning algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Alpern, L. Carter, and J. Ferrante. </author> <title> Space-limited procedures: A methodology for portable high-performance. </title> <booktitle> In International Working Conference on Massively Parallel Programming Models, </booktitle> <year> 1995. </year>
Reference-contexts: Alternatively, the algorithms could be written in a high-level language and fed to an optimizing compiler. While there is a large literature on relevant compiler techniques <ref> [14, 10, 11, 1, 6, 12] </ref> that can be used to generate reasonably good code in general, they tend not to generate near-peak code for any one operation. Moreover, it takes significant time and investment before compiler research appears in production compilers, so these capabilities are often unavailable. <p> The actual micro-architectural features along with the resulting C coding guidelines are fully described in [5]. The second component of the PHiPAC methodology is to, rather than hand-code particular routines, write parameterized generators <ref> [1, 11] </ref> that produce code according to our guidelines. A generator has several advantages over a single instance of a routine. First, the algorithm's entire design space can be explored by varying the generator parameters and timing the resulting routines.
Reference: [2] <author> D. Anguita and B. Gomes. </author> <title> MBP on T0: mixing floating- and fixed-point formats in BP learning. </title> <type> Technical Report 94-038, ICSI, </type> <year> 1994. </year>
Reference-contexts: An alternate strategy, which we call bunch-mode (also called block-mode <ref> [2, 3] </ref>), uses more than one training pattern simultaneously to update the weight matrices. Let n p equal the number of training patterns being processed simultaneously (i.e., the bunch size). When n p = 1, the back-propagation learning algorithm inherently uses matrix-vector operations.
Reference: [3] <author> D. Anguita, G. Parodi, and R. Zunino. </author> <title> An efficient implementation of BP on RISC-based workstations. </title> <journal> Neu-rocomputing, </journal> <volume> 6 </volume> <pages> 57-65, </pages> <year> 1994. </year>
Reference-contexts: An alternate strategy, which we call bunch-mode (also called block-mode <ref> [2, 3] </ref>), uses more than one training pattern simultaneously to update the weight matrices. Let n p equal the number of training patterns being processed simultaneously (i.e., the bunch size). When n p = 1, the back-propagation learning algorithm inherently uses matrix-vector operations.
Reference: [4] <author> J. Bilmes, K. Asanovic, J. Demmel, D. Lam, and C.W. Chin. </author> <note> The PHiPAC WWW home page. http://www.icsi.berkeley.edu/~bilmes/phipac. </note>
Reference-contexts: The PHiPAC matrix multiply generator, search scripts, and a demonstration of bunch-mode back-propagation code in C are all available from the PHiPAC WWW site <ref> [4] </ref>.
Reference: [5] <author> J. Bilmes, K. Asanovic, J. Demmel, D. Lam, and C.W. Chin. PHiPAC: </author> <title> A portable, high-performance, ANSI C coding methodology and its application to matrix multiply. </title> <note> LAPACK working note 111, </note> <institution> University of Tennessee, </institution> <year> 1996. </year>
Reference-contexts: However, more sophisticated optimizations, including pointer alias disambiguation, register and cache blocking, loop unrolling, and software pipelining, are best performed manually. The actual micro-architectural features along with the resulting C coding guidelines are fully described in <ref> [5] </ref>. The second component of the PHiPAC methodology is to, rather than hand-code particular routines, write parameterized generators [1, 11] that produce code according to our guidelines. A generator has several advantages over a single instance of a routine. <p> Details of the resulting code are described in <ref> [5] </ref>. code, the vendor supplied optimized and assembly coded BLAS routine, and our matrix multiply code. See [5] for a set of additional plots showing similar PHiPAC performance advantages. nested loops), an SGI supplied matrix library, and PHiPAC matrix multiply for square matrices on an SGI Indigo R4K 100 MHz. <p> Details of the resulting code are described in <ref> [5] </ref>. code, the vendor supplied optimized and assembly coded BLAS routine, and our matrix multiply code. See [5] for a set of additional plots showing similar PHiPAC performance advantages. nested loops), an SGI supplied matrix library, and PHiPAC matrix multiply for square matrices on an SGI Indigo R4K 100 MHz.
Reference: [6] <author> L. Carter, J. Ferrante, and S. Flynn Hummel. </author> <title> Hierarchical tiling for improved superscalar performance. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Alternatively, the algorithms could be written in a high-level language and fed to an optimizing compiler. While there is a large literature on relevant compiler techniques <ref> [14, 10, 11, 1, 6, 12] </ref> that can be used to generate reasonably good code in general, they tend not to generate near-peak code for any one operation. Moreover, it takes significant time and investment before compiler research appears in production compilers, so these capabilities are often unavailable.
Reference: [7] <author> J. Dongarra, J. Du Croz, I. Duff, and S. Hammarling. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: We have developed a methodology, named PHiPAC, for developing Portable High-Performance numerical libraries in ANSI C. Our goal is to produce, with minimal effort, high-performance numerical libraries for a wide range of systems. Using this methodology, we have produced a portable, BLAS-compatible <ref> [7] </ref>, matrix multiply generator. The resulting code can achieve over 90% of peak performance on a variety of current workstations, and is often faster than the vendor-supplied optimized libraries. <p> When n p = 1, the back-propagation learning algorithm inherently uses matrix-vector operations. When n p &gt; 1, however, the algorithm can be formulated to use matrix-matrix operations. It is well known that matrix-matrix operations can be coded much more efficiently, especially for larger matrix sizes <ref> [7] </ref>. Therefore, larger bunch size back-propagation learning should have a speed advantage. Using the PHiPAC derived matrix-multiply routines, we have therefore implemented a bunch-mode backpropagation learning algorithm.
Reference: [8] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: We have applied these techniques to the development of a generator and search scripts for the matrix multiply operation. mm gen is a generator that produces blocked matrix multiply code <ref> [8] </ref>, following the PHiPAC coding guidelines. It generates code for the operation C = ffop (A)op (B) + fiC where op (A), op (B), and C, are respectively MfiK, KfiN, and MfiN matrices, ff and fi are scalar parameters, and op (X) is either transpose (X) or just X.
Reference: [9] <author> J. Hertz, A. Krogh, and R.G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison Wesley, </publisher> <year> 1991. </year>
Reference-contexts: In this paper we concentrate on matrix multiplication and back-propagation, but we have produced other generators including convolution, dot-product, and AXPY, which have similarly demonstrated portable high performance. 3. BUNCH-MODE BACK-PROPAGATION LEARNING In <ref> [9] </ref>, two modes of back-propagation learning were defined, on-line mode where only one training pattern is used at a time to update the weight matrices, and batch mode where all training patterns are used simultaneously to update the weight matrices.
Reference: [10] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Alternatively, the algorithms could be written in a high-level language and fed to an optimizing compiler. While there is a large literature on relevant compiler techniques <ref> [14, 10, 11, 1, 6, 12] </ref> that can be used to generate reasonably good code in general, they tend not to generate near-peak code for any one operation. Moreover, it takes significant time and investment before compiler research appears in production compilers, so these capabilities are often unavailable.
Reference: [11] <author> J.D. McCalpin and M. Smotherman. </author> <title> Automatic benchmark generation for cache optimization of matrix algorithms. </title> <editor> In R. Geist and S. Junkins, editors, </editor> <booktitle> Proceedings of the 33rd Annual Southeast Conference, </booktitle> <pages> pages 195-204, </pages> <address> New York, NY, </address> <month> March </month> <year> 1995. </year> <institution> Association for Computing Machinery, ACM. </institution>
Reference-contexts: Alternatively, the algorithms could be written in a high-level language and fed to an optimizing compiler. While there is a large literature on relevant compiler techniques <ref> [14, 10, 11, 1, 6, 12] </ref> that can be used to generate reasonably good code in general, they tend not to generate near-peak code for any one operation. Moreover, it takes significant time and investment before compiler research appears in production compilers, so these capabilities are often unavailable. <p> The actual micro-architectural features along with the resulting C coding guidelines are fully described in [5]. The second component of the PHiPAC methodology is to, rather than hand-code particular routines, write parameterized generators <ref> [1, 11] </ref> that produce code according to our guidelines. A generator has several advantages over a single instance of a routine. First, the algorithm's entire design space can be explored by varying the generator parameters and timing the resulting routines.
Reference: [12] <author> R. Saavedra, W. Mao, D. Park, J. Chame, and S. Moon. </author> <title> The combined effectiveness of unimodular transformations, tiling, and software prefetching. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium. IEEE Computer Society, </booktitle> <month> April 15-19 </month> <year> 1996. </year>
Reference-contexts: Alternatively, the algorithms could be written in a high-level language and fed to an optimizing compiler. While there is a large literature on relevant compiler techniques <ref> [14, 10, 11, 1, 6, 12] </ref> that can be used to generate reasonably good code in general, they tend not to generate near-peak code for any one operation. Moreover, it takes significant time and investment before compiler research appears in production compilers, so these capabilities are often unavailable.
Reference: [13] <author> Dilip Sarkar. </author> <title> Methods to speed up error backpropagation learning algorithm. </title> <journal> ACM Computing Surveys, </journal> <volume> 27(4) </volume> <pages> 519-544, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Note that as n p increases from one, the set of operations performed during a training doesn't change. In order to further decrease training time by changing the algorithmic convergence, additional techniques such as dynamic learning rate adjustment, random pattern presentation, momentum, etc <ref> [13] </ref> can be used together with the bunch-mode backpropagation algorithm. Some combination of both algorithmic techniques and the use of matrix-matrix multiply will probably result in an optimal back-propagation learning algorithm for a given task.
Reference: [14] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year> <month> 4 </month>
Reference-contexts: Alternatively, the algorithms could be written in a high-level language and fed to an optimizing compiler. While there is a large literature on relevant compiler techniques <ref> [14, 10, 11, 1, 6, 12] </ref> that can be used to generate reasonably good code in general, they tend not to generate near-peak code for any one operation. Moreover, it takes significant time and investment before compiler research appears in production compilers, so these capabilities are often unavailable.
References-found: 14

