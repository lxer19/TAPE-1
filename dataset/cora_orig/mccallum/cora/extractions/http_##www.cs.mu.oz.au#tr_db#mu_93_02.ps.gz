URL: http://www.cs.mu.oz.au/tr_db/mu_93_02.ps.gz
Refering-URL: http://www.cs.mu.oz.au/tr_db/TR.html
Root-URL: 
Title: Compression and Fast Indexing for Multi-Gigabyte Text Databases  
Author: Alistair Moffat Justin Zobel 
Abstract: In the last two years we have developed improved techniques for indexing and retrieval of text data, including algorithms for inversion, for compression of the data and index, and for economical ranking. These techniques were, however, tested on relatively small databases. In this paper we describe our experiences in scaling these techniques up to a large (2 Gb) heterogeneous text database. Our experiments show that compression performance does not degrade with the increase in size, and that response times remain small, confirming that our techniques are suitable for large volumes of data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Al-Hawamdeh and P. Willett. </author> <title> Comparison of index term weighting schemes for the ranking of paragraphs in full-text documents. </title> <journal> International Journal of Information and Library Research, </journal> <pages> pages 116-130, </pages> <year> 1990. </year>
Reference-contexts: The process of matching documents to an informally phrased query, returning as answers the documents that most closely match the query, is called ranking. As an example, consider the cosine measure, one of the more effective ranking techniques <ref> [1, 12, 13] </ref>.
Reference: [2] <author> T.C. Bell, A. Moffat, C.G. Nevill-Manning, I.H. Witten, and J. Zobel. </author> <title> Data compression in full-text retrieval systems. </title> <journal> Journal of the American Society for Information Science. </journal> <note> To appear. </note>
Reference-contexts: It must also be possible to create and access these indexes in a reasonable amount of time, and to store them, and the data itself, in a reasonable amount of space. In the last two years we have developed techniques that allow the text to be stored compressed <ref> [2, 10] </ref>; provide fast access to document collections via compressed indexes [2, 11, 16]; create indexes with an in-memory inversion technique [9]; and permit economical ranking of large document collections [17]. <p> In the last two years we have developed techniques that allow the text to be stored compressed [2, 10]; provide fast access to document collections via compressed indexes <ref> [2, 11, 16] </ref>; create indexes with an in-memory inversion technique [9]; and permit economical ranking of large document collections [17]. <p> The effect of applying these techniques to the TREC collection are described in Section 6. 4 Text compression Using a word-based model of the text, the space required to store the documents comprising a database can be reduced to less than 30% of the original size <ref> [2, 10, 14, 15] </ref>. Each word occurrence in the text is replaced by a canonical Huffman code [7], the length of which is dependent on the frequency of the word, and the intervening `non-words' are similarly coded against a vocabulary of non-words. <p> Good compression of index entries can be achieved by numbering documents sequentially from 1, sorting index entries, representing each sequence of identifiers as a sequence of differences or gaps, and then using compact representations of the generally small integers that result <ref> [2, 3, 4, 11, 16] </ref>.
Reference: [3] <author> A. Bookstein, S.T. Klein, and T. Raita. </author> <title> Model based concordance compression. </title> <editor> In J.A. Storer and M. Cohn, editors, </editor> <booktitle> Proc. IEEE Data Compression Conf., </booktitle> <pages> pages 82-91, </pages> <address> Snowbird, Utah, March 1992. </address> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California. </address>
Reference-contexts: Good compression of index entries can be achieved by numbering documents sequentially from 1, sorting index entries, representing each sequence of identifiers as a sequence of differences or gaps, and then using compact representations of the generally small integers that result <ref> [2, 3, 4, 11, 16] </ref>.
Reference: [4] <author> A. Bookstein, S.T. Klein, and D.A. Ziff. </author> <title> A systematic approach to compressing a full-text retrieval system. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 28(5), </volume> <year> 1992. </year>
Reference-contexts: Good compression of index entries can be achieved by numbering documents sequentially from 1, sorting index entries, representing each sequence of identifiers as a sequence of differences or gaps, and then using compact representations of the generally small integers that result <ref> [2, 3, 4, 11, 16] </ref>.
Reference: [5] <author> P. Elias. </author> <title> Universal codeword sets and representations of the integers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-21:194-203, </volume> <month> March </month> <year> 1975. </year>
Reference-contexts: The most useful parameter is probably the mean gap, or, equivalently, the frequency of the term. 6 One simple global method is the gamma code of Elias <ref> [5] </ref>, in which positive integer x is represented by coding blog 2 xc in unary followed by the value of x 2 blog 2 xc in binary. Some value of gamma are shown in Table 2.
Reference: [6] <author> D. Harman and G. Candela. </author> <title> Retrieving records from a gigabyte of text on a minicomputer using statistical ranking. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(8) </volume> <pages> 581-589, </pages> <year> 1990. </year>
Reference-contexts: This technique is also used by Harman and Candela <ref> [6] </ref>. The query length p P q;t ) is, for a given query, a constant; it does not affect the ranking and so need not be calculated. <p> There are very few systems on which there is 2 Gb of surplus disk space available to allow the inversion of a 2 Gb database; certainly, there were none available to us! Another alternative approach is the disk-based method described by Harman and Candela <ref> [6] </ref>; they report that the inversion of an 806 Mb database required 313 hours and 163 Mb of disk work space to produce an inverted index of 112 Mb.
Reference: [7] <author> D. Hirschberg and D. Lelewer. </author> <title> Efficient decoding of prefix codes. </title> <journal> Communications of the ACM, </journal> <volume> 33(4) </volume> <pages> 449-459, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Each word occurrence in the text is replaced by a canonical Huffman code <ref> [7] </ref>, the length of which is dependent on the frequency of the word, and the intervening `non-words' are similarly coded against a vocabulary of non-words.
Reference: [8] <author> J.B. Lovins. </author> <title> Development of a stemming algorithm. Mechanical Translation and Computation, </title> <address> 11(1-2):22-31, </address> <year> 1968. </year> <month> 14 </month>
Reference-contexts: We did, however, stem all words before indexing, using Lovins's algorithm <ref> [8] </ref>. This meant that the index vocabulary stored fewer words than the compression model, and reduced the number of pointers that had to be stored in the index. 6 Experimental results This section describes the performance of our techniques on the 2055 Mb TREC collection.
Reference: [9] <author> A. Moffat. </author> <title> Economical inversion of large text files. </title> <journal> Computing Systems, </journal> <volume> 5(2) </volume> <pages> 125-139, </pages> <year> 1992. </year>
Reference-contexts: In the last two years we have developed techniques that allow the text to be stored compressed [2, 10]; provide fast access to document collections via compressed indexes [2, 11, 16]; create indexes with an in-memory inversion technique <ref> [9] </ref>; and permit economical ranking of large document collections [17]. These techniques rely on the large main memories of modern machines to store the vocabulary of the document collection, allowing rapid access to index information and a compression model for the text. <p> We have also used these inverted file encodings in the inversion process required during database construction, again capitalising on the large main memory offered by current 7 workstations and the ability to perform two passes over a static database <ref> [9] </ref>. The most common previous technique was to write a file of `word-number, document-number' pairs in document number order and sort the file into word number order. <p> The in-memory technique requires the allocation of memory a little larger than the final size of the compressed inverted file, and our previous experiments had indicated that this would be roughly 10% of the size of the input text <ref> [9] </ref>. That is, we expected to use more than 200 Mb of the 256 Mb memory available.
Reference: [10] <author> A. Moffat and J. Zobel. </author> <title> Coding for compression in full-text retrieval systems. </title> <booktitle> In Proc. IEEE Data Compression Conf., </booktitle> <pages> pages 72-81, </pages> <address> Snowbird, Utah, March 1992. </address> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California. </address>
Reference-contexts: It must also be possible to create and access these indexes in a reasonable amount of time, and to store them, and the data itself, in a reasonable amount of space. In the last two years we have developed techniques that allow the text to be stored compressed <ref> [2, 10] </ref>; provide fast access to document collections via compressed indexes [2, 11, 16]; create indexes with an in-memory inversion technique [9]; and permit economical ranking of large document collections [17]. <p> The effect of applying these techniques to the TREC collection are described in Section 6. 4 Text compression Using a word-based model of the text, the space required to store the documents comprising a database can be reduced to less than 30% of the original size <ref> [2, 10, 14, 15] </ref>. Each word occurrence in the text is replaced by a canonical Huffman code [7], the length of which is dependent on the frequency of the word, and the intervening `non-words' are similarly coded against a vocabulary of non-words. <p> We have experimented with several different techniques for compressing these run-lengths, in two broad classes <ref> [10, 11] </ref>. Global methods use the same encoding for all entries, and so have the advantage of being general, but are insensitive to the frequency of each term. <p> Hence, a local compression method that adjusts its codes according to word frequency can be expected to perform better than a global method, and this is indeed the case [11]. In our implementation we have chosen to use the `local V G ' code <ref> [10, 11] </ref>. In this code, integers x 1 are represented by coding q = b (x 1)=bc in unary, and then r = x 1 b fi q in binary.
Reference: [11] <author> A. Moffat and J. Zobel. </author> <title> Parameterised compression for sparse bitmaps. </title> <booktitle> In Proc. ACM-SIGIR International Conf. on Research and Development in Information Retrieval, </booktitle> <pages> pages 274-285, </pages> <address> Copenhagen, Denmark, June 1992. </address> <publisher> ACM Press. </publisher>
Reference-contexts: In the last two years we have developed techniques that allow the text to be stored compressed [2, 10]; provide fast access to document collections via compressed indexes <ref> [2, 11, 16] </ref>; create indexes with an in-memory inversion technique [9]; and permit economical ranking of large document collections [17]. <p> Good compression of index entries can be achieved by numbering documents sequentially from 1, sorting index entries, representing each sequence of identifiers as a sequence of differences or gaps, and then using compact representations of the generally small integers that result <ref> [2, 3, 4, 11, 16] </ref>. <p> We have experimented with several different techniques for compressing these run-lengths, in two broad classes <ref> [10, 11] </ref>. Global methods use the same encoding for all entries, and so have the advantage of being general, but are insensitive to the frequency of each term. <p> Hence, a local compression method that adjusts its codes according to word frequency can be expected to perform better than a global method, and this is indeed the case <ref> [11] </ref>. In our implementation we have chosen to use the `local V G ' code [10, 11]. In this code, integers x 1 are represented by coding q = b (x 1)=bc in unary, and then r = x 1 b fi q in binary. <p> Hence, a local compression method that adjusts its codes according to word frequency can be expected to perform better than a global method, and this is indeed the case [11]. In our implementation we have chosen to use the `local V G ' code <ref> [10, 11] </ref>. In this code, integers x 1 are represented by coding q = b (x 1)=bc in unary, and then r = x 1 b fi q in binary. <p> Some example V G codes for b = 6 are shown in Table 2. Term `frequency-within-document' values are efficiently represented by use of the gamma code <ref> [11] </ref>. These values are interleaved with encoded run-lengths in the index entries. After compression, the index entries for a text database will typically occupy less than 10% of the space required for the text itself.
Reference: [12] <author> G. Salton. </author> <title> Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: The process of matching documents to an informally phrased query, returning as answers the documents that most closely match the query, is called ranking. As an example, consider the cosine measure, one of the more effective ranking techniques <ref> [1, 12, 13] </ref>.
Reference: [13] <author> G. Salton and M.J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: The process of matching documents to an informally phrased query, returning as answers the documents that most closely match the query, is called ranking. As an example, consider the cosine measure, one of the more effective ranking techniques <ref> [1, 12, 13] </ref>.
Reference: [14] <author> I.H. Witten, T.C. Bell, and C. Nevill. </author> <title> Models for compression in full-text retrieval systems. </title> <editor> In J.A. Storer and J.H. Reif, editors, </editor> <booktitle> Proc. IEEE Data Compression Conf., </booktitle> <pages> pages 23-32, </pages> <address> Snowbird, Utah, April 1991. </address> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California. </address>
Reference-contexts: The effect of applying these techniques to the TREC collection are described in Section 6. 4 Text compression Using a word-based model of the text, the space required to store the documents comprising a database can be reduced to less than 30% of the original size <ref> [2, 10, 14, 15] </ref>. Each word occurrence in the text is replaced by a canonical Huffman code [7], the length of which is dependent on the frequency of the word, and the intervening `non-words' are similarly coded against a vocabulary of non-words.
Reference: [15] <author> J. Zobel and A. Moffat. </author> <title> Adding compression to a full-text retrieval system. </title> <booktitle> In Aus-tralian Computer Science Conf., </booktitle> <pages> pages 1077-1089, </pages> <address> Hobart, Australia, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: The effect of applying these techniques to the TREC collection are described in Section 6. 4 Text compression Using a word-based model of the text, the space required to store the documents comprising a database can be reduced to less than 30% of the original size <ref> [2, 10, 14, 15] </ref>. Each word occurrence in the text is replaced by a canonical Huffman code [7], the length of which is dependent on the frequency of the word, and the intervening `non-words' are similarly coded against a vocabulary of non-words. <p> Because the compression is performed with a static model, documents can be decompressed individually, given only the address 5 of the compressed document. For further details of the scheme used the reader is referred to the description in <ref> [15] </ref>. The decompression process is very fast. The canonical Huffman code used is particularly amenable to a table-based `look up and copy' implementation, and each such decoding step generates several output bytes. As a result, the compression regime has only limited impact on retrieval time.
Reference: [16] <author> J. Zobel, A. Moffat, and R. Sacks-Davis. </author> <title> An efficient indexing technique for full-text database systems. </title> <booktitle> In Proc. International Conf. on Very Large Databases, </booktitle> <pages> pages 352-362, </pages> <address> Vancouver, Canada, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: In the last two years we have developed techniques that allow the text to be stored compressed [2, 10]; provide fast access to document collections via compressed indexes <ref> [2, 11, 16] </ref>; create indexes with an in-memory inversion technique [9]; and permit economical ranking of large document collections [17]. <p> We assume that access is via an inverted file indexing scheme such as the method we have described elsewhere <ref> [16] </ref>. In such a scheme each distinct word in the database is held in a vocabulary, which may be an array, or may be a search structure such as a B-tree. <p> Good compression of index entries can be achieved by numbering documents sequentially from 1, sorting index entries, representing each sequence of identifiers as a sequence of differences or gaps, and then using compact representations of the generally small integers that result <ref> [2, 3, 4, 11, 16] </ref>.

References-found: 16

