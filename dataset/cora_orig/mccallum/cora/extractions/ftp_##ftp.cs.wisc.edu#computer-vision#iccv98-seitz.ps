URL: ftp://ftp.cs.wisc.edu/computer-vision/iccv98-seitz.ps
Refering-URL: http://www.cs.wisc.edu/computer-vision/pubs.html
Root-URL: 
Email: seitz@cs.wisc.edu  kyros@cs.rochester.edu  
Title: Plenoptic Image Editing  
Author: Steven M. Seitz Kiriakos N. Kutulakos 
Address: Madison, WI53706  Rochester, NY14607  
Affiliation: Computer Sciences Department University of WisconsinMadison  Department of Computer Science University of Rochester  
Note: To appear in Proc. ICCV 98  
Abstract: This paper presents a new class of interactive image editing operations designed to maintain consistency between multiple images of a physical 3D scene. The distinguishing feature of these operations is that edits to any one image propagate automatically to all other images as if the (unknown) 3D scene had itself been modified. The modified scene can then be viewed interactively from any other camera viewpoint and under different scene illuminations. The approach is useful first as a power-assist that enables a user to quickly modify many images by editing just a few, and second as a means for constructing and editing image-based scene representations by manipulating a set of photographs. The approach works by extending operations like image painting, scissoring, and morphing so that they alter a scene's generalized plenoptic function in a physically-consistent way, thereby affecting scene appearance from all viewpoints simultaneously. A key element in realizing these operations is a new volumetric decomposition technique for reconstructing an scene's plenoptic function from an incomplete set of camera viewpoints. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. H. Adelson and J. R. Bergen, </author> <title> The plenoptic function and the elements of early vision, in Computation Models of Visual Processing (M. </title> <editor> Landy and J. A. Movshon, eds.), </editor> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: It is therefore convenient to cast them in terms of the plenoptic function <ref> [1, 2] </ref>, which encodes scene appearance from all possible viewpoints.
Reference: [2] <author> L. McMillan and G. Bishop, </author> <title> Plenoptic modeling: An image-based rendering system, </title> <booktitle> in Proc. SIGGRAPH'95, </booktitle> <pages> pp. 3946, </pages> <year> 1995. </year>
Reference-contexts: It is therefore convenient to cast them in terms of the plenoptic function <ref> [1, 2] </ref>, which encodes scene appearance from all possible viewpoints.
Reference: [3] <author> M. Levoy and P. Hanrahan, </author> <title> Light field rendering, </title> <booktitle> in Proc. SIGGRAPH '96, </booktitle> <pages> pp. 3142, </pages> <year> 1996. </year>
Reference-contexts: Furthermore, no techniques are currently available for modifying this function in response to image editing operations, unless an a priori 3D model is available [7]. For instance, a number of researchers <ref> [3, 4, 8] </ref> have proposed ray-based representations of the plenoptic function. While these models might in principle be extended to include illumination parameters, the lack of correspondence information does not facilitate plenoptic image editing operations.
Reference: [4] <author> S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Co-hen, </author> <booktitle> The lumigraph, in Proc. SIGGRAPH'96, </booktitle> <pages> pp. 4354, </pages> <year> 1996. </year>
Reference-contexts: Furthermore, no techniques are currently available for modifying this function in response to image editing operations, unless an a priori 3D model is available [7]. For instance, a number of researchers <ref> [3, 4, 8] </ref> have proposed ray-based representations of the plenoptic function. While these models might in principle be extended to include illumination parameters, the lack of correspondence information does not facilitate plenoptic image editing operations.
Reference: [5] <author> A. Shashua, </author> <title> Geometry and Photometry in 3D Visual Recognition. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1992. </year>
Reference-contexts: A key question is how should the plenoptic function be represented in order to enable both synthesis and editing operations. Previous approaches for reconstructing the 1 plenoptic function enabled synthesis of views [24] or il-luminations <ref> [5, 6] </ref> but not both. Furthermore, no techniques are currently available for modifying this function in response to image editing operations, unless an a priori 3D model is available [7]. For instance, a number of researchers [3, 4, 8] have proposed ray-based representations of the plenoptic function. <p> images, ~ C m holds those predicted by the ideal radiance model, and c is a threshold term. 3.2.1 Lambertian Radiance To account for shading effects due to changes in the relative positions of an object and light sources, we use a Lambertian model for modeling a voxel's ideal radiance <ref> [5, 15, 16] </ref>: " X ff i ~n i M ~ l i C base (3) The model treats each voxel as an independent Lambertian surface that is illuminated by multiple light sources at infinity, has color C base , and has normal ~n.
Reference: [6] <author> P. N. Belhumeur and D. J. Kriegman, </author> <title> What is the set of images of an object under all possible lighting conditions, </title> <booktitle> in Proc. CVPR, </booktitle> <pages> pp. 270277, </pages> <year> 1996. </year>
Reference-contexts: A key question is how should the plenoptic function be represented in order to enable both synthesis and editing operations. Previous approaches for reconstructing the 1 plenoptic function enabled synthesis of views [24] or il-luminations <ref> [5, 6] </ref> but not both. Furthermore, no techniques are currently available for modifying this function in response to image editing operations, unless an a priori 3D model is available [7]. For instance, a number of researchers [3, 4, 8] have proposed ray-based representations of the plenoptic function.
Reference: [7] <author> P. Hanrahan and P. Haeberli, </author> <title> Direct WYSIWYG painting and texturing on 3D shapes, </title> <booktitle> in Proc. SIGGRAPH 90, </booktitle> <pages> pp. 215223, </pages> <year> 1990. </year>
Reference-contexts: Previous approaches for reconstructing the 1 plenoptic function enabled synthesis of views [24] or il-luminations [5, 6] but not both. Furthermore, no techniques are currently available for modifying this function in response to image editing operations, unless an a priori 3D model is available <ref> [7] </ref>. For instance, a number of researchers [3, 4, 8] have proposed ray-based representations of the plenoptic function. While these models might in principle be extended to include illumination parameters, the lack of correspondence information does not facilitate plenoptic image editing operations.
Reference: [8] <author> K. N. Kutulakos, </author> <title> Shape from the light field boundary, </title> <booktitle> in Proc. CVPR, </booktitle> <pages> pp. 5359, </pages> <year> 1997. </year>
Reference-contexts: Furthermore, no techniques are currently available for modifying this function in response to image editing operations, unless an a priori 3D model is available [7]. For instance, a number of researchers <ref> [3, 4, 8] </ref> have proposed ray-based representations of the plenoptic function. While these models might in principle be extended to include illumination parameters, the lack of correspondence information does not facilitate plenoptic image editing operations.
Reference: [9] <author> T. Beier and S. Neely, </author> <title> Feature-based image metamorphosis, </title> <booktitle> in Proc. SIGGRAPH 92, </booktitle> <pages> pp. 3542, </pages> <year> 1992. </year>
Reference-contexts: The extrapolation occurs automatically whenever the user performs a scissoring operation. 2.3 Plenoptic Morphing Image warping or morphing <ref> [9, 10] </ref> is a popular way of producing shape changes and animations from one or more images. <p> While the motion of rays is determined, the motion of voxels along rays is not. Our implementation of plenoptic image morphing fixed this variable by constraining voxels to move parallel to the image plane and used Beier and Neely's method <ref> [9] </ref> to generate image warps. Morph propagation is achieved by using the projected voxel displacement to define image warps in new views.
Reference: [10] <author> S. M. Seitz and C. R. Dyer, </author> <title> View morphing, </title> <booktitle> in Proc. </booktitle> <volume> SIG-GRAPH 96, </volume> <pages> pp. 2130, </pages> <year> 1996. </year>
Reference-contexts: The extrapolation occurs automatically whenever the user performs a scissoring operation. 2.3 Plenoptic Morphing Image warping or morphing <ref> [9, 10] </ref> is a popular way of producing shape changes and animations from one or more images.
Reference: [11] <author> D. Wilkes and J. K. Tsotsos, </author> <title> Active object recognition, </title> <booktitle> in Proc. CVPR, </booktitle> <pages> pp. 136141, </pages> <year> 1992. </year>
Reference-contexts: The ability to generate new views is also useful for edit operations, because it allows the user to interactively choose a good image for editing. For instance, a flat surface can be rotated to a front-on view <ref> [11, 12] </ref> to facilitate painting and avoid foreshortening effects.
Reference: [12] <author> K. N. Kutulakos and C. R. Dyer, </author> <title> Recovering shape by purposive viewpoint adjustment, </title> <journal> Int. J. Computer Vision, </journal> <volume> vol. 12, no. 2, </volume> <pages> pp. 113136, </pages> <year> 1994. </year>
Reference-contexts: The ability to generate new views is also useful for edit operations, because it allows the user to interactively choose a good image for editing. For instance, a flat surface can be rotated to a front-on view <ref> [11, 12] </ref> to facilitate painting and avoid foreshortening effects.
Reference: [13] <author> S. Benton, </author> <title> Survey of holographic stereograms, in Processing and Display of 3D Data, </title> <booktitle> Proc. SPIE, </booktitle> <year> 1983. </year>
Reference-contexts: Furthermore, the color and intensity of corresponding pixels can be thought of as samples of the radiance function of a hypothetical scene point positioned at V i . The second step of the method recovers the shape component of the plenoptic function representation 1 Holographic imaging <ref> [13] </ref> is one notable application where this ambiguity is put into practical use: it relies on our inability to distinguish views of flat holographic images from views of objects that are truly 3D. 3 by carving away from V all voxels whose projections are not consistent with the a priori-specified radiance
Reference: [14] <author> S. M. Seitz and C. R. Dyer, </author> <title> Photorealistic scene reconstruction by voxel coloring, </title> <booktitle> in Proc. CVPR, </booktitle> <pages> pp. 1067 1073, </pages> <year> 1997. </year>
Reference-contexts: The plenoptic function of a 3D scene describes the flow of light along every oriented ray in space, and encodes the scene's appearance from every direction <ref> [14] </ref>. While the plenoptic function is determined uniquely by the 3D surfaces in a scene and their reflectance properties, we can generate the same plenoptic function by combining many different shapes and radiance functions 1 (Figure 3 (b)). <p> The carving algorithm takes advantage of a voxel enumeration strategy that visits voxels in depth-order to account for occlusions <ref> [14] </ref>. Here we employ this enumeration strategy to facilitate plenoptic decomposition, i.e., recovery of shape and parametric radiance functions. The voxel carving algorithm operates as follows: the scene is initialized to a solid block of voxels. <p> The voxels that remain at the end represent the shape component of the plenoptic decomposition. The steps are as follows: 1. Enumerate the voxels fV 1 ; : : : ; V k g in order of in creasing distance from the camera volume, as in <ref> [14] </ref> 2. <p> The camera was raised slightly above the object to be compatible with the ordinal visibility constraint <ref> [14] </ref>. The illumination was fixed relative to the camera, causing changes in shading as the object rotated.
Reference: [15] <author> R. J. Woodham, Y. Iwahori, and R. A. Barman, </author> <title> Photometric stereo: Lambertian reflectance and light sources with unknown direction and strength, </title> <type> Tech. Rep. 91-18, </type> <institution> Univ. of B.C., Lab. for Computational Intelligence, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: images, ~ C m holds those predicted by the ideal radiance model, and c is a threshold term. 3.2.1 Lambertian Radiance To account for shading effects due to changes in the relative positions of an object and light sources, we use a Lambertian model for modeling a voxel's ideal radiance <ref> [5, 15, 16] </ref>: " X ff i ~n i M ~ l i C base (3) The model treats each voxel as an independent Lambertian surface that is illuminated by multiple light sources at infinity, has color C base , and has normal ~n. <p> Intuitively, Eq. (4) expresses the radiance of a voxel directly in terms of image measurements, without attempting to recover the normal and light source vectors as in traditional photometric stereo techniques <ref> [15] </ref>.
Reference: [16] <author> R. Epstein, A. L. Yuille, and P. N. Belhumeur, </author> <title> Learning object representations from lighting variations, in Object Rep. in Computer Vision II (J. </title> <editor> Ponce, A. Zisserman, and M. Hebert, eds.), pp. </editor> <volume> 179199, </volume> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: images, ~ C m holds those predicted by the ideal radiance model, and c is a threshold term. 3.2.1 Lambertian Radiance To account for shading effects due to changes in the relative positions of an object and light sources, we use a Lambertian model for modeling a voxel's ideal radiance <ref> [5, 15, 16] </ref>: " X ff i ~n i M ~ l i C base (3) The model treats each voxel as an independent Lambertian surface that is illuminated by multiple light sources at infinity, has color C base , and has normal ~n.
Reference: [17] <author> P. E. Debevec, C. J. Taylor, and J. Malik, </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach, </title> <booktitle> in Proc. SIG-GRAPH'96, </booktitle> <pages> pp. 1120, </pages> <year> 1996. </year>
Reference-contexts: linear system of equations for the unknowns I amb and X ij in terms of known m ij and observed C (M ). 2 3.2.2 Modeling Residuals In plenoptic decomposition, radiance residuals are used to ensure that local radiance variations are approximated accurately for views close to the input images <ref> [17] </ref>. Residual modeling is an instance of scattered data approximation on the sphere [18]a rich literature on the topic exists, partly motivated by the problem of BRDF estimation [19, 20].
Reference: [18] <author> G. Nielson, </author> <title> Scattered data modeling, </title> <journal> IEEE Computer Graphics and Applications, </journal> <pages> pp. 6070, </pages> <year> 1993. </year>
Reference: [19] <author> P. Schroder and W. Sweldens, </author> <title> Spherical wavelets: Efficiently representing functions on the sphere, </title> <booktitle> in Proc. SIG-GRAPH'95, </booktitle> <pages> pp. 161172, </pages> <year> 1995. </year>
Reference-contexts: Residual modeling is an instance of scattered data approximation on the sphere [18]a rich literature on the topic exists, partly motivated by the problem of BRDF estimation <ref> [19, 20] </ref>. Rather than treating the problem in its full generality, we consider a simpler approach that can be used to model residuals for views taken along a single-axis rotation of the object. This allows us to further reduce the dimensionality of the approximation problem and simplify computations.

References-found: 19

