URL: http://www.cs.utexas.edu/users/rvdg/papers/jacobi.concurrency.ps
Refering-URL: http://www.cs.utexas.edu/users/rvdg/abstracts/JACOBI.html
Root-URL: 
Email: (domingo@dif.um.es)  (cpvhg@dsic.upv.es, cpavm@dsic.upv.es)  (rvdg@cs.utexas.edu)  e-mail: domingo@dif.um.es  (domingo@dif.um.es)  
Phone: 2  3  telephone: 34 68 307100 fax: 34 68 364151  
Title: A JACOBI METHOD BY BLOCKS ON A MESH OF PROCESSORS  
Author: Domingo Gimenez Vicente Hernandez Robert van de Geijn Antonio M. Vidal Domingo Gimenez 
Note: contact author:  
Address: 30001 Murcia. Spain.  Aptdo 22012. 46071 Valencia. Spain.  Texas, 78712.  street address: Departamento  Aptdo 4021. 30001 Murcia. Spain.  
Affiliation: 1 Departamento de Informatica Sistemas. Univ de Murcia. Aptdo 4021.  Departamento de Sistemas Informaticos Computacion. Univ Politecnica de Valencia.  Department of Computer Sciences. University of Texas at Austin. Austin,  de Informatica Sistemas. Univ de Murcia.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov and D. Sorensen. </author> <note> LAPACK Users' Guide. SIAM, </note> <year> 1992. </year>
Reference-contexts: The primary purpose of this paper is to show how to obtain highly efficient parallel Jacobi algorithms. To achieve this goal we have used the very well known techniques of reformulating dense linear algebra algorithms to operate on blocks (submatrices) to obtain algorithms rich in matrix-matrix operations <ref> [1] </ref>. As a result, performance of a sequential implementation improves by overcoming the common discrepancy between CPU performance and bandwidth to main memory.
Reference: [2] <author> L. Auslander and A. Tsao. </author> <title> On parallelizable eigensolvers. </title> <booktitle> Advances in Applied Mathematics, </booktitle> <volume> 13 </volume> <pages> 253-261, </pages> <year> (1992). </year> <month> 20 </month>
Reference-contexts: Introduction Many different methods exist for computing the eigenvalues of a symmetric matrix, including bisection methods, the QR algorithm, divide and conquer (Cuppen's algorithm), and Jacobi's method. More recently, novel algorithms based on invariant subspace decomposition that use matrix-matrix multiplication have been proposed <ref> [2, 13] </ref>. For a complete survey and references, we suggest the reader turn to [11, 12]. Jacobi's method is the oldest, dating back to the mid-1800's. It has fallen out of favor and been resurrected on many occasions.
Reference: [3] <author> Christian H. Bischof. </author> <title> Computing the singular value decomposition on a distributed system of vector processors. </title> <journal> Parallel Computing, </journal> <volume> 11 </volume> <pages> 171-186, </pages> <year> 1989. </year>
Reference-contexts: The use of level-3 BLAS In this subsection we show how to obtain a Jacobi algorithm by blocks. The method is similar to this used by Bischof for the SVD problem <ref> [3] </ref>. If matrix A is divided into columns and rows of square blocks of size s, it is possible to make a sweep over these blocks using some Jacobi order, treating blocks as units. For reasons that will become apparent later, we will use the odd-even order.
Reference: [4] <author> R. P. Brent and F. T. Luk. </author> <title> The solution of singular-value and symmetric eigenvalue problems on multiprocessor arrays. </title> <journal> SIAM Journal on Scientific and Statistic Computation, </journal> <volume> 6(1) </volume> <pages> 69-84, </pages> <year> (1985). </year>
Reference-contexts: Jacobi's method is the oldest, dating back to the mid-1800's. It has fallen out of favor and been resurrected on many occasions. Its most recent resurgence is mainly due to better stability properties [5] and straight-forward parallelization <ref> [4, 7, 8, 17, 18] </ref>. The primary purpose of this paper is to show how to obtain highly efficient parallel Jacobi algorithms.
Reference: [5] <author> J. Demmel and K. Veselic. </author> <title> Jacobi's method is more accurate than QR. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 13 </volume> <pages> 1204-1245, </pages> <year> (1992). </year>
Reference-contexts: For a complete survey and references, we suggest the reader turn to [11, 12]. Jacobi's method is the oldest, dating back to the mid-1800's. It has fallen out of favor and been resurrected on many occasions. Its most recent resurgence is mainly due to better stability properties <ref> [5] </ref> and straight-forward parallelization [4, 7, 8, 17, 18]. The primary purpose of this paper is to show how to obtain highly efficient parallel Jacobi algorithms.
Reference: [6] <author> J. J. Dongarra, J. Du Croz, S. Hammarling and I. Duff. </author> <title> A set of Level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> (1990). </year>
Reference-contexts: As a result, these operations can take advantage of hierarchical memories that include data caches. A suite of such matrix-matrix operations are part of the Basic Linear Algebra Subprograms (BLAS) <ref> [6] </ref>, which are typ 5 ically optimized for various architectures. As a result, an algorithm coded in terms of call to the BLAS becomes a portable high performance implementation. In this section, we show how blocking can be used for Jacobi's method.
Reference: [7] <author> P. J. Eberlein. </author> <title> On one-sided Jacobi methods for parallel computation. </title> <journal> SIAM Journal on Algebraic Discrete Mathematic, </journal> <volume> 8(4) </volume> <pages> 790-796, </pages> <year> (1987). </year>
Reference-contexts: Jacobi's method is the oldest, dating back to the mid-1800's. It has fallen out of favor and been resurrected on many occasions. Its most recent resurgence is mainly due to better stability properties [5] and straight-forward parallelization <ref> [4, 7, 8, 17, 18] </ref>. The primary purpose of this paper is to show how to obtain highly efficient parallel Jacobi algorithms.
Reference: [8] <author> P. J. Eberlein and H. Park. </author> <title> Efficient implementation of Jacobi algorithms and Jacobi sets on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 358-366, </pages> <year> (1990). </year>
Reference-contexts: Jacobi's method is the oldest, dating back to the mid-1800's. It has fallen out of favor and been resurrected on many occasions. Its most recent resurgence is mainly due to better stability properties [5] and straight-forward parallelization <ref> [4, 7, 8, 17, 18] </ref>. The primary purpose of this paper is to show how to obtain highly efficient parallel Jacobi algorithms.
Reference: [9] <author> A. Edelman. </author> <title> Large dense linear algebra in 1993: The parallel computing influence. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 7(2) </volume> <pages> 113-128, </pages> <year> 1993. </year>
Reference-contexts: We combine this improvement in the sequential implementation with an assignment of data and work to the processors of a multicomputer to yield a parallel implementation that is both efficient and scalable. The symmetric eigenvalue problem appears in many applications. In some applications in computational chemistry <ref> [9] </ref> it is necessary to solve the eigenvalue problem obtaining all the eigenvalues of a symmetric, dense and real matrix of size of some hundred or a few thousand. In this case it could be interesting to solve the problem by efficient Jacobi methods on Multicomputers.
Reference: [10] <author> G. E. Forsythe and P. Henrici. </author> <title> The cyclic Jacobi method for computing the principal values of a complex matrix. </title> <journal> Transactions of American Mathematical Society, </journal> <volume> 94 </volume> <pages> 1-23, </pages> <year> (1960). </year>
Reference-contexts: Under certain conditions <ref> [10] </ref>, sequence fA l g converges to a diagonal matrix D, D = Q k Q k1 : : : Q 2 Q 1 AQ t 2 : : : Q t k (2) whose diagonal elements are then the eigenvalues of A. <p> A sweep consists of successively nullifying the n (n 1)=2 nondiagonal elements in the lower-triangular part of the matrix (and the corresponding symmetrical part). The convergence of the cyclic-by-row method is analyzed in <ref> [10] </ref>. More recently, [15, 16] demonstrate that the Jacobi method converges with other ordering schemes when the cyclic-by-row 4 method is convergent. These orderings are intended to simplify paralleliza--tion of the method. In this paper the odd-even order will be used [17].
Reference: [11] <author> K. A. Gallivan, R. J. Plemmons and A. H. Sameh. </author> <title> Parallel algorithms for dense linear algebra computations. </title> <journal> SIAM Review, </journal> <volume> 32(1) </volume> <pages> 54-135, </pages> <year> 1990. </year>
Reference-contexts: More recently, novel algorithms based on invariant subspace decomposition that use matrix-matrix multiplication have been proposed [2, 13]. For a complete survey and references, we suggest the reader turn to <ref> [11, 12] </ref>. Jacobi's method is the oldest, dating back to the mid-1800's. It has fallen out of favor and been resurrected on many occasions. Its most recent resurgence is mainly due to better stability properties [5] and straight-forward parallelization [4, 7, 8, 17, 18].
Reference: [12] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: More recently, novel algorithms based on invariant subspace decomposition that use matrix-matrix multiplication have been proposed [2, 13]. For a complete survey and references, we suggest the reader turn to <ref> [11, 12] </ref>. Jacobi's method is the oldest, dating back to the mid-1800's. It has fallen out of favor and been resurrected on many occasions. Its most recent resurgence is mainly due to better stability properties [5] and straight-forward parallelization [4, 7, 8, 17, 18]. <p> Matrix Q l coincides with the identity matrix except in elements q ii = cos , q ij = sin , q ji = sin , and q jj = cos , and <ref> [12] </ref> 2a ij : (3) Notice that this transformation only affects rows i and j when applied from the left, and columns i and j when applied from the right. The different ways of choosing pairs (i; j) have given rise to different versions of the Jacobi method.
Reference: [13] <author> S. Huss-Lederman, A. Tsao and G. Zhang. </author> <title> A parallel implementation of the invariant subspace decomposition algorithm for dense symmetric matrices. </title> <booktitle> In Proceedings Sixth SIAM Conf. on Parallel Processing for Scientific Computing. </booktitle> <publisher> SIAM, </publisher> <year> 1993. </year> <month> 21 </month>
Reference-contexts: Introduction Many different methods exist for computing the eigenvalues of a symmetric matrix, including bisection methods, the QR algorithm, divide and conquer (Cuppen's algorithm), and Jacobi's method. More recently, novel algorithms based on invariant subspace decomposition that use matrix-matrix multiplication have been proposed <ref> [2, 13] </ref>. For a complete survey and references, we suggest the reader turn to [11, 12]. Jacobi's method is the oldest, dating back to the mid-1800's. It has fallen out of favor and been resurrected on many occasions.
Reference: [14] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing Scalability of Parallel Al--gorithms and Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22 </volume> <pages> 379-391, </pages> <year> 1994. </year>
Reference-contexts: Since the overhead related with the communication and load imbalance is of lower order, 100% efficiency can be approached by this algorithm when the matrix size is large enough. 14 Scalability To study theoretically the scalability of the method it is possible to study the isoefficiency function of the algorithm <ref> [14] </ref>. This function is of order O (p) if we consider n the problem size and p the number of processors. But this is also the isoefficiency function of the algorithm for a logical ring presented in [18]. However, the mesh topology is more appropriate to obtain good scalability.
Reference: [15] <author> F. T. Luk and H. Park. </author> <title> A proof of convergence for two parallel Jac-obi SVD algorithms. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 28(6) </volume> <pages> 806-811, </pages> <year> (1989). </year>
Reference-contexts: A sweep consists of successively nullifying the n (n 1)=2 nondiagonal elements in the lower-triangular part of the matrix (and the corresponding symmetrical part). The convergence of the cyclic-by-row method is analyzed in [10]. More recently, <ref> [15, 16] </ref> demonstrate that the Jacobi method converges with other ordering schemes when the cyclic-by-row 4 method is convergent. These orderings are intended to simplify paralleliza--tion of the method. In this paper the odd-even order will be used [17].
Reference: [16] <author> G. Schroff and R. Schreiber. </author> <title> On the convergence of the cyclic Jacobi method for parallel block orderings. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 10(3) </volume> <pages> 326-346, </pages> <year> (1989). </year>
Reference-contexts: A sweep consists of successively nullifying the n (n 1)=2 nondiagonal elements in the lower-triangular part of the matrix (and the corresponding symmetrical part). The convergence of the cyclic-by-row method is analyzed in [10]. More recently, <ref> [15, 16] </ref> demonstrate that the Jacobi method converges with other ordering schemes when the cyclic-by-row 4 method is convergent. These orderings are intended to simplify paralleliza--tion of the method. In this paper the odd-even order will be used [17]. <p> After this the algorithm continues similarly until a sweep is completed. The Jacobi algorithm using this ordering by blocks converges, as can be easily proved using results in <ref> [16] </ref>. The swapping of row and column blocks is illustrated in Figures 6 and 7. In these figures a bidirectional arrow ! represents a swap and a unidirectional arrow ! a copy.
Reference: [17] <author> G. W. Stewart. </author> <title> A Jacobi-like algorithm for computing the Schur decomposition of a nonhermitian matrix. </title> <journal> SIAM Journal on Scientific and Statistic Computation, </journal> <volume> 4 </volume> <pages> 853-864, </pages> <year> (1985). </year>
Reference-contexts: Jacobi's method is the oldest, dating back to the mid-1800's. It has fallen out of favor and been resurrected on many occasions. Its most recent resurgence is mainly due to better stability properties [5] and straight-forward parallelization <ref> [4, 7, 8, 17, 18] </ref>. The primary purpose of this paper is to show how to obtain highly efficient parallel Jacobi algorithms. <p> More recently, [15, 16] demonstrate that the Jacobi method converges with other ordering schemes when the cyclic-by-row 4 method is convergent. These orderings are intended to simplify paralleliza--tion of the method. In this paper the odd-even order will be used <ref> [17] </ref>. Again, this order was intended to allow parallelization of the method, but, as we shall see, it also simplifies a block based implementation of the sequential algorithm. We will demonstrate this ordering by examining the case where n = 8.

References-found: 17

