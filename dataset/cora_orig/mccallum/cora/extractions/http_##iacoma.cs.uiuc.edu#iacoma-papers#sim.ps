URL: http://iacoma.cs.uiuc.edu/iacoma-papers/sim.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Email: venkat,torrella@cs.uiuc.edu  
Title: A Direct-Execution Framework for Fast and Accurate Simulation of Superscalar Processors 1  
Author: Venkata Krishnan and Josep Torrellas 
Web: http://iacoma.cs.uiuc.edu  
Address: IL 61801  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign,  
Abstract: Multiprocessor system evaluation has traditionally been based on direct-execution based Execution-Driven Simulations (EDS). In such environments, the processor component of the system is not fully modeled. With wide-issue superscalar processors being the norm in today's multiprocessor nodes, there is an urgent need for modeling the processor accurately. However, using direct-execution to model a superscalar processor has been considered an open problem. Hence, current approaches model the processor by interpreting the application executable. Unfortunately, this approach can be slow. In this paper, we propose a novel direct-execution framework that allows accurate simulation of wide-issue superscalar processors without the need for code interpretation. This is achieved with the aid of an Interface Window between the front-end and the architectural simulator, that buffers the necessary information. This eliminates the need for full-fledged instruction emulation. Overall, this approach enables detailed yet fast EDS of superscalar processors. Finally, we evaluate the framework and show good performance for uni- and multiprocessor configurations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Bitar. </author> <title> A Critique of Trace-Driven Simulation for Shared-Memory Multiprocessors, </title> <address> pages 37-52. </address> <publisher> Kluwer Academic Publishers, </publisher> <editor> Editors: M. Dubois and S. Thakkar, </editor> <month> May </month> <year> 1990. </year>
Reference-contexts: The actual interleaving depends, among other things, on the memory system used. Since the trace is generated using an existing memory system and the TDS uses a different, simulated memory system, TDS may result in the incorrect interleaving of the traces <ref> [1] </ref>. It is in this area that EDS plays a crucial role. Here, the execution of the application and the simulation is completely interleaved. The application is instrumented at appropriate points so as to generate events for the back-end simulator. When the simulator is called, it processes the event.
Reference: [2] <author> E. Brewer, C. Dellarocas, A. Colbrook, and W. Weihl. PRO-TEUS: </author> <title> A High-Performance Parallel-Architecture Simulator. </title> <type> Technical Report MIT/LCS-TR-516, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: When the simulator is called, it processes the event. Then, it returns control to the application. This method facilitates feedback from the simulator to guide the execution of the parallel application. There have been many EDS-based multiprocessor system simulators for both RISC and CISC architectures <ref> [2, 4, 6, 15, 16, 20, 23, 31] </ref>. Most of these EDS-based systems are based on direct execution. Here, the instrumented application is executed directly on the host machine where the simulation runs. This results in fast simulation. Unfortunately, using simulation based on direct execution comes at a price.
Reference: [3] <author> P. P. Chang, S. Mahlke, W. Y. Chen, N. J. Water, and W. Hwu. </author> <title> IMPACT: An Architectural Framework for Multiple-Instruction-Issue Processors. </title> <booktitle> In 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 266-275, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: A typical compiler for a superscalar or VLIW machine performs inter basic-block code scheduling in a window of instructions that represents the most-frequently executed path. The most-frequently executed path may be either estimated statically or acquired through profiles. The Superblock in the IMPACT compiler <ref> [3] </ref> or the Trace Window in the Multiflow compiler [12] are typical examples of this approach. In our work, we take the instructions in the interface window and perform a resource-constrained list-scheduling of instructions [14] as well as limited register renaming to remove false dependences, before invoking the processor simulator.
Reference: [4] <author> H. Davis, S. Goldschmidt, and J. Hennessy. </author> <title> Multiprocessor Simulation and Tracing using Tango. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages II99-107, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: When the simulator is called, it processes the event. Then, it returns control to the application. This method facilitates feedback from the simulator to guide the execution of the parallel application. There have been many EDS-based multiprocessor system simulators for both RISC and CISC architectures <ref> [2, 4, 6, 15, 16, 20, 23, 31] </ref>. Most of these EDS-based systems are based on direct execution. Here, the instrumented application is executed directly on the host machine where the simulation runs. This results in fast simulation. Unfortunately, using simulation based on direct execution comes at a price.
Reference: [5] <author> D.A.Wood, M.D.Hill, and R.E.Kessler. </author> <title> A Model for Estimating Trace-Sample Miss Ratios. </title> <booktitle> In ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 79-89, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The generated information is later used to drive a simulator of the system under study. Given that an application may execute billions of instructions, to reduce the storage requirements of the traces, sampling <ref> [5, 21, 26] </ref> is often used. 1 This work was supported in part by the National Science Foundation under grants NSF Young Investigator Award MIP-9457436, ASC-9612099 and MIP-9619351, DARPA Contract DABT63-95-C-0097, NASA Contract NAG-1-613 and gifts from IBM and Intel.
Reference: [6] <author> S. Dwarkadas, J.R.Jump, and J.B.Sinclair. </author> <title> Execution-Driven Simulation of Multiprocessors: Address and Timing Analysis. </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> 4(4) </volume> <pages> 314-338, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: When the simulator is called, it processes the event. Then, it returns control to the application. This method facilitates feedback from the simulator to guide the execution of the parallel application. There have been many EDS-based multiprocessor system simulators for both RISC and CISC architectures <ref> [2, 4, 6, 15, 16, 20, 23, 31] </ref>. Most of these EDS-based systems are based on direct execution. Here, the instrumented application is executed directly on the host machine where the simulation runs. This results in fast simulation. Unfortunately, using simulation based on direct execution comes at a price.
Reference: [7] <author> L. Hammond, B. Nayfeh, and K. Olukotun. </author> <title> A Single-Chip Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 30(9) </volume> <pages> 79-85, </pages> <month> September </month> <year> 1997. </year>
Reference-contexts: Our framework also uses the superscalar core as a building block to model advanced processor architectures such as a chip multiprocessor (CMP), where multiple superscalar cores share a single chip <ref> [7] </ref>; a simultaneous mul-tithreaded (SMT) processor, which adds multiple threads to a superscalar and allows instructions from different threads to be issued in the same cycle [29]; and finally a hybrid of the two architectures that supports several SMT processors on a single chip [8].
Reference: [8] <author> V. Krishnan and J. Torrellas. </author> <title> A Clustered Approach to Multi-threaded Processors. </title> <booktitle> In 12th International Parallel Processing Symposium (IPPS), </booktitle> <month> April </month> <year> 1998. </year>
Reference-contexts: superscalar cores share a single chip [7]; a simultaneous mul-tithreaded (SMT) processor, which adds multiple threads to a superscalar and allows instructions from different threads to be issued in the same cycle [29]; and finally a hybrid of the two architectures that supports several SMT processors on a single chip <ref> [8] </ref>. By varying parameters such as the number of on-chip processors and the number of threads in a SMT processor, we can model different architectures. We also model a multiprocessor system, where each node in the system can be any of the above processor types.
Reference: [9] <author> M. Lam. </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Processors. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The only informa tion that we use is the branch-prediction of the simulated processor that is used for growing the window. Unfortunately, unlike a compiler, which can look at huge instruction windows spanning thousands of instructions and can perform advanced optimizations such as software pipelining <ref> [9] </ref>, we are restricted by the size of the run-time interface window in our scheme. Thus, our approach falls short of a pure compiler-based approach to scheduling. Nevertheless, as we show in our evaluations, our approach considerably improves the ILP when modeling in-order issue superscalars. <p> The figure also shows a detailed breakup of the total execution time in terms of the different categories mentioned in Section 2.4. This provides valuable information in identifying performance bottlenecks. For instance, a large data contribution in the loop-intensive floating-point applications highlights the need for software pipelining <ref> [9] </ref>. 3.2 Slowdowns for Uniprocessor and Multiprocessor Simulation In this section, we evaluate the slowdown of the simulator when modeling a uni- and multi-processor system. To isolate the slowdowns caused by the processor simulator, we do not include the memory back-end in these experiments.
Reference: [10] <author> C. Lee, M. Potkonjak, and W. </author> <title> Mangione-Smith. </title> <booktitle> Media-Bench: A Tool for Evaluating and Synthesizing Multimedia and Communications Systems . In 30th International Symposium on Microarchitecture (MICRO-30), </booktitle> <pages> pages 330-335, </pages> <month> De-cember </month> <year> 1997. </year>
Reference-contexts: We use this mode for a set of sample inputs before using the fast mode for our experiments. We evaluate our framework for both a uni- and a multiprocessor environment. The evaluation for the uniprocessor environment is performed with two SPECint95 (compress and ijpeg), one multimedia <ref> [10] </ref> (mpeg) and four SPECfp95 (swim, tomcatv , hydro2d and mgrid ) applications. Two SPLASH-2 [33] (radix and cholesky) applications are used to evaluate the multiprocessor environment. The applications are compiled for a MIPS R4400 processor, using the highest level of optimization and the -mips2 option.
Reference: [11] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: We also model a multiprocessor system, where each node in the system can be any of the above processor types. The memory subsystem for the uniprocessor is based on a conventional L1-L2 cache hierarchy, while the multiprocessor system (Figure 3) supports a DASH-like <ref> [11] </ref> directory-based cache-coherence protocol. Some of the processor and memory subsystem parameters that can be specified at startup time for modeling the machine are given in Table 2. 2.4 Statistics Collection The simulator allows detailed statistics collection each cycle for each thread and processor.
Reference: [12] <author> G. Lowney, S. Freudenberger, T. Karzes, W. D. Lichtenstein, R. Nix, J. O'Donnell, and J. C. Ruttenberg. </author> <title> The Multiflow Trace Scheduling Compiler. </title> <journal> The Journal of Supercomputing, </journal> <volume> 7(1-2):51-142, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: The most-frequently executed path may be either estimated statically or acquired through profiles. The Superblock in the IMPACT compiler [3] or the Trace Window in the Multiflow compiler <ref> [12] </ref> are typical examples of this approach. In our work, we take the instructions in the interface window and perform a resource-constrained list-scheduling of instructions [14] as well as limited register renaming to remove false dependences, before invoking the processor simulator.
Reference: [13] <institution> MIPS Technologies, Inc. R10000 Microprocessor Chipset, Product Overview, </institution> <year> 1994. </year>
Reference-contexts: The base processor models that are supported include an in-order issue superscalar that can speculatively fetch across branches and an out-of-order issue superscalar that is modeled on the lines of the MIPS R10000 <ref> [13] </ref> and is shown in Figure 2. The model is quite detailed and incorporates all superscalar features such as a register renaming mechanism, a large associative instruction queue for out-of-order issue, speculation across multiple branches and in-order instruction retirement. We model the pipeline in great detail.
Reference: [14] <author> S. Muchnick. </author> <title> Advanced Compiler Design Implementation. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: The Superblock in the IMPACT compiler [3] or the Trace Window in the Multiflow compiler [12] are typical examples of this approach. In our work, we take the instructions in the interface window and perform a resource-constrained list-scheduling of instructions <ref> [14] </ref> as well as limited register renaming to remove false dependences, before invoking the processor simulator. Furthermore, loops with small bodies are automatically unrolled inside the window, with the amount of unrolling constrained by the size of the window.
Reference: [15] <author> S. Mukherjee, S. Reinhardt, B. Falsafi, M. Litzkow, S. Huss-Lederman, M. D. Hill, J. R. Larus, and D. A. Wood. </author> <title> Wiscon-sin Wind Tunnel II: A Fast and Portable Parallel Architecture Simulator. In Workshop on Performance Analysis and its Impact on Design (PAID) (held in conjunction with ISCA'97), </title> <month> June </month> <year> 1997. </year>
Reference-contexts: When the simulator is called, it processes the event. Then, it returns control to the application. This method facilitates feedback from the simulator to guide the execution of the parallel application. There have been many EDS-based multiprocessor system simulators for both RISC and CISC architectures <ref> [2, 4, 6, 15, 16, 20, 23, 31] </ref>. Most of these EDS-based systems are based on direct execution. Here, the instrumented application is executed directly on the host machine where the simulation runs. This results in fast simulation. Unfortunately, using simulation based on direct execution comes at a price.
Reference: [16] <author> A.-T. Nguyen, M. Michael, A. Sharma, and J. Torrellas. </author> <title> The Augmint Multiprocessor Simulation Toolkit for Intel x86 Architectures. </title> <booktitle> In International Conference of Computer Design, </booktitle> <pages> pages 486-490, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: When the simulator is called, it processes the event. Then, it returns control to the application. This method facilitates feedback from the simulator to guide the execution of the parallel application. There have been many EDS-based multiprocessor system simulators for both RISC and CISC architectures <ref> [2, 4, 6, 15, 16, 20, 23, 31] </ref>. Most of these EDS-based systems are based on direct execution. Here, the instrumented application is executed directly on the host machine where the simulation runs. This results in fast simulation. Unfortunately, using simulation based on direct execution comes at a price.
Reference: [17] <author> V. S. Pai, P. Ranganathan, and S. V. Adve. RSIM: </author> <title> An Execution-Driven Simulator for ILP-Based Shared-Memory Multiprocessors and Uniprocessors. </title> <booktitle> In Proceedings of the Third Workshop on Computer Architecture Education, </booktitle> <month> Febru-ary </month> <year> 1997. </year>
Reference-contexts: This is because current multiprocessor systems are typically based on complicated processors like wide-issue dynamic superscalars. Accurately simulating current dynamic superscalar processors using direct execution has been considered an open problem <ref> [17] </ref>. Consequently, when it is necessary to model such a processor, the simulators have to resort to interpreting the application executable [18, 23]. This often results in slowdowns of orders of magnitude. Addressing this problem is the motivation behind this paper. <p> One way of achieving this would be to interpret each instruction in a simulated processor. However, this is likely to lead to significant slowdowns in many cases <ref> [17, 23] </ref>. Our approach, instead, is to perform a limited functional emulation. We elaborate on this approach in the rest of this section. We instrument the application to generate events at basic-block boundaries. This tracks the control flow of the application.
Reference: [18] <author> V. S. Pai, P. Ranganathan, and S. V. Adve. </author> <title> The Impact of Instruction-Level Parallelism on Multiprocessor Performance and Simulation Methodolgy. </title> <booktitle> In Proceedings of the Third International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 72-83, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: Unfortunately, though direct-execution based EDS is used commonly for multiprocessor environments where the main focus is to study memory subsystems, not modeling the processor in detail may give rise to inaccurate results <ref> [18] </ref>. This is because current multiprocessor systems are typically based on complicated processors like wide-issue dynamic superscalars. Accurately simulating current dynamic superscalar processors using direct execution has been considered an open problem [17]. <p> Accurately simulating current dynamic superscalar processors using direct execution has been considered an open problem [17]. Consequently, when it is necessary to model such a processor, the simulators have to resort to interpreting the application executable <ref> [18, 23] </ref>. This often results in slowdowns of orders of magnitude. Addressing this problem is the motivation behind this paper.
Reference: [19] <author> S.-T. Pan, K. So, and J. Rameh. </author> <title> Improving the Accuracy of Dynamic Branch Prediction Using Branch Correlation. </title> <booktitle> In 5th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 76-84, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: First, the simulation would be too slow. Secondly and more importantly, the simulator cannot consume just one instruction at a time. Indeed, since the size of a basic block is only a few instructions in integer applications [32], wide-issue superscalars use advanced branch-prediction schemes <ref> [19, 34] </ref> to fetch instructions across multiple basic blocks. Thus our model should permit sufficient information from the front-end to be gathered as dictated by the branch-prediction scheme of the simulated processor, before invoking the simulator. <p> reason i In the above example, the four wasted slots are assigned as follows: 4fi4=8 = 2 memory slots, 4fi3=8 = 1:5 Parameter Subparameter Processor type Number of processors/chip (CMP-level) Threads/Processor (SMT-level) Issue mechanism Issue policy (in-order/out-of-order) Issue width Dynamic issue mechanism Renaming registers Instruction queue entries Branch prediction (correlating) <ref> [19] </ref> Prediction table entries Misprediction penalty (cycles) Functional units (int, load/store, fp) Number of units Type of instructions handled Instruction latency and repeat rate (for non-pipelined units) Multiprocessor support Number of processing nodes Memory-related Number of outstanding loads and stores L1 and L2 cache size, associativity, block size and banks Local <p> The architecture modeled is a 4-way dynamically- or statically-scheduled superscalar processor. The value of some of its architectural parameters is specified in Table 3. The processor has a 2K-entry direct-mapped correlating branch prediction table <ref> [19] </ref> that allows multiple branch predictions in a cycle. All functional units are fully pipelined, with most instructions taking 1 cycle to complete.
Reference: [20] <author> D. Poulsen and P.-C. Yew. </author> <title> Execution-Driven Tools for Parallel Simulation of Parallel Architecture and Applications. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <pages> pages 860-869, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: When the simulator is called, it processes the event. Then, it returns control to the application. This method facilitates feedback from the simulator to guide the execution of the parallel application. There have been many EDS-based multiprocessor system simulators for both RISC and CISC architectures <ref> [2, 4, 6, 15, 16, 20, 23, 31] </ref>. Most of these EDS-based systems are based on direct execution. Here, the instrumented application is executed directly on the host machine where the simulation runs. This results in fast simulation. Unfortunately, using simulation based on direct execution comes at a price.
Reference: [21] <author> R.E.Kessler, M.D.Hill, and D.A.Wood. </author> <title> A Comparison of Trace-Sampling Techniques for Multi-Megabyte Caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-43:664-675, </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: The generated information is later used to drive a simulator of the system under study. Given that an application may execute billions of instructions, to reduce the storage requirements of the traces, sampling <ref> [5, 21, 26] </ref> is often used. 1 This work was supported in part by the National Science Foundation under grants NSF Young Investigator Award MIP-9457436, ASC-9612099 and MIP-9619351, DARPA Contract DABT63-95-C-0097, NASA Contract NAG-1-613 and gifts from IBM and Intel.
Reference: [22] <author> R.F.Cmelik and D. Keppel. Shade: </author> <title> A Fast Instruction-Set Simulator for Execution Profiling. </title> <booktitle> In ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 128-137, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Software simulation plays an integral role in the design and validation of high-performance uni- and multiprocessor systems. Two major simulation methodologies used are Trace-Driven Simulation (TDS) and Execution-Driven Simulation (EDS). In TDS, hardware probes [27, 30] or software instrumentation of the application <ref> [22, 24, 25] </ref> allow information like basic block or data addresses to be collected in a trace buffer during the application's execution. The generated information is later used to drive a simulator of the system under study. <p> This is in contrast to the several thousand-time slowdowns of interpretation-based systems, such as the MXS processor simulator in SimOS [23]. In addition, since our processor simulator is independent of the front-end, we have the flexibility of using a faster front-end such as Shade <ref> [22] </ref>, which utilizes dynamic compilation and caching for improved performance. Finally, we examine the slowdowns of the simulator when simulating a multiprocessor configuration in which each node is the 4-way dynamic superscalar. Two parallel applications, namely radix and cholesky are used for this purpose. Table 7 gives the results.
Reference: [23] <author> M. Rosenblum, S. A. Herrod, E. Witchel, and A. Gupta. </author> <title> Complete Computer System Simulation: The SimOS Approach. </title> <journal> IEEE Parallel and Distributed Technology: Systems and Applications, </journal> <volume> 3(4) </volume> <pages> 34-43, </pages> <month> Winter </month> <year> 1995. </year>
Reference-contexts: When the simulator is called, it processes the event. Then, it returns control to the application. This method facilitates feedback from the simulator to guide the execution of the parallel application. There have been many EDS-based multiprocessor system simulators for both RISC and CISC architectures <ref> [2, 4, 6, 15, 16, 20, 23, 31] </ref>. Most of these EDS-based systems are based on direct execution. Here, the instrumented application is executed directly on the host machine where the simulation runs. This results in fast simulation. Unfortunately, using simulation based on direct execution comes at a price. <p> Accurately simulating current dynamic superscalar processors using direct execution has been considered an open problem [17]. Consequently, when it is necessary to model such a processor, the simulators have to resort to interpreting the application executable <ref> [18, 23] </ref>. This often results in slowdowns of orders of magnitude. Addressing this problem is the motivation behind this paper. <p> One way of achieving this would be to interpret each instruction in a simulated processor. However, this is likely to lead to significant slowdowns in many cases <ref> [17, 23] </ref>. Our approach, instead, is to perform a limited functional emulation. We elaborate on this approach in the rest of this section. We instrument the application to generate events at basic-block boundaries. This tracks the control flow of the application. <p> In our case, we see slowdowns in the range of 70-200. Factoring this, the full simulator is around 1300 times slower than native execution. This is in contrast to the several thousand-time slowdowns of interpretation-based systems, such as the MXS processor simulator in SimOS <ref> [23] </ref>. In addition, since our processor simulator is independent of the front-end, we have the flexibility of using a faster front-end such as Shade [22], which utilizes dynamic compilation and caching for improved performance.
Reference: [24] <author> M. Smith. </author> <title> Tracing with Pixie. </title> <type> Technical Report CSL-TR-91-497, </type> <institution> Center for Integrated Systems, Stanford University, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Software simulation plays an integral role in the design and validation of high-performance uni- and multiprocessor systems. Two major simulation methodologies used are Trace-Driven Simulation (TDS) and Execution-Driven Simulation (EDS). In TDS, hardware probes [27, 30] or software instrumentation of the application <ref> [22, 24, 25] </ref> allow information like basic block or data addresses to be collected in a trace buffer during the application's execution. The generated information is later used to drive a simulator of the system under study.
Reference: [25] <author> A. Srivastava and A. Eustace. </author> <title> ATOM: A System for Building Customized Program Analysis Tools. </title> <booktitle> In SIGPLAN 1994 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 196-205, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Software simulation plays an integral role in the design and validation of high-performance uni- and multiprocessor systems. Two major simulation methodologies used are Trace-Driven Simulation (TDS) and Execution-Driven Simulation (EDS). In TDS, hardware probes [27, 30] or software instrumentation of the application <ref> [22, 24, 25] </ref> allow information like basic block or data addresses to be collected in a trace buffer during the application's execution. The generated information is later used to drive a simulator of the system under study.
Reference: [26] <author> T.M.Conte and W.W.Hwu. </author> <title> Systematic Prototyping of Superscalar Computer Architectures. </title> <booktitle> In 3rd IEEE International Workshop on Rapid System Prototyping, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: The generated information is later used to drive a simulator of the system under study. Given that an application may execute billions of instructions, to reduce the storage requirements of the traces, sampling <ref> [5, 21, 26] </ref> is often used. 1 This work was supported in part by the National Science Foundation under grants NSF Young Investigator Award MIP-9457436, ASC-9612099 and MIP-9619351, DARPA Contract DABT63-95-C-0097, NASA Contract NAG-1-613 and gifts from IBM and Intel.
Reference: [27] <author> J. Torrellas, A. Gupta, and J. Hennessy. </author> <title> Characterizing the Caching and Synchronization Performance of a Multiprocessor Operating System. </title> <booktitle> In 5th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 162-174, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Software simulation plays an integral role in the design and validation of high-performance uni- and multiprocessor systems. Two major simulation methodologies used are Trace-Driven Simulation (TDS) and Execution-Driven Simulation (EDS). In TDS, hardware probes <ref> [27, 30] </ref> or software instrumentation of the application [22, 24, 25] allow information like basic block or data addresses to be collected in a trace buffer during the application's execution. The generated information is later used to drive a simulator of the system under study.
Reference: [28] <author> M. Tremblay, D. Greenley, and K. Normoyle. </author> <booktitle> The Design of the Microarchitecture of UltraSPARC-I. Proceedings of the IEEE, </booktitle> <address> 83(12):1995, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Nevertheless, as we show in our evaluations, our approach considerably improves the ILP when modeling in-order issue superscalars. It must be noted that a similar strategy was adopted by the UltraSparc team to generate code for the in-order issue UltraSparc-I processor <ref> [28] </ref>. 2.2.2 Speculative Execution We model speculative execution by using the branch prediction mechanism of the simulated processor as indicated before. This allows multiple branch predictions to be performed when there are pending unresolved branches.
Reference: [29] <author> D. Tullsen, S. Eggers, J. Emer, H. Levy, J. Lo, and R. Stamm. </author> <title> Exploiting Choice: Instruction Fetch and Issue on an Implementable Simultaneous Multithreading Processor. </title> <booktitle> In 23rd International Symposium on Computer Architecture, </booktitle> <pages> pages 191-202, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: core as a building block to model advanced processor architectures such as a chip multiprocessor (CMP), where multiple superscalar cores share a single chip [7]; a simultaneous mul-tithreaded (SMT) processor, which adds multiple threads to a superscalar and allows instructions from different threads to be issued in the same cycle <ref> [29] </ref>; and finally a hybrid of the two architectures that supports several SMT processors on a single chip [8]. By varying parameters such as the number of on-chip processors and the number of threads in a SMT processor, we can model different architectures.
Reference: [30] <author> R. Uhlig, D. Nagle, T. Stanley, T. Mudge, S. Sechrest, and R. Brown. </author> <title> Design Tradeoff for Software-Managed TLBs. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(3) </volume> <pages> 206-235, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Software simulation plays an integral role in the design and validation of high-performance uni- and multiprocessor systems. Two major simulation methodologies used are Trace-Driven Simulation (TDS) and Execution-Driven Simulation (EDS). In TDS, hardware probes <ref> [27, 30] </ref> or software instrumentation of the application [22, 24, 25] allow information like basic block or data addresses to be collected in a trace buffer during the application's execution. The generated information is later used to drive a simulator of the system under study.
Reference: [31] <author> J. Veenstra and R. Fowler. MINT: </author> <title> A Front End for Efficient Simulation of Shared-Memory Multiprocessors. </title> <booktitle> In MASCOTS'94, </booktitle> <pages> pages 201-207, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: When the simulator is called, it processes the event. Then, it returns control to the application. This method facilitates feedback from the simulator to guide the execution of the parallel application. There have been many EDS-based multiprocessor system simulators for both RISC and CISC architectures <ref> [2, 4, 6, 15, 16, 20, 23, 31] </ref>. Most of these EDS-based systems are based on direct execution. Here, the instrumented application is executed directly on the host machine where the simulation runs. This results in fast simulation. Unfortunately, using simulation based on direct execution comes at a price. <p> The complete set of RISCier instructions are shown in Table 1. Overall, our simulation framework consists of three modules. First, we have a front-end that instruments and then directly executes an executable file. For this, we use MINT <ref> [31] </ref>, a well-known system that operates on MIPS executables. We have modified MINT to handle MIPS-II binaries. Then, unlike a conventional simple direct-execution simulator, events are no longer passed directly to the memory simulator back-end. Instead, they are intercepted by the processor simulator. <p> Column 4 shows the MINT slowdown over the native execution of the applications. MINT has been reported to have a slowdown of around 40-70 <ref> [31] </ref>. <p> MINT.1 represents the time taken by MINT for a single-node configuration. applications used in <ref> [31] </ref> are quite different from those that we use. In our case, we see slowdowns in the range of 70-200. Factoring this, the full simulator is around 1300 times slower than native execution.
Reference: [32] <author> D. Wall. </author> <title> Limits of Instruction Level Parallelism. </title> <booktitle> In 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 176-189, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: There are two reasons for it. First, the simulation would be too slow. Secondly and more importantly, the simulator cannot consume just one instruction at a time. Indeed, since the size of a basic block is only a few instructions in integer applications <ref> [32] </ref>, wide-issue superscalars use advanced branch-prediction schemes [19, 34] to fetch instructions across multiple basic blocks. Thus our model should permit sufficient information from the front-end to be gathered as dictated by the branch-prediction scheme of the simulated processor, before invoking the simulator.
Reference: [33] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: We can also selectively turn off the processor simulator while allowing memory references to the memory back-end. This is particularly helpful when running parallel applications in which the initialization phase need not be processor-simulated, while the caches have to be warmed-up before the program enters the parallel section <ref> [33] </ref>. Additionally, it allows the development and testing of the memory back-end independently of the processor simulator. 3 Evaluation The complete framework is implemented in C/C++ and is around 20K lines of code. Both accuracy and speed were kept in mind during the implementation. <p> We evaluate our framework for both a uni- and a multiprocessor environment. The evaluation for the uniprocessor environment is performed with two SPECint95 (compress and ijpeg), one multimedia [10] (mpeg) and four SPECfp95 (swim, tomcatv , hydro2d and mgrid ) applications. Two SPLASH-2 <ref> [33] </ref> (radix and cholesky) applications are used to evaluate the multiprocessor environment. The applications are compiled for a MIPS R4400 processor, using the highest level of optimization and the -mips2 option. The MIPS R4400 is a simple single-issue processor.
Reference: [34] <author> T. Yeh and Y. Patt. </author> <title> Alternative Implementations of Two-Level Adaptive Branch Prediction. </title> <booktitle> In 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 124-134, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: First, the simulation would be too slow. Secondly and more importantly, the simulator cannot consume just one instruction at a time. Indeed, since the size of a basic block is only a few instructions in integer applications [32], wide-issue superscalars use advanced branch-prediction schemes <ref> [19, 34] </ref> to fetch instructions across multiple basic blocks. Thus our model should permit sufficient information from the front-end to be gathered as dictated by the branch-prediction scheme of the simulated processor, before invoking the simulator.
References-found: 34

