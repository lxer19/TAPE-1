URL: http://ceylon.lcs.mit.edu/6891/papers/4_ieee.ps
Refering-URL: http://ceylon.lcs.mit.edu/6891/reading_list.html
Root-URL: 
Email: Email: w-hwu@uiuc.edu  
Title: Compiler Technology for Future Microprocessors  
Author: Wen-mei W. Hwu Richard E. Hank David M. Gallagher Scott A. Mahlke Daniel M. Lavery Grant E. Haab John C. Gyllenhaal David I. August Correspondent: Wen-mei W. Hwu 
Address: Urbana, IL 61801  (217)-244-8270  
Affiliation: Center for Reliable and High-Performance Computing University of Illinois  Tel:  
Abstract: Advances in hardware technology have made it possible for microprocessors to execute a large number of instructions concurrently (i.e., in parallel). These microprocessors take advantage of the opportunity to execute instructions in parallel to increase the execution speed of a program. As in other forms of parallel processing, the performance of these microprocessors can vary greatly depending on the quality of the software. In particular, the quality of compilers can make an order of magnitude difference in performance. This paper presents a new generation of compiler technology that has emerged to deliver the large amount of instruction-level-parallelism that is already required by some current state-of-the-art microprocessors and will be required by more future microprocessors. We introduce critical components of the technology which deal with difficult problems that are encountered when compiling programs for a high degree of instruction-level-parallelism. We present examples to illustrate the functional requirements of these components. To provide more insight into the challenges involved, we present in-depth case studies on predicated compilation and maintenance of dependence information, two of the components that are largely missing from most current commercial compilers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Asprey, G. Averill, E. DeLano, R. Mason, B. Weiner, and J. Yetter, </author> <title> "Performance features of the PA7100 microprocessor," </title> <journal> IEEE Micro, </journal> <volume> vol. 13, </volume> <pages> pp. 22-35, </pages> <month> June </month> <year> 1993. </year> <month> 31 </month>
Reference-contexts: 1 Introduction The microprocessor industry continues to successfully meet the demand for increased performance in the market place. In 1994, high-end microprocessors <ref> [1] </ref> [2] executed integer code at about 100 fl With the Department of Electrical and Computer Engineering, Air Force Institute of Technology, WPAFB, OH 45433 y With Hewlett-Packard Laboratories, 1501 Page Mill Rd., MS 3L-5, Palo Alto, CA 94304 times the performance of a high-end microprocessor introduced in 1984 [3]. 1 <p> The low-level (assembly-level) code 2 for the loop, after traditional optimization [8], is presented in Figure 1 (b). The descriptions of the register contents are represented using C-language syntax. For example, &B <ref> [1] </ref> is the address of the second element of array B. The horizontal lines partition the code into basic blocks. These are regions of straight-line code separated by decision points and merge points. Basic block L0 performs the initialization before the loop.
Reference: [2] <author> E. McLellan, </author> <title> "The Alpha AXP architecture and 21064 processor," </title> <journal> IEEE Micro, </journal> <volume> vol. 13, </volume> <pages> pp. </pages> <address> 36--47, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: 1 Introduction The microprocessor industry continues to successfully meet the demand for increased performance in the market place. In 1994, high-end microprocessors [1] <ref> [2] </ref> executed integer code at about 100 fl With the Department of Electrical and Computer Engineering, Air Force Institute of Technology, WPAFB, OH 45433 y With Hewlett-Packard Laboratories, 1501 Page Mill Rd., MS 3L-5, Palo Alto, CA 94304 times the performance of a high-end microprocessor introduced in 1984 [3]. 1 Such
Reference: [3] <author> J. Emer and D. Clark, </author> <title> "A characterization of processor performance in the VAX-11/780," </title> <booktitle> in Proceedings of the 11th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1984. </year>
Reference-contexts: microprocessors [1] [2] executed integer code at about 100 fl With the Department of Electrical and Computer Engineering, Air Force Institute of Technology, WPAFB, OH 45433 y With Hewlett-Packard Laboratories, 1501 Page Mill Rd., MS 3L-5, Palo Alto, CA 94304 times the performance of a high-end microprocessor introduced in 1984 <ref> [3] </ref>. 1 Such rapid growth in microprocessor performance has stimulated the development and sales of powerful system and application programs that demand even more performance. This positive cycle has helped create a very high rate of growth for the microprocessor industry.
Reference: [4] <author> J. H. Crawford, </author> <title> "The i486 CPU: Executing instructions in one clock cycle," </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 27-36, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: An area of innovation that is important to the next major boost of microprocessor performance is instruction-level parallel processing. This is reflected by the fact that high-performance microprocessors are increasingly designed to exploit instruction-level parallelism (ILP). While mainstream microprocessors in 1990 executed one instruction per clock cycle <ref> [4] </ref> [5], those in 1995 execute up to four instructions per cycle [6] [7]. By the year 2000, hardware technology is expected to produce microprocessors that can execute up to sixteen instructions per clock cycle.
Reference: [5] <author> M. Forsyth, S. Mangelsdorf, E. Delano, C. Gleason, and J. Yetter, </author> <title> "CMOS PA-RISC processor for a new family of workstations," </title> <booktitle> in Proceedings of COMPCON, </booktitle> <pages> pp. 202-207, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: An area of innovation that is important to the next major boost of microprocessor performance is instruction-level parallel processing. This is reflected by the fact that high-performance microprocessors are increasingly designed to exploit instruction-level parallelism (ILP). While mainstream microprocessors in 1990 executed one instruction per clock cycle [4] <ref> [5] </ref>, those in 1995 execute up to four instructions per cycle [6] [7]. By the year 2000, hardware technology is expected to produce microprocessors that can execute up to sixteen instructions per clock cycle. Such rapid, dramatic increases in the hardware parallelism have placed tremendous pressure on the compiler technology.
Reference: [6] <author> J. H. Edmondson, P. Rubinfeld, R. Preston, and V. Rajagopalan, </author> <title> "Superscalar instruction execution in the 21164 Alpha microprocessor," </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 33-43, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: This is reflected by the fact that high-performance microprocessors are increasingly designed to exploit instruction-level parallelism (ILP). While mainstream microprocessors in 1990 executed one instruction per clock cycle [4] [5], those in 1995 execute up to four instructions per cycle <ref> [6] </ref> [7]. By the year 2000, hardware technology is expected to produce microprocessors that can execute up to sixteen instructions per clock cycle. Such rapid, dramatic increases in the hardware parallelism have placed tremendous pressure on the compiler technology. <p> For example, most of the high performance microprocessors introduced in 1995 are capable of issuing four instructions every clock cycle, but only one of these four instructions can be a branch <ref> [6] </ref> [7]. Frequent occurrences of branches can seriously limit the execution speed of programs.
Reference: [7] <author> P. Wayner, </author> <title> "SPARC strikes back," </title> <journal> Byte, </journal> <volume> vol. 19, </volume> <pages> pp. 105-112, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: This is reflected by the fact that high-performance microprocessors are increasingly designed to exploit instruction-level parallelism (ILP). While mainstream microprocessors in 1990 executed one instruction per clock cycle [4] [5], those in 1995 execute up to four instructions per cycle [6] <ref> [7] </ref>. By the year 2000, hardware technology is expected to produce microprocessors that can execute up to sixteen instructions per clock cycle. Such rapid, dramatic increases in the hardware parallelism have placed tremendous pressure on the compiler technology. <p> For example, most of the high performance microprocessors introduced in 1995 are capable of issuing four instructions every clock cycle, but only one of these four instructions can be a branch [6] <ref> [7] </ref>. Frequent occurrences of branches can seriously limit the execution speed of programs.
Reference: [8] <author> A. Aho, R. Sethi, and J. Ullman, </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: By the year 2000, hardware technology is expected to produce microprocessors that can execute up to sixteen instructions per clock cycle. Such rapid, dramatic increases in the hardware parallelism have placed tremendous pressure on the compiler technology. Traditionally, optimizing compilers improve program execution speed by eliminating unnecessary instruction processing <ref> [8] </ref>. By keeping memory data and intermediate computation results in high speed processor registers, optimizing compilers reduce the program execution cycles spent waiting for the memory system and performing redundant computation. <p> The body of the loop consists of a simple if-then-else construct. In each iteration, it evaluates the condition A [i] 6= 0 to determine if the then part or the else part should be executed. The low-level (assembly-level) code 2 for the loop, after traditional optimization <ref> [8] </ref>, is presented in Figure 1 (b). The descriptions of the register contents are represented using C-language syntax. For example, &B [1] is the address of the second element of array B. The horizontal lines partition the code into basic blocks. <p> Hyperblock formation combines basic blocks from multiple control flow paths into a single block for optimization and scheduling. The presence of multiple control flow paths within a hyperblock exposes more opportunities for the compiler to apply classical optimizations, such as common subexpression elimination and copy propagation <ref> [8] </ref>. Hyperblock formation also increases the applicability of more aggressive ILP techniques by transforming complex control flow constructs into constructs that are better understood. For example, if-conversion transformed the complex control flow found in the wc example of Figure 7 into a single-block loop which greatly facilitates loop optimizations.
Reference: [9] <author> D. W. Wall, </author> <title> "Limits of instruction-level parallelism," </title> <booktitle> in Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 176-188, </pages> <month> April </month> <year> 1991. </year>
Reference: [10] <author> M. S. Lam and R. P. Wilson, </author> <title> "Limits of control flow on parallelism," </title> <booktitle> in Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pp. 46-57, </pages> <month> May </month> <year> 1992. </year>
Reference: [11] <author> J. A. Fisher, </author> <title> "Trace scheduling: A technique for global microcode compaction," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-30, </volume> <pages> pp. 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: They differ in their treatment of the decision point associated with instruction 2 in Figure 1 (b). This instruction is highlighted both in Figure 1 (b) and later examples to emphasize the difference between the two approaches. One approach is to form either a trace <ref> [11] </ref> or a superblock [12], which is a region consisting of a sequence of basic blocks along a frequently executed path. This path is then optimized, sometimes at the expense of the infrequently executed paths.
Reference: [12] <author> W. W. Hwu, S. A. Mahlke, W. Y. Chen, P. P. Chang, N. J. Warter, R. A. Bringmann, R. G. Ouellette, R. E. Hank, T. Kiyohara, G. E. Haab, J. G. Holm, and D. M. Lavery, </author> <title> "The 32 Superblock: An effective technique for VLIW and superscalar compilation," </title> <journal> The Journal of Supercomputing, </journal> <volume> vol. 7, </volume> <pages> pp. 229-248, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: They differ in their treatment of the decision point associated with instruction 2 in Figure 1 (b). This instruction is highlighted both in Figure 1 (b) and later examples to emphasize the difference between the two approaches. One approach is to form either a trace [11] or a superblock <ref> [12] </ref>, which is a region consisting of a sequence of basic blocks along a frequently executed path. This path is then optimized, sometimes at the expense of the infrequently executed paths.
Reference: [13] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Papworth, and P. K. Rodman, </author> <title> "A VLIW architecture for a trace scheduling compiler," </title> <booktitle> in Proceedings of the 2nd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 180-192, </pages> <month> April </month> <year> 1987. </year>
Reference: [14] <author> M. D. Smith, M. A. Horowitz, and M. S. Lam, </author> <title> "Efficient superscalar performance through boosting," </title> <booktitle> in Proceedings of the Fifth International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 248-259, </pages> <month> October </month> <year> 1992. </year>
Reference: [15] <author> S. A. Mahlke, W. Y. Chen, R. A. Bringmann, R. E. Hank, W. W. Hwu, B. R. Rau, and M. S. Schlansker, </author> <title> "Sentinel scheduling: A model for compiler-controlled speculative execution," </title> <journal> Transactions on Computer Systems, </journal> <volume> vol. 11, </volume> <month> November </month> <year> 1993. </year>
Reference: [16] <author> R. A. Bringmann, S. A. Mahlke, R. E. Hank, J. C. Gyllenhaal, and W. W. Hwu, </author> <title> "Speculative execution exception recovery using write-back suppression," </title> <booktitle> in Proceedings of 26th Annual International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1993. </year>
Reference: [17] <author> P. Y. Hsu and E. S. Davidson, </author> <title> "Highly concurrent scalar processing," </title> <booktitle> in Proceedings of the 13th International Symposium on Computer Architecture, </booktitle> <pages> pp. 386-395, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Also, when there is more than one important path, the branches can be less predictable, leading to more performance problems at run time. If more than one important path exists, a larger region can be formed using predicated execution <ref> [17] </ref> [18]. This technique merges several paths into a single block and eliminates from the instruction stream the branches associated with those paths. Predicated or guarded execution refers to the conditional execution of an instruction based on the value of a boolean source operand, referred to as the predicate.
Reference: [18] <author> B. R. Rau, D. W. L. Yen, W. Yen, and R. A. Towle, </author> <title> "The Cydra 5 departmental supercomputer," </title> <journal> IEEE Computer, </journal> <volume> vol. 22, </volume> <pages> pp. 12-35, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Also, when there is more than one important path, the branches can be less predictable, leading to more performance problems at run time. If more than one important path exists, a larger region can be formed using predicated execution [17] <ref> [18] </ref>. This technique merges several paths into a single block and eliminates from the instruction stream the branches associated with those paths. Predicated or guarded execution refers to the conditional execution of an instruction based on the value of a boolean source operand, referred to as the predicate.
Reference: [19] <author> J. R. Allen, K. Kennedy, C. Porterfield, and J. Warren, </author> <title> "Conversion of control dependence to data dependence," </title> <booktitle> in Proceedings of the 10th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 177-189, </pages> <month> January </month> <year> 1983. </year> <month> 33 </month>
Reference-contexts: Through a process known as if-conversion, the compiler converts conditional branches into predicate define instructions, and assigns predicates to instructions along alternative paths of each branch <ref> [19] </ref> [20] [21]. Predicated instructions are fetched regardless 8 of their predicate value. Instructions whose predicate is true are executed normally. Conversely, instructions with false predicates are nullified, and thus are prevented from modifying the processor state. <p> Control flow may leave from one or more blocks in the hyperblock. All control flow between basic blocks in a hyperblock is removed by eliminating branch instructions and introducing conditional instructions through if-conversion <ref> [19] </ref> [20]. While forming hyperblocks, the compiler must trade off branch performance with resource usage, hazard conditions, and critical path length. Selecting blocks on either side of a branch instruction eliminates the branch and any associated misprediction penalties.
Reference: [20] <author> J. C. Park and M. S. Schlansker, </author> <title> "On predicated execution," </title> <type> Tech. Rep. </type> <institution> HPL-91-58, Hewlett Packard Laboratories, </institution> <address> Palo Alto, CA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Through a process known as if-conversion, the compiler converts conditional branches into predicate define instructions, and assigns predicates to instructions along alternative paths of each branch [19] <ref> [20] </ref> [21]. Predicated instructions are fetched regardless 8 of their predicate value. Instructions whose predicate is true are executed normally. Conversely, instructions with false predicates are nullified, and thus are prevented from modifying the processor state. <p> Control flow may leave from one or more blocks in the hyperblock. All control flow between basic blocks in a hyperblock is removed by eliminating branch instructions and introducing conditional instructions through if-conversion [19] <ref> [20] </ref>. While forming hyperblocks, the compiler must trade off branch performance with resource usage, hazard conditions, and critical path length. Selecting blocks on either side of a branch instruction eliminates the branch and any associated misprediction penalties.
Reference: [21] <author> S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank, and R. A. Bringmann, </author> <title> "Effective compiler support for predicated execution using the hyperblock," </title> <booktitle> in Proceedings of the 25th International Symposium on Microarchitecture, </booktitle> <pages> pp. 45-54, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Through a process known as if-conversion, the compiler converts conditional branches into predicate define instructions, and assigns predicates to instructions along alternative paths of each branch [19] [20] <ref> [21] </ref>. Predicated instructions are fetched regardless 8 of their predicate value. Instructions whose predicate is true are executed normally. Conversely, instructions with false predicates are nullified, and thus are prevented from modifying the processor state. <p> Initially, instructions 3-6 are all conditionally executed on predicate p1. However, a technique referred to as predicate promotion allows instructions 3, 4, and 5 from the then path to be executed before the result of the comparison is known <ref> [21] </ref> by removing the predicates from those instructions. <p> Furthermore, the reduced frequency of branches will increase the performance of processors with limited branch handling resources. 14 3.1 Predicate-Based Compilation A suite of compiler techniques to effectively use predicated execution has been developed based on the hyperblock structure <ref> [21] </ref>. A hyperblock is a collection of connected basic blocks in which control may only enter at the first block, designated as the entry block. Control flow may leave from one or more blocks in the hyperblock.
Reference: [22] <author> P. G. Lowney, S. M. Freudenberger, T. J. Karzes, W. D. Lichtenstein, R. P. Nix, J. S. O'Donell, and J. C. Ruttenberg, </author> <title> "The Multiflow trace scheduling compiler," </title> <journal> The Journal of Supercomputing, </journal> <volume> vol. 7, </volume> <pages> pp. 51-142, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The examples in the rest of the section will continue to optimize this predicated code. In Figure 4, the compiler is limited to a single iteration of the loop when looking for independent instructions. Loop unrolling <ref> [22] </ref> and software pipelining [23][24][25] are two techniques which allow the compiler to overlap the execution of multiple iterations. A loop is unrolled by placing several copies of the loop body sequentially in memory. This forms a much larger block in much the same way as forming a superblock. <p> There are two exceptions that the authors are aware of. The compiler for the Cydrome Cydra-5 [37] performed detailed memory dependence analysis for inner loops and used that information to support optimization of memory reference instructions and software pipelining. The Multiflow Trace Scheduling Compiler <ref> [22] </ref> also performed detailed memory dependence analysis for low-level code, but within traces instead of 22 inner loops, and used that information to support optimization and scheduling for the TRACE series of VLIW computers.
Reference: [23] <author> B. R. Rau, </author> <title> "Iterative modulo scheduling: An algorithm for software pipelining loops," </title> <booktitle> in Proceedings of the 27th International Symposium on Microarchitecture, </booktitle> <pages> pp. 63-74, </pages> <month> December </month> <year> 1994. </year>
Reference: [24] <author> K. Ebcioglu and T. Nakatani, </author> <title> "A new compilation technique for parallelizing loops with unpredictable branches on a VLIW architecture," </title> <booktitle> in Languages and Compilers for Parallel Computing, </booktitle> <pages> pp. 213-229, </pages> <year> 1989. </year>
Reference: [25] <author> P. Tirumalai, M. Lee, and M. Schlansker, </author> <title> "Parallelization of loops with exits on pipelined architectures," </title> <booktitle> in Proceedings of Supercomputing '90, </booktitle> <month> November </month> <year> 1990. </year>
Reference: [26] <author> W. Y. Chen, </author> <title> Data Preload for Superscalar and VLIW Processors. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois, Urbana, IL, </institution> <year> 1993. </year>
Reference: [27] <author> D. M. Gallagher, </author> <title> Memory Disambiguation to Facilitate Instruction-Level Parallelism Compilation. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois, Urbana, IL, </institution> <year> 1995. </year>
Reference-contexts: For this case, memory dependence information is propagated to the low-level code in the form of dependence arcs. Both versions of the compiled code were then evaluated using a detailed simulation of the processor and memory system. For more details of the experimental evaluation presented here, see <ref> [27] </ref>. An experimental evaluation of the Multiflow TRACE 14/300 computer, including the bottlenecks due to memory dependences, has been published elsewhere [46]. 4.4.1 Integer Benchmark Results figure are from the SPEC CINT92 suite and the rest are UNIX T M utilities.
Reference: [28] <author> J. E. Smith, </author> <title> "A study of branch prediction strategies," </title> <booktitle> in Proceedings of the 8th International Symposium on Computer Architecture, </booktitle> <pages> pp. 135-148, </pages> <month> May </month> <year> 1981. </year> <month> 34 </month>
Reference-contexts: In a processor that exploits instruction-level parallelism, branches limit ILP in two principal ways. First, branches force high performance processors to perform branch prediction at the cost of large performance penalty if prediction fails <ref> [28] </ref> [29] [30]. For example, if the execution takes path A ! B ! D ! E ! F ! H ! A, it will encounter five branch instructions.
Reference: [29] <author> J. Lee and A. J. Smith, </author> <title> "Branch prediction strategies and branch target buffer design," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 6-22, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: In a processor that exploits instruction-level parallelism, branches limit ILP in two principal ways. First, branches force high performance processors to perform branch prediction at the cost of large performance penalty if prediction fails [28] <ref> [29] </ref> [30]. For example, if the execution takes path A ! B ! D ! E ! F ! H ! A, it will encounter five branch instructions.
Reference: [30] <author> T. Y. Yeh and Y. N. Patt, </author> <title> "A comparison of dynamic branch predictors that use two levels of branch history," </title> <booktitle> in Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 257-266, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In a processor that exploits instruction-level parallelism, branches limit ILP in two principal ways. First, branches force high performance processors to perform branch prediction at the cost of large performance penalty if prediction fails [28] [29] <ref> [30] </ref>. For example, if the execution takes path A ! B ! D ! E ! F ! H ! A, it will encounter five branch instructions.
Reference: [31] <author> V. Kathail, M. S. Schlansker, and B. R. Rau, </author> <title> "HPL PlayDoh architecture specification: Version 1.0," </title> <type> Tech. Rep. </type> <institution> HPL-93-80, Hewlett-Packard Laboratories, </institution> <address> Palo Alto, CA, </address> <month> February </month> <year> 1994. </year>
Reference-contexts: Blocks D through M are duplicated so that the control flow does not reenter the middle of the hyperblock. This is done to eliminate the effect the hazard conditions in C can have on the parallelization of the resulting hyperblock. The Hewlett-Packard Laboratories PlayDoh architecture <ref> [31] </ref> helps us illustrate the compiler extensions required to support predicated execution. Within the PlayDoh architecture, all instructions have an operand that serves as a predicate specifier, that is, all instructions are predicated. The predicates are located in a centralized predicate register file.
Reference: [32] <author> S. A. Mahlke, </author> <title> Exploiting Instruction Level Parallelism in the Presence of Conditional Branches. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois, Ur-bana, IL, </institution> <year> 1995. </year>
Reference-contexts: The targets of the remaining branches were not included in the hyperblock because the contents of those blocks would have resulted in a less efficient hyperblock. For these cases, the compiler can employ a transformation, referred to as branch combining <ref> [32] </ref>, to further eliminate exit branches from the hyperblock. Branch combining replaces a group of exit branches with a corresponding group of predicate define instructions. All of the predicate defines write into the same predicate register using the OR-type semantics. <p> Additional Predicate-Based Optimizations. Two other important optimizations which take advantage of predicated execution support are loop peeling and control height reduction. Loop peeling targets inner loops that tend to iterate infrequently <ref> [32] </ref>. For these loops, loop unrolling and 19 software pipelining are usually ineffective for exposing sufficient ILP since the number of iterations available to overlap is small. Loop peeling is a technique whereby the compiler "peels" away the first several iterations of a loop. <p> Using height reduction, a compiler can significantly shorten dependence chain lengths to compute predicates, thereby enhancing ILP. A complete description of these and other predicate optimizations are beyond the scope of this paper. The interested reader is referred to [33] and <ref> [32] </ref> for more details. 3.3 Experimental Evaluation To illustrate the effect of full predication on processor performance, experiments were conducted on a set of ten C benchmark programs, including programs from SPEC CFP92 (052.alvinn, 056.ear) SPEC CINT92 (008.espresso, 023.eqntott, 072.sc), and common UNIX T M utilities (cmp, eqn, grep, lex, wc).
Reference: [33] <author> M. Schlansker, V. Kathail, and S. Anik, </author> <title> "Height reduction of control recurrences for ILP processors," </title> <booktitle> in Proceedings of the 27th International Symposium on Microarchitecture, </booktitle> <pages> pp. 40-51, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Control height reduction, is a technique which reduces the dependence chain length to compute the execution conditions of instructions <ref> [33] </ref>. In the control flow domain, an execution path is a sequence of directions taken by the branches leading to a particular instruction. In the predicate domain, the execution path is a sequence of predicate values used to compute the predicate of a particular instruction. <p> Using height reduction, a compiler can significantly shorten dependence chain lengths to compute predicates, thereby enhancing ILP. A complete description of these and other predicate optimizations are beyond the scope of this paper. The interested reader is referred to <ref> [33] </ref> and [32] for more details. 3.3 Experimental Evaluation To illustrate the effect of full predication on processor performance, experiments were conducted on a set of ten C benchmark programs, including programs from SPEC CFP92 (052.alvinn, 056.ear) SPEC CINT92 (008.espresso, 023.eqntott, 072.sc), and common UNIX T M utilities (cmp, eqn, grep,
Reference: [34] <author> G. S. Tyson, </author> <title> "The effects of predicated execution on branch prediction," </title> <booktitle> in Proceedings of the 27th International Symposium on Microarchitecture, </booktitle> <pages> pp. 196-206, </pages> <month> December </month> <year> 1994. </year>
Reference: [35] <author> D. N. Pnevmatikatos and G. S. Sohi, </author> <title> "Guarded execution and branch prediction in dynamic ILP processors," </title> <booktitle> in Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pp. 120-129, </pages> <month> April </month> <year> 1994. </year>
Reference: [36] <author> S. A. Mahlke, R. E. Hank, J. McCormick, D. I. August, and W. W. Hwu, </author> <title> "A comparison of full and partial predicated execution support for ILP processors," </title> <booktitle> in Proceedings of the 22th International Symposium on Computer Architecture, </booktitle> <pages> pp. 138-150, </pages> <month> June </month> <year> 1995. </year>
Reference: [37] <author> J. C. Dehnert and R. A. Towle, </author> <title> "Compiling for the Cydra 5," </title> <journal> The Journal of Supercomputing, </journal> <volume> vol. 7, </volume> <pages> pp. 181-227, </pages> <month> January </month> <year> 1993. </year> <month> 35 </month>
Reference-contexts: For example, commercial compilers can often determine the independence of references to separate global variables or to separate locations on the stack. However, they often have trouble analyzing array and pointer references. There are two exceptions that the authors are aware of. The compiler for the Cydrome Cydra-5 <ref> [37] </ref> performed detailed memory dependence analysis for inner loops and used that information to support optimization of memory reference instructions and software pipelining.
Reference: [38] <author> M. J. Wolfe, </author> <title> Optimizing Compilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <year> 1982. </year>
Reference: [39] <author> U. Banerjee, </author> <title> Dependence Analysis for Supercomputing. </title> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference: [40] <author> W. Pugh and D. Wonnacott, </author> <title> "Eliminating false data dependences using the omega test," </title> <booktitle> in Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 140-151, </pages> <month> June </month> <year> 1992. </year>
Reference: [41] <author> J. Banning, </author> <title> "An efficient way to find the side effects of procedure calls and the aliases of variables," </title> <booktitle> in Proceedings of the 6th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 29-41, </pages> <month> January </month> <year> 1979. </year>
Reference: [42] <author> K. Cooper, </author> <title> "Analyzing aliases of reference formal parameters," </title> <booktitle> in Proceedings of the 12th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 281-290, </pages> <month> January </month> <year> 1985. </year>
Reference: [43] <author> W. Landi and B. G. Ryder, </author> <title> "A safe approximate algorithm for interprocedural pointer aliasing," </title> <booktitle> in Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 235-248, </pages> <month> June </month> <year> 1992. </year>
Reference: [44] <author> E. Ruf, </author> <title> "Context-insensitive alias analysis reconsidered," </title> <booktitle> in Proceedings of the ACM SIGPLAN 95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 13-22, </pages> <month> June </month> <year> 1995. </year>
Reference: [45] <author> W. Pugh, </author> <title> "A practical algorithm for exact array dependence analysis," </title> <journal> Communications of the ACM, </journal> <volume> vol. 35, </volume> <pages> pp. 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The base-level dependence analysis uses only symbol table information and inexpensive memory address analysis, and thus cannot determine the independence of many pointer and array references. In contrast, the aggressive memory dependence analysis includes both the sophisticated array dependence analysis performed by the Omega Test <ref> [45] </ref>, as 28 well as inter-procedural analysis of pointer aliases and function side effects. For this case, memory dependence information is propagated to the low-level code in the form of dependence arcs.
Reference: [46] <author> M. A. Schuette and J. P. Shen, </author> <title> "An instruction-level experimental evaluation of the Multi-flow TRACE 14/300 VLIW computer," </title> <journal> The Journal of Supercomputing, </journal> <volume> vol. 7, </volume> <pages> pp. 181-227, </pages> <month> January </month> <year> 1993. </year> <title> 36 graph. 37 38 path. 39 40 (b) after renaming and memory dependence analysis. 41 42 after if-conversion. 43 after branch combining. 44 45 46 47 48 Table 1: Desired dependence information. Category Possible Values type flow, anti, output, input distance (integer), unknown carrying loop none, (loop identifier) certainty definite, </title> <type> maybe 49 50 51 52 </type>
Reference-contexts: For more details of the experimental evaluation presented here, see [27]. An experimental evaluation of the Multiflow TRACE 14/300 computer, including the bottlenecks due to memory dependences, has been published elsewhere <ref> [46] </ref>. 4.4.1 Integer Benchmark Results figure are from the SPEC CINT92 suite and the rest are UNIX T M utilities. The speedup of code compiled with and without memory dependence arcs is shown for an 8-issue architecture compared to a baseline single-issue architecture.
References-found: 46

