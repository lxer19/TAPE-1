URL: http://www.cs.cmu.edu/~thrun/papers/thrun.arbib-handbook.ps.gz
Refering-URL: http://www.cs.cmu.edu/~thrun/papers/full.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: thrun@carbon.cs.bonn.edu  
Phone: Phone: +49-228-550-373, FAX: +49-228-550-382  
Title: Exploration in Active Learning  
Author: Sebastian Thrun 
Address: Romerstr. 164, D-53117 Bonn, Germany  
Affiliation: Universitat Bonn Institut fur Informatik III  
Note: to appear in: Handbook of Brain Science and Neural Networks Michael Arbib (ed.)  
Abstract-found: 0
Intro-found: 1
Reference: <author> Angluin, D., </author> <year> 1988, </year> <title> Queries and concept learning, </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342. </pages>
Reference-contexts: Order-free active learning rests on the assumption that what is observed in the environment depends only upon the most recently executed action. Perhaps the best-studied approach of this kind is learning by queries <ref> ( Angluin, 1988 ) </ref> , ( Atlas et al., 1990 ) , ( Baum and Lang, 1991 ) . In query learning, the available actions are queries for values of an unknown target function. The environment provides immediate responses (answers) to these queries. <p> Similar results exist in the query learning framework <ref> ( Angluin, 1988 ) </ref> , ( Baum and Lang, 1991 ) . Although most results apply to certain deterministic environments only, in practice they often carry over to stochastic environments. 3 EXAMPLE This section briefly describes an artificial neural network approach to exploration in real-valued domains.
Reference: <author> Atlas, L., Cohn, D., Ladner, R., El-Sharkawi, M. A., Marks, R. J., Aggoune, M. E., and Park, D. C., </author> <year> 1990, </year> <title> Training connectionist networks with queries and selective sampling, Sebastian Thrun Exploration in Active Learning 9 Advances in Neural Information Processing Systems 2, </title> <editor> (Touretzky, D., </editor> <publisher> Ed.), </publisher> <pages> pp. 567-573, </pages> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Order-free active learning rests on the assumption that what is observed in the environment depends only upon the most recently executed action. Perhaps the best-studied approach of this kind is learning by queries ( Angluin, 1988 ) , <ref> ( Atlas et al., 1990 ) </ref> , ( Baum and Lang, 1991 ) . In query learning, the available actions are queries for values of an unknown target function. The environment provides immediate responses (answers) to these queries. <p> Indeed, this "greedy" principle, the optimization of knowledge gain, has been employed in most approaches to action selection in active learning. 2.2 Query Selection Recent research on query learning has led to a variety of approaches for the active selection of queries. For example, in <ref> ( Atlas et al., 1990 ) </ref> , ( Cohn, 1994 ) two approaches to learning by queries are described that both use a neural network model of the learner's uncertainty. During learning, queries are favored that have the least predictable outcome. <p> During learning, queries are favored that have the least predictable outcome. Uncertainty is estimated either by the difference of two models constructed from the same observations <ref> ( Atlas et al., 1990 ) </ref> , or based on an analysis of the parameters of the estimator ( Cohn, 1994 ) .
Reference: <author> Barto, A. G. and Singh, S. P., </author> <year> 1990, </year> <title> On the computational economics of reinforcement learning, Connectionist Models, </title> <booktitle> Proceedings of the 1990 Summer School, </booktitle> <editor> (Touretzky, D. S., El-man, J. L., Sejnowski, T. J., and Hinton, G. E., </editor> <booktitle> Eds.), </booktitle> <pages> pp. 35-44, </pages> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Barto, A. G., Bradtke, S. J., and Singh, S. P., </author> <title> to appear, Learning to act using real-time dynamic programming, </title> <journal> Artificial Intelligence. </journal>
Reference: <author> Baum, E. B. and Lang, K. J., </author> <year> 1991, </year> <title> Constructing hidden units using examples and queries, </title> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <editor> (Lippmann, R. P., Moody, J. E., and Touretzky, D. S., </editor> <booktitle> Eds.), </booktitle> <pages> pp. 904-910, </pages> <address> San Mateo, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Order-free active learning rests on the assumption that what is observed in the environment depends only upon the most recently executed action. Perhaps the best-studied approach of this kind is learning by queries ( Angluin, 1988 ) , ( Atlas et al., 1990 ) , <ref> ( Baum and Lang, 1991 ) </ref> . In query learning, the available actions are queries for values of an unknown target function. The environment provides immediate responses (answers) to these queries. In order-sensitive approaches, on the other hand, observations may depend on many actions. <p> Similar results exist in the query learning framework ( Angluin, 1988 ) , <ref> ( Baum and Lang, 1991 ) </ref> . Although most results apply to certain deterministic environments only, in practice they often carry over to stochastic environments. 3 EXAMPLE This section briefly describes an artificial neural network approach to exploration in real-valued domains.
Reference: <author> Cohn, D., </author> <year> 1994, </year> <title> Queries and exploration using optimal experiment design, </title> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <editor> (Cowan, J.D., Tesauro, G., and Alspector, J., Eds.), </editor> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, in ( Atlas et al., 1990 ) , <ref> ( Cohn, 1994 ) </ref> two approaches to learning by queries are described that both use a neural network model of the learner's uncertainty. During learning, queries are favored that have the least predictable outcome. <p> During learning, queries are favored that have the least predictable outcome. Uncertainty is estimated either by the difference of two models constructed from the same observations ( Atlas et al., 1990 ) , or based on an analysis of the parameters of the estimator <ref> ( Cohn, 1994 ) </ref> . Both approaches have proven superior to random sampling in empirical comparisons. ( Paass and Kindermann, 1995 ) propose a method that integrates an external cost function into the active learning framework.
Reference: <author> Kaelbling, L. P., </author> <year> 1993, </year> <title> Learning in Embedded Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In addition, a variety of heuristic estimators have been used for estimating the expected knowledge gain. Estimators typically use related quantities such as frequency, density, recency, or empirical prediction errors. For example, <ref> ( Kaelbling, 1993 ) </ref> suggests an approach to exploration in which actions are favored unless they have repeatedly been found to be disadvantageous. As a consequence, actions are selected that exhibit good performance or that are unexplored or both.
Reference: <author> Koenig, S. and Simmons, R. G., </author> <year> 1993, </year> <title> Complexity analysis of real-time reinforcement learning, </title> <booktitle> Proceeding of the Eleventh National Conference on Artificial Intelligence AAAI-93, </booktitle> <pages> pp. 99-105, </pages> <address> Menlo Park, CA, </address> <publisher> AAAI, AAAI Press/The MIT Press. </publisher>
Reference-contexts: As a consequence, actions are selected that exhibit good performance or that are unexplored or both. A similar approach following the same line of thought is proposed in <ref> ( Koenig and Simmons, 1993 ) </ref> . Their approach bears close resemblance with heuristic search techniques for graphs, ( Korf, 1988 ) , although it differs in that it does not assume the availability of a model of the environment. <p> in ( Whitehead, 1991 ) , which shows that random walk exploration can require exponential learning time in various cases, it has been shown that directed exploration techniques can reduce the complexity of active learning from exponential training time (random exploration) to polynomial training time ( Thrun, 1992a ) , <ref> ( Koenig and Simmons, 1993 ) </ref> . Similar results exist in the query learning framework ( Angluin, 1988 ) , ( Baum and Lang, 1991 ) .
Reference: <author> Korf, R. E., </author> <year> 1988, </year> <title> Real-time heuristic search: New results, </title> <booktitle> Proceedings of the sixth National Conference on Artificial Intelligence (AAAI-88), </booktitle> <pages> pp. 139-143, </pages> <address> Los Angeles, CA 90024, </address> <institution> Computer Science Department, University of California, </institution> <note> AAAI Press/MIT Press. </note>
Reference-contexts: As a consequence, actions are selected that exhibit good performance or that are unexplored or both. A similar approach following the same line of thought is proposed in ( Koenig and Simmons, 1993 ) . Their approach bears close resemblance with heuristic search techniques for graphs, <ref> ( Korf, 1988 ) </ref> , although it differs in that it does not assume the availability of a model of the environment. Koenig and Simmons also derive worst-case bounds for the complexity of exploration for deterministic shortest-path problems.
Reference: <author> Moore, A. W., </author> <year> 1990, </year> <title> Efficient Memory-based Learning for Robot Control, </title> <type> PhD thesis, </type> <institution> Trinity Hall, University of Cambridge, </institution> <address> England. </address>
Reference-contexts: In ( Thrun, 1992a ) several of these approaches are compared empirically, along with a combined approach taking frequency and recency into account. Approaches specific to memory-based learning can be found in <ref> ( Moore, 1990 ) </ref> and ( Schaal and Atkeson, 1994 ) . Memory-based learning memorizes all training data explicitly. In these approaches, the density of previous data points is used to asses the utility of actions for exploration.
Reference: <author> Paass, G. and Kindermann, J., </author> <year> 1995, </year> <title> Bayesian query construction for neural network models, </title> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <address> San Mateo, CA, </address> <note> Morgan Kaufmann, (to appear). </note>
Reference-contexts: Uncertainty is estimated either by the difference of two models constructed from the same observations ( Atlas et al., 1990 ) , or based on an analysis of the parameters of the estimator ( Cohn, 1994 ) . Both approaches have proven superior to random sampling in empirical comparisons. <ref> ( Paass and Kindermann, 1995 ) </ref> propose a method that integrates an external cost function into the active learning framework. More specifically, their approach favors queries that minimize the decision costs, which allows to focus learning on performance-relevant areas.
Reference: <author> Schaal, S. and Atkeson, C. G., </author> <year> 1994, </year> <title> Assessing the quality of learned local models, </title> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In ( Thrun, 1992a ) several of these approaches are compared empirically, along with a combined approach taking frequency and recency into account. Approaches specific to memory-based learning can be found in ( Moore, 1990 ) and <ref> ( Schaal and Atkeson, 1994 ) </ref> . Memory-based learning memorizes all training data explicitly. In these approaches, the density of previous data points is used to asses the utility of actions for exploration.
Reference: <author> Sutton, R. S., </author> <year> 1990, </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming, </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <month> June </month> <year> 1990, </year> <pages> pp. 216-224, </pages> <address> San Mateo, CA, </address> <note> Morgan Kaufmann. Sebastian Thrun Exploration in Active Learning 10 Thrun, </note> <author> S. B., </author> <year> 1992, </year> <title> Efficient exploration in reinforcement learning, </title> <type> Technical Report CMU-CS-92-102, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213. </address>
Reference-contexts: Koenig and Simmons also derive worst-case bounds for the complexity of exploration for deterministic shortest-path problems. In <ref> ( Sutton, 1990 ) </ref> , a so-called exploration bonus is assigned to actions. This bonus measures, for each environment state, the elapsed time since each available action Sebastian Thrun Exploration in Active Learning 4 was executed. <p> Moreover, depending on what goal the learner aims to achieve, sometimes only parts of the environment have to be known in order to perform optimally. This is typically the case, for example, in the context of reinforcement learning ( Barto et al., to appear ) , <ref> ( Sutton, 1990 ) </ref> , ( Watkins and Dayan, 1992 ) . In reinforcement learning, the learning task is to generate control, i.e., to learn action policies that maximize a given reward function.
Reference: <author> Thrun, S. B., </author> <year> 1992, </year> <title> The role of exploration in learning control, Handbook of intelligent control: neural, fuzzy and adaptive approaches, (White, </title> <editor> David A. and Sofge, Donald A., Eds.). </editor> <publisher> Van Nostrand Reinhold, </publisher> <address> Florence, Kentucky 41022. </address>
Reference: <author> Watkins, C. J. C. H. and Dayan, P., </author> <year> 1992, </year> <title> Q-learning, </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292. </pages>
Reference-contexts: This is typically the case, for example, in the context of reinforcement learning ( Barto et al., to appear ) , ( Sutton, 1990 ) , <ref> ( Watkins and Dayan, 1992 ) </ref> . In reinforcement learning, the learning task is to generate control, i.e., to learn action policies that maximize a given reward function.
Reference: <author> Whitehead, S. D., </author> <year> 1991, </year> <title> Complexity and cooperation in Q-learning, </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, (Birnbaum, L.A. and Collins, G.C., Eds.), </booktitle> <pages> pp. 363-367, </pages> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: paper we will make the simplifying and restrictive assumption that the state of the environment is fully observable. 2 ACTION SELECTION STRATEGIES 2.1 Principles How can a learner pick the right action for learning? At first glance, it might seem appropriate to use random action selection mechanisms to generate actions <ref> ( Whitehead, 1991 ) </ref> . Random action selection is frequently used, primarily for two reasons: (a) it is simple, and (b) it usually ensures that any possible finite sequence of actions will be executed eventually. <p> Often exploration Sebastian Thrun Exploration in Active Learning 5 and exploitation are traded off dynamically so that exploration fades in time. 2.5 Complexity Results In addition to empirical studies, theoretical results emphasize the importance of exploration in active learning. Based on a result in <ref> ( Whitehead, 1991 ) </ref> , which shows that random walk exploration can require exponential learning time in various cases, it has been shown that directed exploration techniques can reduce the complexity of active learning from exponential training time (random exploration) to polynomial training time ( Thrun, 1992a ) , ( Koenig
References-found: 16

