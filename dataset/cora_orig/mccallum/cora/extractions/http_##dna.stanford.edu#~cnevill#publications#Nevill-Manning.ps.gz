URL: http://dna.stanford.edu/~cnevill/publications/Nevill-Manning.ps.gz
Refering-URL: http://dna.stanford.edu/~cnevill/resume.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Inferring Sequential Structure  
Author: Craig G. Nevill-Manning 
Degree: in partial fulfilment of the requirements for the degree of Doctor of Philosophy in  
Affiliation: Computer Science at the University of Waikato.  
Date: submitted  May 1996  
Note: This thesis is  1996 Craig G. Nevill-Manning  
Abstract-found: 0
Intro-found: 1
Reference: <author> Abelson, H., and deSessa, A.A. </author> <title> (1982) Turtle geometry . Cambridge, </title> <address> Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: They have also been used to describe plants graphically, where sentences produced by L-systems are interpreted as instructions to a LOGO turtle, as described by Prusinkiewicz (1986). The LOGO language, devised by Seymour Papert <ref> (Abelson and deSessa, 1982) </ref>, defines a set of instructions that direct a software turtle to draw graphical figures.
Reference: <author> Andreae, J.H. </author> <title> (1977) Thinking with the teachable machine. </title> <publisher> London: Academic Press. </publisher>
Reference: <author> Angluin, D. </author> <title> (1982) Inference of reversible languages, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 29, </volume> <pages> 741-765. </pages>
Reference: <author> Bell, T.C., Cleary, J.G., and Witten, I.H. </author> <title> (1990) Text compression. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Berwick, R.C., and Pilato, S. </author> <title> (1987) Learning syntax by automata induction, </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 9-38. </pages>
Reference-contexts: the k-reversible inference algorithm is O (kn 3 ), where n is defined in the same way. 2.2 GRAMMATICAL INFERENCE 31 3 Judy does|did gives|gave give get has|had Modals (may, might, etc.) have been be giving|given given give being given bread 8 Figure2.6 One-reversible automaton for the English auxiliary system <ref> (after Berwick and Pilato, 1987) </ref> 2.2.2 Applications K-reversible grammars form an important class because there exists a polynomial-time inference procedure for them that guarantees identification in the limit. The complexity of the inference algorithm means that inference can be performed on some real-world problems.
Reference: <author> Biermann, A.W., and Feldman, J.A. </author> <title> (1972) A survey of results in grammatical inference, in Frontiers of Pattern Recognition , edited by S. </title> <editor> Watanabe, </editor> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Brown, R.W. </author> <title> (1973) A first language: the early stages . Cambridge, </title> <address> Massachusetts: </address> <publisher> Harvard University Press. </publisher>
Reference: <author> Chomsky, N. </author> <title> (1957b) Syntactic structures. </title> <publisher> Gravenhage: Mouton. </publisher>
Reference: <author> Chomsky, N., and Miller, </author> <title> G.A. </title> <booktitle> (1957a) Pattern conception, </booktitle> <address> AFCRC-TN-57-57. </address>
Reference: <author> Cleary, J.G. </author> <title> (1980) An associative and impressible computer, </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical and Electronic Engineering, University of Canterbury, Christchurch. </institution>
Reference-contexts: B C D A D A B C most recent event tail head memory sequence of events seen by PUSS successive traces stored in memory Learning Prediction A B C tail D predicted head Figure2.2 The finite context prediction mechanism for PUSS and FLM <ref> (after Cleary, 1980) </ref> techniques for finding an automaton, and provides extensions for inferring push and pop operations. Constructive techniques for finite context predictors Finite context modellers are based on the observation that a symbol in a sequence is often determined by the symbols that immediately precede it.
Reference: <author> Cleary, J.G., and Witten, I.H. </author> <title> (1984) Data compression using adaptive coding and partial string matching, </title> <journal> IEEE Transactions on Communications, </journal> <volume> COM-32(4), </volume> <pages> 396-402. </pages>
Reference-contexts: Of course, S EQUITUR is not alone in relating learning to compression. Clearys (1980) FLM system was transformed, with the help of arithmetic coding, from a predictive learning system into a practical compression scheme <ref> (Cleary and Witten, 1984) </ref>. It immediately set a prodigious new record for data compression, and today, variations on the algorithm remain at the leading edge of compression technology (Teahan and Cleary, 1996). 2.4 DATA COMPRESSION 41 a Humpty Dumpty sat on a wall. <p> A more recent implementation of LZ77, gzip, outperforms compress, resulting in a compressed size of 312275 bytes, 41% of the original. The best general purpose compression scheme, PPM <ref> (Cleary and Witten, 1984) </ref>, described in Section 2.4, compressed the novel to 242558 bytes, 32% of the original. 6.1.3 Sending rules implicitly SEQUITUR should be able to rival the best dictionary schemes, because forming rules is similar to forming a dictionary.
Reference: <author> Cohen, A., Ivry, R.I., and Keele, </author> <title> S.W. (1990) Attention and structure in sequence learning, </title> <journal> Journal of Experimental Psychology, </journal> <volume> 16(1), </volume> <pages> 17-30. </pages>
Reference: <author> Craven, </author> <title> T.C. (1993) A computer-aided abstracting tool kit, </title> <journal> Canadian Journal of Information and Library Science, </journal> <volume> 18(2), </volume> <pages> 19-31. </pages>
Reference: <author> Darragh, J.J., and Witten, I.H. </author> <title> (1992) The reactive keyboard . Cambridge, </title> <address> England: </address> <publisher> Cambridge University Press. </publisher>
Reference: <author> Dietterich, T.G., and Michalski, </author> <title> R.S. (1986) Learning to predict sequences, in Machine learning: an artificial intelligence approach II, edited by R.S. </title> <editor> Michalski, J.G. Carbonell and T.M. Mitchell, </editor> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 63-106. </pages>
Reference: <author> Francis, W.N., and Kucera, H. </author> <title> (1979) Manual of Information to Accompany a Standard Corpus of Present-Day Edited American English, for Use with Digital Computers, </title> <address> Providence, Rhode Island: </address> <institution> Department of Linguistics, Brown University. </institution> <note> 192 REFERENCES Frijters, </note> <editor> D., and Lindenmayer, A. </editor> <title> (1974) A model for the growth and flowering of Ater novae-angliae on the basis of table (1, 0) L-systems, </title> <editor> in L-systems, edited by G. Rozenberg and A. Salomaa, </editor> <publisher> Berlin: Springer-Verlag, </publisher> <pages> 24-52. </pages>
Reference-contexts: However, many of the correct parses were simply determiner-noun, and adjective-noun combinations. A similar experiment was performed using S EQUITUR on a large corpus, the London-Oslo/Bergen (LOB) corpus (Johansson et al. , 1978). The LOB corpus is a British English counterpart of the Brown Corpus <ref> (Francis and Kucera, 1979) </ref> and contains 500 text samples selected from texts printed in Great Britain in 1961. Each word in the corpus has been tagged with its part of speech. The tagging was performed using a combination of automatic techniques and manual post-processing.
Reference: <author> Gaines, B.R. </author> <title> (1976) Behaviour/structure transformations under uncertainty, </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 8, </volume> <month> 337-365. </month> <title> GEDCOM Standard: Draft release 5.4, </title> <address> Salt Lake City, Utah: </address> <institution> Family History Department, The Church of Jesus Christ of Latter-day Saints. </institution>
Reference-contexts: t t h h h h h h h t t t t t t h h h h h h 2 state 4 state h 1 state h t h t 3 state c states p o e f f i t Figure2.1 Structure detection by enumeration of automata <ref> (after Gaines, 1976) </ref> (a) results of a rigged coin toss (b) admissible 1, 2, 3 and 4 state models (c) tradeoff between automaton size and fit cases, enumerating all models guarantees to choose the one that maximises the preference criterion. <p> a [j]) if (j &lt; i) t = a [j - 1]; a [j] = t; c j = 1 if (j &lt; i) if (a [j - 1] &gt; a [j]) t = a [j - 1] a [j] = t Figure2.3 Inferring a program from an execution trace <ref> (after Gaines, 1976) </ref> (a) bubblesort program (b) part of the trace produced by (a) (c) finite state automaton produced from the bubblesort trace Constructive techniques for automata A particularly straightforward method of constructing an automaton from a sequence is to create a state for each unique symbol in the sequence, and <p> &lt; i) if (a [j - 1] &gt; a [j]) t = a [j - 1] a [j] = t i = 10 if (i &gt;= 0) if (j &lt; i) f t f a [j - 1] = a [j] t Figure5.6 Inferring a program from an execution trace <ref> (after Gaines, 1976) </ref> (a) bubblesort program (b) part of the trace produced by (a) (c) finite state automaton produced from the bubblesort trace (d) labels added to the conditional branches in (c) attempting to infer an automaton.
Reference: <author> Gold, M. </author> <title> (1967) Language identification in the limit, </title> <journal> Information and Control, </journal> <volume> 10, </volume> <pages> 447-474. </pages>
Reference-contexts: This revised view fits more naturally into a biological perspective and allows us to sidestep certain logical problems which arise from the first view <ref> (Gold 1967...) </ref>. [see Section 2.2] ... If there is no target grammar but merely an evolutionary process of improving the efficiency of a cognitive system then this proof no longer presents a problem. <p> It is known from data compression that the problem of finding the smallest hierarchy for a sequence is NP-complete (Storer, 1982). It is also known from grammatical inference that no algorithm can guarantee to infer a grammar from the sequences that it produces <ref> (Gold, 1967) </ref>. Furthermore, in many 8.2 THE APPLICATIONS 185 domains, the sequence does not necessarily emanate from a member of a known class of grammars.
Reference: <author> Gottlieb, D., Hagerth, S.A., Lehot, P.G.H., and Rabinowitz, H.S. </author> <title> (1975) A classification of compression methods and their usefulness for a large data processing center, </title> <booktitle> Proc. National Computer Conference, </booktitle> <pages> 453-458. </pages>
Reference-contexts: As described in Section 5.4, the words were presented to S EQUITUR as if they were symbols drawn from a large alphabet. The encoding scheme was that described in Section 6.1. The dictionary was compressed in two stages: front coding followed by compression by PPM. Front coding <ref> (Gottlieb et al. , 1975) </ref> involves sorting the dictionary, and whenever an entry shares a prefix with the preceding entry, replacing the prefix by its length.
Reference: <author> Guazzo, M. </author> <title> (1980) A general minimum-redundancy source-coding algorithm, </title> <journal> IEEE Trans. Information Theory, </journal> <volume> IT-26(1), </volume> <pages> 15-25. </pages>
Reference: <author> Harman, D.K.E. </author> <booktitle> (1992) "Proc. TREC Text Retrieval Conference," </booktitle> <address> Gaithersburg, </address> <institution> MD: National Institute of Standards Special Publication, </institution> <month> 500-207. </month>
Reference-contexts: Retrieval of matching documents involves both precision (not returning irrelevant documents) and recall (not overlooking any relevant documents) (Salton, 1989). There has been much research on how to maximise precision and recall given a particular query <ref> (Harman, 1992) </ref>, but here we suggest a technique for aiding a user to formulate a query that is easier for the retrieval mechanism to fulfil. The technique allows a user to become familiar with the content of the database, and to include phrases that make the query much more specific.
Reference: <author> Hayes, J.R., and Clark, H.H. </author> <title> (1970) Experiments on the segmentation of an artificial speech analogue, in Cognition and the Development of Language , edited by J.R. Hayes, </title> <address> New York: </address> <publisher> John Wiley & Sons, Inc., </publisher> <pages> 221-234. </pages>
Reference: <author> Hogeweg, P., and Hesper, B. </author> <title> (1974) A model study on biomorphological description, </title> <journal> pattern Recognition, </journal> <volume> 6, </volume> <pages> 165-179. </pages>
Reference: <author> Johansson, S., Leech, G., and Goodluck, H. </author> <title> (1978) Manual of Information to Accompany the Lancaster-Oslo/Bergen Corpus of British English, for Use with Digital Computers, </title> <institution> Oslo: Department of English, University of Oslo. </institution>
Reference-contexts: He found that the automatically-generated hierarchy was better than random, and that about half the time it parsed the sequence correctly. However, many of the correct parses were simply determiner-noun, and adjective-noun combinations. A similar experiment was performed using S EQUITUR on a large corpus, the London-Oslo/Bergen (LOB) corpus <ref> (Johansson et al. , 1978) </ref>. The LOB corpus is a British English counterpart of the Brown Corpus (Francis and Kucera, 1979) and contains 500 text samples selected from texts printed in Great Britain in 1961. Each word in the corpus has been tagged with its part of speech.
Reference: <editor> Kevles, D.J., and Hood, L. (Eds.) </editor> <title> (1992) The code of codes: Scientific and social issues in the human genome project. </title> <address> Cambridge, Massachusetts: </address> <publisher> Harvard University Press. </publisher>
Reference: <author> Knuth, D.E. </author> <booktitle> (1968) The art of computer programming 1: fundamental algorithms . Addison-Wesley. </booktitle>
Reference: <author> Laird, P., and Saul, R. </author> <title> (1994) Discrete sequence prediction and its applications, </title> <journal> Machine Learning, </journal> <volume> 15, </volume> <pages> 43-68. </pages>
Reference: <author> Langley, P. </author> <booktitle> (1996) Elements of Machine Learning. </booktitle> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lashley, </author> <title> K.S. (1951) The problem of serial order in behavior, in Cerebral mechanisms in behavior, edited by L.A. </title> <address> Jeffress, New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Li, M., and Vitanyi, P. </author> <title> (1993) An introduction to Kolmogorov complexity and its applications. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Butterick 3722 Sizes 8-10-12 Jumper b TopSkirt Top Skirt Jumper Dress Jumper Dress DressDress Dress Jumper 11/12 10108-10-1212 1012 12 11/1212 11/12 Size SuzeSize SizeSize SizeSize SizeSizeSizeSizes 3035 3611367437224198 4352 6171 4864 5377 59065057 5424 5465 SimplicityMcCallsButterick :NULL start terminal c Figure2.7 Inferring a customised data entry form from input <ref> (after Schlimmer and Hermens, 1993) </ref> (a) the input data (b) a reversible automaton inferred from (a) (c) a customised form based on (b) specification of a member of a class such as sewing patterns or computer models. <p> It is now possible to substantiate this claim. The information content of a sequence can be measured by its Kolmogorov complexity <ref> (Li and Vitanyi, 1993) </ref>, which is the size of the smallest universal Turing machine capable of reproducing the sequence. Since this quantity is not computable, it is necessary to devise other metrics that approximate it.
Reference: <author> Lindenmayer, A. </author> <title> (1968) Mathematical models for cellular interaction in development, Parts I and II, </title> <journal> Journal of Theoretical Biology, </journal> <volume> 18, </volume> <pages> 280-315. </pages>
Reference-contexts: They were devised by a biologist, Aristid Lindenmayer, after whom the grammars were named <ref> (Lindenmayer, 1968) </ref>. They produce sentences that can be interpreted graphically to produce images of fractals or organisms. Prusinkiewicz and Hanan (1989) describe how L-systems are interpreted, and distinguish them from Chomsky grammars: The essential difference between Chomsky grammars and L-systems lies in the method of applying productions.
Reference: <editor> Mainous, F.D., and Ottman, R.W. </editor> <address> (1966) The 371 chorales of Johann Sebastian Bach . New York: </address> <publisher> Holt, Rinehart and Winston, Inc. </publisher> <address> REFERENCES 193 Mandelbrot, </address> <month> B.B. </month> <title> (1982) The fractal geometry of nature . San Francisco: W.H. </title> <publisher> Freeman. </publisher>
Reference-contexts: This section investigates whether these repetitions, and hierarchies of repetitions, can be detected in music. A prerequisite for such an investigation is a suitably large computer-readable corpus of music. Fortunately, several such corpora exist, and one consisting of transcriptions of Bachs chorales <ref> (Mainous and Ottman, 1966) </ref> was selected for the experiment. Bachs prolific output of 371 chorales, of which the corpus contains 97, makes it an excellent example of a homogeneous single-genre 170 CHAPTER 7: APPLICATIONS collection.
Reference: <author> Maulsby, D.L., Witten, I.H., and Kittlitz, K.A. </author> <year> (1989) </year> <month> Metamouse: </month> <title> specifying graphical procedures by example, </title> <journal> Computer Graphics, </journal> <volume> 23(3), </volume> <pages> 127-136. </pages>
Reference: <author> Miller, </author> <title> V.S., and Wegman, M.N. (1984) Variations on a theme by Ziv and Lempel, in Combinatorial algorithms on words, edited by A. </title> <editor> Apostolico and Z. Galil, </editor> <publisher> Berlin: Springer-Verlag, </publisher> <pages> 131140. </pages>
Reference-contexts: LZ78 and its variants are also CPM schemes, while LZ77 is an OPM scheme. LZ78 builds its phrases from one dictionary entry and one terminal symbol, so its hierarchy is of a restricted form. A variant, LZMW <ref> (Miller and Wegman, 1984) </ref>, forms phrases by concatenating the previous two phrases encoded, which allows phrases to grow more quickly.
Reference: <author> Moffat, A. </author> <title> (1987) "Word based text compression," </title> <type> Parkville, </type> <institution> Victoria, Australia: Department of Computer Science, University of Melbourne. </institution>
Reference-contexts: W ORD uses a Markov model that predicts words based on the previous word and nonwords based on the previous nonword, resorting to character-level coding whenever a new word or nonword is encountered <ref> (Moffat, 1987) </ref>. We used both a zero-order context (WORD-0) and a first-order one ( WORD-1). MG is a designed for full-text retrieval and uses a semi-static zero-order word-based model, along with a separate dictionary (Witten et al., 1994).
Reference: <author> Moffat, A., Neal, R., and Witten, I.H. </author> <title> (1995) Arithmetic coding revisited, </title> <booktitle> Proc. Data Compression Conference, Snowbird, Utah, </booktitle> <pages> 202-211. </pages>
Reference: <author> Nevill-Manning, </author> <title> C.G. (1993) Programming by demonstration, </title> <journal> New Zealand Journal of Computing, </journal> <volume> 4(2), </volume> <pages> 15-24. </pages>
Reference: <author> Nevill-Manning, C.G., Witten, I.H., and Maulsby, </author> <title> D.L. (1994a) Modelling sequences using grammars and automata, </title> <booktitle> Proc. Canadian Machine Learning Workshop, </booktitle> <address> Banff, Canada, xv-1518. </address>
Reference: <author> Nevill-Manning, C.G., Witten, I.H., and Maulsby, </author> <title> D.L. (1994b) Compression by induction of hierarchical grammars, </title> <booktitle> Proc. Data Compression Conference , Snowbird, Utah, </booktitle> <pages> 244-253. </pages>
Reference: <author> Nevill-Manning, </author> <title> C.G. (1995) Learning from experience, </title> <booktitle> Proc. New Zealand Computer Science Research Students Conference, </booktitle> <address> Hamilton, New Zealand. </address>
Reference: <author> Nevill-Manning, C.G., and Witten, I.H. </author> <title> (1995) Detecting sequential structure, </title> <booktitle> Proc. Workshop on Programming by Demonstration, </booktitle> <address> ML95, Tahoe City, CA. </address>
Reference: <author> Nevill-Manning, C.G., Witten, I.H., and Olsen, </author> <title> D.R. (1996) Compressing semi-structured text using hierarchical phrase identification, </title> <booktitle> Proc. Data Compression Conference, </booktitle> <address> Snowbird, Utah, </address> <month> 5372. </month>
Reference: <author> Nissen, M.F., and Bullemer, P. </author> <title> (1987) Attentional requirements of learning: evidence from performance measures, </title> <journal> Cognitive Psychology, </journal> <volume> 19, </volume> <pages> 1-32. </pages>
Reference: <author> Olivier, </author> <title> D.C. (1968) Stochastic grammars and language acquisition devices, </title> <type> Ph.D. thesis, </type> <institution> Harvard University, </institution> <note> Pasco, </note> <author> R. </author> <title> (1976) Source coding algorithms for fast data compression, </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <address> Palo Alto, CA. </address>
Reference: <editor> Piatetsky-Shapiro, G., and Frawley, W.J.E. </editor> <booktitle> (1991) Knowledge discovery in databases . Cambridge, </booktitle> <address> Mass.: </address> <publisher> MIT Press. </publisher>
Reference: <author> Prusinkiewicz, P. </author> <title> (1986) Graphical Applications of L-systems to computer imagery, </title> <booktitle> Proc. Graphics Interface 86Vision Interface 86, </booktitle> <pages> 247-253. </pages>
Reference: <author> Prusinkiewicz, P., and Hanan, J. </author> <title> (1989) Lindenmayer systems, </title> <booktitle> fractals, </booktitle> <address> and plants . New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Prusinkiewicz, P., and Lindenmayer, A. </author> <title> (1989) The algorithmic beauty of plants . New York: </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Quinlan, J.R. </author> <title> (1986) Induction of decision trees, </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages> <note> 194 REFERENCES Quinlan, </note> <author> J.R., and Rivest, </author> <title> R.L. (1989) Inferring decision trees using the minimum description length principle, </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <pages> 227-248. </pages>
Reference: <author> Restle, F. </author> <title> (1970) Theory of serial pattern learning: structural trees, </title> <journal> Psychological Review, </journal> <volume> 77(6), </volume> <pages> 481-495. </pages>
Reference: <author> Restle, F., and Brown, </author> <title> E.R. (1970) Serial pattern learning, </title> <journal> Journal of Experimental Psychology, </journal> <volume> 83(1), </volume> <pages> 120-125. </pages>
Reference: <author> Rissanen, J. </author> <title> (1978) Modeling by shortest data description, </title> <journal> Automatica, </journal> <pages> 14 , 465-471. </pages>
Reference: <author> Rissanen, J.J. </author> <title> (1976) Generalized Kraft inequality and arithmetic coding, </title> <journal> IBM Journal of Research and Development, </journal> <volume> 20, </volume> <pages> 198-203. </pages>
Reference: <author> Rissanen, J.J., and Langdon, G.G. </author> <title> (1979) Arithmetic coding, </title> <journal> IBM Journal of Research and Development, </journal> <volume> 23(2), </volume> <pages> 149-162. </pages>
Reference: <author> Rosenbaum, D.A., Kenny, S.B., and Derr, M.A. </author> <title> (1983) Hierarchical control of rapid movement sequences, </title> <journal> Journal of experimental psychology: Human perception and performance, </journal> <volume> 9(1), </volume> <pages> 86-102. </pages>
Reference: <author> Rubin, F. </author> <title> (1979) Arithmetic stream coding using fixed precision registers, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-25(6), </volume> <pages> 672-675. </pages>
Reference: <author> Salton, G. </author> <title> (1989) Automatic Text Processing: the transformation, analysis and retrieval of information by computer. </title> <address> Reading, Mass.: </address> <publisher> Addison Wesley. </publisher>
Reference-contexts: Retrieval of matching documents involves both precision (not returning irrelevant documents) and recall (not overlooking any relevant documents) <ref> (Salton, 1989) </ref>. There has been much research on how to maximise precision and recall given a particular query (Harman, 1992), but here we suggest a technique for aiding a user to formulate a query that is easier for the retrieval mechanism to fulfil.
Reference: <author> Sapir, E. </author> <booktitle> (1921) Language. </booktitle> <address> New York: </address> <publisher> Harcourt, Brace & World. </publisher>
Reference: <author> Schlimmer, J.C., and Hermens, L.A. </author> <title> (1993) Software agents: completing patterns and constructing user interfaces, </title> <journal> Journal of Artificial Intelligence Research, </journal> <pages> 1 , 61-89. </pages>
Reference-contexts: Butterick 3722 Sizes 8-10-12 Jumper b TopSkirt Top Skirt Jumper Dress Jumper Dress DressDress Dress Jumper 11/12 10108-10-1212 1012 12 11/1212 11/12 Size SuzeSize SizeSize SizeSize SizeSizeSizeSizes 3035 3611367437224198 4352 6171 4864 5377 59065057 5424 5465 SimplicityMcCallsButterick :NULL start terminal c Figure2.7 Inferring a customised data entry form from input <ref> (after Schlimmer and Hermens, 1993) </ref> (a) the input data (b) a reversible automaton inferred from (a) (c) a customised form based on (b) specification of a member of a class such as sewing patterns or computer models.
Reference: <author> Shannon, C.E. </author> <title> (1948) A mathematical theory of communication, </title> <journal> Bell System Technical Journal, </journal> <volume> 27, </volume> <pages> 398-403. </pages>
Reference: <author> Smith, </author> <title> A.R. (1978) About the cover: reconfigurable machines, </title> <journal> Computer, </journal> <volume> 11(7), </volume> <pages> 3-4. </pages>
Reference: <author> Smith, </author> <title> A.R. (1984) Plants, fractals, </title> <journal> and formal languages, Computer Graphics, </journal> <volume> 18(3), </volume> <pages> 1-10. </pages>
Reference: <author> Solomonoff, R. </author> <title> (1959) A new method for discovering the grammars of phrase structure languages, </title> <booktitle> Information Processing, </booktitle> <pages> 258-290. </pages>
Reference: <author> Storer, J.A. </author> <title> (1977) NP-completeness results concerning data compression, </title> <type> 234. </type>
Reference: <author> Storer, J.A. </author> <title> (1982) Data compression via textual substitution, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 29(4), </volume> <pages> 928-951. </pages>
Reference-contexts: It is known from data compression that the problem of finding the smallest hierarchy for a sequence is NP-complete <ref> (Storer, 1982) </ref>. It is also known from grammatical inference that no algorithm can guarantee to infer a grammar from the sequences that it produces (Gold, 1967). Furthermore, in many 8.2 THE APPLICATIONS 185 domains, the sequence does not necessarily emanate from a member of a known class of grammars.
Reference: <author> Szilard, A.L., and Quinton, R.E. </author> <title> (1979) An interpretation for D0L systems by computer graphics, </title> <journal> The Science Terrapin, </journal> <volume> 4, </volume> <pages> 8-13. </pages>
Reference: <author> Tarjan, R.E. </author> <title> (1975) Efficiency of a good but not linear set union algorithm, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 22(2), </volume> <pages> 215-225. </pages>
Reference-contexts: The algorithm for forming a k-reversible automaton is identical, except for replacing the determinism constraint with determinism with lookahead k. The time complexity of the zero-reversible inference algorithm is O (na (n)), where n is the sum of the lengths of the sentences. and a (n) grows very slowly <ref> (Tarjan, 1975) </ref>.
Reference: <author> Teahan, W.J., and Cleary, J.G. </author> <title> (1996) The entropy of English using PPM-based models, </title> <booktitle> Proc. Data Compression Conference, </booktitle> <address> Snowbird, Utah, </address> <note> 5362. REFERENCES 195 Thomas, S.W., </note> <author> McKie, J., Davies, S., Turkowski, K., Woods, J.A., and Orost, J.W. </author> <note> (1985) Compress (version 4.0) program and documentation, available from joe@petsd.uucp. </note> <author> von Koch, H. </author> <title> (1905) Une mthode gomtrique lmentaire pour ltude de certaines questions de la thorie des courbes planes, </title> <journal> Acta Mathematica, </journal> <pages> 30 , 145-174. </pages>
Reference-contexts: It immediately set a prodigious new record for data compression, and today, variations on the algorithm remain at the leading edge of compression technology <ref> (Teahan and Cleary, 1996) </ref>. 2.4 DATA COMPRESSION 41 a Humpty Dumpty sat on a wall.
Reference: <author> Wallace, C.S., and Freeman, P.R. </author> <title> (1987) Estimation and inference by compact coding, </title> <journal> Journal of the Royal Statistical Society (B), </journal> <volume> 49, </volume> <pages> 240-265. </pages>
Reference: <author> Wallace, C.S., and Patrick, J.D. </author> <title> (1993) Coding decision trees, </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 7-22. </pages>
Reference: <author> Williams, H., and Zobel, J. </author> <title> (1996) Practical compression of nucleotide databases, </title> <booktitle> Proc. Proceedings of the Australian Computer Science Conference , Melbourne, </booktitle> <address> Australia. </address>
Reference: <author> Witten, I.H. </author> <title> (1979) Approximate, nondeterministic modelling of behaviour sequences, </title> <journal> International Journal of General Systems, </journal> <volume> 5, </volume> <pages> 1-12. </pages>
Reference: <author> Witten, I.H. </author> <title> (1981a) Programming by example for the casual user: a case study, </title> <booktitle> Proc. Canadian Man-Computer Communication Conference , Waterloo, Ontario, </booktitle> <pages> 105-113. </pages>
Reference: <author> Witten, I.H. </author> <title> (1981b) Some recent results on nondeterministic modelling of behaviour sequences, </title> <booktitle> Proc. Proc. Society for General Systems Research , Toronto, Ontario, </booktitle> <pages> 265-274. </pages>
Reference: <author> Witten, </author> <title> I.H. </title> <booktitle> (1982) Principles of computer speech. </booktitle> <address> London, England: </address> <publisher> Academic Press. </publisher>
Reference: <author> Witten, I.H., and Maulsby, </author> <title> D.L. (1991) Evaluating programs formed by example: an informational heuristic, in New results and new trends in computer science , edited by H. Maurer, </title> <publisher> Berlin: Springer-Verlag, </publisher> <pages> 388402. </pages>
Reference: <author> Witten, I.H., and Mo, D. </author> <year> (1993) </year> <month> TELS: </month> <title> Learning text editing tasks from examples, in Watch what I do: programming by demonstration , edited by A. Cypher, </title> <address> Cambridge, Massachusetts: </address> <publisher> MIT Press, </publisher> <pages> 182-203. </pages>
Reference: <author> Witten, I.H., Moffat, A., and Bell, </author> <title> T.C. (1994) Managing Gigabytes: compressing and indexing documents and images. </title> <address> New York: </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference-contexts: We used both a zero-order context (WORD-0) and a first-order one ( WORD-1). MG is a designed for full-text retrieval and uses a semi-static zero-order word-based model, along with a separate dictionary <ref> (Witten et al., 1994) </ref>. In this scheme, as in WORD-0, the code for a word is determined solely by its frequency, and does not depend on any preceding words. This proves rather ineffective on the genealogical database, indicating the importance of inter-word relationships. <p> It also identifies the cadences by comparison between chorales. 7.4 Phrase identification It has been observed that in many large full-text retrieval systems, more information is stored than is ever retrieved <ref> (Witten et al., 1994) </ref>. This is a necessary consequence of the information explosion, where, as mentioned in Chapter 1, the amount of information in the world is claimed to double every twenty months. This asymmetry of storage and retrieval has implications for the kind of technology used for each process. <p> Whereas the efficiency of indexing and storage are straightforward to evaluate, and practical techniques have been developed <ref> (Witten et al ., 1994) </ref>, the task of formulating appropriate queries and retrieving only relevant documents remains problematic. Retrieval of matching documents involves both precision (not returning irrelevant documents) and recall (not overlooking any relevant documents) (Salton, 1989).
Reference: <author> Witten, I.H., Neal, R., and Cleary, J.G. </author> <title> (1987) Arithmetic coding for data compression, </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 30(6), </volume> <pages> 520-540. </pages>
Reference: <author> Witten, I.H., Nevill-Manning, C.G., and Cunningham, S.J. </author> <title> (1996) Building a digital library for computer science research: technical issues, </title> <booktitle> Proc. Australasian Computer Science Conference, </booktitle> <address> Melbourne, Australia, </address> <pages> 534-542. </pages>
Reference-contexts: SEQUITUR was invoked on a large body of computer science technical reports, part of the 700 Mb corpus contained in the New Zealand Digital Library <ref> (Witten et al., 1996) </ref>. The reports were presented as a sequence of words, and all words were case folded before processing. A 22 Mb sample was chosen, which included 350 technical reports from six sites. The sample represented a sequence of 3.5 million words with a vocabulary size of 30,000.
Reference: <author> Wolff, J.G. </author> <title> (1975) An algorithm for the segmentation of an artificial language analogue, </title> <journal> British Journal of Psychology, </journal> <volume> 66(1), </volume> <pages> 79-90. </pages>
Reference: <author> Wolff, J.G. </author> <title> (1977) The discovery of segments in natural language, </title> <journal> British Journal of Psychology, </journal> <volume> 68, </volume> <pages> 97-106. </pages>
Reference: <author> Wolff, J.G. </author> <title> (1980) Language acquisition and the discovery of phrase structure, Language and Speech, </title> <type> 23(3), 255-269. 196 REFERENCES Wolff, </type> <month> J.G. </month> <title> (1982) Language acquisition, data compression and generalization, </title> <journal> Language and Communication, </journal> <volume> 2(1), </volume> <pages> 57-89. </pages>
Reference: <author> Ziv, J., and Lempel, A. </author> <title> (1977) A universal algorithm for sequential data compression, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-23(3), </volume> <pages> 337-343. </pages>
Reference: <author> Ziv, J., and Lempel, A. </author> <title> (1978) Compression of individual sequences via variable-rate coding, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-24(5), </volume> <pages> 530-536. </pages>
Reference-contexts: While this is a distinct improvement over the textual version, it nevertheless falls short of most data compression schemes. Table 6.1 summarises the performance of some notable techniques. U NIX compress (Thomas, et al., 1985), which is based on LZ78 <ref> (Ziv and Lempel, 1978) </ref>, popularised the use of Ziv-Lempel coding. It has remained unchanged for over a decade, and performs poorly relative to more recent schemes. It slightly outperforms the simple grammar encoding, S EQUITUR-0, reducing b o o k 1 to 332,056 bytes, 43% of the original.
Reference: <author> Zobel, J., Moffat, A., Wilkinson, R., and Sacks-Davis, R. </author> <title> (1995) Efficient retrieval of partial documents, </title> <booktitle> Information Processing and Management, </booktitle> <volume> 31 (3), </volume> <pages> 361-377. </pages>
Reference-contexts: Figure3.12b shows the linear growth of the total number of symbols in the grammar. The growth of the number of unique words in the text, shown in Figure3.12c, is high at the start and slows toward the end. It has been observed in much larger samples <ref> (Zobel, et al., 1995) </ref> that new words continue to appear at a fairly constant rate.
References-found: 86

