URL: http://www.eecs.umich.edu/UMichMP/Publications/dundas-ics.ps
Refering-URL: http://www.eecs.umich.edu/UMichMP/abstracts.html
Root-URL: http://www.eecs.umich.edu
Email: -dundas, tnm-@eecs.umich.edu  
Title: Improving Data Cache Performance by Pre-executing Instructions Under a Cache Miss  
Author: James Dundas and Trevor Mudge 
Address: Ann Arbor, Michigan 48109-2122  
Affiliation: Department of Electrical Engineering and Computer Science The University of Michigan  
Abstract: In this paper we propose and e valuate a technique that improves first level data cache performance by pre-executing future instructions under a data cache miss. W e sho w that these pre-executed instructions can generate highly accurate data prefetches, particularly when the first le vel cache is small. The technique is referred to as runahead processing. The hardw are required to implement runahead is modest, because, when a miss occurs, it makes use of an otherwise idle resource, the e xecution logic. The principal hardw are cost is an e xtra re gister file. T o measure the impact of runahead, we simulated a processor executing five integer Spec95 benchmarks. Our results sho w that runahead w as able to significantly reduce data cache CPI for four of the five benchmarks. We also compared runahead to a simple form of prefetching, sequential prefetching, which w ould seem to be suitable for scientific benchmarks. W e confirm this by enlar ging the scope of our e xperiments to include a scientific benchmark. Ho wever, we show that runahead w as also able to outperform sequential prefetching on the scientific benchmark. W e also conduct studies that demonstrate that runahead can generate many useful prefetches for lines that show little spatial locality with the misses that initiate runahead episodes. Finally, we discuss some further enhancements of our baseline runahead prefetching scheme. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Callahan, K. Kennedy, and A. Porterfield, </author> <booktitle> Software Prefetch-ing, In the Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference: [2] <author> T.C. Mowry, M.S. Lam, and A. Gupta, </author> <title> Design and evaluation of a compiler algorithm for prefetching, </title> <booktitle> In the Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: This resulted in multiprocessor performance for scientific benchmarks comparable in some cases to software prefetching, with an instruction stride table as small as 4 entries. The IST concept w as subsequently combined with the prefetch predicates of <ref> [2] </ref> in [9]. Another hardw are prefetching scheme that avoids the need for significant amounts of hardware is the wrong path prefetching described in [10]. This actually prefetches instructions from the not-taken path, in the expectation that they will be executed during a later iteration.
Reference: [3] <author> A.C. Klaiber and H.M. Levy, </author> <title> An Architecture for Software-Controlled Data Prefetching, </title> <booktitle> In the Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <year> 1991. </year>
Reference: [4] <author> A.J. Smith, </author> <title> Cache Memories, </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 18, num. 3, </volume> <month> September </month> <year> 1982. </year>
Reference-contexts: Simulations of the prefetch-on-miss and always-prefetch sequential prefetching techniques described in <ref> [4] </ref> were performed. Prefetch-on-miss generates a prefetch for line i+1 whenever an access of line i misses in the L1 data cache. Always-prefetch generates a prefetch for line i+1 whenever line i is accessed.
Reference: [5] <author> N.P. Jouppi, </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers, </title> <booktitle> In the Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference: [6] <author> J.L. Baer and T.F. Chen, </author> <title> An Effective On-Chip Preloading Scheme To Reduce Data Access Penalty, </title> <booktitle> In the Proceedings of Supercomputing, </booktitle> <year> 1991. </year>
Reference-contexts: The DMAQ thus provides the functionality of a store queue, outstanding request list <ref> [6] </ref>, and miss status holding registers [14]. It is further assumed that the DMAQ cannot coalesce store-throughs, or allow demand fetches or store-throughs to either pass or squash outstanding prefetches.
Reference: [7] <author> J.W.C. Fu and J.H.Patel, </author> <title> Stride directed prefetching in scalar processors, </title> <booktitle> In the Proceedings of the 25th International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1992. </year>
Reference: [8] <author> R. Bianchini and T.J. LeBlanc, </author> <title> A Preliminary Evaluation of Cache-Miss-Initiated Prefetching Techniques in Scalable Multiprocessors, </title> <institution> University of Rochester Computer Science Department Technical Report 515, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: The stride and reference prediction tables in [6][7], are a case in point. The hardware requirements can be reduced if the table entries are allocated by the softw are. This hybrid hardwaresoftware technique was presented in <ref> [8] </ref>. Their instruction stride table (IST) selectively generates cache miss initiated prefetches for accesses chosen beforehand by the compiler. This resulted in multiprocessor performance for scientific benchmarks comparable in some cases to software prefetching, with an instruction stride table as small as 4 entries.
Reference: [9] <author> T.F. Chen, </author> <title> An Effective Programmable Prefetch Engine for On-Chip Caches, </title> <booktitle> In the Proceedings of the 28th International Symposium on Microarchitecture, </booktitle> <year> 1995. </year>
Reference-contexts: This resulted in multiprocessor performance for scientific benchmarks comparable in some cases to software prefetching, with an instruction stride table as small as 4 entries. The IST concept w as subsequently combined with the prefetch predicates of [2] in <ref> [9] </ref>. Another hardw are prefetching scheme that avoids the need for significant amounts of hardware is the wrong path prefetching described in [10]. This actually prefetches instructions from the not-taken path, in the expectation that they will be executed during a later iteration.
Reference: [10] <author> J. Pierce and T. Mudge, </author> <title> Wrong-Path Instruction Prefetching, </title> <booktitle> In the Proceedings of the 29th International Symposium on Microar-chitecture, </booktitle> <year> 1996. </year>
Reference-contexts: The IST concept w as subsequently combined with the prefetch predicates of [2] in [9]. Another hardw are prefetching scheme that avoids the need for significant amounts of hardware is the wrong path prefetching described in <ref> [10] </ref>. This actually prefetches instructions from the not-taken path, in the expectation that they will be executed during a later iteration. Most prefetching techniques, software or hardware-based, tend to perform poorly on an important class of applications ha ving recursive data structures such as link ed-lists.
Reference: [11] <author> M. Lipasti, W. Schmidt, S. Kunkel, and R. Roediger, SPAID: </author> <title> Software Prefetching in Pointer and Call-Intensive Environments, </title> <booktitle> In the Proceedings of the 28th International Symposium on Microarchitecture, </booktitle> <year> 1995. </year>
Reference-contexts: Most prefetching techniques, software or hardware-based, tend to perform poorly on an important class of applications ha ving recursive data structures such as link ed-lists. A software technique that overcomes this limitation w as presented recently in <ref> [11] </ref>, in which software prefetches were inserted at subroutine call sites that passed pointers as arguments. Another pointer-based approach was described in [12]. This approach uses pointers stored within the data structures to generate software prefetches.
Reference: [12] <author> C.K. Luk and T.C. Mowry, </author> <title> Compiler-Based Prefetching for Recursive Data Structures, </title> <booktitle> In the Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: A software technique that overcomes this limitation w as presented recently in [11], in which software prefetches were inserted at subroutine call sites that passed pointers as arguments. Another pointer-based approach was described in <ref> [12] </ref>. This approach uses pointers stored within the data structures to generate software prefetches.
Reference: [13] <author> A. Eustace and A. Srivastava, </author> <title> ATOM: A Flexible Interface for Building High Performance Program Analysis Tools, </title> <institution> Digital Equipment Corporation Western Research Laboratory Technical Note TN-44, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Our simulations of the aggressi ve runahead policy assume that the processor al ways stays on the proper path of execution during runahead. 3. Experiments The simulator was created using ATOM <ref> [13] </ref>. The simulated processor fetches, decodes, and executes one instruction per cycle. It did not model pipeline stalls or penalties due to instruction cache misses, just the effects of data cache misses and an y resulting prefetching.
Reference: [14] <author> D. Kroft, </author> <title> Lockup-Free Instruction Fetch/Prefetch Cache Organization, </title> <booktitle> In the Proceedings of the 8th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1981. </year>
Reference-contexts: The DMAQ thus provides the functionality of a store queue, outstanding request list [6], and miss status holding registers <ref> [14] </ref>. It is further assumed that the DMAQ cannot coalesce store-throughs, or allow demand fetches or store-throughs to either pass or squash outstanding prefetches.
Reference: [15] <author> J. Dundas and T. Mudge, </author> <title> Using Stall Cycles to Improve Microprocessor Performance, </title> <institution> University of Michigan Department of Electrical Engineering and Computer Science Technical Report CSE-TR-301-96, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: Pre-executed conditional branch outcomes can also be used to train a traditional dynamic branch prediction scheme during runahead episodes, which is used to predict conditional branches when the shift register is empty. Some preliminary runahead branch prediction simulation results were reported in <ref> [15] </ref>. Although the initial results were encouraging, the fact that the simulator could not e xplore wrong paths during runahead meant that the results were some what optimistic. We will present more accurate results once we ha ve a simulator that can explore wrong paths.
References-found: 15

