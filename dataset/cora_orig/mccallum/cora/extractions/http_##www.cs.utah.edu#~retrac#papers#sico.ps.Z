URL: http://www.cs.utah.edu/~retrac/papers/sico.ps.Z
Refering-URL: http://www.cs.utah.edu/~retrac/papers.html
Root-URL: 
Title: SiCO A Simple COMA Implementation Review draft  
Abstract: Cache-only memory architecture (COMA) machines treat their entire memory as cache, thereby allowing data items to migrate dynamically to the processing nodes that are using them. This aspect of COMA machines can lead to significant performance compared to conventional CC-NUMA designs for programs with large working sets, at the expense of requiring more complex hardware. A variant of COMA, Simple COMA, requires significantly simpler hardware that traditional COMA. In Simple COMA, the node cache memory is allocated in page-sized blocks using software, thus simplifying the design, while coherence is maintained by hardware at the cache line level, and thus maintaining good performance. We present the design of SiCO, a Simple COMA machine, including a description of its cache coherence protocol which is independent of network topology or ordering capability. We show how this enables the directories to migrate automatically around the machine enabling the reduction of hotspots and latency. We performed a detailed simulation of this design and compared its performance with CC-NUMA. We found that Simple COMA has the potential to outperform CC-NUMA for workingsets which fill up to 80% of the machine memory. As the problem and memory sizes are increased, approaching more realistic workloads, the performance of Simple COMA gets close to that of the more complex all-hardware COMA, while the performance of CC-NUMA degrades. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Authors Anonymous, </author> <title> An Argument for Simple COMA. </title> <note> To appear XXX, IEEE 1995. </note>
Reference-contexts: The fact that there is no longer a reserved home storage space for a datum, introduces complexity to handle the preservation of the last copy of a datum as copies are replaced out from cache. 1.1 Simple COMA background Simple COMA <ref> [1, 2] </ref> divides the task of managing the global virtual address space between hardware and software to provide the automatic data migration and greater data replication of a traditional COMA with much less complex hardware and coherence protocols, as illustrated in Figure 1c. <p> network that preserves the order between transactions sent from a node. 1.3 Paper outline The high-level architectural issues affecting Simple COMA designs; including attraction memory page allocation and replacement policies, and methods for converting local physical addresses generated by the MMU into unique global identifiers are introduced in prior papers <ref> [2, 1] </ref>, where also initial performance estimations are presented. In section 2 of this paper we present a detailed description of how the SiCO Simple COMA implementation is designed. <p> Fortunately, however, this is a power term, which means the effect reduces dramitcally as the number of allocation units increases. In our prelimiary studies of Simple COMA, our simulations were of relatively small problems, with only a few tens of pages per simulated node <ref> [1] </ref>.
Reference: [2] <author> Authors Anonymous, </author> <title> Simple COMA Node Implementations. </title> <booktitle> In XXX, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: The fact that there is no longer a reserved home storage space for a datum, introduces complexity to handle the preservation of the last copy of a datum as copies are replaced out from cache. 1.1 Simple COMA background Simple COMA <ref> [1, 2] </ref> divides the task of managing the global virtual address space between hardware and software to provide the automatic data migration and greater data replication of a traditional COMA with much less complex hardware and coherence protocols, as illustrated in Figure 1c. <p> The AM had limited associativity, and the machines used hierarchical network-based search strategies to find remote copies. Typhoon The Stache replication policy for the proposed Typhoon architecture [10] has many similarities with the Simple COMA principle. They both rely on principles first described in <ref> [2] </ref>. The SiCO hardware is much simpler than the Typhoon. S3MP The S3 MP project [15] aims to build a distributed shared memory architecture by adding a relatively small amount of electronics to an existing workstation's memory bus (typically a SPARC-10). <p> network that preserves the order between transactions sent from a node. 1.3 Paper outline The high-level architectural issues affecting Simple COMA designs; including attraction memory page allocation and replacement policies, and methods for converting local physical addresses generated by the MMU into unique global identifiers are introduced in prior papers <ref> [2, 1] </ref>, where also initial performance estimations are presented. In section 2 of this paper we present a detailed description of how the SiCO Simple COMA implementation is designed.
Reference: [3] <author> Authors Anonymous, </author> <title> The SiCO Cache Coherence Protocol Tech. </title> <type> Report, </type> <month> Dec </month> <year> 1994. </year>
Reference-contexts: A full discussion of the various methods <ref> [3, 13] </ref> is beyond the scope of this paper, however we describe the method we chose to accompany our baseline protocol described above (see Figure 13). The easiest case to consider is when a node in a sharing chain decides it wishes to unhook it's copy of a cache line.
Reference: [4] <author> H. Burkhardt, S. Frank, B. Knobe, and J. Rothnie. </author> <title> Overview of the KSR1 Computer System. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendall Square Research, </institution> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction There is an increasing demand for hardware supported shared memory machines. The current generation of scalable parallel machines clearly reflects this (e.g., the Convex Exemplar, IBM SP-2 Cray T3E, and KSR-1 <ref> [4] </ref>). The two most popular principles for scalable shared memory architectures are CC-NUMA [9, 5] (cache-coherent non-uniform memory access) and COMA [14, 7, 4] (cache-only memory architecture). CC-NUMA machines distribute physical memory across the nodes in the machine, as depicted in Figure 1a. <p> The current generation of scalable parallel machines clearly reflects this (e.g., the Convex Exemplar, IBM SP-2 Cray T3E, and KSR-1 [4]). The two most popular principles for scalable shared memory architectures are CC-NUMA [9, 5] (cache-coherent non-uniform memory access) and COMA <ref> [14, 7, 4] </ref> (cache-only memory architecture). CC-NUMA machines distribute physical memory across the nodes in the machine, as depicted in Figure 1a. Any node can map and access any page of physical memory, whether it is on the same node or not. <p> This problem leads to pressure when designing CC-NUMA machines to build large (and expensive) caches. In a COMA machine, such as the SICS DDM [7] and the KSR-1 <ref> [4] </ref>, additional hardware is used to implement a large DRAM cache (or attraction memory) instead of the large local memory of a CC-NUMA. <p> KSR The multiprocessors that were sold by the company Kendall Square Research <ref> [4] </ref>, KSR-1 & KSR-2 have several similarities with our work. However, the architectures also differed in central aspects, for example: The KSR machines required special-purpose hardware in processors and low-level caches to manage the attraction memory.
Reference: [5] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the 4th Annual Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991. </year>
Reference-contexts: 1 Introduction There is an increasing demand for hardware supported shared memory machines. The current generation of scalable parallel machines clearly reflects this (e.g., the Convex Exemplar, IBM SP-2 Cray T3E, and KSR-1 [4]). The two most popular principles for scalable shared memory architectures are CC-NUMA <ref> [9, 5] </ref> (cache-coherent non-uniform memory access) and COMA [14, 7, 4] (cache-only memory architecture). CC-NUMA machines distribute physical memory across the nodes in the machine, as depicted in Figure 1a. Any node can map and access any page of physical memory, whether it is on the same node or not.
Reference: [6] <author> E. Hagersten. </author> <title> Toward Scalable Cache Only Memory Architectures. </title> <type> PhD thesis, </type> <institution> Royal Institute of Technology, Stockholm/ Swedish Institute of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: This dynamic rebalancing of the data means that COMA can exhibit all of the properties of CC-NUMA for CC-NUMA tailored applications, as well as run applications that do not map well to CC-NUMAs, such as applications that have a per-node working set larger than the size of each node's cache <ref> [6] </ref>. Unfortunately, COMA's flexibility requires non-standard memory system hardware, introducing a price-performance tradeoff.
Reference: [7] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM A Cache-Only Memory Architecture. </title> <journal> IEEE Computer, </journal> <volume> 25(9) </volume> <pages> 44-54, </pages> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: The current generation of scalable parallel machines clearly reflects this (e.g., the Convex Exemplar, IBM SP-2 Cray T3E, and KSR-1 [4]). The two most popular principles for scalable shared memory architectures are CC-NUMA [9, 5] (cache-coherent non-uniform memory access) and COMA <ref> [14, 7, 4] </ref> (cache-only memory architecture). CC-NUMA machines distribute physical memory across the nodes in the machine, as depicted in Figure 1a. Any node can map and access any page of physical memory, whether it is on the same node or not. <p> This problem leads to pressure when designing CC-NUMA machines to build large (and expensive) caches. In a COMA machine, such as the SICS DDM <ref> [7] </ref> and the KSR-1 [4], additional hardware is used to implement a large DRAM cache (or attraction memory) instead of the large local memory of a CC-NUMA. <p> In section 2 of this paper we present a detailed description of how the SiCO Simple COMA implementation is designed. Section 3 presents a scalable, sequentially consistent cache protocol that works across an arbitrary network topology (unlike the original COMA design, which required a hierarchical network topology <ref> [7] </ref>). In addition we describe how the "directory" (hostel) for each page can be migrated to where the data it manages is being used. This form of directory localization improved the performance of a hierarchical COMA [7], but cannot be supported on a COMA Flat [12] without additional hardware support. <p> arbitrary network topology (unlike the original COMA design, which required a hierarchical network topology <ref> [7] </ref>). In addition we describe how the "directory" (hostel) for each page can be migrated to where the data it manages is being used. This form of directory localization improved the performance of a hierarchical COMA [7], but cannot be supported on a COMA Flat [12] without additional hardware support. In section 4 we discuss the relative impact of page-granularity Attraction Memory management for real sized machines.
Reference: [8] <author> T. Joe and J.L. Hennessy. </author> <title> Evaluating the memory overhead required for coma architectures. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 82-93, </pages> <year> 1994. </year>
Reference-contexts: miss is a miss in which we fail to detect an appropriate allocation unit in a hardware COMA this is a failure to find a local cache line with matching tags, in a Simple COMA it is a page fault generated by the processor's MMU. 4.1 Conventional Hardware COMAs In <ref> [8] </ref> Joe and Hennessy has studied the consequences of limited Attraction Memory associativity on true replacement activity.
Reference: [9] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <year> 1990. </year> <month> 17 </month>
Reference-contexts: 1 Introduction There is an increasing demand for hardware supported shared memory machines. The current generation of scalable parallel machines clearly reflects this (e.g., the Convex Exemplar, IBM SP-2 Cray T3E, and KSR-1 [4]). The two most popular principles for scalable shared memory architectures are CC-NUMA <ref> [9, 5] </ref> (cache-coherent non-uniform memory access) and COMA [14, 7, 4] (cache-only memory architecture). CC-NUMA machines distribute physical memory across the nodes in the machine, as depicted in Figure 1a. Any node can map and access any page of physical memory, whether it is on the same node or not.
Reference: [10] <author> S. Reinhardt, J. Larus, and D. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory, </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <year> 1994. </year>
Reference-contexts: The AM had limited associativity, and the machines used hierarchical network-based search strategies to find remote copies. Typhoon The Stache replication policy for the proposed Typhoon architecture <ref> [10] </ref> has many similarities with the Simple COMA principle. They both rely on principles first described in [2]. The SiCO hardware is much simpler than the Typhoon.
Reference: [11] <author> J.S. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <institution> Stanford University, </institution> <type> Report, </type> <month> April </month> <year> 1991. </year>
Reference-contexts: We were interested in the performance trend of Simple COMA as problem sizes were scaled towards those one might expect for real sized computations. 5.1 Applications The simulations in this study modeled moderately sized machines running applications from the Splash <ref> [11] </ref> benchmark suite. The applications chosen from the SPLASH benchmark suite were: MP3D A simple simulator for rarefied gas flow over an object in a wind tunnel. OCEAN An SOR application modeling the behavior of a small section of ocean surface. Barnes-Hut A simulation of gravitational body attraction.
Reference: [12] <author> P. Stenstrom, T. Joe, and A. </author> <title> Gupta Comparative Performance Evaluation of Cache-Coherent NUMA and COMA Architectures. </title> <booktitle> In 19th Annual Intrnational Symposium on Computer Architecture, ACM 1992. </booktitle>
Reference-contexts: As the result of an informal cooperation with the authors, S3MP has been modified to also support a form of Simple COMA. COMA Flat The COMA Flat, by Stenstrom et. al <ref> [12] </ref>, is a traditional COMA with a protocol for a non-hierarchical network. It differs from the protocol described here, in that it is based on a full-mapped scheme and lacks the dynamic properties of Simple COMA, that allows the directory location to migrate. <p> In addition we describe how the "directory" (hostel) for each page can be migrated to where the data it manages is being used. This form of directory localization improved the performance of a hierarchical COMA [7], but cannot be supported on a COMA Flat <ref> [12] </ref> without additional hardware support. In section 4 we discuss the relative impact of page-granularity Attraction Memory management for real sized machines.
Reference: [13] <author> M. Thapar and B. Delagi. </author> <title> Stanford Distributed-Directory Protocol. </title> <booktitle> IEEE Computer, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: However, we mention some design alternatives and optimizations that may be valuable for improving performance. The SiCO design uses a scalable sequentially consistent cache coherence protocol for managing individual coherence units (covered in section 3). The protocol is based on the singly linked list principle <ref> [13] </ref>. While a sequentially consistent model probably is desired as baseline, those interested in maximizing performance should consider a relaxed consistency model to allow more aggressive memory access interleaving, trading off ease of programming. <p> This scalable protocol is implementable on any network as it allows out of order delivery of messages by the network. It is a singly linked list protocol in a similar vein to the Stanford Distributed Directory protocol <ref> [13] </ref>. The root of the sharing list is held in a directory, or hostel, which is identical to the other nodes on the machine. <p> A full discussion of the various methods <ref> [3, 13] </ref> is beyond the scope of this paper, however we describe the method we chose to accompany our baseline protocol described above (see Figure 13). The easiest case to consider is when a node in a sharing chain decides it wishes to unhook it's copy of a cache line.
Reference: [14] <author> D.H.D. Warren and S. Haridi. </author> <title> Data Diffusion Machine-a scalable shared virtual memory multiprocessor. </title> <booktitle> In International Conference on Fifth Generation Computer Systems 1988. </booktitle> <publisher> ICOT, </publisher> <year> 1988. </year>
Reference-contexts: The current generation of scalable parallel machines clearly reflects this (e.g., the Convex Exemplar, IBM SP-2 Cray T3E, and KSR-1 [4]). The two most popular principles for scalable shared memory architectures are CC-NUMA [9, 5] (cache-coherent non-uniform memory access) and COMA <ref> [14, 7, 4] </ref> (cache-only memory architecture). CC-NUMA machines distribute physical memory across the nodes in the machine, as depicted in Figure 1a. Any node can map and access any page of physical memory, whether it is on the same node or not.
Reference: [15] <author> Nowatzyk,A. and Monger,M. and Parkin,M. and Kelly,E. and Browne,M. and Aybay,G. and Lee,D. S3.mp: </author> <title> A Multiprocessor in a Matchbox. </title> <institution> Sun Microsystems Computer Corporation, </institution> <year> 1993. </year> <note> Available as : parcftp.xerox.com:/pub/dlee/PASA proc.ps </note>
Reference-contexts: Typhoon The Stache replication policy for the proposed Typhoon architecture [10] has many similarities with the Simple COMA principle. They both rely on principles first described in [2]. The SiCO hardware is much simpler than the Typhoon. S3MP The S3 MP project <ref> [15] </ref> aims to build a distributed shared memory architecture by adding a relatively small amount of electronics to an existing workstation's memory bus (typically a SPARC-10). It is essentially a CC-NUMA design, but with the remote data cache being constructed from a reserved portion of the host machine's main memory. <p> The protocol engine is easily implementable within a micro-coded or state driven protocol controller. Transient states of the coherence units are then implicitly described by the current code position. This method is well understood, and implemented in other machines, for example the Sun S3MP <ref> [15] </ref>. 7 3.1 Fundamental states The two state bits associated with each coherence unit represent the following fundamental states: Invalid The coherence unit is invalid, the data must be fetched from another node. Shared The coherence unit contains valid data which is shared with other nodes.
Reference: [16] <institution> Fowler,R. Personal communication Rice University, Texas. </institution> <month> 18 </month>
Reference-contexts: This strategy has been shown to yield reasonable results for a range of applications, with full page migration providing only a small incremental improvement <ref> [16] </ref>. The Simple COMA machine A trivial random replacement strategy was modeled for Simple COMA, except that pages which were not shared with other nodes were given a second chance before being replaced. The second chance flag would also be reset if the page was accessed.
References-found: 16

