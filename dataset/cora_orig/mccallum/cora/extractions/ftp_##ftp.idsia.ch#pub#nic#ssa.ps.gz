URL: ftp://ftp.idsia.ch/pub/nic/ssa.ps.gz
Refering-URL: http://www.cnl.salk.edu/~schraudo/pubs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: REINFORCEMENT LEARNING WITH SELF-MODIFYING POLICIES  
Author: In S. Thrun and L. Pratt, Jurgen Schmidhuber, Jieyu Zhao, Nicol N. Schraudolph 
Address: Corso Elvezia 36, CH-6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Date: 1997  
Note: eds., "Learning to learn", pages 293-309, Kluwer,  
Abstract: A learner's modifiable components are called its policy. An algorithm that modifies the policy is a learning algorithm. If the learning algorithm has modifiable components represented as part of the policy, then we speak of a self-modifying policy (SMP). SMPs can modify the way they modify themselves etc. They are of interest in situations where the initial learning algorithm itself can be improved by experience | this is what we call "learning to learn". How can we force some (stochastic) SMP to trigger better and better self-modifications? The success-story algorithm (SSA) addresses this question in a lifelong reinforcement learning context. During the learner's life-time, SSA is occasionally called at times computed according to SMP itself. SSA uses backtracking to undo those SMP-generated SMP-modifications that have not been empirically observed to trigger lifelong reward accelerations (measured up until the current SSA call | this evaluates the long-term effects of SMP-modifications setting the stage for later SMP-modifications). SMP-modifications that survive SSA represent a lifelong success history. Until the next SSA call, they build the basis for additional SMP-modifications. Solely by self-modifications our SMP/SSA-based learners solve a complex task in a partially observable environment (POE) whose state space is far bigger than most reported in the POE literature.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Schmidhuber. </author> <title> On learning how to learn learning strategies. </title> <type> Technical Report FKI-198-94, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen, </institution> <month> November </month> <year> 1994. </year> <note> Revised January 1995. </note>
Reference-contexts: 1 Introduction Bias towards algorithmic regularity. There is no miraculous universal learning algorithm that will perform well in arbitrary environments <ref> [1, 2, 3] </ref>. Appropriate inductive bias [4] is essential. In unknown environments, however, we do not want too specific a bias. <p> But AORB is highly unspecific in the sense that it can help to solve all kinds of "typical", "regular", interesting problems occurring in our atypical, regular universe. This paper, based on <ref> [1] </ref>, goes beyond our other recent papers exploiting algorithmic regularities for practical machine learning purposes [9, 3, 10]. Levin search (LS). <p> In this sense we are interested in learning to learn. Restricted notion of "learning to learn". In contrast to [16] and other chapters in the book at hand, but in the spirit of the first author's earlier work <ref> [17, 18, 1] </ref> we will reserve the expressions metalearning or learning to learn to characterize learners that (1) can evaluate and compare learning methods, (2) measure the benefits of early learning on subsequent learning, (3) use such evaluations to reason about learning strategies and to select "useful" ones while discarding others. <p> We will use fl = 15; * = 0:001, k = 3. Although our PLAs do not allow for composing arbitrary probabilistic learning algorithms, they do allow for a wide variety of them (in <ref> [1] </ref> a universal programming language is used to create the potential for even more powerful learning algorithms). Later we will integrate Q-learning [37, 21] into SMP as a primitive instruction. Evaluation instruction.
Reference: [2] <author> D. H. Wolpert. </author> <title> The lack of a priori distinctions between learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 8(7) </volume> <pages> 1341-1390, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Bias towards algorithmic regularity. There is no miraculous universal learning algorithm that will perform well in arbitrary environments <ref> [1, 2, 3] </ref>. Appropriate inductive bias [4] is essential. In unknown environments, however, we do not want too specific a bias.
Reference: [3] <author> J. Schmidhuber. </author> <title> Discovering neural nets with low Kolmogorov complexity and high generalization capability. </title> <booktitle> Neural Networks, </booktitle> <year> 1997. </year> <note> In press. </note>
Reference-contexts: 1 Introduction Bias towards algorithmic regularity. There is no miraculous universal learning algorithm that will perform well in arbitrary environments <ref> [1, 2, 3] </ref>. Appropriate inductive bias [4] is essential. In unknown environments, however, we do not want too specific a bias. <p> But AORB is highly unspecific in the sense that it can help to solve all kinds of "typical", "regular", interesting problems occurring in our atypical, regular universe. This paper, based on [1], goes beyond our other recent papers exploiting algorithmic regularities for practical machine learning purposes <ref> [9, 3, 10] </ref>. Levin search (LS). References [9, 3] show how a variant of Levin search (LS) [11, 12, 13, 8] can find problem solutions with low Kolmogorov complexity and high generalization ability in non-random but otherwise quite general settings. <p> This paper, based on [1], goes beyond our other recent papers exploiting algorithmic regularities for practical machine learning purposes [9, 3, 10]. Levin search (LS). References <ref> [9, 3] </ref> show how a variant of Levin search (LS) [11, 12, 13, 8] can find problem solutions with low Kolmogorov complexity and high generalization ability in non-random but otherwise quite general settings. <p> Despite this strong result, until recently LS has not received much attention except in purely theoretical studies | see, e.g., [14]. 1 Adaptive LS. References <ref> [15, 3, 10] </ref> note that LS is not necessarily optimal if algorithmic information (e.g., [8]) between solutions to successive problems can be exploited to reduce future search costs based on experience. Our adaptive LS extension (ALS) [15, 10] does use experience to modify LS' underlying probability distribution.
Reference: [4] <author> P. Utgoff. </author> <title> Shift of bias for inductive concept learning. </title> <editor> In R. Michalski, J. Carbonell, and T. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> volume 2, </volume> <pages> pages 163-190. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Bias towards algorithmic regularity. There is no miraculous universal learning algorithm that will perform well in arbitrary environments [1, 2, 3]. Appropriate inductive bias <ref> [4] </ref> is essential. In unknown environments, however, we do not want too specific a bias. How unspecific may it be? The algorithmic Occam's razor bias (AORB) assumes that problem solutions are non-random but regular [5, 6, 7, 8], without specifying the way in which they are non-random.
Reference: [5] <author> R.J. Solomonoff. </author> <title> A formal theory of inductive inference. Part I. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22, </pages> <year> 1964. </year>
Reference-contexts: Appropriate inductive bias [4] is essential. In unknown environments, however, we do not want too specific a bias. How unspecific may it be? The algorithmic Occam's razor bias (AORB) assumes that problem solutions are non-random but regular <ref> [5, 6, 7, 8] </ref>, without specifying the way in which they are non-random. AORB is highly specific in the sense that it tends to be useless for solving an arbitrary problem from the set of all well-defined problems, almost all of which have irregular solutions [5, 6, 7, 8]. <p> solutions are non-random but regular <ref> [5, 6, 7, 8] </ref>, without specifying the way in which they are non-random. AORB is highly specific in the sense that it tends to be useless for solving an arbitrary problem from the set of all well-defined problems, almost all of which have irregular solutions [5, 6, 7, 8]. But AORB is highly unspecific in the sense that it can help to solve all kinds of "typical", "regular", interesting problems occurring in our atypical, regular universe.
Reference: [6] <author> A.N. </author> <title> Kolmogorov. Three approaches to the quantitative definition of information. </title> <journal> Problems of Information Transmission, </journal> <volume> 1 </volume> <pages> 1-11, </pages> <year> 1965. </year>
Reference-contexts: Appropriate inductive bias [4] is essential. In unknown environments, however, we do not want too specific a bias. How unspecific may it be? The algorithmic Occam's razor bias (AORB) assumes that problem solutions are non-random but regular <ref> [5, 6, 7, 8] </ref>, without specifying the way in which they are non-random. AORB is highly specific in the sense that it tends to be useless for solving an arbitrary problem from the set of all well-defined problems, almost all of which have irregular solutions [5, 6, 7, 8]. <p> solutions are non-random but regular <ref> [5, 6, 7, 8] </ref>, without specifying the way in which they are non-random. AORB is highly specific in the sense that it tends to be useless for solving an arbitrary problem from the set of all well-defined problems, almost all of which have irregular solutions [5, 6, 7, 8]. But AORB is highly unspecific in the sense that it can help to solve all kinds of "typical", "regular", interesting problems occurring in our atypical, regular universe.
Reference: [7] <author> G.J. Chaitin. </author> <title> On the length of programs for computing finite binary sequences: statistical considerations. </title> <journal> Journal of the ACM, </journal> <volume> 16 </volume> <pages> 145-159, </pages> <year> 1969. </year>
Reference-contexts: Appropriate inductive bias [4] is essential. In unknown environments, however, we do not want too specific a bias. How unspecific may it be? The algorithmic Occam's razor bias (AORB) assumes that problem solutions are non-random but regular <ref> [5, 6, 7, 8] </ref>, without specifying the way in which they are non-random. AORB is highly specific in the sense that it tends to be useless for solving an arbitrary problem from the set of all well-defined problems, almost all of which have irregular solutions [5, 6, 7, 8]. <p> solutions are non-random but regular <ref> [5, 6, 7, 8] </ref>, without specifying the way in which they are non-random. AORB is highly specific in the sense that it tends to be useless for solving an arbitrary problem from the set of all well-defined problems, almost all of which have irregular solutions [5, 6, 7, 8]. But AORB is highly unspecific in the sense that it can help to solve all kinds of "typical", "regular", interesting problems occurring in our atypical, regular universe.
Reference: [8] <author> M. Li and P. M. B. Vitanyi. </author> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer, </publisher> <year> 1993. </year>
Reference-contexts: Appropriate inductive bias [4] is essential. In unknown environments, however, we do not want too specific a bias. How unspecific may it be? The algorithmic Occam's razor bias (AORB) assumes that problem solutions are non-random but regular <ref> [5, 6, 7, 8] </ref>, without specifying the way in which they are non-random. AORB is highly specific in the sense that it tends to be useless for solving an arbitrary problem from the set of all well-defined problems, almost all of which have irregular solutions [5, 6, 7, 8]. <p> solutions are non-random but regular <ref> [5, 6, 7, 8] </ref>, without specifying the way in which they are non-random. AORB is highly specific in the sense that it tends to be useless for solving an arbitrary problem from the set of all well-defined problems, almost all of which have irregular solutions [5, 6, 7, 8]. But AORB is highly unspecific in the sense that it can help to solve all kinds of "typical", "regular", interesting problems occurring in our atypical, regular universe. <p> This paper, based on [1], goes beyond our other recent papers exploiting algorithmic regularities for practical machine learning purposes [9, 3, 10]. Levin search (LS). References [9, 3] show how a variant of Levin search (LS) <ref> [11, 12, 13, 8] </ref> can find problem solutions with low Kolmogorov complexity and high generalization ability in non-random but otherwise quite general settings. LS is of interest because it has the optimal order of computational complexity for a broad class of search problems. <p> Despite this strong result, until recently LS has not received much attention except in purely theoretical studies | see, e.g., [14]. 1 Adaptive LS. References [15, 3, 10] note that LS is not necessarily optimal if algorithmic information (e.g., <ref> [8] </ref>) between solutions to successive problems can be exploited to reduce future search costs based on experience. Our adaptive LS extension (ALS) [15, 10] does use experience to modify LS' underlying probability distribution. ALS can dramatically reduce the search time consumed by successive LS calls in certain regular environments [10].
Reference: [9] <author> J. Schmidhuber. </author> <title> Discovering solutions with low Kolmogorov complexity and high generalization capability. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 488-496. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1995. </year>
Reference-contexts: But AORB is highly unspecific in the sense that it can help to solve all kinds of "typical", "regular", interesting problems occurring in our atypical, regular universe. This paper, based on [1], goes beyond our other recent papers exploiting algorithmic regularities for practical machine learning purposes <ref> [9, 3, 10] </ref>. Levin search (LS). References [9, 3] show how a variant of Levin search (LS) [11, 12, 13, 8] can find problem solutions with low Kolmogorov complexity and high generalization ability in non-random but otherwise quite general settings. <p> This paper, based on [1], goes beyond our other recent papers exploiting algorithmic regularities for practical machine learning purposes [9, 3, 10]. Levin search (LS). References <ref> [9, 3] </ref> show how a variant of Levin search (LS) [11, 12, 13, 8] can find problem solutions with low Kolmogorov complexity and high generalization ability in non-random but otherwise quite general settings.
Reference: [10] <author> M.A. Wiering and J. Schmidhuber. </author> <title> Solving POMDPs with Levin search and EIRA. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 534-542. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: But AORB is highly unspecific in the sense that it can help to solve all kinds of "typical", "regular", interesting problems occurring in our atypical, regular universe. This paper, based on [1], goes beyond our other recent papers exploiting algorithmic regularities for practical machine learning purposes <ref> [9, 3, 10] </ref>. Levin search (LS). References [9, 3] show how a variant of Levin search (LS) [11, 12, 13, 8] can find problem solutions with low Kolmogorov complexity and high generalization ability in non-random but otherwise quite general settings. <p> Despite this strong result, until recently LS has not received much attention except in purely theoretical studies | see, e.g., [14]. 1 Adaptive LS. References <ref> [15, 3, 10] </ref> note that LS is not necessarily optimal if algorithmic information (e.g., [8]) between solutions to successive problems can be exploited to reduce future search costs based on experience. Our adaptive LS extension (ALS) [15, 10] does use experience to modify LS' underlying probability distribution. <p> References [15, 3, 10] note that LS is not necessarily optimal if algorithmic information (e.g., [8]) between solutions to successive problems can be exploited to reduce future search costs based on experience. Our adaptive LS extension (ALS) <ref> [15, 10] </ref> does use experience to modify LS' underlying probability distribution. ALS can dramatically reduce the search time consumed by successive LS calls in certain regular environments [10]. Exploiting arbitrary regularities? This paper goes one step further. Let us call a learner's modifiable components its policy. <p> Our adaptive LS extension (ALS) [15, 10] does use experience to modify LS' underlying probability distribution. ALS can dramatically reduce the search time consumed by successive LS calls in certain regular environments <ref> [10] </ref>. Exploiting arbitrary regularities? This paper goes one step further. Let us call a learner's modifiable components its policy. Policy-modifying algorithms are called learning algorithms. We observe that if our learner wants to exploit arbitrary algorithmic regularities then it must be able to execute arbitrary, problem-specific learning algorithms.
Reference: [11] <author> L. A. Levin. </author> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266, </pages> <year> 1973. </year>
Reference-contexts: This paper, based on [1], goes beyond our other recent papers exploiting algorithmic regularities for practical machine learning purposes [9, 3, 10]. Levin search (LS). References [9, 3] show how a variant of Levin search (LS) <ref> [11, 12, 13, 8] </ref> can find problem solutions with low Kolmogorov complexity and high generalization ability in non-random but otherwise quite general settings. LS is of interest because it has the optimal order of computational complexity for a broad class of search problems.
Reference: [12] <author> L. A. Levin. </author> <title> Randomness conservation inequalities: Information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37, </pages> <year> 1984. </year>
Reference-contexts: This paper, based on [1], goes beyond our other recent papers exploiting algorithmic regularities for practical machine learning purposes [9, 3, 10]. Levin search (LS). References [9, 3] show how a variant of Levin search (LS) <ref> [11, 12, 13, 8] </ref> can find problem solutions with low Kolmogorov complexity and high generalization ability in non-random but otherwise quite general settings. LS is of interest because it has the optimal order of computational complexity for a broad class of search problems.
Reference: [13] <author> R.J. Solomonoff. </author> <title> An application of algorithmic probability to problems in artificial intelligence. </title> <editor> In L. N. Kanal and J. F. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 473-491. </pages> <publisher> Elsevier Science Publishers, </publisher> <year> 1986. </year>
Reference-contexts: This paper, based on [1], goes beyond our other recent papers exploiting algorithmic regularities for practical machine learning purposes [9, 3, 10]. Levin search (LS). References [9, 3] show how a variant of Levin search (LS) <ref> [11, 12, 13, 8] </ref> can find problem solutions with low Kolmogorov complexity and high generalization ability in non-random but otherwise quite general settings. LS is of interest because it has the optimal order of computational complexity for a broad class of search problems.
Reference: [14] <author> O. Watanabe. </author> <title> Kolmogorov complexity and computational complexity. </title> <booktitle> EATCS Monographs on Theoretical Computer Science, </booktitle> <publisher> Springer, </publisher> <year> 1992. </year>
Reference-contexts: Then universal LS will solve the same problems in at most O (f (n)) steps (although a large constant may be buried in the O () notation). Despite this strong result, until recently LS has not received much attention except in purely theoretical studies | see, e.g., <ref> [14] </ref>. 1 Adaptive LS. References [15, 3, 10] note that LS is not necessarily optimal if algorithmic information (e.g., [8]) between solutions to successive problems can be exploited to reduce future search costs based on experience.
Reference: [15] <author> J. Schmidhuber. </author> <title> Discovering problem solutions with low Kolmogorov complexity and high generalization capability. </title> <type> Technical Report FKI-194-94, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen, </institution> <year> 1994. </year> <note> Short version in A. </note> <editor> Prieditis and S. Russell, eds., </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> pages 488-496, </pages> <address> San Francisco, CA, </address> <year> 1995. </year>
Reference-contexts: Despite this strong result, until recently LS has not received much attention except in purely theoretical studies | see, e.g., [14]. 1 Adaptive LS. References <ref> [15, 3, 10] </ref> note that LS is not necessarily optimal if algorithmic information (e.g., [8]) between solutions to successive problems can be exploited to reduce future search costs based on experience. Our adaptive LS extension (ALS) [15, 10] does use experience to modify LS' underlying probability distribution. <p> References [15, 3, 10] note that LS is not necessarily optimal if algorithmic information (e.g., [8]) between solutions to successive problems can be exploited to reduce future search costs based on experience. Our adaptive LS extension (ALS) <ref> [15, 10] </ref> does use experience to modify LS' underlying probability distribution. ALS can dramatically reduce the search time consumed by successive LS calls in certain regular environments [10]. Exploiting arbitrary regularities? This paper goes one step further. Let us call a learner's modifiable components its policy.
Reference: [16] <author> R. Caruana, D. L. Silver, J. Baxter, T. M. Mitchell, L. Y. Pratt, and S. Thrun. </author> <title> Learning to learn: </title> <booktitle> knowledge consolidation and transfer in inductive systems, 1995. Workshop held at NIPS-95, </booktitle> <address> Vail, CO, </address> <note> see http://www.cs.cmu.edu/afs/user/caruana/pub/transfer.html. </note>
Reference-contexts: In particular, this includes learning algorithms for creating better learning algorithms, given environmental conditions. In this sense we are interested in learning to learn. Restricted notion of "learning to learn". In contrast to <ref> [16] </ref> and other chapters in the book at hand, but in the spirit of the first author's earlier work [17, 18, 1] we will reserve the expressions metalearning or learning to learn to characterize learners that (1) can evaluate and compare learning methods, (2) measure the benefits of early learning on
Reference: [17] <author> J. Schmidhuber. </author> <title> Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... </title> <type> hook. </type> <institution> Institut fur Informatik, Technische Universitat Munchen, </institution> <year> 1987. </year>
Reference-contexts: In this sense we are interested in learning to learn. Restricted notion of "learning to learn". In contrast to [16] and other chapters in the book at hand, but in the spirit of the first author's earlier work <ref> [17, 18, 1] </ref> we will reserve the expressions metalearning or learning to learn to characterize learners that (1) can evaluate and compare learning methods, (2) measure the benefits of early learning on subsequent learning, (3) use such evaluations to reason about learning strategies and to select "useful" ones while discarding others. <p> Previous work. Unlike SMP/SSA, Lenat's approach [39] requires human interaction defining the "interestingness" of concepts to be explored. Also, many essential aspects of system behavior in previous work on metalearning [39, 40] are not accessible to self-modifications. The meta-evolution scheme <ref> [17] </ref> attempts to learn learning algorithms by a potentially infinite hierarchy of Genetic Programming levels. Higher-level pools contain program-modifying programs which are applied to lower-level programs, and rewarded recursively based on lower-level performance. Unfortunately there is no theoretical assurance that meta-evolution will spend overall computation time wisely.
Reference: [18] <author> J. Schmidhuber. </author> <title> A self-referential weight matrix. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 446-451. </pages> <publisher> Springer, </publisher> <year> 1993. </year> <month> 9 </month>
Reference-contexts: In this sense we are interested in learning to learn. Restricted notion of "learning to learn". In contrast to [16] and other chapters in the book at hand, but in the spirit of the first author's earlier work <ref> [17, 18, 1] </ref> we will reserve the expressions metalearning or learning to learn to characterize learners that (1) can evaluate and compare learning methods, (2) measure the benefits of early learning on subsequent learning, (3) use such evaluations to reason about learning strategies and to select "useful" ones while discarding others. <p> Higher-level pools contain program-modifying programs which are applied to lower-level programs, and rewarded recursively based on lower-level performance. Unfortunately there is no theoretical assurance that meta-evolution will spend overall computation time wisely. Self-modifying recurrent neural nets <ref> [18, 41] </ref> are able to run their own weight change algorithms. Activations of special output units are used to address, read, and modify all of the network's own weights.
Reference: [19] <author> S.D. Whitehead and D. H. Ballard. </author> <title> Active perception and reinforcement learning. </title> <journal> Neural Computa--tion, </journal> <volume> 2(4) </volume> <pages> 409-419, </pages> <year> 1990. </year>
Reference: [20] <author> J. Schmidhuber. </author> <title> Reinforcement learning in Markovian and non-Markovian environments. </title> <editor> In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: [21] <author> L.J. Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Although our PLAs do not allow for composing arbitrary probabilistic learning algorithms, they do allow for a wide variety of them (in [1] a universal programming language is used to create the potential for even more powerful learning algorithms). Later we will integrate Q-learning <ref> [37, 21] </ref> into SMP as a primitive instruction. Evaluation instruction. <p> Still, some authors occasionally apply Q-learning variants to partially observable environments, sometimes even successfully [38]. To test whether our problem is indeed too difficult for Q-learning, we tried to solve it using various T D () Q-variants <ref> [21] </ref>. We first used primitive actions and perceptions similar to SMP's instructions. There are 33 possible Q-actions. The first 32 are "turn to one of the 8 different directions relative to the agent's key/door/current direction/goal, and move 3 steps forward". <p> Typical results are shown in Figures 4 and 4. Q-learning as an instruction for SMP. Q-learning is not designed for POEs. This does not mean, however, that Q-learning cannot be plugged into SMP/SSA as a sometimes useful instruction. To examine this issue, we add T D () Q-learning <ref> [21] </ref>. to the instruction set of both agents' SMPs: B 13 : Q-learning (a1) | with probability a1 200fln ops , keep executing actions according to the Q-table until there is non-zero reward, and update the Q-table according to the T D () Q-learning rules [21]. <p> add T D () Q-learning <ref> [21] </ref>. to the instruction set of both agents' SMPs: B 13 : Q-learning (a1) | with probability a1 200fln ops , keep executing actions according to the Q-table until there is non-zero reward, and update the Q-table according to the T D () Q-learning rules [21]. Otherwise, execute only a single action according to the current Q-table. Interestingly, this Q-plug-in leads to even better results near system death (see Figure 4). Essen tially, the SMPs learn when to trust the Q-table. 7 Observations. Final stack sizes never exceeded 250, corresponding to about 250 surviving SMP-modifications.
Reference: [22] <author> M. B. </author> <title> Ring. Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, Austin, Texas 78712, </institution> <month> August </month> <year> 1994. </year>
Reference: [23] <author> M.I. Littman. </author> <title> Memoryless policies: Theoretical limitations and practical results. </title> <editor> In J. A. Meyer D. Cliff, P. Husbands and S. W. Wilson, editors, </editor> <booktitle> Proc. of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats 3, </booktitle> <pages> pages 297-305. </pages> <publisher> MIT Press/Bradford Books, </publisher> <year> 1994. </year>
Reference-contexts: Our POE, however, is far larger than most of those purported to demonstrate the usefulness of previous POE algorithms, most of which appear to work only for tiny state spaces <ref> [23] </ref>. 2 Basic Concepts System resources. The learner lives from time 0 to unknown time T in an unknown environment E. It has an internal state S and a policy SMP. Both S and SMP are variable data structures influencing probabilities of instructions to be executed. <p> Figure 4 shows a partially observable environment (POE) with 600 fi 800 pixels. This POE has many more states and obstacles than most reported in the POE literature | for instance, Littman et al.'s largest problem <ref> [23] </ref> involves less than 1000 states. There are two SMP/SSA-based agents A and B. Each has circular shape and a diameter of 30 pixels. At a given time, each is rotated in one of eight different directions. <p> Application. A particular SMP/SSA implementation based on very simple, assembler-like instructions has been used successfully to solve a complex POE. To our knowledge, such complex 8 POEs have not been solved by previous POE algorithms, which appear to work only for tiny state spaces <ref> [23] </ref>. Outlook. There are many alternative ways of implementing SMP/SSA | for instance, instructions may be highly complex learning algorithms. Future work will focus on plugging a whole variety of well-known algorithms into SMP/SSA, and letting it pick and combine the best, problem-specific ones.
Reference: [24] <author> D. Cliff and S. Ross. </author> <title> Adding temporary memory to ZCS. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 3 </volume> <pages> 101-150, </pages> <year> 1994. </year>
Reference: [25] <author> L. Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth International Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188. </pages> <publisher> AAAI Press, </publisher> <address> San Jose, California, </address> <year> 1992. </year>
Reference: [26] <author> T. Jaakkola, S. P. Singh, and M. I. Jordan. </author> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 345-352. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1995. </year>
Reference: [27] <author> L.P. Kaelbling, </author> <title> M.L. Littman, and A.R. Cassandra. Planning and acting in partially observable stochastic domains. </title> <type> Technical report, </type> <institution> Brown University, Providence RI, </institution> <year> 1995. </year>
Reference: [28] <author> R. A. McCallum. </author> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 387-395. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1995. </year>
Reference: [29] <author> M. Wiering and J. Schmidhuber. HQ-Learning: </author> <title> Discovering Markovian subgoals for non-Markovian reinforcement learning. </title> <type> Technical Report IDSIA-95-96, </type> <institution> IDSIA, </institution> <year> 1996. </year>
Reference: [30] <author> S. Russell and E. </author> <title> Wefald. </title> <booktitle> Principles of Metareasoning. Artificial Intelligence, </booktitle> <volume> 49 </volume> <pages> 361-395, </pages> <year> 1991. </year>
Reference-contexts: Between time 0 and T , the learner repeats the following cycle over and over again (A denotes a set of possible instructions): select and execute a 2 A with conditional probability P (a j SMP ; S; E). Instruction a will consume time <ref> [30, 31] </ref> and may change E, S, and even SMP. (Somewhat related, but more restricted limited resource scenarios were studied in [32, 33, 34] and references therein.) Primitive learning algorithms (PLAs). Actions in A that modify SMP are called primitive learning algorithms (PLAs).
Reference: [31] <author> M. Boddy and T. L. Dean. </author> <title> Deliberation scheduling for problem solving in time-constrained environments. </title> <journal> Artificial Intelligence, </journal> <volume> 67 </volume> <pages> 245-285, </pages> <year> 1994. </year>
Reference-contexts: Between time 0 and T , the learner repeats the following cycle over and over again (A denotes a set of possible instructions): select and execute a 2 A with conditional probability P (a j SMP ; S; E). Instruction a will consume time <ref> [30, 31] </ref> and may change E, S, and even SMP. (Somewhat related, but more restricted limited resource scenarios were studied in [32, 33, 34] and references therein.) Primitive learning algorithms (PLAs). Actions in A that modify SMP are called primitive learning algorithms (PLAs).
Reference: [32] <author> D. A. Berry and B. Fristedt. </author> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1985. </year>
Reference-contexts: Instruction a will consume time [30, 31] and may change E, S, and even SMP. (Somewhat related, but more restricted limited resource scenarios were studied in <ref> [32, 33, 34] </ref> and references therein.) Primitive learning algorithms (PLAs). Actions in A that modify SMP are called primitive learning algorithms (PLAs). To ensure non-vanishing exploration potential PLAs may not generate SMP-modifications that will let certain action probabilities vanish entirely.
Reference: [33] <author> J. C. Gittins. </author> <title> Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems and optimization. </title> <publisher> Wiley, </publisher> <address> Chichester, NY, </address> <year> 1989. </year>
Reference-contexts: Instruction a will consume time [30, 31] and may change E, S, and even SMP. (Somewhat related, but more restricted limited resource scenarios were studied in <ref> [32, 33, 34] </ref> and references therein.) Primitive learning algorithms (PLAs). Actions in A that modify SMP are called primitive learning algorithms (PLAs). To ensure non-vanishing exploration potential PLAs may not generate SMP-modifications that will let certain action probabilities vanish entirely.
Reference: [34] <author> R. Greiner. </author> <title> PALO: A probabilistic hill-climbing algorithm. </title> <journal> Artificial Intelligence, </journal> <volume> 83(2), </volume> <year> 1996. </year>
Reference-contexts: Instruction a will consume time [30, 31] and may change E, S, and even SMP. (Somewhat related, but more restricted limited resource scenarios were studied in <ref> [32, 33, 34] </ref> and references therein.) Primitive learning algorithms (PLAs). Actions in A that modify SMP are called primitive learning algorithms (PLAs). To ensure non-vanishing exploration potential PLAs may not generate SMP-modifications that will let certain action probabilities vanish entirely.
Reference: [35] <author> P. R. Kumar and P. Varaiya. </author> <title> Stochastic Systems: Estimation, Identification, and Adaptive Control. </title> <publisher> Prentice Hall, </publisher> <year> 1986. </year>
Reference-contexts: Let us assume, however, that E; S and A (representing the system's initial bias) do indeed allow for SMP-modifications 3 triggering long-term reward accelerations. This is an instruction set-dependent assumption much weaker than the typical Markovian assumptions made in previous RL work, e.g., <ref> [35, 36, 37] </ref>. Now, since we prevent all instruction probabilities from vanishing (see implementation below), SMP cannot help but trigger occasional SMP-modifications, and keep those consistent with SSC: in this sense, the learner can only improve.
Reference: [36] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Let us assume, however, that E; S and A (representing the system's initial bias) do indeed allow for SMP-modifications 3 triggering long-term reward accelerations. This is an instruction set-dependent assumption much weaker than the typical Markovian assumptions made in previous RL work, e.g., <ref> [35, 36, 37] </ref>. Now, since we prevent all instruction probabilities from vanishing (see implementation below), SMP cannot help but trigger occasional SMP-modifications, and keep those consistent with SSC: in this sense, the learner can only improve.
Reference: [37] <author> C. J. C. H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Let us assume, however, that E; S and A (representing the system's initial bias) do indeed allow for SMP-modifications 3 triggering long-term reward accelerations. This is an instruction set-dependent assumption much weaker than the typical Markovian assumptions made in previous RL work, e.g., <ref> [35, 36, 37] </ref>. Now, since we prevent all instruction probabilities from vanishing (see implementation below), SMP cannot help but trigger occasional SMP-modifications, and keep those consistent with SSC: in this sense, the learner can only improve. <p> Although our PLAs do not allow for composing arbitrary probabilistic learning algorithms, they do allow for a wide variety of them (in [1] a universal programming language is used to create the potential for even more powerful learning algorithms). Later we will integrate Q-learning <ref> [37, 21] </ref> into SMP as a primitive instruction. Evaluation instruction.
Reference: [38] <author> R.H. Crites and A.G. Barto. </author> <title> Improving elevator performance using reinforcement learning. </title> <editor> In D.S. Touretzky, M.C. Mozer, and M.E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 1017-1023, </pages> <address> Cambridge MA, 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Results with Q-Learning. Q-learning assumes that the environment is fully observable; otherwise it is not guaranteed to work. Still, some authors occasionally apply Q-learning variants to partially observable environments, sometimes even successfully <ref> [38] </ref>. To test whether our problem is indeed too difficult for Q-learning, we tried to solve it using various T D () Q-variants [21]. We first used primitive actions and perceptions similar to SMP's instructions. There are 33 possible Q-actions.
Reference: [39] <author> D. Lenat. </author> <title> Theory formation by heuristic search. </title> <journal> Machine Learning, </journal> <volume> 21, </volume> <year> 1983. </year>
Reference-contexts: The more general the bias shifts, the harder the credit assignment problem | but SSA provides a way of forcing even SMPs with very general (e.g., Turing machine-equivalent) instruction sets to focus on bias shifts causing histories of long-term performance improvements. Previous work. Unlike SMP/SSA, Lenat's approach <ref> [39] </ref> requires human interaction defining the "interestingness" of concepts to be explored. Also, many essential aspects of system behavior in previous work on metalearning [39, 40] are not accessible to self-modifications. The meta-evolution scheme [17] attempts to learn learning algorithms by a potentially infinite hierarchy of Genetic Programming levels. <p> Previous work. Unlike SMP/SSA, Lenat's approach [39] requires human interaction defining the "interestingness" of concepts to be explored. Also, many essential aspects of system behavior in previous work on metalearning <ref> [39, 40] </ref> are not accessible to self-modifications. The meta-evolution scheme [17] attempts to learn learning algorithms by a potentially infinite hierarchy of Genetic Programming levels. Higher-level pools contain program-modifying programs which are applied to lower-level programs, and rewarded recursively based on lower-level performance.
Reference: [40] <author> P. S. Rosenbloom, J. E. Laird, and A. Newell. </author> <title> The SOAR Papers. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Previous work. Unlike SMP/SSA, Lenat's approach [39] requires human interaction defining the "interestingness" of concepts to be explored. Also, many essential aspects of system behavior in previous work on metalearning <ref> [39, 40] </ref> are not accessible to self-modifications. The meta-evolution scheme [17] attempts to learn learning algorithms by a potentially infinite hierarchy of Genetic Programming levels. Higher-level pools contain program-modifying programs which are applied to lower-level programs, and rewarded recursively based on lower-level performance.
Reference: [41] <author> J. Schmidhuber. </author> <title> A neural network that embeds its own meta-levels. </title> <booktitle> In Proc. of the International Conference on Neural Networks '93, </booktitle> <address> San Francisco. </address> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: Higher-level pools contain program-modifying programs which are applied to lower-level programs, and rewarded recursively based on lower-level performance. Unfortunately there is no theoretical assurance that meta-evolution will spend overall computation time wisely. Self-modifying recurrent neural nets <ref> [18, 41] </ref> are able to run their own weight change algorithms. Activations of special output units are used to address, read, and modify all of the network's own weights.

References-found: 41

