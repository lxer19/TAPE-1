URL: http://www.caip.rutgers.edu/~yuk/papers/asru97.ps
Refering-URL: 
Root-URL: 
Title: Automatic Speech Recognition and Understanding  
Affiliation: Santa Babara, California  
Date: December 14 17, 1997  
Note: 1997 IEEE Workshop on  pp. 474 481  
Abstract-found: 0
Intro-found: 1
Reference: [Barbier91] <author> L. Barbier, G. Chollet, </author> <title> "Robust Speech Parameters Extraction for Word Recognition in Noise Using Neural Networks", </title> <booktitle> IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 145-148, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Furthermore, since the multi-layer neural network is known to be able to compute nonlinear mapping functions [Lippmann87], the neural network method is able to handle nonlinear distortion. Similar approaches have been used for noise reduction [Tamura88], isolated word recognition using dynamic time warping (DTW) <ref> [Barbier91] </ref>, and speaker normalization [Huang92]. Tamura and Waibel used raw speech samples as the input to the network, and the network was trained to reduce the effect of noise in a noisy speech signal.
Reference: [Huang92] <author> X. Huang, </author> <title> "Speaker Normalization for Speech Recognition", </title> <booktitle> IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 465-468, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Furthermore, since the multi-layer neural network is known to be able to compute nonlinear mapping functions [Lippmann87], the neural network method is able to handle nonlinear distortion. Similar approaches have been used for noise reduction [Tamura88], isolated word recognition using dynamic time warping (DTW) [Barbier91], and speaker normalization <ref> [Huang92] </ref>. Tamura and Waibel used raw speech samples as the input to the network, and the network was trained to reduce the effect of noise in a noisy speech signal.
Reference: [Leggetter94] <author> C. Leggetter, P. Woodland, </author> <title> "Speaker Adaptation of HMMs Using Linear Regression", </title> <type> Technical Report 181, </type> <institution> Cambridge University Engineering Department, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: The last column is the result of MLLR <ref> [Leggetter94] </ref> adaptation, which is one of the widely used adaptation methods.
Reference: [Lin96] <author> Q. Lin, C. Che, D. Yuk, L. Jin, B. Vries, J. Pearson, J. Flanagan, </author> <title> "Robust Distant Talking Speech Recognition", </title> <booktitle> IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 21-24, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Source to microphone distance was 12 feet in a computer laboratory room having 0.2 seconds reverberation time and 68 dBC noise level. To aid sound quality, a microphone array is used for sound pick-up <ref> [Lin96] </ref>. The recognizer uses a word-pair grammar, and the average perplexity of this grammar on the 75 test utterances has been measured as 35. Table 1 shows the word recognition accuracy for the baseline system.
Reference: [Lippmann87] <author> R. Lippmann, </author> <title> "An introduction to computing with neural nets", </title> <journal> IEEE ASSP Magazine, </journal> <pages> pp. 4-22, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: The advantage of the neural network based transformation approach is that it does not require any prior knowledge about the distortion. Furthermore, since the multi-layer neural network is known to be able to compute nonlinear mapping functions <ref> [Lippmann87] </ref>, the neural network method is able to handle nonlinear distortion. Similar approaches have been used for noise reduction [Tamura88], isolated word recognition using dynamic time warping (DTW) [Barbier91], and speaker normalization [Huang92].
Reference: [Mitchell97] <author> T. Mitchell, </author> <title> Machine Learning, </title> <publisher> McGraw-Hill, </publisher> <year> 1997. </year>
Reference-contexts: It is known that equation (1) maximizes the likelihood of network output given the target if the target value is distorted by Gaussian noise <ref> [Mitchell97] </ref>. However, we do not impose such assumption, and directly maximize the conditional probability of each state. The error backprogation algorithm [Rumelhart86] can still be used with the new objective function.
Reference: [Rabiner93] <author> L. Rabiner, B. Juang, </author> <title> Fundamentals of Speech Recognition, </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: Huang used a codeword dependent neural network to transform speech data between two speakers to improve speaker independent recognition performance. In this paper, the neural network based transformation methods are used for large vocabulary continuous speech recognition using continuous density hidden Markov models (HMM) <ref> [Rabiner93] </ref>. In section 2, the neural network based feature transformation method which makes use of simultaneously recorded data, so-called stereo data, is explained, and its performance upper bound is discussed. <p> On the other hand, continuous speech recognition is accomplished by finding the word sequence which gives the highest score of a Viterbi path <ref> [Rabiner93] </ref>.
Reference: [Rumelhart86] <author> D. Rumelhart, G. Hinton, R. Williams, </author> <title> "Learning Internal Representations by Error Propagation" in D. </title> <editor> Rumelhart, J. McClelland, (Eds), </editor> <booktitle> Parallel Distributed Processing: Exploration in the Micro-Structure of Cognition, </booktitle> <volume> Vol. 1, </volume> <publisher> MIT Press, </publisher> <pages> pp. 318-362, </pages> <year> 1986. </year>
Reference-contexts: It is known that equation (1) maximizes the likelihood of network output given the target if the target value is distorted by Gaussian noise [Mitchell97]. However, we do not impose such assumption, and directly maximize the conditional probability of each state. The error backprogation algorithm <ref> [Rumelhart86] </ref> can still be used with the new objective function. Since this neural network maximizes the output probability of each state, and eventually maximizes the likelihood of the observation for the given HMMs, we call it the maximum likelihood neural network (MLNN). <p> rule can be derived by differentiating the logarithm of equation (2) with respect to weight w ij (connection between output node i and hidden node j): @ ln P (ojq) = @o i @w ij where the second term is the same as in any other error back propagation algorithm <ref> [Rumelhart86] </ref>. <p> Talking Speech (CDHMM) Neural Networks Distant Talking Speech Recognized Speech Microphone Array Distant Talking Speech HMM Parameters Close Talking Speech Input Target matrix case: @ ln P (ojq) = P (ojq) m o i q;m;i q;m;i This weighted difference is propagated from the output layer using the error backpropagation algorithm <ref> [Rumelhart86] </ref> to optimize the neural network. When the network is trained using 10 utterances of distorted speech data collected under a similar environment as the test data, the mean transformation MLNN system achieves 75.9% word recognition accuracy.
Reference: [Tamura88] <author> S. Tamura, A. Waibel, </author> <title> "Noise Reduction Using Connectionist Models", </title> <booktitle> IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 553-556, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: Furthermore, since the multi-layer neural network is known to be able to compute nonlinear mapping functions [Lippmann87], the neural network method is able to handle nonlinear distortion. Similar approaches have been used for noise reduction <ref> [Tamura88] </ref>, isolated word recognition using dynamic time warping (DTW) [Barbier91], and speaker normalization [Huang92]. Tamura and Waibel used raw speech samples as the input to the network, and the network was trained to reduce the effect of noise in a noisy speech signal.
Reference: [Woodland96] <author> P. Woodland, M. Gales, D. Pye, </author> <title> "Improving Environmental Robustness in Large Vocabulary Speech Recognition", </title> <booktitle> IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 65-68, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: systems. "clean" is the clean close-talking speech test using the recognizer trained on clean close-talking speech. "retrained" is the distant-talking speech test using the recognizer trained on distant-talking speech. "distorted" is the distant-talking speech test using the recognizer trained on clean close-talking speech. condition using the single pass retraining algorithm <ref> [Woodland96] </ref>, its accuracy goes up to 87.3%. The baseline speech recognizer trained on clean close-talking speech is used for the experiments throughout the rest of this paper, unless stated otherwise. 2.2 Feature Transformation Neural Network robust speech recognition.
Reference: [Yuk96] <author> D. Yuk, C. Che, L. Jin, Q. Lin, </author> <title> "Environment-Independent Continuous Speech Recognition Using Neural Networks and Hidden Markov Models", </title> <booktitle> IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> Vol. 6, </volume> <pages> pp. 3358-3361, </pages> <month> May </month> <year> 1996. </year> <month> 8 </month>
Reference-contexts: In section 4, the application of this neural network to feature transformation and model transformation is discussed. 2 Feature Transformation Neural Network Us ing Stereo Data The details of the feature transformation neural network are discussed in <ref> [Yuk96] </ref>. <p> Since the distorted speech feature vectors are transformed to clean speech feature vectors, it may outperform a retrained recognizer. Therefore, the performance upper bound of the system is not constrained to that of the retrained recognizer but the matched clean speech recognizer <ref> [Yuk96] </ref>. 3 Maximum Likelihood Neural Network The error criterion of neural networks is traditionally represented by the mean squared error, and that of HMM recognizers is represented by word recognition error. There is an inconsistency between the error criteria of these two components.
References-found: 11

