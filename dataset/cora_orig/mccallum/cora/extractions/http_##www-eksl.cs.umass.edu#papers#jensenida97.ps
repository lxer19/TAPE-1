URL: http://www-eksl.cs.umass.edu/papers/jensenida97.ps
Refering-URL: http://eksl-www.cs.umass.edu/publications.html
Root-URL: 
Email: @cs.umass.edu  
Title: f g pruning Abstract.  Building Simple Models: A Case Study with Decision Trees  
Author: jensen|oates|cohen 
Abstract-found: 0
Intro-found: 1
Reference: <institution> breast cancer lymphography breast-cancer-wisc Classification and Regression Trees Empirical Methods for Artificial Intelligence Preliminary Papers of the Sixth International Workshop on Artificial Intelligence and Statistics Proceedings of the First International Conference on Knowledge Discovery and Data Mining Applied Statistics Proceedings of the Tenth National Conference on Artificial Intelligence C4.5 : programs for machine learning International Journal of Man-Machine Studies Information and Computation Machine Learning </institution>
Reference: 1. <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. . Wadsworth International, </author> <year> 1984. </year>
Reference-contexts: Using these models requires the collection of unnecessary data. Portions of these models are wrong and mislead users. Finally, excess structure can reduce the accuracy of induced models on new data [8]. For induction algorithms that build decision trees <ref> [1, 7, 10] </ref>, is a common approach to remove excess structure. Pruning methods take an induced tree, examine individual subtrees, and remove those subtrees deemed unnecessary. Pruning methods differ primarily in the criterion used to judge subtrees. <p> The pruning methods are error-based ( the default) [7], reduced error ( ) [8], minimum description length ( ) [9], and cost-complexity with the rule ( ) <ref> [1] </ref>.
Reference: 2. <editor> Paul R. Cohen. </editor> . <publisher> The MIT Press, </publisher> <address> Cambridge, </address> <year> 1995. </year>
Reference-contexts: A dataset, , with instances is divided into disjoint sets, , each containing instances. Then for 1 , a tree is built on the instances in and tested on the instances in , and the results are averaged over all folds <ref> [2] </ref>. That procedure was augmented for this paper by building trees on subsets of of various sizes, and testing them on .
Reference: 3. <author> Paul R. Cohen and David Jensen. </author> <title> Overfitting explained. </title> <booktitle> In , pages 115-122, </booktitle> <year> 1997. </year>
Reference-contexts: Then, ( 4) = 3 7 = 0 43 The distribution of ( 4) underestimates by almost half its value. In comparison, ( ( ) 4) = 0 67. We have examined this relationship, and how it leads to excess structure, in greater detail elsewhere <ref> [3] </ref>. This analysis applies to , , and . However, cost-complexity pruning ( ) is unaffected by the difference in the distributions of and .
Reference: 4. <author> George H. John. </author> <title> Robust decision trees: Removing outliers from databases. </title> <booktitle> In , 1995. </booktitle>
Reference-contexts: All reported accuracy figures in this paper are based on separate test sets, distinct from any data used for training. The datasets are the same ones used in <ref> [4] </ref> with two exceptions. The dataset was omitted because it is roughly the same as the dataset, and the dataset was omitted because it was unclear which attribute was used as the class label. Note that the dataset was created by removing the attribute from the dataset.
Reference: 5. <author> G.V. Kass. </author> <title> An exploratory technique for investigating large quantities of categorical data. </title> , <booktitle> 29(2) </booktitle> <pages> 199-127, </pages> <year> 1980. </year>
Reference-contexts: and is the total of all cells in the table. 1 P tba tba discrete k i continuous k j k G r c j k i n k ff Selecting partitions: Selecting attributes: Pruning: within among During tree construction, attribute partitions are selected using an approach suggested by Kass <ref> [5] </ref> and Kerber [6]. For each attribute, a contingency table is constructed with a row for each class value and a column for each of attribute values | every possible value for discrete attributes or every unique interval for discretized continuous attributes.
Reference: 6. <author> Randy Kerber. Chimerge: </author> <title> Discretization of numeric attributes. </title> <publisher> In . MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: total of all cells in the table. 1 P tba tba discrete k i continuous k j k G r c j k i n k ff Selecting partitions: Selecting attributes: Pruning: within among During tree construction, attribute partitions are selected using an approach suggested by Kass [5] and Kerber <ref> [6] </ref>. For each attribute, a contingency table is constructed with a row for each class value and a column for each of attribute values | every possible value for discrete attributes or every unique interval for discretized continuous attributes.
Reference: 7. <author> J. R. </author> <title> Quinlan. </title> . <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1993. </year>
Reference-contexts: Using these models requires the collection of unnecessary data. Portions of these models are wrong and mislead users. Finally, excess structure can reduce the accuracy of induced models on new data [8]. For induction algorithms that build decision trees <ref> [1, 7, 10] </ref>, is a common approach to remove excess structure. Pruning methods take an induced tree, examine individual subtrees, and remove those subtrees deemed unnecessary. Pruning methods differ primarily in the criterion used to judge subtrees. <p> Pruning methods take an induced tree, examine individual subtrees, and remove those subtrees deemed unnecessary. Pruning methods differ primarily in the criterion used to judge subtrees. Many criteria have been proposed, including statistical significance tests [10], corrected error estimates <ref> [7] </ref>, and minimum description length calculations [9]. In this paper, we bring together three threads of our research on excess structure and decision tree pruning. First, we show that several common methods for pruning decision trees still retain excess structure. <p> On the left-hand side, no training instances are available and the best one can do with test instances is to assign them a class label at random. On the right-hand side, the entire dataset (excluding test instances) is available to the tree building process. <ref> [7] </ref> and error-based pruning (the default) are used to build and prune trees, respectively. Note that accuracy on this dataset stops increasing at a rather small training set size, thereafter remaining essentially constant. Surprisingly, tree size continues to grow nearly linearly despite the use of error-based pruning. <p> The relationship between training set size and tree size was explored with 4 pruning methods and 19 datasets taken from the UCI repository. The pruning methods are error-based ( the default) <ref> [7] </ref>, reduced error ( ) [8], minimum description length ( ) [9], and cost-complexity with the rule ( ) [1].
Reference: 8. <author> J. Ross Quinlan. </author> <title> Simplifying decision trees. </title> , <booktitle> 27 </booktitle> <pages> 221-234, </pages> <year> 1987. </year>
Reference-contexts: Such models are less efficient to store and use than their correctly-sized counterparts. Using these models requires the collection of unnecessary data. Portions of these models are wrong and mislead users. Finally, excess structure can reduce the accuracy of induced models on new data <ref> [8] </ref>. For induction algorithms that build decision trees [1, 7, 10], is a common approach to remove excess structure. Pruning methods take an induced tree, examine individual subtrees, and remove those subtrees deemed unnecessary. Pruning methods differ primarily in the criterion used to judge subtrees. <p> The relationship between training set size and tree size was explored with 4 pruning methods and 19 datasets taken from the UCI repository. The pruning methods are error-based ( the default) [7], reduced error ( ) <ref> [8] </ref>, minimum description length ( ) [9], and cost-complexity with the rule ( ) [1].
Reference: 9. <author> J. Ross Quinlan and R. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> , <booktitle> 80 </booktitle> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: Pruning methods take an induced tree, examine individual subtrees, and remove those subtrees deemed unnecessary. Pruning methods differ primarily in the criterion used to judge subtrees. Many criteria have been proposed, including statistical significance tests [10], corrected error estimates [7], and minimum description length calculations <ref> [9] </ref>. In this paper, we bring together three threads of our research on excess structure and decision tree pruning. First, we show that several common methods for pruning decision trees still retain excess structure. Second, we explain this phenomenon in terms of statistical decision making with incorrect reference distributions. <p> The relationship between training set size and tree size was explored with 4 pruning methods and 19 datasets taken from the UCI repository. The pruning methods are error-based ( the default) [7], reduced error ( ) [8], minimum description length ( ) <ref> [9] </ref>, and cost-complexity with the rule ( ) [1].

References-found: 10

