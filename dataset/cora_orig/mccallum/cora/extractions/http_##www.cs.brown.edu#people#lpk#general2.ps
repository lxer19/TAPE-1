URL: http://www.cs.brown.edu/people/lpk/general2.ps
Refering-URL: http://www.cs.brown.edu/people/lpk/
Root-URL: 
Title: Associative Reinforcement Learning: A Generate and Test Algorithm  
Author: LESLIE PACK KAELBLING 
Keyword: reinforcement learning, generalization, generate-and-test  
Address: Box  Providence, RI 02912-1910 USA  
Affiliation: Computer Science Department  Brown University  
Note: Small Journal Name, 1-20 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: lpk@cs.brown.edu  Editor:  
Date: 1910  
Abstract: An agent that must learn to act in the world by trial and error faces the reinforcement learning problem, which is quite different from standard concept learning. Although good algorithms exist for this problem in the general case, they are often quite inefficient and do not exhibit generalization. One strategy is to find restricted classes of action policies that can be learned more efficiently. This paper pursues that strategy by developing an algorithm that performans an on-line search through the space of action mappings, expressed as Boolean formulae. The algorithm is compared with existing methods in empirical trials and is shown to have very good performance. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Duda, R. O., Gaschnig, J., & Hart, P. E. </author> <year> (1979). </year> <title> Model design in the Prospector consultant system for mineral exploration. </title> <editor> In D. Michie (Ed.), </editor> <booktitle> Expert Systems in the Micro Electronic Age. </booktitle> <address> Edinburgh, U.K.: </address> <publisher> Edinburgh University Press. </publisher>
Reference: <author> Duda, R. O., Hart, P. E., & Nilsson, N. J. </author> <year> (1976). </year> <title> Subjective Bayesian Methods for Rule-Based Inference Systems. </title> <type> Technical Report 124, </type> <institution> Artificial Intelligence Center, SRI International, Menlo Park, California. </institution>
Reference: <author> Kaelbling, L. P. </author> <year> (1993a). </year> <title> Associative reinforcement learning: Functions in k-DNF. </title> <journal> Machine Learning. </journal> <note> To appear. </note>
Reference-contexts: An important issue in reinforcement learning is the problem of exploration: especially when the results of actions are stochastic, it may be necessary for the agent to take actions that do not appear to be good in order to gain more information about their effects. A companion paper <ref> (Kaelbling, 1993a) </ref> explores the background of reinforcement learning and presents a number of algorithms for solving a reinforcement-learning problem. These include the IE algorithm, which gathers statistics about the results of every action in every situation and chooses actions based on a tradeoff between exploration and exploitation. <p> The success of an algorithm is measured by the average reinforcement received per tick, averaged over the entire run. The other algorithms' parameters were also tuned experimentally <ref> (Kaelbling, 1993a) </ref>. For each task, a series of 100 trials of length 3000 were run with different parameter values. Table 2 shows the best set of parameter values found for each task. 16 KAELBLING Table 3. Average reinforcement for tasks over 100 runs of length 3000.
Reference: <author> Kaelbling, L. P. </author> <year> (1993b). </year> <title> Learning in Embedded Systems. </title> <address> Cambridge, Massachusetts: </address> <note> The MIT Press. Also available as a PhD Thesis from Stanford University, </note> <year> 1990. </year>
Reference-contexts: The algorithms for updating the statistical information and computing statistical quantities are modularly separated from the rest of the GTRL algorithm. The choice of statistical module will depend on the distribution of reinforcement values received from the environment. A more detailed description of this work <ref> (Kaelbling, 1993b) </ref> provides definitions of statistics modules for cases in which the reinforcement values are binomially or normally distributed; in addition, it contains a non-parametric statistics module for use when there is no known model of the distribution of reinforcement values. <p> The GTRL algorithm currently uses a very simple simplification process whose complexity is linear in the original size of the formula and that seems, empirically, to work well. This simplification process is described in detail in elsewhere <ref> (Kaelbling, 1993b) </ref>.
Reference: <author> Saxena, S. </author> <year> (1991). </year> <title> Predicting the Effect of Instance Representations on Inductive Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Amherst, Massachusetts. </institution>
Reference: <author> Schlimmer, J. C. </author> <year> (1987). </year> <title> Concept Acquisition Through Representational Adjustment. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> Irvine, Irvine, California. </address>
Reference: <author> Schlimmer, J. C. & Granger, Jr., R. H. </author> <year> (1986). </year> <title> Incremental learning from noisy data. </title> <journal> Machine Learning, </journal> <volume> 1(3), </volume> <pages> 317354. </pages>
References-found: 7

