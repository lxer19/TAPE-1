URL: http://www.physics.utah.edu/~basko/root/home/grad/condella/classwork/ma575/notes/pvm_system.ps.Z
Refering-URL: http://www.physics.utah.edu/~basko/root/home/grad/condella/classwork/ma575/notes/
Root-URL: 
Title: PVM A Framework for Parallel Distributed Computing*  
Author: V. S. Sunderam 
Address: Atlanta, GA 30322  
Affiliation: Department of Math and Computer Science Emory University,  
Abstract: The PVM system is a programming environment for the development and execution of large concurrent or parallel applications that consist of many interacting, but relatively independent, components. It is intended to operate on a collection of heterogeneous computing elements interconnected by one or more networks. The participating processors may be scalar machines, multiprocessors, or special-purpose computers, enabling application components to execute on the architecture most appropriate to the algorithm. PVM provides a straightforward and general interface that permits the description of various types of algorithms (and their interactions), while the underlying infrastructure permits the execution of applications on a virtual computing environment that supports multiple parallel computation models. PVM contains facilities for concurrent, sequential, or conditional execution of application components, is portable to a variety of architectures, and supports certain forms of error detection and recovery. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Hwang, F. A. Briggs, </author> <title> Computer Architecture and Parallel Processing, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: Significant advances in parallel algorithms and architectures have demonstrated the potential for applying concurrent computation techniques to a wide variety of problems. However, most of the research efforts have concentrated either upon computational models <ref> [1] </ref>, parallel versions of algorithms, or machine architectures; relatively little attention has been given to software development environments or program construction techniques that are required in order to translate algorithms into operational programs.
Reference: [2] <author> H. Narang, R. Flanery, J. Drake, </author> <title> Design of a Simulation Interface for a Parallel Computing Environment, </title> <booktitle> Proc. ACM Southeastern Regional Conference, </booktitle> <month> April </month> <year> 1990, </year> <note> to appear. </note>
Reference-contexts: However, typical multiprocessors rarely support high-bandwidth external I/O or high performance graphics, thereby necessitating the separation of the computation, output management, and graphical display components of the system. Another example of an application with differing requirements for individual sub-algorithms is the Global Environment Simulation project <ref> [2] </ref>, a large simulation effort to study contaminant concentrations and dispersal characteristics as a function of various environmental factors. <p> Contaminant Transport Simulation To further exercise the heterogeneous computing facilities in PVM, an interactive simulation program was ported to the system. This simulation models contaminant transport in groundwater flow, and is part of a larger and more comprehensive simulation effort. Preliminary findings from this work are reported in <ref> [2] </ref>. The major components of the existing software are a graphical interface and the solution of sets of differential equations involving transport calculations and flow calculations. The transport calculations are highly parallelizable, while the flow equations may be well solved on a fast scalar machine.
Reference: [3] <author> G. A. Geist, M. T. Heath, B. W. Peyton, P. H. Worley, </author> <title> A Machine Independent Communication Library, </title> <booktitle> Proc. Hypercube Concurrent Computers Conference 1989, </booktitle> <institution> J. </institution> <note> Gustafson ed., to appear. </note>
Reference-contexts: It should be mentioned that the PVM user-interface primitives have been partly derived from and are a superset of the portable programming constructs described in <ref> [3] </ref>; an application written using these primitives may therefore also execute directly on a specific multiprocessor when necessary. The PVM system consists of support software that executes on participating hosts on a network; the network may be local, wide-area or a combination, and the host pool may be varied dynamically. <p> Data Transfer and Barrier Synchronization Inter-process communication via message passing is one of the basic facilities supported by PVM. In the interest of portability and wide applicability, the primitives to accomplish message transfer have been derived from existing implementations (e.g [9]), including those described in <ref> [3] </ref>. Certain aspects, however, are necessarily different; primary among them is addressing. Since the physical location of processes is deliberately transparent to user programs, message destinations are identified by a -component name, instance number- pair.
Reference: [4] <author> T. J. Gardner, et. al., </author> <title> DPUP : A Distributed Processing Utilities Package, </title> <institution> Computer Science Technical Report, University of Colorado, Boulder, </institution> <year> 1986. </year>
Reference-contexts: Several projects similar to PVM have been undertaken in the past, and some are ongoing. A few representative examples are listed below, with comparisons to PVM. The DPUP library <ref> [4] </ref> emulates a loosely coupled multiprocessor on a local network, as does the dsim [5] system and the Cosmic environment [6]. The two latter systems require the preconfiguration of a virtual machine on which applications execute and support only basic message passing mechanisms. <p> */ /*-*/ initsend (); /* Initialize send buffer */ putstring ("The square root of "); /* Store values in */ putint (2); /* machine independent */ putstring ("is "); /* form */ putfloat (1.414); send ("receiver",4,99); /* Instance 4; type 99 */ /* Receiving Process */ /*-*/ char msg1 [32],msg2 <ref> [4] </ref>; int num; float sqnum; recv (99); /* Receive msg of type 99 */ getstring (msg1); /* Extract values in */ getint (&num); /* a machine specific */ getstring (msg2); /* manner */ getfloat (&sqnum); In order for a receiving process to obtain additional information about the most recently received message,
Reference: [5] <author> T. H. Dunigan, </author> <title> Hypercube Simulation on a Local Area Network, </title> <institution> Oak Ridge National Laboratory Report ORNL/TM-10685, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: Several projects similar to PVM have been undertaken in the past, and some are ongoing. A few representative examples are listed below, with comparisons to PVM. The DPUP library [4] emulates a loosely coupled multiprocessor on a local network, as does the dsim <ref> [5] </ref> system and the Cosmic environment [6]. The two latter systems require the preconfiguration of a virtual machine on which applications execute and support only basic message passing mechanisms. The Amber project [15] is somewhat different in that the targeted environment is a collection of homogeneous multiprocessors. <p> It should also be pointed out that these figures are 2 to 4 times better than those for other distributed multiprocessor simulators such as dsim <ref> [5] </ref>. Furthermore, the factorization program was built for performance measurement purposes and therefore internally generated the matrix elements and did not output the factorized results.
Reference: [6] <author> C. Seitz, J. Seizovic, W. K. Su, </author> <title> The C programmer's Abbreviated Guide to Multicomputer Programming, </title> <institution> Caltech Computer Science Report, CS-TR-88-1, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: Several projects similar to PVM have been undertaken in the past, and some are ongoing. A few representative examples are listed below, with comparisons to PVM. The DPUP library [4] emulates a loosely coupled multiprocessor on a local network, as does the dsim [5] system and the Cosmic environment <ref> [6] </ref>. The two latter systems require the preconfiguration of a virtual machine on which applications execute and support only basic message passing mechanisms. The Amber project [15] is somewhat different in that the targeted environment is a collection of homogeneous multiprocessors.
Reference: [7] <author> M. Sullivan, D. Anderson, Marionette: </author> <title> A System for Parallel Distributed Programming Using a Master/Slave Model, </title> <booktitle> Proc. 9th ICDCS, </booktitle> <month> June </month> <year> 1989, </year> <pages> pp. 181-188. </pages>
Reference-contexts: The Amber project [15] is somewhat different in that the targeted environment is a collection of homogeneous multiprocessors. One of the operating modes within DPUP, as well as projects such as Marionette <ref> [7] </ref> and MSPCM [8], uses the master-slave approach, where a central controlling process is responsible for, or is involved in, every system event.
Reference: [8] <author> G. Riccardi, B. Traversat, U. Chandra, </author> <title> A Master-Slaves Parallel Computation Model, </title> <institution> Supercomputer Research Institute Report, Florida State University, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: The Amber project [15] is somewhat different in that the targeted environment is a collection of homogeneous multiprocessors. One of the operating modes within DPUP, as well as projects such as Marionette [7] and MSPCM <ref> [8] </ref>, uses the master-slave approach, where a central controlling process is responsible for, or is involved in, every system event.
Reference: [9] <institution> Intel iPSC/2 Programmer's Reference Manual, Intel Corporation, Beaverton, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: Data Transfer and Barrier Synchronization Inter-process communication via message passing is one of the basic facilities supported by PVM. In the interest of portability and wide applicability, the primitives to accomplish message transfer have been derived from existing implementations (e.g <ref> [9] </ref>), including those described in [3]. Certain aspects, however, are necessarily different; primary among them is addressing. Since the physical location of processes is deliberately transparent to user programs, message destinations are identified by a -component name, instance number- pair.
Reference: [10] <author> A. Karp, </author> <title> Programming for Parallelism, </title> <booktitle> IEEE Computer, </booktitle> <month> May </month> <year> 1987, </year> <pages> pp. 43-57. </pages>
Reference-contexts: In such situations, live processes return from a barrier call with a negative result value. In addition to barriers, or as an alternative, the waituntil construct is also provided as a means of synchronization. This construct (suggested in <ref> [10] </ref>) takes an event name as an argument and blocks until another process indicates the occurrence of that event by using the ready primitive mentioned earlier. 2.3.
Reference: [11] <author> J. B. Postel, </author> <title> User Datagram Protocol, Internet Request for Comments RFC-768, </title> <month> August </month> <year> 1980. </year>
Reference-contexts: In the test implementations of PVM, the UDP <ref> [11] </ref> protocol was used; this deliberate choice of a simple datagram protocol also permits relatively simple porting or protocol conversion when PVM is to be installed under a different operating system environment. Across the network, pvmd processes communicate using UDP datagrams.
Reference: [12] <author> N. Maekawa, </author> <title> A dd n Algorithm for Mutual Exclusion in Decentralized Systems, </title> <journal> ACM Transactions on Computer Systems, </journal> <month> May </month> <year> 1985, </year> <pages> pp. 145-159. </pages>
Reference-contexts: Different strategies, varying in their approach, efficiency, and level of failure resiliency have been proposed and representative methods are described in <ref> [12, 13, 14] </ref>. The strategy adopted in PVM is somewhat different from these approaches, but the - 14 - algorithm is efficient and, more importantly, is integrated with the required distribution to all pvmds of lock location information.
Reference: [13] <author> K. Raymond, </author> <title> A Tree Based Algorithm for Distributed Mutual Exclusion, </title> <journal> ACM Transactions on Computer Systems, </journal> <month> February </month> <year> 1989, </year> <pages> pp. 61-77. </pages>
Reference-contexts: Different strategies, varying in their approach, efficiency, and level of failure resiliency have been proposed and representative methods are described in <ref> [12, 13, 14] </ref>. The strategy adopted in PVM is somewhat different from these approaches, but the - 14 - algorithm is efficient and, more importantly, is integrated with the required distribution to all pvmds of lock location information.
Reference: [14] <author> D. Agarwal, A. E. Abbadi, </author> <title> An Efficient Solution to the Distributed Mutual Exclusion Problem, </title> <booktitle> Proc. Principles of Distributed Computing Conference, </booktitle> <address> Edmonton, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Different strategies, varying in their approach, efficiency, and level of failure resiliency have been proposed and representative methods are described in <ref> [12, 13, 14] </ref>. The strategy adopted in PVM is somewhat different from these approaches, but the - 14 - algorithm is efficient and, more importantly, is integrated with the required distribution to all pvmds of lock location information.
Reference: [15] <author> J. S. Chase et. al., </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors, </title> <note> to appear in 12th SOSP, </note> <institution> Litchfield Park, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: The DPUP library [4] emulates a loosely coupled multiprocessor on a local network, as does the dsim [5] system and the Cosmic environment [6]. The two latter systems require the preconfiguration of a virtual machine on which applications execute and support only basic message passing mechanisms. The Amber project <ref> [15] </ref> is somewhat different in that the targeted environment is a collection of homogeneous multiprocessors.
Reference: [16] <author> G. A. Geist, M. T. Heath, </author> <title> Matrix Factorization on Hypercube Multiprocessors, in Hypercube Multiprocessors 1986, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1986, </year> <pages> pp. 161-180. </pages>
Reference-contexts: Furthermore, the performance ratio between PVM and the iPSC/2 multiprocessor for this problem was constant and consistent with the inherent processor speed differences. The second application is Chole- sky matrix factorization <ref> [16] </ref> an application that has a relatively high communication to computation ratio. Table 4 shows the elapsed times for this problem run on a network of Sun 3/50 machines for varying problem sizes.
Reference: [17] <author> D. W. Lozier, R. G. Rehm, </author> <title> Some Performance Comparisons for a Fluid Dynamics Code, </title> <journal> Parallel Computing, </journal> <volume> Vol. 11, </volume> <pages> pp. 305-320, </pages> <year> 1989. </year>
Reference-contexts: These requirements are often diverse enough that it is difficult to program them within a unified framework, and cumbersome, and sometime impractical, to execute the system on a single machine. An example of such a system is the fluid dynamics application termed BF3D, described in <ref> [17] </ref>. This application is not only compute intensive; it uses large amounts of memory, creates very large quantities of output data, and requires 2D and 3D graphics terminals to display results.
Reference: [18] <author> G. C. Fox, S. W. Otto, </author> <title> Matrix Algorithms on a Hypercube I: Matrix Multiplication, </title> <journal> Parallel Computing, </journal> <volume> Vol. 4, </volume> <pages> pp. 17-31, </pages> <year> 1987. </year>
Reference-contexts: A straightforward, but illustrative example is matrix multiplication using subblock decompositions <ref> [18] </ref>. In an environment consisting of different types of scalar machines and multiprocessors, highly effective and efficient algorithms for matrix multiplication can be implemented, with some subblocks being multiplied using static prescheduling on shared memory machines and others using "pipe-multiply-roll" strategies on distributed memory computers. <p> On distributed memory machines, matrices are decomposed into subblocks and multiplied; and a regular communication pattern between the processing elements helps minimize the overheads. A detailed description and analysis of block matrix multiplication on hypercube architectures may be found in <ref> [18] </ref>. A modified version of the block matrix multiplication algorithm was implemented on the PVM system and executed on various hardware combinations.
Reference: [19] <author> A. Osterhaug, </author> <title> Guide to Parallel Programming on Sequent Computer Systems, Sequent Computer Systems, </title> <publisher> Inc., </publisher> <address> Beaverton, </address> <year> 1986. </year>
Reference-contexts: Once again, a component that executes on such a machine would be programmed to use internal locking constructs where appropriate, and PVM locking facilities for resource sharing between other components that executed on other architectures. An example code skeleton depicting this situation on a Sequent shared memory multiprocessor <ref> [19] </ref> is shown in Figure 6. 2.4. Miscellaneous Facilities In addition to the primary constructs described in the preceding sections, a few miscellaneous constructs are also provided. The status construct takes a component name and instance number as arguments and returns status and location information regarding that component.
Reference: [20] <author> R. Finkel, E. Styer, U. Manber, </author> <title> Designing Efficient Barriers in Communication Networks, </title> <journal> University of Kentucky Technical Report, </journal> <volume> No. </volume> <pages> 165-90, </pages> <month> March </month> <year> 1990. </year> <month> - 25 </month> - 
Reference-contexts: should be noted that creation, locking, unlocking, and deallocation (resulting in file removal) events are broadcast to all pvmds; given the conflict resolution rules and highest priority to incoming requests, undesirable inconsistencies are avoided. - 16 - Barrier synchronization in PVM is accomplished by using an efficient algorithm described in <ref> [20] </ref>. The algorithm considers the pvmd processes as being the vertices in a logical quadratic network.
References-found: 20

