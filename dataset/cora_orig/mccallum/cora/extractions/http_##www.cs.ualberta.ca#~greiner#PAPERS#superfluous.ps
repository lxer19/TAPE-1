URL: http://www.cs.ualberta.ca/~greiner/PAPERS/superfluous.ps
Refering-URL: http://www.cs.ualberta.ca/~greiner/PAPERS/
Root-URL: 
Email: fbharat, greiner, hancockg@learning.scr.siemens.com  
Title: Exploiting the Absence of Irrelevant Information: What You Don't Know Can Help You  
Author: R. Bharat Rao, Russell Greiner and Thomas Hancock 
Date: November 1994.  
Note: Appears in the Proceedings of the AAAI Fall Symposium on "Relevance", New Orleans,  
Address: 755 College Road East Princeton, NJ 08540-6632  
Affiliation: Learning Systems Department Siemens Corporate Research  
Abstract: Most inductive inference algorithms are designed to work most effectively when their training data contain completely specified labeled samples. In many environments, however, the person collecting the data may record the values of only some of the attributes, and so provide the learner with only partially specified samples. This can be modeled as a blocking process that hides the values of certain attributes from the learner. While blockers that remove the values of critical attributes can handicap a learner, this paper instead focuses on blockers that remove only superfluous attribute values, i.e., values that are not needed to classify an instance, given the values of the other unblocked attributes. We first motivate and formalize this model of "superfluous-value blocking", and then demonstrate that these omissions can be quite useful, showing that a certain class that is seemingly hard to learn in the general PAC model | viz., decision trees | is trivial to learn in this setting. We also show how this model can be applied to the theory revision problem. 
Abstract-found: 1
Intro-found: 1
Reference: <author> H. Almuallim and T.G. Dietterich. </author> <title> Learning with many irrelevant features. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 547-552, </pages> <year> 1991. </year>
Reference: <author> L. Breiman, J. Friedman, J. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth and Brooks, </publisher> <address> Monterey, CA, </address> <year> 1984. </year>
Reference: <author> A. Blum, L. Hellerstein, and N. Littlestone. </author> <title> Learning in the presence of finitely or infinitely many irrelevant attributes. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 157-166. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA, </address> <year> 1991. </year>
Reference: <author> A. Blum. </author> <title> Learning boolean functions in an infinite attribute space. </title> <journal> Machine Learning, </journal> <volume> 9(4) </volume> <pages> 373-386, </pages> <month> October </month> <year> 1992. </year>
Reference: <author> A. Ehrenfeucht and D. Haussler. </author> <title> Learning decision trees from random examples. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 231-246, </pages> <year> 1989. </year>
Reference: <author> Russell Greiner, Thomas Hancock, and R. Bharat Rao. </author> <title> Knowing what doesn't matter: Exploiting (intentionally) omitted superfluous data. </title> <type> Technical report, </type> <institution> Siemens Corporate Research, </institution> <year> 1994. </year>
Reference: <author> D. Haussler, M. Kearns, N. Littlestone, and M. K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Inform. Comput., </journal> <volume> 95(2) </volume> <pages> 129-161, </pages> <month> December </month> <year> 1991. </year>
Reference: <author> G.H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 121-129, </pages> <year> 1994. </year>
Reference: <author> M. Kearns and M. Li. </author> <title> Learning in the presence of malicious errors. </title> <booktitle> In Proc. 20th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 267-280. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1988. </year>
Reference: <author> Michael Kearns, Ming Li, Leonard Pitt, and Leslie Valiant. </author> <title> On the learnability of boolean formulae. </title> <booktitle> In Proceedings of the 19th Symposium on the Theory of Computations, </booktitle> <pages> pages 285-295, </pages> <address> New York, </address> <month> May </month> <year> 1987. </year>
Reference: <author> Pat Langley, George Drastal, R. Bharat Rao, and Russ Greiner. </author> <title> Theory revision in fault hierarchies. </title> <type> Technical report, </type> <institution> SCR Techreport, Siemens Corporate Research, </institution> <year> 1994. </year>
Reference: <author> Nick Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning Journal, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference: <author> S. Muggleton and W. Buntine. </author> <title> Machine invention of first order predicates by inverting resolution. </title> <booktitle> In Proceedings of IML-88, </booktitle> <pages> pages 339-51. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference: <author> Dirk Ourston and Raymond J. Mooney. </author> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of AAAI-90, </booktitle> <pages> pages 815-20, </pages> <year> 1990. </year>
Reference: <author> B. W. Porter, R. Bareiss, and R. C. Holte. </author> <title> Concept learning and heuristic classification in weak-theory domains. </title> <journal> Artificial Intelligence, </journal> <volume> 45(1-2):229-63, </volume> <year> 1990. </year>
Reference: <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, </address> <year> 1992. </year>
Reference: <author> K. Ruberg, S.M. Cornick, and K.A. James. </author> <title> House calls: Building and maintaining a diagnostic rule-base. </title> <booktitle> In Proceedings Third Knowledge Acquistion for Knowledge-Based Systems Workshop, </booktitle> <year> 1988. </year>
Reference: <author> Dale Schuurmans and Russell Greiner. </author> <title> Learning default concepts. </title> <booktitle> In CSSCI-94, </booktitle> <year> 1994. </year>
Reference: <author> G. Shackelford and D. Volper. </author> <title> Learning k-DNF with noise in the attributes. </title> <booktitle> In Proc. 1st Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 97-103, </pages> <address> San Mateo, CA, </address> <year> 1988. </year> <note> published by Morgan Kaufmann. </note>
Reference: <author> Geoff Towell. </author> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement and Extraction. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1991. </year>
Reference: <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-42, </pages> <year> 1984. </year>
Reference: <author> James Wogulis and Michael J. Pazzani. </author> <title> A methodology for evaluating theory revision systems: Results with Audrey II. </title> <booktitle> In Proceedings of IJCAI-93, </booktitle> <year> 1993. </year>
References-found: 22

