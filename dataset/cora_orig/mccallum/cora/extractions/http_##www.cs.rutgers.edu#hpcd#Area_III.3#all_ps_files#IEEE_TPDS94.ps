URL: http://www.cs.rutgers.edu/hpcd/Area_III.3/all_ps_files/IEEE_TPDS94.ps
Refering-URL: http://www.cs.rutgers.edu/hpcd/Area_III.3/all_html_files/papers.html
Root-URL: http://www.cs.rutgers.edu
Email: tyang@cs.ucsb.edu  gerasoulis@cs.rutgers.edu  
Title: DSC: Scheduling Parallel Tasks on an Unbounded Number of Processors  
Author: Tao Yang Apostolos Gerasoulis 
Keyword: Index Terms Clustering, directed acyclic graph, heuristic algorithm, optimality, parallel processing, scheduling, task precedence.  
Address: Santa Barbara, CA 93106  New Brunswick, NJ 08903  
Affiliation: Department of Computer Science University of California  Department of Computer Science Rutgers University  
Note: To appear in IEEE Trans. on Parallel and Distributed Systems  
Abstract: We present a low complexity heuristic named the Dominant Sequence Clustering algorithm (DSC) for scheduling parallel tasks on an unbounded number of completely connected processors. The performance of DSC is comparable or even better on average than other higher complexity algorithms. We assume no task duplication and nonzero communication overhead between processors. Finding the optimum solution for arbitrary directed acyclic task graphs (DAGs) is NP-complete. DSC finds optimal schedules for special classes of DAGs such as fork, join, coarse grain trees and some fine grain trees. It guarantees a performance within a factor of two of the optimum for general coarse grain DAGs. We compare DSC with three higher complexity general scheduling algorithms: the ETF by Hwang, Chow, Anger and Lee [11], Sarkar's clustering algorithm [17] and the MD by Wu and Gajski [19]. We also give a sample of important practical applications where DSC has been found useful. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. A. Al-Mouhamed, </author> <title> "Lower bound on the number of processors and time for scheduling precedence 32 graphs with communication costs," </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> vol. 16, no. 12, </volume> <pages> pp. </pages> <address> 1390--1401, </address> <year> 1990. </year>
Reference-contexts: Chretienne [3] has proposed an optimal algorithm for a fork and join, which zeroes multiple edges. The complexity of his algorithm is O (m log B) where B = minf P m i=1 t i ; fi 1 + t 1 g + t x . Al-Mouhamed <ref> [1] </ref> has also used the idea of zeroing multiple incoming edges of a task to compute a lower bound for scheduling a DAG, using an O (m 2 ) algorithm, but no feasible schedules that reach the bound are produced by his algorithm.
Reference: [2] <author> F.D. Anger, J. Hwang, and Y. Chow, </author> <title> "Scheduling with sufficient loosely coupled processors," </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> vol. 9, </volume> <pages> pp. 87-92, </pages> <year> 1990. </year>
Reference-contexts: Only for special classes of DAGs, such as join, fork, and coarse grain tree, special polynomial algorithms are known, Chretienne [4], Anger, Hwang and Chow <ref> [2] </ref>. There have been two approaches in the literature addressing the general scheduling problem. The first approach considers heuristics for arbitrary DAGs and the second studies optimal algorithms for special classes of DAGs. <p> It can be verified that the condition g (G) 1 for the in-tree optimality of DSC can be relaxed as g (J x ) 1 for all join structures J x in this tree. Under the same or similar condition, Chretienne [4] and Anger, Hwang and Chow <ref> [2] </ref> developed O (v) optimal tree scheduling algorithms. These two algorithms are only specific to this kind of trees. For an out-tree, the forward clustering of DSC may not find the optimum directly.
Reference: [3] <author> P. Chretienne, </author> <title> "Task scheduling over distributed memory machines," </title> <booktitle> in Proc. of International Workshop on Parallel and Distributed Algorithms, </booktitle> <publisher> (North Holland, Ed.), </publisher> <year> 1989. </year>
Reference-contexts: The general problem is NP-complete and even for simple graphs such as fine grain trees or the concatenation of a fork and a join together the complexity is still NP-complete, Chretienne <ref> [3] </ref>, Papadimitriou and Yannakakis [15] and Sarkar [17]. Only for special classes of DAGs, such as join, fork, and coarse grain tree, special polynomial algorithms are known, Chretienne [4], Anger, Hwang and Chow [2]. There have been two approaches in the literature addressing the general scheduling problem. <p> To achieve such a greedy goal, a low complexity minimization procedure that zeroes multiple incoming edges of the selected free node is needed to be incorporated in the DSC algorithm. Chretienne <ref> [3] </ref> has proposed an optimal algorithm for a fork and join, which zeroes multiple edges. The complexity of his algorithm is O (m log B) where B = minf P m i=1 t i ; fi 1 + t 1 g + t x . <p> Since this scheduling problem is NP-complete for a general fine grain tree and for a DAG which is a concatenation of a fork and a join (series parallel DAG) <ref> [3, 5] </ref>, the analysis shows that DSC not only has a low complexity but also attains an optimality degree that a general polynomial algorithm could achieve. 5.1 Performance bounds for general DAGs A DAG consists of fork (F x ) and/or join (J x ) structures such as the ones shown
Reference: [4] <author> P. Chretienne, </author> <title> "A polynomial algorithm to optimially schedule tasks over an ideal distributed system under tree-like presedence constraints", </title> <journal> European J. of Operational Research, </journal> <volume> 2:43, </volume> <pages> pp. 225-230, </pages> <year> 1989. </year>
Reference-contexts: Only for special classes of DAGs, such as join, fork, and coarse grain tree, special polynomial algorithms are known, Chretienne <ref> [4] </ref>, Anger, Hwang and Chow [2]. There have been two approaches in the literature addressing the general scheduling problem. The first approach considers heuristics for arbitrary DAGs and the second studies optimal algorithms for special classes of DAGs. <p> It can be verified that the condition g (G) 1 for the in-tree optimality of DSC can be relaxed as g (J x ) 1 for all join structures J x in this tree. Under the same or similar condition, Chretienne <ref> [4] </ref> and Anger, Hwang and Chow [2] developed O (v) optimal tree scheduling algorithms. These two algorithms are only specific to this kind of trees. For an out-tree, the forward clustering of DSC may not find the optimum directly.
Reference: [5] <author> P. Chretienne, </author> <title> "Complexity of tree scheduling with interprocessor communication delays," </title> <type> Tech. Report, M.A.S.I. </type> <institution> 90.5, Universite Pierre et Marie Curie, </institution> <year> 1990. </year>
Reference-contexts: Since this scheduling problem is NP-complete for a general fine grain tree and for a DAG which is a concatenation of a fork and a join (series parallel DAG) <ref> [3, 5] </ref>, the analysis shows that DSC not only has a low complexity but also attains an optimality degree that a general polynomial algorithm could achieve. 5.1 Performance bounds for general DAGs A DAG consists of fork (F x ) and/or join (J x ) structures such as the ones shown <p> An out-tree is a directed tree in which the root has incoming degree zero and other nodes have the incoming degree one. Scheduling in/out trees is still NP-complete in general as shown by Chretienne <ref> [5] </ref> and DSC will not give the optimal solution. However, DSC will yield optimal solutions for coarse grain trees and a class of fine grain trees. Coarse grain trees Theorem 5.5 DSC gives an optimal solution for a coarse grain in-tree.
Reference: [6] <author> J. Y. Colin and P. Chretienne, </author> <title> "C.P.M. Scheduling with Small Communication Delays and Task Duplication," </title> <journal> Operation Research, </journal> <volume> Vol. 39, No. 4, </volume> <pages> pp. 680-684. </pages>
Reference: [7] <author> M. Cosnard, M. Marrakchi, Y. Robert, and D. Trystram, </author> <title> "Parallel Gaussian elimination on an MIMD computer," </title> <journal> Parallel Computing, </journal> <volume> vol. 6, </volume> <pages> pp. 275-296, </pages> <year> 1988. </year>
Reference-contexts: To see the differences in the complexity between DSC and ETF and demonstrate the practicality of DSC, we consider an important DAG in numerical computing, the Choleski decomposition (CD) DAG, Cosnard et al. <ref> [7] </ref>. For a matrix size n the degree of parallelism is n, the number of tasks v is about n 2 =2 and the number of edges e is about n 2 . The average R/C is 2. The performance of the two algorithms is given in Table 7.
Reference: [8] <author> A. Gerasoulis and T. Yang, </author> <title> "On the granularity and clustering of directed acyclic task graphs," </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 4, no. 6, </volume> <month> June </month> <year> 1993, </year> <pages> pp 686-701. </pages>
Reference-contexts: A linear clustering preserves the parallelism present in a DAG while nonlinear clustering reduces parallelism by sequentializing parallel tasks. A further discussion of this issue can be found in Gerasoulis and Yang <ref> [8] </ref>. The critical path of a clustered graph is the longest path in that graph including both nonzero communica tion edge cost and task weights in that path. <p> ENDWHILE 4.2 The minimization procedure for zeroing multiple incoming edges To reduce tlevel (n x ) in DSC a minimization procedure that zeroes multiple incoming edges of free task n x is needed. An optimal algorithm for a join DAG has been described in <ref> [8] </ref> and an optimal solution is shown in Fig. 6 (c). <p> In <ref> [8] </ref>, we define the grain of DAG as follows: Let k=1:m k=1:m k=1:m k=1:m Then the granularity of G is: g (G) = min n x 2V fg x g where g x = minfg (F x ); g (J x )g. <p> We call a DAG coarse grain if g (G) 1. For a coarse grain DAG, each task receives or sends a small amount of communication compared to the computation of its neighboring tasks. In <ref> [8] </ref>, we prove the following two theorems: 22 Theorem 5.1 For a coarse grain DAG, there exists a linear clustering that attains the optimal solution.
Reference: [9] <author> A. Gerasoulis and T. Yang, </author> <title> "A comparison of clustering heuristics for scheduling DAGs on multiprocessors," J. of Parallel and Distributed Computing, Special issues on scheduling and load balancing, </title> <journal> Vol. </journal> <volume> 16, No 4. </volume> <month> Dec. </month> <year> 1992, </year> <pages> pp. 276-291. </pages>
Reference-contexts: Without task duplication, many heuristic scheduling algorithms for arbitrary DAGs have been proposed in the literature, e.g. Kim and Browne [12], Sarkar [17], Wu and Gajski [19]. A detailed comparison of four heuristic algorithms is given in Gerasoulis and Yang <ref> [9] </ref>. One difficulty with most existing algorithms for general DAGs is their high complexity. As far as we know no scheduling algorithm exists that works well for arbitrary graphs, finds optimal schedules for special DAGs and also has a low complexity. <p> As a matter of fact, most of the existing algorithms can be characterized by using such a framework <ref> [9] </ref>. The initial step assumes that each node is mapped in a unit cluster. At each step, the algorithm tries to improve on the previous clustering by merging appropriate clusters. A merging operation is performed by zeroing an edge cost connecting two clusters 1 . <p> Sarkar [17], Kim and Browne [12], and Wu and Gajski [19]. A comparison of these algorithms is given in Gerasoulis and Yang <ref> [9] </ref>. In this section, we compare DSC with the MD algorithm of Wu and Gajski [19] and the ETF by Hwang, Chow, Anger and Lee [11]. <p> A summary of the comparison is described in Table 2. Notice the similarities and differences between MCP and ETF and between MD and DSC. For a detailed comparison of MCP and DSC, see <ref> [9] </ref>.
Reference: [10] <author> M. Girkar and C. Polychronopoulos, </author> <title> "Partitioning programs for parallel execution," </title> <booktitle> in Proc. of 1988 ACM Inter. Conf. on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July 4-8, </month> <year> 1988. </year>
Reference: [11] <author> J. J. Hwang, Y. C. Chow, F. D. Anger, and C. Y. Lee, </author> <title> "Scheduling precedence graphs in systems with interprocessor communication times," </title> <journal> SIAM J. Comput., </journal> <pages> pp. 244-257, </pages> <year> 1989. </year>
Reference-contexts: We present one such algorithm in this paper with a complexity of O ((v + e) log v), called the Dominant Sequence Clustering (DSC) algorithm. We compare DSC with ETF algorithm by Hwang, Chow, Anger, and Lee <ref> [11] </ref> and discuss the similarities and differences with the MD algorithm proposed by Wu and Gajski [19]. The organization is as follows: Section 2 introduces the basic concepts. Section 3 describes an initial design of the DSC algorithm and analyzes its weaknesses. <p> Sarkar [17], Kim and Browne [12], and Wu and Gajski [19]. A comparison of these algorithms is given in Gerasoulis and Yang [9]. In this section, we compare DSC with the MD algorithm of Wu and Gajski [19] and the ETF by Hwang, Chow, Anger and Lee <ref> [11] </ref>. We also provide an experimental comparison for ETF, DSC and Sarkar's algorithms. 6.1 The MD algorithm Wu and Gajski's [19] have proposed two algorithms the MCP and MD. We refer the reader to [19] for the description of both algorithms as well as the terms used in this paragraph. <p> Since the MD schedules a task to a processor which does not necessarily decrease the tlevel at each step, the MD may not produce the optimum in general. A summary of this comparison is given in Table 2. 6.2 The ETF algorithm ETF <ref> [11] </ref> is a scheduling algorithm for a bounded number of processors with arbitrary network topologies. At each scheduling step, ETF finds a free task whose starting time is the smallest and then assigns this task to a processor in which the task execution can be started as early-as-possible. <p> If there is a tie then the task with the highest blevel is scheduled and this heuristic is called the ETF/CP in <ref> [11] </ref>. ETF is designed for scheduling on a bounded number of processors. Thus to compare the performance of DSC and ETF we first apply DSC to determine the number of processors (clusters), which we then use as an input to the ETF algorithm. <p> In our case, p = O (v), w = O (v), thus the worst complexity is O (v 3 ). We have used the balanced searching tree structure for the ETF algorithm in finding the values of a clock variable, NM, <ref> [11] </ref>, pp. 249-250. However, the complexity of ETF for finding the earliest task at each step cannot be reduced since the 28 earliest starting time of a task depends on the location of processors to be assigned. <p> A summary of the comparison is described in Table 2. Notice the similarities and differences between MCP and ETF and between MD and DSC. For a detailed comparison of MCP and DSC, see [9]. MCP [19] ETF <ref> [11] </ref> MD [19] DSC Task priority blevel earliest task first relative mobility tlevel + blevel (tlevel) DS task first no no yes yes Processor processor for processor for first processor minimization selection earliest starting earliest starting satisfying Fact 1 procedure Complexity O (v 2 log v) O (pv 2 ) O
Reference: [12] <author> S.J. Kim and J.C. Browne, </author> <title> "A general approach to mapping of parallel computation upon multiprocessor architectures," </title> <booktitle> in Int'l Conf. on Parallel Processing, </booktitle> <volume> Vol 3, </volume> <pages> pp. 1-8, </pages> <year> 1988. </year>
Reference-contexts: Without task duplication, many heuristic scheduling algorithms for arbitrary DAGs have been proposed in the literature, e.g. Kim and Browne <ref> [12] </ref>, Sarkar [17], Wu and Gajski [19]. A detailed comparison of four heuristic algorithms is given in Gerasoulis and Yang [9]. One difficulty with most existing algorithms for general DAGs is their high complexity. <p> Sarkar [17], Kim and Browne <ref> [12] </ref>, and Wu and Gajski [19]. A comparison of these algorithms is given in Gerasoulis and Yang [9]. In this section, we compare DSC with the MD algorithm of Wu and Gajski [19] and the ETF by Hwang, Chow, Anger and Lee [11].
Reference: [13] <author> B. Kruatrachue B. and T. Lewis, </author> <title> "Grain size determination for parallel processing," </title> <journal> IEEE Software, </journal> <pages> pp. 23-32, </pages> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: This algorithm has a complexity of O (v 3 (v log v + e)) where v is the number of tasks and e is the number of edges. Kruatrachue and Lewis <ref> [13] </ref> have also given an O (v 4 ) algorithm for a general DAG based on task duplication. One difficulty with allowing task duplication is that duplicated tasks may require duplicated data among processors and thus the space complexity could increase when executing parallel programs on real machines.
Reference: [14] <author> C. McCreary and H. Gill, </author> <title> "Automatic determination of grain size for efficient parallel processing," </title> <journal> Comm. of ACM, </journal> <volume> vol. 32, </volume> <pages> pp. 1073-1078, </pages> <month> Sept., </month> <year> 1989. </year>
Reference: [15] <author> C. Papadimitriou and M. Yannakakis, </author> <title> "Towards on an architecture-independent analysis of parallel algorithms," </title> <journal> SIAM J. Comput., </journal> <volume> vol. 19, </volume> <pages> pp. 322-328, </pages> <year> 1990. </year>
Reference-contexts: The general problem is NP-complete and even for simple graphs such as fine grain trees or the concatenation of a fork and a join together the complexity is still NP-complete, Chretienne [3], Papadimitriou and Yannakakis <ref> [15] </ref> and Sarkar [17]. Only for special classes of DAGs, such as join, fork, and coarse grain tree, special polynomial algorithms are known, Chretienne [4], Anger, Hwang and Chow [2]. There have been two approaches in the literature addressing the general scheduling problem. <p> There have been two approaches in the literature addressing the general scheduling problem. The first approach considers heuristics for arbitrary DAGs and the second studies optimal algorithms for special classes of DAGs. When task duplication is allowed, Papadimitriou and Yannakakis <ref> [15] </ref> have proposed an approximate algorithm for a DAG with equal task weights and equal edge weights, which guarantees a performance within 50% of the optimum.
Reference: [16] <author> R. Pozo, </author> <title> "Performance modeling of sparse matrix methods for distributed memory architectures," </title> <booktitle> in Lecture Notes in Computer Science, </booktitle> <volume> No. 634, </volume> <booktitle> Parallel Processing: CONPAR 92 - VAPP V, </booktitle> <address> Springer-Varlag, </address> <year> 1992, </year> <pages> pp. 677-688. 33 </pages>
Reference-contexts: DSC is also useful for partitioning and clustering of parallel programs [17, 21]. A particular area that DSC could be useful is scheduling irregular task graphs. Pozo <ref> [16] </ref> has used DSC to investigate the performance of sparse matrix methods for distributed memory architectures. Wolski and Feo [18] have extended the applicability of DSC to program partitioning for NUMA architectures.
Reference: [17] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors, </title> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction We study the scheduling problem of directed acyclic weighted task graphs (DAGs) on unlimited processor resources and a completely connected interconnection network. The task and edge weights are determinis tic. This problem is important in the development of parallel programming tools and compilers for scalable MIMD architectures <ref> [17, 19, 21, 23] </ref>. Sarkar [17] has proposed a two step method for scheduling with com munication: (1) Schedule on unbounded number of a completely connected architecture. <p> The task and edge weights are determinis tic. This problem is important in the development of parallel programming tools and compilers for scalable MIMD architectures [17, 19, 21, 23]. Sarkar <ref> [17] </ref> has proposed a two step method for scheduling with com munication: (1) Schedule on unbounded number of a completely connected architecture. <p> The general problem is NP-complete and even for simple graphs such as fine grain trees or the concatenation of a fork and a join together the complexity is still NP-complete, Chretienne [3], Papadimitriou and Yannakakis [15] and Sarkar <ref> [17] </ref>. Only for special classes of DAGs, such as join, fork, and coarse grain tree, special polynomial algorithms are known, Chretienne [4], Anger, Hwang and Chow [2]. There have been two approaches in the literature addressing the general scheduling problem. <p> Without task duplication, many heuristic scheduling algorithms for arbitrary DAGs have been proposed in the literature, e.g. Kim and Browne [12], Sarkar <ref> [17] </ref>, Wu and Gajski [19]. A detailed comparison of four heuristic algorithms is given in Gerasoulis and Yang [9]. One difficulty with most existing algorithms for general DAGs is their high complexity. <p> The task execution model is the compile time macro-dataflow model. A task receives all input before starting execution in parallel, executes to completion without interruption, and immediately sends the output to all successor tasks in parallel, see Wu and Gajski [19], Sarkar <ref> [17] </ref>. <p> At each step, the algorithm tries to improve on the previous clustering by merging appropriate clusters. A merging operation is performed by zeroing an edge cost connecting two clusters 1 . Sarkar's algorithm We consider Sarkar's algorithm <ref> [17] </ref>, pp. 123-131, as an example of an edge-zeroing clustering refinement algorithm. This algorithm first sorts the e edges of the DAG in a decreasing order of edge weights and then performs e clustering steps by examining edges from left to right in the sorted list. <p> Sarkar <ref> [17] </ref>, Kim and Browne [12], and Wu and Gajski [19]. A comparison of these algorithms is given in Gerasoulis and Yang [9]. In this section, we compare DSC with the MD algorithm of Wu and Gajski [19] and the ETF by Hwang, Chow, Anger and Lee [11]. <p> The low complexity makes DSC very attractive in practice. DSC can be used in the first step of Sarkar's <ref> [17] </ref> two step approach to scheduling on a bounded number of processors. We have already incorporated DSC in our programming environment PYRROS [23] that has produced very good results on real architectures such as nCUBE-II and INTEL/i860. DSC is also useful for partitioning and clustering of parallel programs [17, 21]. <p> We have already incorporated DSC in our programming environment PYRROS [23] that has produced very good results on real architectures such as nCUBE-II and INTEL/i860. DSC is also useful for partitioning and clustering of parallel programs <ref> [17, 21] </ref>. A particular area that DSC could be useful is scheduling irregular task graphs. Pozo [16] has used DSC to investigate the performance of sparse matrix methods for distributed memory architectures. Wolski and Feo [18] have extended the applicability of DSC to program partitioning for NUMA architectures. <p> Wolski and Feo [18] have extended the applicability of DSC to program partitioning for NUMA architectures. Acknowledgments We are very grateful to Vivek Sarkar for providing us with the programs of his system in <ref> [17] </ref> and his comments and suggestions, to Min-You Wu for his help in clarifying the MD algorithm. We also thank referees for their useful suggestions in improving the presentation of this paper. We thank Weining Wang for programming the random graph generator, Alain Darte, P.
Reference: [18] <author> R. Wolski and J. Feo, </author> <title> "Program parititoning for NUMA multiprocessor computer systems," </title> <type> Report UCRL-JC-112183, </type> <institution> Lawrence Livermore Nat. Lab., </institution> <year> 1992. </year> <note> To appear in J. of Parallel and Distibuted Computing, Special issue on Performance of Supercomputers. </note>
Reference-contexts: DSC is also useful for partitioning and clustering of parallel programs [17, 21]. A particular area that DSC could be useful is scheduling irregular task graphs. Pozo [16] has used DSC to investigate the performance of sparse matrix methods for distributed memory architectures. Wolski and Feo <ref> [18] </ref> have extended the applicability of DSC to program partitioning for NUMA architectures. Acknowledgments We are very grateful to Vivek Sarkar for providing us with the programs of his system in [17] and his comments and suggestions, to Min-You Wu for his help in clarifying the MD algorithm.
Reference: [19] <author> M. Y. Wu and D. Gajski, "Hypertool: </author> <title> A programming aid for message-passing systems," </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 1, no. 3, pp.330-343, </volume> <year> 1990. </year>
Reference-contexts: 1 Introduction We study the scheduling problem of directed acyclic weighted task graphs (DAGs) on unlimited processor resources and a completely connected interconnection network. The task and edge weights are determinis tic. This problem is important in the development of parallel programming tools and compilers for scalable MIMD architectures <ref> [17, 19, 21, 23] </ref>. Sarkar [17] has proposed a two step method for scheduling with com munication: (1) Schedule on unbounded number of a completely connected architecture. <p> Without task duplication, many heuristic scheduling algorithms for arbitrary DAGs have been proposed in the literature, e.g. Kim and Browne [12], Sarkar [17], Wu and Gajski <ref> [19] </ref>. A detailed comparison of four heuristic algorithms is given in Gerasoulis and Yang [9]. One difficulty with most existing algorithms for general DAGs is their high complexity. <p> We compare DSC with ETF algorithm by Hwang, Chow, Anger, and Lee [11] and discuss the similarities and differences with the MD algorithm proposed by Wu and Gajski <ref> [19] </ref>. The organization is as follows: Section 2 introduces the basic concepts. Section 3 describes an initial design of the DSC algorithm and analyzes its weaknesses. <p> The task execution model is the compile time macro-dataflow model. A task receives all input before starting execution in parallel, executes to completion without interruption, and immediately sends the output to all successor tasks in parallel, see Wu and Gajski <ref> [19] </ref>, Sarkar [17]. <p> Sarkar [17], Kim and Browne [12], and Wu and Gajski <ref> [19] </ref>. A comparison of these algorithms is given in Gerasoulis and Yang [9]. In this section, we compare DSC with the MD algorithm of Wu and Gajski [19] and the ETF by Hwang, Chow, Anger and Lee [11]. <p> Sarkar [17], Kim and Browne [12], and Wu and Gajski <ref> [19] </ref>. A comparison of these algorithms is given in Gerasoulis and Yang [9]. In this section, we compare DSC with the MD algorithm of Wu and Gajski [19] and the ETF by Hwang, Chow, Anger and Lee [11]. We also provide an experimental comparison for ETF, DSC and Sarkar's algorithms. 6.1 The MD algorithm Wu and Gajski's [19] have proposed two algorithms the MCP and MD. We refer the reader to [19] for the description of both algorithms <p> In this section, we compare DSC with the MD algorithm of Wu and Gajski <ref> [19] </ref> and the ETF by Hwang, Chow, Anger and Lee [11]. We also provide an experimental comparison for ETF, DSC and Sarkar's algorithms. 6.1 The MD algorithm Wu and Gajski's [19] have proposed two algorithms the MCP and MD. We refer the reader to [19] for the description of both algorithms as well as the terms used in this paragraph. <p> MD algorithm of Wu and Gajski <ref> [19] </ref> and the ETF by Hwang, Chow, Anger and Lee [11]. We also provide an experimental comparison for ETF, DSC and Sarkar's algorithms. 6.1 The MD algorithm Wu and Gajski's [19] have proposed two algorithms the MCP and MD. We refer the reader to [19] for the description of both algorithms as well as the terms used in this paragraph. The authors use the notion of as-soon-as-possible (ASAP) starting time T S (n i ) and the as-late-as-possible (ALAP) time T L (n i ) and latest finishing time, T F (n i ). <p> It then examines the available processors starting from P E 0 and it schedules n p to the first processor that satisfies a condition called Fact 1 <ref> [19] </ref>, pp. 336 3 . An intuitive explanation for Fact 1 is that scheduling a task n p to a processor m should not increase the length of the current critical path (DS). The complexity of the original algorithm is O (v 3 ) as shown in [19], pp. 337. <p> called Fact 1 <ref> [19] </ref>, pp. 336 3 . An intuitive explanation for Fact 1 is that scheduling a task n p to a processor m should not increase the length of the current critical path (DS). The complexity of the original algorithm is O (v 3 ) as shown in [19], pp. 337. The corrected version of MD has a better performance but slightly higher complexity. This is because of the re-computation of T S and T F for each scanned processor. <p> The idea of identifying the important tasks in DSC is the same as in MD, i.e. the smallest relative mobility 3 In a recent personal communication [20], the authors have made the following corrections to the MD algorithm presented in <ref> [19] </ref>: (1) For Fact 1 when considering processor m, the condition "for each k" should change to "there exists k", [19], pp. 336. (2) The T F and T S computation [19], pp. 336, should assume that task n p is scheduled on processor m. (3) When n p is scheduled <p> the same as in MD, i.e. the smallest relative mobility 3 In a recent personal communication [20], the authors have made the following corrections to the MD algorithm presented in <ref> [19] </ref>: (1) For Fact 1 when considering processor m, the condition "for each k" should change to "there exists k", [19], pp. 336. (2) The T F and T S computation [19], pp. 336, should assume that task n p is scheduled on processor m. (3) When n p is scheduled to processor m, n p is inserted before the first task in the task sequence of processor m that satisfies <p> In a recent personal communication [20], the authors have made the following corrections to the MD algorithm presented in <ref> [19] </ref>: (1) For Fact 1 when considering processor m, the condition "for each k" should change to "there exists k", [19], pp. 336. (2) The T F and T S computation [19], pp. 336, should assume that task n p is scheduled on processor m. (3) When n p is scheduled to processor m, n p is inserted before the first task in the task sequence of processor m that satisfies the inequality listed in Fact 1. 27 identifies DS nodes which <p> However, the technique for choosing a processor is different. ETF places a task to the processor that allows the earliest starting time without re-scheduling its predecessors, while DSC uses the minimization procedure that could re-schedule some of the predecessors. It should be mentioned that the MCP <ref> [19] </ref> algorithm also schedules a task to a processor that allows its earliest starting time as in ETF. However, the node priority for MCP is blevel as opposed to tlevel used by ETF. The complexity of ETF is higher than DSC. <p> A summary of the comparison is described in Table 2. Notice the similarities and differences between MCP and ETF and between MD and DSC. For a detailed comparison of MCP and DSC, see [9]. MCP <ref> [19] </ref> ETF [11] MD [19] DSC Task priority blevel earliest task first relative mobility tlevel + blevel (tlevel) DS task first no no yes yes Processor processor for processor for first processor minimization selection earliest starting earliest starting satisfying Fact 1 procedure Complexity O (v 2 log v) O (pv 2 <p> A summary of the comparison is described in Table 2. Notice the similarities and differences between MCP and ETF and between MD and DSC. For a detailed comparison of MCP and DSC, see [9]. MCP <ref> [19] </ref> ETF [11] MD [19] DSC Task priority blevel earliest task first relative mobility tlevel + blevel (tlevel) DS task first no no yes yes Processor processor for processor for first processor minimization selection earliest starting earliest starting satisfying Fact 1 procedure Complexity O (v 2 log v) O (pv 2 ) O (pv (v
Reference: [20] <author> M. Y. Wu, </author> <type> Personal Communications, </type> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: The idea of identifying the important tasks in DSC is the same as in MD, i.e. the smallest relative mobility 3 In a recent personal communication <ref> [20] </ref>, the authors have made the following corrections to the MD algorithm presented in [19]: (1) For Fact 1 when considering processor m, the condition "for each k" should change to "there exists k", [19], pp. 336. (2) The T F and T S computation [19], pp. 336, should assume that
Reference: [21] <author> J. Yang, L. Bic and A. Nicolau, </author> <title> "A mapping strategy for MIMD computers," </title> <booktitle> Proc. of 1991 Inter. Conf. on Parallel Processing, </booktitle> <volume> vol I, </volume> <pages> pp. 102-109. </pages>
Reference-contexts: 1 Introduction We study the scheduling problem of directed acyclic weighted task graphs (DAGs) on unlimited processor resources and a completely connected interconnection network. The task and edge weights are determinis tic. This problem is important in the development of parallel programming tools and compilers for scalable MIMD architectures <ref> [17, 19, 21, 23] </ref>. Sarkar [17] has proposed a two step method for scheduling with com munication: (1) Schedule on unbounded number of a completely connected architecture. <p> We have already incorporated DSC in our programming environment PYRROS [23] that has produced very good results on real architectures such as nCUBE-II and INTEL/i860. DSC is also useful for partitioning and clustering of parallel programs <ref> [17, 21] </ref>. A particular area that DSC could be useful is scheduling irregular task graphs. Pozo [16] has used DSC to investigate the performance of sparse matrix methods for distributed memory architectures. Wolski and Feo [18] have extended the applicability of DSC to program partitioning for NUMA architectures.
Reference: [22] <author> T. Yang and A. Gerasoulis, </author> <title> "A fast static scheduling algorithm for DAGs on an unbounded number of processors," </title> <booktitle> Proc. of Supercomputing '91, IEEE, </booktitle> <address> Albuquerque, NM, </address> <month> Nov. </month> <year> 1991, </year> <pages> pp. 633-642. </pages>
Reference-contexts: DMS-8706122 from NSF, by the Air Force Office of Scientific Research and the Office of Naval research under grant N00014-90-J-4018, and by Sophia Shen Fellowship of Rutgers University. A preliminary report on this work has appeared in Proceedings of Supercomputing 91 <ref> [22] </ref>. 1 further to the number of physical processors and also incorporate the network topology in the merging step. In this paper, we present an efficient algorithm for the first step of Sarkar's approach. Algorithms for the second step are discussed elsewhere [23].
Reference: [23] <author> T. Yang and A. Gerasoulis, </author> <title> "PYRROS: Static scheduling and code generation for message passing multiprocessors," </title> <booktitle> Proc. of 6th ACM International Conference on Supercomputing, </booktitle> <address> Washington D.C., </address> <month> July </month> <year> 1992, </year> <pages> pp. 428-437. 34 </pages>
Reference-contexts: 1 Introduction We study the scheduling problem of directed acyclic weighted task graphs (DAGs) on unlimited processor resources and a completely connected interconnection network. The task and edge weights are determinis tic. This problem is important in the development of parallel programming tools and compilers for scalable MIMD architectures <ref> [17, 19, 21, 23] </ref>. Sarkar [17] has proposed a two step method for scheduling with com munication: (1) Schedule on unbounded number of a completely connected architecture. <p> In this paper, we present an efficient algorithm for the first step of Sarkar's approach. Algorithms for the second step are discussed elsewhere <ref> [23] </ref>. The objective of scheduling is to allocate tasks onto the processors and then order their execution so that task dependence is satisfied and the length of the schedule, known as the parallel time, is minimized. <p> The low complexity makes DSC very attractive in practice. DSC can be used in the first step of Sarkar's [17] two step approach to scheduling on a bounded number of processors. We have already incorporated DSC in our programming environment PYRROS <ref> [23] </ref> that has produced very good results on real architectures such as nCUBE-II and INTEL/i860. DSC is also useful for partitioning and clustering of parallel programs [17, 21]. A particular area that DSC could be useful is scheduling irregular task graphs.
References-found: 23

