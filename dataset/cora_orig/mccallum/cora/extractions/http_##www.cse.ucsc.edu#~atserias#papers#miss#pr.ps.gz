URL: http://www.cse.ucsc.edu/~atserias/papers/miss/pr.ps.gz
Refering-URL: http://www.cse.ucsc.edu/~atserias/
Root-URL: http://www.cse.ucsc.edu
Email: atserias@cse.ucsc.edu  
Title: Miss prediction strategies for direct-mapped caches  
Author: Albert Atserias 
Date: May 26, 1998  
Address: Santa Cruz  
Affiliation: Computer and Information Sciences Department University of California,  
Abstract: We revisit the dynamic exclusion technique introduced to reduce the miss rate of direct-mapped instruction caches. We state the problem in a formal setting, analyse the mentioned solution, and propose a modification, called anonymous exclusion, that improves performance while being more generally applicable. The new technique slightly outperforms dynamic exclusion and can be used for single-level caches. Our claims are confirmed by software cache simulations on real benchmark programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. McFarling. </author> <title> Cache Replacement with Dynamic Exclusion, </title> <booktitle> In Proc. 19th Symp. Computer Architecture, </booktitle> <pages> 191-200, </pages> <year> 1992. </year>
Reference-contexts: On the other hand, direct-mapped caches have higher miss rates due to their limited replacement policy. The final decision on the mapping strategy relies on a tradeoff between these contradictory features. In 1992, McFarling proposed a replacement policy for direct-mapped instruction caches called dynamic exclusion <ref> [1] </ref>. The key idea is to avoid the replacement of cache blocks that are predictably not used more than once before they need to be replaced again. <p> Note that the first case is a particular case of the second where n = 1. When m is large, the optimal miss rate is close to one half of the actual miss rate. The two patterns presented above are actually common in real code <ref> [1] </ref>. Therefore, if we could detect that a given pattern is being executed then we would be able to improve the miss rate by simulating the optimal cache replacement policy. We define next the problem in a more formal setting in terms of online prediction [2]. <p> The first level of instruction cache has size 16 Kbytes and a block length of 4 bytes. We have used a short block length for simplic ity; scaled results can be obtained with longer blocks using the techniques discussed by Mc-Farling <ref> [1] </ref>. The second level of instruction cache has size 512 Kbytes.
Reference: [2] <author> N. Littlestone, M. Warmuth. </author> <title> The Weighted Majority Algorithm, </title> <journal> Information and Computation, </journal> <volume> 108(2): </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: Therefore, if we could detect that a given pattern is being executed then we would be able to improve the miss rate by simulating the optimal cache replacement policy. We define next the problem in a more formal setting in terms of online prediction <ref> [2] </ref>. Consider a scenario in which a learner associated with a given block in cache receives at time t 2 IN an income x t 2 IN representing the address of an incoming instruction. <p> Namely, for instruction a, h [a] 2 f0; 1g indicates if a has to be loaded in the cache regardless of the value of the bit 1 Note here that in the on-line prediction setting presented in <ref> [2] </ref>, the learner is required to do well with respect to a comparision class of predictors. Here we adopt a different approach by restricting the se quences of incomes. 3 s. The finite state machine is shown in figure 1.
Reference: [3] <author> J. Hennessy, D. Patterson. </author> <title> Computer architecture: a quantitative approach, </title> <publisher> Morgan Kaufmann, </publisher> <address> Second Edition, </address> <year> 1996. </year>
Reference-contexts: It has been shown that the higher miss rate of direct-mapped caches is mainly due to conflict misses generated by this restricted replacement policy <ref> [3] </ref>. In instruction caches, however, one can benefit from common patterns in loops to avoid a number of misses. Consider the following piece of code written in C: inst a; inst b;- where instructions a and b are mapped to the same block p in a direct-mapped instruction cache.
Reference: [4] <author> N. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers, </title> <booktitle> In Proc. 17th Symp. Computer Architecture, </booktitle> <year> 1990. </year> <month> 8 </month>
Reference-contexts: The motivation for that work was the fact that direct-mapped caches have lower access times than associative caches, and consequently, they match better with short CPU cycle times <ref> [4] </ref>. This issue, however, needs to be complemented with a decrease in the miss 1 rate of direct-mapped caches, and this is the purpose of the cited algorithm. In this paper we revisit the dynamic exclusion technique for instruction caches.
References-found: 4

