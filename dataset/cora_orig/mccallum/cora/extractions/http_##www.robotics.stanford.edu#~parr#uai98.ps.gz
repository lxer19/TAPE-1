URL: http://www.robotics.stanford.edu/~parr/uai98.ps.gz
Refering-URL: http://www.robotics.stanford.edu/~parr/
Root-URL: http://www.robotics.stanford.edu
Email: parr@cs.stanford.edu  
Title: Flexible Decomposition Algorithms for Weakly Coupled Markov Decision Problems  
Author: Ronald Parr 
Address: Stanford, CA 94305-9010  
Affiliation: Computer Science Department Stanford University  
Abstract: This paper presents two new approaches to decomposing and solving large Markov decision problems (MDPs), a partial decoupling method and a complete decoupling method. In these approaches, a large, stochastic decision problem is divided into smaller pieces. The first approach builds a cache of policies for each part of the problem independently, and then combines the pieces in a separate, light-weight step. A second approach also divides the problem into smaller pieces, but information is communicated between the different problem pieces, allowing intelligent decisions to be made about which piece requires the most attention. Both approaches can be used to find optimal policies or approximately optimal policies with provable bounds. These algorithms also provide a framework for the efficient transfer of knowledge across problems that share similar structure. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Andre, D., Friedman, N., & Parr, R. </author> <year> (1997). </year> <title> Generalized prioritized sweeping. </title> <booktitle> In Advances in Neural Information Processing Systems 10: Proceedings of the 1997 Conference Denver, </booktitle> <address> Colorado. </address> <publisher> MIT Press. </publisher>
Reference: <author> Bellman, R. E. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey. </address>
Reference-contexts: Value iteration, policy iteration or linear programming can be used to determine the optimal policy for an MDP. These algorithms all use some form of the Bellman equation <ref> (Bellman, 1957) </ref>: a X T (s; a; s 0 )V fl (s 0 ): When the Bellman equation is satisfied, the maximizing action for each state is the optimal action.
Reference: <author> Bertsekas, D. C., & Tsitsiklis, J. N. </author> <year> (1989). </year> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey. </address>
Reference: <author> Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. </author> <year> (1994). </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> pp. </pages> <address> 1023-1028 Seattle, Wash-ington. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: the linear program to break ties between policies. (See Parr (1998) for a more detailed discussion of these points.) The value space search algorithm has some similarities to and was inspired by algorithms for partially observable MDPs (POMDPs) (see Lovejoy (1991) for a survey), and in particular, the Witness algorithm <ref> (Cassandra, Kaelbling, & Littman, 1994) </ref>. The treatment of policies as linear functions, the maximum over which forms a convex surface, is common in the POMDP literature. The approach used here can be seen as a multi-dimensional generalization of an observation made in Russell and Norvig (1995, Ex. 17.4).
Reference: <author> Dean, T., & Lin, S.-H. </author> <year> (1995). </year> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pp. </pages> <address> 1121-1127 Montreal, Canada. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Forestier, J.-P., & Varaiya, P. </author> <year> (1978). </year> <title> Multilayer control of large Markov chains. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-23, </volume> <pages> 298-304. </pages>
Reference: <author> Hauskrecht, M. </author> <year> (1998). </year> <title> Planning with temporally abstract actions. </title> <type> Tech. rep. </type> <institution> CS-98-01, Computer Science Department, Brown University, </institution> <address> Providence, Rhode Island. </address>
Reference: <author> Hauskrecht, M., Meuleau, N., Boutilier, C., Kaelbling, L. P., & Dean, T. </author> <year> (1998). </year> <title> Hierarchical solution of Markov decision processes using macro-actions. </title> <booktitle> In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI-98). To appear. </booktitle>
Reference: <author> Lin, S.-H. </author> <year> (1997). </year> <title> Exploiting Structure for Planning and Control. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, Brown University, </institution> <address> Providence, Rhode Island. </address>
Reference: <author> Lovejoy, W. S. </author> <year> (1991). </year> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <pages> 28(1-4), 47-66. </pages>
Reference: <author> Moore, A. W., & Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweepingreinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 103-130. </pages>
Reference: <author> Parr, R. </author> <year> (1998). </year> <title> Hierarchical Control and Learning for Markov Decision Processes. </title> <type> Ph.D. thesis, </type> <institution> University of California, Computer Science Division, Berkeley, Califor-nia. </institution>
Reference-contexts: A non-stationary policy of this type can be converted easily to a stationary policy that is at least as good <ref> (Parr, 1998) </ref>. The relationship between the size of the out-spaces and the complexity of the high-level problem should make the importance of weak coupling clear.
Reference: <author> Parr, R., & Russell, S. </author> <year> (1997). </year> <title> Reinforcement learning with hierarchies of machines. </title> <booktitle> In Advances in Neural Information Processing Systems 10: Proceedings of the 1997 Conference Denver, </booktitle> <address> Colorado. </address> <publisher> MIT Press. </publisher>
Reference: <author> Precup, D., & Sutton, R. S. </author> <year> (1997). </year> <title> Multi-time models for temporally abstract planning. </title> <booktitle> In Advances in Neural Information Processing Systems 10: Proceedings of the 1997 Conference Denver, </booktitle> <address> Colorado. </address> <publisher> MIT Press. </publisher>
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Processes. </title> <address> Wi-ley, New York. </address>
Reference: <author> Russell, S. J., & Norvig, P. </author> <year> (1995). </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey. </address>
Reference: <author> Singh, S. P. </author> <year> (1992). </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8(3), </volume> <pages> 323-340. </pages>
Reference: <author> Sutton, R. S., Precup, D., & Singh, S. P. </author> <year> (1998). </year> <title> Between MDPs and semi-MDPs: Learning, planning, and representing knowledge at multiple temporal scales. </title> <booktitle> In prep. </booktitle>
Reference: <author> Williams, R. J., & Baird, L. C. I. </author> <year> (1993). </year> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Tech. rep. </type> <institution> NU-CCS-93-14, College of Computer Science, Northeastern University, Boston, Massachusetts. </institution>
Reference-contexts: policy and the right-hand side of the Bellman equation: BE (V (s)) = max R (s; a)+fi s 0 For any policy, the maximum Bellman error over all states, BE (V ) = max s BE (V (s)), is a well-known bound on the distance from the optimal value function <ref> (Williams & Baird, 1993) </ref>: V fl (s) V (s) + 1 fi This is used below to produce error bounds on policy caches. 3 A Class of Decomposition Algorithms The first step for a decomposition method for MDPs is the division of the state space into disjoint subsets, G 1 :
References-found: 19

