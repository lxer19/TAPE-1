URL: http://www.is.cs.cmu.edu/papers/multimodal/94.cmu-cs-94-155.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.multimodal.publications.html
Root-URL: 
Title: Locating and Tracking of Human Faces with Neural Networks  
Author: H. Martin Hunke c H. Martin Hunke, 
Note: This research was done in partial fullfillment for a Diplom in Informatik at the University of Karlsruhe (Master of Science in Computer Science). I gratefully acknowledge the collaboration between the  that made this research possible. This research was sponsored by the Department of the Navy, Office of Naval Research under Grant No. N00014-93-1-0806. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  University of Karlsruhe and Carnegie Mellon University,  
Date: August 1994  1994  
Pubnum: CMU-CS-94-155  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Baluja and D. Pomerleau. </author> <title> Non-Intrusive Gaze Tracking Using Artificial Neural Networks. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Especially in situations providing background noise or several persons talking simultaneously (cocktail party effect), the recognition rate significantly improves by considering visual aspects of communication. Eye tracking allows the computer to determine the point of the screen, where an individual looks at <ref> [1] </ref>. In combination with speech recognition text editing systems could offer much more comfortable user interfaces than today's systems. Spoken commands like "delete this word" while focussing the gaze on a word on the screen would make human-computer communication more natural.
Reference: [2] <author> C. Bregler, H. Hild, S. Manke, and A. Waibel. </author> <title> Improving Connected Letter Recognition by Lipreading. </title> <booktitle> In International Conference on Acoustics, Speech and Signal Processing, </booktitle> <year> 1993. </year>
Reference-contexts: Further development in this area could improve efficiency and robustness of human-computer communication. Recent research deals with integrating visual aspects of communication. Speech recognition systems using only auditory signals can be improved by recognizing lip movements <ref> [2] </ref> [5]. Similar sounds often correspond to very different lip movements (for example "m" and "n"). Especially in situations providing background noise or several persons talking simultaneously (cocktail party effect), the recognition rate significantly improves by considering visual aspects of communication.
Reference: [3] <author> G. W. Cottrell. </author> <title> Extracting Features from Faces Using Compression Networks: Face, Identity, Emotion, and Gender Recognition Using Holons. In Connectionist Models: </title> <booktitle> Proceedings of the 1990 Summer School, </booktitle> <pages> pages 328-337, </pages> <address> San Mateo, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In combination with speech recognition text editing systems could offer much more comfortable user interfaces than today's systems. Spoken commands like "delete this word" while focussing the gaze on a word on the screen would make human-computer communication more natural. Recognizing the identity of an individual <ref> [3] </ref> by extracting features of a camera image could be used for matters of security or to automatically load an user specific setup. Some additional research projects are described in [20]. <p> Another auto-associative method uses back-propagation to compress images. The network is trained with identical input and desired output, so that the hidden units are forced to find a representation of the images. A small number of hidden units leads to a high compression rate <ref> [3] </ref>. The features extracted by these units can be used to identify sex [8], identity [7], and gesture [3]. All these methods only operate on images containing one face in a given position and size. <p> A small number of hidden units leads to a high compression rate <ref> [3] </ref>. The features extracted by these units can be used to identify sex [8], identity [7], and gesture [3]. All these methods only operate on images containing one face in a given position and size. More similar to the problems considered in this technical report are projects to locate features in human faces.
Reference: [4] <author> A. Despopoulos. </author> <title> Color Atlas of Physiology. </title> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Only in this area an individual is able to detect colors. Outside the fovea centralis the retina is much more sensitive for motion and light <ref> [4] </ref>. Therefore interesting objects are brought into the fovea centralis by eye motion for further examination. This method reduces the amount of image data without loosing important information. The face tracking system uses a similar method to reduce computation data.
Reference: [5] <author> P. Duchnowski, U. Meier, and A. Waibel. </author> <title> See Me, Hear Me: Integrating Automatic Speech Recognition And Lip-Reading. </title> <booktitle> In ICSLP, </booktitle> <year> 1994. </year>
Reference-contexts: Further development in this area could improve efficiency and robustness of human-computer communication. Recent research deals with integrating visual aspects of communication. Speech recognition systems using only auditory signals can be improved by recognizing lip movements [2] <ref> [5] </ref>. Similar sounds often correspond to very different lip movements (for example "m" and "n"). Especially in situations providing background noise or several persons talking simultaneously (cocktail party effect), the recognition rate significantly improves by considering visual aspects of communication.
Reference: [6] <author> J. L. Flanagan, J. D. Johnston, R. Zahn, and G. W. Elko. </author> <title> Computer Steered Microphone Arrays for Sound Transduction in Large Rooms. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 78 </volume> <pages> 236-255, </pages> <month> November </month> <year> 1985. </year>
Reference-contexts: Selecting one of several faces for tracking could be supported by an microphone-array, so that faces are preferably located in directions, in which a speech signal could be detected <ref> [6] </ref>. The information about the position of a speaker can be used to stress speech signals in this direction. 1.2 Approach and Chapter Overview A listing of some related research is given in chapter 2. Chapter 3 discusses features used for locating faces, mainly motion and color.
Reference: [7] <author> M. K. Fleming and G. W. Cottrell. </author> <title> Categorization of Faces Using Unsupervised Feature Extraction. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 62-70, </pages> <address> San Diego, CA., </address> <year> 1990. </year>
Reference-contexts: A small number of hidden units leads to a high compression rate [3]. The features extracted by these units can be used to identify sex [8], identity <ref> [7] </ref>, and gesture [3]. All these methods only operate on images containing one face in a given position and size. More similar to the problems considered in this technical report are projects to locate features in human faces.
Reference: [8] <author> B. A. Golomb, D. T. Lawrence, and T. J. Sejnowski. Sexnet: </author> <title> A Neural Network Identifies Sex From Human Faces. </title> <editor> In David Touretzky, editor, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 572-577, </pages> <address> San Mateo, CA., 1991. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 74 </pages>
Reference-contexts: The network is trained with identical input and desired output, so that the hidden units are forced to find a representation of the images. A small number of hidden units leads to a high compression rate [3]. The features extracted by these units can be used to identify sex <ref> [8] </ref>, identity [7], and gesture [3]. All these methods only operate on images containing one face in a given position and size. More similar to the problems considered in this technical report are projects to locate features in human faces.
Reference: [9] <author> V. Govindaraju, D. B. Sher, and S. N. Srihari. </author> <title> Locating human faces in newspaper photographs. </title> <booktitle> In Proc. of IEEE-CS Conf. Computer Vision and Pattern Recognition, </booktitle> <pages> pages 278-285, </pages> <address> San Diego, CA, </address> <year> 1989. </year>
Reference-contexts: This work shows that template matching is very useful to find known features. A different approach is used successfully for locating faces in newspaper articles. After detecting edges in the image, a structural model is matched against the located features <ref> [9] </ref>. Eigenfaces can be used to identify faces [18]. 5 The eigenfaces are determined by performing a principal component analysis on a set of example images with centered faces of the same size. In addition the existence of a face in a given image can be determined.
Reference: [10] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: This representation allows a smooth changing between distinct output classes. 40 Consequently in all following networks the gaussian distribution is used for output representation. 5.1.3 The Back-Propagation Method A more detailed description of this material is given in <ref> [10] </ref>. One training step consists of the selection of a training sample and its application to the network. Each training sample is a pair of given input and desired output data.
Reference: [11] <author> R. A. Hutchinson and W. J. Welch. </author> <title> Comparison of Neural Networks and Conventional Techniques for Feature Location in Facial Images. </title> <booktitle> In First IEE Int. Conf. on Artificial Neural Networks, </booktitle> <pages> pages 201-205, </pages> <address> London, U.K., </address> <year> 1989. </year>
Reference-contexts: All these methods only operate on images containing one face in a given position and size. More similar to the problems considered in this technical report are projects to locate features in human faces. Template matching with an average face was used to locate eyes in imageas of faces <ref> [11] </ref>. The same publication discusses this problem with multi-layered perceptrons. A window covering a subimage is moved line by line over the entire image and determines the input of a network with three layers.
Reference: [12] <author> T. Kohonen, P. Lehtio, E. Oja, A. Kortekangas, and K. </author> <title> Makisara. Demonstration of Patter Processing Properties of the Optimal Associative Mappings. </title> <booktitle> In Proceedings International Confernece on Cybernetics and Society, </booktitle> <address> Washington, D.C., </address> <year> 1977. </year>
Reference-contexts: In early works distances of striking points were used to identify faces [16]. Recent works mostly use artificial neural networks for this purpose. The first twolayered linear networks could store faces, identify them and restore their complete image, given a noisy and uncomplete version of the face <ref> [12] </ref>. Another auto-associative method uses back-propagation to compress images. The network is trained with identical input and desired output, so that the hidden units are forced to find a representation of the images. A small number of hidden units leads to a high compression rate [3].
Reference: [13] <author> D. Pomerleau. </author> <title> Neural Network Perception for Mobile Robot Guidance. </title> <type> PhD thesis, </type> <institution> School of Computer Sience, </institution> <address> CMU, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: This information is used to switch between the two basic states of the system search face and track face. A basic structure for a multilayer perceptron designed for this task is illustrated in figure 5.1. The ALVINN-project <ref> [13] </ref> demonstrated the ability of this network to stear a car automatically. 37 38 The creation of the training set, which is necessary to train the neural net-work, is described in chapter 6.1. 5.1.1 Representation of the Net Input The activations of the retina units are determined by the color or
Reference: [14] <author> W. K. Pratt. </author> <title> Digital Image Processing. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Low-pass filtering can perform the task of building neighborhood classes, as shown in figure 3.10. A local operator (figure 3.9) is applied to the color distribution. This method is explained more detailed in <ref> [14] </ref>.
Reference: [15] <author> P. W. Rander. </author> <title> Real-Time Image-Based Face Tracking. </title> <type> Master's thesis, </type> <institution> Carnegie Mellon University, Pittsburgh, U.S.A., </institution> <year> 1993. </year>
Reference-contexts: The single output unit of the network determines, if an eye is located in the subimage or not. Another research project uses template matching to track features of faces like eyes and lips, which are marked interactivly <ref> [15] </ref>. This work shows that template matching is very useful to find known features. A different approach is used successfully for locating faces in newspaper articles. After detecting edges in the image, a structural model is matched against the located features [9].
Reference: [16] <author> T. Sakai, M. Nagao, and T. Kanade. </author> <title> Computer Analysis and Classification of Photographs of Human Faces. </title> <booktitle> In First USA-Japan computer conference, </booktitle> <address> Yokyo, Japan, </address> <year> 1972. </year>
Reference-contexts: Last but not least I would like to thank the nearly 100 students, who took part in the camera recordings. 4 Chapter 2 Related Work Many research projects dealt with the recognition of features in human faces. In early works distances of striking points were used to identify faces <ref> [16] </ref>. Recent works mostly use artificial neural networks for this purpose. The first twolayered linear networks could store faces, identify them and restore their complete image, given a noisy and uncomplete version of the face [12]. Another auto-associative method uses back-propagation to compress images.
Reference: [17] <author> M. Stone. </author> <title> Cross-validation: A review. </title> <journal> Math. Operationsforsch. Statist., Ser. Statistics, </journal> <volume> 9(1), </volume> <year> 1978. </year>
Reference-contexts: When the performance on this test set has reached its maximum, it's a sign to stop training. Continuing the learning process would result in overlearning of the examples. The performance of the network finally must be evaluated with another independet validation set (crossvalidation <ref> [17] </ref>). The neural network consists of neurons arranged in layers. The input of each neuron j is coherent with the outputs o k of all neurons k of the previous layer. The connection from neuron k to neuron j is named w kj .
Reference: [18] <author> M. A. Turk and A. P. Pentland. </author> <title> Eigenfaces for Recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <month> March </month> <year> 1991. </year>
Reference-contexts: This work shows that template matching is very useful to find known features. A different approach is used successfully for locating faces in newspaper articles. After detecting edges in the image, a structural model is matched against the located features [9]. Eigenfaces can be used to identify faces <ref> [18] </ref>. 5 The eigenfaces are determined by performing a principal component analysis on a set of example images with centered faces of the same size. In addition the existence of a face in a given image can be determined.
Reference: [19] <author> S. Uras, F. Girosi, A. Verri, and V. Torre. </author> <title> A Computational Approach to Motion Perception. </title> <journal> Biological Cybernetics, </journal> <volume> 60(2) </volume> <pages> 79-87, </pages> <year> 1988. </year>
Reference-contexts: This procedure is much faster then determining the optical flow <ref> [19] </ref> but has the following disadvantages: * The camera position and zoom lens adjustment must be constant during the motion analysis.
Reference: [20] <author> A. Waibel, M. T. Voe, P. Duchnowski, and S. Manke. </author> <title> Multimodal Interfaces. </title> <journal> Artificial Intelligence Review Journal, </journal> <year> 1994. </year> <month> 75 </month>
Reference-contexts: Recognizing the identity of an individual [3] by extracting features of a camera image could be used for matters of security or to automatically load an user specific setup. Some additional research projects are described in <ref> [20] </ref>. Though encouraging results in using visual aspects of communications could be achieved, the developed systems mostly require the individual to be in a given position in front of the camera. Subsequently these systems are 1 limited to not moving faces. Human beings are always in motion, especially while communicating.
Reference: [21] <author> G. Wyszecki and W. S. Stiles. </author> <title> Color Science. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1967. </year> <month> 76 </month>
Reference-contexts: The pixels Q 1 = (0:1; 0:5; 0:1) and Q 2 = (0:2; 1:0; 0:2) have the same color but different brightness. Actually the brightness is irrelevant for specifying skin-color. Chromatic colors are defined as colors with normalized brightness, so that the sum of the rgb-values is constant <ref> [21] </ref>. Because of the nonlinearity of the framegrabber and the CCD-chip in the camera, the determined chromatic colors still have a small dependency on brightness, which can be neglected.
References-found: 21

