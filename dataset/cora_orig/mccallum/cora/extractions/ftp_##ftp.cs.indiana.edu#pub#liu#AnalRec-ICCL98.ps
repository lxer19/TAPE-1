URL: ftp://ftp.cs.indiana.edu/pub/liu/AnalRec-ICCL98.ps
Refering-URL: http://www.cs.indiana.edu/~liu/papers/AnalRec-ICCL98.html
Root-URL: http://www.cs.indiana.edu
Title: Dependence Analysis for Recursive Data  
Author: Yanhong A. Liu 
Abstract: This paper describes a general and powerful method for dependence analysis in the presence of recursive data constructions. The particular analysis presented is for identifying partially dead recursive data, but the general framework for representing and manipulating recursive substructures applies to all dependence analyses. The method uses projections based on general regular tree grammars extended with notions of live and dead, and defines the analysis as mutually recursive grammar transformers. To guarantee that the analysis terminates, we use carefully designed approximations. We describe how to approximate argument projections with grammars that can be computed without iterating and how to approximate resulting projections with a widening operation. We design an approximation operation that combines two grammars to give the most precise deterministic result possible. All grammar operations used in the analysis have efficient algorithms. The overall analysis yields significantly more precise results than other known methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> S. Abramsky and C. Hankin, editors. </editor> <title> Abstract Interpretation of Declarative Languages. Ellis Horwood Series in Computers and Their Applications. </title> <editor> E. </editor> <publisher> Horwood, Chichester; Halsted Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction Dependence analysis is the basis of compiler optimizations and program manipulation. Examples include liveness analysis for dead-code elimination [3], flow analysis for storage allocation [38], binding-time anal ysis for partial evaluation [25], and strictness analysis for optimizing lazy evaluation <ref> [1] </ref>. In particular, dead code produces values that never get used. Such code appears often as a result of program optimization, modification, and reuse [38, 3]. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions.
Reference: [2] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1974. </year>
Reference-contexts: Our analysis is needed for pruning and is crucial for reducing the space consumption. We have used this cache-and-prune method in deriving a collection of dynamic programming programs found in standard texts <ref> [2, 43, 11] </ref>. Memory allocation and compile-time garbage collection. In high-level languages with automatic memory management, an important compiler optimization is to reduce run-time overhead by reducing the memory allocated and collected.
Reference: [3] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Dependence analysis is the basis of compiler optimizations and program manipulation. Examples include liveness analysis for dead-code elimination <ref> [3] </ref>, flow analysis for storage allocation [38], binding-time anal ysis for partial evaluation [25], and strictness analysis for optimizing lazy evaluation [1]. In particular, dead code produces values that never get used. Such code appears often as a result of program optimization, modification, and reuse [38, 3]. <p> In particular, dead code produces values that never get used. Such code appears often as a result of program optimization, modification, and reuse <ref> [38, 3] </ref>. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [54, 46], program specializa tion [46], and compile-time garbage collection [27, 41]. <p> This is called backward slicing [54], and it helps debug and reuse program pieces. It can also be regarded as a kind of program specialization with respect to program output [46]. Dead-code elimination <ref> [3] </ref> is a straightforward application of our work.
Reference: [4] <author> A. Aiken and B. R. Murphy. </author> <title> Implementing regular tree expressions. </title> <booktitle> In Proceedings of the 5th International Conference on Functional Programming Languages and Computer Architecture, volume 523 of Lecture Notes in Computer Science, </booktitle> <pages> pages 427-447. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: This ordering is decidable because, given G 1 and G 2 , we can construct G 0 1 and G 0 2 such that G 1 G 2 iff L G 0 2 , and the latter is decidable <ref> [19, 4] </ref>. The construction of G 0 i (for i = 1; 2) from G 1 and G 2 has three steps; assuming nonterminals are renamed so that those in G 1 are distinct from those in G 2 : 1. Let G 0 2. <p> This approximates the possibly non-existing fixed point, forcing the iteration to terminate while guaranteeing that the sufficiency conditions are satisfied. Widening operations have been defined implicitly <ref> [4, 48] </ref> or explicitly [14] for regular tree grammars. <p> ordering test used after each iteration to determine whether further iterations are needed? We have shown in Section 3 that this is decidable by reducing it to an inclusion test for regular tree grammars, since we know that inclusion test for general regular tree expressions is decidable, in particular, EXPTIME-hard <ref> [4] </ref>. In fact, inclusion test for regular tree grammars is exponential. In particular, it is at least PSPACE-complete, since inclusion test for regular string grammars is [49, 40]. Nevertheless, for deterministic regular tree grammars, a quadratic-time inclusion test exists [35].
Reference: [5] <author> A. Aiken and B. R. Murphy. </author> <title> Static type inference in a dynamically typed language. </title> <booktitle> In Conference Record of the 18th Annual ACM Symposium on Principles of Programming Languages. ACM, </booktitle> <address> New York, </address> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Grammar-based projections. Regular tree grammars have been used to describe recursive substructures and other data flow information <ref> [26, 36, 37, 5, 48, 14, 46] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [26], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [36, 37, 5, 16, 48, 46] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [46] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [6] <author> B. Blanchet. </author> <title> Escape analysis: correctness proof, implementation and experimental results. </title> <booktitle> In Conference Record of the 25th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 25-37. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1998. </year>
Reference-contexts: Memory allocation and compile-time garbage collection. In high-level languages with automatic memory management, an important compiler optimization is to reduce run-time overhead by reducing the memory allocated and collected. Such optimization heavily depends on analyzing various dependencies on program data, especially recursive data <ref> [26, 27, 41, 44, 17, 6] </ref>. Our analysis and framework can be used for these analyses. Efficient implementation of lazy functional languages.
Reference: [7] <author> R. Bodik and R. Gupta. </author> <title> Partial dead code elimination using slicing transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '97 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 159-170. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: We call this dead-code analysis, bearing in mind that it may be used for many other purposes. In recent years, dependence analyses have been made more precise so as to be effective in more complicated computations [53, 30, 37, 16, 41], and this is particularly true for dead-code analysis <ref> [23, 15, 29, 33, 46, 7] </ref>. Since recursive data constructions are used increasingly widely in high-level languages, an important problem is dependence analysis for recursive data. The goal of dead-code analysis here is to identify par fl This work is supported in part by NSF Grant CCR-9711253.
Reference: [8] <author> W.-N. Chin. </author> <title> Safe fusion of functional expressions. </title> <booktitle> In Proceedings of the 1992 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 11-20. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Then dead-code elimination can simply replace all subex-pressions that are completely dead with . Deforestation and fusion <ref> [52, 8] </ref> combine function applications to avoid building large intermediate results. To guarantee that the optimization can be done effectively, the functions and subexpressions must satisfy certain conditions, e.g., be in blazed treeless form [52].
Reference: [9] <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of op erator strength. </title> <journal> Commun. ACM, </journal> 20(11) 850-856, Nov. 1977. 
Reference-contexts: Our analysis helps identify and eliminate functions and subexpressions that do not satisfy these conditions, thus making deforestation more widely applicable. Incrementalization, finite differencing, and strength reduction <ref> [9, 39, 34, 32] </ref> focus on replacing subcompu-tations whose values can be retrieved from the result of a previous computation; they can achieve asymptotic speedup. Dead-code elimination is used as the last step to remove computations whose values were used only in the replaced subcomputations.
Reference: [10] <author> J. Cocke and J. T. Schwartz. </author> <title> Programming Languages and Their Compilers; Preliminary Notes. </title> <type> Technical report, </type> <institution> Courant Institute of Mathematical Sciences, </institution> <address> New York University, </address> <year> 1970. </year>
Reference: [11] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press/McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: Our analysis is needed for pruning and is crucial for reducing the space consumption. We have used this cache-and-prune method in deriving a collection of dynamic programming programs found in standard texts <ref> [2, 43, 11] </ref>. Memory allocation and compile-time garbage collection. In high-level languages with automatic memory management, an important compiler optimization is to reduce run-time overhead by reducing the memory allocated and collected.
Reference: [12] <author> P. Cousot and R. Cousot. </author> <title> Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. </title> <booktitle> In Conference Record of the 4th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 238-252. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1977. </year>
Reference-contexts: Approximating resulting projections after each iteration. To guarantee that all needed f i (G)'s stabilize after a finite number of iterations, we use a widening operation. The idea of widening was first proposed by Cousot and Cousot <ref> [12] </ref>. A widening operation G 1 O G 2 of two grammars G 1 and G 2 has two properties: 1. G 1 G 1 O G 2 and G 2 G 1 O G 2 . 2.
Reference: [13] <author> P. Cousot and R. Cousot. </author> <title> Higher-order abstract interpretation (and application to comportment analysis generalizing strictness, termination, projection and PER analysis of functional languages). </title> <booktitle> In Proceedings of the 1994 International Conference on Computer Languages, </booktitle> <pages> pages 95-112. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: In a typed language, we may also obtain a finite grammar abstract domain directly from the data type information and use it to guarantee the termination of the analysis. We can also extend the analysis to handle higher-order functions, similar to extensions of other analyses to higher-order functions <ref> [13] </ref>. Finally, the analysis can be extended to handle side effects as well. Efficient algorithms for implementations. We have shown that the three approximation operations can be performed efficiently to give very precise analysis results.
Reference: [14] <author> P. Cousot and R. Cousot. </author> <title> Formal language, grammar and set--constraint-based program analysis by abstract interpretation. </title> <booktitle> In Proceedings of the 7th International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 170-181. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Grammar-based projections. Regular tree grammars have been used to describe recursive substructures and other data flow information <ref> [26, 36, 37, 5, 48, 14, 46] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> We have studied this method and applied it to caching intermediate results for program improvement [33]. Finite transformers can be obtained by restricting them to be written in a specific meta-language <ref> [14] </ref>. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors [26, 14] and can be rewritten as a set of constraints [21, 22, 14]. This is essentially a masking of the explicit use of an approximation operation, called widening, when defining transformers [14]. <p> We have studied this method and applied it to caching intermediate results for program improvement [33]. Finite transformers can be obtained by restricting them to be written in a specific meta-language [14]. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors <ref> [26, 14] </ref> and can be rewritten as a set of constraints [21, 22, 14]. This is essentially a masking of the explicit use of an approximation operation, called widening, when defining transformers [14]. Approximation operations provide a more general solution and make the analysis framework more modular and flexible. <p> Finite transformers can be obtained by restricting them to be written in a specific meta-language [14]. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors [26, 14] and can be rewritten as a set of constraints <ref> [21, 22, 14] </ref>. This is essentially a masking of the explicit use of an approximation operation, called widening, when defining transformers [14]. Approximation operations provide a more general solution and make the analysis framework more modular and flexible. <p> a specific meta-language <ref> [14] </ref>. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors [26, 14] and can be rewritten as a set of constraints [21, 22, 14]. This is essentially a masking of the explicit use of an approximation operation, called widening, when defining transformers [14]. Approximation operations provide a more general solution and make the analysis framework more modular and flexible. We describe three efficient approximation operations that together allow our analysis to give more precise results than previous methods. 5 Approximation operations Approximating argument projections. <p> This approximates the possibly non-existing fixed point, forcing the iteration to terminate while guaranteeing that the sufficiency conditions are satisfied. Widening operations have been defined implicitly [4, 48] or explicitly <ref> [14] </ref> for regular tree grammars. <p> Finding an appropriate widening operation is difficult. For example, while trying to use the intended widening operation proposed by Cousot and Cousot <ref> [14] </ref>, we found that it does not satisfy the second property above. We describe below an appropriate widening operation that we have developed. To facilitate widening, for every f i (G) used in the iteration, we associate a unique tag with its initial value AB. <p> Compared with that work, we also handle more program constructs, namely, binding expressions and user-defined constructors of arbitrary arity. We believe that our treatment is also more rigorous, since we adopt the view that regular-tree-grammar-based program analysis is also abstract interpretation <ref> [14] </ref>. We extend the grammars and handle ID and AB specially in grammar ordering, equivalence, and approximation operations. We combine carefully designed approximation operations to produce significantly more precise analysis results than previous methods. Such operations are difficult to design. <p> We extend the grammars and handle ID and AB specially in grammar ordering, equivalence, and approximation operations. We combine carefully designed approximation operations to produce significantly more precise analysis results than previous methods. Such operations are difficult to design. While regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis <ref> [21, 22, 14] </ref>, we do not know any work that treats precise and efficient dependence analysis for recursive data as we do.
Reference: [15] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. M. Wegman, and F. K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Trans. Program. Lang. and Syst., </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: We call this dead-code analysis, bearing in mind that it may be used for many other purposes. In recent years, dependence analyses have been made more precise so as to be effective in more complicated computations [53, 30, 37, 16, 41], and this is particularly true for dead-code analysis <ref> [23, 15, 29, 33, 46, 7] </ref>. Since recursive data constructions are used increasingly widely in high-level languages, an important problem is dependence analysis for recursive data. The goal of dead-code analysis here is to identify par fl This work is supported in part by NSF Grant CCR-9711253.
Reference: [16] <author> A. De Niel, E. Bevers, and K. De Vlaminck. </author> <title> Program bifurcation for a polymorphically typed functional langauge. </title> <booktitle> In Proceedings of the Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 142-153. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: We call this dead-code analysis, bearing in mind that it may be used for many other purposes. In recent years, dependence analyses have been made more precise so as to be effective in more complicated computations <ref> [53, 30, 37, 16, 41] </ref>, and this is particularly true for dead-code analysis [23, 15, 29, 33, 46, 7]. Since recursive data constructions are used increasingly widely in high-level languages, an important problem is dependence analysis for recursive data. <p> Several methods have been studied, but all have limitations [27, 23, 33, 46]. Similar limitations exist also in other analyses for recursive data <ref> [53, 30, 37, 16, 41] </ref>. This paper describes a general and powerful method for analyzing dependencies for recursive data. We represent partially dead recursive data using projections based on general regular tree grammars extended with notions of live and dead. <p> Partial evaluation. Binding-time analysis identifies computations that depend only on the static part of the input. It is a forward analysis that is equivalent to strictness analysis [31]. Analyzing partially static recursive data is important <ref> [30, 37, 16] </ref>. Again, our analysis framework can be applied. Extensions. Our method is described here for an untyped language, but they apply to typed languages as well. <p> It is a forward analysis equivalent to strictness analysis and uses a fixed finite abstract domain as well [31]. Mo-gensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars <ref> [37, 16] </ref>. Another kind of analysis for recursive data is escape analysis [41, 17], but existing methods can not express as precise information as our method. Several analyses are in the same spirit as ours, even though some do not use the name projection. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [26], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [36, 37, 5, 16, 48, 46] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [46] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [17] <author> A. Deutsch. </author> <title> On the complexity of escape analysis. </title> <booktitle> In Conference Record of the 24th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 358-371. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: Memory allocation and compile-time garbage collection. In high-level languages with automatic memory management, an important compiler optimization is to reduce run-time overhead by reducing the memory allocated and collected. Such optimization heavily depends on analyzing various dependencies on program data, especially recursive data <ref> [26, 27, 41, 44, 17, 6] </ref>. Our analysis and framework can be used for these analyses. Efficient implementation of lazy functional languages. <p> Mo-gensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars [37, 16]. Another kind of analysis for recursive data is escape analysis <ref> [41, 17] </ref>, but existing methods can not express as precise information as our method. Several analyses are in the same spirit as ours, even though some do not use the name projection. The necessity interpretation by Jones and Le Metayer [27] uses necessity patterns that correspond to projections.
Reference: [18] <institution> Proceedings of the 4th International Conference on Functional Programming Languages and Computer Architecture. ACM, </institution> <address> New York, </address> <month> Sept. </month> <year> 1989. </year>
Reference: [19] <author> F. Gecseg and M. Steinb. </author> <title> Tree Automata. </title> <publisher> Akademiai Kiado, </publisher> <address> Budapest, </address> <year> 1984. </year>
Reference-contexts: This ordering is decidable because, given G 1 and G 2 , we can construct G 0 1 and G 0 2 such that G 1 G 2 iff L G 0 2 , and the latter is decidable <ref> [19, 4] </ref>. The construction of G 0 i (for i = 1; 2) from G 1 and G 2 has three steps; assuming nonterminals are renamed so that those in G 1 are distinct from those in G 2 : 1. Let G 0 2. <p> As a safe case, if size (G 2 ) &lt; size (G 1 ) for a given well-founded measure size, then we continue normal on-demand evaluation. Each regular tree grammar corresponds to a finite tree automata, which can be minimized <ref> [19] </ref>, so we can use the number of states as the measure.
Reference: [20] <author> C. A. Gunter. </author> <title> Semantics of Programming Languages. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1992. </year>
Reference-contexts: If the language is call-by-value, then the resulting program terminates with the correct value whenever the original program does; if the language is call-by-name, then the resulting program terminates with the correct value exactly when the original program does. 3 Representing partially dead recur sive data Projections. Domain projections <ref> [47, 20] </ref> can be used to project out parts of data that are of interest [53, 30, 37, 46], in our case, the live parts. Let X be the domain of all possible values computed by our programs, including ? and values containing .
Reference: [21] <author> N. Heintze. </author> <title> Set-Based Program Analysis. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Finite transformers can be obtained by restricting them to be written in a specific meta-language [14]. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors [26, 14] and can be rewritten as a set of constraints <ref> [21, 22, 14] </ref>. This is essentially a masking of the explicit use of an approximation operation, called widening, when defining transformers [14]. Approximation operations provide a more general solution and make the analysis framework more modular and flexible. <p> We extend the grammars and handle ID and AB specially in grammar ordering, equivalence, and approximation operations. We combine carefully designed approximation operations to produce significantly more precise analysis results than previous methods. Such operations are difficult to design. While regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis <ref> [21, 22, 14] </ref>, we do not know any work that treats precise and efficient dependence analysis for recursive data as we do.
Reference: [22] <author> N. Heintze. </author> <title> Set-based analysis of ML programs. </title> <booktitle> In Proceedings of the 1994 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 306-317. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Finite transformers can be obtained by restricting them to be written in a specific meta-language [14]. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors [26, 14] and can be rewritten as a set of constraints <ref> [21, 22, 14] </ref>. This is essentially a masking of the explicit use of an approximation operation, called widening, when defining transformers [14]. Approximation operations provide a more general solution and make the analysis framework more modular and flexible. <p> We extend the grammars and handle ID and AB specially in grammar ordering, equivalence, and approximation operations. We combine carefully designed approximation operations to produce significantly more precise analysis results than previous methods. Such operations are difficult to design. While regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis <ref> [21, 22, 14] </ref>, we do not know any work that treats precise and efficient dependence analysis for recursive data as we do.
Reference: [23] <author> J. Hughes. </author> <title> Compile-time analysis of functional programs. </title> <editor> In D. Turner, editor, </editor> <booktitle> Research Topics in Functional Programming, </booktitle> <pages> pages 117-153. </pages> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1990. </year>
Reference-contexts: We call this dead-code analysis, bearing in mind that it may be used for many other purposes. In recent years, dependence analyses have been made more precise so as to be effective in more complicated computations [53, 30, 37, 16, 41], and this is particularly true for dead-code analysis <ref> [23, 15, 29, 33, 46, 7] </ref>. Since recursive data constructions are used increasingly widely in high-level languages, an important problem is dependence analysis for recursive data. The goal of dead-code analysis here is to identify par fl This work is supported in part by NSF Grant CCR-9711253. <p> It is difficult because the structures of general recursive data can be user-defined, and dead recursive substructures are interleaved with other recursive substructures on which the computations are live. Several methods have been studied, but all have limitations <ref> [27, 23, 33, 46] </ref>. Similar limitations exist also in other analyses for recursive data [53, 30, 37, 16, 41]. This paper describes a general and powerful method for analyzing dependencies for recursive data. <p> Several analyses are in the same spirit as ours, even though some do not use the name projection. The necessity interpretation by Jones and Le Metayer [27] uses necessity patterns that correspond to projections. Necessity patterns specify only heads and tails of list values. The absence analysis by Hughes <ref> [23] </ref> uses the name context in place of projection. Even if it is extended for recursive data types, it handles only a finite domain of list contexts where every head context and every tail context is the same.
Reference: [24] <author> R. J. M. Hughes. </author> <title> Strictness detection in non-flat demains. </title> <editor> In N. Jones and H. Ganzinger, editors, </editor> <booktitle> Proceedings of the Workshop on Programs as Data Objects, volume 217 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: Our analysis and framework can be used for these analyses. Efficient implementation of lazy functional languages. Strictness analysis identifies arguments of functions that are necessarily evaluated so that we can evaluate them immediately rather than building data structures to be evaluated later <ref> [24, 51, 53] </ref>. While dead-code analysis looks for the minimum sufficient information for producing an output, strictness analysis looks for the maximum necessary information.
Reference: [25] <author> N. D. Jones, C. K. Gomard, and P. Sestoft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Dependence analysis is the basis of compiler optimizations and program manipulation. Examples include liveness analysis for dead-code elimination [3], flow analysis for storage allocation [38], binding-time anal ysis for partial evaluation <ref> [25] </ref>, and strictness analysis for optimizing lazy evaluation [1]. In particular, dead code produces values that never get used. Such code appears often as a result of program optimization, modification, and reuse [38, 3].
Reference: [26] <author> N. D. Jones and S. S. Muchnick. </author> <title> Flow analysis and optimization of LISP-like structures. </title> <editor> In S. S. Muchnick and N. D. Jones, editors, </editor> <booktitle> Program Flow Analysis, </booktitle> <pages> pages 102-131. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1981. </year>
Reference-contexts: Grammar-based projections. Regular tree grammars have been used to describe recursive substructures and other data flow information <ref> [26, 36, 37, 5, 48, 14, 46] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> The construction and proof are similar to when only the selector form is used <ref> [26] </ref>. Hereafter, we simply call an extended regular tree grammar a regular tree grammar. Grammar abstract domains. Program analysis associates abstract values, such as grammar-based projections, with particular indices, such as functions, parameters, and subexpressions. <p> We have studied this method and applied it to caching intermediate results for program improvement [33]. Finite transformers can be obtained by restricting them to be written in a specific meta-language [14]. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors <ref> [26, 14] </ref> and can be rewritten as a set of constraints [21, 22, 14]. This is essentially a masking of the explicit use of an approximation operation, called widening, when defining transformers [14]. Approximation operations provide a more general solution and make the analysis framework more modular and flexible. <p> Memory allocation and compile-time garbage collection. In high-level languages with automatic memory management, an important compiler optimization is to reduce run-time overhead by reducing the memory allocated and collected. Such optimization heavily depends on analyzing various dependencies on program data, especially recursive data <ref> [26, 27, 41, 44, 17, 6] </ref>. Our analysis and framework can be used for these analyses. Efficient implementation of lazy functional languages. <p> However, methods used there for handling unbounded growth of such projections are crude. The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick <ref> [26] </ref>, where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times [36, 37, 5, 16, 48, 46].
Reference: [27] <author> S. B. Jones and D. Le Metayer. </author> <title> Compile-time garbage collection by sharing analysis. </title> <booktitle> In FPCA 1989 [18], </booktitle> <pages> pages 54-74. </pages>
Reference-contexts: Such code appears often as a result of program optimization, modification, and reuse [38, 3]. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [54, 46], program specializa tion [46], and compile-time garbage collection <ref> [27, 41] </ref>. Analysis for identifying live or dead code, or code of similar relevance, has been studied and used widely [10, 9, 28, 39, 3, 27, 23, 15, 29, 34, 33, 50, 46]. <p> It is difficult because the structures of general recursive data can be user-defined, and dead recursive substructures are interleaved with other recursive substructures on which the computations are live. Several methods have been studied, but all have limitations <ref> [27, 23, 33, 46] </ref>. Similar limitations exist also in other analyses for recursive data [53, 30, 37, 16, 41]. This paper describes a general and powerful method for analyzing dependencies for recursive data. <p> Memory allocation and compile-time garbage collection. In high-level languages with automatic memory management, an important compiler optimization is to reduce run-time overhead by reducing the memory allocated and collected. Such optimization heavily depends on analyzing various dependencies on program data, especially recursive data <ref> [26, 27, 41, 44, 17, 6] </ref>. Our analysis and framework can be used for these analyses. Efficient implementation of lazy functional languages. <p> Several analyses are in the same spirit as ours, even though some do not use the name projection. The necessity interpretation by Jones and Le Metayer <ref> [27] </ref> uses necessity patterns that correspond to projections. Necessity patterns specify only heads and tails of list values. The absence analysis by Hughes [23] uses the name context in place of projection.
Reference: [28] <author> K. Kennedy. </author> <title> Use-definition chains with applications. </title> <journal> J. Com put. Lang., </journal> <volume> 3(3) </volume> <pages> 163-179, </pages> <year> 1978. </year>
Reference: [29] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Partial dead code elimination. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 147-158. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: We call this dead-code analysis, bearing in mind that it may be used for many other purposes. In recent years, dependence analyses have been made more precise so as to be effective in more complicated computations [53, 30, 37, 16, 41], and this is particularly true for dead-code analysis <ref> [23, 15, 29, 33, 46, 7] </ref>. Since recursive data constructions are used increasingly widely in high-level languages, an important problem is dependence analysis for recursive data. The goal of dead-code analysis here is to identify par fl This work is supported in part by NSF Grant CCR-9711253.
Reference: [30] <author> J. Launchbury. </author> <title> Projection Factorisations in Partial Evaluation. </title> <type> PhD thesis, </type> <institution> Department of Computing, University of Glasgow, </institution> <address> Glasgow, Scotland, </address> <year> 1989. </year>
Reference-contexts: We call this dead-code analysis, bearing in mind that it may be used for many other purposes. In recent years, dependence analyses have been made more precise so as to be effective in more complicated computations <ref> [53, 30, 37, 16, 41] </ref>, and this is particularly true for dead-code analysis [23, 15, 29, 33, 46, 7]. Since recursive data constructions are used increasingly widely in high-level languages, an important problem is dependence analysis for recursive data. <p> Several methods have been studied, but all have limitations [27, 23, 33, 46]. Similar limitations exist also in other analyses for recursive data <ref> [53, 30, 37, 16, 41] </ref>. This paper describes a general and powerful method for analyzing dependencies for recursive data. We represent partially dead recursive data using projections based on general regular tree grammars extended with notions of live and dead. <p> Domain projections [47, 20] can be used to project out parts of data that are of interest <ref> [53, 30, 37, 46] </ref>, in our case, the live parts. Let X be the domain of all possible values computed by our programs, including ? and values containing . <p> Partial evaluation. Binding-time analysis identifies computations that depend only on the static part of the input. It is a forward analysis that is equivalent to strictness analysis [31]. Analyzing partially static recursive data is important <ref> [30, 37, 16] </ref>. Again, our analysis framework can be applied. Extensions. Our method is described here for an untyped language, but they apply to typed languages as well. <p> Wadler and Hughes use projections for strictness analysis [53]. Their analysis is also backward but seeks necessary rather than sufficient information, and it uses a fixed finite abstract domain for all programs. Launchbury uses projections for binding-time analysis of partially static data structures in partial evaluation <ref> [30] </ref>. It is a forward analysis equivalent to strictness analysis and uses a fixed finite abstract domain as well [31].
Reference: [31] <author> J. Launchbury. </author> <title> Strictness and binding-time analysis: Two for the price of one. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 80-91. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Partial evaluation. Binding-time analysis identifies computations that depend only on the static part of the input. It is a forward analysis that is equivalent to strictness analysis <ref> [31] </ref>. Analyzing partially static recursive data is important [30, 37, 16]. Again, our analysis framework can be applied. Extensions. Our method is described here for an untyped language, but they apply to typed languages as well. <p> Launchbury uses projections for binding-time analysis of partially static data structures in partial evaluation [30]. It is a forward analysis equivalent to strictness analysis and uses a fixed finite abstract domain as well <ref> [31] </ref>. Mo-gensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars [37, 16].
Reference: [32] <author> Y. A. Liu, S. D. Stoller, and T. Teitelbaum. </author> <title> Discovering auxiliary information for incremental computation. </title> <booktitle> In Conference Record of the 23rd Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 157-170. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Our analysis helps identify and eliminate functions and subexpressions that do not satisfy these conditions, thus making deforestation more widely applicable. Incrementalization, finite differencing, and strength reduction <ref> [9, 39, 34, 32] </ref> focus on replacing subcompu-tations whose values can be retrieved from the result of a previous computation; they can achieve asymptotic speedup. Dead-code elimination is used as the last step to remove computations whose values were used only in the replaced subcomputations.
Reference: [33] <author> Y. A. Liu, S. D. Stoller, and T. Teitelbaum. </author> <title> Static caching for incremental computation. </title> <journal> ACM Trans. Program. Lang. and Syst., </journal> <volume> 20(2), </volume> <month> March </month> <year> 1998. </year>
Reference-contexts: We call this dead-code analysis, bearing in mind that it may be used for many other purposes. In recent years, dependence analyses have been made more precise so as to be effective in more complicated computations [53, 30, 37, 16, 41], and this is particularly true for dead-code analysis <ref> [23, 15, 29, 33, 46, 7] </ref>. Since recursive data constructions are used increasingly widely in high-level languages, an important problem is dependence analysis for recursive data. The goal of dead-code analysis here is to identify par fl This work is supported in part by NSF Grant CCR-9711253. <p> It is difficult because the structures of general recursive data can be user-defined, and dead recursive substructures are interleaved with other recursive substructures on which the computations are live. Several methods have been studied, but all have limitations <ref> [27, 23, 33, 46] </ref>. Similar limitations exist also in other analyses for recursive data [53, 30, 37, 16, 41]. This paper describes a general and powerful method for analyzing dependencies for recursive data. <p> Appropriate finite abstract domains can often be obtained for various applications of the analysis, and they can provide sufficiently precise analysis results on a per-program basis. We have studied this method and applied it to caching intermediate results for program improvement <ref> [33] </ref>. Finite transformers can be obtained by restricting them to be written in a specific meta-language [14]. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors [26, 14] and can be rewritten as a set of constraints [21, 22, 14]. <p> Dead-code elimination is used as the last step to remove computations whose values were used only in the replaced subcomputations. This is crucial for the speedup. Static caching <ref> [33] </ref> caches all intermediate results in a loop body, uses them from one iteration to the next, and prunes out those that are not useful. Our analysis is needed for pruning and is crucial for reducing the space consumption. <p> Even if it is extended for recursive data types, it handles only a finite domain of list contexts where every head context and every tail context is the same. The analysis for pruning by Liu and Teitelbaum <ref> [33] </ref> uses projections to specify specific components of tuple values and thus provide more accurate information. However, methods used there for handling unbounded growth of such projections are crude.
Reference: [34] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Systematic derivation of incre mental programs. </title> <journal> Sci. Comput. Program., </journal> <volume> 24(1) </volume> <pages> 1-39, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Our analysis helps identify and eliminate functions and subexpressions that do not satisfy these conditions, thus making deforestation more widely applicable. Incrementalization, finite differencing, and strength reduction <ref> [9, 39, 34, 32] </ref> focus on replacing subcompu-tations whose values can be retrieved from the result of a previous computation; they can achieve asymptotic speedup. Dead-code elimination is used as the last step to remove computations whose values were used only in the replaced subcomputations.
Reference: [35] <author> B. </author> <title> Long. An algorithm for comparing deterministic regular tree grammars. </title> <type> Technical Report TR 503, </type> <institution> Computer Science Department, Indiana University, Bloomington, Indiana, </institution> <month> Feb. </month> <year> 1998. </year>
Reference-contexts: In fact, inclusion test for regular tree grammars is exponential. In particular, it is at least PSPACE-complete, since inclusion test for regular string grammars is [49, 40]. Nevertheless, for deterministic regular tree grammars, a quadratic-time inclusion test exists <ref> [35] </ref>. Recall from Section 5 that applying widening operation after each iteration results in a deterministic grammar. Therefore, we can indeed perform grammar ordering test using the quadratic-time algorithm. Moreover, using deterministic grammars also within each iteration actually produces overall more precise analysis results.
Reference: [36] <author> P. Mishra and U. Reddy. </author> <title> Declaration-free type checking. </title> <booktitle> In Conference Record of the 12th Annual ACM Symposium on POPL, </booktitle> <pages> pages 7-21. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1985. </year>
Reference-contexts: Grammar-based projections. Regular tree grammars have been used to describe recursive substructures and other data flow information <ref> [26, 36, 37, 5, 48, 14, 46] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [26], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [36, 37, 5, 16, 48, 46] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [46] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [37] <author> T. Mogensen. </author> <title> Separating binding times in language specifications. </title> <booktitle> In FPCA 1989 [18], </booktitle> <pages> pages 12-25. </pages>
Reference-contexts: We call this dead-code analysis, bearing in mind that it may be used for many other purposes. In recent years, dependence analyses have been made more precise so as to be effective in more complicated computations <ref> [53, 30, 37, 16, 41] </ref>, and this is particularly true for dead-code analysis [23, 15, 29, 33, 46, 7]. Since recursive data constructions are used increasingly widely in high-level languages, an important problem is dependence analysis for recursive data. <p> Several methods have been studied, but all have limitations [27, 23, 33, 46]. Similar limitations exist also in other analyses for recursive data <ref> [53, 30, 37, 16, 41] </ref>. This paper describes a general and powerful method for analyzing dependencies for recursive data. We represent partially dead recursive data using projections based on general regular tree grammars extended with notions of live and dead. <p> Domain projections [47, 20] can be used to project out parts of data that are of interest <ref> [53, 30, 37, 46] </ref>, in our case, the live parts. Let X be the domain of all possible values computed by our programs, including ? and values containing . <p> Grammar-based projections. Regular tree grammars have been used to describe recursive substructures and other data flow information <ref> [26, 36, 37, 5, 48, 14, 46] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> Partial evaluation. Binding-time analysis identifies computations that depend only on the static part of the input. It is a forward analysis that is equivalent to strictness analysis [31]. Analyzing partially static recursive data is important <ref> [30, 37, 16] </ref>. Again, our analysis framework can be applied. Extensions. Our method is described here for an untyped language, but they apply to typed languages as well. <p> It is a forward analysis equivalent to strictness analysis and uses a fixed finite abstract domain as well [31]. Mo-gensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars <ref> [37, 16] </ref>. Another kind of analysis for recursive data is escape analysis [41, 17], but existing methods can not express as precise information as our method. Several analyses are in the same spirit as ours, even though some do not use the name projection. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [26], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [36, 37, 5, 16, 48, 46] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [46] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [38] <editor> S. S. Muchnick and N. D. Jones, editors. </editor> <title> Program Flow Analysis: Theory and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1981. </year>
Reference-contexts: 1 Introduction Dependence analysis is the basis of compiler optimizations and program manipulation. Examples include liveness analysis for dead-code elimination [3], flow analysis for storage allocation <ref> [38] </ref>, binding-time anal ysis for partial evaluation [25], and strictness analysis for optimizing lazy evaluation [1]. In particular, dead code produces values that never get used. Such code appears often as a result of program optimization, modification, and reuse [38, 3]. <p> In particular, dead code produces values that never get used. Such code appears often as a result of program optimization, modification, and reuse <ref> [38, 3] </ref>. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [54, 46], program specializa tion [46], and compile-time garbage collection [27, 41].
Reference: [39] <author> R. Paige and S. Koenig. </author> <title> Finite differencing of computable ex pressions. </title> <journal> ACM Trans. Program. Lang. and Syst., </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Our analysis helps identify and eliminate functions and subexpressions that do not satisfy these conditions, thus making deforestation more widely applicable. Incrementalization, finite differencing, and strength reduction <ref> [9, 39, 34, 32] </ref> focus on replacing subcompu-tations whose values can be retrieved from the result of a previous computation; they can achieve asymptotic speedup. Dead-code elimination is used as the last step to remove computations whose values were used only in the replaced subcomputations.
Reference: [40] <author> C. H. Papadimitriou. </author> <title> Computational Complexity. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1994. </year>
Reference-contexts: In fact, inclusion test for regular tree grammars is exponential. In particular, it is at least PSPACE-complete, since inclusion test for regular string grammars is <ref> [49, 40] </ref>. Nevertheless, for deterministic regular tree grammars, a quadratic-time inclusion test exists [35]. Recall from Section 5 that applying widening operation after each iteration results in a deterministic grammar. Therefore, we can indeed perform grammar ordering test using the quadratic-time algorithm.
Reference: [41] <author> Y. G. Park and B. Goldber. </author> <title> Escape analysis on lists. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 116-127. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Such code appears often as a result of program optimization, modification, and reuse [38, 3]. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [54, 46], program specializa tion [46], and compile-time garbage collection <ref> [27, 41] </ref>. Analysis for identifying live or dead code, or code of similar relevance, has been studied and used widely [10, 9, 28, 39, 3, 27, 23, 15, 29, 34, 33, 50, 46]. <p> We call this dead-code analysis, bearing in mind that it may be used for many other purposes. In recent years, dependence analyses have been made more precise so as to be effective in more complicated computations <ref> [53, 30, 37, 16, 41] </ref>, and this is particularly true for dead-code analysis [23, 15, 29, 33, 46, 7]. Since recursive data constructions are used increasingly widely in high-level languages, an important problem is dependence analysis for recursive data. <p> Several methods have been studied, but all have limitations [27, 23, 33, 46]. Similar limitations exist also in other analyses for recursive data <ref> [53, 30, 37, 16, 41] </ref>. This paper describes a general and powerful method for analyzing dependencies for recursive data. We represent partially dead recursive data using projections based on general regular tree grammars extended with notions of live and dead. <p> Memory allocation and compile-time garbage collection. In high-level languages with automatic memory management, an important compiler optimization is to reduce run-time overhead by reducing the memory allocated and collected. Such optimization heavily depends on analyzing various dependencies on program data, especially recursive data <ref> [26, 27, 41, 44, 17, 6] </ref>. Our analysis and framework can be used for these analyses. Efficient implementation of lazy functional languages. <p> Mo-gensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars [37, 16]. Another kind of analysis for recursive data is escape analysis <ref> [41, 17] </ref>, but existing methods can not express as precise information as our method. Several analyses are in the same spirit as ours, even though some do not use the name projection. The necessity interpretation by Jones and Le Metayer [27] uses necessity patterns that correspond to projections.
Reference: [42] <author> W. Pugh and E. Rosser. </author> <title> Iteration space slicing and its application to communication optimization. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: This paper discusses techniques for recursion. The basic ideas should extend to loops. A recent work has just started this direction; it extends slicing to symbolically capture particular iterations in a loop <ref> [42] </ref>. Object-oriented programming style is used widely, but cross-class optimization heavily depends on inlining, which often causes code blow-up. Grammar-based analysis and transformation can be applied to methods across classes without inlining.
Reference: [43] <author> P. W. Purdom and C. A. Brown. </author> <title> The Analysis of Algorithms. </title> <publisher> Holt, Rinehart and Winston, </publisher> <year> 1985. </year>
Reference-contexts: Our analysis is needed for pruning and is crucial for reducing the space consumption. We have used this cache-and-prune method in deriving a collection of dynamic programming programs found in standard texts <ref> [2, 43, 11] </ref>. Memory allocation and compile-time garbage collection. In high-level languages with automatic memory management, an important compiler optimization is to reduce run-time overhead by reducing the memory allocated and collected.
Reference: [44] <author> T. Reps. </author> <title> Shape analysis as a generalized path problem. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 1-11. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Memory allocation and compile-time garbage collection. In high-level languages with automatic memory management, an important compiler optimization is to reduce run-time overhead by reducing the memory allocated and collected. Such optimization heavily depends on analyzing various dependencies on program data, especially recursive data <ref> [26, 27, 41, 44, 17, 6] </ref>. Our analysis and framework can be used for these analyses. Efficient implementation of lazy functional languages.
Reference: [45] <author> T. Reps and T. Teitelbaum. </author> <title> The Synthesizer Generator: A System for Constructing Language-Based Editors. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Therefore, we can indeed perform grammar ordering test using the quadratic-time algorithm. Moreover, using deterministic grammars also within each iteration actually produces overall more precise analysis results. These algorithms are being implemented using the Synthesizer Generator <ref> [45] </ref>. The last question left is: how many iterations are needed in the fixed-point computation? We do not have an exact characterization yet, but our experience with small examples so far is that the number is small.
Reference: [46] <author> T. Reps and T. Turnidge. </author> <title> Program specialization via program slicing. </title> <editor> In O. Danvy, R. Gluck, and P. Thiemann, editors, </editor> <booktitle> Proceedings of the Dagstuhl Seminar on Partial Evaluation, volume 1110 of Lecture Notes in Computer Science, </booktitle> <pages> pages 409-429. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: In particular, dead code produces values that never get used. Such code appears often as a result of program optimization, modification, and reuse [38, 3]. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing <ref> [54, 46] </ref>, program specializa tion [46], and compile-time garbage collection [27, 41]. Analysis for identifying live or dead code, or code of similar relevance, has been studied and used widely [10, 9, 28, 39, 3, 27, 23, 15, 29, 34, 33, 50, 46]. <p> Such code appears often as a result of program optimization, modification, and reuse [38, 3]. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [54, 46], program specializa tion <ref> [46] </ref>, and compile-time garbage collection [27, 41]. Analysis for identifying live or dead code, or code of similar relevance, has been studied and used widely [10, 9, 28, 39, 3, 27, 23, 15, 29, 34, 33, 50, 46]. <p> We call this dead-code analysis, bearing in mind that it may be used for many other purposes. In recent years, dependence analyses have been made more precise so as to be effective in more complicated computations [53, 30, 37, 16, 41], and this is particularly true for dead-code analysis <ref> [23, 15, 29, 33, 46, 7] </ref>. Since recursive data constructions are used increasingly widely in high-level languages, an important problem is dependence analysis for recursive data. The goal of dead-code analysis here is to identify par fl This work is supported in part by NSF Grant CCR-9711253. <p> It is difficult because the structures of general recursive data can be user-defined, and dead recursive substructures are interleaved with other recursive substructures on which the computations are live. Several methods have been studied, but all have limitations <ref> [27, 23, 33, 46] </ref>. Similar limitations exist also in other analyses for recursive data [53, 30, 37, 16, 41]. This paper describes a general and powerful method for analyzing dependencies for recursive data. <p> Domain projections [47, 20] can be used to project out parts of data that are of interest <ref> [53, 30, 37, 46] </ref>, in our case, the live parts. Let X be the domain of all possible values computed by our programs, including ? and values containing . <p> Grammar-based projections. Regular tree grammars have been used to describe recursive substructures and other data flow information <ref> [26, 36, 37, 5, 48, 14, 46] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> This is called backward slicing [54], and it helps debug and reuse program pieces. It can also be regarded as a kind of program specialization with respect to program output <ref> [46] </ref>. Dead-code elimination [3] is a straightforward application of our work. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [26], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [36, 37, 5, 16, 48, 46] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [46] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis. <p> It is later used to describe other data flow information such as types and binding times [36, 37, 5, 16, 48, 46]. In particular, the analysis for backward slicing by Reps and Turnidge <ref> [46] </ref> explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [47] <author> D. S. Scott. </author> <title> Lectures on a mathematical theory of computation. </title> <editor> In M. Broy and G. Schmidt, editors, </editor> <booktitle> Theoretical Foundations of Programming Methodology, </booktitle> <pages> pages 145-292. </pages> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1982. </year>
Reference-contexts: If the language is call-by-value, then the resulting program terminates with the correct value whenever the original program does; if the language is call-by-name, then the resulting program terminates with the correct value exactly when the original program does. 3 Representing partially dead recur sive data Projections. Domain projections <ref> [47, 20] </ref> can be used to project out parts of data that are of interest [53, 30, 37, 46], in our case, the live parts. Let X be the domain of all possible values computed by our programs, including ? and values containing .
Reference: [48] <author> M. H. Sorensen. </author> <title> A grammar-based data-flow analysis to stop deforestation. </title> <editor> In S. Tison, editor, CAAP'94: </editor> <booktitle> Proceedings of the 19th International Colloquium on Trees in Algebra and Programming, volume 787 of Lecture Notes in Computer Science, </booktitle> <pages> pages 335-351. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Grammar-based projections. Regular tree grammars have been used to describe recursive substructures and other data flow information <ref> [26, 36, 37, 5, 48, 14, 46] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> This approximates the possibly non-existing fixed point, forcing the iteration to terminate while guaranteeing that the sufficiency conditions are satisfied. Widening operations have been defined implicitly <ref> [4, 48] </ref> or explicitly [14] for regular tree grammars. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [26], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [36, 37, 5, 16, 48, 46] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [46] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [49] <author> L. J. Stockmeyer and A. R. Meyer. </author> <title> Word problems requiring expoenetial time. </title> <booktitle> In Conference Proceedings of the 5th Annual ACM STOC, </booktitle> <pages> pages 1-9. </pages> <publisher> ACM, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: In fact, inclusion test for regular tree grammars is exponential. In particular, it is at least PSPACE-complete, since inclusion test for regular string grammars is <ref> [49, 40] </ref>. Nevertheless, for deterministic regular tree grammars, a quadratic-time inclusion test exists [35]. Recall from Section 5 that applying widening operation after each iteration results in a deterministic grammar. Therefore, we can indeed perform grammar ordering test using the quadratic-time algorithm.
Reference: [50] <author> F. </author> <title> Tip. A survey of program slicing techniques. </title> <journal> Journal of Programming Languages, </journal> <volume> 3(3) </volume> <pages> 121-189, </pages> <month> Sept. </month> <year> 1995. </year>
Reference: [51] <author> P. Wadler. </author> <title> Strictness analysis on non-flat domains (by abstract interpretation over finite domains). </title> <editor> In S. Abramsky and C. Hankin, editors, </editor> <booktitle> Abstract Interpretation of Declarative Languages, </booktitle> <pages> pages 266-275. </pages> <editor> E. </editor> <publisher> Horwood, Chichester; Hal-sted Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Our analysis and framework can be used for these analyses. Efficient implementation of lazy functional languages. Strictness analysis identifies arguments of functions that are necessarily evaluated so that we can evaluate them immediately rather than building data structures to be evaluated later <ref> [24, 51, 53] </ref>. While dead-code analysis looks for the minimum sufficient information for producing an output, strictness analysis looks for the maximum necessary information.
Reference: [52] <author> P. Wadler. </author> <title> Deforestation: Transforming programs to eliminate trees. </title> <journal> Theoret. Comput. Sci., </journal> <volume> 73 </volume> <pages> 231-248, </pages> <year> 1990. </year> <booktitle> Special issue of selected papers from the 2nd European Symposium on Programming. </booktitle>
Reference-contexts: Then dead-code elimination can simply replace all subex-pressions that are completely dead with . Deforestation and fusion <ref> [52, 8] </ref> combine function applications to avoid building large intermediate results. To guarantee that the optimization can be done effectively, the functions and subexpressions must satisfy certain conditions, e.g., be in blazed treeless form [52]. <p> Deforestation and fusion [52, 8] combine function applications to avoid building large intermediate results. To guarantee that the optimization can be done effectively, the functions and subexpressions must satisfy certain conditions, e.g., be in blazed treeless form <ref> [52] </ref>. Our analysis helps identify and eliminate functions and subexpressions that do not satisfy these conditions, thus making deforestation more widely applicable.
Reference: [53] <author> P. Wadler and R. J. M. Hughes. </author> <title> Projections for strictness analysis. </title> <booktitle> In Proceedings of the 3rd International Conference on Functional Programming Languages and Computer Architecture, volume 274 of Lecture Notes in Computer Science, </booktitle> <pages> pages 385-407. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> Sept. </month> <year> 1987. </year>
Reference-contexts: We call this dead-code analysis, bearing in mind that it may be used for many other purposes. In recent years, dependence analyses have been made more precise so as to be effective in more complicated computations <ref> [53, 30, 37, 16, 41] </ref>, and this is particularly true for dead-code analysis [23, 15, 29, 33, 46, 7]. Since recursive data constructions are used increasingly widely in high-level languages, an important problem is dependence analysis for recursive data. <p> Several methods have been studied, but all have limitations [27, 23, 33, 46]. Similar limitations exist also in other analyses for recursive data <ref> [53, 30, 37, 16, 41] </ref>. This paper describes a general and powerful method for analyzing dependencies for recursive data. We represent partially dead recursive data using projections based on general regular tree grammars extended with notions of live and dead. <p> Domain projections [47, 20] can be used to project out parts of data that are of interest <ref> [53, 30, 37, 46] </ref>, in our case, the live parts. Let X be the domain of all possible values computed by our programs, including ? and values containing . <p> Our analysis and framework can be used for these analyses. Efficient implementation of lazy functional languages. Strictness analysis identifies arguments of functions that are necessarily evaluated so that we can evaluate them immediately rather than building data structures to be evaluated later <ref> [24, 51, 53] </ref>. While dead-code analysis looks for the minimum sufficient information for producing an output, strictness analysis looks for the maximum necessary information. <p> Wadler and Hughes use projections for strictness analysis <ref> [53] </ref>. Their analysis is also backward but seeks necessary rather than sufficient information, and it uses a fixed finite abstract domain for all programs. Launchbury uses projections for binding-time analysis of partially static data structures in partial evaluation [30].
Reference: [54] <author> M. Weiser. </author> <title> Program slicing. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> SE-10(4):352-357, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: In particular, dead code produces values that never get used. Such code appears often as a result of program optimization, modification, and reuse [38, 3]. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing <ref> [54, 46] </ref>, program specializa tion [46], and compile-time garbage collection [27, 41]. Analysis for identifying live or dead code, or code of similar relevance, has been studied and used widely [10, 9, 28, 39, 3, 27, 23, 15, 29, 34, 33, 50, 46]. <p> Slicing. Starting at a particular index in the program, not necessarily the final result of the entire program, the analysis helps slice out data and computations that are possibly needed for that index. This is called backward slicing <ref> [54] </ref>, and it helps debug and reuse program pieces. It can also be regarded as a kind of program specialization with respect to program output [46]. Dead-code elimination [3] is a straightforward application of our work.
References-found: 54

