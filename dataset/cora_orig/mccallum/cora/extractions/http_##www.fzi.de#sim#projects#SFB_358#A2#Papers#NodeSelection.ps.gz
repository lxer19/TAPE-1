URL: http://www.fzi.de/sim/projects/SFB_358/A2/Papers/NodeSelection.ps.gz
Refering-URL: http://www.fzi.de/sim/people/trautw/index.ok.html
Root-URL: http://www.fzi.de
Email: EMail: -trautw|rosen-@peanuts.informatik.uni-tuebingen.de  
Title: Selecting a Node and an Algorithm in a Heterogeneous Net of Workstations heterogeneous network of
Author: Ch. Trautwein W. Rosenstiel Universitt Tbingen 
Keyword: distributed systems, remote procedure call (RPC), asynchronous RPC, parallelism, load management, local area networks, performance, functional programming, parallel programming  
Note: a  
Address: Sand 13 72076 Tbingen Germany  
Affiliation: Technische Informatik  
Abstract: Remote function calls (RFC) proved to be a workable paradigm for writing distributed applications. Asynchronous RFCs (ARFC) can be used to support parallelism. At the time the ARFC is executed a set of networked machines is available. The machine with the lowest expected latency has to be selected. A method for selecting the optimal node and an implementation using pvm is presented. To get massive parallelism one can use recursive ARFCs (RARFC). This paradigm fits good to a large number of applications. To prevent excessive parallelism one has to decide (at runtime) when the recursion has to stop. Speaking more general: The fastest algorithm for a actual problem has to be selected. A way to determine the fastest algorithm and an implementation is presented. Measurements on a network of workstations illustrate the effect of node and algorithm selection. 
Abstract-found: 1
Intro-found: 1
Reference: [AA92] <author> B.H. Tay A.L. Ananda. </author> <title> A Survey os Asynchronous Remote Procedure Calls. Operating Systems Review, </title> <address> 29(2):92109, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Widely used languages (e.g. C) are supported and a reasonable number of programmers are familiar with it. Using minor changes in of the RFC paradigm enables the use of workstation clusters as parallel computer. To get parallelism the RFCs have to be asynchronous <ref> [AA92] </ref>. The semantic of the call is not changed by this. Syntactic changes enable the programmer to get a parallel version of his program. As a second change compared to standard RFCs one need not specify the server host for the call. The runtime system decides which host to use.
Reference: [CCDS93] <author> Timothy G. Mattson Craig C. Douglas and Martin H. Schultz. </author> <title> Parallel Programming Systems for Workstation Clusters. </title> <type> Technical Report YALEU/DCS/TR-975, </type> <institution> Yale University Department of Computer Science, </institution> <month> August </month> <year> 1993. </year> <note> URL: http://www.hensa.ac.uk/parallel/papers/surveys/par-prog-workstation-clusters.ps.gz. </note>
Reference-contexts: Today exist a number of languages with support for distributed systems [HEB89]. We want to use networked workstations as distributed systems [Tur93]. For this we cannot use languages based on shared memory. Exchanging messages has a high latency on the networked workstations <ref> [CCDS93] </ref>. This makes virtual shared memory based languages inefficient. The language to use has to support logically distributed address spaces.
Reference: [DLE86] <author> John Zahorjan Derek L. Eager, Ewdard D. Lazowska. </author> <title> Adaptive Load Sharing in Homogeneous Distributed Systems. </title> <journal> IEEE Trans on Software Engineering, </journal> <volume> SE-12(5):662675, </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: This work gives an example how such a support system can be build and shows our experience in building a prototype system. Load management is a key component in a distributed system <ref> [DLE86] </ref>. Two components can be identified in a load management system, the transfer policy and the location policy. The transfer policy determines if a task is to be executed locally or remotely. This decision is made implicitly by the algorithm selection module.
Reference: [HEB89] <author> Andrew S. Tanenbaum Henri E. Bal, Jennifer G. </author> <title> Steiner. Programming Languages for Distributed Computing Systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(3), </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: 1. Introduction Programming a distributed computing system has to be supported by an programming language. The first support offered by distributed systems were libraries for sending and receiving messages. Using these message passing libraries was labouresome and error prone. Today exist a number of languages with support for distributed systems <ref> [HEB89] </ref>. We want to use networked workstations as distributed systems [Tur93]. For this we cannot use languages based on shared memory. Exchanging messages has a high latency on the networked workstations [CCDS93]. This makes virtual shared memory based languages inefficient.
Reference: [NC89] <author> David Gelernter Nicholas Carriero. </author> <title> How to Write Parallel Programs: A Guide to the Perplexed. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(3), </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: The runtime system decides which host to use. The workstations networks are not seen as a distributed system by most programmers. This is not caused by the deficits of the hardware but of the difficulty to program a pool of workstations <ref> [NC89] </ref>. Using an easy paradigm for programming these networked workstations and an automatic support in choosing the best algorithm and node for execution supports the inexperienced programmer of distributed application.
Reference: [Tur93] <author> Louis H. Turcotte. </author> <title> A Survery of Software Envorinments for Exploiting Networked Computing Resources. </title> <address> URL: file://netlib2.cs.utk.edu/nse/docs/soft-env-net-report.ps, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The first support offered by distributed systems were libraries for sending and receiving messages. Using these message passing libraries was labouresome and error prone. Today exist a number of languages with support for distributed systems [HEB89]. We want to use networked workstations as distributed systems <ref> [Tur93] </ref>. For this we cannot use languages based on shared memory. Exchanging messages has a high latency on the networked workstations [CCDS93]. This makes virtual shared memory based languages inefficient. The language to use has to support logically distributed address spaces.
Reference: [vdS93] <author> Aad J. van der Steen. </author> <title> An overview of (almost) available parallel systems. </title> <type> Technical report, </type> <note> Publication of the NCF, December 1993. URL: http://www.netlib.org/benchmark/mp-computers.ps. </note>
References-found: 7

