URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3305/3305.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Vision and Action  
Author: Cornelia Fermuller Yiannis Aloimonos 
Note: Written on the occasion of the 30 th anniversary of the  
Address: College Park, MD 20742-3275  
Affiliation: Computer Vision Laboratory Center for Automation Research University of Maryland  Computer Vision Laboratory at Maryland.  
Date: June 1994  
Pubnum: CAR-TR-722  DACA76-92-C-0009 IRI-90-57934  
Abstract: Our work on Active Vision has recently focused on the computational modelling of navigational tasks, where our investigations were guided by the idea of approaching vision for behavioral systems in form of modules that are directly related to perceptual tasks. These studies led us to branch in various directions and inquire into the problems that have to be addressed in order to obtain an overall understanding of perceptual systems. In this paper we present our views about the architecture of vision systems, about how to tackle the design and analysis of perceptual systems, and promising future research directions. Our suggested approach for understanding behavioral vision to realize the relationship of perception and action builds on two earlier approaches, the Medusa philosophy [3] and the Synthetic approach [15]. The resulting framework calls for synthesizing an artificial vision system by studying vision competences of increasing complexity and at the same time pursuing the integration of the perceptual components with action and learning modules. We expect that Computer Vision research in the future will progress in tight collaboration with many other disciplines that are concerned with empirical approaches to vision, i.e. the understanding of biological vision. Throughout the paper we describe biological findings that motivate computational arguments which we believe will influence studies of Computer Vision in the near future. The support of the Advanced Research Projects Agency (ARPA Order No. 8459) and the U.S. Army Topographic Engineering Center under Contract DACA76-92-C-0009, the National Science Foundation under Grant IRI-90-57934, and the Office of Naval Research under Contract N00014-93-1-0257, is gratefully acknowledged, as is the help of Sandy German in preparing this paper. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Aloimonos and C. Brown. </author> <title> Direct processing of curvilinear sensor motion from a sequence of perspective images. </title> <booktitle> In Proc. Workshop on Computer Vision: Representation and Control, </booktitle> <pages> pages 72-77, </pages> <year> 1984. </year> <month> 34 </month>
Reference-contexts: As a matter of fact, in part only the sign of the normal flow is employed. It should be mentioned that a few techniques using normal flow have appeared in the literature; however, they deal only with restricted cases (only translation or only rotation <ref> [1, 27] </ref>). Another characteristic is that the constraints developed for the motion modules, for which the rigid motion module is the correct one globally, are such that the input also is utilized globally.
Reference: [2] <author> J. Aloimonos, I. Weiss, and A. Bandopadhay. </author> <title> Active vision. </title> <journal> International Journal of Computer Vision, </journal> <volume> 2 </volume> <pages> 333-356, </pages> <year> 1988. </year>
Reference-contexts: A good theory of vision would be one that can create an interface between perception and other cognitive abilities. However, with a formal theory integrating perception and action still lacking, most studies have treated Active Vision <ref> [2, 4] </ref> as an extension of the classical reconstruction theory, employing activities only as a means to regularize the classical ill-posed inverse problems.
Reference: [3] <author> J.Y. Aloimonos. </author> <title> Purposive and qualitative active vision. </title> <booktitle> In Proc. DARPA Image Understanding Workshop, </booktitle> <pages> pages 816-828, </pages> <year> 1990. </year>
Reference-contexts: To give some examples from navigation: The problem of independent motion detection by a moving observer usually has been addressed 9 with techniques for segmenting optical flow fields. But it also may be tackled through the recog-nition of non-rigid flow fields for a moving observer partially knowing its motion <ref> [3, 41, 61] </ref>. The problem of obstacle detection could be solved by recognizing a set of locations on the retina that represent the image of a part of the 3D world being on a collision course with the observer.
Reference: [4] <author> R. </author> <title> Bajcsy. Active perception. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 76 </volume> <pages> 996-1005, </pages> <year> 1988. </year>
Reference-contexts: A good theory of vision would be one that can create an interface between perception and other cognitive abilities. However, with a formal theory integrating perception and action still lacking, most studies have treated Active Vision <ref> [2, 4] </ref> as an extension of the classical reconstruction theory, employing activities only as a means to regularize the classical ill-posed inverse problems.
Reference: [5] <author> D. Boussaud, L. Ungerleider, and R. DeSimone. </author> <title> Pathways for motion analysis: cortical connections of the medial superior temporal fundus of the superior temporal visual areas in the macaque monkey. </title> <journal> Journal of Comparative Neurology, </journal> <volume> 296 </volume> <pages> 462-495, </pages> <year> 1990. </year>
Reference-contexts: Also, recently the existence of a third pathway leading to the identification of actions has been suggested <ref> [5] </ref>. Results from the brain sciences show us that there doesn't exist just one hierarchy of visual processes, but various different computations are performed in parallel. Also, it isn't our intention to propose one strict hierarchy for developing visual competences.
Reference: [6] <author> M. Brady, J. Hollerbach, T. Johnson, T. Lozano-Perez, and M. Mason, </author> <title> editors. Robot Motion. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: In the same way, the problem of hand/eye coordination can be dealt with using stereo and other techniques to compute the depth map and then solve the inverse kinematics problem in order to move the arm. While the arm is moving the system is blind <ref> [6] </ref>. However the same problem can be solved by creating a mapping (the perceptual kinematic map) from image features to the robot's joints; the positioning of the arm is achieved by recognizing the image features [25].
Reference: [7] <author> R. Brooks. </author> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 2 </volume> <pages> 14-23, </pages> <year> 1986. </year>
Reference-contexts: The synthetic approach to Medusa has some similarities at the philosophical level with Brooks' proposal about understanding intelligent behavior through the construction of working mechanisms <ref> [7] </ref>. In proposing the subsumption architecture, Brooks suggested a hierarchy of of competences such as avoiding contact with objects, exploring the world by seeing places, reasoning about the world in terms of identifiable objects, etc. This proposal, however, suffered from the same curse of generality that weakened Marr's approach.
Reference: [8] <author> T. Collett, E. Dillmann, A. Giger, and R. Wehner. </author> <title> Visual landmarks and route following in desert ants. </title> <journal> Journal of Comparative Physiology A, </journal> <volume> 170 </volume> <pages> 435-442, </pages> <year> 1992. </year>
Reference-contexts: However, research from studies on arthropods <ref> [8, 9, 67] </ref> shows that already in these simple animals, the competence of homing is realized in many ways. A large variety of different ways employing combinations of information from action and perception have been discovered.
Reference: [9] <author> T. Collett, S. Fry, and R. Wehner. </author> <title> Sequence learning by honeybees. </title> <journal> Journal of Comparative Physiology A, </journal> <volume> 172 </volume> <pages> 693-706, </pages> <year> 1993. </year>
Reference-contexts: However, research from studies on arthropods <ref> [8, 9, 67] </ref> shows that already in these simple animals, the competence of homing is realized in many ways. A large variety of different ways employing combinations of information from action and perception have been discovered.
Reference: [10] <author> T. Collett, U. Schwartz, and E. Sobel. </author> <title> The interaction of oculomotor cues and stimulus size in stereoscopic depth constancy. </title> <journal> Perception, </journal> <volume> 20 </volume> <pages> 733-754, </pages> <year> 1991. </year>
Reference-contexts: Maybe it is a hopeless task to aim at deriving metric shape or depth information. Psychophysical experiments indicate that binocular stereopsis in the human visual system does not produce an explicit representation of the metric depth structure of the scene. Psychophysical evidence <ref> [10, 33] </ref> suggests that human performance in tasks involving metric structure from binocular disparities is very poor. Also, other cues don't seem to allow humans to extract the kind of depth information that has usually been considered.
Reference: [11] <author> C. Duffy and R. Wurtz. </author> <title> Sensitivity of MST neurons to optical flow stimuli I: a continuum of response selectivity to large field stimuli. </title> <journal> Journal of Neurophysiology, </journal> <volume> 65 </volume> <pages> 1329-1345, </pages> <year> 1991. </year>
Reference-contexts: Combining the computed values allows us to derive the direction of an object's translation [18]. 20 3.5 A look at the motion pathway There is a very large amount of literature <ref> [11, 38, 60, 65] </ref> on the properties of neurons involved in motion analysis.
Reference: [12] <author> G. Ernst and A. Newell. </author> <title> GPS: A Case Study in Generality and Problem Solving. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1969. </year>
Reference-contexts: This is not surprising, since Computer Vision was considered as a subfield of Artificial Intelligence and thus studied using the same methodology, influenced by the ideas and computational theories of the last decades <ref> [12, 21, 44] </ref>. The strict hierarchical organization of representational steps in the Marr paradigm makes the development of learning, adaptation and generalization processes practically impossible (so that there hasn't been much work on "vision and learning").
Reference: [13] <author> M. Farah. </author> <title> Visual Agnosia: Disorders of Object Recognition and What They Tell us about Normal Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Recent results from Cognitive Neurophysiology|the discipline which is concerned, among other topics, with the study of visual agnosia (a condition exhibited by patients with partially damaged brains) <ref> [13, 30] </ref>| indicate that the human brain is not designed in a clean, modular fashion, but consists of several processes working in a cooperative, distributed manner.
Reference: [14] <author> O. Faugeras. </author> <title> Three Dimensional Computer Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year> <month> 35 </month>
Reference-contexts: The body of projective geometry has been investigated to prove results about the computation of structure and motion from a set of views under perspective projection <ref> [14] </ref>. The learning of object recognition capabilities has been studied for neuronal networks using nodes that store viewer-centered projections [52], and geometric studies on so-called aspect graphs have investigated how different kinds of geometric properties change with the views the observer has of the geometric model [34].
Reference: [15] <author> C. Fermuller. </author> <title> Basic Visual Capabilities. </title> <type> PhD thesis, </type> <institution> Institute for Automation, University of Technology, Vienna, Austria, </institution> <note> available as Technical Report CAR-TR-668, </note> <institution> Center for Automation Research, University of Maryland, </institution> <year> 1993. </year>
Reference: [16] <editor> C. Fermuller. Navigational preliminaries. In Y. Aloimonos, editor, </editor> <title> Active Perception, </title> <booktitle> Advances in Computer Vision. </booktitle> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1993. </year>
Reference-contexts: As we will demonstrate, the estimation of an observer's motion can indeed be based on only the rigid motion model. A geometric analysis of motion fields reveals that the rigid motion parameters manifest themselves in the form of patterns defined on partial components of the motion fields <ref> [16] </ref>. Algorithmically speaking, the estimation of motion thus can be performed through pattern recognition techniques. Another competence, the estimation of partial information about an object's motion (its direction of translation), can be based on the same model. <p> These findings allow us to formulate the problem of ego-motion estimation as a pattern recognition problem. By localizing for different classes of normal flow vectors the positive and negative areas in the image plane, the parameters for the axis of translation and direction of rotation can be derived <ref> [16] </ref>. Also, based on the same basic constraints, a process for the detection of independent motion has been designed. Since the observer is moving rigidly, an area with a motion field not possibly due to only one rigid motion must contain an independently moving object.
Reference: [17] <author> C. Fermuller and Y. Aloimonos. </author> <title> Tracking facilitates 3-d motion estimation. </title> <journal> Biological Cybernetics, </journal> <volume> 67 </volume> <pages> 259-268, </pages> <year> 1992. </year>
Reference-contexts: are directly related to the visual task the observer is engaged in, we argue that in many cases when an object is moving in an unrestricted manner (translation and rotation) in the 3D world, we are only interested in the object's translational component, which can be extracted using dynamic fixation <ref> [17] </ref>. Next in the hierarchy follow the capabilities of independent motion detection and obstacle avoidance.
Reference: [18] <author> C. Fermuller and Y. Aloimonos. </author> <title> The role of fixation in visual motion analysis. </title> <journal> International Journal of Computer Vision, </journal> <note> Special Issue on Active Vision, </note> <editor> M. Swain (Ed.), </editor> <volume> 11 </volume> <pages> 165-186, </pages> <year> 1993. </year>
Reference-contexts: By tracking the object over a small amount of time, the observer derives additional information about the translation perpendicular to the image plane. Combining the computed values allows us to derive the direction of an object's translation <ref> [18] </ref>. 20 3.5 A look at the motion pathway There is a very large amount of literature [11, 38, 60, 65] on the properties of neurons involved in motion analysis.
Reference: [19] <author> K. Frisch. </author> <title> Die Polarisation des Himmelslichts als orientierender Faktor bei den Tanzen der Bienen. </title> <journal> Experientia, </journal> <volume> 5 </volume> <pages> 142-148, </pages> <year> 1949. </year>
Reference-contexts: In particular, effort has been spent on investigating the sensory basis of animals' perception; discoveries were made about the use of sensory guidance by sunlight, light patterns in the sky, and moonlight, such as the use of ultraviolet light by ants [36] and polarized light by bees <ref> [19] </ref>. Recently, research has also started on investigations of how particular species organize the spatial information acquired through their motor sequences and sensors [56, 59]. Zoologists differentiate two mechanisms of acquiring orientation: the use of ego-centered and geo-centered systems of reference.
Reference: [20] <author> D. </author> <title> Gabor. </title> <journal> Theory of communication. Journal of the IEE, </journal> <volume> 93 (part III):429-457, </volume> <year> 1946. </year>
Reference-contexts: The problem of solving both localization and recognition is exactly the antagonistic conflict at the heart of pattern recognition. From the point of signal processing, it has been proved <ref> [20] </ref> that any single (linear) operator can answer only one of these questions with sufficient accuracy. In theory, thus, a number of processes are required to solve tasks related to space perception.
Reference: [21] <author> H. Gelernter. </author> <title> Realization of a geometry theorem-proving machine. </title> <booktitle> In Information Processing: Proceedings of the International Conference on Information Processing, UNESCO, </booktitle> <year> 1959. </year>
Reference-contexts: This is not surprising, since Computer Vision was considered as a subfield of Artificial Intelligence and thus studied using the same methodology, influenced by the ideas and computational theories of the last decades <ref> [12, 21, 44] </ref>. The strict hierarchical organization of representational steps in the Marr paradigm makes the development of learning, adaptation and generalization processes practically impossible (so that there hasn't been much work on "vision and learning").
Reference: [22] <author> E.B.S. Geman and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: Standard neural networks (such as feed-forward networks) have been shown to yield unpredictable results when required to extrapolate <ref> [22] </ref>. Learning an input output behavior amounts to learning the probability distribution p (a; b) of the input output pairs. Thus, given an input a 0 , the system would pick an answer from the distribution p (a 0 ; b). <p> A more flexible system will be able to reduce the error in bias, but as a consequence it will have to be punished with a large error due to variance. Geman et al. <ref> [22] </ref> claim that learning of complex tasks is essentially impossible without carefully introducing systems bias. In other words, it is essential that we choose an appropriate stimulus representation. To learn the map m : A ! B amounts then to learning the distribution p (a; b) of the stimulus-response pairs.
Reference: [23] <author> M. Goodale, A. Milner, L. Jacobson, and D. Carey. </author> <title> A neurological dissociation between perceiving objects and grasping them. </title> <journal> Nature, </journal> <volume> 349 </volume> <pages> 154-156, </pages> <year> 1991. </year>
Reference-contexts: The dorsal path is concerned with either the computations concerned with "where" (object localization) or "how" (the visual guidance of movements <ref> [23] </ref>), and the ventral path with the computations concerned with "what" (object identification).
Reference: [24] <author> R. Gregory. </author> <title> Distortion of visual space as inappropriate constancy scaling. </title> <booktitle> Nature, </booktitle> <address> 119:678, </address> <year> 1963. </year>
Reference-contexts: The assumption was that the brain is designed in a modular, principled fashion, and thus from the study of perceptual malfunctions (illusions <ref> [24] </ref>), information about its design can be deduced.
Reference: [25] <author> J. Herve. </author> <title> Navigational Vision. </title> <type> PhD thesis, </type> <institution> University of Maryland, Computer Vision Laboratory, Center for Automation Research, University of Maryland, </institution> <year> 1993. </year>
Reference-contexts: While the arm is moving the system is blind [6]. However the same problem can be solved by creating a mapping (the perceptual kinematic map) from image features to the robot's joints; the positioning of the arm is achieved by recognizing the image features <ref> [25] </ref>. Instead of reconstructing the world, the problems described above are solved through the recognition of entities that are directly relevant to the task at hand. These entities are represented by only those parameters sufficient to solve the specific task.
Reference: [26] <author> J. </author> <title> Hopfield. Neural networks and physical systems with emergent collective computational capabilities. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 79 </volume> <pages> 2554-2558, </pages> <year> 1982. </year>
Reference-contexts: In such networks, all neurons must participate in the computation, even if only a few contribute to the output. The effort to process an input in a fully connected network capable of exact classification of n memories is O (n 2 ) <ref> [26] </ref>. In a one-layer net, each neuron defines a hyperplane. The input is processed by letting each neuron determine on which side of the hyperplane the input lies. This results in a waste of computational power since many of the comparisons will be unnecessary.
Reference: [27] <author> B. Horn and E. Weldon. </author> <title> Computationally efficient methods for recovering translational motion. </title> <booktitle> In Proc. International Conference on Computer Vision, </booktitle> <pages> pages 2-11, </pages> <year> 1987. </year> <month> 36 </month>
Reference-contexts: As a matter of fact, in part only the sign of the normal flow is employed. It should be mentioned that a few techniques using normal flow have appeared in the literature; however, they deal only with restricted cases (only translation or only rotation <ref> [1, 27] </ref>). Another characteristic is that the constraints developed for the motion modules, for which the rigid motion module is the correct one globally, are such that the input also is utilized globally.
Reference: [28] <author> G. Horridge. </author> <title> The evolution of visual processing and the construction of seeing systems. </title> <journal> Pro--ceedings of the Royal Society, London B, </journal> <volume> 230 </volume> <pages> 279-292, </pages> <year> 1987. </year>
Reference-contexts: Other scientific disciplines give us some answer. Much simpler than the human visual system are the perceptual systems of lower animals, like medusae, worms, crustaceans, insects, spiders and molluscs. Researchers in neuroethology have been studying such systems and have by now gained a great deal of understanding. Horridge <ref> [28, 29] </ref>, working on insect vision, studied the evolution of visual mechanisms and proposed hierarchical classifications of visual capabilities. He argued that the most basic capabilities found in animals are based on motion. Animals up to the complexity of insects perceive objects entirely by relative motion.
Reference: [29] <author> G. Horridge. </author> <title> Evolution of visual processing. </title> <editor> In J. Cronly-Dillon and R. Gregory, editors, </editor> <title> Vision and Visual Dysfunction. </title> <publisher> MacMillan, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Other scientific disciplines give us some answer. Much simpler than the human visual system are the perceptual systems of lower animals, like medusae, worms, crustaceans, insects, spiders and molluscs. Researchers in neuroethology have been studying such systems and have by now gained a great deal of understanding. Horridge <ref> [28, 29] </ref>, working on insect vision, studied the evolution of visual mechanisms and proposed hierarchical classifications of visual capabilities. He argued that the most basic capabilities found in animals are based on motion. Animals up to the complexity of insects perceive objects entirely by relative motion.
Reference: [30] <author> G. Humphreys and M. Riddoch. </author> <title> To See But Not To See: A Case Study of Visual Agnosia. </title> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, New Jersey, </address> <year> 1992. </year>
Reference-contexts: Recent results from Cognitive Neurophysiology|the discipline which is concerned, among other topics, with the study of visual agnosia (a condition exhibited by patients with partially damaged brains) <ref> [13, 30] </ref>| indicate that the human brain is not designed in a clean, modular fashion, but consists of several processes working in a cooperative, distributed manner.
Reference: [31] <author> D. Jacobs. </author> <title> Space efficient 3d model indexing. </title> <booktitle> In Proc. IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 439-444, </pages> <year> 1992. </year>
Reference-contexts: results in the literature of structure from motion, that show that under parallel projection any view of an object can be constructed as a linear combination of a small number of views of the same object, a series of studies on recognition using orthographic and paraperspective projections have been conducted <ref> [31, 64] </ref>. The body of projective geometry has been investigated to prove results about the computation of structure and motion from a set of views under perspective projection [14].
Reference: [32] <author> G. Johansson. </author> <title> Visual perception of biological motion and a model for its analysis. </title> <journal> Perception and Psychophysics, </journal> <volume> 14 </volume> <pages> 201-211, </pages> <year> 1973. </year>
Reference-contexts: Representations might be object-centered, ego-centered, or action-driven. Actions can be very typical for objects. Early perceptual studies have shown that humans are able to interpret moving scenes correctly, even when the static view does not contain information about the structure at all. In the experiments of Johansson <ref> [32] </ref> subjects were able to recognize animals, as well as specific human beings, given only the motions of light bulbs mounted on the object's joints. <p> Humans possess a remarkable capability for recognizing situations, scenes, and objects in the space surrounding them from actions being performed. In the Computer Vision literature a number of experiments are often cited <ref> [32] </ref> in which it has been shown that humans can recognize specific animals and humans that move in the dark and are visible only from a set of light bulbs attached to their joints. These experiments demonstrate very well the power of motion cues.
Reference: [33] <author> E. Johnston. </author> <title> Systematic distortions of shape from stereopsis. </title> <journal> Vision Research, </journal> <volume> 31 </volume> <pages> 1351-1360, </pages> <year> 1991. </year>
Reference-contexts: Maybe it is a hopeless task to aim at deriving metric shape or depth information. Psychophysical experiments indicate that binocular stereopsis in the human visual system does not produce an explicit representation of the metric depth structure of the scene. Psychophysical evidence <ref> [10, 33] </ref> suggests that human performance in tasks involving metric structure from binocular disparities is very poor. Also, other cues don't seem to allow humans to extract the kind of depth information that has usually been considered.
Reference: [34] <author> J. Koenderink and A. van Doorn. </author> <title> The internal representation of solid shape with respect to vision. </title> <journal> Biological Cybernetics, </journal> <volume> 32 </volume> <pages> 211-216, </pages> <year> 1979. </year>
Reference-contexts: The learning of object recognition capabilities has been studied for neuronal networks using nodes that store viewer-centered projections [52], and geometric studies on so-called aspect graphs have investigated how different kinds of geometric properties change with the views the observer has of the geometric model <ref> [34] </ref>. The problem of solving both localization and recognition is exactly the antagonistic conflict at the heart of pattern recognition. From the point of signal processing, it has been proved [20] that any single (linear) operator can answer only one of these questions with sufficient accuracy.
Reference: [35] <author> J. Koenderink and A. van Doorn. </author> <title> Affine structure from motion. </title> <journal> Journal of the Optical Society of America, </journal> <volume> 8 </volume> <pages> 377-385, </pages> <year> 1991. </year>
Reference-contexts: By concentrating on simpler shape descriptions, new mathematical models and new constraints might be found. Purely mathematical considerations can reveal what kind of information could possibly be computed from a certain input allowing a defined class of operations. The study of Koenderink and van Doorn <ref> [35] </ref> on affine structure from motion might serve as an inspiration; in it they investigated a hierarchy of shape descriptions based on a stratification of geometries. 3.7 Space understanding Since in the past the actions of the observer were not considered as an integral part of perceptual investigations, computational modelling, and
Reference: [36] <author> J. </author> <title> Lubbock. On the Senses, Instincts, and Intelligence of Animals with Special Reference to Insects. </title> <editor> K. Paul Trench, </editor> <address> London, </address> <month> 1889. </month>
Reference-contexts: In particular, effort has been spent on investigating the sensory basis of animals' perception; discoveries were made about the use of sensory guidance by sunlight, light patterns in the sky, and moonlight, such as the use of ultraviolet light by ants <ref> [36] </ref> and polarized light by bees [19]. Recently, research has also started on investigations of how particular species organize the spatial information acquired through their motor sequences and sensors [56, 59]. Zoologists differentiate two mechanisms of acquiring orientation: the use of ego-centered and geo-centered systems of reference.
Reference: [37] <author> D. Marr. </author> <title> Vision. W.H. </title> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1982. </year>
Reference-contexts: Let us summarize the key features of the classical theory of Vision in order to point out its drawbacks as an overall framework for studying and building perceptual systems: In the theory of Marr <ref> [37] </ref>, the most influential in recent times, Vision is described as a reconstruction process, that is, a problem of creating representations at increasingly high levels of abstraction, leading from 2D images through the primal sketch and the 2 1 2 D sketch to object-centered descriptions ("from pixels to predicates") [48].
Reference: [38] <author> J. Maunsell and D.V. Essen. </author> <title> Functional properties of neurons in middle temporal visual area of the macaque monkey I. Selectivity for stimulus direction, speed and orientation. </title> <journal> Journal of Neurophysiology, </journal> <volume> 49 </volume> <pages> 1127-1147, </pages> <year> 1983. </year>
Reference-contexts: Combining the computed values allows us to derive the direction of an object's translation [18]. 20 3.5 A look at the motion pathway There is a very large amount of literature <ref> [11, 38, 60, 65] </ref> on the properties of neurons involved in motion analysis.
Reference: [39] <author> J. Moody and C. Darken. </author> <title> Fast learning in networks of locally tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 281-293, </pages> <year> 1989. </year>
Reference-contexts: The popular technique refers to neural networks. There is a large amount of literature on neural networks that estimate the average mapping from stimuli to responses <ref> [39, 53] </ref>. As described before, the success of this approach depends largely 31 on whether the distribution of the examples is appropriate, i.e. on whether the network is required to interpolate or extrapolate.
Reference: [40] <author> A. Movshon. </author> <title> Visual processing of moving images. </title> <editor> In H. Barlow, C. Blakemore, and M. Weston-Smith, editors, </editor> <booktitle> Images and Understanding, </booktitle> <pages> pages 122-137. </pages> <publisher> Cambridge University Press, </publisher> <year> 1990. </year> <month> 37 </month>
Reference-contexts: Neurons of the first kind could be involved in the estimation of the local retinal motion perpendicular to the local edge (normal 21 levels of the motion pathway (from <ref> [40] </ref>: The spatial scales of the receptive fields (0.1 degree, etc.) listed here are for neurons at the center of gaze; in the periphery these dimensions would be larger.
Reference: [41] <author> R. Nelson. </author> <title> Qualitative detection of motion by a moving observer. </title> <journal> International Journal of Computer Vision, </journal> <volume> 7 </volume> <pages> 33-46, </pages> <year> 1991. </year>
Reference-contexts: To give some examples from navigation: The problem of independent motion detection by a moving observer usually has been addressed 9 with techniques for segmenting optical flow fields. But it also may be tackled through the recog-nition of non-rigid flow fields for a moving observer partially knowing its motion <ref> [3, 41, 61] </ref>. The problem of obstacle detection could be solved by recognizing a set of locations on the retina that represent the image of a part of the 3D world being on a collision course with the observer. <p> It thus seems to make sense to develop processes that detect anything moving very fast <ref> [41] </ref>. If some upper bound on the observer's motion is known (maximal speed), it is possible to detect even for small areas where motions above the speed threshold appear.
Reference: [42] <author> R. Nelson and J. Aloimonos. </author> <title> Finding motion parameters from spherical flow fields (or the advantage of having eyes in the back of your head). </title> <journal> Biological Cybernetics, </journal> <volume> 58 </volume> <pages> 261-273, </pages> <year> 1988. </year>
Reference-contexts: To perform this task it is not necessary to compute the exact motion between the observer and any object in the scene, but only to recognize that certain patterns of flow evolve in a way that signifies the collision of the corresponding scene points with the observer <ref> [42] </ref>. Pursuing a target amounts to recognizing the target's location on the image plane along with a set of labels representing aspects of its relative motion sufficient for the observer to plan its actions.
Reference: [43] <author> R. Nelson and R. Polana. </author> <title> Qualitative recognition of motion using temporal texture. CVGIP: Image Understanding, Special Issue on Purposive, Qualitative, Active Vision, </title> <editor> Y. Aloimonos (Ed.), </editor> <volume> 56 </volume> <pages> 78-89, </pages> <year> 1992. </year>
Reference-contexts: It should be mentioned that recently some effort along this line has started; a few studies have been conducted exploiting motion cues for 28 recognition tasks. In particular, periodic movements, such as the motion of certain animal species, have been characterized in frequency space <ref> [43, 57] </ref>. Statistical pattern recognition techniques have been applied in the time domain to model highly structured motions occurring in nature, such as the motions of flowing water or fluttering leaves [54].
Reference: [44] <author> N. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga Publishing Co., </publisher> <address> Palo Alto, CA, </address> <year> 1980. </year>
Reference-contexts: This is not surprising, since Computer Vision was considered as a subfield of Artificial Intelligence and thus studied using the same methodology, influenced by the ideas and computational theories of the last decades <ref> [12, 21, 44] </ref>. The strict hierarchical organization of representational steps in the Marr paradigm makes the development of learning, adaptation and generalization processes practically impossible (so that there hasn't been much work on "vision and learning").
Reference: [45] <author> S. Omohundro. </author> <title> Bumptrees for efficient function, constraint and classification learning. </title> <type> Technical report, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA, </address> <year> 1991. </year>
Reference-contexts: Different data structures will decompose the stimulus-response space in different ways, producing different organizations of the memory. The goal will be to discover structures that adapt themselves to the underlying distribution so as to support fast access with little effort <ref> [45] </ref>. The available evidence from our knowledge about human and animal learning in vision, along with complexity arguments, suggests that the more adaptive of the hierarchical look-up structures are a more promising approach than standard feed-forward networks. The reason is that neural networks maintain a complete model of their domain. <p> It is thus preferable to make use of a structure with local receptive fields so that divide and conquer techniques can be applied, i.e. every time a piece of information is derived about the data it is used to prune away unnecessary further computations. Using hierarchical look-up structures <ref> [45] </ref> the computational time can be reduced on the average to O (log n). The problem we address next is what maps to learn.
Reference: [46] <author> S. Omohundro. </author> <title> Best-first model merging for dynamic learning and recognition. </title> <type> Technical report, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: The architecture is chosen before the data is presented and the processing in the early phases of the training is similar to the processing in the later ones. Animal and human learning, on the other hand, seems to proceed in a different manner <ref> [46] </ref>. When a system has only few experiences in a domain, every experience (stimulus-response pair) is critical. Individual experiences are remembered more or less in detail in the early phases, and new responses are formed by generalizing from these small numbers of stored experiences.
Reference: [47] <author> G. Orban. </author> <title> The analysis of motion signals and the nature of processing in the primate visual system. </title> <editor> In G. Orban and H.-H. Nagel, editors, </editor> <booktitle> Artificial and Biological Vision Systems, ESPRIT Basic Research Series, </booktitle> <pages> pages 24-57. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: From the primary visual cortex the visual signals are sent to about 30 extrastriate or higher-order visual cortical areas, among which about 300 connections have been reported. Figure 2, taken from <ref> [47] </ref>, shows the major areas involved in visual processing. According to Orban the modules in the primate visual cortex can be divided into four hierarchical levels of processing. <p> Zeki [70], for example, suggests that V3 is responsible for the understanding of form from motion information, and V4 derives form and color information. At later stages the modules process both kinds of information in a combined way. four tentative levels of cortical visual processing (from <ref> [47] </ref>). 12 On the basis of anatomical evidence and behavioral studies (studies on patients with lesions of specific cortical areas) the hypothesis has been advanced [66] that there exist two visual pathways originating from V1: a dorsal one leading to the parietal cortex and a ventral one leading to the infero-temporal
Reference: [48] <author> A. Pentland, </author> <title> editor. From Pixels to Predicates: Recent Advances in Computational and Robot Vision. </title> <publisher> Ablex, </publisher> <address> Norwood, NJ, </address> <year> 1986. </year>
Reference-contexts: Marr [37], the most influential in recent times, Vision is described as a reconstruction process, that is, a problem of creating representations at increasingly high levels of abstraction, leading from 2D images through the primal sketch and the 2 1 2 D sketch to object-centered descriptions ("from pixels to predicates") <ref> [48] </ref>.
Reference: [49] <author> A. Pentland, B. Horowitz, and S. Sclaroff. </author> <title> Non-rigid motion and structure from contour. </title> <booktitle> In Proc. IEEE Workshop on Visual Motion, </booktitle> <pages> pages 288-293, </pages> <year> 1991. </year>
Reference-contexts: Attempts have been made to model walking or running humans by describing the motion of single limbs rigidly [55], and various deformable spatial models like superquadrics and snakes have been utilized to model non-rigid motions of rigid bodies <ref> [49] </ref>, for example for the purpose of face recognition. Representations used for understanding space should be allowed to be of any of three kinds: with regard to the viewer, with regard to an object, or action-driven.
Reference: [50] <author> D. Perrett, M. Harries, A. Mistlin, and A. Chitty. </author> <title> Three stages in the classification of body movements by visual neurons. </title> <editor> In H. Barlow, C. Blakemore, and M. Weston-Smith, editors, </editor> <booktitle> Images and Understanding, </booktitle> <pages> pages 94-107. </pages> <publisher> Cambridge University Press, </publisher> <year> 1990. </year>
Reference-contexts: For example, in an area called TEA, cells have been reported which are involved in the coding of hand movements <ref> [50] </ref>. These cells respond when an action is directed towards a particular goal, but they do not respond to the component actions and motions when there is no causal connection between them.
Reference: [51] <author> D. Perrett, A. Mistlin, and M.H.A. Chitty. </author> <title> Vision and action: The control of grasping. </title> <publisher> Ablex, </publisher> <address> Norwood, NJ, </address> <year> 1988. </year>
Reference-contexts: Representations used for understanding space should be allowed to be of any of three kinds: with regard to the viewer, with regard to an object, or action-driven. An appropriate representation might allow us to solve tasks straightforwardly that would require very elaborate computations and descriptions otherwise. Perrett et al. <ref> [51] </ref> give a good example supporting this point of view.
Reference: [52] <author> T. Poggio, S. Edelman, and M. Fahle. </author> <title> Learning of visual modules from examples: A framework for understanding adaptive visual performance. CVGIP: Image Understanding, Special Issue on Purposive, Qualitative, Active Vision, </title> <editor> Y. Aloimonos (Ed.), </editor> <volume> 56 </volume> <pages> 22-30, </pages> <year> 1992. </year> <month> 38 </month>
Reference-contexts: The body of projective geometry has been investigated to prove results about the computation of structure and motion from a set of views under perspective projection [14]. The learning of object recognition capabilities has been studied for neuronal networks using nodes that store viewer-centered projections <ref> [52] </ref>, and geometric studies on so-called aspect graphs have investigated how different kinds of geometric properties change with the views the observer has of the geometric model [34]. The problem of solving both localization and recognition is exactly the antagonistic conflict at the heart of pattern recognition.
Reference: [53] <author> T. Poggio and F. Girosi. </author> <title> Networks for approximation and learning. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78 </volume> <pages> 113-125, </pages> <year> 1990. </year>
Reference-contexts: The popular technique refers to neural networks. There is a large amount of literature on neural networks that estimate the average mapping from stimuli to responses <ref> [39, 53] </ref>. As described before, the success of this approach depends largely 31 on whether the distribution of the examples is appropriate, i.e. on whether the network is required to interpolate or extrapolate.
Reference: [54] <author> R. Polana and R. Nelson. </author> <title> Detecting activities. </title> <booktitle> In Proc. IEEE Image Understanding Workshop, </booktitle> <pages> pages 569-574, </pages> <year> 1993. </year>
Reference-contexts: In particular, periodic movements, such as the motion of certain animal species, have been characterized in frequency space [43, 57]. Statistical pattern recognition techniques have been applied in the time domain to model highly structured motions occurring in nature, such as the motions of flowing water or fluttering leaves <ref> [54] </ref>. Attempts have been made to model walking or running humans by describing the motion of single limbs rigidly [55], and various deformable spatial models like superquadrics and snakes have been utilized to model non-rigid motions of rigid bodies [49], for example for the purpose of face recognition.
Reference: [55] <author> R. Qian and T. Huang. </author> <title> Motion analysis of articulated objects. </title> <booktitle> In Proc. International Conference on Pattern Recognition, </booktitle> <pages> pages A220-223, </pages> <year> 1992. </year>
Reference-contexts: Statistical pattern recognition techniques have been applied in the time domain to model highly structured motions occurring in nature, such as the motions of flowing water or fluttering leaves [54]. Attempts have been made to model walking or running humans by describing the motion of single limbs rigidly <ref> [55] </ref>, and various deformable spatial models like superquadrics and snakes have been utilized to model non-rigid motions of rigid bodies [49], for example for the purpose of face recognition.
Reference: [56] <author> G. Sandini, F. Gandolfo, E. Grosso, and M. Tistarelli. </author> <title> Vision during action. </title> <editor> In Y. Aloimonos, editor, </editor> <booktitle> Active Perception, </booktitle> <pages> pages 151-190. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1993. </year>
Reference-contexts: Recently, research has also started on investigations of how particular species organize the spatial information acquired through their motor sequences and sensors <ref> [56, 59] </ref>. Zoologists differentiate two mechanisms of acquiring orientation: the use of ego-centered and geo-centered systems of reference. Simple animals, like most arthropods, represent spatial information in the form of positional information obtained by some kind of route integration relative to their homes.
Reference: [57] <author> E. Shavit and A. Jepson. </author> <title> Motion using qualitative dynamics. </title> <booktitle> In Proc. IEEE Workshop on Qualitative Vision, </booktitle> <year> 1993. </year>
Reference-contexts: It should be mentioned that recently some effort along this line has started; a few studies have been conducted exploiting motion cues for 28 recognition tasks. In particular, periodic movements, such as the motion of certain animal species, have been characterized in frequency space <ref> [43, 57] </ref>. Statistical pattern recognition techniques have been applied in the time domain to model highly structured motions occurring in nature, such as the motions of flowing water or fluttering leaves [54].
Reference: [58] <author> G. Sommer. </author> <title> Architektur und Funktion visueller Systeme. </title> <journal> Kunstliche Intelligenz, </journal> <volume> 12. Fruhjahrsschule, </volume> <month> March </month> <year> 1994. </year>
Reference-contexts: Not the isolated modelling of observer and world (as closed systems), but the modelling of observer and world in a synergistic manner, will contribute to the understanding of perceptual information processing systems <ref> [58] </ref>. The question, of course, still remains how such a synergistic modelling should be realized.
Reference: [59] <author> M. Srinivasan, M. Lehrer, S. Zhang, and G. Horridge. </author> <title> How honeybees measure their distance from objects of unknown size. </title> <journal> Journal of Comparative Physiology A, </journal> <volume> 165 </volume> <pages> 605-613, </pages> <year> 1989. </year>
Reference-contexts: Recently, research has also started on investigations of how particular species organize the spatial information acquired through their motor sequences and sensors <ref> [56, 59] </ref>. Zoologists differentiate two mechanisms of acquiring orientation: the use of ego-centered and geo-centered systems of reference. Simple animals, like most arthropods, represent spatial information in the form of positional information obtained by some kind of route integration relative to their homes.
Reference: [60] <author> K. Tanaka and H. Saito. </author> <title> Analysis of motion of the visual field by direction, expansion/contraction, and rotation cells illustrated in the dorsal part of the Medial Superior Temporal area of the macaque monkey. </title> <journal> Journal of Neurophysiology, </journal> <volume> 62 </volume> <pages> 626-641, </pages> <year> 1989. </year>
Reference-contexts: Combining the computed values allows us to derive the direction of an object's translation [18]. 20 3.5 A look at the motion pathway There is a very large amount of literature <ref> [11, 38, 60, 65] </ref> on the properties of neurons involved in motion analysis.
Reference: [61] <author> W. Thompson and T.-C. Pong. </author> <title> Detecting moving objects. </title> <journal> International Journal of Computer Vision, </journal> <volume> 4 </volume> <pages> 39-57, </pages> <year> 1990. </year>
Reference-contexts: To give some examples from navigation: The problem of independent motion detection by a moving observer usually has been addressed 9 with techniques for segmenting optical flow fields. But it also may be tackled through the recog-nition of non-rigid flow fields for a moving observer partially knowing its motion <ref> [3, 41, 61] </ref>. The problem of obstacle detection could be solved by recognizing a set of locations on the retina that represent the image of a part of the 3D world being on a collision course with the observer.
Reference: [62] <author> J. Todd and Reichel. </author> <title> Ordinal structure in the visual perception and cognition of smoothly curved surfaces. </title> <journal> Psychological Review, </journal> <volume> 96 </volume> <pages> 643-657, </pages> <year> 1989. </year>
Reference-contexts: Psychophysical evidence [10, 33] suggests that human performance in tasks involving metric structure from binocular disparities is very poor. Also, other cues don't seem to allow humans to extract the kind of depth information that has usually been considered. In their experiments, Todd and Reichel <ref> [62] </ref> had subjects estimate the depths of points on a drape-like surface shown on video images.
Reference: [63] <author> S. Ullman. </author> <title> The Interpretation of Visual Motion. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1979. </year>
Reference-contexts: These particular competences allow us to demonstrate a hierarchy of models concerned with the representation of motion, form and shape. 14 In the past, navigational tasks, since they inherently involve metric relationships between the observer and the environment, have been considered as subproblems of the general "structure-from-motion" problem <ref> [63] </ref>. The idea was to recover the relative 3D-motion and the structure of the scene in view from a given sequence of images taken by an observer in motion relative to its environment.
Reference: [64] <author> S. Ullman and R. Basri. </author> <title> Recognition by linear combination of models. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13 </volume> <pages> 992-1006, </pages> <year> 1991. </year>
Reference-contexts: results in the literature of structure from motion, that show that under parallel projection any view of an object can be constructed as a linear combination of a small number of views of the same object, a series of studies on recognition using orthographic and paraperspective projections have been conducted <ref> [31, 64] </ref>. The body of projective geometry has been investigated to prove results about the computation of structure and motion from a set of views under perspective projection [14].
Reference: [65] <author> L. Ungerleider and R. DeSimone. </author> <title> Cortical connections of visual area MT in the macaque. </title> <journal> Journal of Comparative Neurology, </journal> <volume> 248 </volume> <pages> 190-222, </pages> <year> 1986. </year> <month> 39 </month>
Reference-contexts: Combining the computed values allows us to derive the direction of an object's translation [18]. 20 3.5 A look at the motion pathway There is a very large amount of literature <ref> [11, 38, 60, 65] </ref> on the properties of neurons involved in motion analysis.
Reference: [66] <author> L. Ungerleider and M. Mishkin. </author> <title> Two cortical visual systems. </title> <editor> In D. Ingle, M. Goodale, and R. Mansfield, editors, </editor> <booktitle> Analysis of Visual Behavior, </booktitle> <pages> pages 549-586. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1982. </year>
Reference-contexts: At later stages the modules process both kinds of information in a combined way. four tentative levels of cortical visual processing (from [47]). 12 On the basis of anatomical evidence and behavioral studies (studies on patients with lesions of specific cortical areas) the hypothesis has been advanced <ref> [66] </ref> that there exist two visual pathways originating from V1: a dorsal one leading to the parietal cortex and a ventral one leading to the infero-temporal cortex.
Reference: [67] <editor> R. Wehner. Homing in arthropods. In F. Papi, editor, </editor> <booktitle> Animal Homing, </booktitle> <pages> pages 45-144. </pages> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: However, research from studies on arthropods <ref> [8, 9, 67] </ref> shows that already in these simple animals, the competence of homing is realized in many ways. A large variety of different ways employing combinations of information from action and perception have been discovered.
Reference: [68] <author> P. Winston. </author> <title> Learning structural descriptions from examples. </title> <editor> In P. Winston, editor, </editor> <booktitle> The Psychology of Computer Vision. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: For example, some early systems <ref> [68] </ref> dealt with the spatial relationship of objects in a blocks world. Assuming that objects can be recognized and thus can be stored as symbols, the spatial configuration of these objects under changing conditions was studied.
Reference: [69] <author> S. Zeki. </author> <title> The visual image in mind and brain. </title> <journal> Scientific American, </journal> <volume> 267(3) </volume> <pages> 69-76, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction "The past two decades : : : have led to a powerful conceptual change in our view of what the brain does : : : It is no longer possible to divide the process of seeing from that of understanding : : :". <ref> [69] </ref>. These lines of Zeki's article express in a concise way what has been realized in different disciplines concerned with the understanding of perception. Vision (and perception in general) should not be studied in isolation but in conjunction with the physiology and the tasks that systems perform.
Reference: [70] <author> S. Zeki. </author> <title> A Vision of the Brain. </title> <publisher> Blackwell Scientific Publications, </publisher> <year> 1993. </year> <month> 40 </month>
Reference-contexts: MT (also called V5), MST, and FST seem to be involved in motion processing, and V4 in color processing. Form vision seems to be accomplished by different lower modules which use both static and dynamic information. Zeki <ref> [70] </ref>, for example, suggests that V3 is responsible for the understanding of form from motion information, and V4 derives form and color information. <p> It would be an oversimplification to conceive of these two pathways as being mutually exclusive and hierarchically organized <ref> [70] </ref>; one of the reasons is that this theory fails to provide an answer to where and how the knowledge of "what" an object is might be integrated with the knowledge of "where" it is. <p> Very interestingly, we find the following results: In the visual cortex cells have been found which are "gaze-locked", in the sense that they only respond to a certain stimulus if the subject is gazing in a particular direction. These cells probably respond to absolute positions in the ego-centric space <ref> [70] </ref>. It seems that nature has invented a number of ways for perceiving space through recognition and localization of objects in the 3D world.
References-found: 70

