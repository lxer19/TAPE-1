URL: ftp://ftp.cs.rochester.edu/pub/u/rosca/gp/95.tr566.ps.gz
Refering-URL: http://www.cs.bham.ac.uk/~wbl/biblio/gp-bibliography.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: rosca@cs.rochester.edu  
Title: An Analysis of Hierarchical Genetic Programming  
Author: Justinian P. Rosca 
Note: This material is based on work supported by the National Science Foundation under grant numbered IRI-9406481 and by DARPA research grant no. MDA972-92-J-1012. The government has certain rights in this material.  
Date: March 1995  
Address: Rochester, New York 14627  
Affiliation: The University of Rochester Computer Science Department  
Pubnum: Technical Report 566  
Abstract: Hierarchical genetic programming (HGP) approaches rely on the discovery, modification, and use of new functions to accelerate evolution. This paper provides a qualitative explanation of the improved behavior of HGP, based on an analysis of the evolution process from the dual perspective of diversity and causality. From a static point of view, the use of an HGP approach enables the manipulation of a population of higher diversity programs. Higher diversity increases the exploratory ability of the genetic search process, as demonstrated by theoretical and experimental fitness distributions and expanded structural complexity of individuals. From a dynamic point of view, this report analyzes the causality of the crossover operator. Causality relates changes in the structure of an object with the effect of such changes, i.e. changes in the properties or behavior of the object. The analyses of crossover causality suggests that HGP discovers and exploits useful structures in a bottom-up, hierarchical manner. Diversity and causality are complementary, affecting exploration and exploitation in genetic search. Unlike other machine learning techniques that need extra machinery to control the tradeoff between them, HGP automatically trades off exploration and exploitation. 
Abstract-found: 1
Intro-found: 1
Reference: [Altenberg, 1994] <author> Lee Altenberg, </author> <title> "The Evolution of Evolvability in Genetic Programming," </title> <editor> In Kim Kinnear, editor, </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The lens effect [Koza, 1994b] is the idea that the tails of the fitness distribution for randomly generated programs are larger for ADF-GP than for standard GP. The effect is attributed to the introduction of new functions into the representation. <ref> [Altenberg, 1994] </ref> outlines that a similar property should be observed in general in order to make GP search more efficient than random search: the upper tail of the offspring fitness distribution should be wider than that for random search. <p> First, how much code of a parse tree representing an individual is effective? It is well known that GP evolves non parsimonious trees if no size pressure is included in the fitness 12 evaluation, a phenomenon suggestively called "defense against crossover" <ref> [Altenberg, 1994] </ref> However, the useless regions of code may represent reservoirs of genetic material [Angeline, 1994a]. They either preserve or evolve good fragments of code to be activated later during evolution as a result of crossover. One such example is presented in figure 4.
Reference: [Angeline, 1994a] <author> Peter J. Angeline, </author> <title> Evolutionary Algorithms and Emergent Intelligence, </title> <type> PhD thesis, </type> <institution> Computer Science Department, Ohio State University, </institution> <year> 1994. </year>
Reference-contexts: a parse tree representing an individual is effective? It is well known that GP evolves non parsimonious trees if no size pressure is included in the fitness 12 evaluation, a phenomenon suggestively called "defense against crossover" [Altenberg, 1994] However, the useless regions of code may represent reservoirs of genetic material <ref> [Angeline, 1994a] </ref>. They either preserve or evolve good fragments of code to be activated later during evolution as a result of crossover. One such example is presented in figure 4.
Reference: [Angeline, 1994b] <author> Peter J. Angeline, </author> <title> "Genetic Programming and Emergent Intelligence," </title> <editor> In Kim Kinnear, editor, </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference: [Angeline and Pollack, 1994] <author> Peter J. Angeline and Jordan B. Pollack, </author> <title> "Coevolving High Level Representations," </title> <editor> In Christofer G. Langton, editor, </editor> <booktitle> Artificial Life III, SFI Studies in the Sciences of Complexity, </booktitle> <volume> volume XVII, </volume> <pages> pages 55-71, </pages> <address> Redwood City, CA, 1994. </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The analysis of building blocks in AR-GP [Rosca and Ballard, 1994a] starts from this hypothesis and takes a functional approach. The ADF approach, presented earlier, is also a method of representing and using modularity in GP. Another method, module acquisition ([Angeline, 1994b], <ref> [Angeline and Pollack, 1994] </ref>) introduced many inspirational ideas. A module is a function with a unique name defined by selecting and chopping off branches of a subtree selected randomly from an individual.
Reference: [Cormen et al., 1990] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest, </author> <title> Introduction to Algorithms, </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: It follows that P robfX = kg = 1 k (3) The expected value of X is n 2 and its variance is n 4 and can be computed as follows (or see <ref> [Cormen et al., 1990] </ref>): E [X] = k=0 k P robfX = kg = 1 n (4) 1 n X k 2 n ! n 2 = 1 " n X k n !# n 2 = 4 3.1 Program diversity In order to understand the role of representation and the
Reference: [Cramer, 1985] <author> Nichael Lynn Cramer, </author> <title> "A Representation for the Adaptive Generation of Simple Sequential Programs," </title> <booktitle> In Proceedings of the First International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <year> 1985. </year>
Reference: [Eshelman and Schaffer, 1993] <editor> Larry J. Eshelman and J. David Schaffer, "Crossover's Niche," In Stephanie Forrest, editor, </editor> <booktitle> Proceedings of the International Conference on Genetic Algorithms, </booktitle> <pages> pages 9-14. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: The building block hypothesis ([Holland, 1992], [Goldberg, 1989]) outlines the importance of small schemata, called building blocks, in the proper functioning of a GA. More recently, crossover has been considered the differentiating feature that gives a GA advantages over other stochastic methods in certain types of problems. For example <ref> [Eshelman and Schaffer, 1993] </ref> brings evidence that crossover with pair-wise mating helps propagating middle order building blocks. The arguments presented so far have analyzed a static picture of GP. The wider hit distributions and the increased expanded structural complexity suggested an increased exploration potential of GP with functions.
Reference: [Goldberg, 1989] <author> David E. Goldberg, </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: The traditional analysis of GAs by Holland [Holland, 1992] focuses on the propagation of schemata from one generation to the next. The building block hypothesis ([Holland, 1992], <ref> [Goldberg, 1989] </ref>) outlines the importance of small schemata, called building blocks, in the proper functioning of a GA. More recently, crossover has been considered the differentiating feature that gives a GA advantages over other stochastic methods in certain types of problems.
Reference: [Hillis, 1990] <author> W. Daniel Hillis, </author> <title> "Co-evolving Parasites Improve Simulated Evolution as an Optimization Procedure," </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proceedings of the Ninth Annual International Conference of the Center for Nonlinear Studies on Self-organizing, Collective, and Cooperative Phenomena in Natural and Artificial Computing Networks, </booktitle> <pages> pages 228-234. </pages> <publisher> North Holland, </publisher> <year> 1990. </year>
Reference-contexts: Each fitness evaluation becomes more expensive as the problem is scaled up. For instance, the number of fitness cases in the even-6-parity problem is twice that of the even-5-parity problem and it doubles with every unitary increase in the order of the problem. A co-evolutionary approach such as in <ref> [Hillis, 1990] </ref> could reduce the cost of a fitness evaluation by relying on a subset of fitness cases which evolves dynamically by being controlled in its turn with a genetic algorithm.
Reference: [Holland, 1992] <author> John H. Holland, </author> <title> Adaptation in Natural and Artificial Systems, An Introductory Analysis with Applications to Biology, </title> <booktitle> Control and Artificial Intelligence, </booktitle> <publisher> MIT Press, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: The use of an HGP approach enables the manipulation of a population of higher diversity programs, which positively affects the efficiency of an HGP algorithm for complex problems. 11 4 HGP Evolution Dynamics GP evolution dynamics has been very difficult to analyze. The traditional analysis of GAs by Holland <ref> [Holland, 1992] </ref> focuses on the propagation of schemata from one generation to the next. The building block hypothesis ([Holland, 1992], [Goldberg, 1989]) outlines the importance of small schemata, called building blocks, in the proper functioning of a GA.
Reference: [Kaelbling, 1993] <author> Leslie Pack Kaelbling, </author> <title> Learning in Embedded Systems, </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: However, by using directly the size constraint the GP algorithm is prevented from finding solutions. The algorithm improves convergence to a better optimum while maintaining speed. Exploration and exploitation are recurring themes in search and learning problems [Hol-land, 1992], <ref> [Kaelbling, 1993] </ref>. Exploitation takes place when search proceeds based on the action prescribed by the current system knowledge. Exploration is usually based on random actions, taken in order to experiment with more situations. For example, in learning classifier systems, roulette wheel action selection is a means of choosing exploratory actions.
Reference: [Kinnear, 1994] <author> Kim Kinnear, </author> <title> "Alternatives in Automatic Function Definition," </title> <editor> In Kim Kinnear, editor, </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: subject to genetic operations unless they are subsequently decompressed. 4 It has been conjectured that problems whose solutions present symmetry patterns or opportunities to parameterize and reuse code can be solved easier in ADF-GP [Koza, 1994b] but there exists no formal explanation of why ADF-GP works better than standard GP. <ref> [Kinnear, 1994] </ref> explains why ADF-GP works by introducing the notion of structural regularity. He compares ADF-GP against the module acquisition approach and points out that the module acquisition approach does not directly create structural regularity.
Reference: [Koza, 1992] <author> John R. Koza, </author> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year> <month> 23 </month>
Reference-contexts: Such GP extensions have been designed with the goal of automating the discovery of functions that are beneficial during the search for solutions by exploiting opportunities to parameterize and reuse code. Two such techniques are automatic definition of functions (ADF) <ref> [Koza, 1992] </ref> and adaptive representation (AR) [Rosca and Ballard, 1994a]. The former is a GP extension that allows the evolution of reusable subroutines. The latter is based on the discovery of useful building blocks of code. <p> Discovered functions represent an adaptive control mechanism in the exploration-exploitation tradeoff. In conclusion the paper discusses the results and suggests future research. 2 Hierarchical Genetic Programming Genetic programming departs from the genetic algorithm (GA) paradigm by using trees to represent genotypes ([Cramer, 1985], <ref> [Koza, 1992] </ref>). Trees provide a flexible representation for creating and manipulating programs. This paper uses the denotations tree and subtree to refer to the parse tree of a program or a part of it respectively. Problem representation in GP is defined by a set of problem-dependent primitive functions. <p> For example, the inclusion of more complex functions, known to be part of a final solution, will result in less computational effort spent during search and thus will enable a shorter time to finding a final solution. The HGP approaches presented below, automatic definition of functions (ADF-GP) <ref> [Koza, 1992] </ref> and adaptive representation (AR-GP) [Rosca and Ballard, 1994a] use the above observation in different ways in order to accelerate search. 2.1 Automatic Definition of Functions The automatic definition of functions approach (ADF-GP) assumes that parsimonious problem solutions can be specified in terms of a main program and a hierarchical <p> The main goal was understanding if GP problems have building block structure and when GP is superior to other search techniques. The approach was to generalize the definition of a GP schema from <ref> [Koza, 1992] </ref> to a collection of tree fragments, that is a collection of trees possibly having subtrees removed. An individual instantiates a schema in case it "covers" (matches) all the schema fragments, overlappings between fragments not being allowed. The probability of disruption by crossover is estimated based on these definitions. <p> The odd-n-parity and even-n-parity functions appear to be difficult to learn in GP, especially for values of n greater than five <ref> [Koza, 1992] </ref>. <p> Second, we vary the composition of the primitive function set and analyze again the fitness distribution of randomly generated GP programs. Third, we analyze the expanded structural complexity of GP and HGP solutions. The method of generating GP individuals in the second experiment, borrowed from <ref> [Koza, 1992] </ref>, is the ramped-half-and-half method. <p> From the static point of view of creating an initial population, using ADFs is equivalent to considering a larger initial function set. A more formal interpretation of this remarks can be stated by considering the closure requirement in GP <ref> [Koza, 1992] </ref>. Closure requires that any function be well defined for any combination of arguments (terminals or results of other function calls) that it may encounter.
Reference: [Koza, 1994a] <author> John R. Koza, </author> <title> "Architecture-Altering Operations for Evolving the Architec--ture of a Multi-Part Program in Genetic Programming," </title> <institution> Computer Science Department STAN-CS-TR-94-1528, Stanford University, </institution> <year> 1994. </year>
Reference-contexts: The problem of determining the appropriate architectural choices in ADF-GP has generated work on evolution of the GP architecture. The architecture itself can be evolutionarily selected in case the initial population is architecturally diverse and care is taken when crossing over individuals having different architectures [Koza, 1994b]. <ref> [Koza, 1994a] </ref> introduces six new genetic operations for altering the architecture of an individual program: branch duplication, argument duplication, branch deletion, argument deletion, branch creation and argument creation. These operations are causal in the sense discussed later in this paper. <p> One such example is presented in figure 4. Moreover, redundant regions of code may be created artificially, in analogy with the natural phenomenon of gene duplication, in order to evolve a better program by specializing its treatment of a subclass of inputs <ref> [Koza, 1994a] </ref>. introduced in [Rosca and Ballard, 1994a] with the goal of qualitatively analyzing program transformations during evolution. A structure tree has its nodes labeled with the most recent generation number when the node played a pivot role in a crossover operation. <p> Table 3: Correlation between causality and exploratory ability in GP search. Time (generation) Early Advanced Crossover changes Non-causal Causal Exploration High Low Exploitation Low High An interesting extension of ADF-GP confirms the importance of the idea of causality in GP. <ref> [Koza, 1994a] </ref> introduces six new genetic operations for altering the architecture of an individual program: branch duplication, argument duplication, branch deletion, argument deletion, branch creation and argument creation. All addition operations respect the principle of strong causality discussed before. <p> All addition operations respect the principle of strong causality discussed before. They are performed such that they preserve the behavior of the resulting programs. They merely increase the potential for program refinement and thus they resemble the process of gene duplication in natural evolution <ref> [Koza, 1994a] </ref>. The duplication of elements of program architecture (branches or arguments) is done in conjunction with a random replacement of the invocations of the corresponding element to the duplicated copy. Such an operation decreases the probability that a future random change will drastically change the behavior of the program.
Reference: [Koza, 1994b] <editor> John R. Koza, </editor> <booktitle> Genetic Programming II, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1994. </year>
Reference-contexts: Second modules become frozen portions of genetic material, which are not subject to genetic operations unless they are subsequently decompressed. 4 It has been conjectured that problems whose solutions present symmetry patterns or opportunities to parameterize and reuse code can be solved easier in ADF-GP <ref> [Koza, 1994b] </ref> but there exists no formal explanation of why ADF-GP works better than standard GP. [Kinnear, 1994] explains why ADF-GP works by introducing the notion of structural regularity. <p> He compares ADF-GP against the module acquisition approach and points out that the module acquisition approach does not directly create structural regularity. Kinnear attributes the better performance of ADF to the repeated use of calls to automatically defined functions and to the multiple use of parameters. The lens effect <ref> [Koza, 1994b] </ref> is the idea that the tails of the fitness distribution for randomly generated programs are larger for ADF-GP than for standard GP. <p> The problem of determining the appropriate architectural choices in ADF-GP has generated work on evolution of the GP architecture. The architecture itself can be evolutionarily selected in case the initial population is architecturally diverse and care is taken when crossing over individuals having different architectures <ref> [Koza, 1994b] </ref>. [Koza, 1994a] introduces six new genetic operations for altering the architecture of an individual program: branch duplication, argument duplication, branch deletion, argument deletion, branch creation and argument creation. These operations are causal in the sense discussed later in this paper. <p> For the even-3-parity problem <ref> [Koza, 1994b] </ref> reports that no solution is discovered after the random generation of 10 million parity functions. <p> The effect is called the lens effect in <ref> [Koza, 1994b] </ref>. Varying the composition of the primitive function set varied. <p> It may be difficult to determine the appropriate functions from F total necessary to solve a given problem. It is unrealistic to consider huge functions sets in either GP or ADF-GP. However, GP can be used to select primitives that can be better combined to yield candidate solution improvements <ref> [Koza, 1994b] </ref>. In this case, automatic selection of primitives will have its computational cost. The increased fitness diversity is determined by a larger set of functions that is given to express candidate solutions. <p> In an example for the problem of finding an impulse response function, Koza showed that crossover determines an improved offspring performance by improving one parent's performance for one portion of the time domain, and inheriting the behavior of the other parent for the rest of the domain <ref> [Koza, 1994b] </ref>. Such a behavior has been interpreted as "case splitting": GP refines a partial solution by changing a subtree so that the program treats separately, in a more detailed way, a particular input case. In this case, structures are exploited through the function they have when computing fitness. <p> Also, the number of fitness evaluations necessary to find a solution with high probability increases with problem size <ref> [Koza, 1994b] </ref> in the standard GP implementation. One explanation of the poor GP convergence is the inability of standard GP to exploit opportunities for code generalization and reuse. In contrast, by using ADFs or adapting the representation as in AR-GP the same problems can be solved more easily ([Koza, 1994b], [Rosca
Reference: [Lohmann, 1992] <author> Reinhard Lohmann, </author> <title> "Structure Evolution and Incomplete Induction," </title> <booktitle> In Parallel Problem Solving from Nature 2, </booktitle> <pages> pages 175-185. </pages> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference-contexts: The principle of strong causality states that small alterations in the underlying structure of an object, or small departures from the cause determine small changes of the object's behavior, or small changes of the effects, respectively ([Rechenberg, 1994], <ref> [Lohmann, 1992] </ref>). In GP small alterations of the programs may generate big changes in behavior. From this perspective GP is weakly causal. In this report, the trend of structures called birth certificates are presented as evidence for the way HGP inherits useful structures.
Reference: [O'Reilly and Oppacher, 1994] <author> Una-May O'Reilly and Franz Oppacher, </author> <title> "The troubling aspects of a building block hypothesis for genetic programming," </title> <booktitle> In Proceedings of the Third Workshop on Foundations of Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <year> 1994. </year>
Reference-contexts: This issue has generated research efforts towards defining the notion of building block in GP and finding useful ways to manipulate modules of code. A GP analogy along the lines of GA schemata theory and GA building block hypothesis has been attempted in <ref> [O'Reilly and Oppacher, 1994] </ref>. The main goal was understanding if GP problems have building block structure and when GP is superior to other search techniques.
Reference: [Rechenberg, 1994] <author> Ingo Rechenberg, </author> <title> "Evolution Strategy," </title> <editor> In Jacek M. Zurada, Robert J. Marks-II, and Charles J. Robinson, editors, </editor> <booktitle> Computational Intelligence Imitating Life, </booktitle> <pages> pages 147-159. </pages> <publisher> IEEE Press, </publisher> <year> 1994. </year>
Reference: [Rosca and Ballard, 1994a] <author> Justinian P. Rosca and Dana H. Ballard, </author> <title> "Genetic Programming with Adaptive Representations," </title> <type> Technical Report 489, </type> <institution> University of Rochester, Computer Science Department, </institution> <year> 1994. </year>
Reference-contexts: Such GP extensions have been designed with the goal of automating the discovery of functions that are beneficial during the search for solutions by exploiting opportunities to parameterize and reuse code. Two such techniques are automatic definition of functions (ADF) [Koza, 1992] and adaptive representation (AR) <ref> [Rosca and Ballard, 1994a] </ref>. The former is a GP extension that allows the evolution of reusable subroutines. The latter is based on the discovery of useful building blocks of code. <p> The HGP approaches presented below, automatic definition of functions (ADF-GP) [Koza, 1992] and adaptive representation (AR-GP) <ref> [Rosca and Ballard, 1994a] </ref> use the above observation in different ways in order to accelerate search. 2.1 Automatic Definition of Functions The automatic definition of functions approach (ADF-GP) assumes that parsimonious problem solutions can be specified in terms of a main program and a hierarchical collection of subroutines. <p> Consequently, AR-GP has a bottom-up approach to function discovery [Rosca and Ballard, 1994b]. The generation intervals with no function set changes represent evolutionary epochs. At the beginning of each new epoch, part of the population is extinguished and replaced with random individuals built using the extended function set <ref> [Rosca and Ballard, 1994a] </ref>. The extinction step was introduced in order to make use of the newly discovered functions. The discovery of functions in AR can be guided by domain knowledge. Most generally, the population itself represents a pool of statistical information. <p> The authors concluded that schema analysis is difficult and does not offer an appropriate perspective for analyzing GP. A GP structural theory analogous to GA schemata theory fundamentally ignores the functional role of the GP representation. The analysis of building blocks in AR-GP <ref> [Rosca and Ballard, 1994a] </ref> starts from this hypothesis and takes a functional approach. The ADF approach, presented earlier, is also a method of representing and using modularity in GP. Another method, module acquisition ([Angeline, 1994b], [Angeline and Pollack, 1994]) introduced many inspirational ideas. <p> One such example is presented in figure 4. Moreover, redundant regions of code may be created artificially, in analogy with the natural phenomenon of gene duplication, in order to evolve a better program by specializing its treatment of a subclass of inputs [Koza, 1994a]. introduced in <ref> [Rosca and Ballard, 1994a] </ref> with the goal of qualitatively analyzing program transformations during evolution. A structure tree has its nodes labeled with the most recent generation number when the node played a pivot role in a crossover operation. Zero labeled nodes remained unchanged from the initial generation. <p> The emergent structure in ADF-GP as an effect of causality is an explicit policy in AR-GP. The bottom-up evolution of HGP discussed in the paper justifies this explicit search for building blocks and the expansion of the problem representation, which was successfully used in AR-GP <ref> [Rosca and Ballard, 1994a] </ref>. AR-GP uses fit, small blocks to define new functions. AR-GP evolves a variable hierarchy of functions, each having a variable number of arguments. A process of extinction of population individuals accelerates the use of the discovered functions.
Reference: [Rosca and Ballard, 1994b] <author> Justinian P. Rosca and Dana H. Ballard, </author> <title> "Hierarchical Self-Organization in Genetic Programming," </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 251-258. </pages> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <year> 1994. </year>
Reference-contexts: Useful blocks tend to be small and the process can be applied recursively to discover more and more complex useful blocks. Consequently, AR-GP has a bottom-up approach to function discovery <ref> [Rosca and Ballard, 1994b] </ref>. The generation intervals with no function set changes represent evolutionary epochs. At the beginning of each new epoch, part of the population is extinguished and replaced with random individuals built using the extended function set [Rosca and Ballard, 1994a]. <p> In HGP, a true measure of the size of an individual is obtained by counting all the nodes in the tree resulting after an "inline" expansion of all the called functions down to the primitive functions. This complexity measure is called "expanded structural complexity" in <ref> [Rosca and Ballard, 1994b] </ref> and is based on the structural complexity (i.e. the number of tree nodes) of all the functions in the hierarchy which are called directly or indirectly by the individual.
Reference: [Rosca and Ballard, 1994c] <author> Justinian P. Rosca and Dana H. Ballard, </author> <title> "Learning by Adapting Representations in Genetic Programming," </title> <booktitle> In Proceedings of the IEEE World Congress on Computational Intelligence, </booktitle> <pages> pages 407-412. </pages> <publisher> IEEE Press, </publisher> <address> Orlando, </address> <year> 1994. </year>
Reference-contexts: One explanation of the poor GP convergence is the inability of standard GP to exploit opportunities for code generalization and reuse. In contrast, by using ADFs or adapting the representation as in AR-GP the same problems can be solved more easily ([Koza, 1994b], <ref> [Rosca and Ballard, 1994c] </ref>). We gave a qualitative explanation of the improved behavior of HGP, based on an analysis of the evolution process on two dimensions: diversity and causality. Next we relate these ideas to the tradeoff between exploration and exploitation.
Reference: [Ryan, 1994] <editor> Conor. O. Ryan, "Pygmies and Civil Servants," In Kim Kinnear, editor, </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: These operations are causal in the sense discussed later in this paper. A rule of thumb in GA literature postulates that population diversity is important for avoiding premature convergence. A comparison of research on this topic is provided in <ref> [Ryan, 1994] </ref>. Ryan shows that maintaining increased diversity in GP leads to better performance. His algorithm is called "disassortative mating" because it selects parents for crossover from two different lists of individuals.
Reference: [Simon, 1973] <author> Herbert A. Simon, </author> <title> "The Organization of Complex Systems," </title> <editor> In G. Braziller Howard H. Pattee, editor, </editor> <booktitle> Hierarchy Theory; The Challenge of Complex Systems, </booktitle> <pages> pages 3-27. </pages> <address> New York, </address> <year> 1973. </year>
Reference-contexts: The effect of this principle is a stabilization on lower level ADFs that will be useful. The evolutionary process freezes good subroutines at a given hierarchy level and looks for changes at higher levels <ref> [Simon, 1973] </ref>. Hence: Hypothesis. Hierarchical genetic programming (and in particular ADF-GP) discovers and exploits useful structures in a bottom-up manner. Note that this hypothesis is the basic idea in the AR-GP extension. <p> This effect is amplified in GP with function discovery. Poor causality has been discussed related to the exploration-exploitation tradeoff in search problems. Arguments for a bottom-up evolutionary thesis of GP were discussed. Early stages of evolution in GP usually discover stable components <ref> [Simon, 1973] </ref>. Replaying backwards the genealogy tree that resulted in a problem solution shows that most changes in later stages of evolution are performed at the higher hierarchical levels. This suggests that in the unconstrained HGP approach such as ADF-GP there are implicit constraints.
Reference: [Wills, 1993] <author> Christopher Wills, </author> <title> The Runaway Brain, </title> <address> BasicBooks, </address> <year> 1993. </year>
Reference-contexts: In particular, offspring of individuals that are already partially adapted to the "environment" and already have a complex structure are more likely to have a worse fitness. This is close to the conclusions on the role of mutation in natural evolution <ref> [Wills, 1993] </ref>. It is also in agreement with our intuition that a small change in a program may drastically change the program behavior. In addition there is the following simple argument.
Reference: [Wilson, 1994] <author> Stewart W. Wilson, "ZCS: </author> <title> A Zeroth Level Classifier System," </title> <journal> Evolutionary Computation, </journal> <volume> 2(1) </volume> <pages> 1-18, </pages> <year> 1994. </year> <month> 24 </month>
Reference-contexts: Exploration is usually based on random actions, taken in order to experiment with more situations. For example, in learning classifier systems, roulette wheel action selection is a means of choosing exploratory actions. In the reinforcement phase of the control loop of a classifier system <ref> [Wilson, 1994] </ref>, matching classifiers that do not get activated are weakened. This lowers the chances of choosing unpromising actions in the near future. The weakening magnitude is usually controlled by an explicit parameter, although more elaborate schemes are possible [Wilson, 1994]. <p> the reinforcement phase of the control loop of a classifier system <ref> [Wilson, 1994] </ref>, matching classifiers that do not get activated are weakened. This lowers the chances of choosing unpromising actions in the near future. The weakening magnitude is usually controlled by an explicit parameter, although more elaborate schemes are possible [Wilson, 1994].
References-found: 25

