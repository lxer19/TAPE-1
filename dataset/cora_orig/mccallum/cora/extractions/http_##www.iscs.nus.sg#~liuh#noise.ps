URL: http://www.iscs.nus.sg/~liuh/noise.ps
Refering-URL: 
Root-URL: 
Email: liuh@iscs.nus.sg  
Phone: Tel: (+65)-772-6563; Fax: (+65) 779-4580  
Title: Efficient Rule Induction from Noise Data Many thanks to Rudy Setiono and Tiow Seng Tan
Author: Huan Liu 
Address: Ridge, Singapore 0511  
Affiliation: Department of Information Systems and Computer Science National University of Singapore Kent  
Note: To appear in Expert Systems with Applications: An International Journal, Vol.10(1996)  Acknowledgments  
Abstract-found: 0
Intro-found: 1
Reference: [ Agrawal et al., 1993 ] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 5(6), </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Classification rules are sought in many areas from automatic knowledge acquisition [ Russell and Norvig, 1995 ] to data mining <ref> [ Agrawal et al., 1993 ] </ref> , neural network rule extraction [ Towell and Shavlik, 1993; Setiono and Liu, 1995 ] . This is because classification rules possess some attractive features.
Reference: [ Fisher, 1936 ] <author> R.A. Fisher. </author> <title> The use of multiple measurements in taxonomic problems. </title> <journal> Ann. Eugenics, </journal> <volume> 7(2) </volume> <pages> 179-188, </pages> <year> 1936. </year>
Reference-contexts: Three data sets are used in experiments. They are (1) Iris <ref> [ Fisher, 1936 ] </ref> , (2) CAR [ Yen and Chen, 1995 ] (given in Table 1), and (3) Golf-Playing [ Pham and Aksoy, 1994 ] (given in Table 2).
Reference: [ Han et al., 1993 ] <author> J. Han, Y. Cai, and H. Cercone. </author> <title> Data-driven discovery of quantitative rules in relational dataabases. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 5(1) </volume> <pages> 15-28, </pages> <year> 1993. </year>
Reference-contexts: 75; Hit: 73; Miss: 1; Acc: .97 Experiment 3 The CAR data is used to compare RND with the results reported in [ Yen and Chen, 1995 ] since they have done comparisons with other methods such as ID3 [ Quinlan, 1986 ] and the one by Han et al <ref> [ Han et al., 1993 ] </ref> . It also show that some redundant rules can be removed due to their narrow coverage. The details of the rule set are shown below. Rules 3 and 4 are deleted because the patterns they cover can all be covered by Rule 5.
Reference: [ Liu and Setiono, 1995 ] <author> H. Liu and R. Setiono. Chi2: </author> <title> Feature selection and discretization of numeric attributes. </title> <booktitle> In Proceedings of the 7th IEEE International Conference on Tools with Artificial Intelligence, </booktitle> <year> 1995. </year> <note> http://www.iscs.nus.sg/ ~ liuh/tai95.ps </note>
Reference-contexts: The first line in RND is designed to find a good pattern (the most frequently occured one). The frequency of each pattern can be obtained by counting each distinct pattern's occurrence and removing duplicates. The output of Chi2 <ref> [ Liu and Setiono, 1995 ] </ref> , which is a program that does discretization and feature selection for numeric data, also provides such information. <p> They are (1) Iris [ Fisher, 1936 ] , (2) CAR [ Yen and Chen, 1995 ] (given in Table 1), and (3) Golf-Playing [ Pham and Aksoy, 1994 ] (given in Table 2). For numeric attributes (e.g., in Golf-Playing and Iris data), Chi2 <ref> [ Liu and Setiono, 1995 ] </ref> is applied to discretizing these attributes. After a data set is processed by Chi2, its numeric or ordinal attributes are discretized, normally, irrelevant or redundant attributes will be discretized into one value only, and the number of patterns decreases. <p> | Total #Item: 75; Hit: 74; Miss: 1; Acc: 0.99 | Testing rules on the testing data: | Total #Item: 75; Hit: 73; Miss: 2; Acc: 0.97 Translating the discretized data back to their original meanings (if a reader is inter ested in how this is achieved, please refer to <ref> [ Liu and Setiono, 1995 ] </ref> ), we have: Rule 0: If Petal-length 1.9 then Iris setosa Rule 1: If Petal-length 4.9 and Petal-width 1.6 then Iris versicolor Default Rule: Iris virginica. Experiment 2 The Iris data is used, assuming that the frequency information is not given.
Reference: [ Lu et al., 1995 ] <author> H. Lu, R. Setiono, and H. Liu. NeuroRule: </author> <title> A connectionist approach to data mining. </title> <booktitle> In Proceedings of VLDB'95, </booktitle> <year> 1995. </year> <note> http://www.iscs.nus.sg/ ~ liuh/vldb95.ps </note>
Reference-contexts: The immediate benefit of RND is that it can be easily used as a component of other systems due to its efficiency and simplicity. RND has been incorporated in several systems and successfully applied to neural network rule extraction [ Setiono and Liu, 1995 ] and data mining <ref> [ Lu et al., 1995 ] </ref> . The practice indicates that combining RND with other methods, the rule induction power is significantly improved. For example, the single-pattern-based-rule induction algorithm (such as RND) might be weaker than a neural net for applications like parity checking.
Reference: [ Michie et al., 1994 ] <editor> D. Michie, D.J. Spiegelhalter, and C.C. Taylor. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <booktitle> Ellis Horwood Series in Artificial Intelligence, </booktitle> <year> 1994. </year>
Reference-contexts: The evaluation of rules should be performed at least in two aspects. One is the estimation of error rates <ref> [ Michie et al., 1994 ] </ref> . The lower the error rate for the testing set is, the better the rules generalize from the data. Rules that do not generalize have minimum utility. <p> In total, there are three rules that can classify 150 patterns with high accuracy (75 patterns used for training, and 75 patterns for testing). This set of rules is among the best rule sets found by many other methods <ref> [ Michie et al., 1994 ] </ref> . | Complete set of rules | Rule0: 0 0 1 0 &gt; 0 Hit: 25 and Miss: 0 Rule1: 0 0 2 2 &gt; 1 Hit: 24 and Miss: 0 Default: 2 Hit: 25 and Miss: 1 | Testing rules on the training data:
Reference: [ Pham and Aksoy, 1994 ] <author> D.T. Pham and M.S. Aksoy. </author> <title> An algorithm for automatic rule induction. </title> <journal> Artificial Intelligence in Engineering, </journal> <volume> 8, </volume> <year> 1994. </year> <month> 15 </month>
Reference-contexts: Three data sets are used in experiments. They are (1) Iris [ Fisher, 1936 ] , (2) CAR [ Yen and Chen, 1995 ] (given in Table 1), and (3) Golf-Playing <ref> [ Pham and Aksoy, 1994 ] </ref> (given in Table 2). For numeric attributes (e.g., in Golf-Playing and Iris data), Chi2 [ Liu and Setiono, 1995 ] is applied to discretizing these attributes. <p> Rewriting the three rules in their original terms, they are: Rule 0: If Outlook = sunny & Humid. 85 then don't play Rule 1: If Outlook = rainy & Wind = strong then don't play Default Rule: play. As reported in <ref> [ Pham and Aksoy, 1994; Pham and Aksoy, 1995 ] </ref> , RULES-2 produced 8 and 14 rules for options 1 and 2, and RULES produced 13 rules. The three systems (RND, RULES and RULES-2) all have accuracy of 100%. <p> Given in <ref> [ Pham and Aksoy, 1994 ] </ref> , the computational complexity of RULES-2 in its worst case is: N RULES2 = O (n 2 m X m! i) O (n 2 2 m ); where m &gt; 1.
Reference: [ Pham and Aksoy, 1995 ] <author> D.T. Pham and M.S. Aksoy. </author> <title> Rules: A simple rule extraction system. </title> <journal> Expert Systems With Applications, </journal> <volume> 8(1), </volume> <year> 1995. </year>
Reference-contexts: Rewriting the three rules in their original terms, they are: Rule 0: If Outlook = sunny & Humid. 85 then don't play Rule 1: If Outlook = rainy & Wind = strong then don't play Default Rule: play. As reported in <ref> [ Pham and Aksoy, 1994; Pham and Aksoy, 1995 ] </ref> , RULES-2 produced 8 and 14 rules for options 1 and 2, and RULES produced 13 rules. The three systems (RND, RULES and RULES-2) all have accuracy of 100%.
Reference: [ Quinlan, 1986 ] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: .99 | Testing rules on the testing data: | Total #Item: 75; Hit: 73; Miss: 1; Acc: .97 Experiment 3 The CAR data is used to compare RND with the results reported in [ Yen and Chen, 1995 ] since they have done comparisons with other methods such as ID3 <ref> [ Quinlan, 1986 ] </ref> and the one by Han et al [ Han et al., 1993 ] . It also show that some redundant rules can be removed due to their narrow coverage. The details of the rule set are shown below.
Reference: [ Russell and Norvig, 1995 ] <author> S. Russell and P. Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentic Hall, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction Classification rules are sought in many areas from automatic knowledge acquisition <ref> [ Russell and Norvig, 1995 ] </ref> to data mining [ Agrawal et al., 1993 ] , neural network rule extraction [ Towell and Shavlik, 1993; Setiono and Liu, 1995 ] . This is because classification rules possess some attractive features. <p> A simpler set is preferred. This choice is in line with a general principle of inductive learning often called Ockham's razor <ref> [ Russell and Norvig, 1995 ] </ref> : The most likely hypothesis is the simplest one that is consistent with all observations. Therefore, other things being equal, an ideal rule set has a minimum number of rules and each rule is as short as possible.
Reference: [ Setiono and Liu, 1995 ] <author> R. Setiono and H. Liu. </author> <title> Understanding neural networks via rule extraction. </title> <booktitle> In IJCAI-95, Proceedings International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year> <note> http://www.iscs.nus.sg/ ~ liuh/ijcai95.ps </note>
Reference-contexts: 1 Introduction Classification rules are sought in many areas from automatic knowledge acquisition [ Russell and Norvig, 1995 ] to data mining [ Agrawal et al., 1993 ] , neural network rule extraction <ref> [ Towell and Shavlik, 1993; Setiono and Liu, 1995 ] </ref> . This is because classification rules possess some attractive features. They are explicit, understandable and verifiable by domain experts, and can be modified, extended and passed on as modular knowledge. <p> The immediate benefit of RND is that it can be easily used as a component of other systems due to its efficiency and simplicity. RND has been incorporated in several systems and successfully applied to neural network rule extraction <ref> [ Setiono and Liu, 1995 ] </ref> and data mining [ Lu et al., 1995 ] . The practice indicates that combining RND with other methods, the rule induction power is significantly improved. <p> For example, the single-pattern-based-rule induction algorithm (such as RND) might be weaker than a neural net for applications like parity checking. Instead of inducing rules directly from the data, RND is employed to extract rules from the weights of a neural net <ref> [ Setiono and Liu, 1995 ] </ref> . Further work will be to enhance RND with pre-and post-processing such as combining feature selection, rule reorganization based on estimated errors. 14
Reference: [ Towell and Shavlik, 1993 ] <author> G.G. Towell and J.W. Shavlik. </author> <title> Extracting refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 71-101, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Classification rules are sought in many areas from automatic knowledge acquisition [ Russell and Norvig, 1995 ] to data mining [ Agrawal et al., 1993 ] , neural network rule extraction <ref> [ Towell and Shavlik, 1993; Setiono and Liu, 1995 ] </ref> . This is because classification rules possess some attractive features. They are explicit, understandable and verifiable by domain experts, and can be modified, extended and passed on as modular knowledge.
Reference: [ Wallace and Freeman, 1987 ] <author> C. Wallace and P. Freeman. </author> <title> Estimation and inference by compact function. </title> <journal> J. Royal Statistical Soc. Series B., </journal> <volume> 49B(3):240-265, </volume> <year> 1987. </year>
Reference-contexts: Learning from sample data can be described as estimating a model's parameters. There are strong theoretical reasons for developing learning methods that cover patterns in the most efficient and compact manner <ref> [ Wallace and Freeman, 1987; 1 Weiss and Indurkhya, 1991 ] </ref> . One of the purposes of a rule induction method is to find compact rules that generalize well on the data.
Reference: [ Weiss and Indurkhya, 1991 ] <author> S.M. Weiss and N. Indurkhya. </author> <title> Optimized rule induction. </title> <journal> IEEE Expert, </journal> <volume> 8(6) </volume> <pages> 61-69, </pages> <month> December </month> <year> 1991. </year>
Reference: [ Yen and Chen, 1995 ] <author> S-J Yen and A.L.P. Chen. </author> <title> An efficient algorithm for deriving compact rules from databases. </title> <booktitle> In Proceedings of the Fourth International Conference on Database Systems for Advanced Applications, </booktitle> <year> 1995. </year> <month> 16 </month>
Reference-contexts: Thus, the concept of compactness of a rule is multi-dimensional. These dimensions are: 1. the number of rules in a set; 2. the number of conditions in a rule; 3. the number of patterns covered and correctly classified by a rule. The following compactness measure was proposed in <ref> [ Yen and Chen, 1995 ] </ref> : E = r i=1 n c i where r is the cardinality of the rule set, n the number of patterns, t i the number of patterns correctly classified by rule i, 1 i r, and c i the number of conditions in rule <p> Three data sets are used in experiments. They are (1) Iris [ Fisher, 1936 ] , (2) CAR <ref> [ Yen and Chen, 1995 ] </ref> (given in Table 1), and (3) Golf-Playing [ Pham and Aksoy, 1994 ] (given in Table 2). For numeric attributes (e.g., in Golf-Playing and Iris data), Chi2 [ Liu and Setiono, 1995 ] is applied to discretizing these attributes. <p> 1 | Testing rules on the training data: | Total #Item: 75; Hit: 74; Miss: 1; Acc: .99 | Testing rules on the testing data: | Total #Item: 75; Hit: 73; Miss: 1; Acc: .97 Experiment 3 The CAR data is used to compare RND with the results reported in <ref> [ Yen and Chen, 1995 ] </ref> since they have done comparisons with other methods such as ID3 [ Quinlan, 1986 ] and the one by Han et al [ Han et al., 1993 ] . It also show that some redundant rules can be removed due to their narrow coverage. <p> In total, five rules remain after pruning. This set of rules has an E equal to 0.166 according to formula (1). Using the same formula, the compactness values for results by the three systems reported in <ref> [ Yen and Chen, 1995 ] </ref> are 0.12, 0.062, and 0.014 for LCR of [ Yen and Chen, 1995 ] , ID3 and Han et al's algorithm, respectively. The higher the compactness value is, the more compact a rule set is. <p> This set of rules has an E equal to 0.166 according to formula (1). Using the same formula, the compactness values for results by the three systems reported in <ref> [ Yen and Chen, 1995 ] </ref> are 0.12, 0.062, and 0.014 for LCR of [ Yen and Chen, 1995 ] , ID3 and Han et al's algorithm, respectively. The higher the compactness value is, the more compact a rule set is. For this data 11 set, RND generates the most compact rule set.
References-found: 15

