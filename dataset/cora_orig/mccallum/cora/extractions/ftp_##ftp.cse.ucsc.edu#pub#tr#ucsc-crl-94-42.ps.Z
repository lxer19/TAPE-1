URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-94-42.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: On the Worst-case Analysis of Temporal-difference Learning Algorithms  
Author: Robert E. Schapire Manfred K. Warmuth 
Note: Address: AT&T Bell Laboratories, 600  Address:  
Address: Santa Cruz, CA 95064 USA  Mountain Avenue, Room 2A-424, Murray Hill, NJ 07974.  Santa Cruz, CA 95064.  
Affiliation: Baskin Center for Computer Engineering Information Sciences University of California, Santa Cruz  Computer and Information Sciences, University of California,  
Pubnum: UCSC-CRL-94-42  
Email: Email address: schapire@research.att.com  Email address: manfred@cse.ucsc.edu  
Date: October 27, 1994  
Abstract: We study the worst-case behavior of a family of learning algorithms based on Sutton's method of temporal differences. In our on-line learning framework, learning takes place in a sequence of trials, and the goal of the learning algorithm is to estimate a discounted sum of all the reinforcements that will be received in the future. In this setting, we are able to prove general upper bounds on the performance of a slightly modified version of Sutton's so-called TD() algorithm. These bounds are stated in terms of the performance of the best linear predictor on the given training sequence, and are proved without making any statistical assumptions of any kind about the process producing the learner's observed training sequence. We also prove lower bounds on the performance of any algorithm for this learning problem, and give a similar analysis of the closely related problem of learning to predict in a model in which the learner must produce predictions for a whole batch of observations before receiving reinforcement. fl A preliminary extended abstract of this paper appeared in Machine Learning: Proceedings of the Eleventh International Conference, 1994. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Nicolo Cesa-Bianchi, Philip M. Long, and Manfred K. Warmuth. </author> <title> Worst-case quadratic loss bounds for a generalization of the Widrow-Hoff rule. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 429-438, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Various methods are known for guessing these parameters when they are unknown; see, for instance, Cesa-Bianchi, Long and Warmuth's paper <ref> [1] </ref>.) Thus, TD fl (1) will perform reasonably well, provided that there exists some linear predictor u that gives a good fit to the training sequence. <p> Our bounds for the special case when = 0 or = 1 can be stated in closed form. The proof techniques used in this paper are similar but more general than those used by Cesa-Bianchi, Long and Warmuth <ref> [1] </ref> in their analysis of the Widrow-Hoff algorithm (corresponding to the case that fl = 0). 1 In this paper we only use one vector norm, the L 2 -norm: jjujj = q P N i : Note that minfL ` (u; S) : u 2 R N g is the <p> We again prove worst-case bounds for this model (extending Cesa-Bianchi, Long and Warmuth's previous analysis <ref> [1] </ref> for the noise-free case). We also prove matching lower bounds for this very general model, thus proving that our upper bounds are the optimal worst-case bounds. The paper is outlined as follows. Section 2 describes the on-line model for temporal difference learning. <p> Each of these algorithms is parameterized by a real number 2 <ref> [0; 1] </ref>. For any sequence S and t = 1; 2; , let X t X (fl) tk x k : (3) The learning algorithm T D () works by maintaining a weight vector w t 2 R N . <p> The weight vector w t is then updated to the new weight vector w t+1 using the following update rule: w t+1 := w t + t (r t + fl ^y t+1 ^y t )X 4 Algorithm TD fl () Parameters: discount rate fl 2 <ref> [0; 1) 2 [0; 1] </ref> start vector w 1 2 R N method of computing learning rate t Given: training sequence x 1 ; r 1 ; x 2 ; r 2 ; : : : Predict: ^y 1 ; ^y 2 ; : : : Procedure: get x 1 1 <p> The weight vector w t is then updated to the new weight vector w t+1 using the following update rule: w t+1 := w t + t (r t + fl ^y t+1 ^y t )X 4 Algorithm TD fl () Parameters: discount rate fl 2 [0; 1) 2 <ref> [0; 1] </ref> start vector w 1 2 R N method of computing learning rate t Given: training sequence x 1 ; r 1 ; x 2 ; r 2 ; : : : Predict: ^y 1 ; ^y 2 ; : : : Procedure: get x 1 1 x 1 1 <p> We then apply the lemma to derive an analysis of some special cases of interest. Lemma 1: Let fl 2 <ref> [0; 1), 2 [0; 1] </ref>, and let S be an arbitrary training sequence such that jjX t jj X for all trials t. <p> We then apply the lemma to derive an analysis of some special cases of interest. Lemma 1: Let fl 2 [0; 1), 2 <ref> [0; 1] </ref>, and let S be an arbitrary training sequence such that jjX t jj X for all trials t. <p> Ultimately, we hope to extend our analysis to facilitate the optimal choice of &gt; 0 and 2 <ref> [0; 1] </ref>. In the meantime, we can numerically find the choices of and that minimize the worst-case bound given in Lemma 1. <p> Theorem 4: Let fl 2 <ref> [0; 1] </ref>, X &gt; 0, K 0, U 0 and ` a positive integer. For every algorithm A, there exists a sequence S such that the following hold: 1. jjx t jj X, 2. K = minfL ` (u; S) : jjujj U g, and 3. <p> There is a straightforward generalization of the above scenario when more than one instance is received in each trial t. In this generalization, which was previously analyzed in the noise-free case by Cesa-Bianchi, Long and Warmuth <ref> [1] </ref>, the learner does the following in each trial: 1. receives a real-valued matrix M t with N columns; 2. predicts with ^y t = M t w t ; 3. gets reinforcement y t , a real column vector whose dimension is the number of rows of M t ; <p> Similarly, the total loss of a weight vector u 2 R N is defined as L ` (u; S) := t=1 The proof of the following lemma and theorem are a straightforward generalization of the worst-case analysis of the Widrow-Hoff algorithm given by Cesa-Bianchi, Long and Warmuth <ref> [1] </ref>. <p> The bound for K = 0 was previously proved by Cesa-Bianchi, Long and Warmuth <ref> [1] </ref>. An alternate proof of the above theorem via a reduction from the corresponding theorem for the original Widrow-Hoff algorithm was recently provided by Kivinen and Warmuth [5]. The following lower bound shows that the bounds of the above theorem are best possible. <p> Open problems. There remain many open research problems in this area. The first of these is to reduce the bound given in Lemma 1 to closed form to facilitate the optimal choice of 2 <ref> [0; 1] </ref>. However, as clearly indicated by Figure 2, even when and are chosen so as to minimize this bound, there remains a significant gap between the upper bounds proved in Section 4 and the lower bound proved in Section 5.
Reference: [2] <author> Peter Dayan. </author> <title> The convergence of T D() for general . Machine Learning, </title> 8(3/4):341-362, May 1992. 
Reference-contexts: We study the worst-case behavior of a family of learning algorithms based on Sutton's method of temporal differences [6]. Specifically, we analyze (a slightly modified version of) Sutton's so-called TD () algorithm in a worst-case framework that makes no statistical assumptions of any kind. All previous analyses <ref> [2, 3, 4, 6, 7] </ref> of TD () have relied heavily on stochastic assumptions about the nature of the environment that is generating the data observed by the learner; for instance, the learner's environment is often modeled by a Markov process.
Reference: [3] <author> Peter Dayan and Terrence J. Sejnowski. </author> <title> T D() converges with probability 1. </title> <journal> Machine Learning, </journal> <volume> 14(3) </volume> <pages> 295-301, </pages> <year> 1994. </year>
Reference-contexts: We study the worst-case behavior of a family of learning algorithms based on Sutton's method of temporal differences [6]. Specifically, we analyze (a slightly modified version of) Sutton's so-called TD () algorithm in a worst-case framework that makes no statistical assumptions of any kind. All previous analyses <ref> [2, 3, 4, 6, 7] </ref> of TD () have relied heavily on stochastic assumptions about the nature of the environment that is generating the data observed by the learner; for instance, the learner's environment is often modeled by a Markov process.
Reference: [4] <author> Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <type> Technical Report 9307, </type> <institution> MIT Computational Cognitive Science, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: We study the worst-case behavior of a family of learning algorithms based on Sutton's method of temporal differences [6]. Specifically, we analyze (a slightly modified version of) Sutton's so-called TD () algorithm in a worst-case framework that makes no statistical assumptions of any kind. All previous analyses <ref> [2, 3, 4, 6, 7] </ref> of TD () have relied heavily on stochastic assumptions about the nature of the environment that is generating the data observed by the learner; for instance, the learner's environment is often modeled by a Markov process.
Reference: [5] <author> Jyrki Kivinen and Manfred K. Warmuth. </author> <title> Additive versus exponentiated gradient updates for learning linear functions. </title> <type> Technical Report UCSC-CRL-94-16, </type> <institution> University of California Santa Cruz, Computer Research Laboratory, </institution> <year> 1994. </year>
Reference-contexts: The bound for K = 0 was previously proved by Cesa-Bianchi, Long and Warmuth [1]. An alternate proof of the above theorem via a reduction from the corresponding theorem for the original Widrow-Hoff algorithm was recently provided by Kivinen and Warmuth <ref> [5] </ref>. The following lower bound shows that the bounds of the above theorem are best possible. Theorem 7: Let N; m 1, K; U 0 and M &gt; 0. <p> Although one might expect such a pessimistic approach to give rather weak results, we have found, somewhat surprisingly, that very strong bounds can often be proved even in the worst case. Worst-case bounds for on-line linear learning algorithms can be very tight even on artificial data <ref> [5] </ref>. Good experimental performance of a particular algorithm might be seen as weak evidence for showing that the algorithm is good since every algorithm performs well on some data, particularly when the data is artificial. <p> As described in Section 3, TD fl () can be motivated using gradient descent. Rules of this kind can alternatively be derived within a framework described by Kivinen and Warmuth <ref> [5] </ref>. Moreover, by modifying one of the parameters of their framework, they show that rules having a qualitatively different flavor can be derived.
Reference: [6] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Or, rather than cutting off the average after a fixed amount of time, we might ask that the learner predict a weighted (or discounted) sum of the stock prices on each day in the future, giving lesser weight to days farther in the future. In this paper, following Sutton <ref> [6] </ref>, we take up the study of this latter type of prediction problem. More precisely, we study the problem of learning to predict the discounted sum y t := k=0 where fl 2 [0; 1) is some fixed constant called the discount rate parameter. <p> Thus, the goal of the learning algorithm is to minimize its loss over a sequence of observation/feedback trials. We study the worst-case behavior of a family of learning algorithms based on Sutton's method of temporal differences <ref> [6] </ref>. Specifically, we analyze (a slightly modified version of) Sutton's so-called TD () algorithm in a worst-case framework that makes no statistical assumptions of any kind. <p> We study the worst-case behavior of a family of learning algorithms based on Sutton's method of temporal differences [6]. Specifically, we analyze (a slightly modified version of) Sutton's so-called TD () algorithm in a worst-case framework that makes no statistical assumptions of any kind. All previous analyses <ref> [2, 3, 4, 6, 7] </ref> of TD () have relied heavily on stochastic assumptions about the nature of the environment that is generating the data observed by the learner; for instance, the learner's environment is often modeled by a Markov process. <p> A special case of this model is when the algorithm has to make predictions on a whole batch of instances before receiving the same outcome for all of them (a case studied by Sutton <ref> [6] </ref>). We again prove worst-case bounds for this model (extending Cesa-Bianchi, Long and Warmuth's previous analysis [1] for the noise-free case). We also prove matching lower bounds for this very general model, thus proving that our upper bounds are the optimal worst-case bounds. The paper is outlined as follows. <p> use the variables y 0 t = y t (1 fl). (For instance, if all r t equal r, then the modified variables y 0 t all equal r as well.) However, for the sake of notational simplicity, we use the variables y t instead (as was done by Sutton <ref> [6] </ref> and others). The infinite sequence of pairs of instances x t and reinforcement signals r t is called a training sequence (usually denoted by S). <p> would like to show that if there exists a weight vector u that fits the training sequence well, then the learner's predictions will also be reasonably good. 3 Temporal-difference algorithms We focus now on a family of learning algorithms that are only a slight modification of those considered by Sutton <ref> [6] </ref>. Each of these algorithms is parameterized by a real number 2 [0; 1]. <p> To model a particular reinforcement learning problem, we have the freedom to make up the matrices M t and reinforcements y t to suit our purpose. For example, for the case considered by Sutton <ref> [6] </ref> in which the goal is to predict a single outcome following a sequence of observations, we let M t contain the instances of a run and set y t = (z t ; ; z t ) T , where z t 13 is the reinforcement received for the tth
Reference: [7] <author> C. J. C. H. Watkins. </author> <title> Learning from delayed rewards. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <address> England, </address> <year> 1989. </year>
Reference-contexts: We study the worst-case behavior of a family of learning algorithms based on Sutton's method of temporal differences [6]. Specifically, we analyze (a slightly modified version of) Sutton's so-called TD () algorithm in a worst-case framework that makes no statistical assumptions of any kind. All previous analyses <ref> [2, 3, 4, 6, 7] </ref> of TD () have relied heavily on stochastic assumptions about the nature of the environment that is generating the data observed by the learner; for instance, the learner's environment is often modeled by a Markov process. <p> Lastly, Sutton's TD () algorithm can be viewed as a special case of Watkin's "Q-learning" algorithm <ref> [7] </ref>. This algorithm is meant to handle a setting in which the learner has a set of actions to choose from, and attempts to choose its actions so as to maximize its total payoff.
References-found: 7

