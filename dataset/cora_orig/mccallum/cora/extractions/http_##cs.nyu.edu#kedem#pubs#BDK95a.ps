URL: http://cs.nyu.edu/kedem/pubs/BDK95a.ps
Refering-URL: http://cs.nyu.edu/courses/spring98/G22.3033.10/index.html
Root-URL: http://www.cs.nyu.edu
Title: Calypso: An Environment for Reliable Distributed Parallel Processing  
Author: Arash Baratloo Partha Dasgupta Zvi M. Kedem 
Date: October 2, 1995  
Affiliation: New York University  Arizona State University  New York University  
Abstract: The importance of adapting networks of workstations for use as parallel processing platforms is well established. However, current solutions do not always satisfactorily address important issues that exist in real networks. External factors like the sharing of resources, unpredictable behavior of the network and machines including slowdowns and failures, are present in multiuser networks and cause poor performance. In using todays available toolkits for distributed programming, in general, the responsibility of handling these external factors is left to the programmer, a task that further complicates the development of an already difficult job of parallel programming on distributed systems. Calypso is a prototype software system for writing and executing parallel programs on non-dedicated platforms, using Commercial Off-The-Shelf (COTS) networked workstations, operating systems, and compilers. Among notable properties of the system are: (1) simple programming paradigm incorporating shared memory constructs, (2) separation of the program and the execution parallelism to allow programs to scale as computers join an ongoing computation, (3) transparent utilization of unreliable shared resources by providing dynamic load balancing and fault tolerance, and (4) effective performance for large classes of coarse-grained computations. In this paper we introduce Calypso, present its goals, and describe the design and the implementation of the current prototype. We also report on our initial experiments and performance results in settings that closely resemble the dynamic behavior of a "real" network. Under varying work-load conditions, resource availability and process failures, the efficiency of our test program ranged from 94% to 87% on five networked workstations, obtaining close to optimal speedups|highly competitive with systems that are less robust. fl This research was partially supported by the National Science Foundation under grant numbers CCR-94-11590, and CCR-95-05519. y A short preliminary version of this paper has appeared in Proc. 4th IEEE Intl. Symp. on High Performance Distributed Computing, August 1995. z Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer St., New York, NY 10012-1185, (212) 998-3350, baratloo@cs.nyu.edu. x Department of Computer Science, Arizona State University, Tempe, AZ 85287-5406, (602) 965-5583, partha@cs.eas.asu.edu. Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, 251 Mercer St., New York, NY 10012-1185, (212) 998-3101, kedem@cs.nyu.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Ahuja, N. Carriero, and D. Gelernter. </author> <title> Linda and friends. </title> <journal> IEEE Computer, </journal> <volume> 19(8) </volume> <pages> 26-34, </pages> <year> 1986. </year>
Reference-contexts: Such a platform has been perceived by some as an inadequate substitute for a "real" parallel machine. Nevertheless, it is known that many applications run quite well on networks of workstations, and this has contributed to the popularity of systems such as, Linda <ref> [1, 14] </ref>, MPI [24], and PVM [40, 22]. Calypso is an integrated system that allows the programming and execution of computationally complex applications on a set of workstations on a network. It can utilize publicly used workstations, and takes care of dynamic availabilities, slowdown and failures prevalent in such configurations.
Reference: [2] <author> B. Anderson and D. Shasha. </author> <title> Persistent Linda: Linda + transactions + query processing. </title> <editor> In J.P. Banatre and D. Le Metayer, editors, </editor> <booktitle> Research Directions in High-Level Parallel Programming Languages, number 57 in LNCS, </booktitle> <pages> pages 93-109. </pages> <publisher> Springer, </publisher> <year> 1991. </year>
Reference-contexts: There have been three major mechanisms: checkpointing, replication, and process groups. Such approaches have been implemented in CIRCUS [15], LOCUS [36] and Clouds [20], Isis [12, 39, 11], FT-PVM [33], FT-Linda [5], and PLinda <ref> [2, 26] </ref>. However, all these systems 3 add significant overhead, even when there is no failure. More recently several prominent projects have similar goals to us. These include the NOWs project at Berkeley, the HPC++ project and the Dome project at CMU.
Reference: [3] <author> Y. Aumann, Z. Kedem, K. Palem, and M. Rabin. </author> <title> Highly efficient asynchronous execution of large-grained parallel programs. </title> <booktitle> In 34th IEEE Ann. Symp. on Foundations of Computer Science, </booktitle> <pages> pages 271-280, </pages> <year> 1993. </year>
Reference: [4] <author> Y. Aumann and M. Rabin. </author> <title> Clock construction in fully asynchronous parallel systems and PRAM simulation. </title> <booktitle> In 33rd IEEE Ann. Symp. on Foundations of Computer Science, </booktitle> <pages> pages 147-156, </pages> <year> 1992. </year>
Reference: [5] <author> D. Bakken and R. Schlichting. </author> <title> Supporting fault-tolerant parallel programming in Linda. </title> <type> Tech--nical Report TR93-18, </type> <institution> The University of Arizona, </institution> <year> 1993. </year>
Reference-contexts: However, it does not implement a fault-tolerant manager, described in [19], which relied on dispersal and evasion. We now briefly summarize other related work. A large body of experimental results exist in the attempt to make parallel programs run on distributed hardware <ref> [22, 33, 40, 5, 14, 8, 10, 6, 7] </ref>. These systems can be loosely divided into two types, those that depend on a message passing scheme and those that use some form of global address spaces. <p> The issues of providing fault tolerance have generally been addressed separately from the issues of parallel processing. There have been three major mechanisms: checkpointing, replication, and process groups. Such approaches have been implemented in CIRCUS [15], LOCUS [36] and Clouds [20], Isis [12, 39, 11], FT-PVM [33], FT-Linda <ref> [5] </ref>, and PLinda [2, 26]. However, all these systems 3 add significant overhead, even when there is no failure. More recently several prominent projects have similar goals to us. These include the NOWs project at Berkeley, the HPC++ project and the Dome project at CMU.
Reference: [6] <author> H. Bal and A. Tanenbaum. </author> <title> Distributed programming with shared data. </title> <booktitle> In Proceedings of ICCL, </booktitle> <pages> pages 82-91, </pages> <address> Miami, FL, </address> <month> October </month> <year> 1988. </year> <title> IEEE, </title> <publisher> Computer Society Press. </publisher>
Reference-contexts: However, it does not implement a fault-tolerant manager, described in [19], which relied on dispersal and evasion. We now briefly summarize other related work. A large body of experimental results exist in the attempt to make parallel programs run on distributed hardware <ref> [22, 33, 40, 5, 14, 8, 10, 6, 7] </ref>. These systems can be loosely divided into two types, those that depend on a message passing scheme and those that use some form of global address spaces.
Reference: [7] <author> H. E. Bal and A. S. Tanenbaum. Orca: </author> <title> A language for distributed object-based programming. </title> <journal> SIGPLAN Notices, </journal> <volume> 25(5) </volume> <pages> 17-24, </pages> <month> may </month> <year> 1990. </year>
Reference-contexts: However, it does not implement a fault-tolerant manager, described in [19], which relied on dispersal and evasion. We now briefly summarize other related work. A large body of experimental results exist in the attempt to make parallel programs run on distributed hardware <ref> [22, 33, 40, 5, 14, 8, 10, 6, 7] </ref>. These systems can be loosely divided into two types, those that depend on a message passing scheme and those that use some form of global address spaces. <p> Many systems provide message passing, or Remote Procedure Call facility built on top of a message passing. These include PVM [40, 22], Orca <ref> [7] </ref>, GLU [25], Isis and Horus [13, 11], Concert/C and so on. These systems provide a runtime library (and sometimes compiler support) to enable the writing of parallel programs as concurrently executable units. These units are then spawned on different machines on the network, typically by a control site.
Reference: [8] <author> R. Balter, J. Bernadat, D. Decouchant, A. Duda, A. Freyssinet, S. Krakowiak, M. Meysembourg, P. Le Dot, H. Nguyen Van, E. Paire, M. Riveill, C. Roisin, X. Rousset de Pina, R. Scioville, and Vandome. </author> <title> Architecture and implementation of Guide, an object-oriented distributed system. </title> <journal> Computing Systems, </journal> <volume> 4(1) </volume> <pages> 31-67, </pages> <year> 1991. </year>
Reference-contexts: However, it does not implement a fault-tolerant manager, described in [19], which relied on dispersal and evasion. We now briefly summarize other related work. A large body of experimental results exist in the attempt to make parallel programs run on distributed hardware <ref> [22, 33, 40, 5, 14, 8, 10, 6, 7] </ref>. These systems can be loosely divided into two types, those that depend on a message passing scheme and those that use some form of global address spaces.
Reference: [9] <author> J. Bennett, J. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proc. 2nd Annual Symp. on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA (USA), </address> <year> 1990. </year> <journal> ACM SIGPLAN. </journal>
Reference-contexts: Using global memory to make programs communicate has been established as a "natural" interface for parallel programming. Distributed systems do not support global memory in hardware, and hence, this feature has to be implemented in software. While systems built around Distributed Shared Memory (DSM) like IVY [34], Munin <ref> [9] </ref>, Clouds [16, 20, 18], Mether-NFS [35]) provide a more natural programming model, they still suffer from the high cost of distributed synchronization and the inability to provide suitable fault tolerance. A mature system that uses a variant of the DSM concept is Linda [14].
Reference: [10] <author> B. Bershad, E. Lazowska, and H. Levy. </author> <title> PRESTO: A system for object-oriented parallel programming. </title> <journal> Software|Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <year> 1988. </year>
Reference-contexts: However, it does not implement a fault-tolerant manager, described in [19], which relied on dispersal and evasion. We now briefly summarize other related work. A large body of experimental results exist in the attempt to make parallel programs run on distributed hardware <ref> [22, 33, 40, 5, 14, 8, 10, 6, 7] </ref>. These systems can be loosely divided into two types, those that depend on a message passing scheme and those that use some form of global address spaces.
Reference: [11] <author> K. Birman. </author> <title> The process group approach to reliable distributed computing. </title> <type> Technical report, </type> <institution> Cornell University, </institution> <year> 1993. </year>
Reference-contexts: Many systems provide message passing, or Remote Procedure Call facility built on top of a message passing. These include PVM [40, 22], Orca [7], GLU [25], Isis and Horus <ref> [13, 11] </ref>, Concert/C and so on. These systems provide a runtime library (and sometimes compiler support) to enable the writing of parallel programs as concurrently executable units. These units are then spawned on different machines on the network, typically by a control site. <p> The issues of providing fault tolerance have generally been addressed separately from the issues of parallel processing. There have been three major mechanisms: checkpointing, replication, and process groups. Such approaches have been implemented in CIRCUS [15], LOCUS [36] and Clouds [20], Isis <ref> [12, 39, 11] </ref>, FT-PVM [33], FT-Linda [5], and PLinda [2, 26]. However, all these systems 3 add significant overhead, even when there is no failure. More recently several prominent projects have similar goals to us.
Reference: [12] <author> K. Birman, T. Joseph, T. Raeuchle, and A. Abbadi. </author> <title> Implementing fault-tolerant distributed objects. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(6):502-508, </volume> <year> 1985. </year>
Reference-contexts: The issues of providing fault tolerance have generally been addressed separately from the issues of parallel processing. There have been three major mechanisms: checkpointing, replication, and process groups. Such approaches have been implemented in CIRCUS [15], LOCUS [36] and Clouds [20], Isis <ref> [12, 39, 11] </ref>, FT-PVM [33], FT-Linda [5], and PLinda [2, 26]. However, all these systems 3 add significant overhead, even when there is no failure. More recently several prominent projects have similar goals to us.
Reference: [13] <author> K. Birman, A. Schiper, and P. Stephenson. </author> <title> Lightweight causal and atomic group multicast. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(3) </volume> <pages> 272-314, </pages> <year> 1991. </year>
Reference-contexts: Many systems provide message passing, or Remote Procedure Call facility built on top of a message passing. These include PVM [40, 22], Orca [7], GLU [25], Isis and Horus <ref> [13, 11] </ref>, Concert/C and so on. These systems provide a runtime library (and sometimes compiler support) to enable the writing of parallel programs as concurrently executable units. These units are then spawned on different machines on the network, typically by a control site.
Reference: [14] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communication of ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <year> 1989. </year>
Reference-contexts: Such a platform has been perceived by some as an inadequate substitute for a "real" parallel machine. Nevertheless, it is known that many applications run quite well on networks of workstations, and this has contributed to the popularity of systems such as, Linda <ref> [1, 14] </ref>, MPI [24], and PVM [40, 22]. Calypso is an integrated system that allows the programming and execution of computationally complex applications on a set of workstations on a network. It can utilize publicly used workstations, and takes care of dynamic availabilities, slowdown and failures prevalent in such configurations. <p> However, it does not implement a fault-tolerant manager, described in [19], which relied on dispersal and evasion. We now briefly summarize other related work. A large body of experimental results exist in the attempt to make parallel programs run on distributed hardware <ref> [22, 33, 40, 5, 14, 8, 10, 6, 7] </ref>. These systems can be loosely divided into two types, those that depend on a message passing scheme and those that use some form of global address spaces. <p> A mature system that uses a variant of the DSM concept is Linda <ref> [14] </ref>. Instead of having a global address space, Linda employs a database type environment, the tuple-space. The tuple space (managed by tuple-servers) provide the functions of shared memory, data storage service, control information provider and synchronization.
Reference: [15] <author> E. Cooper. </author> <title> Replicated distributed programs. </title> <journal> Operating Systems Review, </journal> <volume> 19(5) </volume> <pages> 63-78, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: The issues of providing fault tolerance have generally been addressed separately from the issues of parallel processing. There have been three major mechanisms: checkpointing, replication, and process groups. Such approaches have been implemented in CIRCUS <ref> [15] </ref>, LOCUS [36] and Clouds [20], Isis [12, 39, 11], FT-PVM [33], FT-Linda [5], and PLinda [2, 26]. However, all these systems 3 add significant overhead, even when there is no failure. More recently several prominent projects have similar goals to us.
Reference: [16] <author> P. Dasgupta, R. Ananthanarayanan, S. Menon, A. Mohindra, and R. Chen. </author> <title> Distributed programming with objects and threads in the Clouds system. </title> <journal> Computing Sytems, </journal> <volume> 4(3) </volume> <pages> 243-276, </pages> <year> 1991. </year>
Reference-contexts: Distributed systems do not support global memory in hardware, and hence, this feature has to be implemented in software. While systems built around Distributed Shared Memory (DSM) like IVY [34], Munin [9], Clouds <ref> [16, 20, 18] </ref>, Mether-NFS [35]) provide a more natural programming model, they still suffer from the high cost of distributed synchronization and the inability to provide suitable fault tolerance. A mature system that uses a variant of the DSM concept is Linda [14].
Reference: [17] <author> P. Dasgupta and R. C. Chen. </author> <title> Memory semantics for large grained persistent objects. </title> <editor> In G. Shaw A. Dearle and S. Zdonick, editors, </editor> <title> Implementation of Persistent Object Systems. </title> <publisher> Morgan Kauf-man, </publisher> <year> 1990. </year>
Reference: [18] <author> P. Dasgupta, R. C. Chen, S. Menon, M. P. Pearson, R. Ananthanarayanan, U. Ramachandran, M. Ahamad, R. J. LeBlanc, W. F. Appelbe, J. M. Bernabeu-Auban, P. W. Hutto, M. Y. A. Kha-lidi, and C. J. Wilkenloh. </author> <title> The design and implementation of the Clouds distributed operating system. </title> <journal> Computing Systems, </journal> <volume> 3, </volume> <year> 1990. </year>
Reference-contexts: Distributed systems do not support global memory in hardware, and hence, this feature has to be implemented in software. While systems built around Distributed Shared Memory (DSM) like IVY [34], Munin [9], Clouds <ref> [16, 20, 18] </ref>, Mether-NFS [35]) provide a more natural programming model, they still suffer from the high cost of distributed synchronization and the inability to provide suitable fault tolerance. A mature system that uses a variant of the DSM concept is Linda [14].
Reference: [19] <author> P. Dasgupta, Z. M. Kedem, and M. O. Rabin. </author> <title> Parallel processing on networks of workstations: A fault-tolerant, high performance approach. </title> <booktitle> In Proceedings of the 15th Intl. Conf on Distributed Computing Systems, to appear, </booktitle> <month> June </month> <year> 1995. </year> <month> 24 </month>
Reference-contexts: An outline of a network of workstations-based system for parallel computing based on earlier formal work was presented in <ref> [19] </ref>. Calypso is an evolution of this design, and is the result of considerable redesign and extensive experimentation on progressively more and more sophisticated implementations. <p> However, it does not implement a fault-tolerant manager, described in <ref> [19] </ref>, which relied on dispersal and evasion. We now briefly summarize other related work. A large body of experimental results exist in the attempt to make parallel programs run on distributed hardware [22, 33, 40, 5, 14, 8, 10, 6, 7].
Reference: [20] <author> P. Dasgupta, R. J. LeBlanc, M. Ahamad, and U. Ramachandran. </author> <title> The Clouds distributed operating system. </title> <journal> IEEE Computer, </journal> <volume> 24, </volume> <year> 1991. </year>
Reference-contexts: Distributed systems do not support global memory in hardware, and hence, this feature has to be implemented in software. While systems built around Distributed Shared Memory (DSM) like IVY [34], Munin [9], Clouds <ref> [16, 20, 18] </ref>, Mether-NFS [35]) provide a more natural programming model, they still suffer from the high cost of distributed synchronization and the inability to provide suitable fault tolerance. A mature system that uses a variant of the DSM concept is Linda [14]. <p> The issues of providing fault tolerance have generally been addressed separately from the issues of parallel processing. There have been three major mechanisms: checkpointing, replication, and process groups. Such approaches have been implemented in CIRCUS [15], LOCUS [36] and Clouds <ref> [20] </ref>, Isis [12, 39, 11], FT-PVM [33], FT-Linda [5], and PLinda [2, 26]. However, all these systems 3 add significant overhead, even when there is no failure. More recently several prominent projects have similar goals to us.
Reference: [21] <author> M. Fu and P. Dasgupta. </author> <title> Programming support for memory mapped persistent objects. </title> <booktitle> In COMPSAC, </booktitle> <year> 1993. </year>
Reference: [22] <author> G.A. Geist and V.S. Sunderam. </author> <title> Network-based concurrent computing on the PVM system. </title> <journal> Concurrency: Practice and experience, </journal> <volume> 4(4) </volume> <pages> 293-311, </pages> <year> 1992. </year>
Reference-contexts: Such a platform has been perceived by some as an inadequate substitute for a "real" parallel machine. Nevertheless, it is known that many applications run quite well on networks of workstations, and this has contributed to the popularity of systems such as, Linda [1, 14], MPI [24], and PVM <ref> [40, 22] </ref>. Calypso is an integrated system that allows the programming and execution of computationally complex applications on a set of workstations on a network. It can utilize publicly used workstations, and takes care of dynamic availabilities, slowdown and failures prevalent in such configurations. <p> However, it does not implement a fault-tolerant manager, described in [19], which relied on dispersal and evasion. We now briefly summarize other related work. A large body of experimental results exist in the attempt to make parallel programs run on distributed hardware <ref> [22, 33, 40, 5, 14, 8, 10, 6, 7] </ref>. These systems can be loosely divided into two types, those that depend on a message passing scheme and those that use some form of global address spaces. <p> These systems can be loosely divided into two types, those that depend on a message passing scheme and those that use some form of global address spaces. Many systems provide message passing, or Remote Procedure Call facility built on top of a message passing. These include PVM <ref> [40, 22] </ref>, Orca [7], GLU [25], Isis and Horus [13, 11], Concert/C and so on. These systems provide a runtime library (and sometimes compiler support) to enable the writing of parallel programs as concurrently executable units.
Reference: [23] <author> David Gelernter, Marc Jourdenais, and David Kaminsky. </author> <title> Piranha scheduling: Strategies and their implementation. </title> <type> Technical report, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1993. </year>
Reference-contexts: Worker processes in the network retrieve work tuples from the tuple-space and generate result tuples. The system is interesting, and receptive to additional enhancements. However, programs still have to be re-written to use Linda. Pirhana <ref> [23] </ref> provides features similar to Calypso in that is allows dynamic load sharing via the ability to add and subtract workers on the fly. However the programming strategy is different, deleting workers need backing up tuples, and fault-tolerance is not supported.
Reference: [24] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing-Interface. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Such a platform has been perceived by some as an inadequate substitute for a "real" parallel machine. Nevertheless, it is known that many applications run quite well on networks of workstations, and this has contributed to the popularity of systems such as, Linda [1, 14], MPI <ref> [24] </ref>, and PVM [40, 22]. Calypso is an integrated system that allows the programming and execution of computationally complex applications on a set of workstations on a network. It can utilize publicly used workstations, and takes care of dynamic availabilities, slowdown and failures prevalent in such configurations.
Reference: [25] <author> R. Jagannathan and E. A. Ashcroft. </author> <title> Fault tolerance in parallel implementations of functional languages. </title> <booktitle> In The Twenty First International Symposium on Fault-Tolerant Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Many systems provide message passing, or Remote Procedure Call facility built on top of a message passing. These include PVM [40, 22], Orca [7], GLU <ref> [25] </ref>, Isis and Horus [13, 11], Concert/C and so on. These systems provide a runtime library (and sometimes compiler support) to enable the writing of parallel programs as concurrently executable units. These units are then spawned on different machines on the network, typically by a control site.
Reference: [26] <author> K. Jeong and D. Shasha. Plinda 2.0: </author> <title> A transactional/checkpointing approach to fault tolerant linda. </title> <booktitle> In Proceedings of the 13th Symposium on Reliable Distributed Systems, </booktitle> <year> 1994. </year>
Reference-contexts: There have been three major mechanisms: checkpointing, replication, and process groups. Such approaches have been implemented in CIRCUS [15], LOCUS [36] and Clouds [20], Isis [12, 39, 11], FT-PVM [33], FT-Linda [5], and PLinda <ref> [2, 26] </ref>. However, all these systems 3 add significant overhead, even when there is no failure. More recently several prominent projects have similar goals to us. These include the NOWs project at Berkeley, the HPC++ project and the Dome project at CMU.
Reference: [27] <author> Z. Kedem. </author> <title> Methods for handling faults and asynchrony in parallel computations. </title> <booktitle> In 1992 DARPA Software Technology Conference, </booktitle> <pages> pages 189-193, </pages> <month> May </month> <year> 1992. </year>
Reference: [28] <author> Z. Kedem and K. Palem. </author> <title> Transformations for the automatic derivation of resilient parallel programs. </title> <booktitle> In IEEE Workshop on Fault-Tolerant Parallel and Distributed Systems, </booktitle> <pages> pages 15-25, </pages> <year> 1992. </year>
Reference: [29] <author> Z. Kedem, K. Palem, M. Rabin, and A. Raghunathan. </author> <title> Efficient program transformations for resilient parallel computation via randomization. </title> <booktitle> In 24th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 306-317, </pages> <month> May </month> <year> 1992. </year>
Reference: [30] <author> Z. Kedem, K. Palem, A. Raghunathan, and P. Spirakis. </author> <title> Combining tentative and definite algorithms for very fast dependable parallel computing. </title> <booktitle> In 23rd ACM Symp. on Theory of Computing, </booktitle> <pages> pages 381-390, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: That paper details a methodology for instrumenting general parallel programs to automatically obtain their fault-tolerant counterparts that can run on a abstract, shared memory multiprocessing machine. While the solutions in [32] were formulated in the context of synchronous faults, they were later applied in <ref> [30] </ref> to a certain variant of asynchronous behavior i.e. could run on a machine whose processors take arbitrary amounts of time to execute each step.
Reference: [31] <author> Z. Kedem, K. Palem, A. Raghunathan, and P. Spirakis. </author> <title> Resilient parallel computing on unreliable parallel machines. </title> <editor> In A. Gibbons and P. Spirakis, editors, </editor> <booktitle> Lectures on Parallel Computation. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1993. </year>
Reference: [32] <author> Z. Kedem, K. Palem, and P. Spirakis. </author> <title> Efficient robust parallel computations. </title> <booktitle> In 22nd ACM Symp. on Theory of Computing, </booktitle> <pages> pages 138-148, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Some of the mechanisms used in Calypso are described next. 3.2 Mechanisms The paradigm that Calypso embodies in a working system, was first described in <ref> [32] </ref>. That paper details a methodology for instrumenting general parallel programs to automatically obtain their fault-tolerant counterparts that can run on a abstract, shared memory multiprocessing machine. While the solutions in [32] were formulated in the context of synchronous faults, they were later applied in [30] to a certain variant of <p> Calypso are described next. 3.2 Mechanisms The paradigm that Calypso embodies in a working system, was first described in <ref> [32] </ref>. That paper details a methodology for instrumenting general parallel programs to automatically obtain their fault-tolerant counterparts that can run on a abstract, shared memory multiprocessing machine. While the solutions in [32] were formulated in the context of synchronous faults, they were later applied in [30] to a certain variant of asynchronous behavior i.e. could run on a machine whose processors take arbitrary amounts of time to execute each step. <p> The idempotence property is fundamental in Calypso: a code segment can be executed multiple times (with possibly some partial executions), with exactly-once semantics. The importance of idempotence, and the utilization of the eager scheduling to take advantage of it, was discovered in <ref> [32] </ref> in an abstract context. (The term "eager scheduling" itself was coined later.) Eager scheduling is a mechanism for assigning concurrently executable tasks to the available machines. Any machine can execute any "enabled" task, independent of whether this task is already under execution by another machine. <p> And finally, any of the machines that are "helping out" the parallel computation can fail or slow down at any time. 5 The mechanism of collating differential memory provides logical coherence and synchronization while avoiding false sharing. It is an adaption and refinement of the two-phase idempotent execution strategy <ref> [32] </ref> among others. Memory updates are collated to assure exactly-once logical execution, and they are transmitted as bitwise differences, preventing false sharing. This supports efficient implementation of idempotence in addition to other performance benefits that we shall see later. Details about these mechanisms are discussed later.
Reference: [33] <author> J. Leon, A. Fisher, and P. Steenkiste. </author> <title> Fail-safe PVM: A portable package for distributed programming with transparent recovery. </title> <type> Technical Report CMU-CS-93-124, CMU, </type> <year> 1993. </year>
Reference-contexts: However, it does not implement a fault-tolerant manager, described in [19], which relied on dispersal and evasion. We now briefly summarize other related work. A large body of experimental results exist in the attempt to make parallel programs run on distributed hardware <ref> [22, 33, 40, 5, 14, 8, 10, 6, 7] </ref>. These systems can be loosely divided into two types, those that depend on a message passing scheme and those that use some form of global address spaces. <p> The issues of providing fault tolerance have generally been addressed separately from the issues of parallel processing. There have been three major mechanisms: checkpointing, replication, and process groups. Such approaches have been implemented in CIRCUS [15], LOCUS [36] and Clouds [20], Isis [12, 39, 11], FT-PVM <ref> [33] </ref>, FT-Linda [5], and PLinda [2, 26]. However, all these systems 3 add significant overhead, even when there is no failure. More recently several prominent projects have similar goals to us. These include the NOWs project at Berkeley, the HPC++ project and the Dome project at CMU.
Reference: [34] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Using global memory to make programs communicate has been established as a "natural" interface for parallel programming. Distributed systems do not support global memory in hardware, and hence, this feature has to be implemented in software. While systems built around Distributed Shared Memory (DSM) like IVY <ref> [34] </ref>, Munin [9], Clouds [16, 20, 18], Mether-NFS [35]) provide a more natural programming model, they still suffer from the high cost of distributed synchronization and the inability to provide suitable fault tolerance. A mature system that uses a variant of the DSM concept is Linda [14].
Reference: [35] <author> R. Minnich and D. Farber. </author> <title> The Mether system: Distributed shared memory for SunOS 4.0. </title> <booktitle> In USENIX-Summer, </booktitle> <pages> pages 51-60, </pages> <address> Baltimore, Maryland (USA), </address> <year> 1989. </year> <month> 25 </month>
Reference-contexts: Distributed systems do not support global memory in hardware, and hence, this feature has to be implemented in software. While systems built around Distributed Shared Memory (DSM) like IVY [34], Munin [9], Clouds [16, 20, 18], Mether-NFS <ref> [35] </ref>) provide a more natural programming model, they still suffer from the high cost of distributed synchronization and the inability to provide suitable fault tolerance. A mature system that uses a variant of the DSM concept is Linda [14].
Reference: [36] <author> G. Popek, B. Walker, J. Chow, D. Edwards, C. Kline, G. Rudisin, and G. Thiel. </author> <title> Locus: A net-work transparent, high reliability distributed system. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 169-177. </pages> <publisher> ACM, </publisher> <year> 1981. </year>
Reference-contexts: The issues of providing fault tolerance have generally been addressed separately from the issues of parallel processing. There have been three major mechanisms: checkpointing, replication, and process groups. Such approaches have been implemented in CIRCUS [15], LOCUS <ref> [36] </ref> and Clouds [20], Isis [12, 39, 11], FT-PVM [33], FT-Linda [5], and PLinda [2, 26]. However, all these systems 3 add significant overhead, even when there is no failure. More recently several prominent projects have similar goals to us.
Reference: [37] <author> M. Rabin. </author> <title> Fingerprinting by random polynomials. </title> <type> Technical report, </type> <institution> Harvard University, </institution> <year> 1981. </year>
Reference: [38] <author> M. Rabin. </author> <title> Efficient dispersal of information for security, load balancing and fault tolerance. </title> <journal> J. ACM, </journal> <volume> 36 </volume> <pages> 335-348, </pages> <year> 1989. </year>
Reference: [39] <author> A. Ricciardi and K. Birman. </author> <title> Using process groups to implement failure detection in asynchronous environments. </title> <booktitle> In ACM, editor, Proceding of Distributed Computing, </booktitle> <pages> pages 341-353. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: The issues of providing fault tolerance have generally been addressed separately from the issues of parallel processing. There have been three major mechanisms: checkpointing, replication, and process groups. Such approaches have been implemented in CIRCUS [15], LOCUS [36] and Clouds [20], Isis <ref> [12, 39, 11] </ref>, FT-PVM [33], FT-Linda [5], and PLinda [2, 26]. However, all these systems 3 add significant overhead, even when there is no failure. More recently several prominent projects have similar goals to us.
Reference: [40] <author> V.S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <year> 1990. </year> <month> 26 </month>
Reference-contexts: Such a platform has been perceived by some as an inadequate substitute for a "real" parallel machine. Nevertheless, it is known that many applications run quite well on networks of workstations, and this has contributed to the popularity of systems such as, Linda [1, 14], MPI [24], and PVM <ref> [40, 22] </ref>. Calypso is an integrated system that allows the programming and execution of computationally complex applications on a set of workstations on a network. It can utilize publicly used workstations, and takes care of dynamic availabilities, slowdown and failures prevalent in such configurations. <p> However, it does not implement a fault-tolerant manager, described in [19], which relied on dispersal and evasion. We now briefly summarize other related work. A large body of experimental results exist in the attempt to make parallel programs run on distributed hardware <ref> [22, 33, 40, 5, 14, 8, 10, 6, 7] </ref>. These systems can be loosely divided into two types, those that depend on a message passing scheme and those that use some form of global address spaces. <p> These systems can be loosely divided into two types, those that depend on a message passing scheme and those that use some form of global address spaces. Many systems provide message passing, or Remote Procedure Call facility built on top of a message passing. These include PVM <ref> [40, 22] </ref>, Orca [7], GLU [25], Isis and Horus [13, 11], Concert/C and so on. These systems provide a runtime library (and sometimes compiler support) to enable the writing of parallel programs as concurrently executable units.
References-found: 40

