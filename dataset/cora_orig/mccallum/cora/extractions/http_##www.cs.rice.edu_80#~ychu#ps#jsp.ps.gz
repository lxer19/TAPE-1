URL: http://www.cs.rice.edu:80/~ychu/ps/jsp.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~ychu/papers.html
Root-URL: 
Phone: 2  
Title: Implementing O(N) N-body Algorithms Efficiently in Data-Parallel Languages focus on managing the data distribution and
Author: YU HU AND S. LENNART JOHNSSON ;; 
Keyword: High Performance Fortran (HPF) and Connection Machine Fortran (CMF). The  
Note: here  Of the  has been measured. c fl1996 John Wiley Sons, Inc.  
Address: Cambridge, Massachusetts 02138  Houston, Houston, Texas 77204-3475  
Affiliation: 1 Aiken Computtaion Laboratory, Harvard University,  University of  
Abstract: We show how the techniques can be expressed in data-parallel languages, such as effectiveness of our techniques is demonstrated on an implementation of Anderson's hierarchical O(N ) N -body method for the Connection Machine system CM-5/5E.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Thinking Machines Corporation, </institution> <note> CM Fortran Reference Manual, Version 2.1, </note> <year> 1993. </year>
Reference-contexts: This article examines techniques for achieving high efficiency in implementing nonadaptive O (N ) N -body algorithms on Massively Parallel Processors (MPPs). It also provides Connection Machine Fortran (CMF) <ref> [1] </ref> code fragments that fl Work conducted when author was with Thinking Machines Corporation and Harvard University, Cambridge, Massachu-setts. Received October 1994 Revised December 1995 c fl1996 John Wiley & Sons, Inc. <p> its children's inner-sphere approximations (T 3 ) and converting the outer-sphere approximations of a box's interactive-field boxes (T 2 ) to add to the box's inner-sphere approximation. 3 HIGH PERFORMANCE FORTRAN Since no HPF compiler was available when this work was initiated, we used the Connection Machine Fortran (CMF) language <ref> [1] </ref> for our implementation. All CMF constructs used, except the array aliasing mechanism, are available in HPF. Below we briefly summarize the new features in HPF.
Reference: [2] <author> High Performance Fortran Forum, </author> <title> "High Performance Fortran language specification," </title> <journal> Sci. Prog., </journal> <volume> Vol. 2, </volume> <pages> pp. 1-170, </pages> <year> 1993. </year>
Reference-contexts: Received October 1994 Revised December 1995 c fl1996 John Wiley & Sons, Inc. Scientific Programming, Vol. 5, pp. 000-000 (1996) CCC 1058-9244/96/000000-00 illustrates how to express the techniques in a data-parallel language. CMF was chosen because no High Performance Fortran (HPF) <ref> [2] </ref> compiler was available at the time of this project. <p> Though each of the eight children of a parent requires 875 matrices, the siblings share many matrices. The interactive-field boxes of the eight siblings have offsets in the range [5 + i; 4 + i] fi [5 + j; 4 + j] fi [5 + k; 4 + k]n <ref> [2; 2] </ref> fi [2; 2] fi [2; 2]; i; j; k 2 f0; 1g, respectively. For illustration, see Figure 11. Each offset corresponds to a different translation matrix. <p> The interactive-field boxes of the eight siblings have offsets in the range [5 + i; 4 + i] fi [5 + j; 4 + j] fi [5 + k; 4 + k]n <ref> [2; 2] </ref> fi [2; 2] fi [2; 2]; i; j; k 2 f0; 1g, respectively. For illustration, see Figure 11. Each offset corresponds to a different translation matrix. <p> The interactive-field boxes of the eight siblings have offsets in the range [5 + i; 4 + i] fi [5 + j; 4 + j] fi [5 + k; 4 + k]n <ref> [2; 2] </ref> fi [2; 2] fi [2; 2]; i; j; k 2 f0; 1g, respectively. For illustration, see Figure 11. Each offset corresponds to a different translation matrix. <p> Each offset corresponds to a different translation matrix. The union of the interactive-fields of the eight siblings has 11 fi 11 fi 11 5 fi 5 fi 5 = 1206 boxes with 1206 offsets in the range [5; 5] fi [5; 5] fi [5; 5]n <ref> [2; 2] </ref> fi [2; 2] fi [2; 2]. For ease of indexing, we also generate the translation matrices for the 125 subdo-mains excluded from the interactive-field, or a total of 11fi11fi11 = 1331 matrices. <p> Each offset corresponds to a different translation matrix. The union of the interactive-fields of the eight siblings has 11 fi 11 fi 11 5 fi 5 fi 5 = 1206 boxes with 1206 offsets in the range [5; 5] fi [5; 5] fi [5; 5]n <ref> [2; 2] </ref> fi [2; 2] fi [2; 2]. For ease of indexing, we also generate the translation matrices for the 125 subdo-mains excluded from the interactive-field, or a total of 11fi11fi11 = 1331 matrices. <p> The union of the interactive-fields of the eight siblings has 11 fi 11 fi 11 5 fi 5 fi 5 = 1206 boxes with 1206 offsets in the range [5; 5] fi [5; 5] fi [5; 5]n <ref> [2; 2] </ref> fi [2; 2] fi [2; 2]. For ease of indexing, we also generate the translation matrices for the 125 subdo-mains excluded from the interactive-field, or a total of 11fi11fi11 = 1331 matrices.
Reference: [3] <author> J. M. Anderson and M. S. Lam, </author> <title> "Global optimizations for parallelism and locality on scalable parallel machines," </title> <booktitle> in Proc. ACM SIGPLAN '93 Conf. on Programming Languages Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year> <note> 26 HU AND JOHNSSON </note>
Reference-contexts: Efficient memory management is the most challenging issue in high performance computing. Techniques for the automatic determination of data distributions with balanced load and efficient communication have been the focus of parallel compiler research in the last several years (for example, see <ref> [3, 4, 5] </ref>). However, no general technique that balances load and generates efficient communication has emerged so far.
Reference: [4] <author> J. Li and M. Chen, </author> <title> "Generating explicit communication from shared-memory program references," </title> <booktitle> In Proc. Supercomputing '90, </booktitle> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: Efficient memory management is the most challenging issue in high performance computing. Techniques for the automatic determination of data distributions with balanced load and efficient communication have been the focus of parallel compiler research in the last several years (for example, see <ref> [3, 4, 5] </ref>). However, no general technique that balances load and generates efficient communication has emerged so far.
Reference: [5] <author> M. E. Wolf and M. S. Lam, </author> <title> "A data locality optimizing algorithm," </title> <booktitle> in Proc. ACM SIGPLAN '91 Conf. on Programming Languages Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Efficient memory management is the most challenging issue in high performance computing. Techniques for the automatic determination of data distributions with balanced load and efficient communication have been the focus of parallel compiler research in the last several years (for example, see <ref> [3, 4, 5] </ref>). However, no general technique that balances load and generates efficient communication has emerged so far. <p> For illustration, see Figure 11. Each offset corresponds to a different translation matrix. The union of the interactive-fields of the eight siblings has 11 fi 11 fi 11 5 fi 5 fi 5 = 1206 boxes with 1206 offsets in the range <ref> [5; 5] </ref> fi [5; 5] fi [5; 5]n [2; 2] fi [2; 2] fi [2; 2]. For ease of indexing, we also generate the translation matrices for the 125 subdo-mains excluded from the interactive-field, or a total of 11fi11fi11 = 1331 matrices. <p> For illustration, see Figure 11. Each offset corresponds to a different translation matrix. The union of the interactive-fields of the eight siblings has 11 fi 11 fi 11 5 fi 5 fi 5 = 1206 boxes with 1206 offsets in the range <ref> [5; 5] </ref> fi [5; 5] fi [5; 5]n [2; 2] fi [2; 2] fi [2; 2]. For ease of indexing, we also generate the translation matrices for the 125 subdo-mains excluded from the interactive-field, or a total of 11fi11fi11 = 1331 matrices. <p> For illustration, see Figure 11. Each offset corresponds to a different translation matrix. The union of the interactive-fields of the eight siblings has 11 fi 11 fi 11 5 fi 5 fi 5 = 1206 boxes with 1206 offsets in the range <ref> [5; 5] </ref> fi [5; 5] fi [5; 5]n [2; 2] fi [2; 2] fi [2; 2]. For ease of indexing, we also generate the translation matrices for the 125 subdo-mains excluded from the interactive-field, or a total of 11fi11fi11 = 1331 matrices.
Reference: [6] <institution> Thinking Machines Corporation, </institution> <type> CM-5 Technical Summary, </type> <year> 1991. </year>
Reference-contexts: This issue is of particular importance in gathering boxes for the interactive-field computations. In Section 6 we show how to use array sectioning and array aliasing to implement an effective gathering of nonlocal interactive-field boxes. On the Connection Machine systems CM-5/5E <ref> [6] </ref>, the communication time for the straightforward use of CMF for interactive-field computations is more than one order of magnitude higher than the communication time for the technique we use. In Anderson's version of the fast multipole method [7], all translation operators can be represented by matrices acting on vectors.
Reference: [7] <author> C. R. Anderson, </author> <title> "An implementation of the fast multipole method without multipoles," </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> Vol. 13 no. 4, </volume> <pages> pp. 923-947, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: On the Connection Machine systems CM-5/5E [6], the communication time for the straightforward use of CMF for interactive-field computations is more than one order of magnitude higher than the communication time for the technique we use. In Anderson's version of the fast multipole method <ref> [7] </ref>, all translation operators can be represented by matrices acting on vectors. Moreover, the translation matrices are scale invariant and only depend upon the relative locations of the sources and destinations. <p> Section 10 reports some performance results of our implementation. Additional performance data can be found in [28]. Section 11 discusses load-balancing issues and Section 12 summarizes the paper. 2 HIERARCHICAL N -BODY METHODS Hierarchical methods <ref> [7, 29, 8, 9] </ref> for the N -body problem exploit the linearity of the potential field by partitioning the field into two parts, total = nearfield + farfield ; (1) where nearfield is the potential due to nearby particles and farfield is the potential due to faraway particles. <p> The accuracy is controlled by the number of terms included in the expansion. Estimates of the approximation errors as a function of the number of terms in the expansion have O (N ) N -BODY ALGORITHMS 5 been derived for the different methods <ref> [7, 9, 10] </ref>, and are not discussed here. Hierarchical methods compute farfield of (1) in two hierarchy-traversing passes. <p> method up to about 4,500 particles, the Barnes-Hut algorithm up to about 6,000 particles, and the Greengard-Rokhlin method for up to about 9,000 particles in three dimensions and with an accuracy of an error decay rate of four in the multipole methods. 2.1 Anderson's Multipole Method Without Mul tipoles Anderson <ref> [7] </ref> used Poisson's formula to represent solutions of Laplace equation. Let g (x; y; z) denote potential values on a sphere of radius a and denote by the harmonic function external to the sphere with these boundary values. <p> The potential value at ~x is (equation (14) of <ref> [7] </ref>) (~x) = 4 S 2 n=0 a ) n+1 P n (~s i ~x p ) g (a~s)ds; (2) where the integration is carried out over S 2 , the surface of the unit sphere, and P n is the nth Legendre function. <p> Given a numerical formula for integrating functions on the surface of the sphere with K integration points ~s i and weights w i , the following formula (equation (15) of <ref> [7] </ref>) is used to approximate the potential at ~x: K X " M X (2n + 1)( r # (3) This approximation is called an outer-sphere approximation. Note that in this approximation the series is truncated and the integral is evaluated with a finite number of terms. <p> Note that in this approximation the series is truncated and the integral is evaluated with a finite number of terms. The approximation used to represent potentials inside a given region of radius a is (equation (16) of <ref> [7] </ref>) K X " M X (2n + 1)( a # (4) and is called an inner-sphere approximation. The outer-sphere and the inner-sphere approximations define the computational elements in Anderson's hierarchical method. Outer-sphere approximations are first constructed for clusters of particles in leaf-level boxes.
Reference: [8] <author> L. Greengard and V. Rokhlin, </author> <title> "A fast algorithm for particle simulations," </title> <journal> J. of Computational Physics, </journal> <volume> Vol. 73, </volume> <pages> pp. 325-348, </pages> <year> 1987. </year>
Reference-contexts: Hence, the matrices are the same for all levels of the hierarchy, and all source destination pairs with the same relative locations use the same matrices at any level in the hierarchy. Thus, all matrices can be pre-computed. The translation operators in Greengard-Rokhlin's <ref> [8, 9] </ref> and Zhao's [10] fast multipole methods can also be viewed as matrix-vector multiplications [11]. We discuss this arithmetic optimization in Section 7. <p> Section 10 reports some performance results of our implementation. Additional performance data can be found in [28]. Section 11 discusses load-balancing issues and Section 12 summarizes the paper. 2 HIERARCHICAL N -BODY METHODS Hierarchical methods <ref> [7, 29, 8, 9] </ref> for the N -body problem exploit the linearity of the potential field by partitioning the field into two parts, total = nearfield + farfield ; (1) where nearfield is the potential due to nearby particles and farfield is the potential due to faraway particles.
Reference: [9] <author> L. Greengard and W. D. Gropp, </author> <title> "A parallel version of the fast multipole method," </title> <booktitle> in Parallel Processing for Scientific Computing, </booktitle> <pages> pp. 213-222, </pages> <publisher> SIAM, </publisher> <year> 1989. </year>
Reference-contexts: Hence, the matrices are the same for all levels of the hierarchy, and all source destination pairs with the same relative locations use the same matrices at any level in the hierarchy. Thus, all matrices can be pre-computed. The translation operators in Greengard-Rokhlin's <ref> [8, 9] </ref> and Zhao's [10] fast multipole methods can also be viewed as matrix-vector multiplications [11]. We discuss this arithmetic optimization in Section 7. <p> Section 10 reports some performance results of our implementation. Additional performance data can be found in [28]. Section 11 discusses load-balancing issues and Section 12 summarizes the paper. 2 HIERARCHICAL N -BODY METHODS Hierarchical methods <ref> [7, 29, 8, 9] </ref> for the N -body problem exploit the linearity of the potential field by partitioning the field into two parts, total = nearfield + farfield ; (1) where nearfield is the potential due to nearby particles and farfield is the potential due to faraway particles. <p> The accuracy is controlled by the number of terms included in the expansion. Estimates of the approximation errors as a function of the number of terms in the expansion have O (N ) N -BODY ALGORITHMS 5 been derived for the different methods <ref> [7, 9, 10] </ref>, and are not discussed here. Hierarchical methods compute farfield of (1) in two hierarchy-traversing passes.
Reference: [10] <author> F. Zhao, </author> <title> "An O(N) algorithm for three-dimensional N-body simulations," </title> <type> AI Memo 995, </type> <institution> AI Lab, MIT, </institution> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: Hence, the matrices are the same for all levels of the hierarchy, and all source destination pairs with the same relative locations use the same matrices at any level in the hierarchy. Thus, all matrices can be pre-computed. The translation operators in Greengard-Rokhlin's [8, 9] and Zhao's <ref> [10] </ref> fast multipole methods can also be viewed as matrix-vector multiplications [11]. We discuss this arithmetic optimization in Section 7. <p> The accuracy is controlled by the number of terms included in the expansion. Estimates of the approximation errors as a function of the number of terms in the expansion have O (N ) N -BODY ALGORITHMS 5 been derived for the different methods <ref> [7, 9, 10] </ref>, and are not discussed here. Hierarchical methods compute farfield of (1) in two hierarchy-traversing passes. <p> In our implementation of interactive-field computations (which does not exploit the parallelism among the boxes in the interactive-field), each target box needs to fetch the potential vectors of its 875 neighbor boxes (if supern-odes <ref> [10] </ref> are not used). 6.3.1 Interactive-Field Box-Box Communi cation In CMF, the simplest way to express the fetching of neighbor potential vectors for a target box uses individual CSHIFTs, one for each neighbor, as shown in System, composite CSHIFTs are implemented as a sequence of independent shifts, one for each axis. <p> A more detailed analysis of the effectiveness of the techniques is given in [28]. In considering the execution times it should be mentioned that our implementation uses the idea of supernodes. Zhao <ref> [10] </ref> made the observation that of the 875 boxes in the interactive-field, in many cases all eight siblings of a parent are included in the interactive-field.
Reference: [11] <author> K. Nabors and J. White, "Fastcap: </author> <title> A multipole accelerated 3-d capacitance extraction program," </title> <type> Tech. Rep., </type> <institution> MIT Dept. of Electrical Engineering and Computer Science, </institution> <year> 1991. </year>
Reference-contexts: Thus, all matrices can be pre-computed. The translation operators in Greengard-Rokhlin's [8, 9] and Zhao's [10] fast multipole methods can also be viewed as matrix-vector multiplications <ref> [11] </ref>. We discuss this arithmetic optimization in Section 7.
Reference: [12] <institution> Thinking Machines Corporation, CMSSL for CM Fortran, </institution> <note> Version 3.1, </note> <year> 1993. </year>
Reference-contexts: translation operations as matrix-vector multiplications and aggregating the matrix-vector multiplications into multiple-instance matrix-matrix multiplications, many of the translation operations can be performed at an efficiency of about 80% of peak, or at a rate of 127 Mflop/s per node of a CM-5E using the Connection Machine Scientific Software Library, CMSSL <ref> [12] </ref>. Recognizing and aggregating BLAS operations and using library functions can significantly improve the performance O (N ) N -BODY ALGORITHMS 3 of the computations on most architectures, including uniprocessor architectures. <p> Multiple-instance BLAS forms a part of the Connection Machine Scientific Software Library <ref> [12] </ref>, CMSSL. For K = 12 and a depth eight hierarchy on a 256 node CM-5/5E, the use of CMSSL results in an efficiency of 54% and 74% (87 and 119 Mflops/s per node) for the translation operations T 1 (T 3 ) and T 2 at the leaf-level, respectively.
Reference: [13] <author> R. G. Brickner, </author> <type> personal communication. </type>
Reference-contexts: The array aliasing feature has the clear advantage over extrinsic procedures that a programmer can express the optimizations in the data-parallel programming model instead of resorting to the message-passing SPMD (Single Program, Multiple Data) style programming model. An array aliasing mechanism is being considered for inclusion in HPF II <ref> [13] </ref>. Most of our optimization techniques apply to any distributed memory machine. However, the relative merit of the techniques depend upon machine metrics. We report on the performance trade-offs on the CM-5/5E.
Reference: [14] <author> J. K. Salmon, </author> <title> "Parallel hierarchical n-body methods," </title> <type> Tech. Rep. </type> <institution> CRPC-90-14, California Institute of Technology, </institution> <year> 1990. </year>
Reference-contexts: The efficiency numbers should be viewed with some caution since the various implementations used different algorithms, different problem sizes and parameters controlling the accuracies. The Barnes-Hut O (N log N ) algorithm has been implemented using the message passing programming paradigm by Salmon and Warren <ref> [14, 15, 16] </ref> on the Intel Touchstone Delta and by Liu and Bhatt [17, 18] on the CM-5. Both groups used assembly language for time critical kernels and achieved efficiencies in the range 24% - 28% and 30%, respectively. <p> Efficiencies of various parallel implementations of hierarchical N -body methods % of Peak Author Programming model efficiency Machine Salmon, Warren-Salmon <ref> [14, 15, 16] </ref> F77+ message passing 24 - 28% 512 node Intel Delta Liu-Bhatt [17, 18] C+message passing+assembly 30% 256 node CM-5 Leathrum-Board [20, 21] F77 20% 32 node KSR-1 Elliott-Board [22] F77 14% 32 node KSR-1 Zhao-Johnsson [19] *Lisp+assembly 12% 256 node (8k) CM-2 Hu-Johnsson [this article] CMF 27 -
Reference: [15] <author> M. Warren and J. Salmon, </author> <title> "Astrophysical N-body simulations using hierarchical tree data structures," </title> <booktitle> in Proc. Supercomputing '92, </booktitle> <address> Min-neapolis, MN, </address> <year> 1992. </year>
Reference-contexts: The efficiency numbers should be viewed with some caution since the various implementations used different algorithms, different problem sizes and parameters controlling the accuracies. The Barnes-Hut O (N log N ) algorithm has been implemented using the message passing programming paradigm by Salmon and Warren <ref> [14, 15, 16] </ref> on the Intel Touchstone Delta and by Liu and Bhatt [17, 18] on the CM-5. Both groups used assembly language for time critical kernels and achieved efficiencies in the range 24% - 28% and 30%, respectively. <p> Efficiencies of various parallel implementations of hierarchical N -body methods % of Peak Author Programming model efficiency Machine Salmon, Warren-Salmon <ref> [14, 15, 16] </ref> F77+ message passing 24 - 28% 512 node Intel Delta Liu-Bhatt [17, 18] C+message passing+assembly 30% 256 node CM-5 Leathrum-Board [20, 21] F77 20% 32 node KSR-1 Elliott-Board [22] F77 14% 32 node KSR-1 Zhao-Johnsson [19] *Lisp+assembly 12% 256 node (8k) CM-2 Hu-Johnsson [this article] CMF 27 -
Reference: [16] <author> M. Warren and J. Salmon, </author> <title> "A parallel hashed oct-tree N-body algorithm," </title> <booktitle> in Proc. Supercomputing '93, </booktitle> <address> Portland, OR, </address> <year> 1993. </year>
Reference-contexts: The efficiency numbers should be viewed with some caution since the various implementations used different algorithms, different problem sizes and parameters controlling the accuracies. The Barnes-Hut O (N log N ) algorithm has been implemented using the message passing programming paradigm by Salmon and Warren <ref> [14, 15, 16] </ref> on the Intel Touchstone Delta and by Liu and Bhatt [17, 18] on the CM-5. Both groups used assembly language for time critical kernels and achieved efficiencies in the range 24% - 28% and 30%, respectively. <p> Efficiencies of various parallel implementations of hierarchical N -body methods % of Peak Author Programming model efficiency Machine Salmon, Warren-Salmon <ref> [14, 15, 16] </ref> F77+ message passing 24 - 28% 512 node Intel Delta Liu-Bhatt [17, 18] C+message passing+assembly 30% 256 node CM-5 Leathrum-Board [20, 21] F77 20% 32 node KSR-1 Elliott-Board [22] F77 14% 32 node KSR-1 Zhao-Johnsson [19] *Lisp+assembly 12% 256 node (8k) CM-2 Hu-Johnsson [this article] CMF 27 -
Reference: [17] <author> P. Liu and S. N. Bhatt, </author> <title> "Experiences with parallel N-body simulation," </title> <booktitle> in Proc. 6th Annual ACM Symposium on Parallel Algorithms and Architecture, </booktitle> <address> Cape May, NJ, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The Barnes-Hut O (N log N ) algorithm has been implemented using the message passing programming paradigm by Salmon and Warren [14, 15, 16] on the Intel Touchstone Delta and by Liu and Bhatt <ref> [17, 18] </ref> on the CM-5. Both groups used assembly language for time critical kernels and achieved efficiencies in the range 24% - 28% and 30%, respectively. <p> Efficiencies of various parallel implementations of hierarchical N -body methods % of Peak Author Programming model efficiency Machine Salmon, Warren-Salmon [14, 15, 16] F77+ message passing 24 - 28% 512 node Intel Delta Liu-Bhatt <ref> [17, 18] </ref> C+message passing+assembly 30% 256 node CM-5 Leathrum-Board [20, 21] F77 20% 32 node KSR-1 Elliott-Board [22] F77 14% 32 node KSR-1 Zhao-Johnsson [19] *Lisp+assembly 12% 256 node (8k) CM-2 Hu-Johnsson [this article] CMF 27 - 35% 256 node CM-5/5E The hierarchy of computational elements is established through a hierarchy
Reference: [18] <author> P. Liu, </author> <title> "The parallel implementation of N-body algorithms," </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1994. </year>
Reference-contexts: The Barnes-Hut O (N log N ) algorithm has been implemented using the message passing programming paradigm by Salmon and Warren [14, 15, 16] on the Intel Touchstone Delta and by Liu and Bhatt <ref> [17, 18] </ref> on the CM-5. Both groups used assembly language for time critical kernels and achieved efficiencies in the range 24% - 28% and 30%, respectively. <p> Efficiencies of various parallel implementations of hierarchical N -body methods % of Peak Author Programming model efficiency Machine Salmon, Warren-Salmon [14, 15, 16] F77+ message passing 24 - 28% 512 node Intel Delta Liu-Bhatt <ref> [17, 18] </ref> C+message passing+assembly 30% 256 node CM-5 Leathrum-Board [20, 21] F77 20% 32 node KSR-1 Elliott-Board [22] F77 14% 32 node KSR-1 Zhao-Johnsson [19] *Lisp+assembly 12% 256 node (8k) CM-2 Hu-Johnsson [this article] CMF 27 - 35% 256 node CM-5/5E The hierarchy of computational elements is established through a hierarchy
Reference: [19] <author> F. Zhao and S. L. Johnsson, </author> <title> "The parallel multi-pole method on the Connection Machine," </title> <journal> SIAM J. Stat. Sci. Comp., </journal> <volume> Vol. 12, no. 6, </volume> <pages> pp. 1420-1437, </pages> <year> 1991. </year>
Reference-contexts: Both groups used assembly language for time critical kernels and achieved efficiencies in the range 24% - 28% and 30%, respectively. Zhao and Johns-son <ref> [19] </ref> developed a data-parallel implementation of Zhao's method on the CM-2, and achieved an efficiency of 12% for expansions in Cartesian coordinates that result in more costly multipole expansion calculations. <p> -body methods % of Peak Author Programming model efficiency Machine Salmon, Warren-Salmon [14, 15, 16] F77+ message passing 24 - 28% 512 node Intel Delta Liu-Bhatt [17, 18] C+message passing+assembly 30% 256 node CM-5 Leathrum-Board [20, 21] F77 20% 32 node KSR-1 Elliott-Board [22] F77 14% 32 node KSR-1 Zhao-Johnsson <ref> [19] </ref> *Lisp+assembly 12% 256 node (8k) CM-2 Hu-Johnsson [this article] CMF 27 - 35% 256 node CM-5/5E The hierarchy of computational elements is established through a hierarchy of grids (see Figure 1). Grid level 0 represents the entire domain.
Reference: [20] <author> J. F. Leathrum, </author> <title> "The parallel fast multipole method in three dimensions," </title> <type> PhD thesis, </type> <institution> Duke University, </institution> <year> 1992. </year>
Reference-contexts: Zhao and Johns-son [19] developed a data-parallel implementation of Zhao's method on the CM-2, and achieved an efficiency of 12% for expansions in Cartesian coordinates that result in more costly multipole expansion calculations. Leathrum and Board <ref> [20, 21] </ref> and Elliott and Board [22] achieved efficiencies in the range 14% - 20% in implementing Greengard-Rokhlin's method (see [23]) on the KSR-1. Schmidt and Lee [24] vec-torized this method for the Cray Y-MP and achieved an efficiency of 39% on a single processor. <p> Efficiencies of various parallel implementations of hierarchical N -body methods % of Peak Author Programming model efficiency Machine Salmon, Warren-Salmon [14, 15, 16] F77+ message passing 24 - 28% 512 node Intel Delta Liu-Bhatt [17, 18] C+message passing+assembly 30% 256 node CM-5 Leathrum-Board <ref> [20, 21] </ref> F77 20% 32 node KSR-1 Elliott-Board [22] F77 14% 32 node KSR-1 Zhao-Johnsson [19] *Lisp+assembly 12% 256 node (8k) CM-2 Hu-Johnsson [this article] CMF 27 - 35% 256 node CM-5/5E The hierarchy of computational elements is established through a hierarchy of grids (see Figure 1).
Reference: [21] <author> J. A. Board Jr., Z. S. Hakura, W. D. Elliott, D. C. Gray, W. J. Blanke, and J.F. Leathrum Jr., </author> <title> "Scalable implementations of multipole-accelerated algorithms for molecular dynamics," </title> <booktitle> in Proc. Scalable High Performance Computing Conf. </booktitle> <address> SHPCC94, </address> <year> 1994. </year>
Reference-contexts: Zhao and Johns-son [19] developed a data-parallel implementation of Zhao's method on the CM-2, and achieved an efficiency of 12% for expansions in Cartesian coordinates that result in more costly multipole expansion calculations. Leathrum and Board <ref> [20, 21] </ref> and Elliott and Board [22] achieved efficiencies in the range 14% - 20% in implementing Greengard-Rokhlin's method (see [23]) on the KSR-1. Schmidt and Lee [24] vec-torized this method for the Cray Y-MP and achieved an efficiency of 39% on a single processor. <p> Efficiencies of various parallel implementations of hierarchical N -body methods % of Peak Author Programming model efficiency Machine Salmon, Warren-Salmon [14, 15, 16] F77+ message passing 24 - 28% 512 node Intel Delta Liu-Bhatt [17, 18] C+message passing+assembly 30% 256 node CM-5 Leathrum-Board <ref> [20, 21] </ref> F77 20% 32 node KSR-1 Elliott-Board [22] F77 14% 32 node KSR-1 Zhao-Johnsson [19] *Lisp+assembly 12% 256 node (8k) CM-2 Hu-Johnsson [this article] CMF 27 - 35% 256 node CM-5/5E The hierarchy of computational elements is established through a hierarchy of grids (see Figure 1).
Reference: [22] <author> W. D. Elliott and J. A. </author> <title> Board, "Fast Fourier transform accelerated fast multipole algorithm," </title> <type> Tech. Rep. 94-001, </type> <institution> Dept. of Electrical Engineering, Duke University, </institution> <year> 1994. </year>
Reference-contexts: Zhao and Johns-son [19] developed a data-parallel implementation of Zhao's method on the CM-2, and achieved an efficiency of 12% for expansions in Cartesian coordinates that result in more costly multipole expansion calculations. Leathrum and Board [20, 21] and Elliott and Board <ref> [22] </ref> achieved efficiencies in the range 14% - 20% in implementing Greengard-Rokhlin's method (see [23]) on the KSR-1. Schmidt and Lee [24] vec-torized this method for the Cray Y-MP and achieved an efficiency of 39% on a single processor. <p> of various parallel implementations of hierarchical N -body methods % of Peak Author Programming model efficiency Machine Salmon, Warren-Salmon [14, 15, 16] F77+ message passing 24 - 28% 512 node Intel Delta Liu-Bhatt [17, 18] C+message passing+assembly 30% 256 node CM-5 Leathrum-Board [20, 21] F77 20% 32 node KSR-1 Elliott-Board <ref> [22] </ref> F77 14% 32 node KSR-1 Zhao-Johnsson [19] *Lisp+assembly 12% 256 node (8k) CM-2 Hu-Johnsson [this article] CMF 27 - 35% 256 node CM-5/5E The hierarchy of computational elements is established through a hierarchy of grids (see Figure 1). Grid level 0 represents the entire domain.
Reference: [23] <author> L. Greengard and V. Rokhlin, </author> <title> "On the efficient implementation of the fast multipole method," </title> <type> Tech. Rep. </type> <institution> YALEU/DCS/RR-602, Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: Leathrum and Board [20, 21] and Elliott and Board [22] achieved efficiencies in the range 14% - 20% in implementing Greengard-Rokhlin's method (see <ref> [23] </ref>) on the KSR-1. Schmidt and Lee [24] vec-torized this method for the Cray Y-MP and achieved an efficiency of 39% on a single processor.
Reference: [24] <author> K. E. Schmidt and M. A. Lee, </author> <title> "Implementing the fast multipole method in three dimensions," </title> <journal> J. Stat. Phy., </journal> <volume> Vol. 63, no. 5/6, </volume> <pages> pp. 1223-1235, </pages> <year> 1991. </year>
Reference-contexts: Leathrum and Board [20, 21] and Elliott and Board [22] achieved efficiencies in the range 14% - 20% in implementing Greengard-Rokhlin's method (see [23]) on the KSR-1. Schmidt and Lee <ref> [24] </ref> vec-torized this method for the Cray Y-MP and achieved an efficiency of 39% on a single processor. Singh et al. [25, 26] have implemented both O (N log N ) and O (N ) methods on the Stanford DASH machine, but no measures of the achieved efficiency is available.
Reference: [25] <author> J. P. Singh, C. Holt, T. Ttsuka, A. Gupta, and J. L. Hennessey, </author> <title> "Load balancing and data locality in hierarchical N-body methods," </title> <type> Tech. Rep. </type> <institution> CSL-TR-92-505, Stanford University, </institution> <year> 1992. </year>
Reference-contexts: Schmidt and Lee [24] vec-torized this method for the Cray Y-MP and achieved an efficiency of 39% on a single processor. Singh et al. <ref> [25, 26] </ref> have implemented both O (N log N ) and O (N ) methods on the Stanford DASH machine, but no measures of the achieved efficiency is available.
Reference: [26] <author> J. P. Singh, C. Holt, J. L. Hennessey, and A. Gupta, </author> <title> "A parallel adaptive fast multipole method," </title> <booktitle> in Proc. Supercomputing '93, </booktitle> <pages> pp. 54-65. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: Schmidt and Lee [24] vec-torized this method for the Cray Y-MP and achieved an efficiency of 39% on a single processor. Singh et al. <ref> [25, 26] </ref> have implemented both O (N log N ) and O (N ) methods on the Stanford DASH machine, but no measures of the achieved efficiency is available.
Reference: [27] <author> L. S. Nyland, J. F. Prins, and J. H. Reif, </author> <title> "A data-parallel implementation of the adaptive fast multipole algorithm," </title> <booktitle> in Proc. DAGS '93 Symposium, Dartmouth Institute for Advanced Graduated Studies in Parallel Computation, </booktitle> <address> Hanover, NH, </address> <year> 1993. </year>
Reference-contexts: Singh et al. [25, 26] have implemented both O (N log N ) and O (N ) methods on the Stanford DASH machine, but no measures of the achieved efficiency is available. Nyland et al. <ref> [27] </ref> discussed how to express the three-dimensional adaptive version of the Greengard-Rokhlin method in a data-parallel subset of the Pro-teus language, which is still under implementation. This paper is organized as follows: Section 2 describes the computational structure of hierarchical methods, and details the computational elements of Anderson's method.
Reference: [28] <author> Y. Hu and S. L. Johnsson, </author> <title> "A data parallel implementation of hierarchical N -body methods," </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> Vol. 10, no. 1, </volume> <pages> pp. 3-40, </pages> <year> 1996. </year> <title> O(N ) N -BODY ALGORITHMS 27 </title>
Reference-contexts: Section 4 presents the architecture of the Connection Machine systems CM-5/5E. The optimization techniques for programming hierarchical methods in CMF (HPF) are presented in Sections 5 - 9. Section 10 reports some performance results of our implementation. Additional performance data can be found in <ref> [28] </ref>. <p> We first give a summary of the timings breakdown in computing the potential field for 100 million uniformly distributed particles on a 256 node CM-5E, then demonstrate the scalability of the implementation. A more detailed analysis of the effectiveness of the techniques is given in <ref> [28] </ref>. In considering the execution times it should be mentioned that our implementation uses the idea of supernodes. Zhao [10] made the observation that of the 875 boxes in the interactive-field, in many cases all eight siblings of a parent are included in the interactive-field. <p> By converting the far-field of the parent box instead of the far-fields of all eight siblings the number of far-field to local-field conversions are reduced to 189 from 875. The supernode idea must be modified somewhat for Anderson's method, but the same reduction in computational complexity can be achieved <ref> [28] </ref>. For gravitational and Coulombic fields division and square roots represent a significant fraction of the arithmetic time. We report floating-point rates for three different weights of these operations as specified in Table 5.
Reference: [29] <author> J. Barnes and P. Hut, </author> <title> "A hierarchical O(N log N ) force calculation algorithm," </title> <journal> Nature, </journal> <volume> Vol. 324, </volume> <pages> pp. 446-449, </pages> <year> 1986. </year>
Reference-contexts: Section 10 reports some performance results of our implementation. Additional performance data can be found in [28]. Section 11 discusses load-balancing issues and Section 12 summarizes the paper. 2 HIERARCHICAL N -BODY METHODS Hierarchical methods <ref> [7, 29, 8, 9] </ref> for the N -body problem exploit the linearity of the potential field by partitioning the field into two parts, total = nearfield + farfield ; (1) where nearfield is the potential due to nearby particles and farfield is the potential due to faraway particles.
Reference: [30] <author> L. Greengard and V. Rokhlin, </author> <title> "Rapid evaluation of potential fields in three dimensions," </title> <type> Tech. Rep. </type> <institution> YALEU/DCS/RR-515, Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: Subdomains that are not further decomposed are leaves. In two dimensions, the near-field contains those subdomains that share a boundary point with the considered subdomain. In three dimensions, Greengard-Rokhlin's formulation <ref> [30] </ref> defines the near-field to contain nearest neighbor subdo-mains which share a boundary point with the considered subdomain and second nearest neighbor subdo-mains which share a boundary point with the nearest neighbor subdomains.
Reference: [31] <author> Y. Hu and S. L. Johnsson, </author> <title> "On the error in Anderson's fast N-body algorithm," </title> <type> Tech. Rep., </type> <institution> Harvard University, Division of Applied Sciences, </institution> <year> 1995. </year>
Reference-contexts: In two dimensions, the near-field contains eight subdomains and the interactive-field contains 27 subdomains, respectively. A discussion of a less stringent definition of the near-field in three dimensions can be found in <ref> [31] </ref>. The idea of the hierarchical combining and evaluation used in the O (N ) algorithms is illustrated in subdomains marked with 'i' are well separated from subdomain B. Thus, the computational elements for those two subdomains can be evaluated at the particles in subdomain B.
Reference: [32] <author> J. Katzenelson, </author> <title> "Computational structure of the N-body problem," </title> <journal> SIAM J. Sci. Statist. Com-put., </journal> <volume> Vol. 4, </volume> <pages> pp. 787-815, </pages> <year> 1989. </year>
Reference-contexts: Let l i represent the contribution to the potential field in subdomain i at level l due to particles in the far-field of subdomain i, i.e., the local-field potential in subdo-main i at level l. Then, the computational structure is described in the recursive formulation by Katzenel son <ref> [32] </ref>: Algorithm: (A generic hierarchical method) 1. Compute h i for all boxes i at the leaf-level h. 2. Upward-pass: for l = h 1; h 2; :::; 2, compute l X i2fchildren (n)g T 1 ( l+1 3.
Reference: [33] <author> J. H. Applegate, M. R. Douglas, Y. Gursel, P. Hunter, C. L. Seitz, and G. J. Sussman, </author> <title> "A digital orrery," </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-34, no. 9, </volume> <pages> pp. 822-831, </pages> <month> Sept. </month> <year> 1985. </year>
Reference-contexts: Finally, the two contributions to box 0 will be combined with interactions among particles in box 0. Exploiting symmetry saves almost a factor of two in both communication and computation. The idea of exploiting symmetry is similar to the idea used for the linear or-rery by Applegate et al. <ref> [33] </ref>. Here, a linear ordering is imposed on the 62 neighbor boxes in 3-D, which contain partially ordered particles. 6.3 Box-Box Interactions Excessive data movement can easily happen in programs written in data-parallel languages, such as HPF, which provide a global address space.
Reference: [34] <author> J. J. Dongarra, J. D. Croz, I. Duff, and S. Ham-marling, </author> <title> "A Set of Level 3 Basic Linear Algebra Subprograms," </title> <type> Tech. Rep. </type> <institution> Reprint No. 1, Ar-gonne National Laboratories, Mathematics and Computer Science Division, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: Since there is no other computation in the hierarchy, the entire hierarchical part takes the form of a collection of matrix-matrix multiplications, which is implemented efficiently on most computers as part of the BLAS (Basic Linear Algebra Subroutines) <ref> [34, 35, 36] </ref>. Multiple-instance BLAS forms a part of the Connection Machine Scientific Software Library [12], CMSSL.
Reference: [35] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and R. J. Hanson, </author> <title> "An Extended Set of Fortran Basic Linear Algebra Subprograms," </title> <type> Tech. Rep. Technical Memorandum 41, </type> <institution> Argonne National Laboratories, Mathematics and Computer Science Division, </institution> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: Since there is no other computation in the hierarchy, the entire hierarchical part takes the form of a collection of matrix-matrix multiplications, which is implemented efficiently on most computers as part of the BLAS (Basic Linear Algebra Subroutines) <ref> [34, 35, 36] </ref>. Multiple-instance BLAS forms a part of the Connection Machine Scientific Software Library [12], CMSSL.
Reference: [36] <author> C.L. Lawson, R.J. Hanson, D.R. Kincaid, and F.T. Krogh, </author> <title> "Basic Linear Algebra Subprograms for Fortran Usage," </title> <journal> ACM TOMS, </journal> <volume> Vol. 5 no. 3, </volume> <pages> pp. 308-323, </pages> <month> Sept. </month> <year> 1979. </year>
Reference-contexts: Since there is no other computation in the hierarchy, the entire hierarchical part takes the form of a collection of matrix-matrix multiplications, which is implemented efficiently on most computers as part of the BLAS (Basic Linear Algebra Subroutines) <ref> [34, 35, 36] </ref>. Multiple-instance BLAS forms a part of the Connection Machine Scientific Software Library [12], CMSSL.
Reference: [37] <author> S. L. Johnsson and C.-T. Ho, </author> <title> "Spanning graphs for optimum broadcasting and personalized communication in hypercubes," </title> <journal> IEEE Trans. Computers, </journal> <volume> Vol. 38, no. 9, </volume> <pages> pp. 1249-1268, </pages> <month> Sept. </month> <year> 1989. </year>
Reference-contexts: Each group compute the entire collection of matrices, followed by spreads within groups when a matrix is needed. The replication may also be performed as an all-to-all broadcast <ref> [37] </ref>. The load-balance with this amount of redundant computation is the same as with no redundancy, but the communication cost may be reduced.
Reference: [38] <author> J. L. Hennessy and D. A. Patterson, </author> <title> "Computer Architecture: A Quantative Approach," </title> <publisher> Mor-gan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: The timings breakdown for the potential field calculation of 100 million particles on a 256 node CM-5E Table 5. Weights for floating-point operations in our three methods for FLOP counts Method FLOP count I Native always 1 II Hennessy & Patterson <ref> [38] </ref> ADD,SUB,MULT - 1 DIV,SQRT - 4 III CM-5E/VU normalized ADD,SUB,MULT - 1 DIV - 5 SQRT - 8 is shown in Table 6 for K = 12 and K = 72. The hierarchy depths are 8 and 7, respectively.
References-found: 38

