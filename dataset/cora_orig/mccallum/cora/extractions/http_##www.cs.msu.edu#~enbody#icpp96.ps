URL: http://www.cs.msu.edu/~enbody/icpp96.ps
Refering-URL: http://www.cs.msu.edu/~enbody/
Root-URL: http://www.cs.msu.edu
Email: crs@msu.edu, enbody@cps.msu.edu  
Title: AUTOMATIC SELF-ALLOCATING THREADS (ASAT) ON AN SGI CHALLENGE  
Author: Charles Severance and Richard Enbody 
Address: East Lansing, MI 48824-1027  
Affiliation: Department of Computer Science Michigan State University  
Abstract: Automatic Self Allocating Threads (ASAT) is proposed as a way to balance the number of active threads across a shared-memory multiprocessing system. Our approach is significant in that it is designed for a system running multiple jobs, and it considers the load of all running jobs in its thread allocation. In addition, the overhead of ASAT is sufficiently small so that the run times of all jobs improve when it is in use. In this paper we consider the application of ASAT for improving the scheduling of threads on an SGI Challenge. We demonstrate how the number of threads of an ASAT job adjusts to the overall system load to maintain thread balance and improve system throughput. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Convex Computer Corporation, </author> <title> "Convex Architecture Reference Manual (C-Series)", Document DHW-300, </title> <month> April </month> <year> 1992. </year>
Reference-contexts: As we point out in this paper, not having a dedicated system can seriously degrade the effectiveness of the FTS approach. Other dynamic, run-time, thread management techniques which are geared toward compiler detected parallelism include: Automatic Self-Adjusting Processors (ASAP) from Convex <ref> [1] </ref> and Autotasking on Cray Research [2] computers. A previous study of the benefits of Automatic Self-Allocating Threads (ASAT) for the Convex Exemplar was done in [6], details on multiple ASAT jobs appears in [7]. <p> Most compiler run-time libraries are designed to check the number of threads at the beginning of each parallel section. AN EXISTING MECHANISM An good example of dynamic thread balancing is the mechanism available on the Convex C-Series (C-240, C-3X00, C4XXX) supercomputers is called Automatic Self Allocating Processing (ASAP) <ref> [1] </ref>. We use ASAP as a model for comparison. The ASAP processing in the Convex C-Series systems is made possible because of an architectural feature called "Communication Registers" which are shared by all of the CPUs.
Reference: [2] <author> Cray Research, </author> <title> CF77 Compiling System, Volume 4: Parallel Processing Guide. </title>
Reference-contexts: As we point out in this paper, not having a dedicated system can seriously degrade the effectiveness of the FTS approach. Other dynamic, run-time, thread management techniques which are geared toward compiler detected parallelism include: Automatic Self-Adjusting Processors (ASAP) from Convex [1] and Autotasking on Cray Research <ref> [2] </ref> computers. A previous study of the benefits of Automatic Self-Allocating Threads (ASAT) for the Convex Exemplar was done in [6], details on multiple ASAT jobs appears in [7].
Reference: [3] <author> J. Liu, V. Saletore, </author> <title> "Self Scheduling on Distributed-Memory Machines," </title> <journal> IEEE Supercomputing'93, </journal> <pages> pp. 814-823, </pages> <year> 1993. </year>
Reference-contexts: The goal is to have balanced execution times on the processors while minimizing the overhead for partitioning the iterations. An excellent survey of these techniques is presented in <ref> [3] </ref>. The implementation of these techniques on most shared-memory parallel processors works with a fixed number of threads determined when the program is initially started. For the purpose of this paper, we call this technique Fixed Thread Scheduling (FTS).
Reference: [4] <author> J. C. Mogul and A. Borg, </author> <title> The Effect of Context Switches on Cache Performance, </title> <institution> DEC Western Research Laboratory TN-16, </institution> <month> Dec., </month> <year> 1990. </year> <note> http://www.research.digital.com/wrl/techreports /abstracts/TN-16.html </note>
Reference-contexts: They identified a number of the major problems with having too many threads including: 1. Preemption during spin-lock critical section, 2. Preemption of the wrong thread in a producer consumer relationship, 3. Unnecessary context switch overhead, and 4. Corruption of caches due to context switches (also see <ref> [4] </ref>). The general topic of scheduling for parallel loops is one that is well studied. The basic approach of these techniques is to partition the iterations of a parallel loop among a number of executing threads in a parallel process.
Reference: [5] <author> C. Polychronopoulos, D. J. Kuck, </author> <title> "Guided Self Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers," </title> <journal> IEEE Transactions on Computers, </journal> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: GSS is designed to even out wide variations in the execution times of the iterations of the parallel loop. GSS is described in <ref> [5] </ref>. application on an unloaded 4-CPU SGI with various compiler options: The Dynamic and GSS options add overhead to the loops. Unlike the Convex, this overhead is in software and has a greater impact on the performance of the application.
Reference: [6] <author> Severance C, Enbody R, Wallach S, </author> <title> Funkhouser B, </title> <booktitle> "Automatic Self Allocating Threads (ASAT) on the Convex Exemplar" Proceedings 1995 International Conference on Parallel Processing (ICPPP95), </booktitle> <month> August </month> <year> 1995, </year> <note> pages I-24 - I-31. </note>
Reference-contexts: Other dynamic, run-time, thread management techniques which are geared toward compiler detected parallelism include: Automatic Self-Adjusting Processors (ASAP) from Convex [1] and Autotasking on Cray Research [2] computers. A previous study of the benefits of Automatic Self-Allocating Threads (ASAT) for the Convex Exemplar was done in <ref> [6] </ref>, details on multiple ASAT jobs appears in [7]. ASAT The general goal of our Automatic Self-Allocating Threads (ASAT) is to eliminate thread imbalance by detecting thrashing and then dynamically reducing the number of active threads to achieve balanced execution over the long term.
Reference: [7] <author> Severance C, Enbody R, Peterson P, </author> <title> "Managing the Overall Balance of Operating System Threads on a MultiProcessor using Automatic Self-Allocating Threads (ASAT)," </title> <journal> Journal of Parallel and Distributed Computing Special Issue on Multithreading on Multiprocessors, </journal> <note> to appear. </note>
Reference-contexts: A previous study of the benefits of Automatic Self-Allocating Threads (ASAT) for the Convex Exemplar was done in [6], details on multiple ASAT jobs appears in <ref> [7] </ref>. ASAT The general goal of our Automatic Self-Allocating Threads (ASAT) is to eliminate thread imbalance by detecting thrashing and then dynamically reducing the number of active threads to achieve balanced execution over the long term.
Reference: [8] <author> Silicon Graphics, Inc., </author> <title> "Power FORTRAN Accelerator User's Guide," Document 007-0715-040, </title> <year> 1993. </year>
Reference-contexts: COMPILER OPTIONS ON THE SGI The SGI has several compiler options for load loop scheduling provided as part of its parallel FORTRAN compiler <ref> [8] </ref> [9]. Similar options are typically available on most parallel FORTRAN compilers.
Reference: [9] <institution> Silicon Graphics, Inc., </institution> <note> "FORTRAN77 Programmer's Guide," Document 007-0711-030, </note> <year> 1993. </year>
Reference-contexts: COMPILER OPTIONS ON THE SGI The SGI has several compiler options for load loop scheduling provided as part of its parallel FORTRAN compiler [8] <ref> [9] </ref>. Similar options are typically available on most parallel FORTRAN compilers.
Reference: [10] <author> Silicon Graphics, Inc., </author> <title> "Symmetric Multiprocessing Systems," </title> <type> Technical Report, </type> <year> 1993. </year>
Reference: [11] <author> A. Tucker and A. </author> <title> Gupta , "Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multiprocessors," </title> <booktitle> ACM SOSP Conf., </booktitle> <year> 1989, </year> <editor> p. </editor> <volume> 159 - 166. </volume>
Reference-contexts: Our approach does not necessarily apply to all multi-threaded environments. Database or network server environments may want to have significantly more operating system threads than available CPU resources in order to mask latencies due to I/O from the network, disk or other sources. 1 PREVIOUS WORK In <ref> [11] </ref> the problem of matching the overall systemwide number of threads to the number of processors was studied on an Encore Multimax. They identified a number of the major problems with having too many threads including: 1. Preemption during spin-lock critical section, 2.
References-found: 11

