URL: http://www.cs.cmu.edu/~jr6b/papers/cora-aaaiss99.ps.gz
Refering-URL: http://www.cs.cmu.edu/~jr6b/
Root-URL: http://www.cs.cmu.edu/~jr6b
Email: mccallum@justresearch.com  knigam@cs.cmu.edu  jr6b@andrew.cmu.edu  kseymore@ri.cmu.edu  
Title: Building Domain-Specific Search Engines with Machine Learning Techniques  
Author: Andrew McCallum zy Kamal Nigam Jason Rennie Kristie Seymore 
Address: 4616 Henry Street Pittsburgh, PA 15213  Pittsburgh, PA 15213  
Affiliation: Just Research  School of Computer Science Carnegie Mellon University  
Abstract: Domain-specific search engines are growing in popularity because they offer increased accuracy and extra functionality not possible with the general, Web-wide search engines. For example, www.campsearch.com allows complex queries by age-group, size, location and cost over summer camps. Unfortunately these domain-specific search engines are difficult and time-consuming to maintain. This paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific search engines. We describe new research in reinforcement learning, information extraction and text classification that enables efficient spidering, identifying informative text segments, and populating topic hierarchies. Using these techniques, we have built a demonstration system: a search engine for computer science research papers. It already contains over 50,000 papers and is publicly available at www.cora.justresearch.com. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baum, L. E. </author> <year> 1972. </year> <title> An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process. </title> <booktitle> Inequalities 3 </booktitle> <pages> 1-8. </pages>
Reference-contexts: Additionally, manually labeling large amounts of training data is costly and error-prone. Specifically, if we are willing to fix the model structure, we can use the Baum-Welch estimation technique <ref> (Baum 1972) </ref> to estimate model parameters. The Baum-Welch method is an Expectation-Maximization procedure for HMMs that finds local likelihood maxima, and is used extensively for acoustic model estimation in automatic speech recognition systems.
Reference: <author> Bellman, R. E. </author> <year> 1957. </year> <title> Dynamic Programming. </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton University Press. </publisher>
Reference: <author> Bikel, D. M.; Miller, S.; Schwartz, R.; and Weischedel, R. </author> <year> 1997. </year> <title> Nymble: a high-performance learning name-finder. </title> <booktitle> In Proceedings of ANLP-97, </booktitle> <pages> 194-201. </pages>
Reference-contexts: HMMs have been used in other systems for information extraction and the closely related problems of topic detection and text segmentation. Leek (1997) uses hidden Markov models to extract information about gene names and locations from scientific abstracts. The Nymble system <ref> (Bikel et al. 1997) </ref> deals with named-entity extraction, and a system by Yam-ron et al. (1998) uses an HMM for topic detection and tracking.
Reference: <author> Bollacker, K. D.; Lawrence, S.; and Giles, C. L. </author> <year> 1998. </year> <title> CiteSeer: An autonomous web agent for automatic retrieval and identification of interesting publications. </title> <booktitle> In Agents '98, </booktitle> <pages> 116-123. </pages>
Reference-contexts: This project also has a strong emphasis on using machine learning techniques, including text classification and information extraction, to promote easy re-use across domains. Two example domains, computer science departments and companies, have been developed. The CiteSeer project <ref> (Bollacker, Lawrence, & Giles 1998) </ref> has also developed a search engine for computer science research papers. It provides similar functionality for searching and linking of research papers, but does not currently provide a hierarchy of the field.
Reference: <author> Boyan, J.; Freitag, D.; and Joachims, T. </author> <year> 1996. </year> <title> A machine learning architecture for optimizing web search engines. </title> <booktitle> In AAAI workshop on Internet-Based Information Systems. </booktitle>
Reference-contexts: WebWatcher (Joachims, Freitag, & Mitchell 1997) is a browsing assistant that uses a combination of supervised and reinforcement learning to help a user find information by recommending which hyperlinks to follow. Laser uses reinforcement learning to tune the parameters of a search engine <ref> (Boyan, Freitag, & Joachims 1996) </ref>. 3.1 Reinforcement Learning In machine learning, the term "reinforcement learning" refers to a framework for learning optimal decision making from rewards or punishment (Kaelbling, Littman, & Moore 1996).
Reference: <author> Charniak, E. </author> <year> 1993. </year> <title> Statistical Language Learning. </title> <address> Cam-bridge, Massachusetts: </address> <publisher> The MIT Press. </publisher>
Reference: <author> Cho, J.; Garcia-Molina, H.; and Page, L. </author> <year> 1998. </year> <title> Efficient crawling through URL ordering. </title> <note> In WWW7. </note>
Reference: <author> Cohen, W., and Fan, W. </author> <year> 1999. </year> <title> Learning page-independent heuristics for extracting data from web pages. </title> <booktitle> In AAAI Spring Symposium on Intelligent Agents in Cyberspace. </booktitle>
Reference-contexts: Two demonstration domains of computer games and North American birds integrate information from many sources. The emphasis is on providing soft matching for information retrieval searching. Information is extracted from web pages by hand-written extraction patterns that are customized for each web source. Recent WHIRL research <ref> (Cohen & Fan 1999) </ref> learns general wrapper extractors from examples. 7 Conclusions and Future Work The amount of information available on the Internet continues to grow exponentially.
Reference: <author> Cohen, W. </author> <year> 1998. </year> <title> A web-based information system that reasons with structured collections of text. </title> <booktitle> In Agents '98. </booktitle>
Reference-contexts: The web sources for their libraries are manually identified. No high-level organization of the information is given. No information extraction is performed and, for the paper repositories, no citation linking is provided. The WHIRL project <ref> (Cohen 1998) </ref> is an effort to integrate a variety of topic-specific sources into a single domain-specific search engine. Two demonstration domains of computer games and North American birds integrate information from many sources. The emphasis is on providing soft matching for information retrieval searching.
Reference: <author> Craven, M.; DiPasquo, D.; Freitag, D.; McCallum, A.; Mitchell, T.; Nigam, K.; and Slattery, S. </author> <year> 1998. </year> <title> Learning to extract symbolic knowledge from the World Wide Web. </title> <booktitle> In AAAI-98, </booktitle> <pages> 509-516. </pages>
Reference-contexts: We are also investigating principled methods of re-weighting the word features for "semi-supervised" clustering that will provide better discriminative training with unlabeled data. 6 Related Work Several related research projects investigate the gathering and organization of specialized information. The WebKB <ref> (Craven et al. 1998) </ref> project focuses on the collection and organization of information from the Web into knowledge bases. This project also has a strong emphasis on using machine learning techniques, including text classification and information extraction, to promote easy re-use across domains.
Reference: <author> Dempster, A. P.; Laird, N. M.; and Rubin, D. B. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> Series B 39(1) </volume> <pages> 1-38. </pages>
Reference-contexts: These pseudo-labels are noisy, and the majority of documents remain unlabeled. However, we then build an improved classifier by using all the documents and any pseudo-labels to bootstrap a naive Bayes text classifier that has been combined with Expectation-Maximization (EM) <ref> (Dempster, Laird, & Rubin 1977) </ref> and a powerful technique from statistics called shrinkage. EM serves to incorporate the evidence from the unlabeled data, and to correct, to some extent, the pseudo-labels. Hierarchical shrinkage serves to alleviate poor parameter estimates caused by sparse training data. <p> In previous work (Nigam et al. 1999), we have shown this technique significantly increases classification accuracy with limited amounts of labeled data and large amounts of unlabeled data. EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data <ref> (Dempster, Laird, & Rubin 1977) </ref>. Given a model of data generation, and data with some missing values, EM iteratively uses the current model to estimate the missing values, and then uses the missing value estimates to improve the model.
Reference: <author> Hofmann, T., and Puzicha, J. </author> <year> 1998. </year> <title> Statistical models for co-occurrence data. </title> <type> Technical Report AI Memo 1625, </type> <institution> Artificial Intelligence Laboratory, MIT. </institution>
Reference-contexts: For these three cases, limiting state transitions to occur only after words with punctuation improves accuracy by about 1% absolute. by naive Bayes and shrinkage with vertical word redistribution <ref> (Hofmann & Puzicha 1998) </ref>. Words that were not among the keywords for that class are indicated with italics. 4.3 Future Work All of the experiments presented above use HMMs where the model structure and parameters were estimated directly from labeled training instances.
Reference: <author> Joachims, T.; Freitag, D.; and Mitchell, T. </author> <year> 1997. </year> <title> Web-watcher: A tour guide for the World Wide Web. </title> <booktitle> In Proceedings of IJCAI-97. </booktitle>
Reference-contexts: Cho, Garcia-Molina, & Page (1998) suggest a number of heuristic ordering metrics for choosing which link to crawl next when searching for certain categories of web pages. Additionally, there are systems that use reinforcement learning for non-spidering Web tasks. WebWatcher <ref> (Joachims, Freitag, & Mitchell 1997) </ref> is a browsing assistant that uses a combination of supervised and reinforcement learning to help a user find information by recommending which hyperlinks to follow.
Reference: <author> Kaelbling, L. P.; Littman, M. L.; and Moore, A. W. </author> <year> 1996. </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research 237-285. </journal>
Reference-contexts: When aiming to populate a domain-specific search engine, the spider need not explore the Web indiscriminantly, but should explore in a directed fashion in order to find domain-relevant documents efficiently. We frame the spidering task in a reinforcement learning framework <ref> (Kaelbling, Littman, & Moore 1996) </ref>, allowing us to precisely and mathematically define "optimal behavior." This approach provides guidance for designing an intelligent spider that aims to select hyperlinks optimally. <p> Laser uses reinforcement learning to tune the parameters of a search engine (Boyan, Freitag, & Joachims 1996). 3.1 Reinforcement Learning In machine learning, the term "reinforcement learning" refers to a framework for learning optimal decision making from rewards or punishment <ref> (Kaelbling, Littman, & Moore 1996) </ref>.
Reference: <author> Leek, T. R. </author> <year> 1997. </year> <title> Information extraction using hidden Markov models. </title> <type> Master's thesis, </type> <institution> UC San Diego. </institution>
Reference: <author> Lewis, D. D. </author> <year> 1998. </year> <title> Naive (Bayes) at forty: The independence assumption in information retrieval. </title> <booktitle> In ECML-98. </booktitle>
Reference-contexts: (c j jd i ) The class frequency parameters are set in the same way, where jCj indicates the number of classes: P (c j ) = P jCj + jDj Empirically, when given a large number of training documents, naive Bayes does a good job of classifying text documents <ref> (Lewis 1998) </ref>.
Reference: <author> McCallum, A., and Nigam, K. </author> <year> 1998. </year> <title> A comparison of event models for naive Bayes text classification. </title> <booktitle> In AAAI-98 Workshop on Learning for Text Categorization. </booktitle> <address> http://www.cs.cmu.edu/~mccallum. </address>
Reference: <author> McCallum, A.; Rosenfeld, R.; Mitchell, T.; and Ng, A. </author> <year> 1998. </year> <title> Improving text clasification by shrinkage in a hierarchy of classes. </title> <booktitle> In ICML-98, </booktitle> <pages> 359-367. </pages>
Reference: <author> Menczer, F. </author> <year> 1997. </year> <title> ARACHNID: Adaptive retrieval agents choosing heuristic neighborhoods for information discovery. </title> <booktitle> In ICML '97. </booktitle>
Reference-contexts: We use reinforcement learning to perform efficient spidering. Several other systems have also studied spidering, but without a framework defining optimal behavior. Arachnid <ref> (Menczer 1997) </ref> maintains a collection of competitive, reproducing and mutating agents for finding information on the Web. Cho, Garcia-Molina, & Page (1998) suggest a number of heuristic ordering metrics for choosing which link to crawl next when searching for certain categories of web pages.
Reference: <author> Mitchell, T. M. </author> <year> 1997. </year> <title> Machine Learning. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Cho, Garcia-Molina, & Page (1998) suggest a number of heuristic ordering metrics for choosing which link to crawl next when searching for certain categories of web pages. Additionally, there are systems that use reinforcement learning for non-spidering Web tasks. WebWatcher <ref> (Joachims, Freitag, & Mitchell 1997) </ref> is a browsing assistant that uses a combination of supervised and reinforcement learning to help a user find information by recommending which hyperlinks to follow.
Reference: <author> Nigam, K.; McCallum, A.; Thrun, S.; and Mitchell, T. </author> <year> 1999. </year> <title> Text classification from labeled and unlabeled documents using EM. </title> <journal> Machine Learning. </journal> <note> To appear. </note>
Reference-contexts: This results in parameters that are more likely given all the data, both the labeled and the unlabeled. In previous work <ref> (Nigam et al. 1999) </ref>, we have shown this technique significantly increases classification accuracy with limited amounts of labeled data and large amounts of unlabeled data. EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data (Dempster, Laird, & Rubin 1977).
Reference: <author> Rabiner, L. R. </author> <year> 1989. </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE 77(2) </booktitle> <pages> 257-286. </pages>
Reference-contexts: Information extraction, the process of automatically finding specific textual substrings in a document, is well suited to this task. We approach information extraction with a technique from statistical language modeling and speech recognition, namely hidden Markov models <ref> (Rabiner 1989) </ref>. Our initial algorithm extracts fields such as the title, authors, institution, and journal name from research paper reference sections with 93% accuracy. Search engines often provide a hierarchical organization of materials into relevant topics; Yahoo is the prototypical example.
Reference: <author> Stolcke, A. </author> <year> 1994. </year> <title> Bayesian Learning of Probabilistic Language Models. </title> <type> Ph.D. Dissertation, </type> <institution> UC Berkeley. </institution>
Reference-contexts: We can remove the assumption of a fixed model structure and estimate both model structure and parameters directly from the data using Bayesian Model Merging <ref> (Stolcke 1994) </ref>. Bayesian Model Merging involves starting out with a maximally specific hidden Markov model, where each training observation is represented by a single state.
Reference: <author> Torgo, L., and Gama, J. </author> <year> 1997. </year> <title> Regression using classification algorithms. Intelligent Data Analysis 1(4). </title>
Reference-contexts: We represent the mapping using a collection of naive Bayes text classifiers (see Section 5.2). We perform the mapping by casting this regression problem as classification <ref> (Torgo & Gama 1997) </ref>. We discretize the discounted sum of future reward values of our training data into bins, place the hyperlinks into the bin corresponding to their Q values by dynamic programming, and use the hyperlinks' neighborhood text as training data for a naive Bayes text classifier.
Reference: <author> Viterbi, A. J. </author> <year> 1967. </year> <title> Error bounds for convolutional codes and an asymtotically optimum decoding algorithm. </title> <journal> IEEE Transactions on Information Theory IT-13:260-269. </journal>
Reference-contexts: the state sequence V (xjM ) that has the highest probability of having pro duced an observation sequence: V (xjM ) = arg max l+1 Y P (q k1 ! q k )P (q k " x k ): (4) Fortunately, there is an efficient algorithm, called the Viterbi algorithm <ref> (Viterbi 1967) </ref>, that efficiently recovers this state sequence. HMMs may be used for information extraction from research papers by formulating a model in the following way: each state is associated with a field class that we want to extract, such as title, author or institution.
Reference: <author> Witten, I. H.; Nevill-Manning, C.; McNab, R.; and Cunnningham, S. J. </author> <year> 1998. </year> <title> A public digital library based on full-text retrieval: Collections and experience. </title> <journal> Communications of the ACM 41(4) </journal> <pages> 71-75. </pages>
Reference-contexts: It provides similar functionality for searching and linking of research papers, but does not currently provide a hierarchy of the field. CiteSeer focuses on the domain of research papers, but not as much on using machine learning techniques to automate search engine creation. The New Zealand Digital Library project <ref> (Witten et al. 1998) </ref> has created publicly-available search engines for domains from computer science technical reports to song melodies. The emphasis of this project is on the creation of full-text searchable digital libraries, and not on machine learning techniques that can be used to autonomously generate such repositories.
Reference: <author> Yamron, J.; Carp, I.; Gillick, L.; Lowe, S.; and van Mul-bregt, P. </author> <year> 1998. </year> <title> A hidden Markov model approach to text segmentation and event tracking. </title> <booktitle> In Proceedings of the IEEE ICASSP. </booktitle>
References-found: 27

