URL: ftp://ftp.comlab.ox.ac.uk/pub/Packages/ILP/Papers/poslearn1.ps
Refering-URL: http://gruffle.comlab.ox.ac.uk/oucl/groups/machlearn/mlg_pub.html
Root-URL: 
Title: Learning from positive data  
Author: Stephen Muggleton 
Address: Parks Road, Oxford, OX1 3QD, United Kingdom.  
Affiliation: Oxford University Computing Laboratory,  
Abstract: Gold showed in 1967 that not even regular grammars can be exactly identified from positive examples alone. Since it is known that children learn natural grammars almost exclusively from positives examples, Gold's result has been used as a theoretical support for Chomsky's theory of innate human linguistic abilities. In this paper new results are presented which show that within a Bayesian framework not only grammars, but also logic programs are learnable with arbitrarily low expected error from positive examples only. In addition, we show that the upper bound for expected error of a learner which maximises the Bayes' posterior probability when learning from positive examples is within a small additive term of one which does the same from a mixture of positive and negative examples. An Inductive Logic Programming implementation is described which avoids the pitfalls of greedy search by global optimisation of this function during the local construction of individual clauses of the hypothesis. Results of testing this implementation on artificially-generated data-sets are reported. These results are in agreement with the theoretical predictions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Inference of reversible languages. </title> <journal> Journal of the ACM, </journal> <volume> 29 </volume> <pages> 741-765, </pages> <year> 1982. </year>
Reference-contexts: This has provided a strong impetus for the investigation of constrained hypothesis languages, within which learning from positive examples is possible. For instance, Plotkin [15] demonstrated the existence of unique least general generalisations of positive examples represented as first-order clauses. Biermann and Feldman [2] and later Angluin <ref> [1] </ref> demonstrated that certain parameterised subsets of the regular languages could be identified in the limit from positive examples only. Within the framework of PAC-learning Valiant demonstrated [19] that k-CNF propositional logic formulae are learnable from positive examples.
Reference: [2] <author> A.W. Biermann and J.A. Feldman. </author> <title> On the synthesis of finite-state machines from samples of their behaviour. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C(21):592-597, </volume> <year> 1972. </year>
Reference-contexts: This has provided a strong impetus for the investigation of constrained hypothesis languages, within which learning from positive examples is possible. For instance, Plotkin [15] demonstrated the existence of unique least general generalisations of positive examples represented as first-order clauses. Biermann and Feldman <ref> [2] </ref> and later Angluin [1] demonstrated that certain parameterised subsets of the regular languages could be identified in the limit from positive examples only. Within the framework of PAC-learning Valiant demonstrated [19] that k-CNF propositional logic formulae are learnable from positive examples.
Reference: [3] <author> W. Buntine. </author> <title> A Theory of Learning Classification Rules. </title> <type> PhD thesis, </type> <institution> School of Computing Science, University of Technology, </institution> <address> Sydney, </address> <year> 1990. </year>
Reference-contexts: Owing to limitations of time it was only feasible to test three such domains, for which the bounds held in all three cases. Further testing of domains is necessary to determine whether the bounds hold on average for existing Progol datasets. Various researchers including <ref> [3, 6] </ref> have advocated and demonstrated the use of Bayesian analysis in machine learning. The success of the Bayesian solution to learning from positive examples reinforces this trend. Several techniques [16, 17, 8] for learning from positive examples only have been investigated within Inductive Logic Programming.
Reference: [4] <author> N. Chomsky. </author> <title> Knowledge of language: its nature, origin and use. Praeger, </title> <address> New York, </address> <year> 1986. </year> <note> First published 1965. </note>
Reference-contexts: He notes that psycholinguistic studies by McNeill and others had shown that ... children are rarely informed when they make grammatical errors and those that are informed take little heed. 1 Gold's negative results have been taken by [14] as theoretical support for Chom--sky's theory <ref> [4] </ref> of innate human linguistic abilities. In this paper Gold's requirements for exact identification of a language are replaced by a need to converge with arbitrarily low error. In a previous paper [10] the author derived a function for learning logic programs from positive examples only.
Reference: [5] <author> E.M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: 1 Introduction Gold's <ref> [5] </ref> seminal paper not only formed the foundations of learnability theory but also provided an important negative result for the learnability of grammars.
Reference: [6] <author> D. Haussler, M Kearns, and R. Shapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <booktitle> In COLT-91: Proceedings of the 4th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 61-74, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kauffmann. </publisher>
Reference-contexts: Owing to limitations of time it was only feasible to test three such domains, for which the bounds held in all three cases. Further testing of domains is necessary to determine whether the bounds hold on average for existing Progol datasets. Various researchers including <ref> [3, 6] </ref> have advocated and demonstrated the use of Bayesian analysis in machine learning. The success of the Bayesian solution to learning from positive examples reinforces this trend. Several techniques [16, 17, 8] for learning from positive examples only have been investigated within Inductive Logic Programming.
Reference: [7] <author> D. Haussler, M Kearns, and R. Shapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <journal> Machine Learning Journal, </journal> <volume> 14(1) </volume> <pages> 83-113, </pages> <year> 1994. </year>
Reference-contexts: This result indicates a form of convergence, somewhat different from Gold's identification in the limit. 4 Analysis of expected error Haussler et al. <ref> [7] </ref> argue the advantages of analysing expected error over VC dimension analysis. Analysis of expected error is the approach taken below. It is assumed that class membership of instances is decidable for all hypotheses.
Reference: [8] <author> R.J. Mooney and M.E. Califf. </author> <title> Induction of first-order decision lists: Results on learning the past tense of english verbs. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3 </volume> <pages> 1-24, </pages> <year> 1995. </year>
Reference-contexts: Various researchers including [3, 6] have advocated and demonstrated the use of Bayesian analysis in machine learning. The success of the Bayesian solution to learning from positive examples reinforces this trend. Several techniques <ref> [16, 17, 8] </ref> for learning from positive examples only have been investigated within Inductive Logic Programming. However, all these approaches differ from this paper in assuming some form of completeness within the example set.
Reference: [9] <author> S. Muggleton. </author> <title> Bayesian inductive logic programming. </title> <editor> In M. Warmuth, editor, </editor> <booktitle> Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 3-11, </pages> <address> New York, 1994. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Experiments on three separate domains (animal taxonomy, KRK-illegal and grammar learning) are shown to be in accordance with the theoretical predictions. This paper is organised as follows. In Section 2 a Bayes' framework is described which is compatible with the U-learnability framework <ref> [9, 13] </ref>. The Bayes' function for the posterior probability of hypotheses given positive examples only is derived in Section 3. The expected error of an algorithm which maximises the Bayes' function over a high prior probability segment of the hypothesis space is given in Section 4. <p> The results of these experiments are discussed in Section 7. The paper is concluded in Section 8 by a comparison to related work and a discussion of directions for future research. 2 Bayes' positive example framework The following is a simplified version of the U-learnability framework presented in <ref> [9, 13] </ref>. X is taken to be a countable class of instances and H 2 X to be a countable class of concepts. D X and D H are probability distributions over X and H respectively.
Reference: [10] <author> S. Muggleton. </author> <title> Inverse entailment and Progol. </title> <journal> New Generation Computing, </journal> <volume> 13 </volume> <pages> 245-286, </pages> <year> 1995. </year>
Reference-contexts: In this paper Gold's requirements for exact identification of a language are replaced by a need to converge with arbitrarily low error. In a previous paper <ref> [10] </ref> the author derived a function for learning logic programs from positive examples only. In the present paper the Bayes' function for maximising posterior probability is derived. The solution is representation independent, and therefore equally applicable to grammar learning, scientific theory formation or even automatic programming. <p> The function ln p (HjE) decreases with increases in sz (H) and g (H). Additionally, as m grows, the requirements on generality of an hypothesis become stricter. A function f m with similar properties was defined in <ref> [10] </ref> and it was shown there that for every hypothesis H except T there is a value of m such that for all m 0 &gt; m it is the case that f m 0 (H) &lt; f m (T ). <p> For the purposes of analysis the distribution D H (H i ) = a i 2 is assumed, where a is a normalising constant. This is similar to the prior distribution assumptions used in Progol4.1 <ref> [10] </ref> and is a smoothed version of a distribution which assigns equal probability to the 2 b hypotheses describable in b bits, where the sum of the probabilities of such hypotheses is 2 b . Within this distribution i has infinite mean and variance. <p> Again the result is independent of the choice of D X and again L considers only O (m) hypotheses to achieve an expected error of O ( ln m m ). 5 Implementation The Bayes' function f m has been implemented to guide the search of the ILP system Progol <ref> [10] </ref> when learning from positive examples only. The new version, Progol4.2, is available by anonymous ftp from ftp.comlab.ox.ac.uk in directory pub/Packages/ILP/progol4.2. The earlier version, Progol4.1, uses a cover-set algorithm to construct the set of clauses, but for each clause does a pruned admissible search to maximise compression. <p> The polynomial time-complexity bounds on the search carried out by Progol4.1 <ref> [10] </ref> are unaltered for Progol4.2. 5.1 Estimation of g (H i ) The function g (H i ) in the above is estimated in Progol4.2 using Stochastic Logic Programs (SLPs) [11]. An SLP is a range-restricted logic program P with numeric labels associated with each of the clauses. <p> If s 0 of these instances are entailed by p=n then the Laplace corrected estimate of g (H i ) is s 0 +1 s+2 . In order to construct the SLP for the domain of p=n, Progol4.2 uses the modeh declaration of p=n (see <ref> [10] </ref>). For instance, suppose in a chess domain the mode declaration is modeh (1,move (+piece,pos (+file,+rank),pos (+file,+rank))). Then Progol4.2 will construct the following generating clause for the domain. '*move'(A,pos (B,C),pos (D,E)) :- piece (A), file (B), rank (C), file (D), rank (E).
Reference: [11] <author> S. Muggleton. </author> <title> Stochastic logic programs. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Advances in Inductive Logic Programming. </booktitle> <publisher> IOS Press/Ohmsha, </publisher> <year> 1996. </year> <month> 16 </month>
Reference-contexts: The polynomial time-complexity bounds on the search carried out by Progol4.1 [10] are unaltered for Progol4.2. 5.1 Estimation of g (H i ) The function g (H i ) in the above is estimated in Progol4.2 using Stochastic Logic Programs (SLPs) <ref> [11] </ref>. An SLP is a range-restricted logic program P with numeric labels associated with each of the clauses. An SLP can be used to randomly derive elements of the Herbrand Base of P using SLD derivation with a stochastic selection rule.
Reference: [12] <author> S. Muggleton, M.E. Bain, J. Hayes-Michie, and D. Michie. </author> <title> An experimental comparison of human and machine learning formalisms. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> Los Altos, CA, 1989. </address> <publisher> Kaufmann. </publisher>
Reference-contexts: Animal taxonomy. Figure 2 shows the target and form of examples for the animal taxonomy domain. KRK illegality. Figure 3 shows the target and form of examples for the KRK illegality domain (originally described in <ref> [12] </ref>). Natural language grammar. Figure 4 shows the target and form of examples for the natural language grammar domain. Examples sets and background knowledge for the domains above are available from the ftp site described in Section 5. 8 Examples. illegal (3,5,6,7,6,2). illegal (3,6,7,6,7,4). illegal (5,1,2,1,2,1). illegal (4,3,1,1,4,2).
Reference: [13] <author> S. </author> <title> Muggleton and C.D. Page. A learnability model for universal representations. </title> <type> Technical Report PRG-TR-3-94, </type> <institution> Oxford University Computing Laboratory, Oxford, </institution> <year> 1994. </year>
Reference-contexts: Experiments on three separate domains (animal taxonomy, KRK-illegal and grammar learning) are shown to be in accordance with the theoretical predictions. This paper is organised as follows. In Section 2 a Bayes' framework is described which is compatible with the U-learnability framework <ref> [9, 13] </ref>. The Bayes' function for the posterior probability of hypotheses given positive examples only is derived in Section 3. The expected error of an algorithm which maximises the Bayes' function over a high prior probability segment of the hypothesis space is given in Section 4. <p> The results of these experiments are discussed in Section 7. The paper is concluded in Section 8 by a comparison to related work and a discussion of directions for future research. 2 Bayes' positive example framework The following is a simplified version of the U-learnability framework presented in <ref> [9, 13] </ref>. X is taken to be a countable class of instances and H 2 X to be a countable class of concepts. D X and D H are probability distributions over X and H respectively.
Reference: [14] <author> S. Pinker. </author> <title> Language learnability and language development. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, Mass., </address> <year> 1984. </year>
Reference-contexts: He notes that psycholinguistic studies by McNeill and others had shown that ... children are rarely informed when they make grammatical errors and those that are informed take little heed. 1 Gold's negative results have been taken by <ref> [14] </ref> as theoretical support for Chom--sky's theory [4] of innate human linguistic abilities. In this paper Gold's requirements for exact identification of a language are replaced by a need to converge with arbitrarily low error.
Reference: [15] <author> G.D. Plotkin. </author> <title> A note on inductive generalisation. </title> <editor> In B. Meltzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence 5, </booktitle> <pages> pages 153-163. </pages> <publisher> Edinburgh University Press, Edinburgh, </publisher> <year> 1969. </year>
Reference-contexts: This has provided a strong impetus for the investigation of constrained hypothesis languages, within which learning from positive examples is possible. For instance, Plotkin <ref> [15] </ref> demonstrated the existence of unique least general generalisations of positive examples represented as first-order clauses. Biermann and Feldman [2] and later Angluin [1] demonstrated that certain parameterised subsets of the regular languages could be identified in the limit from positive examples only.
Reference: [16] <author> J.R. Quinlan and R.M. Cameron. </author> <title> Induction of logic programs: FOIL and related systems. </title> <journal> New Generation Computing, </journal> <volume> 13 </volume> <pages> 287-312, </pages> <year> 1995. </year>
Reference-contexts: Various researchers including [3, 6] have advocated and demonstrated the use of Bayesian analysis in machine learning. The success of the Bayesian solution to learning from positive examples reinforces this trend. Several techniques <ref> [16, 17, 8] </ref> for learning from positive examples only have been investigated within Inductive Logic Programming. However, all these approaches differ from this paper in assuming some form of completeness within the example set.
Reference: [17] <author> L. De Raedt and M. Bruynooghe. </author> <title> A theory of clausal discovery. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Various researchers including [3, 6] have advocated and demonstrated the use of Bayesian analysis in machine learning. The success of the Bayesian solution to learning from positive examples reinforces this trend. Several techniques <ref> [16, 17, 8] </ref> for learning from positive examples only have been investigated within Inductive Logic Programming. However, all these approaches differ from this paper in assuming some form of completeness within the example set.
Reference: [18] <author> T. Shinohara. </author> <title> Inductive inference of monotonic formal systems from positive data. </title> <booktitle> In Proceedings of the first international workshop on algorithmic learning theory, </booktitle> <address> Tokyo, 1990. </address> <publisher> Ohmsha. </publisher>
Reference-contexts: Biermann and Feldman [2] and later Angluin [1] demonstrated that certain parameterised subsets of the regular languages could be identified in the limit from positive examples only. Within the framework of PAC-learning Valiant demonstrated [19] that k-CNF propositional logic formulae are learnable from positive examples. More recently Shinohara <ref> [18] </ref> demonstrated that certain size-bounded classes of elementary formal systems are identifiable in the limit from positive examples. Unlike the approaches above, the techniques used in this paper for learning from positive examples are representation independent.
Reference: [19] <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: Biermann and Feldman [2] and later Angluin [1] demonstrated that certain parameterised subsets of the regular languages could be identified in the limit from positive examples only. Within the framework of PAC-learning Valiant demonstrated <ref> [19] </ref> that k-CNF propositional logic formulae are learnable from positive examples. More recently Shinohara [18] demonstrated that certain size-bounded classes of elementary formal systems are identifiable in the limit from positive examples. Unlike the approaches above, the techniques used in this paper for learning from positive examples are representation independent.
References-found: 19

