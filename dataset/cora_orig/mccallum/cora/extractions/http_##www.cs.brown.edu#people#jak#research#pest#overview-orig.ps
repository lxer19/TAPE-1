URL: http://www.cs.brown.edu/people/jak/research/pest/overview-orig.ps
Refering-URL: http://www.cs.brown.edu/people/jak/research/pest/
Root-URL: http://www.cs.brown.edu
Email: jak@cs.brown.edu  
Phone: (401) 863-7695  
Title: PEST overview  
Author: Jak Kirman 
Date: July 5, 1995  
Address: Box 1910, Providence, RI 02912  
Affiliation: Department of Computer Science Brown University,  
Abstract-found: 0
Intro-found: 1
Reference: [ Barto et al., 1993 ] <author> Barto, Andrew G.; Bradtke, Steven J.; and Singh, Satin-der P. </author> <year> 1993. </year> <title> Learning to act using real-time dynamic programming. </title> <note> Submitted to AIJ Special Issue on Computational Theories of Interaction and Agency. </note>
Reference-contexts: true performance measure, but in practice it is an accurate approximation. 26 8.3 Racetrack (RA) 8.3.1 Motivation The racetrack domains are based on a game called Race Track described by Martin Gardner [ Gardner, 1973 ] , and adapted to the Markov decision problem formalism by Barto, Bradtke and Singh <ref> [ Barto et al., 1993 ] </ref> . The problem models a race car moving on a track discretized into a grid; a state of the world encodes the location of the car and its velocity vector. The agent controls the race car's acceleration, not its speed directly. <p> in which case it is 0. 9 Agents and worlds I have implemented a number of different agents and worlds in order to conduct the experiments described in this dissertation; they are described below. 9.1 RTDP RTDP implements the real-time dynamic programming algorithm presented by Barto, Bradtke and Singh in <ref> [ Barto et al., 1993 ] </ref> . This agent requires a 30 world model, as defined below. I present some comparisons of the perfor-mance of RTDP and the Plexus planner in my thesis.
Reference: [ Basye, 1992 ] <author> Basye, </author> <title> Kenneth 1992. A framework for map construction. </title> <type> Technical report, </type> <institution> Brown University Department of Computer Science, Providence, RI. </institution>
Reference: [ Dean et al., 1990 ] <author> Dean, Thomas; Camus, Theodore; and Kirman, </author> <month> Jak </month> <year> 1990. </year> <title> Sequential decision making for active perception. </title> <booktitle> In Proceedings of the DARPA Image Understanding Workshop. DARPA. </booktitle> <pages> 889-894. </pages>
Reference: [ Dean et al., 1993 ] <author> Dean, Thomas; Kaelbling, Leslie Pack; Kirman, Jak; and Nicholson, </author> <title> Ann 1993. Planning with deadlines in stochastic domains. </title> <booktitle> In AAAI-93. AAAI. </booktitle> <pages> 32 </pages>
Reference-contexts: At the moment, no flow-control is provided; this would be a useful addition, since conditioning phase execution on results obtained during previous phases would be a simple step towards the more general deliberation-scheduling planners described in earlier Plexus papers (e.g., <ref> [ Dean et al., 1993 ] </ref> ). The phases currently implemented are: strengthen, prune, explore, policy iteration and value iteration. [This is the section I will be focusing most of my attention on; I want to augment the components with modules to apply prioritized sweeping and other approximation techniques, for example.]
Reference: [ Gardner, 1973 ] <author> Gardner, </author> <title> Martin 1973. Mathematical games. </title> <publisher> Scientific American (228:108). </publisher>
Reference-contexts: This is therefore an approximation of the true performance measure, but in practice it is an accurate approximation. 26 8.3 Racetrack (RA) 8.3.1 Motivation The racetrack domains are based on a game called Race Track described by Martin Gardner <ref> [ Gardner, 1973 ] </ref> , and adapted to the Markov decision problem formalism by Barto, Bradtke and Singh [ Barto et al., 1993 ] .
Reference: [ Kirman et al., 1991 ] <author> Kirman, Jak; Basye, Kenneth; and Dean, </author> <title> Thomas 1991. Sensor abstractions for control of navigation. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation. </booktitle> <pages> 2812-2817. </pages>
Reference: [ Leonard and Durrant-Whyte, 1989 ] <author> Leonard, John J. and Durrant-Whyte, Hugh F. </author> <year> 1989. </year> <title> Active sensor control for mobile robotics. </title> <type> Technical Report OUEL-1756/89, </type> <institution> Oxford University Robotics Research Group. </institution>
Reference-contexts: One of the main problems with navigation using mobile robots is the speed with which small errors in dead reckoning accumulate, due to irregularities in the floor surface, wheel slippage, or misalignment of the wheels. Analysis of the sonar readings in an office building <ref> [ Leonard and Durrant-Whyte, 1989 ] </ref> is more reliable when the sonars are known to be roughly perpendicular to a flat surface, since deviations from the normal will lead to specular reflections, giving wildly incorrect results.
Reference: [ Moore and Atkeson, 1993 ] <author> Moore, Andrew W. and Atkeson, Christopher G. </author> <year> 1993. </year> <title> Memory-based reinforcement learning: Efficient computation with prioritized sweeping. </title> <booktitle> In Advances in Neural Information Processing 5, </booktitle> <address> San Mateo, California. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 33 </pages>
Reference-contexts: This agent requires a 30 world model, as defined below. I present some comparisons of the perfor-mance of RTDP and the Plexus planner in my thesis. My implementation of RTDP is very simple, and its performance could be greatly improved by techniques such as prioritized sweeping <ref> [ Moore and Atkeson, 1993 ] </ref> . RTDP starts by assigning an optimistic value to each state. In all the cases I discuss, rewards are negative, so we ascribe a value of 0 to every state. Then RTDP goes into a cycle of trials.
References-found: 8

