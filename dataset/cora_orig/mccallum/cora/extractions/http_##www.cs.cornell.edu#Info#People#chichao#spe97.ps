URL: http://www.cs.cornell.edu/Info/People/chichao/spe97.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/chichao/papers.htm
Root-URL: 
Title: MRPC: A High Performance RPC System for MPMD Parallel Computing  
Author: ChiChao Chang, Grzegorz Czajkowski, and Thorsten von Eicken 
Keyword: remote procedure call, active messages, multithreading, MPMD, parallel computing  
Address: Ithaca, NY 14853  
Affiliation: Department of Computer Science Cornell University  
Abstract: MRPC is an RPC system that is designed and optimized for MPMD parallel computing. Existing systems based on standard RPC incur an unnecessarily high cost when used on high-performance multi-computers, limiting the appeal of RPC-based languages in the parallel computing community. MRPC combines the efficient control and data transfer provided by Active Messages (AM) with a minimal multithreaded runtime system that extends AM with the features required to support MPMD. This approach introduces only the necessary runtime RPC overheads for an MPMD environment. MRPC has been integrated into Compositional C++ (CC++), a parallel extension of C++ that offers an MPMD programming model. Basic RPC performance in MRPC is within a factor of two from those of Split-C, a highly tuned SPMD language, and other messaging layers. CC++ applications perform within a factor of two to six from comparable Split-C versions, which represent an order of magnitude improvement over previous CC++ implementations. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> H. Bal, R. Bhoedjang, R. Hofman, C. Jacobs, K. Langendoen, T. Ruhl, and K. Verstoep. </author> <title> Performance of a High-Level Parallel Language on a HighSpeed Network. </title> <note> In Technical Report IR-400, </note> <institution> Vrije Universiteit, </institution> <address> Amsterdam, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Panda has been recently ported to run on a network of eight 50 MHz Sparc workstations using a modified version of Fast Messages (in order to support software interrupt and multicast) over Myrinet <ref> [1] </ref>. The performance optimizations used in Panda include user-level threads that are tightly integrated with message reception (a full contextswitch is only required if the handler blocks) and the elimination of buffer fragmentation VLQFHflLWflLVflDOUHDG"flSURYLGHGflE"fl)0fl7KHflDXWKRUVflUHSRUWflDQfl53&flURXQGWULSflODWHQF"flLQfl3DQGDflRIflfl V*fltfl VflffRUfl~fi higher than a round-trip latency of FM.
Reference: 2. <author> H. Bal, M. F. Kaashoek, and A. Tanenbaum. Orca: </author> <title> A Language for Parallel Programming of Distributed Systems. </title> <journal> In IEEE Transactions on Software Engineering, </journal> <volume> 18(3) </volume> <pages> 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: objects virtual method table. 7KHflDXWKRUVflUHSRUWflDflURXQGWULSfl53&flODWHQF"flRIflDERXWflfl V*flRUflflLQVWUXFWLRQflFRXQWVfl7KHflVHQGflRYHUKHDGflffUHTXHVW and reply) accounts for 40 instructions, and the receiver consumes a total of 60 instructions for polling, extracting the message and managing data buffers, and dispatching the method. 7.3 Orca and Fast Messages Orca is a parallel language that supports coherent caching of shared-objects <ref> [2] </ref>. It is implemented on top of Panda, a communication library that provides RPC and totally ordered multicast at user-level.
Reference: 3. <author> B. Bershad, T. Anderson, E. Lazowska, and H. Levy. </author> <title> Lightweight Remote Procedure Call. </title> <journal> In ACM Transactions on Computer Systems (TOCS), </journal> <volume> 8(1) </volume> <pages> 37-55, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: In the late 80s, the main research was on tuning the RPC implementation for best performance across the network. In the early 90s, focus shifted from cross-machine RPC to cross-domain, or local RPC. Bershad et. al. <ref> [3] </ref> argued that in micro-kernel operating systems RPC calls occur predominantly between different protection domains (i.e. processes) within the same machine. Lightweight RPC (LRPC) [3] was motivated by this observation and specializes RPC for the local case by reducing the role of the kernel without compromising safety. <p> In the early 90s, focus shifted from cross-machine RPC to cross-domain, or local RPC. Bershad et. al. <ref> [3] </ref> argued that in micro-kernel operating systems RPC calls occur predominantly between different protection domains (i.e. processes) within the same machine. Lightweight RPC (LRPC) [3] was motivated by this observation and specializes RPC for the local case by reducing the role of the kernel without compromising safety.
Reference: 4. <author> B. Bershad, T. Anderson, E. Lazowska, and H. Levy. </author> <title> User-Level Interprocess Communication for Shared Memory Multiprocessors. </title> <journal> In ACM Transactions on Computer Systems (TOCS), </journal> <volume> 9(2) </volume> <pages> 175-198, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Simple Data and Control Transfer: LRPC uses a single kernel thread during control transfer, and a shared kernel stack between the caller and the callee for data transfer. On shared-memory multiprocessor machines, the User-Level RPC (URPC) <ref> [4] </ref> improves LRPC further by transferring the control between caller and callee through user-level threads and the data through shared memory, relying on the kernel only for processor allocation.
Reference: 5. <author> A. Birrel and G. Nelson. </author> <title> Implementing Remote Procedure Calls. </title> <journal> In ACM Transactions on Computer Systems (TOCS), </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: 1 Introduction Remote procedure call (RPC) <ref> [5] </ref> is widely used in distributed systems as the primary communication abstraction. In its most general form, an RPC specifies the data that is to be transferred and the remote operation that is to be performed with the data.
Reference: 6. <author> A. Birrel, G. Nelson, S. Owicki, and E. Wobber. </author> <title> Network Objects. </title> <booktitle> In Proceedings of the 14 th ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Asheville, NC, </address> <month> December </month> <year> 1993. </year>
Reference: 7. <author> R. Blumofe, C. Joerg, B. Kuszmaul, C. Leiserson, K. Randall, and Y. Zhou. Cilk: </author> <title> An Efficient Multithreaded Runtime System. </title> <booktitle> In Proceedings of 5 th ACM Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference: 8. <author> K. Chandy and C. Kesselman. </author> <title> Compositional C++: Compositional Parallel Programming. </title> <booktitle> In Proceedings of 6 th International Workshop in Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 124-144, </pages> <year> 1993. </year>
Reference-contexts: clientserver type of setting; it is more appropriate for meta-computing in a heterogeneous, large-scale environments because MPMD programs are a lot more loosely-coupled than those written in the SPMD style; and it encourages programmers to treat parallel computing software as components, promoting code reuse and the ability to compose programs <ref> [8] </ref>. As an alternative to RPC, lower-level messaging layers such as MPI [35] and PVM [32] can be used for MPMD programming and often achieve good performance. <p> For example, Nexus and Legion are both highly portable runtime systems that support a variety of user-level communication protocols and have been used as a compiler target for parallel languages such as CC++ <ref> [8] </ref>, Fortran-M [13] and Mentat [16]. Both of these systems are aimed for wide-area, large-scale parallel computing. These languages and systems implement RPC on top of user-level messaging layers such as MPI, PVM, or even TCP sockets.
Reference: 9. <author> CC. Chang, G. Czajkowski, C. Hawblitzel, and T. von Eicken. </author> <title> Low-Latency Communication on the IBM RISC System/6000 SP. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <address> Pittsburgh, PA, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: These implementations usually achieve a performance close to that of the raw hardware while hiding sophisticated architectural features of different kinds of network interfaces in state-of-the-art MPPs. AM have been ported to the CM-5 [34], Intel Paragon [21], Cray T3D [21], Meiko CS-2 [29], and IBM SP-2 <ref> [9] </ref>. 3 RPC Design Issues Traditional RPC introduces a number of issues into the communication layer that are absent from AM: method names must be resolved to entry point addresses, potentially complex arguments must be marshaled, and multiple threads of control must be supported. <p> This doesnt work because of possible collisions in the hash table. 3 The size of the S- and R-buffers is a compile-time constant (currently 16 Kbytes). It is not difficult to implement a data transmission protocol (as the one documented in <ref> [9] </ref>) to handle data transfers of arbitrary sizes. 10 To eliminate a data copy on the caller side, the caller can allocate an R-buffer and pass its address into the RPC message (add step 5.1). The callee can ship the return value directly into this buffer. <p> Message Reception and Scheduling Due to the high cost of software interrupts on message arrival on the IBM SP, message reception is based on polling that occurs on a node every time a message is sent <ref> [9] </ref>. In order to avoid deadlocks when there is no runnable thread, MRPC forks a polling thread one each processing node at initialization.
Reference: 10. <author> D. Culler, S. Goldstein, K. Schauser, and T. von Eicken. </author> <title> TAM A Compiler Controlled Threaded Abstract Machine. </title> <journal> Journal of Parallel and Distributed Computing (JPDC), </journal> <month> June </month> <year> 1993. </year>
Reference: 11. <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumeta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: As a result, AM has simpler semantics than traditional RPC, and achieves performance close to that of raw hardware. It has been ported to a variety of multi-computers [9,26,29] and used in the implementation of SPMD parallel languages such as Split-C <ref> [11] </ref> and CRL [18]. 2 Applications that benefit from the generality of a Multiple-Program-Multiple-Data (MPMD) model cannot use AM directly and instead require a full-blown RPC system. MPMD allows for multiple programs to dynamically create concurrently executing tasks that communicate with one another at any point in time.
Reference: 12. <author> B. Ford and J. Lepreau. </author> <title> Evolving Mach 3.0 to a Migrating Thread Model. </title> <booktitle> In Proceedings of Winter 1994 USENIX Conference, </booktitle> <address> San Francisco, CA, </address> <month> January </month> <year> 1994. </year>
Reference: 13. <author> I. Foster and K. Chandy. Fortran-M: </author> <title> A Language for Modular Parallel Programming. </title> <journal> Journal of Parallel and Distributed Computing (JPDC), </journal> <volume> 25(1), </volume> <year> 1994. </year>
Reference-contexts: For example, Nexus and Legion are both highly portable runtime systems that support a variety of user-level communication protocols and have been used as a compiler target for parallel languages such as CC++ [8], Fortran-M <ref> [13] </ref> and Mentat [16]. Both of these systems are aimed for wide-area, large-scale parallel computing. These languages and systems implement RPC on top of user-level messaging layers such as MPI, PVM, or even TCP sockets. While the implementation is tuned for performance, maintaining portability, flexibility, and heterogeneity is not questioned.
Reference: 14. <author> I. Foster, C. Kesselman, and S. Tuecke. </author> <title> The Nexus Approach for Integrating Multithreading and Communication. </title> <journal> Journal of Parallel and Distributed Computing (JPDC), </journal> <volume> 37, </volume> <year> 1996. </year> <month> 18 </month>
Reference-contexts: To date, implementations of RPC for MPMD parallel programming have used runtime systems (e.g. Nexus <ref> [14] </ref>, Legion [17]) that implement RPC on top of the above messaging layers and add some combination of dynamic task creation, load balancing, global name space, concurrency, and heterogeneity. <p> In Split-C, Total is just AM plus Runtime. The DCE threads time in Nexus is just an estimate since Foster et. al. <ref> [14] </ref> only report the total overhead on top of MPL.
Reference: 15. <author> S. Goldstein, K. Schauser, and D. Culler. </author> <title> Lazy Threads, Stacklets, and Synchronizers: Enabling Primitives for Compiling Parallel Languages. </title> <booktitle> In 3 rd Workshop on Languages, Compilers, and Runtime Systems for Scalable Computers, </booktitle> <year> 1995. </year>
Reference-contexts: Most threaded SPMD systems [7,10] minimize the threading costs by making threads run to completion to eliminate context switches, by performing custom stack management (e.g. using stacklets <ref> [15] </ref>, spaghetti stacks), or by reducing synchronization costs through custom code generation [15]. Others like Split-C take an even more radical approach offering only a single computation thread and rely on split-phase remote accesses to tolerate latencies. <p> Most threaded SPMD systems [7,10] minimize the threading costs by making threads run to completion to eliminate context switches, by performing custom stack management (e.g. using stacklets <ref> [15] </ref>, spaghetti stacks), or by reducing synchronization costs through custom code generation [15]. Others like Split-C take an even more radical approach offering only a single computation thread and rely on split-phase remote accesses to tolerate latencies. Optimistic Active Messages (OAM) [36] augments AM with threads, removing some of the restrictions in AM handlers.
Reference: 16. <author> A. Grimshaw. </author> <title> An Introduction to Parallel ObjectOriented Programming with Mentat, </title> <type> Technical Report 91-07, </type> <institution> University of Virginia, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: For example, Nexus and Legion are both highly portable runtime systems that support a variety of user-level communication protocols and have been used as a compiler target for parallel languages such as CC++ [8], Fortran-M [13] and Mentat <ref> [16] </ref>. Both of these systems are aimed for wide-area, large-scale parallel computing. These languages and systems implement RPC on top of user-level messaging layers such as MPI, PVM, or even TCP sockets. While the implementation is tuned for performance, maintaining portability, flexibility, and heterogeneity is not questioned.
Reference: 17. <author> A. Grimshaw, and W. Wulf. </author> <title> Legion - A View From 50,000 Feet. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <address> Los Alamitos, California, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: To date, implementations of RPC for MPMD parallel programming have used runtime systems (e.g. Nexus [14], Legion <ref> [17] </ref>) that implement RPC on top of the above messaging layers and add some combination of dynamic task creation, load balancing, global name space, concurrency, and heterogeneity.
Reference: 18. <author> K. Johnson, M. F. Kaashoek, and D. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 15 th ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Cooper Mountain, CO, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: As a result, AM has simpler semantics than traditional RPC, and achieves performance close to that of raw hardware. It has been ported to a variety of multi-computers [9,26,29] and used in the implementation of SPMD parallel languages such as Split-C [11] and CRL <ref> [18] </ref>. 2 Applications that benefit from the generality of a Multiple-Program-Multiple-Data (MPMD) model cannot use AM directly and instead require a full-blown RPC system. MPMD allows for multiple programs to dynamically create concurrently executing tasks that communicate with one another at any point in time. <p> First, the data transfer could be streamlined if the compiler can pre-allocate storage required for data marshalling. Second, the thread management cost could be reduced through code generation that uses special calling conventions to implement cactus stacks, stacklets <ref> [18] </ref>, or other innovative call stacks. However, our application-benchmark results suggest that future efforts should be aimed at eliminating the data marshalling overhead whenever possible, as in some applications it accounted for more than 50% (and as high as 90%) of the performance difference between CC++ and Split-C.
Reference: 19. <author> V. Karamcheti, and A. Chien. </author> <title> Concert Efficient Runtime Support for Concurrent ObjectOriented Programming Languages on Stock Hardware. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: The performance of RPC using Concert is documented in the context of two languages: Concurrent Aggregates (CA) <ref> [19] </ref> and ICC++ [20]. The round-trip time of a 2-word RPC in CA running on the CM-5 with 33 MHz Sparc1 QRGHVflLVflfl VflRUflt-flLQVWUXFWLRQflF"FOHV*flDERXWflflKLJKHUflWKDQflWKHflFRVWflRIflE"WHflDUUD"flWUDQVIHUflXVLQJflWKHfl&0~ Active Messages. Most of the overhead is due to activation frame creation, frame scheduling, and method name translation.
Reference: 20. <author> V. Karamcheti, J. Plevyak, and A. Chien. </author> <title> Runtime Mechanisms for Efficient Dynamic Multi-threading. </title> <journal> In Journal of Parallel and Distributed Computing (JPDC), </journal> <volume> 37(1) </volume> <pages> 21-40, </pages> <year> 1996. </year>
Reference-contexts: referenced by a global pointer and waiting for its completion): 0-Word Simple (no thread switches at the sender 4 The CC++ version of these applications is heavily based on the original Split-C implementations [23] to allow for a fair comparison. // Split-C definitions double lx; double *global gpY; double lA <ref> [20] </ref>; void *global gpA; // 0-Word N/A // 1-Word N/A // 2-Word N/A // 0-Word Atomic RPC atomic (foo, 0); // GP 2-Word Read lx = *gpY; // GP 2-Word Write *gpY = lx; // Bulk Read bulk_read (&lA,gpA,20*sizeof (double)); // Bulk Write bulk_write (gpA,&lA,20*sizeof (double)); // CC++ definitions double lx; <p> The performance of RPC using Concert is documented in the context of two languages: Concurrent Aggregates (CA) [19] and ICC++ <ref> [20] </ref>. The round-trip time of a 2-word RPC in CA running on the CM-5 with 33 MHz Sparc1 QRGHVflLVflfl VflRUflt-flLQVWUXFWLRQflF"FOHV*flDERXWflflKLJKHUflWKDQflWKHflFRVWflRIflE"WHflDUUD"flWUDQVIHUflXVLQJflWKHfl&0~ Active Messages. Most of the overhead is due to activation frame creation, frame scheduling, and method name translation.
Reference: 21. <author> A. Krishnamurthy, K. Schauser, C. Scheiman, R. Wang, D. Culler, and K. Yelick. </author> <title> Evaluation of Architectural Support for Global Address-Based Communication in Large-Scale Parallel Machines. </title> <booktitle> In Proceedings of the 7 th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VII), </booktitle> <address> Cambridge, MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: These implementations usually achieve a performance close to that of the raw hardware while hiding sophisticated architectural features of different kinds of network interfaces in state-of-the-art MPPs. AM have been ported to the CM-5 [34], Intel Paragon <ref> [21] </ref>, Cray T3D [21], Meiko CS-2 [29], and IBM SP-2 [9]. 3 RPC Design Issues Traditional RPC introduces a number of issues into the communication layer that are absent from AM: method names must be resolved to entry point addresses, potentially complex arguments must be marshaled, and multiple threads of control <p> These implementations usually achieve a performance close to that of the raw hardware while hiding sophisticated architectural features of different kinds of network interfaces in state-of-the-art MPPs. AM have been ported to the CM-5 [34], Intel Paragon <ref> [21] </ref>, Cray T3D [21], Meiko CS-2 [29], and IBM SP-2 [9]. 3 RPC Design Issues Traditional RPC introduces a number of issues into the communication layer that are absent from AM: method names must be resolved to entry point addresses, potentially complex arguments must be marshaled, and multiple threads of control must be supported.
Reference: 22. <author> J. Liedtke. </author> <title> Improving IPC by Kernel Design. </title> <booktitle> In Proceedings of the 14 th ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Asheville, NC, </address> <month> December </month> <year> 1993. </year>
Reference: 23. <author> B-H. Lim, CC. Chang, G. Czajkowski, and T. von Eicken. </author> <title> Performance Implications of Communication Mechanisms in All-Software Global Address Space Systems. </title> <booktitle> In Proceedings of the 6 th ACM Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> Las Vegas, NV, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Ping-Pong measure the r/t time of a null RPC (calling a null method of a remote object referenced by a global pointer and waiting for its completion): 0-Word Simple (no thread switches at the sender 4 The CC++ version of these applications is heavily based on the original Split-C implementations <ref> [23] </ref> to allow for a fair comparison. // Split-C definitions double lx; double *global gpY; double lA [20]; void *global gpA; // 0-Word N/A // 1-Word N/A // 2-Word N/A // 0-Word Atomic RPC atomic (foo, 0); // GP 2-Word Read lx = *gpY; // GP 2-Word Write *gpY = lx;
Reference: 24. <author> N. Madsen. </author> <title> Divergence Preserving Discrete Surface Integral Methods for Maxwells Curl Equations Using Non-Orthogonal Unstructured Grids. </title> <type> Technical Report 92-04, </type> <institution> RIACS, </institution> <month> February </month> <year> 1992. </year>
Reference: 25. <author> S. OMalley, T. Proebsting, and A. Montz. </author> <title> USC: A Universal Stub Compiler. </title> <booktitle> In Proceedings of the Symposium on Communication Architectures and Protocols (SIGCOMM), </booktitle> <address> London, UK, </address> <month> August </month> <year> 1994. </year>
Reference: 26. <author> R. Martin. HPAM: </author> <title> An Active Message Layer for a Network of Workstations. </title> <booktitle> In Proceedings of Hot Interconnects II, </booktitle> <address> Palo Alto CA, </address> <month> August </month> <year> 1994. </year>
Reference: 27. <author> S. Pakin, M. Lauria, and A. Chien. </author> <title> High Performance Messaging on Workstations: Illinois Fast Messages (FM) for Myrinet. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <address> San Diego, CA, </address> <month> November </month> <year> 1995. </year>
Reference: 28. <author> J. Plevyak, V. Karamcheti, X. Zhang, and A. Chien. </author> <title> A Hybrid Execution Model for Fine-Grained Languages on Distributed Memory Multicomputers. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: To implement a fast RPC, OAM optimistically executes the handler code on the stack; the handler is aborted and restarted on a separate thread if it blocks (i.e. similar ideas are used in the Concert runtime system <ref> [28] </ref> and ABCL/f [33]).
Reference: 29. <author> K. E. Schauser and C. J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium (IPPS), </booktitle> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: These implementations usually achieve a performance close to that of the raw hardware while hiding sophisticated architectural features of different kinds of network interfaces in state-of-the-art MPPs. AM have been ported to the CM-5 [34], Intel Paragon [21], Cray T3D [21], Meiko CS-2 <ref> [29] </ref>, and IBM SP-2 [9]. 3 RPC Design Issues Traditional RPC introduces a number of issues into the communication layer that are absent from AM: method names must be resolved to entry point addresses, potentially complex arguments must be marshaled, and multiple threads of control must be supported.
Reference: 30. <author> M. Schroeder and M. Burrows. </author> <title> Performance of Firefly RPC. </title> <journal> In ACM Transactions on Computer Systems (TOCS), </journal> <volume> 8(1) </volume> <pages> 1-17, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Over the last decade, RPC has been extensively studied and optimized in operating systems. The focus gradually moved from the original inter-machine RPC <ref> [30] </ref> to local RPC [3,4,12,22] in which the role of the kernel is minimized during cross-domain calls on uniprocessor and shared-memory multiprocessor machines. The performance of RPC on such systems is well understood, and RPC is now fully incorporated into commercial operating systems such as Solaris or Windows NT. <p> The overhead introduced by these requirements is well understood and thoroughly reported by Schroeder and Burrows in the context of the DEC Firefly OS <ref> [30] </ref>. In the late 80s, the main research was on tuning the RPC implementation for best performance across the network. In the early 90s, focus shifted from cross-machine RPC to cross-domain, or local RPC.
Reference: 31. <author> J. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The fraction of edges that cross processor boundaries is varied from 10% to 100% in order to change the computation to communication ratio. 5.3 Water Water is an N-body molecular dynamics application taken from the SPLASH benchmark suite <ref> [31] </ref> that computes the forces and energies of a system of water molecules. The computation iterates over a number of steps, and each of which involves computing the intra- and inter-molecular forces for molecules contained in a cubical box, which runs in O (N 2 ) time. <p> The communication is staggered to avoid hot spots at the destination. Both versions are run with a data size of 1 million points. 5.5 Blocked LU Decomposition This application implements LU factorization of a dense matrix as described in the SPLASH benchmark suite <ref> [31] </ref>. The matrix is divided into blocks distributed among processors. Every step comprises three substeps: first, the pivot block (i,i) is factored by its owner; second, all processors which have blocks in the ith row or ith column obtain the updated pivot block; third, all internal blocks are updated.
Reference: 32. <author> V. Sunderam, G. Geist, J. Dongarra, and R.Manchek. </author> <title> The PVM Concurrent Computing System: Evolution, Experiences, and Trends. </title> <journal> Parallel Computing, </journal> <volume> 20(4), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: As an alternative to RPC, lower-level messaging layers such as MPI [35] and PVM <ref> [32] </ref> can be used for MPMD programming and often achieve good performance. However, these systems generally sacrifice the elegance (and IDL support) of an RPC system, and require that the receiver know when to expect incoming communication, limiting the range of MPMD applications that can be expressed conveniently.
Reference: 33. <author> K. Taura, A. Yonezawa. </author> <title> Fine-Grain Multithreading with Minimal Compiler Support A Cost Effective Approach to Implementing Efficient Multithreading Languages. </title> <booktitle> In Proceedings of ACM Conference on Programming Language Design and Implementation (PLDI), </booktitle> <address> Las Vegas, NV, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: To implement a fast RPC, OAM optimistically executes the handler code on the stack; the handler is aborted and restarted on a separate thread if it blocks (i.e. similar ideas are used in the Concert runtime system [28] and ABCL/f <ref> [33] </ref>). OAM still assumes an SPMD model and does not specifically address the communication bottlenecks when that assumption is no longer valid. 3.5 Message Reception and Dispatch A critical component of the communication latency is the queuing delay incurred by messages at the receiving end before they are serviced.
Reference: 34. <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19 th International Symposium in Computer Architecture (ISCA), </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Typically, message passing and distributed shared memory abstractions are used in a Single-Program-Multiple-Data (SPMD) model, where a fixed number of identical programs operate on their local data and communicate with one another at well defined points in time. The research on Active Messages (AM) <ref> [34] </ref> has shown that a simplified RPC can be implemented effectively in SPMD environments. <p> These implementations usually achieve a performance close to that of the raw hardware while hiding sophisticated architectural features of different kinds of network interfaces in state-of-the-art MPPs. AM have been ported to the CM-5 <ref> [34] </ref>, Intel Paragon [21], Cray T3D [21], Meiko CS-2 [29], and IBM SP-2 [9]. 3 RPC Design Issues Traditional RPC introduces a number of issues into the communication layer that are absent from AM: method names must be resolved to entry point addresses, potentially complex arguments must be marshaled, and multiple
Reference: 35. <author> D. Walker and J. Dongarra. </author> <title> MPI: A Standard Message Passing Interface. </title> <booktitle> Supercomputing, </booktitle> <volume> 12(1), </volume> <year> 1996. </year>
Reference-contexts: As an alternative to RPC, lower-level messaging layers such as MPI <ref> [35] </ref> and PVM [32] can be used for MPMD programming and often achieve good performance.
Reference: 36. <author> D. Wallach, W. Hsieh, K. Johnson, M. Kaashoek, and W. Weihl. </author> <title> Optimistic Active Messages: A Mechanism for Scheduling Communication with Computation. </title> <booktitle> In Proceedings of 5 th ACM Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Others like Split-C take an even more radical approach offering only a single computation thread and rely on split-phase remote accesses to tolerate latencies. Optimistic Active Messages (OAM) <ref> [36] </ref> augments AM with threads, removing some of the restrictions in AM handlers.
Reference: 37. <author> A. Yonezawa, </author> <title> editor. ABCL: An ObjectOriented Concurrent System Theory, Language, Programming, Implementation and Application. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In the case of ICC++ running on the Cray T3D (150 MHz DEC Alpha 21064 nodes) over Fast 0HVVDJHV*flWKHflURXQGWULSflODWHQF"flRIflDfl-ZRUGfl53&flLVflUHSRUWHGflDWfltfl V*flRIflZKLFKfltflff~fl VfiflLVflGXHflWRflWKHflVHQG DQGflUHFHLYHflRYHUKHDGVflLQfl)DVWfl0HVVDJHV*flDQGflflff~~fl VfiflLVflGXHflWRflWKHfl&RQFHUWflUXQWLPHflV"VWHP 7.2 ABCL ABCL is a parallel language that adopts a programming model based on concurrent objects that have their own thread of control and are message driven <ref> [37] </ref>. It has been implemented on an EM-4, a multi-computer with special hardware support for packet-driven data-flow architectures, and on an AP1000, a multi-computer with 25 MHz Sparc processors.
References-found: 37

