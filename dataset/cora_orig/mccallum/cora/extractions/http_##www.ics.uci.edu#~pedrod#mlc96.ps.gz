URL: http://www.ics.uci.edu/~pedrod/mlc96.ps.gz
Refering-URL: http://www.ics.uci.edu/~pedrod/
Root-URL: 
Email: pedrod@ics.uci.edu  pazzani@ics.uci.edu  
Title: Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier  
Author: Pedro Domingos Michael Pazzani 
Address: Irvine, CA 92717, U.S.A.  Irvine, CA 92717, U.S.A.  
Affiliation: Dept. Information and Computer Science University of California, Irvine  Dept. Information and Computer Science University of California, Irvine  
Abstract: The simple Bayesian classifier (SBC) is commonly thought to assume that attributes are independent given the class, but this is apparently contradicted by the surprisingly good performance it exhibits in many domains that contain clear attribute dependences. No explanation for this has been proposed so far. In this paper we show that the SBC does not in fact assume attribute independence, and can be optimal even when this assumption is violated by a wide margin. The key to this finding lies in the distinction between classification and probability estimation: correct classification can be achieved even when the probability estimates used contain large errors. We show that the previously-assumed region of optimality of the SBC is a second-order infinitesimal fraction of the actual one. This is followed by the derivation of several necessary and several sufficient conditions for the optimality of the SBC. For example, the SBC is optimal for learning arbitrary conjunctions and disjunctions, even though they violate the independence assumption. The paper also reports empirical evidence of the SBC's competitive performance in domains containing substantial degrees of attribute dependence. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Clark, P. & Boswell, R. </author> <year> (1991). </year> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proceedings of the Sixth European Working Session on Learning, </booktitle> <pages> (pp. 151-163), </pages> <address> Porto, Portugal. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The SBC was compared with state-of-the art representatives of three major approaches to classification learning: decision tree induction (C4.5 ( Quinlan, 1993 ) ), instance-based learning (PEBLS, ( Cost & Salzberg, 1993 ) ) and rule induction (CN2, <ref> ( Clark & Boswell, 1991 ) </ref> ). The default classifier, which assigns the most frequent class to all test examples, was also included. Twenty runs were conducted for each dataset, randomly selecting 2 3 of the data for training and the remainder for testing.
Reference: <author> Clark, P. & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-283. </pages>
Reference-contexts: This avoids losing potentially useful information. (For example, in medical domains, missing values often indicate that the doctors considered the corresponding tests unnecessary.) Null attribute probabilities P (v j jC i ) were replaced by P (C i )=e, where e is the number of training examples, as done in <ref> ( Clark & Niblett, 1989 ) </ref> and elsewhere.
Reference: <author> Cost, S. & Salzberg, S. </author> <year> (1993). </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 57-78. </pages>
Reference-contexts: The SBC was compared with state-of-the art representatives of three major approaches to classification learning: decision tree induction (C4.5 ( Quinlan, 1993 ) ), instance-based learning (PEBLS, <ref> ( Cost & Salzberg, 1993 ) </ref> ) and rule induction (CN2, ( Clark & Boswell, 1991 ) ). The default classifier, which assigns the most frequent class to all test examples, was also included.
Reference: <author> Dougherty, J, Kohavi, R, & Sahami, M. </author> <year> (1995). </year> <title> Supervised and unsupervised discretization of continuous features. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> (pp. 194-202), </pages> <address> Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For the SBC, numeric values were discretized into ten equal-length intervals (or one per observed value, whichever was least). This has been found to give good results, more so than assuming normal distributions, as is often done in the pattern recognition literature <ref> ( Dougherty, Kohavi & Sahami, 1995 ) </ref> . Missing values were treated as having the value "?", at both training and testing times.
Reference: <author> Duda, R. O. & Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference-contexts: 1 THE SIMPLE BAYESIAN CLASSIFIER Bayes' theorem tells us how to optimally predict the class of a previously unseen example, given a training sample <ref> ( Duda & Hart, 1973 ) </ref> . <p> In the next section we examine the general case and formalize this result. 4 LOCAL OPTIMALITY We begin with some necessary definitions. Definition 1 The Bayes rate for an example is the lowest error rate achievable by any classifier on that example <ref> ( Duda & Hart, 1973 ) </ref> . Definition 2 A classifier is locally optimal for a given example iff its error rate on that example is equal to the Bayes rate. <p> Then, by taking the logarithm of Eq. 1, the SBC is equivalent to a linear machine <ref> ( Duda & Hart, 1973 ) </ref> whose discriminant function for class C i is log P (C i ) + P j;k log P (A j = v jk jC i ) b jk (i.e., the weight of each Boolean feature is the log-probability of the corre sponding attribute value given <p> For example, it narrowly fails on the concept 3-of-7 (i.e., the concept composed of examples where at least 3 of 7 Boolean attributes are true) ( Kohavi, 1995 ) . Thus in Boolean domains the SBC's range of optimality is a subset of the perceptron's <ref> ( Duda & Hart, 1973 ) </ref> . However, in numeric domains the SBC is not restricted to linearly separable problems; for example, if classes are normally distributed, nonlinear boundaries and multiple disconnected regions can arise, and the SBC is able to identify them (see ( Duda & Hart, 1973 ) ). <p> is a subset of the perceptron's <ref> ( Duda & Hart, 1973 ) </ref> . However, in numeric domains the SBC is not restricted to linearly separable problems; for example, if classes are normally distributed, nonlinear boundaries and multiple disconnected regions can arise, and the SBC is able to identify them (see ( Duda & Hart, 1973 ) ). 5.2 SUFFICIENT CONDITIONS In this section we establish the SBC's optimality for some common concept classes.
Reference: <author> Holte, R. C. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 63-91. </pages>
Reference-contexts: Overall the SBC is quite competitive with the other approaches. This is a remarkably good result for such a simple and apparently limited classifier. However, it can be due to the datasets themselves representing "easy" concepts <ref> ( Holte, 1993 ) </ref> , and does not by itself disprove the notion that the SBC relies on the assumption of attribute independence. To investigate this, we need to measure the degree of attribute dependence in the data in some way.
Reference: <author> Kohavi, R. </author> <year> (1995). </year> <title> Wrappers for Performance Enhancement and Oblivious Decision Graphs. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stan-ford University, Stanford, </institution> <address> CA. </address>
Reference-contexts: For the SBC, numeric values were discretized into ten equal-length intervals (or one per observed value, whichever was least). This has been found to give good results, more so than assuming normal distributions, as is often done in the pattern recognition literature <ref> ( Dougherty, Kohavi & Sahami, 1995 ) </ref> . Missing values were treated as having the value "?", at both training and testing times. <p> For example, it narrowly fails on the concept 3-of-7 (i.e., the concept composed of examples where at least 3 of 7 Boolean attributes are true) <ref> ( Kohavi, 1995 ) </ref> . Thus in Boolean domains the SBC's range of optimality is a subset of the perceptron's ( Duda & Hart, 1973 ) .
Reference: <author> Kononenko, I. </author> <year> (1991). </year> <title> Semi-naive Bayesian classifier. </title> <booktitle> In Proceedings of the Sixth European Working Session on Learning, </booktitle> <pages> (pp. 206-219), </pages> <editor> Porto, Por-tugal. </editor> <publisher> Springer-Verlag. </publisher>
Reference: <author> Langley, P. </author> <year> (1993). </year> <title> Induction of recursive Bayesian classifiers. </title> <booktitle> In Proceedings of the Eighth European Conference on Machine Learning, </booktitle> <pages> (pp. 153-164), </pages> <address> Vienna, Austria. </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Langley, P, Iba, W, & Thompson, K. </author> <year> (1992). </year> <title> An analysis of Bayesian classifiers. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 223-228), </pages> <address> San Jose, CA. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994). </year> <title> Induction of selective Bayesian classifiers. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> (pp. 399-406), </pages> <address> Seattle, WA. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1995). </year> <title> UCI repository of machine learning databases. Machine-readable data repository, </title> <institution> Department of Information and Computer Science, University of Califor-nia at Irvine, </institution> <address> Irvine, CA. </address>
Reference-contexts: learning conjunctions and disjunctions. 2 EMPIRICAL EVIDENCE In order to investigate the SBC's performance compared to that of other classifiers, and relate it to the degree of attribute dependence in the data, an empirical study was carried out on a large and varied selection of datasets from the UCI repository <ref> ( Murphy & Aha, 1995 ) </ref> . For the SBC, numeric values were discretized into ten equal-length intervals (or one per observed value, whichever was least).
Reference: <author> Pazzani, M. </author> <year> (1995). </year> <title> Searching for attribute dependencies in Bayesian classifiers. </title> <booktitle> In Preliminary Papers of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> (pp. 424-429), </pages> <address> Fort Lauderdale, FL. </address> <booktitle> Society for Artificial Intelligence and Statistics. </booktitle>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The SBC was compared with state-of-the art representatives of three major approaches to classification learning: decision tree induction (C4.5 <ref> ( Quinlan, 1993 ) </ref> ), instance-based learning (PEBLS, ( Cost & Salzberg, 1993 ) ) and rule induction (CN2, ( Clark & Boswell, 1991 ) ). The default classifier, which assigns the most frequent class to all test examples, was also included.
Reference: <author> Rachlin, J, Kasif, S, Salzberg, S, & Aha, D. W. </author> <year> (1994). </year> <title> Towards a better understanding of memory-based reasoning systems. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> (pp. 242-250), </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Wan, S. J. & Wong, S. K. M. </author> <year> (1989). </year> <title> A measure for concept dissimilarity and its applications in machine learning. </title> <booktitle> In Proceedings of the International Conference on Computing and Information, </booktitle> <pages> (pp. 267-273), </pages> <address> Toronto, Canada. </address> <publisher> North-Holland. </publisher>
References-found: 16

