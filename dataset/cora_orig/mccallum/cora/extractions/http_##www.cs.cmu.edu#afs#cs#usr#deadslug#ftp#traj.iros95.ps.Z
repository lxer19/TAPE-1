URL: http://www.cs.cmu.edu/afs/cs/usr/deadslug/ftp/traj.iros95.ps.Z
Refering-URL: http://www.cs.cmu.edu/afs/cs/usr/deadslug/ftp/publications.html
Root-URL: 
Title: Tactile Gestures for Human/Robot Interaction  
Author: Richard M. Voyles, Jr. Pradeep K. Khosla 
Date: 7  
Address: Pittsburgh, PA 15213  
Affiliation: Robotics Ph.D. Program Dept. of Electrical and Computer Engineering Carnegie Mellon University  
Abstract: Gesture-Based Programming is a new paradigm to ease the burden of programming robots. By tapping in to the users wealth of experience with contact transitions, compliance, uncertainty and operations sequencing, we hope to provide a more intuitive programming environment for complex, real-world tasks based on the expressiveness of non-verbal communication. A requirement for this to be accomplished is the ability to interpret gestures to infer the intentions behind them. As a first step toward this goal, this paper presents an application of distributed perception for inferring a users intentions by observing tactile gestures. These gestures consist of sparse, inexact, physical nudges applied to the robots end effector for the purpose of modifying its trajectory in free space. A set of independent agents - each with its own local, fuzzified, heuristic model of a particular trajectory parameter - observes data from a wrist force/torque sensor to evaluate the gestures. The agents then independently determine the confidence of their respective findings and distributed arbitration resolves the interpretation through voting. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brooks, R.A., </author> <title> A Robust Layered Control System for a Mobile Robot, </title> <journal> IEEE Journal of Robotics and Automation, v.RA-2, </journal> <volume> n.1, </volume> <month> March </month> <year> 1986, </year> <pages> pp. 14-23. </pages>
Reference-contexts: Gesture-Based Programming is an extension of the successful aspects of lead-through teaching and also of the emerging learning by observation paradigms [6][9]. It extends these ideas by combining multi-agent <ref> [1] </ref> and skill-based [11] concepts to form encapsulated expertise which guides the recognition and segmentation of gestures during teaching and guides error recovery and exception handling of the robot during execution.
Reference: [2] <author> Gertz, M.W., D.B. Stewart and P.K. Khosla, </author> <title> A Software Architecture-Based Human-Machine Interface for Reconfigurable Sensor-Based Control Systems, </title> <booktitle> in Proc. of the 8th IEEE Symp. on Intelligent Control, </booktitle> <address> Chicago, IL, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: For gesture recognition, all agents execute as periodic tasks in a multiprocessor environment, some actually sharing the same processor. Dynamic reconfiguration of the agents during run-time is accomplished via a high-level GUI called Onika <ref> [2] </ref>, also developed at Carnegie Mellon. This allows the easy testing of agents alone and in variable configurations. We plan to implement dynamic self-reconfiguration and full gesture-based programming because we have found visual programming using Onika to be rather awkward for very complex, sensor-based tasks.
Reference: [3] <author> Hebb, </author> <title> D.O., The Organization of Behavior; a Neuropsycho logical Theory, </title> <address> New York, </address> <publisher> Wiley, </publisher> <year> 1949. </year>
Reference: [4] <author> Hirai, S. and T. Sato, </author> <title> Motion Understanding for World Model Management of Telerobot, </title> <booktitle> in Proc. of the 5th Interna tional Symp. on Robotics Research, </booktitle> <year> 1989, </year> <pages> pp. 5-12. </pages>
Reference-contexts: Key differences are their models are learned over repeated trials while ours are explicitly programmed and their system is monolithic versus our multi-agent approach. The work of Hirai and Sato <ref> [4] </ref> also carries some similarities in their symbolizers. Symbolizers are agents that help maintain an internal world model during telerobotic manipulation. They are a cross between our preprocessors and perceiving agents: they process and produce virtual sensor information.
Reference: [5] <author> Hoffman, </author> <title> D.D. and W.A. Richards, Parts of Recognition, </title> <journal> Cognition, v. </journal> <volume> 18, </volume> <pages> pp. 65-96, </pages> <year> 1984. </year>
Reference: [6] <author> Kang, S.B. and K. </author> <title> Ikeuchi, Grasp Recognition and Manipulative Motion Characterization from Human Hand Motion Sequences, </title> <booktitle> in Proc. of the 1994 IEEE International Conf. on Robotics and Automation, v. </booktitle> <volume> 2, </volume> <month> May, </month> <year> 1994, </year> <pages> pp. 1759-1764. </pages>
Reference: [7] <author> Kass, M., A. Witkin, and D. Terzopolous, Snakes: </author> <title> Active Contour Models, </title> <journal> International Journal of Computer Vision, v.1, </journal> <volume> n.4, </volume> <year> 1987, </year> <pages> pp. 321-331. </pages>
Reference-contexts: The symbolizers are significantly more limited, however, because they have minimal decision capability. We attempted two other monolithic approaches on a subset of the task, but neither was successful. Our first alternative was snake-based <ref> [7] </ref>. We assumed a snake in the shape of the current trajectory and tried to apply the measured force impulses to deform it. The deformed snake was continuously compared to all candidate shapes to see which it matched (in a minimum potential energy sense).
Reference: [8] <author> Khatib, O., </author> <title> The Operational Space Formulation in Robot Manipulator Control, </title> <booktitle> in Proc. of the 15th International Symp. on Industrial Robots, v. </booktitle> <volume> 1, </volume> <pages> pp. 165-172, </pages> <month> Sept. </month> <year> 1985. </year>
Reference-contexts: Peer interpretation agents (height, width, etc.) interpret the gestures, collectively arbitrate the most probable interpretation, and modify the execution parameters of the robot agent and its operational space controller <ref> [8] </ref>. 4.2 Interpretation agents The interpretation agents consist of a two-stage combination of characteristics from both fuzzy and neural systems. State information associated with each gesture is represented by fuzzy variables but the final confidence value is computed by a biological neuron-like accumulator.

References-found: 8

