URL: http://www-eksl.cs.umass.edu/papers/tai95-rsa.ps
Refering-URL: http://eksl-www.cs.umass.edu/publications.html
Root-URL: 
Email: stamant@cs.umass.edu, kuwata@rd.nttdata.jp, cohen@cs.umass.edu  
Title: Monitoring Progress with Dynamic Programming Envelopes  
Author: Robert St. Amant, Yoshitaka Kuwata Paul R. Cohen 
Address: Amherst, MA 01003-4610  
Affiliation: Computer Science Dept., LGRC University of Massachusetts  
Abstract: Envelopes are a form of decision rule for monitoring plan execution. We describe one type, the DP envelope, that draws its decisions from a look-up table computed off-line by dynamic programming. Based on an abstract model of agent progress, DP envelopes let a developer approach execution monitoring as a problem independent of issues in agent design. We discuss the application of DP envelopes to a small transportation planning simulation, and discuss the issues that arise in an empirical analysis of the results. 
Abstract-found: 1
Intro-found: 1
Reference: [Barto89] <author> Barto A. G., Sutton R.S. and Watkins C.J.C.H, </author> <year> 1989. </year> <title> Learning and Sequential Decision Making, </title> <institution> Dept. of Computer Science Technical Report 89-95, University of Massachusetts, Amherst. </institution>
Reference: [Barto93] <author> Barto, A.G., Bradtke, S.J., Singh, S.P., </author> <year> 1993. </year> <title> Learning to Act using Real- Time Dynamic Programming. </title> <institution> Dept. of Computer Science Technical Report 93-02. University of Massachusetts, Amherst. </institution>
Reference-contexts: Finally, envelopes can be added to a system and tuned with a relatively small design effort. Thus envelopes can offer an attractive alternative to the development of a hand-built set of monitoring rules. Much progress has been made in theoretical areas relevant to DP envelopes <ref> [Barto93] </ref>. Our work aims toward an empirical evaluation of the benefits and limitations of DP envelopes, to some extent in contrast with a heuristic approach. We attempted not only to design envelopes for different environments, but also to explain and if possible predict their behavior under different conditions. <p> Dean and Wellman [Dean91] outline a relationship between reactive planning and control theory. Dynamic programming is described in the context of stochastic control of dynamical systems. Dean has also recently outlined a scheme for using a related notion of envelopes for planning in uncertain environments [Dean93]. Barto and others <ref> [Barto93] </ref> address the area of learning reactive plans automatically. <p> It is possible, for example, to explore the search space selectively, which allows us to increase the complexity of the model. We can also learn a policy without an explicit model <ref> [Barto93, Watkins92] </ref>. These and other variations require that we change the basic approach and address other issues. The DP envelopes approach to reactive planning can also be compared to Schopper's universal plans. Tradeoffs between deliberation at runtime and universal plans have been discussed elsewhere, in particular in [Ginsberg89a, Chapman89, Ginsberg89b].
Reference: [Beetz92] <author> Beetz, and McDermott, D., </author> <year> 1992. </year> <title> Declarative Goals in Reactive Plans. </title> <booktitle> In Artificial Intelligence Planning Systems. </booktitle> <publisher> Morgan Kaufmann. </publisher> <pages> 3-12. </pages>
Reference-contexts: Adjusting plans to respond to environmental change is an issue addressed by research in reactive planning. There is currently no widely accepted theory of how to generate reactive plans. One approach is suggested by <ref> [Beetz92] </ref>: ": : : Most (reactive) plans will be made up of large canned segments retrieved from a library, pasted together, and debugged." In some circumstances, however, one can apply a more principled approach.
Reference: [Bertsekas87] <author> Bertsekas, </author> <year> 1987. </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall. </publisher>
Reference: [Boddy91] <author> Boddy, M., </author> <year> 1991. </year> <title> Any Time Problem Solving Using Dynamic Programming. </title> <booktitle> In Proceedings AAAI-91. </booktitle> <publisher> AAAI. </publisher>
Reference: [Chapman89] <author> Chapman D., </author> <year> 1989. </year> <title> Penguins Can Make Cake. </title> <journal> AI Magazine, </journal> <month> Winter </month> <year> 1989, </year> <pages> 45-50. </pages>
Reference-contexts: These and other variations require that we change the basic approach and address other issues. The DP envelopes approach to reactive planning can also be compared to Schopper's universal plans. Tradeoffs between deliberation at runtime and universal plans have been discussed elsewhere, in particular in <ref> [Ginsberg89a, Chapman89, Ginsberg89b] </ref>. A DP envelope is not a universal plan, however, in that envelopes may not be solely responsible for the decisions they make, but may act in a larger context. 6 Conclusion We return now to our three questions.
Reference: [Cohen89] <author> Cohen, P.R.; Greenberg, M.L.; Hart, D.M.; and Howe, A.E., </author> <year> 1989. </year> <title> Trial by Fire: Understanding the Design Requirements for Agents in Complex Environments. </title> <journal> AI Magazine, </journal> <volume> 10(3) </volume> <pages> 32-48. </pages>
Reference-contexts: All ship and cargo activities proceed autonomously; the key decision required from the envelope is whether to allocate more or fewer ships on each simulated day. 3 Envelope Construction Envelopes were originally developed in the Phoenix planning testbed as a means of monitoring progress in a task <ref> [Cohen89, Hart90] </ref>. Imagine a plan that requires an agent to make 10 units of progress (distance toward a goal, for example) in 100 time units. Figure 1 diagrams the progress of the agent toward its goal.
Reference: [Cohen92] <author> Cohen, P.R.; St. Amant, R. and Hart, D.M., </author> <year> 1992. </year> <title> Early warnings of plan failure, false positives and envelopes: Experiments and a model. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society. </booktitle> <publisher> Lawrence Erlbaum Associates. </publisher> <pages> 773-778. </pages>
Reference-contexts: We have developed a technique for building decision rules, called envelopes, that monitor plan execution <ref> [Hart90, Cohen92, Hansen92] </ref>. In this paper we discuss the role of envelopes in reactive planning, the advantages and disadvantages of their application, and conclusions we have drawn from experimental evaluation. An envelope monitors a task to determine if and when steps should be taken to improve progress in the task. <p> The curved line represents the distance between the agent and its goal at each point in time. The boundary of the shaded region is an envelope, specifically a slack-time envelope, for this task <ref> [Cohen92] </ref>. When the agent crosses this boundary, the envelope violates, or signals a failure, which typically requires some modification to a plan [Howe93, Howe95]. The slack-time aspect of the envelope refers to an initial period of time during which no failure predictions are made.
Reference: [Cormen90] <author> Cormen, Leiserson, and Rivest, </author> <year> 1990. </year> <title> Introduction to Algorithms. </title> <publisher> Morgan Kauf-mann. </publisher>
Reference: [Dean91] <author> Dean, T.L., and Wellman, </author> <title> M.P., 1991. Planning and Control. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In further work [Hansen93], they showed that envelopes can be learned using reinforcement learning. Reactive planning has been considered in many other contexts as well. Dean and Wellman <ref> [Dean91] </ref> outline a relationship between reactive planning and control theory. Dynamic programming is described in the context of stochastic control of dynamical systems. Dean has also recently outlined a scheme for using a related notion of envelopes for planning in uncertain environments [Dean93].
Reference: [Dean93] <author> Dean, T.L., Kaelbling, L.P., Kirman, J., and Nicholson, A., </author> <year> 1993. </year> <title> Planning With Deadlines in Stochastic Domains. </title> <booktitle> In Proceedings AAAI-93. </booktitle> <publisher> AAAI. </publisher>
Reference-contexts: Dean and Wellman [Dean91] outline a relationship between reactive planning and control theory. Dynamic programming is described in the context of stochastic control of dynamical systems. Dean has also recently outlined a scheme for using a related notion of envelopes for planning in uncertain environments <ref> [Dean93] </ref>. Barto and others [Barto93] address the area of learning reactive plans automatically.
Reference: [Ginsberg89a] <author> Ginsberg M.L., </author> <year> 1989a. </year> <title> Universal Planning: An (Almost) Universal Bad Idea. </title> <journal> AI Magazine, </journal> <month> Winter </month> <year> 1989. </year> <pages> 40-44. </pages>
Reference-contexts: These and other variations require that we change the basic approach and address other issues. The DP envelopes approach to reactive planning can also be compared to Schopper's universal plans. Tradeoffs between deliberation at runtime and universal plans have been discussed elsewhere, in particular in <ref> [Ginsberg89a, Chapman89, Ginsberg89b] </ref>. A DP envelope is not a universal plan, however, in that envelopes may not be solely responsible for the decisions they make, but may act in a larger context. 6 Conclusion We return now to our three questions.
Reference: [Ginsberg89b] <author> Ginsberg M.L., </author> <year> 1989b. </year> <title> Ginsberg Replies to Chapman and Schoppers Universal Planning Research: A Good or Bad Idea ? AI Magazine, </title> <booktitle> Winter 1989. </booktitle> <pages> 61-62. </pages>
Reference-contexts: These and other variations require that we change the basic approach and address other issues. The DP envelopes approach to reactive planning can also be compared to Schopper's universal plans. Tradeoffs between deliberation at runtime and universal plans have been discussed elsewhere, in particular in <ref> [Ginsberg89a, Chapman89, Ginsberg89b] </ref>. A DP envelope is not a universal plan, however, in that envelopes may not be solely responsible for the decisions they make, but may act in a larger context. 6 Conclusion We return now to our three questions.
Reference: [Hansen92] <author> Hansen, E.A. and Cohen, P.R., </author> <year> 1992. </year> <title> Learning a decision rule for monitoring tasks with deadlines. </title> <institution> Dept. of Computer Science Technical Report 92-80, University of Massachusetts, Amherst. </institution> <month> 11 </month>
Reference-contexts: We have developed a technique for building decision rules, called envelopes, that monitor plan execution <ref> [Hart90, Cohen92, Hansen92] </ref>. In this paper we discuss the role of envelopes in reactive planning, the advantages and disadvantages of their application, and conclusions we have drawn from experimental evaluation. An envelope monitors a task to determine if and when steps should be taken to improve progress in the task. <p> If we extend the range of the envelope's decision choices, we have a form of reactive plan. One kind of envelope, called a DP envelope, treats execution monitoring as a sequential decision problem <ref> [Hansen92] </ref>. Dynamic programming (DP) is used to approximate an optimal solution to the problem of when to modify a plan or warn of impending plan failure. Hansen and Cohen have also applied DP to the combined problem of when to monitor as well as modify a plan. <p> Simple applications include matrix chain multiplication and longest common subsequence search, i.e., problems that contain identical recurring subproblems. Monitoring can be treated as a sequential decision problem <ref> [Hansen92] </ref>, in that an optimal monitoring policy must take into account the future ramifications of each of its decisions. Dynamic programming is well-suited to such problems. <p> It may be small, but it may also be very large. Using an envelope in a situation where costs may change can be attractive for this reason. 5 Related Work This work has been most strongly influenced by the results of Hansen and Cohen <ref> [Hansen92] </ref>, who showed that the envelope problem was amenable to dynamic programming. In further work [Hansen93], they showed that envelopes can be learned using reinforcement learning. Reactive planning has been considered in many other contexts as well. Dean and Wellman [Dean91] outline a relationship between reactive planning and control theory.
Reference: [Hansen93] <author> Hansen, E. and Cohen, P.R., </author> <year> 1993. </year> <title> Learning Monitoring Strategies to Compensate for Model Uncertainty. </title> <booktitle> Working Notes of the AAAI-93 Workshop on Learning Action Models. </booktitle> <publisher> AAAI. </publisher>
Reference-contexts: Using an envelope in a situation where costs may change can be attractive for this reason. 5 Related Work This work has been most strongly influenced by the results of Hansen and Cohen [Hansen92], who showed that the envelope problem was amenable to dynamic programming. In further work <ref> [Hansen93] </ref>, they showed that envelopes can be learned using reinforcement learning. Reactive planning has been considered in many other contexts as well. Dean and Wellman [Dean91] outline a relationship between reactive planning and control theory. Dynamic programming is described in the context of stochastic control of dynamical systems.
Reference: [Hart90] <author> Hart, D.M.; Anderson, S.D.; and Cohen, P.R., </author> <year> 1990. </year> <title> Envelopes as a Vehicle for Improving the Efficiency of Plan Execution. In Proceedings of the Workshop on Innovative Approaches to Planning, Scheduling, and Control. </title> <editor> K. Sycara (Ed.). </editor> <publisher> Morgan Kaufmann. </publisher> <pages> 71-76. </pages>
Reference-contexts: We have developed a technique for building decision rules, called envelopes, that monitor plan execution <ref> [Hart90, Cohen92, Hansen92] </ref>. In this paper we discuss the role of envelopes in reactive planning, the advantages and disadvantages of their application, and conclusions we have drawn from experimental evaluation. An envelope monitors a task to determine if and when steps should be taken to improve progress in the task. <p> All ship and cargo activities proceed autonomously; the key decision required from the envelope is whether to allocate more or fewer ships on each simulated day. 3 Envelope Construction Envelopes were originally developed in the Phoenix planning testbed as a means of monitoring progress in a task <ref> [Cohen89, Hart90] </ref>. Imagine a plan that requires an agent to make 10 units of progress (distance toward a goal, for example) in 100 time units. Figure 1 diagrams the progress of the agent toward its goal.
Reference: [Howe93] <author> Howe, Adele E., </author> <year> 1993. </year> <title> Accepting the Inevitable: The Role of Failure Recovery in the Design of Planners. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Massachusetts, Amherst. </institution>
Reference-contexts: The boundary of the shaded region is an envelope, specifically a slack-time envelope, for this task [Cohen92]. When the agent crosses this boundary, the envelope violates, or signals a failure, which typically requires some modification to a plan <ref> [Howe93, Howe95] </ref>. The slack-time aspect of the envelope refers to an initial period of time during which no failure predictions are made. The agent is thus allowed to fall slightly behind at first, under the assumption that it can make up for lost time later.
Reference: [Howe95] <author> Howe, Adele E. and Cohen, P.R., </author> <year> 1995. </year> <title> Understanding Planner Behavior. </title> <journal> AI Journal. </journal> <note> To appear. </note>
Reference-contexts: The boundary of the shaded region is an envelope, specifically a slack-time envelope, for this task [Cohen92]. When the agent crosses this boundary, the envelope violates, or signals a failure, which typically requires some modification to a plan <ref> [Howe93, Howe95] </ref>. The slack-time aspect of the envelope refers to an initial period of time during which no failure predictions are made. The agent is thus allowed to fall slightly behind at first, under the assumption that it can make up for lost time later.
Reference: [Kuwata92] <author> Kuwata, Y., Hart, D.M. and Cohen, P.R., </author> <year> 1992. </year> <title> Steering the Execution of a Large-Scale Planning and Scheduling System. </title> <booktitle> Working Notes of the AAAI-92 SIG-MAN Workshop on Knowledge-Based Production Planning, Scheduling, and Control. AAAI. </booktitle> <pages> 62-72. </pages>
Reference-contexts: We describe a method by which we can measure the effectiveness of envelopes. 2 Example Domain Drawing on a background of work in transportation planning <ref> [Kuwata92] </ref>, we designed a simple model of the shipment of cargo between sea ports. The task is to transfer a fixed amount of cargo from a single source to a single destination.
Reference: [Watkins92] <author> Watkins, C.J.C.H. and Dayan, P., </author> <year> 1992. </year> <title> Q-learning. </title> <booktitle> Machine Learning 8, </booktitle> <pages> 279-292. 12 </pages>
Reference-contexts: It is possible, for example, to explore the search space selectively, which allows us to increase the complexity of the model. We can also learn a policy without an explicit model <ref> [Barto93, Watkins92] </ref>. These and other variations require that we change the basic approach and address other issues. The DP envelopes approach to reactive planning can also be compared to Schopper's universal plans. Tradeoffs between deliberation at runtime and universal plans have been discussed elsewhere, in particular in [Ginsberg89a, Chapman89, Ginsberg89b].
References-found: 20

