URL: ftp://cse.ogi.edu/pub/neural/papers/kambhatlaLeen93.nips.ps.Z
Refering-URL: http://www.cse.ogi.edu/~tleen/
Root-URL: http://www.cse.ogi.edu
Title: In  Fast Non-Linear Dimension Reduction  
Author: Cowan, J.D., Tesauro, G., and Alspector, J. Nanda Kambhatla and Todd K. Leen 
Address: P.O. Box 91000 Portland, OR 97291-1000  
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute of Science Technology  
Note: (eds.) Advances in Neural Information Processing Systems 6, 1994. San Francisco, CA, Morgan Kaufmann Publishers.  
Abstract: We present a fast algorithm for non-linear dimension reduction. The algorithm builds a local linear model of the data by merging PCA with clustering based on a new distortion measure. Experiments with speech and image data indicate that the local linear algorithm produces encodings with lower distortion than those built by five layer auto-associative networks. The local linear algorithm is also more than an order of magnitude faster to train.
Abstract-found: 1
Intro-found: 1
Reference: <author> H. Bourlard and Y. Kamp. </author> <title> (1988) Auto-association by multilayer perceptrons and singular value decomposition. </title> <journal> Biological Cybernetics, </journal> <volume> 59 </volume> <pages> 291-294. </pages>
Reference-contexts: Cottrell and Metcalfe 1991) have used layered feedforward auto-associative networks with a bottle-neck middle layer to perform dimension reduction. It is well known that auto-associative nets with a single hidden layer cannot provide lower distortion than PCA <ref> (Bourlard and Kamp, 1988) </ref>. Recent work (e.g. Oja 1991) shows that five layer auto-associative networks can improve on PCA. These networks have three hidden layers (see Figure 1 (a)). The first and third hidden layers have non-linear response, and are referred to as the mapping layers.
Reference: <author> G. Cottrell and J. Metcalfe. </author> <year> (1991) </year> <month> EMPATH: </month> <title> Face, emotion, and gender recognition using holons. </title> <editor> In R. Lippmann, John Moody and D. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 564-571. </pages> <publisher> Morgan Kauffmann. </publisher>
Reference-contexts: This paper introduces a local linear technique for non-linear dimension reduction. We demonstrate its superiority to a recently proposed global non-linear technique, and show that both non-linear algorithms provide better performance than PCA for speech and image data. 2 Global Non-Linear Dimension Reduction Several researchers <ref> (e.g. Cottrell and Metcalfe 1991) </ref> have used layered feedforward auto-associative networks with a bottle-neck middle layer to perform dimension reduction. It is well known that auto-associative nets with a single hidden layer cannot provide lower distortion than PCA (Bourlard and Kamp, 1988). Recent work (e.g.
Reference: <author> D. DeMers and G. Cottrell. </author> <title> (1993) Non-linear dimensionality reduction. </title> <editor> In Giles, Hanson, and Cowan, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Each is a 64x64, 8-bit/pixel grayscale image. We extracted the first 50 principal components of each image and use these as our experimental data. This is the same data and preparation that DeMers and Cottrell used in their study of dimension reduction with five layer auto-associative nets <ref> (DeMers and Cottrell 1993) </ref>. They trained auto-associators to reduce the 50 principal components to 5 dimensions. We divided the data into a training set containing 120 images, a validation set (for architecture selection) containing 20 images and a test set containing 20 images. <p> We notice that a five layer net obtains the encoding with the least error for this data, but it takes a long time to train. Presumably more training data would improve the best VQPCA results. For comparison with DeMers and Cottrell's <ref> (DeMers and Cottrell 1993) </ref> work, we also conducted experiments training with all the data. The results are summarized 3 in Table 3 and Figure 3 shows two sample faces. Both non-linear techniques produce encodings with lower error than PCA, indicating significant non-linear structure in the data. <p> The results are summarized 3 in Table 3 and Figure 3 shows two sample faces. Both non-linear techniques produce encodings with lower error than PCA, indicating significant non-linear structure in the data. With the same data, and with a 5LN with 30 nodes in each mapping layer, DeMers <ref> (DeMers and Cottrell 1993) </ref> obtains a reconstruction error E norm 0:1317 4 .
Reference: <author> W. M. Fisher and G. R. Doddington. </author> <title> (1986) The DARPA speech recognition research database: specification and status. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <pages> pages 93-99, </pages> <address> Palo Alto, CA. </address>
Reference-contexts: The multistage architecture reduces the number of distance calculations and hence the training time for VQPCA (Gray 1984). 4.2 Dimension Reduction of Speech We used examples of the twelve monothongal vowels extracted from continuous speech drawn from the TIMIT database <ref> (Fisher and Doddington 1986) </ref>. Each input vector consists of 32 DFT coefficients (spanning the frequency range 0-4kHz), time-averaged over the central third of the utterance. We divided the data set into a training set containing 1200 vectors, a validation set containing 408 vectors and a test set containing 408 vectors.
Reference: <author> A. Gersho and R. M. Gray. </author> <title> (1992) Vector Quantization and Signal Compression. </title> <publisher> Kluwer academic publishers. </publisher>
Reference-contexts: Partition the input space using a VQ with the reconstruction distance mea sure 1 in (4). 2. Perform a local PCA (same as in steps 2 and 3 of the algorithm as described in section 3.1). 1 The VQ is trained using the (batch mode) generalized Lloyd's algorithm <ref> (Gersho and Gray, 1992) </ref> rather than an on-line competitive learning. This avoids recomputing the matrix P c (which depends on r c ) for each input vector. 4 Experimental Results We apply PCA, five layer networks (5LNs), and VQPCA to dimension reduction of speech and images.
Reference: <author> R. M. Gray. </author> <title> (1984) Vector quantization. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-29. </pages>
Reference-contexts: Training the VQ and performing the local PCA are very fast relative to training a five layer network. The training time is dominated by the distance computations for the competitive learning. This computation can be speeded up significantly by using a multi-stage architecture for the VQ <ref> (Gray 1984) </ref>. 3.2 Projection partitioning The VQPCA algorithm as described above is not optimal because the clustering is done independently of the PCA projection. The goal is to minimize the expected error in reconstruction (2). <p> For the VQPCA with Euclidean distance, clustering was implemented using standard VQ (VQPCA-Eucl) and multistage quantization (VQPCA-MS-E). The multistage architecture reduces the number of distance calculations and hence the training time for VQPCA <ref> (Gray 1984) </ref>. 4.2 Dimension Reduction of Speech We used examples of the twelve monothongal vowels extracted from continuous speech drawn from the TIMIT database (Fisher and Doddington 1986). Each input vector consists of 32 DFT coefficients (spanning the frequency range 0-4kHz), time-averaged over the central third of the utterance.
Reference: <author> N. Kambhatla and T. K. Leen. </author> <title> (1993) Fast non-linear dimension reduction. </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <volume> Vol. 3, </volume> <pages> pages 1213-1218. </pages> <publisher> IEEE. </publisher>
Reference-contexts: Motivated by the desire to capture formant structure in the vowel encodings, we reduced the data from 32 to 2 dimensions. (Experiments on reduction to 3 dimensions gave similar results to those reported here <ref> (Kambhatla and Leen 1993) </ref>.) Table 1 gives the test set reconstruction errors and the training times. The VQPCA encodings have significantly lower reconstruction error than the global PCA or five layer nets. The best 5LNs have slightly lower reconstruction error than PCA, but are very slow to train.
Reference: <author> E. Oja. </author> <title> (1991) Data compression, feature extraction, and autoassociation in feed-forward neural networks. </title> <booktitle> In Artificial Neural Networks, </booktitle> <pages> pages 737-745. </pages> <publisher> Elsevier Science Publishers B.V. (North-Holland). </publisher>
Reference-contexts: Cottrell and Metcalfe 1991) have used layered feedforward auto-associative networks with a bottle-neck middle layer to perform dimension reduction. It is well known that auto-associative nets with a single hidden layer cannot provide lower distortion than PCA (Bourlard and Kamp, 1988). Recent work <ref> (e.g. Oja 1991) </ref> shows that five layer auto-associative networks can improve on PCA. These networks have three hidden layers (see Figure 1 (a)). The first and third hidden layers have non-linear response, and are referred to as the mapping layers.
Reference: <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <title> (1987) Numerical Recipes the Art of Scientific Computing. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge/New York. </address>
Reference-contexts: The distortion measure is the normalized reconstruction error: E norm = E recon E [ jjxjj 2 ] = fi E [ jjxjj 2 ] : 4.1 Model Construction The 5LNs were trained using three optimization techniques: conjugate gradient descent (CGD), the BFGS algorithm (a quasi-Newton method <ref> (Press et al 1987) </ref>), and stochastic gradient descent (SGD). In order to limit the space of architectures, the 5LNs have the same number of nodes in both of the mapping (second and fourth) layers.
References-found: 9

