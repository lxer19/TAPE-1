URL: http://www.cs.wustl.edu/~dmath/teach2.ps
Refering-URL: http://www.cs.wustl.edu/~dmath/teaching2.html
Root-URL: 
Email: dmath@cs.wustl.edu  Editor:  
Title: An Interactive Model of Teaching  
Author: H. DAVID MATHIAS 
Keyword: computational learning theory, teaching, query learning, DNF formulas  
Address: St. Louis, MO 63130  
Affiliation: Dept. of Computer Science, Washington University,  
Abstract: Previous teaching models in the learning theory community have been batch models. That is, in these models the teacher has generated a single set of helpful examples to present to the learner. In this paper we present an interactive model in which the learner has the ability to ask queries as in the query learning model of Angluin [1]. We show that this model is at least as powerful as previous teaching models. We also show that anything learnable with queries, even by a randomized learner, is teachable in our model. In all previous teaching models, all classes shown to be teachable are known to be efficiently learnable. An important concept class that is not known to be learnable is DNF formulas. We demonstrate the power of our approach by providing a deterministic teacher and learner for the class of DNF formulas. The learner makes only equivalence queries and all hypotheses are also DNF formulas. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: D. MATHIAS models. Lange and Wiehagen [20] examine learning pattern languages and show that this can be achieved with good examples. 3. Preliminaries The teaching model that we present in this paper is based on the model of learning with queries developed by Angluin <ref> [1] </ref>. In this model the learner's goal is to infer an unknown target concept f chosen from known concept class C. More precisely, C is a representation class, a set of representations of functions. Throughout this paper, however, we use representation class and concept class interchangeably. <p> Another consequence of the results in this section follows from the work of An-gluin <ref> [1] </ref> in which she gives an algorithm for learning pattern languages of length n. Her algorithm uses restricted superset queries and runs in time polynomial in n. <p> restricted, our learner can ignore the particular counterexample received and simply determine whether the counterexample is positive or negative. (For obvious reasons, it is not possible to simulate restricted superset queries using restricted equivalence queries.) The last result we discuss in this section also follows from a result of Angluin <ref> [1] </ref>. The "double sunflower" is a concept class defined by participants in the learning seminar at the University of California, Santa Cruz in the Fall of 1987. The class is defined as follows. Let N = 2 n for some given positive n.
Reference: 2. <author> Dana Angluin, </author> <year> 1994. </year> <type> Personal communication. </type>
Reference-contexts: We demonstrate a system implemented by the teacher and learner that allows collusion in the presence of this adversary. This construction is due to Dana Angluin <ref> [2] </ref>. 24 H. D. MATHIAS Let p be an n=2 bit prime. In polynomial time a randomized teacher can generate p with high probability. If the teacher is computationally unbounded then it can generate p with probability 1.
Reference: 3. <author> Dana Angluin and Martin Krikis. </author> <title> Learning with malicious membership queries and exceptions. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 57-66, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: It is easily seen that this model is robust against some types of noise. Specifically, the model can easily be extended to handle both incomplete membership queries [4] and malicious membership queries <ref> [3] </ref>. We briefly discuss the issue of noise in Section 7. 4.2. <p> Finally, we consider malicious membership queries as introduced by Angluin and Krikis <ref> [3] </ref>. In this model the membership queries are answered incorrectly at the discretion of an omniscient adversary. As with incomplete membership queries the noise is persistent.
Reference: 4. <author> Dana Angluin and Donna K. </author> <title> Slonim. Randomly fallible teachers: learning monotone DNF with an incomplete membership oracle. </title> <journal> Machine Learning, </journal> <volume> 14(1) </volume> <pages> 7-26, </pages> <year> 1994. </year>
Reference-contexts: A probabilistic teacher defines a distribution over the space of possible sequences of answers to the learner's queries. It is easily seen that this model is robust against some types of noise. Specifically, the model can easily be extended to handle both incomplete membership queries <ref> [4] </ref> and malicious membership queries [3]. We briefly discuss the issue of noise in Section 7. 4.2. <p> It seems unlikely, however, that this change increases the power of the model. The next change we consider to the model concerns allowing noise. Specifically we examine noise in the membership queries. Angluin and Slonim <ref> [4] </ref> introduced the model of incomplete membership queries in which any membership query can be answered "I don't know" independently at random. The only restriction is that the answers are persistent the answer given for a query the first time it is asked is given every time it is asked.
Reference: 5. <author> Martin Anthony, Graham Brightwell, Dave Cohen, and John Shawe-Taylor. </author> <title> On exact specification by examples. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 311-318. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: Aside from the work on formal models of teaching there has also been interest in complexity measures of various concept classes in existing learning models. Perhaps most general is the work of Heged-us [15], [16] who defines several general combinatorial measures on the complexity of teaching. Anthony, et.al. <ref> [5] </ref> consider subclasses of linearly separable boolean functions. They compute bounds on the size of the smallest sample with which only the target function is consistent.
Reference: 6. <author> Nader H. Bshouty. </author> <title> Exact learning via the monotone theory. </title> <booktitle> In 34th Annual Symposium on Foundations of Computer Science, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: We return to this point in Section 5, when we show the power of such interaction using the work of Bshouty <ref> [6] </ref>. When we discuss the desirability of preventing collusion we beg the question of what constitutes collusion. Collusion is difficult to define formally. Unfortunately, in an interactive model of teaching, it is even more difficult to prevent. <p> Relationship to Monotone Theory One of the most interesting recent results in learning theory research is the development of the monotone theory, and its application to the learning of decision trees, by Bshouty <ref> [6] </ref>. Bshouty defines a complexity measure for concept classes called the monotone dimension, denoted M dim (C) for concept class C.
Reference: 7. <author> Nader H. Bshouty, Richard Cleve, Sampath Kannan, and Christino Tamon. </author> <title> Oracles and queries that are sufficient for exact learning. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 130-139, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: While many subclasses of DNF have been shown to be learnable not much progress has been made for the general case. Two recent results illustrate the state-of-the-art in DNF learning. Bshouty et.al. <ref> [7] </ref> gave a randomized algorithm, using restricted subset and superset queries, to learn DNF. In the PAC model, Jackson [17] has given an algorithm using membership queries to learn DNF against the uniform distribution. <p> Superset queries are symmetric. Thus, even if the learner is randomized, a T I L pair can simulate either subset of superset queries using equivalence queries. Some interesting results follow from the above theorems. The first of these uses a result of Bshouty, Cleve, Kannan and Tamon <ref> [7] </ref> in which they show that DNF formulas and polynomial-size circuits are learnable by a randomized learner using only subset and superset queries. Corollary 2 DNF formulas and polynomial size circuits are T I L -teachable with a randomized learner that uses only equivalence queries. 14 H. D.
Reference: 8. <author> Nader H. Bshouty, Paul W. Goldberg, Sally A. Goldman, and H. David Mathias. </author> <title> Exact learning of discretized concepts. </title> <type> Technical Report WUCS-94-19, </type> <institution> Washington University, </institution> <year> 1994. </year>
Reference-contexts: The remainder of the algorithm generalizes trivially. A learning algorithm for this class, efficient only for constant values of d, is given by Bshouty et.al. <ref> [8] </ref>. 7. Variations of the Model In this section we briefly discuss several variants of our teaching model. The first of these allows the teacher to give multiple counterexamples to a single query. The next variant allows for incomplete or malicious membership queries. 22 H. D.
Reference: 9. <author> Rusi~ns Freivalds, Efim Kinber, and Rolf Wiehagen. </author> <title> Inference from good examples. </title> <journal> Theoretical Computer Science, </journal> <volume> 110 </volume> <pages> 131-144, </pages> <year> 1993. </year>
Reference-contexts: There has been some work on teaching in the inductive inference community. Though neither presents a complete model of teaching, both Freivalds, Kinber and Wiehagen <ref> [9] </ref> and Lange and Wiehagen [20] have examined inference from "good examples" chosen by a helpful teacher. By presenting the learner with a superset of the teaching set prepared by the teacher, encoding is prevented in both of these 6 H. D. MATHIAS models.
Reference: 10. <author> Paul W. Goldberg, Sally A. Goldman, and H. David Mathias. </author> <title> Learning unions of boxes with membership and equivalence queries. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <month> July </month> <year> 1993. </year> <title> AN INTERACTIVE MODEL OF TEACHING 27 </title>
Reference-contexts: In fact, even Read-Twice DNF have monotone dimensions that are exponential in the number of boolean variables in the domain. 6.3. A Geometric Extension Unions of d-dimensional, axis-parallel boxes in discretized d-dimensional space generalize DNF formulas. We use the notation of Goldberg, Goldman and Math-ias <ref> [10] </ref> to define the class formally. box d n denotes the class of axis-parallel boxes over f1; : : : ; ng d . So d represents the number of dimensions and n represents the number of discrete values that exist in each dimension.
Reference: 11. <author> S. Goldman and D. Mathias. </author> <title> Teaching a smarter learner. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 67-76. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year> <note> To appear, Journal of Computer and System Sciences. </note>
Reference-contexts: Finally, as illustrated above, there are environments in which the learner and teacher speak different languages and thus, encoding or programming are impossible and learning must be used. Goldman and Mathias <ref> [11] </ref> (GM) extended T/L pairs so that prevention of collusion does not reduce the model to teaching any consistent learner. In that model, the teacher prepares a teaching set a collection of labeled examples that allow the learner to infer the target concept. <p> These trusted bits allow the teacher to communicate a stopping condition or a size parameter of the target. When trusted bits are allowed, they show that any class that is learnable is teachable in their model. Our model is derived from that of Goldman and Mathias <ref> [11] </ref>. They developed a model that pairs teachers and learners but prevents collusion without forcing the teacher to teach any consistent learner. In their model the teacher constructs a teaching set designed to teach optimally, to a particular learner, the target concept. <p> Let h represent the learner's hypothesis. A learning algorithm achieves exact identification of a concept class if for all instances x 2 X , h (x) = f (x). The teaching model of Goldman and Mathias <ref> [11] </ref> is important to this work.
Reference: 12. <author> S. A. Goldman and M. J. Kearns. </author> <title> On the complexity of teaching. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50(1) </volume> <pages> 20-31, </pages> <year> 1995. </year>
Reference-contexts: Since the introduction of the teacher-directed model, several interesting models have been proposed to study the complexity of teaching. The first formal model of teaching was introduced by Goldman and Kearns <ref> [12] </ref>. In this model they defined the teaching complexity as the minimum number of examples that a teacher must present to any consistent learner to enable the learner to exactly identify the target concept.
Reference: 13. <author> S. A. Goldman, R. L. Rivest, and R. E. Schapire. </author> <title> Learning binary relations and total orders. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22(5) </volume> <pages> 1006-1034, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: All hypotheses are DNF formulas. In this section we also discuss an extension of our algorithms to an interesting geometric class that generalizes DNF formulas. In Section 7 we explore several variations of the model. Finally, we conclude and list open problems. 2. Previous Work Goldman, Rivest and Schapire <ref> [13] </ref> first introduced the model of teacher-directed learning, a variant of the on-line learning model, in which examples are chosen by a helpful teacher.
Reference: 14. <author> S. A. Goldman and R. H. Sloan. </author> <title> The power of self-directed learning. </title> <journal> Machine Learning, </journal> <volume> 14(3) </volume> <pages> 271-294, </pages> <year> 1994. </year>
Reference-contexts: Recently, Rivest and Yin [22] have demonstrated the power of a helpful teacher by giving concept classes that are efficiently teachable in the teacher-directed model but that are not efficiently learnable in the self-directed learning model of Goldman and Sloan <ref> [14] </ref>. Since the introduction of the teacher-directed model, several interesting models have been proposed to study the complexity of teaching. The first formal model of teaching was introduced by Goldman and Kearns [12].
Reference: 15. <author> Tibor Heged-us. </author> <title> Combinatorial results on the complexity of teaching and learning. </title> <booktitle> In Proceedings of the 19th International Symposium on Mathematical Foundations of Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: Aside from the work on formal models of teaching there has also been interest in complexity measures of various concept classes in existing learning models. Perhaps most general is the work of Heged-us <ref> [15] </ref>, [16] who defines several general combinatorial measures on the complexity of teaching. Anthony, et.al. [5] consider subclasses of linearly separable boolean functions. They compute bounds on the size of the smallest sample with which only the target function is consistent.
Reference: 16. <author> Tibor Heged-us. </author> <title> Generalized teaching dimensions and the query complexity of learning. </title> <booktitle> In Proceedings of the Eighth Annual ACM Conference on Computational Learning Theory, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Aside from the work on formal models of teaching there has also been interest in complexity measures of various concept classes in existing learning models. Perhaps most general is the work of Heged-us [15], <ref> [16] </ref> who defines several general combinatorial measures on the complexity of teaching. Anthony, et.al. [5] consider subclasses of linearly separable boolean functions. They compute bounds on the size of the smallest sample with which only the target function is consistent.
Reference: 17. <author> Jeffrey Jackson. </author> <title> An efficient membership-query algorithm for learning DNF with respect to the uniform distribution. </title> <booktitle> In 35th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 42-53, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Two recent results illustrate the state-of-the-art in DNF learning. Bshouty et.al. [7] gave a randomized algorithm, using restricted subset and superset queries, to learn DNF. In the PAC model, Jackson <ref> [17] </ref> has given an algorithm using membership queries to learn DNF against the uniform distribution. We demonstrate the power of our approach by giving a deterministic teacher/learner pair for the class of DNF formulas. The learner runs in polynomial time and uses only equivalence queries.
Reference: 18. <author> Jeffrey Jackson and Andrew Tomkins. </author> <title> A computational model of teaching. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 319-326. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: In fact, if a teacher is required to teach any consistent learner then there AN INTERACTIVE MODEL OF TEACHING 3 are classes for which efficient learning algorithms are known but an exponential number of examples are required for teaching. Obviously, this is counterintuitive. Jackson and Tomkins <ref> [18] </ref> introduced the notion of a T/L pair in which the teacher and learner were designed to cooperate with each other. The motivation for T/L pairs is clear. First, they capture the intuition of a one-on-one teacher-student relationship. <p> The notion that, to avoid collusion, a teacher should be required to teach any consistent learner runs counter to the intuition motivating models of teaching. To remedy this, Jackson and Tomkins <ref> [18] </ref> introduced teacher/learner pairs. In their model a teacher and learner are paired together to cooperate. To prevent collusion they require that the learner must output a hypothesis logically equivalent to the target, or no concept at all, even if the teacher is replaced by an adversarial substitute.
Reference: 19. <author> Michael J. Kearns. </author> <title> The Computational Complexity of Machine Learning. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1990. </year>
Reference: 20. <author> Stefan Lange and Rolf Wiehagen. </author> <title> Polynomial-time inference of arbitrary pattern languages. </title> <journal> New Generation Computing, </journal> <volume> 8 </volume> <pages> 361-370, </pages> <year> 1991. </year>
Reference-contexts: There has been some work on teaching in the inductive inference community. Though neither presents a complete model of teaching, both Freivalds, Kinber and Wiehagen [9] and Lange and Wiehagen <ref> [20] </ref> have examined inference from "good examples" chosen by a helpful teacher. By presenting the learner with a superset of the teaching set prepared by the teacher, encoding is prevented in both of these 6 H. D. MATHIAS models. Lange and Wiehagen [20] examine learning pattern languages and show that this <p> Kinber and Wiehagen [9] and Lange and Wiehagen <ref> [20] </ref> have examined inference from "good examples" chosen by a helpful teacher. By presenting the learner with a superset of the teaching set prepared by the teacher, encoding is prevented in both of these 6 H. D. MATHIAS models. Lange and Wiehagen [20] examine learning pattern languages and show that this can be achieved with good examples. 3. Preliminaries The teaching model that we present in this paper is based on the model of learning with queries developed by Angluin [1].
Reference: 21. <author> B. K. Natarajan. </author> <title> On learning Boolean functions. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 296-304, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Anthony, et.al. [5] consider subclasses of linearly separable boolean functions. They compute bounds on the size of the smallest sample with which only the target function is consistent. Natarajan <ref> [21] </ref> defines a dimension measure for classes of Boolean functions that measures the complexity of a class C by the length of the shortest example sequence for which the target function is the unique, most specific function from C consistent with the sample.
Reference: 22. <author> Ronald L. Rivest and Yiqun Lisa Yin. </author> <title> Being taught can be faster than asking questions. </title> <booktitle> In Proceedings of the Eighth Annual ACM Conference on Computational Learning Theory, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Finally, we conclude and list open problems. 2. Previous Work Goldman, Rivest and Schapire [13] first introduced the model of teacher-directed learning, a variant of the on-line learning model, in which examples are chosen by a helpful teacher. Recently, Rivest and Yin <ref> [22] </ref> have demonstrated the power of a helpful teacher by giving concept classes that are efficiently teachable in the teacher-directed model but that are not efficiently learnable in the self-directed learning model of Goldman and Sloan [14].
Reference: 23. <author> Kathleen Romanik. </author> <title> Approximate testing and learnability. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 327-332. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: In a model by Salzberg, et.al. [25] a helpful teacher presents a shortest example sequence allowing the learner, using the nearest-neighbor algorithm, to learn the target concept. Romanik and Smith <ref> [23] </ref>, [24] propose a testing problem in which the goal is to construct, for a given target concept, a set of examples such that any concept that is consistent with this test set is "close" to the target in a probabilistic sense.
Reference: 24. <author> Kathleen Romanik and Carl Smith. </author> <title> Testing geometric objects. </title> <type> Technical Report UMIACS-TR-90-69, </type> <institution> University of Maryland College Park, Department of Computer Science, </institution> <year> 1990. </year>
Reference-contexts: In a model by Salzberg, et.al. [25] a helpful teacher presents a shortest example sequence allowing the learner, using the nearest-neighbor algorithm, to learn the target concept. Romanik and Smith [23], <ref> [24] </ref> propose a testing problem in which the goal is to construct, for a given target concept, a set of examples such that any concept that is consistent with this test set is "close" to the target in a probabilistic sense.
Reference: 25. <author> Steven Salzberg, Arthur Delcher, David Heath, and Simon Kasif. </author> <title> Learning with a helpful teacher. </title> <booktitle> In 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 705-711, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: In a model by Salzberg, et.al. <ref> [25] </ref> a helpful teacher presents a shortest example sequence allowing the learner, using the nearest-neighbor algorithm, to learn the target concept.
Reference: 26. <author> Ayumi Shinohara and Satoru Miyano. </author> <title> Teachability in computational learning. </title> <journal> New Generation Computing, </journal> <volume> 8 </volume> <pages> 337-347, </pages> <year> 1991. </year>
Reference-contexts: In this model they defined the teaching complexity as the minimum number of examples that a teacher must present to any consistent learner to enable the learner to exactly identify the target concept. In independent work, Shinohara and Miyano <ref> [26] </ref> introduced a model in which a class is teachable by examples if there AN INTERACTIVE MODEL OF TEACHING 5 exists a sample of polynomial size that allows all consistent learners to achieve exact identification of the target concept.
Reference: 27. <author> Robert H. Sloan and Gyorgy Turan. </author> <title> Learning with queries and incomplete information. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 237-245, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: As with incomplete membership queries the noise is persistent. The adversary has a bound of ` on the number of instances on which it can lie and the learner is allowed time polynomial in `. (Sloan and Turan <ref> [27] </ref> introduced a similar model in which membership queries are answered "I don't know" at the discretion of an adversary.) This change is easily incorporated into our model since we can allow the adversary to change the answer to membership queries at its discretion with a bound of ` on the
References-found: 27

