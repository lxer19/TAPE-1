URL: ftp://theory.lcs.mit.edu/pub/people/edelman/scicom/fft.ps
Refering-URL: http://theory.lcs.mit.edu/~fftw/fft-links.html
Root-URL: 
Title: THE FUTURE FAST FOURIER TRANSFORM?  
Author: ALAN EDELMAN PETER McCORQUODALE AND SIVAN TOLEDO 
Keyword: Key words. parallel Fourier transforms, fast multipole method  
Note: AMS subject classifications. 65T20, 65Y05, 65Y20  
Date: MARCH 18, 1997  
Abstract: It seems likely that improvements in arithmetic speed will continue to outpace advances in communications bandwidth. Furthermore, as more and more problems are working on huge datasets, it is becoming increasingly likely that data will be distributed across many processors because one processor does not have sufficient storage capacity. For these reasons, we propose that an inexact DFT such as an approximate matrix-vector approach based on singular values or a variation of the Dutt-Rokhlin fast-multipole-based algorithm may outperform any exact parallel FFT. The speedup may be as large as a factor of three in situations where FFT run time is dominated by communication. For the multipole idea we further propose that a method of "virtual charges" may improve accuracy, and we provide an analysis of the singular values that are needed for the approximate matrix-vector approaches. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. C. Agarwal, F. G. Gustavson, and M. Zubair, </author> <title> Exploiting functional parallelism of POWER2 to design high-performance numerical algorithms, </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38 (1994), </volume> <pages> pp. 563-576. </pages>
Reference-contexts: degree t. (This is equivalent to using a finite series of derivatives of cot (=2).) To approximate the potential at angular positions 2 [6a; 2 6a], due to charges in the interval 2a 2a, the Dutt-Rokhlin interpolation scheme sets x = 3 tan a cot (=2) and for x 2 <ref> [1; 1] </ref> sets e f (x) = j=1 Y ( c j c k where c 1 ; : : :c n are the Chebyshev nodes, c j = cos ((j 1=2)=t). <p> The peak floating-point performance of POWER2-based nodes is 266 million operations per seconds (Mflops), thanks to two floating-point functional units that can each execute a multiply-add operation in every cycle. While many dense matrix operations run on these nodes at close to peak performance <ref> [1] </ref>, FFT codes run at lower rates. Large power-of-two one-dimensional FFTs from FFTPACK run at 20-30 Mflops, and similar routines from IBM's Engineering and Scientific Subroutine 16 A. EDELMAN, P. McCORQUODALE, AND S.
Reference: [2] <author> T. Agerwala, J. L. Martin, J. H. Mirza, D. C. Sadler, D. M. Dias, and M. Snir, </author> <title> SP2 system architecture, </title> <journal> IBM Systems Journal, </journal> <volume> 34 (1995), </volume> <pages> pp. 152-184. </pages>
Reference-contexts: The software is portable and runs without modifications on both the IBM SP2 scalable parallel computer and a cluster of Sun UltraSparc symmetric multiprocessors (SMPs). The first set of experiments were conducted on an IBM SP2 parallel computer <ref> [2] </ref>. The machine was configured with so-called thin nodes with 128 Mbytes of main memory. Thin nodes have a 66.7 MHz POWER2 processor [22], 64 Kbytes 4-way set associative level-1 data-cache, no level-2 cache, and a 64-bit-wide main memory bus.
Reference: [3] <author> H. C. Andrews and C. L. Patterson, </author> <title> Singular value decomposition (SVD) image coding, </title> <journal> IEEE Trans. Commun., </journal> <month> COM-24 </month> <year> (1976), </year> <pages> pp. 425-432. </pages>
Reference-contexts: The most important underlying critical idea is the notion of near-rank deficiency. The operators that represent the relationship between the input on one processor and the output on another processor are nearly rank-deficient. Therefore, as is well known to those who wish to compress images <ref> [3] </ref>, this represents an opportunity to replace the operator with its more economical rank-deficient cousin, thereby gaining speedups on parallel supercomputers. We shall see later that the existence of a multipole algorithm is really a way of taking advantage of this fact. <p> Grunbaum [14] makes the first step in this direction by finding a tridiagonal matrix that commutes with G njp . 3. Algorithm 1: A Matrix-Vector Algorithm. Given that the sections F njp are nearly rank-deficient, we may borrow an idea from image compression <ref> [3] </ref> and take a singular-value decomposition. Our first algorithm for the DFT involves nothing more than matrix-vector multiplication by SVD matrices.
Reference: [4] <author> D. H. Bailey, </author> <title> FFTs in external or hierarchical memory, </title> <journal> Journal of Supercomputing, </journal> <volume> 4 (1990), </volume> <pages> pp. 23-35. </pages>
Reference-contexts: All such algorithms are in effect variations of the original algorithm of Cooley and Tukey [7]. A few important variants are the Stockham framework [6], which reorders data at each step, the Bailey method <ref> [4] </ref>, which minimizes the number of passes through external data sets, Swarztrauber's method [19] for hypercubes and vector supercomputers, and the recent algorithm by Cormen and Nicol [8] which reorganizes data for out-of-core algorithms. Many other important references may be found in Van Loan [20].
Reference: [5] <author> E. G. Benson, D. C. P. LaFrance-Linden, R. A. Warren, and S. Wiryaman, </author> <title> Design of Digital's Parallel Software Environment, </title> <journal> Digital Technical Journal, </journal> <volume> 7 (1995), </volume> <pages> pp. 24-38. </pages>
Reference-contexts: The nodes can have up to 12 processors each, with peak floating-point performance of 600-874 Mflops each. Digital has measured the bandwidth of the network at about 11:9 Mbytes per second <ref> [5] </ref>. With nodes consisting of twelve 600 Mflops processors each, the ratio is 0:0017 bytes/flop. The ratio in our SP2 experiments with Ethernet is about 0:0022 bytes/flop when we use 2 nodes, 0:0010 with 4 nodes, and 0:0005 with 8 nodes.
Reference: [6] <author> W. T. Cochrane, J. W. Cooley, J. W. Favin, D. L. Helms, R. A. Kaenel, W. W. Lang, G. C. Maling, D. E. Nelson, C. M. Rader, and P. D. Welch, </author> <title> What is the fast Fourier transform?, </title> <journal> IEEE Trans. Audio and Electroacoustics, </journal> <month> AU-15 </month> <year> (1967), </year> <pages> pp. 45-55. </pages>
Reference-contexts: Traditional research into algorithmic design for the Fast Fourier Transform focuses on memory and cache management and organization. All such algorithms are in effect variations of the original algorithm of Cooley and Tukey [7]. A few important variants are the Stockham framework <ref> [6] </ref>, which reorders data at each step, the Bailey method [4], which minimizes the number of passes through external data sets, Swarztrauber's method [19] for hypercubes and vector supercomputers, and the recent algorithm by Cormen and Nicol [8] which reorganizes data for out-of-core algorithms.
Reference: [7] <author> J. W. Cooley and J. W. Tukey, </author> <title> An algorithm for the machine calculation of complex Fourier series, </title> <journal> Mathematics of Computation, </journal> <volume> 19 (1965), </volume> <pages> pp. 297-301. </pages>
Reference-contexts: Traditional research into algorithmic design for the Fast Fourier Transform focuses on memory and cache management and organization. All such algorithms are in effect variations of the original algorithm of Cooley and Tukey <ref> [7] </ref>.
Reference: [8] <author> T. H. Cormen and D. M. Nicol, </author> <title> Performing out-of-core FFTs on parallel disk systems, </title> <type> Technical Report PCS-TR96-294, </type> <institution> Dartmouth College, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: A few important variants are the Stockham framework [6], which reorders data at each step, the Bailey method [4], which minimizes the number of passes through external data sets, Swarztrauber's method [19] for hypercubes and vector supercomputers, and the recent algorithm by Cormen and Nicol <ref> [8] </ref> which reorganizes data for out-of-core algorithms. Many other important references may be found in Van Loan [20]. In this paper, we believe that we are first to propose a parallel Fourier transform algorithm that would not be exact in the absence of roundoff error.
Reference: [9] <author> A. Dutt, M. Gu, and V. Rokhlin, </author> <title> Fast algorithms for polynomial interpolation, integration and differentiation, </title> <institution> Research Report YALEU/DCS/RR-977, Yale University, </institution> <month> July </month> <year> 1993. </year> <title> THE FUTURE FAST FOURIER TRANSFORM? 21 </title>
Reference-contexts: For fast multiplication by C (s) , we can use the one-dimensional fast multipole method (FMM) of Dutt, Gu and Rokhlin <ref> [9] </ref>. Dutt and Rokhlin [10] use a simi lar approach in a serial algorithm to compute the discrete Fourier transform on a nonequispaced set of points. 4.2. General Approach.
Reference: [10] <author> A. Dutt and V. Rokhlin, </author> <title> Fast Fourier transforms for nonequispaced data, II., </title> <journal> Appl. Comput. Harmon. Anal., </journal> <volume> 2 (1995), </volume> <pages> pp. 85-100. </pages>
Reference-contexts: Section 3 introduces a matrix-vector multiply algorithm that uses an off-line singular value analysis. Section 4 introduces our parallel fast multipole algorithm, an equispaced variation of the non-equispaced Fourier transform proposed by Dutt and Rokhlin <ref> [10] </ref>. Section 5 discusses the results of numerical experiments. <p> For fast multiplication by C (s) , we can use the one-dimensional fast multipole method (FMM) of Dutt, Gu and Rokhlin [9]. Dutt and Rokhlin <ref> [10] </ref> use a simi lar approach in a serial algorithm to compute the discrete Fourier transform on a nonequispaced set of points. 4.2. General Approach. <p> In the description of the algorithm below, each of the vectors v (s) , of length m, is distributed blockwise across the p processors, as are x and y, which are of length n. Algorithm 2. Fast multipole DFT (Dutt, Rokhlin <ref> [10] </ref>) 1. for s = 1 : p 1 P m1 end 10 A. EDELMAN, P. McCORQUODALE, AND S. <p> Here the charges and potential evaluation points are both spaced equally along the circumference of the circle. In the s-th potential mapping, the charge locations and evaluation points are offset from each other by an arc of length 2s=n. Dutt and Rokhlin <ref> [10] </ref> showed how the non-equispaced Fourier transform can be computed using the FMM. In this article, we are restricted to an equispaced DFT but we use a different set of interpolating functions that offer greater accuracy in this restricted case. <p> The initial far-field expansion of Step 1 is obtained by multiplying the vector of b charges by a real t fi b 12 A. EDELMAN, P. McCORQUODALE, AND S. TOLEDO matrix. 4.4. Interpolation Functions and Accuracy. In their t-term interpolations for the Fourier transform, Dutt and Rokhlin <ref> [10] </ref> use polynomial functions of cot (=2), of degree t. (This is equivalent to using a finite series of derivatives of cot (=2).) To approximate the potential at angular positions 2 [6a; 2 6a], due to charges in the interval 2a 2a, the Dutt-Rokhlin interpolation scheme sets x = 3 tan <p> Using the proof methods as Dutt and Rokhlin <ref> [10] </ref>, one can show that the the relative error with the new interpolation functions is O ((3=5) t ). This theoretical bound compares unfavorably with the error bound of O ((1=5) t ) using Chebyshev polynomials. However, in practice, the virtual-charge approximation is found to be more accurate.
Reference: [11] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins University Press, Baltimore and London, </publisher> <editor> 2nd ed., </editor> <year> 1989. </year>
Reference-contexts: Proposition 1. The singular values of F njp are strictly between 0 and 1. Proof. Since F njp is a submatrix of a unitary matrix, its singular values are at most 1. Moreover, F njp is a Vandermonde matrix and hence nonsingular. The CS decomposition <ref> [11, p. 77] </ref> of 1 p n F n shows that if any singular value of F njp is equal to 1, then 0 occurs as a singular value of a rectangular block of F n , but this is not possible because the rectangular block would be a section of
Reference: [12] <author> L. Greengard and W. D. Gropp, </author> <title> A parallel version of the fast multipole method, </title> <journal> Computers Math. Applic., </journal> <volume> 20 (1990), </volume> <pages> pp. 63-71. </pages>
Reference-contexts: In this article, we are restricted to an equispaced DFT but we use a different set of interpolating functions that offer greater accuracy in this restricted case. We also compute it in parallel, using the method of Greengard and Gropp <ref> [12] </ref> and Katzenelson [16]. We divide the input, m particles, into 2 h boxes, each containing b = m=2 h particles. In the tree code for the 1-dimensional FMM, there will be h 1 levels in the tree.
Reference: [13] <author> U. Grenander and G. </author> <title> Szeg -o, Toeplitz Forms and their Applications, </title> <institution> University of California Press, Berkeley and Los Angeles, </institution> <year> 1958. </year>
Reference-contexts: And then from equations (2) and (3), we also have S n (0; *) ~ m=p and S n (1 *; 1) ~ m (1 1=p). A better understanding of the transition region is suggested by examining the generating function <ref> [13] </ref>, g (x) = P 1 l=1 bg (l)e 2ilx , of the infinite Toeplitz matrix with entries g jk = bg (k j) given by equation (1), for j; k 2 Z.
Reference: [14] <author> F. A. Gr unbaum, </author> <title> Eigenvectors of a Toeplitz matrix: discrete version of the prolate spheroidal wave functions, </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 2 (1981), </volume> <pages> pp. 136-141. </pages>
Reference-contexts: A similar analysis may, in principle, be applicable to G njp . Grunbaum <ref> [14] </ref> makes the first step in this direction by finding a tridiagonal matrix that commutes with G njp . 3. Algorithm 1: A Matrix-Vector Algorithm. Given that the sections F njp are nearly rank-deficient, we may borrow an idea from image compression [3] and take a singular-value decomposition.
Reference: [15] <author> S. K. S. Gupta, C.-H. Huang, P. Sadayappan, and R. W. Johnson, </author> <title> Implementing fast Fourier transforms on distributed-memory multiprocessors using data redistributions, </title> <journal> Parallel Processing Letters, </journal> <volume> 4 (1994), </volume> <pages> pp. 477-488. </pages>
Reference-contexts: The global shu*es in steps (1) and (6) each require an amount of communication equivalent to the transpose in step (3). They may be saved if the order is not important. The communication pattern is as indicated in Figure 1, which is based on Gupta et al. <ref> [15] </ref>. This paper presents a method which can save up to a factor of three in communication cost, by using an approximate algorithm that essentially combines the three global transposes into one. Accuracy can be extended to full machine precision with negligible effect on communication complexity.
Reference: [16] <author> J. Katzenelson, </author> <title> Computational structure of the N-body problem, </title> <journal> SIAM J. Sci. Statist. Com-put., </journal> <volume> 10 (1989), </volume> <pages> pp. 787-815. </pages>
Reference-contexts: In this article, we are restricted to an equispaced DFT but we use a different set of interpolating functions that offer greater accuracy in this restricted case. We also compute it in parallel, using the method of Greengard and Gropp [12] and Katzenelson <ref> [16] </ref>. We divide the input, m particles, into 2 h boxes, each containing b = m=2 h particles. In the tree code for the 1-dimensional FMM, there will be h 1 levels in the tree.
Reference: [17] <author> D. Slepian, </author> <title> Prolate spheroidal wave functions, Fourier analysis, and uncertainty V: The discrete case, </title> <journal> Bell System Tech. J., </journal> <volume> 57 (1978), </volume> <pages> pp. 1371-1430. </pages>
Reference-contexts: Observe that G njp converges elementwise to W p : lim g jk = w jk : Slepian <ref> [17] </ref> observed that the prolate matrix W p also has eigenvalues clustered near 0 and 1, with a fraction 1=p of them near 1 and the rest near 0. <p> In the region 0:2 &lt; j &lt; 0:8, Slepian found that a good approximation <ref> [17, eqns. 61-62] </ref> is j (W p ) ~ 1 + exp ( b fi) where b fi = log (8mj sin (=p)j) + fl and fl = 0:5772156649 is the Euler-Mascheroni constant.
Reference: [18] <author> P. N. Swarztrauber, </author> <title> Vectorizing the FFT, in Parallel Computations, </title> <editor> G. Rodrigue, ed., </editor> <publisher> Academic Press, </publisher> <year> 1982, </year> <pages> pp. </pages> <month> 51-83. </month> <title> [19] , Multiprocessor FFTs, </title> <booktitle> Parallel Computing, 5 (1987), </booktitle> <pages> pp. 197-210. </pages>
Reference-contexts: Performance Results. This section compares the performance of our implementations of the new algorithm and a conventional high-performance FFT algorithm. Both algorithms are coded in Fortran 77. We use a publicly available FFT package, FFTPACK <ref> [18] </ref>, for performing local FFTs on individual processors, and the Message Passing Interface (MPI) for interprocessor communication. The software is portable and runs without modifications on both the IBM SP2 scalable parallel computer and a cluster of Sun UltraSparc symmetric multiprocessors (SMPs).
Reference: [20] <author> C. F. Van Loan, </author> <title> Computational Frameworks for the Fast Fourier Transform, </title> <publisher> SIAM, </publisher> <address> Philadel-phia, </address> <year> 1992. </year>
Reference-contexts: Many other important references may be found in Van Loan <ref> [20] </ref>. In this paper, we believe that we are first to propose a parallel Fourier transform algorithm that would not be exact in the absence of roundoff error. <p> The output vector should be distributed the same way. In this model, the standard approach to the parallel FFT is known as the "six-step framework" <ref> [20, pages 173-174] </ref>, consisting of: (1) a global bit reversal or shu*e, (2) local FFTs, (3) a global transpose, (4) multiplication by twiddle factors, (5) local FFTs, (6) a global shu*e or bit fl A preliminary version of this paper was presented at the Eighth SIAM Conference on Parallel Processing, March <p> We shall see that commuting the so-called "twiddle factor" matrix through the "butterfly" operations, leads to just this sort of identification. 4.1. Matrix factorizations. For parallel FFT computations over p processors, the standard "six-step framework" <ref> [20, pages 173-174] </ref> is based on the radix-p splitting [20, eqn. 2.1.5], a factorization of the Fourier matrix as F n = (F p I m )T (I p F m )(8) where again m = n=p and T is a diagonal matrix of twiddle factors, T = diag (I m <p> We shall see that commuting the so-called "twiddle factor" matrix through the "butterfly" operations, leads to just this sort of identification. 4.1. Matrix factorizations. For parallel FFT computations over p processors, the standard "six-step framework" [20, pages 173-174] is based on the radix-p splitting <ref> [20, eqn. 2.1.5] </ref>, a factorization of the Fourier matrix as F n = (F p I m )T (I p F m )(8) where again m = n=p and T is a diagonal matrix of twiddle factors, T = diag (I m ; ; 2 ; : : : ; p1
Reference: [21] <author> J. M. Varah, </author> <title> The prolate matrix, </title> <journal> Linear Algebra Appl., </journal> <volume> 187 (1993), </volume> <pages> pp. 269-278. </pages>
Reference-contexts: = 1 if x 2 [ 1 2p ]; 2 ; 1 2p ; 1 which is the generating function of an infinite Toeplitz matrix with entries w jk = bw (k j) = (k j) The finite version W p of this matrix is known as the prolate matrix <ref> [21] </ref>, and it has been well studied in the signal processing literature. <p> Such improvements would make the new algorithm faster than a conventional parallel FFT on machines with higher communication-to-computation-rates ratio than the ratios that we have indicated in this paper. Acknowledgments. We wish to thank Jim Varah for bringing reference <ref> [21] </ref> and the papers cited therein to our attention. We gratefully acknowledge the use of IBM SP2 parallel computers at MIT and IBM Watson Research Center, and also the prototype Ultra HPC cluster of SMPs being developed at MIT in collaboration with Sun Microsystems, Inc.
Reference: [22] <author> S. W. White and S. Dhawan, POWER2: </author> <title> Next generation of the RISC System/6000 family, </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38 (1994), </volume> <pages> pp. 493-502. </pages>
Reference-contexts: The first set of experiments were conducted on an IBM SP2 parallel computer [2]. The machine was configured with so-called thin nodes with 128 Mbytes of main memory. Thin nodes have a 66.7 MHz POWER2 processor <ref> [22] </ref>, 64 Kbytes 4-way set associative level-1 data-cache, no level-2 cache, and a 64-bit-wide main memory bus. They have smaller data paths between the cache and the floating-point units than all other POWER2-based SP2 nodes.
References-found: 21

