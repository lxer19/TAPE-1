URL: http://www.pdl.cs.cmu.edu/PDL-FTP/Declustering/FTCS.ps
Refering-URL: http://www.pdl.cs.cmu.edu/PDL-FTP/RAID/ComputingSurveys.biblio.html
Root-URL: 
Abstract: This paper describes and evaluates two algorithms for performing on-line failure recovery (data reconstruction) in redundant disk arrays. It presents an implementation of disk-oriented reconstruction, a data recovery algorithm that allows the reconstruction process to absorb essentially all the disk bandwidth not consumed by the user processes, and then compares this algorithm to a previous- proposed parallel stripe-oriented approach. The disk-oriented approach yields better overall failure-recovery performance. The paper evaluates performance via detailed simulation on two different disk array architectures: the RAID level 5 organization, and the declustered parity organization. The benefits of the disk-oriented algorithm can be achieved using controller or host buffer memory no larger than the size of three disk tracks per disk in the array. This paper also investigates the tradeoffs involved in selecting the size of the disk accesses used by the failure recovery process. 
Abstract-found: 1
Intro-found: 1
Reference: [Bitton88] <author> D. Bitton and J. Gray, </author> <title> Disk Shadowing, </title> <booktitle> Proceedings of the 14th Conference on Very Large Data Bases, </booktitle> <year> 1988, </year> <pages> pp. 331-338. </pages>
Reference-contexts: Fault-tolerance in a data storage subsystem is generally achieved either by disk mirroring <ref> [Bitton88, Cope- land89] </ref>, or parity encoding [Gibson93, Kim86, Patterson88]. In the former, one or more duplicate copies of all data are stored on separate disks.
Reference: [Chen90a] <author> P. Chen, et. al., </author> <title> An Evaluation of Redundant Arrays of Disks using an Amdahl 5890, </title> <booktitle> Proceedings of the Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1990, </year> <pages> pp. 74-85. </pages>
Reference-contexts: In the latter, popularized as Redundant Arrays of Inexpensive Disks (RAID) [Patterson88, Gibson92], a portion of the data blocks in the array is used to store an error-correcting code computed over the remaining blocks. Mirroring can potentially deliver higher performance than parity-encoding <ref> [Chen90a, Gray90] </ref>, but it is expensive in that it incurs a capacity overhead of at least 100%. Furthermore, recent work on overcoming the bottlenecks in parity-encoded arrays [Stodolsky93, Menon92, Rosenblum91] has demonstrated techniques that allow the performance of these arrays to approach that of mirroring.
Reference: [Chen90b] <author> P. Chen and D. Patterson, </author> <title> Maximizing Performance in a Striped Disk Array, </title> <booktitle> Proceedings of International Symposium on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: The functionality of the controller is sometimes implemented in host software rather than as dedicated hardware. The algorithms and analyses presented in this paper apply equally well to either implementation. comprising the array, using the left-symmetric variant of the RAID level 5 architecture <ref> [Chen90b, Lee91] </ref>. Logi- cally contiguous user data is broken down into blocks and striped across the disks to allow for concurrent access by independent processes. The shaded blocks, labelled Pi, store the parity (cumulative exclusive-or) computed over the corresponding data blocks, labelled Di.0 through Di.3. <p> Comparing reconstruction algorithms This section presents the results of a simulation study comparing the two reconstruction algorithms: parallel stripe-oriented and disk-oriented. This and all subsequent performance analyses in this paper were performed using an event-driven disk array simulator called raidSim <ref> [Lee91, Chen90b, Holland92] </ref>. RaidSim contains a realistic disk model, which was calibrated to an IBM Model 0661 (Lightning) drive [IBM0661]. The simulated array consists of 21 spin-synchronized disks, using data units that are one track (24KB) in size [Chen90b]. <p> RaidSim contains a realistic disk model, which was calibrated to an IBM Model 0661 (Lightning) drive [IBM0661]. The simulated array consists of 21 spin-synchronized disks, using data units that are one track (24KB) in size <ref> [Chen90b] </ref>. Our synthetically-generated workload was derived from access statistics taken from a trace of an airline-reservation OLTP system [Ramakrishnan92], and consisted of 80% 4KB reads, 16% 4KB writes, 2% 24KB reads, and 2% 24KB writes.
Reference: [Copeland89] <author> G. Copeland and T. Keller, </author> <title> A Comparison of High-Availability Media Recovery Techniques, </title> <booktitle> Proceedings of the ACM Conference on Management of Data, </booktitle> <year> 1989, </year> <pages> pp. 98-109. </pages>
Reference: [Gibson92] <author> G. Gibson, </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage, </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Fault-tolerance in a data storage subsystem is generally achieved either by disk mirroring [Bitton88, Cope- land89], or parity encoding [Gibson93, Kim86, Patterson88]. In the former, one or more duplicate copies of all data are stored on separate disks. In the latter, popularized as Redundant Arrays of Inexpensive Disks (RAID) <ref> [Patterson88, Gibson92] </ref>, a portion of the data blocks in the array is used to store an error-correcting code computed over the remaining blocks. Mirroring can potentially deliver higher performance than parity-encoding [Chen90a, Gray90], but it is expensive in that it incurs a capacity overhead of at least 100%.
Reference: [Gibson93] <author> G. Gibson and D. Patterson, </author> <title> Designing Disk Arrays for High Data Reliability, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> January, </month> <year> 1993. </year>
Reference-contexts: Fault-tolerance in a data storage subsystem is generally achieved either by disk mirroring [Bitton88, Cope- land89], or parity encoding <ref> [Gibson93, Kim86, Patterson88] </ref>. In the former, one or more duplicate copies of all data are stored on separate disks. <p> Because disk failures are detectable <ref> [Patterson88, Gibson93] </ref>, arrays of disks constitute an erasure channel [Peterson72], and so a parity code can correct any single disk failure. <p> Gibson <ref> [Gibson93] </ref> shows that a small number of spare disks suffices to provide a high degree of protection against data loss in relatively large arrays (&gt;70 disks). Although the above organization can be easily extended to tolerate multiple disk failures, this paper focuses on single-failure toleration. 2.2.
Reference: [Gray90] <author> G. Gray, B. Horst, and M. Walker, </author> <title> Parity Striping of Disc Arrays: Low-Cost Reliable Storage with Acceptable Throughput, </title> <booktitle> Proceedings of the Conference on Very Large Data Bases, </booktitle> <year> 1990, </year> <pages> pp. 148-160. </pages>
Reference-contexts: In the latter, popularized as Redundant Arrays of Inexpensive Disks (RAID) [Patterson88, Gibson92], a portion of the data blocks in the array is used to store an error-correcting code computed over the remaining blocks. Mirroring can potentially deliver higher performance than parity-encoding <ref> [Chen90a, Gray90] </ref>, but it is expensive in that it incurs a capacity overhead of at least 100%. Furthermore, recent work on overcoming the bottlenecks in parity-encoded arrays [Stodolsky93, Menon92, Rosenblum91] has demonstrated techniques that allow the performance of these arrays to approach that of mirroring.
Reference: [Holland92] <author> M. Holland and G. Gibson, </author> <title> Parity Declus 10 tering for Continuous Operation in Redundant Disk Arrays, </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1992, </year> <pages> pp. 23-25. </pages>
Reference-contexts: The long queueing delays caused by saturation can violate these requirements. The declustered parity disk array organization <ref> [Muntz90, Holland92, Merchant92] </ref> addresses this problem. This scheme reduces the per-disk load increase caused by a failure from over 100% to an arbitrarily small percentage by increasing the amount of error correcting information that is stored in the array. <p> Parity is not balanced in this diagram because it shows only a portion of the data layout. A complete figure would show that the layout does actually assign an equal number of parity units to each disk. 4 (refer to <ref> [Holland92] </ref> or [Merchant92]). We use a parity-declustered architecture for comparing reconstruction algorithms in Section 4, and employ the data layout described in Holland and Gibson [Holland92]. To evaluate each algorithm for RAID level 5 arrays, consider only the points where a = 1.0. 3. <p> A complete figure would show that the layout does actually assign an equal number of parity units to each disk. 4 (refer to <ref> [Holland92] </ref> or [Merchant92]). We use a parity-declustered architecture for comparing reconstruction algorithms in Section 4, and employ the data layout described in Holland and Gibson [Holland92]. To evaluate each algorithm for RAID level 5 arrays, consider only the points where a = 1.0. 3. The reconstruction algorithm This section identifies the problems with the simple algorithm presented in Section 2.2, and develops the disk- oriented approach. <p> One way to address these problems is to reconstruct P1 P3 P0 D3.2 D0.2 D3.1 D0.1 D2.1 D0.0 D2.0 DISK0 DISK1 DISK2 DISK3 DISK4 0 2 Offset organization. multiple parity stripes in parallel <ref> [Holland92] </ref>. In this approach, the host or array controller creates a set of N identical, independent reconstruction processes instead of just one. <p> Comparing reconstruction algorithms This section presents the results of a simulation study comparing the two reconstruction algorithms: parallel stripe-oriented and disk-oriented. This and all subsequent performance analyses in this paper were performed using an event-driven disk array simulator called raidSim <ref> [Lee91, Chen90b, Holland92] </ref>. RaidSim contains a realistic disk model, which was calibrated to an IBM Model 0661 (Lightning) drive [IBM0661]. The simulated array consists of 21 spin-synchronized disks, using data units that are one track (24KB) in size [Chen90b]. <p> Refer to Holland and Gibson <ref> [Holland92] </ref> or Merchant and Yu [Merchant92] for more details on the advantages of parity declustering.
Reference: [Hou93] <author> R. Hou, J. Menon, and Y. Patt, </author> <title> Balancing I/O Response Time and Disk Rebuild Time in a RAID5 Disk Array, </title> <booktitle> Proceedings of the Hawaii International Conference on Systems Sciences, </booktitle> <year> 1993, </year> <pages> pp. 70-79. </pages>
Reference-contexts: The reconstruction algorithm This section identifies the problems with the simple algorithm presented in Section 2.2, and develops the disk- oriented approach. The main idea behind this technique has been suggested in previous studies <ref> [Merchant92, Hou93] </ref>, but personal communications with disk array manufacturers indicate that both algorithms are currently used in disk array products. This paper provides a detailed analysis of the trade-offs between these algorithms. 3.1.
Reference: [IBM0661] <author> IBM Corporation, </author> <title> IBM 0661 Disk Drive Product Description, Model 370, First Edition, Low End Storage Products, </title> <type> 504/114-2, </type> <year> 1989. </year>
Reference-contexts: This and all subsequent performance analyses in this paper were performed using an event-driven disk array simulator called raidSim [Lee91, Chen90b, Holland92]. RaidSim contains a realistic disk model, which was calibrated to an IBM Model 0661 (Lightning) drive <ref> [IBM0661] </ref>. The simulated array consists of 21 spin-synchronized disks, using data units that are one track (24KB) in size [Chen90b].
Reference: [Kim86] <author> M. Kim, </author> <title> Synchronized Disk Interleaving, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 35 (11), </volume> <year> 1986, </year> <pages> pp. 978-988. </pages>
Reference-contexts: Fault-tolerance in a data storage subsystem is generally achieved either by disk mirroring [Bitton88, Cope- land89], or parity encoding <ref> [Gibson93, Kim86, Patterson88] </ref>. In the former, one or more duplicate copies of all data are stored on separate disks.
Reference: [Lee91] <author> E. Lee and R. Katz, </author> <title> Performance Consequences of Parity Placement in Disk Arrays, </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991, </year> <pages> pp. 190-199. </pages>
Reference-contexts: The functionality of the controller is sometimes implemented in host software rather than as dedicated hardware. The algorithms and analyses presented in this paper apply equally well to either implementation. comprising the array, using the left-symmetric variant of the RAID level 5 architecture <ref> [Chen90b, Lee91] </ref>. Logi- cally contiguous user data is broken down into blocks and striped across the disks to allow for concurrent access by independent processes. The shaded blocks, labelled Pi, store the parity (cumulative exclusive-or) computed over the corresponding data blocks, labelled Di.0 through Di.3. <p> Comparing reconstruction algorithms This section presents the results of a simulation study comparing the two reconstruction algorithms: parallel stripe-oriented and disk-oriented. This and all subsequent performance analyses in this paper were performed using an event-driven disk array simulator called raidSim <ref> [Lee91, Chen90b, Holland92] </ref>. RaidSim contains a realistic disk model, which was calibrated to an IBM Model 0661 (Lightning) drive [IBM0661]. The simulated array consists of 21 spin-synchronized disks, using data units that are one track (24KB) in size [Chen90b].
Reference: [Menon92] <author> J. Menon and J. Kasson, </author> <title> Methods for Improved Update Performance of Disk Arrays, </title> <booktitle> Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <year> 1992, </year> <pages> pp. 74-83. </pages>
Reference-contexts: Mirroring can potentially deliver higher performance than parity-encoding [Chen90a, Gray90], but it is expensive in that it incurs a capacity overhead of at least 100%. Furthermore, recent work on overcoming the bottlenecks in parity-encoded arrays <ref> [Stodolsky93, Menon92, Rosenblum91] </ref> has demonstrated techniques that allow the performance of these arrays to approach that of mirroring. This paper focuses on parity-encoded arrays. Section 2 of this paper provides background material about disk arrays, and describes the failure-recovery problem.
Reference: [Merchant92] <author> A. Merchant and P. Yu, </author> <title> Design and Modeling of Clustered RAID, </title> <booktitle> Proceedings of the International Symposium on Fault-Tolerant Computing, </booktitle> <year> 1992, </year> <pages> pp. 140-149. </pages>
Reference-contexts: The long queueing delays caused by saturation can violate these requirements. The declustered parity disk array organization <ref> [Muntz90, Holland92, Merchant92] </ref> addresses this problem. This scheme reduces the per-disk load increase caused by a failure from over 100% to an arbitrarily small percentage by increasing the amount of error correcting information that is stored in the array. <p> Parity is not balanced in this diagram because it shows only a portion of the data layout. A complete figure would show that the layout does actually assign an equal number of parity units to each disk. 4 (refer to [Holland92] or <ref> [Merchant92] </ref>). We use a parity-declustered architecture for comparing reconstruction algorithms in Section 4, and employ the data layout described in Holland and Gibson [Holland92]. To evaluate each algorithm for RAID level 5 arrays, consider only the points where a = 1.0. 3. <p> The reconstruction algorithm This section identifies the problems with the simple algorithm presented in Section 2.2, and develops the disk- oriented approach. The main idea behind this technique has been suggested in previous studies <ref> [Merchant92, Hou93] </ref>, but personal communications with disk array manufacturers indicate that both algorithms are currently used in disk array products. This paper provides a detailed analysis of the trade-offs between these algorithms. 3.1. <p> Refer to Holland and Gibson [Holland92] or Merchant and Yu <ref> [Merchant92] </ref> for more details on the advantages of parity declustering.
Reference: [Muntz90] <author> R. Muntz and J. Lui, </author> <title> Performance Analysis of Disk Arrays Under Failure, </title> <booktitle> Proceedings of the 16th Conference on Very Large Data Bases, </booktitle> <year> 1990, </year> <pages> pp. 162-173. </pages>
Reference-contexts: The long queueing delays caused by saturation can violate these requirements. The declustered parity disk array organization <ref> [Muntz90, Holland92, Merchant92] </ref> addresses this problem. This scheme reduces the per-disk load increase caused by a failure from over 100% to an arbitrarily small percentage by increasing the amount of error correcting information that is stored in the array.
Reference: [Patterson88] <author> D. Patterson, G. Gibson, and R. Katz, </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proceedings of the Conference on Management of Data, </booktitle> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: Fault-tolerance in a data storage subsystem is generally achieved either by disk mirroring [Bitton88, Cope- land89], or parity encoding <ref> [Gibson93, Kim86, Patterson88] </ref>. In the former, one or more duplicate copies of all data are stored on separate disks. <p> Fault-tolerance in a data storage subsystem is generally achieved either by disk mirroring [Bitton88, Cope- land89], or parity encoding [Gibson93, Kim86, Patterson88]. In the former, one or more duplicate copies of all data are stored on separate disks. In the latter, popularized as Redundant Arrays of Inexpensive Disks (RAID) <ref> [Patterson88, Gibson92] </ref>, a portion of the data blocks in the array is used to store an error-correcting code computed over the remaining blocks. Mirroring can potentially deliver higher performance than parity-encoding [Chen90a, Gray90], but it is expensive in that it incurs a capacity overhead of at least 100%. <p> Section 3 develops the disk-oriented reconstruction algorithm and Section 4 evaluates it using a disk-accurate event-driven simulator. Section 5 provides a summary and some conclusions. 2. Background This section presents a brief overview of the redundant disk array systems considered in this paper. 2.1. Disk arrays Patterson et. al. <ref> [Patterson88] </ref> present a number of possible redundant disk array architectures, which they call RAID levels 1 through 5. Some of these are intended to provide large amounts of data to a single process at high speeds, while others are intended to provide highly concurrent access to shared files. <p> Because disk failures are detectable <ref> [Patterson88, Gibson93] </ref>, arrays of disks constitute an erasure channel [Peterson72], and so a parity code can correct any single disk failure.
Reference: [Peterson72] <author> W. Peterson and E. Weldon Jr., </author> <title> Error-Correcting Codes, second edition, </title> <publisher> MIT Press, </publisher> <year> 1972. </year>
Reference-contexts: Because disk failures are detectable [Patterson88, Gibson93], arrays of disks constitute an erasure channel <ref> [Peterson72] </ref>, and so a parity code can correct any single disk failure.
Reference: [Ramakrishnan92] <author> K. Ramakrishnan, P. Biswas, and R. Karedla, </author> <title> Analysis of File I/O Traces in Commercial Computing Environments, </title> <booktitle> Proceedings of the Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1992, </year> <pages> pp. 78-90. </pages>
Reference-contexts: The simulated array consists of 21 spin-synchronized disks, using data units that are one track (24KB) in size [Chen90b]. Our synthetically-generated workload was derived from access statistics taken from a trace of an airline-reservation OLTP system <ref> [Ramakrishnan92] </ref>, and consisted of 80% 4KB reads, 16% 4KB writes, 2% 24KB reads, and 2% 24KB writes. Accesses were randomly distributed in the data space of the array, and the access rate was regulated to present 294 user accesses per second to the array.
Reference: [Rosenblum91] <author> M. Rosenblum and J. Ousterhout, </author> <title> The Design and Implementation of a Log-Structured File System, </title> <booktitle> Proceedings of the Symposium on Operating System Principles, </booktitle> <year> 1991, </year> <pages> pp. 1-15. </pages>
Reference-contexts: Mirroring can potentially deliver higher performance than parity-encoding [Chen90a, Gray90], but it is expensive in that it incurs a capacity overhead of at least 100%. Furthermore, recent work on overcoming the bottlenecks in parity-encoded arrays <ref> [Stodolsky93, Menon92, Rosenblum91] </ref> has demonstrated techniques that allow the performance of these arrays to approach that of mirroring. This paper focuses on parity-encoded arrays. Section 2 of this paper provides background material about disk arrays, and describes the failure-recovery problem.
Reference: [Stodolsky93] <author> D. Stodolsky, G. Gibson, and M. Holland, </author> <title> Parity Logging: Overcoming the Small-Write Problem in Redundant Disk Arrays, </title> <booktitle> Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1993. </year>
Reference-contexts: Mirroring can potentially deliver higher performance than parity-encoding [Chen90a, Gray90], but it is expensive in that it incurs a capacity overhead of at least 100%. Furthermore, recent work on overcoming the bottlenecks in parity-encoded arrays <ref> [Stodolsky93, Menon92, Rosenblum91] </ref> has demonstrated techniques that allow the performance of these arrays to approach that of mirroring. This paper focuses on parity-encoded arrays. Section 2 of this paper provides background material about disk arrays, and describes the failure-recovery problem.
Reference: [TPCA89] <author> The TPC-A Benchmark: </author> <title> A Standard Specification, Transaction Processing Performance Council, </title> <year> 1989. </year>
Reference-contexts: The authors can be reached as mark.holland@ece.cmu.edu, garth.gibson@cs.cmu.edu, and dan.siewiorek@cs.cmu.edu. Proceedings of the 23rd Annual International Symposium on Fault-Tolerant Computing, 1993. 2 cations are characterized by a large number of independent processes concurrently requesting access to relatively small units of data <ref> [TPCA89] </ref>. For this reason, this paper focuses on architectures derived from the RAID level 5 organization. Figure 1a illustrates a storage subsystem employing this organization. The array controller is responsible for all system-related activity: communicating with the host, controlling the individual disks, maintaining redundant information, recovering from failures, etc. <p> Disk saturation is unacceptable for OLTP applications because they mandate a minimum acceptable level of responsiveness; the TPC-A benchmark <ref> [TPCA89] </ref>, for example, requires that 90% of all transactions complete in under two seconds. The long queueing delays caused by saturation can violate these requirements. The declustered parity disk array organization [Muntz90, Holland92, Merchant92] addresses this problem.
References-found: 21

