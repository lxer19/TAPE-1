URL: http://www.cs.bu.edu/techreports/93-010-learning-rect.ps.Z
Refering-URL: http://cs-www.bu.edu/techreports/Home.html
Root-URL: 
Email: Email address: homer@cs.bu.edu, zchen@cs.bu.edu  
Title: Learning Unions of Rectangles with Queries  
Author: Zhixiang Chen Steven Homer 
Note: The authors were supported by NSF grant CCR91-9103055. The first author was also supported by a Boston University Presidential Graduate Fellowship.  
Address: Boston, MA 02215  
Affiliation: Department of Computer Science Boston University  
Abstract-found: 0
Intro-found: 1
Reference: [Aa] <author> D. Angluin, </author> <title> "Queries and concept learning", </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <year> 1988, </year> <pages> pages 319-342. </pages>
Reference-contexts: Our result provides a first approach to on-line learning of nontrivial subclasses of unions of intersections of halfspaces with equivalence and membership queries. 2 Preliminaries Our learning model is the standard model for on-line learning with equivalence and membership queries (see <ref> [Aa] </ref>, [L] and [MTc]). A learning process for a concept class C over a domain X is viewed as a dialogue between a learner A and the environment.
Reference: [Ab] <author> D. Angluin, </author> <title> "Learning k-term DNF formulas using queries and counterexamples, </title> <type> Technical Report YaleU/DCS/RR-559, </type> <institution> Yale University department of Computer Science, </institution> <year> 1987. </year>
Reference-contexts: With the added ability of the learner to make membership queries, a wider collection of DNF subclasses are known to be learnable. These include monotone DNF formulas in [V] and read-twice DNF formulas in [AP] and [H]. For learning k-term DNF formulas with equivalence queries and membership queries, Angluin <ref> [Ab] </ref> designed an algorithm whose running time is O (d k 2 ); another remarkable result is due to Blum and Rudich [BR], they exhibited an O (d2 O (k) ) learning algorithm.
Reference: [AP] <author> H. Aizenstein, L. Pitt, </author> <title> "Exact learning of read-twice DNF formulas", </title> <booktitle> Proc of the 32th Annual Symposium on Foundations of Computer Science, </booktitle> <year> 1991, </year> <pages> pages 170-179. </pages>
Reference-contexts: With the added ability of the learner to make membership queries, a wider collection of DNF subclasses are known to be learnable. These include monotone DNF formulas in [V] and read-twice DNF formulas in <ref> [AP] </ref> and [H]. For learning k-term DNF formulas with equivalence queries and membership queries, Angluin [Ab] designed an algorithm whose running time is O (d k 2 ); another remarkable result is due to Blum and Rudich [BR], they exhibited an O (d2 O (k) ) learning algorithm.
Reference: [AU] <author> P. Auer, </author> <title> "On-line learning of rectangles in noisy environment", </title> <booktitle> Proc of the 6th Annual Workshop on Computational Learning Theory, </booktitle> <year> 1993. </year>
Reference-contexts: Recently, Auer <ref> [AU] </ref> has designed a variation of the learning algorithm for BOX d n that also learns with O (d 2 log n) counterexamples, and that tolerates a fraction of at most 1=(16d 2 ) false counterexamples (in arbitrary distribution).
Reference: [Bb] <author> E.B. Baum, </author> <title> "Polynomial time algorithms for learning neural nets, </title> <booktitle> Proc of the 3th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 258-272. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: He has shown that intersections of halfspaces are probably almost correctly learnable in an extended version of the PAC model where the learner may also ask membership queries, provided that the distribution and the target concept are chosen in a "non-malicious manner". Baum <ref> [Bb] </ref> also studied the pac-learnabilty of a union of halfspaces. Bultman and Maass [BM] have shown that a variety of intersections of halfspaces over the domain f1; : : :; ng 2 are efficiently learnable using only membership queries.
Reference: [Ba] <author> E.B. Baum, </author> <title> "On learning a union of halfspaces", </title> <journal> Journal of Complexity, </journal> <volume> 6(1990), </volume> <pages> pages 67-101. </pages>
Reference-contexts: Our result provides a contrast to the known upper bounds for the learning complexity of k-term DNF formulas, since all those known bounds are exponential in k. Baum <ref> [Ba] </ref> studied the learnability of intersections of halfspaces. He has shown that intersections of halfspaces are probably almost correctly learnable in an extended version of the PAC model where the learner may also ask membership queries, provided that the distribution and the target concept are chosen in a "non-malicious manner".
Reference: [BR] <author> A. Blum, S. Rudich, </author> <title> "Fast learning of k-term DNF formulas with queries", </title> <booktitle> Proc of the 24th Annual Symposium on Theory of Computing, </booktitle> <year> 1992, </year> <pages> pages 382-389. </pages>
Reference-contexts: These include monotone DNF formulas in [V] and read-twice DNF formulas in [AP] and [H]. For learning k-term DNF formulas with equivalence queries and membership queries, Angluin [Ab] designed an algorithm whose running time is O (d k 2 ); another remarkable result is due to Blum and Rudich <ref> [BR] </ref>, they exhibited an O (d2 O (k) ) learning algorithm. Enlightened by those positive results on learning k-term DNF formulas with queries, we begin to investigate the efficient learnability of unions of k rectangles, a general class of k-term DNF formulas. <p> Finally, since U-k-BOX d n is a general case of k-term DNF formulas with at most d variables, it would be very interesting to investigate whether one can design an efficient algorithm for learning U-O (log d)-BOX d n by extending Blum and Rudich's technique developed in <ref> [BR] </ref> for learning O (log n)-term DNF formulas. Acknowledgment We would like to thank Wolfgang Maass and Ming Li for their discussions. We would also like to thank Avrim Blum for explaining the algorithm developed in [BR] to us, Anselm Blumer for his critiques on the draft of this paper, and <p> learning U-O (log d)-BOX d n by extending Blum and Rudich's technique developed in <ref> [BR] </ref> for learning O (log n)-term DNF formulas. Acknowledgment We would like to thank Wolfgang Maass and Ming Li for their discussions. We would also like to thank Avrim Blum for explaining the algorithm developed in [BR] to us, Anselm Blumer for his critiques on the draft of this paper, and Phil Long for his valuable comments on our work. 16
Reference: [BS] <author> A. Blum, M. Singh, </author> <title> "Learning functions of k terms", </title> <booktitle> Proc of the 3th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 144-153. </pages> <publisher> Morgan Kauf-mann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Another O (d k ) learning algorithm which uses a represen-tation of general DNF formulas was also obtained in <ref> [BS] </ref>. With the added ability of the learner to make membership queries, a wider collection of DNF subclasses are known to be learnable. These include monotone DNF formulas in [V] and read-twice DNF formulas in [AP] and [H].
Reference: [BEHW] <author> A. Blumer, A. Ehrenfeucht, D. David, and M. Warmuth, </author> <title> "Learnability and the Vapnik-Chervonenkis dimension", </title> <journal> J. ACM, </journal> <pages> pages 929-965, </pages> <year> 1989. </year>
Reference-contexts: Thus, any efficient learning algorithm for unions of rectangles may find applications in practice. In the pac-model, the class of single rectangles over the domain f1; : : :; ng d was shown to be polynomial time predictable by Blumer, Ehrenfeucht, Haussler and War-muth <ref> [BEHW] </ref>. Long and Warmuth [LW] further proved that unions of a constant number of rectangles over the domain f1; : : :; ng d is polynomial time predictable. However, the problem of predicting unions of nonconstant number of rectangles is open [LW].
Reference: [BM] <author> W. Bultman, W. Maass, </author> <title> "Fast identification of geometric objects with membership queries", </title> <booktitle> Proc of the 4th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 337-353. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Baum [Bb] also studied the pac-learnabilty of a union of halfspaces. Bultman and Maass <ref> [BM] </ref> have shown that a variety of intersections of halfspaces over the domain f1; : : :; ng 2 are efficiently learnable using only membership queries. One may note that, in essence, Bultman and Maass' learning algorithm requires exactly one equivalence query.
Reference: [C] <author> Z. Chen, </author> <title> "Learning unions of two rectangles in the plane with equivalence queries", </title> <booktitle> Proc of the 6th Annual Workshop on Computational Learning Theory, </booktitle> <year> 1993. </year>
Reference-contexts: Based on this design technique and more powerful local search strategies which can tolerate certain types of two-sided errors, Chen <ref> [C] </ref> exhibited an O (log 2 n) algorithm for learning unions of two rectangles in the discrete plane f1; : : :; ng 2 with only equivalence queries while the hypothesis space of the learning algorithm is the same as the target concept class.
Reference: [CMa] <author> Z. Chen, W. Maass, </author> <title> "On-line learning of rectangles", </title> <booktitle> Proc of the 5th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 16-28. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: Another O (2 d log n) learning algorithm was exhibited in [MTa] and [MTb]. The question whether there is a learning algorithm for single rectangles whose complexity is O (poly (d; log n)) was proposed by David Haussler (see also [MTb]). Chen and Maass <ref> [CMa, CMb] </ref> gave a positive solution to this open question by introducing a new design technique. The learning algorithm in [CMa, CMb] for rectangles consists of 2d separate search strategies which search for the 2d boundaries of the target rectangle. <p> Chen and Maass <ref> [CMa, CMb] </ref> gave a positive solution to this open question by introducing a new design technique. The learning algorithm in [CMa, CMb] for rectangles consists of 2d separate search strategies which search for the 2d boundaries of the target rectangle.
Reference: [CMb] <author> Z. Chen, W. Maass, </author> <title> "On-line learning of rectangles and unions of rectangles", </title> <note> to appear in Machine Learning. </note>
Reference-contexts: Another O (2 d log n) learning algorithm was exhibited in [MTa] and [MTb]. The question whether there is a learning algorithm for single rectangles whose complexity is O (poly (d; log n)) was proposed by David Haussler (see also [MTb]). Chen and Maass <ref> [CMa, CMb] </ref> gave a positive solution to this open question by introducing a new design technique. The learning algorithm in [CMa, CMb] for rectangles consists of 2d separate search strategies which search for the 2d boundaries of the target rectangle. <p> Chen and Maass <ref> [CMa, CMb] </ref> gave a positive solution to this open question by introducing a new design technique. The learning algorithm in [CMa, CMb] for rectangles consists of 2d separate search strategies which search for the 2d boundaries of the target rectangle.
Reference: [CMc] <author> Z. Chen, W. Maass, </author> " <title> A solution of the credit assignment problem in the case of learning rectangles", </title> <booktitle> Proc of the Second International Workshop on Analogical Inductive and Inference, </booktitle> <pages> pages 26-34, </pages> <year> 1992. </year>
Reference-contexts: A learning algorithm with this type of modular design tends to fail because of the well-known " credit assignment problem": which of the 2d local search strategies should be "blamed" when the global algorithm makes an error? This difficulty was overcome there (see also <ref> [CMc] </ref>) by employing local search strategies that are able to tolerate certain types of one-sided errors.
Reference: [H] <author> T. Hancock, </author> <title> "Learning 2 DNF formulas and k decision trees", </title> <booktitle> Proc of the 4th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 199-209. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1991. </year> <month> 17 </month>
Reference-contexts: With the added ability of the learner to make membership queries, a wider collection of DNF subclasses are known to be learnable. These include monotone DNF formulas in [V] and read-twice DNF formulas in [AP] and <ref> [H] </ref>. For learning k-term DNF formulas with equivalence queries and membership queries, Angluin [Ab] designed an algorithm whose running time is O (d k 2 ); another remarkable result is due to Blum and Rudich [BR], they exhibited an O (d2 O (k) ) learning algorithm.
Reference: [L] <author> N. Littlestone, </author> <title> "Learning quickly when irrelevant attributes abound: a new linear threshold algorithm", </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <year> 1987, </year> <pages> pages 285-318. </pages>
Reference-contexts: In the on-line model with queries, the problem of learning a single rectangle with only equivalence queries over the domain f1; : : : ; ng d has been well-studied. The first known lower bound for this problem is (d log n) (see <ref> [L] </ref> and [MTc]). There is a learning algorithm with complexity O (dn) which always issues as its next hypothesis the smallest rectangle that is consistent with all preceding counterexamples (see the algorithm for the complementary class 1-CNF in Valiant's seminal paper [V]). <p> Our result provides a first approach to on-line learning of nontrivial subclasses of unions of intersections of halfspaces with equivalence and membership queries. 2 Preliminaries Our learning model is the standard model for on-line learning with equivalence and membership queries (see [Aa], <ref> [L] </ref> and [MTc]). A learning process for a concept class C over a domain X is viewed as a dialogue between a learner A and the environment.
Reference: [BM] <author> P. Long, M. Warmuth, </author> <title> "Composite geometric concepts and polynomial predictability", </title> <booktitle> Proc of the 3th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 273-287. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Baum [Bb] also studied the pac-learnabilty of a union of halfspaces. Bultman and Maass <ref> [BM] </ref> have shown that a variety of intersections of halfspaces over the domain f1; : : :; ng 2 are efficiently learnable using only membership queries. One may note that, in essence, Bultman and Maass' learning algorithm requires exactly one equivalence query.
Reference: [MTa] <author> W. Maass, G. Turan, </author> <title> "On the complexity of learning from counterexamples", </title> <booktitle> Proc of the 30th Annual Symposium on Foundations of Computer Science, </booktitle> <year> 1988, </year> <pages> pages 262-267. </pages>
Reference-contexts: Another O (2 d log n) learning algorithm was exhibited in <ref> [MTa] </ref> and [MTb]. The question whether there is a learning algorithm for single rectangles whose complexity is O (poly (d; log n)) was proposed by David Haussler (see also [MTb]). Chen and Maass [CMa, CMb] gave a positive solution to this open question by introducing a new design technique.
Reference: [MTb] <author> W. Maass, G. Turan, </author> <title> "Algorithms and lower bounds for on-line learning of geometric concepts", </title> <type> Report 316 (Oct. </type> <year> 1991), </year> <note> IIG-Report Series, Technische universitaet Graz; to appear in Machine Learning. </note>
Reference-contexts: Another O (2 d log n) learning algorithm was exhibited in [MTa] and <ref> [MTb] </ref>. The question whether there is a learning algorithm for single rectangles whose complexity is O (poly (d; log n)) was proposed by David Haussler (see also [MTb]). Chen and Maass [CMa, CMb] gave a positive solution to this open question by introducing a new design technique. <p> Another O (2 d log n) learning algorithm was exhibited in [MTa] and <ref> [MTb] </ref>. The question whether there is a learning algorithm for single rectangles whose complexity is O (poly (d; log n)) was proposed by David Haussler (see also [MTb]). Chen and Maass [CMa, CMb] gave a positive solution to this open question by introducing a new design technique. The learning algorithm in [CMa, CMb] for rectangles consists of 2d separate search strategies which search for the 2d boundaries of the target rectangle.
Reference: [MTc] <author> W. Maass, G. Turan, </author> <title> "Lower bound methods and separation results for on-line learning models", </title> <booktitle> Machine Learning, </booktitle> <year> 1992, </year> <pages> pages 107-145. </pages>
Reference-contexts: In the on-line model with queries, the problem of learning a single rectangle with only equivalence queries over the domain f1; : : : ; ng d has been well-studied. The first known lower bound for this problem is (d log n) (see [L] and <ref> [MTc] </ref>). There is a learning algorithm with complexity O (dn) which always issues as its next hypothesis the smallest rectangle that is consistent with all preceding counterexamples (see the algorithm for the complementary class 1-CNF in Valiant's seminal paper [V]). <p> Our result provides a first approach to on-line learning of nontrivial subclasses of unions of intersections of halfspaces with equivalence and membership queries. 2 Preliminaries Our learning model is the standard model for on-line learning with equivalence and membership queries (see [Aa], [L] and <ref> [MTc] </ref>). A learning process for a concept class C over a domain X is viewed as a dialogue between a learner A and the environment. The goal of the learner A is to learn an unknown target concept C t 2 C that has been fixed by the environment.
Reference: [V] <author> L. Valiant, </author> <title> "A theory of the learnable", </title> <journal> Comm. of the ACM, </journal> <volume> 27, </volume> <year> 1984, </year> <pages> pages 1134-1142. </pages>
Reference-contexts: There is a learning algorithm with complexity O (dn) which always issues as its next hypothesis the smallest rectangle that is consistent with all preceding counterexamples (see the algorithm for the complementary class 1-CNF in Valiant's seminal paper <ref> [V] </ref>). Another O (2 d log n) learning algorithm was exhibited in [MTa] and [MTb]. The question whether there is a learning algorithm for single rectangles whose complexity is O (poly (d; log n)) was proposed by David Haussler (see also [MTb]). <p> The standard strategy for learning k-term DNF formulas uses a hypothesis space of l-CNF formulas and runs in time O (d k ), where d is the 3 number of input variables <ref> [V] </ref>. Another O (d k ) learning algorithm which uses a represen-tation of general DNF formulas was also obtained in [BS]. With the added ability of the learner to make membership queries, a wider collection of DNF subclasses are known to be learnable. These include monotone DNF formulas in [V] and <p> variables <ref> [V] </ref>. Another O (d k ) learning algorithm which uses a represen-tation of general DNF formulas was also obtained in [BS]. With the added ability of the learner to make membership queries, a wider collection of DNF subclasses are known to be learnable. These include monotone DNF formulas in [V] and read-twice DNF formulas in [AP] and [H]. <p> For any C t 2 U-k-BOX 2 n , one says that C t is a tree if there is an i 2 [1; n] such that, for any z = (u; v) 2 C t , [i; u] fi <ref> [v; v] </ref> C t if i u, or [u; i] fi [v; v] C t if u &lt; i. One calls the y-axis-parallel line x = i the trunk of the tree C t . For a given tree, there may exist many different trunks for it. <p> any C t 2 U-k-BOX 2 n , one says that C t is a tree if there is an i 2 [1; n] such that, for any z = (u; v) 2 C t , [i; u] fi <ref> [v; v] </ref> C t if i u, or [u; i] fi [v; v] C t if u &lt; i. One calls the y-axis-parallel line x = i the trunk of the tree C t . For a given tree, there may exist many different trunks for it.
References-found: 21

