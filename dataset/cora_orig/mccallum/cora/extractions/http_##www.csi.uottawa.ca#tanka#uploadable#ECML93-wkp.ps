URL: http://www.csi.uottawa.ca/tanka/uploadable/ECML93-wkp.ps
Refering-URL: http://www.csi.uottawa.ca/tanka/papers.html
Root-URL: 
Email: fdelannoy,cfeng,stan,szpakg@csi.uottawa.ca  
Title: Knowledge Extraction from Text: Machine Learning for Text-to-rule Translation  
Author: J. F. Delannoy, C. Feng, S. Matwin, and S. Szpakowicz 
Address: Ottawa, Ontario, Canada K1N 6N5  
Affiliation: Department of Computer Science University of Ottawa  
Abstract: Learning from texts is a noble but distant research goal. We investigate the practicality of a more modest enterprise in which machine learning (ML) and natural language processing (NLP) would mutually reinforce each other. We are designing a system that will process technical expository texts, in which narratives (embodying general knowledge) and examples (specific knowledge) are interleaved to best instruct the reader. A broad-coverage parser of English technical texts and an interactive case analyzer provide the front end. Their output will be trans lated into Horn clauses; some user participation will be required throughout the process. We discuss how the application of absorption will provide a useful hierarchization of the domain theory. We then apply the EBL approach, using translation of the narratives as the domain theory, and translation of the examples as the training examples. We illustrate with examples that such learning may result in a reformulation of knowledge that would not be attainable by NLP techniques alone. We present the overall design of the system, with the emphasis on the ML contribution to the synergistic effect we expect. We also briefly consider the applicability of ML techniques to the generalization of the user directed translation itself.
Abstract-found: 1
Intro-found: 1
Reference: [ Cohen, 1990 ] <author> W. Cohen. </author> <title> Learning from textbook knowledge: a case study. </title> <booktitle> In AAAI-90 : proceedings of the Eighth National Conference on Artificial Intelligence., </booktitle> <pages> pages 743 - 748, </pages> <address> Boston,MA, </address> <year> 1990. </year>
Reference-contexts: The ability to extract knowledge from texts will no doubt prove rewarding, as much human knowledge is written down in textual format, and texts are one of the basic media of human communication. So far, there have been only a few answers to this challenge, like the work of <ref> [ Cohen, 1990 ] </ref> and [ Moulin and Rousseau, 1992 ] . Cohen, working on the card game of bridge, manually converted to Horn clauses a text containing almost complete knowledge about the rules of the game. <p> In this sense we go far beyond the initial work exampli-fied by [ Moulin and Rousseau, 1992 ] and <ref> [ Cohen, 1990 ] </ref> . For example, our current domain part of the personal income tax law as described in (Revenue Canada 1991) - contains deep knowledge which cannot be made explicit and used efficiently by a simple learning strategy such as in Cohen (1991). <p> This issue is further complicated by the fact that there may be variant operationalities for different rules and to different readers. Ideally, this can be solved by including many examples of past claims and generalizing from them (e.g. using the approach in <ref> [ Cohen, 1990 ] </ref> ). To control generalization to obtain good operationality, we need to trade-off the coverage of rules against efficiency when executed. In the tax guide the examples are not numerous, so these examples have to come from additional sources (e.g. past claim records).
Reference: [ Copeck et al., 1992 ] <author> T. Copeck, S. Delisle, and S. Szpako-wicz. </author> <title> Parsing and case analysis in TANKA. </title> <booktitle> In Proceedings of the 15th Intl. Conf. on Computational Linguistics COLING-92, </booktitle> <pages> pages 1008-1012, </pages> <address> Nantes, </address> <year> 1992. </year>
Reference-contexts: Handling cross-references within the text will be facilitated by the hierarchical representation of the text in MaLTe's text-base, using a set of text organization and management tools developed for the TANKA case analyzer <ref> [ Copeck et al., 1992 ] </ref> . References pointing to a different text can be considered part of background knowledge. Some examples contain rather complicated conditional rules. <p> The linguistic justification of TANKA's case system is given in <ref> [ Copeck et al., 1992 ] </ref> . <p> This is addressed through the incremental building of a knowledge base in interaction with the user, rather than by separate handcrafting. Linguistic analysis will be performed by the DIPETT parser and the TANKA case analyzer <ref> [ Copeck et al., 1992 ] </ref> . Knowledge converted in this way can be implicit and ill--organized. To overcome this, we exploit various abstraction and EBG techniques developed in machine learning.
Reference: [ DeJong and Mooney, 1986 ] <author> G. DeJong and R. Mooney. </author> <title> Explanation-Based Learning: An Alternative View. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: The following are part of the sentence-to-sentence translations of the facts available about Jim's family. deemed_resident (jim). deemed_resident (louise). work_agt_manr_lat (louise,part_time,a_store). pay_agt_obj_recp (jim,dollar_1500,a_neighbour). take_care_agt_obj (a_neighbour,child1). take_care_agt_obj (a_neighbour,child2). child_care_expenses (dollar_1500). 3 Learning strategies for text-to-rule translation Learning can contribute positively to elicit semantic relationships <ref> [ DeJong and Mooney, 1986 ] </ref> e.g., to complete, correct, and integrate the text-to-rule translations. We are interested in two roles that learning can play; they are knowledge abstraction and explanation-based generalization (EBG) [ Mitchell et al., 1986 ] .
Reference: [ Delisle and Szpakowicz, 1991 ] <author> S. Delisle and S. Szpako-wicz. </author> <title> A broad-coverage parser for knowledge acquisition from technical texts. </title> <editor> In E. Johnson, ed., </editor> <booktitle> Proc. Fifth Intl. Conf. on Symbolic and Logic Computing, </booktitle> <pages> pages 169-183, </pages> <year> 1991. </year>
Reference-contexts: Alternatively, there are methods for learning with incomplete knowledge. period you were separated was less than 90 days etc.''. A clue may appear before the last item, but the ambiguity remains until then. In such situations, the parser <ref> [ Delisle and Szpakowicz, 1991 ] </ref> will have to resort to the user for resolving the problem. Explanations (elements of proof trace referring to some nodes, but not to rules) are given in some examples: it could be interesting to use them to help EBG and validate its results.
Reference: [ Dietterich, 1986 ] <author> T. Dietterich. </author> <title> Learning at the Knowledge Level. </title> <journal> Machine Learning, </journal> <volume> 1(3) </volume> <pages> 287-316, </pages> <year> 1986. </year>
Reference-contexts: Moulin and Rousseau chose a more realistic domain (the National Building Code of Canada a set of safety rules and constraints for building design) and used natural language processing techniques to generate rules from the text. Their system, however, performs no learning at either knowledge or symbol level <ref> [ Dietterich, 1986 ] </ref> . Each of these two methods tackles only one part of knowledge extraction from texts, without addressing the other. We propose to work on knowledge extraction from texts in its entirety, i.e., incorporating both text conversion and learning.
Reference: [ Gomez, 1991 ] <author> F. Gomez. </author> <title> The Acquisition of Word Syntactic Knowledge from Sample Sentences 6th Knowledge Acquisition from Knowledge-Based Systems Workshop, </title> <address> Banff, Alberta (Canada), </address> <pages> pages 9-1 - 9-17, </pages> <year> 1991. </year>
Reference-contexts: it will be entered by the user in order to resolve ambiguities, address inconsistencies, and define synonyms encountered in the extraction process, rather than engage in an open ended knowledge acquisition exercise. * Most existing systems performing knowledge extraction from texts have difficulty dealing with examples, and many ignore them <ref> [ Gomez, 1991 ] </ref> , [ Reimer, 1990 ] . We believe, however, that examples often provide knowledge not represented elsewhere in the text, and are meant to be generalized by the user. We propose some specific machine learning techniques to provide this capability in MaLTe.
Reference: [ Matwin and Szpakowicz, 1992 ] <author> S. Matwin and S. Szpa-kowicz. </author> <title> Machine learning techniques in knowledge acquisition from text. </title> <journal> THINK, </journal> <volume> 1 </volume> (2):37-50, 1992. 
Reference-contexts: We are developing a system called MaLTe (Machine Learning from Text), Figure 1, that will accept a broad class of real-life texts [ Delisle and Szpako-wicz, 1991 ] , and return a representation of the knowledge contained in the text <ref> [ Matwin and Szpakowicz, 1992 ] </ref> , including the examples that belong to the text as long as they are expressed in natural language. <p> The linguistic justification of TANKA's case system is given in [ Copeck et al., 1992 ] . The clause above translates the whole sentence; however, we cannot be certain at this stage whether the 2 In fact, the constants and variables are typed <ref> [ Matwin and Szpakowicz, 1992 ] </ref> , but for the purpose of this paper we decided to ignore them. location and time information will be relevant for producing rules, so we must bear in mind that the following are also possible candidates for the sentence: post_agt_obj (nil,jim). post_agt_obj_lto (nil,jim,lahr). post_agt_obj_tat (nil,jim,year_1989).
Reference: [ Mitchell et al., 1986 ] <author> T.M. Mitchell, R.M. Keller, and S.T. Kedar-Cabelli. </author> <title> Explanation-Based Generalization: A Unifying View. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: We are interested in two roles that learning can play; they are knowledge abstraction and explanation-based generalization (EBG) <ref> [ Mitchell et al., 1986 ] </ref> . A significant characteristic of texts is the need to abstract implicit knowledge (Section 2). <p> Explanation-based generalization is a well-established technique in machine learning <ref> [ Mitchell et al., 1986 ] </ref> to compile pieces of knowledge into useful units. EBG can integrate the translated rules so that the most useful features (e.g., "a mentally disabled child") are explicit.
Reference: [ Moulin and Rousseau, 1992 ] <author> B. Moulin and D. Rousseau. </author> <title> Automated knowledge acquisition from regulatory texts. </title> <journal> IEEE Expert, </journal> <volume> Oct.:27-35, </volume> <year> 1992. </year>
Reference-contexts: So far, there have been only a few answers to this challenge, like the work of [ Cohen, 1990 ] and <ref> [ Moulin and Rousseau, 1992 ] </ref> . Cohen, working on the card game of bridge, manually converted to Horn clauses a text containing almost complete knowledge about the rules of the game. <p> We believe firmly that learning will be a necessary component of our system, i.e., that text analysis without learning will produce less knowledge, and of a shallower nature, than text analysis combined with learning techniques. In this sense we go far beyond the initial work exampli-fied by <ref> [ Moulin and Rousseau, 1992 ] </ref> and [ Cohen, 1990 ] .
Reference: [ Muggleton and Feng, 1992 ] <author> S. Muggleton and C. Feng. </author> <title> Efficient Induction of Logic Programs. In Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: We want to abstract a set of facts acquired from examples in the text into higher-level concepts present in the narrative. To achieve this we propose to use absorption an operator used in inductive logic programming (ILP) for knowledge abstraction ( [ Muggle-ton, 1992 ] , <ref> [ Muggleton and Feng, 1992 ] </ref> , [ Sammut and Banerji, 1986 ] ). Texts can also be ill-organized when they contain multiple levels of concepts, each level supporting its immediate upper level (Section 2).
Reference: [ Muggleton, 1992 ] <author> S.H. Muggleton. </author> <title> Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference: [ Powers and Turk, 1991 ] <author> D. Powers and C. Turk. </author> <title> Machine Learning of Natural Language. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: our objective is to apply learning within a system that extracts knowledge from texts, and not to address cognitively motivated problems of 1. text 2. output from parsing 3. isolated case structuresMach ine Learning 4. conceptual network narratives examples to the performance module Machine Learning full-fledged learning of natural language <ref> [ Powers and Turk, 1991 ] </ref> . We start with the following tenets: * Fully automatic knowledge extraction from text is practically impossible unless the text describes a toy domain, and unless a thorough representation of common-sense and domain knowledge is available.
Reference: [ Reimer, 1990 ] <author> U. Reimer. </author> <title> Automatic Knowledge Acquisition from Texts: Learning Terminological Knowledge via Text Understanding and Inductive Generalization 6th Knowledge Acquisition from Knowledge-Based Systems Workshop, </title> <address> Banff, Alberta (Canada), </address> <pages> pp 17-1 - 17-16, </pages> <year> 1990. </year>
Reference-contexts: the user in order to resolve ambiguities, address inconsistencies, and define synonyms encountered in the extraction process, rather than engage in an open ended knowledge acquisition exercise. * Most existing systems performing knowledge extraction from texts have difficulty dealing with examples, and many ignore them [ Gomez, 1991 ] , <ref> [ Reimer, 1990 ] </ref> . We believe, however, that examples often provide knowledge not represented elsewhere in the text, and are meant to be generalized by the user. We propose some specific machine learning techniques to provide this capability in MaLTe.
Reference: [ Sammut and Banerji, 1986 ] <author> C. Sammut and R. Banerji. </author> <title> Learning Concepts by Asking Questions. </title> <editor> In R. Michalski, J. Carbonell, and T. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <volume> Vol. 2, </volume> <pages> pages 167-192. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1986. </year>
Reference-contexts: To achieve this we propose to use absorption an operator used in inductive logic programming (ILP) for knowledge abstraction ( [ Muggle-ton, 1992 ] , [ Muggleton and Feng, 1992 ] , <ref> [ Sammut and Banerji, 1986 ] </ref> ). Texts can also be ill-organized when they contain multiple levels of concepts, each level supporting its immediate upper level (Section 2).
References-found: 14

