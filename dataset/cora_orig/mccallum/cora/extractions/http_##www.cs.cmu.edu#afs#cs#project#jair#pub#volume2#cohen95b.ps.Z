URL: http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume2/cohen95b.ps.Z
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/cohen95b.html
Root-URL: 
Keyword: Pac-learning Recursive Logic Programs:  
Note: Journal of Artificial Intelligence Research 2 (1995) 541-573 Submitted 9/94; published 5/95  
Abstract: Negative Results Abstract In a companion paper it was shown that the class of constant-depth determinate k-ary recursive clauses is efficiently learnable. In this paper we present negative results showing that any natural generalization of this class is hard to learn in Valiant's model of pac-learnability. In particular, we show that the following program classes are cryptographically hard to learn: programs with an unbounded number of constant-depth linear recursive clauses; programs with one constant-depth determinate clause containing an unbounded number of recursive calls; and programs with one linear recursive clause of constant locality. These results immediately imply the non-learnability of any more general class of programs. We also show that learning a constant-depth determinate program with either two linear recursive clauses or one linear recursive clause and one non-recursive clause is as hard as learning boolean DNF. Together with positive results from the companion paper, these negative results establish a boundary of efficient learnability for recursive function-free clauses.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., Lapointe, S., Ling, C. X., & Matwin, S. </author> <year> (1994). </year> <title> Inverting implication with small training sets. In Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. Lecture Notes in Computer Science # 784. 570 Pac-Learning Recursive Logic Programs: Negative Results Biermann, </title> <editor> A. </editor> <year> (1978). </year> <title> The inference of regular lisp programs from examples. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 8 (8). </volume>
Reference: <author> Chandra, A. K., Kozen, D. C., & Stockmeyer, L. J. </author> <year> (1981). </year> <title> Alternation. </title> <journal> Journal of the ACM, </journal> <volume> 28, </volume> <pages> 114-113. </pages>
Reference-contexts: Precisely the same construction used above can be used to reduce the class of nondeterministic log-space bounded Turing machines to the constant-depth determinate linear recursive programs. Further, a slight modification to the construction can be used to reduce the class of log-space bounded alternating Turing machines <ref> (Chandra, Kozen, & Stockmeyer, 1981) </ref> to constant-depth determinate 2-ary recursive programs.
Reference: <author> Cohen, W. W. </author> <year> (1993a). </year> <title> Cryptographic limitations on learning one-clause logic programs. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence Washington, </booktitle> <address> D.C. </address>
Reference-contexts: Another reasonable question to ask is if linear closed recursive programs can be learned without the restriction of constant-depth determinacy. In earlier papers <ref> (Cohen, 1993a, 1994a, 1993b) </ref> we have studied the conditions under which the constant-depth determinacy restriction can be relaxed while still allowing learn-ability for nonrecursive clauses. It turns out that most generalizations of constant-depth determinate clauses are not predictable, even without recursion. <p> Proof: We will produce a DB n;r 2 DB and Dec n;r 2 2-DetDEC such that predicting DNF can be reduced to predicting 1-Depth-2-Clause [DB n;r ; Dec n;r ]. The construction makes use of a trick first used in Theorem 3 of <ref> (Cohen, 1993a) </ref>, in which a DNF formula is emulated by a conjunction containing a single variable Y which is existentially quantified over a restricted range. We begin with the instance mapping f i .
Reference: <author> Cohen, W. W. </author> <year> (1993b). </year> <title> Pac-learning non-recursive Prolog clauses. </title> <note> To appear in Artificial Intelligence. </note>
Reference-contexts: The locality of a clause is the size of its largest locale. Let k-LocalNonRec denote the language of nonrecursive clauses with locality k or less. (That is, k-LocalNonRec is the set of logic programs containing a single nonrecursive k-local clause.) The following facts are known <ref> (Cohen, 1993b) </ref>: * For fixed k and a, the language family k-LocalNonRec [a-DB; aDEC] is uniformly pac-learnable. * For every constant d, every constant a, every database DB 2 aDB, every declaration Dec 2 aDetDEC, and every clause C 2 d-DepthNonRec [DB; Dec], there is an 553 Cohen equivalent clause C <p> Among the most closely related prior results are the negative results we have previously obtained for certain classes of nonrecursive function-free logic programs <ref> (Cohen, 1993b) </ref>. These results are similar in character to the results described here, but apply to nonrecursive languages. Similar cryptographic results have been obtained by Frazier and Page (1993) for certain classes of programs (both recursive and nonrecursive) that contain function symbols but disallow background knowledge. <p> These results contribute to machine learning in several ways. From the point of view of computational learning theory, several results are technically interesting. One is the prediction-equivalence of several classes of restricted logic programs and boolean DNF; this result, together with others like it <ref> (Cohen, 1993b) </ref>, reinforces the importance of the learn-ability problem for DNF. This paper also gives a dramatic example of how adding recursion can have widely differing effects on learnability: while constant-depth determinate clauses remain pac-learnable when linear recursion is added, constant-locality clauses become cryptographically hard.
Reference: <author> Cohen, W. W. </author> <year> (1993c). </year> <title> Rapid prototyping of ILP systems using explicit bias. </title> <booktitle> In Proceedings of the 1993 IJCAI Workshop on Inductive Logic Programming Chambery, </booktitle> <address> France. </address>
Reference: <author> Cohen, W. W. </author> <year> (1994a). </year> <title> Pac-learning nondeterminate clauses. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence Seattle, </booktitle> <address> WA. </address>
Reference: <author> Cohen, W. W. </author> <year> (1994b). </year> <title> Recovering software specifications with inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence Seattle, </booktitle> <address> WA. </address>
Reference: <author> Cohen, W. W. </author> <year> (1995). </year> <title> Pac-learning recursive logic programs: efficient algorithms. </title> <journal> Journal of AI Research, </journal> <volume> 2, </volume> <pages> 501-539. </pages>
Reference-contexts: All rights reserved. Cohen research where these limits lie: there are few provably fast methods for learning recursive logic programs, and even fewer meaningful negative results. The starting point for this investigation is a series of positive learnability results appearing in a companion paper <ref> (Cohen, 1995) </ref>. These results show that a single constant-depth determinate clause with a constant number of "closed" recursive calls is pac-learnable. <p> Finally, we summarize the results of this paper and its companion, discuss related work, and conclude. Although this paper can be read independently of its companion paper we suggest that readers planning to read both papers begin with the companion paper <ref> (Cohen, 1995) </ref>. 2. Background For completeness, we will now present the technical background needed to state our results; however, aside from Sections 2.2 and 2.3, which introduce polynomial predictability and prediction-preserving reducibilities, respectively, this background closely follows that presented in the companion paper (Cohen, 1995). <p> both papers begin with the companion paper <ref> (Cohen, 1995) </ref>. 2. Background For completeness, we will now present the technical background needed to state our results; however, aside from Sections 2.2 and 2.3, which introduce polynomial predictability and prediction-preserving reducibilities, respectively, this background closely follows that presented in the companion paper (Cohen, 1995). <p> However, if programs are interpreted as sets of extended instances, such trivial learning algorithms become impossible; even for extremely restricted program classes there are still an exponential number of extended instances of size n. Further discussion can be found in the companion paper <ref> (Cohen, 1995) </ref>. <p> PacLearn satisfies all the requirements in the definition of polynomial predictability, and 2. on inputs S + and S , PacLearn always outputs a hypothesis H 2 Lang. Thus if a language is pac-learnable it is predictable. In the companion paper <ref> (Cohen, 1995) </ref>, our positive results are all expressed in the model of identifiability from equivalence queries, which is strictly stronger than pac-learnability; that is, anything that is learnable from equivalence queries is also necessarily pac-learnable. 1 Since this paper contains only negative results, we will use the the relatively weak model <p> To account for these additional inputs it is necessary to extend the framework described above to a setting where the learner accepts inputs other than training examples. Following the formalization used in the companion paper <ref> (Cohen, 1995) </ref>, we will adopt the notion of a "language family". <p> In this section we will make use of Theorem 1 and previous cryptographic hardness results to show that certain restricted classes of recursive logic programs are hard to learn. 3.1 Programs With n Linear Recursive Clauses In a companion paper <ref> (Cohen, 1995) </ref> we showed that a single linear closed recursive clause was identifiable from equivalence queries. In this section we will show that a program with a polynomial number of such clauses is not identifiable from equivalence queries, nor even polynomially predictable. <p> easily verified that the size of this program is polynomial in n and s, and that the clause is determinate and of depth three. 3.3 One k-Local Linear Closed Recursive Clause So far we have considered only one class of extensions to the positive result given in the companion paper <ref> (Cohen, 1995) </ref>|namely, relaxing the restrictions imposed on the recursive structure of the target program. Another reasonable question to ask is if linear closed recursive programs can be learned without the restriction of constant-depth determinacy. <p> Similarly, Theorem 3 shows that a single recursive k-local clause is not predictable for k 4. It is still reasonable to ask, however, if the positive result for bounded-depth determinate recursive clauses <ref> (Cohen, 1995) </ref> can be extended to k-ary closed recursive k-local clauses. Unfortunately, we have the following negative result, which shows that even linear closed recursive clauses are not learnable. <p> Thus concept membership is preserved by the mapping. This completes the proof. 4. DNF-Hardness Results for Recursive Programs To summarize previous results for determinate clauses, it was shown that while a single k-ary closed recursive depth-d clause is pac-learnable <ref> (Cohen, 1995) </ref>, a set of n linear closed recursive depth-d clauses is not; further, even a single n-ary closed recursive depth-d clauses is not pac-learnable. <p> in computational learning theory. 557 Cohen 4.1 A Linear Recursive Clause Plus a Base Clause Previous work has established that two-clause constant-depth determinate programs consisting of one linear recursive clause and one nonrecursive clause can be identified, given two types of oracles: the standard equivalence-query oracle, and a "basecase oracle' <ref> (Cohen, 1995) </ref>. (The basecase oracle determines if an example is covered by the nonrecursive clause alone.) In this section we will show that in the absence of the basecase oracle, the learning problem is as hard as learning boolean DNF. <p> Thus concept membership is preserved by the mapping. This concludes the proof. 4.2 Two Linear Recursive Clauses Recall again that a single linear closed recursive clause is identifiable from equivalence queries <ref> (Cohen, 1995) </ref>. A construction similar to that used in Theorem 5 can be used to show that this result cannot be extended to programs with two linear recursive clauses. <p> Hence if either of these language families is uniformly polynomially predictable, then Dnf [n; fl] is polynomially predictable. Proof: The proof relies on several facts established in the companion paper <ref> (Cohen, 1995) </ref>. * For every declaration Dec, there is a clause BOTTOM fl d (Dec) such that every nonre-cursive depth-d determinate clause C is equivalent to some subclause of BOTTOM fl d . Further, the size of BOTTOM fl d is polynomial in Dec. <p> Finally, none of these languages allow recursion. To our knowledge, there are no other negative learnability results for first-order languages. A discussion of prior positive learnability results for first-order languages can be found in the companion paper <ref> (Cohen, 1995) </ref>. 6. Summary This paper and its companion (Cohen, 1995) have considered a large number of different subsets of Datalog. <p> Finally, none of these languages allow recursion. To our knowledge, there are no other negative learnability results for first-order languages. A discussion of prior positive learnability results for first-order languages can be found in the companion paper <ref> (Cohen, 1995) </ref>. 6. Summary This paper and its companion (Cohen, 1995) have considered a large number of different subsets of Datalog. Our aim has been to be not comprehensive, but systematic: in particular, we wished to find precisely where the boundaries of learnability lie as various syntactic restrictions are imposed and relaxed. <p> Since it is all too easy for a reader to "miss the forest for the trees", we will now briefly summarize the results contained in this paper, together with the positive results of the companion paper <ref> (Cohen, 1995) </ref>. 566 Pac-Learning Recursive Logic Programs: Negative Results Local Constant-Depth Determinate Clauses Clauses nC R nC R jC B k fi nC R R kC + B kC R ; C DNF R n fi kC 1C R 1C R jC + B 2 fi 1C =DNF R Table 1: <p> We have also assumed that recursion is closed , meaning that no output variables appear in a recursive clause; however, we believe that this restriction can be relaxed without fundamentally changing the results of the paper. In the companion paper <ref> (Cohen, 1995) </ref> we showed that a single nonrecursive constant-depth determinate clause was learnable in the strong model of identification from equivalence queries.
Reference: <author> Cohen, W. W., & Hirsh, H. </author> <year> (1994). </year> <title> The learnability of description logics with equality constraints. </title> <journal> Machine Learning, </journal> <volume> 17 (2/3). </volume>
Reference-contexts: Similar results have also been obtained for two restricted languages of Horn clauses (Kietz, 1993); a simple description logic <ref> (Cohen & Hirsh, 1994) </ref>; and for the language of sorted first-order terms (Page & Frisch, 1992). All of these results, however, are specific to the model pac-learnability, and none can be easily extended to the polynomial predictability model considered here.
Reference: <author> De Raedt, L., & Dzeroski, S. </author> <year> (1994). </year> <title> First-order jk-clausal theories are PAC-learnable. </title>
Reference-contexts: It is possible that complete datasets allow a more expressive class of programs to be learned than random datasets; in fact, some progress has been recently made toward formalizing this conjecture <ref> (De Raedt & Dzeroski, 1994) </ref>. Finally, and most importantly, this paper has established the boundaries of learnability for determinate recursive programs in the pac-learnability model. In many plausible automatic programming contexts it would be highly desirable to have a system that offered some formal guarantees of correctness.
Reference: <editor> In Wrobel, S. (Ed.), </editor> <booktitle> Proceedings of the Fourth International Workshop on Inductive Logic Programming Bad Honnef/Bonn, </booktitle> <address> Germany. </address>
Reference: <author> Dzeroski, S., Muggleton, S., & Russell, S. </author> <year> (1992). </year> <title> Pac-learnability of determinate logic programs. </title> <booktitle> In Proceedings of the 1992 Workshop on Computational Learning Theory Pittsburgh, </booktitle> <address> Pennsylvania. </address>
Reference-contexts: Then there is a polynomial poly 1 so that for any database DB 2 DB, Subclause C [DB ; Dec] Monomial [poly 1 (jjDB jj)] 563 Cohen Proof of lemma: Follows immediately from the construction used in Theorem 1 of Dzeroski, Muggleton, and Russell <ref> (Dzeroski et al., 1992) </ref>. (The basic idea of the construction is to introduce a propositional variable representing the "success" of each connected chain of literals in C. Any subclause of C can then be represented as a conjunction of these propositions.) This lemma can be extended as follows. <p> This result implies that a single nonrecursive constant-depth determinate clause is pac-learnable (as the counterexample oracle can be emulated by drawing random examples in the pac setting). The result is not novel <ref> (Dzeroski et al., 1992) </ref>; however the proof given is independent, and is also of independent interest. Notably, it is somewhat more rigorous than earlier proofs, and also proves the result directly, rather than via reduction to a propositional learning problem.
Reference: <author> Frazier, M., & Page, C. D. </author> <year> (1993). </year> <title> Learnability of recursive, non-determinate theories: Some basic results and techniques. </title> <booktitle> In Proceedings of the Third International Workshop on Inductive Logic Programming Bled, </booktitle> <pages> Slovenia. </pages>
Reference: <author> Haussler, D. </author> <year> (1989). </year> <title> Learning conjunctive concepts in structural domains. </title> <journal> Machine Learning, </journal> <volume> 4 (1). </volume>
Reference: <author> Hopcroft, J. E., & Ullman, J. D. </author> <year> (1979). </year> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley. </publisher>
Reference: <author> Kearns, M., & Valiant, L. </author> <year> (1989). </year> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <booktitle> In 21th Annual Symposium on the Theory of Computing. </booktitle> <publisher> ACM Press. </publisher> <address> 571 Cohen Kietz, J.-U. </address> <year> (1993). </year> <title> Some computational lower bounds for the computational complexity of inductive logic programming. </title> <booktitle> In Proceedings of the 1993 European Conference on Machine Learning Vienna, </booktitle> <address> Austria. </address>
Reference-contexts: This result holds because all of these crypto graphic problems can be reduced to learning DLOG Turing machines <ref> (Kearns & Valiant, 1989) </ref>. 549 Cohen * write either a 1 or a 0 on the work tape, * shift the input tape head left or right, * shift the work tape head left or right, and * transition to a new internal state q 0 A deterministic machine can thus
Reference: <author> King, R. D., Muggleton, S., Lewis, R. A., & Sternberg, M. J. E. </author> <year> (1992). </year> <title> Drug design by machine learning: the use of inductive logic programming to model the structure-activity relationships of trimethoprim analogues binding to dihydrofolate reductase. </title> <booktitle> Proceedings of the National Academy of Science, </booktitle> <pages> 89. </pages>
Reference: <author> Lavrac, N., & Dzeroski, S. </author> <year> (1992). </year> <title> Background knowledge and declarative bias in inductive concept learning. </title> <editor> In Jantke, K. P. (Ed.), </editor> <title> Analogical and Inductive Inference: </title> <booktitle> International Workshop AII'92. </booktitle> <publisher> Springer Verlag, </publisher> <address> Daghstuhl Castle, Germany. </address> <booktitle> Lectures in Artificial Intelligence Series #642. </booktitle>
Reference: <author> Lloyd, J. W. </author> <year> (1987). </year> <title> Foundations of Logic Programming: Second Edition. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Readers are encouraged to skip this section if they are already familiar with the material. 2.1 Logic Programs We will assume that the reader has some familiarity in logic programming (such as can be obtained by reading one of the standard texts <ref> (Lloyd, 1987) </ref>.) Our treatment of logic programs differs only in that we will usually consider the body of a clause to be an ordered set of literals. We will also consider only logic programs without function symbols|i.e., programs written in Datalog.
Reference: <author> Muggleton, S., & De Raedt, L. </author> <year> (1994). </year> <title> Inductive logic programming: Theory and methods. </title> <journal> Journal of Logic Programming, </journal> <volume> 19/20 (7), </volume> <pages> 629-679. </pages>
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1992). </year> <title> Efficient induction of logic programs. In Inductive Logic Programming. </title> <publisher> Academic Press. </publisher>
Reference-contexts: A clause is determinate if all of its literals are determinate. Informally, determinate clauses are those that can be evaluated without backtracking by a Prolog interpreter. The term ij-determinate <ref> (Muggleton & Feng, 1992) </ref> is sometimes used for programs that are depth i, determinate, and contain literals of arity j or less. A number of experimental systems exploit restrictions associated with limited depth and determinacy (Muggleton & Feng, 1992; Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c).
Reference: <author> Muggleton, S., King, R. D., & Sternberg, M. J. E. </author> <year> (1992). </year> <title> Protein secondary structure prediction using logic-based machine learning. </title> <journal> Protein Engineering, </journal> <volume> 5 (7), </volume> <pages> 647-657. </pages>
Reference-contexts: A clause is determinate if all of its literals are determinate. Informally, determinate clauses are those that can be evaluated without backtracking by a Prolog interpreter. The term ij-determinate <ref> (Muggleton & Feng, 1992) </ref> is sometimes used for programs that are depth i, determinate, and contain literals of arity j or less. A number of experimental systems exploit restrictions associated with limited depth and determinacy (Muggleton & Feng, 1992; Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c).
Reference: <author> Muggleton, S. H. (Ed.). </author> <year> (1992). </year> <title> Inductive Logic Programming. </title> <publisher> Academic Press. </publisher>
Reference-contexts: A clause is determinate if all of its literals are determinate. Informally, determinate clauses are those that can be evaluated without backtracking by a Prolog interpreter. The term ij-determinate <ref> (Muggleton & Feng, 1992) </ref> is sometimes used for programs that are depth i, determinate, and contain literals of arity j or less. A number of experimental systems exploit restrictions associated with limited depth and determinacy (Muggleton & Feng, 1992; Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c).
Reference: <author> Page, C. D., & Frisch, A. M. </author> <year> (1992). </year> <title> Generalization and learnability: A study of constrained atoms. In Inductive Logic Programming. </title> <publisher> Academic Press. </publisher>
Reference-contexts: Similar results have also been obtained for two restricted languages of Horn clauses (Kietz, 1993); a simple description logic (Cohen & Hirsh, 1994); and for the language of sorted first-order terms <ref> (Page & Frisch, 1992) </ref>. All of these results, however, are specific to the model pac-learnability, and none can be easily extended to the polynomial predictability model considered here. The results also do not extend to languages more expressive than these specific constrained languages. Finally, none of these languages allow recursion.
Reference: <author> Pazzani, M., & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 (1). </volume>
Reference: <author> Pitt, L., & Warmuth, M. K. </author> <year> (1988). </year> <title> Reductions among prediction problems: On the difficulty of predicting automata. </title> <booktitle> In Proceedings of the 3rd Annual IEEE Conference on Structure in Complexity Theory Washington, </booktitle> <address> D.C. </address> <publisher> Computer Society Press of the IEEE. </publisher>
Reference-contexts: Some prior negative results have also been obtained on the learnability of other first-order languages using the proof technique of consistency hardness <ref> (Pitt & Valiant, 1988) </ref>. Haussler (1989) showed that the language of "existential conjunction concepts" is not pac-learnable by showing that it can be hard to find a concept in the language consistent with a given set of examples.
Reference: <author> Pitt, L., & Valiant, L. </author> <year> (1988). </year> <title> Computational limitations on learning from examples. </title> <journal> Journal of the ACM, </journal> <volume> 35 (4), </volume> <pages> 965-984. </pages>
Reference-contexts: Some prior negative results have also been obtained on the learnability of other first-order languages using the proof technique of consistency hardness <ref> (Pitt & Valiant, 1988) </ref>. Haussler (1989) showed that the language of "existential conjunction concepts" is not pac-learnable by showing that it can be hard to find a concept in the language consistent with a given set of examples.
Reference: <author> Pitt, L., & Warmuth, M. </author> <year> (1990). </year> <title> Prediction-preserving reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 41, </volume> <pages> 430-467. </pages>
Reference-contexts: Notice that this is a worst case learning model, as the definition allows an adversarial choice of all the inputs of the learner. 546 Pac-Learning Recursive Logic Programs: Negative Results 2.2.3 Relation to Other Models The model of polynomial predictability has been well-studied <ref> (Pitt & Warmuth, 1990) </ref>, and is a weaker version of Valiant's (1984) criterion of pac-learnability. A language Lang is pac-learnable iff there is an algorithm PacLearn so that 1.
Reference: <author> Quinlan, J. R., & Cameron-Jones, R. M. </author> <year> (1993). </year> <title> FOIL: A midterm report. </title> <editor> In Brazdil, P. B. (Ed.), </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-93 Vienna, Austria. </address> <note> Springer-Verlag. Lecture notes in Computer Science # 667. </note>
Reference-contexts: For example, in many examples in which FOIL has learned recursive logic programs, it has made use of "complete example sets"| datasets containing all examples of or below a certain size, rather than sets of randomly selected examples <ref> (Quinlan & Cameron-Jones, 1993) </ref>. It is possible that complete datasets allow a more expressive class of programs to be learned than random datasets; in fact, some progress has been recently made toward formalizing this conjecture (De Raedt & Dzeroski, 1994).
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. Machine Learning, 5 (3). 572 Pac-Learning Recursive Logic Programs: Negative Results Quinlan, </title> <editor> J. R. </editor> <booktitle> (1991). Determinate literals in inductive logic programming. In Proceedings of the Eighth International Workshop on Machine Learning Ithaca, </booktitle> <address> New York. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rouveirol, C. </author> <year> (1994). </year> <title> Flattening and saturation: two representation changes for generalization. </title> <journal> Machine Learning, </journal> <volume> 14 (2). </volume>
Reference: <author> Summers, P. D. </author> <year> (1977). </year> <title> A methodology for LISP program construction from examples. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 24 (1), </volume> <pages> 161-175. </pages>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 (11). </volume>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1994). </year> <title> Inducing deterministic Prolog parsers from treebanks: a machine learning approach. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence Seattle, </booktitle> <address> Washington. </address> <publisher> MIT Press. </publisher> <pages> 573 </pages>
References-found: 34

