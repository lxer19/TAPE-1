URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95607.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: A Portable Run-Time System for Object-Parallel Systems  
Author: Peter Beckman, Dennis Gannon 
Date: November 8, 1995  
Address: Bloomington, Indiana, U.S.A.  
Affiliation: Department of Computer Science Indiana University,  
Abstract: This paper describes a parallel run-time system (RTS) that is used as part of the pC++ parallel programming language. The RTS has been implemented on a variety of scalable, MPP computers including the IBM SP2, Intel Paragon, Meiko CS2, SGI Power Challenge, and Cray T3D. This system differs from other data-parallel RTS implementations; it is designed to support the operations from object-parallel programming that require remote member function execution and load and store operations on remote data. The implementation is designed to provide the thinnest possible layer atop the vendor-supplied machine interface. That thin veneer can then be used by other run-time layers to build machine independent class libraries, compiler back ends, and more fl This research is supported in part by: ARPA DABT63-94-C-0029 and Rome Labs Contract AF 30602-92-C-0135 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Gannon, F. Bodin, P. Beckman, S. Yang, and S. Narayana. </author> <title> Distributed pC++: Basic ideas for an object parallel language, </title> <journal> Journal of Scientific Programming, </journal> <volume> Vol. 2, </volume> <pages> pp. </pages> <month> 7-22 </month> <year> (1993). </year>
Reference-contexts: In an object-parallel system, the programmer creates and manipulates collection or container objects that encapsulate a distributed data structure on a massively parallel computer. Examples of this style of programming include 1. pC++ <ref> [1] </ref>, a collection based parallel programming language. 2. Single Program Multiple Data (SPMD) C++ class libraries that provide user de fined, encapsulated support for parallel, distributed data structures. Examples include LPARX [2], P++[3], the POOMA library [4] and CHAOS++[5]. 3.
Reference: 2. <author> Scott B. Baden, Scott R. Kohn, Silvia M. Figueria, and Stephen J. Fink, </author> <title> The LPARX 23 User's Guide v1.0, </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of California, </institution> <address> San Diego, La Jolla, CA 92093-0114 USA, </address> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Examples of this style of programming include 1. pC++ [1], a collection based parallel programming language. 2. Single Program Multiple Data (SPMD) C++ class libraries that provide user de fined, encapsulated support for parallel, distributed data structures. Examples include LPARX <ref> [2] </ref>, P++[3], the POOMA library [4] and CHAOS++[5]. 3. Data-parallel extensions to the C++ Standard Templates Library such as the Amelia Vector Template Library (AVTL)[6]. While C++ is the most common language for this work, it is not the only way object parallelism may be expressed.
Reference: 3. <author> R. Parsons, D. Quinlan, </author> <title> A++/P++ Array Classes for Architecture Independent Finite Difference Computations, </title> <booktitle> Proceedings, </booktitle> <address> OONSKI, </address> <year> 1994. </year> <pages> pp. 408-418. </pages>
Reference: 4. <author> J. Reynders, D. Forslund, P. Hinker, M. Tholburn, D. Kilman, W. Humphrey, </author> <title> Object Oriented Particle Simulation on Parallel Computers, </title> <booktitle> Proceedings, </booktitle> <address> OONSKI, </address> <year> 1994. </year> <pages> pp. 266-279. </pages>
Reference-contexts: Examples of this style of programming include 1. pC++ [1], a collection based parallel programming language. 2. Single Program Multiple Data (SPMD) C++ class libraries that provide user de fined, encapsulated support for parallel, distributed data structures. Examples include LPARX [2], P++[3], the POOMA library <ref> [4] </ref> and CHAOS++[5]. 3. Data-parallel extensions to the C++ Standard Templates Library such as the Amelia Vector Template Library (AVTL)[6]. While C++ is the most common language for this work, it is not the only way object parallelism may be expressed.
Reference: 5. <author> C. Chang, A. Sussman, J. Saltz, </author> <title> Support for Distributed Dynamic Data Structures in C++, </title> <institution> Univ. of Maryland, Dept. of Computer Science Technical Report CS-TR-3266, </institution> <year> 1995. </year>
Reference: 6. <author> T. She*er, </author> <title> A Portable MPI-based parallel vector template library, </title> <type> RIACS Technical Report 95.04, </type> <institution> RIACS, NASA Ames Research Center. </institution> <year> 1995. </year>
Reference: 7. <author> A. S. Grimshaw. </author> <title> An introduction to parallel object-oriented programming with Mentat, </title> <type> Technical Report 91 07, </type> <institution> University of Virginia, </institution> <year> 1991. </year>
Reference-contexts: While C++ is the most common language for this work, it is not the only way object parallelism may be expressed. There are many other parallel programming models that exploit object-oriented concepts. These include Mentat <ref> [7] </ref>, CC++[8], CORBA [9], Charm++[10], and UC++[11]. In these systems, the emphasis is on task-level parallelism on networked 2 and heterogeneous systems. As will be explained later, the run-time architectures for these systems have different requirements than for the data-parallel extensions addressed here.
Reference: 8. <author> K. M. Chandy and C. F. Kesselman. </author> <title> CC++: A declarative concurrent object-oriented programming notation, </title> <booktitle> In Research Directions in Object-Oriented Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference: 9. <institution> CORBA 2.0 specification available through OMG. </institution> <note> See WWW reference below </note>
Reference-contexts: While C++ is the most common language for this work, it is not the only way object parallelism may be expressed. There are many other parallel programming models that exploit object-oriented concepts. These include Mentat [7], CC++[8], CORBA <ref> [9] </ref>, Charm++[10], and UC++[11]. In these systems, the emphasis is on task-level parallelism on networked 2 and heterogeneous systems. As will be explained later, the run-time architectures for these systems have different requirements than for the data-parallel extensions addressed here.
Reference: 10. <author> L. Kale, S. Krisnan, Charm++: </author> <title> A Portable Concurrent Object Oriented System Based on C++. </title> <type> Technical Report. </type> <institution> Univ. of Illinois, Urbana-Champaign, </institution> <year> 1994. </year>
Reference: 11. <institution> UC++ - Europa Parallel C++ Project. </institution> <note> WWW reference only: http://www.lpac.qmw.ac.uk/europa/. 24 </note>
Reference: 12. <author> W. Gropp, E. Lusk and A. Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface. </title> <publisher> MIT Press, </publisher> <year> 1994. </year> <note> 13. </note> <author> von Eicken, Thorsten; Culler, David E.; Goldstein, Seth Copen; Schauser, Klaus Erik. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <address> UCB//CSD-92-675, </address> <month> March </month> <year> 1992. </year> <pages> 20 pages. </pages>
Reference-contexts: However, it should also provide a path to a more general multi-threaded implementation style. In particular, future support for dynamic, nested data parallelism and mixed task-data parallelism will require multi-threaded implementations. 3. Basic message passing should be a fundamental part of the RTS and the MPI standard <ref> [12] </ref> is now well supported on many systems. However, object parallelism differs from data parallelism. Unlike uniform arrays, distributed complex linked structures change size and shape over the lifetime of the program. Consequently, translating all data movements into predetermined send-receive pairs is not always easy.
Reference: 14. <editor> PORTS- POrtable Runtime System consortium. </editor> <title> The PORTS0 Interface, </title> <type> Technical Report, </type> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: The work described here addresses three of these four concerns. This paper does not consider the interface to multi-threaded execution. A working group, known as POrtable Run Time System (PORTS) <ref> [14] </ref>, is currently considering the design of a multi-threaded implementation. We also emphasize that we have not considered issues related to nested data parallelism, which may be handled by a multi-threaded system or by compiler technology such as that pioneered in the design the NESL system [15].
Reference: 15. <author> Guy E. Blelloch, NESL: </author> <note> A Nested Data-Parallel Language CMU CS Technical Report CMU-CS-92-103.ps January 1992 36 pages </note>
Reference-contexts: We also emphasize that we have not considered issues related to nested data parallelism, which may be handled by a multi-threaded system or by compiler technology such as that pioneered in the design the NESL system <ref> [15] </ref>. In addition, a complete RTS would address the issue of interoperability between parallel class libraries, parallel programming languages like pC++, and data-parallel languages like HPF. Such a RTS should provide a foundation for a low-level distributed data structure library that can be shared by many language and library implementations.
Reference: 16. <institution> PCRC- Parallel Compiler Runtime Consortium. </institution> <type> Private communication. </type> <note> see http://www.extreme.indiana.edu/pcrc/index.html. </note>
Reference-contexts: Such a RTS should provide a foundation for a low-level distributed data structure library that can be shared by many language and library implementations. A second consortium of researchers, the Parallel Compiler Run Time Consortium (PCRC) <ref> [16] </ref>, is considering this problem.
Reference: 17. <author> Ian Foster, Carl Kesselman, Robert Olson, and Steven Tuecke Nexus: </author> <title> An Interoper ability Layer for Parallel and Distributed Computer Systems Technical Memorandum ANL/MCS-TM-189, </title> <month> May </month> <year> 1994 </year> <month> TM189.dvi.Z TM189.ps.Z </month>
Reference-contexts: The message size is bounded by the architecture of the processor network interface adaptor. Messages can either interrupt the addressed processor, or a remote-queue and polling mechanism can be used. Our approach differs in that we allow arbitrary argument lengths, and a single (complex) RPC handler per processor. NEXUS <ref> [17] </ref> is an example of a run-time system that is designed for multi-threaded task parallelism on heterogeneous networks. Communication in NEXUS is based on an RPC mechanism and there is no attempt to exploit special hardware for collective communication or remote shared memory.
Reference: 18. <author> Hubertus Franke, </author> <title> MPI-F An MPI Implementation for the IBM SP-1/SP-2, Version 1.39, internal document, </title> <institution> IBM T.J. Watson Research Center, </institution> <month> Feb. 95, </month>
Reference-contexts: IBM interconnects the SP2 nodes with a multi-stage high performance switch (HPS) 7 capable of 40 MB/sec bandwidth and application to application latency of 40 microseconds. While there are several software interfaces for communication on the SP2, we chose an experimental IBM implementation <ref> [18] </ref> of MPI installed on the SP2 at the Cornell Theory Center for this version of the run-time system. 3.2 Network DMA: The Cray T3D The T3D is a distributed memory 3D torus-connected compute array of DEC Alpha microprocessors.
Reference: 19. <author> M. Haines, D. Cronk, P. Mehrotra, </author> <title> On the design of Chant: A talking threads package. </title> <booktitle> Proceedings of Supercomputing 1994, </booktitle> <address> Washington, D.C. </address> <month> Nov. </month> <year> 1994. </year> <month> 25 </month>
Reference-contexts: To agree on a message type to match up each send/receive pairs requires the message type 15 field be split into two logical fields: a message ID (MID) and a message flavor. This technique is quite similar to the way Chant <ref> [19] </ref> uses MPI to send messages to threads, except in this case, the extra bits stolen from the message type field are not used to encode destination, but to distinguish between pending FetchStart () messages.
References-found: 18

