URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/multi-grain.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/multigrain.html
Root-URL: 
Title: MGS: A Multigrain Shared Memory System  
Author: Donald Yeung, John Kubiatowicz, and Anant Agarwal 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Date: May 1996.  
Note: Appears in Proceedings of the 23rd Annual International Symposium on Computer Architecture,  
Abstract: Parallel workstations, each comprising 10-100 processors, promise cost-effective general-purpose multiprocessing. This paper explores the coupling of such small- to medium-scale shared memory multiprocessors through software over a local area network to synthesize larger shared memory systems. We call these systems Distributed Scalable Shared-memory Multiprocessors (DSSMPs). This paper introduces the design of a shared memory system that uses multiple granularities of sharing, and presents an implementation on the Alewife multiprocessor, called MGS. Multigrain shared memory enables the collaboration of hardware and software shared memory, and is effective at exploiting a form of locality called multi-grain locality. The system provides efficient support for fine-grain cache-line sharing, and resorts to coarse-grain page-level sharing only when locality is violated. A framework for characterizing application performance on DSSMPs is also introduced. Using MGS, an in-depth study of several shared memory applications is conducted to understand the behavior of DSSMPs. We find that unmodified shared memory applications can exploit multigrain sharing. Keeping the number of processors fixed, applications execute up to 85% faster when each DSSMP node is a multiprocessor as opposed to a uniprocessor. We also show that tightly-coupled multiprocessors hold a significant performance advantage over DSSMPs on unmodified applications. However, a best-effort implementation of a kernel from one of the applications allows a DSSMP to almost match the performance of a tightly-coupled multiprocessor. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedingsof the 15th ACM Symposium on Operating Systems Principles, </booktitle> <address> Copper Mountain, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Achieving good performance across a wide range of applications, however, is difficult on these systems. While communication interfaces for commodity workstations have made impressive improvements, the best reported inter-workstation latency numbers are still an order of magnitude higher than for tightly-coupled machines <ref> [1] </ref>. High latency limits the granularity of sharing that can be effectively supported by a shared memory implementation that runs on uniprocessor workstations. We believe that an important class of systems is quickly emerging that enables another way to build large-scale multiprocessors. This class of systems is the parallel workstation.
Reference: [2] <author> Alan L. Cox, Sandhya Dwarkadas,Pete Keleher, Honghui Lu, Ramakr-ishnan Rajamony, and Willy Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <address> San Diego, California, </address> <year> 1994. </year>
Reference-contexts: We call a large-scale system built from SSMPs a Distributed Scalable Shared-memory Multiprocessor (DSSMP). Although the DSSMP concept has received attention in recent literature <ref> [2] </ref>, the design issues remain unexplored, no working system has been developed, and no analysis of how applications behave on real DSSMP systems has been provided. This paper proposes a shared memory system for DSSMPs. <p> The goal of the synchronization primitives is to contain communication within an SSMP whenever possible. In this section, we discuss the techniques used for barriers and locks; both of these have been proposed by Cox et al <ref> [2] </ref>. The MGS barrier is a tree barrier that is structured to match the hierarchical structure of the DSSMP. The first level in the tree synchronizes all processors on the same SSMP. The second level synchronizes all the SSMPs. <p> As cluster size increases, performance improves because the amount of work in each tile grows thus reducing the frequency of page-based coherence at phase boundaries. 6 Related Work Several researchers have investigated the potential for clustering in shared memory systems. Cox et al <ref> [2] </ref> study a system built from 8-way bus-based multiprocessors connected over an ATM network. Three key differences distinguish our work from the Cox paper. First, the Cox paper is a simulation study, whereas our work presents a full implementation.
Reference: [3] <author> Anant Agarwal et. al. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Our work introduces a complete shared memory protocol that allows hardware and software shared memory to collaborate in a way that is transparent to the programmer. The feasibility and correctness of our design is demonstrated in a working system that runs on the MIT Alewife platform <ref> [3] </ref>. We call this system MGS. Our work also provides a framework for reasoning about how applications behave on DSSMPs. We define a form of locality, called multigrain locality, that multigrain shared memory systems can efficiently exploit. Applications that demonstrate multigrain locality can achieve good performance on DSSMPs. <p> They do not include the overhead of address translation. The entry labeled Remote Software reports the cost of a read miss to a cache line under software directory control. All measurements are taken for load misses; store misses take slightly longer, and can be found in <ref> [3] </ref>. The second group of measurements shows the cost of software translation. Translation for both distributed array objects and general pointers are shown. Finally, the bottom group of measurements report on the cost of MGS' software coherence protocol.
Reference: [4] <author> Harjinder S. Sandhu, Benjamin Gamsa, and Songnian Zhou. </author> <title> The Shared Regions Approach to Software Cache Coherence on Multiprocessors. </title> <booktitle> In Principles and Practices of Parallel Programming, </booktitle> <year> 1993, </year> <pages> pages 229-238, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This suggests the use of two separate grains: a cache-line sized grain when using hardware shared memory, and a larger page-sized grain when using software shared memory. Other solutions to the grain size problem have explored variable grains <ref> [4, 5] </ref>. These solutions leverage application-specific knowledge to select the optimal grain. While this can be effective, it is difficult for a compiler and painful for a programmer. Multigrain shared memory does not rely on application-specific knowledge. <p> Galactica Net [16] allows both cache-line size and page-size grains of sharing; however, they rely on hardware support between clusters to implement update-based protocols at cache-line grain to efficiently support fine-grain write sharing between clusters. Chan-dra et al [17], Shared Regions <ref> [4] </ref>, and CRL [5] allow coherence to happen at variable grains, but treat all processors equally (i.e., no clustering), and require user annotations to identify the regions. MGS runs shared memory applications unmodified. The only requirement is that the programmer uses a release consistent memory model.
Reference: [5] <author> Kirk Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <address> Copper Mountain, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: This suggests the use of two separate grains: a cache-line sized grain when using hardware shared memory, and a larger page-sized grain when using software shared memory. Other solutions to the grain size problem have explored variable grains <ref> [4, 5] </ref>. These solutions leverage application-specific knowledge to select the optimal grain. While this can be effective, it is difficult for a compiler and painful for a programmer. Multigrain shared memory does not rely on application-specific knowledge. <p> Galactica Net [16] allows both cache-line size and page-size grains of sharing; however, they rely on hardware support between clusters to implement update-based protocols at cache-line grain to efficiently support fine-grain write sharing between clusters. Chan-dra et al [17], Shared Regions [4], and CRL <ref> [5] </ref> allow coherence to happen at variable grains, but treat all processors equally (i.e., no clustering), and require user annotations to identify the regions. MGS runs shared memory applications unmodified. The only requirement is that the programmer uses a release consistent memory model.
Reference: [6] <author> P. J. Denning. </author> <title> The Working Set Model for Program Behavior. </title> <journal> Communications of the ACM, </journal> <volume> 11(5) </volume> <pages> 323-333, </pages> <month> May </month> <year> 1968. </year>
Reference-contexts: Instead, it supports fine-grain sharing when access patterns exhibit locality, and resorts to coarse-grain sharing only when locality is violated. 2.3 Locality DSSMPs exploit cluster locality and multigrain locality, each of which can be informally defined based on the notion of working sets. The working set property <ref> [6] </ref> exhibited by a processor's memory references is a statement about locality of reference. The working set property states that the size of the set of unique memory blocks 2 function of cluster size.
Reference: [7] <author> Timothy Mark Pinkston and Sandra Johnson Baylor. </author> <title> Parallel Processor Memory Reference Analysis: Examining Locality and Clustering Potential. </title> <type> RC 15801, </type> <institution> IBM T. J. Watson Research Center, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: Breakup penalty, multigrain potential, and multigrain curvature are labeled for curve A. accessedby a processor in a given interval of time is small compared to the total number of memory blocks in the program's dataset. Cluster locality, as defined in <ref> [7] </ref>, states that the working sets of processors in the same cluster are more likely to intersect than those of processors in different clusters. Cluster locality can be exploited by any hierarchical shared memory system. In addition to cluster locality, we introduce multigrain locality.
Reference: [8] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the 13th Annual Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: One of the processors has a read-write mapping, while another has a read-only mapping. 3.1.1 Memory Consistency The page-based software protocol in MGS is release consistent 1 , invalidation-based, and supports multiple writers. The specifics of the page-based protocol borrow heavily from Munin <ref> [8] </ref>. Like Munin, MGS uses a delayed update queue (DUQ) to track dirty pages and to propagate their changes back to the home location at release time. <p> MGS runs shared memory applications unmodified. The only requirement is that the programmer uses a release consistent memory model. Finally, there is a large body of work on software distributed shared memory. Our particular DSM implementation borrows from the Munin system <ref> [8] </ref>. Other approaches have tried to minimize message traffic and optimize memory management policies [18, 19, 20, 21]. MGS would benefit from these techniques. 7 Conclusion Using small- to medium-scale multiprocessors as the basic building block for large-scale multiprocessors is attractive for two reasons.
Reference: [9] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In Proceedings of the International Supercomputing Conference, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Alewife supports sequential consistency, and maintains cache coherence using a single-writer write-invalidate cache coherence protocol. Also, Alewife provides a fast messaging interface with DMA capability <ref> [9] </ref>. DMA data in messages are locally coherent. 4.2 Support for MGS 4.2.1 Software Virtual Memory MGS relies on virtual memory. On machines with hardware support for virtual memory, MGS would require special TLB fault handlers. Because Alewife does not support virtual memory, MGS performs address translation in software. <p> On Alewife, DMA in messages is only locally coherent; global coherence <ref> [9] </ref> is synthesized in software through a process called page cleaning. Cleaning a page requires generating invalidations for every cache line in the page.
Reference: [10] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-92-526, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Jacobi is a 2-D grid relaxation program. Matrix Multiply multiplies two square matrices. TSP computes the solution to a 10-city traveling salesman problem using a branch and bound algorithm, and a centralized work queue to distribute work. Water is a benchmark from the original SPLASH suite <ref> [10] </ref>. It is a 3-D simulation of the motion of water molecules. Barnes-Hut is also taken from the original SPLASH suite. It is a 3-D hierarchical n-body simulation. A modification was made in the way cells are allocated to relieve severe contention on a centralized lock.
Reference: [11] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Barnes-Hut is also taken from the original SPLASH suite. It is a 3-D hierarchical n-body simulation. A modification was made in the way cells are allocated to relieve severe contention on a centralized lock. The modification is similar to a modification that is available in the SPLASH-2 <ref> [11] </ref> version of this code. Table 4 lists the five applications, including the Water-kernel, which is a special version of Water to be explained in Section 5.2.3. For each application, the problem size used, the sequential run-time, and the speedup achieved on 32 processors is shown.
Reference: [12] <author> David R. Cheriton, Hendrik A. Goosen, and Patrick D. Boyle. </author> <title> MultiLevel Shared Caching Techniques for Scalability in VMP-MC. </title> <booktitle> In Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <pages> pages 16-24, </pages> <address> Jerusalem, Israel, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Lastly, the Cox study uses bus-based multiprocessors. Our study provides an implementation for NUMA machines. Other systems exploit clustering at a level closer to the processor, typically in the first or second level cache. These systems include VMP-MC <ref> [12] </ref>, DASH [13], and KSR [14]. The benefit of clustering on these systems has been studied [15]. Interference misses due to limited cache capacity and associativity can reduce the benefits of clustering close to the processor.
Reference: [13] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hen-nessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Lastly, the Cox study uses bus-based multiprocessors. Our study provides an implementation for NUMA machines. Other systems exploit clustering at a level closer to the processor, typically in the first or second level cache. These systems include VMP-MC [12], DASH <ref> [13] </ref>, and KSR [14]. The benefit of clustering on these systems has been studied [15]. Interference misses due to limited cache capacity and associativity can reduce the benefits of clustering close to the processor. MGS clusters at the main memory level and thus does not suffer from these effects.
Reference: [14] <institution> Kendall Square Research, Inc., </institution> <address> 170 Tracer Lane, Waltham, MA 02154. </address> <institution> Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: Lastly, the Cox study uses bus-based multiprocessors. Our study provides an implementation for NUMA machines. Other systems exploit clustering at a level closer to the processor, typically in the first or second level cache. These systems include VMP-MC [12], DASH [13], and KSR <ref> [14] </ref>. The benefit of clustering on these systems has been studied [15]. Interference misses due to limited cache capacity and associativity can reduce the benefits of clustering close to the processor. MGS clusters at the main memory level and thus does not suffer from these effects.
Reference: [15] <author> Andrew Erlichson, Basem A. Nayfeh, Jaswinder P. Singh, and Kunle Olukotun. </author> <title> The Benefits of Clustering in Shared Address Space Multiprocessors: An Applications-Driven Investigation. </title> <type> Technical Report CSL-TR-94-632, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Our study provides an implementation for NUMA machines. Other systems exploit clustering at a level closer to the processor, typically in the first or second level cache. These systems include VMP-MC [12], DASH [13], and KSR [14]. The benefit of clustering on these systems has been studied <ref> [15] </ref>. Interference misses due to limited cache capacity and associativity can reduce the benefits of clustering close to the processor. MGS clusters at the main memory level and thus does not suffer from these effects.
Reference: [16] <author> Andrew W. Wilson Jr. and Richard P. LaRowe Jr. </author> <title> Hiding Shared Memory Reference Latency on the Galactica Net Distributed Shared Memory Architecture. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 351-367, </pages> <year> 1992. </year>
Reference-contexts: Also, the use of MGS-style multiple grains is prohibitive in these systems because support for both intra- and inter-cluster accessesoccurs in hardware; using a single cache-line grain is desirable for simplicity. Several studies have explored multiple or variable grains of sharing. Galactica Net <ref> [16] </ref> allows both cache-line size and page-size grains of sharing; however, they rely on hardware support between clusters to implement update-based protocols at cache-line grain to efficiently support fine-grain write sharing between clusters.
Reference: [17] <author> Rohit Chandra, Kourosh Gharachorloo, Vijayaraghavan Soundarara-jan, and Anoop Gupta. </author> <title> Performance Evaluation of Hybrid Hardware and Software Distributed Shared Memory Protocols. </title> <booktitle> In Proceedingsof the Eighth ACM International Conference on Supercomputing, </booktitle> <pages> pages 274-288, </pages> <address> Manchester, England, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Several studies have explored multiple or variable grains of sharing. Galactica Net [16] allows both cache-line size and page-size grains of sharing; however, they rely on hardware support between clusters to implement update-based protocols at cache-line grain to efficiently support fine-grain write sharing between clusters. Chan-dra et al <ref> [17] </ref>, Shared Regions [4], and CRL [5] allow coherence to happen at variable grains, but treat all processors equally (i.e., no clustering), and require user annotations to identify the regions. MGS runs shared memory applications unmodified. The only requirement is that the programmer uses a release consistent memory model.
Reference: [18] <author> Brian N. Bershad and Matthew J. Zekauskas. Midway: </author> <title> Shared Memory Parallel Programming with Entry Consistency for Distributed Memory Multiprocessors. </title> <type> CMU-CS 91-170, </type> <institution> Carnegie Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: The only requirement is that the programmer uses a release consistent memory model. Finally, there is a large body of work on software distributed shared memory. Our particular DSM implementation borrows from the Munin system [8]. Other approaches have tried to minimize message traffic and optimize memory management policies <ref> [18, 19, 20, 21] </ref>. MGS would benefit from these techniques. 7 Conclusion Using small- to medium-scale multiprocessors as the basic building block for large-scale multiprocessors is attractive for two reasons. First, small- to medium-scale multiprocessors are economically viable and will have an ever increasing presence in the local area environment.
Reference: [19] <author> Alan L. Cox and Robert J. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <type> Technical Report 263, </type> <institution> University of Rochester Computer Science Department, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: The only requirement is that the programmer uses a release consistent memory model. Finally, there is a large body of work on software distributed shared memory. Our particular DSM implementation borrows from the Munin system [8]. Other approaches have tried to minimize message traffic and optimize memory management policies <ref> [18, 19, 20, 21] </ref>. MGS would benefit from these techniques. 7 Conclusion Using small- to medium-scale multiprocessors as the basic building block for large-scale multiprocessors is attractive for two reasons. First, small- to medium-scale multiprocessors are economically viable and will have an ever increasing presence in the local area environment.
Reference: [20] <author> Pete Keleher, Alan Cox, Sandhya Dwarkadas, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> Proceedings of the 1994 Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: The only requirement is that the programmer uses a release consistent memory model. Finally, there is a large body of work on software distributed shared memory. Our particular DSM implementation borrows from the Munin system [8]. Other approaches have tried to minimize message traffic and optimize memory management policies <ref> [18, 19, 20, 21] </ref>. MGS would benefit from these techniques. 7 Conclusion Using small- to medium-scale multiprocessors as the basic building block for large-scale multiprocessors is attractive for two reasons. First, small- to medium-scale multiprocessors are economically viable and will have an ever increasing presence in the local area environment.
Reference: [21] <author> Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <month> 12 </month>
Reference-contexts: The only requirement is that the programmer uses a release consistent memory model. Finally, there is a large body of work on software distributed shared memory. Our particular DSM implementation borrows from the Munin system [8]. Other approaches have tried to minimize message traffic and optimize memory management policies <ref> [18, 19, 20, 21] </ref>. MGS would benefit from these techniques. 7 Conclusion Using small- to medium-scale multiprocessors as the basic building block for large-scale multiprocessors is attractive for two reasons. First, small- to medium-scale multiprocessors are economically viable and will have an ever increasing presence in the local area environment.
References-found: 21

