URL: http://L2R.cs.uiuc.edu/~danr/Papers/visualJ.ps.gz
Refering-URL: http://L2R.cs.uiuc.edu/~danr/publications.html
Root-URL: http://www.cs.uiuc.edu
Title: On Learning Visual Concepts and DNF Formulae  
Author: EYAL KUSHILEVITZ DAN ROTH 
Keyword: Computational Learning, DNF, Visual Concepts  
Address: Cambridge, MA 02138.  
Affiliation: Aiken Computation Laboratory, Harvard University,  
Note: 1?? c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  Editor: Lenny Pitt  
Email: eyalk@das.harvard.edu  danr@das.harvard.edu  
Date: Received November 1, 1993  
Abstract: We consider the problem of learning DNF formulae in the mistake-bound and the PAC models. We develop a new approach, which is called polynomial explainability, that is shown to be useful for learning some new subclasses of DNF (and CNF) formulae that were not known to be learnable before. Unlike previous learnability results for DNF (and CNF) formulae, these subclasses are not limited in the number of terms or in the number of variables per term; yet, they contain the subclasses of k-DNF and k-term-DNF (and the corresponding classes of CNF) as special cases. We apply our DNF results to the problem of learning visual concepts and obtain learning algorithms for several natural subclasses of visual concepts that appear to have no natural boolean counterpart. On the other hand, we show that learning some other natural subclasses of visual concepts is as hard as learning the class of all DNF formulae. We also consider the robustness of these results under various types of noise. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. A. Aslam and S. E. Decatur. </author> <title> General bounds on statistical query learning and PAC learning with noise via hypothesis boosting. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 282-291, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: In the proof of Theorem 7 we present a statistical queries algorithm for C M with tolerance t = * 2 16p 2 (n)q (n) ln p (n) . It is shown in <ref> [1] </ref> that by using hypothesis boosting techniques, this tolerance can be made smaller. In particular, the " 2 factor can be reduced to " ln 1 . Putting those together yields the desirable error rate.
Reference: 2. <author> D. Angluin, M. Frazier, and L. Pitt. </author> <title> Learning conjunctions of Horn clauses. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 147-164, </pages> <year> 1992. </year>
Reference-contexts: Technion, Israel. email:eyalk@cs.technion.ac.il y Research supported by NSF grant CCR-92-00884 and by DARPA AFOSR-F4962-92-J-0466. 2 added ability of the learner to make various kinds of queries, a wider collection of subclasses are known to be learnable; these include, for example, monotone DNF [33], read-twice DNF [15], [5], [28], Horn formulae <ref> [2] </ref>, log n-term DNF [11], Read-k-Satisfy-j DNF [6], [9], and Decision Trees [13]. Most recently, Jackson [17] shows (using the Fourier algorithm of [22]) how to learn the class of DNF formulae with respect to the uniform distribution using membership queries.
Reference: 3. <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: We define obstruction noise, a type of noise that occurs in pictures, and give a learning algorithm that can tolerate this type of noise. Then we consider more general types of noise that have been considered previously in learning, namely, classification noise <ref> [3] </ref> and malicious noise [34], [20]. We show that any polynomially explainable class can be learned using statistical queries [19], and therefore can be learned in the presence of classification noise with error rate of up to 1=2, and in the presence of a certain amount of malicious error. 5.1. <p> Classification Noise In this section we discuss PAC learning with classification noise <ref> [3] </ref>. In this case, whenever we get an example, there is some probability &lt; 1=2 (usually referred to as the error rate) that the label of this example is flipped (from 0 to 1 or vice versa).
Reference: 4. <author> D. Angluin. </author> <title> Finding patterns common to a set of strings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 21(1) </volume> <pages> 46-62, </pages> <month> August </month> <year> 1980. </year>
Reference-contexts: It may be the case, however, that these functions are still in some "polynomially explainable" class, using another representation (other than DNF or CNF). We believe that the approach used in this paper will be found useful in tackling other problems as well. For example, Angluin <ref> [4] </ref> considered the problem of learning pattern languages 8 , where a pattern p is a string consists of bits (f0; 1g) and variables, and its language, L (p), is the set of all strings that can be obtained from p by substituting a string (in f0; 1g fl ) for
Reference: 5. <author> H. Aizenstein and L. Pitt. </author> <title> Exact learning of read-twice DNF formulas. </title> <booktitle> In Proceedings of the IEEE Symp. on Foundation of Computer Science, </booktitle> <volume> number 32, </volume> <pages> pages 170-179, </pages> <address> San Juan, </address> <year> 1991. </year>
Reference-contexts: Department of Computer Science, Technion, Israel. email:eyalk@cs.technion.ac.il y Research supported by NSF grant CCR-92-00884 and by DARPA AFOSR-F4962-92-J-0466. 2 added ability of the learner to make various kinds of queries, a wider collection of subclasses are known to be learnable; these include, for example, monotone DNF [33], read-twice DNF [15], <ref> [5] </ref>, [28], Horn formulae [2], log n-term DNF [11], Read-k-Satisfy-j DNF [6], [9], and Decision Trees [13]. Most recently, Jackson [17] shows (using the Fourier algorithm of [22]) how to learn the class of DNF formulae with respect to the uniform distribution using membership queries.
Reference: 6. <author> H. Aizenstein and L. Pitt. </author> <title> Exact learning of read-k disjoint DNF and not-so-disjoint DNF. </title> <booktitle> In Proceedings of COLT '92, </booktitle> <pages> pages 71-76, </pages> <address> Pittsburgh, Pennsylvania, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: NSF grant CCR-92-00884 and by DARPA AFOSR-F4962-92-J-0466. 2 added ability of the learner to make various kinds of queries, a wider collection of subclasses are known to be learnable; these include, for example, monotone DNF [33], read-twice DNF [15], [5], [28], Horn formulae [2], log n-term DNF [11], Read-k-Satisfy-j DNF <ref> [6] </ref>, [9], and Decision Trees [13]. Most recently, Jackson [17] shows (using the Fourier algorithm of [22]) how to learn the class of DNF formulae with respect to the uniform distribution using membership queries. <p> Negative examples are then used to eliminate "false" explanations 2 . (Similar ideas were developed in different contexts, e.g. in [10], [11], <ref> [6] </ref>.) The same approach is used for learning subclasses of CNF formulae. <p> It is interesting to note that in particular, all classes learnable within this framework (e.g. in the context of membership queries, [10], [11], <ref> [6] </ref>, [9].) are shown here to be learnable even in the presence of noise. Acknowledgments We wish to thank Les Valiant for helpful discussions and comments. We thank the referees for helpful suggestions that improved the presentation of this work. Notes 1.
Reference: 7. <author> R. Basri. </author> <title> Private Communication, </title> <year> 1994. </year>
Reference-contexts: It is a natural question whether the ideas presented here (along with some "engineering") can be used to solve some "real-world" problems. We believe that they do; indeed, some experiments that use these idea to solve problems concerning the recognition of human motion [12] and character recognition <ref> [7] </ref> seem to be encouraging. A major problem from the engineering side of the problem is how to generate the "explanations" in an efficient and compact way. A more intrinsic difficulty is that our model is very "clean" while "real-world" data (both the examples and their classifications) is often noisy.
Reference: 8. <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Given a sample of m labeled examples, we show how to construct in polynomial time a consistent hypothesis of size that is independent of m. Blumer et al. <ref> [8] </ref> show that this is sufficient for PAC-learnability. Given a sample of size m, the learner starts by enumerating all the shapes. <p> To represent this hypothesis we need O (n 2 ) bits (n 2 bits to specify the shape, n 2 bits to specify values for the bits of the shape, and 2 log n to specify the threshold) which is independent of the sample size, m. As shown in <ref> [8] </ref>, using a sample size that is proportional to log jC P 0 ;k 0 (n) j is sufficient for PAC-learnability. Note that we do not use any "structure" of the patterns in P, except the fact that the corresponding shapes are polynomially enumerable.
Reference: 9. <author> A. Blum, R. Khardon, A. Kushilevitz, L. Pitt, and D. Roth. </author> <booktitle> On learning read-k satisfy-j DNF. In Proceedings of the Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 110-117, </pages> <year> 1994. </year>
Reference-contexts: grant CCR-92-00884 and by DARPA AFOSR-F4962-92-J-0466. 2 added ability of the learner to make various kinds of queries, a wider collection of subclasses are known to be learnable; these include, for example, monotone DNF [33], read-twice DNF [15], [5], [28], Horn formulae [2], log n-term DNF [11], Read-k-Satisfy-j DNF [6], <ref> [9] </ref>, and Decision Trees [13]. Most recently, Jackson [17] shows (using the Fourier algorithm of [22]) how to learn the class of DNF formulae with respect to the uniform distribution using membership queries. <p> It is interesting to note that in particular, all classes learnable within this framework (e.g. in the context of membership queries, [10], [11], [6], <ref> [9] </ref>.) are shown here to be learnable even in the presence of noise. Acknowledgments We wish to thank Les Valiant for helpful discussions and comments. We thank the referees for helpful suggestions that improved the presentation of this work. Notes 1.
Reference: 10. <author> A. Blum. </author> <title> Learning boolean functions in an infinite attribute space. </title> <journal> Machine Learning, </journal> <volume> 9(4) </volume> <pages> 373-386, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Negative examples are then used to eliminate "false" explanations 2 . (Similar ideas were developed in different contexts, e.g. in <ref> [10] </ref>, [11], [6].) The same approach is used for learning subclasses of CNF formulae. <p> Proof: We present an expected mistake-bound algorithm that learns the class C M (with expected number of mistakes which is O (p (n) q (n) g (n))). The algorithm is similar to an algorithm presented in <ref> [10] </ref>. The algorithm maintains an hypothesis h which is a disjunction of monomials. Initially h contains no monomials (i.e., h F ALSE). Upon receiving an example e, the algorithm predicts h (e); if the prediction is correct, h is not updated. <p> It is interesting to note that in particular, all classes learnable within this framework (e.g. in the context of membership queries, <ref> [10] </ref>, [11], [6], [9].) are shown here to be learnable even in the presence of noise. Acknowledgments We wish to thank Les Valiant for helpful discussions and comments. We thank the referees for helpful suggestions that improved the presentation of this work. Notes 1.
Reference: 11. <author> A. Blum and S. Rudich. </author> <title> Fast learning of k-term DNF formulas with queries. </title> <booktitle> In Proceedings of Twenty-Fourth ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-389. </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference-contexts: Research supported by NSF grant CCR-92-00884 and by DARPA AFOSR-F4962-92-J-0466. 2 added ability of the learner to make various kinds of queries, a wider collection of subclasses are known to be learnable; these include, for example, monotone DNF [33], read-twice DNF [15], [5], [28], Horn formulae [2], log n-term DNF <ref> [11] </ref>, Read-k-Satisfy-j DNF [6], [9], and Decision Trees [13]. Most recently, Jackson [17] shows (using the Fourier algorithm of [22]) how to learn the class of DNF formulae with respect to the uniform distribution using membership queries. <p> Negative examples are then used to eliminate "false" explanations 2 . (Similar ideas were developed in different contexts, e.g. in [10], <ref> [11] </ref>, [6].) The same approach is used for learning subclasses of CNF formulae. <p> It is interesting to note that in particular, all classes learnable within this framework (e.g. in the context of membership queries, [10], <ref> [11] </ref>, [6], [9].) are shown here to be learnable even in the presence of noise. Acknowledgments We wish to thank Les Valiant for helpful discussions and comments. We thank the referees for helpful suggestions that improved the presentation of this work. Notes 1.
Reference: 12. <author> M. Bender and D. Roth. </author> <title> Learning human motion as DNF formulae. </title> <type> (Unpublished), </type> <year> 1994. </year>
Reference-contexts: It is a natural question whether the ideas presented here (along with some "engineering") can be used to solve some "real-world" problems. We believe that they do; indeed, some experiments that use these idea to solve problems concerning the recognition of human motion <ref> [12] </ref> and character recognition [7] seem to be encouraging. A major problem from the engineering side of the problem is how to generate the "explanations" in an efficient and compact way.
Reference: 13. <author> N. H. Bshouty. </author> <title> Exact learning via the monotone theory. </title> <booktitle> In Proceedings of the IEEE Symp. on Foundation of Computer Science, </booktitle> <pages> pages 302-311, </pages> <address> Palo Alto, CA., </address> <year> 1993. </year>
Reference-contexts: DARPA AFOSR-F4962-92-J-0466. 2 added ability of the learner to make various kinds of queries, a wider collection of subclasses are known to be learnable; these include, for example, monotone DNF [33], read-twice DNF [15], [5], [28], Horn formulae [2], log n-term DNF [11], Read-k-Satisfy-j DNF [6], [9], and Decision Trees <ref> [13] </ref>. Most recently, Jackson [17] shows (using the Fourier algorithm of [22]) how to learn the class of DNF formulae with respect to the uniform distribution using membership queries.
Reference: 14. <author> S. E. Decatur. </author> <title> Statistical queries and faulty PAC oracles. </title> <booktitle> In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 262-268. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: We show: Theorem 8 If C M is polynomially explainable then C M is PAC learnable in the presence of malicious error of rate less than fi = * ln 1 ffi Proof: By a result of Decatur <ref> [14] </ref>, the existence of a "statistical-queries" learning algorithm with tolerance t for C implies that there exists a PAC learning algorithm for C that can tolerate malicious error rate (t ).
Reference: 15. <author> T. Hancock. </author> <title> Learning 2 DNF formulas and k decision trees. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 199-209, </pages> <address> Santa Cruz, California, August 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: address: Department of Computer Science, Technion, Israel. email:eyalk@cs.technion.ac.il y Research supported by NSF grant CCR-92-00884 and by DARPA AFOSR-F4962-92-J-0466. 2 added ability of the learner to make various kinds of queries, a wider collection of subclasses are known to be learnable; these include, for example, monotone DNF [33], read-twice DNF <ref> [15] </ref>, [5], [28], Horn formulae [2], log n-term DNF [11], Read-k-Satisfy-j DNF [6], [9], and Decision Trees [13]. Most recently, Jackson [17] shows (using the Fourier algorithm of [22]) how to learn the class of DNF formulae with respect to the uniform distribution using membership queries.
Reference: 16. <author> D. Haussler, M. Kearns, N. Littlestone, and M. K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Information and Computation, </journal> <volume> 95(2) </volume> <pages> 129-161, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: It can be shown that if a concept class C is learnable in the expected mistake-bound model (and thus in the mistake bound model) then it is PAC-learnable <ref> [16] </ref>. 3. The DNF Problem In this section we present a mistake-bound algorithm for subclasses of DNF formulae satisfying certain properties. This, in particular, implies the PAC-learnability of these subclasses [25].
Reference: 17. <author> J. Jackson. </author> <title> An efficient membership-query algorithm for learning DNF with respect to the uniform distribution. </title> <booktitle> In Proceedings of the IEEE Symp. on Foundation of Computer Science, </booktitle> <year> 1994. </year> <note> To Appear. </note>
Reference-contexts: Most recently, Jackson <ref> [17] </ref> shows (using the Fourier algorithm of [22]) how to learn the class of DNF formulae with respect to the uniform distribution using membership queries.
Reference: 18. <author> M. Jerrum. </author> <title> Simple translation-invariant concepts are hard to learn. </title> <type> Technical Report CSR-12-91, </type> <institution> University of Edinburgh, Department of Computer Science, </institution> <year> 1991. </year> <month> 23 </month>
Reference-contexts: Then, we enumerate only the patterns in which all the substrings that correspond to the same variable are the same (i.e., the substitution is consistent). As long as c is fixed, this gives a polynomial algorithm. Jerrum <ref> [18] </ref> showed that learning translation-invariant DNF (when the learner is required to use the same representation as the output representation) is NP-hard.
Reference: 19. <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 392-401, </pages> <year> 1993. </year>
Reference-contexts: Then we consider more general types of noise that have been considered previously in learning, namely, classification noise [3] and malicious noise [34], [20]. We show that any polynomially explainable class can be learned using statistical queries <ref> [19] </ref>, and therefore can be learned in the presence of classification noise with error rate of up to 1=2, and in the presence of a certain amount of malicious error. 5.1. <p> This contradicts Eq. (1) above, and proves the lemma. For the main theorem of this section we use a the "statistical queries" (SQ) model recently introduced by Kearns <ref> [19] </ref>. The SQ learning model can be viewed as a tool for demonstrating that a PAC learning algorithm is noise-tolerant. We first introduce the SQ learning model and state Kearns result, and in the next theorem give a statistical queries algorithm for the class C M .
Reference: 20. <author> M. Kearns and M. Li. </author> <title> Learning in the precence of malicious error. </title> <journal> Siam Journal of Computing, </journal> <volume> 22(4), </volume> <year> 1993. </year>
Reference-contexts: We define obstruction noise, a type of noise that occurs in pictures, and give a learning algorithm that can tolerate this type of noise. Then we consider more general types of noise that have been considered previously in learning, namely, classification noise [3] and malicious noise [34], <ref> [20] </ref>. We show that any polynomially explainable class can be learned using statistical queries [19], and therefore can be learned in the presence of classification noise with error rate of up to 1=2, and in the presence of a certain amount of malicious error. 5.1. <p> This completes the proof of the theorem. We note that in the proof we have used "statistical-queries" with tolerance t = * 2 ffi 5.3. Malicious Noise In the model of PAC learning with malicious error ([34], <ref> [20] </ref>), when a learner sees an example, only with probability 1fi it is drawn from the probability distribution D, and is labeled correctly according to the target concept.
Reference: 21. <author> M. Kearns, M. Li, L. Pitt, and L. G. Valiant. </author> <title> On the learnability of boolean formulae. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 285-295, </pages> <address> New York, New York, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: Most recently, Jackson [17] shows (using the Fourier algorithm of [22]) how to learn the class of DNF formulae with respect to the uniform distribution using membership queries. However, for the DNF problem in the PAC model, simple reductions <ref> [21] </ref> show that the restrictions suggested by researchers to tackle the problem when queries are available, e.g., limiting the number of occurrences of a variable, considering monotone formulae etc., are not useful in the query-less models.
Reference: 22. <author> E. Kushilevitz and Y. Mansour. </author> <title> Learning decision trees using the fourier spectrum. </title> <journal> Siam Journal of Computing, </journal> <volume> 22(6) </volume> <pages> 1331-1348, </pages> <year> 1993. </year> <note> Earlier version appeared in Proc. </note> <editor> 23rd Ann. </editor> <booktitle> IEEE Symp. on Foundations of Computer Science, </booktitle> <year> 1991. </year>
Reference-contexts: Most recently, Jackson [17] shows (using the Fourier algorithm of <ref> [22] </ref>) how to learn the class of DNF formulae with respect to the uniform distribution using membership queries.
Reference: 23. <author> M. Kearns and L. Pitt. </author> <title> A polynomial-time algorithm for learning k-variable pattern languages from examples. </title> <booktitle> In Proceedings of the Second Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 57-71, </pages> <address> Santa Cruz, California, </address> <month> July </month> <year> 1989. </year>
Reference-contexts: of all strings that can be obtained from p by substituting a string (in f0; 1g fl ) for each of the variables (for example, L (x00xy) contains the string 1100110 which is obtained by substituting x = 11; y = 0). [30] showed the hardness of learning pattern languages. <ref> [23] </ref> showed how to PAC-learn such languages, assuming that the number of variables is constant (though each variable may appear many times in the pattern) and with some limitations on the underlying distribution. <p> (i.e., L is the language of all strings that can be obtained from any of the patterns), where t is polynomial in n, and the total number of occurrences of variables in each pattern p i is constant (note that the patterns here are more restricted than those considered in <ref> [23] </ref>; however, we are not restricted to a single pattern but rather allow a collection of patterns).
Reference: 24. <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: Preliminaries We begin by formally defining the learning models discussed in this work the standard PAC model [33] and the mistake-bound model <ref> [24] </ref>. The instance space X is f0; 1g n , the set of all possible assignments to n boolean variables. A concept f is a boolean function on X. Positive (respectively, negative) examples of f are examples (instances) on which f is 1 (respectively, 0).
Reference: 25. <author> N. Littlestone. </author> <title> Mistake bounds and logarithmic linear-threshold learning algorithms. </title> <type> PhD thesis, </type> <address> U. C. Santa Cruz, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: The DNF Problem In this section we present a mistake-bound algorithm for subclasses of DNF formulae satisfying certain properties. This, in particular, implies the PAC-learnability of these subclasses <ref> [25] </ref>. The main idea is the following: Consider, for example, Valiant`s algorithm for learning k-DNF [33] (many other algorithms share the same structure). Before seeing any example, the algorithm enumerates the set of all (polynomially many) monomials of size at most k.
Reference: 26. <author> M. Li and P. M. B. Vitanyi. </author> <title> A theory of learning simple concepts under simple distributions and average case complexity for the universal distribution. </title> <booktitle> In Proceedings of the Thirtieth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 34-39, </pages> <institution> Research Triangle Park, North Carolina, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: Example: Consider the class of all DNF formulae in which each term contains at least n k literals (for some constant k). This class was previously considered in <ref> [26] </ref>. In spite of its similarity to the class of k-DNF formulae (where each term contains at most k literals) its learnability seems to be more difficult.
Reference: 27. <author> T.M. Mitchell, R.M. Keller, and S.T. Kedar-Cabelli. </author> <title> Explanation Based Learning. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: Notes 1. We assume that each pixel can be either black or white. The results can be extended to handle more values in a straightforward way. 2. This should not be confused with the approach of explanation based learning (EBL) (see, e.g., <ref> [27] </ref>).
Reference: 28. <author> K. Pillapakkamnatt and V. Raghavan. </author> <title> Read twice DNF formulas are properly learnable. </title> <type> Technical Report TR-CS-93-59, </type> <institution> Vanderbilt University, Computer Science Department, </institution> <year> 1993. </year> <title> To appear, </title> <booktitle> Proceedings of the 1st European Conference on Computational Learning Theory (EuroColt 93). </booktitle>
Reference-contexts: of Computer Science, Technion, Israel. email:eyalk@cs.technion.ac.il y Research supported by NSF grant CCR-92-00884 and by DARPA AFOSR-F4962-92-J-0466. 2 added ability of the learner to make various kinds of queries, a wider collection of subclasses are known to be learnable; these include, for example, monotone DNF [33], read-twice DNF [15], [5], <ref> [28] </ref>, Horn formulae [2], log n-term DNF [11], Read-k-Satisfy-j DNF [6], [9], and Decision Trees [13]. Most recently, Jackson [17] shows (using the Fourier algorithm of [22]) how to learn the class of DNF formulae with respect to the uniform distribution using membership queries.
Reference: 29. <author> R. L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(3) </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: The widest subclass of boolean formulae known to be PAC-learnable is that of k-decision lists (k-DL) <ref> [29] </ref> which contains the above two subclasses as special cases. <p> As mentioned, polynomial-explainable classes discussed here contain the subclasses of k-DNF and k-term-DNF as special cases. It is natural to ask how these results relate to the learnability of k-DL <ref> [29] </ref> (for some constant k), which was the widest subclass of boolean formulae 20 known to be PAC-learnable from examples. We show that the results are incomparable.
Reference: 30. <author> R. E. Schapire. </author> <title> Pattern languages are not learnable. </title> <booktitle> In Proceedings of COLT '90, </booktitle> <pages> pages 122-129. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: and its language, L (p), is the set of all strings that can be obtained from p by substituting a string (in f0; 1g fl ) for each of the variables (for example, L (x00xy) contains the string 1100110 which is obtained by substituting x = 11; y = 0). <ref> [30] </ref> showed the hardness of learning pattern languages. [23] showed how to PAC-learn such languages, assuming that the number of variables is constant (though each variable may appear many times in the pattern) and with some limitations on the underlying distribution.
Reference: 31. <author> H. Shvaytser. </author> <title> Learnable and nonlearnable visual concepts. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(5) </volume> <pages> 459-466, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Thus the PAC-learnability of DNF formulae (beyond the above mentioned subclasses) is still a mystery. One reason for the attraction of the class of DNF, aside from its being a theoretical puzzle, is that people appear to like it for representing knowledge [34]. Indeed, in <ref> [31] </ref> the observation is made, that learnability results on learning DNF and CNF formulae are useful for the task of learning to recognize visual concepts in digital pictures. By mapping boolean variables to pixels in an n fi n digital picture, the equivalence to learning DNF is shown 1 .
Reference: 32. <author> G. Shackelford and D. Volper. </author> <title> Learning k-DNF with noise in the attributes. </title> <booktitle> In First Workshop on Computatinal Learning Theory, </booktitle> <pages> pages 97-103, </pages> <address> Cambridge, Mass. August 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Putting those together yields the desirable error rate. We have considered above classification noise, malicious noise and a type of attribute noise that is relevant to visual concepts. The known technique for handling (unrestricted) attribute noise in learning DNF formulae <ref> [32] </ref> works in cases where the noise-free algorithm uses at any point only a small number of attributes to update its hypothesis. In this way, with non-negligible probability, the noise-tolerant algorithm will get examples in which this small set of attributes is noise-free, and will learn using the noise-free algorithm. <p> In this way, with non-negligible probability, the noise-tolerant algorithm will get examples in which this small set of attributes is noise-free, and will learn using the noise-free algorithm. This technique was used to learn k-DNF in the presence of attribute noise (with a fixed error rate) <ref> [32] </ref>. It is not hard to see that the same technique, coupled with the SQ algorithm presented in Section 5.2, can be used to learn a wider class of functions. <p> Hence, the collection succeeds with high probability in spite of the attribute noise. Then, the elimination step uses the algorithm of <ref> [32] </ref>. 6. Discussion We present a new approach, polynomial explainability, to the problem of learning DNF formulae from examples and use it to learn some subclasses of DNF (and CNF) which were not known to be learnable before.
Reference: 33. <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: 1. Introduction A central question in computational learning theory is deciding which subclasses of boolean formulae are learnable under the standard learning models. One of the main open problems, which has remained open since proposed by Valiant in 1984 <ref> [33] </ref>, [34] is the question of the PAC-learnability of disjunction-normal-form (DNF) formulae. Despite the efforts devoted to resolving this problem, success was obtained only for relatively simple subclasses, such as k-DNF (DNF formulae in which each term consists of at most k literals) and k-term-DNF (DNF formulae with k terms). <p> Author's current address: Department of Computer Science, Technion, Israel. email:eyalk@cs.technion.ac.il y Research supported by NSF grant CCR-92-00884 and by DARPA AFOSR-F4962-92-J-0466. 2 added ability of the learner to make various kinds of queries, a wider collection of subclasses are known to be learnable; these include, for example, monotone DNF <ref> [33] </ref>, read-twice DNF [15], [5], [28], Horn formulae [2], log n-term DNF [11], Read-k-Satisfy-j DNF [6], [9], and Decision Trees [13]. Most recently, Jackson [17] shows (using the Fourier algorithm of [22]) how to learn the class of DNF formulae with respect to the uniform distribution using membership queries. <p> In section 5 we discuss learning in the presence of noise, and in Section 6 we discuss the results and briefly describe some other applications of our technique. 2. Preliminaries We begin by formally defining the learning models discussed in this work the standard PAC model <ref> [33] </ref> and the mistake-bound model [24]. The instance space X is f0; 1g n , the set of all possible assignments to n boolean variables. A concept f is a boolean function on X. Positive (respectively, negative) examples of f are examples (instances) on which f is 1 (respectively, 0). <p> The DNF Problem In this section we present a mistake-bound algorithm for subclasses of DNF formulae satisfying certain properties. This, in particular, implies the PAC-learnability of these subclasses [25]. The main idea is the following: Consider, for example, Valiant`s algorithm for learning k-DNF <ref> [33] </ref> (many other algorithms share the same structure). Before seeing any example, the algorithm enumerates the set of all (polynomially many) monomials of size at most k. Then, it uses the examples to "eliminate" those monomials which are not consistent with the examples.
Reference: 34. <author> L. G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings of the International Joint Conference of Artificial Intelligence, </booktitle> <pages> pages 560-566. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1985. </year>
Reference-contexts: 1. Introduction A central question in computational learning theory is deciding which subclasses of boolean formulae are learnable under the standard learning models. One of the main open problems, which has remained open since proposed by Valiant in 1984 [33], <ref> [34] </ref> is the question of the PAC-learnability of disjunction-normal-form (DNF) formulae. Despite the efforts devoted to resolving this problem, success was obtained only for relatively simple subclasses, such as k-DNF (DNF formulae in which each term consists of at most k literals) and k-term-DNF (DNF formulae with k terms). <p> Thus the PAC-learnability of DNF formulae (beyond the above mentioned subclasses) is still a mystery. One reason for the attraction of the class of DNF, aside from its being a theoretical puzzle, is that people appear to like it for representing knowledge <ref> [34] </ref>. Indeed, in [31] the observation is made, that learnability results on learning DNF and CNF formulae are useful for the task of learning to recognize visual concepts in digital pictures. <p> We define obstruction noise, a type of noise that occurs in pictures, and give a learning algorithm that can tolerate this type of noise. Then we consider more general types of noise that have been considered previously in learning, namely, classification noise [3] and malicious noise <ref> [34] </ref>, [20]. We show that any polynomially explainable class can be learned using statistical queries [19], and therefore can be learned in the presence of classification noise with error rate of up to 1=2, and in the presence of a certain amount of malicious error. 5.1. <p> Note that the notion of a translation-invariant term and our notion of dynamic pattern are closely related; that is, the dynamic version of our visual learning problem is a subclass of translation-invariant DNF. In contrast to the general translation-invariant DNF, this subclass is still PAC-learnable. In <ref> [34] </ref> a hierarchical approach for learning DNF was discussed in which one learns a collection of monomials, in a supervised or unsupervised manner, and only then learns a DNF formula as a disjunction over this set of monomials.
References-found: 34

