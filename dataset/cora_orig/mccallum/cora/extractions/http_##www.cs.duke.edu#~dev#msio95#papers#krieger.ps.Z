URL: http://www.cs.duke.edu/~dev/msio95/papers/krieger.ps.Z
Refering-URL: http://www.cs.duke.edu/~dev/msio95/abstracts/krieger.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: -okrieg,reid,stumm-@eecg.toronto.edu  
Title: Exploiting Mapped Files for Parallel I/O  
Author: Orran Krieger, Karen Reid and Michael Stumm 
Web: http://www.eecg.toronto.edu/parallel/  
Address: Toronto  
Affiliation: Department of Electrical and Computer Engineering Department of Computer Science University of  
Abstract: Harnessing the full I/O capabilities of a large-scale multiprocessor is difficult and requires a great deal of cooperation between the application programmer, the compiler and the operating (/file) system. Hence, the parallel I/O interface used by the application to communicate with the system is crucial in achieving good performance. We present a set of properties we believe that a good I/O interface should have and consider current parallel I/O interfaces from the perspective of these properties. We describe the advantages and disadvantages of mapped-file I/O and argue that if properly implemented it can be a good basis for a parallel I/O interface that can fulfill the suggested properties. To demonstrate that such an implementation is feasible, we describe methodology used in our previous work on the Hurricane operating system and in our current work on the Tornado operating system to implement mapped files. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rajesh Bordawekar, Alok Choudhary, Ken Kennedy, Charles Koelbel, and Michael Paleczny. </author> <title> A model and compilation strategy for out-of-core data parallel programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 1-10, </pages> <month> July </month> <year> 1995. </year> <note> Also available as the following technical reports: NPAC Technical Report SCCS-0696, CRPC Technical Report CRPC-TR94507-S, SIO Technical Report CACR SIO-104. </note>
Reference-contexts: Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems. Research has also been carried out specifically on developing application interfaces [14, 3, 13] and compiler interfaces <ref> [26, 1] </ref>. All of these approaches to parallel I/O systems consider interface issues to a varying degree. 2 Most existing parallel I/O systems are based on a read/write interface.
Reference: [2] <author> D. Cheriton. UIO: </author> <title> A Uniform I/O system interface for distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(1) </volume> <pages> 12-46, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: In contrast, read/write interfaces like Unix I/O allow applications to use the same operations whether the I/O is directed to a file, terminal or network connection. Such a uniform I/O interface allows a program to be independent of the type of data sources and sinks with which it communicates <ref> [2] </ref>. Another problem with the mapped-file I/O interface is that it is very different from more popular I/O interfaces like Unix I/O, and applications written to use these interfaces have to be rewritten to exploit the advantages of mapped-file I/O.
Reference: [3] <author> Peter Corbett, Dror Feitelson, Sam Fineberg, Yarsun Hsu, Bill Nitzberg, Jean-Pierre Prost, Marc Snir, Bernard Traversat, and Parkson Wong. </author> <title> Overview of the MPI-IO parallel I/O interface. </title> <booktitle> In IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 1-15, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Most current interfaces embed policy decisions in the operations used to access file data, forcing the applications to be rewrit ten when these policy decisions are changed. dynamic policy choice: Applications can have multiple phases, each with a different file access pattern <ref> [14, 4, 27, 3] </ref>. <p> The same mechanisms for specifying policy should apply in both cases. portability: The interface should be applicable to the full range of parallel systems, from distributed systems to multicomputers to shared memory multiprocessors <ref> [25, 22, 3, 12, 11] </ref>. <p> Some have developed complete parallel files systems [10, 19, 4, 8]. Others have developed servers or run-time libraries for optimizing I/O performance that run on multiple systems <ref> [25, 22, 27, 11, 3, 12] </ref>. Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems. Research has also been carried out specifically on developing application interfaces [14, 3, 13] and compiler interfaces [26, 1]. <p> Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems. Research has also been carried out specifically on developing application interfaces <ref> [14, 3, 13] </ref> and compiler interfaces [26, 1]. All of these approaches to parallel I/O systems consider interface issues to a varying degree. 2 Most existing parallel I/O systems are based on a read/write interface. <p> These interfaces decouple the specification of policy from the accesses to file data, allowing an application to dictate how its data should be distributed across the system disks while hiding the distribution of the data from subsequent file accesses. A few parallel I/O systems have made portability a priority <ref> [25, 22, 11, 3, 12] </ref>. These systems have been built for distributed memory systems on top of native file systems and portable communication interfaces such as PVM or MPI. <p> All systems support concurrent file access, some relying on file types to define which parts of the file will be accessed independently <ref> [3, 14, 4] </ref>, some by changing the semantics of file pointers [14]. PIOUS uses a transaction-based system to solve the synchronization problem and provide some fault tolerance [22]. In general, systems that implement new file types tend not to worry about compatibility with a traditional Unix interface.
Reference: [4] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, and Sandra Johnson Baylor. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <year> 1993. </year> <month> 9 </month>
Reference-contexts: Finally, Section 6 describes techniques used to specify file system policies. 2 Interface properties A good parallel I/O interface will have the following set of properties: flexibility: The interface should be simple for novice programmers while still satisfying the performance requirements of expert programmers <ref> [14, 4, 13, 12] </ref>. The application should be able to choose how much, if any, policy related information it specifies to the system. <p> Most current interfaces embed policy decisions in the operations used to access file data, forcing the applications to be rewrit ten when these policy decisions are changed. dynamic policy choice: Applications can have multiple phases, each with a different file access pattern <ref> [14, 4, 27, 3] </ref>. <p> On the other hand, it should not be necessary to synchronize on a common file offset when the application threads are randomly accessing the file. compatibility: The interface must be compatible with traditional I/O interfaces, such as Unix I/O <ref> [9, 4] </ref>. Existing tools (e.g., editors, Unix filters, data visualization tools) should be able to access parallel files created using the parallel interface. <p> Some have developed complete parallel files systems <ref> [10, 19, 4, 8] </ref>. Others have developed servers or run-time libraries for optimizing I/O performance that run on multiple systems [25, 22, 27, 11, 3, 12]. Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems. <p> There has been some work in defining interfaces that can specify to the system the policies it should use, especially in allowing applications to control how data is distributed across the system disks <ref> [4, 6] </ref>. These interfaces decouple the specification of policy from the accesses to file data, allowing an application to dictate how its data should be distributed across the system disks while hiding the distribution of the data from subsequent file accesses. <p> All systems support concurrent file access, some relying on file types to define which parts of the file will be accessed independently <ref> [3, 14, 4] </ref>, some by changing the semantics of file pointers [14]. PIOUS uses a transaction-based system to solve the synchronization problem and provide some fault tolerance [22]. In general, systems that implement new file types tend not to worry about compatibility with a traditional Unix interface. <p> A common characteristic of recently developed interfaces is that the application can specify per-processor views of file data, where non-contiguous portions of the file appear logically contiguous to the requesting processor <ref> [4, 27, 25] </ref>. These interfaces give the application a great deal of flexibility in dictating how its matrix should be distributed across the system disks. Another advantage of providing multiple logical views of a file is that applications can easily change their logical access patterns.
Reference: [5] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution>
Reference-contexts: The performance of such array based systems are impressive, and certainly interfaces tuned for arrays must be supported by any parallel I/O system that seeks to address the requirements of scientific applications. However, not all I/O intensive parallel applications are array based <ref> [5, 29] </ref>, and the specialized nature of these interfaces makes them inappropriate for any other types of file access. Also, these interfaces typically still have the disadvantage that the application specifies the target buffer for an I/O request. <p> For other examples, we refer to a paper by Cormen and Kotz, where they describe a number of I/O intensive algorithms that are not matrix based <ref> [5] </ref>. In this section we briefly describe building-block composition, a low-level technique for specifying policy that we employ in the Tornado operating system [23]. While allowing matrix-based interfaces to be implemented in a layer above it, building-block composition allows the expert user much greater control over operating system policy.
Reference: [6] <author> Erik P. DeBenedictis and Juan Miguel del Rosario. </author> <title> Modular scalable I/O. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):122-128, January and February 1993. </note>
Reference-contexts: There has been some work in defining interfaces that can specify to the system the policies it should use, especially in allowing applications to control how data is distributed across the system disks <ref> [4, 6] </ref>. These interfaces decouple the specification of policy from the accesses to file data, allowing an application to dictate how its data should be distributed across the system disks while hiding the distribution of the data from subsequent file accesses.
Reference: [7] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <year> 1993. </year> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. </pages>
Reference-contexts: Some have developed complete parallel files systems [10, 19, 4, 8]. Others have developed servers or run-time libraries for optimizing I/O performance that run on multiple systems [25, 22, 27, 11, 3, 12]. Some research has concentrated on developing specific techniques to improve I/O performance <ref> [15, 7] </ref> that could be incorporated into larger systems. Research has also been carried out specifically on developing application interfaces [14, 3, 13] and compiler interfaces [26, 1]. <p> To avoid the limitations and performance problems of a simple independent read/write interface some researchers have turned to much higher-level interfaces where the programmer specifies I/O requests in terms of entire arrays or large portions of arrays, for example, and the underlying system can optimize each type of high-level request <ref> [15, 7, 11, 25] </ref>. The performance of such array based systems are impressive, and certainly interfaces tuned for arrays must be supported by any parallel I/O system that seeks to address the requirements of scientific applications. <p> This enables the system to handle all requests for a single file block at the same time, avoiding multiple reads of the same block from disk. It also makes it possible to use techniques such as disk-directed I/O <ref> [15, 7] </ref> that allow the layout of the data on disk to be taken into account to minimize disk seeks. The interfaces for supporting processor specific views and collective I/O are all built on read/write interfaces for accessing the file data.
Reference: [8] <author> Peter Dibble, Michael Scott, and Carla Ellis. </author> <title> Bridge: A high-performance file system for parallel processors. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computer Systems, </booktitle> <pages> pages 154-161, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Some have developed complete parallel files systems <ref> [10, 19, 4, 8] </ref>. Others have developed servers or run-time libraries for optimizing I/O performance that run on multiple systems [25, 22, 27, 11, 3, 12]. Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems.
Reference: [9] <author> Dror G. Feitelson, Peter F. Corbett, Sandra Johnson Baylor, and Yarsun Hsu. </author> <title> Parallel I/O subsystems in massively parallel supercomputers. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <pages> pages 33-47, </pages> <month> Fall </month> <year> 1995. </year>
Reference-contexts: On the other hand, it should not be necessary to synchronize on a common file offset when the application threads are randomly accessing the file. compatibility: The interface must be compatible with traditional I/O interfaces, such as Unix I/O <ref> [9, 4] </ref>. Existing tools (e.g., editors, Unix filters, data visualization tools) should be able to access parallel files created using the parallel interface.
Reference: [10] <author> James C. French, Terrence W. Pratt, and Mriganka Das. </author> <title> Performance measurement of a parallel input/output system for the Intel iPSC/2 hypercube. </title> <booktitle> In Proceedings of the 1991 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 178-187, </pages> <year> 1991. </year>
Reference-contexts: Some have developed complete parallel files systems <ref> [10, 19, 4, 8] </ref>. Others have developed servers or run-time libraries for optimizing I/O performance that run on multiple systems [25, 22, 27, 11, 3, 12]. Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems.
Reference: [11] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: The same mechanisms for specifying policy should apply in both cases. portability: The interface should be applicable to the full range of parallel systems, from distributed systems to multicomputers to shared memory multiprocessors <ref> [25, 22, 3, 12, 11] </ref>. <p> Some have developed complete parallel files systems [10, 19, 4, 8]. Others have developed servers or run-time libraries for optimizing I/O performance that run on multiple systems <ref> [25, 22, 27, 11, 3, 12] </ref>. Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems. Research has also been carried out specifically on developing application interfaces [14, 3, 13] and compiler interfaces [26, 1]. <p> To avoid the limitations and performance problems of a simple independent read/write interface some researchers have turned to much higher-level interfaces where the programmer specifies I/O requests in terms of entire arrays or large portions of arrays, for example, and the underlying system can optimize each type of high-level request <ref> [15, 7, 11, 25] </ref>. The performance of such array based systems are impressive, and certainly interfaces tuned for arrays must be supported by any parallel I/O system that seeks to address the requirements of scientific applications. <p> These interfaces decouple the specification of policy from the accesses to file data, allowing an application to dictate how its data should be distributed across the system disks while hiding the distribution of the data from subsequent file accesses. A few parallel I/O systems have made portability a priority <ref> [25, 22, 11, 3, 12] </ref>. These systems have been built for distributed memory systems on top of native file systems and portable communication interfaces such as PVM or MPI.
Reference: [12] <author> Jay Huber, Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Finally, Section 6 describes techniques used to specify file system policies. 2 Interface properties A good parallel I/O interface will have the following set of properties: flexibility: The interface should be simple for novice programmers while still satisfying the performance requirements of expert programmers <ref> [14, 4, 13, 12] </ref>. The application should be able to choose how much, if any, policy related information it specifies to the system. <p> The same mechanisms for specifying policy should apply in both cases. portability: The interface should be applicable to the full range of parallel systems, from distributed systems to multicomputers to shared memory multiprocessors <ref> [25, 22, 3, 12, 11] </ref>. <p> Some have developed complete parallel files systems [10, 19, 4, 8]. Others have developed servers or run-time libraries for optimizing I/O performance that run on multiple systems <ref> [25, 22, 27, 11, 3, 12] </ref>. Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems. Research has also been carried out specifically on developing application interfaces [14, 3, 13] and compiler interfaces [26, 1]. <p> These interfaces decouple the specification of policy from the accesses to file data, allowing an application to dictate how its data should be distributed across the system disks while hiding the distribution of the data from subsequent file accesses. A few parallel I/O systems have made portability a priority <ref> [25, 22, 11, 3, 12] </ref>. These systems have been built for distributed memory systems on top of native file systems and portable communication interfaces such as PVM or MPI.
Reference: [13] <author> John F. Karpovich, Andrew S. Grimshaw, and James C. </author> <title> French. Extensible file systems ELFS: An object-oriented approach to high performance file I/O. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <pages> pages 191-204, </pages> <month> Octo-ber </month> <year> 1994. </year>
Reference-contexts: Finally, Section 6 describes techniques used to specify file system policies. 2 Interface properties A good parallel I/O interface will have the following set of properties: flexibility: The interface should be simple for novice programmers while still satisfying the performance requirements of expert programmers <ref> [14, 4, 13, 12] </ref>. The application should be able to choose how much, if any, policy related information it specifies to the system. <p> Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems. Research has also been carried out specifically on developing application interfaces <ref> [14, 3, 13] </ref> and compiler interfaces [26, 1]. All of these approaches to parallel I/O systems consider interface issues to a varying degree. 2 Most existing parallel I/O systems are based on a read/write interface. <p> While allowing matrix-based interfaces to be implemented in a layer above it, building-block composition allows the expert user much greater control over operating system policy. Also, application level libraries, such as ELFS <ref> [13] </ref> and ASF [18], can exploit the power of building-block composition, while hiding the low-level details from the application programmer.
Reference: [14] <author> David Kotz. </author> <title> Multiprocessor file system interfaces. </title> <booktitle> In Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 194-201, </pages> <year> 1993. </year>
Reference-contexts: Finally, Section 6 describes techniques used to specify file system policies. 2 Interface properties A good parallel I/O interface will have the following set of properties: flexibility: The interface should be simple for novice programmers while still satisfying the performance requirements of expert programmers <ref> [14, 4, 13, 12] </ref>. The application should be able to choose how much, if any, policy related information it specifies to the system. <p> Most current interfaces embed policy decisions in the operations used to access file data, forcing the applications to be rewrit ten when these policy decisions are changed. dynamic policy choice: Applications can have multiple phases, each with a different file access pattern <ref> [14, 4, 27, 3] </ref>. <p> Similarly, the amount of inter-process communication (e.g., system calls) entailed by the interface should be minimized. concurrency support: The interface must have well defined semantics when multiple threads access the same file, should impose no constraint on concur-rency, and should support common synchronization requirements with minimal overhead <ref> [22, 14] </ref>. For example, if the threads of a parallel application are accessing a file as a shared stream of data, then the interface should be defined so that the cost to atomically update the shared file offset is minimal. <p> Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems. Research has also been carried out specifically on developing application interfaces <ref> [14, 3, 13] </ref> and compiler interfaces [26, 1]. All of these approaches to parallel I/O systems consider interface issues to a varying degree. 2 Most existing parallel I/O systems are based on a read/write interface. <p> All systems support concurrent file access, some relying on file types to define which parts of the file will be accessed independently <ref> [3, 14, 4] </ref>, some by changing the semantics of file pointers [14]. PIOUS uses a transaction-based system to solve the synchronization problem and provide some fault tolerance [22]. In general, systems that implement new file types tend not to worry about compatibility with a traditional Unix interface. <p> All systems support concurrent file access, some relying on file types to define which parts of the file will be accessed independently [3, 14, 4], some by changing the semantics of file pointers <ref> [14] </ref>. PIOUS uses a transaction-based system to solve the synchronization problem and provide some fault tolerance [22]. In general, systems that implement new file types tend not to worry about compatibility with a traditional Unix interface.
Reference: [15] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year> <note> Updated as Dart-mouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: An application ported from one platform to another should not have to be rewritten; it should only be necessary to change the policy related information used to optimize performance. low overhead: Since performance is the central goal for exploiting parallelism, the interface should enable a low overhead implementation <ref> [15] </ref>. For example, it should not be necessary to copy data between multiple buffers when servicing application requests. <p> Some have developed complete parallel files systems [10, 19, 4, 8]. Others have developed servers or run-time libraries for optimizing I/O performance that run on multiple systems [25, 22, 27, 11, 3, 12]. Some research has concentrated on developing specific techniques to improve I/O performance <ref> [15, 7] </ref> that could be incorporated into larger systems. Research has also been carried out specifically on developing application interfaces [14, 3, 13] and compiler interfaces [26, 1]. <p> To avoid the limitations and performance problems of a simple independent read/write interface some researchers have turned to much higher-level interfaces where the programmer specifies I/O requests in terms of entire arrays or large portions of arrays, for example, and the underlying system can optimize each type of high-level request <ref> [15, 7, 11, 25] </ref>. The performance of such array based systems are impressive, and certainly interfaces tuned for arrays must be supported by any parallel I/O system that seeks to address the requirements of scientific applications. <p> This enables the system to handle all requests for a single file block at the same time, avoiding multiple reads of the same block from disk. It also makes it possible to use techniques such as disk-directed I/O <ref> [15, 7] </ref> that allow the layout of the data on disk to be taken into account to minimize disk seeks. The interfaces for supporting processor specific views and collective I/O are all built on read/write interfaces for accessing the file data. <p> A much more interesting alternative is to have the memory manager directly support these facilities, replacing the per-processor buffers required by the read/write interface with mapped regions. Providing this support in the memory manager could result in a large improvement in performance. Consider Kotz's disk-directed I/O <ref> [15] </ref> modified to use mapped file I/O, and assume that the memory manager makes each page available to the application process as soon as all the I/O nodes have completed accessing it.
Reference: [16] <author> Orran Krieger. </author> <title> HFS: A flexible file system for shared-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: and D may each store data on a single disk, object B might be a distribution object that distributes the file data to C and D, and object A might be a compression/decompression object that de-compresses data read from B and compresses data being written to B. cane file system <ref> [16] </ref> (of which the Alloc Stream Facility is one layer). Each file (and open file instance) is implemented by a different building block composition, where each of the building blocks may define a portion of the file's structure or implement a simple set of policies.
Reference: [17] <author> Orran Krieger, Michael Stumm, and Ronald Un-rau. </author> <title> The Alloc Stream Facility: A redesign of application-level stream I/O. </title> <type> Technical Report CSRI-275, </type> <institution> Computer Systems Research Institute, University of Toronto, Toronto, Canada, M5S 1A1, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: However, some systems (e.g., AIX) do not incur any page faults when pages in the file cache are accessed. Also, the cost of a page fault is substantially less than the cost of a read system call on many systems <ref> [17] </ref>. Finally, mapped-file I/O places a lower storage demand 4 on main memory. When an application uses a read/write interface, file data is buffered both in the cache of the memory manager and in application buffers.
Reference: [18] <author> Orran Krieger, Michael Stumm, and Ronald Un-rau. </author> <title> The Alloc Stream Facility: A redesign of application-level stream I/O. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 75-83, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Other parallel I/O interfaces are provided as extensions to Unix I/O, and only the I/O intensive portions of an applications need to be rewritten to exploit the advantages of the parallel interface. We have developed an application level I/O library, called the Alloc Stream Facility (ASF) <ref> [18] </ref>, which addresses these problems. ASF provides an interface, called the Alloc Stream Interface (ASI), which preserves the advantages of mapped-file I/O while still allowing uniform access for all types of I/O (e.g., terminals, pipes, and network connections). <p> While allowing matrix-based interfaces to be implemented in a layer above it, building-block composition allows the expert user much greater control over operating system policy. Also, application level libraries, such as ELFS [13] and ASF <ref> [18] </ref>, can exploit the power of building-block composition, while hiding the low-level details from the application programmer. Building-block composition can be considered both a technique for structuring flexible system software (that can support many policies) and a technique for giving applications the ability to control operating system policies.
Reference: [19] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopou-los, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference-contexts: Some have developed complete parallel files systems <ref> [10, 19, 4, 8] </ref>. Others have developed servers or run-time libraries for optimizing I/O performance that run on multiple systems [25, 22, 27, 11, 3, 12]. Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems.
Reference: [20] <author> Todd C. Mowry and Angela Demke. </author> <title> Information on modifying a prefetching compiler to prefetch file data. </title> <type> personal communication, </type> <year> 1995. </year>
Reference-contexts: A compiler that automatically generates prefetch instructions for cache lines [21] was recently modified to generate prefetch requests to Hurricane mapped pages. Modifying the compiler involved less than two weeks effort, while modifying the compiler to generate asynchronous read requests would have been more difficult <ref> [20] </ref>. Even when using a system-level read/write interface, a sophisticated compiler can hide from the application the explicit read and write operations, giving the application an abstraction similar to mapped files.
Reference: [21] <author> Todd C. Mowry, Monica S. Lam, and Anoop Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating System (ASPLOS), </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year> <journal> Published as SIGPLAN Notices, </journal> <volume> volume 27, number 9. </volume>
Reference-contexts: The Hurricane memory manager [28] supports prefetch and poststore operations that allow the application to make asynchronous requests for memory-mapped pages to be fetched from or stored to disk. A compiler that automatically generates prefetch instructions for cache lines <ref> [21] </ref> was recently modified to generate prefetch requests to Hurricane mapped pages. Modifying the compiler involved less than two weeks effort, while modifying the compiler to generate asynchronous read requests would have been more difficult [20].
Reference: [22] <author> Steven A. Moyer and V. S. Sunderam. </author> <title> A parallel I/O system for high-performance distributed computing. </title> <booktitle> In Proceedings of the IFIP WG10.3 Working Conference on Programming Environments for Massively Parallel Distributed Systems, </booktitle> <year> 1994. </year>
Reference-contexts: The same mechanisms for specifying policy should apply in both cases. portability: The interface should be applicable to the full range of parallel systems, from distributed systems to multicomputers to shared memory multiprocessors <ref> [25, 22, 3, 12, 11] </ref>. <p> Similarly, the amount of inter-process communication (e.g., system calls) entailed by the interface should be minimized. concurrency support: The interface must have well defined semantics when multiple threads access the same file, should impose no constraint on concur-rency, and should support common synchronization requirements with minimal overhead <ref> [22, 14] </ref>. For example, if the threads of a parallel application are accessing a file as a shared stream of data, then the interface should be defined so that the cost to atomically update the shared file offset is minimal. <p> Some have developed complete parallel files systems [10, 19, 4, 8]. Others have developed servers or run-time libraries for optimizing I/O performance that run on multiple systems <ref> [25, 22, 27, 11, 3, 12] </ref>. Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems. Research has also been carried out specifically on developing application interfaces [14, 3, 13] and compiler interfaces [26, 1]. <p> These interfaces decouple the specification of policy from the accesses to file data, allowing an application to dictate how its data should be distributed across the system disks while hiding the distribution of the data from subsequent file accesses. A few parallel I/O systems have made portability a priority <ref> [25, 22, 11, 3, 12] </ref>. These systems have been built for distributed memory systems on top of native file systems and portable communication interfaces such as PVM or MPI. <p> All systems support concurrent file access, some relying on file types to define which parts of the file will be accessed independently [3, 14, 4], some by changing the semantics of file pointers [14]. PIOUS uses a transaction-based system to solve the synchronization problem and provide some fault tolerance <ref> [22] </ref>. In general, systems that implement new file types tend not to worry about compatibility with a traditional Unix interface. Vesta, however, provides a utility to convert parallel files to traditional ones that might be used in editors and visualization programs.
Reference: [23] <author> Eric Parsons, Ben Gamsa, Orran Krieger, and Michael Stumm. </author> <title> (de-)clustering objects for multiprocessor system software. </title> <booktitle> In Proceedings of the 1995 International Workshop on Object Orientation in Operating Systems, </booktitle> <year> 1995. </year> <month> 10 </month>
Reference-contexts: For other examples, we refer to a paper by Cormen and Kotz, where they describe a number of I/O intensive algorithms that are not matrix based [5]. In this section we briefly describe building-block composition, a low-level technique for specifying policy that we employ in the Tornado operating system <ref> [23] </ref>. While allowing matrix-based interfaces to be implemented in a layer above it, building-block composition allows the expert user much greater control over operating system policy.
Reference: [24] <author> R. Hugo Patterson, Garth A. Gibson, and M. Satya--narayanan. </author> <title> Informed prefetching: Converting high throughput to low latency. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 41-55, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution>
Reference-contexts: Asynchronous read and write interfaces allow the application to overlap I/O and computation, but they tend to be difficult to use, since the application must check to see if the request has completed before it can (re)use the buffer <ref> [24] </ref>. Also, once an application has initiated an asynchronous request, it cannot use any part of the buffer until the entire request has completed. Hence, the application is still implicitly making the policy decision of the granularity of I/O requests to disk.
Reference: [25] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: The same mechanisms for specifying policy should apply in both cases. portability: The interface should be applicable to the full range of parallel systems, from distributed systems to multicomputers to shared memory multiprocessors <ref> [25, 22, 3, 12, 11] </ref>. <p> Some have developed complete parallel files systems [10, 19, 4, 8]. Others have developed servers or run-time libraries for optimizing I/O performance that run on multiple systems <ref> [25, 22, 27, 11, 3, 12] </ref>. Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems. Research has also been carried out specifically on developing application interfaces [14, 3, 13] and compiler interfaces [26, 1]. <p> To avoid the limitations and performance problems of a simple independent read/write interface some researchers have turned to much higher-level interfaces where the programmer specifies I/O requests in terms of entire arrays or large portions of arrays, for example, and the underlying system can optimize each type of high-level request <ref> [15, 7, 11, 25] </ref>. The performance of such array based systems are impressive, and certainly interfaces tuned for arrays must be supported by any parallel I/O system that seeks to address the requirements of scientific applications. <p> These interfaces decouple the specification of policy from the accesses to file data, allowing an application to dictate how its data should be distributed across the system disks while hiding the distribution of the data from subsequent file accesses. A few parallel I/O systems have made portability a priority <ref> [25, 22, 11, 3, 12] </ref>. These systems have been built for distributed memory systems on top of native file systems and portable communication interfaces such as PVM or MPI. <p> A common characteristic of recently developed interfaces is that the application can specify per-processor views of file data, where non-contiguous portions of the file appear logically contiguous to the requesting processor <ref> [4, 27, 25] </ref>. These interfaces give the application a great deal of flexibility in dictating how its matrix should be distributed across the system disks. Another advantage of providing multiple logical views of a file is that applications can easily change their logical access patterns.
Reference: [26] <author> R. Thakur, R. Bordawekar, and A. Choudhary. </author> <title> Compiler and Runtime Support for Out-of-Core HPF Programs. </title> <booktitle> In Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <pages> pages 382-391, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems. Research has also been carried out specifically on developing application interfaces [14, 3, 13] and compiler interfaces <ref> [26, 1] </ref>. All of these approaches to parallel I/O systems consider interface issues to a varying degree. 2 Most existing parallel I/O systems are based on a read/write interface.
Reference: [27] <author> Rajeev Thakur, Rajesh Bordawekar, Alok Choud-hary, Ravi Ponnusamy, and Tarvinder Singh. </author> <title> PASSION runtime library for parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 119-128, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Most current interfaces embed policy decisions in the operations used to access file data, forcing the applications to be rewrit ten when these policy decisions are changed. dynamic policy choice: Applications can have multiple phases, each with a different file access pattern <ref> [14, 4, 27, 3] </ref>. <p> Some have developed complete parallel files systems [10, 19, 4, 8]. Others have developed servers or run-time libraries for optimizing I/O performance that run on multiple systems <ref> [25, 22, 27, 11, 3, 12] </ref>. Some research has concentrated on developing specific techniques to improve I/O performance [15, 7] that could be incorporated into larger systems. Research has also been carried out specifically on developing application interfaces [14, 3, 13] and compiler interfaces [26, 1]. <p> A common characteristic of recently developed interfaces is that the application can specify per-processor views of file data, where non-contiguous portions of the file appear logically contiguous to the requesting processor <ref> [4, 27, 25] </ref>. These interfaces give the application a great deal of flexibility in dictating how its matrix should be distributed across the system disks. Another advantage of providing multiple logical views of a file is that applications can easily change their logical access patterns.
Reference: [28] <author> Ronald C. Unrau. </author> <title> Scalable Memory Management through Hierarchical Symmetric Multiprocessing. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: We have found that making secondary storage accessible as a layer in the memory hierarchy allows the techniques used to tolerate memory latency to be exploited for tolerating disk latency. The Hurricane memory manager <ref> [28] </ref> supports prefetch and poststore operations that allow the application to make asynchronous requests for memory-mapped pages to be fetched from or stored to disk. A compiler that automatically generates prefetch instructions for cache lines [21] was recently modified to generate prefetch requests to Hurricane mapped pages.
Reference: [29] <author> Darren Erik Vengroff and Jeffrey Scott Vitter. </author> <title> I/O-efficient scientific computation using TPIE. </title> <booktitle> In Proceedings of the 1995 IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> October </month> <year> 1995. </year> <note> To appear. 11 </note>
Reference-contexts: The performance of such array based systems are impressive, and certainly interfaces tuned for arrays must be supported by any parallel I/O system that seeks to address the requirements of scientific applications. However, not all I/O intensive parallel applications are array based <ref> [5, 29] </ref>, and the specialized nature of these interfaces makes them inappropriate for any other types of file access. Also, these interfaces typically still have the disadvantage that the application specifies the target buffer for an I/O request.
References-found: 29

