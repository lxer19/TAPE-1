URL: http://www.cs.umr.edu/techreports/94-05.ps
Refering-URL: http://www.cs.umr.edu/techreports/
Root-URL: 
Title: A Reduction Semantics for Array Expressions: The PSI Compiler  
Author: Lenore Mullin and Scott Thibault 
Address: Rolla, Missouri 65401  
Affiliation: Department of Computer Science University of Missouri-Rolla  
Date: March 9, 1994  
Pubnum: CSC-94-05  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> Gaetan Hains and Lenore M.R. Mullin. </author> <title> An algebra of multidimensional arrays. </title> <type> Technical Report 782, </type> <institution> Universite de Montreal, </institution> <year> 1991. </year> <booktitle> (0) Submitted to the 12th Conference on the Foundations of Software Technology and Theoretical Computer Science, </booktitle> <month> 18-20 December </month> <year> 1992, </year> <title> New Delhi, </title> <booktitle> India (1) Presented at the 2nd Montreal Workshop on Programming Language Theory Algebraic and Logical Approaches in Programming Languages, </booktitle> <month> December </month> <year> 1991. </year>
Reference: [2] <author> L. Mullin, D. Dooling, E. Sandberg, and S. Thibault. </author> <title> Formal methods for the scheduling, routing and communications protocol of a logarithmic scan on a message passing distributed operating system: Pram algorithms revisited. </title> <type> Technical Report CSEE/92/07-10, </type> <institution> University of Vermont, Dept of CSEE, </institution> <year> 1992. </year> <title> Under Review, </title> <journal> IEEE Transactions on Parallel and Distributed Computing. </journal>
Reference-contexts: The input program for our example is shown in figure 2. The resulting C output from compiling the test procedure is shown in figure 3. Conclusion Our decomposition and mapping technologies to one or more processors also employs the Psi Calculus <ref> [2, 5, 6] </ref>. The techniques employed in [5] were extended and scaled to a CM-5, a topic of our most recent paper [6]. Our recent efforts are to interface scientific languages such as Fortran 90 and to benchmark our performance with other Fortran 90 compilers. <p> We are also decomposing problems using PVM. We will determine if our automatic techniques compare to PVM's performance and versatility. Future work is to support heterogeneous processing and load balancing. 9 #include &lt;stdlib.h&gt; #include "moalib.e" test (float A [], float C []) f int i1; int step1 <ref> [2] </ref>; int step2 [2]; int shift,offset; float B []=f1.000000, 2.000000, 3.000000, 4.000000, 1.000000, 2.000000, 3.000000, 4.000000g; shift=0*4+0; offset=0*4+0; for (i0=0; i0&lt;2; i0++) f *( A+shift)=*( B+offset); shift++; offset++; g shift=2*4+0; offset=1*6+2; step1 [0]=2; for (i0=0; i0&lt;3; i0++) f *( A+shift)=*( C+offset); shift++; offset++; g offset+=step1 [0]; g 10 <p> We will determine if our automatic techniques compare to PVM's performance and versatility. Future work is to support heterogeneous processing and load balancing. 9 #include &lt;stdlib.h&gt; #include "moalib.e" test (float A [], float C []) f int i1; int step1 <ref> [2] </ref>; int step2 [2]; int shift,offset; float B []=f1.000000, 2.000000, 3.000000, 4.000000, 1.000000, 2.000000, 3.000000, 4.000000g; shift=0*4+0; offset=0*4+0; for (i0=0; i0&lt;2; i0++) f *( A+shift)=*( B+offset); shift++; offset++; g shift=2*4+0; offset=1*6+2; step1 [0]=2; for (i0=0; i0&lt;3; i0++) f *( A+shift)=*( C+offset); shift++; offset++; g offset+=step1 [0]; g 10
Reference: [3] <author> L. Mullin and S. Thibault. </author> <title> The psi compiler project: Backend to massively parallel scientific programming languages. </title> <booktitle> In Fourth International Workshop on Compilers for Parallel Computers, </booktitle> <year> 1993. </year>
Reference: [4] <author> Lenore M. Restifo Mullin. </author> <title> A Mathematics of Arrays. </title> <type> Ph.D. dissertation, </type> <institution> Syracuse University, </institution> <month> December </month> <year> 1988. </year>
Reference-contexts: 1 Order is irrelevant since the -calculus has the Church-Rosser Property. 2 node is visited that the shapes of it's arguments (children) have already been computed. The shape of the internal nodes is computed in terms of the shapes of it's children by the following rules <ref> [4] </ref>: 2 Shape Calculation * Dimension: (ffi~) fi * Shape: (~) &lt; ffi~ &gt; * Psi: (~v ~ r ) (t~v) 5 (~ r ) * Concatenation: (~ l ++~ r ) ((1 4 (~ l )) + (1 4 (~ r ))) ++(1 5 (~ l )) * Algebraic operators:
Reference: [5] <author> L.R. Mullin, D. Dooling, E. Sandberg, and S. Thibault. </author> <title> Formal methods for scheduling and communication protocol. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Distributed Computing, </booktitle> <month> july </month> <year> 1993. </year>
Reference-contexts: The input program for our example is shown in figure 2. The resulting C output from compiling the test procedure is shown in figure 3. Conclusion Our decomposition and mapping technologies to one or more processors also employs the Psi Calculus <ref> [2, 5, 6] </ref>. The techniques employed in [5] were extended and scaled to a CM-5, a topic of our most recent paper [6]. Our recent efforts are to interface scientific languages such as Fortran 90 and to benchmark our performance with other Fortran 90 compilers. <p> The input program for our example is shown in figure 2. The resulting C output from compiling the test procedure is shown in figure 3. Conclusion Our decomposition and mapping technologies to one or more processors also employs the Psi Calculus [2, 5, 6]. The techniques employed in <ref> [5] </ref> were extended and scaled to a CM-5, a topic of our most recent paper [6]. Our recent efforts are to interface scientific languages such as Fortran 90 and to benchmark our performance with other Fortran 90 compilers. We are also decomposing problems using PVM.
Reference: [6] <author> L.R. Mullin, D. Dooling, E. Sandberg, and S. Thibault. </author> <title> Formal methods for portable, scalable, scheduling and communication protocol. </title> <type> Technical Report CSC 94-04, </type> <institution> University of Missouri-Rolla, </institution> <year> 1994. </year> <booktitle> submitted toProceedings of the Third International Symposium on High Performance Distributed Computing. </booktitle> <pages> 11 </pages>
Reference-contexts: The input program for our example is shown in figure 2. The resulting C output from compiling the test procedure is shown in figure 3. Conclusion Our decomposition and mapping technologies to one or more processors also employs the Psi Calculus <ref> [2, 5, 6] </ref>. The techniques employed in [5] were extended and scaled to a CM-5, a topic of our most recent paper [6]. Our recent efforts are to interface scientific languages such as Fortran 90 and to benchmark our performance with other Fortran 90 compilers. <p> Conclusion Our decomposition and mapping technologies to one or more processors also employs the Psi Calculus [2, 5, 6]. The techniques employed in [5] were extended and scaled to a CM-5, a topic of our most recent paper <ref> [6] </ref>. Our recent efforts are to interface scientific languages such as Fortran 90 and to benchmark our performance with other Fortran 90 compilers. We are also decomposing problems using PVM. We will determine if our automatic techniques compare to PVM's performance and versatility.
References-found: 6

