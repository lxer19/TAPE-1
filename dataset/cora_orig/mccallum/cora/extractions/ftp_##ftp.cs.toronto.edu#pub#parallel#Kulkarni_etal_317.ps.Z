URL: ftp://ftp.cs.toronto.edu/pub/parallel/Kulkarni_etal_317.ps.Z
Refering-URL: http://www.cs.toronto.edu/~kulki/pubs_abs.html
Root-URL: 
Email: kulki@cs.toronto.edu  
Title: Generalized Theory of Linear Loop Transformations  
Author: Dattatraya Kulkarni and Michael Stumm Ron Unrau Wei Li 
Address: Toronto, Toronto, Canada, M5S 1A4  Toronto, Canada, M3C 1V7  Rochester, NY 14627  
Affiliation: Department of Computer Science Department of Electrical and Computer Engineering University of  IBM Toronto Laboratory  Department of Computer Science University of Rochester  
Note: A  
Abstract: Technical Report CSRI-317, Computer Systems Research Institute, University of Toronto, December 1994. Abstract In this paper we present a new theory of linear loop transformations called Computation Decomposition and Alignment (CDA). A CDA transformation has two components: Computation Decomposition first decomposes the computations in the loop into computations of finer granularity, from iterations to instances of subexpressions. Computation Alignment subsequently, linearly transforms each of these sets of computations, possibly by using a different transformation for each set. This framework subsumes all existing linear transformation frameworks in that it reduces to a conventional linear loop transformation when the smallest granularity is an iteration, and it reduces to some of the more recently extended frameworks when the smallest granularity is a statement instance. The possibility of being able to align computations at arbitrary granularities adds a new dimensions to performance optimization on high performance computing platforms. We describe Computation Decomposition and Alignment and provide examples of CDA transformations. We present some heuristics to derive appropriate CDA transformations, given a desired optimization objective. We present the results of experiments run on the KSR1 multiprocessor and various RS6000 and Sparc platforms that demonstrate that CDA can result in substantial performance improvements. 
Abstract-found: 1
Intro-found: 1
Reference: [AH91] <author> S. G. Abraham and D. E. Hudak. </author> <title> Compile-time partitioning of iterative parallel loops to reduce cache coherency traffic. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 318-328, </pages> <month> July 91. </month>
Reference-contexts: a class of flexible computation rules with the aid of owner-computes. 5 Assuming appropriate padding of A t and T r when the dimension of the array is different from n. 12 Reducing communication for a reference stencil A communication optimal distribution of an array depends on its reference stencil <ref> [AH91, IT88, AI91] </ref>. A CDA transformation can modify the reference stencil, thus providing an additional dimension of optimization in the choice of distribution. Conversely, if an array distribution is given, then it is possible to change the reference stencil to suit the given distribution.
Reference: [AC87] <author> Randy Allen, David Callahan, and Ken Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Conference Record of the 14th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 63-76, </pages> <address> Munich, West Germany, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: CDA is also closely related to what we call Computational Alignment (CA), which applies a separate linear transformation to each statement in the loop body. This notion of applying a separate transformation to each statement has origins in loop alignment <ref> [Pad79, AC87] </ref>, where the transformations are simple offsets. Several researchers independently recognized the advantages of such transformations over the last two years. For example, Kelly and Pugh [KP92] proposed a framework to include separate schedules for each statement.
Reference: [AI91] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with DO loops. </title> <booktitle> In Proceedings of the 3rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <volume> volume 26, </volume> <pages> pages 39-50, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: In this framework, a transformation or a sequence thereof is represented by a single non-singular matrix, which can be used to directly derive the transformed loop bounds, references and dependences using well known techniques <ref> [Ban90, WL90, KKB92, LP92, Ram92, AI91, CFR93] </ref>. <p> a class of flexible computation rules with the aid of owner-computes. 5 Assuming appropriate padding of A t and T r when the dimension of the array is different from n. 12 Reducing communication for a reference stencil A communication optimal distribution of an array depends on its reference stencil <ref> [AH91, IT88, AI91] </ref>. A CDA transformation can modify the reference stencil, thus providing an additional dimension of optimization in the choice of distribution. Conversely, if an array distribution is given, then it is possible to change the reference stencil to suit the given distribution.
Reference: [AL93] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 28, </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: a single matrix to represent a compound transformation of many existing transformations, ii) it allowed the development of a set of generic techniques that could transform loops in a systematic way, independent of the particular sequence of transformations being applied, and iii) it made possible the quantification of optimization objectives <ref> [LP92, AL93, Wol92, KKB92, KKB92] </ref>. A linear loop transformation does not change the makeup of an iteration as it maps one iteration space (IS) onto another; each iteration is mapped from a point in the original IS to a point in the new IS in its entirety (unchanged).
Reference: [Ban90] <author> Utpal Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of Third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Being able to align computations at arbitrary granularities adds a new dimension to performance optimization on high performance computing platforms, thus extending significantly the existing frameworks both in power and flexibility. CDA has its roots in, and is a generalization of linear loop transformations <ref> [Ban90, WL90, Dow90, KKB91, LP92] </ref>. Most of the loop transformations proposed to date [PW86, Wol90, Ban93, Ban94] can be viewed as special instances of linear transformations. <p> In this framework, a transformation or a sequence thereof is represented by a single non-singular matrix, which can be used to directly derive the transformed loop bounds, references and dependences using well known techniques <ref> [Ban90, WL90, KKB92, LP92, Ram92, AI91, CFR93] </ref>.
Reference: [Ban93] <author> Utpal Banerjee. </author> <title> Loop Transformations for Restructuring Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: CDA has its roots in, and is a generalization of linear loop transformations [Ban90, WL90, Dow90, KKB91, LP92]. Most of the loop transformations proposed to date <ref> [PW86, Wol90, Ban93, Ban94] </ref> can be viewed as special instances of linear transformations.
Reference: [Ban94] <author> Utpal Banerjee. </author> <title> Loop Parallelization. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference-contexts: CDA has its roots in, and is a generalization of linear loop transformations [Ban90, WL90, Dow90, KKB91, LP92]. Most of the loop transformations proposed to date <ref> [PW86, Wol90, Ban93, Ban94] </ref> can be viewed as special instances of linear transformations.
Reference: [CFR93] <author> J. Collard, P. Feautrier, and T. Risset. </author> <title> Construction of do loops from systems of affine constraints. </title> <type> Technical report, Research Report 93-15, </type> <institution> Ecole Normale Superieure de Lyon, France, </institution> <year> 1993. </year>
Reference-contexts: In this framework, a transformation or a sequence thereof is represented by a single non-singular matrix, which can be used to directly derive the transformed loop bounds, references and dependences using well known techniques <ref> [Ban90, WL90, KKB92, LP92, Ram92, AI91, CFR93] </ref>.
Reference: [CF87] <author> R. Cytron and J. Ferrante. </author> <booktitle> What's in a name? In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <year> 1987. </year>
Reference-contexts: ] j i j w i [d 2 (i; j; 1) t ] ! r [(i; j; 1) t ] j i j where d 2 = 6 6 4 0 1 0 3 7 7 Memory related dependences or output and anti-dependences are either eliminated using some standard technique <ref> [Fea88, CF87] </ref>, or are represented similar to flow dependences. The CDA transformation, among other things, updates the dependence relations.
Reference: [Dow90] <author> M.L. Dowling. </author> <title> Optimum code parallelization using unimodular transformations. </title> <journal> Parallel Computing, </journal> <volume> 16 </volume> <pages> 155-171, </pages> <year> 1990. </year>
Reference-contexts: Being able to align computations at arbitrary granularities adds a new dimension to performance optimization on high performance computing platforms, thus extending significantly the existing frameworks both in power and flexibility. CDA has its roots in, and is a generalization of linear loop transformations <ref> [Ban90, WL90, Dow90, KKB91, LP92] </ref>. Most of the loop transformations proposed to date [PW86, Wol90, Ban93, Ban94] can be viewed as special instances of linear transformations.
Reference: [Fea88] <author> P. Feautrier. </author> <title> Array expansion. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <year> 1988. </year>
Reference-contexts: ] j i j w i [d 2 (i; j; 1) t ] ! r [(i; j; 1) t ] j i j where d 2 = 6 6 4 0 1 0 3 7 7 Memory related dependences or output and anti-dependences are either eliminated using some standard technique <ref> [Fea88, CF87] </ref>, or are represented similar to flow dependences. The CDA transformation, among other things, updates the dependence relations.
Reference: [Fea91] <author> P. Feautrier. </author> <title> Dataflow analysis of array and scalar references. </title> <journal> International Journal of Parallel Programming, </journal> <volume> volume 20(1), </volume> <year> 1991. </year>
Reference-contexts: We represent data flow constraints in the loop with dependence relations [Pug91], and we keep the exact dependence information between each pair of read and write <ref> [Fea91, MHL91, Pug92] </ref>. Keeping exact dependence information is important in our case, since our transformations can be applied at any granularity of computation. <p> In such case, we just have to verify that the last column in the new d wr is lexicographically negative (i.e. write is earlier). If the dependences are non-uniform, then sophisticated techniques that reason with symbolic affine constraints can be used <ref> [PW93, Fea91] </ref>. There are cases, when the only dependences violated are (0) flow dependences between statements, and textual in terchange will be required to make the dependence legal [KS94]. Disregarding the loop bounds for the moment, consider again our running example. Figure 7a shows the transformed loop body.
Reference: [HPF93] <author> H. Forum. </author> <title> Hpf: High performance fortran language specification. </title> <type> Technical report, HPF Forum, </type> <year> 1993. </year>
Reference-contexts: For multiprocessors, besides maximizing cache locality, CDA can be used for data alignment [KN90, LC91, OH92], in order to reduce the number of remote memory accesses, for access normalization [LP92] so that the references match the data alignments, or for the elimination of ownership tests <ref> [HKKKT91, HPF93] </ref>. A CDA transformation has two components: First, Computation Decomposition decomposes the computations of the loop into computations of finer granularity; the granularity of the computations can vary from iterations to instances of subexpressions.
Reference: [GJG88] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: It can be used to pursue a variety of optimization objectives, both when targeting a uniprocessor system or a parallel system. For uniprocessor systems, CDA can be used to minimize reference windows <ref> [GJG88] </ref> in order to maximize cache reuse and to reduce register pressure. As we will show, it can also be used to move computations to make it easier for later stages of the compiler to optimize processor pipelines. <p> In comparison, a CA transformed loop would still need to access a total of 6 elements. * Cache locality : The CDA transformed loop has better cache locality. The transformation reduced the reference window <ref> [GJG88] </ref> of B along the i th dimension from 2n to n, thus halving B's cache context.
Reference: [HKKKT91] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the fortran d programming system. </title> <type> Technical Report CRPC-TR91121, </type> <institution> Dept of computer Science, Rice University, </institution> <year> 1991. </year>
Reference-contexts: For multiprocessors, besides maximizing cache locality, CDA can be used for data alignment [KN90, LC91, OH92], in order to reduce the number of remote memory accesses, for access normalization [LP92] so that the references match the data alignments, or for the elimination of ownership tests <ref> [HKKKT91, HPF93] </ref>. A CDA transformation has two components: First, Computation Decomposition decomposes the computations of the loop into computations of finer granularity; the granularity of the computations can vary from iterations to instances of subexpressions.
Reference: [IT88] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Conference Record of the 15th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 319-329, </pages> <address> San Diego, CA, </address> <year> 1988. </year>
Reference-contexts: a class of flexible computation rules with the aid of owner-computes. 5 Assuming appropriate padding of A t and T r when the dimension of the array is different from n. 12 Reducing communication for a reference stencil A communication optimal distribution of an array depends on its reference stencil <ref> [AH91, IT88, AI91] </ref>. A CDA transformation can modify the reference stencil, thus providing an additional dimension of optimization in the choice of distribution. Conversely, if an array distribution is given, then it is possible to change the reference stencil to suit the given distribution.
Reference: [KKB91] <author> K.G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Generalized unimodular loop transformations for distributed memory multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <address> Chicago, MI, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Being able to align computations at arbitrary granularities adds a new dimension to performance optimization on high performance computing platforms, thus extending significantly the existing frameworks both in power and flexibility. CDA has its roots in, and is a generalization of linear loop transformations <ref> [Ban90, WL90, Dow90, KKB91, LP92] </ref>. Most of the loop transformations proposed to date [PW86, Wol90, Ban93, Ban94] can be viewed as special instances of linear transformations.
Reference: [KKB92] <author> K.G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Deriving good transformations for mapping nested loops on hierarchical parallel machines in polynomial time. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: In this framework, a transformation or a sequence thereof is represented by a single non-singular matrix, which can be used to directly derive the transformed loop bounds, references and dependences using well known techniques <ref> [Ban90, WL90, KKB92, LP92, Ram92, AI91, CFR93] </ref>. <p> a single matrix to represent a compound transformation of many existing transformations, ii) it allowed the development of a set of generic techniques that could transform loops in a systematic way, independent of the particular sequence of transformations being applied, and iii) it made possible the quantification of optimization objectives <ref> [LP92, AL93, Wol92, KKB92, KKB92] </ref>. A linear loop transformation does not change the makeup of an iteration as it maps one iteration space (IS) onto another; each iteration is mapped from a point in the original IS to a point in the new IS in its entirety (unchanged).
Reference: [KKBP91] <author> D. Kulkarni, K.G. Kumar, A. Basu, and A. Paulraj. </author> <title> Loop partitioning for distributed memory multiprocessors as unimodular transformations. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year> <month> 24 </month>
Reference: [KKB92] <author> K. Kumar, D. Kulkarni, and A. Basu. </author> <title> Mapping nested loops on hierarchical parallel machines us-ing unimodular transformations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> (under revision). </note>
Reference-contexts: In this framework, a transformation or a sequence thereof is represented by a single non-singular matrix, which can be used to directly derive the transformed loop bounds, references and dependences using well known techniques <ref> [Ban90, WL90, KKB92, LP92, Ram92, AI91, CFR93] </ref>. <p> a single matrix to represent a compound transformation of many existing transformations, ii) it allowed the development of a set of generic techniques that could transform loops in a systematic way, independent of the particular sequence of transformations being applied, and iii) it made possible the quantification of optimization objectives <ref> [LP92, AL93, Wol92, KKB92, KKB92] </ref>. A linear loop transformation does not change the makeup of an iteration as it maps one iteration space (IS) onto another; each iteration is mapped from a point in the original IS to a point in the new IS in its entirety (unchanged).
Reference: [KN90] <author> K. Knobe and V. Natarajan. </author> <title> Data optimization: Minimizing residual interprocessor data motion on simd machines. </title> <booktitle> In Proceedings of the Symposium on frontiers of massively parallel computation, </booktitle> <pages> pages 416-423, </pages> <year> 1990. </year>
Reference-contexts: As we will show, it can also be used to move computations to make it easier for later stages of the compiler to optimize processor pipelines. For multiprocessors, besides maximizing cache locality, CDA can be used for data alignment <ref> [KN90, LC91, OH92] </ref>, in order to reduce the number of remote memory accesses, for access normalization [LP92] so that the references match the data alignments, or for the elimination of ownership tests [HKKKT91, HPF93]. <p> An algorithm to find data alignment transformations, typically identifies a data alignment constraint, and derives a data alignment transformation that satisfies it <ref> [KN90, LC91, OH92] </ref>. We believe that by using both data alignment and CDA transformations together, one can satisfy more constraints than existing algorithms do. Data alignment and CDA have their own advatanges and drawbacks.
Reference: [KP92] <author> W. Kelly and W. Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> Technical Report UMIACS-TR-92-126, </type> <institution> University of Maryland, </institution> <year> 1992. </year>
Reference-contexts: This notion of applying a separate transformation to each statement has origins in loop alignment [Pad79, AC87], where the transformations are simple offsets. Several researchers independently recognized the advantages of such transformations over the last two years. For example, Kelly and Pugh <ref> [KP92] </ref> proposed a framework to include separate schedules for each statement. Torres and Aygude [TALV93] integrated loop alignment with linear transformations and applied it to improve the efficiency of SPMD code.
Reference: [KPR94] <author> W. Kelly, W. Pugh, and E. Rosser. </author> <title> Code generation for multiple mappings. </title> <type> Technical Report UMIACS-TR-94-87, </type> <institution> University of Maryland, </institution> <year> 1994. </year>
Reference-contexts: An algorithm to generate a guard-free loop for T when all the statements require the same loop stride is described in [KS94]. Kelly et al. also developed an algorithm to generate code for general linear transformations but with conditionals <ref> [KPR94] </ref>. These algorithms reduce to the algorithm developed by Torres et al. when the transformations are simple offsets corresponding to loop alignments [TALV93]. We illustrate a typical way to generate guard free code with the aid of Figure 8a. The full details can be found in [KS94].
Reference: [KS94] <author> D. Kulkarni and M. Stumm. </author> <title> Computational alignment: A new, unified program transformation for local and global optimization. </title> <type> Technical Report CSRI-292, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: For example, Kelly and Pugh [KP92] proposed a framework to include separate schedules for each statement. Torres and Aygude [TALV93] integrated loop alignment with linear transformations and applied it to improve the efficiency of SPMD code. Kukarni and Stumm <ref> [KS94] </ref> presented a framework where spaces of computation are transformed relative to each other; and apply the framework for both local optimizations and global optimizations that maximize the uniformity of data accesses across loops. CDA, as presented in this paper, goes a significant step further than CA. <p> Although CDA applies to both perfect and imperfect loop nests <ref> [KS94] </ref>, we restrict ourselves in this paper to computations at the same nesting level. Moreover, for simplicity, we assume that the loop body contains only assignment statements. <p> If the dependences are non-uniform, then sophisticated techniques that reason with symbolic affine constraints can be used [PW93, Fea91]. There are cases, when the only dependences violated are (0) flow dependences between statements, and textual in terchange will be required to make the dependence legal <ref> [KS94] </ref>. Disregarding the loop bounds for the moment, consider again our running example. Figure 7a shows the transformed loop body. Note that this is an example where it was necessary to interchange statements in order to keep the CDA legal. <p> An algorithm to generate a guard-free loop for T when all the statements require the same loop stride is described in <ref> [KS94] </ref>. Kelly et al. also developed an algorithm to generate code for general linear transformations but with conditionals [KPR94]. These algorithms reduce to the algorithm developed by Torres et al. when the transformations are simple offsets corresponding to loop alignments [TALV93]. <p> These algorithms reduce to the algorithm developed by Torres et al. when the transformations are simple offsets corresponding to loop alignments [TALV93]. We illustrate a typical way to generate guard free code with the aid of Figure 8a. The full details can be found in <ref> [KS94] </ref>. The basic idea of the algorithm is to partition the new IS into segments that contain iterations with S 1 computation only, or those that contain S 2 computations only, or those with both computations. <p> One way to eliminate ownership test is to ensure that all SEIs of an iteration are to be executed by the same processor. With owner-computes rule, this is achieved by collocating the lhs references of all statements of the loop body <ref> [TALV93, KS94] </ref>. Even then, it may be more efficient to execute some subexpressions on another processor, say because it owns most of the data required for the computation. CDA can effect this modification to the owner-computes rule.
Reference: [KS95] <author> D. Kulkarni and M. Stumm. </author> <title> Computational decomposition and alignment and its applications. </title> <note> Technical Report In preparation, </note> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> January </month> <year> 1995. </year>
Reference: [KSR91] <institution> Kendall Square Research (KSR). KSR1 Principles of Operation. </institution> <address> Waltham, MA, </address> <year> 1991. </year>
Reference-contexts: The experiments are designed primarily to illustrate individual performance aspects of CDA, and not to show absolute performance of any particular application. We conducted the experiments on a KSR1 multiprocessor with a COMA architecture <ref> [KSR91] </ref>, on a cluster of RS6000 workstations, and on RS6000, Sparc-10 and Sparc-20 workstations. 7.1 NAS mg The mg program of the NAS benchmark suite is a multigrid solver that computes a three dimensional potential field.
Reference: [LC91] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed memory machines. </title> <journal> Journal of parallel and distributed computing, </journal> <volume> 13 </volume> <pages> 213-221, </pages> <year> 1991. </year>
Reference-contexts: As we will show, it can also be used to move computations to make it easier for later stages of the compiler to optimize processor pipelines. For multiprocessors, besides maximizing cache locality, CDA can be used for data alignment <ref> [KN90, LC91, OH92] </ref>, in order to reduce the number of remote memory accesses, for access normalization [LP92] so that the references match the data alignments, or for the elimination of ownership tests [HKKKT91, HPF93]. <p> An algorithm to find data alignment transformations, typically identifies a data alignment constraint, and derives a data alignment transformation that satisfies it <ref> [KN90, LC91, OH92] </ref>. We believe that by using both data alignment and CDA transformations together, one can satisfy more constraints than existing algorithms do. Data alignment and CDA have their own advatanges and drawbacks.
Reference: [Li92] <author> C.H. Li. </author> <title> Program wanal1. </title> <note> ftp ftp.cs.rice.edu, </note> <institution> Rice University, </institution> <year> 1992. </year>
Reference-contexts: We ran the resulting code on a cluster of four RS6000 workstations. Due to the elimination of ownership tests, the the CDA version took 10.6s compared to 12.1s by the original loops. 7.4 Wanal Wanal is a wave equation solver that is part of the Riceps benchmark suite <ref> [Li92] </ref>.
Reference: [LP92] <author> W. Li and K. Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <booktitle> In Proceedings of the Fifth Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: For multiprocessors, besides maximizing cache locality, CDA can be used for data alignment [KN90, LC91, OH92], in order to reduce the number of remote memory accesses, for access normalization <ref> [LP92] </ref> so that the references match the data alignments, or for the elimination of ownership tests [HKKKT91, HPF93]. <p> Being able to align computations at arbitrary granularities adds a new dimension to performance optimization on high performance computing platforms, thus extending significantly the existing frameworks both in power and flexibility. CDA has its roots in, and is a generalization of linear loop transformations <ref> [Ban90, WL90, Dow90, KKB91, LP92] </ref>. Most of the loop transformations proposed to date [PW86, Wol90, Ban93, Ban94] can be viewed as special instances of linear transformations. <p> In this framework, a transformation or a sequence thereof is represented by a single non-singular matrix, which can be used to directly derive the transformed loop bounds, references and dependences using well known techniques <ref> [Ban90, WL90, KKB92, LP92, Ram92, AI91, CFR93] </ref>. <p> a single matrix to represent a compound transformation of many existing transformations, ii) it allowed the development of a set of generic techniques that could transform loops in a systematic way, independent of the particular sequence of transformations being applied, and iii) it made possible the quantification of optimization objectives <ref> [LP92, AL93, Wol92, KKB92, KKB92] </ref>. A linear loop transformation does not change the makeup of an iteration as it maps one iteration space (IS) onto another; each iteration is mapped from a point in the original IS to a point in the new IS in its entirety (unchanged). <p> can be represented by an m fi (n + 1) reference matrix, 2 rm (r), and the loop bounds can be represented by an l fi (n + 1) bounds matrix, 3 fi (L), where l is the number of expressions in the upper and lower bounds of the loop <ref> [WL90, LP92, Ram92] </ref>. Although CDA applies to both perfect and imperfect loop nests [KS94], we restrict ourselves in this paper to computations at the same nesting level. Moreover, for simplicity, we assume that the loop body contains only assignment statements. <p> A CDA transformation can modify the reference stencil, thus providing an additional dimension of optimization in the choice of distribution. Conversely, if an array distribution is given, then it is possible to change the reference stencil to suit the given distribution. Matching loop structure to data distribution Access normalization <ref> [LP92] </ref> is a linear loop transformation that matches the references to the data distributions. For instance, when A is distributed by row, then it is preferable to transform the loop so that the reference patterns are A (i; fl), instead of say, A (i j; fl).
Reference: [LRW91] <author> M.S. Lam, E.E. Rothberg, and M.E. Wolf. </author> <title> The cache performance and optimizations of block algorithms. </title> <booktitle> In 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: In contrast to linear loop transformations, CDA can perform partial access normalization by segregating the references that need similar normalization into groups, and applying a separate transformations to each group. Improving Cache locality Linear loop transformations and tiling (or blocking) <ref> [LRW91] </ref> improve cache locality by maximizing the reuse of the data in the cache before it is displaced. CDA can enhance the effectiveness of these techniques.
Reference: [LS93] <author> Hui Li and Kenneth C. Sevcik. NUMACROS: </author> <title> Data Parallel Programming on NUMA Multiprocessors. </title> <booktitle> In Proceedings of Fourth Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS IV), </booktitle> <pages> pages 247-263, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: We ran the same experiments on a Sparc-10 system, where the improvements werer even larger. Figure 11, compares the improvements obtained on both systems. Parallel mg on the KSR1 We implemented both the original and the CDA versions of this code in C using the NUMACROS package <ref> [LS93] </ref> on the KSR1. We parallelized the CDA version of mg only along the j iterator, and we parallelized the original version along both the i and j iterators.
Reference: [MHL91] <author> D.E. Maydan, J.L. Hennessy, and M.S. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 26, </volume> <pages> pages 1-14, </pages> <address> Toronto, Ontario, Canada, </address> <year> 1991. </year>
Reference-contexts: We represent data flow constraints in the loop with dependence relations [Pug91], and we keep the exact dependence information between each pair of read and write <ref> [Fea91, MHL91, Pug92] </ref>. Keeping exact dependence information is important in our case, since our transformations can be applied at any granularity of computation.
Reference: [OH92] <author> M.F.P O'Boyle and G.A. Hedayat. </author> <title> Data alignment: Transformations to reduce communication on distributed memory architectures. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <publisher> IEE Press, </publisher> <address> Williamsburg, </address> <year> 1992. </year>
Reference-contexts: As we will show, it can also be used to move computations to make it easier for later stages of the compiler to optimize processor pipelines. For multiprocessors, besides maximizing cache locality, CDA can be used for data alignment <ref> [KN90, LC91, OH92] </ref>, in order to reduce the number of remote memory accesses, for access normalization [LP92] so that the references match the data alignments, or for the elimination of ownership tests [HKKKT91, HPF93]. <p> An algorithm to find data alignment transformations, typically identifies a data alignment constraint, and derives a data alignment transformation that satisfies it <ref> [KN90, LC91, OH92] </ref>. We believe that by using both data alignment and CDA transformations together, one can satisfy more constraints than existing algorithms do. Data alignment and CDA have their own advatanges and drawbacks.
Reference: [Pad79] <author> D. Padua. </author> <title> Multiprocessors: Discussion of some theoretical and practical problems. </title> <type> Phd thesis, </type> <institution> University of Illinois, Urbana-Champaign, </institution> <year> 1979. </year>
Reference-contexts: CDA is also closely related to what we call Computational Alignment (CA), which applies a separate linear transformation to each statement in the loop body. This notion of applying a separate transformation to each statement has origins in loop alignment <ref> [Pad79, AC87] </ref>, where the transformations are simple offsets. Several researchers independently recognized the advantages of such transformations over the last two years. For example, Kelly and Pugh [KP92] proposed a framework to include separate schedules for each statement.
Reference: [Pug91] <author> W. Pugh. </author> <title> Uniform techniques for loop optimization. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <year> 1991. </year>
Reference-contexts: We represent data flow constraints in the loop with dependence relations <ref> [Pug91] </ref>, and we keep the exact dependence information between each pair of read and write [Fea91, MHL91, Pug92]. Keeping exact dependence information is important in our case, since our transformations can be applied at any granularity of computation.
Reference: [Pug92] <author> W. Pugh. </author> <title> A practical algorithm for exact array dependence analysis. </title> <journal> In Communications of the ACM, </journal> <volume> volume 35, </volume> <pages> pages 102-114, </pages> <year> 1992. </year>
Reference-contexts: We represent data flow constraints in the loop with dependence relations [Pug91], and we keep the exact dependence information between each pair of read and write <ref> [Fea91, MHL91, Pug92] </ref>. Keeping exact dependence information is important in our case, since our transformations can be applied at any granularity of computation.
Reference: [PW86] <author> D. Padua and M. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: CDA has its roots in, and is a generalization of linear loop transformations [Ban90, WL90, Dow90, KKB91, LP92]. Most of the loop transformations proposed to date <ref> [PW86, Wol90, Ban93, Ban94] </ref> can be viewed as special instances of linear transformations.
Reference: [PW93] <author> W. Pugh and D. Wonnacott. </author> <title> An exact method for analysis of value-based array data dependences. </title> <type> Technical Report CS-TR-3196, </type> <institution> University of Maryland, </institution> <year> 1993. </year>
Reference-contexts: In such case, we just have to verify that the last column in the new d wr is lexicographically negative (i.e. write is earlier). If the dependences are non-uniform, then sophisticated techniques that reason with symbolic affine constraints can be used <ref> [PW93, Fea91] </ref>. There are cases, when the only dependences violated are (0) flow dependences between statements, and textual in terchange will be required to make the dependence legal [KS94]. Disregarding the loop bounds for the moment, consider again our running example. Figure 7a shows the transformed loop body.
Reference: [Ram92] <author> J. Ramanujam. </author> <title> Non-singular transformations of nested loops. </title> <booktitle> In Supercomputing 92, </booktitle> <pages> pages 214-223, </pages> <year> 1992. </year>
Reference-contexts: In this framework, a transformation or a sequence thereof is represented by a single non-singular matrix, which can be used to directly derive the transformed loop bounds, references and dependences using well known techniques <ref> [Ban90, WL90, KKB92, LP92, Ram92, AI91, CFR93] </ref>. <p> can be represented by an m fi (n + 1) reference matrix, 2 rm (r), and the loop bounds can be represented by an l fi (n + 1) bounds matrix, 3 fi (L), where l is the number of expressions in the upper and lower bounds of the loop <ref> [WL90, LP92, Ram92] </ref>. Although CDA applies to both perfect and imperfect loop nests [KS94], we restrict ourselves in this paper to computations at the same nesting level. Moreover, for simplicity, we assume that the loop body contains only assignment statements.
Reference: [Sim94] <author> B. Simmons. </author> <type> Personal communication. </type> <year> 1994. </year>
Reference: [TALV93] <author> J. Torres, E. Ayguade, J. Labarta, and M. Valero. </author> <title> Align and distribute-based linear loop transformations. </title> <booktitle> In Proceedings of Sixth Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <year> 1993. </year>
Reference-contexts: Several researchers independently recognized the advantages of such transformations over the last two years. For example, Kelly and Pugh [KP92] proposed a framework to include separate schedules for each statement. Torres and Aygude <ref> [TALV93] </ref> integrated loop alignment with linear transformations and applied it to improve the efficiency of SPMD code. <p> Kelly et al. also developed an algorithm to generate code for general linear transformations but with conditionals [KPR94]. These algorithms reduce to the algorithm developed by Torres et al. when the transformations are simple offsets corresponding to loop alignments <ref> [TALV93] </ref>. We illustrate a typical way to generate guard free code with the aid of Figure 8a. The full details can be found in [KS94]. <p> One way to eliminate ownership test is to ensure that all SEIs of an iteration are to be executed by the same processor. With owner-computes rule, this is achieved by collocating the lhs references of all statements of the loop body <ref> [TALV93, KS94] </ref>. Even then, it may be more efficient to execute some subexpressions on another processor, say because it owns most of the data required for the computation. CDA can effect this modification to the owner-computes rule.
Reference: [WL90] <author> M.E. Wolf and M.S. Lam. </author> <title> An algorithmic approach to compound loop transformation. </title> <booktitle> In Proceedings of Third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Being able to align computations at arbitrary granularities adds a new dimension to performance optimization on high performance computing platforms, thus extending significantly the existing frameworks both in power and flexibility. CDA has its roots in, and is a generalization of linear loop transformations <ref> [Ban90, WL90, Dow90, KKB91, LP92] </ref>. Most of the loop transformations proposed to date [PW86, Wol90, Ban93, Ban94] can be viewed as special instances of linear transformations. <p> In this framework, a transformation or a sequence thereof is represented by a single non-singular matrix, which can be used to directly derive the transformed loop bounds, references and dependences using well known techniques <ref> [Ban90, WL90, KKB92, LP92, Ram92, AI91, CFR93] </ref>. <p> can be represented by an m fi (n + 1) reference matrix, 2 rm (r), and the loop bounds can be represented by an l fi (n + 1) bounds matrix, 3 fi (L), where l is the number of expressions in the upper and lower bounds of the loop <ref> [WL90, LP92, Ram92] </ref>. Although CDA applies to both perfect and imperfect loop nests [KS94], we restrict ourselves in this paper to computations at the same nesting level. Moreover, for simplicity, we assume that the loop body contains only assignment statements.
Reference: [Wol90] <author> Michael Wolfe. </author> <title> Optimizing supercompilers for supercomputer. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: CDA has its roots in, and is a generalization of linear loop transformations [Ban90, WL90, Dow90, KKB91, LP92]. Most of the loop transformations proposed to date <ref> [PW86, Wol90, Ban93, Ban94] </ref> can be viewed as special instances of linear transformations.
Reference: [Wol92] <author> M.E. Wolf. </author> <title> Improving locality and parallelism in nested loops. </title> <type> Phd thesis, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: a single matrix to represent a compound transformation of many existing transformations, ii) it allowed the development of a set of generic techniques that could transform loops in a systematic way, independent of the particular sequence of transformations being applied, and iii) it made possible the quantification of optimization objectives <ref> [LP92, AL93, Wol92, KKB92, KKB92] </ref>. A linear loop transformation does not change the makeup of an iteration as it maps one iteration space (IS) onto another; each iteration is mapped from a point in the original IS to a point in the new IS in its entirety (unchanged).
References-found: 44

