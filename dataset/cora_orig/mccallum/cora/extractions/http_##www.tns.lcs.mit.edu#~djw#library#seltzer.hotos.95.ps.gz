URL: http://www.tns.lcs.mit.edu/~djw/library/seltzer.hotos.95.ps.gz
Refering-URL: http://www.tns.lcs.mit.edu/~djw/library/
Root-URL: 
Title: The Case for Geographical Push-Caching  
Author: James S. Gwertzman Margo Seltzer 
Address: Cambridge, MA 02138 Cambridge, MA 02138  
Affiliation: Division of Applied Sciences Division of Applied Sciences Harvard University Harvard University  
Abstract: Most wide-area caching schemes are client initiated. Decisions on when and where to cache information are made without the benefit of the server's global knowledge of the situation. We believe that the server should play a role in making these caching decisions, and we propose geographical push-caching as a way of bringing the server back into the loop. The World Wide Web is an excellent example of a wide-area system that will benefit from geographical push-caching, and we present an architecture that allows a Web server to autonomously replicate HTML pages. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Berners-Lee, R. Cailliau, J-F. Groff, and B. Pollermann. </author> <title> World-wide web: The information universe. </title> <journal> Electronic Networking Research, Applications and Policy, </journal> <volume> 2(1) </volume> <pages> 52-58, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction The World-Wide Web <ref> [1] </ref> operates for the most part as a cache-less distributed system. When two neighboring clients retrieve a document from the same server, the document is sent twice. This is inefficient, especially considering the ease with which Web browsers allow users to transfer large multimedia documents.
Reference: [2] <author> Azer Bestavros. </author> <title> Demand-based document dissemination for the world-wide web. </title> <type> Technical Report 95-003, </type> <institution> Boston University, </institution> <year> 1995. </year>
Reference: [3] <author> Matthew A. </author> <title> Blaze. Caching in large-scale distributed file systems. </title> <type> Technical Report TR-397-92, </type> <institution> Princeton University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: As for resource location, there are several groups working on this problem. Until this problem is solved we are using a technique proposed by Blaze <ref> [3] </ref>, which we call the "1-800 technique". Clients "call" the primary server to ask for the "server nearest you." This is not elegant, but it works because latency is currently more critical than bandwidth. <p> Their object caching subsystem provides a hierarchically organized means for efficiently retrieving Internet objects such as FTP and HTML files. The group at the University of Colorado at Boulder has also examined the resource location problem [7] and the consistency problem [13]. Blaze <ref> [3] </ref> has addressed caching in a large-scale system. His research focused on distributed file systems, but can be applied to FTP or the Web. Finally, the Alex system [5] was designed to provide a means of caching FTP files.
Reference: [4] <author> C. Mic Bowman, Peter B. Danzig, Darren R. Hardy, Udi Manber, and Mich ael F. Schwartz. Harvest: </author> <title> A scalable, customizable discovery and access system. </title> <type> Technical Report CU-CS-732-94, </type> <institution> University of Colorado, Boulder, </institution> <year> 1994. </year>
Reference-contexts: Several groups are working on similar problems, but most are focused on client-side caching. Only one group that we know of, at Boston University, is also working on server-initiated caching.[2] One such client-side system is included in The Harvest system <ref> [4] </ref>. Their object caching subsystem provides a hierarchically organized means for efficiently retrieving Internet objects such as FTP and HTML files. The group at the University of Colorado at Boulder has also examined the resource location problem [7] and the consistency problem [13].
Reference: [5] <author> Vincent Cate. </author> <title> Alex A global filesystem. </title> <booktitle> In USENIX File Systems Workshop Proceedings, </booktitle> <pages> pages 1-12, </pages> <address> Ann Arbor, MI, May 21 - 22 1992. </address> <publisher> USENIX. </publisher>
Reference-contexts: Since weak-consistency should be acceptable for the Web, we are using a scheme developed for the Alex <ref> [5] </ref> file system. With the exception of dynamic pages which can not be cached we expect the Web to obey the same principle as FTP: the older a file is, the less likely it is to be modified. <p> Blaze [3] has addressed caching in a large-scale system. His research focused on distributed file systems, but can be applied to FTP or the Web. Finally, the Alex system <ref> [5] </ref> was designed to provide a means of caching FTP files. Of these three systems, Blaze's design comes closest to our own since it supports replication when demand becomes too high, and because it lets clients use any nearby cache.
Reference: [6] <author> Alan Emtage and Peter Deutsch. </author> <title> Archie an electronic directory service for the internet. </title> <booktitle> In Proceedings of the USENIX Winter Conference. USENIX, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: We expect our results to be applicable to any wide-area distributed system, however; not just the World Wide Web. One application for geographical push-caching that we have in mind is to replicate not only data files but also services themselves. A good example would be Archie <ref> [6] </ref>, whose load problems are notorious. If Archie were to be written in a machine-independent network-service scripting lan guage (e.g. Tcl [11]), its code could be replicated and cached just like a Web file.
Reference: [7] <author> James D. Guyton and Michael F. Schwartz. </author> <title> Locating nearby copies of replicated internet servers. </title> <type> Technical Report CU-CS-762-95, </type> <institution> University of Colorado at Boulder, </institution> <year> 1995. </year>
Reference-contexts: Their object caching subsystem provides a hierarchically organized means for efficiently retrieving Internet objects such as FTP and HTML files. The group at the University of Colorado at Boulder has also examined the resource location problem <ref> [7] </ref> and the consistency problem [13]. Blaze [3] has addressed caching in a large-scale system. His research focused on distributed file systems, but can be applied to FTP or the Web. Finally, the Alex system [5] was designed to provide a means of caching FTP files.
Reference: [8] <author> Ari Luotonen and Kevin Altis. </author> <title> World-wide web proxies. </title> <booktitle> In Computer Networks and ISDN systems. First International Conference on the World-Wide Web, </booktitle> <publisher> Elsevier Science BV, </publisher> <year> 1994. </year> <note> available from 'http://www.cern.ch/ Pa-persWWW94/ luotonen.ps'. </note>
Reference-contexts: To combat this problem, some Web browsers have begun to add local client caches. These prevent one client from transferring the same document twice. Some networks are also beginning to add Web proxies <ref> [8, 9] </ref> that prevent two clients on the same campus network from transferring the same document twice. The problem with both these schemes is that they are myopic. A client cache does not help a neighboring computer, and a campus proxy does not help a neighboring campus. <p> The modified server is responsible for tracking geographical access information for its files and for accepting and offering cached replicas of other files. This can be done by modifying a proxy server, such as the CERN proxy server <ref> [8] </ref>, to accept files for replication using a modified POST request. The replication service keeps track of modified HTTP servers that are willing to serve replicated files, the amount of available free space on each server, and each server's average load.
Reference: [9] <author> Mosaic-x@ncsa.uiuc.edu. </author> <title> Using proxy gateways. World-Wide Web. </title> <note> available from 'http: //www.ncsa.uiuc.edu/SDG/Software/Mosaic/ Docs/proxy-gateways.html'. [10] nic.merit.edu. ftp://nic.merit.edu/nsfnet/ announced.networks/nets.unl.now. </note>
Reference-contexts: To combat this problem, some Web browsers have begun to add local client caches. These prevent one client from transferring the same document twice. Some networks are also beginning to add Web proxies <ref> [8, 9] </ref> that prevent two clients on the same campus network from transferring the same document twice. The problem with both these schemes is that they are myopic. A client cache does not help a neighboring computer, and a campus proxy does not help a neighboring campus.
Reference: [11] <author> John K. Ousterhout. </author> <title> Tcl: An embeddable com-mand language. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 133-146, </pages> <address> Washington, D.C., </address> <month> January 22-26 </month> <year> 1990. </year> <booktitle> USENIX. </booktitle>
Reference-contexts: One application for geographical push-caching that we have in mind is to replicate not only data files but also services themselves. A good example would be Archie [6], whose load problems are notorious. If Archie were to be written in a machine-independent network-service scripting lan guage (e.g. Tcl <ref> [11] </ref>), its code could be replicated and cached just like a Web file.
Reference: [12] <author> Nicolas Pioch. Le weblouvre. </author> <note> WorldWide Web. http: //mistral.enst.fr/ pi-och/louvre/louvre.shtml. </note>
Reference-contexts: Likewise, server-initiated caching can not cope very well with sudden, localized jumps in popularity, but is best suited to handling long-term file request trends. We close with this reminder of why server-initiated caching is necessary, taken from the Web home page of the WebLouvre <ref> [12] </ref>. Note: Starting end of October 1994, we are currently experiencing severe network problems on our 256 Kb school Internet connection. Please be understanding! I am still looking for a site willing to mirror the We-bLouvre exhibit (30 Mb in all), preferably in the USA.
Reference: [13] <author> Kurt Jeffery Worrell. </author> <title> Invalidation in Large Scale Network Object Caches. </title> <type> PhD thesis, </type> <institution> University of Colorado-Boulder, </institution> <year> 1994. </year>
Reference-contexts: Their object caching subsystem provides a hierarchically organized means for efficiently retrieving Internet objects such as FTP and HTML files. The group at the University of Colorado at Boulder has also examined the resource location problem [7] and the consistency problem <ref> [13] </ref>. Blaze [3] has addressed caching in a large-scale system. His research focused on distributed file systems, but can be applied to FTP or the Web. Finally, the Alex system [5] was designed to provide a means of caching FTP files.
References-found: 12

