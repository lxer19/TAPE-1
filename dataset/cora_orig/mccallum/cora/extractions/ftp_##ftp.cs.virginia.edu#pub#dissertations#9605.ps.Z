URL: ftp://ftp.cs.virginia.edu/pub/dissertations/9605.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/dissertations/README.html
Root-URL: http://www.cs.virginia.edu
Title: Maintaining Retrieval Effectiveness in Distributed, Dynamic Information Retrieval Systems  
Author: Charles L. Viles 
Degree: A Dissertation Presented to the Faculty of the  In Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy Computer Science by  
Date: May 1996  
Affiliation: School of Engineering and Applied Science University of Virginia  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> I. J. Aalbersberg. </author> <title> Posting Compression in Dynamic Retrieval Environments. </title> <booktitle> In Proceedings of the 14th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 72-81, </pages> <address> Chicago, IL, </address> <year> 1991. </year>
Reference-contexts: The former work does not appear to use term weights in their indexes. The latter work concentrates on fast and effective query processing and assumes a static database. We are specifically interested in ad-hoc search in a dynamic database using corpus-wide derived term weights. In <ref> [1] </ref> and [2], inverse document frequency (idf ) information is kept separate from the document term weights and is applied only to query terms, thus avoiding recalculation of term weights whenever new documents were inserted.
Reference: [2] <author> I. J. Aalbersberg and Frans Sijstermans. </author> <title> High-Quality and High-Performance Full-text Document Retrieval: </title> <booktitle> the Parallel InfoGuide System. In Proceedings First International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 142-150, </pages> <address> Miami Beach, FL, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: It includes efficient mechanisms for gathering and indexing topic-specific information at a central location. Mechanisms for caching and replicating the indexes are provided. Harvest concentrates on making efficient use of network resources. Effectiveness considerations are secondary. In the Parallel InfoGuide system <ref> [2] </ref>, Aalbersberg and Sijstermans use a distributed-memory multi-processor to get very fast query response times. They use the Vector Space Model [75] as the IR engine. <p> The former work does not appear to use term weights in their indexes. The latter work concentrates on fast and effective query processing and assumes a static database. We are specifically interested in ad-hoc search in a dynamic database using corpus-wide derived term weights. In [1] and <ref> [2] </ref>, inverse document frequency (idf ) information is kept separate from the document term weights and is applied only to query terms, thus avoiding recalculation of term weights whenever new documents were inserted.
Reference: [3] <author> Peter G. Anick and Rex A. Flynn. </author> <title> Integrating a Dynamic Lexicon with a Dynamic Full-Text Retrieval System. </title> <booktitle> In Proceedings of the 16th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 136-145, </pages> <address> Pittsburgh, PA, </address> <year> 1993. </year>
Reference-contexts: We also consider four different collections and a variety of update rates. Several systems have made a conscious decision to keep structures derived from the entire corpus separate from index structures. In AI-STARS <ref> [3] </ref>, in order to support the addition of new documents and terms, linguistic processing is functionally isolated from article indexing. Updates to the lexicon and the indexes can proceed asynchronously. In [67], document vector lengths and document frequency information are kept separately from the inverted index.
Reference: [4] <author> Brian Bartell, Garrison Cottrell, and Richard K. Belew. </author> <title> Automatic Combination of Multiple Ranked Retrieval Systems. </title> <booktitle> In Proceedings of the 17th Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 173-181, </pages> <address> Dublin, Ireland, </address> <year> 1994. </year>
Reference-contexts: Belkin et al. [6] define two aspects. Query combination involves submitting different forms 2.3. Related Work:Dynamism 22 of queries to systems managing the same documents. Several studies have shown that combining the results of these queries can yield higher quality result lists than any of the constituent lists <ref> [4, 5, 31] </ref>. The second aspect in combination of evidence is called data fusion or collection fusion where results from the same query are run on disjunct archives and then combined. This distributed query processing is of most relevance to our work.
Reference: [5] <author> Nicholas J. Belkin, C. Cool, W. Bruce Croft, and James P. Callan. </author> <title> The Effect of Multiple Query Representations on Information Retrieval System Performance. </title> <booktitle> In Proceedings of the 16th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 339-346, </pages> <address> Pittsburgh, PA, </address> <year> 1993. </year> <note> 135 Bibliography136 </note>
Reference-contexts: Belkin et al. [6] define two aspects. Query combination involves submitting different forms 2.3. Related Work:Dynamism 22 of queries to systems managing the same documents. Several studies have shown that combining the results of these queries can yield higher quality result lists than any of the constituent lists <ref> [4, 5, 31] </ref>. The second aspect in combination of evidence is called data fusion or collection fusion where results from the same query are run on disjunct archives and then combined. This distributed query processing is of most relevance to our work.
Reference: [6] <author> Nicholas J. Belkin, Paul Kantor, Edward A. Fox, and J. A. Shaw. </author> <title> Combining the Evidence of Multiple Query Representations for Information Retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 31(4) </volume> <pages> 431-448, </pages> <year> 1995. </year>
Reference-contexts: The efficacy of GlOSS remains undetermined for larger more topically homogeneous environments. The combination of evidence from multiple sources or runs is an area of active research. Belkin et al. <ref> [6] </ref> define two aspects. Query combination involves submitting different forms 2.3. Related Work:Dynamism 22 of queries to systems managing the same documents. Several studies have shown that combining the results of these queries can yield higher quality result lists than any of the constituent lists [4, 5, 31]. <p> There is also considerable work occurring in methods to intelligently merge result lists from separate searches into a single result. The issues and several approaches are well-described in Vorhees [96] and Belkin et al. <ref> [6] </ref>. An early study by Fox et al. [31] assumed no inter-site communication and the use of local information to build the idf . Sites were 4.2. Collection Wide Information 35 identified by breaking down the TREC collection by document source.
Reference: [7] <author> Tim Berners-Lee, Robert Calliau, Ari Luotonen, Henrik Neilsen, and Arthur Secret. </author> <title> The World Wide Web. </title> <journal> Communications of the ACM, </journal> <volume> 37(8) </volume> <pages> 76-82, </pages> <year> 1994. </year>
Reference-contexts: It is unclear how these content labels are organized for browse purposes and whether the content hierarchy is manually or automatically maintained. With the explosion of the World Wide Web <ref> [7] </ref>, the amount of "Information Retrieval" research going on in the WWW venue is difficult to track and impossible to summarize. The best source for technical and substantive research efforts are the International World Wide Web Conferences, held twice annually.
Reference: [8] <author> C. Mic Bowman, Peter B. Danzig, Darren Hardy, Udi Manber, and Michael F. Schwartz. Harvest: </author> <title> A Scalable, Customizable Discovery and Access System. </title> <type> Technical Report CU-CS-732-94, </type> <institution> University of Colorado, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: These search services employ automatons (also called "robots", "spiders", or "agents") that attempt to intelligently walk web-space, downloading documents for indexing. The search itself occurs at a single location, but result lists generally contain hypertext links pointing to the original location of the document. The Harvest <ref> [8, 9] </ref> System provides a set of software tools and abstractions to enable collecting, indexing, and access to distributed Internet-based information.
Reference: [9] <author> C. Mic Bowman, Peter B. Danzig, Darren Hardy, Udi Manber, and Michael F. Schwartz. </author> <title> The Harvest Information Discovery and Access System. </title> <booktitle> In 2nd International World Wide Web Conference, </booktitle> <pages> pages 763-771, </pages> <address> Chicago, Illinois, </address> <year> 1994. </year>
Reference-contexts: These search services employ automatons (also called "robots", "spiders", or "agents") that attempt to intelligently walk web-space, downloading documents for indexing. The search itself occurs at a single location, but result lists generally contain hypertext links pointing to the original location of the document. The Harvest <ref> [8, 9] </ref> System provides a set of software tools and abstractions to enable collecting, indexing, and access to distributed Internet-based information. <p> In particular, NCSTRL does distributed search and maintains distributed document collections. NCSTRL is the latest in a series of efforts to make computer science technical reports more accessible over the Internet. Chief among these efforts were the Unified Computer Science Technical Report Index (UCSTRI)[90] at Indiana University, Harvest <ref> [9] </ref>, the CS-TR project which developed Dienst [22, 54], the WATERS (Wide Area TEchnical Report Service) project [34, 35], and the precursor to WATERS, techrep [32]. NCSTRL is the result of a combined effort of the CS-TR and WATERS groups. <p> Collection-wide information was assumed to be available. In our work, we do not consider end-to-end issues, but concentrate specifically on the level at which CWI must be maintained to ensure reasonable search quality. Harvest <ref> [9, 10] </ref> is a prototype system designed to address some of the problems in resource discovery and information access on the Internet. It includes efficient mechanisms for gathering and indexing topic-specific information at a central location. Mechanisms for caching and replicating the indexes are provided.
Reference: [10] <author> C. Mic Bowman, Peter B. Danzig, Udi Manber, and Michael F. Schwartz. </author> <title> Scalable Internet Resource Discovery: Research Problems and Approaches. </title> <journal> Communications of the ACM, </journal> <volume> 37(8) </volume> <pages> 98-107, </pages> <year> 1994. </year>
Reference-contexts: Collection-wide information was assumed to be available. In our work, we do not consider end-to-end issues, but concentrate specifically on the level at which CWI must be maintained to ensure reasonable search quality. Harvest <ref> [9, 10] </ref> is a prototype system designed to address some of the problems in resource discovery and information access on the Internet. It includes efficient mechanisms for gathering and indexing topic-specific information at a central location. Mechanisms for caching and replicating the indexes are provided.
Reference: [11] <author> Eric W. Brown, James P. Callan, and W. Bruce Croft. </author> <title> Fast Incremental Indexing for Full-Text Information Retrieval. </title> <booktitle> In Proceedings of the 20th VLDB Conference, </booktitle> <pages> pages 192-202, </pages> <address> Santiago, Chile, </address> <year> 1994. </year>
Reference-contexts: In the face of updates, it is impossible to predict which lists will out-grow their initially allocated space (causing expensive relocation) and which lists will never grow at all (wasting most of the initially allocated space). Brown et al. <ref> [11] </ref> use a persistent object management system that provides efficient update of disk-resident objects, and implement the inverted index using this abstraction. Tomasic and Garcia-Molina [88] maintain two kinds of lists. "Short" lists are kept in memory, and "long" lists are kept on disk. <p> Often, when new documents need to be added to a collection, the entire collection is re-indexed. This is clearly undesirable, since the cost of an update is proportional to the size of the database not the size of the update. Several groups <ref> [11, 88, 99] </ref> have proposed schemes for the efficient, incremental update of disk-resident indexes in IR systems. In all of these schemes, the underlying IR engine is either boolean or the vector space model (VSM) [75] with simple term frequency 63 64 (tf) based term weights. <p> Zobel et al. [99] improve inverted file access through compression techniques. Tomasic et al. [88] keep short inverted lists in a fixed sized memory, and only move the longest "short" list to disk when memory runs out. Brown et al. <ref> [11] </ref> implemented the inverted index on top of a generic persistent object management system. The run-time system of the object store provides efficient update of its objects, in this case the inverted lists. None of these systems specifically address the question of updating existing postings 5.1. <p> Still, for large collections, updating on the granularity of individual documents is unrealistic. The equation above ignores disk operations associated with relocating lists that have outgrown the space allocated for them, so even if an incremental update facility performs this chore efficiently <ref> [11, 88] </ref> the I/O cost is large. 5.2.
Reference: [12] <author> Chris Buckley. </author> <note> SMART version 11.0, 1992. ftp://ftp.cs.cornell.edu/pub/smart. </note>
Reference-contexts: A typical experiment consists of setting values for a particular configuration, running Drift using the specified test collection, and evaluating the result lists for each query using a standard evaluation package called trec eval <ref> [12] </ref>. Because there is a stochastic element to the assignment of documents to collections, each configuration is run multiple times, and the results of the evaluations are averaged. For all experiments with all test collections, we fixed the number of sites at 20.
Reference: [13] <author> Chris Buckley, Amit Singhal, Mandar Mitra, and Gerard Salton. </author> <title> New Retrieval Approaches Using SMART: </title> <booktitle> TREC 4. In Proceedings of the Fourth Text Retrieval Conference (TREC-4), </booktitle> <address> Gaithersburg, MD, </address> <year> 1995. </year>
Reference-contexts: That is, longer documents are more likely to be relevant to queries than shorter 2.1. Information Retrieval Background 15 documents. A more appropriate normalization, at least for TREC, is to do a document length based normalization <ref> [13, 81, 82] </ref>. The idf is an important example of collection wide information (CWI), information derived from the entire collection of documents.
Reference: [14] <author> James P. Callan, W. Bruce Croft, and John Broglio. </author> <title> TREC and TIPSTER Experiments with INQUERY. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 31(3) </volume> <pages> 327-344, </pages> <year> 1995. </year> <month> Bibliography137 </month>
Reference-contexts: Sites must agree on the identity of the k-th term. 4.2.1 Using CWI How the idf is actually used varies greatly from system to system. For example, in systems like INQUERY <ref> [14] </ref> that are based on the probabilistic IR model [89], the idf is used to 4.2. Collection Wide Information 36 adjust the probability that a document is relevant given that it does or does not contain terms in common with the query.
Reference: [15] <author> James P. Callan, Zhihong Lu, and W. Bruce Croft. </author> <title> Searching Distributed Collections with Inference Networks. </title> <booktitle> In Proceedings of the 18th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 21-29, </pages> <address> Seattle, WA, </address> <year> 1995. </year>
Reference-contexts: The second aspect in combination of evidence is called data fusion or collection fusion where results from the same query are run on disjunct archives and then combined. This distributed query processing is of most relevance to our work. Callan <ref> [15] </ref> adapted the inference net approach developed for document retrieval to collection identification. CWI is assumed to be readily available. <p> Work 33 In this chapter we document this investigation, paying particular attention to the methods we use to model dissemination and content-based document allocation, and to the differences in results between our early, small collection experiments and later experiments with large subsets of the TREC data. 4.1 Related Work Callan <ref> [15] </ref> adapted the inference net approach used in the INQUERY system to the identification of collections likely to contain relevant documents. A search query was then broadcast to these collections, searches performed and results merged. <p> In dynamic applications like filtering and routing [33, 55], completely up-to-date CWI may not be needed, so re-calculation of CWI need be done only intermittently. Many resource discovery systems (e.g., Callan et al. <ref> [15] </ref> or Gravano et al. [38]) use CWI to select a small number of collections to send queries to. Our work indicates that this CWI may drift considerably without unduly harming search quality. 4.8. Summary 61 Mean At Home (Q) Avg. <p> Patents (1993). Several sources cover multiple years or time periods. A natural way to consider the TREC data as a distributed collection is to make each source and year a site. This is the method that is used in work reported by Callan et al. <ref> [15] </ref> and Vorhees et al. [96] on the "Collection Fusion" problem. 4 This set of candidate sites is described in Table 6.4. 6.5.2 Topic Identification As in our Drift experiments, we used the set of topics provided with the TREC collection and the set of accompanying relevant documents to identify the
Reference: [16] <author> A. F. Cardenas. </author> <title> Analysis and Performance of Inverted Data Base Structures. </title> <journal> Communications of the ACM, </journal> <volume> 18(5) </volume> <pages> 253-263, </pages> <year> 1975. </year>
Reference-contexts: Several algorithms to generate it are given in Harman et al. [44]. Due to its importance, there has been a considerable amount of attention given to analyzing and optimizing its performance (see <ref> [16, 83, 99] </ref> for just a few of many papers in this area). 2.1.5 Effectiveness Experimentation in IR Because measurements of effectiveness are necessarily based on human judgements of relevance, the evaluation process is inherently subjective. <p> Number of Affected Postings 73 5.2.3 Expected I/O Activity To examine the expected disk activity for the problematic term weighting schemes, we consider the CWI-using, unnormalized (category f x) term weights. The following analysis can be considered "back of the envelope", check <ref> [16] </ref> for more sophisticated analyses of access to disk-resident inverted structures. By Equation 5.2, a total of M 2 N K terms are affected by the addition of a single document.
Reference: [17] <author> Charles L. A. Clarke and Gordon V. Cormack. </author> <title> Dynamic Inverted Indexes for a Distributed Full-Text Retrieval System. </title> <type> Technical Report MT-95-01, </type> <institution> Department of Computer Science, University of Waterloo, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: Tomasic and Garcia-Molina [88] maintain two kinds of lists. "Short" lists are kept in memory, and "long" lists are kept on disk. Upon update, the longest short lists are moved to disk when memory exceeds some threshold. An alternative approach promoted by Clarke and Cormack <ref> [17] </ref> is to continuously re-index the document corpus, adding in new documents each time. These solutions all assume that existing postings in the index do not need to be changed in any way. <p> These researchers still use idf for query weights, so the problem of update of the idf still exists. Clarke and Cormack <ref> [17] </ref> take a different approach to the incremental update problem. The idea is to maintain a continuous cyclic update process that essentially rewrites the entire index continuously, incorporating any new documents since the last cycle. At any one time there is an old and a new index.
Reference: [18] <author> W. J. Conover. </author> <title> Practical Nonparametric Statistics. </title> <publisher> John Wiley and Sons Inc., </publisher> <year> 1971. </year>
Reference-contexts: Since the random variable D from which the differences are drawn may not be normally distributed, a non-parametric (distribution-free) statistical test is in order. The one we use here is the Wilcoxon Signed Ranks Test (WSRT). A complete description of WSRT is available in Conover <ref> [18] </ref> a synopsis appears here. The general notion is to order the n observations by their absolute value and assign ranks 1; 2; : : : ; n accordingly.
Reference: [19] <author> Douglas Cutting, David Karger, Jan Pedersen, and John Tukey. Scatter/Gather: </author> <title> A Cluster-based Approach to Browsing Large Document Collections. </title> <booktitle> In Proceedings of the 15th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 318-329, </pages> <address> Copenhagen, Denmark, </address> <year> 1992. </year>
Reference-contexts: Topic-based 97 * Topic Tracking looking for topics that are new, "hot", or have very little or very much activity; * Efficiency caching documents in the same topic together or nearby; * Intelligent Browsing in interactive systems, a topical map can provide a set of related starting points for browsing <ref> [19, 53] </ref>; and * Pre-Fetching the fetching of one or more documents in the same topic, might signal the system to start pre-fetching other documents in the topic. 6.2 Skew Definition: Topic-based The notation we use in this chapter is given in Table 6.1. b s;t n t , proportional size
Reference: [20] <author> Douglas Cutting and Jan Pedersen. </author> <title> Optimizations for Dynamic Inverted Index Maintenance. </title> <booktitle> In Proceedings of the 13th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 405-411, </pages> <address> Brussels, </address> <year> 1990. </year>
Reference-contexts: In our work, we are concerned with the support of effective ad-hoc search in a dynamic environment and not filtering of a document stream. There is a good deal of recent work looking at efficient maintenance and access to inverted files in the face of updates. Cutting and Pedersen <ref> [20] </ref> present optimizations to B-tree based inverted files. Zobel et al. [99] improve inverted file access through compression techniques. Tomasic et al. [88] keep short inverted lists in a fixed sized memory, and only move the longest "short" list to disk when memory runs out.
Reference: [21] <author> James R. Davis. </author> <title> Creating a Networked Computer Science Technical Report Library. </title> <journal> D-Lib Magazine, </journal> <month> September </month> <year> 1995. </year> <note> http://www.dlib.org/dlib/september95/. </note>
Reference-contexts: We address the degree of inconsistency or drift that is permissible later in this thesis. 3.2. Scenarios 28 An Example System The Networked Computer Science Technical Report Library (NCSTRL) <ref> [21] </ref> is a distributed collection of CS technical reports that fits the scenario we have just described. In particular, NCSTRL does distributed search and maintains distributed document collections. NCSTRL is the latest in a series of efforts to make computer science technical reports more accessible over the Internet. <p> the skew of the parametrically constructed collections of Chapter 4 using 94 95 both skew metrics, * measuring the skew of the multi-year, multi-source TREC collection using the topically based metric, and * measuring the skew of an operational distributed document collection, the Networked Computer Science Technical Report Library (NCSTRL) <ref> [21] </ref> using the statistically based metric. The primary goal of this chapter is to begin to better understand the nature of content-skew and how it may arise in distributed document collections.
Reference: [22] <author> James R. Davis and Carl Lagoze. </author> <title> Drop-in Publishing with the World-Wide Web. </title> <booktitle> In Second International World Wide Web Conference, </booktitle> <pages> pages 749-758, </pages> <address> Chicago, Illinois, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: NCSTRL is the latest in a series of efforts to make computer science technical reports more accessible over the Internet. Chief among these efforts were the Unified Computer Science Technical Report Index (UCSTRI)[90] at Indiana University, Harvest [9], the CS-TR project which developed Dienst <ref> [22, 54] </ref>, the WATERS (Wide Area TEchnical Report Service) project [34, 35], and the precursor to WATERS, techrep [32]. NCSTRL is the result of a combined effort of the CS-TR and WATERS groups. The architecture of NCSTRL document and index repositories is depicted in Figure 3.2.
Reference: [23] <author> Andrzej Duda and Mark A. Sheldon. </author> <title> Content Routing in a Network of WAIS servers. </title> <booktitle> In Proceedings of the 14th IEEE International Conference on Distributed Computing Systems, </booktitle> <address> Poznan, Poland, </address> <month> June </month> <year> 1994. </year> <month> Bibliography138 </month>
Reference-contexts: The query could then be sent to any subset of these databases for searching there. Term weighting methods and results merging were simple but somewhat difficult to justify. 2.2. Related Work: Distribution 19 Sheldon, Duda, et al. <ref> [79, 23] </ref> present content routing as a resource discovery system. They use the short content descriptions of almost 500 WAIS servers to build content labels for each site. Each label is supplemented by short document descriptions that are called "headlines" in WAIS terminology.
Reference: [24] <author> David Ellis. </author> <title> The Dilemma of Measurement in Information Retrieval Research. </title> <journal> Journal of the American Society of Information Science, </journal> <volume> 47(1) </volume> <pages> 23-36, </pages> <year> 1996. </year>
Reference-contexts: Raghavan et al. [69] examine recall and precision in particular and identify situations leading to ambiguous results. Tague-Sutcliffe and Blustein [84] look specifically at evaluation in the Text Retrieval Conferences [43]. Harter [46] and Ellis <ref> [24] </ref> are the latest in a long-line of researchers who have questioned the wisdom of using permanent one-time relevance judgements in some situations. Despite the lively and continued debate over evaluation in IR, the vast majority of IR researchers still use the time-honored recall and precision measures.
Reference: [25] <author> Alan Emtage and Peter Deutsch. </author> <title> Archie An Electronic Directory Service for the Internet. </title> <booktitle> In Proceedings of the Winter Usenix Conference, </booktitle> <pages> pages 93-110, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: These include "white-page" services like netfind, whois, X.500 and Nomenclator, coarse-grained search services like Archie, browsing technologies represented by Gopher, World Wide Web, and Hyper-G, and full-text searching of distributed document archives using WAIS. We describe some of these efforts below. The Internet Archie <ref> [25] </ref> is a replicated index of thousands of ftp archives. It has proved especially useful for finding public domain source code of all kinds. Periodically, the archie indexing engine down-loads and indexes directory listings. Search is then conducted on a single, central index of these directory listings.
Reference: [26] <author> Christos Faloutsos. </author> <title> Access methods for text. </title> <journal> Computing Surveys, </journal> <volume> 17(1) </volume> <pages> 49-74, </pages> <month> March </month> <year> 1985. </year>
Reference-contexts: We provide an example inverted index for a "toy" document collection in From an efficiency standpoint, the inverted index is one of the most important data structures in text retrieval systems <ref> [26] </ref>. Several algorithms to generate it are given in Harman et al. [44].
Reference: [27] <author> M. J. Fischer and A. Michael. </author> <title> Sacrificing Serializability to Attain High Availability of Data in an Unreliable Network. </title> <booktitle> In Proceedings of the ACM Symposium on Principles of Database Systems, </booktitle> <pages> pages 70-75, </pages> <year> 1982. </year>
Reference-contexts: Contributions 7 The document collections we use are standard for effectiveness-based IR research and experimentation. We give more detail on these collections in Chapter 2. 1.3 Contributions Though the notion of tolerating incomplete knowledge or inconsistency in globally shared data has been around for some time <ref> [27] </ref>, only recently has the case been made that more relaxed notions of consistency are needed in the Internet environment [66].
Reference: [28] <author> David Flater and Yelena Yesha. </author> <title> Towards Flexible Distributed Information Retrieval. </title> <editor> In Nabil R. Adam and Bharat K Bhargava, editors, </editor> <booktitle> Lecture Notes in Computer Science 759: Advanced Database Systems. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: They assume very slow line speeds (9600 baud) connecting the central search and document server to clients, thus making interpretation of their results difficult in systems with orders of magnitude greater bandwidth. In ALIBI (Adaptive Location of Internetworked Bases of Information), <ref> [28, 29] </ref>, Flater and Yesha provide a non-hierarchical approach to resource discovery. A virtual, unparti-tioned network of sites is formed.
Reference: [29] <author> David Flater and Yelena Yesha. ALIBI: </author> <title> A Novel Approach to Resource Discovery. </title> <journal> Journal of Internet Research, </journal> <note> To Appear, </note> <year> 1996. </year>
Reference-contexts: They assume very slow line speeds (9600 baud) connecting the central search and document server to clients, thus making interpretation of their results difficult in systems with orders of magnitude greater bandwidth. In ALIBI (Adaptive Location of Internetworked Bases of Information), <ref> [28, 29] </ref>, Flater and Yesha provide a non-hierarchical approach to resource discovery. A virtual, unparti-tioned network of sites is formed.
Reference: [30] <author> Edward A. Fox. </author> <title> Characterization of Two New Experimental Collections in Computer and Information Science Containing Textual and Bibliographic Concepts. </title> <type> Technical Report TR83-561, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1983. </year>
Reference-contexts: For the majority of the work we report in this thesis, we consider four (4) test collections, two small and two large. The two small collections are MED (also called Medlars), a collection of medical abstracts, and CACM, a collection of citations from the Communications of the ACM <ref> [30] </ref>. The two large collections are WSJ, a group of articles taken from the Wall Street Journal from 1990-1992, and AP88, documents taken from the AP Newswire during 1988. These collections are substantive subsets of the TREC collections and can be found on Disk 2 of the TREC CD-ROMs.
Reference: [31] <author> Edward A. Fox, M. Prabhakar Koushik, Joseph Shaw, Russell Modlin, and Durgesh Rao. </author> <title> Combining Evidence from Multiple Searches. </title> <booktitle> In The First Text Retrieval Conference (TREC-1), </booktitle> <pages> pages 319-328, </pages> <address> Gaithersburg, MD, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Belkin et al. [6] define two aspects. Query combination involves submitting different forms 2.3. Related Work:Dynamism 22 of queries to systems managing the same documents. Several studies have shown that combining the results of these queries can yield higher quality result lists than any of the constituent lists <ref> [4, 5, 31] </ref>. The second aspect in combination of evidence is called data fusion or collection fusion where results from the same query are run on disjunct archives and then combined. This distributed query processing is of most relevance to our work. <p> There is also considerable work occurring in methods to intelligently merge result lists from separate searches into a single result. The issues and several approaches are well-described in Vorhees [96] and Belkin et al. [6]. An early study by Fox et al. <ref> [31] </ref> assumed no inter-site communication and the use of local information to build the idf . Sites were 4.2. Collection Wide Information 35 identified by breaking down the TREC collection by document source.
Reference: [32] <author> James C. </author> <note> French. Electronic Distribution of Technical Reports and Working Papers: </note>
Reference-contexts: Chief among these efforts were the Unified Computer Science Technical Report Index (UCSTRI)[90] at Indiana University, Harvest [9], the CS-TR project which developed Dienst [22, 54], the WATERS (Wide Area TEchnical Report Service) project [34, 35], and the precursor to WATERS, techrep <ref> [32] </ref>. NCSTRL is the result of a combined effort of the CS-TR and WATERS groups. The architecture of NCSTRL document and index repositories is depicted in Figure 3.2. NCSTRL-standard sites maintain an Index (I) and Repository (R) for the reports maintained at that site.
References-found: 32

