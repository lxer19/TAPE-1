URL: http://www.cs.wustl.edu/cs/techreports/1994/wucs-94-19.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Email: bshouty@cpsc.ucalgary.ca  pwgoldb@cs.sandia.gov  sg@cs.wustl.edu  dmath@cs.wustl.edu  
Title: Exact Learning of Discretized Geometric Concepts  
Author: Nader H. Bshouty Paul W. Goldberg Sally A. Goldman H. David Mathias 
Date: July 5, 1994  
Address: Calgary, Alberta, Canada T2N 1N4  MS 1110 P.O. Box 5800 Albuquerque, NM 87185-1110  St. Louis, MO 63130  St. Louis, MO 63130  
Affiliation: Department of Computer Science The University of Calgary  Department 1423 Sandia National Labs,  Dept. of Computer Science Washington University  Dept. of Computer Science Washington University  
Pubnum: WUCS-94-19  
Abstract: We first present an algorithm that uses membership and equivalence queries to exactly identify a discretized geometric concept defined by the union of m axis-parallel boxes in d-dimensional discretized Euclidean space where each coordinate can have n discrete values. This algorithm receives at most md counterexamples and uses time and membership queries polynomial in m and log n for d any constant. Furthermore, all equivalence queries can be formulated as the union of O(md log m) axis-parallel boxes. Next, we show how to extend our algorithm to efficiently learn, from only equivalence queries, any discretized geometric concept generated from any number of halfspaces with any number of known (to the learner) slopes in a constant dimensional space. In particular, our algorithm exactly learns (from equivalence queries only) unions of discretized axis-parallel boxes in constant dimensional space in polynomial time. Further, this algorithm can be modified to handle a polynomial number of lies in the counterexamples provided by the environment. Finally, we introduce a new complexity measure that better captures the complexity of the union of m boxes than simply the number of boxes and the dimension. Our new measure, , is the number of segments in the target where a segment is a maximum portion of one of the sides of the target that lies entirely inside or entirely outside each of the other halfspaces defining the target. We present a modification of our first algorithm that uses time and queries polynomial in and log n. In fact, the time and queries (both membership and equivalence) used by this single algorithm are polynomial for either m or d constant. fl Portions of this paper appear in preliminary form in [16] and [7]. y This research was supported in part by the NSERC of Canada. z This research was performed while visiting Washington University. Currently supported by the U.S. Department of Energy under contract DE-AC04-76AL85000. x Supported in part by NSF Grant CCR-9110108 and an NSF NYI Grant CCR-9357707. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: Observe that the class considered by Frazier et al. is a generalization of the class of DNF formulas in which all variables only appear negated. While there has been some work addressing the general issue of mislabelled training examples in the PAC model <ref> [1, 18, 28, 17] </ref>, there has been little research on learning geometric concepts with noise. Auer [3] investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [2] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research. We study the problem of learning geometric concepts under the model of learning with queries <ref> [2] </ref> in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. To apply such a learning model to a geometric domain, it is necessary to look at a discretized (or digitalized) version of the domain. <p> Finally, in Section 10 we conclude with some open problems. 2 Learning Model The learning model we use in this paper is that of learning with queries developed by An-gluin <ref> [2] </ref>. When applied to our class of discretized geometric concepts, the learner's goal is to learn exactly how an unknown target concept, g, drawn from the concept class G 2 N d n , classifies as positive or negative all instances from the instance space N d n .
Reference: [3] <author> Peter Auer. </author> <title> On-line learning of rectangles in noisy environments. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 253-261, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: They showed that if the learner was restricted to only make equivalence queries in which each hypothesis was drawn from box d n then (d log n) queries are needed to achieve exact identification [19, 24]. Auer <ref> [3] </ref> improves this lower bound to ( d 2 log d log n). If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries. <p> While there has been some work addressing the general issue of mislabelled training examples in the PAC model [1, 18, 28, 17], there has been little research on learning geometric concepts with noise. Auer <ref> [3] </ref> investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [4] <author> Avrim Blum. </author> <title> Separating PAC and mistake-bound learning models over the Boolean domain. </title> <booktitle> In 31st Annual Symposium on Foundations of Computer Science, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: One must select hypotheses for the equivalence queries so that sufficient progress is made with each counterexample. This requirement of selecting a "smart" hypothesis makes the problem of obtaining an efficient algorithm to exactly learn the class S n significantly harder than obtaining the corresponding PAC result. Also Blum <ref> [4] </ref> has shown that if one-way functions exist then there exist functions that are PAC-learnable but not exactly learnable.
Reference: [5] <author> Avrim Blum and Steven Rudich. </author> <title> Fast learning of k-term DNF formulas with queries. </title> <booktitle> In Proceedings of the Twenty Fourth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-389, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: it would be interesting to see if S n can be efficiently learned in time polynomial in m and log n for d = O (log m) or in time polynomial in d and log n for m = O (log d) (i.e. a generalization of the Blum and Rudich <ref> [5] </ref> result that O (log n)-term DNF formulas are exactly learnable). Of course, since S n generalizes the class of DNF formulas, it seems very unlikely that one could develop an algorithm for the unrestricted case of S n that is polynomial in m, log n, and d.
Reference: [6] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Closely related to the problem of learning the union of discretized boxes, is the problem of learning the union of non-discretized boxes in the PAC model [29]. Blumer et al. <ref> [6] </ref> present an algorithm to PAC-learn an m-fold union of boxes in E d by drawing a sufficiently large sample of size m 0 = poly * ; lg 1 , and then performing a greedy covering over the at most 2d boxes defined by the sample. <p> As discussed by Maass and Turan [24] the task of a concept learning algorithm is to provide a "smart" hypothesis based on the data available. The results from Blumer et. al <ref> [6] </ref> show that under the PAC model any concise hypothesis that is consistent with the data is "smart enough". In other words, the PAC model provides no suitable basis for distinction among different consistent hypotheses.
Reference: [7] <author> Nader H. Bshouty. </author> <title> On learning discretized geometric concepts. </title> <type> Unpublished manuscript, </type> <month> May </month> <year> 1994. </year>
Reference: [8] <author> Nader H. Bshouty, Sally A. Goldman, Thomas R. Hancock, and Sleiman Matar. </author> <title> Asking questions to minimize errors. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 41-50, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: We present this algorithm, in part, because it introduces the approach used to obtain our other results and also because it uses 3 very few equivalence queries which is of interest if one's goal is to minimize the number of prediction errors made by the learner <ref> [8] </ref>. Next in Section 6 we describe a modification of this algorithm that efficiently learns the union of boxes in constant dimensional space with only equivalence queries. <p> serves two purposes: (1) the other algorithms presented build upon this basic algorithm and thus for ease of exposition we present it here, and (2) it uses very few equivalence queries which is of interest if one's goal is to minimize the number of prediction errors made by the learner <ref> [8] </ref> The following definition will be used throughout this section Definition 1 For a discretized geometric concept g, we define a += i pair to be a positive point x + = (x 1 ; : : : ; x i ; : : : ; x d ) paired with
Reference: [9] <author> William J. Bultman and Wolfgang Maass. </author> <title> Fast identification of geometric objects with membership queries. </title> <booktitle> In Fourth Workshop on Computational Learning Theory, </booktitle> <pages> pages 337-353, </pages> <year> 1991. </year>
Reference-contexts: Maass and Turan [25] show an (n) information theoretic lower bound on the number of equivalence queries when the hypotheses must be drawn from the concept class. Contrasting this lower bound, Bultman and Maass <ref> [9] </ref> give an efficient algorithm that uses membership and equivalence queries to efficiently learn this class using O (log n) equivalence queries. 4 Preliminaries Let N ; Z and R be the set of nonnegative integers, integers and reals, respectively.
Reference: [10] <author> Zhixiang Chen. </author> <title> Learning unions of two rectangles in the plane with equivalence queries. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 243-252. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: More recently, Chen <ref> [10] </ref> gave an algorithm that used equivalence queries to learn general unions of two boxes in the (discretized) plane. The algorithm uses O (log 2 n) equivalence queries, and involves a detailed case analysis of the shapes formed by the two rectangles.
Reference: [11] <author> Zhixiang Chen and Steven Homer. </author> <title> The bounded injury priority method and the learn-ability of unions of rectangles. </title> <type> Unpublished manuscript, </type> <month> May </month> <year> 1994. </year>
Reference-contexts: The hypothesis class of their algorithm is the union of 8m 2 2 rectangles. Also in work independent of our work, Chen and Homer <ref> [11] </ref> have improved upon their earlier result by giving an algorithm that learns any concept from S n using O (m 2 (d+1) d 2 log 2d+1 n) equivalence queries by applying techniques from recursive function theory.
Reference: [12] <author> Zhixiang Chen and Wolfgang Maass. </author> <title> On-line learning of rectangles. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 16-27. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year> <month> 28 </month>
Reference-contexts: An algorithm making O (2 d log n) equivalence queries was given by Maass and Turan [21, 23]. The best known result for learning the class box d n was provided by the work of Chen and Maass <ref> [12] </ref> in which they gave an algorithm making O (d 2 log n) equivalence queries.
Reference: [13] <author> V. Chvatal. </author> <title> A greedy heuristic for the set covering problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4(3) </volume> <pages> 233-235, </pages> <year> 1979. </year>
Reference-contexts: Thus our goal is to find the union of as few boxes as possible that "cover" all of the positive regions. We now describe how to formulate this problem as a set covering problem for which we can then use the standard greedy set covering heuristic <ref> [13] </ref> to perform the conversion. The set X of objects to cover will simply contain all positive regions in h. Thus jXj (4m + 1) d . Then the set F of subsets of X will be made as follows.
Reference: [14] <author> Mike Frazier, Sally Goldman, Nina Mishra, and Leonard Pitt. </author> <title> Learning from a consistently ignorant teacher. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Also Blum [4] has shown that if one-way functions exist then there exist functions that are PAC-learnable but not exactly learnable. Finally, under a variation of the PAC model in which membership queries can be made, Frazier et al. <ref> [14] </ref> have given an algorithm to PAC-learn the m-fold union of boxes in E d for which each box is entirely contained within the positive quadrant and contains the origin. Furthermore, their algorithm learns this subclass of general unions of boxes in time polynomial in both m and d.
Reference: [15] <author> Steven Homer and Zhixiang Chen. </author> <title> Fast learning unions of rectangles with queries. </title> <type> Unpublished manuscript, </type> <month> July </month> <year> 1993. </year>
Reference-contexts: The algorithm uses O (log 2 n) equivalence queries, and involves a detailed case analysis of the shapes formed by the two rectangles. In work independent of ours, Chen and Homer <ref> [15] </ref> have given an algorithm to learn he union of m rectangles in the plane using O (m 3 log n) queries (both membership and equivalence) and O (m 5 log n) time. The hypothesis class of their algorithm is the union of 8m 2 2 rectangles.
Reference: [16] <author> Paul W. Goldberg, Sally A. Goldman, and H. David Mathias. </author> <title> Learning unions of boxes with membership and equivalence queries. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <month> July </month> <year> 1994. </year>
Reference: [17] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22(4) </volume> <pages> 807-837, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Observe that the class considered by Frazier et al. is a generalization of the class of DNF formulas in which all variables only appear negated. While there has been some work addressing the general issue of mislabelled training examples in the PAC model <ref> [1, 18, 28, 17] </ref>, there has been little research on learning geometric concepts with noise. Auer [3] investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [18] <author> Philip D. Laird. </author> <title> Learning from Good and Bad Data. </title> <booktitle> Kluwer international series in engineering and computer science. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: Observe that the class considered by Frazier et al. is a generalization of the class of DNF formulas in which all variables only appear negated. While there has been some work addressing the general issue of mislabelled training examples in the PAC model <ref> [1, 18, 28, 17] </ref>, there has been little research on learning geometric concepts with noise. Auer [3] investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [19] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: One of the geometric concepts that they studied was the class box d n . They showed that if the learner was restricted to only make equivalence queries in which each hypothesis was drawn from box d n then (d log n) queries are needed to achieve exact identification <ref> [19, 24] </ref>. Auer [3] improves this lower bound to ( d 2 log d log n). If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries.
Reference: [20] <author> Philip M. Long and Manfred K. Warmuth. </author> <title> Composite geometric concepts and polynomial predictability. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 273-287. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Thus for d constant this algorithm runs in 5 polynomial time. Long and Warmuth <ref> [20] </ref> present an algorithm to PAC-learn this same class by again drawing a sufficiently large sample and constructing a hypothesis that consists of at most m (2d) m boxes consistent with the sample.
Reference: [21] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> On the complexity of learning from counterexamples. </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 262-267, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries. An algorithm making O (2 d log n) equivalence queries was given by Maass and Turan <ref> [21, 23] </ref>. The best known result for learning the class box d n was provided by the work of Chen and Maass [12] in which they gave an algorithm making O (d 2 log n) equivalence queries.
Reference: [22] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> On the complexity of learning from counterexamples and membership queries. </title> <booktitle> In 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 203-210, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The 4 learner is permitted time polynomial in 1=*, 1=ffi and relevant size measures to formulate a hypothesis. 3 Previous Work The problem of learning geometric concepts over a discrete domain was extensively studied by Maass and Turan <ref> [22, 23, 24, 25] </ref>. One of the geometric concepts that they studied was the class box d n .
Reference: [23] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Algorithms and lower bounds for on-line learning of geometrical concepts. </title> <type> Technical Report IIG-Report 316, </type> <institution> Technische Universitat Graz, TU Graz, Austria, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: The 4 learner is permitted time polynomial in 1=*, 1=ffi and relevant size measures to formulate a hypothesis. 3 Previous Work The problem of learning geometric concepts over a discrete domain was extensively studied by Maass and Turan <ref> [22, 23, 24, 25] </ref>. One of the geometric concepts that they studied was the class box d n . <p> If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries. An algorithm making O (2 d log n) equivalence queries was given by Maass and Turan <ref> [21, 23] </ref>. The best known result for learning the class box d n was provided by the work of Chen and Maass [12] in which they gave an algorithm making O (d 2 log n) equivalence queries.
Reference: [24] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Lower bound methods and separation results for on-line learning models. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 107-145, </pages> <year> 1992. </year>
Reference-contexts: The 4 learner is permitted time polynomial in 1=*, 1=ffi and relevant size measures to formulate a hypothesis. 3 Previous Work The problem of learning geometric concepts over a discrete domain was extensively studied by Maass and Turan <ref> [22, 23, 24, 25] </ref>. One of the geometric concepts that they studied was the class box d n . <p> One of the geometric concepts that they studied was the class box d n . They showed that if the learner was restricted to only make equivalence queries in which each hypothesis was drawn from box d n then (d log n) queries are needed to achieve exact identification <ref> [19, 24] </ref>. Auer [3] improves this lower bound to ( d 2 log d log n). If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries. <p> So for m constant this yields an efficient PAC algorithm. We note that either of these PAC algorithms can be applied to the class S n giving efficient PAC algorithms for this class for either d constant or m constant. As discussed by Maass and Turan <ref> [24] </ref> the task of a concept learning algorithm is to provide a "smart" hypothesis based on the data available. The results from Blumer et. al [6] show that under the PAC model any concise hypothesis that is consistent with the data is "smart enough".
Reference: [25] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Algorithms and lower bounds for on-line learning of geometrical concepts. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 251-269, </pages> <year> 1994. </year> <month> 29 </month>
Reference-contexts: The 4 learner is permitted time polynomial in 1=*, 1=ffi and relevant size measures to formulate a hypothesis. 3 Previous Work The problem of learning geometric concepts over a discrete domain was extensively studied by Maass and Turan <ref> [22, 23, 24, 25] </ref>. One of the geometric concepts that they studied was the class box d n . <p> There has also been work on learning 6 non axis-parallel discretized rectangles with only equivalence queries. Maass and Turan <ref> [25] </ref> show an (n) information theoretic lower bound on the number of equivalence queries when the hypotheses must be drawn from the concept class.
Reference: [26] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> How fast can a threshold gate learn? Computational Learning Theory and Natural Learning Systems: Constraints and Prospects, </title> <editor> G. Drastal, S.J. Hanson and R. Rivest eds., </editor> <publisher> MIT Press, to appear. </publisher>
Reference-contexts: There has also been some work on learning discretized geometric concepts defined by non axis-parallel hyperplanes. Maass and Turan <ref> [26] </ref> study the problem of learning a single discretized halfspace using only equivalence queries. They given an efficient algorithm using O (d 2 (log d+log n)) queries and give an information theoretic lower bound of d on the number of queries when all hypotheses are discretized halfspaces.
Reference: [27] <author> Wolfgang Maass and Manfred K. Warmuth. </author> <title> Efficient learning with virtual threshold gates. </title> <type> Personal communication. </type>
Reference-contexts: Finally, in other independent work, Maass and Warmuth <ref> [27] </ref> have developed, as part of a more general result, an algorithm to learn any concept from S n using O (md log n) equivalence queries and O computation time.
Reference: [28] <author> Robert H. Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 91-96. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Observe that the class considered by Frazier et al. is a generalization of the class of DNF formulas in which all variables only appear negated. While there has been some work addressing the general issue of mislabelled training examples in the PAC model <ref> [1, 18, 28, 17] </ref>, there has been little research on learning geometric concepts with noise. Auer [3] investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [29] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year> <month> 30 </month>
Reference-contexts: An l-liar equivalence oracle is an oracle that is allowed to lie at most l times during the learning session when providing a counterexample to an equivalence query. Another important learning model is the PAC model introduced by Valiant <ref> [29] </ref>. In this model the learner is presented with labeled examples chosen at random according to an unknown, arbitrary distribution D over the instance space. The learner's goal is to output a hypothesis that with high probability, at least (1 ffi), correctly classifies most of the instance space. <p> Closely related to the problem of learning the union of discretized boxes, is the problem of learning the union of non-discretized boxes in the PAC model <ref> [29] </ref>.
References-found: 29

