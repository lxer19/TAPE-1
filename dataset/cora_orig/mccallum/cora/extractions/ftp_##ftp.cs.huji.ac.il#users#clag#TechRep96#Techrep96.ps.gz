URL: ftp://ftp.cs.huji.ac.il/users/clag/TechRep96/Techrep96.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~clag/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email:clag@cs.huji.ac.il, jeff@cs.huji.ac.il  
Phone: ph: 011-972-2-658-5188 fax: 011-972-2-658-5439  
Title: Incremental and Mutual Adaptation in Multiagent Systems  
Author: Claudia V. Goldman Jeffrey S. Rosenschein 
Note: Technical Report, CS96-15 Supported by the Eshkol Fellowship, Israeli Ministry of Science  
Web: url: http://www.cs.huji.ac.il/~ clag  
Address: Givat Ram, Jerusalem, Israel  
Affiliation: Insitute of Computer Science The Hebrew University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Minoru Asada, Eiji Uchibe, and Koh Hosoda. </author> <title> Agents that learn from other competitive agents. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth international conference,Workshop on Agents that Learn from Other Agents, </booktitle> <year> 1995. </year>
Reference-contexts: Another example of achieving global coordination without sharing any knowledge can be found in [9]. Experiments were conducted in multiagent scenarios using the Q-learning algorithm in [7]. Another multiagent scenario, in which the agents don't learn from each other appears in <ref> [1] </ref>. The agent, facing the environment, regards the other agents as a single unit. The second interesting paradigm is concerned with the issue of a multiagent learning system. The focus in these systems is on the interactions among the agents while they learn.
Reference: [2] <author> Claudia V. Goldman and Jeffrey S. Rosenschein. </author> <title> Mutually supervised learning in mul-tiagent systems. Adaptation and Learning in Multi-Agent Systems, </title> <publisher> LNAI 1042 </publisher> <pages> 85-96, </pages> <year> 1996. </year>
Reference-contexts: This work deals with homogeneous societies, i.e., every agent's benefit is the system's benefit. We are interested in heterogenous agents that learn to coordinate their courses of actions. We have already suggested <ref> [2] </ref> the model of a teacher and learner, where both agents are involved in a mutually supervised learning process. The agents went through a training period, and then through a generalization period in which they generalize what they have been taught and act accordingly. <p> Nevertheless, we think that more formal work needs to be done in order to define what it means to be "fairly adapted" for problems that are solved within a multiagent system. In a previous paper <ref> [2] </ref> we gave several learning rules and a theoretical proof for the learnability of a cooperative solution for a multiagent system. The model used was based on a teacher-learner model, where the agents went first through a training stage and then generalized what they have learned.
Reference: [3] <editor> John J. Grefenstette. </editor> <booktitle> Lamarckian learning in multi-agent environments. In Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 303|310, </pages> <year> 1991. </year>
Reference-contexts: In this case, the environment is influenced by the actions of all the agents operating in it. The learning agent "knows" about the existence of the other agents in the same system indirectly through the reinforcement value it gets from the world. For example, in <ref> [3] </ref>, a central critic gives the payoff to each agent in the system. Each agent learns about the dynamic structure of its environment in [6]. Reinforcement techniques are widely used in the DAI literature.
Reference: [4] <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh international conference, </booktitle> <pages> pages 157|163, </pages> <year> 1994. </year>
Reference-contexts: The second interesting paradigm is concerned with the issue of a multiagent learning system. The focus in these systems is on the interactions among the agents while they learn. Each agent learns directly from each of the others. Markov games were proposed as a framework for multi-adaptive agents <ref> [4] </ref>, although this framework is not suitable for analyzing the emergence of cooperation in such systems. Learning social rules [5] requires three types of reinforcement, including the reinforcement from other agents that operate in the same world.
Reference: [5] <editor> Maja J. Mataric. </editor> <title> Learning to behave socially. </title> <booktitle> In Proceedings From Animals to Animats 3, Third International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 157|163, </pages> <year> 1994. </year>
Reference-contexts: Each agent learns directly from each of the others. Markov games were proposed as a framework for multi-adaptive agents [4], although this framework is not suitable for analyzing the emergence of cooperation in such systems. Learning social rules <ref> [5] </ref> requires three types of reinforcement, including the reinforcement from other agents that operate in the same world. This work deals with homogeneous societies, i.e., every agent's benefit is the system's benefit. We are interested in heterogenous agents that learn to coordinate their courses of actions.
Reference: [6] <editor> Maja J. Mataric. </editor> <title> Reward functions for accelerated learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh international conference, </booktitle> <pages> pages 157|163, </pages> <year> 1994. </year>
Reference-contexts: For example, in [3], a central critic gives the payoff to each agent in the system. Each agent learns about the dynamic structure of its environment in <ref> [6] </ref>. Reinforcement techniques are widely used in the DAI literature. Agents that learn according to the Q-learning algorithm [11] and also cooperate with other agents by exchanging information (partial solutions, plans of action) can learn more quickly than agents that do not cooperate [10].
Reference: [7] <author> Tuomas W. Sandholm and Robert H. Crites. </author> <title> Multiagent reinforcement learning in the iterated prisoner's dilemma. </title> <journal> Biosystems Journal Special Issue on the Prisoner's Dilemma, </journal> <note> submitted. </note>
Reference-contexts: In [8], agents learned complementary policies without sharing any information with the other agent. Another example of achieving global coordination without sharing any knowledge can be found in [9]. Experiments were conducted in multiagent scenarios using the Q-learning algorithm in <ref> [7] </ref>. Another multiagent scenario, in which the agents don't learn from each other appears in [1]. The agent, facing the environment, regards the other agents as a single unit. The second interesting paradigm is concerned with the issue of a multiagent learning system.
Reference: [8] <author> S. Sen, M. Sekaran, and J. Hale. </author> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 426-431, </pages> <year> 1994. </year>
Reference-contexts: Reinforcement techniques are widely used in the DAI literature. Agents that learn according to the Q-learning algorithm [11] and also cooperate with other agents by exchanging information (partial solutions, plans of action) can learn more quickly than agents that do not cooperate [10]. In <ref> [8] </ref>, agents learned complementary policies without sharing any information with the other agent. Another example of achieving global coordination without sharing any knowledge can be found in [9]. Experiments were conducted in multiagent scenarios using the Q-learning algorithm in [7].
Reference: [9] <author> Sandip Sen and Mahendra Sekaran. </author> <title> Multiagent coordination with learning classifier systems. </title> <booktitle> In Workshop on Adaptation and Learningin Multiagent Systems at the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Quebec, </address> <year> 1995. </year>
Reference-contexts: In [8], agents learned complementary policies without sharing any information with the other agent. Another example of achieving global coordination without sharing any knowledge can be found in <ref> [9] </ref>. Experiments were conducted in multiagent scenarios using the Q-learning algorithm in [7]. Another multiagent scenario, in which the agents don't learn from each other appears in [1]. The agent, facing the environment, regards the other agents as a single unit.
Reference: [10] <author> M. Tan. </author> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Machine Learning: Proceedings of the Tenth international conference, </booktitle> <pages> pages 330|337, </pages> <year> 1993. </year>
Reference-contexts: Reinforcement techniques are widely used in the DAI literature. Agents that learn according to the Q-learning algorithm [11] and also cooperate with other agents by exchanging information (partial solutions, plans of action) can learn more quickly than agents that do not cooperate <ref> [10] </ref>. In [8], agents learned complementary policies without sharing any information with the other agent. Another example of achieving global coordination without sharing any knowledge can be found in [9]. Experiments were conducted in multiagent scenarios using the Q-learning algorithm in [7].
Reference: [11] <author> Christopher J.C.H. Watkins and Peter Dayan. </author> <note> Technical note Q-learning. Machine Learning, 8:279|292, 1992. 13 </note>
Reference-contexts: For example, in [3], a central critic gives the payoff to each agent in the system. Each agent learns about the dynamic structure of its environment in [6]. Reinforcement techniques are widely used in the DAI literature. Agents that learn according to the Q-learning algorithm <ref> [11] </ref> and also cooperate with other agents by exchanging information (partial solutions, plans of action) can learn more quickly than agents that do not cooperate [10]. In [8], agents learned complementary policies without sharing any information with the other agent.
References-found: 11

