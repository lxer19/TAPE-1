URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1995/GIT-CC-95-17.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.95.html
Root-URL: 
Title: A Parallel Spectral Model for Atmospheric Transport Processes  several shared memory parallel machines, including SGI multiprocessors.  
Author: Thomas Kindler ; Karsten Schwan Dilma Silva Mary Trauner Fred Alyea 
Keyword: Performance measurements  
Note: Submitted to Concurrency: Practice and Experience  are performed on a 64-node KSR2 supercomputer. However, the parallel code has been ported to  
Address: Atlanta, Georgia 30332 2 Paul Scherrer Institute, CH-5232 Villigen, Switzerland  Atlanta, Georgia 30332  
Affiliation: 1 School of Earth and Atmospheric Sciences, Georgia Institute of Technology,  College of Computing,Georgia Institute of Technology,  
Phone: 3  
Date: April 28, 1995  
Abstract: This paper describes a parallel implementation of a grand challenge problem: global atmospheric modeling. The novel contributions of our work include: (1) a detailed investigation of opportunities for parallelism in atmospheric transport based on spectral solution methods, (2) the experimental evaluation of overheads arising from load imbalances and data movement for alternative parallelization methods, and (3) the development of a parallel code that can be monitored and steered interactively based on output data visualizations and animations of program functionality or performance. Code parallelization takes advantage of the relative independence of computations at different levels in the earth's atmosphere, resulting in parallelism of up to 40 processors, each independently performing computations for different atmospheric levels and requiring few communications between different levels across model time steps. Next, additional parallelism is attained within each level by taking advantage of the natural parallelism offered by the spectral computations being performed (eg., taking advantage of independently computable terms in equations). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Weiming Gu, Jeffrey Vetter, and Karsten Schwan. </author> <title> An annotated bibliography of interactive program steering. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(9) </volume> <pages> 140-148, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Evidence of the utility of on-line program steering can be found in numerous past publications, many of which are reviewed in <ref> [1] </ref>. The specific global atmospheric model implemented as part of this research represents atmospheric fields with spherical basis functions, which have been widely used for modeling phenomena like weather prediction, global warming or global change of atmospheric constituents [2, 3]. <p> The mechanisms used for this research are part of the Falcon system for on-line program monitoring and steering now being developed by our group and described in several publications [37, 31]. An overview of related work on program steering appears in <ref> [1] </ref>. We are continuing such research with this model, by controlling load imbalances and by controlling the relative speeds (and allocated computational resources) with which model input, model computation, and data visualization are performed.
Reference: [2] <author> D. Dent and A. Simmons. </author> <title> The ecmwf multi tasking weather prediction model. </title> <journal> Computer Physics Reports, </journal> <volume> 11 </volume> <pages> 153-194, </pages> <year> 1989. </year>
Reference-contexts: The specific global atmospheric model implemented as part of this research represents atmospheric fields with spherical basis functions, which have been widely used for modeling phenomena like weather prediction, global warming or global change of atmospheric constituents <ref> [2, 3] </ref>. Spec- tral models have some advantages over the grid-based models commonly parallelized in past and current work [4, 5, 6, 7]. <p> Cthreads are already running on the SGI multiprocessor machines). 3.7 Related Research Research related to our efforts falls into two categories: 18 * the parallelization of weather forecast models, as performed prior to our work by research groups at the European Center for Medium-Range Weather Forecast, using spectral weather models <ref> [2] </ref>, and * the parallelization of climate models, as performed concurrently with our work at Oakridge National Laboratories in the U.S., using NCAR's CCM2 model [3]. Both of these research groups employ distributed rather than shared memory machines in their research.
Reference: [3] <author> J.J. Hack, B.A. Boville, B.P. Briegleb, J.T. Kiehl, P.J. Rasch, and D.L. Williamson. </author> <title> Description of the ncar community climate model (ccm2). </title> <type> NCAR Tech. </type> <institution> Note TN-382+STR, National Center for Atmospheric Research, </institution> <year> 1992. </year>
Reference-contexts: The specific global atmospheric model implemented as part of this research represents atmospheric fields with spherical basis functions, which have been widely used for modeling phenomena like weather prediction, global warming or global change of atmospheric constituents <ref> [2, 3] </ref>. Spec- tral models have some advantages over the grid-based models commonly parallelized in past and current work [4, 5, 6, 7]. <p> Current transport models us up to 42 spectral waves (lM ax = 42 which corresponds to 946 spectral points, see Appendix A for explanation) per model layer <ref> [3] </ref>, which results in a resolution of about 128 longitude and 64 latitude gridpoints. The vertical resolutions used in most models assumes between 10 and 40 levels which corresponds to an altitude of about 50 km. <p> of weather forecast models, as performed prior to our work by research groups at the European Center for Medium-Range Weather Forecast, using spectral weather models [2], and * the parallelization of climate models, as performed concurrently with our work at Oakridge National Laboratories in the U.S., using NCAR's CCM2 model <ref> [3] </ref>. Both of these research groups employ distributed rather than shared memory machines in their research. However, differences in their results to the work presented in this paper are due primarily to differences in parallelization approaches.
Reference: [4] <author> M.F. Wehner, J.J. Ambrosiano, J.C. Brown, W.P. Dannevik, P.G. Eltgroth, A.A. Mirin, J.D. Farrara, C.C. Ma, C.R. Mechoso, and J.A. Spahr. </author> <title> Toward a high performance distributed memory climate model. </title> <journal> IEEE, </journal> <pages> pages 102 - 113, </pages> <year> 1993. </year>
Reference-contexts: Spec- tral models have some advantages over the grid-based models commonly parallelized in past and current work <ref> [4, 5, 6, 7] </ref>. For example, spectral models naturally conserve the area averaged mean square kinetic energy and the mean square vorticity of wind fields, whereas in grid based models these quantities are either not conserved or require additional computation when such conservation is important [8]. <p> As a result, no movement of grid data is necessary during model computation, whereas spectral data is shared frequently. 2.3.1 Layer Parallelization Grid based global models are typically decomposed (e.g., see <ref> [4] </ref>) using a two-dimensional latitude/longitude domain decomposition, where each subdomain consists of several neighboring vertical columns extending from the earth's surface to the top layer of the atmosphere addressed by the model.
Reference: [5] <author> J.J. Ambrosiano, J. Bolstad, A.J. Bourgeois, J.C. Brown, and B. Chan. </author> <title> High-performance climate modeling using a domain and task decomposition message-passing approach. </title> <address> pages 397-405. </address> <publisher> IEEE, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: Spec- tral models have some advantages over the grid-based models commonly parallelized in past and current work <ref> [4, 5, 6, 7] </ref>. For example, spectral models naturally conserve the area averaged mean square kinetic energy and the mean square vorticity of wind fields, whereas in grid based models these quantities are either not conserved or require additional computation when such conservation is important [8].
Reference: [6] <author> S. R. M. Barros and T. Kauranne. </author> <title> Spectral and multigrid spherical helmholtz equation solvers on distributed memory parallel computers. </title> <booktitle> In Fourth Workshop on use of parallel pocessors in meteorology, </booktitle> <pages> pages 1-27. </pages> <institution> European Centre for Mefium-Range Weather Forecasts, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: Spec- tral models have some advantages over the grid-based models commonly parallelized in past and current work <ref> [4, 5, 6, 7] </ref>. For example, spectral models naturally conserve the area averaged mean square kinetic energy and the mean square vorticity of wind fields, whereas in grid based models these quantities are either not conserved or require additional computation when such conservation is important [8]. <p> These levels of parallelism are achieved by using an alternative parallelization to the one used in previous work on parallel spectral transport models <ref> [6] </ref>, where parallelism is attained by decomposing atmospheric data along latitudes and/or longitudes. <p> Therefore, the detailed study of spectral transport parallelization with the TRANS model can be considered a necessary prerequisite or element of any parallelization effort involving large-scale atmospheric codes. Specific comments on each paper describing related work are presented next. In the European models, Barros et al. <ref> [6] </ref> investigate spectral and grid based parallelized solutions to Helmholtz-type equations on an iPSC/2 hypercube with 32 nodes. parallelization of the Laplace transformation is employed (as in our work), comparing two types of data exchanges between processors: (1) the rotation approach, where data is communicated stepwise between processors, with each processor
Reference: [7] <author> R. Jakob and J. Hack. </author> <title> Parallel mimd programming for global models of atmospheric flow. </title> <booktitle> ACM, </booktitle> <pages> pages 106-112, </pages> <year> 1989. </year> <month> 21 </month>
Reference-contexts: Spec- tral models have some advantages over the grid-based models commonly parallelized in past and current work <ref> [4, 5, 6, 7] </ref>. For example, spectral models naturally conserve the area averaged mean square kinetic energy and the mean square vorticity of wind fields, whereas in grid based models these quantities are either not conserved or require additional computation when such conservation is important [8]. <p> In the U.S., Jacob et al. <ref> [7] </ref> solve the shallow water model in a plane on a 20-processor Encore Multimax shared memory machine, comparing finite difference vs. spectral transport solution methods.
Reference: [8] <author> W.M. Washington and C.L. Parkinson. </author> <title> An introduction to three-dimensional climate modeling. </title> <publisher> Oxford University Press, </publisher> <year> 1986. </year>
Reference-contexts: For example, spectral models naturally conserve the area averaged mean square kinetic energy and the mean square vorticity of wind fields, whereas in grid based models these quantities are either not conserved or require additional computation when such conservation is important <ref> [8] </ref>. Despite such advantages, grid based models have found wider acceptance in recent research in part because they are believed (1) to give rise to larger amounts of parallelism than spectral models, and (2) more easily coupled with grid based models simulating local phenomena (e.g., pollution modeling). <p> Chemical reaction mechanisms extending the transport model will be described in future publications. The mathematical formulation of the model is explained briefly in Appendix A and is described in more detail in several other publications, including <ref> [12, 13, 14, 15, 8] </ref>. The Spectral Approach to Solving Global Transport. Any atmospheric constituent Y with mixing ratio X 1 is subject to a continuity equation involving the constituent.
Reference: [9] <author> I.T. Foster and P.H. Worley. </author> <title> Parallel algorithms for the spectral transform method. </title> <type> Technical Report ORNL/TM-12507, </type> <institution> Oak Ridge National Laboratory, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Our parallelization strategy and results also differ from the recent, extensive work on parallel climate models by Foster and Worley <ref> [9, 10] </ref>, most of which was performed concurrently with our research. Specifically, Foster and Worley investigate message passing machines like the Intel Paragon, while our work includes a detailed study of performance overheads arising on shared memory multiprocessors. <p> Also important is the model's extensibility to include additional code modules, such as modules performing chemical calculations for specific constituents. While the implementation of TRANS does not offer a uniform or self-contained framework for inclusion of additional or complementary code modules as described in other research <ref> [9, 26] </ref>, TRANS attains limited extensibility and more importantly, the ability to interact on-line with other programs potentially running on different machines by using a uniform format for exchange of binary input and output files. This format is described in detail in [27]. <p> Additional measurements appear in [10], where results are attained and compared on a 1024 processor NCube/2 machine, a 128 19 processor iPSC/860, a 512 processor Paragon, and a 512 processor Connection Machine. Specific optimizations address each of the machines being used. In <ref> [9] </ref>, Oakridge investigators compare several possible parallelization of the spectral method for solving the Shallow Water Equation, including (1) parallelization of the Laplace transformation using the (a) transpose technique versus the (b) rotation technique, (2) parallelization of the FFT using the (a) transpose versus (b) rotation techniques.
Reference: [10] <author> P.H. Worley and I.T. Foster. </author> <title> Parallel spectral transform shallow water model: A runtimetunable parallel benchmark code. </title> <booktitle> In SHPCC '94, </booktitle> <pages> pages 207-214. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: Our parallelization strategy and results also differ from the recent, extensive work on parallel climate models by Foster and Worley <ref> [9, 10] </ref>, most of which was performed concurrently with our research. Specifically, Foster and Worley investigate message passing machines like the Intel Paragon, while our work includes a detailed study of performance overheads arising on shared memory multiprocessors. <p> This necessitates a parallelization approach relying on multiple methods for program and data decomposition, such as term and parallelization. Models that require extensive computation integrating over vertical atmospheric columns <ref> [10] </ref> may prefer to layer parallelization, but they still remain subject to the other performance issues studied in this paper, including load balancing, the minimization of grid data exchange, etc. <p> This mixed parallelization approach results in a composite efficiency of about 0:1 for T21 on 64 processors and of about 0:15 for T42 on 128 processors. Load imbalances appear the primary causes of reduced efficiencies on large-scale machines. Additional measurements appear in <ref> [10] </ref>, where results are attained and compared on a 1024 processor NCube/2 machine, a 128 19 processor iPSC/860, a 512 processor Paragon, and a 512 processor Connection Machine. Specific optimizations address each of the machines being used.
Reference: [11] <author> P.J. Rasch, X. Tie, B.A. Boville, and D.L. Williamson. </author> <title> A three-dimensional transport model for the middle atmosphere. </title> <journal> J. Geophys. Res., </journal> <volume> 99 </volume> <pages> 999-1017, </pages> <year> 1994. </year>
Reference-contexts: Conclusions and future research addressing model parallelization, steering, and visualization appear in Section 4. 3 2 Model Functionality 2.1 The Modeling of Atmospheric Transport Processes Global transport models are important tools for understanding the distribution of relevant atmospheric parameters like the mixing ratios of chemical species and aerosol particles <ref> [11] </ref>. Transport models are often coupled with a variety of chemical reaction mechanisms to describe selected chemical changes of the simulated species during transport. <p> For further validation, we have also simulated the distribution of radioactive Carbon 14 ( 14 C) in the atmosphere after the nuclear bomb tests in the 1950s and 1960s. A detailed outline of this experiment appears in <ref> [11] </ref>. Since natural sources and sinks of 14 C are negligible, simulation of the excess 14 C after the bomb tests and comparison with observational data provides a reliable test of the TRANS model's ability to transport material correctly.
Reference: [12] <author> B. Haurwitz. </author> <title> The motion of atmospheric disturbances on the spherical earth. </title> <journal> Journal of Mar. Res., </journal> <volume> 3 </volume> <pages> 254-267, </pages> <year> 1940. </year>
Reference-contexts: Chemical reaction mechanisms extending the transport model will be described in future publications. The mathematical formulation of the model is explained briefly in Appendix A and is described in more detail in several other publications, including <ref> [12, 13, 14, 15, 8] </ref>. The Spectral Approach to Solving Global Transport. Any atmospheric constituent Y with mixing ratio X 1 is subject to a continuity equation involving the constituent.
Reference: [13] <author> I.S. Silberman. </author> <title> Planetary waves in the atmosphere. </title> <journal> J. Meteorol., </journal> <volume> 11 </volume> <pages> 27-34, </pages> <year> 1954. </year>
Reference-contexts: Chemical reaction mechanisms extending the transport model will be described in future publications. The mathematical formulation of the model is explained briefly in Appendix A and is described in more detail in several other publications, including <ref> [12, 13, 14, 15, 8] </ref>. The Spectral Approach to Solving Global Transport. Any atmospheric constituent Y with mixing ratio X 1 is subject to a continuity equation involving the constituent.
Reference: [14] <author> G.W. Platzmann. </author> <title> The spectral form of the vorticity equation. </title> <journal> J. Meteorol., </journal> <volume> 17 </volume> <pages> 635-644, </pages> <year> 1960. </year>
Reference-contexts: Chemical reaction mechanisms extending the transport model will be described in future publications. The mathematical formulation of the model is explained briefly in Appendix A and is described in more detail in several other publications, including <ref> [12, 13, 14, 15, 8] </ref>. The Spectral Approach to Solving Global Transport. Any atmospheric constituent Y with mixing ratio X 1 is subject to a continuity equation involving the constituent.
Reference: [15] <author> S. Kubota, M. Hirose, Y.Kichuchi, and Y. Kurihara. </author> <title> Barotropic forecasting with the use of surface spherical harmonic representation. Pap. </title> <journal> Meteorol. Geophys., </journal> <volume> 12 </volume> <pages> 199-215, </pages> <year> 1961. </year>
Reference-contexts: Chemical reaction mechanisms extending the transport model will be described in future publications. The mathematical formulation of the model is explained briefly in Appendix A and is described in more detail in several other publications, including <ref> [12, 13, 14, 15, 8] </ref>. The Spectral Approach to Solving Global Transport. Any atmospheric constituent Y with mixing ratio X 1 is subject to a continuity equation involving the constituent.
Reference: [16] <author> M. Prather. </author> <title> Special numerical experiment: Simulation of cfcl3 as a test for 3-d atmospheric models. Technical report, Report for WCRP Workshop on Long-Range transport of trace gases, </title> <year> 1992. </year>
Reference-contexts: Temporal time steps of about 15 minutes are necessary to maintain numerical stability for the model calculations. A general overview of the performance of several different kinds of transport models appears in <ref> [16] </ref>. 2.2 The Atmospheric Transport Code The research presented in this paper is based on three codes: 1) STRAT: a sub-program of a global circulation model described in [17, 18]. This code is originally written in Fortran, with a newer version also available in C.
Reference: [17] <author> Derek M. Cunnold, Fred Alyea, N. Philips, and R. Prinn. </author> <title> A three-dimensional dynamicalchemical model of atmospheric ozone. </title> <journal> J. Atmos. Sci., </journal> <volume> 32 </volume> <pages> 170-194, </pages> <year> 1975. </year>
Reference-contexts: A general overview of the performance of several different kinds of transport models appears in [16]. 2.2 The Atmospheric Transport Code The research presented in this paper is based on three codes: 1) STRAT: a sub-program of a global circulation model described in <ref> [17, 18] </ref>. This code is originally written in Fortran, with a newer version also available in C. STRAT runs on workstations like the IBM RS-6000 machines. 2) TRANS21: a transport code which produces identical results as STRAT but written in C and the Cthreads parallel programming library [19]. <p> scientists to avoid time-consuming mistakes or simply to evaluate more alternatives in their scientific endeavors. 3 Model and Implementation Evaluation 3.1 Model Validation The parallelized transport model (TRANS) has been validated and compared with the sequential Fortran version of the same model, called STRAT, which is described in detail in <ref> [17, 18] </ref>. For further validation, we have also simulated the distribution of radioactive Carbon 14 ( 14 C) in the atmosphere after the nuclear bomb tests in the 1950s and 1960s. A detailed outline of this experiment appears in [11].
Reference: [18] <author> Derek M. Cunnold, Fred Alyea, and R. Prinn. </author> <title> Preliminary calculations concerning the mainte-nance of the zonal mean ozone distribution in the northern hemisphere. </title> <journal> Pure Appl. Geophys., </journal> <volume> 118 </volume> <pages> 329-354, </pages> <year> 1980. </year>
Reference-contexts: A general overview of the performance of several different kinds of transport models appears in [16]. 2.2 The Atmospheric Transport Code The research presented in this paper is based on three codes: 1) STRAT: a sub-program of a global circulation model described in <ref> [17, 18] </ref>. This code is originally written in Fortran, with a newer version also available in C. STRAT runs on workstations like the IBM RS-6000 machines. 2) TRANS21: a transport code which produces identical results as STRAT but written in C and the Cthreads parallel programming library [19]. <p> scientists to avoid time-consuming mistakes or simply to evaluate more alternatives in their scientific endeavors. 3 Model and Implementation Evaluation 3.1 Model Validation The parallelized transport model (TRANS) has been validated and compared with the sequential Fortran version of the same model, called STRAT, which is described in detail in <ref> [17, 18] </ref>. For further validation, we have also simulated the distribution of radioactive Carbon 14 ( 14 C) in the atmosphere after the nuclear bomb tests in the 1950s and 1960s. A detailed outline of this experiment appears in [11].
Reference: [19] <author> Karsten Schwan, Harold Forbes, Ahmed Gheith, Bodhisattwa Mukherjee, and Yiannis Samiotakis. </author> <title> A cthread library for multiprocessors. </title> <type> Technical report, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <address> Atlanta, GA 30332, GIT-ICS-91/02, </address> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: This code is originally written in Fortran, with a newer version also available in C. STRAT runs on workstations like the IBM RS-6000 machines. 2) TRANS21: a transport code which produces identical results as STRAT but written in C and the Cthreads parallel programming library <ref> [19] </ref>. This code is a prototype used for exploration of parallelism in atmospheric modeling and will next be employed for certain scientific investigations, including studies of the global cycle of Nitrous Oxide (N 2 O). 3) TRANS42: a version of TRANS21 employing a higher resolution in the horizontal direction.
Reference: [20] <author> R. Swinbank and A. O'Neill. </author> <title> A stratosphere troposphere data assimilation system. </title> <note> Climate Research Technical Note CRTN 35, </note> <institution> Hadley Centre Meteorological Office, </institution> <address> London Road Bracknell Berkshire RG12 2SY, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: In the remeining part of the paper we will refer to either one of TRANS21 or TRANS42 simply by TRANS. Instead of computing the windfields inside the model, all codes use observed UKMO windfields <ref> [20] </ref>, which are concurrently read from files while the simulation proceeds. The models contain 37 vertical levels starting at the surface (1000mbar) and going up to about 48 km (1mbar).
Reference: [21] <author> E.N. Lorenz. </author> <title> An n-cycle time-differencing scheme for stepwise numerical integration. </title> <journal> Monthly Weather review, </journal> <volume> 99(8) </volume> <pages> 644-648, </pages> <year> 1971. </year>
Reference-contexts: In TRANS42 the truncation is lM ax = 21 (T42, which corresponds to 946 complex spectral data points) which corresponds to 128 longitudinal and 64 latitudinal coordinates in the grid domain. The stepwise numerical time integration is calculated with an 8-cycle Lorenz-scheme <ref> [21] </ref> 12 times per day, which leads to an overall time step size of 15 minutes.
Reference: [22] <author> Bodhisattwa Mukherjee. </author> <title> A portable and reconfigurable threads package. </title> <booktitle> In Proceedings of Sun User Group Technical Conference, </booktitle> <pages> pages 101-112, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: An additional optimization used in our implementation is to pre-create helper threads, pre-map them to the appropriate processors, then simply wake up such threads when they are needed, using the low-level conditional wait and signal primitives offered by the Cthreads library <ref> [22, 23] </ref>. Such wakeups are performed whenever layer processors have reached the point in time when terms A, B, and C have to be computed. Layer processors compute the term D themselves, while waiting for the completion of helper threads computing A, B, and C. <p> The TRANS models' implementation is portable to any shared memory parallel machine, currently including SUN Sparcstations, SGI uni- and multiprocessors, and the KSR-1 and KSR-2 supercomputers. This portability is achieved by use of the Cthreads portable parallel programmming library described in detail in several publications, including <ref> [22, 23] </ref>. This library hides underlying differences in parallel machines and operating systems by providing a standard set of constructs for creating and controlling parallel execution threads.
Reference: [23] <author> Kaushik Ghosh, Bodhisattwa Mukherjee, and Karsten Schwan. </author> <title> Experimentation with con-figurable, lightweight threads on a ksr multiprocessor. </title> <booktitle> In In the Proceedings of the First International Workshop on Parallel Processing, </booktitle> <address> Bangalore, India, </address> <month> December </month> <year> 1994. </year> <note> Expanded version available as Georgia Tech TR# GIT-CC-93/37. 22 </note>
Reference-contexts: An additional optimization used in our implementation is to pre-create helper threads, pre-map them to the appropriate processors, then simply wake up such threads when they are needed, using the low-level conditional wait and signal primitives offered by the Cthreads library <ref> [22, 23] </ref>. Such wakeups are performed whenever layer processors have reached the point in time when terms A, B, and C have to be computed. Layer processors compute the term D themselves, while waiting for the completion of helper threads computing A, B, and C. <p> The TRANS models' implementation is portable to any shared memory parallel machine, currently including SUN Sparcstations, SGI uni- and multiprocessors, and the KSR-1 and KSR-2 supercomputers. This portability is achieved by use of the Cthreads portable parallel programmming library described in detail in several publications, including <ref> [22, 23] </ref>. This library hides underlying differences in parallel machines and operating systems by providing a standard set of constructs for creating and controlling parallel execution threads.
Reference: [24] <author> Ahmed Gheith, Bodhi Mukherjee, Dilma Silva, and Karsten Schwan. Ktk: </author> <title> Kernel support for configurable objects and invocations. </title> <booktitle> In Second International Workshop on Configurable Distributed Systems, </booktitle> <pages> pages 236-240. </pages> <publisher> IEEE, ACM, </publisher> <month> March </month> <year> 1994. </year>
Reference-contexts: As a result, the performance of mutex locks differs across both machines, but the interface and functionality provided to application programmers does not change. Model portability to non-shared memory machines is a topic of research that is addressed in other work by our research group (e.g., see <ref> [24, 25, 26] </ref>). Also important is the model's extensibility to include additional code modules, such as modules performing chemical calculations for specific constituents.
Reference: [25] <author> Christian Clemencon, Bodhisattwa Mukherjee, and Karsten Schwan. </author> <title> Distributed shared ab-stractions (dsa) on large-scale multiprocessors. </title> <booktitle> In Proc. of the Fourth USENIX Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pages 227-246. </pages> <publisher> USENIX, </publisher> <month> September </month> <year> 1993. </year> <note> Also as TR# GIT-CC-93/37. </note>
Reference-contexts: As a result, the performance of mutex locks differs across both machines, but the interface and functionality provided to application programmers does not change. Model portability to non-shared memory machines is a topic of research that is addressed in other work by our research group (e.g., see <ref> [24, 25, 26] </ref>). Also important is the model's extensibility to include additional code modules, such as modules performing chemical calculations for specific constituents.
Reference: [26] <author> Greg Eisenhauer and Karsten Schwan. </author> <title> Md a flexible framework for high-speed parallel molecular dynamics. </title> <editor> In Adrian Tentner, editor, </editor> <booktitle> High Performance Computing, Proceedings of the 1994 SCS Simulation Multiconference, </booktitle> <pages> pages 70-75. </pages> <institution> Society for Computer Simulation, Society for Computer Simulation, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: As a result, the performance of mutex locks differs across both machines, but the interface and functionality provided to application programmers does not change. Model portability to non-shared memory machines is a topic of research that is addressed in other work by our research group (e.g., see <ref> [24, 25, 26] </ref>). Also important is the model's extensibility to include additional code modules, such as modules performing chemical calculations for specific constituents. <p> Also important is the model's extensibility to include additional code modules, such as modules performing chemical calculations for specific constituents. While the implementation of TRANS does not offer a uniform or self-contained framework for inclusion of additional or complementary code modules as described in other research <ref> [9, 26] </ref>, TRANS attains limited extensibility and more importantly, the ability to interact on-line with other programs potentially running on different machines by using a uniform format for exchange of binary input and output files. This format is described in detail in [27].
Reference: [27] <author> Greg Eisenhauer. </author> <title> Portable self-describing binary data streams. </title> <type> Technical Report GIT-CC-9445, </type> <institution> College of Computing,Georgia Institute of Technology, </institution> <address> Atlanta, GA 300332, </address> <year> 1994. </year> <note> (anon. ftp from ftp.cc.gatech.edu). </note>
Reference-contexts: This format is described in detail in <ref> [27] </ref>. Briefly, it permits programmers to define the formats of expected input and output data used by their programs, then provides support for translation of these formats across the language and machine barriers existing between those programs.
Reference: [28] <author> Greg Eisenhauer, Weiming Gu, Karsten Schwan, and Niru Mallavarupu. </author> <title> Falcon toward inter-active parallel programs: The on-line steering of a molecular dynamics application. </title> <type> Technical Report GIT-CC-94-08, </type> <institution> Georgia Institute of Technology, College of Computing, </institution> <address> Atlanta, GA 30332-0280, </address> <year> 1994. </year> <note> also in High-Performance Distributed Computing (HPDC-3). </note>
Reference-contexts: In addition, selected program attributes are directly captured by the Falcon on-line monitoring system described in more detail in <ref> [28] </ref>.
Reference: [29] <author> H.S. Johnston. </author> <title> Evaluation of excess carbon 14 and strontium 90 data for suitability to test two-dimensional stratospheric models. </title> <journal> J. Geophys. Res., </journal> <volume> 94 </volume> <pages> 18485-18493, </pages> <year> 1989. </year>
Reference-contexts: Since natural sources and sinks of 14 C are negligible, simulation of the excess 14 C after the bomb tests and comparison with observational data provides a reliable test of the TRANS model's ability to transport material correctly. For this validation, input data is taken from Johnston <ref> [29] </ref> for October 1963.
Reference: [30] <author> J. B. Drake, R.E. Flanery, D.W. Walker, I.T. Foster P.H. Worley, J.G. Michalakes, R.L. Stevens, J.J. Hack, and D.L. Williamson. </author> <title> The message passing version of the parallel commu-nity climate model. </title> <editor> In G-R. Hoffman and T. Kauranne, editors, </editor> <booktitle> Parallel Supercomputing in Atmospheric Science, </booktitle> <pages> pages 500-513. </pages> <publisher> World Scientific, </publisher> <year> 1993. </year>
Reference-contexts: Examples of such processes exhibiting excessive vertical data exchanges include radiation modeling, cloud modeling, etc. (e.g. <ref> [30] </ref>) Furthermore, computational loads are balanced equally across layers, resulting in near linear speedup. As described in Section 2.2 and as commonly done in climate models, the scientific version of TRANS will use a total of 37 atmospheric layers.
Reference: [31] <author> Weiming Gu, Greg Eisenhauer, Eileen Kraemer, Karsten Schwan, John Stasko, Jeffrey Vetter, and Nirupama Mallavarupu. </author> <title> Falcon: On-line monitoring and steering of large-scale paral-lel programs. </title> <type> Technical Report GIT-CC-94-21, </type> <institution> Georgia Institute of Technology, College of Computing, </institution> <address> Atlanta, GA 30332-0280, </address> <month> April </month> <year> 1994. </year> <note> Submitted to Frontiers 95. </note>
Reference-contexts: Thirdly, some additional overhead results from the necessary synchronization required before and after terms are computed. The measurements depicted in Figure 8 demonstrate the primary contribution of load imbalance to the reduction in parallel efficiency. In these measurements, monitoring support available in Cthreads (see <ref> [31] </ref>) is used to measure term execution times for a three-layer run of the STRAT42 model. These measurements assume locally resident grid data. Load imbalance and reductions in parallel efficiency are further aggravated when such locality of grid data is not assured (see Section 3.4). Parallelization. <p> The mechanisms used for this research are part of the Falcon system for on-line program monitoring and steering now being developed by our group and described in several publications <ref> [37, 31] </ref>. An overview of related work on program steering appears in [1]. We are continuing such research with this model, by controlling load imbalances and by controlling the relative speeds (and allocated computational resources) with which model input, model computation, and data visualization are performed.
Reference: [32] <author> Karsten Schwan and Anita K. Jones. </author> <title> Specifying resource allocation for the cm* multiprocessor. </title> <journal> IEEE Software, </journal> <volume> 3(3) </volume> <pages> 60-70, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: Formulations of the suitable restrictions of mappings of threads to processors have been widely explored in the literature (e.g., see early work described in <ref> [32] </ref>), but they have not been added to TRANS-D. The exchange of spectral information among different helper threads does not affect program performance, as readily seen from the improvements in speedup attained when balancing helper loads (see Figure 10) while also exchanging spectral data among helpers.
Reference: [33] <author> U. Gaertel, W. Joppich, and A. Schueller. </author> <title> Parallelizing the ecmwf's weather forecast program: the d case. </title> <booktitle> Parallel Computing 19, </booktitle> <volume> 19 </volume> <pages> 1427-1429, </pages> <year> 1993. </year>
Reference-contexts: Further parallelizations by this research group concern a large-scale nCube/2, where they attain an efficiency of 86% on 86 processors using the 2-D ECMF weather forecast model <ref> [33] </ref>, and efficiencies of about 85% on 8 processors on an IBM SP1 machine with the 3-D ECMF weather forecast model [34].
Reference: [34] <author> U. Gaertel, W. Joppich, and A. </author> <title> Schueller. </title> <booktitle> Medium-range weather forecast on parallel systems. </booktitle> <pages> pages 388-391. </pages> <publisher> IEEE, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: Further parallelizations by this research group concern a large-scale nCube/2, where they attain an efficiency of 86% on 86 processors using the 2-D ECMF weather forecast model [33], and efficiencies of about 85% on 8 processors on an IBM SP1 machine with the 3-D ECMF weather forecast model <ref> [34] </ref>. In the U.S., Jacob et al. [7] solve the shallow water model in a plane on a 20-processor Encore Multimax shared memory machine, comparing finite difference vs. spectral transport solution methods.
Reference: [35] <author> P.H. Worley and J.B. Drake. </author> <title> Parallelizing the spectral transform method. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 4(4) </volume> <pages> 269-291, </pages> <year> 1992. </year>
Reference-contexts: Mixed parallelization methods are used, where the three different equations of the shallow water model are computed concurrently, while also using parallelization when solving each of the three unknowns. Efficiencies of up to 96% are achieved for 20 processors. Concurrent with our work, Worley et al. at Oakridge <ref> [35] </ref> solve the shallow water model in one plane on a Intel iPSC/860 with 128 nodes, using parallelization for the Laplace transformation and the aforementioned rotational data exchange. Efficiencies of :31 and :35 are attained for T21 on 16 processors and for T42 on 32 processors.
Reference: [36] <author> D.W. Walker, P.H. Worley, and J. B. Drake. </author> <title> Parallelizing the spectral transform method - part ii. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 4(7) </volume> <pages> 509-531, </pages> <month> October </month> <year> 1992. </year> <month> 23 </month>
Reference-contexts: Efficiencies of :31 and :35 are attained for T21 on 16 processors and for T42 on 32 processors. Initial results are limited to using up to a maximum of 32 processors for the T42 model. These restrictions are removed in <ref> [36] </ref>, where additional parallelism is attained by parallelizing the FFT computations performed across the data in vertical atmospheric patches. This mixed parallelization approach results in a composite efficiency of about 0:1 for T21 on 64 processors and of about 0:15 for T42 on 128 processors.
Reference: [37] <author> Greg Eisenhauer, Weiming Gu, Karsten Schwan, and Niru Mallavarupu. </author> <title> Falcon toward interactive parallel programs: The on-line steering of a molecular dynamics application. </title> <booktitle> In Third IEEE International Symposium on High-Performance Distributed Computing (HPDC3), </booktitle> <pages> pages 26-34. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1994. </year> <note> also available as technical report GIT-CC-94-08, </note> <institution> College of Computing, </institution> <address> Atlanta, GA. </address>
Reference-contexts: All of these processes may be performed and steered by human interactive or algorithmic interfaces. Our current work with a molecular dynamics application <ref> [37] </ref> and past work in real-time systems [38] has already shown that on-line program steering can improve program performance significantly, in part due to its ability to react dynamically to events like load imbalances, improper 20 data decompositions, etc. <p> The mechanisms used for this research are part of the Falcon system for on-line program monitoring and steering now being developed by our group and described in several publications <ref> [37, 31] </ref>. An overview of related work on program steering appears in [1]. We are continuing such research with this model, by controlling load imbalances and by controlling the relative speeds (and allocated computational resources) with which model input, model computation, and data visualization are performed.
Reference: [38] <author> T. Bihari and K. Schwan. </author> <title> Dynamic adaptation of real-time software. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(2) </volume> <pages> 143-174, </pages> <month> May </month> <year> 1991. </year> <note> Older version available from the Department of Computer and Information Science, </note> <institution> The Ohio State University, OSU-CISRC-5/88-TR, </institution> <note> newer version available from College of Computing, </note> <institution> Georgia Institute of Technology, </institution> <address> Atlanta GA, GTRC-TR-90/67. </address>
Reference-contexts: All of these processes may be performed and steered by human interactive or algorithmic interfaces. Our current work with a molecular dynamics application [37] and past work in real-time systems <ref> [38] </ref> has already shown that on-line program steering can improve program performance significantly, in part due to its ability to react dynamically to events like load imbalances, improper 20 data decompositions, etc.
Reference: [39] <author> E. Jahnke and F. Emde. </author> <title> Tables of functions. </title> <publisher> Dover Publications Inc., </publisher> <year> 1945. </year>
References-found: 39

