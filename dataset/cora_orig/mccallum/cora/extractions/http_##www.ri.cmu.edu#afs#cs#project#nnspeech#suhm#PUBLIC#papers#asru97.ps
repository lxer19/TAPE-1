URL: http://www.ri.cmu.edu/afs/cs/project/nnspeech/suhm/PUBLIC/papers/asru97.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs/project/nnspeech/suhm/Mosaic/Pub/home_page.html
Root-URL: 
Email: bsuhm@cs.cmu.edu  
Title: Empirical Evaluation of Interactive Multimodal Error Correction  
Author: Bernhard Suhm 
Address: (Germany)  
Affiliation: Interactive Systems Laboratories Carnegie Mellon University, Pittsburgh (USA) and Karlsruhe University  
Abstract: Commercial dictation systems for continues speech have recently become available. Although they generally received positive reviews, error correction continuous to be limited to choosing from a list of alternatives, speaking or typing. We developed a set of interactive methods to correct errors without using keyboard or mouse, allowing the user to switch between the modalities continuous speech, spelling, handwriting and pen gestures. These correction methods were integrated with our large vocabulary speech recognition system to build a prototypical multimodal listening typewriter. The efficiency of different error correction methods was evaluated in a user study. The experiment compares multimodal correction with other methods available in current speech recognition applications. Results confirm the hypothesis that switching between modalities can significantly expedite corrections. Thus, state-of-the-art speech recognition technology with multimodal error correction makes it possible to input text at a faster speed than unskilled typing, including the time necessary to correct errors. In applications where a keyboard is acceptable, however, typing still remains the fastest way to correct errors for users with good typing skills. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Lai, J., and Vergo, J. MedSpeak: </author> <title> Report Creation with Continuous Speech Recognition, </title> <booktitle> Proceedings of CHI 97, </booktitle> <pages> pp. 431-438, </pages> <address> Atlanta (USA) </address>
Reference-contexts: 1. Introduction Our research focuses on the problem of designing more flexible speech user interfaces that overcome the unreliability of automatic speech recognition technology. There is evidence that baseline accuracy is the main factor determining user acceptance of speech recognition applications <ref> [1] </ref>. The ease of error correction is obviously another important factor, which has not received enough attention. We believe that developing more intuitive methods of recovering from errors will raise user tolerance of recognition errors.
Reference: [2] <author> Oviatt, S., and VanGent, R. </author> <title> Error Resolution During Multimodal Human-Computer Interaction, </title> <booktitle> Proceedings of ICSLP96, Vol.2, </booktitle> <pages> pp. 204-207, </pages> <address> Philadelphia (USA) </address>
Reference-contexts: We believe that developing more intuitive methods of recovering from errors will raise user tolerance of recognition errors. Multimodal interactive correction methods [3] allow the user to switch between different input modalities, including continuous speech, oral spelling, handwriting, hand-drawn gestures. A high-fidelity wizard-of-oz simulation <ref> [2] </ref> suggests that switching modalities for repeated errors should significantly expedite error correction and alleviate user frustration. To evaluate multimodal correction methods and to test this hypothesis, we engineered a prototypical multimodal listening typewriter.
Reference: [3] <author> Suhm, B., Myers, B., and Waibel, A. </author> <title> Interactive Recovery from Speech Recognition Errors in Speech User Interfaces, </title> <booktitle> Proceedings of ICSLP 96, Vol.2, </booktitle> <pages> pp. 861-864, </pages> <address> Yokohama (Japan) </address>
Reference-contexts: The ease of error correction is obviously another important factor, which has not received enough attention. We believe that developing more intuitive methods of recovering from errors will raise user tolerance of recognition errors. Multimodal interactive correction methods <ref> [3] </ref> allow the user to switch between different input modalities, including continuous speech, oral spelling, handwriting, hand-drawn gestures. A high-fidelity wizard-of-oz simulation [2] suggests that switching modalities for repeated errors should significantly expedite error correction and alleviate user frustration. <p> The size of the effect is surprising since we employed a technique of adapting the language model context at the beginning and ending of a sentence to the current repair, which has been shown to improve accuracy of speech repairs significantly <ref> [3] </ref>. Recognizing speech repairs is difficult because the words were misrecognized on the first trial, and because they are hyperarticulated. Although the magnitude of the performance degradation probably depends on the used continuous speech recognizer, we are confident the problem is present in all state-of-the art systems.
Reference: [4] <author> Suhm, B., and Waibel, A. </author> <title> Exploiting Repair Context in Interactive Error Recovery, </title> <booktitle> Proceedings of EUROSPEECH 97, </booktitle> <volume> Vol. 3, </volume> <pages> pp. 1659-1662, </pages> <address> Rhodes (Greece) </address>
Reference-contexts: The pen gestures employed to perform simple editing tasks (e.g., deleting or placing the cursor) are similar to those used by text editing professionals. We had to define a new pen gesture for selecting letters within a word. To apply the concept of exploiting repair context <ref> [4] </ref> to partial word corrections, constraints on the word level are exploited in the following way. After letters within a word have been deleted or selected, decoding of the next repair input is limited to all words that complete the word fragment to a word within the vocabulary.
Reference: [5] <author> Baber, C., and Hone, </author> <title> K.S. Modeling error recovery and repair in automatic speech recognition, </title> <journal> Int. J. Man-Machine Studies, </journal> <volume> Vol. 39, pp.495-515, </volume> <year> 1993 </year>
Reference-contexts: As the pronunciation of parts of a word in general is not intuitive, we exclude speech as input modality for partial word correction, limiting it to spelling orally and handwriting. 3. The Experiment 3.1 Evaluation Measures Early work of Baber et al. on modeling error correction <ref> [5] </ref> pointed out that correction techniques are difficult to compare because their performance is closely related to their implementation. A systematic evaluation framework for error correction therefore must initially define measures that overcome the dependence on implementation.
Reference: [6] <author> Rogina, I., and Waibel, A. </author> <title> The JANUS Speech Recognizer, </title> <booktitle> ARPA Workshop on Spoken Language Technology, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 166-169, </pages> <year> 1995 </year>
Reference-contexts: Spell only X X X Write only X X X Free Choice X X X X X Free Choice PWC X X X X X X Emacs-like X Table 1 : Experimental Conditions (shown as rows) The prototypical multimodal listening typewriter uses the JANUS continuous speech recognizer trained on WSJ <ref> [6] </ref>, the connected letter recognizer NSpell [7] and the online cursive handwriting recognizer NPen++ [8], all with the standard 20,000 vocabulary from the November 1995 Hub 1 WSJ evaluation.
Reference: [7] <author> Betz, M., Hild, H., and Waibel, A. </author> <title> Recogntion of Spelled Names over the Telephone, </title> <booktitle> Proceedings of ICSLP 96, </booktitle> <address> Philadelphia (USA) </address>
Reference-contexts: only X X X Free Choice X X X X X Free Choice PWC X X X X X X Emacs-like X Table 1 : Experimental Conditions (shown as rows) The prototypical multimodal listening typewriter uses the JANUS continuous speech recognizer trained on WSJ [6], the connected letter recognizer NSpell <ref> [7] </ref> and the online cursive handwriting recognizer NPen++ [8], all with the standard 20,000 vocabulary from the November 1995 Hub 1 WSJ evaluation.
Reference: [8] <author> Manke, S., Finke, M., and Waibel, A. Npen++: </author> <title> A Writer Independent, Large Vocabulary OnLine Cursive Handwriting Recognition System, </title> <booktitle> Proceedings of Int. Conf. On Document Analysis and Recognition, </booktitle> <address> Montreal, </address> <year> 1995 </year>
Reference-contexts: X X X Free Choice PWC X X X X X X Emacs-like X Table 1 : Experimental Conditions (shown as rows) The prototypical multimodal listening typewriter uses the JANUS continuous speech recognizer trained on WSJ [6], the connected letter recognizer NSpell [7] and the online cursive handwriting recognizer NPen++ <ref> [8] </ref>, all with the standard 20,000 vocabulary from the November 1995 Hub 1 WSJ evaluation.
Reference: [9] <author> Rubine, D. </author> <title> Specifying Gestures by Example, </title> <journal> ACM Joural on Computer Graphics, </journal> <volume> Vol. 25, No. 4, </volume> <pages> pp. 329-337, </pages> <month> July </month> <year> 1991 </year>
Reference: [10] <author> Gould, J.D. </author> <title> How Experts Dictate, </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> Vol. 4 (4), </volume> <pages> pp. 648-661, </pages> <year> 1978 </year>
Reference-contexts: However, the usefulness of an automatic dictation system for text composition tasks remains unclear since for such tasks, not input speed, but composition skill has shown to be the main limiting factor <ref> [10] </ref>. In future work, we will perform a more extended user study using a touchscreen that is significantly easier to handle.
References-found: 10

