URL: http://www.cs.princeton.edu/~liv/papers/mp-aurc.ps
Refering-URL: http://www.cs.princeton.edu/~liv/papers/papers.html
Root-URL: http://www.cs.princeton.edu
Email: fbilas, jpsg@cs.princeton.edu, iftode@cs.rutgers.edu  
Phone: 2  
Title: Evaluation of Hardware Support for Next-Generation Shared Virtual Memory Clusters  
Author: Angelos Bilas Liviu Iftode and Jaswinder Pal Singh 
Address: Princeton, NJ 08544  Piscataway, NJ 08855  
Affiliation: 1 Department of Computer Science, Princeton University  Department of Computer Science, Rutgers University  
Abstract: Clusters of symmetric multiprocessors (SMPs), connected by commodity system-area networks (SANs) and interfaces are fast being adopted as platforms for parallel computing. A lot of research is being done in supporting a coherent shared address space abstraction on these clusters using page-grained shared virtual memory protocols as well as fine-grained software protocols. A key question for such systems is how much and what kind of hardware support is particularly useful for improving the performance of software coherence protocols and bringing it closer to that of hardware coherence. This paper examines one type of hardware support for page-grained software Shared Virtual Memory (SVM) systems, namely support for hardware propagation of writes to remote memories. This is the most popular form of hardware support proposed and used so far, supported in the form of automatic update (AU) by the SHRIMP network interface, remote writes by the Telegraphos communication architecture and explicit propagation by DEC Memory Channel interface. This paper examines several important issues related to the design and performance of automatic update mechanisms for next-generation clusters: 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Bianchini, L. Kontothanassis, R. Pinto, M. D. Maria, M. Abud, and C. Amorim. </author> <title> Hiding communication latency and coherence overhead in software dsms. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Obviously f N 2 <ref> [0; 1] </ref>. Protocol efficiency is the ratio of total compute and local stall time of all processors over the total elapsed time of all processors. Note that the sequential execution time is not involved in this definition. <p> HLRC and LRC have been compared for some applications on the Intel Paragon in [27]. The problem of write through caches is touched upon there and in [20], in the context of different SVM protocols and uniprocessor systems. Bianchini et al. in <ref> [1] </ref> propose diffing in hardware as another form of hardware support for SVM. They assume write through caches and uniprocessor nodes, making this work applicable to that context as well. An alternative type of hardware support for SVM can be found in [1]. <p> Bianchini et al. in <ref> [1] </ref> propose diffing in hardware as another form of hardware support for SVM. They assume write through caches and uniprocessor nodes, making this work applicable to that context as well. An alternative type of hardware support for SVM can be found in [1]. A dedicated protocol processor performs diffs on-the-fly and offloads overheads from the computation processor. Their simulations show that using the coprocessor can double the performance of TreadMarks [17] on a 16-node configuration. Holt et al. in [10] present a metric similar to protocol efficiency.
Reference: [2] <author> A. Bilas, L. Iftode, D. Martin, and J. Singh. </author> <title> Shared virtual memory across SMP nodes using automatic update: Protocols and performance. </title> <type> Technical Report TR-517-96, </type> <institution> Princeton, NJ, </institution> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: Finally, an important trend in building clusters is to use commodity SMPs rather than uniprocessors as the building blocks. Previous work has shown that using SMP nodes as building blocks can improve performance of applications compared to using uniprocessor nodes <ref> [6, 15, 2, 22] </ref>.
Reference: [3] <author> A. Bilas, L. Iftode, R. Samanta, and J. P. Singh. </author> <title> Supporting a coherent shared address space across SMP nodes: An application-driven investigation. </title> <booktitle> In IMA Workshop on Parallel Algorithms and Parallel Systems, </booktitle> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: The performance improvement in the other applications, which do not suffer from very high AU traffic, is marginal. Customized NIs make AU-based SVM more attractive for some applications. 9 Related Work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [6, 15, 8, 22, 3] </ref> for different protocols. We focus here on studies that propose or evaluate hardware support.
Reference: [4] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A virtual memory mapped network interface for the shrimp multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: The most popular form of hardware support used so far is the hardware propagation of fine-grained writes to remote memories <ref> [4, 16, 9] </ref>. This support inspired the design of a family of new, "home-based" protocols for page-based software shared virtual memory (SVM), which differ from earlier all-software protocols not only in hardware support but also in the manner in which they propagate changes and solve the multiple-writer problem. <p> This cost must be paid in these commodity NIs proposed for use in next-generation systems. In Section 8, we will examine the impact of using a customized NI that greatly reduces the cost of packet setup, such as that used in the SHRIMP system <ref> [4] </ref>. Packets are delivered directly to memory, without processor intervention at the receive side. The packet size in the network is 256 bytes for DMA transfers and 4 bytes (1 word) for AU transfers. <p> We also compared the protocols under consideration in a system with uniprocessor nodes. We find that the conclusions from the SMP node configuration hold in this case as well. 8 Customized Network Interfaces We also examined the impact of customized network interfaces like the one in SHRIMP <ref> [4] </ref> on AU based protocols. A customized NI allows for much shorter packet preparation overheads since it consists of a dedicated state machine instead of a more general purpose processor on commodity oriented interfaces.
Reference: [5] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Ku-lawik, C. L. Seitz, J. N. Seizovic, and W.-K. Su. Myrinet: </author> <title> A gigabit-per-second local area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Given these results for all-software and hardware-supported home-based protocols, it is important to compare the two in a consistent framework to determine the value of the hardware support itself. Another important development is the appearance of efficient commodity NIs such as Myrinet <ref> [5] </ref> in the marketplace. How beneficial AU support will be with these NIs (which will still require snooping support for AU) is unclear. Finally, an important trend in building clusters is to use commodity SMPs rather than uniprocessors as the building blocks. <p> The simulated architecture (Figure 1) assumes a cluster of c-processor SMPs connected with a commodity interconnect like Myrinet <ref> [5] </ref>. Contention is modeled at all levels except in the network links. The processor is P6-like, but is assumed to be a 1 IPC processor.
Reference: [6] <author> A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software versus hardware shared-memory implementation: A case study. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Finally, an important trend in building clusters is to use commodity SMPs rather than uniprocessors as the building blocks. Previous work has shown that using SMP nodes as building blocks can improve performance of applications compared to using uniprocessor nodes <ref> [6, 15, 2, 22] </ref>. <p> The performance improvement in the other applications, which do not suffer from very high AU traffic, is marginal. Customized NIs make AU-based SVM more attractive for some applications. 9 Related Work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [6, 15, 8, 22, 3] </ref> for different protocols. We focus here on studies that propose or evaluate hardware support.
Reference: [7] <author> C. Dubnicki, A. Bilas, K. Li, and J. Philbin. </author> <title> Design and implementation of Virtual Memory-Mapped Communication on Myrinet. </title> <booktitle> In Proceedings of the 1997 International Parallel Processing Symposium, </booktitle> <pages> pages 388-396, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: Each network interface (NI) has two 1 MByte memory queues for incoming and outgoing packets. Network links operate at processor speed and are 16 bits wide. We assume a fast messaging system <ref> [7] </ref> that supports explicit messages. Initiating a message takes on the order of tens of I/O bus cycles. If the network queues fill, the NI interrupts the main processor and delays it to allow queues to drain. A snooping device on the memory bus forwards AU traffic to the NI.
Reference: [8] <author> A. Erlichson, N. Nuckolls, G. Chesson, and J. Hennessy. SoftFLASH: </author> <title> analyzing the performance of clustered distributed virtual shared memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 210-220, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: The performance improvement in the other applications, which do not suffer from very high AU traffic, is marginal. Customized NIs make AU-based SVM more attractive for some applications. 9 Related Work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [6, 15, 8, 22, 3] </ref> for different protocols. We focus here on studies that propose or evaluate hardware support.
Reference: [9] <author> R. Gillett, M. Collins, and D. Pimm. </author> <title> Overview of network memory channel for PCI. </title> <booktitle> In Proceedings of the IEEE Spring COMPCON '96, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: The most popular form of hardware support used so far is the hardware propagation of fine-grained writes to remote memories <ref> [4, 16, 9] </ref>. This support inspired the design of a family of new, "home-based" protocols for page-based software shared virtual memory (SVM), which differ from earlier all-software protocols not only in hardware support but also in the manner in which they propagate changes and solve the multiple-writer problem.
Reference: [10] <author> C. Holt, M. Heinrich, J. P. Singh, , and J. L. Hennessy. </author> <title> The effects of latency and occupancy on the performance of dsm multiprocessors. </title> <type> Technical Report CSL-TR-95-xxx, </type> <institution> Stanford University, </institution> <year> 1995. </year>
Reference-contexts: An alternative type of hardware support for SVM can be found in [1]. A dedicated protocol processor performs diffs on-the-fly and offloads overheads from the computation processor. Their simulations show that using the coprocessor can double the performance of TreadMarks [17] on a 16-node configuration. Holt et al. in <ref> [10] </ref> present a metric similar to protocol efficiency. They use it to characterize the performance of applications on a hardware cache coherent machine. 10 Summary and Conclusions In this work we discuss the issue of automatic update support for SVM in the context of SMP systems.
Reference: [11] <author> L. Iftode, C. Dubnicki, E. W. Felten, and K. Li. </author> <title> Improving release-consistent shared virtual memory using automatic update. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Essentially, every shared page has a home node, and writes observed to a page are automatically propagated to the home at a fine granularity in hardware <ref> [11, 13, 19] </ref>. Shared pages are mapped write-through in the caches so that writes appear on the memory bus. When a node incurs a page fault, the page fault handler retrieves the page from the home where it is guaranteed to be up to date. <p> In WB-AURC changes are propagated to remote homes either when modified lines are 1 Pages at the home can either be mapped write-through, as assumed in <ref> [11] </ref>, or can be mapped write-back since coherent DMA will provide the latest data to an incoming page fetch request; we assume the latter here since it performs better especially with SMPs). replaced or by flushing modified cache lines at release time 2 .
Reference: [12] <author> L. Iftode, J. Singh, and K. Li. </author> <title> Scope consistency: a bridge between release consistency and entry consistency. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: So we need to develop AU mechanisms that work with write-back caches and can be used to design AURC protocols. Recently, inspired by the hardware-supported protocols, all-software versions of the home based protocols have also been developed and demonstrated to perform well <ref> [12] </ref>. The result is a protocol very much like the hardware-supported AURC, except that propagation of changes to the home is done either at release or acquire time rather than at the time of the writes themselves.
Reference: [13] <author> L. Iftode, J. P. Singh, and K. Li. </author> <title> Understanding application performance on shared virtual memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Essentially, every shared page has a home node, and writes observed to a page are automatically propagated to the home at a fine granularity in hardware <ref> [11, 13, 19] </ref>. Shared pages are mapped write-through in the caches so that writes appear on the memory bus. When a node incurs a page fault, the page fault handler retrieves the page from the home where it is guaranteed to be up to date. <p> Alternative definitions are also possible. 4 Applications We use the SPLASH-2 [26] application suite in our evaluation. A more detailed classification and description of the application behavior for SVM systems with uniprocessor nodes is provided in the context of AURC and LRC in <ref> [13] </ref>. Here we describe only the characteristics of greatest relevance to this study. The applications can be divided in two groups, regular and irregular. The regular applications are FFT, LU and Ocean. <p> Thus, in AURC we do not need to use a write through cache policy, and in HLRC we do not need to compute diffs. Protocol action is required only to fetch pages. The applications have different inherent and induced communication patterns <ref> [26, 13] </ref>, which affect their overall performance and the impact on SMP nodes, but we should expect the different protocols to perform very similarly on the best versions of these applications (the so-called contiguous versions). The irregular applications in our suite are Barnes, Radix, Raytrace, Volrend and Water. <p> We will separate these out too, calling the former wait time and the latter the protocol cost. Idle time for locks is often increased greatly in SVM systems due to page misses occurring frequently inside critical sections and serialization <ref> [13] </ref>. 6 Automatic update support with write-back caches This section presents a new protocol (WB-AURC) that provides AU support with write-back caches, and its differences from AURC (uniprocessor and SMP versions), answering the feasibility part of the question raised in the introduction. <p> We focus here on studies that propose or evaluate hardware support. Our study on uniprocessor systems (for which we do not present results due to space limitations) is consistent with the results obtained in <ref> [13] </ref> for their "slow bus" case, although we compare AURC against HLRC instead of LRC. This paper examines a system with SMP nodes, where nontrivial protocol extensions are necessary to support multiple processors per node and also a third approach (WB-AURC) is investigated as well.
Reference: [14] <author> D. Jiang, H. Shan, and J. P. Singh. </author> <title> Application restructuring and performance portability across shared virtual memory and hardware-coherent multiprocessors. </title> <booktitle> In Proceedings of the 6th ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Volrend (Figure 5): Data traffic and wait time are not very substantial in Volrend. AURC does very well. The only problem is created from imbalances in the compute time due to the high overhead of task stealing <ref> [14] </ref>. HLRC performs worse that AURC since it makes the locks that are used for task stealing even more expensive due to diffs. Also lock times are imbalanced due to the different numbers of local and remote locks acquired by each processor. The behavior of WB-AURC is similar to AURC.
Reference: [15] <author> M. Karlsson and P. Stenstrom. </author> <title> Performance evaluation of cluster-based multiprocessor built from atm switches and bus-based multiprocessor servers. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Finally, an important trend in building clusters is to use commodity SMPs rather than uniprocessors as the building blocks. Previous work has shown that using SMP nodes as building blocks can improve performance of applications compared to using uniprocessor nodes <ref> [6, 15, 2, 22] </ref>. <p> The performance improvement in the other applications, which do not suffer from very high AU traffic, is marginal. Customized NIs make AU-based SVM more attractive for some applications. 9 Related Work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [6, 15, 8, 22, 3] </ref> for different protocols. We focus here on studies that propose or evaluate hardware support.
Reference: [16] <author> M. G. H. Katevenis, E. P. Markatos, G. Kalokerinos, and A. Dollas. Telegraphos: </author> <title> A substrate for high-performance computing on workstation clusters. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 43(2) </volume> <pages> 94-108, </pages> <month> 15 June </month> <year> 1997. </year>
Reference-contexts: The most popular form of hardware support used so far is the hardware propagation of fine-grained writes to remote memories <ref> [4, 16, 9] </ref>. This support inspired the design of a family of new, "home-based" protocols for page-based software shared virtual memory (SVM), which differ from earlier all-software protocols not only in hardware support but also in the manner in which they propagate changes and solve the multiple-writer problem.
Reference: [17] <author> P. Keleher, A. Cox, S. Dwarkadas, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: An alternative type of hardware support for SVM can be found in [1]. A dedicated protocol processor performs diffs on-the-fly and offloads overheads from the computation processor. Their simulations show that using the coprocessor can double the performance of TreadMarks <ref> [17] </ref> on a 16-node configuration. Holt et al. in [10] present a metric similar to protocol efficiency.
Reference: [18] <author> P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Lazy consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: When a node incurs a page fault, the page fault handler retrieves the page from the home where it is guaranteed to be up to date. Data are kept consistent according to a page-based software consistency protocol such as lazy release consistency <ref> [18] </ref>. Thus, consistency in maintained at page granularity, while there is some hardware support for fine-grained communication. The SHRIMP system provides both the write snooping hardware as well as a customized network interface (NI) to support automatic update.
Reference: [19] <author> L. I. Kontothanassis, G. Hunt, R. Stets, N. Hardavellas, M. Cierniak, S. Parthasarathy, W. Meira, Jr., S. Dwarkadas, and M. L. Scott. </author> <title> VM-based shared memory on low-latency, remote-memory-access networks. </title> <booktitle> In Proc. of the 24th Annual Int'l Symp. on Computer Architecture (ISCA'97), </booktitle> <pages> pages 157-169, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Essentially, every shared page has a home node, and writes observed to a page are automatically propagated to the home at a fine granularity in hardware <ref> [11, 13, 19] </ref>. Shared pages are mapped write-through in the caches so that writes appear on the memory bus. When a node incurs a page fault, the page fault handler retrieves the page from the home where it is guaranteed to be up to date.
Reference: [20] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> High performance software coherence for current and future architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: HLRC and LRC have been compared for some applications on the Intel Paragon in [27]. The problem of write through caches is touched upon there and in <ref> [20] </ref>, in the context of different SVM protocols and uniprocessor systems. Bianchini et al. in [1] propose diffing in hardware as another form of hardware support for SVM. They assume write through caches and uniprocessor nodes, making this work applicable to that context as well.
Reference: [21] <author> R. Samanta, A. Bilas, L. Iftode, and J. P. Singh. </author> <title> Home-based svm protocols for smp clusters: Design, simulations, implementation and performance. </title> <booktitle> In Proceedings of the 4th International Symposium on High Performance Computer Architecture, </booktitle> <address> Las Vegas, </address> <month> February </month> <year> 1998. </year>
Reference-contexts: We present results for three forms of an SVM protocol based on lazy release consistency (LRC). AURC, HLRC and WB-AURC (Write Back based AURC) 3 . Details about extending these protocols for systems with SMP nodes, can be found in <ref> [21] </ref>. In implementing the protocols, we try to keep basic aspects fixed across protocols.
Reference: [22] <author> D. Scales, K. Gharachorloo, and C. Thekkath. </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grain shared memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Finally, an important trend in building clusters is to use commodity SMPs rather than uniprocessors as the building blocks. Previous work has shown that using SMP nodes as building blocks can improve performance of applications compared to using uniprocessor nodes <ref> [6, 15, 2, 22] </ref>. <p> The performance improvement in the other applications, which do not suffer from very high AU traffic, is marginal. Customized NIs make AU-based SVM more attractive for some applications. 9 Related Work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [6, 15, 8, 22, 3] </ref> for different protocols. We focus here on studies that propose or evaluate hardware support.
Reference: [23] <author> A. Sharma, A. T. Nguyen, J. Torellas, M. Michael, and J. Carbajal. Augmint: </author> <title> a multiprocessor simulation environment for Intel x86 architectures. </title> <type> Technical report, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: NIs customized for AU packet generation can make the performance benefits larger in applications that suffer from raw traffic problems. Customized NIs may be important to using automatic update effectively. 2 Simulated Platforms The simulation environment we use is built on top of aug-mint <ref> [23] </ref>, an execution driven simulator using the x86 instruction set and runs on x86 systems. The simulated architecture (Figure 1) assumes a cluster of c-processor SMPs connected with a commodity interconnect like Myrinet [5]. Contention is modeled at all levels except in the network links.
Reference: [24] <author> K. Skadron and D. W. Clark. </author> <title> Design issues and tradeoffs for write buffers. </title> <booktitle> In The 3nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb </month> <year> 1997. </year>
Reference-contexts: The processor is P6-like, but is assumed to be a 1 IPC processor. The data cache hierarchy consists of a 8 KBytes first-level direct mapped write-through cache and a 512 KBytes second-level two-way set associative cache, each with a line size of 32 Bytes. The write buffer <ref> [24] </ref> has 26 entries, 1 cache line wide each, and a retire-at-4 policy. Write buffer stalls are simulated. The read hit cost is one cycle in the write buffer and first level cache and 10 cycles in the second-level cache. The memory subsystem is fully pipelined.
Reference: [25] <author> D. Stodolsky, J. B. Chen, and B. Bershad. </author> <title> Fast interrupt priority management in operating system kernels. </title> <booktitle> In USENIX Association, editor, Proceedings of the USENIX Symposium on Microkernels and Other Kernel Architectures: </booktitle> <address> Septem-ber 20-21, 1993, San Diego, California, USA, </address> <pages> pages 105-110, </pages> <address> Berkeley, CA, USA, Sept. 1993. </address> <publisher> USENIX. </publisher>
Reference-contexts: AU writes within a cache line are combined in the network interface to reduce the number of packets. Issuing an Interprocessor Interrupt (IPI) costs 500 processor cycles, and invoking the handler is another 500 cycles. This is aggressive compared to what current operating systems provide, but is implementable <ref> [25] </ref> and prevents interrupt cost from swamping out the effects of other system parameters. The page size is 4 KBytes, and the cost to access the TLB from a handler running in the kernel is 50 processor cycles.
Reference: [26] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> Methodological considerations and characterization of the SPLASH-2 parallel application suite. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Protocol efficiency is the ratio of total compute and local stall time of all processors over the total elapsed time of all processors. Note that the sequential execution time is not involved in this definition. Alternative definitions are also possible. 4 Applications We use the SPLASH-2 <ref> [26] </ref> application suite in our evaluation. A more detailed classification and description of the application behavior for SVM systems with uniprocessor nodes is provided in the context of AURC and LRC in [13]. Here we describe only the characteristics of greatest relevance to this study. <p> Thus, in AURC we do not need to use a write through cache policy, and in HLRC we do not need to compute diffs. Protocol action is required only to fetch pages. The applications have different inherent and induced communication patterns <ref> [26, 13] </ref>, which affect their overall performance and the impact on SMP nodes, but we should expect the different protocols to perform very similarly on the best versions of these applications (the so-called contiguous versions). The irregular applications in our suite are Barnes, Radix, Raytrace, Volrend and Water.
Reference: [27] <author> Y. Zhou, L. Iftode, and K. Li. </author> <title> Performance evaluation of two home-based lazy release consistency protocols for shared virtual memory systems. </title> <booktitle> In Proceedings of the Operating Systems Design and Implementation Symposium, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: The result is a protocol very much like the hardware-supported AURC, except that propagation of changes to the home is done either at release or acquire time rather than at the time of the writes themselves. In an evaluation on the Intel Paragon multiprocessor <ref> [27] </ref>, this software home-based protocol (called HLRC) was also found to outperform earlier distributed all-software protocols. Given these results for all-software and hardware-supported home-based protocols, it is important to compare the two in a consistent framework to determine the value of the hardware support itself. <p> This paper examines a system with SMP nodes, where nontrivial protocol extensions are necessary to support multiple processors per node and also a third approach (WB-AURC) is investigated as well. HLRC and LRC have been compared for some applications on the Intel Paragon in <ref> [27] </ref>. The problem of write through caches is touched upon there and in [20], in the context of different SVM protocols and uniprocessor systems. Bianchini et al. in [1] propose diffing in hardware as another form of hardware support for SVM.
References-found: 27

