URL: ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/SPS-98.ps
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Email: rich@cs.umass.edu  dprecup@cs.umass.edu  baveja@cs.colorado.edu  
Title: Between MDPs and Semi-MDPs: Learning, Planning, and Representing Knowledge at Multiple Temporal Scales  
Author: Richard S. Sutton Doina Precup Satinder Singh 
Address: Amherst, MA 01003 USA  Boulder, CO 80309 USA  
Affiliation: University of Massachusetts,  University of Colorado,  
Note: Journal of Artificial Intelligence Research 1 (1998) 1-39 Submitted 3/98; published NOT  
Abstract: Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key challenges for AI. In this paper we develop an approach to these problems based on the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action to include options|whole courses of behavior that may be temporally extended, stochastic, and contingent on events. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Options may be given a priori, learned by experience, or both. They may be used interchangeably with actions in a variety of planning and learning methods. The theory of semi-Markov decision processes (SMDPs) can be applied to model the consequences of options and as a basis for planning and learning methods using them. In this paper we develop these connections, building on prior work by Bradtke and Duff (1995), Parr (in prep.) and others. Our main novel results concern the interface between the MDP and SMDP levels of analysis. We show how a set of options can be altered by changing only their termination conditions to improve over SMDP methods with no additional cost. We also introduce intra-option temporal-difference methods that are able to learn from fragments of an option's execution. Finally, we propose a notion of subgoal which can be used to improve the options themselves. Overall, we argue that options and their models provide hitherto missing aspects of a powerful, clear, and expressive framework for representing and organizing knowledge.
Abstract-found: 1
Intro-found: 1
Reference: <author> Araujo, E.G., Grupen, R.A. </author> <year> (1996). </year> <title> Learning control composition in a complex environment. </title> <booktitle> Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pp. 333-342. </pages>
Reference: <author> Asada, M., Noda, S., Tawaratsumida, S., Hosada, K. </author> <year> (1996). </year> <title> Purposive behavior acquisition for a real robot by vision-based reinforcement learning. </title> <booktitle> Machine Learning 23 </booktitle> <pages> 279-303. </pages>
Reference: <author> Barto, A.G., Bradtke, S.J., Singh, S.P. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic programming. </title> <booktitle> Artificial Intelligence 72 </booktitle> <pages> 81-138. </pages>
Reference: <author> Boutilier, C., Brafman, R.I., Geib, C. </author> <year> (1997). </year> <title> Prioritized goal Decomposition of Markov decision processes: Toward a synthesis of classical and decision theoretic planning. </title> <booktitle> Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1165-1162. </pages>
Reference: <author> Bradtke, S.J., and Duff, M.O. </author> <year> (1995). </year> <title> Reinforcement learning methods for continuous-time Markov decision problems. </title> <booktitle> Advances in Neural Information Processing Systems 8 </booktitle> <pages> 393-400. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: When the execution of option o is started in state s, we next jump to the state s 0 in which o terminates. Based on this experience, an approximate option-value function Q (s; o) is updated. For example, the SMDP version of one-step Q-learning <ref> (Bradtke and Duff, 1995) </ref>, which we call SMDP Q-learning, updates after each option termination by Q (s; o) Q (s; o) + ff r + fl k max Q (s 0 ; a) Q (s; o) ; where k denotes the number of time steps elapsing between s and s 0
Reference: <author> Brafman, R.I., Moshe, T. </author> <year> (1997). </year> <title> Modeling agents as qualitative decision makers. </title> <booktitle> Artificial Intelligence 94(1) </booktitle> <pages> 217-268. </pages>
Reference: <author> Brockett, R.W. </author> <year> (1993). </year> <title> Hybrid models for motion control systems. </title> <booktitle> In Essays in Control: Perspectives in the Theory and and its Applications, </booktitle> <pages> pp. 29-53. </pages> <publisher> Birkhauser, </publisher> <address> Boston. </address> <note> 34 Between MDPs and Semi-MDPs (Submitted) Brooks, </note> <author> R. </author> <year> (1986). </year> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE journal of Robotics and Automation, </journal> <pages> 14-23. </pages>
Reference: <author> Chrisman, L. </author> <year> (1994). </year> <title> Reasoning about probabilistic actions at multiple levels of granularity, </title> <booktitle> AAAI Spring Symposium: Decision-Theoretic Planning, </booktitle> <publisher> Stanford University. </publisher>
Reference: <author> Colombetti, M., Dorigo, M., Borghi, G. </author> <year> (1996). </year> <title> Behavior analysis and training: A methodology for behavior engineering. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics-Part B 26(3) </journal> <note> 365-380 Crites, R.H., </note> <author> and Barto, A.G. </author> <year> (1996). </year> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> Advances in Neural Information Processing Systems 9 </booktitle> <pages> 1017-1023. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Dayan, P. </author> <year> (1993). </year> <title> Improving generalization for temporal difference learning: </title> <booktitle> The successor representation. Neural Computation 5 </booktitle> <pages> 613-624. </pages>
Reference: <author> Dayan, P., Hinton, G.E. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <booktitle> Advances in Neural Information Processing Systems 5 </booktitle> <pages> 271-278. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <editor> de Kleer, J., Brown, J.S. </editor> <year> (1984). </year> <title> A qualitative physics based on confluences. </title> <journal> Artificial Intelligence 24(1-3):7-83. </journal>
Reference: <author> Dean, T., Kaelbling, L.P., Kirman, J., Nicholson, A. </author> <year> (1995). </year> <title> Planning under time constraints in stochastic domains. </title> <booktitle> Artificial Intelligence 76(1-2): </booktitle> <pages> 35-74. </pages>
Reference-contexts: We call this the value achievement property of planning with options. This contrasts with planning methods that abstract over state space, which generally cannot be guaranteed to achieve their planned values even if their models are correct <ref> (e.g., Dean and Lin, 1995) </ref>. As a simple illustration of planning with options, consider the rooms example, a grid-world environment of four rooms shown in Figure 2. The cells of the grid correspond to the states of the environment.
Reference: <author> Dean, T., Lin, S.-H. </author> <year> (1995). </year> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1121-1127. </pages> <note> Morgan Kaufmann. See also Technical Report CS-95-10, </note> <institution> Brown University, Department of Computer Science, </institution> <year> 1995. </year>
Reference-contexts: We call this the value achievement property of planning with options. This contrasts with planning methods that abstract over state space, which generally cannot be guaranteed to achieve their planned values even if their models are correct <ref> (e.g., Dean and Lin, 1995) </ref>. As a simple illustration of planning with options, consider the rooms example, a grid-world environment of four rooms shown in Figure 2. The cells of the grid correspond to the states of the environment.
Reference: <author> Dejong, </author> <title> G.F. (1994). Learning to plan in continuous domains. </title> <booktitle> Artificial Intelligence 65 </booktitle> <pages> 71-141. </pages>
Reference: <author> Dietterich, T.G. </author> <year> (1997). </year> <title> Hierarchical reinforcement learning with the MAXQ value function decomposition. </title> <type> Technical Report, </type> <institution> Department of Computer Science, Oregon State University. </institution>
Reference: <author> Dorigo, M., Colombetti, M. </author> <year> (1994). </year> <title> Robot shaping: Developing autonomous agents through learning. </title> <booktitle> Artificial Intelligence 71 </booktitle> <pages> 321-370. </pages>
Reference: <author> Drescher, G.L. </author> <year> (1991). </year> <title> Made Up Minds: A Constructivist Approach to Artificial Intelligence. </title> <publisher> MIT Press. </publisher>
Reference: <author> Drummond, C. </author> <year> (1998). </year> <title> Composing functions to speed up reinforcement learning in a changing world. </title> <booktitle> Proceedings of the Tenth European Conference on Machine Learning. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: <author> Fikes, R.E., Hart, P.E., Nilsson, N.J. </author> <year> (1972). </year> <title> Learning and executing generalized robot plans. </title> <booktitle> Artificial Intelligence 3 </booktitle> <pages> 251-288. </pages>
Reference: <author> Geffner, H., Bonet, B. </author> <title> (in preparation). High-level planning and control with incomplete information using POMDPs. 35 Sutton, </title> <editor> Precup, & Singh (Submitted) Grossman, R.L., Nerode, A., Ravn, A.P., Rischel, H. </editor> <year> (1993). </year> <title> Hybrid Systems. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference: <author> Haigh, K.Z., Shewchuk, J., Veloso, M.M. </author> <year> (1997). </year> <title> Exploring geometry in analogical route planning. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence 9 </journal> <pages> 509-541. </pages>
Reference: <author> Hansen, E. </author> <year> (1994). </year> <title> Cost-effective sensing during plan execution. </title> <booktitle> Proc. AAAI-94, </booktitle> <pages> pp. 1029-1035. </pages>
Reference: <author> Hauskrecht, M., Meuleau, N., Boutilier, C., Kaelbling, L.P., Dean, T. </author> <title> (in preparation). Hierarchical solution of Markov decision processes using macro-actions. </title>
Reference: <author> Huber, M., Grupen, R.A. </author> <year> (1997). </year> <title> A feedback control structure for on-line learning tasks. </title> <booktitle> Robotics and Autonomous Systems 22(3-4):303-315. </booktitle>
Reference: <editor> Iba, </editor> <address> G.A. </address> <year> (1989). </year> <title> A heuristic approach to the discovery of macro-operators. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 285-317. </pages>
Reference: <author> Jaakkola, T., Jordan, M.I., and Singh, S.P. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <booktitle> Neural Computation 6(6) </booktitle> <pages> 1185-1201. </pages>
Reference: <author> Kaelbling, </author> <title> L.P. (1993). Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> Proc. of the Tenth Int. Conf. on Machine Learning, </booktitle> <pages> pp. 167-173, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kalmar, Z., Szepesvari, C., Lorincz, A. </author> <year> (1997). </year> <title> Module based reinforcement learning for a real robot. </title> <booktitle> Proceedings of the Sixth European Workshop on Learning Robots, </booktitle> <pages> pp. 22-32. </pages>
Reference: <author> Kalmar, Z., Szepesvari, C., Lorincz, A. </author> <title> (in preparation). Module based reinforcement learning: Experiments with a real robot. </title>
Reference: <author> Korf, R.E. </author> <year> (1985). </year> <title> Learning to Solve Problems by Searching for Macro-Operators. </title> <address> Boston: </address> <publisher> Pitman Publishers. </publisher>
Reference: <author> Korf, R.E. </author> <year> (1987). </year> <title> Planning as search: A quantitative approach. </title> <booktitle> Artificial Intelligence 33 </booktitle> <pages> 65-88. </pages>
Reference: <author> Koza, J.R., Rice, J.P. </author> <year> (1992). </year> <title> Automatic programming of robots using genetic programming. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 194-201. </pages>
Reference: <author> Kuipers, B.J. </author> <year> (1979). </year> <title> Commonsense knowledge of space: Learning from experience. </title> <booktitle> Proc. IJCAI-79, </booktitle> <pages> pp. 499-501. </pages>
Reference: <author> Laird, J.E., Rosenbloom, P.S., Newell, A. </author> <year> (1986). </year> <title> Chunking in SOAR: The anatomy of a general learning mechanism. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 11-46. </pages>
Reference: <author> Levinson, R., Fuchs, G. </author> <year> (1994). </year> <title> A pattern-weight formulation of search knowledge. </title> <type> Technical Report UCSC-CRL-94-10, </type> <institution> University of California at Santa Cruz. </institution>
Reference: <author> Lin, L.-J. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University. </institution> <note> Technical Report CMU-CS-93-103. 36 Between MDPs and Semi-MDPs (Submitted) Maes, </note> <author> P. </author> <year> (1991). </year> <title> A bottom-up mechanism for behavior selection in an artificial creature. </title> <booktitle> Proceedings of the First International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Maes, P., Brooks, R. </author> <year> (1990). </year> <title> Learning to coordinate behaviors. </title> <booktitle> Proceedings of AAAI-90, </booktitle> <pages> pp. 796-802. </pages>
Reference: <author> Mahadevan, S., Connell, J. </author> <year> (1992). </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence 55(2-3):311-365. </journal>
Reference: <author> Mahadevan, S., Marchalleck, N., Das, T., Gosavi, A. </author> <year> (1997). </year> <title> Self-improving factory simulation using continuous-time average-reward reinforcement learning. </title> <booktitle> Proceedings of the 14th International Conference on Machine Learning. </booktitle>
Reference: <author> Marbach, P., Mihatsch, O., Schulte, M., Tsitsiklis, J.N. </author> <year> (1998). </year> <title> Reinforcement learning for call admission control in routing in integrated service networks. </title> <booktitle> Advances in Neural Information Processing Systems 10. </booktitle> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mataric, M.J. </author> <year> (1997). </year> <title> Behavior-based control: Examples from navigation, learning, and group behavior. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence 9(2-3). </journal>
Reference: <author> McGovern, A., Sutton, R.S., Fagg, A.H. </author> <year> (1997). </year> <title> Roles of macro-actions in accelerating reinforcement learning. </title> <booktitle> Proceedings of the 1997 Grace Hopper Celebration of Women in Computing. </booktitle>
Reference: <author> McGovern, A., Sutton, </author> <title> R.S., (in prep.). Roles of temporally extended actions in accelerating reinforcement learning. </title>
Reference: <author> Meuleau, N., Hauskrecht, M., Kim, K.-E., Peshkin, L., Kaelbling, L.P., Dean, T., Boutilier, C. </author> <title> (in preparation). Solving very large weakly coupled Markov decision processes. </title>
Reference: <author> Millan, J. del R. </author> <year> (1994). </year> <title> Learning reactive sequences from basic reflexes. </title> <booktitle> Proceedings of the Third International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pp. 266-274. </pages>
Reference: <author> Minton, S. </author> <year> (1988). </year> <title> Learning Search Control Knowledge: An Explanation-based Approach. </title> <publisher> Kluwer Academic. </publisher>
Reference: <author> Moore, A.W. </author> <year> (1994). </year> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional spaces, </title> <booktitle> Advances in Neural Information Processing Systems 7 </booktitle> <pages> 711-718, </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The actions are movements of 0.01 in any direction from the current state. Rather than work with these low-level actions, infinite in number, we introduce seven landmark locations in the space. For each landmark we define a controller that takes us to the landmark in a direct path <ref> (cf. Moore, 1994) </ref>. Each controller is only applicable within a limited range of states, in this case within a certain distance of the corresponding landmark.
Reference: <author> Newell, A., Simon, H.A. </author> <year> (1972). </year> <title> Human Problem Solving. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Nie, J., and Haykin, S. </author> <title> (to appear). A Q-learning based dynamic channel assignment technique for mobile communication systems. </title> <journal> IEEE Transactions on Vehicular Technology. </journal>
Reference: <author> Nilsson, N.J. </author> <year> (1973). </year> <title> Hierarchical robot planning and execution system. SRI AI Center Technical Note 76, SRI International, </title> <publisher> Inc., </publisher> <address> Menlo Park, CA. </address> <note> 37 Sutton, Precup, & Singh (Submitted) Nilsson, </note> <author> N. </author> <year> (1994). </year> <title> Teleo-reactive programs for agent control. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 139-158. </pages>
Reference: <author> Parr, R., Russell, S. </author> <year> (1998). </year> <title> Reinforcement learning with hierarchies of machines. </title> <booktitle> Advances in Neural Information Processing Systems 11. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Parr, R. </author> <title> (in preparation). Hierarchical control and learning for Markov decision processes, </title> <note> chapter 3. </note>
Reference: <author> Precup, D., Sutton, R.S. </author> <year> (1997). </year> <title> Multi-time models for reinforcement learning. </title> <booktitle> Proceedings of the ICML'97 Workshop on Modelling in Reinforcement Learning. </booktitle>
Reference-contexts: Thus, p o ss 0 is a combination of the likelihood that s 0 is the state in which o terminates together with a measure of how delayed that outcome is relative to fl. We call this kind of model a multi-time model <ref> (Precup and Sutton, 1997, 1998) </ref> because it describes the outcome of an option not at a single time but at potentially many different times, appropriately combined. 3 Using multi-time models we can write Bellman equations for general policies and options.
Reference: <author> Precup, D., Sutton, R.S. </author> <year> (1998). </year> <title> Multi-time models for temporally abstract planning. </title> <booktitle> Advances in Neural Information Processing Systems 11. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Precup, D., Sutton, R.S., Singh, S.P. </author> <year> (1997). </year> <title> Planning with closed-loop macro actions. </title> <booktitle> Working notes of the 1997 AAAI Fall Symposium on Model-directed Autonomous Systems. </booktitle>
Reference-contexts: Thus, p o ss 0 is a combination of the likelihood that s 0 is the state in which o terminates together with a measure of how delayed that outcome is relative to fl. We call this kind of model a multi-time model <ref> (Precup and Sutton, 1997, 1998) </ref> because it describes the outcome of an option not at a single time but at potentially many different times, appropriately combined. 3 Using multi-time models we can write Bellman equations for general policies and options.
Reference: <author> Precup, D., Sutton, R.S., Singh, S.P. </author> <year> (1998). </year> <title> Theoretical results on reinforcement learning with temporally abstract options. </title> <booktitle> Proceedings of the Tenth European Conference on Machine Learning. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Problems. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: SMDP (Option-to-Option) Methods Options are closely related to the actions in a special kind of decision problem known as a semi-Markov decision process, or SMDP <ref> (e.g., see Puterman, 1994) </ref>. In fact, any MDP with 2. This and other similarities suggest that the concepts of policy and option can be unified. In such a unification, options would select other options, and thus arbitrary hierarchical structures would be permitted.
Reference: <author> Rosenstein, M.T., Cohen, P.R. </author> <year> (1998). </year> <title> Concepts from time series. </title> <booktitle> Proceedings of the Fifteenth National Conference on Artificial Intelligence. </booktitle>
Reference: <author> Ring, M. </author> <year> (1991). </year> <title> Incremental development of complex behaviors through automatic construction of sensory-motor hierarchies. </title> <booktitle> Proceedings of the Eighth International Conference on Machine Learning, </booktitle> <pages> pp. 343-347, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rudy, D., Kibler, D. </author> <year> (1992). </year> <title> Learning episodes for optimization. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sacerdoti, E.D. </author> <year> (1974). </year> <title> Planning in a hierarchy of abstraction spaces. </title> <booktitle> Artificial Intelligence 5 </booktitle> <pages> 115-135. </pages>
Reference: <author> Sastry, S. </author> <year> (1997). </year> <title> Algorithms for design of hybrid systems. </title> <booktitle> Proceedings of the International Conference of Information Sciences. </booktitle>
Reference: <author> Say, A.C.C., Selahattin, K. </author> <year> (1996). </year> <title> Qualitative system identification: Deriving structure from behavior. </title> <booktitle> Artificial Intelligence 83(1) </booktitle> <pages> 75-141. </pages>
Reference: <author> Schmidhuber, J. </author> <year> (1991). </year> <title> Neural Sequence Chunkers. </title> <institution> Technische Universitat Munchen TR FKI-148-91. </institution>
Reference: <author> Simmons, R., Koenig, S. </author> <year> (1995). </year> <title> Probabilistic robot navigation in partially observable environments. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1080-1087. </pages> <note> Morgan Kaufmann. 38 Between MDPs and Semi-MDPs (Submitted) Singh, S.P. </note> <year> (1992a). </year> <title> Reinforcement learning with a hierarchy of abstract models. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 202-207. </pages> <publisher> MIT/AAAI Press. </publisher>
Reference: <author> Singh, S.P. </author> <year> (1992b). </year> <title> Scaling reinforcement learning by learning variable temporal resolution models. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. 406-415, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Singh, S.P. </author> <year> (1992c). </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <booktitle> Machine Learning </booktitle> 8(3/4):323-340. 
Reference: <author> Singh, S.P. </author> <year> (1992d). </year> <title> The efficient learning of multiple task sequences. </title> <booktitle> In Advances in Neural Information Processing Systems 4 </booktitle> <pages> 251-258, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Singh S.P., Barto A.G., Grupen R.A., Connolly C.I. </author> <year> (1994). </year> <title> Robust reinforcement learning in motion planning. </title> <booktitle> Advances in Neural Information Processing Systems 6 </booktitle> <pages> 655-662, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Singh, S.P., Bertsekas, D. </author> <year> (1997). </year> <title> Reinforcement learning for dynamic channel allocation in cellular telephone systems. </title> <booktitle> Advances in Neural Information Processing Systems 9 </booktitle> <pages> 974-980. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Sutton, R.S. </author> <year> (1995). </year> <title> TD models: Modeling the world at a mixture of time scales. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. 531-539, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, a handcrafted policy for a mobile robot to dock with its battery charger might be defined only for states I in which the battery charger 1. The termination condition fi plays a role similar to the fi in fi-models <ref> (Sutton, 1995) </ref>, but with an opposite sense. That is, fi (s) in this paper corresponds to 1 fi (s) in that earlier paper. 7 Sutton, Precup, & Singh (Submitted) is within sight. <p> The method we call one-step intra-option model learning applies these updates to every option consistent with every action taken, a t . Of course, this is just the simplest intra-option model-learning method. Others may be possible using eligibility traces and standard tricks for off-policy learning <ref> (as in Sutton, 1995) </ref>. As an illustration, consider the use of SMDP and intra-option model learning in the rooms example. As before, we assume that the eight hallway options are given, but now we assume that their models are not given and must be learned.
Reference: <author> Sutton, R.S., Barto, A.G. </author> <year> (1998). </year> <title> Reinforcement Learning: An Introduction. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Intra-option methods can even be used to learn about the model of an option without ever executing the option, as long as some selections are made that are consistent with the option. Intra-option methods are examples of off-policy learning methods <ref> (Sutton and Barto, 1998) </ref> because they learn about the consequences of one policy while actually behaving according to another, potentially different policy.
Reference: <author> Sutton, R.S., Pinette, B. </author> <year> (1985). </year> <title> The learning of world models by connectionist networks. </title> <booktitle> Proc. of the Seventh Annual Conf. of the Cognitive Science Society, </booktitle> <pages> pp. 54-64. </pages>
Reference: <author> Tenenberg, J. Karlsson, J., Whitehead, S. </author> <year> (1992). </year> <title> Learning via task decomposition. </title> <booktitle> Proc. Second Int. Conf. on the Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Tesauro, G.J. </author> <year> (1995). </year> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM 38 </journal> <pages> 58-68. </pages>
Reference: <author> Thrun, T., Schwartz, A. </author> <year> (1995). </year> <title> Finding structure in reinforcement learning. </title> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Toth, G.J., Kovacs, S., Lorincz, A. </author> <year> (1995). </year> <title> Genetic algorithm with alphabet optimization. </title> <booktitle> Biological Cybernetics 73 </booktitle> <pages> 61-68. </pages>
Reference: <author> Uchibe, M., Asada, M., Hosada, K. </author> <year> (1996). </year> <title> Behavior coordination for a mobile robot using modular reinforcement learning. </title> <booktitle> Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <pages> pp. 1329-1336. </pages>
Reference: <author> Watkins, C.J.C.H. </author> <year> (1989). </year> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University. </institution>
Reference: <author> Wiering, M., Schmidhuber, J. </author> <year> (1997). </year> <title> HQ-learning. Adaptive Behavior 6(2). </title> <type> 39 Sutton, </type> <institution> Precup, & Singh (Submitted) Wixson, L.E. </institution> <year> (1991). </year> <title> Scaling reinforcement learning techniques via modularity, </title> <booktitle> Proc. Eighth Int. Conf. on Machine Learning, </booktitle> <pages> pp. 368-372, </pages> <publisher> Morgan Kaufmann. </publisher> <pages> 40 </pages>
References-found: 80

