URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/1523.ps.gz
Refering-URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/
Root-URL: http://www.cs.tamu.edu
Email: -zhang2,torrella@cs.uiuc.edu  rwerger@cs.tamu.edu  
Title: Hardware for Speculative Run-Time Parallelization in Distributed Shared-Memory Multiprocessors  
Author: Ye Zhang Lawrence Rauchwerger and Josep Torrellas 
Keyword: scalable shared-memory multiprocessors, cache coherence protocols, run-time par-allelization, speculative execution.  
Web: http://iacoma.cs.uiuc.edu/iacoma/  
Address: IL 61801  Texas A&M University, College Station, TX 77843  
Affiliation: Computer Science Department University of Illinois at Urbana-Champaign,  Computer Science Department  
Abstract: Center for Supercomputing Research and Development Technical Report 1523 July 1997 Abstract Run-time parallelization is often the only way to generate parallel code for multiprocessors when data dependence information is incomplete at compile time. This situation is common in many important applications, where arrays are accessed with subscripted subscripts. Unfortunately, known techniques for run-time parallelization are often computationally expensive or not general enough. To address this problem, we propose a new hardware support for efficient run-time parallelization in distributed shared-memory multiprocessors (DSMs). The idea is to execute the code in parallel speculatively and use the cache coherence protocol hardware to flag any cross-iteration data dependence. Often, such dependences naturally trigger a coherence transaction, like an invalidation when a processor writes to a variable that was read by another processor. However, with appropriate extensions to the cache coherence protocol, all such dependences are detected. If a dependence is detected, execution stops, the state is restored, and the code is re-executed serially. This scheme, which we apply to loops, has low overhead and detects serial loops very quickly. We present the algorithms and a hardware design of the scheme. Overall, the scheme delivers a speedup of 7 for 16 processors and is twice faster than a related software-only scheme. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: To determine whether or not the order of the iterations affects the semantics of the loop, we need to analyze the data dependences across iterations <ref> [1] </ref> (or cross-iteration dependences). There are three possible types of data dependences, namely flow (read after write), anti (write after read), and output (write after write). If there are no anti, output, or flow dependences across iterations, the loop can be executed in parallel. <p> If this condition is true, it means that an element A [i] 1 any returns the "OR" of its vector operand's elements: any (v [1 : n]) = (v <ref> [1] </ref> _ v [2] _ : : : _ v [n]). 4 is written in one iteration and read (and not written) in another. There is, therefore, at least one flow or anti dependence.
Reference: [2] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: If this condition is true, it means that an element A [i] 1 any returns the "OR" of its vector operand's elements: any (v [1 : n]) = (v [1] _ v <ref> [2] </ref> _ : : : _ v [n]). 4 is written in one iteration and read (and not written) in another. There is, therefore, at least one flow or anti dependence. <p> These figures correspond to an unloaded machine; they increase with resource contention. Processors do not stall on write misses. 5.2 Workloads Since running long applications is not feasible with our limited resources, we prove our hardware algorithm by examining four loops from four Perfect Club applications <ref> [2] </ref>. These are loops that Polaris cannot analyze at the time of writing this paper and whose parallelization, therefore, can only be done at run-time. They account for a significant fraction of the time after the application has been parallelized by Polaris.
Reference: [3] <author> W. Blume, R. Doallo, R. Eigenmann, J. Grout, J. Hoeflinger, T. Lawrence, J. Lee, D. Padua, Y. Paek, B. Pot-tenger, L. Rauchwerger, and P. Tu. </author> <title> Advanced Program Restructuring for High-Performance Computers with Polaris. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 78-82, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction While there has been much work on parallelism extraction at compile time for multiprocessors <ref> [3, 6, 9] </ref>, current parallelizing compilers often have only limited success. One of the reasons for this is that access patterns sometimes depend on the input data and, therefore, there is no information available at compile time. <p> The multiprocessor modeled has the hardware support described in Section 4.2 to implement the proposed hardware scheme. In addition, since we also evaluate the software scheme of Section 2.2, the simulator is interfaced directly to the output of the Polaris parallelizing compiler <ref> [3] </ref>. For the loops that we want to parallelize at run time, Polaris inserts all the instructions to perform the marking and analysis phases. The architecture modeled has 200-MHz RISC processors. Each processor has a 32-Kbyte on 18 chip primary cache and a 512-Kbyte off-chip secondary cache.
Reference: [4] <author> W. Blume and R. Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Therefore, it has become clear that static analysis must be complemented with methods capable of extracting parallelism at run-time <ref> [4] </ref>. Most previous software approaches to run-time parallelization for multiprocessors have concentrated on developing methods for constructing execution schedules for partially parallel loops. These are loops whose parallelization may require synchronization to ensure that the iterations are executed in the correct order.
Reference: [5] <author> D. K. Chen, J. Torrellas, and P. C. Yew. </author> <title> An Efficient Algorithm for the Run-Time Parallelization of Do-Across Loops. </title> <booktitle> In Supercomputing '94, </booktitle> <pages> pages 518-527, </pages> <month> November </month> <year> 1994. </year>
Reference: [6] <author> K. Cooper et al. </author> <title> The ParaScope Parallel Programming Environment. </title> <booktitle> Proc. IEEE, </booktitle> <pages> pages 84-89, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction While there has been much work on parallelism extraction at compile time for multiprocessors <ref> [3, 6, 9] </ref>, current parallelizing compilers often have only limited success. One of the reasons for this is that access patterns sometimes depend on the input data and, therefore, there is no information available at compile time.
Reference: [7] <author> D. Gallagher, W. Chen, S. Mahlke, J. Gyllenhaal, and W. Hwu. </author> <title> Dynamic Memory Disambiguation Using the Memory Conflict Buffer. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183-193, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: This form of speculative execution is different from the commonly proposed for advanced unipro-cessors. Indeed, much work has been done in uniprocessors trying to issue loads ahead of stores even though the addresses of the locations accessed are unknown <ref> [7, 10, 15, 16] </ref>. This type of speculation is orthogonal to our work. Our framework exploits parallelism across processors in a multiprocessor. It can use these traditional techniques within each thread. We are effectively adding a second dimension of speculation. There are several advantages to our framework.
Reference: [8] <author> S. Goldschmidt. </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: In this section, we present our evaluation methodology and, in the next section, we present the results. 5.1 Simulation Environment Our evaluation is based on execution-driven simulations of a CC-NUMA shared-memory multiprocessor using Tangolite <ref> [8] </ref>. The multiprocessor modeled has the hardware support described in Section 4.2 to implement the proposed hardware scheme. In addition, since we also evaluate the software scheme of Section 2.2, the simulator is interfaced directly to the output of the Polaris parallelizing compiler [3].
Reference: [9] <author> M. Hall, J. Anderson, S. Amarasinghe, B. Murphy, S.-W. Liao, E. Bugnion, and M. Lam. </author> <title> Maximizing Multiprocessor Performance with the SUIF Compiler. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 84-89, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction While there has been much work on parallelism extraction at compile time for multiprocessors <ref> [3, 6, 9] </ref>, current parallelizing compilers often have only limited success. One of the reasons for this is that access patterns sometimes depend on the input data and, therefore, there is no information available at compile time.
Reference: [10] <author> A. S. Huang, G. Slavenburg, and J. P. Shen. </author> <title> Speculative Disambiguation: A Compilation Technique for Dynamic Memory Disambiguation. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 200-210, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This form of speculative execution is different from the commonly proposed for advanced unipro-cessors. Indeed, much work has been done in uniprocessors trying to issue loads ahead of stores even though the addresses of the locations accessed are unknown <ref> [7, 10, 15, 16] </ref>. This type of speculation is orthogonal to our work. Our framework exploits parallelism across processors in a multiprocessor. It can use these traditional techniques within each thread. We are effectively adding a second dimension of speculation. There are several advantages to our framework.
Reference: [11] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Both caches are direct-mapped and have 64-byte lines. We selected such small caches because the only workloads that we can run have smaller working sets than real-life applications. The caches are kept coherent with a DASH-like cache coherence protocol <ref> [11] </ref>. Each node has part of the global memory and the corresponding section of the directory. We model contention in the whole system except in the global network, which is abstracted away as a constant latency.
Reference: [12] <author> S.-T. Leung and J. Zahorjan. </author> <title> Improving the Performance of Runtime Parallelization. </title> <booktitle> In 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference: [13] <author> L. Rauchwerger and D. Padua. </author> <title> The PRIVATIZING DOALL Test: A Run-Time Technique for DOALL Loop Identification and Array Privati-zation. </title> <booktitle> In Proceedings of the 1994 International Conference on Supercomputing, </booktitle> <pages> pages 33-43, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Unfortunately, the inspector may be both computationally expensive and have side-effects. Consequently, it can be argued that the inspector-executor approach is not a generally applicable method. Recently, we have introduced a new framework for software run-time parallelization for multiprocessors <ref> [13, 14] </ref>. We apply the scheme to loops, but it can also be applied to other code. It has two main characteristics. First, instead of finding a valid parallel execution schedule for the loop, it focuses on simply deciding whether or not the loop is fully parallel. <p> in hardware; Section 4 presents a hardware design of our scheme; Section 5 discusses the methodology used to evaluate our scheme; and Section 6 presents the evaluation. 2 2 Speculative Run-Time Parallelization in Software We have recently proposed a new algorithm that uses speculation to parallelize loops at run time <ref> [13, 14] </ref>. The algorithm is called the LRPD test. <p> To parallelize these loops, it can be shown that the algorithm needs to use an extra shadow array (A w min [1 : s]) similar to the write shadow array. 5 tization algorithm. Overall, our algorithm can be implemented efficiently as described in <ref> [13, 14] </ref>. For example, in a DSM system, each processor allocates a private copy of the shadow arrays in its local memory. The marking phase is performed locally on the private copy. Then, the contents of the private shadow arrays are merged into global shadow arrays in parallel.
Reference: [14] <author> L. Rauchwerger and D. Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatization and Reduction Paralleliza-tion. </title> <booktitle> In Proceedings of the SIGPLAN 1995 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Unfortunately, the inspector may be both computationally expensive and have side-effects. Consequently, it can be argued that the inspector-executor approach is not a generally applicable method. Recently, we have introduced a new framework for software run-time parallelization for multiprocessors <ref> [13, 14] </ref>. We apply the scheme to loops, but it can also be applied to other code. It has two main characteristics. First, instead of finding a valid parallel execution schedule for the loop, it focuses on simply deciding whether or not the loop is fully parallel. <p> in hardware; Section 4 presents a hardware design of our scheme; Section 5 discusses the methodology used to evaluate our scheme; and Section 6 presents the evaluation. 2 2 Speculative Run-Time Parallelization in Software We have recently proposed a new algorithm that uses speculation to parallelize loops at run time <ref> [13, 14] </ref>. The algorithm is called the LRPD test. <p> To parallelize these loops, it can be shown that the algorithm needs to use an extra shadow array (A w min [1 : s]) similar to the write shadow array. 5 tization algorithm. Overall, our algorithm can be implemented efficiently as described in <ref> [13, 14] </ref>. For example, in a DSM system, each processor allocates a private copy of the shadow arrays in its local memory. The marking phase is performed locally on the private copy. Then, the contents of the private shadow arrays are merged into global shadow arrays in parallel.
Reference: [15] <author> G.S. Sohi, </author> <title> S.E. Breach, </title> <booktitle> and T.N. Vijayakumar. Multiscalar Processors. In 22nd International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: This form of speculative execution is different from the commonly proposed for advanced unipro-cessors. Indeed, much work has been done in uniprocessors trying to issue loads ahead of stores even though the addresses of the locations accessed are unknown <ref> [7, 10, 15, 16] </ref>. This type of speculation is orthogonal to our work. Our framework exploits parallelism across processors in a multiprocessor. It can use these traditional techniques within each thread. We are effectively adding a second dimension of speculation. There are several advantages to our framework.
Reference: [16] <author> J.Y. Tsai and P.C. Yew. </author> <title> The Superthreaded Architecture: Thread Pipelining with Run-Time Data Dependence Checking and Control Speculation. </title> <booktitle> In Proceedings of International Conference on Parallel Architectures and Compilation Techniques (PACT '96), </booktitle> <year> 1996. </year>
Reference-contexts: This form of speculative execution is different from the commonly proposed for advanced unipro-cessors. Indeed, much work has been done in uniprocessors trying to issue loads ahead of stores even though the addresses of the locations accessed are unknown <ref> [7, 10, 15, 16] </ref>. This type of speculation is orthogonal to our work. Our framework exploits parallelism across processors in a multiprocessor. It can use these traditional techniques within each thread. We are effectively adding a second dimension of speculation. There are several advantages to our framework.
Reference: [17] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime Compilation Methods for Multicomputers. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 26-30. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle> <pages> 24 </pages>
References-found: 17

