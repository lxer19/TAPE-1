URL: http://www.cs.wustl.edu/~schmidt/ICDCS-97.ps.gz
Refering-URL: http://www.cs.wustl.edu/~gokhale/vita.html
Root-URL: 
Email: fgokhale,schmidtg@cs.wustl.edu  
Title: Evaluating CORBA Latency and Scalability Over High-Speed ATM Networks  
Author: Aniruddha S. Gokhale and Douglas C. Schmidt 
Note: The paper appeared in the Proceedings of ICDCS '97, May 27-30, 1997 in  
Address: St. Louis, MO 63130 1  Baltimore, Maryland.  
Affiliation: Department of Computer Science Washington University  
Abstract: Conventional implementations of CORBA communication middleware incur significant overhead when used for performance-sensitive applications over high-speed networks. As gigabit networks become pervasive, inefficient middleware will force programmers to continue using lower-level mechanisms to achieve necessary transfer rates and end-to-end latency. This is a serious problem for mission/life-critical applications (such as real-time avionics, process control systems, and medical imaging). This paper provides two contributions to the study of CORBA performance over high-speed networks. First, we measure the latency of various types and sizes of oneway and twoway client requests using a pair of widely used implementations of two C++ implementations of CORBA - Orbix 2.1 and VisiBroker 2.0. Second, we use Orbix and VisiBroker to measure the scalability of CORBA servers in terms of the number of objects they can support efficiently. These experiments extend our previous work on CORBA performance for bandwidth-sensitive applications (such as satellite surveillance, medical imaging, and teleconferencing). Our results show that the latency for CORBA implementations is relatively high and server scalability is relatively low. Our latency experiments show that non-optimized internal buffering and presentation layer conversion overhead in CORBA implementations can cause substantial delay variance, which is unacceptable in many real-time or constrained-latency applications. Likewise, our scalability experiments reveal that neither Orbix nor VisiBroker can handle a large number of objects in a single server process. The paper concludes by outlining optimizations we are developing to overcome the performance limitations with existing 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David D. Clark and David L. Tennenhouse, </author> <title> Architectural Considerations for a New Generation of Protocols, </title> <booktitle> in Proceedings of the Symposium on Communications Architectures and Protocols (SIGCOMM), </booktitle> <address> Philadelphia, PA, </address> <month> Sept. </month> <year> 1990, </year> <booktitle> ACM, </booktitle> <pages> pp. 200-208. </pages>
Reference-contexts: In addition, communication software must allow bandwidth-sensitive applications to realize high-speed data transfers over gigabit networks. Reliability, flexibility, and reusability are essential to respond rapidly to changing application requirements that span a wide range of media types and access patterns <ref> [1] </ref>. These requirements motivate the use of the Common Object Request Broker Architecture (CORBA) [2]. <p> To minimize intra-ORB function calls requires sophisticated compiler optimizations such as integrated layer processing <ref> [1] </ref>. * Dynamic invocation overhead DII performance drops as the size of requests increases. <p> In addition, these ORBs suffer from excessive intra-ORB function call overhead as shown in Section 4.3. In contrast, TAO uses advanced compiler techniques (such as program flow analysis [20, 21] and integrated layer processing (ILP)) <ref> [1] </ref> to automatically omit unnecessary data copies between the CORBA infrastructure and applications. In addition, ILP reduces the overhead of excessive intra-ORB function calls. <p> The variation between request algorithms revealed that the server-side did not cache any information. We plan to incorporate caching behavior in ourTAO ORB to improve latency. * Presentation Layer and Data Copying The presentation layer is a major bottleneck in high-performance communication subsystems <ref> [1] </ref>. This layer transforms typed data objects from higher-level representations to lower-level representations (marshaling) and vice versa (demarshaling). In both RPC toolkits and CORBA, this transformation process is performed by client-side stubs and server-side skeletons that are generated by interface definition language (IDL) compilers. <p> One proposed remedy for this problem is to use Application Level Framing (ALF) <ref> [1, 30, 31] </ref> and Integrated Layer Processing (ILP) [1, 32, 33]. ALF ensures that lower layer protocols deal with data in units specified by the application. <p> One proposed remedy for this problem is to use Application Level Framing (ALF) [1, 30, 31] and Integrated Layer Processing (ILP) <ref> [1, 32, 33] </ref>. ALF ensures that lower layer protocols deal with data in units specified by the application.
Reference: [2] <institution> Object Management Group, </institution> <note> The Common Object Request Broker: Architectureand Specification, 2.0 edition, </note> <month> July </month> <year> 1995. </year>
Reference-contexts: Reliability, flexibility, and reusability are essential to respond rapidly to changing application requirements that span a wide range of media types and access patterns [1]. These requirements motivate the use of the Common Object Request Broker Architecture (CORBA) <ref> [2] </ref>. CORBA is designed to enhance distributed applications by automating common networking tasks such as parameter marshaling, object location and object activation, as well as providing the basis for defining higher layer distributed services (such as naming, events, replication, and transactions) [3]. <p> CORBA implementations over ATM; Section 5 describes our research on developing optimizations that eliminate the performance bottlenecks in existing Computing Model CORBA implementations; Section 6 describes related work; and Section 7 presents concluding remarks. 2 Overview of the CORBA Architec ture CORBA is an open standard for distributed object computing <ref> [2] </ref>. Figure 1 illustrates the primary components in the CORBA architecture. The CORBA standard defines a set of components that allow client applications to invoke operations (op) with arguments (args) on object implementations. <p> In both RPC toolkits and CORBA, this transformation process is performed by client-side stubs and server-side skeletons that are generated by interface definition language (IDL) compilers. IDL compilers translate interfaces written in an IDL (such as Sun RPC XDR [29], DCE NDR, or CORBA CDR <ref> [2] </ref>) to other forms such as a network wire format. Eliminating the overhead of presentation layer conversions requires highly optimized stub compilers (e.g., Universal Stub Compiler [16]) and the Flick IDL compiler [17].
Reference: [3] <institution> Object Management Group, </institution> <month> CORBAServices: </month> <title> Common Object Services Specification, </title> <note> Revised Edition, 95-3-31 edition, </note> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: CORBA is designed to enhance distributed applications by automating common networking tasks such as parameter marshaling, object location and object activation, as well as providing the basis for defining higher layer distributed services (such as naming, events, replication, and transactions) <ref> [3] </ref>.
Reference: [4] <author> Irfan Pyarali, Timothy H. Harrison, and Douglas C. Schmidt, </author> <title> Design and Performance of an Object-Oriented Framework for High-Performance Electronic Medical Imaging, </title> <booktitle> USENIX Computing Systems, </booktitle> <volume> vol. 9, no. 4, </volume> <month> November/December </month> <year> 1996. </year>
Reference-contexts: If not corrected, this overhead will force developers to avoid CORBA middleware and continue to use lower-level tools (such as sockets). Unfortunately, lower-level tools fail to provide other key benefits of CORBA such as reliability, flexibility and reusability, which are crucial to the success of complex constrained-latency distributed applications <ref> [4] </ref>. Therefore, it is imperative to eliminate the sources of CORBA overhead shown in this paper. Previous work [5, 6, 7] has focused on the throughput performance of CORBA implementations that transfer large amounts of untyped and richly-typed data. This paper extends earlier work by focusing on latency and scalability. <p> Each ENI card has 512 Kbytes of on-board memory. A maximum of 32 Kbytes is allotted per ATM virtual circuit connection for receiving and transmitting frames (for a total of 64 K). This allows up to eight switched virtual connections per card. 3.2 Traffic Generators Our earlier studies <ref> [5, 6, 4, 7] </ref> tested bulk data performance using flooding models that transferred untyped bytestream data, as well as richly typed data between hosts using several implementations of CORBA and other lower-level mechanisms like sockets. <p> The CORBA implementation transferred the data types using IDL sequences, which are dynamically-sized arrays. The IDL definition used in the test is shown in the Appendix A. 3.3 TTCP Parameter Settings Earlier studies <ref> [11, 12, 13, 4, 7] </ref> of transport protocol performance over ATM demonstrate the performance impact of parameters such as the size of socket queues, data buffer, and number of target objects on an endsystem (e.g., a server).
Reference: [5] <author> Aniruddha Gokhale and Douglas C. Schmidt, </author> <title> Measuring the Performance of Communication Middleware on High-Speed Networks, </title> <booktitle> in Proceedings of SIGCOMM '96, </booktitle> <address> Stanford, CA, </address> <month> August </month> <year> 1996, </year> <booktitle> ACM, </booktitle> <pages> pp. 306-317. </pages>
Reference-contexts: Unfortunately, lower-level tools fail to provide other key benefits of CORBA such as reliability, flexibility and reusability, which are crucial to the success of complex constrained-latency distributed applications [4]. Therefore, it is imperative to eliminate the sources of CORBA overhead shown in this paper. Previous work <ref> [5, 6, 7] </ref> has focused on the throughput performance of CORBA implementations that transfer large amounts of untyped and richly-typed data. This paper extends earlier work by focusing on latency and scalability. <p> Each ENI card has 512 Kbytes of on-board memory. A maximum of 32 Kbytes is allotted per ATM virtual circuit connection for receiving and transmitting frames (for a total of 64 K). This allows up to eight switched virtual connections per card. 3.2 Traffic Generators Our earlier studies <ref> [5, 6, 4, 7] </ref> tested bulk data performance using flooding models that transferred untyped bytestream data, as well as richly typed data between hosts using several implementations of CORBA and other lower-level mechanisms like sockets. <p> Demultiplexing client requests through all these layers is expensive, particularly when a large number of operations appear in an IDL interface and/or a large number of objects are managed by an ORB. Our prior work <ref> [5, 14] </ref> analyzed the impact of various IDL skeleton demultiplexing techniques (such as linear search and direct demultiplexing). However, in many applications the number of operations defined per-IDL interface is relatively small and static, compared to the number of potential objects, which can be quite large and dynamic. <p> These figures reveal that as the sender buffer size increases the marshaling and data copying overhead also grows <ref> [5, 6] </ref>, thereby increasing latency. <p> These sources of overhead reduce the receiver's performance, thereby triggering the flow control mechanisms of the transport protocol, which impede the sender's progress. <ref> [5, 6] </ref> precisely pinpoint the marshaling and data copying overheads when transferring richly-typed data using SII and DII. The latency for sending octets is significantly less than that for BinStructs due to significantly lower overhead of presentation layer conversions. <p> They show that increasing the socket buffer sizes improves the IPC performance. They also show that the socket layer overhead is more significant on the receiver side. [28] discusses the TCP NODELAY option, which allows TCP to send small packets as soon as possible to reduce latency. Earlier work <ref> [7, 5, 6] </ref> using untyped data and typed data in a similar CORBA/ATM testbed as the one in this paper reveal that the low-level C socket version and the C++ socket wrapper versions of TTCP are roughly equivalent for a given 14 socket queue size. <p> The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) [22]. Our earlier results <ref> [5, 6] </ref> have presented detailed measurements of presentation layer overhead for transmitting richly-typed data. Our results for sending structs reveal that with increasing sender buffer sizes, the marshaling overhead increases, thereby increasing the latency.
Reference: [6] <author> Aniruddha Gokhale and Douglas C. Schmidt, </author> <title> The Performance of the CORBA Dynamic Invocation Interface and Dynamic Skeleton Interface over High-Speed ATM Networks, </title> <booktitle> in Proceedings of GLOBECOM '96, </booktitle> <address> London, Eng-land, </address> <month> November </month> <year> 1996, </year> <journal> IEEE, </journal> <pages> pp. 50-56. </pages>
Reference-contexts: Unfortunately, lower-level tools fail to provide other key benefits of CORBA such as reliability, flexibility and reusability, which are crucial to the success of complex constrained-latency distributed applications [4]. Therefore, it is imperative to eliminate the sources of CORBA overhead shown in this paper. Previous work <ref> [5, 6, 7] </ref> has focused on the throughput performance of CORBA implementations that transfer large amounts of untyped and richly-typed data. This paper extends earlier work by focusing on latency and scalability. <p> Each ENI card has 512 Kbytes of on-board memory. A maximum of 32 Kbytes is allotted per ATM virtual circuit connection for receiving and transmitting frames (for a total of 64 K). This allows up to eight switched virtual connections per card. 3.2 Traffic Generators Our earlier studies <ref> [5, 6, 4, 7] </ref> tested bulk data performance using flooding models that transferred untyped bytestream data, as well as richly typed data between hosts using several implementations of CORBA and other lower-level mechanisms like sockets. <p> These figures reveal that as the sender buffer size increases the marshaling and data copying overhead also grows <ref> [5, 6] </ref>, thereby increasing latency. <p> These sources of overhead reduce the receiver's performance, thereby triggering the flow control mechanisms of the transport protocol, which impede the sender's progress. <ref> [5, 6] </ref> precisely pinpoint the marshaling and data copying overheads when transferring richly-typed data using SII and DII. The latency for sending octets is significantly less than that for BinStructs due to significantly lower overhead of presentation layer conversions. <p> They show that increasing the socket buffer sizes improves the IPC performance. They also show that the socket layer overhead is more significant on the receiver side. [28] discusses the TCP NODELAY option, which allows TCP to send small packets as soon as possible to reduce latency. Earlier work <ref> [7, 5, 6] </ref> using untyped data and typed data in a similar CORBA/ATM testbed as the one in this paper reveal that the low-level C socket version and the C++ socket wrapper versions of TTCP are roughly equivalent for a given 14 socket queue size. <p> The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) [22]. Our earlier results <ref> [5, 6] </ref> have presented detailed measurements of presentation layer overhead for transmitting richly-typed data. Our results for sending structs reveal that with increasing sender buffer sizes, the marshaling overhead increases, thereby increasing the latency.
Reference: [7] <author> Douglas C. Schmidt, Timothy H. Harrison, and Ehab Al-Shaer, </author> <title> Object-Oriented Components for High-speed Network Programming, </title> <booktitle> in Proceedings of the 1 st Conference on Object-Oriented Technologiesand Systems, </booktitle> <address> Monterey, CA, </address> <month> June </month> <year> 1995, </year> <booktitle> USENIX. </booktitle> <pages> 16 </pages>
Reference-contexts: Unfortunately, lower-level tools fail to provide other key benefits of CORBA such as reliability, flexibility and reusability, which are crucial to the success of complex constrained-latency distributed applications [4]. Therefore, it is imperative to eliminate the sources of CORBA overhead shown in this paper. Previous work <ref> [5, 6, 7] </ref> has focused on the throughput performance of CORBA implementations that transfer large amounts of untyped and richly-typed data. This paper extends earlier work by focusing on latency and scalability. <p> Each ENI card has 512 Kbytes of on-board memory. A maximum of 32 Kbytes is allotted per ATM virtual circuit connection for receiving and transmitting frames (for a total of 64 K). This allows up to eight switched virtual connections per card. 3.2 Traffic Generators Our earlier studies <ref> [5, 6, 4, 7] </ref> tested bulk data performance using flooding models that transferred untyped bytestream data, as well as richly typed data between hosts using several implementations of CORBA and other lower-level mechanisms like sockets. <p> The CORBA implementation transferred the data types using IDL sequences, which are dynamically-sized arrays. The IDL definition used in the test is shown in the Appendix A. 3.3 TTCP Parameter Settings Earlier studies <ref> [11, 12, 13, 4, 7] </ref> of transport protocol performance over ATM demonstrate the performance impact of parameters such as the size of socket queues, data buffer, and number of target objects on an endsystem (e.g., a server). <p> These parameters influence the size of the TCP segment window, which has been shown <ref> [13, 7] </ref> to significantly affect CORBA-level and TCP-level performance on high-speed networks. * TCP No Delay option Since the request sizes for our tests are relatively small, the TCP NODELAY option is set on the client side. <p> They show that increasing the socket buffer sizes improves the IPC performance. They also show that the socket layer overhead is more significant on the receiver side. [28] discusses the TCP NODELAY option, which allows TCP to send small packets as soon as possible to reduce latency. Earlier work <ref> [7, 5, 6] </ref> using untyped data and typed data in a similar CORBA/ATM testbed as the one in this paper reveal that the low-level C socket version and the C++ socket wrapper versions of TTCP are roughly equivalent for a given 14 socket queue size.
Reference: [8] <author> Douglas C. Schmidt, David L. Levine, and Timothy H. Har--rison, </author> <title> An ORB Endsystem Architecture for Hard Real-Time Scheduling, </title> <month> Feb. </month> <year> 1997, </year> <note> Submitted to OMG in response to RFI ORBOS/96-09-02. </note>
Reference-contexts: In addition, the Object Adapter associates object implementations with the ORB. Object Adapters can be specialized to provide support for certain object implementation styles (such as OODB Object Adapters for persistence, library Object Adapters for non-remote objects, and real-time Object Adapters <ref> [8] </ref> for applications that require QoS guarantees). The use of CORBA as communication middleware enhances application flexibility and portability by automating many common development tasks such as object location, parameter marshaling, and object activation. <p> Figure 19 depicts the optimizations we are employing to eliminate the bottlenecks with existing ORBs identified in Section 3. These optimizations are being integrated into a high-performance, real-time ORB called TAO <ref> [18, 8] </ref>. The research issue we are addressing in TAO is how to provide end-to-end quality of service guarantees to CORBA-compliant applications and services.
Reference: [9] <author> Kenneth Birman and Robbert van Renesse, </author> <title> RPC Considered Inadequate, </title> <booktitle> in Reliable Distributed Computing with the Isis Toolkit, </booktitle> <pages> pp. 68-78. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <year> 1994. </year>
Reference-contexts: In Section 4.3, we show the corresponding path for the Orbix 2.1 and VisiBroker 2.0 ORBs and precisely pinpoint the sources of overhead existing along the data path. It is beyond the scope of this paper to discuss the limitations with the CORBA communication model (see <ref> [9] </ref> for a synopsis). 3 CORBA/ATM Testbed and Experi mental Methods This section describes our CORBA/ATM testbed and outlines our experimental methods. 3.1 Hardware and Software Platforms The experiments in this section were conducted using a FORE systems ASX-1000 ATM switch connected to two dual-processor UltraSPARC-2s running SunOS 5.5.1.
Reference: [10] <author> USNA, TTCP: </author> <title> a test of TCP and UDP Performance, </title> <month> Dec </month> <year> 1984. </year>
Reference-contexts: In addition, we measure CORBA scalability by determining the performance impact from increasing the number of objects in an endsystem server process. Traffic for the latency experiment was generated and consumed by the Orbix 2.1 and VisiBroker 2.0 implementations of TTCP <ref> [10] </ref> (TTCP is a widely used benchmarking tool to evaluate the performance of TCP/IP and UDP/IP networks). In this case, we defined both oneway and twoway CORBA operations.
Reference: [11] <author> Sudheer Dharnikota, Kurt Maly, and C. M. Overstreet, </author> <title> Performance Evaluation of TCP(UDP)/IP over ATM networks, </title> <institution> Department of Computer Science, </institution> <type> Technical Report CSTR 94 23, </type> <institution> Old Dominion University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: The CORBA implementation transferred the data types using IDL sequences, which are dynamically-sized arrays. The IDL definition used in the test is shown in the Appendix A. 3.3 TTCP Parameter Settings Earlier studies <ref> [11, 12, 13, 4, 7] </ref> of transport protocol performance over ATM demonstrate the performance impact of parameters such as the size of socket queues, data buffer, and number of target objects on an endsystem (e.g., a server). <p> In general, less attention has been paid to integrating the following topics related to communication middleware: * Transport Protocol Performance over ATM Networks The underlying transport protocols used by the ORB must be flexible and possess the necessary hooks to tune different parameters of the underlying transport protocol. <ref> [11, 12, 13] </ref> present results on performance of TCP/IP (and UDP/IP [11]) on ATM networks by varying a number of parameters (such as TCP window size, socket queue size, and user data size). <p> the following topics related to communication middleware: * Transport Protocol Performance over ATM Networks The underlying transport protocols used by the ORB must be flexible and possess the necessary hooks to tune different parameters of the underlying transport protocol. [11, 12, 13] present results on performance of TCP/IP (and UDP/IP <ref> [11] </ref>) on ATM networks by varying a number of parameters (such as TCP window size, socket queue size, and user data size). <p> This work indicates that in addition to the host architecture and host network interface, parameters configurable in software (like TCP window size, socket queue size, and user data size) significantly affect TCP throughput. <ref> [11] </ref> shows that UDP performs better than TCP over ATM networks, which is attributed to redundant TCP processing overhead on highly-reliable ATM links. [11] also describes techniques to tune TCP to be a less bulky protocol so that its performance can be comparable to UDP. <p> to the host architecture and host network interface, parameters configurable in software (like TCP window size, socket queue size, and user data size) significantly affect TCP throughput. <ref> [11] </ref> shows that UDP performs better than TCP over ATM networks, which is attributed to redundant TCP processing overhead on highly-reliable ATM links. [11] also describes techniques to tune TCP to be a less bulky protocol so that its performance can be comparable to UDP.
Reference: [12] <author> Minh DoVan, Louis Humphrey, Geri Cox, and Carl Ravin, </author> <title> Initial Experience with AsynchronousTransfer Mode for Use in a Medical Imaging Network, </title> <journal> Journal of Digital Imaging, </journal> <volume> vol. 8, no. 1, </volume> <pages> pp. 43-48, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The CORBA implementation transferred the data types using IDL sequences, which are dynamically-sized arrays. The IDL definition used in the test is shown in the Appendix A. 3.3 TTCP Parameter Settings Earlier studies <ref> [11, 12, 13, 4, 7] </ref> of transport protocol performance over ATM demonstrate the performance impact of parameters such as the size of socket queues, data buffer, and number of target objects on an endsystem (e.g., a server). <p> In general, less attention has been paid to integrating the following topics related to communication middleware: * Transport Protocol Performance over ATM Networks The underlying transport protocols used by the ORB must be flexible and possess the necessary hooks to tune different parameters of the underlying transport protocol. <ref> [11, 12, 13] </ref> present results on performance of TCP/IP (and UDP/IP [11]) on ATM networks by varying a number of parameters (such as TCP window size, socket queue size, and user data size).
Reference: [13] <author> K. Modeklev, E. Klovning, and O. Kure, </author> <title> TCP/IP Behavior in a High-Speed Local ATM Network Environment, </title> <booktitle> in Proceedings of the 19 th Conference on Local Computer Networks, </booktitle> <address> Minneapolis, MN, Oct. 1994, </address> <publisher> IEEE, </publisher> <pages> pp. 176-185. </pages>
Reference-contexts: The CORBA implementation transferred the data types using IDL sequences, which are dynamically-sized arrays. The IDL definition used in the test is shown in the Appendix A. 3.3 TTCP Parameter Settings Earlier studies <ref> [11, 12, 13, 4, 7] </ref> of transport protocol performance over ATM demonstrate the performance impact of parameters such as the size of socket queues, data buffer, and number of target objects on an endsystem (e.g., a server). <p> These parameters influence the size of the TCP segment window, which has been shown <ref> [13, 7] </ref> to significantly affect CORBA-level and TCP-level performance on high-speed networks. * TCP No Delay option Since the request sizes for our tests are relatively small, the TCP NODELAY option is set on the client side. <p> In general, less attention has been paid to integrating the following topics related to communication middleware: * Transport Protocol Performance over ATM Networks The underlying transport protocols used by the ORB must be flexible and possess the necessary hooks to tune different parameters of the underlying transport protocol. <ref> [11, 12, 13] </ref> present results on performance of TCP/IP (and UDP/IP [11]) on ATM networks by varying a number of parameters (such as TCP window size, socket queue size, and user data size).
Reference: [14] <author> Aniruddha Gokhale, Douglas C. Schmidt, and Stan Moyer, </author> <title> Evaluating the Performance of Demultiplexing Strategies for Real-time CORBA, </title> <note> in Submitted to GLOBECOM '97, , Phoenix, AZ, </note> <month> November </month> <year> 1997, </year> <note> IEEE. </note>
Reference-contexts: Demultiplexing client requests through all these layers is expensive, particularly when a large number of operations appear in an IDL interface and/or a large number of objects are managed by an ORB. Our prior work <ref> [5, 14] </ref> analyzed the impact of various IDL skeleton demultiplexing techniques (such as linear search and direct demultiplexing). However, in many applications the number of operations defined per-IDL interface is relatively small and static, compared to the number of potential objects, which can be quite large and dynamic. <p> In contrast, TAO utilizes active delayered demultiplexing and explicit dynamic linking <ref> [14] </ref> shown in Figure 21 (C), which makes it possible to adapt and configure optimal strategies for dispatching client requests within ORB endsystems and CORBA bridges. * Excessive data copying and intra-ORB calls Existing ORBs are not optimized to reduce the overhead of data copies. <p> Layered multiplexing and demultiplexing is generally disparaged for high-performance communication systems [35] due to the additional overhead incurred at each layer.[15] describes a fast and flexible message demultiplexing strategy based on dynamic code generation. <ref> [14] </ref> evaluates the performance of alternative demultiplexing strategies for real-time CORBA. Our results for latency measurements have shown that with increasing number of objects, the latency increases. This is partly due to the additional overhead of demultiplexing the request to the appropriate method of the appropriate object. <p> Our results for latency measurements have shown that with increasing number of objects, the latency increases. This is partly due to the additional overhead of demultiplexing the request to the appropriate method of the appropriate object. We propose to use a delayered demultiplexing architecture <ref> [14] </ref> that can select optimal demultiplexing strategies based on compile-time and run-time analysis of CORBA IDL interfaces. 7 Concluding Remarks An important class of applications (such as avionics, distributed interactive simulation, and telecommunication systems) require scalable, low latency communication.
Reference: [15] <author> Dawson R. Engler and M. Frans Kaashoek, DPF: </author> <title> Fast, Flexible Message Demultiplexing using Dynamic Code Generation, </title> <booktitle> in Proceedings of ACM SIGCOMM '96 Conference in Computer Communication Review, </booktitle> <institution> Stanford University, Cali-fornia, USA, </institution> <month> August </month> <year> 1996, </year> <pages> pp. 53-59, </pages> <publisher> ACM Press. </publisher>
Reference-contexts: Eliminating the demultiplexing overhead requires delayered strategies and fast, flexible message de-multiplexing <ref> [15] </ref>. Eliminating the presentation layer overhead requires optimized stub generators [16, 17] for richly-typed data. * Demultiplexing overhead The Orbix demultiplexing performs worse than VisiBroker demultiplexing since Orbix uses a linear search strategy based on string comparisons for operation demultiplexing.
Reference: [16] <author> Sean W. O'Malley, Todd A. Proebsting, and Allen B. Montz, </author> <title> USC: A Universal Stub Compiler, </title> <booktitle> in Proceedings of the Symposium on Communications Architectures and Protocols (SIGCOMM), </booktitle> <address> London, UK, </address> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: Eliminating the demultiplexing overhead requires delayered strategies and fast, flexible message de-multiplexing [15]. Eliminating the presentation layer overhead requires optimized stub generators <ref> [16, 17] </ref> for richly-typed data. * Demultiplexing overhead The Orbix demultiplexing performs worse than VisiBroker demultiplexing since Orbix uses a linear search strategy based on string comparisons for operation demultiplexing. <p> IDL compilers translate interfaces written in an IDL (such as Sun RPC XDR [29], DCE NDR, or CORBA CDR [2]) to other forms such as a network wire format. Eliminating the overhead of presentation layer conversions requires highly optimized stub compilers (e.g., Universal Stub Compiler <ref> [16] </ref>) and the Flick IDL compiler [17]. The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) [22].
Reference: [17] <author> Eric Eide, Kevin Frei, Bryan Ford, Jay Lepreau, and Gary Lindstrom, Flick: </author> <title> A Flexible, Optimizing IDL Compiler, </title> <booktitle> in Proceedings of ACM SIGPLAN '97 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <address> Las Vegas, NV, </address> <month> June </month> <year> 1997, </year> <note> ACM. </note>
Reference-contexts: Eliminating the demultiplexing overhead requires delayered strategies and fast, flexible message de-multiplexing [15]. Eliminating the presentation layer overhead requires optimized stub generators <ref> [16, 17] </ref> for richly-typed data. * Demultiplexing overhead The Orbix demultiplexing performs worse than VisiBroker demultiplexing since Orbix uses a linear search strategy based on string comparisons for operation demultiplexing. <p> Eliminating the overhead of presentation layer conversions requires highly optimized stub compilers (e.g., Universal Stub Compiler [16]) and the Flick IDL compiler <ref> [17] </ref>. The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) [22]. Our earlier results [5, 6] have presented detailed measurements of presentation layer overhead for transmitting richly-typed data.
Reference: [18] <author> Douglas C. Schmidt, Aniruddha Gokhale, Tim Harrison, and Guru Parulkar, </author> <title> A High-Performance Endsystem Architecture for Real-time CORBA, </title> <journal> IEEE Communications Magazine, </journal> <volume> vol. 14, no. 2, </volume> <month> February </month> <year> 1997. </year>
Reference-contexts: Figure 19 depicts the optimizations we are employing to eliminate the bottlenecks with existing ORBs identified in Section 3. These optimizations are being integrated into a high-performance, real-time ORB called TAO <ref> [18, 8] </ref>. The research issue we are addressing in TAO is how to provide end-to-end quality of service guarantees to CORBA-compliant applications and services.
Reference: [19] <author> Aniruddha Gokhale and Douglas C. Schmidt, </author> <title> Optimizing the Performance of the CORBA Internet Inter-ORB Protocol Over ATM, </title> <note> in Submitted for publication (Washington University Technical Report #WUCS-97-10), </note> <month> February </month> <year> 1997. </year>
Reference-contexts: The central focus of our TAO ORB effort is a portable and feature-rich CORBA kernel that implements the standard CORBA Internet Inter-Operability Protocol (IIOP) as shown in Figure 20. Our IIOP kernel is based on a highly optimized implementation of SunSoft's implementation of the IIOP <ref> [19] </ref>. <p> Latency for sending types data in both ORBS increases as the size of the data increases. This is attributed to the additional overhead of presentation layer conversions. We are currently working towards eliminating presentation layer overhead by using measurement and principle driven optimizations <ref> [19] </ref>. Our optimizations are based on principles such as eliminating waste, precomputing and storing to avoid unnecessary repetitive computation, and optimizing for the common case. In general, our latency experiments indicate that CORBA implementations have not been optimized to support low-latency quality of service.
Reference: [20] <author> Jong-Deok Choi, Ron Cytron, and Jeanne Ferrante, </author> <title> Automatic Construction of Sparse Data Flow Evaluation Graphs, </title> <booktitle> in Conference Record of the Eighteenth Annual ACE Symposium on Principles of Programming Languages. ACM, </booktitle> <month> Jan-uary </month> <year> 1991. </year>
Reference-contexts: In addition, these ORBs suffer from excessive intra-ORB function call overhead as shown in Section 4.3. In contrast, TAO uses advanced compiler techniques (such as program flow analysis <ref> [20, 21] </ref> and integrated layer processing (ILP)) [1] to automatically omit unnecessary data copies between the CORBA infrastructure and applications. In addition, ILP reduces the overhead of excessive intra-ORB function calls.
Reference: [21] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Weg-man, and F. Kenneth Zadeck, </author> <title> Efficiently Computing Static Single Assignment Form and the Control DependenceGraph, </title> <journal> in ACM Transactions on Programming Languages and Systems. ACM, </journal> <month> October </month> <year> 1991. </year>
Reference-contexts: In addition, these ORBs suffer from excessive intra-ORB function call overhead as shown in Section 4.3. In contrast, TAO uses advanced compiler techniques (such as program flow analysis <ref> [20, 21] </ref> and integrated layer processing (ILP)) [1] to automatically omit unnecessary data copies between the CORBA infrastructure and applications. In addition, ILP reduces the overhead of excessive intra-ORB function calls.
Reference: [22] <author> Phillip Hoschka and Christian Huitema, </author> <title> Automatic Generation of Optimized Code for Marshalling Routines, </title> <booktitle> in IFIP Conference of Upper Layer Protocols, Architectures and Applications ULPAA'94, </booktitle> <address> Barcelona, Spain, 1994, </address> <publisher> IFIP. </publisher>
Reference-contexts: In contrast, TAO produces and configures multiple encoding/decoding strategies for CORBA interface definition language (IDL) descriptions. Each strategy can be configured for different time/space tradeoffs between compiled vs. interpreted CORBA IDL stubs and skeletons <ref> [22] </ref>, and the application's use of parameters (e.g., pass-without-touching, read-only, mutable). * Non-optimized buffering algorithms used for network reads and writes Existing ORBS utilize non-optimized internal buffers for writing to and reading from the network, as shown in Section 4.3. <p> The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) <ref> [22] </ref>. Our earlier results [5, 6] have presented detailed measurements of presentation layer overhead for transmitting richly-typed data. Our results for sending structs reveal that with increasing sender buffer sizes, the marshaling overhead increases, thereby increasing the latency.
Reference: [23] <author> R. Gopalakrishnan and G. Parulkar, </author> <title> A Real-time Upcall Facility for Protocol Processing with QoS Guarantees, </title> <booktitle> in 15 th Symposium on Operating System Principles (poster session), </booktitle> <address> Copper Mountain Resort, Boulder, CO, </address> <month> Dec. </month> <year> 1995, </year> <note> ACM. </note>
Reference-contexts: In contrast, TAO utilizes optimal buffer choices to reduce this overhead; We are currently implementing TAO within a prototype real-time OS developed at Washington University. This real-time OS is characterized by features that include (1) a Real-Time Upcall (RTU) scheduling mechanism <ref> [23] </ref>, which achieves end-to-end real-time scheduling and (2) the APIC [24] ATM/host network interface, which provides a zero-copy mechanism that eliminates the excessive data copying overhead. In addition, the ACE framework [25] is used to implement the TAO ORB.
Reference: [24] <author> Zubin D. Dittia, Jr. Jerome R. Cox, and Guru M. Parulkar, </author> <title> Design of the APIC: A High Performance ATM Host-Network Interface Chip, </title> <booktitle> in IEEE INFOCOM '95, </booktitle> <address> Boston, USA, April 1995, </address> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 179-187. </pages>
Reference-contexts: This real-time OS is characterized by features that include (1) a Real-Time Upcall (RTU) scheduling mechanism [23], which achieves end-to-end real-time scheduling and (2) the APIC <ref> [24] </ref> ATM/host network interface, which provides a zero-copy mechanism that eliminates the excessive data copying overhead. In addition, the ACE framework [25] is used to implement the TAO ORB.
Reference: [25] <author> Douglas C. Schmidt and Tatsuya Suda, </author> <title> An Object-Oriented Framework for Dynamically Configuring Extensible Distributed Communication Systems, </title> <journal> IEE/BCS Distributed Systems Engineering Journal (Special Issue on Configurable Distributed Systems), </journal> <volume> vol. 2, </volume> <pages> pp. 280-293, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: This real-time OS is characterized by features that include (1) a Real-Time Upcall (RTU) scheduling mechanism [23], which achieves end-to-end real-time scheduling and (2) the APIC [24] ATM/host network interface, which provides a zero-copy mechanism that eliminates the excessive data copying overhead. In addition, the ACE framework <ref> [25] </ref> is used to implement the TAO ORB. <p> We are currently implementing a lightweight ORB called TAO that eliminates these overheads. The source code for the various tests performed in this paper is made available through the ACE <ref> [25] </ref> software distribution at www.cs.wustl.edu/~schmidt/ACE.html. Acknowledgments We like to thank IONA and Visigenic for their help in supplying the CORBA implementations used for these tests. Both companies are currently working to eliminate the latency overhead and scalability limitations described in this paper.
Reference: [26] <author> Jonathan Kay and Joseph Pasquale, </author> <title> The Importance of Non-Data Touching Processing Overheads in TCP/IP, </title> <booktitle> in Proceedings of SIGCOMM '93, </booktitle> <address> San Francisco, CA, </address> <month> September </month> <year> 1993, </year> <booktitle> ACM, </booktitle> <pages> pp. 259-269. </pages>
Reference-contexts: They also show that the TCP delay characteristics are predictable and that it varies with the throughput. <ref> [26] </ref> present detailed measurements of various categories of processing overhead times of TCP/IP and UDP/IP.
Reference: [27] <author> Christos Papadopoulos and Gurudatta Parulkar, </author> <title> Experimental Evaluation of SUNOS IPC and TCP/IP Protocol Implementation, </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 199-216, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: The authors show that most messages sent are short (less than 200 bytes). They claim that these overheads are hard to eliminate and techniques such as Integrated Layer Processing can be used to reduce the overhead. <ref> [27] </ref> present performance results of the SUNOS IPC and TCP/IP implementations. They show that increasing the socket buffer sizes improves the IPC performance.
Reference: [28] <author> S. J. Leffler, M.K. McKusick, M.J. Karels, and J.S. Quarter-man, </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: They show that increasing the socket buffer sizes improves the IPC performance. They also show that the socket layer overhead is more significant on the receiver side. <ref> [28] </ref> discusses the TCP NODELAY option, which allows TCP to send small packets as soon as possible to reduce latency.
Reference: [29] <author> Sun Microsystems, XDR: </author> <title> External Data Representation Standard, Network Information Center RFC 1014, </title> <month> June </month> <year> 1987. </year>
Reference-contexts: In both RPC toolkits and CORBA, this transformation process is performed by client-side stubs and server-side skeletons that are generated by interface definition language (IDL) compilers. IDL compilers translate interfaces written in an IDL (such as Sun RPC XDR <ref> [29] </ref>, DCE NDR, or CORBA CDR [2]) to other forms such as a network wire format. Eliminating the overhead of presentation layer conversions requires highly optimized stub compilers (e.g., Universal Stub Compiler [16]) and the Flick IDL compiler [17].
Reference: [30] <author> Isabelle Chrisment, </author> <title> Impact of ALF on Communication Subsystems Design and Performance, </title> <booktitle> in First International Workshop on High Performance Protocol Architectures, HIP-PARCH '94, </booktitle> <institution> Sophia Antipolis, France, </institution> <month> December </month> <year> 1994, </year> <institution> IN-RIA France. </institution>
Reference-contexts: One proposed remedy for this problem is to use Application Level Framing (ALF) <ref> [1, 30, 31] </ref> and Integrated Layer Processing (ILP) [1, 32, 33]. ALF ensures that lower layer protocols deal with data in units specified by the application.
Reference: [31] <author> Atanu Ghosh, Jon Crowcroft, Michael Fry, and Mark Hand-ley, </author> <title> Integrated Layer Video Decoding and Application Layer Framed Secure Login: General Lessons from Two or Three Very Different Applications, </title> <booktitle> in First International Workshop on High Performance Protocol Architectures, HIPPARCH '94, </booktitle> <institution> Sophia Antipolis, France, </institution> <month> December </month> <year> 1994, </year> <institution> INRIA France. </institution>
Reference-contexts: One proposed remedy for this problem is to use Application Level Framing (ALF) <ref> [1, 30, 31] </ref> and Integrated Layer Processing (ILP) [1, 32, 33]. ALF ensures that lower layer protocols deal with data in units specified by the application.
Reference: [32] <author> M. Abbott and L. Peterson, </author> <title> Increasing Network Throughput by Integrating Protocol Layers, </title> <journal> ACM Transactions on Networking, </journal> <volume> vol. 1, no. 5, </volume> <month> October </month> <year> 1993. </year>
Reference-contexts: One proposed remedy for this problem is to use Application Level Framing (ALF) [1, 30, 31] and Integrated Layer Processing (ILP) <ref> [1, 32, 33] </ref>. ALF ensures that lower layer protocols deal with data in units specified by the application.
Reference: [33] <author> Antony Richards, Ranil De Silva, Anne Fladenmuller, Aruna Seneviratne, and Michael Fry, </author> <title> The Application of ILP/ALF to Configurable Protocols, </title> <booktitle> in First International Workshop on High Performance Protocol Architectures, HIPPARCH '94, </booktitle> <institution> Sophia Antipolis, France, </institution> <month> December </month> <year> 1994, </year> <institution> INRIA France. </institution>
Reference-contexts: One proposed remedy for this problem is to use Application Level Framing (ALF) [1, 30, 31] and Integrated Layer Processing (ILP) <ref> [1, 32, 33] </ref>. ALF ensures that lower layer protocols deal with data in units specified by the application.
Reference: [34] <author> Torsten Braun and Christophe Diot, </author> <title> Protocol Implementation Using Integrated Layer Processnig, </title> <booktitle> in Proceedings of the Symposium on Communications Architectures and Protocols (SIGCOMM). ACM, </booktitle> <month> September </month> <year> 1995. </year>
Reference-contexts: ALF ensures that lower layer protocols deal with data in units specified by the application. ILP provides the implementor with the option of performing all data manipulations in one or two integrated processing loops, rather than manipulating the data sequentially. <ref> [34] </ref> have shown that although ILP reduces the number of memory accesses, it does not reduce the number of cache misses compared to a carefully designed non-ILP implementation. A major limitation of ILP described in [34] is its applicability to only non-ordering constrained protocol functions and its uses of macros that <p> data manipulations in one or two integrated processing loops, rather than manipulating the data sequentially. <ref> [34] </ref> have shown that although ILP reduces the number of memory accesses, it does not reduce the number of cache misses compared to a carefully designed non-ILP implementation. A major limitation of ILP described in [34] is its applicability to only non-ordering constrained protocol functions and its uses of macros that restrict the protocol implementation from being dynamically adapted to changing requirements.
Reference: [35] <author> David L. Tennenhouse, </author> <title> Layered Multiplexing Considered Harmful, </title> <booktitle> in Proceedings of the 1 st International Workshop on High-Speed Networks, </booktitle> <month> May </month> <year> 1989. </year> <month> 17 </month>
Reference-contexts: In addition, conventional CORBA implementations utilize several extra levels of demultiplexing at the application layer to associate incoming client requests with the appropriate object implementation and method (as shown in Figure 3). Layered multiplexing and demultiplexing is generally disparaged for high-performance communication systems <ref> [35] </ref> due to the additional overhead incurred at each layer.[15] describes a fast and flexible message demultiplexing strategy based on dynamic code generation. [14] evaluates the performance of alternative demultiplexing strategies for real-time CORBA. Our results for latency measurements have shown that with increasing number of objects, the latency increases.
References-found: 35

