URL: http://www.cs.ucsb.edu/~chriss/ps/96-super.ps
Refering-URL: http://www.cs.ucsb.edu/~chriss/ps/index.html
Root-URL: http://www.cs.ucsb.edu
Email: fchriss,bjoern,schauserg@cs.ucsb.edu  
Title: Profiling Techniques for a Fine-Grained Parallel Language  
Author: Chris J. Scheiman, Bjoern Haake, Klaus E. Schauser 
Keyword: Parallel programming, performance analysis, profiling, fine-grained communica tion, Split-C, Active Messages.  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: Fine tuning the performance of large parallel programs is a very difficult task. A profiling tool can provide detailed insight into the utilization and communication of the different processors, which helps identify performance bottlenecks. In this paper we present two profiling techniques for the fine-grained parallel programming language Split-C, which provides a simple global address space memory model. One profiler provides a detailed analysis of a program's execution. The other profiler collects cumulative information. As our experience shows, it is much more challenging to profile programs that make use of efficient, low-overhead communication. We incorporated techniques which minimize profiling effects on the running program. We quantify the profiling overhead and present several Split-C applications which show that the profiler is useful in determining performance bottlenecks. 
Abstract-found: 1
Intro-found: 1
Reference: [BK94] <author> E. Brewer and B. Kuszmaul. </author> <title> How to Get Good Performance from the CM-5 Data Network. </title> <booktitle> In Eighth International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Of course, for flush-on-barrier to be effective, the parallel program must have periodic synchronization points. Fortunately, this is often the case, since such synchronization points are often required for correctness or are often included to make the algorithm more efficient by choreographing the communication <ref> [BK94, CKP + 93] </ref>. Our observations reflect this, since almost all of the Split-C programs we have encountered contain many barriers. 4 Issues and Solutions In Section 3 we examined the architecture of the profiler. In this section we examine its effectiveness.
Reference: [CDG + 93a] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, S. Luna, T. von Eicken, and K. Yelick. </author> <title> Introduction to Split-C. </title> <type> Technical report, </type> <institution> UC Berkeley, </institution> <year> 1993. </year>
Reference-contexts: Paradyn [MCC + 95] and Vista [WRM + 96] are examples of online monitoring systems. Systems such as Falcon [WEK + 95], VIZIR [HKWJ95], or ParAid [RAB + 93] provide both online and offline monitoring. The target for our profiler is the fine-grained parallel programming language Split-C <ref> [CDG + 93a] </ref>. Split-C is based on a global address space memory model, and remote memory can be accessed via global pointers. Data is transferred using split-phase operations.
Reference: [CDG + 93b] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings SUPERCOMPUTING '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Since these assume a send & receive model of communication, while Split-C is based on Active Messages and a global address space, some significant modifications were required. 5 3.1 Split-C Split-C is a simple parallel extension of the C programming language <ref> [CDG + 93b] </ref>. Split-C follows the SPMD model of computation: a single thread of computation is started on each processor. Both the parallelism and data layout are explicit and are specified by the programmer.
Reference: [CKP + 93] <author> D. E. Culler, R. M. Karp, D. A. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Of course, for flush-on-barrier to be effective, the parallel program must have periodic synchronization points. Fortunately, this is often the case, since such synchronization points are often required for correctness or are often included to make the algorithm more efficient by choreographing the communication <ref> [BK94, CKP + 93] </ref>. Our observations reflect this, since almost all of the Split-C programs we have encountered contain many barriers. 4 Issues and Solutions In Section 3 we examined the architecture of the profiler. In this section we examine its effectiveness.
Reference: [GHPW90] <author> G. Geist, M. Heath, B. Peyton, and P. Worley. </author> <title> PICL A Portable Instrumented Communication Library. </title> <type> Technical report, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1990. </year>
Reference-contexts: For this reason, many profilers have already been developed. For distributed memory machines, most profilers are implemented for PVM, MPI, or similar communication mechanisms based on the send & receive protocol. The existing profilers include offline approaches like ParaGraph [HE91], PICL <ref> [GHPW90] </ref>, IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], or AIMS [Yan94]. Paradyn [MCC + 95] and Vista [WRM + 96] are examples of online monitoring systems. <p> We examine and address these limitations in the context of an offline performance profiler we developed for the parallel language Split-C running on a high-end MPP (a 64 processor Meiko CS-2). As the basis of our profiler we used PICL/ParaGraph <ref> [GHPW90, HE91] </ref>, which was originally designed for a send & receive protocol (MPI). We have adapted the profiler to the Active Message model underlying Split-C. We experimentally quantify the profiling overheads of our implementations and study the merits and limitations of offline profiling techniques to systems with fine-grained communication. <p> The collection and management of performance data is done by an instrumentation system [WR95]. It is responsible for inserting the trace calls into the user's program, which can be done statically before compiling like in PICL <ref> [GHPW90] </ref>, or dynamically at run time like in Paradyn [MCC + 95]. The trace calls, along with the modified communication library, generate entries that go into the tracefile. A usual trace entry consists of the event type, the participating processors, and the time of the event. <p> A graphical output might add more convenience, but the statistics, sorted in descending order of time consuming functions, already give a good overview of the program behaviour. 2.2 Existing Profiling Tools Many parallel profiling tools have been developed over the recent years, including ParaGraph [HE91], PICL <ref> [GHPW90] </ref>, IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], AIMS [Yan94], Falcon [WEK + 95], VIZIR [HKWJ95], Vista [WRM + 96], ParAid [RAB + 93], and Paradyn [MCC + 95]. <p> Even though the run time insertion and removal of trace calls is expensive, Paradyn works well on long-running programs, where the offline approach is rendered impossible by the combined size of the tracefiles. An example of off-line monitoring is the combination of PICL <ref> [GHPW90] </ref> and ParaGraph [HE91]. PICL is a library that allows for collecting trace data to produce a file which is accepted as an input to the graphical interface ParaGraph. The instrumentation for the computational part is done explicitly by the user. <p> The overhead comes from generating two time stamps and logging a message at the end of the communication call. For send & receive communication, for which the PICL trace library was originally created, this overhead was already seen as a potential problem <ref> [GHPW90] </ref>. For the Split-C communication routines, which are an order of magnitude faster, the profiling overhead is even more significant. 11 flush-on-barrier, and with tracing using flush-on-full.
Reference: [GKM83] <author> S. Graham, P. Kessler, and M. McKusick. </author> <title> An Execution Profiler for Modular Programs. </title> <journal> Software-Practice and Experience, </journal> <volume> 13(8), </volume> <year> 1983. </year>
Reference-contexts: Since the above mentioned methods have the disadvantage of perturbing the program execution, we explored other solutions. We found that a statistical approach solves some of the problems imposed by an event trace profiler. An example of offline monitoring using statistics is the sequential profiler gprof <ref> [GKM83] </ref>. It produces statistics on how often a certain function is called and how often it is called recursively. This data is stored in a tracefile. The examination of the output data is done directly, so no graphical post-processing tool is needed. <p> To identify which functions take the most time, we modified the statistical profiler gprof <ref> [GKM83] </ref> to run on a parallel platform. Gprof collects the amount of time spent in one function as well as how often this function was called. We extended gprof to collect data on each node. These can then be combined into a single output or examined separately. <p> While these methods do not give a precise picture of what occurs in the parallel program, they can still provide very useful information. 15 5.3.1 Sampling For this approach, we modified the unix gprof <ref> [GKM83] </ref> utility to run on multiple processors. Gprof consists of a library, which is linked with the user code, and a program to parse the statistical information.
Reference: [HE91] <author> M. Heath and J. Etheridge. </author> <title> Visualizing the Performance of Parallel Programs. </title> <journal> IEEE Software, </journal> <volume> 8(5), </volume> <year> 1991. </year> <month> 31 </month>
Reference-contexts: For this reason, many profilers have already been developed. For distributed memory machines, most profilers are implemented for PVM, MPI, or similar communication mechanisms based on the send & receive protocol. The existing profilers include offline approaches like ParaGraph <ref> [HE91] </ref>, PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], or AIMS [Yan94]. Paradyn [MCC + 95] and Vista [WRM + 96] are examples of online monitoring systems. <p> We examine and address these limitations in the context of an offline performance profiler we developed for the parallel language Split-C running on a high-end MPP (a 64 processor Meiko CS-2). As the basis of our profiler we used PICL/ParaGraph <ref> [GHPW90, HE91] </ref>, which was originally designed for a send & receive protocol (MPI). We have adapted the profiler to the Active Message model underlying Split-C. We experimentally quantify the profiling overheads of our implementations and study the merits and limitations of offline profiling techniques to systems with fine-grained communication. <p> A graphical output might add more convenience, but the statistics, sorted in descending order of time consuming functions, already give a good overview of the program behaviour. 2.2 Existing Profiling Tools Many parallel profiling tools have been developed over the recent years, including ParaGraph <ref> [HE91] </ref>, PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], AIMS [Yan94], Falcon [WEK + 95], VIZIR [HKWJ95], Vista [WRM + 96], ParAid [RAB + 93], and Paradyn [MCC + 95]. <p> Even though the run time insertion and removal of trace calls is expensive, Paradyn works well on long-running programs, where the offline approach is rendered impossible by the combined size of the tracefiles. An example of off-line monitoring is the combination of PICL [GHPW90] and ParaGraph <ref> [HE91] </ref>. PICL is a library that allows for collecting trace data to produce a file which is accepted as an input to the graphical interface ParaGraph. The instrumentation for the computational part is done explicitly by the user.
Reference: [HKWJ95] <author> M. Hao, A. Karp, A. Waheed, and M. Jazayeri. VIZIR: </author> <title> An Integrated Environment for Distributed Program Visualization. </title> <booktitle> In Proceedings of Int. Workshop on Modeling, Analysis, </booktitle> <institution> and Simulation of Computer and Telecommunication Systems, Durham, NC, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Paradyn [MCC + 95] and Vista [WRM + 96] are examples of online monitoring systems. Systems such as Falcon [WEK + 95], VIZIR <ref> [HKWJ95] </ref>, or ParAid [RAB + 93] provide both online and offline monitoring. The target for our profiler is the fine-grained parallel programming language Split-C [CDG + 93a]. Split-C is based on a global address space memory model, and remote memory can be accessed via global pointers. <p> functions, already give a good overview of the program behaviour. 2.2 Existing Profiling Tools Many parallel profiling tools have been developed over the recent years, including ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], AIMS [Yan94], Falcon [WEK + 95], VIZIR <ref> [HKWJ95] </ref>, Vista [WRM + 96], ParAid [RAB + 93], and Paradyn [MCC + 95]. We discuss only a few, focusing on ParaGraph and PICL as event trace profilers, since we have extended these to create a profiling tool for Split-C.
Reference: [HM93] <author> M. Homewood and M. McLaren. </author> <title> Meiko CS-2 Interconnect Elan-Elite Design. </title> <booktitle> In Proc. of Hot Interconnects, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: After briefly describing this machine, we present performance numbers evaluating the profiler. 5.1 Meiko CS-2 The Meiko CS-2 consists of Sparc based nodes connected via a fat tree communication network <ref> [HM93] </ref>. Running a slightly enhanced version of the Solaris 2.3 operating system on every node, it closely resembles a cluster of workstations. Each node contains a 40 MHz SuperSparc processor with 1 MB external cache and 32 MB of main memory.
Reference: [HM95] <author> S. Hackstadt and A. Malony. </author> <title> Visualizing Parallel Programs and Performance. </title> <journal> IEEE Computer Graphics and Applications, </journal> <year> 1995. </year>
Reference-contexts: Collecting all the trace data is useless unless we can conveniently display it. Raw data does not tell what is going on in a user's program, so a good graphical output is crucial to the effectiveness of a profiler <ref> [HM95, RJ93, KS93] </ref>. The graphical display unit should identify the compute time, the idle time, the communication time (including the communication overhead), the message flow, as well as provide some mechanism to distinguish programming language specific from application level overhead. <p> send, receive, or idle), the timestamp, the processors that send and receive the message, the message length, and some PICL internal information. 4 ParaGraph is a graphical user interface which reads in a tracefile, such as the one produced by PICL, and displays the data in a convenient graphical output <ref> [HM95, RJ93, KS93] </ref>. The main aspects ParaGraph keeps track of are communication, computation, and tasks. The traces can be shown in different formats, such as a Gantt diagram over time, a communication profile, or as an animation showing what various processors are doing at a certain time (busy/idle/sending/receiving).
Reference: [IBM93] <author> IBM. </author> <title> VT A Visualization Tool, </title> <note> 1993. Online at: http://www.tc.cornell.edu/UserDoc/Software/PTools/vt/. </note>
Reference-contexts: For this reason, many profilers have already been developed. For distributed memory machines, most profilers are implemented for PVM, MPI, or similar communication mechanisms based on the send & receive protocol. The existing profilers include offline approaches like ParaGraph [HE91], PICL [GHPW90], IBM's VT <ref> [IBM93] </ref>, Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], or AIMS [Yan94]. Paradyn [MCC + 95] and Vista [WRM + 96] are examples of online monitoring systems. Systems such as Falcon [WEK + 95], VIZIR [HKWJ95], or ParAid [RAB + 93] provide both online and offline monitoring. <p> A graphical output might add more convenience, but the statistics, sorted in descending order of time consuming functions, already give a good overview of the program behaviour. 2.2 Existing Profiling Tools Many parallel profiling tools have been developed over the recent years, including ParaGraph [HE91], PICL [GHPW90], IBM's VT <ref> [IBM93] </ref>, Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], AIMS [Yan94], Falcon [WEK + 95], VIZIR [HKWJ95], Vista [WRM + 96], ParAid [RAB + 93], and Paradyn [MCC + 95].
Reference: [KS93] <author> E. Kraemer and J. Stasko. </author> <title> The Visualization of Parallel Systems: An Overview. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18, </volume> <year> 1993. </year>
Reference-contexts: Collecting all the trace data is useless unless we can conveniently display it. Raw data does not tell what is going on in a user's program, so a good graphical output is crucial to the effectiveness of a profiler <ref> [HM95, RJ93, KS93] </ref>. The graphical display unit should identify the compute time, the idle time, the communication time (including the communication overhead), the message flow, as well as provide some mechanism to distinguish programming language specific from application level overhead. <p> send, receive, or idle), the timestamp, the processors that send and receive the message, the message length, and some PICL internal information. 4 ParaGraph is a graphical user interface which reads in a tracefile, such as the one produced by PICL, and displays the data in a convenient graphical output <ref> [HM95, RJ93, KS93] </ref>. The main aspects ParaGraph keeps track of are communication, computation, and tasks. The traces can be shown in different formats, such as a Gantt diagram over time, a communication profile, or as an animation showing what various processors are doing at a certain time (busy/idle/sending/receiving).
Reference: [Lus93] <author> E. Lusk. </author> <title> Visualizing Parallel Program Behavior. </title> <booktitle> In High Performance Computing Symposium, </booktitle> <address> Arlington, VA, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: For this reason, many profilers have already been developed. For distributed memory machines, most profilers are implemented for PVM, MPI, or similar communication mechanisms based on the send & receive protocol. The existing profilers include offline approaches like ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot <ref> [Lus93] </ref>, VAMPIR [NAW + 96], Pablo [RRA + 93], or AIMS [Yan94]. Paradyn [MCC + 95] and Vista [WRM + 96] are examples of online monitoring systems. Systems such as Falcon [WEK + 95], VIZIR [HKWJ95], or ParAid [RAB + 93] provide both online and offline monitoring. <p> graphical output might add more convenience, but the statistics, sorted in descending order of time consuming functions, already give a good overview of the program behaviour. 2.2 Existing Profiling Tools Many parallel profiling tools have been developed over the recent years, including ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot <ref> [Lus93] </ref>, VAMPIR [NAW + 96], Pablo [RRA + 93], AIMS [Yan94], Falcon [WEK + 95], VIZIR [HKWJ95], Vista [WRM + 96], ParAid [RAB + 93], and Paradyn [MCC + 95].
Reference: [MCC + 95] <author> B. Miller, M. Callaghan, J. Cargille, J. Hollingsworth, B. Irvin, K. Karavanic, K. Kun-chithapadam, and T. Newhall. </author> <title> The Paradyn Parallel Performance Measurement Tool. </title> <journal> IEEE Computer, </journal> <volume> 28(11), </volume> <year> 1995. </year>
Reference-contexts: The existing profilers include offline approaches like ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], or AIMS [Yan94]. Paradyn <ref> [MCC + 95] </ref> and Vista [WRM + 96] are examples of online monitoring systems. Systems such as Falcon [WEK + 95], VIZIR [HKWJ95], or ParAid [RAB + 93] provide both online and offline monitoring. The target for our profiler is the fine-grained parallel programming language Split-C [CDG + 93a]. <p> The collection and management of performance data is done by an instrumentation system [WR95]. It is responsible for inserting the trace calls into the user's program, which can be done statically before compiling like in PICL [GHPW90], or dynamically at run time like in Paradyn <ref> [MCC + 95] </ref>. The trace calls, along with the modified communication library, generate entries that go into the tracefile. A usual trace entry consists of the event type, the participating processors, and the time of the event. <p> Existing Profiling Tools Many parallel profiling tools have been developed over the recent years, including ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], AIMS [Yan94], Falcon [WEK + 95], VIZIR [HKWJ95], Vista [WRM + 96], ParAid [RAB + 93], and Paradyn <ref> [MCC + 95] </ref>. We discuss only a few, focusing on ParaGraph and PICL as event trace profilers, since we have extended these to create a profiling tool for Split-C. <p> In order to trace a program we have to capture the communication pattern by monitoring the calls to the communication functions of the underlying protocol. This can be achieved either by an on-line or by an off-line monitoring tool. Paradyn <ref> [MCC + 95] </ref> or Vista [WRM + 96] are examples of online monitoring. Paradyn's goal is to significantly cut the overhead of collecting trace data by dynamically inserting trace calls during run time and, once the interesting part of the program is passed, to remove it again.
Reference: [NAW + 96] <author> W. Nagel, A. Arnold, M. Weber, H. C. Hoppe, et al. </author> <title> VAMPIR: Visualization and Analysis of MPI Resources. </title> <journal> Supercomputer, </journal> <volume> 12(1), </volume> <year> 1996. </year>
Reference-contexts: For this reason, many profilers have already been developed. For distributed memory machines, most profilers are implemented for PVM, MPI, or similar communication mechanisms based on the send & receive protocol. The existing profilers include offline approaches like ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR <ref> [NAW + 96] </ref>, Pablo [RRA + 93], or AIMS [Yan94]. Paradyn [MCC + 95] and Vista [WRM + 96] are examples of online monitoring systems. Systems such as Falcon [WEK + 95], VIZIR [HKWJ95], or ParAid [RAB + 93] provide both online and offline monitoring. <p> might add more convenience, but the statistics, sorted in descending order of time consuming functions, already give a good overview of the program behaviour. 2.2 Existing Profiling Tools Many parallel profiling tools have been developed over the recent years, including ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR <ref> [NAW + 96] </ref>, Pablo [RRA + 93], AIMS [Yan94], Falcon [WEK + 95], VIZIR [HKWJ95], Vista [WRM + 96], ParAid [RAB + 93], and Paradyn [MCC + 95]. <p> The traces can be shown in different formats, such as a Gantt diagram over time, a communication profile, or as an animation showing what various processors are doing at a certain time (busy/idle/sending/receiving). A commercial product based on ParaGraph is VAMPIR <ref> [NAW + 96] </ref>, which additionally can zoom into arbitrary parts and display the corresponding source code. Since the above mentioned methods have the disadvantage of perturbing the program execution, we explored other solutions. We found that a statistical approach solves some of the problems imposed by an event trace profiler.
Reference: [RAB + 93] <author> B. Ries, R. Anderson, D. Breazal, et al. </author> <title> The Paragon Performance Monitoring Environment. </title> <booktitle> In Proceedings SUPERCOMPUTING '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Paradyn [MCC + 95] and Vista [WRM + 96] are examples of online monitoring systems. Systems such as Falcon [WEK + 95], VIZIR [HKWJ95], or ParAid <ref> [RAB + 93] </ref> provide both online and offline monitoring. The target for our profiler is the fine-grained parallel programming language Split-C [CDG + 93a]. Split-C is based on a global address space memory model, and remote memory can be accessed via global pointers. Data is transferred using split-phase operations. <p> of the program behaviour. 2.2 Existing Profiling Tools Many parallel profiling tools have been developed over the recent years, including ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], AIMS [Yan94], Falcon [WEK + 95], VIZIR [HKWJ95], Vista [WRM + 96], ParAid <ref> [RAB + 93] </ref>, and Paradyn [MCC + 95]. We discuss only a few, focusing on ParaGraph and PICL as event trace profilers, since we have extended these to create a profiling tool for Split-C.
Reference: [RJ93] <author> D. Rover and C. Wright Jr. </author> <title> Visualizing the Performance of SPMD and Data-Parallel Programs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18, </volume> <year> 1993. </year>
Reference-contexts: Collecting all the trace data is useless unless we can conveniently display it. Raw data does not tell what is going on in a user's program, so a good graphical output is crucial to the effectiveness of a profiler <ref> [HM95, RJ93, KS93] </ref>. The graphical display unit should identify the compute time, the idle time, the communication time (including the communication overhead), the message flow, as well as provide some mechanism to distinguish programming language specific from application level overhead. <p> send, receive, or idle), the timestamp, the processors that send and receive the message, the message length, and some PICL internal information. 4 ParaGraph is a graphical user interface which reads in a tracefile, such as the one produced by PICL, and displays the data in a convenient graphical output <ref> [HM95, RJ93, KS93] </ref>. The main aspects ParaGraph keeps track of are communication, computation, and tasks. The traces can be shown in different formats, such as a Gantt diagram over time, a communication profile, or as an animation showing what various processors are doing at a certain time (busy/idle/sending/receiving).
Reference: [RRA + 93] <author> D. Reed, P. Roth, R. Aydt, K. Shields, et al. </author> <title> Scalable Performance Analysis: the Pablo Performance Analysis Environment. </title> <booktitle> In Scalable Parallel Libraries Conference, </booktitle> <institution> Mississippi State, MS, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: For distributed memory machines, most profilers are implemented for PVM, MPI, or similar communication mechanisms based on the send & receive protocol. The existing profilers include offline approaches like ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo <ref> [RRA + 93] </ref>, or AIMS [Yan94]. Paradyn [MCC + 95] and Vista [WRM + 96] are examples of online monitoring systems. Systems such as Falcon [WEK + 95], VIZIR [HKWJ95], or ParAid [RAB + 93] provide both online and offline monitoring. <p> but the statistics, sorted in descending order of time consuming functions, already give a good overview of the program behaviour. 2.2 Existing Profiling Tools Many parallel profiling tools have been developed over the recent years, including ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo <ref> [RRA + 93] </ref>, AIMS [Yan94], Falcon [WEK + 95], VIZIR [HKWJ95], Vista [WRM + 96], ParAid [RAB + 93], and Paradyn [MCC + 95]. We discuss only a few, focusing on ParaGraph and PICL as event trace profilers, since we have extended these to create a profiling tool for Split-C.
Reference: [SM93] <author> S. Sarukkai and A. Malony. </author> <title> Perturbation Analysis of High Level Instrumentation for SPMD Programs. </title> <journal> SIGPLAN Notices, </journal> <volume> 28(7), </volume> <year> 1993. </year>
Reference-contexts: For collecting data, the user program is modified to recognize certain events and to record them for display. Note that collecting and displaying data will perturb the user program <ref> [SM93] </ref>. This is especially problematic in the context of low-overhead communication mechanisms. There are two different approaches to profiling: Online monitoring directly displays the important features during run time; while offline monitoring collects data during the execution and runs the actual profiling tool afterwards. Online monitoring works like a debugger.
Reference: [SS95] <author> K. E. Schauser and C. J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In 9th International Parallel Processing Symposium, </booktitle> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: It contains a DMA engine, as well as a micro-encoded Sparc engine which can run user-level code and directly access the network. Using Active Messages, communication between the processors takes about 12 s and achieves a bandwidth of about 40 MB/s <ref> [SS95] </ref>. 5.2 Data Collection Overhead The addition of profiling code perturbs the execution of the program. <p> Most of the bulk operations have around 64% overhead. To understand the performance of the bulk operations, we must examine the underlying Split-C implementation, which is described in more detail in <ref> [SS95] </ref>. Bulk get and bulk read are essentially the same operation: a message is sent to the destination, which triggers the data transfer back to the requester.
Reference: [SSFK96] <author> K. E. Schauser, C. J. Scheiman, J. M. Ferguson, and P. Z. Kolano. </author> <title> Exploiting the Capabilities of Communications Co-processors. </title> <booktitle> In 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year> <month> 32 </month>
Reference-contexts: To reduce the times, polls must be added. One way to do this on the Meiko CS-2 is to have the communications co-processor do the polling 25 16 processors of the Meiko CS-2. and service read requests. We have designed another Split-C implementation using this approach <ref> [SSFK96] </ref>, and the optimized numbers are shown in the rightmost column of the table. They substantially reduce the wait times to 23-33 s. But these efficient times are gained by using a different version of Split-C which makes use of the co-processor.
Reference: [vECGS92] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: a Mech--anism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Data is transferred using split-phase operations. That is, a request and response are separate operations and a requesting processor does not have to wait for a reply before continuing its work. It can delay synchronization until the data is needed. 1 Most Split-C implementations are based on Active Messages <ref> [vECGS92] </ref>, a fast communication mechanism. Each Active Message has a handler function associated with it which is executed on the destination processor when the message arrives.
Reference: [WEK + 95] <author> G. Weiming, G. Eisenhauer, E. Kramer, K. Schwan, et al. </author> <title> Falcon: On-line Monitoring and Steering of large-scale Parallel Programs. </title> <booktitle> In The Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: The existing profilers include offline approaches like ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], or AIMS [Yan94]. Paradyn [MCC + 95] and Vista [WRM + 96] are examples of online monitoring systems. Systems such as Falcon <ref> [WEK + 95] </ref>, VIZIR [HKWJ95], or ParAid [RAB + 93] provide both online and offline monitoring. The target for our profiler is the fine-grained parallel programming language Split-C [CDG + 93a]. Split-C is based on a global address space memory model, and remote memory can be accessed via global pointers. <p> order of time consuming functions, already give a good overview of the program behaviour. 2.2 Existing Profiling Tools Many parallel profiling tools have been developed over the recent years, including ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], AIMS [Yan94], Falcon <ref> [WEK + 95] </ref>, VIZIR [HKWJ95], Vista [WRM + 96], ParAid [RAB + 93], and Paradyn [MCC + 95]. We discuss only a few, focusing on ParaGraph and PICL as event trace profilers, since we have extended these to create a profiling tool for Split-C.
Reference: [WR95] <author> A. Waheed and D. </author> <title> Rover. A Structured Approach to Instrumentation System Development and Evaluation. </title> <booktitle> In Proceedings SUPERCOMPUTING '95, </booktitle> <address> San Diego, CA, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: A statistical approach, on the other hand, will only record a sampling of the information. We will discuss statistical approaches later, but for now will focus on the traditional online monitoring which logs all relevant information. The collection and management of performance data is done by an instrumentation system <ref> [WR95] </ref>. It is responsible for inserting the trace calls into the user's program, which can be done statically before compiling like in PICL [GHPW90], or dynamically at run time like in Paradyn [MCC + 95]. <p> Traditionally, there have been two ways to do so: Flushing whenever a buffer on the node is full, and flush all the buffers when one of the node buffers is full. Detailed studies have shown that the second strategy is more efficient <ref> [WR95] </ref>. Unfortunately, it is much harder to implement because asynchronous barriers are required. Therefore most existing tools rely on the first strategy. In this paper we introduce a third approach: flush-on-barriers, which attempts to combine the benefits of both strategies.
Reference: [WRM + 96] <author> A. Waheed, D. Rover, M. Mutka, A. Bakic, et al. </author> <title> Vista: a Framework for Instrumentation System Design for Multidisciplinary Applications. </title> <booktitle> In Proceedings of the Fourth International Workshop on Modeling, Analysis, </booktitle> <institution> and Simulation of Computer and Telecommunication Systems, </institution> <address> San Jose, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: The existing profilers include offline approaches like ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], or AIMS [Yan94]. Paradyn [MCC + 95] and Vista <ref> [WRM + 96] </ref> are examples of online monitoring systems. Systems such as Falcon [WEK + 95], VIZIR [HKWJ95], or ParAid [RAB + 93] provide both online and offline monitoring. The target for our profiler is the fine-grained parallel programming language Split-C [CDG + 93a]. <p> give a good overview of the program behaviour. 2.2 Existing Profiling Tools Many parallel profiling tools have been developed over the recent years, including ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], AIMS [Yan94], Falcon [WEK + 95], VIZIR [HKWJ95], Vista <ref> [WRM + 96] </ref>, ParAid [RAB + 93], and Paradyn [MCC + 95]. We discuss only a few, focusing on ParaGraph and PICL as event trace profilers, since we have extended these to create a profiling tool for Split-C. <p> In order to trace a program we have to capture the communication pattern by monitoring the calls to the communication functions of the underlying protocol. This can be achieved either by an on-line or by an off-line monitoring tool. Paradyn [MCC + 95] or Vista <ref> [WRM + 96] </ref> are examples of online monitoring. Paradyn's goal is to significantly cut the overhead of collecting trace data by dynamically inserting trace calls during run time and, once the interesting part of the program is passed, to remove it again.
Reference: [Yan94] <author> J. Yan. </author> <title> Performance Tuning with AIMSAn Automated Instrumentation and Monitoring System for Multicomputers. </title> <booktitle> In Proceedings of the Twenty-Seventh Hawaii Int. Conf. on System Sciences, Hawaii, </booktitle> <month> January </month> <year> 1994. </year> <month> 33 </month>
Reference-contexts: For distributed memory machines, most profilers are implemented for PVM, MPI, or similar communication mechanisms based on the send & receive protocol. The existing profilers include offline approaches like ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], or AIMS <ref> [Yan94] </ref>. Paradyn [MCC + 95] and Vista [WRM + 96] are examples of online monitoring systems. Systems such as Falcon [WEK + 95], VIZIR [HKWJ95], or ParAid [RAB + 93] provide both online and offline monitoring. <p> in descending order of time consuming functions, already give a good overview of the program behaviour. 2.2 Existing Profiling Tools Many parallel profiling tools have been developed over the recent years, including ParaGraph [HE91], PICL [GHPW90], IBM's VT [IBM93], Upshot [Lus93], VAMPIR [NAW + 96], Pablo [RRA + 93], AIMS <ref> [Yan94] </ref>, Falcon [WEK + 95], VIZIR [HKWJ95], Vista [WRM + 96], ParAid [RAB + 93], and Paradyn [MCC + 95]. We discuss only a few, focusing on ParaGraph and PICL as event trace profilers, since we have extended these to create a profiling tool for Split-C.
References-found: 26

