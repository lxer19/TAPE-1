URL: http://www.cs.toronto.edu/~ftp/pub/reports/na/uw-cs-91-33.ps.Z
Refering-URL: http://www.cs.toronto.edu/NA/reports.html
Root-URL: 
Title: Adaptive Linear Equation Solvers in Codes for Large Stiff Systems of ODEs  
Author: K. R. Jackson and W. L. Seward 
Address: Waterloo, Ontario Canada N2L 3G1  
Affiliation: Department of Computer Science University of Waterloo  
Abstract: Research Report CS-91-33 July 1991 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Behie and P. Forsyth, </author> <title> Comparison of fast iterative methods for symmetric systems, </title> <journal> IMA J. Numer. Anal., </journal> <volume> 3 (1983), </volume> <pages> pp. </pages> <month> 41-63. </month> <title> [2] , Incomplete factorization methods for fully implicit simulation of enhanced oil recovery, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 5 (1984), </volume> <pages> pp. 543 - 561. </pages>
Reference-contexts: We note that diagonal scaling is also used with the ILU preconditioners, as it is often found to improve their performance. These preconditioning strategies can be applied to any matrix and have been found to work well in solving time-dependent PDEs. See, for example, <ref> [1, 2] </ref>. We also develop criteria for switching these preconditioners off. In the next section, we briefly describe the codes that were used in this investigation. Heuristics for identifying when to switch preconditioning on and off are discussed in x3. <p> Our type-insensitive ODE solver was developed by altering the SPRINT BDFs module corresponding to the changes made to evolve LSODE into LSODA. The iterative solver, WATSIT, used in our study was developed at the University of Waterloo based on methods described in <ref> [1] </ref> and [2]. This code is designed to solve systems of linear equations of the form Ax = b where the matrix A is large and sparse. The basic technique is an incomplete factorization scheme with acceleration. <p> This value provides a rough comparison of the work being done by the iterative method in the various approaches. For problem (9), it is also interesting to compare the work done in the nonstiff and stiff regimes, which we characterize by the intervals <ref> [0; 1] </ref> and [1; 10] for convenience. Values are reported in Table 23. As expected, AVDIM increases when diagonal scaling is used and also when the type-insensitive method is used. The iterative solver works considerably harder in the stiff regime. <p> This value provides a rough comparison of the work being done by the iterative method in the various approaches. For problem (9), it is also interesting to compare the work done in the nonstiff and stiff regimes, which we characterize by the intervals [0; 1] and <ref> [1; 10] </ref> for convenience. Values are reported in Table 23. As expected, AVDIM increases when diagonal scaling is used and also when the type-insensitive method is used. The iterative solver works considerably harder in the stiff regime. The values for AVDIM found here are similar to those reported in [6]. <p> The iterative solver works considerably harder in the stiff regime. The values for AVDIM found here are similar to those reported in [6]. Table 23 AVDIM | ratio of linear to Newton iterations ILU (0) No switch Switch Type-ins Combined <ref> [0; 1] </ref> 1.82 1.90 2.88 2.97 ILU (1) [1; 10] 5.10 4.22 4.58 4.25 5. Conclusions. We have developed a strategy for the adaptive choice of preconditioning when an iterative linear solver is used in an ODE code. <p> The iterative solver works considerably harder in the stiff regime. The values for AVDIM found here are similar to those reported in [6]. Table 23 AVDIM | ratio of linear to Newton iterations ILU (0) No switch Switch Type-ins Combined [0; 1] 1.82 1.90 2.88 2.97 ILU (1) <ref> [1; 10] </ref> 5.10 4.22 4.58 4.25 5. Conclusions. We have developed a strategy for the adaptive choice of preconditioning when an iterative linear solver is used in an ODE code.
Reference: [3] <author> M. Berzins and R. Furzeland, </author> <title> A user's manual for SPRINT a versatile software package for solving systems of algebraic, ordinary and partial differential equations: Part 1 Algebraic and ordinary differential equations, </title> <type> Tech. Rep. </type> <institution> TNER.85.058, Thornton Research Centre, Shell Research Ltd., Thornton, U.K., </institution> <year> 1985. </year> <title> [4] , An adaptive method for the solution of stiff and non-stiff differential equations, </title> <type> tech. rep., </type> <institution> School of Computer Studies, Leeds University, Leeds, UK, </institution> <year> 1990. </year>
Reference-contexts: Test problems and numerical results are presented in both x3 and x4 to illustrate the performance of the techniques. We end with some conclusions in x5. 2. Codes Used in the Investigation. The ODE package used in this investigation was SPRINT <ref> [3] </ref>. This package is designed to offer to a user a range of methods for the numerical solution of systems of IVPs in ODEs. It allows the user to select from four time-stepping methods and three linear algebra packages to create the complete method appropriate for a particular problem.
Reference: [5] <author> P. Brown and A. Hindmarsh, </author> <title> Matrix-free methods for stiff systems of ODE's, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 23 (1986), </volume> <pages> pp. </pages> <month> 610-638. </month> <title> [6] , Reduced storage matrix methods in stiff ODE systems, </title> <journal> J. Appl. Math. Comp., </journal> <volume> 31 (1989), </volume> <pages> pp. 40-91. </pages>
Reference-contexts: 1. Introduction. In recent years, there have been several investigations of the use of iterative linear equation solvers in codes for the numerical solution of large systems of stiff initial-value problems (IVPs) for ordinary differential equations (ODEs). See, for example, <ref> [5, 6, 8, 11] </ref>. Such work has established clearly the potential effectiveness of the combination of these methods, but there are many open questions concerning both the choice of iterative method and the way in which it interacts with strategies used in the ODE solver. <p> In the case where an iterative linear equation solver is used to find y l n , the residual d l n is the final residual of the linear iteration. Brown and Hindmarsh <ref> [5] </ref> consider iterative linear equation solvers based on Arnoldi's method for use in the ODE code LSODE [12]. In their approach, the matrix vector product required in the iterative method is approximated by a finite difference based on f . <p> Later the method may switch to a more expensive preconditioner described below. It can switch back to diagonal scaling and to the Adams formulas. The basic iterative linear equation solver that we use is Orthomin [18]. Other reasonable choices include GMRES <ref> [5] </ref> and the stabilized version of the conjugate-gradient-squared (CGS) algorithm [17]. To focus on adaptive preconditioning, we chose to use one basic iterative solver only. We selected the well-known scheme Or-thomin, in part because an effective implementation of it along with several precon-ditioners was readily available to us. <p> So there is little point to omitting diagonal scaling if W l n is computed explicitly. Furthermore, diagonal scaling can be used with other approaches to forming W l n (see <ref> [5] </ref>, for example, for a discussion of scaling with "matrix-free" methods), but we have not attempted any elaboration of our basic approach. For brevity, in the remainder of the paper, we occasionally refer to an iterative method with diagonal scaling only as an unpreconditioned method. <p> The user has the choice of diagonal scaling only or preconditioning based on either Level 0 or Level 1 incomplete LU factorization as described earlier in this paper. In Brown and Hindmarsh <ref> [5] </ref>, convergence criteria for the iterative method are discussed in detail in the context of an ODE code. Both Brown and Hindmarsh [5] and Chan and Jackson [8] find that the residual reduction condition (4) is overly expensive | it forces linear iterations that do not seem to improve the accuracy <p> In Brown and Hindmarsh <ref> [5] </ref>, convergence criteria for the iterative method are discussed in detail in the context of an ODE code. Both Brown and Hindmarsh [5] and Chan and Jackson [8] find that the residual reduction condition (4) is overly expensive | it forces linear iterations that do not seem to improve the accuracy of the ODE solution significantly. In [5], the authors propose and justify a strategy that accepts the result produced by the iterative <p> Both Brown and Hindmarsh <ref> [5] </ref> and Chan and Jackson [8] find that the residual reduction condition (4) is overly expensive | it forces linear iterations that do not seem to improve the accuracy of the ODE solution significantly. In [5], the authors propose and justify a strategy that accepts the result produced by the iterative linear solver if the residual is some small fraction of the tolerance level required on the solution of the nonlinear system. A similar strategy was developed in [8]. <p> A similar strategy was developed in [8]. We adopt this approach and also use the same fraction that is suggested in <ref> [5] </ref>, namely, 1=20. The residual is measured in the weighted norm used by the ODE solver. Since the acceleration methods used in WATSIT return the residual directly, we do not need to scale the matrix to obtain the weighted norm, as is done in [5]. <p> same fraction that is suggested in <ref> [5] </ref>, namely, 1=20. The residual is measured in the weighted norm used by the ODE solver. Since the acceleration methods used in WATSIT return the residual directly, we do not need to scale the matrix to obtain the weighted norm, as is done in [5]. The linear iteration is started with a zero vector as its initial guess and always does at least one iteration. <p> An obvious strategy for assessing when the diagonally-scaled method is using "too many" iterations is to wait until the iteration fails to meet the convergence criterion in the maximum number of iterations. Frequently, this maximum number is set fairly small | for example, Brown and Hindmarsh <ref> [5] </ref> found 5 iterations to work well. In this case, switching preconditioning on when the diagonally-scaled iteration "fails" is not unreasonable. In our tests, we have used a maximum value of 10 iterations to try to assess the switching strategy independently of convergence failures. <p> When a differential equation yields linear systems with clustered eigenvalues, un-preconditioned and diagonally-scaled iterative methods can be very effective. In <ref> [5] </ref>, the authors use a reaction-diffusion system with two species in three space dimensions as a test problem. <p> Near the steady state, the interaction terms dominate the Jacobian and the dominant part of the spectrum is clustered in the interval 10 6 to 10 6 (1 + ff). Brown and Hindmarsh <ref> [5] </ref> point out that, consequently, unpreconditioned iterative methods are expected to work well unless ff is large. It does not necessarily follow that diagonal scaling will work well, but, in this case, it does since all eigenvalues of the scaled matrix are of moderate size. In [5], the ODE problem is <p> Brown and Hindmarsh <ref> [5] </ref> point out that, consequently, unpreconditioned iterative methods are expected to work well unless ff is large. It does not necessarily follow that diagonal scaling will work well, but, in this case, it does since all eigenvalues of the scaled matrix are of moderate size. In [5], the ODE problem is solved using a relative error tolerance of 10 6 and an absolute error tolerance of 10 8 . <p> This seems to be due, at least in part, to accuracy considerations related to the use of the diagonally-scaled iterative solver. Following Brown and Hindmarsh <ref> [5] </ref>, the solution of the linear system is accepted if kd l 9 where d l n is the residual at the last linear iteration. <p> Checking the Gerschgorin circles of the unscaled matrix confirms that in the tests above, whenever the code switched preconditioning off, the circles of the unscaled matrix made up one small cluster. This is not the case for the following test problem, used in <ref> [5] </ref> and [7]. <p> 0) = 10 6 ff (x)fi (z); c 2 (x; z; 0) = 10 12 ff (x)fi (z); fi (z) = 1 (0:1z 4) 2 + (0:1z 4) 4 =2: If the differential equations are discretized using central differences for both the diffusion and convection terms, as is done in <ref> [5] </ref> and [7], the eigenvalues of the Jacobian are found in two clusters, as discussed in [5]. The problem was solved with m = 9 subdivisions (200 equations) and relative and absolute error tolerances of 10 5 and 10 3 respectively. <p> (x)fi (z); fi (z) = 1 (0:1z 4) 2 + (0:1z 4) 4 =2: If the differential equations are discretized using central differences for both the diffusion and convection terms, as is done in <ref> [5] </ref> and [7], the eigenvalues of the Jacobian are found in two clusters, as discussed in [5]. The problem was solved with m = 9 subdivisions (200 equations) and relative and absolute error tolerances of 10 5 and 10 3 respectively. <p> Ozone production model. In x3, equation (8) was discretized using centered differences for both the diffusion and convection terms, following <ref> [5] </ref> and [7]. 15 Table 10 ILU (1) preconditioning | 2000 equations No switch Switch Type-ins Combined NFCN 1414 1334 1439 1444 NJAC 65 60 42 44 NNWT 613 595 365 346 NLIN 631 697 379 437 TIME 328 259 260 227 Table 11 ILU (0) preconditioning | 5488 equations No
Reference: [7] <author> G. Byrne, </author> <title> Pragmatic experiments with Krylov methods in the stiff ODE setting, </title> <booktitle> in Proc. IMA Conference on Computational Ordinary Differential Equations, </booktitle> <year> 1989, </year> <editor> J. Cash and I. Glad-well, eds., </editor> <address> London, 1989, </address> <publisher> Oxford University Press. </publisher> <pages> 22 </pages>
Reference-contexts: Checking the Gerschgorin circles of the unscaled matrix confirms that in the tests above, whenever the code switched preconditioning off, the circles of the unscaled matrix made up one small cluster. This is not the case for the following test problem, used in [5] and <ref> [7] </ref>. <p> 10 6 ff (x)fi (z); c 2 (x; z; 0) = 10 12 ff (x)fi (z); fi (z) = 1 (0:1z 4) 2 + (0:1z 4) 4 =2: If the differential equations are discretized using central differences for both the diffusion and convection terms, as is done in [5] and <ref> [7] </ref>, the eigenvalues of the Jacobian are found in two clusters, as discussed in [5]. The problem was solved with m = 9 subdivisions (200 equations) and relative and absolute error tolerances of 10 5 and 10 3 respectively. <p> Ozone production model. In x3, equation (8) was discretized using centered differences for both the diffusion and convection terms, following [5] and <ref> [7] </ref>. 15 Table 10 ILU (1) preconditioning | 2000 equations No switch Switch Type-ins Combined NFCN 1414 1334 1439 1444 NJAC 65 60 42 44 NNWT 613 595 365 346 NLIN 631 697 379 437 TIME 328 259 260 227 Table 11 ILU (0) preconditioning | 5488 equations No switch Switch
Reference: [8] <author> T. Chan and K. Jackson, </author> <title> The use of iterative linear-equation solvers in codes for large systems of stiff IVPs for ODEs, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7 (1986), </volume> <pages> pp. 378 - 417. </pages>
Reference-contexts: 1. Introduction. In recent years, there have been several investigations of the use of iterative linear equation solvers in codes for the numerical solution of large systems of stiff initial-value problems (IVPs) for ordinary differential equations (ODEs). See, for example, <ref> [5, 6, 8, 11] </ref>. Such work has established clearly the potential effectiveness of the combination of these methods, but there are many open questions concerning both the choice of iterative method and the way in which it interacts with strategies used in the ODE solver. <p> These strategies require formation and storage of at least part of W l n in general but are still very efficient in terms of storage. The authors find that, with preconditioning, a wider class of problems can be solved. Chan and Jackson <ref> [8] </ref> also investigate both unpreconditioned and preconditioned Krylov-type iterative methods used with LSODE. The methods they use are not "matrix-free" | W l n is formed, stored and used explicitly to compute the matrix-vector products needed in the iterative method. <p> In Brown and Hindmarsh [5], convergence criteria for the iterative method are discussed in detail in the context of an ODE code. Both Brown and Hindmarsh [5] and Chan and Jackson <ref> [8] </ref> find that the residual reduction condition (4) is overly expensive | it forces linear iterations that do not seem to improve the accuracy of the ODE solution significantly. <p> In [5], the authors propose and justify a strategy that accepts the result produced by the iterative linear solver if the residual is some small fraction of the tolerance level required on the solution of the nonlinear system. A similar strategy was developed in <ref> [8] </ref>. We adopt this approach and also use the same fraction that is suggested in [5], namely, 1=20. The residual is measured in the weighted norm used by the ODE solver. <p> It is well-known that the eigen-values of the associated Jacobian matrix are widely spread without clustering and that unpreconditioned and diagonally-scaled iterative methods are generally inefficient. As described in <ref> [8] </ref>, the PDE has the form u t = u xx + u yy + u zz ;(6) on the unit square with homogeneous Dirichlet boundary conditions, t 2 [0; 10:24] and initial condition u (0; x; y; z) = 64x (1 x)y (1 y)z (1 z): The spatial derivatives were
Reference: [9] <author> R. Dembo, S. Eisenstat, and T. Steihaug, </author> <title> Inexact Newton methods, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 19 (1982), </volume> <pages> pp. 400-408. </pages>
Reference-contexts: If W l n is sparse, though, an iterative linear algebra method may be an effective means of reducing this cost. When an iterative linear equation solver is used in the solve phase of Newton's method, an inexact Newton method <ref> [9] </ref> is obtained.
Reference: [10] <author> C. Gear, </author> <title> Numerical Initial Value Problems in Ordinary Differential Equations, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1971. </year>
Reference-contexts: We study the numerical solution of large systems of stiff IVPs for ODEs of the form y 0 = f (t; y); y (t 0 ) given.(1) Due to the stiffness, such problems are usually discretized using an implicit numerical method, most often the Backward Differentiation Formulas (BDFs) <ref> [10] </ref>. We will concentrate on the BDFs, although many of our ideas apply to implicit numerical methods in general. <p> This approach has been discussed by several authors for a variety of formulas. See, for example, [4, 13, 14, 15]. Petzold [14] developed a type-insensitive method based on the Adams formulas <ref> [10] </ref> and the BDFs. Her method starts integrating with the Adams formulas and monitors the timestep, the eigenvalues of the Jacobian (indirectly) and the region of absolute stability. <p> 2 ); f 2 (c 1 ; c 2 ) = c 2 (b 2 a 21 c 1 a 22 c 2 ); b 1 = b 2 = (1 + ffxyz)(10 6 1 + 10 6 ): The equations are defined on the unit cube with t 2 <ref> [0; 10] </ref>, homogeneous Neumann boundary conditions and initial conditions c 1 (x; y; z; 0) = 500 + 250 cos (x) cos (3y) cos (10z); The authors point out that, as t ! 1, the solution approaches a steady state which is given roughly by the asymptotic solution of the problem <p> a ij = 10 4 ; i &gt; p; j p; a ij = 0; for all other i and j; b i = (1 + ffxyz); i p; b i = (1 + ffxyz); i &gt; p: The system (9) is defined on the unit square with t 2 <ref> [0; 10] </ref>. The problem has homogeneous Neumann boundary conditions and initial conditions c i (x; y; 0) = 10 + i [16x (1 x)y (1 y)] 2 ; 1 i s: Here, we take s = 20 (p = 10) and ff = 50. <p> This value provides a rough comparison of the work being done by the iterative method in the various approaches. For problem (9), it is also interesting to compare the work done in the nonstiff and stiff regimes, which we characterize by the intervals [0; 1] and <ref> [1; 10] </ref> for convenience. Values are reported in Table 23. As expected, AVDIM increases when diagonal scaling is used and also when the type-insensitive method is used. The iterative solver works considerably harder in the stiff regime. The values for AVDIM found here are similar to those reported in [6]. <p> The iterative solver works considerably harder in the stiff regime. The values for AVDIM found here are similar to those reported in [6]. Table 23 AVDIM | ratio of linear to Newton iterations ILU (0) No switch Switch Type-ins Combined [0; 1] 1.82 1.90 2.88 2.97 ILU (1) <ref> [1; 10] </ref> 5.10 4.22 4.58 4.25 5. Conclusions. We have developed a strategy for the adaptive choice of preconditioning when an iterative linear solver is used in an ODE code.
Reference: [11] <author> C. Gear and Y. Saad, </author> <title> Iterative solution of linear equations in ODE codes, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 4 (1983), </volume> <pages> pp. 583 - 601. </pages>
Reference-contexts: 1. Introduction. In recent years, there have been several investigations of the use of iterative linear equation solvers in codes for the numerical solution of large systems of stiff initial-value problems (IVPs) for ordinary differential equations (ODEs). See, for example, <ref> [5, 6, 8, 11] </ref>. Such work has established clearly the potential effectiveness of the combination of these methods, but there are many open questions concerning both the choice of iterative method and the way in which it interacts with strategies used in the ODE solver.
Reference: [12] <author> A. Hindmarsh, LSODE and LSODI, </author> <title> two new initial value ordinary differential equation solvers, </title> <journal> ACM SIGNUM Newsletter, </journal> <year> (1980), </year> <pages> pp. 10-11. </pages>
Reference-contexts: Brown and Hindmarsh [5] consider iterative linear equation solvers based on Arnoldi's method for use in the ODE code LSODE <ref> [12] </ref>. In their approach, the matrix vector product required in the iterative method is approximated by a finite difference based on f . This approach avoids explicitly forming the iteration matrix W l n needed in Newton's method and hence has been referred to as a "matrix-free" method. <p> The particular ODE method used in this investigation is based on the BDFs and is similar in implementation to the well-known code LSODE <ref> [12] </ref>. A variant of LSODE, called LSODA, was written by Hindmarsh and Petzold to incorporate the ideas for a type-insensitive method described in [14]. Our type-insensitive ODE solver was developed by altering the SPRINT BDFs module corresponding to the changes made to evolve LSODE into LSODA.
Reference: [13] <author> S. Norsett and P. Thomsen, </author> <title> Switching between modified Newton and fix-point iteration for implicit ODE-solvers, </title> <journal> BIT, </journal> <volume> 26 (1986), </volume> <pages> pp. 339-348. </pages>
Reference-contexts: A type-insensitive code switches between methods appropriate for nonstiff and stiff IVPs depending on the nature of the problem. This approach has been discussed by several authors for a variety of formulas. See, for example, <ref> [4, 13, 14, 15] </ref>. Petzold [14] developed a type-insensitive method based on the Adams formulas [10] and the BDFs. Her method starts integrating with the Adams formulas and monitors the timestep, the eigenvalues of the Jacobian (indirectly) and the region of absolute stability.
Reference: [14] <author> L. Petzold, </author> <title> Automatic selection of methods for solving stiff and nonstiff systems of ordinary differential equations, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 4 (1983), </volume> <pages> pp. 136 - 148. </pages>
Reference-contexts: A type-insensitive code switches between methods appropriate for nonstiff and stiff IVPs depending on the nature of the problem. This approach has been discussed by several authors for a variety of formulas. See, for example, <ref> [4, 13, 14, 15] </ref>. Petzold [14] developed a type-insensitive method based on the Adams formulas [10] and the BDFs. Her method starts integrating with the Adams formulas and monitors the timestep, the eigenvalues of the Jacobian (indirectly) and the region of absolute stability. <p> A type-insensitive code switches between methods appropriate for nonstiff and stiff IVPs depending on the nature of the problem. This approach has been discussed by several authors for a variety of formulas. See, for example, [4, 13, 14, 15]. Petzold <ref> [14] </ref> developed a type-insensitive method based on the Adams formulas [10] and the BDFs. Her method starts integrating with the Adams formulas and monitors the timestep, the eigenvalues of the Jacobian (indirectly) and the region of absolute stability. <p> When integrating with the BDFs, the code continues to monitor the same information and will switch back to the Adams formulas if it expects to maintain the same stepsize after the switch. We have combined the ideas from <ref> [14] </ref> with our adaptive approach to preconditioning. Our code starts integrating with the Adams formulas and switches to the BDFs as described in [14]. When this switch is made, the linear equation solver uses diagonal scaling only. Later the method may switch to a more expensive preconditioner described below. <p> We have combined the ideas from <ref> [14] </ref> with our adaptive approach to preconditioning. Our code starts integrating with the Adams formulas and switches to the BDFs as described in [14]. When this switch is made, the linear equation solver uses diagonal scaling only. Later the method may switch to a more expensive preconditioner described below. It can switch back to diagonal scaling and to the Adams formulas. The basic iterative linear equation solver that we use is Orthomin [18]. <p> The particular ODE method used in this investigation is based on the BDFs and is similar in implementation to the well-known code LSODE [12]. A variant of LSODE, called LSODA, was written by Hindmarsh and Petzold to incorporate the ideas for a type-insensitive method described in <ref> [14] </ref>. Our type-insensitive ODE solver was developed by altering the SPRINT BDFs module corresponding to the changes made to evolve LSODE into LSODA. The iterative solver, WATSIT, used in our study was developed at the University of Waterloo based on methods described in [1] and [2]. <p> If any circle crosses the axis into the left half plane, we consider the Gerschgorin ratio to be infinite. Here, we expect that the diagonally-scaled iterative method will have difficulty and therefore we do not switch preconditioning off. Van der Pol's equation (see, for example, <ref> [14] </ref>) is often used as an example of an ODE system in which transients recur throughout the integration. This second-order equation is frequently rewritten as system of two first-order ODEs.
Reference: [15] <author> L. Shampine, </author> <title> Type-insensitive ODE codes based on extrapolation methods, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 4 (1983), </volume> <pages> pp. 635 - 644. </pages>
Reference-contexts: A type-insensitive code switches between methods appropriate for nonstiff and stiff IVPs depending on the nature of the problem. This approach has been discussed by several authors for a variety of formulas. See, for example, <ref> [4, 13, 14, 15] </ref>. Petzold [14] developed a type-insensitive method based on the Adams formulas [10] and the BDFs. Her method starts integrating with the Adams formulas and monitors the timestep, the eigenvalues of the Jacobian (indirectly) and the region of absolute stability.
Reference: [16] <author> P. Sonneveld, </author> <title> CGS, a fast Lanczos-type solver for nonsymmetric systems, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10 (1989), </volume> <pages> pp. 36 - 52. </pages>
Reference-contexts: The basic technique is an incomplete factorization scheme with acceleration. The user has several choices for both the acceleration method and the incomplete factorization. Possible acceleration methods include the conjugate gradient method, 4 Orthomin [18], the conjugate-gradient-squared (CGS) algorithm <ref> [16] </ref> and a stabilized version of CGS [17]. As noted in x1, since we wish to focus on our adaptive strategy in this paper, we use Orthomin acceleration only in our numerical tests and have not encountered any test problems where it breaks down.
Reference: [17] <author> H. van de Worst and P. Sonneveld, CGSTAB, </author> <title> a more smoothly converging variant of CGS, </title> <type> Tech. Rep. 90 - 50, </type> <institution> Delft University of Technology, Delft, Netherlands, </institution> <year> 1990. </year>
Reference-contexts: It can switch back to diagonal scaling and to the Adams formulas. The basic iterative linear equation solver that we use is Orthomin [18]. Other reasonable choices include GMRES [5] and the stabilized version of the conjugate-gradient-squared (CGS) algorithm <ref> [17] </ref>. To focus on adaptive preconditioning, we chose to use one basic iterative solver only. We selected the well-known scheme Or-thomin, in part because an effective implementation of it along with several precon-ditioners was readily available to us. There are a variety of possible approaches for the "cheap" preconditioning. <p> The basic technique is an incomplete factorization scheme with acceleration. The user has several choices for both the acceleration method and the incomplete factorization. Possible acceleration methods include the conjugate gradient method, 4 Orthomin [18], the conjugate-gradient-squared (CGS) algorithm [16] and a stabilized version of CGS <ref> [17] </ref>. As noted in x1, since we wish to focus on our adaptive strategy in this paper, we use Orthomin acceleration only in our numerical tests and have not encountered any test problems where it breaks down.
Reference: [18] <author> P. Vinsome, Orthomin, </author> <title> an iterative method for solving sparse sets of simultaneous linear equations, </title> <journal> in Society of Petroleum Engineers of AIME, </journal> <note> Paper SPE 5729, 1976. 23 </note>
Reference-contexts: Frequently, the iterative methods investigated have been of the "Krylov subspace" type, e.g., the conjugate gradient method, Or-thomin <ref> [18] </ref> or GMRES [6]. In this paper, we also consider an iterative method of this type but our results should apply to a broader class since we are primarily concerned with the interaction between the iterative method and the other strategies of the ODE solver. <p> When this switch is made, the linear equation solver uses diagonal scaling only. Later the method may switch to a more expensive preconditioner described below. It can switch back to diagonal scaling and to the Adams formulas. The basic iterative linear equation solver that we use is Orthomin <ref> [18] </ref>. Other reasonable choices include GMRES [5] and the stabilized version of the conjugate-gradient-squared (CGS) algorithm [17]. To focus on adaptive preconditioning, we chose to use one basic iterative solver only. <p> The basic technique is an incomplete factorization scheme with acceleration. The user has several choices for both the acceleration method and the incomplete factorization. Possible acceleration methods include the conjugate gradient method, 4 Orthomin <ref> [18] </ref>, the conjugate-gradient-squared (CGS) algorithm [16] and a stabilized version of CGS [17]. As noted in x1, since we wish to focus on our adaptive strategy in this paper, we use Orthomin acceleration only in our numerical tests and have not encountered any test problems where it breaks down.
References-found: 15

