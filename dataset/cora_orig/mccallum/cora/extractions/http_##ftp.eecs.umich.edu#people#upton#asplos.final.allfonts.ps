URL: http://ftp.eecs.umich.edu/people/upton/asplos.final.allfonts.ps
Refering-URL: http://ftp.eecs.umich.edu/people/upton/
Root-URL: http://www.eecs.umich.edu
Keyword: Pipelining, decoupled architecture, prefetching, non-blocking cache, resource allocation, superscalar, oating point latencies.  
Abstract: This paper discusses the design of a high clock rate (300MHz) processor. The architecture is described, and the goals for the design are explained. The performance of three processor models is evaluated using trace-driven simulation. A cost model is used to estimate the resources required to build processors with varying sizes of on-chip memories, in both single and dual issue models. Recommendations are then made to increase the effectiveness of each of the models. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Diefendorff and M. Allen, </author> <title> Organization of the Motorola 88110 Superscalar RISC Microprocessor, </title> <journal> IEEE MICRO April, </journal> <year> 1992, </year> <pages> pp. 4063. </pages>
Reference-contexts: The state latches and forwarding network for one of our earlier prototypes account for 50% of the execution pipeline area [16]. One way to minimize the area penalty for state latches is to have a short execution pipeline. Like the Motorola 88K <ref> [1] </ref>, we have adopted variable length pipelines, reducing the length of the integer pipelines to four cycles. Memory instructions require an additional two or three cycles to produce their results. Virtual addresses are generated in either of two identical integer ALUs.
Reference: [2] <author> K. Diefendorff, R. Oehler, R. Hochsprung, </author> <title> Evolution of the PowerPC Architecture, </title> <booktitle> IEEE MICRO , April 1994, </booktitle> <pages> pp 34-49. </pages>
Reference-contexts: This document was created with FrameMaker 4.0.4 The IPU is similar to the IBM-Motorola PowerPC 603 and 604 processors <ref> [2] </ref> in that it includes a Bus Interface Unit (BIU), an Integer Execution Unit (IEU), an Instruction Fetch Unit (IFU), and a Load Store Unit (LSU). In addition to these the IPU has a dedicated Prefetch Unit (PFU) for data and instructions.
Reference: [3] <author> D. Ditzel and H. McLellan, </author> <title> Branch Folding in the CRISP Microprocessor: Reducing Branch Delay to Zero, </title> <booktitle> nual Symposium on Computer Architecture </booktitle>
Reference-contexts: If the instruction pair contains a control ow instruction, the NEXT field contains the cache index of the target instruction. This branch folding <ref> [3] </ref> reduces the critical path for a dual issue machine by eliminating the branch pipeline bubble.
Reference: [4] <author> D. Dobberpuhl et al., </author> <title> A 200-MHz 64-b Dual-Issue CMOS Microprocessor, </title> <journal> IEEE Journal of Solid-State Circuits , vol. </journal> <volume> 27 no. 11, </volume> <pages> pp. 1555-1567, </pages> <month> Nov. </month> <year> 1992 </year>
Reference-contexts: Because architectural branch delay slots significantly complicate the design of superscalar machines, they have been eliminated from more recent architectures such as the DEC Alpha <ref> [4] </ref>. The branch delay slot causes problems for a superscalar machine in many of the same ways that variable length instructions cause problems for CISC machines. The delay slot may lie on the next cache line.
Reference: [5] <author> J. Gee, M. Hill, D. Pnevmatikatos et al., </author> <title> Cache Performance of the SPEC 92 Benchmark Suite, </title> <note> IEEE MICRO , August 1993 pp 17-27. </note>
Reference-contexts: The base model instruction cache hit rate is 96.5% and data cache hit rate is 95.4%; these numbers agree with those published in <ref> [5] </ref>. 5.1 Model Performance Evaluation We first examine the performance of each of the three models. For each graph the cost difference between the two curves is caused by the addition of a second execution pipeline, which has a cost of 8192 RBEs.
Reference: [6] <author> P. Hsu, </author> <title> Designing the TFP Microprocessor, </title> <booktitle> IEEE MICRO April 1994, </booktitle> <pages> pp 23-33. </pages>
Reference-contexts: The logic chips are the Floating Point Unit (FPU), Integer Processing Unit (IPU) and Memory Management Unit (MMU). This partitioning is similar to the SGI R8000 TFP processor <ref> [6] </ref>. For the system level performance of our GaAs chipset to be competitive with that of contemporary CMOS processors the GaAs system must overcome with increased clock speed the CMOS advantage of much higher integration density, which allows increased parallelism and larger caches.
Reference: [7] <author> N. Jouppi, </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers, </title> <booktitle> 17th International Symposium on Computer Architecture </booktitle>
Reference-contexts: The prefetch buffers predict future memory requests and bring the data on chip before it is referenced by the IPU. Jouppi proposed the addition of a small set of associative prefetch buffers, called stream buffers, to fetch sequential lines ahead of the current program counter <ref> [7] </ref>. The stream buffer consists of a tag register, a tag comparator, a set of status bits and a number of prefetch cache lines for instructions and data.
Reference: [8] <author> N. Jouppi, </author> <title> Cache Write Policies and Performance , WRL Research Report 91/12 DEC Western Research Laboratory </title>
Reference-contexts: A machine with only one MSHR cannot overlap memory operations, and must process each load or store sequentially. Write Cache The LSU contains a 32-word coalescing write buffer called the Write Cache <ref> [8] </ref>. The 32 data words are organized as four cache lines of eight words per line. The four lines are fully associative. The write cache groups multiple memory references into a single BIU transaction. Memory write behavior has two characteristics that are effectively exploited by the write cache.
Reference: [9] <author> D. Kroft, </author> <title> Lockup-free Instruction Fetch/Prefetch Cache Organization, </title> <booktitle> 8th International Symposium on Computer Architecture </booktitle>
Reference-contexts: Sustained transfer rates of over 1.5 G-bytes per second have been demonstrated in the BIU performance model for typical workloads [14]. Long memory latencies require high bandwidth to support prefetching and nonblocking caches <ref> [9] </ref>. The asynchronous BIU interface allows external communication and internal computation to proceed independently. The IPU is connected to the MMU by a bidirectional 32-bit bus. To tolerate the transaction latency in the system, multiple pending requests are buffered in separate transmit and receive queues.
Reference: [10] <author> N. Kushiyama et al., </author> <title> A 500-Megabyte/s Data-Rate 4.5M DRAM, </title> <journal> IEEE Journal of Solid-State Circuits , vol. </journal> <volume> 28 no. 4, </volume> <pages> pp. 490-498, </pages> <month> Apr. </month> <year> 1993 </year>
Reference-contexts: The line shown represents about a 40% increase per year in frequency. the bus. Using a Rambus-like signaling scheme <ref> [10] </ref>, the clock is generated on-chip and sent with the data to minimize clock skew. Data is transferred on both edges of the clock to maximize IO bandwidth. Instruction Fetch Unit The IFU fetches instructions and maintaining the state of the on-chip Instruction Cache.
Reference: [11] <editor> J. Mulder et al., </editor> <title> An area model for on-chip memories and its application, </title> <journal> IEEE Journal of Solid-State Circuits , vol. </journal> <volume> 26, no. 2, </volume> <year> 1991, </year> <pages> pp. 98106. </pages>
Reference-contexts: Each of the three models is evaluated with one or two execution pipes, and with secondary memory system average latencies of 17 and 35 cycles, corresponding to medium and fast clock rates. Thus, results for 12 configurations are reported. The register bit equivalent (RBE) model of Mulder <ref> [11] </ref> was used to evaluate the implementation cost in chip area of the different configurations. The RBE model provides a normalized measure for the area cost of different microarchitectural components. For our machines the RBE is the area required to implement a 1-bit static latch.
Reference: [12] <author> O.A. Olukotun, T.N. Mudge, and R.B. Brown, </author> <title> Performance optimization of pipelined primary caches, </title> <booktitle> Proc. of the 19th Ann. Int. Symposium on Computer Architecture , May 1992, </booktitle> <pages> pp. 181-190. </pages>
Reference-contexts: This issue is addressed in <ref> [12] </ref>. 5 Study Results The size and quantity of both on-chip memory structures and execution pipelines were varied to study the trade-offs between memory structures and execution logic.
Reference: [13] <author> J. Smith and A. Pleszkun, </author> <title> Implementing precise interrupts in pipelined processors, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C 37, no. 5, </volume> <month> May, </month> <year> 1988, </year> <pages> pp. 562573. </pages>
Reference-contexts: be fetched on the next cycle, without needing to compute a target address from the address of the branch instruction. 2.1 Instruction Execution Unit The key elements of the IEU are the 32x32 integer register file and two execution pipelines, which consist of an ALU and a six-entry reorder buffer <ref> [13] </ref>. A register file scoreboard [15] detects instruction dependencies and stalls execution until the needed operands are ready. To achieve high system clock frequencies, both the area and the number of logic levels needed to implement a logic function must be minimized.
Reference: [14] <author> T. J. Stanley, M. D. Upton, P. J. Sherhart, T. N. Mudge, R. B. Brown, </author> <title> A microarchitectural performance evaluation of a 3.2 GB/s microprocessor bus, </title> <booktitle> MICRO-26, The Ann. ACM/IEEE Int. Symposium on Microarchitecture , Austin, </booktitle> <address> TX, </address> <month> Dec. </month> <year> 1993, </year> <pages> pp. 31-40. </pages>
Reference-contexts: Bus Interface Unit The Aurora III memory system is designed to provide a large memory bandwidth from main memory through the MMU to the primary caches. Sustained transfer rates of over 1.5 G-bytes per second have been demonstrated in the BIU performance model for typical workloads <ref> [14] </ref>. Long memory latencies require high bandwidth to support prefetching and nonblocking caches [9]. The asynchronous BIU interface allows external communication and internal computation to proceed independently. The IPU is connected to the MMU by a bidirectional 32-bit bus.
Reference: [15] <author> J. Thornton, </author> <title> The Design of a Computer, the Control Data 6600 , Scott, </title> <publisher> Foresman and Company, </publisher> <year> 1970. </year> <note> [16]M. </note> <author> Upton, T. Huff, P. Sherhart, P. Barker, R. McVay, T. Stan-ley, R. Brown, R. Lomax, T. Mudge, and K. Sakallah, </author> <title> A 160,000 transistor GaAs microprocessor, </title> <booktitle> Int. Solid-State Circuits Conf. Digest of Technical Papers , vol. </booktitle> <volume> 36, </volume> <month> Feb. </month> <year> 1993, </year> <pages> pp. 92-93. </pages>
Reference-contexts: In contrast, the rate of increase in main memory speed has been lower than that of CPUs. The core-based main memory access time of the CDC6600 was 1000ns <ref> [15] </ref>. Main memory operations on the 1978-vintage VAX 11-780 required 6 cycles of 200ns, or 1200ns. Access times in current high-end workstations such as the HP 9000/735 have decreased to around 150ns, but this may be as many as 20 cycles. <p> A register file scoreboard <ref> [15] </ref> detects instruction dependencies and stalls execution until the needed operands are ready. To achieve high system clock frequencies, both the area and the number of logic levels needed to implement a logic function must be minimized.
References-found: 15

