URL: ftp://grilled.cs.wisc.edu/technical_papers/dyninst.ps.Z
Refering-URL: http://www.cs.wisc.edu/~tamches/tamches.html
Root-URL: 
Email: hollings@cs.wisc.edu bart@cs.wisc.edu jon@cs.wisc.edu  
Title: Dynamic Program Instrumentation for Scalable Performance Tools  
Author: Jeffrey K. Hollingsworth Barton P. Miller Jon Cargille 
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Note: To Appear in Proceedings of the 1994 Scalable High Performance Computing Conference, May 1994 (Knoxville, TN).  
Abstract: In this paper, we present a new technique called dynamic instrumentation that provides efficient, scalable, yet detailed data collection for large-scale parallel applications. Our approach is unique because it defers inserting any instrumentation until the application is in execution. We can insert or change instrumentation at any time during execution by modifying the application's binary image. Only the instrumentation required for the currently selected analysis or visualization is inserted. As a result, our technique collects several orders of magnitude less data than traditional data collection approaches. We have implemented a prototype of our dynamic instrumentation on the CM-5, and present results for several real applications. In addition, we include recommendations to operating system designers, compiler writers, and computer architects about the features necessary to permit efficient monitoring of large-scale parallel systems. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> T. Ball and J. R. Larus, </author> <title> Optimally Profiling and Tracing Programs, </title> <booktitle> in 19th ACM Symposium on Principles of Programming Languages, </booktitle> <address> (Albuquerque, NM, </address> <month> January 19-22, </month> <year> 1992), 1992, </year> <pages> 59-70. </pages>
Reference-contexts: It would be helpful if additional symbol table information were available that indicated exactly what is code and what is data in a program. Many compilers place read-only data into a program's text (code) segment. This creates a problems for post-linker tools (correctness debuggers and performance tools). Ball <ref> [1] </ref> has also noted this problem. Mini-trampolines contain predicate and primitive specific code and there is one for each primitive at each point. A sample instrumentation point with trampolines installed appears in Figure 5. <p> The flexibility afforded by dynamic instrumentation to instrument at different levels than just procedures (e.g. modules and loops) makes it possible to collect data at an appropriate granularity for each application. - -- 5. Related Work Several systems have been built that defer instrumentation until after compilation. Both QPT <ref> [1] </ref> and Mtool [4] use binary re-writing to insert instrumentation into an object file after it has been compiled and assembled. These systems require data collection decisions to be made prior to program execution.
Reference: 2. <author> J. S. Brown, </author> <title> The Application of Code Instrumenation Technology in the Los Alamos Debugger, </title> <institution> Los Alamos National Laboratory, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: The other major area of related work are techniques for modifying a program once it has started execution. A number of correctness debuggers have been built that modify an executing program for assertion checking and conditional breakpoints. Jeff Brown <ref> [2] </ref> developed a debugger for Cray Computers that provided this feature. Kessler at Xerox Parc [7] built a system that used dynamic modification of a program to insert breakpoints. Work has also been done by Wahbe, et. all on fast data break-points [13].
Reference: 3. <author> J. Dongarra, G. A. Geist, R. Manchek and V. S. Sunderam, </author> <title> "Integrated PVM framework supports heterogeneous network computing.", </title> <booktitle> Computers in Physics 7, 2 (March-April 1993), </booktitle> <pages> pp. 166-74. </pages>
Reference-contexts: In future versions of our instrumentation, points will be extended to include basic blocks and individual statements. - -- foo () . . SendMsg (dest, ptr, cnt, size) - . - addCounter (bytes, param <ref> [3] </ref> * param [4]) addCounter (fooCount,1) dynamic instrumentation. Four instrumentation points are used to compute the waiting time due to message passing constrained to a single procedure. The top two primitives maintain a counter fooFlg, which is non-zero whenever the procedure foo is active. <p> We are also in the process of studying and refining our predicated cost model. Work is underway to provide dynamic instrumentation on the Intel Paragon and clusters of workstations (using PVM <ref> [3] </ref>). 7. Acknowledgements We thank Adam Greenberg of TMC for helping to benchmark the CMOS timers, and Babak Falsafi for providing the memory mapped timers for the CM-5.
Reference: 4. <author> A. J. Goldberg and J. L. Hennessy, </author> <title> "Performance Debugging Shared Memory Multiprocessor Programs with MTOOL", </title> <address> Supercomputing'91, Albuquerque, NM, </address> <month> Nov. </month> <year> 1991, </year> <pages> pp. 481-490. </pages>
Reference-contexts: In future versions of our instrumentation, points will be extended to include basic blocks and individual statements. - -- foo () . . SendMsg (dest, ptr, cnt, size) - . - addCounter (bytes, param [3] * param <ref> [4] </ref>) addCounter (fooCount,1) dynamic instrumentation. Four instrumentation points are used to compute the waiting time due to message passing constrained to a single procedure. The top two primitives maintain a counter fooFlg, which is non-zero whenever the procedure foo is active. <p> Related Work Several systems have been built that defer instrumentation until after compilation. Both QPT [1] and Mtool <ref> [4] </ref> use binary re-writing to insert instrumentation into an object file after it has been compiled and assembled. These systems require data collection decisions to be made prior to program execution.
Reference: 5. <author> J. K. Hollingsworth, R. B. Irvin and B. P. Miller, </author> <title> "The Integration of Application and System Based Metrics in A Parallel Program Performance Tool", </title> <booktitle> 1991 ACM SIGPLAN Symposium on Principals and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991, </year> <pages> pp. 189-200. </pages>
Reference-contexts: Varying the sampling rate affects only our rate of decision making and granularity of phase boundaries; it does not affect the accuracy of the underlying performance data. Collected data is stored in a data structure called a time histogram <ref> [5] </ref>. A time histogram is a fixed-size array whose elements store values of a performance metric for successive time intervals. Two parameters determine the granularity of the data stored in time histograms: initial bucket width (timer interval) and number of buckets.
Reference: 6. <author> J. K. Hollingsworth and B. P. Miller, </author> <title> "Dynamic Control of Performance Monitoring on Large Scale Parallel Systems", </title> <booktitle> 7th ACM International Conf. on Supercomputing, </booktitle> <address> Tokyo, </address> <month> July </month> <year> 1993, </year> <pages> pp. 185-194. </pages>
Reference-contexts: Although it is possible for programmers to manually control data collection, this is difficult and tedious. We have built a system called the Performance Consultant <ref> [6] </ref> that liberates the programmer from needing to make these decisions. The rest of this paper describes an instrumentation system we have built to meet goals of being detailed, frugal, and scalable. Section 2 describes the design of our system. Section 3 provides details about its implementation. <p> The second abstraction is metrics. Metrics are time varying functions that characterize some aspect of a parallel program's performance; examples include CPU utilization, counts of floating point operations, and memory usage. They are defined by higher level performance tools that use our instrumentation system (e.g., the Performance Consultant <ref> [6] </ref>). However, to assist tool builders, we provide a standard set of metrics as part of our system. Metrics can be computed for any subset of the resources in the system. <p> The execution frequency of points comes from a static model of procedure call frequency. While the cost model does not perfectly predict the impact of instrumentation on the application, data collected during execution can be used to dynamically tune the estimates during the course of program execution <ref> [6] </ref>. 3.2. Instrumentation Manager The Instrumentation Manager performs two functions: it identifies the potential instrumentation points, and handles requests for primitives and predicates and inserts them into the application program. The requests are translated into small code fragments, called trampolines , and inserted into the program. <p> Both programs are written in C. We compared the execution time of an un-instrumented application to the execution time of an instrumented application. We measured the overall cost of our instrumentation and the overhead of various components of the instrumentation. For this study, we used the Performance Consultant <ref> [6] </ref>, a system that takes full advantage of dynamic instrumentation, to drive our instrumentation. The results for these tests are shown in Figure 13. The second column, Original Execution Time, shows the time to run the application without any instrumentation. <p> This work is part of an ongoing project, called Paradyn, to design and build a complete performance measurement environment for large-scale parallel machines. In the future, we plan to conduct a detailed case study of both the Performance Consultant <ref> [6] </ref>, dynamic instrumentation, and their interactions using several real applications. We are also in the process of studying and refining our predicated cost model. Work is underway to provide dynamic instrumentation on the Intel Paragon and clusters of workstations (using PVM [3]). 7.
Reference: 7. <author> P. B. Kessler, </author> <title> "Fast Breakpoints: </title> <booktitle> Design and Implementation", ACM SIGPLAN'90 Conf. on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June 20-22, </month> <year> 1990, </year> <pages> pp. 78-84. </pages>
Reference-contexts: A number of correctness debuggers have been built that modify an executing program for assertion checking and conditional breakpoints. Jeff Brown [2] developed a debugger for Cray Computers that provided this feature. Kessler at Xerox Parc <ref> [7] </ref> built a system that used dynamic modification of a program to insert breakpoints. Work has also been done by Wahbe, et. all on fast data break-points [13]. They employ sophisticated program analysis techniques to minimize the overhead of instrumentation for data breakpoints.
Reference: 8. <author> H. Massalin and C. Pu, </author> <title> "Threads and Input/Output in the Synthesis Kernel", </title> <booktitle> ACM Symp. on Operating Systems Principles, </booktitle> <address> Litchfield Park, AZ, </address> <month> Dec 3-6 </month> <year> 1989, </year> <pages> pp. 191-201. </pages>
Reference-contexts: Work has also been done by Wahbe, et. all on fast data break-points [13]. They employ sophisticated program analysis techniques to minimize the overhead of instrumentation for data breakpoints. Massalin and Pu <ref> [8] </ref> have a novel application of code patchup in their Synthesis operating system. They modify a program to automatically schedule another thread when it is about to block. 6.
Reference: 9. <author> J. K. Osterhout, </author> <booktitle> USENIX Winter Conf., </booktitle> <month> Jan. </month> <year> 1990, </year> <pages> pp. 133-146. </pages>
Reference: 10. <author> B. Ries, R. Anderson, W. Auld, D. Breazeal, K. Callaghan, E. Richards and W. Smith, </author> <title> "The Paragon Performance Monitoring Environment", </title> <address> Supercomputing'93, Portland, OR, Nov 15-19, </address> <year> 1993, </year> <pages> pp. 850-859. </pages>
Reference-contexts: These systems require data collection decisions to be made prior to program execution. One system that defers instrumentation until the program has started to execute is the TAM facility <ref> [10] </ref> provided by Intel for the Paragon. TAM uses a static set of performance instrumentation profiles (i.e. prof style sampling, or full event tracing) to insert instrumentation into a program after it has been loaded into memory but prior to execution.
Reference: 11. <author> D. A. Schneider and D. J. DeWitt, </author> <title> A Performance Evaluation of Four Parallel Join Algorithms in a Shared-Nothing Multiprocessor Environment, </title> <type> Tech. Report 836, </type> <institution> Dept. of Comp. Sci., University of Wisconsin, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: For the dynamic tests, we ran two parallel applications on the CM-5. The first application does a domain-decomposition method for optimizing large-scale linear models. The second program is a database simulator that implements a parallel Grace Hash join algorithm in a simulated shared-nothing environment <ref> [11] </ref>. Both programs are written in C. We compared the execution time of an un-instrumented application to the execution time of an instrumented application. We measured the overall cost of our instrumentation and the overhead of various components of the instrumentation.
Reference: 12. <author> R. L. Sites, ed., </author> <title> Alpha Architecture Reference Manual, </title> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: This time could be substantially reduced if processors provided a clock that was readable by user level code at register access speed. Few RISC processors, with the exception of the DEC Alpha <ref> [12] </ref>, provide such a clock. We feel this type of clock is a critical architectural feature to permit building efficient performance tools. The numbers presented above reflect the time to execute our primitives after we made a slight modification to the CM-5 operating system.
Reference: 13. <author> R. Wahbe, L. Lucco and S. L. Graham, </author> <title> "Practical data breakpoints: </title> <booktitle> design and implementation", ACM SIGPLAN'93 Conf. on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June 23-25, </month> <year> 1993, </year> <pages> pp. 1-12. </pages> - -- 
Reference-contexts: Jeff Brown [2] developed a debugger for Cray Computers that provided this feature. Kessler at Xerox Parc [7] built a system that used dynamic modification of a program to insert breakpoints. Work has also been done by Wahbe, et. all on fast data break-points <ref> [13] </ref>. They employ sophisticated program analysis techniques to minimize the overhead of instrumentation for data breakpoints. Massalin and Pu [8] have a novel application of code patchup in their Synthesis operating system. They modify a program to automatically schedule another thread when it is about to block. 6.
References-found: 13

