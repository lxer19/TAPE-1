URL: http://www.netlib.org/lapack/lawns/lawn100.ps
Refering-URL: http://www.netlib.org/scalapack/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: LAPACK Working Note 100 A Proposal for a Set of Parallel Basic Linear Algebra Subprograms  
Author: J. Choi J. Dongarra S. Ostrouchov A. Petitet D. Walker and R. C. Whaley 
Address: Knoxville, TN 37996-1301,  Oak Ridge, TN 37831  Knoxville, TN 37996-1301  Oak Ridge, TN 37831  
Note: 156-743, Korea. The author's research was performed at the  
Affiliation: School of Computing, Soogsil University, Seoul  Department of Computer Science of University of Tennessee and Oak Ridge National Laboratory Department of Computer Science, University of Tennessee,  and Mathematical Sciences Section, Oak Ridge National Laboratory,  Department of Computer Science, University of Tennessee,  Mathematical Sciences Section, Oak Ridge National Laboratory,  
Date: May, 1995  
Abstract: This paper describes a proposal for a set of Parallel Basic Linear Algebra Subprograms (PBLAS). The PBLAS are targeted at distributed vector-vector, matrix-vector and matrix-matrix operations with the aim of simplifying the parallelization of linear algebra codes, especially when implemented on top of the sequential BLAS. At first glance, because of the apparent simplicity of its sequential counterpart as well as the regularity of the data structures involved in dense linear algebra computations, implementing an equivalent set of parallel routines in terms of portability, efficiency, and ease-of-use seems relatively simple to achieve. However, when these routines are actually coded, the problem becomes much more complex due to difficulties which do not occur in serial computing. First, there are many different parallel computer architectures available. In view of this fact, it is natural to choose a virtual machine topology that is convenient for dense linear algebra computations and map the virtual machine onto existing topologies. Second, the selected data distribution scheme must ensure good load-balance to guarantee performance and scalability. Finally, for ease-of-use and software reusability reasons, the interface of the top-level routines must closely resemble the sequential BLAS interface yet still be flexible enough to take advantage of efficient parallel algorithmic techniques such as computation and communication overlapping and pipelining. This paper presents a reasonable set of adoptable solutions to successfully design and implement the Parallel Basic Linear Algebra Subprograms. These subprograms can in turn be used to develop parallel libraries such as ScaLAPACK for a large variety of distributed memory MIMD computers. fl This work was supported in part by the National Science Foundation Grant No. ASC-9005933; by the Defense Advanced Research Projects Agency under contract DAAL03-91-C-0047, administered by the Army Research Office; by the Office of Scientific Computing, U.S. Department of Energy, under Contract DE-AC05-84OR21400; and by the National Science Foundation Science and Technology Center Cooperative Agreement No. CCR-8809615. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Aboelaze, N. Chrisochoides, and E. Houstis. </author> <title> "The parallelization of Level 2 and 3 BLAS Operations on Distributed Memory Machines". </title> <type> Technical Report CSD-TR-91-007, </type> <institution> Purdue University, West Lafayette, IN, </institution> <year> 1991. </year>
Reference-contexts: They allow the software writer to focus on performing message passing on subsections of matrices rather than at low level byte transfers. There has been much interest recently in developing parallel versions of the BLAS for distributed memory computers <ref> [1, 3, 15, 16] </ref>. Some of this research proposed parallelizing the BLAS, and some implemented a few important BLAS routines, such as matrix-matrix multiplication.
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Ham-marling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> "LAPACK Users' Guide, Second Edition". </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1995. </year>
Reference-contexts: By relying on these basic kernels, it is possible to develop portable and efficient software across a wide range of architectures, with emphasis on workstations, vector-processors and shared-memory computers, as has been done in LAPACK <ref> [2] </ref>. As opposed to shared-memory systems, distributed-memory computers differ significantly from the software point of view. The underlying interconnection network as well as the vendor supplied communication library are usually machine specific. <p> As such, installing the PBLAS library is part of the ScaLAPACK installation procedure. This process is described in [5]. However, the PBLAS will likely become a stand-alone package in the near future, similar to what has been done for the BLAS and LAPACK libraries <ref> [2] </ref>. In which case, the installation procedure of the standalone PBLAS library will be very close to what has been done for the ScaLAPACK library. This section contains code fragments that demonstrate what needs to be done in order to call a PBLAS routine.
Reference: [3] <author> R. Brent and P. Strazdins. </author> <title> "Implementation of BLAS Level 3 and LINPACK Benchmark on the AP1000". </title> <journal> Fujitsu Scientific and Technical Journal, </journal> <volume> 5(1) </volume> <pages> 61-70, </pages> <year> 1993. </year>
Reference-contexts: They allow the software writer to focus on performing message passing on subsections of matrices rather than at low level byte transfers. There has been much interest recently in developing parallel versions of the BLAS for distributed memory computers <ref> [1, 3, 15, 16] </ref>. Some of this research proposed parallelizing the BLAS, and some implemented a few important BLAS routines, such as matrix-matrix multiplication.
Reference: [4] <author> J. Choi, J. Demmel, I. Dhillon, J. Dongarra, S. Ostrouchov, A. Petitet, K. Stanley, D. Walker, and R. C. Whaley. </author> <title> "ScaLAPACK: A Portable Linear Algebra Library for Distributed Memory Computers Design Issues and Performance". </title> <type> Technical Report UT CS-95-283, </type> <note> LAPACK Working Note #95, </note> <institution> University of Tennessee, </institution> <year> 1995. </year>
Reference-contexts: This decision, however, could also be based on the amount of memory available during the execution, the local BLAS performance, and machine constants such as the latency and bandwidth of the network <ref> [4] </ref>. Internally, the PBLAS currently rely on routines requiring certain alignment properties to be satisfied [9]. These properties have been chosen so that maximum efficiency can be obtained on these restricted operations. <p> The question of making the PBLAS more flexible remains open and its answer largely depends on the needs of the user community. 5.1.2 Auxiliary Subprograms. It is well known <ref> [4, 13, 26] </ref> that certain algorithms based on a two-dimensional block-cyclic data distribution scheme become more efficient and scalable when appropriate communication topologies are used for the broadcast and global combine operations [4, 13, 26]. <p> It is well known <ref> [4, 13, 26] </ref> that certain algorithms based on a two-dimensional block-cyclic data distribution scheme become more efficient and scalable when appropriate communication topologies are used for the broadcast and global combine operations [4, 13, 26]. For example, pipelining the broadcast operation along the rows of the process grid improves the efficiency and scalability of the LU factorization algorithm [4, 13]. The BLACS topologies allow the user to optimize communication patterns for these particular operations. A default topology can also be selected. <p> For example, pipelining the broadcast operation along the rows of the process grid improves the efficiency and scalability of the LU factorization algorithm <ref> [4, 13] </ref>. The BLACS topologies allow the user to optimize communication patterns for these particular operations. A default topology can also be selected. The list of BLACS topologies as well as the different possible scopes are documented in [14]. <p> Should it happen, the PBLAS routines return the correct result only in the process owning the input vector operand and zero in every other grid process. Finally, there are special challenges associated with writing and testing numerical software to be executed on networks containing heterogeneous processors <ref> [4] </ref>, i.e., processors which perform floating point arithmetic differently. This includes not just machines with different floating point formats and semantics such as Cray computers and workstations performing IEEE standard floating point arithmetic, but even supposedly identical machines running different compilers or even just different compiler options.
Reference: [5] <author> J. Choi, J. Demmel, I. Dhillon, J. Dongarra, S. Ostrouchov, A. Petitet, K. Stanley, D. Walker, and R. C. Whaley. </author> <title> "Installation Guide for ScaLAPACK". </title> <type> Technical Report UT CS-95-280, </type> <note> LAPACK Working Note #93, </note> <institution> University of Tennessee, </institution> <year> 1995. </year>
Reference-contexts: As such, installing the PBLAS library is part of the ScaLAPACK installation procedure. This process is described in <ref> [5] </ref>. However, the PBLAS will likely become a stand-alone package in the near future, similar to what has been done for the BLAS and LAPACK libraries [2].
Reference: [6] <author> J. Choi, J. Dongarra, S. Ostrouchov, A. Petitet, D. Walker, and R. C. Whaley. </author> <title> "The Design and Implementation of the ScaLAPACK LU, QR, and Cholesky Factorization Routines". </title> <type> Technical Report UT CS-94-246, </type> <note> LAPACK Working Note #80, </note> <institution> University of Tennessee, </institution> <year> 1994. </year>
Reference-contexts: A matrix transposition routine has been added to the Level 3 subprograms since this operation is much more complex to perform and implement on distributed-memory computers. Second, this proposal does not include routines for matrix factorizations or reductions; these are covered by the ScaLAPACK (Scalable Linear Algebra PACKage) project <ref> [6, 7] </ref>. A reference implementation version of the PBLAS is available on netlib (http://www.netlib.org). Vendors can then supply system optimized versions of the BLAS, the BLACS and eventually the PBLAS.
Reference: [7] <author> J. Choi, J. Dongarra, R. Pozo, and D. Walker. </author> <title> "ScaLAPACK: A Scalable Linear Algebra Library for Distributed Memory Concurrent Computers". </title> <booktitle> In Proceedings of Fourth Symposium on the Frontiers of Massively Parallel Computation (McLean, Virginia), </booktitle> <pages> pages 120-127. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1992. </year> <note> (also LAPACK Working Note #55). </note>
Reference-contexts: A matrix transposition routine has been added to the Level 3 subprograms since this operation is much more complex to perform and implement on distributed-memory computers. Second, this proposal does not include routines for matrix factorizations or reductions; these are covered by the ScaLAPACK (Scalable Linear Algebra PACKage) project <ref> [6, 7] </ref>. A reference implementation version of the PBLAS is available on netlib (http://www.netlib.org). Vendors can then supply system optimized versions of the BLAS, the BLACS and eventually the PBLAS. <p> This allows the routines to achieve scalability, well balanced computations and to minimize synchronization costs. It is not the object of this paper to describe in detail the data mapping onto the processes, for further details see <ref> [7, 13] </ref>. Let us simply say that the set of processes is mapped to a virtual mesh, where every process is naturally identified by its coordinates in this P fi Q grid.
Reference: [8] <author> J. Choi, J. Dongarra, and D. Walker. </author> <title> Parallel matrix transpose algorithms on distributed memory concurrent computers. </title> <booktitle> In Proceedings of Fourth Symposium on the Frontiers of Massively Parallel Computation (McLean, Virginia), </booktitle> <pages> pages 245-252. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1993. </year> <note> (also LAPACK Working Note #65). </note>
Reference-contexts: Because software reusability is one of our major concerns, we wanted the BLAS and PBLAS interfaces to be as similar as possible. Consequently, only one routine, the matrix transposition, has been added to the PBLAS, since this operation is much more complex to perform in a distributed-memory environment <ref> [8] </ref>. One of the major differences between the BLAS and the PBLAS is likely to be found in the Level 1 routines. Indeed, the functions of the former have been replaced by subroutines in the latter.
Reference: [9] <author> J. Choi, J. Dongarra, and D. Walker. "PB-BLAS: </author> <title> A Set of Parallel Block Basic Linear Algebra Subroutines". </title> <booktitle> In "Proceedings of the Scalable High Performance Computing Conference", </booktitle> <pages> pages 534-541, </pages> <address> Knoxville, TN, 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: These algorithms, called SUMMA, have the advantage of requiring much less workspace than PUMMA. These algorithms have, in some sense, already been implemented in terms of internal routines to the PBLAS <ref> [9] </ref>. Therefore, this work [26] will allow us to improve and generalize the model implementation. <p> This decision, however, could also be based on the amount of memory available during the execution, the local BLAS performance, and machine constants such as the latency and bandwidth of the network [4]. Internally, the PBLAS currently rely on routines requiring certain alignment properties to be satisfied <ref> [9] </ref>. These properties have been chosen so that maximum efficiency can be obtained on these restricted operations. Consequently, when redistribution or re-alignment of input or output data has to be performed some performance will be lost. <p> However, it is also necessary that the PBLAS routines recognize the scope of their operands in order to save useless communication and synchronization costs when possible. This specific part of the PBLAS specifications remains an open question. A few features supported by the PBLAS underlying tools <ref> [9] </ref> have been intentionally hidden. For instance, a block of identical vectors operands are sometimes replicated across process rows or columns. When such a situation occurs, it is possible to save some communication and computation operations.
Reference: [10] <author> J. Choi, J. Dongarra, and D. Walker. "PUMMA: </author> <title> Parallel Universal Matrix Multiplication Algorithms on Distributed Memory Concurrent Computers". </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 6(7) </volume> <pages> 543-570, </pages> <year> 1994. </year> <note> (also LAPACK Working Note #57). </note>
Reference-contexts: Transparent to the user, this relatively simple selection process ensures high efficiency independent from the actual computation performed. For example, there are algorithms <ref> [10, 19, 22] </ref>, for matrix-matrix products like PUMMA which are much more efficient for equally sized input/output matrices. Some of these algorithms require a very large amount of workspace making them impractical for library purposes.
Reference: [11] <author> J. Dongarra, J. Du Croz, I. Duff, and S. Hammarling. </author> <title> "A Set of Level 3 Basic Linear Algebra Subprograms". </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: Because these subprograms and their predecessors the Levels 1 and 2 BLAS are an aid to clarity, portability, modularity and maintenance of software, they have been embraced by the community and have become a de facto standard for elementary linear algebra computations <ref> [11] </ref>. Many of the frequently used algorithms of numerical linear algebra can be implemented so that a majority of the computation is performed by calls to the Level 2 and Level 3 BLAS. <p> We do, however, expect the differences to be small relative to working precision. The error bounds are then the same as the ones used in the BLAS testers. A more detailed description of those tests can be found in <ref> [11, 12] </ref>. The test ratio is determined by scaling these error bounds by the inverse of 24 machine epsilon * 1 . This ratio is compared with a constant threshold value defined in the input data file. Test ratios greater than the threshold are flagged as "suspect".
Reference: [12] <author> J. Dongarra, J. Du Croz, S. Hammarling, and R. Hanson. </author> <title> "Algorithm 656: An extended Set of Basic Linear Algebra Subprograms: Model Implementation and Test Programs". </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14(1) </volume> <pages> 18-32, </pages> <year> 1988. </year> <month> 32 </month>
Reference-contexts: Math. Soft. (Vol. 16, no. 1, page 1) defining and proposing a set of Level 3 Basic Linear Algebra Subprograms. That proposal logically concluded a period of reflection and discussion among the mathematical software community <ref> [12, 21, 24] </ref> to define a set of routines that would find wide application in software for numerical linear algebra and provide a useful tool for implementors and users. <p> We do, however, expect the differences to be small relative to working precision. The error bounds are then the same as the ones used in the BLAS testers. A more detailed description of those tests can be found in <ref> [11, 12] </ref>. The test ratio is determined by scaling these error bounds by the inverse of 24 machine epsilon * 1 . This ratio is compared with a constant threshold value defined in the input data file. Test ratios greater than the threshold are flagged as "suspect".
Reference: [13] <author> J. Dongarra, R. van de Geijn, and D. Walker. </author> <title> "Scalability Issues in the Design of a Library for Dense Linear Algebra". </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 523-537, </pages> <year> 1994. </year> <note> (also LAPACK Working Note #43). </note>
Reference-contexts: This allows the routines to achieve scalability, well balanced computations and to minimize synchronization costs. It is not the object of this paper to describe in detail the data mapping onto the processes, for further details see <ref> [7, 13] </ref>. Let us simply say that the set of processes is mapped to a virtual mesh, where every process is naturally identified by its coordinates in this P fi Q grid. <p> The question of making the PBLAS more flexible remains open and its answer largely depends on the needs of the user community. 5.1.2 Auxiliary Subprograms. It is well known <ref> [4, 13, 26] </ref> that certain algorithms based on a two-dimensional block-cyclic data distribution scheme become more efficient and scalable when appropriate communication topologies are used for the broadcast and global combine operations [4, 13, 26]. <p> It is well known <ref> [4, 13, 26] </ref> that certain algorithms based on a two-dimensional block-cyclic data distribution scheme become more efficient and scalable when appropriate communication topologies are used for the broadcast and global combine operations [4, 13, 26]. For example, pipelining the broadcast operation along the rows of the process grid improves the efficiency and scalability of the LU factorization algorithm [4, 13]. The BLACS topologies allow the user to optimize communication patterns for these particular operations. A default topology can also be selected. <p> For example, pipelining the broadcast operation along the rows of the process grid improves the efficiency and scalability of the LU factorization algorithm <ref> [4, 13] </ref>. The BLACS topologies allow the user to optimize communication patterns for these particular operations. A default topology can also be selected. The list of BLACS topologies as well as the different possible scopes are documented in [14]. <p> In this way the update phase can be started as early as possible <ref> [13] </ref>. Such a pipelining effect can easily be achieved within PBLAS based codes by using ring topologies along process rows for the broadcast operations. These particular algorithmic techniques are enabled by the PBLAS auxiliary routines PTOPGET and PTOPSET (see Sect. 5.1.2).
Reference: [14] <author> J. Dongarra and R. C. Whaley. </author> <title> "A User's Guide to the BLACS v1.0". </title> <type> Technical Report UT CS-95-281, </type> <note> LAPACK Working Note #94, </note> <institution> University of Tennessee, </institution> <year> 1995. </year>
Reference-contexts: Nevertheless, a large variety of distributed-memory systems still exists and this motivated the development of a set of portable communication subprograms well suited for linear algebra computations: the Basic Linear Algebra Communication Subprograms (BLACS) <ref> [14, 27] </ref>. In addition to defining a portable interface the BLACS also provide the correct level of abstraction. They allow the software writer to focus on performing message passing on subsections of matrices rather than at low level byte transfers. <p> Let us simply say that the set of processes is mapped to a virtual mesh, where every process is naturally identified by its coordinates in this P fi Q grid. This virtual machine is in fact part of a larger object defined by the BLACS and called a context <ref> [14] </ref>. An M by N matrix operand is first decomposed into MB by NB blocks starting at its upper left corner. These blocks are then uniformly distributed across the process mesh. <p> The BLACS topologies allow the user to optimize communication patterns for these particular operations. A default topology can also be selected. The list of BLACS topologies as well as the different possible scopes are documented in <ref> [14] </ref>. In order to set this low level information, the PBLAS provide two routines having the following FORTRAN 77 interface: SUBROUTINE PTOPSET ( ICTXT, OP, SCOPE, TOP ) SUBROUTINE PTOPGET ( ICTXT, OP, SCOPE, TOP ) INTEGER ICTXT CHARACTER*1 OP, SCOPE, TOP PTOPSET assigns the BLACS topology [14] TOP to be <p> are documented in <ref> [14] </ref>. In order to set this low level information, the PBLAS provide two routines having the following FORTRAN 77 interface: SUBROUTINE PTOPSET ( ICTXT, OP, SCOPE, TOP ) SUBROUTINE PTOPGET ( ICTXT, OP, SCOPE, TOP ) INTEGER ICTXT CHARACTER*1 OP, SCOPE, TOP PTOPSET assigns the BLACS topology [14] TOP to be used in the communication operations OP along the scope specified by SCOPE. PTOPGET returns the BLACS topology TOP used in the communication operations OP along the scope specified by SCOPE. Application examples of these routines are given in appendix B. <p> This code is in fact a slightly simplified version of the ScaLAPACK code. 7.1 Use of the PBLAS In order to call a PBLAS routine, it is necessary to initialize the BLACS and create the process grid. This can be done by calling the routine BLACS GRIDINIT (see <ref> [14] </ref> for more details). The following segment of code will arrange four processes into a 2 fi 2 process grid. <p> Finally, in order to ensure a safe coexistence with other parallel libraries using a distinct message passing layer, such as MPI [17], the BLACS routine BLACS GET queries for an eventual system context (see <ref> [14] </ref> for more details). <p> Note that the routine BLACS GRIDEXIT will free the resources associated with a particular context, while the routine BLACS EXIT will free all BLACS resources (see <ref> [14] </ref> for more details). CALL PBFREEBUF () * CALL BLACS_GRIDEXIT ( ICTXT ) * CALL BLACS_EXIT ( 0 ) 29 7.2 Solving Linear Systems via LU Factorization The primary applications of the PBLAS are in implementing algorithms of numerical linear algebra in terms of operations on submatrices (or blocks).
Reference: [15] <author> A. Elster. </author> <title> "Basic Matrix Subprograms for Distributed Memory Systems". </title> <editor> In D. Walker and Q. Stout, editors, </editor> <booktitle> "Proceedings of the Fifth Distributed Memory Computing Conference", </booktitle> <pages> pages 311-316. </pages> <publisher> IEEE Press, </publisher> <year> 1990. </year>
Reference-contexts: They allow the software writer to focus on performing message passing on subsections of matrices rather than at low level byte transfers. There has been much interest recently in developing parallel versions of the BLAS for distributed memory computers <ref> [1, 3, 15, 16] </ref>. Some of this research proposed parallelizing the BLAS, and some implemented a few important BLAS routines, such as matrix-matrix multiplication.
Reference: [16] <author> R. Falgout, A. Skjellum, S. Smith, and C. </author> <title> Still. "The Multicomputer Toolbox Approach to Concurrent BLAS". </title> <note> submitted to Concurrency: Practice and Experience, 1993. (preprint). </note>
Reference-contexts: They allow the software writer to focus on performing message passing on subsections of matrices rather than at low level byte transfers. There has been much interest recently in developing parallel versions of the BLAS for distributed memory computers <ref> [1, 3, 15, 16] </ref>. Some of this research proposed parallelizing the BLAS, and some implemented a few important BLAS routines, such as matrix-matrix multiplication.
Reference: [17] <author> Message Passing Interface Forum. </author> <title> "MPI: A Message Passing Interface Standard". </title> <journal> International Journal of Supercomputer Applications and High Performance Computing, </journal> <pages> 8(3-4), </pages> <year> 1994. </year>
Reference-contexts: As opposed to shared-memory systems, distributed-memory computers differ significantly from the software point of view. The underlying interconnection network as well as the vendor supplied communication library are usually machine specific. The ongoing Message Passing Interface (MPI) standardization effort <ref> [17] </ref> will undoubtly be of great benefit to the user community. Nevertheless, a large variety of distributed-memory systems still exists and this motivated the development of a set of portable communication subprograms well suited for linear algebra computations: the Basic Linear Algebra Communication Subprograms (BLACS) [14, 27]. <p> However, the essential features of this standard should be easily adaptable to other programming languages. We have attempted to pave the way for such a future evolution by respecting the driving concepts of the HPF [23] and MPI <ref> [17] </ref> projects. 2 Scope of the PBLAS The design of the software is as consistent as possible with that of the BLAS; thus, the experienced linear algebra programmer will have the same basic tools available in both the sequential and parallel programming worlds. <p> Finally, in order to ensure a safe coexistence with other parallel libraries using a distinct message passing layer, such as MPI <ref> [17] </ref>, the BLACS routine BLACS GET queries for an eventual system context (see [14] for more details).
Reference: [18] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> "Solving Problems on Concurrent Processors", volume 1. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J, </address> <year> 1988. </year>
Reference-contexts: The last section illustrates how some common algorithms can be implemented by calls to the proposed routines. There is certainly considerable evidence for the efficiency of such algorithms on various machines <ref> [18] </ref>. Such implementations are portable and efficient across a wide variety of distributed memory MIMD computers, ranging from a heterogeneous network of workstations to a statically connected set of identical processors, provided that efficient machine-specific BLAS and BLACS are available. 4 The scope of this proposal is limited.
Reference: [19] <author> G. Fox, S. Otto, and A. Hey. </author> <title> "Matrix Algorithms on a Hypercube I: Matrix Multiplication". </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 17-31, </pages> <year> 1987. </year>
Reference-contexts: Transparent to the user, this relatively simple selection process ensures high efficiency independent from the actual computation performed. For example, there are algorithms <ref> [10, 19, 22] </ref>, for matrix-matrix products like PUMMA which are much more efficient for equally sized input/output matrices. Some of these algorithms require a very large amount of workspace making them impractical for library purposes.
Reference: [20] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> "PVM: Parallel Virtual Machine. A User's Guide and Tutorial for Networked Parallel Computing". </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: This can be done by calling the routine BLACS GRIDINIT (see [14] for more details). The following segment of code will arrange four processes into a 2 fi 2 process grid. When running on platforms such as PVM <ref> [20] </ref>, where the number of computational nodes available is unknown a priori, it is necessary to call the routine BLACS SETUP, so that copies (3 in our example) of the main program can be spawned on the virtual machine.
Reference: [21] <author> R. Hanson, F. Krogh, and C. Lawson. </author> <title> "A Proposal for Standard Linear Algebra Subprograms". </title> <journal> ACM SIGNUM Newsl., </journal> <volume> 8(16), </volume> <year> 1973. </year>
Reference-contexts: Math. Soft. (Vol. 16, no. 1, page 1) defining and proposing a set of Level 3 Basic Linear Algebra Subprograms. That proposal logically concluded a period of reflection and discussion among the mathematical software community <ref> [12, 21, 24] </ref> to define a set of routines that would find wide application in software for numerical linear algebra and provide a useful tool for implementors and users.
Reference: [22] <author> S. Huss-Lederman, E. Jacobson, A. Tsao, and G. Zhang. </author> <title> "Matrix Multiplication on the Intel Touchstone DELTA". </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 6(7) </volume> <pages> 571-594, </pages> <year> 1994. </year>
Reference-contexts: Transparent to the user, this relatively simple selection process ensures high efficiency independent from the actual computation performed. For example, there are algorithms <ref> [10, 19, 22] </ref>, for matrix-matrix products like PUMMA which are much more efficient for equally sized input/output matrices. Some of these algorithms require a very large amount of workspace making them impractical for library purposes.
Reference: [23] <author> C. Koebel, D. Loveman, R. Schreiber, G. Steele, and M. Zosel. </author> <title> "The High Performance Fortran Handbook". </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: However, the essential features of this standard should be easily adaptable to other programming languages. We have attempted to pave the way for such a future evolution by respecting the driving concepts of the HPF <ref> [23] </ref> and MPI [17] projects. 2 Scope of the PBLAS The design of the software is as consistent as possible with that of the BLAS; thus, the experienced linear algebra programmer will have the same basic tools available in both the sequential and parallel programming worlds.
Reference: [24] <author> C. Lawson, R. Hanson, D. Kincaid, and F. Krogh. </author> <title> "Basic Linear Algebra Subprograms for Fortran Usage". </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5(3) </volume> <pages> 308-323, </pages> <year> 1979. </year>
Reference-contexts: Math. Soft. (Vol. 16, no. 1, page 1) defining and proposing a set of Level 3 Basic Linear Algebra Subprograms. That proposal logically concluded a period of reflection and discussion among the mathematical software community <ref> [12, 21, 24] </ref> to define a set of routines that would find wide application in software for numerical linear algebra and provide a useful tool for implementors and users.
Reference: [25] <author> H. Schildt. </author> <title> "The Annoted ANSI C standard. American National Standard for Programming Languages - C. </title> <address> ANSI/ISO 9899-1990". OsBorne, Berkeley, CA, </address> <year> 1990. </year>
Reference-contexts: A model implementation of the subprograms has been written in ANSI C <ref> [25] </ref>, mainly for its dynamic memory allocation management features, and FORTRAN 77. The FORTRAN 77 BLAS enable the PBLAS to be used on any machine for which the BLACS are available. 2.
Reference: [26] <author> R. van de Geijn and J. Watts. "SUMMA: </author> <title> Scalable Universal Matrix Multiplication Algorithm". </title> <type> Technical Report UT CS-95-286, </type> <note> LAPACK Working Note #96, </note> <institution> University of Tennessee, </institution> <year> 1995. </year>
Reference-contexts: Some of these algorithms require a very large amount of workspace making them impractical for library purposes. However, a simple implementation of common matrix multiplication operations has recently been proven to be highly efficient and scalable <ref> [26] </ref>. These algorithms, called SUMMA, have the advantage of requiring much less workspace than PUMMA. These algorithms have, in some sense, already been implemented in terms of internal routines to the PBLAS [9]. Therefore, this work [26] will allow us to improve and generalize the model implementation. <p> common matrix multiplication operations has recently been proven to be highly efficient and scalable <ref> [26] </ref>. These algorithms, called SUMMA, have the advantage of requiring much less workspace than PUMMA. These algorithms have, in some sense, already been implemented in terms of internal routines to the PBLAS [9]. Therefore, this work [26] will allow us to improve and generalize the model implementation. <p> The question of making the PBLAS more flexible remains open and its answer largely depends on the needs of the user community. 5.1.2 Auxiliary Subprograms. It is well known <ref> [4, 13, 26] </ref> that certain algorithms based on a two-dimensional block-cyclic data distribution scheme become more efficient and scalable when appropriate communication topologies are used for the broadcast and global combine operations [4, 13, 26]. <p> It is well known <ref> [4, 13, 26] </ref> that certain algorithms based on a two-dimensional block-cyclic data distribution scheme become more efficient and scalable when appropriate communication topologies are used for the broadcast and global combine operations [4, 13, 26]. For example, pipelining the broadcast operation along the rows of the process grid improves the efficiency and scalability of the LU factorization algorithm [4, 13]. The BLACS topologies allow the user to optimize communication patterns for these particular operations. A default topology can also be selected.
Reference: [27] <author> R. C. Whaley. </author> <title> "Basic Linear Algebra Communication Subprograms: Analysis and Implementation Across Multiple Parallel Architectures". </title> <type> Technical Report UT CS-94-234, </type> <note> LAPACK Working Note #73, </note> <institution> University of Tennessee, </institution> <year> 1994. </year> <month> 33 </month>
Reference-contexts: Nevertheless, a large variety of distributed-memory systems still exists and this motivated the development of a set of portable communication subprograms well suited for linear algebra computations: the Basic Linear Algebra Communication Subprograms (BLACS) <ref> [14, 27] </ref>. In addition to defining a portable interface the BLACS also provide the correct level of abstraction. They allow the software writer to focus on performing message passing on subsections of matrices rather than at low level byte transfers.
References-found: 27

