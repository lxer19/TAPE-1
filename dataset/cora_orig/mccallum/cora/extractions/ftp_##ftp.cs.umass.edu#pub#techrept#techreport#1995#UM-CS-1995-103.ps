URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1995/UM-CS-1995-103.ps
Refering-URL: http://www-ccs.cs.umass.edu/db/publications/
Root-URL: 
Email: e-mail: lory@cs.umass.edu, krithi@cs.umass.edu  
Title: Database Locking Protocols for Large-Scale Cache-Coherent Shared Memory Multiprocessors: Design, Implementation and Performance  
Author: Lory D. Molesky and Krithi Ramamritham 
Date: June 6, 1995  
Address: Amherst Ma. 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: Significant performance advantages can be realized by implementing a database system on a cache-coherent shared memory multiprocessor. An efficient implementation of a lock manager is a prerequisite for efficient transaction processing in multiprocessor database systems. To this end, we examine two approaches to the implementation of locking in a cache-coherent shared memory multiprocessor database system. The first approach, shared-memory locking (SML), allows each node (processor) of the multiprocessor to acquire and release locks directly via the use of cache-coherent shared memory. The second approach, message-passing locking (MPL), typically requires messages to be sent to a lock manager, located on a remote node. Our empirical evaluation of these approaches on the KSR-1 multiprocessor indicates that for most database locking traffic patterns, the performance of SML is substantially superior to that of MPL. For instance, when contention is high, the performance of SML is nearly an order of magnitude better than MPL. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Copeland, T. Keller, R. Krishnamurthy, and M. Smith. </author> <title> The Case for Safe RAM. </title> <booktitle> Proceedings of the 15th International Conference on Very Large Data Bases, </booktitle> <pages> pages 327-335, </pages> <year> 1989. </year>
Reference-contexts: However, other components of a database management system may also profoundly influence performance especially the commit process of systems which require disk I/O to implement the WAL protocol. Technological trends, such as non-volatile RAM <ref> [1] </ref> suggest that costly disk I/O can be avoided. For such systems, the performance of lock management will have a larger impact on the performance of the entire database management system and our design and implementation strategies will prove to be very useful.
Reference: [2] <author> D. DeWitt and J. Gray. </author> <title> Parallel Database Systems: The Future of High Performance Database Systems. </title> <journal> Communications of the ACM, </journal> <volume> 35(6) </volume> <pages> 85-98, </pages> <year> 1992. </year>
Reference-contexts: Some researchers have argued that SM database systems are not well suited for high performance database implementations, arguing that the platforms that these database systems are implemented on are not scalable <ref> [2] </ref>.
Reference: [3] <author> P. Franaszek, J. Robinson, and A. Thomasian. </author> <title> Concurrency Control for High Contention Environments. </title> <journal> ACM Transactions on Database Systems, </journal> <pages> pages 304-345, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Given these similar recovery mechanisms and overheads, choice of SM lock management architectures should be determined based on performance criterion. 7 Related Work A number of studies have been conducted on the performance of database lock management, to name a few see <ref> [3, 24, 25] </ref>. These studies are based on analytical and simulation models, and consider uniprocessor and SD database systems. In contrast, our study has considered implementation and performance issues in SM database systems, and has focused on empirical measurements.
Reference: [4] <author> A. Gottliieb, B. Lubachevsky, and L. Rudolph. </author> <title> Basic Techniques for the Efficient Coordination of Very Large Numbers of Cooperating Sequential Processors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2) </volume> <pages> 164-189, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: This queue permits concurrent enqueues (client requests), and allows concurrent enqueues and dequeues (done by the server) on different entries in the queue. Maximal concurrency is achieved by exploiting the technique of "slot reservation" discussed in <ref> [4, 8] </ref>. The data structure for this FIFO queue consists of a few bytes of header information and N slots. Each slot is used to transmit messages between one MPL client and the MPL server.
Reference: [5] <author> G. Graefe. </author> <title> Volcano an extensible and parallel query evaluation system. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 6(1) </volume> <pages> 120-135, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: However, in this paper, high contention database locking was only mentioned as one possible source of system performance degradation. 19 Other recent work on SM database implementations have considered parallelism in query processing <ref> [5, 10] </ref>. However, these studies have not addressed the performance of SM database lock management. Other general performance studies of the KSR-1 multiprocessor are reported in [21, 20], but these studies do not assess database related activities.
Reference: [6] <author> J. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kauf-mann, </publisher> <year> 1993. </year>
Reference-contexts: On an SM database system of this caliber, it is conceivable that thousands of transactions may execute concurrently, and, as a result, create very high contention for shared resources. Typically, database systems employ locking to conservatively enforce transaction isolation, such as serializability or cursor stability <ref> [6] </ref>. For example, prior to performing a read or write on a database item, it is necessary to obtain a read or write lock on this database item. In order to minimize transaction execution time, these lock requests should be granted in a timely fashion. <p> The main objective of our experiments was to assess the performance of SML and MPL under various degrees of contention in cases where control returns to the transaction without waiting. Since this is a very common lock manager data path, it is an important path to optimize <ref> [6] </ref>. For lock acquisitions, control returns without waiting when the requested lock is compatible with the current lock mode or the lock is not currently held by any transaction (thus granting the lock request).
Reference: [7] <author> J. N. Gray, R. A. Lorie, G. R. Putzulo, and I. L. Traiger. </author> <title> Granularity of Locks and Degrees of Consistency in a Shared Database. </title> <editor> In Michael Stonebraker, editor, </editor> <booktitle> readings in Database Systems, </booktitle> <pages> pages 94-121. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: In addition to the basic lock modes, intention locks are also included to facilitate multigranularity locking <ref> [7] </ref>. In a multigranularity locking protocol using intention locks (i.e., IX, IS, SIX), the database is characterized by a hierarchy (on a directed acyclic graph), where locks are requested in root to leaf order, and released leaf to root.
Reference: [8] <author> M. Herlihy and J. Wing. </author> <title> Linearizability: A Correctness Condition for Concurrent Objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 463-492, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: This queue permits concurrent enqueues (client requests), and allows concurrent enqueues and dequeues (done by the server) on different entries in the queue. Maximal concurrency is achieved by exploiting the technique of "slot reservation" discussed in <ref> [4, 8] </ref>. The data structure for this FIFO queue consists of a few bytes of header information and N slots. Each slot is used to transmit messages between one MPL client and the MPL server.
Reference: [9] <author> D. Lilja. </author> <title> Cache Coherence in Large-Scale Shared-Memory Multiprocessors: Issues and Comparisons. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(3) </volume> <pages> 303-338, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: the likelihood of concurrent requests for locks on different database objects, motivate our study of lock manager performance. 2.2 KSR-1 Multiprocessor Overview In a cache-coherent shared memory multiprocessor, a coherency protocol, typically implemented in hardware, ensures that any read operation sees the most recently written value for any data item <ref> [9] </ref>.. Each node has its own cache, and before an operation is performed on a data item, the data item must first be brought into the cache.
Reference: [10] <author> T. Martin, P. Larson, and V. Deshpande. </author> <title> Parallel Hash-Based Join Algorithms for a Shared-Everthing Environment. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 6(5) </volume> <pages> 750-763, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: However, in this paper, high contention database locking was only mentioned as one possible source of system performance degradation. 19 Other recent work on SM database implementations have considered parallelism in query processing <ref> [5, 10] </ref>. However, these studies have not addressed the performance of SM database lock management. Other general performance studies of the KSR-1 multiprocessor are reported in [21, 20], but these studies do not assess database related activities.
Reference: [11] <author> C. Mohan and I. Narang. </author> <title> Recovery and Coherency-Control Protocols for Fast Intersystem Page Transfer and Fine-Granularity Locking in a Shared-Disk Transaction Environment. </title> <booktitle> Proceedings of the 17th International Conference on Very Large Data Bases, </booktitle> <volume> 17 </volume> <pages> 193-207, </pages> <year> 1991. </year>
Reference-contexts: The SML strategy is particularly effective in the presence of hot spots. Hot spots occur when a small set of data ele 2 ments are requested concurrently by many transactions <ref> [19, 14, 11] </ref>. When multiple transactions attempt to lock the same data object in a compatible mode, e.g., read locks, (creating a hot lock), the lock manager becomes a performance bottleneck. <p> This strategy is called Message Passing Locking, or MPL. The MPL approach has been taken in many shared-disk (SD) database systems <ref> [11, 15, 17, 22] </ref>. SD systems differ from SM systems in that although both have access to shared disks, SD systems do not have shared memory. In SD systems, there are two basic models of MPL - centralized and distributed [15]. <p> Further details of these SM recovery mechanisms may be found in [13]. 6.2 Recovery in MPL Recovery strategies originally designed for SD systems can be employed for MPL. For example, consider recovery strategies discussed in the SD system proposed in <ref> [11] </ref>. In [11], locking is managed by a single server, called the GLM (Global Lock Manager) and a per node LLM (local 18 lock manager). <p> Further details of these SM recovery mechanisms may be found in [13]. 6.2 Recovery in MPL Recovery strategies originally designed for SD systems can be employed for MPL. For example, consider recovery strategies discussed in the SD system proposed in <ref> [11] </ref>. In [11], locking is managed by a single server, called the GLM (Global Lock Manager) and a per node LLM (local 18 lock manager). Initially, a lock is obtained from the GLM, however, since the requesting node caches lock acquisitions, subsequent requests made by the same node may be satisfied locally. <p> For example, the runtime overheads (in terms of space and time), associated with normal runtime operation of the volatile logging strategies in SML [13] are comparable to the caching strategies implemented in the SD variant of MPL in <ref> [11] </ref>. Also, for both SML and MPL, ensuring that spurious locks can be identified can most effectively be done with the same runtime mechanism storing the node ID in the lock table.
Reference: [12] <author> C. Mohan, I. Narang, and S. Silen. </author> <title> Solutions to Hot Spot Problems in a Shared Disks Transaction Environment. </title> <institution> IBM Research Report, IBM Almaden Research Center, </institution> <month> Decem-ber </month> <year> 1990. </year>
Reference-contexts: There are many cases where locks may be held concurrently by many different transactions. Clearly, many transactions may hold locks on different database items without conflict. In addition, transactions may concurrently hold locks on the same data item or index node. While hot spots <ref> [12, 14] </ref> refer to data which is frequently accessed, we use the term hot locks to refer to locks on database objects which are frequently accessed in a non-conflicting mode. Hot locks may arise under many situations in multi-node database systems.
Reference: [13] <author> L. Molesky and K. Ramamritham. </author> <title> Recovery Protocols for Shared Memory Database Systems. </title> <booktitle> Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 11-22, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: To address this problem for SM database systems implemented on cache 17 coherent architectures, recovery protocols have been proposed <ref> [13] </ref>. These protocols ensure the multi-node recovery objective mentioned earlier in this section. Consider the most popular and practical cache coherency protocol, the write-invalidate protocol. <p> After each surviving nodes determines which node (s) has crashed, the surviving node's restart recovery procedure merely needs to examine all LCB's on its node for any ID's of crashed nodes, and release these locks. Further details of these SM recovery mechanisms may be found in <ref> [13] </ref>. 6.2 Recovery in MPL Recovery strategies originally designed for SD systems can be employed for MPL. For example, consider recovery strategies discussed in the SD system proposed in [11]. <p> For example, the runtime overheads (in terms of space and time), associated with normal runtime operation of the volatile logging strategies in SML <ref> [13] </ref> are comparable to the caching strategies implemented in the SD variant of MPL in [11]. Also, for both SML and MPL, ensuring that spurious locks can be identified can most effectively be done with the same runtime mechanism storing the node ID in the lock table.
Reference: [14] <author> P. Peinl, A. Reuter, and H. Sammer. </author> <title> High Contention in a Stock Trading Database: A Case Study. </title> <booktitle> Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 260-268, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The SML strategy is particularly effective in the presence of hot spots. Hot spots occur when a small set of data ele 2 ments are requested concurrently by many transactions <ref> [19, 14, 11] </ref>. When multiple transactions attempt to lock the same data object in a compatible mode, e.g., read locks, (creating a hot lock), the lock manager becomes a performance bottleneck. <p> There are many cases where locks may be held concurrently by many different transactions. Clearly, many transactions may hold locks on different database items without conflict. In addition, transactions may concurrently hold locks on the same data item or index node. While hot spots <ref> [12, 14] </ref> refer to data which is frequently accessed, we use the term hot locks to refer to locks on database objects which are frequently accessed in a non-conflicting mode. Hot locks may arise under many situations in multi-node database systems.
Reference: [15] <author> E. Rahm. </author> <title> Concurrency and Coherency Control in Database Sharing Systems. </title> <type> Technical Report, </type> <institution> University of Kaiserslautern, Germany, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: This strategy is called Message Passing Locking, or MPL. The MPL approach has been taken in many shared-disk (SD) database systems <ref> [11, 15, 17, 22] </ref>. SD systems differ from SM systems in that although both have access to shared disks, SD systems do not have shared memory. In SD systems, there are two basic models of MPL - centralized and distributed [15]. <p> SD systems differ from SM systems in that although both have access to shared disks, SD systems do not have shared memory. In SD systems, there are two basic models of MPL - centralized and distributed <ref> [15] </ref>. In a centralized MPL architecture, a single node is designated to serve as the lock manager for all database objects. A centralized MPL architecture imposes an inherent sequential bottleneck on the lock acquisition/release process. <p> In distributed MPL, multiple lock managers can be used, where each manages a distinct partition of the lock space <ref> [15] </ref>. Of course, the absence of shared memory in an SD system precludes SML. However, since SML is possible in a SM database system, our objective is to quantitatively assess the performance differences between SML and MPL.
Reference: [16] <author> E. Rahm. </author> <title> Use of Global Extended Memory for Distributed Transaction Processing. </title> <booktitle> Proceedings of the 4th Int. Workshop on High Performance Transaction Systems, Asilomar, </booktitle> <address> CA., </address> <month> September </month> <year> 1991. </year>
Reference-contexts: However, these studies have not addressed the performance of SM database lock management. Other general performance studies of the KSR-1 multiprocessor are reported in [21, 20], but these studies do not assess database related activities. In <ref> [16] </ref> a technique for augmenting an SD system with shared memory is suggested. Instead of using a GLM per object, a global lock table is used in conjunction with a non-volatile global extended memory. Database locks are acquired directly from the global lock table stored in non-volatile shared memory.
Reference: [17] <author> T. Rengarajan, P. Spiro, and W. Wright. </author> <title> High Availability Mechanisms of VAX DBMS Software. </title> <journal> Digital Technical Journal, </journal> (8):88-98, February 1989. 
Reference-contexts: This strategy is called Message Passing Locking, or MPL. The MPL approach has been taken in many shared-disk (SD) database systems <ref> [11, 15, 17, 22] </ref>. SD systems differ from SM systems in that although both have access to shared disks, SD systems do not have shared memory. In SD systems, there are two basic models of MPL - centralized and distributed [15].
Reference: [18] <author> Kendall Square Research. </author> <title> KSR1 Principles of Operation. KSR Research, </title> <address> Waltham, Mass., </address> <year> 1992. </year>
Reference-contexts: Commercial multiprocessors, such as the KSR-1, have been constructed which scale to thousands of nodes (processor/memory pairs), and offer low-latency, high bandwidth access to shared memory via a hardware based cache coherency protocol <ref> [18] </ref>. On an SM database system of this caliber, it is conceivable that thousands of transactions may execute concurrently, and, as a result, create very high contention for shared resources. Typically, database systems employ locking to conservatively enforce transaction isolation, such as serializability or cursor stability [6]. <p> All memory in the system is cache memory, and thus this machine has been referred to as a COMA (Cache-Only Memory Architecture). Low latency access to cache memory is achieved with a hierarchy of slotted rings <ref> [18] </ref>. The first level of the hierarchy, called Ring 0, implements coherency among groups of 32 nodes. <p> If the address is not located in any of the caches, the disk will finally be accessed. A subcache access requires 2 cycles 1 , local cache access 20 - 24 cycles, local ring access 175 cycles, remote ring access 600 cycles <ref> [18] </ref>. These two subsections, 2.1 and 2.2, indicate that (1) in many cases, concurrency of database lock management operations is possible, and (2), a large-scale cache-coherent multiprocessor, such as the KSR-1, could potentially support a large number transactions which concurrently execute on multiple nodes.
Reference: [19] <author> A. Reuter. </author> <title> Concurrency on High-Traffic Data Elements. </title> <booktitle> Proc. 1982 ACM Symposium on Principles of Database Systems, </booktitle> <pages> pages 83-92, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: The SML strategy is particularly effective in the presence of hot spots. Hot spots occur when a small set of data ele 2 ments are requested concurrently by many transactions <ref> [19, 14, 11] </ref>. When multiple transactions attempt to lock the same data object in a compatible mode, e.g., read locks, (creating a hot lock), the lock manager becomes a performance bottleneck.
Reference: [20] <author> R. Saavendra, R. Gaines, and M. Carlton. </author> <title> Micro Benchmark Analysis of the KSR1. </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pages 202-213, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: However, these studies have not addressed the performance of SM database lock management. Other general performance studies of the KSR-1 multiprocessor are reported in <ref> [21, 20] </ref>, but these studies do not assess database related activities. In [16] a technique for augmenting an SD system with shared memory is suggested. Instead of using a GLM per object, a global lock table is used in conjunction with a non-volatile global extended memory.
Reference: [21] <author> J. Singh, T. Joe, A. Gupta, and J. Hennessy. </author> <title> An Empirical Comparison of the Kendall Square Research KSR-1 and Stanford DASH Multiprocessors. </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pages 214-225, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: However, these studies have not addressed the performance of SM database lock management. Other general performance studies of the KSR-1 multiprocessor are reported in <ref> [21, 20] </ref>, but these studies do not assess database related activities. In [16] a technique for augmenting an SD system with shared memory is suggested. Instead of using a GLM per object, a global lock table is used in conjunction with a non-volatile global extended memory.
Reference: [22] <author> W. Snaman and D. Thiel. </author> <title> The VAX/VMS Distributed Lock Manager. </title> <journal> Digital Technical Journal, </journal> (5):29-44, September 1987. 
Reference-contexts: This strategy is called Message Passing Locking, or MPL. The MPL approach has been taken in many shared-disk (SD) database systems <ref> [11, 15, 17, 22] </ref>. SD systems differ from SM systems in that although both have access to shared disks, SD systems do not have shared memory. In SD systems, there are two basic models of MPL - centralized and distributed [15].
Reference: [23] <author> S. Thakkar and M. Sweiger. </author> <title> Performance of an OLTP Application on Symmetry Multiprocessor System. </title> <booktitle> Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 228-238, </pages> <month> May </month> <year> 1990. </year> <month> 23 </month>
Reference-contexts: These studies are based on analytical and simulation models, and consider uniprocessor and SD database systems. In contrast, our study has considered implementation and performance issues in SM database systems, and has focused on empirical measurements. In <ref> [23] </ref>, the performance of on-line transaction processing applications are examined on a Sequent shared memory multiprocessor. This work examined the performance effects of the interaction between many system components, including the caching subsystem, process migration, and disk I/O.
Reference: [24] <author> A. Thomasian. </author> <title> Two-Phase Locking and its Trashing Behavior. </title> <journal> ACM Transactions on Database Systems, </journal> <pages> pages 579-625, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Given these similar recovery mechanisms and overheads, choice of SM lock management architectures should be determined based on performance criterion. 7 Related Work A number of studies have been conducted on the performance of database lock management, to name a few see <ref> [3, 24, 25] </ref>. These studies are based on analytical and simulation models, and consider uniprocessor and SD database systems. In contrast, our study has considered implementation and performance issues in SM database systems, and has focused on empirical measurements.
Reference: [25] <author> A. Thomasian. </author> <title> On a More Realistic Lock Contention Model and its Analysis. </title> <booktitle> Proceedings of the 10'th International Conference on Data Engineering, </booktitle> <pages> pages 2-9, </pages> <year> 1994. </year> <month> 24 </month>
Reference-contexts: Given these similar recovery mechanisms and overheads, choice of SM lock management architectures should be determined based on performance criterion. 7 Related Work A number of studies have been conducted on the performance of database lock management, to name a few see <ref> [3, 24, 25] </ref>. These studies are based on analytical and simulation models, and consider uniprocessor and SD database systems. In contrast, our study has considered implementation and performance issues in SM database systems, and has focused on empirical measurements.
References-found: 25

