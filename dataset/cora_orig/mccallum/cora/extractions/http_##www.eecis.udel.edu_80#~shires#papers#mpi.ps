URL: http://www.eecis.udel.edu:80/~shires/papers/mpi.ps
Refering-URL: http://www.eecis.udel.edu:80/~shires/professional.html
Root-URL: http://www.cis.udel.edu
Email: dshires@arl.mil  
Title: MPI and HPF performance in a cluster environment.  
Author: Dale Shires 
Address: MD 21005,  
Note: Army Research Laboratory, Aberdeen Proving Ground,  
Date: March 2, 1998  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> Center for Research on Parallel Computation, Houston, TX. High Performance Fortran Language Specification, </institution> <note> Version 1.0, </note> <year> 1993. </year>
Reference-contexts: MPI extensions exist for both the C and FORTRAN programming languages. HPF is specifically tailored to augment the FORTRAN 90 language. Both MPI and HPF have adopted published standards. HPF was adopted in 1993 <ref> [1] </ref>, and MPI was standardized in 1994 [2]. These standards have allowed codes to be easily ported between different computer architectures with a minimum of code rewriting. These two systems offer quite different parallel computational models.
Reference: [2] <institution> Computer Science Department, University of Tennessee, Knoxville, TN. MPI: A Message-Passing Interface Standard, </institution> <year> 1994. </year>
Reference-contexts: MPI extensions exist for both the C and FORTRAN programming languages. HPF is specifically tailored to augment the FORTRAN 90 language. Both MPI and HPF have adopted published standards. HPF was adopted in 1993 [1], and MPI was standardized in 1994 <ref> [2] </ref>. These standards have allowed codes to be easily ported between different computer architectures with a minimum of code rewriting. These two systems offer quite different parallel computational models. <p> There were several problems in getting HPF to work with this code. Several times, the executables would get hung up and then exit with: TCP_MsgReadMsg: read error 54ows: ERROR Peer [0] (38) _TCP_Send: send length error - errno 9 (msgsend.c Line:377) ows: ERROR Peer <ref> [2] </ref> (38) _TCP_Send: send length error - errno 9 (msgsend.c Line:377) ows: ERROR Peer [3] (38) _TCP_Send: send length error - errno 9 (msgsend.c Line:377).
Reference: [3] <author> William Gropp, Ewing Lusk, and Anthony Skjellum. </author> <title> Using MPI: Portable Parallel Programming With the Message-Passing Interface. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: This is often classified as an advantage rather than a hindrance. Effective cache use and memory management are becoming extremely important to achieving good parallel performance. Message passing can help in this regard by providing more programmer control of data locality in the memory hierarchy <ref> [3] </ref>. MPI appears to be surpassing the Parallel Virtual Machine (PVM) interface as the defacto standard in message passing parallelism. In contrast, HPF is more closely associated with the data-parallel programming model. Data parallelism attempts to exploit the concurrency of the same operation to multiple data elements [4]. <p> Several times, the executables would get hung up and then exit with: TCP_MsgReadMsg: read error 54ows: ERROR Peer [0] (38) _TCP_Send: send length error - errno 9 (msgsend.c Line:377) ows: ERROR Peer [2] (38) _TCP_Send: send length error - errno 9 (msgsend.c Line:377) ows: ERROR Peer <ref> [3] </ref> (38) _TCP_Send: send length error - errno 9 (msgsend.c Line:377). The (BLOCK, *) distribution produced the following error during runtime. forrtl: error (72): floating overflow TCP_MsgReadMsg: read error 54ows: ERROR Peer [0] (29) _TCP_RecvAvail: Unexpected EOF from peer 0 (msgrecva.c Line:189).
Reference: [4] <author> Ian Foster. </author> <title> Designing and Building Parallel Programs. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Sebastopol, CA, </address> <year> 1995. </year>
Reference-contexts: MPI appears to be surpassing the Parallel Virtual Machine (PVM) interface as the defacto standard in message passing parallelism. In contrast, HPF is more closely associated with the data-parallel programming model. Data parallelism attempts to exploit the concurrency of the same operation to multiple data elements <ref> [4] </ref>. For example, one may wish to add the value 10 to each element of an array. This operation is inherently parallel, since there are no data dependencies inside this atomic operation. <p> In FORTRAN, one can expect column major order accessed by columns to be faster. (2) There should be little difference between the fastest decomposition and agglomeration with MPI and the fastest similar decomposition with HPF. The main fl Complete examples can be found in <ref> [4] </ref> and [5]. 3 limiting factor of both should be the speed of the interconnection network. Ide--ally, both should make the same optimal use of this system.
Reference: [5] <author> Charles Koelbel, David Loveman, and Robert Schreiber. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: HPF is therefore slightly more abstract than MPI. It expresses parallelism at a relatively high level and is intended to remove the programmer from the more mundane tasks of specifying communication behavior between processors <ref> [5] </ref>. Of particular interest is how well these systems perform, both in general and compared to each other, in current computer architectures. The two basic performance metrics of parallel computing systems, speedup and efficiency, were studied in this experiment. <p> In FORTRAN, one can expect column major order accessed by columns to be faster. (2) There should be little difference between the fastest decomposition and agglomeration with MPI and the fastest similar decomposition with HPF. The main fl Complete examples can be found in [4] and <ref> [5] </ref>. 3 limiting factor of both should be the speed of the interconnection network. Ide--ally, both should make the same optimal use of this system.
Reference: [6] <author> Gregory F. Pfister. </author> <title> In Search of Clusters; The Coming Battle in Lowly Parallel Computing. </title> <publisher> Prentice Hall, </publisher> <address> Upper Saddle River, NJ, </address> <year> 1995. </year>
Reference-contexts: Army Research Laboratory. Three factors have played into clusters becoming viable parallel programming platforms. These factors are workstation-level high-performance microprocessors, standardized high-speed communication, and reliable standardized tools for distributed computing <ref> [6] </ref>. Today's workstations have processors that are quite robust. Very few of them need to be coupled to deliver impressive parallel performance. High-performance networks, such as ATM, HiPPI, and FDDI are now capable of delivering bandwidths of around 100 MB/s.
Reference: [7] <author> Stephen T. Barnard. </author> <title> A stochastic approach to stereovision. </title> <booktitle> In Readings in Computer Vision. </booktitle> <publisher> Addison-Wesley, </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: To perform this operation accurately, the entire image must be taken into account. One of the most popular optimization techniques to locate a global optimum is called simulated annealing. This approach can be applied directly to the image-matching problem <ref> [7] </ref>. As the name implies, the approach imitates a natural process. Annealing involves heating a solid to the extent that the molecules may randomly rearrange themselves and then cool gradually. Slowly lowering the temperature allows the molecules to settle into the lowest energy state, commonly described as thermal equilibrium.
Reference: [8] <author> Dale R. Shires. </author> <title> Exploiting parallelism in a monte carlo image-matching algorithm. </title> <type> Technical Report ARL-TR-667, U.S. </type> <institution> Army Research Laboratory, Aberdeen Proving Ground, MD, </institution> <month> January </month> <year> 1995. </year> <note> 17 INTENTIONALLY LEFT BLANK. 18 </note>
Reference-contexts: This helps to prevent the system from sinking into local minima. The processing is complete when the system is in equilibrium at the lowest energy state achievable. A more detailed discussion of this technique and its Army applications may be found in <ref> [8] </ref>. The final result of the algorithm is a 2-D disparity map. The values in the disparity map are integer values ranging from 0 (no disparity) to D M AX (maximum disparity).
References-found: 8

