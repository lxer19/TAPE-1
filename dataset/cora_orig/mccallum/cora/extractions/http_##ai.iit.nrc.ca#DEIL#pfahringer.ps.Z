URL: http://ai.iit.nrc.ca/DEIL/pfahringer.ps.Z
Refering-URL: http://www.ai.mit.edu/people/jude/research/afspaper.html
Root-URL: 
Email: E-mail: bernhard@ai.univie.ac.at  
Phone: Phone: (+43 1) 533-6112 Fax: (+43 1) 532-0652  
Title: Compression-Based Feature Subset Selection  Keywords: Minimum Description Length Principle, Cross Validation, Noise  
Author: Bernhard Pfahringer 
Address: Schottengasse 3 A-1010 Vienna Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: Irrelevant and redundant features may reduce both predictive accuracy and comprehensibility of induced concepts. Most common Machine Learning approaches for selecting a good subset of relevant features rely on cross-validation. As an alternative, we present the application of a particular Minimum Description Length (MDL) measure to the task of feature subset selection. Using the MDL principle allows taking into account all of the available data at once. The new measure is information-theoretically plausible and yet still simple and therefore efficiently computable. We show empirically that this new method for judging the value of feature subsets is more efficient than and performs at least as well as methods based on cross-validation. Domains with both a large number of training examples and a large number of possible features yield the biggest gains in efficiency. Thus our new approach seems to scale up better to large learning problems than previous methods. 
Abstract-found: 1
Intro-found: 1
Reference: [Almuallim & Dietterich 91] <author> Almuallim H., Dietterich T.G.: </author> <title> Learning with Many Irrelevant Features, </title> <booktitle> in Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <booktitle> Vol. II, </booktitle> <address> pp.547-552, </address> <year> 1991. </year>
Reference-contexts: It seems to be a more efficient alternative to standard cross-validation, which optimizes solely predictive accuracy for selecting good subsets of features in inductive learning. Alternative methods for judging feature relevance are e.g. FOCUS <ref> [Almuallim & Dietterich 91] </ref> and RELIEF [Kira & Rendell 92]. Their respective shortcomings are detailed in [John et al. 94].
Reference: [Caruana & Freitag 94] <author> Caruana R., Freitag D.: </author> <title> Greedy Attribute Selection, </title> <editor> in Cohen W.W. and Hirsh H.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference (ML94), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: Best-first search is only one of a few possible search strategies. Clearly exhaustive enumeration of all possible subsets is out of the question except for the simplest domains. <ref> [Caruana & Freitag 94] </ref> compare various forms of greedy hill-climbing. They report favorable results, especially for three methods (FSS, BSE, and BSE-Slash), for their domain of calendar scheduling tasks.
Reference: [Cheeseman 90] <author> Cheeseman P.: </author> <title> On Finding the Most Probable Model, </title> <editor> in Shrager J., Langley P.(eds.): </editor> <title> Computational Models of Discovery and Theory Formation, </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1990. </year>
Reference-contexts: A very good introduction to MDL and also its close relation to Bayesian theory can be found in <ref> [Cheeseman 90] </ref>. He defines the message length of a theory (called model in his article) as: Total message length = Message length to describe the model + Message length to describe the data, given the model.
Reference: [John et al. 94] <author> John G.H., Kohavi R., Pfleger K.: </author> <title> Irrelevant Features and the Subset Selection Problem, </title> <editor> in Cohen W.W. and Hirsh H.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference (ML94), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: As these three methods perform bidirectional hill-climbing and as they are equipped with a mechanism for preventing cycles, they can theoretically explore all possible subsets (given enough time). [Skalak 94] successfully applies random mutation hill-climbing . <ref> [John et al. 94, Kohavi 95] </ref> make good use of best-first search for all experiments they report. <p> Alternative methods for judging feature relevance are e.g. FOCUS [Almuallim & Dietterich 91] and RELIEF [Kira & Rendell 92]. Their respective shortcomings are detailed in <ref> [John et al. 94] </ref>. RELIEF is only able to delete irrelevant features, but it cannot delete redundant features, whereas FOCUS might be trapped into selecting a single feature having a distinct value for every single training example, which may result in poor predictive accuracy.
Reference: [Kira & Rendell 92] <author> Kira K., Rendell L.A.: </author> <title> A Practical Approach to Feature Selection, </title> <editor> in Slee man D. and Edwards P.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Ninth International Workshop (ML92), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.249-256, </address> <year> 1992. </year>
Reference-contexts: It seems to be a more efficient alternative to standard cross-validation, which optimizes solely predictive accuracy for selecting good subsets of features in inductive learning. Alternative methods for judging feature relevance are e.g. FOCUS [Almuallim & Dietterich 91] and RELIEF <ref> [Kira & Rendell 92] </ref>. Their respective shortcomings are detailed in [John et al. 94].
Reference: [Kohavi 95] <author> Kohavi R.: </author> <title> The Power of Decision Tables, </title> <note> Paper, submitted. (also available elec tronically as ftp://starry.stanford.edu/pub/ronnyk/tables.ps) </note>
Reference-contexts: In this paper we describe the application of the Minimum Description Length (MDL) principle [Rissanen 78] as an objective function for judging goodness of a feature subset. Feature subsets are used to construct so called simple decision tables <ref> [Kohavi 95] </ref>. MDL takes into account both the simplicity and the accuracy of the simple decision tables induced by particular feature subsets. MDL uses all of the training data at once eliminating the need for costly cross-validation runs. <p> In section 3 we will describe an MDL coding schema for simple decision tables. Experimental considerations and setup, and empirical results using this coding schema are reported in section 4. Section 5 discusses open problems and further research directions. 2 Simple Decision Tables <ref> [Kohavi 95] </ref> describes simple decision tables and their usage for classification as follows: * For any feature subset construct a decision table by simply projecting all given training examples on the feature subset. * For all after projection identical examples count class frequencies and assign the majority class to every entry. <p> As these three methods perform bidirectional hill-climbing and as they are equipped with a mechanism for preventing cycles, they can theoretically explore all possible subsets (given enough time). [Skalak 94] successfully applies random mutation hill-climbing . <ref> [John et al. 94, Kohavi 95] </ref> make good use of best-first search for all experiments they report. <p> Incremental k-fold cross-validation is described in <ref> [Kohavi 95] </ref>. If it is possible to incrementally add examples to and delete examples from the data-structures maintained by the learning algorithm, cross-validation can be done incrementally, which saves a lot of computational work.
Reference: [Moore & Lee 94] <author> Moore A.W., Lee M.S.: </author> <title> Efficient Algorithms for Minimizing Cross Validation Error, in Cohen W.W. & Hirsh H.(eds.), Machine Learning, </title> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: Both heuristics seem to find reasonable subsets when compared with C4.5 and both might act as a preprocessing filter for C4.5 for domains where C4.5 performs not so well on its own. 4 But maybe the ideas of <ref> [Moore & Lee 94] </ref> could be used to speed up ICV, too. 8 5 Conclusions, Related Work, and Further Re search We have defined an MDL measure for simple decision tables.
Reference: [Pagallo & Haussler 90] <author> Pagallo G., Haussler D.: </author> <title> Boolean Feature Discovery in Empirical Learning, </title> <journal> Machine Learning, </journal> <volume> 5(1), </volume> <pages> 71-100, </pages> <year> 1990. </year>
Reference-contexts: The 27 additional boolean features are random (and therefore irrelevant). The training set size is 4000 each, and 2000 examples each are used for testing. This particular domain was first used by <ref> [Pagallo & Haussler 90] </ref> and is especially difficult to learn for decision tree algorithms. This should not be surprising, as only relatively few subsets of the large number of all possible subsets (2 32 ) yield a classification performance significantly better than random guessing.
Reference: [Pfahringer 95] <author> Pfahringer B.: </author> <title> A New MDL Measure for Robust Rule Induction (Extended Abstract), </title> <booktitle> European Conference on Machine Learning (ECML95), </booktitle> <address> Iraklion, </address> <year> 1995. </year>
Reference-contexts: The theory with the minimal total message length is also the most probable theory explaining the data [Rissanen 78]. We will now describe a coding schema for simple decision tables that basically is just an adaptation of a schema we have devised for propositional rule-sets <ref> [Pfahringer 95] </ref>. Actually we don't really need to encode the tables, we just need a formula for estimating the number of bits needed if we would encode a table and the examples in terms of that table.
Reference: [Quinlan 93] <author> Quinlan J.R.: C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: For further comparisons C4.5 <ref> [Quinlan 93] </ref> was used on both the original complete sets of features and on the best subsets returned by both heuristics for feature subset selection. Best-first search is only one of a few possible search strategies.
Reference: [Rissanen 78] <author> Rissanen J.: </author> <title> Modeling by Shortest Data Description, </title> <journal> in Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: In this paper we describe the application of the Minimum Description Length (MDL) principle <ref> [Rissanen 78] </ref> as an objective function for judging goodness of a feature subset. Feature subsets are used to construct so called simple decision tables [Kohavi 95]. MDL takes into account both the simplicity and the accuracy of the simple decision tables induced by particular feature subsets. <p> This way a more complex theory will need more bits to be encoded, but might save bits when encoding more data correctly. The theory with the minimal total message length is also the most probable theory explaining the data <ref> [Rissanen 78] </ref>. We will now describe a coding schema for simple decision tables that basically is just an adaptation of a schema we have devised for propositional rule-sets [Pfahringer 95].
Reference: [Shannon & Weaver 49] <author> Shannon C.E. and Weaver W.: </author> <title> The Mathematical Theory of Commu nication, </title> <publisher> University of Illinois Press, </publisher> <year> 1949. </year>
Reference-contexts: Actually it is sufficient to just encode the exceptions, i.e. which examples are assigned the wrong class by the decision table. For estimating the cost of any selection in general (7,8,9) we use the well-known information-theoretic bound given by <ref> [Shannon & Weaver 49] </ref>.
Reference: [Skalak 94] <author> Skalak D.B.: </author> <title> Prototype and Feature Selection by Sampling and Random Muta tion Hill Climbing Algorithms, </title> <editor> in Cohen W.W. and Hirsh H.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference (ML94), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: They report favorable results, especially for three methods (FSS, BSE, and BSE-Slash), for their domain of calendar scheduling tasks. As these three methods perform bidirectional hill-climbing and as they are equipped with a mechanism for preventing cycles, they can theoretically explore all possible subsets (given enough time). <ref> [Skalak 94] </ref> successfully applies random mutation hill-climbing . [John et al. 94, Kohavi 95] make good use of best-first search for all experiments they report.
Reference: [Thrun et al. 91] <author> Thrun S.B., et.al.: </author> <title> The MONK's Problems: A Performance Comparison of Different Learning Algorithms, </title> <type> CMU Tech Report, </type> <institution> CMU-CS-91-197, </institution> <year> 1991. </year> <month> 10 </month>
Reference-contexts: The 5 additional boolean features are random (and therefore irrelevant). The training set contains 100 examples, and all 1024 possible examples are used for testing ([John et al. 94] uses the same settings). * Monk1, Monk2, Monk2loc, Monk3: These data sets are taken from <ref> [Thrun et al. 91] </ref>. Monk1, Monk2, and Monk3 all have six features yielding 432 possible different examples. Ten sets each were drawn randomly 4 from these 432 examples for learning, testing used all 432 examples (as is done originally in [Thrun et al. 91]). <p> Monk1, Monk2, Monk2loc, Monk3: These data sets are taken from <ref> [Thrun et al. 91] </ref>. Monk1, Monk2, and Monk3 all have six features yielding 432 possible different examples. Ten sets each were drawn randomly 4 from these 432 examples for learning, testing used all 432 examples (as is done originally in [Thrun et al. 91]). Training set sizes are 124, 169, and 122 respectively. For Monk3 class noise is artificially added by switching the class values of 5% of the training examples.
References-found: 14

