URL: http://www.cs.cmu.edu/afs/cs/project/connect/intro-course/handouts/becker-CNS-TR-1-95.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/project/connect/intro-course/handouts/
Root-URL: 
Title: Unsupervised Neural Network Learning Procedures For Feature Extraction and Classification  
Author: Suzanna Becker Mark Plumbley 
Note: To appear in: International Journal of Applied Intelligence, Special Issue on Applications of Neural Networks, Vol. 6, No. 3, F. Pineda (ed), 1996.  
Address: King's College, London Hamilton, Ont. Canada L8S 4K1 Strand, London WC2R 2LS UK  
Affiliation: Department of Psychology Department of Computer Science McMaster University  
Abstract: Technical report CNS-TR-95-1 Center for Neural Systems McMaster University 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. M. Abbas and M. M. Fahmy. </author> <title> A neural model for adaptive Karhunen Loeve transform (KLT). </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> IJCNN-92 Baltimore, pages II:975-980, </address> <year> 1992. </year>
Reference-contexts: They both reduce to the Oja rule (11) for a network with single output. Abbas and Fahmy <ref> [1] </ref> suggested a related approach. After each successive output unit learns its principal component, the effect of that component is removed by subtracting it from the original data. Using the Oja Rule on the modified input data will find the next principal component.
Reference: [2] <author> J. J. Atick and A. N. Redlich. </author> <title> Predicting ganglion and simple cell receptive field organizations from information theory. </title> <type> Technical Report IASSNS-HEP-89/55, </type> <institution> Institute for Advanced Study, Princeton, </institution> <year> 1989. </year>
Reference-contexts: Atick and Redlich <ref> [2] </ref> proposed an equivalent learning procedure for a spatial predicting unit, to model the development of retinal ganglion cell kernels. The latter two methods could both be applied in the nonlinear case as well; these methods are closely related to Imax, described in the next subsection.
Reference: [3] <author> J. J. Atick and A. N. Redlich. </author> <title> Towards a theory of early visual processing. </title> <type> Technical Report IASSNS-HEP-90/10, </type> <institution> Institute for Advanced Study, Princeton, </institution> <year> 1990. </year>
Reference-contexts: Barlow proposes that one way to achieve featural independence is to find a minimum entropy encoding: an invertible code (i.e., one with no information loss) which minimizes the sum of the feature entropies. In the general case, this problem is intractable. Atick and Redlich <ref> [3] </ref> have proposed a cost function for Barlow's principle for linear systems which minimizes the power (redundancy) in the outputs subject to a minimal information loss constraint.
Reference: [4] <author> P. Baldi and K. Hornik. </author> <title> Neural networks and principal component analysis: Learning from examples without local minima. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 53-58, </pages> <year> 1989. </year>
Reference-contexts: * v * w , and all the selected principal components of the input have variance greater than ff [64]). 2.1.6 Auto-Encoders Instead of using one of the many principal subspace algorithms mentioned so far, the Error Back Propagation (`BackProp') algorithm can also be used to find the principal subspace <ref> [4] </ref>. <p> Note that there is nothing to force the weight vectors w i (t) to be orthogonal: any set of weights which spans the principal subspace is sufficient <ref> [4] </ref>. In common with other networks which use BackProp, the units in an auto-encoder can have nonlinear activation functions, such as the sigmoid function (a) = 1=(1 + exp (a)).
Reference: [5] <author> H. B. Barlow. </author> <title> Unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 295-311, </pages> <year> 1989. </year>
Reference-contexts: Linsker [45] shows that adding a "weakly nonlinear" (cubic) input-output relation to the Infomax principle, for certain translation-invariant input distributions, results in a set of units tuned to different spatial frequencies and spatial locations, much like a wavelet representation. An alternative optimality criterion proposed by Barlow <ref> [5] </ref> is to find a minimally redundant encoding, which should facilitate subsequent learning.
Reference: [6] <author> S. Becker. </author> <title> An Information-theoretic Unsupervised Learning Algorithm for Neural Networks. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <year> 1992. </year>
Reference-contexts: The latter two methods could both be applied in the nonlinear case as well; these methods are closely related to Imax, described in the next subsection. Becker <ref> [6] </ref> has shown that a continuous generalization of GMAX for Gaussian input distributions results in an invariance detector that minimizes the ratio of its output variance divided by the variance that would be expected if the input lines were independent (the sum of the variances of the inputs): V (y i <p> multi-layer learning procedure for discovering higher-order invariants. 17 Image patch 1 Image patch 2 a J J J J] J J 6 6 Maximize I This algorithm is further generalized to apply to a group of units which form a mixture model of different invariant properties of the input patterns <ref> [6] </ref>. Schraudolph and Sejnowski [73] propose a closely related learning scheme, combining a variance-minimizing anti-Hebbian term and a term that prevents the weights all converging to zero. They show that a set of competing units can thereby discover population codes for stereo disparity in random dot stereograms. <p> This learning rule can be applied to two multi-layer modules to learn features that are not linearly separable, such as the shift in random binary shift patterns <ref> [6] </ref>. The two modules receive as input random binary patterns in which the left half is a shifted version of the right half. The inputs to the two modules are unrelated apart from having the same shift. <p> j ) log P (B = b j ) (59) X P (A = a i ; B = b j ) log P (A = a i ; B = b j ) This is a straightforward generalization of the binary case, and similar learning rules can be derived <ref> [6] </ref>. Under Gaussian assumptions, a simpler objective function for Imax learning can be derived. <p> The multi-valued discrete version of Imax has been successfully applied in single-layer networks to toy problems such as extracting combinations of spatial frequency and phase in sinusoidal intensity patterns <ref> [6] </ref>.
Reference: [7] <author> S. Becker. </author> <title> Learning to categorize objects using temporal coherence. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 361-368. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Finally, it has been applied to temporally varying patterns to classify temporally coherent objects over time <ref> [7] </ref>. Zemel and Hinton [81] have applied the multi-dimensional version of Imax to the problem of learning to represent the viewing parameters of simple synthetic two-dimensional objects.
Reference: [8] <author> S. Becker and G. E. Hinton. </author> <title> A self-organizing neural network that discovers surfaces in random-dot stereograms. </title> <journal> Nature, </journal> <volume> 355 </volume> <pages> 161-163, </pages> <year> 1992. </year>
Reference-contexts: Bell [10] has proposed an energy-minimizing algorithm for a single axon that leads to an anti-Hebbian learning rule that predicts the evolution of ion channel densities; this allows an axon to redistribute channels so as to efficiently process recurring spatio-temporal patterns. 4.4 Spatially coherent features Becker and Hinton <ref> [8] </ref> proposed that a good objective function for unsupervised learning is to discover properties of the sensory input that exhibit coherence across space and time. The Imax learning procedure [8] does this by maximizing the mutual information between the outputs, y a and y b , of network modules that receive <p> allows an axon to redistribute channels so as to efficiently process recurring spatio-temporal patterns. 4.4 Spatially coherent features Becker and Hinton <ref> [8] </ref> proposed that a good objective function for unsupervised learning is to discover properties of the sensory input that exhibit coherence across space and time. The Imax learning procedure [8] does this by maximizing the mutual information between the outputs, y a and y b , of network modules that receive input from different parts of the sensory input (e.g. different modalities, or different spatial or temporal samples), as shown in Figure 6. <p> It has also been applied successfully to continuous higher-order feature extraction problems such as learning to represent stereo disparity and surface curvature in random dot stereograms <ref> [8] </ref>. Under a mixture model of the underlying coherent feature, the algorithm can be extended to develop population codes of spatially coherent features such as stereo disparity [8], and to model the locations of discontinuities in depth [9]. <p> been applied successfully to continuous higher-order feature extraction problems such as learning to represent stereo disparity and surface curvature in random dot stereograms <ref> [8] </ref>. Under a mixture model of the underlying coherent feature, the algorithm can be extended to develop population codes of spatially coherent features such as stereo disparity [8], and to model the locations of discontinuities in depth [9]. Finally, it has been applied to temporally varying patterns to classify temporally coherent objects over time [7].
Reference: [9] <author> S. Becker and G. E. Hinton. </author> <title> Learning mixture models of spatial coherence. </title> <journal> Neural Computation, </journal> <volume> 5(2) </volume> <pages> 267-277, </pages> <year> 1993. </year>
Reference-contexts: Under a mixture model of the underlying coherent feature, the algorithm can be extended to develop population codes of spatially coherent features such as stereo disparity [8], and to model the locations of discontinuities in depth <ref> [9] </ref>. Finally, it has been applied to temporally varying patterns to classify temporally coherent objects over time [7]. Zemel and Hinton [81] have applied the multi-dimensional version of Imax to the problem of learning to represent the viewing parameters of simple synthetic two-dimensional objects.
Reference: [10] <author> A. J. Bell. </author> <title> Self-organisation in real neurons: </title> <booktitle> Anti-hebb in `channel space' ? In Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 59-66. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Schraudolph and Sejnowski [73] propose a closely related learning scheme, combining a variance-minimizing anti-Hebbian term and a term that prevents the weights all converging to zero. They show that a set of competing units can thereby discover population codes for stereo disparity in random dot stereograms. Bell <ref> [10] </ref> has proposed an energy-minimizing algorithm for a single axon that leads to an anti-Hebbian learning rule that predicts the evolution of ion channel densities; this allows an axon to redistribute channels so as to efficiently process recurring spatio-temporal patterns. 4.4 Spatially coherent features Becker and Hinton [8] proposed that a
Reference: [11] <author> E. L. Bienenstock, L. N. Cooper, and P. W. Munro. </author> <title> Theory for the development of neuron selectivity; orientation specificity and binocular interaction in visual cortex. </title> <journal> Journal of Neuroscience, </journal> <volume> 2 </volume> <pages> 32-48, </pages> <year> 1982. </year>
Reference-contexts: In this last section, we consider several examples of learning procedures based on the idea of learning particular features of the data. 4.1 Maximally "selective" projections Bienenstock, Cooper and Munro <ref> [11] </ref> proposed a learning rule (commonly referred to as the BCM rule) which results in a form of temporal selectivity of a single unit with respect to some particular environment.
Reference: [12] <author> H. Bourlard and Y. Kamp. </author> <title> Auto-association by multilayer perceptrons and singular value decomposition. </title> <journal> Biological Cybernetics, </journal> <volume> 59 </volume> <pages> 291-294, </pages> <year> 1988. </year>
Reference-contexts: therefore an optimal linear dimension-reduction method. 2.1.1 Classical Techniques for PCA If the entire data sample sequence x (t) for t = 1; : : : ; t is available at once, the required weight vectors for PCA can be calculated using standard techniques such as singular value decomposition (SVD) <ref> [12] </ref>. A sequential version is also available, allowing the SVD to be updated as the data arrives. However, while SVD is an exact method, which will use all of the data in the input data sequence seen so far, it is rather complex for a neural network algorithm. <p> However, opinions are divided as to whether this is achieved in practice, and some have argued that a method based 9 on SVD is more reliable, and not susceptible to local minima <ref> [12] </ref>. A further problem is that the nonlinear hidden layer codes are usually difficult to interpret. <p> Schmidhuber's learning scheme is rather complex, however, and appears to be subject to oscillations and local minima. 2.5 Applications Many of the applications for PCA and subspace methods are related to data compression or preprocessing of speech <ref> [18, 12] </ref> or images [15]. Sanger, for example, used his GHA network for image compression [69]. He also extended this to nonlinear units with a rectification nonlinearity, which he used to discover stereo disparity in random dot stereograms.
Reference: [13] <author> J.S. Bridle. </author> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. </title> <editor> In F. Fougelman-Soulie and J. Herault, editors, </editor> <booktitle> NATO ASI series on systems and computer science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: This can be done, for example, by using the "softmax" activation function suggested by Bridle <ref> [13] </ref>: P (A = a i ) = P n (58) where x i is the total weighted summed input to the ith unit.
Reference: [14] <author> G.A. Carpenter and S. Grossberg. </author> <title> A massively parallel architecture for a self-organizing neural pattern recognition machine. </title> <journal> Computer Vision, </journal> <volume> 37 </volume> <pages> 54-115, </pages> <year> 1983. </year>
Reference-contexts: Competitive learning procedures <ref> [76, 22, 37, 14, 68] </ref> are used primarily for clustering. The general idea underlying competitive learning is to induce a competition between units' responses, either by a winner-take-all activation function or with lateral interactions, so that only one unit in each competitive cluster tends to be active at a time.
Reference: [15] <author> G. W. Cottrell, P. W. Munro, and D. Zipser. </author> <title> Image compression by back propagation: A demonstration of extensional programming. </title> <editor> In N. E. Sharkey, editor, </editor> <booktitle> Advances in Cognitive Science, volume 2. </booktitle> <address> Abbex, Norwood, NJ, </address> <year> 1989. </year>
Reference-contexts: Schmidhuber's learning scheme is rather complex, however, and appears to be subject to oscillations and local minima. 2.5 Applications Many of the applications for PCA and subspace methods are related to data compression or preprocessing of speech [18, 12] or images <ref> [15] </ref>. Sanger, for example, used his GHA network for image compression [69]. He also extended this to nonlinear units with a rectification nonlinearity, which he used to discover stereo disparity in random dot stereograms. Leen, Rudnick and Hammerstrom [43] used PCA networks for signal pre-processing and found improved classification performance.
Reference: [16] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Proceedings of the Royal Statistical Society, </journal> <volume> B-39:1-38, </volume> <year> 1977. </year>
Reference-contexts: point x: p (i j x; f i g ; f i g ; f i g) = P n (43) Given these probabilities, the model parameters can be adapted by performing gradient ascent in the log likelihood of the data given the model, log (L) = P EM algorithm <ref> [16] </ref> alternately applies equation (43) (the Expectation step) and adapts the model 1 As noted by one reviewer, the techniques discussed here under the category of density estimation might more specifically be characterized as maximum likelihood parameter estimation methods. 12 parameters (the Maximization step) to converge on the maximum likelihood mixture
Reference: [17] <author> J. L. Elman. </author> <title> Finding structure in time, </title> <year> 1990. </year>
Reference-contexts: He used this approach to classify Brodatz image textures. Temporal sequence predictors have been applied to a range of prediction problems. Recurrent networks have been used predominantly for symbolic tasks in which the goal is to represent discrete 11 hidden variables by extracting temporal structure. For example, Elman <ref> [17] </ref> has used simple recurrent networks to infer grammatical structure from sentences. Mozer [49] has applied recurrent networks with multiple time decay constants to the problem of extracting musical structure at multiple time scales by predicting notes in a musical score.
Reference: [18] <author> J. L. Elman and D. Zipser. </author> <title> Learning the hidden structure of speech. </title> <type> ICS Report 8701, </type> <institution> Institute of Cognitive Science, University of California, </institution> <address> San Diego, </address> <year> 1987. </year> <month> 21 </month>
Reference-contexts: Schmidhuber's learning scheme is rather complex, however, and appears to be subject to oscillations and local minima. 2.5 Applications Many of the applications for PCA and subspace methods are related to data compression or preprocessing of speech <ref> [18, 12] </ref> or images [15]. Sanger, for example, used his GHA network for image compression [69]. He also extended this to nonlinear units with a rectification nonlinearity, which he used to discover stereo disparity in random dot stereograms.
Reference: [19] <author> F. Fallside. </author> <title> On the analysis of multi-dimensional linear predicitve/autoregressive data by a class of single layer connectionist models. </title> <booktitle> In IEE Conference on Artificial Neural Networks, </booktitle> <pages> pages 176-180, </pages> <year> 1989. </year>
Reference-contexts: Several algorithms for learning invariant features of the input have been proposed. Kohonen and Oja [39] proposed a learning algorithm for a single unit which acts as a "novelty detector", by responding best to patterns which are orthogonal to the principal subspace of the input distribution. Fallside <ref> [19] </ref> proposed a learning procedure which implements a linear prediction filter: a unit receives inputs representing the values of a signal at several time frames, and tries to make its output zero by computing the sum of the signal at the current time slice and a linear combination of the signal
Reference: [20] <author> P. Foldiak. </author> <title> Adaptive network for optimal linear feature extraction. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, IJCNN-89, </booktitle> <pages> pages 401-405, </pages> <address> Washington, DC, </address> <year> 1989. </year>
Reference-contexts: The third output y 3 is decorrelated from the previous two, and so on until all M desired principal components have been extracted in order. Kung and Diamantaras [40] suggested an Adaptive Principal Component Extractor (APEX) which is related to the Rubner and Tavan approach. Foldiak <ref> [20] </ref> suggested that units obeying the Oja rule should be decorrelated using a symmetric decorrelating stage (Fig. 3).
Reference: [21] <author> Y. Freund and D. Haussler. </author> <title> Unsupervised learning of distributions on binary vectors using 2-layer networks. </title> <booktitle> In Advances In Neural Information Processing Systems 4, </booktitle> <pages> pages 912-919. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: In the negative phase, only a random subset of the units are left unclamped. Using this method, they showed that the network was able to learn a set of random patterns, and perform pattern completion when partially specified or noisy patterns were presented. Freund and Haussler <ref> [21] </ref> describe an efficient way to train SBMs unsupervised. The goal is to learn the "hidden causes" of a collection of patterns, with each hidden unit representing one hidden cause. <p> Thus, the algorithm has proven to be able to discriminate nonlinearly separable pattern classes by building in translation-invariance of feature detectors. The methods proposed by Freund and Haussler <ref> [21] </ref> and Neal [50] are both of theoretical interest, and could have potential applications in unsupervised higher-order feature discovery for classification. <p> But how can unsupervised learning be applied beyond these preprocessing stages, to extract higher order features and build more abstract representations? One approach is to build in more sophisticated prior models; we have already seen several examples of this approach (e.g. Freund and Haussler's <ref> [21] </ref> and Neal's [50] methods). Another approach is to restrict our search to particular kinds of structure.
Reference: [22] <author> K. Fukushima. Cognitron: </author> <title> A self-organizing multilayered neural network. </title> <journal> Biological Cybernetics, </journal> <volume> 20 </volume> <pages> 121-136, </pages> <year> 1975. </year>
Reference-contexts: Competitive learning procedures <ref> [76, 22, 37, 14, 68] </ref> are used primarily for clustering. The general idea underlying competitive learning is to induce a competition between units' responses, either by a winner-take-all activation function or with lateral interactions, so that only one unit in each competitive cluster tends to be active at a time.
Reference: [23] <author> K. Fukushima. </author> <title> Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. </title> <journal> Biological Cybernetics, </journal> <volume> 36 </volume> <pages> 193-202, </pages> <year> 1980. </year>
Reference-contexts: In summary, like the hard competitive learning model, Kohonen's algorithm can be viewed as a version of the mixture of Gaussians model with infinitesimally small variances, with the addition of a noise process distorting our estimates of the probabilities of each Gaussian capturing the data. Fukushima's Neocognitron <ref> [23] </ref> generalizes standard (hard) competitive learning to form a multi-resolution hierarchy of translation-invariant feature detectors. This network has produced impressive results on translation- and scale-invariant character recognition.
Reference: [24] <author> K. Fukushima. </author> <title> A hierarchical neural network model for associative memory. </title> <journal> Biological Cybernetics, </journal> <volume> 50 </volume> <pages> 105-113, </pages> <year> 1984. </year>
Reference-contexts: The Neocognitron has been applied to simple character recognition problems <ref> [24] </ref>. Because of the equality constraints implicit in the learning procedure, the model is able to learn classes which are somewhat scale- and shift-invariant. Thus, the algorithm has proven to be able to discriminate nonlinearly separable pattern classes by building in translation-invariance of feature detectors.
Reference: [25] <author> C. Galland. </author> <title> Learning in Deterministic Boltzmann Machine Networks. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <year> 1992. </year>
Reference-contexts: Unfortunately, the mean field approximation used in DBMs is not particularly useful in this case; a DBM cannot form an adequate representation of the unclamped distribution, as all 2 n states of the n units are represented by a single mean state <ref> [25] </ref>. Peterson and Hartman [62] suggest one way around this problem. In the negative phase, only a random subset of the units are left unclamped.
Reference: [26] <author> J. J. Gerbrands. </author> <title> On the relationships between SVD,KLT and PCA. </title> <journal> Pattern Recogntion, </journal> <volume> 14 </volume> <pages> 375-381, </pages> <year> 1981. </year>
Reference-contexts: It appears in the literature under various guises, including Factor Analysis [77], the discrete Karhunen-Loeve transform (KLT) <ref> [26] </ref>, and the Hotelling Transform [28]. This variety of names largely reflects its different applications. <p> The reason that PCA is useful for pre-processing is because of its information preserving properties. Given a number of outputs M , it is the linear transform which minimizes the mean squared reconstruction error of the input sequence x (t) from the output sequence y (t) <ref> [26] </ref>. Also, it is the linear transform which preserves the maximum amount of Shannon information at the output, under assumptions of uncorrelated equal-variance additive Gaussian noise on the input signal [44, 65].
Reference: [27] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> North Oxford Academic, Oxford, </publisher> <year> 1983. </year>
Reference-contexts: If only the first principal component is needed, this can be estimated by repeated multiplication of an initially random vector by the input covariance matrix Q x <ref> [27] </ref>.
Reference: [28] <author> R. C. Gonzalez and P. Wintz. </author> <title> Digital Image Processing. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1987. </year>
Reference-contexts: It appears in the literature under various guises, including Factor Analysis [77], the discrete Karhunen-Loeve transform (KLT) [26], and the Hotelling Transform <ref> [28] </ref>. This variety of names largely reflects its different applications.
Reference: [29] <author> G. E. Hinton and T. J. Sejnowski. </author> <title> Learning and relearning in Boltzmann machines. </title> <editor> In D. E. Rumelhart, J. L. McClelland, and the PDP research group, editors, </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition, </booktitle> <volume> volume I, </volume> <pages> pages 282-317. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Mozer shows that this scheme can be used to discover useful distributed binary features for image compression. A more general way to learn arbitrary probability distributions over data is with the stochastic Boltzmann Machine (SBM) <ref> [29] </ref>, or its computationally more efficient cousin the Deterministic 14 Boltzmann Machine (DBM) [61]. The DBM is considerably faster to train, as the mean field approximation eliminates the stochasticity in both the annealing process, and the sampling of the equilibrium correlation statistics.
Reference: [30] <author> K. Hornik and C.-M. Kuan. </author> <title> Convergence analysis of local feature extraction algorithms. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 229-240, </pages> <year> 1992. </year>
Reference-contexts: If the outputs of the network are allowed to settle so that (22) is satisfied, then we get y (t) = (I V (t)) W (t)x (t) (23) where I is the identity matrix, provided that all the eigenvalues of V (t) are less than unity <ref> [30] </ref>. In practice, it may be sufficient to use the first two terms of the expansion [42] (I V (t)) 1 I + V (t) + (24) in an artificial system.
Reference: [31] <author> N. Intrator. </author> <title> Feature extraction using an unsupervised neural network. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 98-107, </pages> <year> 1992. </year>
Reference-contexts: This learning rule led to the development of tight orientation tuning curves for units, using simple oriented line patterns as inputs. Intrator <ref> [31] </ref> has proposed a related objective function for maximizing selectivity, which causes a unit to discover projections of the data having bimodal or multi-modal distributions. <p> on speech data having dimensionality as large as 5500; compared to supervised BackProp, the algorithm " ... is able to find a richer, linguistically meaningful structure, containing burst locations and formant tracking of the three different stops that allowed a better generalization to other speakers and to voiced stops." (Intrator <ref> [31] </ref>, pp. 105). The multi-valued discrete version of Imax has been successfully applied in single-layer networks to toy problems such as extracting combinations of spatial frequency and phase in sinusoidal intensity patterns [6].
Reference: [32] <author> R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1), </volume> <year> 1991. </year>
Reference-contexts: An appealing aspect of the soft competitive learning algorithm is that it can be generalized to incorporate arbitrary prior information such as supervisory signals, as in the "competing experts" model of Jacobs et al. <ref> [32] </ref>. Jordan and Jacobs [33] show how to generalize the competing experts framework to discover a hierarchical decomposition of structure in the input.
Reference: [33] <author> M. I. Jordan and R. A. Jacobs. </author> <title> Hierarchies of adaptive experts. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 985-992. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: An appealing aspect of the soft competitive learning algorithm is that it can be generalized to incorporate arbitrary prior information such as supervisory signals, as in the "competing experts" model of Jacobs et al. [32]. Jordan and Jacobs <ref> [33] </ref> show how to generalize the competing experts framework to discover a hierarchical decomposition of structure in the input. <p> For example, Nowlan [52] showed that this method is superior to the traditional "hard competitive learning models" on two classification tasks, hand-written digit and vowel recognition. Jordan and Jacobs <ref> [33] </ref> applied the hierarchical version of the competing experts model in a set of competing auto-encoder networks that learn a hierarchical classification of leaf morphology data. Kohonen [38] has applied his algorithm to preprocessed speech data, and found that the clusters found by units usually correspond to phonemes.
Reference: [34] <author> C. Jutten and J. Herault. </author> <title> Blind separation of sources, part I: An adaptive algorithm based on enuromimetic architecture. </title> <booktitle> Signal Processing, </booktitle> <volume> 24 </volume> <pages> 1-10, </pages> <year> 1991. </year>
Reference-contexts: Jutten and Herault <ref> [34] </ref> developed a learning procedure, INCA, for extracting statistically independent components of the input vector, when the input vector x is modelled as an additive mixture of unknown independent signal components x (t) = Az (t) where the matrix A consists of unknown real scalars. <p> Accurate real-valued state information is difficult to maintain over time in recurrent networks unless a very large number of hidden units are used. Jutten and Herault <ref> [34] </ref> applied their independent components analysis algorithm to images of sloped handwriting. When random samples of (x; y) points were used as input, a two-neuron network was able to remove the dependence between the input coordinates, resulting in output images consisting of unsloped text. <p> By learning the appropriate nonlinear mapping, this method was able to outperform standard linear preprocessing using PCA. Note that an alternative algorithm for source separation by Jutten and Herault <ref> [34] </ref> was discussed in section 2.3. 5 Conclusions Unsupervised learning procedures have been applied to many real-world problems to reduce noise, compress data, and extract useful features for subsequent classification.
Reference: [35] <author> C. Jutten and J. Herault. </author> <title> Blind separation of sources, part II: Problems statement. </title> <booktitle> Signal Processing, </booktitle> <volume> 24 </volume> <pages> 11-20, </pages> <year> 1991. </year>
Reference-contexts: When random samples of (x; y) points were used as input, a two-neuron network was able to remove the dependence between the input coordinates, resulting in output images consisting of unsloped text. Further, in a companion paper, Jutten and Herault <ref> [35] </ref> demonstrated the success of the algorithm on a number of difficult synthetic nonlinear source separation problems. 3 Density Estimation Techniques Rather than trying to retain all of the information contained in the input, we could try to develop a more abstract representation by characterizing its underlying probability distribution.
Reference: [36] <author> J. Karhunen and J. Joutsensalo. </author> <title> Tracking of sinusoidal frequencies by neural network learning algorithms. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing ICASSP-91, </booktitle> <address> Toronto, Canada, </address> <year> 1991. </year> <month> 22 </month>
Reference-contexts: He also extended this to nonlinear units with a rectification nonlinearity, which he used to discover stereo disparity in random dot stereograms. Leen, Rudnick and Hammerstrom [43] used PCA networks for signal pre-processing and found improved classification performance. Karhunen and Joutsensalo <ref> [36] </ref> use the SGA principal sub-space algorithm for frequency estimation. They suggest that an undate factor of * 1= gives good initial convergence, with * decreasing with 1=t after the first few cycles. An alternative application of subspace methods is for more direct classification.
Reference: [37] <author> T. Kohonen. </author> <title> Clustering, taxonomy, and topological maps of patterns. </title> <editor> In M. Lang, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Pattern Recognition, </booktitle> <address> Silver Spring, MD, 1982. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Competitive learning procedures <ref> [76, 22, 37, 14, 68] </ref> are used primarily for clustering. The general idea underlying competitive learning is to induce a competition between units' responses, either by a winner-take-all activation function or with lateral interactions, so that only one unit in each competitive cluster tends to be active at a time. <p> One useful variation on competitive learning is to define a neighborhood relation among the units, for example, by arranging them on a two-dimensional lattice; each unit then learns in proportion to its distance from the winning unit <ref> [76, 37] </ref>. This forces the competing units to form a topographic mapping in which nearby points in the input space are mapped to nearby points in the neighborhood space. <p> This forces the competing units to form a topographic mapping in which nearby points in the input space are mapped to nearby points in the neighborhood space. In Kohonen's model of unsupervised topological map formation <ref> [37, 38] </ref> a set N i (t) is defined for each unit i, determining those units within its neighborhood. The "winning unit" c is the one for which the Euclidean distance between its weight vector and the current input vector is minimal.
Reference: [38] <author> T. Kohonen. </author> <title> The `neural' phonetic typewriter. </title> <journal> IEEE Computer, </journal> <volume> 21 </volume> <pages> 11-22, </pages> <year> 1988. </year>
Reference-contexts: This forces the competing units to form a topographic mapping in which nearby points in the input space are mapped to nearby points in the neighborhood space. In Kohonen's model of unsupervised topological map formation <ref> [37, 38] </ref> a set N i (t) is defined for each unit i, determining those units within its neighborhood. The "winning unit" c is the one for which the Euclidean distance between its weight vector and the current input vector is minimal. <p> The 13 neighborhood set N c can be replaced by a continuous function N (i; j) of the distance between units i and j in the lattice; this leads to smoother learning. For neighborhoods of size 1, like standard competitive learning, Kohonen's algorithm is equivalent to k-means clustering <ref> [38] </ref>. For larger neighborhoods, the algorithm is a generalization of k-means which adapts each weight toward the centre of its own cluster of patterns and its neighbors' clusters, resulting in an ordered mapping that tends to preserve the topological structure of the input distribution. <p> Jordan and Jacobs [33] applied the hierarchical version of the competing experts model in a set of competing auto-encoder networks that learn a hierarchical classification of leaf morphology data. Kohonen <ref> [38] </ref> has applied his algorithm to preprocessed speech data, and found that the clusters found by units usually correspond to phonemes.
Reference: [39] <author> T. Kohonen and E. Oja. </author> <title> Fast adaptive formation of orthogonalizing filters and associative memory in recurrent networks of neuron-like elements. </title> <journal> Biological Cybernetics, </journal> <volume> 21 </volume> <pages> 85-95, </pages> <year> 1976. </year>
Reference-contexts: This is an efficient way of transmitting information about the current input. Several algorithms for learning invariant features of the input have been proposed. Kohonen and Oja <ref> [39] </ref> proposed a learning algorithm for a single unit which acts as a "novelty detector", by responding best to patterns which are orthogonal to the principal subspace of the input distribution.
Reference: [40] <author> S. Y. Kung and K. I. Diamantaras. </author> <title> A neural network learning algorithm for adaptive principal component extraction (APEX). </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing ICASSP-90, pages II: </booktitle> <pages> 861-864, </pages> <year> 1990. </year>
Reference-contexts: The third output y 3 is decorrelated from the previous two, and so on until all M desired principal components have been extracted in order. Kung and Diamantaras <ref> [40] </ref> suggested an Adaptive Principal Component Extractor (APEX) which is related to the Rubner and Tavan approach. Foldiak [20] suggested that units obeying the Oja rule should be decorrelated using a symmetric decorrelating stage (Fig. 3).
Reference: [41] <author> A. S. Lapedes and R. M. Farber. </author> <title> Nonlinear signal processing using neural networks: Prediction and system modelling. </title> <type> Technical Report LA-UR-87-2662, </type> <institution> Los Alamos National Laboratory, </institution> <year> 1987. </year>
Reference-contexts: Mozer [49] has applied recurrent networks with multiple time decay constants to the problem of extracting musical structure at multiple time scales by predicting notes in a musical score. The tapped-delay-line method has been used successfully on real-valued, noisy signal prediction problems such as chaotic time series prediction <ref> [41, 78] </ref>. The key to selecting the appropriate architecture for a temporal problem is to have an appropriate characterization of the problem in advance. For example, if the problem requires explicit knowledge of previous real-valued inputs within a short time window, the time-delay architecture is appropriate.
Reference: [42] <author> T. K. Leen. </author> <title> Dynamics of learning in linear feature-discovery networks. </title> <journal> Network, </journal> <volume> 2 </volume> <pages> 85-105, </pages> <year> 1991. </year>
Reference-contexts: In practice, it may be sufficient to use the first two terms of the expansion <ref> [42] </ref> (I V (t)) 1 I + V (t) + (24) in an artificial system. <p> P P P Pq - @ @ @I y 7 x - 1 @ @ @R P P P P Pq - @ @ @I I - - P P P P Pq @ @ 1 - H Hj * J J J J J J] H HY y Leen <ref> [42] </ref> analyzed the stability of Fodiak's network, and suggested that the algorithm (20) for the lateral connections be modified to add a term proportional to the lateral connection weight itself v jk (t + 1) = v jk (t) + * w fiv jk * v y j (t)y k (t) <p> With the latter modification, the weight vectors converge to the principal components themselves, rather than just the principal subspace, provided that * v &gt; 2* w <ref> [42] </ref>.
Reference: [43] <author> T. K. Leen, M. Rudnick, and D. Hammerstrom. </author> <title> Hebbian feature discovery improves classifier efficiency. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, IJCNN-89, </booktitle> <pages> pages I: 51-56, </pages> <address> Washington, DC, </address> <year> 1989. </year>
Reference-contexts: Sanger, for example, used his GHA network for image compression [69]. He also extended this to nonlinear units with a rectification nonlinearity, which he used to discover stereo disparity in random dot stereograms. Leen, Rudnick and Hammerstrom <ref> [43] </ref> used PCA networks for signal pre-processing and found improved classification performance. Karhunen and Joutsensalo [36] use the SGA principal sub-space algorithm for frequency estimation. They suggest that an undate factor of * 1= gives good initial convergence, with * decreasing with 1=t after the first few cycles.
Reference: [44] <author> R. Linsker. </author> <title> Self-organization in a perceptual network. </title> <journal> IEEE Computer, </journal> <volume> 21(3) </volume> <pages> 105-117, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Also, it is the linear transform which preserves the maximum amount of Shannon information at the output, under assumptions of uncorrelated equal-variance additive Gaussian noise on the input signal <ref> [44, 65] </ref>. <p> Other algorithms have been suggested which attempt to find the principal subspace, i.e. the space spanned by the principal components, rather than the principal components themselves. This is still sufficient to minimize mean square reconstruction error and optimally preserve information <ref> [44, 65] </ref>: any linear rotation of the output components will not affect these properties. Therefore unless the principal components themselves are needed for a particular application, it may be sufficient to have some other set of vectors which spans the principal subspace. <p> In the unconstrained noise-free case, all the information can be preserved simply by copying the input. Linsker <ref> [44] </ref> proposes maximizing the information rate in the presence of processing noise at either the input or output layer (the "Infomax principle").
Reference: [45] <author> R. Linsker. </author> <title> Deriving receptive fields using an optimal encoding criterion. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 953-960. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: This results in a tradeoff between maximizing the variances of the outputs, and decorrelating them, depending on the noise level. The above analyses apply to linear networks. Adding the usual form of sigmoid nonlinearity makes an information-theoretic analysis much more difficult. Linsker <ref> [45] </ref> shows that adding a "weakly nonlinear" (cubic) input-output relation to the Infomax principle, for certain translation-invariant input distributions, results in a set of units tuned to different spatial frequencies and spatial locations, much like a wavelet representation.
Reference: [46] <author> S.P. Luttrell. </author> <title> Hierarchical vector quantisation. </title> <journal> In Proceedings of the Inst. of Elec. Eng., </journal> <volume> volume 136, </volume> <pages> pages 405-413, </pages> <year> 1989. </year>
Reference-contexts: For larger neighborhoods, the algorithm is a generalization of k-means which adapts each weight toward the centre of its own cluster of patterns and its neighbors' clusters, resulting in an ordered mapping that tends to preserve the topological structure of the input distribution. Luttrell <ref> [46] </ref> has shown how Kohonen's algorithm can be viewed as a variant of minimum distortion vector quantization (MDVQ). <p> Then each unit will contribute to the distortion error, as a function of its distance to the winner: D = i This noisy version of MDVQ is equivalent to Kohonen's algorithm, where is the neighborhood function; within the same framework, Luttrell <ref> [46] </ref> goes on to generalize the Kohonen algorithm to perform hierarchical MDVQ.
Reference: [47] <author> M. C. Mozer. </author> <title> Discovering discrete distributed representations with iterative competitive learning. </title> <booktitle> In Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 627-634. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: A multiple causes model is more appropriate when the most compact data description consists of several independent parameters (e.g. color, shape, and size of an object in a visual scene). Mozer <ref> [47] </ref> has proposed a distributed version of competitive learning for binary units that discovers multiple classifications of the data. Each layer of competing units receives as input the reconstruction error from the previous layer, and performs clustering on that error.
Reference: [48] <author> M. C. Mozer. </author> <title> Induction of multicale temporal structure. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 275-282. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Unlike delay-line neural networks, recurrent networks are sensitive to information over a potentially infinite time span; however, in practice they have enormous difficulty learning to maintain state information over long time intervals. Schmidhuber [72] and Mozer <ref> [48] </ref> have proposed two different ways of addressing this problem in recurrent networks. 2.3 Independent Components Analysis When the input is composed of a combination of independent signals, linear methods such as principal components analysis, in general, are incapable of separating the independent sources.
Reference: [49] <author> M. C. Mozer. </author> <title> Neural net architectures for temporal sequence procesing. </title> <editor> In A. Weigend and N. Gershenfeld, editors, </editor> <title> Predicting the future and undertanding the past. </title> <address> Redwood City, CA: </address> <publisher> Addison-Wesley Publishing, </publisher> <year> 1993. </year>
Reference-contexts: techniques for providing temporally varying information to a network are to either use tapped delay lines making inputs from several time steps available simultaneously, thereby spatializing the time domain or using recurrent feedback connections; a number of variations on architectures and training methods for these networks are reviewed by Mozer <ref> [49] </ref>. Unlike delay-line neural networks, recurrent networks are sensitive to information over a potentially infinite time span; however, in practice they have enormous difficulty learning to maintain state information over long time intervals. <p> Recurrent networks have been used predominantly for symbolic tasks in which the goal is to represent discrete 11 hidden variables by extracting temporal structure. For example, Elman [17] has used simple recurrent networks to infer grammatical structure from sentences. Mozer <ref> [49] </ref> has applied recurrent networks with multiple time decay constants to the problem of extracting musical structure at multiple time scales by predicting notes in a musical score. The tapped-delay-line method has been used successfully on real-valued, noisy signal prediction problems such as chaotic time series prediction [41, 78].
Reference: [50] <author> R. M. Neal. </author> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56 </volume> <pages> 71-113, </pages> <year> 1992. </year>
Reference-contexts: The network thereby learns to adopt hidden unit states which are good generators of the input pattern set. However, as the model has only a single hidden layer, it could take exponentially many hidden units to model an arbitrary probability distribution (Radford Neal, personal communication). Neal <ref> [50] </ref> presents a more general way of modeling the probability distribution of a pattern set in a multilayer stochastic "connectionist belief network", which falls within the class of Pearl's belief networks [59]. <p> Thus, the algorithm has proven to be able to discriminate nonlinearly separable pattern classes by building in translation-invariance of feature detectors. The methods proposed by Freund and Haussler [21] and Neal <ref> [50] </ref> are both of theoretical interest, and could have potential applications in unsupervised higher-order feature discovery for classification. <p> But how can unsupervised learning be applied beyond these preprocessing stages, to extract higher order features and build more abstract representations? One approach is to build in more sophisticated prior models; we have already seen several examples of this approach (e.g. Freund and Haussler's [21] and Neal's <ref> [50] </ref> methods). Another approach is to restrict our search to particular kinds of structure. If we can make constraining assumptions about the kind of structure we are looking for, we can build these constraints into the network's architecture and/or objective function and thereby develop more efficient, highly specialized learning procedures.
Reference: [51] <author> R. M. Neal and G. E. Hinton. </author> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <note> Submitted for publication. </note>
Reference-contexts: This is an online version of the EM algorithm for Gaussian densities with equal priors, and adaptive means and variances. Neal and Hinton <ref> [51] </ref> have shown that incremental variants of EM perform gradient descent in a global energy function; they treat Nowlan's algorithm as a special case of these, and show that it performs approximate gradient descent.
Reference: [52] <author> S. J. Nowlan. </author> <title> Maximum likelihood competitive learning. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pages 574-582, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus, the same learning rule applies, except that the proportional weighting is replaced by an all-or-none decision. So hard competitive learning can be viewed as a mixture of Gaussians estimation procedure using infinitesimally small variances. Nowlan <ref> [52, 53] </ref> proposed a "soft competitive learning" model for neural networks. <p> For example, Nowlan <ref> [52] </ref> showed that this method is superior to the traditional "hard competitive learning models" on two classification tasks, hand-written digit and vowel recognition.
Reference: [53] <author> S. J. Nowlan. </author> <title> Soft Competitive Adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh PA, </address> <year> 1991. </year> <note> Also published as CMU Technical Report CMU-CS-91-126. </note>
Reference-contexts: Each unit is performing gradient descent in the squared distance between its weight vector and the patterns nearest to its weight vector (subject to a unit length constraint), which in the batch version is equivalent to the standard k-means clustering algorithm. Nowlan <ref> [53] </ref> has pointed out that this standard version of competitive learning is closely related to fitting a mixture of Gaussians model, with equal priors i and equal variances 2 i . <p> Thus, the same learning rule applies, except that the proportional weighting is replaced by an all-or-none decision. So hard competitive learning can be viewed as a mixture of Gaussians estimation procedure using infinitesimally small variances. Nowlan <ref> [52, 53] </ref> proposed a "soft competitive learning" model for neural networks.
Reference: [54] <author> E. Oja. </author> <title> A simplified neuron model as a principal component analyser. </title> <journal> Journal of Mathematical Biology, </journal> <volume> 15 </volume> <pages> 267-273, </pages> <year> 1982. </year>
Reference-contexts: In this case, the algorithm will not fully converge to its stable point, but will make small random movements around it, depending on the size of *. Oja <ref> [54] </ref> modified this algorithm so that w is renormalized after each step, leading to the two-step algorithm ~ w (t + 1) = w (t) + *x (t)y (t) (9) which ensures that w (t) has unit length for all time t &gt; 0. <p> The weight vector w (t) in this algorithm asymptotically converges to the principal component e 1 (or e 1 ). If the update factor * is sufficiently small, Oja <ref> [54] </ref> also showed that the two-step algorithm above can be approximated by the single-step algorithm w (t + 1) = w (t) + * x (t)y (t) w (t)y (t) 2 which is often referred to as "Oja's Rule".
Reference: [55] <author> E. Oja. </author> <title> Neural networks, principal components, and subspaces. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(1) </volume> <pages> 61-68, </pages> <year> 1989. </year> <month> 23 </month>
Reference-contexts: If only the principal subspace is needed, this can be also achieved with the symmetrical version of the Williams [79] Symmetric Error Correction algorithm, which is equivalent to Oja's Subspace Network <ref> [55] </ref>. <p> Karhunen and Joutsensalo [36] use the SGA principal sub-space algorithm for frequency estimation. They suggest that an undate factor of * 1= gives good initial convergence, with * decreasing with 1=t after the first few cycles. An alternative application of subspace methods is for more direct classification. Oja <ref> [55] </ref> allows a network to learn one subspace per class, with modifications to the learning algorithms to make sure the subspaces differentiate between classes. When classifying, the class with the closest subspace to the input vector (calculated from its projection into each subspace) is the `winner'.
Reference: [56] <author> E. Oja. </author> <title> Principal components, minor components, and linear neural networks. </title> <journal> Neural Net--works, </journal> <volume> 5 </volume> <pages> 927-935, </pages> <year> 1992. </year>
Reference-contexts: the algorithm w j (t + 1) = w j (t) + *y j (t)(x (t) ^ x j (t)) (14) where ^ x j (t) is given by ^ x j (t) = k=1 An alternative algorithm, also derived from GSO, is Oja and Karhunen's Stochastic Gradient Algorithm (SGA) <ref> [57, 56] </ref>.
Reference: [57] <author> E. Oja and J. Karhunen. </author> <title> On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 106 </volume> <pages> 69-84, </pages> <year> 1985. </year>
Reference-contexts: the algorithm w j (t + 1) = w j (t) + *y j (t)(x (t) ^ x j (t)) (14) where ^ x j (t) is given by ^ x j (t) = k=1 An alternative algorithm, also derived from GSO, is Oja and Karhunen's Stochastic Gradient Algorithm (SGA) <ref> [57, 56] </ref>.
Reference: [58] <author> E. Oja, H. Ogawa, and J. Wangviwattana. </author> <title> PCA in fully parallel neural networks. </title> <editor> In I. Alek-sander and J. Taylor, editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <volume> 2, </volume> <pages> pages 199-202, </pages> <address> Amsterdam, 1992. </address> <publisher> North-Holland. </publisher>
Reference-contexts: Oja, Ogawa and Wangviwattana <ref> [58] </ref> modified this algorithm to add an additional symmetry-breaking factor to give w j (t + 1) = w j (t) + *y j (t)(x (t) j ^ x (t)) (18) with 0 &lt; 1 &lt; 2 &lt; &lt; M and ^ x (t) given by (17).
Reference: [59] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Neal [50] presents a more general way of modeling the probability distribution of a pattern set in a multilayer stochastic "connectionist belief network", which falls within the class of Pearl's belief networks <ref> [59] </ref>.
Reference: [60] <author> B. A. Pearlmutter and G. E. Hinton. G-maximization: </author> <title> An unsupervised learning procedure for discovering regularities. </title> <editor> In J. S. Denker, editor, </editor> <booktitle> Neural Networks for Computing: American Institute of Physics Conference Proceedings 151, </booktitle> <pages> pages 333-338, </pages> <year> 1986. </year>
Reference-contexts: Intrator discusses the relation of this method to exploratory projection pursuit. The method tends to discover projections having a non-Gaussian, or skewed distribution which may be useful as features for certain classification problems. 16 4.2 Statistical dependencies between the inputs The GMAX algorithm <ref> [60] </ref> is based on the goal of redundancy-detection.
Reference: [61] <author> C. Peterson and J. R. Anderson. </author> <title> A mean field theory learning algorithm for neural networks. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 995-1019, </pages> <year> 1987. </year>
Reference-contexts: Mozer shows that this scheme can be used to discover useful distributed binary features for image compression. A more general way to learn arbitrary probability distributions over data is with the stochastic Boltzmann Machine (SBM) [29], or its computationally more efficient cousin the Deterministic 14 Boltzmann Machine (DBM) <ref> [61] </ref>. The DBM is considerably faster to train, as the mean field approximation eliminates the stochasticity in both the annealing process, and the sampling of the equilibrium correlation statistics. Theoretically one could train an unsupervised DBM exactly as for the SBM.
Reference: [62] <author> C. Peterson and E. Hartman. </author> <title> Explorations of the mean field theory learning algorithm. Neural Networks, </title> <address> 2:475, </address> <year> 1989. </year>
Reference-contexts: Unfortunately, the mean field approximation used in DBMs is not particularly useful in this case; a DBM cannot form an adequate representation of the unclamped distribution, as all 2 n states of the n units are represented by a single mean state [25]. Peterson and Hartman <ref> [62] </ref> suggest one way around this problem. In the negative phase, only a random subset of the units are left unclamped. Using this method, they showed that the network was able to learn a set of random patterns, and perform pattern completion when partially specified or noisy patterns were presented.
Reference: [63] <author> M. D. Plumbley. </author> <title> Efficient information transfer and anti-Hebbian neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 6(6) </volume> <pages> 823-833, </pages> <year> 1993. </year>
Reference-contexts: In the general case, this problem is intractable. Atick and Redlich [3] have proposed a cost function for Barlow's principle for linear systems which minimizes the power (redundancy) in the outputs subject to a minimal information loss constraint. This is closely related to Plumbley's <ref> [63] </ref> objective function, which minimizes the information loss subject to a fixed power constraint, and for which a simple Hebbian learning scheme is derived. Schmidhuber [71] has proposed several ways of approximating Barlow's minimum redundancy principle in the general case, for nonlinear networks.
Reference: [64] <author> M. D. Plumbley. </author> <title> A Hebbian/anti-Hebbian network which optimizes information capacity by orthonormalizing the principal subspace. </title> <booktitle> In Proceedings of the IEE Artificial Neural Networks Conference, ANN-93, </booktitle> <pages> pages 86-90, </pages> <address> Brighton, UK, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: However, this is not necessary from either the information theory or minimum reconstruction error viewpoints, so alternatives are possible. For example, Plumbley <ref> [64] </ref> suggested two networks which can extract the principal subspace, but produce uncorrelated equal variance outputs, i.e. Q y ! fiI. Having outputs of equal variance is desirable in systems with limited channel capacity, and may facilitate subsequent classification by normalizing the scale of the outputs. <p> (y j (t)z k (t) fiv k (t)) (33) for the lateral connections, again the algorithm converges when the outputs are uncorrelated, equal variance, and span the principal subspace (provided that * v * w , and all the selected principal components of the input have variance greater than ff <ref> [64] </ref>). 2.1.6 Auto-Encoders Instead of using one of the many principal subspace algorithms mentioned so far, the Error Back Propagation (`BackProp') algorithm can also be used to find the principal subspace [4].
Reference: [65] <author> M. D. Plumbley and F. Fallside. </author> <title> An information-theoretic approach to unsupervised connectionist models. </title> <editor> In David Touretzky, Geoffrey Hinton, and Terrence Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 239-245. </pages> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: Also, it is the linear transform which preserves the maximum amount of Shannon information at the output, under assumptions of uncorrelated equal-variance additive Gaussian noise on the input signal <ref> [44, 65] </ref>. <p> Other algorithms have been suggested which attempt to find the principal subspace, i.e. the space spanned by the principal components, rather than the principal components themselves. This is still sufficient to minimize mean square reconstruction error and optimally preserve information <ref> [44, 65] </ref>: any linear rotation of the output components will not affect these properties. Therefore unless the principal components themselves are needed for a particular application, it may be sufficient to have some other set of vectors which spans the principal subspace.
Reference: [66] <author> J. Rubner and P. Tavan. </author> <title> A self-organizing network for principal component analysis. </title> <journal> Euro-physics Letters, </journal> <volume> 10 </volume> <pages> 693-698, </pages> <year> 1989. </year>
Reference-contexts: This forces an output to learn to respond to different principal component from other outputs. Rubner and Tavan <ref> [66] </ref>, for example, suggest such a hierarchical decorrelating model with adaptive lateral inhibition from lower-numbered output units to higher-numbered output units (Fig. 2).
Reference: [67] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Consider the N M N linear network of Fig. 5 with y j (t) = i=1 z k (t) = j=1 If BackProp <ref> [67] </ref> is used to minimize the mean squared error E = jx (t) z (t)j 2 ff in this system, the units in the hidden layer y will become some set of vectors which spans the principal subspace.
Reference: [68] <author> D. E. Rumelhart and D. Zipser. </author> <title> Competitive learning. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 75-112, </pages> <year> 1985. </year>
Reference-contexts: Competitive learning procedures <ref> [76, 22, 37, 14, 68] </ref> are used primarily for clustering. The general idea underlying competitive learning is to induce a competition between units' responses, either by a winner-take-all activation function or with lateral interactions, so that only one unit in each competitive cluster tends to be active at a time. <p> Typically only the winning unit learns on each case, by moving its weight vector closer to the current input pattern. For example, Rumelhart and Zipser's <ref> [68] </ref> version of competitive learning sets the activity of the winning unit (the one with the greatest total input, x i ) to one, and the rest to zero, and uses the following learning rule: w ji (t + 1) = w ji (t) + &lt; 0 if unit j loses
Reference: [69] <author> T. D. Sanger. </author> <title> Optimal unsupervised learning in a single-layer feedforward neural network. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 459-473, </pages> <year> 1989. </year>
Reference-contexts: For example, the Sanger <ref> [69] </ref> Generalized Hebbian Algorithm (GHA) incorporates a form of Gram-Schmidt orthogonalization (GSO) to give the algorithm w j (t + 1) = w j (t) + *y j (t)(x (t) ^ x j (t)) (14) where ^ x j (t) is given by ^ x j (t) = k=1 An alternative <p> Sanger, for example, used his GHA network for image compression <ref> [69] </ref>. He also extended this to nonlinear units with a rectification nonlinearity, which he used to discover stereo disparity in random dot stereograms. Leen, Rudnick and Hammerstrom [43] used PCA networks for signal pre-processing and found improved classification performance.
Reference: [70] <author> E. Saund. </author> <title> Dimensionality-reduction using connectionist networks. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11(3) </volume> <pages> 304-314, </pages> <year> 1989. </year>
Reference-contexts: However, opinions are divided as to whether this is achieved in practice, and some have argued that a method based 9 on SVD is more reliable, and not susceptible to local minima [12]. A further problem is that the nonlinear hidden layer codes are usually difficult to interpret. Saund <ref> [70] </ref> and Zemel and Hinton [80] have explored ways of constraining the hidden unit codes adopted by autoencoder networks to obtain more biologically plausible and/or interpretable representations such as value codes or topographic maps. 2.2 Temporal prediction The techniques described so far only deal with static patterns.
Reference: [71] <author> J. Schmidhuber. </author> <title> Learning factorial codes by predictability minimization. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 863-879, </pages> <year> 1992. </year>
Reference-contexts: This is closely related to Plumbley's [63] objective function, which minimizes the information loss subject to a fixed power constraint, and for which a simple Hebbian learning scheme is derived. Schmidhuber <ref> [71] </ref> has proposed several ways of approximating Barlow's minimum redundancy principle in the general case, for nonlinear networks. Without the Gaussian assumptions, this implies a much stronger result of statistically independent, rather than just decorrelated, outputs.
Reference: [72] <author> J. Schmidhuber. </author> <title> Learning unambiguous reduced sequence decriptions. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 291-298. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Unlike delay-line neural networks, recurrent networks are sensitive to information over a potentially infinite time span; however, in practice they have enormous difficulty learning to maintain state information over long time intervals. Schmidhuber <ref> [72] </ref> and Mozer [48] have proposed two different ways of addressing this problem in recurrent networks. 2.3 Independent Components Analysis When the input is composed of a combination of independent signals, linear methods such as principal components analysis, in general, are incapable of separating the independent sources.
Reference: [73] <author> N. N. Schraudolph and T. J. Sejnowski. </author> <title> Competitive anti-hebbian learning of invariants. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 1017-1024. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Schraudolph and Sejnowski <ref> [73] </ref> propose a closely related learning scheme, combining a variance-minimizing anti-Hebbian term and a term that prevents the weights all converging to zero. They show that a set of competing units can thereby discover population codes for stereo disparity in random dot stereograms.
Reference: [74] <author> C. E. Shannon. </author> <title> A mathematical theory of communication. </title> <journal> Bell System Technical Journal, </journal> <volume> 27 379-423,623-656, </volume> <year> 1948. </year>
Reference-contexts: This measure is due to Shannon <ref> [74] </ref> and tells us the amount of information in x less the amount 10 remaining in x when y is known, or the uncertainty in x which is accounted for by y (and vice versa).
Reference: [75] <author> A. Ukrainec and S. Haykin. </author> <title> Application of unsupervised neural networks to the enhancement of polarization targets in dual-polarized radar images. </title> <booktitle> In IEEE Canadian Confrernce on Elecrtical and Computer Engineering, </booktitle> <year> 1991. </year>
Reference-contexts: One way to do this is to try to extract features having minimal mutual information; however, we must rule out trivial solutions in which the features convey no information at all. Ukrainec and Haykin <ref> [75] </ref> have proposed minimizing equation (60), adding a penalty term fi Q y fi 1 to prevent degenerate solutions, where y = [y a ; y b ].
Reference: [76] <author> C. von der Malsburg. </author> <title> Self-organization of orientation sensitive cells in striate cortex. </title> <journal> Kyber-netik, </journal> <volume> 14 </volume> <pages> 85-100, </pages> <year> 1973. </year>
Reference-contexts: Competitive learning procedures <ref> [76, 22, 37, 14, 68] </ref> are used primarily for clustering. The general idea underlying competitive learning is to induce a competition between units' responses, either by a winner-take-all activation function or with lateral interactions, so that only one unit in each competitive cluster tends to be active at a time. <p> One useful variation on competitive learning is to define a neighborhood relation among the units, for example, by arranging them on a two-dimensional lattice; each unit then learns in proportion to its distance from the winning unit <ref> [76, 37] </ref>. This forces the competing units to form a topographic mapping in which nearby points in the input space are mapped to nearby points in the neighborhood space.
Reference: [77] <author> S. Watanabe. </author> <title> Pattern Recognition: Human and Mechanical. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: It appears in the literature under various guises, including Factor Analysis <ref> [77] </ref>, the discrete Karhunen-Loeve transform (KLT) [26], and the Hotelling Transform [28]. This variety of names largely reflects its different applications.
Reference: [78] <author> A. S. Weigend, B. A. Huberman, and D. E. Rumelhart. </author> <title> Predicting the future: A connectionist approach. </title> <type> Technical Report SSL-90-20, </type> <institution> Palo Alto Research Center, </institution> <year> 1990. </year> <note> Submitted to the International Journal of Neural Systems. </note>
Reference-contexts: Mozer [49] has applied recurrent networks with multiple time decay constants to the problem of extracting musical structure at multiple time scales by predicting notes in a musical score. The tapped-delay-line method has been used successfully on real-valued, noisy signal prediction problems such as chaotic time series prediction <ref> [41, 78] </ref>. The key to selecting the appropriate architecture for a temporal problem is to have an appropriate characterization of the problem in advance. For example, if the problem requires explicit knowledge of previous real-valued inputs within a short time window, the time-delay architecture is appropriate.
Reference: [79] <author> R. J. Williams. </author> <title> Feature discovery through error-correction learning. </title> <type> ICS Report 8501, </type> <institution> Institute of Cognitive Science, University of California, </institution> <address> San Diego, </address> <year> 1985. </year>
Reference-contexts: Therefore unless the principal components themselves are needed for a particular application, it may be sufficient to have some other set of vectors which spans the principal subspace. If only the principal subspace is needed, this can be also achieved with the symmetrical version of the Williams <ref> [79] </ref> Symmetric Error Correction algorithm, which is equivalent to Oja's Subspace Network [55].
Reference: [80] <author> R. Zemel and G. E. Hinton. </author> <title> Developing topographic representations by minimizing description length. </title> <booktitle> In Advances in Neural Information Processing Systems 6 (to appear). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: A further problem is that the nonlinear hidden layer codes are usually difficult to interpret. Saund [70] and Zemel and Hinton <ref> [80] </ref> have explored ways of constraining the hidden unit codes adopted by autoencoder networks to obtain more biologically plausible and/or interpretable representations such as value codes or topographic maps. 2.2 Temporal prediction The techniques described so far only deal with static patterns.
Reference: [81] <author> R. S. Zemel and G. E. Hinton. </author> <title> Discovering viewpoint-invariant relationships that characterize objects. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances In Neural Information Processing Systems 3, </booktitle> <pages> pages 299-305. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1991. </year> <month> 25 </month>
Reference-contexts: Zemel and Hinton <ref> [81] </ref> have explored applications of this method to visual object recognition problems. 4.5 Applications Intrator has demonstrated his nonlinear multi-unit version of the BCM rule on speech data having dimensionality as large as 5500; compared to supervised BackProp, the algorithm " ... is able to find a richer, linguistically meaningful structure, <p> Finally, it has been applied to temporally varying patterns to classify temporally coherent objects over time [7]. Zemel and Hinton <ref> [81] </ref> have applied the multi-dimensional version of Imax to the problem of learning to represent the viewing parameters of simple synthetic two-dimensional objects.
References-found: 81

