URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr92/tr92-008.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr92-abstracts.html
Root-URL: http://www.cis.ufl.edu
Email: ted@cis.ufl.edu, davis@cis.ufl.edu  
Title: Space Efficient Parallel Buddy Memory Management  
Author: Theodore Johnson Tim Davis 
Keyword: Memory management, Concurrent data structure, Buddy system, Parallel algorithm.  
Date: April 3, 1992  
Address: TR #92-008  Florida  
Affiliation: University of Florida CIS  Dept. of Computer and Information Science, University of  
Abstract: Shared memory multiprocessor systems need efficient dynamic storage allocators, both for system purposes and to support parallel programs. Memory managers are often based on the buddy system, which provides fast allocation and release. Previous parallel buddy memory managers made no attempt to coordinate the allocation, splitting and release of blocks, and as a result needlessly fragment memory. We a present fast, and simple parallel buddy memory manager that is also as space efficient as a serial buddy memory manager. We test our algorithms using memory allocation/deallocation traces collected from a parallel sparse matrix algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Bigler, S. Allan, and R. Oldehoeft. </author> <title> Parallel dynamic storage allocation. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 272-275. </pages> <publisher> IEEE, </publisher> <year> 1985. </year>
Reference-contexts: In a free list algorithm [12], the free blocks are linked together in a list, ordered by starting address. Several parallel free list algorithms have been proposed <ref> [16, 1, 7, 8] </ref>. Free list algorithms are much slower than buddy algorithms, which are the subject of this paper. Another concurrent memory manager [11] has been proposed, based on the fast fits memory manager [14, 15].
Reference: [2] <author> T. A. Davis and P. C. Yew. </author> <title> A nondeterministic parallel algorithm for general unsymmetric sparse LU factorization. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11(3) </volume> <pages> 383-402, </pages> <year> 1990. </year>
Reference-contexts: A simple concurrent memory manager might consist of a serial memory manager placed in a critical section. While this algorithm would be sufficient for a multiprogrammed uniprocessor, it can create a serial bottleneck in a parallel processor. Example applications of parallel memory managers are parallel sparse matrix factorization algorithms <ref> [2, 4] </ref>, and buffers for message passing in clustered parallel computers [3]. 1 Most heap memory management algorithms use one of two main methods: free lists and buddy systems. In a free list algorithm [12], the free blocks are linked together in a list, ordered by starting address. <p> Parallel sparse matrix factorization algorithms, such as the D2 algorithm, place an even higher demand on this resource, as multiple processes compete for access to the memory manager <ref> [2] </ref>. We use the D2 algorithm as a test case for the parallel buddy memory manager. Its key feature is a non-deterministic parallel pivot search that builds a set of independent pivots, of size m, say, which when permuted to the diagonal form a diagonal m-by-m submatrix.
Reference: [3] <author> F.J. Roeber, </author> <title> Raytheon Submarine Signals Division. </title> <type> Personal communication, </type> <year> 1991. </year>
Reference-contexts: While this algorithm would be sufficient for a multiprogrammed uniprocessor, it can create a serial bottleneck in a parallel processor. Example applications of parallel memory managers are parallel sparse matrix factorization algorithms [2, 4], and buffers for message passing in clustered parallel computers <ref> [3] </ref>. 1 Most heap memory management algorithms use one of two main methods: free lists and buddy systems. In a free list algorithm [12], the free blocks are linked together in a list, ordered by starting address. Several parallel free list algorithms have been proposed [16, 1, 7, 8]. <p> The algorithm that we present is notable for being fast and highly parallel, and therefore is well suited for time-critical systems applications. This algorithm will be used to allocate buffers for message passing in a system in which shared memory processor clusters are connected by an ethernet <ref> [3] </ref>. To continue this work, we plan on parallelizing buddy algorithms that have better memory efficiency, and comparing all available parallel memory managers in a performance study. 6 Acknowledgements We'd like to thank Fred Roeber for his helpful comments.
Reference: [4] <author> I. S. Duff. </author> <title> Multiprocessing a sparse matrix code on the Alliant FX/8. </title> <journal> J. Comp. Appl. Math., </journal> <volume> 27 </volume> <pages> 229-239, </pages> <year> 1989. </year>
Reference-contexts: A simple concurrent memory manager might consist of a serial memory manager placed in a critical section. While this algorithm would be sufficient for a multiprogrammed uniprocessor, it can create a serial bottleneck in a parallel processor. Example applications of parallel memory managers are parallel sparse matrix factorization algorithms <ref> [2, 4] </ref>, and buffers for message passing in clustered parallel computers [3]. 1 Most heap memory management algorithms use one of two main methods: free lists and buddy systems. In a free list algorithm [12], the free blocks are linked together in a list, ordered by starting address.
Reference: [5] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 15 </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: The compressed rows and columns are then released. We generated traces from the D2 algorithm for a set of matrices from the Harwell/Boeing sparse matrix test collection <ref> [5] </ref>. These matrices come from a wide range of real problems in scientific computing. The two matrices we use are gemat11, an order 4929 matrix with 33108 nonzeros, and gre-1107, an order 1107 matrix with 5664 nonzeros.
Reference: [6] <author> I.S. Duff, A.M. Erisman, and J.K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1986. </year> <month> 10 </month>
Reference-contexts: One common data structure stores a matrix as a set of compressed sparse vectors, one for each row and/or column of the matrix <ref> [6] </ref>: a compressed vector y (1 : : : n nz ) holds the n nz nonzero values of a full vector x (1 : : : n), and a corresponding integer index vector i (1 : : : n nz ) holds the location of the nonzero entries of x,
Reference: [7] <author> C.S. Ellis and T. Olson. </author> <title> Concurrent dynamic storage allocation. </title> <booktitle> In Proceedings of the international Conference on Parallel Processing, </booktitle> <pages> pages 502-511, </pages> <year> 1987. </year>
Reference-contexts: In a free list algorithm [12], the free blocks are linked together in a list, ordered by starting address. Several parallel free list algorithms have been proposed <ref> [16, 1, 7, 8] </ref>. Free list algorithms are much slower than buddy algorithms, which are the subject of this paper. Another concurrent memory manager [11] has been proposed, based on the fast fits memory manager [14, 15].
Reference: [8] <author> R. Ford. </author> <title> Concurrent algorithms for real time memory management. </title> <journal> IEEE Software, </journal> <pages> pages 10-23, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: In a free list algorithm [12], the free blocks are linked together in a list, ordered by starting address. Several parallel free list algorithms have been proposed <ref> [16, 1, 7, 8] </ref>. Free list algorithms are much slower than buddy algorithms, which are the subject of this paper. Another concurrent memory manager [11] has been proposed, based on the fast fits memory manager [14, 15].
Reference: [9] <author> A. Gottlieb and J. Wilson. </author> <title> Using the buddy system for concurrent memory allocation. Ultracomputer System Software Note 6, </title> <institution> Courant Institute, </institution> <year> 1981. </year>
Reference-contexts: Fast fits is slower but more space efficient than buddy algorithms, and are faster but less space efficient than free list algorithms. Gottlieb and Wilson developed concurrent buddy systems that use fetch-and-add to coordinate processors. Their first algorithm <ref> [9, 17] </ref> considers a buddy system to be organized as a tree. A count of the number of blocks of each size that are contained in subtree rooted at a node is stored at each node. Concurrent allocators use this information to navigate the tree.
Reference: [10] <author> A. Gottlieb and J. Wilson. </author> <title> Parallelizing the usual buddy algorithm. Ultracomputer System Software Note 37, </title> <institution> Courant Institute, </institution> <year> 1982. </year>
Reference-contexts: A count of the number of blocks of each size that are contained in subtree rooted at a node is stored at each node. Concurrent allocators use this information to navigate the tree. Their second algorithm <ref> [10, 17] </ref> is a concurrent version of the commonly described buddy algorithm.
Reference: [11] <author> T. Johnson. </author> <title> A concurrent fast-fits memory manager. </title> <type> Technical Report Electronic TR91-009, </type> <note> avail-iable at anonymous ftp site cis.ufl.edu:/cis/tech-reports/tr91/tr91-009.ps.Z, </note> <institution> University of Florida, Dept. of CIS, </institution> <year> 1991. </year>
Reference-contexts: Several parallel free list algorithms have been proposed [16, 1, 7, 8]. Free list algorithms are much slower than buddy algorithms, which are the subject of this paper. Another concurrent memory manager <ref> [11] </ref> has been proposed, based on the fast fits memory manager [14, 15]. The fast fits data structure is a free list linked into a tree, which allows fast allocate and release operations.
Reference: [12] <author> D. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> volume 1. </volume> <publisher> Addison Wesley, </publisher> <year> 1968. </year>
Reference-contexts: Example applications of parallel memory managers are parallel sparse matrix factorization algorithms [2, 4], and buffers for message passing in clustered parallel computers [3]. 1 Most heap memory management algorithms use one of two main methods: free lists and buddy systems. In a free list algorithm <ref> [12] </ref>, the free blocks are linked together in a list, ordered by starting address. Several parallel free list algorithms have been proposed [16, 1, 7, 8]. Free list algorithms are much slower than buddy algorithms, which are the subject of this paper. <p> Because the memory manager is based on the buddy system, it will execute quickly. We test our algorithms by simulating them with a trace-driven simulator, and running the simulator with traces collected from several sparse matrix factorization algorithms. 2 Serial Buddy Memory Manager We use a binary buddy system <ref> [12, 13] </ref> to create a parallel memory manager. In a buddy system, memory blocks are available only as one of several fixed sizes. Each memory block has a buddy, with which it can combine and form a larger size block. The available sizes are determined by the choice of buddies. <p> The release procedure first checks if the released block's buddy is free, and of the correct size (i.e, on list i). The address of a block's buddy can be calculated from the block's size and starting address <ref> [12, 13] </ref>. The release procedure can determine whether the buddy is free and of the same size by examining first word of the buddy block, which contains the free/allocated bit and the size of the buddy, so the level i free list doesn't need to be searched.
Reference: [13] <author> J. L. Peterson and T. A. Norman. </author> <title> Buddy systems. </title> <journal> Communications of the ACM, </journal> <volume> 20(6) </volume> <pages> 421-431, </pages> <year> 1977. </year>
Reference-contexts: Because the memory manager is based on the buddy system, it will execute quickly. We test our algorithms by simulating them with a trace-driven simulator, and running the simulator with traces collected from several sparse matrix factorization algorithms. 2 Serial Buddy Memory Manager We use a binary buddy system <ref> [12, 13] </ref> to create a parallel memory manager. In a buddy system, memory blocks are available only as one of several fixed sizes. Each memory block has a buddy, with which it can combine and form a larger size block. The available sizes are determined by the choice of buddies. <p> The release procedure first checks if the released block's buddy is free, and of the correct size (i.e, on list i). The address of a block's buddy can be calculated from the block's size and starting address <ref> [12, 13] </ref>. The release procedure can determine whether the buddy is free and of the same size by examining first word of the buddy block, which contains the free/allocated bit and the size of the buddy, so the level i free list doesn't need to be searched.
Reference: [14] <author> C. J. Stephenson. </author> <title> Fast fits: New methods for dynamic storage allocation. </title> <booktitle> In Proc. of the Ninth ACM Symposium of OPerating System Principles, </booktitle> <pages> pages 30-32, </pages> <year> 1983. </year>
Reference-contexts: Several parallel free list algorithms have been proposed [16, 1, 7, 8]. Free list algorithms are much slower than buddy algorithms, which are the subject of this paper. Another concurrent memory manager [11] has been proposed, based on the fast fits memory manager <ref> [14, 15] </ref>. The fast fits data structure is a free list linked into a tree, which allows fast allocate and release operations. Fast fits is slower but more space efficient than buddy algorithms, and are faster but less space efficient than free list algorithms.
Reference: [15] <author> C. J. Stephenson. </author> <title> Fast fits: New methods for dynamic storage allocation. </title> <type> Technical report, </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <year> 1983. </year>
Reference-contexts: Several parallel free list algorithms have been proposed [16, 1, 7, 8]. Free list algorithms are much slower than buddy algorithms, which are the subject of this paper. Another concurrent memory manager [11] has been proposed, based on the fast fits memory manager <ref> [14, 15] </ref>. The fast fits data structure is a free list linked into a tree, which allows fast allocate and release operations. Fast fits is slower but more space efficient than buddy algorithms, and are faster but less space efficient than free list algorithms.
Reference: [16] <author> H. Stone. </author> <title> Parallel memory allocation using the fetch-and-add instruction. </title> <type> Technical Report RC 9674, </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <year> 1982. </year>
Reference-contexts: In a free list algorithm [12], the free blocks are linked together in a list, ordered by starting address. Several parallel free list algorithms have been proposed <ref> [16, 1, 7, 8] </ref>. Free list algorithms are much slower than buddy algorithms, which are the subject of this paper. Another concurrent memory manager [11] has been proposed, based on the fast fits memory manager [14, 15].
Reference: [17] <author> J. Wilson. </author> <title> Operating System Data Structures for Shared-memory MIMD Machines with Fetch-and-add. </title> <type> PhD thesis, </type> <institution> NYU, </institution> <year> 1988. </year>
Reference-contexts: Fast fits is slower but more space efficient than buddy algorithms, and are faster but less space efficient than free list algorithms. Gottlieb and Wilson developed concurrent buddy systems that use fetch-and-add to coordinate processors. Their first algorithm <ref> [9, 17] </ref> considers a buddy system to be organized as a tree. A count of the number of blocks of each size that are contained in subtree rooted at a node is stored at each node. Concurrent allocators use this information to navigate the tree. <p> A count of the number of blocks of each size that are contained in subtree rooted at a node is stored at each node. Concurrent allocators use this information to navigate the tree. Their second algorithm <ref> [10, 17] </ref> is a concurrent version of the commonly described buddy algorithm.
References-found: 17

