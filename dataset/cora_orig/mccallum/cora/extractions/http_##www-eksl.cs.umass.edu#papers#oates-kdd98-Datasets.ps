URL: http://www-eksl.cs.umass.edu/papers/oates-kdd98-Datasets.ps
Refering-URL: http://eksl-www.cs.umass.edu/publications.html
Root-URL: 
Email: foates, jenseng@cs.umass.edu  
Title: Large Datasets Lead to Overly Complex Models: an Explanation and a Solution  
Author: Tim Oates and David Jensen 
Keyword: large datasets, overfitting, randomization, decision trees, rule learning  
Address: Box 34610 LGRC  Amherst, MA 01003-4610  
Affiliation: Experimental Knowledge Systems Laboratory Department of Computer Science  University of Massachusetts  
Abstract: This paper explores unexpected results that lie at the intersection of two common themes in the KDD community: large datasets and the goal of building compact models. Experiments with many different datasets and several model construction algorithms (including tree learning algorithms such as c4.5 with three different pruning methods, and rule learning algorithms such as c4.5rules and ripper) show that increasing the amount of data used to build a model often results in a linear increase in model size, even when that additional complexity results in no significant increase in model accuracy. Despite the promise of better parameter estimation held out by large datasets, as a practical matter, models built with large amounts of data are often needlessly complex and cumbersome. In the case of decision trees, the cause of this pathology is identified as a bias inherent in several common pruning techniques. Pruning errors made low in the tree, where there is insufficient data to make accurate parameter estimates, are propagated and magnified higher in the tree, working against the accurate parameter estimates that are made possible there by abundant data. We propose a general solution to this problem based on a statistical technique known as randomization testing, and empirically evaluate its utility. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cohen, P. R. </author> <year> 1995a. </year> <title> Empirical Methods for Artificial Intelligence. </title> <publisher> The MIT Press. </publisher>
Reference-contexts: For each combination of dataset and algorithm, plots of model size and accuracy as a function of dataset size were generated via incremental cross-validation <ref> (Cohen 1995a) </ref>. Standard 10-fold cross-validation builds a model on 90% of the data and tests its accuracy on the remaining 10%, averaging the results over ten disjoint subsets of equal size held out for testing.
Reference: <author> Cohen, W. W. </author> <year> 1995b. </year> <title> Fast effective rule induction. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle>
Reference-contexts: The datasets are census-income (32,561 instances), led-24 (30,000 instances) and letter-recognition (20,000 instances). The algorithms include one decision tree learner, c4.5 (Quinlan 1993), and two rule learners, c4.5rules (Quinlan 1993) and ripper <ref> (Cohen 1995b) </ref>. Various decision tree pruning techniques have been developed with the explicit goal of eliminating excess structure in trees.
Reference: <author> Edgington, E. S. </author> <year> 1995. </year> <title> Randomization Tests. </title> <publisher> Marcel Dekker. </publisher>
Reference: <author> Jensen, D., and Cohen, P. R. </author> <year> 1997. </year> <title> Multiple comparisons in induction algorithms. </title> <note> Submitted to Machine Learning. </note>
Reference-contexts: size of the model built on the full dataset to the size of the model built on the smallest amount of data needed to achieve approximately maximum classification accuracy. 1 Additional details on the experimental method and results for 19 datasets and 4 different pruning techniques can be found in <ref> (Oates & Jensen 1997) </ref>. 4 r 2 census led-24 letter c4.5/ebp 0.98 1.00 1.00 c4.5/mdl 0.99 1.00 0.97 c4.5rules 0.91 0.97 0.98 ripper 0.86 0.92 0.83 Size Factor census led-24 letter c4.5/ebp 3.8 1.2 13.3 c4.5/mdl 4.2 19.7 1.3 c4.5rules 2.1 3.9 1.3 ripper 3.5 4.6 1.1 Table 1: Summaries of <p> Other scoring functions may treat high scores as better and prune nodes when S L (N ) S T (N ). (The effects of consistently choosing the maximum or minimum of a set of values on the sampling distribution is discussed in detail in <ref> (Jensen & Cohen 1997) </ref>.) Third, the score of a node as a tree (S T ) is directly dependent on the scores of all of that node's children.
Reference: <author> Jensen, D. </author> <year> 1991. </year> <title> Knowledge discovery through induction with randomization testing. </title> <booktitle> In Proceedings of the 1991 Knowledge Discovery in Databases Workshop, </booktitle> <pages> 148-159. </pages>
Reference: <author> Jensen, D. </author> <year> 1992. </year> <title> Induction with Randomization Testing: Decision-Oriented Analysis of Large Data Sets. </title> <type> Ph.D. Dissertation, </type> <institution> Washington University. </institution>
Reference: <author> Oates, T., and Jensen, D. </author> <year> 1997. </year> <title> The effects of training set size on decision tree complexity. </title> <booktitle> In Proceedings of The Fourteenth International Conference on Machine Learning, </booktitle> <pages> 254-262. </pages>
Reference-contexts: size of the model built on the full dataset to the size of the model built on the smallest amount of data needed to achieve approximately maximum classification accuracy. 1 Additional details on the experimental method and results for 19 datasets and 4 different pruning techniques can be found in <ref> (Oates & Jensen 1997) </ref>. 4 r 2 census led-24 letter c4.5/ebp 0.98 1.00 1.00 c4.5/mdl 0.99 1.00 0.97 c4.5rules 0.91 0.97 0.98 ripper 0.86 0.92 0.83 Size Factor census led-24 letter c4.5/ebp 3.8 1.2 13.3 c4.5/mdl 4.2 19.7 1.3 c4.5rules 2.1 3.9 1.3 ripper 3.5 4.6 1.1 Table 1: Summaries of
Reference: <author> Quinlan, J. R., and Rivest, R. </author> <year> 1989. </year> <title> Inferring decision trees using the minimum description length principle. </title> <booktitle> Information and Computation 80 </booktitle> <pages> 227-248. </pages>
Reference-contexts: Various decision tree pruning techniques have been developed with the explicit goal of eliminating excess structure in trees. Therefore, we ran c4.5 with three different pruning algorithms: error-based pruning (ebp the c4.5 default) (Quinlan 1993), reduced error pruning (rep) (Quinlan 1987), and minimum description length pruning (mdl) <ref> (Quinlan & Rivest 1989) </ref>. For each combination of dataset and algorithm, plots of model size and accuracy as a function of dataset size were generated via incremental cross-validation (Cohen 1995a).
Reference: <author> Quinlan, J. R. </author> <year> 1987. </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies 27 </journal> <pages> 221-234. </pages>
Reference-contexts: Various decision tree pruning techniques have been developed with the explicit goal of eliminating excess structure in trees. Therefore, we ran c4.5 with three different pruning algorithms: error-based pruning (ebp the c4.5 default) (Quinlan 1993), reduced error pruning (rep) <ref> (Quinlan 1987) </ref>, and minimum description length pruning (mdl) (Quinlan & Rivest 1989). For each combination of dataset and algorithm, plots of model size and accuracy as a function of dataset size were generated via incremental cross-validation (Cohen 1995a).
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann. </publisher> <pages> 10 </pages>
Reference-contexts: The datasets are census-income (32,561 instances), led-24 (30,000 instances) and letter-recognition (20,000 instances). The algorithms include one decision tree learner, c4.5 <ref> (Quinlan 1993) </ref>, and two rule learners, c4.5rules (Quinlan 1993) and ripper (Cohen 1995b). Various decision tree pruning techniques have been developed with the explicit goal of eliminating excess structure in trees. Therefore, we ran c4.5 with three different pruning algorithms: error-based pruning (ebp the c4.5 default) (Quinlan 1993), reduced error pruning <p> The datasets are census-income (32,561 instances), led-24 (30,000 instances) and letter-recognition (20,000 instances). The algorithms include one decision tree learner, c4.5 <ref> (Quinlan 1993) </ref>, and two rule learners, c4.5rules (Quinlan 1993) and ripper (Cohen 1995b). Various decision tree pruning techniques have been developed with the explicit goal of eliminating excess structure in trees. Therefore, we ran c4.5 with three different pruning algorithms: error-based pruning (ebp the c4.5 default) (Quinlan 1993), reduced error pruning (rep) (Quinlan 1987), and minimum description length <p> decision tree learner, c4.5 <ref> (Quinlan 1993) </ref>, and two rule learners, c4.5rules (Quinlan 1993) and ripper (Cohen 1995b). Various decision tree pruning techniques have been developed with the explicit goal of eliminating excess structure in trees. Therefore, we ran c4.5 with three different pruning algorithms: error-based pruning (ebp the c4.5 default) (Quinlan 1993), reduced error pruning (rep) (Quinlan 1987), and minimum description length pruning (mdl) (Quinlan & Rivest 1989). For each combination of dataset and algorithm, plots of model size and accuracy as a function of dataset size were generated via incremental cross-validation (Cohen 1995a).
References-found: 10

